0|244|Public
50|$|After {{check is}} completed, users receive {{plagiarism}} reports which include links to any <b>duplicate</b> content <b>detected</b> by the algorithm.|$|R
5000|$|KSM {{performs}} the memory sharing by scanning {{through the main}} memory and finding <b>duplicate</b> pages. Each <b>detected</b> <b>duplicate</b> pair is then merged into a single page, and mapped into both original locations. The page is also marked as [...] "copy-on-write" [...] (COW), so the kernel will automatically separate them again should one process modify its data.|$|R
5000|$|A {{number of}} {{different}} algorithms have been proposed to <b>detect</b> <b>duplicate</b> code. For example: ...|$|R
40|$|Illustrations: 77 {{reproductions of}} drawings, paintings, etc. by Frederic Remington, {{comprising}} 40 plates (8 colored) and 67 illustrations. Pages 401 - 411 (sig. <b>26)</b> in <b>duplicate.</b> Published also with title: The California and Oregon trail. Mode of access: Internet. Spencer copy: Binding, publisher's, of brown leather decorated {{in gold and}} colors...|$|R
50|$|Analytics offers {{extensive}} {{analysis and}} statistical functions about search behavior and document access. Data quality {{can be improved}} by <b>detecting</b> <b>duplicates.</b>|$|R
5000|$|Tools for searching large EGL code bases, {{comparing}} individual EGL {{files for}} changes, and <b>detecting</b> <b>duplicated</b> code {{are available from}} Semantic Designs ...|$|R
40|$|Abstract <b>Detecting</b> <b>duplicates</b> in data streams is an {{important}} problem that has {{a wide range of}} applications. In general, precisely <b>detecting</b> <b>duplicates</b> in an unbounded data stream is not feasible in most streaming scenarios, and, on the other hand, the elements in data streams are always time sensitive. These make it particular significant approximately <b>detecting</b> <b>duplicates</b> among newly arrived elements of a data stream within a fixed time frame. In this paper, we present a novel data structure, Decaying Bloom Filter (DBF), {{as an extension of the}} Counting Bloom Filter, that effectively removes stale elements as new elements continuously arrive over sliding windows. On the DBF basis we present an efficient algorithm to approximately <b>detect</b> <b>duplicates</b> over sliding windows. Our algorithm may produce false positive errors, but not false negative errors as in many previous results. We analyze the time complexity and detection accuracy, and give a tight upper bound of false positive rate. For a given space G bits and sliding window size W, our algorithm has an amortized time complexity of O(G/W). Both analytical and experimental results on synthetic data demonstrate that our algorithm is superior in both execution time and detection accuracy to the previous results...|$|R
50|$|The End-to-End Identifier is an {{unsigned}} 32-bit integer field (in network byte order) {{that is used}} to <b>detect</b> <b>duplicate</b> messages {{along with}} the combination of the Origin-Host AVP.|$|R
40|$|Duplicate message {{detection}} {{is at the}} heart of several current approaches to filtering out unwanted commercial electronic mail ("spam"). It uses the spammer's most annoying characteristic, that of sending out lots of copies of the same message relatively indiscriminately, against him. Examples of techniques employing this idea are filtering by duplicate detection, collaborative filtering, and manual filtering. However, in this paper I show that a relatively simple technique, list splitting, is an effective countermeasure to all these approaches. Even though the <b>duplicate</b> <b>detecting</b> techniques can be made more effective through increased effort on the part of the user, list-splitting can be made even more effective with at worst logarithmically more effort; this tips the arms race in favor of the spammer, leading me to conclude we should look at other anti-spam techniques than those based on duplicate detection for long term, large scale solutions to the spam problem. Keywords: spam, el [...] ...|$|R
30|$|The {{classification}} of aggregator samples in the aggregation pool proactively prevents duplicate outcomes that increase communication overhead. The mutual membership checks reactively <b>detect</b> <b>duplicate</b> outcomes not <b>detected</b> during classification due to false positives. Mutual membership checks guarantee highly accurate aggregates, {{especially in the}} case of duplicate-sensitive aggregation functions such as SUMMATION, without introducing additional communication cost. The performance of RANDOM shows the large communication cost that duplicate outcomes cause and the large savings achieved by EXPLOITATION and UPDATE. Other future work concerns the evaluation of DIAS and its applications in various network conditions, such as churn (Kennedy et al. 2009) and latency.|$|R
40|$|Analysis (2 of 4), 1990 - 1991 June <b>26.</b> <b>Duplicates</b> (Some may be superceded). PART OF A SERIES: To {{ascertain}} misconduct on {{the part}} of LAPD officers, the Independent Commission set out to probe databases maintained by the LAPD related to four areas: personnel complaints, use of force, officer-involved shootings (OIS), and the use of dogs (K- 9). This task was delegated to the firm Freeman & Mills, which was responsible for obtaining these databases from the LAPD, converting pertinent data into useable formats, creating additional databases of additional information, and analyzing its findings. The series contains items related to Freeman & Mills' work product, including numerous analyses of use-of-force, OIS, and K- 9 data; weekly complaint summary printouts; graphs, charts, tables, and other visual representations of this data; and computer directory lists that provide the names and locations of files within the LAPD's databases. At the end of the series is a draft copy of the Independent Commission's final report, which incorporates many of the aforementioned data and figures...|$|R
50|$|By September 2011 {{the network}} had been re-branded as Wessex Red and had grown from three routes {{requiring}} five buses in 2007 to eight routes requiring <b>26,</b> with regular <b>duplicate</b> vehicles in operation particularly on route U4.|$|R
40|$|In this report, {{we present}} a system that can automatical-ly detect and 3 D {{reconstruct}} the target objects. After the reconstruction, users can edit the image by scaling, trans-lating, rotating and deleting the target objects. The system consists of three parts. The first part is the detection part. We use HOG (Histogram of Gradient) template matching for detection and adopt sliding window strategy with multi-ple scales and positions. We allow a small deformation of the HOG template. Non-maximum suppression is applied to remove <b>duplicate</b> <b>detected</b> windows. The second part is the 3 D transformable model matching part. We will search for the optimal 3 D models that match the edge maps of the detected target objects. The third part is the image editing part, the users can rotate, translate and scale multiple tar-get objects detected in the image using simple operations of their mouse. The whole system will reduce the effort the user need to take, especially in the situation that the user need to 3 D reconstruct near-duplicate objects in multiple images. 1...|$|R
40|$|The {{advancement}} of Internet technology brings an overwhelming deluge of information. However, before this {{information can be}} effectively used, it must first be collected, filtered, processed, analyzed, and finally, presented in a meaningful manner. One such challenge is that of <b>detecting</b> <b>duplicate</b> documents. With the advent of peer-to-peer (P 2 P) computing, information sources or repositories are no longer centralized. Consequently, this further exasperates the challenges of <b>detecting</b> <b>duplicate</b> documents. We propose a multi-agent peer-to-peer (P 2 P) architecture {{that is based on}} a gladiator metaphor to address this challenge. This architecture allows for a divide and conquer approach for complex information retrieval problems in dynamic P 2 P networks. 1...|$|R
40|$|Abstract—We {{describe}} a {{system based on}} exact-duplicate matching for detecting and localizing TV commercials in a video stream, clustering the exact <b>duplicates,</b> and <b>detecting</b> <b>duplicate</b> exact-duplicate clusters across video streams. A two-stage temporal recurrence hashing algorithm {{is used for the}} detection, localization, and clustering. The algorithm is fully unsupervised, generic, and ultrahigh speed. Another algorithm is used to integrate the video and audio streams to achieve higher performance extraction. Its sequence- and frame-level accuracies in testing were respectively 98. 1 % and 97. 4 %. A third algorithm uses a new bag-of-fingerprints model to <b>detect</b> <b>duplicate</b> exact-duplicate clusters across multiple streams. It is robust against decoding errors. The contributions include: 1) fully unsupervised detection, extraction, and matching of exact duplicates; 2) more generic commercial detection than with the knowledge-based techniques; 3) ultrahigh-speed processing, which detected the TV commercials from a 1 -month video stream in less than 42 minutes, which is more than ten times faster than with state-of-the-art algorithms; 4) more generic operation in terms of signal input, the performance of which is consistent between video and audio streams. Testing using a video database containing a 10 -hour, a 1 -month, and a 5 -year video stream comprehensively demonstrated the effectiveness and efficiency of this system. Index Terms—Content-based retrieval, data mining, search methods, video signal processing. I...|$|R
40|$|In recent years, near <b>duplicate</b> image <b>detecting</b> {{becomes one}} of the most {{important}} problems in image retrieval, and it is widely used in many application fields, such as copyright violations and detecting forged images. Therefore, in this paper, we propose a novel approach to automatically <b>detect</b> near <b>duplicate</b> images based on visual word model. SIFT descriptors are utilized to represent image visual content which is an effective method in computer vision research field to detect local features of images. Afterwards, we cluster the SIFT features of a given image into several clusters by the K-means algorithm. The centroid of each cluster is regarded as a visual word, and all the centroids are used to construct the visual word vocabulary. To reduce the time cost of near <b>duplicate</b> image <b>detecting</b> process, locality sensitive hashing is utilized to map high-dimensional visual features into low-dimensional hash bucket space, and then the image visual features are converted to a histogram. Next, for a pair of images, we present a local feature based image similarity estimating method by computing histogram distance, and then near duplicate images can be detected. Finally, a series of experiments are constructed to make performance evaluation, and related analyses about experimental results are also give...|$|R
40|$|Systematic Literature Review (SLR) is a {{means to}} {{synthesize}} relevant and high quality studies related to a specific topic or research questions. In the Primary Selection stage of an SLR, the selection of studies is usually performed manually by reading title, abstract and keywords of each study. In the last years, the number of published scientific studies has grown increasing the effort to perform this sort of reviews. In this paper, we proposed strategies to <b>detect</b> non-papers and <b>duplicated</b> references in results exported by search engines, and strategies to rank the references in decreasing order of importance for an SLR, regarding the terms in the search string. These strategies are based on Information Retrieval techniques. We implemented the strategies and carried out an experimental evaluation of their applicability using two real datasets. As results, the strategy to detect non-papers presented 100 % of precision and 50 % of recall; the strategy to <b>detect</b> <b>duplicates</b> <b>detected</b> more <b>duplicates</b> than the manual inspection; {{and one of the}} strategies to rank relevant references presented 50 % of precision and 80 % of recall. Therefore, the results show that the proposed strategies can minimize the effort in the Primary Selection stage of an SLR...|$|R
40|$|Several {{important}} aspects of the use of molecular markers in cassava germplasm characterization and in plant breeding programs are indicated. Basic concepts considered include protein and DNA electrophoresis. In the case of cassava, isoenzyme electrophoresis was applied to confirm <b>duplicates</b> <b>detected</b> in CIAT`s cassava collection through morphological characteristics and thus identify new duplicates. In initial evaluations, the isoenzyme alpha-beta- esterase was used and 16 bands were identified in 100 cassava clones; these same bands have been identified in 1200 cassava clones assessed to date. Therefore 2 individuals may possibly be duplicates when they have the same morphological or physiological characteristics and the same electrophoretic pattern of the isoenzyme alpha-beta-esterase. These data should be confirmed with new electrophoretic markers or by direct genome evaluation using the RFLP technique. Indications are given on how to determine the relative electrophoretic mobility (REM) of each band within a zymogram; the duplicity of 2 individuals can then be determined. Band intensity gives an idea of the concn. and catalytic activity of the enzyme in crude tissue extract, but again other markers or RFLP should be used to confirm results. The applications and advantages of RFLPs are analyzed as well as the use of genetic maps and the analysis of multigenic characteristics. (CIAT...|$|R
5000|$|Silhouette - One of the Master's minions. He is a supervillain {{who could}} absorb victims into his [...] "shadow box" [...] while {{transforming}} their shadows into duplicates under his control. His <b>duplicates</b> could be <b>detected</b> {{by the fact}} that they cast no shadows of their own.|$|R
40|$|Today’s {{important}} {{task is to}} clean data in data warehouses which has complex hierarchical structure. This is possibly done by <b>detecting</b> <b>duplicates</b> in large databases {{to increase the efficiency}} of data mining and to make it effective. Recently new algorithms are proposed that consider relations in a single table; hence by comparing records pairwise they can easily find out duplications. But now a day the data is being stored in more complex and semi-structured or hierarchical structure and the problem arose is how to <b>detect</b> <b>duplicates</b> on XML data. Also due to differences between various data models, the algorithms which are for single relations cannot be applied on XML data. The objective of this project is to <b>detect</b> <b>duplicates</b> in hierarchical data which contain textual data and multimedia data like images, audio and video. It also focuses on eliminating the duplicates by using elimination technique such as delete. Here Bayesian network is used with modified pruning algorithm for duplicate detection, and experiments are performed on both artificial and real world datasets. The new XMLMultiDup method is able to perform duplicate detection with high efficiency and effectiveness on multimedia datasets. This method compares each level of XML tree from root to the leaves computing probabilities of similarity by assigning weights. It goes through the comparison of structure, each descendant of both datasets and find duplicates despite difference in data...|$|R
50|$|Document layout {{analysis}} is {{the union of}} geometric and logical labeling. It is typically performed before a document image is sent to an OCR engine, {{but it can be}} used also to <b>detect</b> <b>duplicate</b> copies of the same document in large archives, or to index documents by their structure or pictorial content.|$|R
40|$|Abstract: It is {{generally}} assumed that fee paying bibliographic databases guarantee {{the quality of}} the data they sell. This technical report shows that this assertion can be challenged. It demonstrates a software method of <b>detecting</b> <b>duplicate</b> and fake publications. The charged-for services (such as IEEE Xplore) accept and index these kinds of publications...|$|R
5|$|Early {{production}} Type 26s have no external markings. The revolvers have markings {{that indicate}} they were arsenal re-worked and {{believed to have}} been produced in late 1893 or early 1894 before official adoption. It is possible that around 300 revolvers with no external marking were produced. No known examples of Type <b>26</b> revolvers have <b>duplicate</b> serial numbers.|$|R
40|$|Wild annual Cicer gene pools contain {{valuable}} germplasm for chickpea improvement programs. Previous research {{showed that}} duplication might exist in accessions collected from these gene pools, which would hinder chickpea breeding and related research. AFLP (amplified fragment length polymorphism) markers {{were used to}} fingerprint the world collections of the primary and secondary gene pools including C. reticulatum Lad., C. bijugum K. H. Rech., C. judaicum Boiss. and C. pinnatifidum Jaub. et Sp. <b>Duplicates</b> were <b>detected</b> {{in a total of}} 24 accessions in both the gene pools, highlighting the necessity to fingerprint the germplasm. Genotypic difference was detected as gene pool specific, species specific and accession specific AFLP markers. These were developed into fingerprinting keys for accession identification between and within species and gene pools. Use of AFLP markers to <b>detect</b> <b>duplicates</b> and to identify accessions is a reliable method which will assist in the characterisation and use of wild annual Cicer germplasm in chickpea improvement programs. We recommend the procedure presented in this paper as a standard approach for the precise genetic identification and characterisation of future world collections of wild Cicer, to keep germplasm integrity and to benefit chickpea breeding and related research programs...|$|R
40|$|The {{presentation}} {{is focused on}} a comparative analysis of systems for <b>detecting</b> <b>duplicates</b> (called anti-plagiarism systems) suitable for repositories of ETDs in the Czech Republic. The paper describes the testing process and results. These results can help to decide, which system will be suitable as a anti-plagiarism tool for the control of theses {{and other types of}} gray literature...|$|R
40|$|Most of the {{existing}} forensic methods aimed at <b>detecting</b> <b>duplicated</b> regions in images are sensitive to geometrical changes in the replicated areas. Consequently, a simple reflection/rotation of the copied region {{could be used to}} not only suit the scene of the image, but also hinder its proper detection. In this paper, we propose a novel forensic method to <b>detect</b> <b>duplicated</b> regions, even when the copied portion have experienced reflection, rotation or scaling. To achieve this, overlapping blocks of pixels are re-sampled into log-polar coordinates, and then summed along the angle axis, to obtain a one-dimensional descriptor invariant to reflection and rotation. Moreover, scaling in rectangular coordinates results in a simple translation of the descriptor. This approach allows us to perform an efficient search of similar blocks, by means of the correlation coefficient of its Fourier magnitudes. Results are presented to demonstrate the effectiveness of the proposed method. 1...|$|R
40|$|Purpose: Effective use {{of routine}} {{data to support}} {{integrated}} chronic disease management (CDM) and population health is dependent on underlying data quality (DQ) and, for cross system use of data, semantic interoperability. An ontological approach to DQ is a potential solution but {{research in this area}} is limited and fragmented. Objective: Identify mechanisms, including ontologies, to manage DQ in integrated CDM and whether improved DQ will better measure health outcomes. Methods: A realist review of English language studies (January 2001 –March 2011) which addressed data quality, used ontology-based approaches and is relevant to CDM. Results: We screened 245 papers, excluded <b>26</b> <b>duplicates,</b> 135 on abstract review and 31 on full-text review; leaving 61 papers for critical appraisal. Of the 33 papers that examined ontologies in chronic disease management, 13 defined data quality and 15 used ontologies for DQ. Most saw DQ as a multidimensional construct, the most used dimensions being completeness, accuracy, correctness, consistency and timeliness. The majority of studies reported tool design and development (80 %), implementation (23 %), and descriptive evaluations (15 %). Ontological approaches were used to address semantic interoperability, decision support, flexibility of information management and integration/linkage, and complexity of information models. Conclusion: DQ lacks a consensus conceptual framework and definition. DQ and ontological research is relatively immature with little rigorous evaluation studies published. Ontology-based applications could support automated processes to address DQ and semantic interoperability in repositories of routinely collected data to deliver integrated CDM. We advocate moving to ontology-based design of information systems to enable more reliable use of routine data to measure health mechanisms and impacts...|$|R
40|$|Les rapports de {{recherche}} du LIG - ISSN: 2105 - 0422 It {{is generally}} assumed that fee paying bibliographic databases guarantee {{the quality of}} the data they sell. This technical report shows that this assertion can be challenged. It demonstrates a software method of <b>detecting</b> <b>duplicate</b> and fake publications. The charged-for services (such as IEEE Xplore) accept and index these kinds of publications...|$|R
40|$|Program {{often have}} a lot of {{duplicated}} code, which makes both understanding and maintenance more difficult. This problem can be alleviated by <b>detecting</b> <b>duplicate</b> code, extracting it into a separate new procedure, and replacing all the clones by call to the new procedure. This paper describes the design and initial implementation of a tool that finds clones and displays to the programmer...|$|R
40|$|It is {{generally}} assumed that hashing {{is essential to}} solve many language process-ing problems efficiently; e. g., symbol table formation and maintenance, grammar manipulation, basic block optimization, and global optimization. This paper questions this assumption, and initiates development of an efficient alternative compiler methodology without hashing or sorting. The methodology rests on efficient solutions to the basic problem of <b>detecting</b> <b>duplicate</b> values in a multi-set, which we call multiset discrimination. Paige and Tarjan [22] gav e an efficient solution to multiset discrimination for <b>detecting</b> <b>duplicate</b> elements occurring in a multiset of varying length strings. The technique was used to develop an improved algorithm for lexicographic sorting, whose importance stems largely from its use in solving a variety of iso-morphism problems [2]. The current paper and a related paper [23] show that full lexicographic sorting is not needed to solve these isomorphism problems, {{because they can be}} solved more efficiently using straightforward extensions t...|$|R
40|$|The {{problem of}} finding {{relevant}} documents {{has become much}} more prominent due to the presence of duplicate data on the WWW. This redundancy in results increases the users’ seek time to find the desired information within the search results, while in general most users just want to cull through tens of result pages to find new/different results. The identification of similar or near-duplicate pairs in a large collection is a significant problem with wide-spread applications. Another contemporary materialization of the problem is the efficient identification of near-duplicate Web pages. This is certainly challenging in the web-scale due to the voluminous data. Therefore, a mechanism needs to be introduced for <b>detecting</b> <b>duplicate</b> data so that relevant search results can be provided to the user. In this paper, architecture is being proposed that introduces methods that run online as well as offline on the basis of favored and disfavored user queries to <b>detect</b> <b>duplicates</b> andnear duplicates...|$|R
40|$|Bug {{localisation}} {{techniques are}} proposed {{as a method}} to reduce the time developers spend on maintenance, allowing them to quickly and source code relevant to a bug. Some techniques are based on information retrieval methods, treating the source code as a corpus and the bug report as a query. While these have shown success, there remain a number of little-exploited additional sources of information which could enhance the techniques, including the textual similarity between bug reports themselves. Based on successful results in <b>detecting</b> <b>duplicate</b> bug reports, this work asks: if duplicate bugs reports, which by denition are xed in the same source location, can be detected {{through the use of}} similar language, can bugs which are in the same location but not <b>duplicates</b> be <b>detected</b> in the same way? A technique using this information is implemented and evaluated on 372 bugs across 4 projects, and is found to improve performance on all projects. In particular, the technique increases the number of bugs where the rst relevant method presented to developers is the rst result from 6 to 27, and those in the top- 10 from 50 to 57, showing that it can be successfully used to enhance existing bug localisation techniques...|$|R
30|$|As {{previously}} discussed, replay {{attacks are}} one of the threats to data integrity that may be present in network virtualization environments. The addition of unique identifiers inside encrypted messages makes it possible to <b>detect</b> <b>duplicated</b> messages, and therefore, replay attacks. For this purpose, the architecture proposed by Fernandes and Duarte [26, 31] inserts timestamps inside encrypted messages {{in order to ensure that}} messages are non-reproducible.|$|R
40|$|Code {{duplication}} in {{a program}} can make understanding and maintenance difficult. The problem can be reduced by <b>detecting</b> <b>duplicated</b> code, refactoring it into a separate procedure, and replacing all the clones by appropriate calls to the new procedure. In this paper, we report on a confirmatory replication of a tool {{that was used to}} detect such refactorable clones based on program dependence graphs and program slicing...|$|R
40|$|Code {{duplication}} {{is considered}} as bad practice that complicates the maintenance {{and evolution of}} software. <b>Detecting</b> <b>duplicated</b> code is a difficult task {{because of the large}} amount of data to be checked and the fact that aprioriit is unknown which code part has been duplicated. In this paper, we present a tool called DUPLOC that supports code duplication detection in a visual and exploratory or an automatic way...|$|R
40|$|This article {{describes}} {{the development of a}} free test collection for Chinese text categorization. A novel retrieval-based approach was developed to <b>detect</b> <b>duplicates</b> and label inconsistency in this corpus and in Reuters- 21578 for comparison. The method was able to detect certain types of similar and/or duplicated documents that were overlooked by an alternative repetition-based method [1]. Experiments showed that effectiveness was not affected by the confusing documents...|$|R
