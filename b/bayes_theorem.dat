398|28|Public
25|$|Lastly <b>Bayes</b> <b>theorem</b> is coherent. It is {{considered}} the most appropriate way to update beliefs by welcoming the incorporation of new information, as is seen through the probability distributions (see Savage and De Finetti). This is further complemented {{by the fact that}} Bayes inference satisfies the likelihood principle, which states that models or inferences for datasets leading to the same likelihood function should generate the same statistical information.|$|E
25|$|Bayes {{methods are}} more cost {{effective}} than the traditional frequentist take on marketing research and subsequent decision making. The probability can be assessed from a degree of belief before and after accounting for evidence, instead of calculating the probabilities of a certain decision by carrying out {{a large number of}} trials with each one producing an outcome from a set of possible outcomes. The planning and implementation of trials to see how a decision impacts in the ‘field’ e.g. observing consumers reaction to a relabeling of a product, is time consuming and costly, a method many firms cannot afford. In place of taking the frequentist route in aiming for a universally acceptable conclusion through iteration, it is sometimes more effective to take advantage of all the information available to the firm to work out the ‘best’ decision at the time, and then subsequently when new knowledge is obtained, revise the posterior distribution to be then used as the prior, thus the inferences continue to logically contribute to one another based on <b>Bayes</b> <b>theorem.</b>|$|E
25|$|The Ensemble Kalman Filter (EnKF) is a Monte Carlo {{implementation}} of the Bayesian update problem: given a probability density function (pdf) {{of the state of}} the modeled system (the prior, called often the forecast in geosciences) and the data likelihood, the <b>Bayes</b> <b>theorem</b> is used to obtain the pdf after the data likelihood has been taken into account (the posterior, often called the analysis). This is called a Bayesian update. The Bayesian update is combined with advancing the model in time, incorporating new data from time to time. The original Kalman Filter assumes that all pdfs are Gaussian (the Gaussian assumption) and provides algebraic formulas for the change of the mean and the covariance matrix by the Bayesian update, as well as a formula for advancing the covariance matrix in time provided the system is linear. However, maintaining the covariance matrix is not feasible computationally for high-dimensional systems. For this reason, EnKFs were developed. EnKFs represent the distribution of the system state using a collection of state vectors, called an ensemble, and replace the covariance matrix by the sample covariance computed from the ensemble. The ensemble is operated with as if it were a random sample, but the ensemble members are really not independent – the EnKF ties them together. One advantage of EnKFs is that advancing the pdf in time is achieved by simply advancing each member of the ensemble. For a survey of EnKF and related data assimilation techniques, see G. Evensen.|$|E
40|$|Words {{can have}} more than one {{distinct}} meaning called as polysemous words. This paper concentrates on Word Sense Disambiguation (WSD) which refers to the resolution of lexical ambiguity that arises when a given word has several different meanings. The paper presents a hybrid approach for this problem based on the basic principle by Yarowsky’s unsupervised algorithm for WSD. It also employs Naïve <b>Baye’s</b> <b>theorem</b> to find the likelihood ratio of the sense in the given context. This way, the approach preserves the advantage of principles of Yarowsky’s (one sense per discourse and one sense per collocation) and utilizes <b>Baye’s</b> <b>theorem</b> for the better performance of the system. The seed/sense selection can be done either manually or automatically to find the local or global dependency of a given sense in a given window. To find the local dependency, the system uses the definitions provided by the dictionary (Word Net) for the target word whereas the global dependency is determined on the basis of fact that the word that occurs with significantly higher frequency in an entire corpus can be used as seed word. The proposed approach is applied on some ambiguous words for which training and test data is developed and the performance of the system is determined, listed in the form of tables. Finally the comparison is made between the two seed selection methods i. e. local and global. I...|$|R
40|$|Forecasting {{reservoir}} {{production has}} a large associated uncertainty, since this is the final part of a very complex process, this process is based on sparse and indirect data measurements. One the methodologies used {{in the oil industry}} to predict reservoir production is based on the <b>Baye’s</b> <b>theorem.</b> <b>Baye’s</b> <b>theorem</b> applied to reservoir forecasting, samples parameters from a prior understanding of the uncertainty to generate reservoir models and updates this prior information by comparing reservoir production data with model production response. In automatic history matching it is challenging to generate reservoir models that preserve geological realism (obtain reservoir models with geological features that have been seen in nature). One way to control the geological realism in reservoir models is by controlling the realism of the geological prior information. The aim of this thesis is to encapsulate sedimentological information in order to build prior information that can control the geological realism of the history-matched models. This “intelligent” prior information is introduced into the automatic history-matching framework rejecting geologically unrealistic reservoir models. Machine Learning Techniques (MLT) were used to build realistic sedimentological prior information models. Another goal of this thesis was to include geological parameters into the automatic history-match framework that have an impact on reservoir model performance: vertical variation of facies proportions, connectivity of geobodies, and the use of multiple training images as a source of realistic sedimentological prior information. The main outcome of this thesis is that the use of “intelligent” sedimentological prior information guarantees the realism of reservoir models and reduces computing time and uncertainty in reservoir production prediction...|$|R
50|$|In 1761 Thomas <b>Bayes</b> proved Bayes' <b>theorem</b> and in 1765 Joseph Priestley {{invented the}} first {{timeline}} charts.|$|R
2500|$|The pdf of {{the state}} and the data {{likelihood}} are combined to give the new probability density of the system state [...] conditional on the value of the data [...] (the posterior) by the <b>Bayes</b> <b>theorem,</b> ...|$|E
2500|$|Among {{the simple}} solutions, the [...] "combined doors solution" [...] comes closest to a {{conditional}} solution, {{as we saw}} in the discussion of approaches using the concept of odds and <b>Bayes</b> <b>theorem.</b> It is based on the deeply rooted intuition that revealing information that is already known does not affect probabilities. But, knowing that the host can open one of the two unchosen doors to show a goat does not mean that opening a specific door would not affect the probability that the car is behind the initially chosen door. The point is, though we know in advance that the host will open a door and reveal a goat, we do not know which door he will open. If the host chooses uniformly at random between doors hiding a goat (as is the case in the standard interpretation), this probability indeed remains unchanged, but if the host can choose non-randomly between such doors, then the specific door that the host opens reveals additional information. The host can always open a door revealing a goat and (in the standard interpretation of the problem) the probability that the car is behind the initially chosen door does not change, but it is not because of the former that the latter is true. Solutions based on the assertion that the host's actions cannot affect the probability that the car is behind the initially chosen appear persuasive, but the assertion is simply untrue unless each of the host's two choices are equally likely, if he has a choice (...) [...] The assertion therefore needs to be justified; without justification being given, the solution is at best incomplete. The answer can be correct but the reasoning used to justify it is defective.|$|E
5000|$|For {{a binary}} {{symmetric}} channel (with error probability [...] ) the Fano metric {{can be derived}} via <b>Bayes</b> <b>theorem.</b> We are interested in following the most likely path [...] given an explored state of the tree [...] and a received sequence [...] Using the language of probability and <b>Bayes</b> <b>theorem</b> we want to choose the maximum over [...] of: ...|$|E
40|$|A {{study was}} carried out {{assessing}} the practical use of a simple system of scoring information which can help in making a diagnosis or establishing a prognosis in an individual patient. The system was introduced in 1984 for use in gastroenterology, {{but it can be}} employed {{in a wide range of}} medical and surgical settings. This series was concerned with predicting postoperative respiratory complications in a group of elderly surgical patients. The system combines elements from <b>Baye's</b> <b>theorem</b> and logistic regression, though no mathematical knowledge is required to apply it in clinical practice. The method by which results are presented is easy to understand, yet at the same time more complex ideas such as conflict of evidence and doubt may be embraced if the clinician so desires...|$|R
40|$|Image Segmentation {{is one of}} {{the most}} {{important}} concerns in digital image processing. It's a long standing problem in computer vision. Often it is viewed as an ill-defined problem in comparison to other vision tasks which have apparently well defined objectives, such as detection, recognition, and tracking. It is fair to say that computer vision or image understanding is all about parsing images. Different fundamental algorithms of image segmentation are implemented like Hough Transform Algorithm (Edge Linking & Boundary Detection), Region Growing (Similarity Based Segmentation), Iterative Thresholding Method. A new approach of Image Segmentation based on Maximum A posteriori is implemented which uses the concept of <b>Baye's</b> <b>Theorem.</b> A segmentation based problem to count number of white and black nano particles approximately in a microscopic image of a ceramic material is successfully solved. ...|$|R
40|$|Monitoring {{the marine}} {{environment}} for leaks from geological storage projects is a challenge due to the variability {{of the environment and}} the extent of the area that migrating CO 2 might seep through the seafloor. Due to the environmental risk associated leaks {{should not be allowed to}} continue undetected. There is also a cost issue since marine operations are expensive, so false alarms should be avoided. The main question is then: how large a deviation in the monitoring data should cause mobilization of confirmation and localization procedures? Here <b>Baye’s</b> <b>theorem</b> and Bayesian decision theory is suggested as a tool for quantifying certainties and to implement costs for false positives (false alarms) and false negatives (undetected leaks) in the decision procedure. The procedure is exemplified using modeled natural CO 2 content variability and the predicted CO 2 signal from a simulated leak...|$|R
5000|$|... whereis the {{conditional}} probability of y given [...]The conditional probability {{is related to}} the joint probability through <b>Bayes</b> <b>theorem</b> ...|$|E
5000|$|The generic set {{up is the}} following:Let s = {{the hidden}} state of the data that we wish to know. i = {{observed}} image. <b>Bayes</b> <b>theorem</b> gives ...|$|E
5000|$|According to <b>Bayes</b> <b>theorem,</b> the {{posterior}} probability distribution for quantities {{of interest is}} proportional to the product of a prior distribution for the quantities and a likelihood function: ...|$|E
40|$|The {{different}} FEC techniques like convolution code, RS {{code and}} turbo code {{are used to}} improve the performance of communication system. In this paper, we study {{the performance of the}} MAP, Log-MAP, Max-Log-MAP and APP decoding algorithms for turbo codes, in terms of the a priori information, a posteriori information, extrinsic information and channel reliability. We also analyze how important an accurate estimate of channel reliability factor is to the good performances of the iterative turbo decoder. The simulations are made for parallel concatenation of two recursive systematic convolution codes with a block interleaver at the transmitter, AWGN channel and iterative decoding with different algorithms at the receiver side. The comparison of these detection techniques in term of BER performance is discussed in result section. Keywords-component; MAP decoding, Log-MAP decoding, MAX-Log-MAP decoding, <b>Baye’s</b> <b>Theorem,</b> APP decoding. I...|$|R
40|$|OBJECTIVE: To {{determine}} whether the serum screening test for Down syndrome provides equal detection efficacy for women of all ages, to improve the data available for patient counseling both before testing and afterward {{in the event of}} a positive result. METHODS: We examined the effect of age on Down screening by generating a set of "normal" and "Down syndrome" likelihood ratios by computer simulation. The expected false-positive and detection rates were derived for different age groups by counting the proportion of cases in which the likelihood ratio could modify the age-specific risk to be greater than the cutoff risk of one in 300 (equivalent to an incidence of 3. 33 per 1000). The predictive value of a positive result was calculated using <b>Baye's</b> <b>theorem.</b> RESULTS: Detection rates, false-positive rates, and predictive values were shown to be age-dependent. CONCLUSIONS: Knowledge of the age dependency of Down syndrome screening results may be useful in explaining to patients that the Down screen can only detect a proportion of cases and that a negative result does not guarantee normality. This knowledge may also be helpful in minimizing psychological stress, as a positive result indicates only a small chance that the fetus will have Down syndrome...|$|R
40|$|Data Mining is the {{extraction}} of hidden predictive information from large database. Classification {{is the process of}} finding a model that describes and distinguishes data classes or concept. This paper performs the study of prediction of class label using C 4. 5 and Naïve Bayesian algorithm. C 4. 5 generates classifiers expressed as decision trees from a fixed set of examples. The resulting tree is used to classify future samples. The leaf nodes of the decision tree contain the class name whereas a non-leaf node is a decision node. The decision node is an attribute test with each branch (to another decision tree) being a possible value of the attribute. C 4. 5 uses information gain to help it decide which attribute goes into a decision node. A Naïve Bayesian classifier is a simple probabilistic classifier based on applying <b>Baye’s</b> <b>theorem</b> with strong (naive) independence assumptions. Naive Bayesian classifier assumes that the effect of an attribute value on a given class is independent of the values of the other attribute. This assumption is called class conditional independence. The results indicate that Predicting of class label using Naïve Bayesian classifier is very effective and simple compared to C 4. 5 classifier...|$|R
5000|$|Given {{our current}} {{estimate}} of the parameters θ(t), the conditional distribution of the Zi is determined by <b>Bayes</b> <b>theorem</b> to be the proportional height of the normal density weighted by τ: ...|$|E
5000|$|The pdf of {{the state}} and the data {{likelihood}} are combined to give the new probability density of the system state [...] conditional on the value of the data [...] (the posterior) by the <b>Bayes</b> <b>theorem,</b> ...|$|E
5000|$|The {{principle}} of vacant places follows from Conditional Probability theory, {{which is based}} on <b>Bayes</b> <b>Theorem.</b> For a good background to bridge probabilities, and vacant places in particular, see Kelsey; see also the Official Encyclopedia of Bridge ...|$|E
40|$|This {{research}} aims {{to explore}} and identify political risks on a large infrastructure project in an exaggerated environment to ascertain whether sufficient objective information can be gathered by project managers to utilise risk modelling techniques. During the study, the author proposes a new definition of political risk; performs a detailed project study of the Neelum Jhelum Hydroelectric Project in Pakistan; implements a probabilistic model using the principle of decomposition and <b>Bayes</b> probabilistic <b>theorem</b> and answers the question: was it possible for project managers to obtain all the relevant objective data to implement a probabilistic model...|$|R
5000|$|Thomas Bayes ( [...] ; c. 1701 7 April 1761) was an English statistician, {{philosopher and}} Presbyterian {{minister}} {{who is known}} for having formulated a specific case of the theorem that bears his name: Bayes' <b>theorem.</b> <b>Bayes</b> never published what would eventually become his most famous accomplishment; his notes were edited and published after his death by Richard Price.|$|R
30|$|Naive Bayesian {{classifiers}} [14] use <b>Baye's</b> <b>theorem</b> {{to classify}} the new instances of a data sample X. Each instance {{is a set}} of attribute values described by a vector, X[*]=[*](x 1, x 2,…, xn). Considering m classes, the sample X is assigned to the class Ci if and only if P(X | Ci) P(Ci)[*]>[*]P(X | Cj) P(Cj) for all i and j in (1, m) such that j < > i. The sample belongs to the class with maximum posterior probability for the sample. For categorical data, P(Xk | Ci) is calculated as the ratio of frequency of value Xk for attribute Ak and the total number of samples in the training set. For continuous valued attributes, Gaussian distribution can be assumed without loss of generality. In naive Bayesian approach, the attributes are assumed to be conditionally independent. In spite of this assumption, naive Bayesian classifiers give satisfactory results because focus is on identifying the classes for the instances, not the exact probabilities. Applications like spam mail classification and text classification can use naïve Bayesian classifiers. Theoretically, Bayesian classifiers are least prone to errors. The limitation is the requirement of the prior probabilities. The amount of probability information required is exponential in terms of number of attributes, number of classes, and the maximum cardinality of attributes. With increase in number of classes or attributes, the space and computational complexity of Bayesian classifiers increase exponentially.|$|R
5000|$|... that is, the {{codeword}} [...] that maximizes {{the probability}} that [...] was received, given that [...] was sent. If all codewords are equally likely to be sent then this scheme is equivalent to ideal observer decoding.In fact, by <b>Bayes</b> <b>Theorem,</b> ...|$|E
5000|$|According to <b>Bayes</b> <b>theorem</b> for a {{continuous}} event space, the posterior probability {{is given by}} {{the product of the}} prior probability and the likelihood function (given the evidence s and f = n − s), normalized so that the area under the curve equals one, as follows: ...|$|E
5000|$|Nicholas Saunderson [...] (1682 - 19 April 1739) was a blind English {{scientist}} and mathematician. According to one historian of statistics, {{he may have}} been the earliest discoverer of <b>Bayes</b> <b>theorem.</b> [...] He worked as Lucasian Professor, a post also held by Isaac Newton, Charles Babbage and Stephen Hawking.|$|E
40|$|In {{this paper}} we {{demonstrate}} a methodology {{to remove the}} power of the drift induced from random acceleration on LISA proof mass in the frequency domain. The drift must be cleaned from LISA time series data in advance of any further analysis. The cleaning is usually performed in the time domain by using a quadratic function to fit the time series data, and then removing the fitted part from the data. Having Fourier transformed the residuals, and then convolved with LISA transfer function, LISA sensitivity curve can be obtained. However, cosmic gravitational-wave background cannot be retrieved with this approach due to its random nature. Here we provide a new representation of power spectrum given by discrete Fourier transform, which is applied to find the function of the drift power for the cleaning in the frequency domain. We also give the probability distribution used to analyze the data in the frequency domain. We combine several techniques, including Markov Chain Monte Carlo method, simulated annealing, and Gelman & Rubin's method, with <b>Baye's</b> <b>theorem</b> to build the algorithm. The algorithm is utilized to analyze 24 simulations of LISA instrumental noise. We prove that the LISA sensitivity can be recovered through this approach. It can help us to build algorithms for some tasks which are must accomplished in the frequency domain for LISA data analysis. This method can be applied to other space-borne interferometers if charges on their proof masses cannot be perfectly cancelled. Comment: 12 pages, 6 figure...|$|R
40|$|This thesis {{presents}} {{the use of}} pattern recognition and data mining techniques into risk prediction models in the clinical domain of cardiovascular medicine. The data is modelled and classified by using a number of alternative pattern recognition and data mining techniques in both supervised and unsupervised learning methods. Specific investigated techniques include multilayer perceptrons, radial basis functions, and support vector machines for supervised classification, and self organizing maps, KMIX and WKMIX algorithms for unsupervised clustering. The Physiological and Operative Severity Score for enUmeration of Mortality and morbidity (POSSUM), and Portsmouth POSSUM (PPOSSUM) are introduced as the risk scoring systems used in British surgery, which provide a tool for predicting risk adjustment and comparative audit. These systems could not detect all possible interactions between predictor variables whereas these may be possible {{through the use of}} pattern recognition techniques. The thesis presents KMIX and WKMIX as an improvement of the K-means algorithm; both use Euclidean and Hamming distances to measure the dissimilarity between patterns and their centres. The WKMIX is improved over the KMIX algorithm, and utilises attribute weights derived from mutual information values calculated based on a combination of <b>Baye’s</b> <b>theorem,</b> the entropy, and Kullback Leibler divergence. The research in this thesis suggests that a decision support system, for cardiovascular medicine, can be built utilising the studied risk prediction models and pattern recognition techniques. The same may be true for other medical domains. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|A {{method is}} {{presented}} for matching two scene descriptions, {{each of which}} consists {{of a set of}} measured feature vectors with estimated uncertainties. The two scenes differ by a transformation that depends on a few unknown parameters. The method performs a search by sequentially matching features of one scene to those of the other scene, solving for the transformation parameters by means of a generalized least-squares adjustment, computing the probabilities of these matches by means of <b>Bayes</b> * <b>theorem,</b> and using these probabilities to prune the search. An example is given using scene descriptions of the Martian surface in which the features are rocks approximated by ellipsoids...|$|R
5000|$|If [...] and [...] are jointly Gaussian, {{then the}} MMSE {{estimator}} is linear, i.e., {{it has the}} form [...] for matrix [...] and constant [...] This can be directly shown using the <b>Bayes</b> <b>theorem.</b> As a consequence, to find the MMSE estimator, it is sufficient to find the linear MMSE estimator.|$|E
50|$|Statistical PT makes {{ubiquitous}} use of {{conditional probability}} {{in the form}} of <b>Bayes</b> <b>theorem</b> and Markov Models. Both these concepts are used to express the relation between hidden states (configurations) and observed states (images). Markov Models also captures the local properties of the stimulus, reminiscent of the purpose of bond table for regularity.|$|E
5000|$|Entropic {{estimation}} is {{a method}} for revising beliefs when faced with new data. [...] It combines <b>Bayes</b> <b>theorem</b> and Occam's Razor to find a probabilistic model that minimizes the coding costs of both the data and the model, {{which in turn has}} high probability of generalizing correctly. It is a popular technique in the analysis of bioinformatic data.|$|E
40|$|Background: Identification of sex plays a {{vital role}} in {{forensic}} and medico legal investigations. Fingerprints are considered to be the most precise and reliable indicators for personal and gender identification. Objectives: The objective {{of this study was to}} determine any significant difference in the thumbprint ridge density of males and females in a central Indian (Marathi) population to enable the determination of gender. Methods and materials: The study was conducted on 200 subjects (100 males and 100 females) in the age group of 18 – 30  years. Ridge densities on the right- and left-hand thumbprints were determined using a newly designed layout and analysed statistically. Results: The results showed that females tend to have a higher thumbprint ridge density in both the areas examined, individually and combined. Applying the t-test, the differences in the ridge densities of males and females at LoC (Left of Centre), RoC (Right of Centre) and Combined (LoC + RoC) were found to be statistically significant at p <  0. 01 levels, proving the association between gender and fingerprint ridge density. Probability densities for men and women derived from the frequency distribution (at LoC, RoC and Combined) were used to calculate the likelihood ratio and posterior probabilities of gender designation for the given ridge count for subjects using <b>Baye’s</b> <b>theorem.</b> Conclusion: It was concluded that differences in the thumb ridge density can be used as an important tool for the determination of gender in cases where partial thumbprints are encountered as evidence either at the crime scene or on any document(s) of forensic significance...|$|R
40|$|AbstractBackgroundIdentification of sex plays a {{vital role}} in {{forensic}} and medico legal investigations. Fingerprints are considered to be the most precise and reliable indicators for personal and gender identification. ObjectivesThe objective {{of this study was to}} determine any significant difference in the thumbprint ridge density of males and females in a central Indian (Marathi) population to enable the determination of gender. Methods and materialsThe study was conducted on 200 subjects (100 males and 100 females) in the age group of 18 – 30 years. Ridge densities on the right- and left-hand thumbprints were determined using a newly designed layout and analysed statistically. ResultsThe results showed that females tend to have a higher thumbprint ridge density in both the areas examined, individually and combined. Applying the t-test, the differences in the ridge densities of males and females at LoC (Left of Centre), RoC (Right of Centre) and Combined (LoC+RoC) were found to be statistically significant at p< 0. 01 levels, proving the association between gender and fingerprint ridge density. Probability densities for men and women derived from the frequency distribution (at LoC, RoC and Combined) were used to calculate the likelihood ratio and posterior probabilities of gender designation for the given ridge count for subjects using <b>Baye’s</b> <b>theorem.</b> ConclusionIt was concluded that differences in the thumb ridge density can be used as an important tool for the determination of gender in cases where partial thumbprints are encountered as evidence either at the crime scene or on any document(s) of forensic significance...|$|R
40|$|Abstract:- Physiologically the {{frequency}} distribution of biological data of living systems may {{be approximated by}} a lognormal curve, pathologically by a normal = bell curve. The Chi-square and the Kolmogorov-Smirnov tests are used for the data analysis. A chaos is the most frequent abnormal result. We investigated in 492 chaotically regulated patients 6 important diagnoses: Chronic Fatigue Syndrome, Guillain-Barré-Syndrome, allergy, auto-aggression, cancer, and neurosis. The application of <b>Bayes</b> ` <b>theorem</b> allowed the calculation of probabilities. In this way in medical decision making suspicion diagnoses may be strengthened or weakened, or a new tentative diagnosis arises. The problem of cancer is discussed. Our results display the relevance of this system analysis of the human network in medicine. Key-Words:- Chaos, frequency distribution analysis, medical decision making...|$|R
