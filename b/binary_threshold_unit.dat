1|475|Public
40|$|Recently {{statistical}} physicists {{have extended}} their research fields to complex systems {{which have not}} been regarded as subjects of traditional physics. Statistical mechanics has originally been developed as a useful tool to investigate the various physical properties of condensed matters in which about 10 23 elements interact {{each other in the}} complicated fashion. In real life, {{there are a lot of}} similar situations, including social science such as economics, in which lots of simple elements interact each other and their collective behavior is sometimes beyond our expectation. Among such complicated systems, the brain has been one of the most attractive subject. In the real brain, there exist about 10 12 neurons and each of them is connected with the other neurons of order 10 5 by synaptic efficacy. For this highly complicated non-linear system, many researchers in the statistical mechanics community have partially succeeded in explaining some aspect of brain functions. They assume that the synaptic connections are randomly distributed and a single neuron is a <b>binary</b> <b>threshold</b> <b>unit</b> which takes + 1 or − 1 in analogy with random spin systems like spin glasses. However, if success of statistical mechanics consists only in explaining some aspect of brain functions, or giving a way for understanding of macroscopic psychological phenomena from a microscopic point of view, we are obliged to think that researchers who have used statistical mechanics only changed their subjects and succeeded in giving an explanation for experiments of the new subjects. I think the success of statistical mechanics consists not only in giving an explanation of physiological experiments but in providing the engineers or information scientists, who need practical applications, with some mathematical concepts or ideas to construct a new type of computer machine based on the way of information processing in the real brain. Thus, statistical mechanics has two aspects of contribution to the brain science; • Explanation for physiological phenomena. • Providing mathematical foundation for constructing new type of information processing or opti-mization. For a remarkable example of the former contribution, statistical mechanics has succeeded in showing...|$|E
50|$|The {{units in}} Hopfield nets are <b>binary</b> <b>threshold</b> <b>units,</b> i.e. the units only take on two {{different}} values for their states and the value is determined by {{whether or not the}} units' input exceeds their threshold. Hopfield nets normally have units that take on values of 1 or -1, and this convention will be used throughout this page. However, other literature might use units that take values of 0 and 1.|$|R
40|$|In this paper, {{we present}} a new decompositional {{approach}} for the extraction of propositional rules from feed-forward neural networks of <b>binary</b> <b>threshold</b> <b>units.</b> After decomposing the network into single units, we show how to extract rules describing a unit’s behavior. This is done using a suitable search tree which allows the pruning of the search space. Furthermore, we present some experimental results, showing a good average runtime behavior of the approach. A...|$|R
40|$|Summary. Hölldobler and Kalinke showed how, given a {{propositional logic}} pro-gram P, a 3 -layer {{feedforward}} {{artificial neural network}} may be constructed, using only <b>binary</b> <b>threshold</b> <b>units,</b> which can compute the familiar immediate-consequence operator TP associated with P. In this chapter, essentially these results are estab-lished for a class of logic programs which can handle many-valued logics, constraints and uncertainty; these programs therefore represent a considerable extension of con-ventional propositional programs. The work of the chapter basically falls into two parts. In the first of these, the programs considered extend the syntax of conven-tional logic programs by allowing elements of quite general algebraic structures {{to be present in}} clause bodies. Such programs include many-valued logic programs, and semiring-based constraint logic programs. In the second part, the programs considered are bilattice-based annotated logic programs in which body literals are annotated by elements drawn from bilattices. These programs are well-suited to handling uncertainty. Appropriate semantic operators are defined for the programs considered in both parts of the chapter, and it is shown that one may construct artificial neural networks for computing these operators. In fact, in both cases only <b>binary</b> <b>threshold</b> <b>units</b> are used, but it simplifies the treatment conceptually to ar-range them in so-called multiplication and addition units {{in the case of the}} programs of the first part. ...|$|R
40|$|We {{address the}} problem of {{training}} a special kind of Boolean feedforward neural networks, namely networks built from linear <b>binary</b> <b>threshold</b> <b>units</b> with ternary weights called majority units. We propose an original constructive training algorithm, inspired from both the approaches of the Tiling and the Upstart algorithms, and we give a simple proof of its convergence. Numerical experiments were carried out to test the size of the constructed networks as well as their generalization ability. Comparisons are made with classical <b>threshold</b> <b>unit</b> networks. 1 Introduction The model we consider is a multilayer feedforward neural network with binary activations, represented as the set IB = fΓ 1; + 1 g. Each neuron is a perceptron whose weights values are restricted to the set fΓ 1; 0; + 1 g. The motivation of studying this model is the quantization of weights, which appears to be crucial when the network must be realized in hardware. Formally, each neuron computes a majority funct [...] ...|$|R
40|$|According to its {{mathematical}} description, a Hopfield Neural Network {{serves as}} a content addressable memory with <b>binary</b> <b>threshold</b> <b>units.</b> The elements in that memory consist of the correlations between elements of memory vectors. In this thesis, thefeasibility of a Hopfield Neural Network using DNA molecules as the working substance is introduced. In addition, I present an experimental study proving that forming a DNA based memory storing the information of two different 6 -bit black and white images,representing memory vectors, and recalling one of original images {{with the use of}} a partial image are possible. It is observed that the recalling with a DNA based Hopfield Neural Network using incomplete inputs is more powerful comparing to theoretical oneusing corrupted inputs. Moreover, as a supplementary work, I show that application of T 4 Gene 32 Protein to Isothermal Linear Amplification (ILA) reduces the production of fragment DNA strands, one of the biggest problems of this type of amplification...|$|R
40|$|Hölldobler and Kalinke showed how, given a {{propositional logic}} program P, a 3 -layer {{feedforward}} {{artificial neural network}} may be constructed, using only <b>binary</b> <b>threshold</b> <b>units,</b> which can compute the familiar immediate-consequence operator TP associated with P. In this chapter, essentially these results are established for a class of logic programs which can handle many-valued logics, constraints and uncertainty; these programs therefore represent a considerable extension of conventional propositional programs. The work of the chapter basically falls into two parts. In the first of these, the programs considered extend the syntax of conventional logic programs by allowing elements of quite general algebraic structures {{to be present in}} clause bodies. Such programs include many-valued logic programs, and semiring-based constraint logic programs. In the second part, the programs considered are bilattice-based annotated logic programs in which body literals are annotated by elements drawn from bilattices. These programs are well-suited to handling uncertainty. Appropriate semantic operators are defined for the programs considered in both parts of the chapter, and it is shown that one may construc...|$|R
40|$|In {{this paper}} weshow that a {{feedforward}} neural network {{with at least one}} hiddenlayer canapproximate the meaning function TP for an acceptable logic program P. This is found by using the property of acceptable logic programs that for this class of programs the meaning function TP is a contraction mapping on the complete metric space of the interpretations for P as shown by Fitting in [3]. Using this result it can be shown that for an acceptable program such a network can be extended to a recurrent neural networks that is able to approximate the iteration of the meaning function TP, that is the semantics of the logic program P. 1 Introduction In [6] we have shown, that 3 [...] layer recurrent networks are able to compute the semantics of propositional logic programs. For such programs an interpretation I of the propositional variables in P can easily be encoded by a set of <b>binary</b> <b>threshold</b> <b>units</b> such that there is a bijective mapping between the set of units and the set of propositional [...] ...|$|R
40|$|Original Backprop (Version 1. 2) is an MS-DOS {{package of}} four {{stand-alone}} C-language programs that enable users to develop neural network solutions {{to a variety}} of practical problems. Original Backprop generates three-layer, feed-forward (series-coupled) networks which map fixed-length input vectors into fixed length output vectors through an intermediate (hidden) layer of <b>binary</b> <b>threshold</b> <b>units.</b> Version 1. 2 can handle up to 200 input vectors at a time, each having up to 128 real-valued components. The first subprogram, TSET, appends a number (up to 16) of classification bits to each input, thus creating a training set of input output pairs. The second subprogram, BACKPROP, creates a trilayer network to do the prescribed mapping and modifies the weights of its connections incrementally until the training set is leaned. The learning algorithm is the 'back-propagating error correction procedures first described by F. Rosenblatt in 1961. The third subprogram, VIEWNET, lets the trained network be examined, tested, and 'pruned' (by the deletion of unnecessary hidden units). The fourth subprogram, DONET, makes a TSR routine by which the finished product of the neural net design-and-training exercise can be consulted under other MS-DOS applications...|$|R
40|$|Randomly coupled Ising spins {{constitute}} the classical model of collective phenomena in disordered systems, with applications covering ferromagnetism, combinatorial optimization, protein folding, stock market dynamics, and social dynamics. The phase diagram {{of these systems}} is obtained in the thermodynamic limit by averaging over the quenched randomness of the couplings. However, many applications require the statistics of activity for a single realization of the possibly asymmetric couplings in finite-sized networks. Examples include reconstruction of couplings from the observed dynamics, learning in {{the central nervous system}} by correlation-sensitive synaptic plasticity, and representation of probability distributions for sampling-based inference. The systematic cumulant expansion for kinetic <b>binary</b> (Ising) <b>threshold</b> <b>units</b> with strong, random and asymmetric couplings presented here goes beyond mean-field theory and is applicable outside thermodynamic equilibrium; a system of approximate non-linear equations predicts average activities and pairwise covariances in quantitative agreement with full simulations down to hundreds of units. The linearized theory yields an expansion of the correlation- and response functions in collective eigenmodes, leads to an efficient algorithm solving the inverse problem, and shows that correlations are invariant under scaling of the interaction strengths...|$|R
40|$|We {{describe}} the trace representations of two families of binary sequences derived from Fermat quotients modulo an odd prime p (one is the <b>binary</b> <b>threshold</b> sequences, {{the other is}} the Legendre-Fermat quotient sequences) via determining the defining pairs of all binary characteristic sequences of cosets, which coincide with the sets of pre-images modulo p^ 2 of each fixed value of Fermat quotients. From the defining pairs, we can obtain an earlier result of linear complexity for the <b>binary</b> <b>threshold</b> sequences and a new result of linear complexity for the Legendre-Fermat quotient sequences under the assumption of 2 ^p- 1 ≡ 1 p^ 2. Comment: 14 pages, no figure...|$|R
40|$|We {{define a}} family of 2 e+ 1 -periodic <b>binary</b> <b>threshold</b> {{sequences}} and {{a family of}} p 2 -periodic <b>binary</b> <b>threshold</b> sequences by using Carmichael quotients modulo 2 e (e > 2) and 2 p (p is an odd prime), respectively. These are extensions of the construction derived from Fermat quotients modulo an odd prime in our earlier work. We determine exact values of the linear complexity, which are larger {{than half of the}} period. For cryptographic purpose, the linear complexities of the sequences in this letter are of desired values. © 2012 The Institute of Electronics, Information and Communication Engineers. National Natural Science Foundation of China 61170246, 61102093, 61063041; Program for New Century Excellent Talents in Fujian Province University of China JK 2010047; State Key Laboratory of Information Security (Chinese Academy of Sciences) 01 - 01 - 1; Education Department of Gansu Province 1001 - 09 We define a family of 2 e+ 1 -periodic <b>binary</b> <b>threshold</b> sequences and a family of p 2 -periodic <b>binary</b> <b>threshold</b> sequences by using Carmichael quotients modulo 2 e (e > 2) and 2 p (p is an odd prime), respectively. These are extensions of the construction derived from Fermat quotients modulo an odd prime in our earlier work. We determine exact values of the linear complexity, which are larger than half of the period. For cryptographic purpose, the linear complexities of the sequences in this letter are of desired values. © 2012 The Institute of Electronics, Information and Communication Engineers...|$|R
40|$|Testing {{uncovered}} {{interest rate}} parity and term structure using three-regime <b>threshold</b> <b>unit</b> root VECM: an {{application to the}} Swiss “Isle ” of interest rates KRISHNAKUMAR, Jaya, NETO, David KRISHNAKUMAR, Jaya, NETO, David. Testing uncovered interest rate parity and term structure using three-regime <b>threshold</b> <b>unit</b> root VECM: an application to the Swiss “Isle ” of interest rates...|$|R
40|$|In this {{empirical}} study, {{we apply}} the <b>threshold</b> <b>unit</b> root test proposed by Caner and Hansen (2001) to re-examine the hysteresis hypothesis in unemployment for G- 7 countries {{over the period}} 1992 M 1 to 2008 M 9. The hysteresis in unemployment is confirmed for three countries, namely France, Germany and Italy when Caner and Hansen’s (2001) <b>threshold</b> <b>unit</b> root test is conducted...|$|R
5000|$|Although {{a single}} <b>threshold</b> <b>unit</b> is quite {{limited in its}} {{computational}} power, {{it has been shown}} that networks of parallel <b>threshold</b> <b>units</b> can approximate any continuous function from a compact interval of the real numbers into the interval -1,1. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass [...] "A learning rule for very simple universal approximators consisting of a single layer of perceptrons".|$|R
40|$|ABSTRACT. Several graph theoretic luster {{techniques}} {{aimed at}} the automatic generation of thesauri for information retrieval systems are explored. Experimental cluster analysis is performed on a sample corpus of 2267 documents. A term-term similarity matrix is constructed for the 3950 unique terms used to index the documents. "Various threshold values, T, are ap-plied to the similarity matrix to provide a series of <b>binary</b> <b>threshold</b> matrices. The correspond-ing graph of each <b>binary</b> <b>threshold</b> matrix is used to obtain the term clusters. Three definitions of a cluster are analyzed: (1) the connected components of the threshold matrix; (2) the maximal complete subgraphs of the connected components of the threshold matrix; (3) clusters of the maximal complete subgraphs of the threshold matrix, as described by Gotlieb and Kumar. Algorithms are described and analyzed for obtaining each cluster type. The algorithms {{are designed to be}} useful for large document and index collections. Two algorithms have been tested that find maximal complete subgraphs. An algorithm developed by Bierstone offers a significant ime improvement over one suggested by Bonner. For threshold levels T> 0. 6, basically the same clusters are developed regardless of the cluster definition used. In such situations one need only find the connected components of the graph to develop the clusters. KEY WORDS AND PHRA. SES: cluster analysis, graph-theoretic, connected component, maximal complete subgraph, clique, <b>binary</b> <b>threshold</b> matrix, information storage and retrieval, simi...|$|R
40|$|We have {{investigated}} information transmission in {{an array of}} <b>threshold</b> <b>units</b> with multiplicative noise that have a common input signal. We demonstrate a phenomenon similar to stochastic resonance with additive noise, and show that information transmission can be enhanced by a non-zero multiplicative noise level. Given that sensory neurons in the nervous system have multiplicative as well as additive noise sources, and they act approximately like <b>threshold</b> <b>units,</b> our results suggest that multiplicative noise might be an essential part of neural coding...|$|R
40|$|We propose two {{algorithms}} {{for constructing}} and training compact feedforward networks of linear <b>threshold</b> <b>units.</b> The Shift procedure constructs networks {{with a single}} hidden layer while the PTI constructs multilayered networks. The resulting networks are guaranteed to perform any given task with binary or real-valued inputs. The various experimental results reported for tasks with binary and real inputs indicate that our methods compare favorably with alternative procedures deriving from similar strategies, {{both in terms of}} size of the resulting networks and of their generalization properties. Keywords: Feedforward networks, <b>threshold</b> <b>units,</b> constructive algorithms, greedy strategy. 1 Introduction Since the beginning of the 90 's, methods for constructing feedforward networks of <b>threshold</b> <b>units</b> have attracted a considerable interest (see for instance [14] and the included references). Constructive algorithms do not only tune the weights of the network but also vary its topology, in oth [...] ...|$|R
40|$|This paper {{investigates the}} {{generation}} of neural networks through the induction of <b>binary</b> trees of <b>threshold</b> logic <b>units</b> (TLUs). Initially, we describe the framework for our tree construction algorithm and show how it helps {{to bridge the gap}} between pure connectionist (neural network) and symbolic (decision tree) paradigms. We also show how the trees of <b>threshold</b> <b>units</b> that we induce can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node), but produce trees that are smaller and thus easier to understand. Moreover, our results also show that it is possible to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are initially gi [...] ...|$|R
40|$|Post {{inhibitory}} rebound is a nonlinear phenomenon {{present in}} a variety of nerve cells. It is an important mechanism underlying central pattern generation for heartbeat, swimming and other motor patterns in many neuronal systems. In this paper we propose an extension of the <b>binary</b> <b>threshold</b> neuron model to incorporate the effects of post inhibitory rebound...|$|R
40|$|We wish to learn, within error {{tolerance}} ε, an unknown value, given access only to sequential <b>binary</b> <b>thresholded</b> observations under an additive Gaussian noise model. At t = 0, 1, 2, [...] ., we maintain a probability distribution pt(v) over V {{and set a}} threshold θt. The observation (stochastic binary signal) is xt = sign(V + zt – θt) where...|$|R
40|$|It {{is shown}} that {{there exists a}} monomorphism from a multithreshold {{symmetric}} automata into a <b>binary</b> <b>threshold</b> symmetric automata. This implies that both automata have the same transient and cyclic behaviour. By a theorem proved in Goles and Olivos (1981) we conclude that a multithrehold symmetric automata does not have cycles of length greater than two...|$|R
40|$|Image {{segmentation}} {{remains one}} of the greatest problems in machine vision. The technique described here takes an image and a geometric description of the object required, determines multiple <b>binary</b> <b>thresholds</b> to segment the image, and combines the information from the appropriate thresholds. By utilizing region-growing hardware it is possible to achieve segmentation in less than 2 seconds...|$|R
50|$|For low {{intensity}} tasks, smaller motor {{units with}} fewer muscle fibers are used. These smaller motor units {{are known as}} low <b>threshold</b> motor <b>units.</b> They consist of type I fibers that contract much slower and thus provide less force for daily basic movement such as typing on the keyboard.For more intense tasks, motor units containing Type II muscle fibers are used. These fast twitch motor units are known as high <b>threshold</b> motor <b>units.</b> The major difference between low <b>threshold</b> motor <b>units</b> (slow twitch motor <b>unit)</b> and high <b>threshold</b> motor <b>units</b> (fast twitch motor unit) is that high <b>threshold</b> motor <b>units</b> control more muscle fibers and contain larger muscle fibers, in comparison to low <b>threshold</b> motor <b>unit.</b> On the other hand, {{the main difference between}} the slow twitch muscle fiber (Type I) and fast twitch muscle fiber (Type II) has the same theory of the size deviations.|$|R
40|$|Lately, {{interference}} cancellation {{is a hard}} {{challenge in}} a cellular communication system. In this paper, we consider the interference cancellation using logic OR gate from the {{multiple input multiple output}} (MIMO) interference channel where each transmitter and receiver equipped with single or multiple antennas. In our system, we have used a <b>threshold</b> logic <b>unit</b> which is operated by logic OR with NOT operation in the MIMO receiver for interference cancellation. The <b>threshold</b> <b>unit</b> has two logic threshold signals which are separated by two vectors. These two logic thresholds are denoted by High and Low signal in the <b>threshold</b> <b>unit.</b> The High and Low signals are represented by the desired and interference signal, respectively. Our approach is to practically achieve interference alignment and absolutel...|$|R
3000|$|Average dark frames: {{we first}} acquire a {{sequence}} of frames when no light is present, and compute the average and standard deviation of each pixel and readout block. We set a <b>binary</b> <b>threshold</b> to define bad (noisy) pixels or bad (noisy) ADC channels, when the standard deviation is above a threshold or if the standard deviation is equal to 0; [...]...|$|R
40|$|We {{determine}} the linear complexity of p 2 -periodic <b>binary</b> <b>threshold</b> sequences derived from polynomial quotient, which {{is defined by}} the function. When w = (p - 1) / 2 and, we show that the linear complexity is equal to one of the following values, depending whether. But it seems that the method can't be applied to the case of general w. © 2012 Springer-Verlag. Univ. Waterloo, Dep. Electr. Comput. Eng.; The Fields Institute for Research in Mathematical Sciences; The Mprime Network Inc.; The Ontario Research Fund Research ExcellenceWe {{determine the}} linear complexity of p 2 -periodic <b>binary</b> <b>threshold</b> sequences derived from polynomial quotient, which {{is defined by the}} function. When w = (p - 1) / 2 and, we show that the linear complexity is equal to one of the following values, depending whether. But it seems that the method can't be applied to the case of general w. © 2012 Springer-Verlag...|$|R
3000|$|Step 1 with random sensing pattern [...] _r with a 2 D uniform {{distribution}} U(0, 1)× U(0, 1) and <b>binary</b> <b>thresholding.</b> The completely random sampling, which acquires pixels at edges and smooth regions uniformly, captures the image profile information and guarantees the RIP and incoherence condition. 1 We recover a low-quality image f_l = f̂_ 1 with the completely m_r random measurements y_r = [...] _r∘ f.|$|R
40|$|We {{discuss how}} to extract {{symbolic}} rules from a given <b>binary</b> <b>threshold</b> feed-forward network. The proposed decompositional approach {{is based on}} an internal representation using binary decision diagrams. They allow for an efficient composition of the intermediate results as well as for an easy integration of integrity constraints into the extraction. We also discuss some experimental results indicating a good performance of the approach. ...|$|R
50|$|A Hopfield {{network is}} a form of {{recurrent}} artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable memory systems with <b>binary</b> <b>threshold</b> nodes. They are guaranteed to converge to a local minimum, but will sometimes converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum). Hopfield networks also provide a model for understanding human memory.|$|R
40|$|This study applies {{nonlinear}} <b>threshold</b> <b>unit</b> root test {{to assess}} the nonstationary properties of the Real Exchange Rate (RER) for seven major Organization of the Petroleum Exporting Countries (OPEC). We found that nonlinear <b>threshold</b> <b>unit</b> root test has higher power than linear method suggested by Caner and Hansen (2001) if the true data-generating process of exchange rate {{is in fact a}} stationary nonlinear process. We examined the validity of Purchasing Power Parity (PPP) from the nonlinear point of view and provided robust evidence clearly indicating that PPP holds true for three countries, namely Angola, Indonesia and Iran. Our findings point out that their exchange rate adjustment is mean reversion towards PPP equilibrium values in a nonlinear way. ...|$|R
3000|$|... 0 ∈[0, 1], the {{generated}} {{sequence is}} {{in the state of}} chaos. In this case, the <b>binary</b> quantization <b>threshold</b> is 0.5.|$|R
30|$|BF {{is used as}} a noise {{reduction}} algorithm [18 – 21]. The original BF comprises the following two Gaussian filters: one is for the distance weight between pixel locations and the other is for the difference weight between pixel intensities. To simplify these two Gaussian filters, the Gaussian functions are replaced by fixed-point, <b>binary</b> <b>threshold</b> functions in the proposed modified BF. The threshold values are determined by pre-calculating the Gaussian filter coefficients for the pixel locations and pixel intensities.|$|R
40|$|Gaussian {{white noise}} is {{frequently}} used to model fluctuations in physical systems. In Fokker-Planck theory, {{this leads to}} a vanishing probability density near the absorbing boundary of threshold models. Here we derive the boundary condition for the stationary density of a first-order stochastic differential equation for additive finite-grained Poisson noise and show that the response properties of <b>threshold</b> <b>units</b> are qualitatively altered. Applied to the integrate-and-fire neuron model, the response turns out to be instantaneous rather than exhibiting low-pass characteristics, highly non-linear, and asymmetric for excitation and inhibition. The novel mechanism is exhibited on the network level and is a generic property of pulse-coupled systems of <b>threshold</b> <b>units.</b> Comment: Consists of two parts: main article (3 figures) plus supplementary text (3 extra figures...|$|R
40|$|Summary. * We {{study the}} {{distribution}} of s-dimensional points of digital explicit inversive pseudorandom numbers with arbitrary lags. We prove a discrepancy bound and derive results on the pseudorandomness of the <b>binary</b> <b>threshold</b> sequence derived from digital explicit in-versive pseudorandom numbers in terms of bounds on the correlation measure of order k and the linear complexity profile. The proofs are based on bounds on exponential sums and earlier relations of Mauduit, Niederreiter and Sárközy between discrepancy and correlation measure of order k and of Brandstätter and the third author between correlation measure of order k and linear complexity profile, respectively. Summary. We study {{the distribution of}} s-dimensional points of digital explicit inversive pseudorandom numbers with arbitrary lags. We prove a discrepancy bound and derive re-sults on the pseudorandomness of the <b>binary</b> <b>threshold</b> sequence derived from digital explicit inversive pseudorandom numbers in terms of bounds on the correlation measure of order k and the linear complexity profile. The proofs are based on bounds on exponential sums and earlier relations of Mauduit, Niederreiter and Sárközy between discrepancy and correlation measure of order k and of Brandstätter and the third author between correlation measure of order k and linear complexity profile, respectively. 2 Zhixiong Chen, Domingo Gomez and Arne Winterhof...|$|R
40|$|Neurons, modeled as linear <b>threshold</b> <b>unit</b> (LTU), can {{in theory}} compute all thresh- old {{functions}}. In practice, however, {{some of these}} functions require synaptic weights of arbitrary large precision. We show here that dendrites can alleviate this requirement. We introduce here the non-Linear <b>Threshold</b> <b>Unit</b> (nLTU) that integrates synaptic input sub-linearly within distinct subunits {{to take into account}} local saturation in dendrites. We systematically search parameter space of the nTLU and TLU to compare them. Firstly, this shows that the nLTU can compute all threshold functions with smaller precision weights than the LTU. Secondly, we show that a nLTU can compute significantly more functions than a LTU when an input can only make a single synapse. This work paves the way {{for a new generation of}} network made of nLTU with binary synapses. Comment: 5 pages 3 figure...|$|R
