20|93|Public
60|$|In the Carthusian Monastery {{outside the}} Roman Gate, {{mutilated}} and profaned though it is, one may still snuff up a strong if stale redolence of old Catholicism and old Italy. The road {{to it is}} ugly, being encumbered with vulgar waggons and fringed with tenements suggestive of an Irish-American suburb. Your interest begins as you come in sight of the convent perched on its little mountain and lifting against the sky, around the bell-tower of its gorgeous chapel, a coronet of clustered cells. You make your way into the lower gate, through a clamouring press of deformed beggars who thrust at you their stumps of limbs, and you climb the steep hillside through a shabby plantation which it is proper to fancy was better tended in the monkish time. The monks are not totally abolished, the government having the grace to await the natural extinction of the half-dozen old brothers who remain, and who shuffle doggedly about the cloisters, looking, with their white robes and their pale blank old faces, quite anticipatory ghosts of their future selves. A prosaic, profane {{old man in a}} coat and trousers serves you, however, as custodian. The melancholy friars have not even the privilege of doing you the honours of their dishonour. One must imagine the pathetic effect of their former silent pointings to this and that conventual treasure under stress of the feeling that such pointings were narrowly numbered. The convent is vast and irregular--it bristles with those picture-making arts and accidents which one notes as one lingers and passes, but which in Italy the overburdened memory learns to resolve into <b>broadly</b> <b>general</b> images. I rather deplore its position at the gates of a bustling city--it ought rather to be lodged in some lonely fold of the Apennines. And yet to look out from the shady porch of one of the quiet cells upon the teeming vale of the Arno and the clustered towers of Florence must have deepened the sense of monastic quietude.|$|E
50|$|In {{generative}} morphology, the righthand head rule is a rule {{of grammar}} that specifies that the rightmost morpheme in a morphological structure is always the head. What {{this means is that}} it is the righthand element that provides the primary syntactic and/or semantic information. The projection of syntactic information from the righthand element onto the output word is known as feature percolation. The righthand head rule is considered a <b>broadly</b> <b>general</b> and universal principle of morphology.|$|E
50|$|Loesch {{was born}} in Chicago on March 10, 1916. He {{was the son of}} Joseph B. Loesch and Constance Harrison Loesch and {{grandson}} of prominent Chicago attorney Frank J. Loesch. He was raised in Montrose, Colorado where his parents owned a ranch. He received a B.A. from Colorado College in 1936 and his LL.B. from Yale Law School in 1939. Loesch returned from Yale to practice law in Montrose in 1939, at the firm of Moynihan, Hughes & Knous. He married his wife Louise Mills in 1940. He volunteered and served in World War II with the United States Army Air Forces. He served in North Africa, then participated in the invasions of Sicily and Italy, and finally in the Normandy Landings, rising from private to the wartime temporary rank of colonel and permanent rank of major. After he returned to Montrose, he became a partner at the firm of Strang, Loesch & Kreidler. He then founded his own firm, Loesch, Kreidler & Durham. In 1961 Loesch was elected president of the Colorado Bar Association. His practice was <b>broadly</b> <b>general,</b> but with considerable specialization in resource matters. His clients included numerous mining, electric, and other resource-oriented companies, as well as farmers, ranchers and other individuals and companies.|$|E
50|$|The term also refers more <b>broadly</b> to a <b>general</b> {{absence of}} normal or {{expected}} feelings.|$|R
50|$|Civets have a <b>broadly</b> cat-like <b>general</b> appearance, {{though the}} muzzle is {{extended}} and often pointed, rather {{like that of}} an otter or a mongoose. They range in length from about 17 to 28 in (excluding their long tails) and in weight from about 3 to 10 lb.|$|R
50|$|The Buddhist asuras are <b>broadly</b> derived, in <b>general</b> character, {{from the}} wicked asuras of Hinduism, but have {{acquired}} some very distinctive myths which are only found in Buddhist texts.|$|R
40|$|Using {{mutation}} {{libraries and}} deep sequencing, Aakre et al. study {{the evolution of}} protein-protein interactions using a toxin-antitoxin model. The results indicate probable trajectories via “intermediate” proteins that are promiscuous, thus avoiding transitions via non-interactions. These results extend observations about other biological interactions and enzyme evolution, suggesting <b>broadly</b> <b>general</b> principles...|$|E
40|$|This paper {{presents}} a computational {{framework for the}} integrated simulation of complex multi-physics problems. It results from interfacing existing structural dynamics, multidisciplinary software and different computational fluid dynamics codes {{by means of a}} newly developed, <b>broadly</b> <b>general</b> communication scheme that accounts both for solution synchronization and field interpolation. A first application to the fluid structure interaction problem based on a free wake code is presented. More complex applications will be presented in the near future...|$|E
40|$|We {{report the}} {{creation}} of a Bose-Einstein condensate using buffer-gas cooling, the first realization of Bose-Einstein condensation using a <b>broadly</b> <b>general</b> method which relies neither on laser cooling nor unique atom-surface properties. Metastable helium ([superscript 4]He[superscript *]) is buffer-gas cooled, magnetically trapped, and evaporatively cooled to quantum degeneracy. 10 [superscript 11] atoms are initially trapped, leading to Bose-Einstein condensation at a critical temperature of 5 [*][*]μK and threshold atom number of 1. 1 × 10 [superscript 6]. This method is applicable to a wide array of paramagnetic atoms and molecules, many of which are impractical to laser cool and impossible to surface cool. National Science Foundatio...|$|E
5000|$|The Halliwick Ten-Point-Programme {{has been}} applied for {{teaching}} swimming to people with disability, {{as well as more}} <b>broadly</b> as a <b>general</b> approach to teaching swimming and working with disability.|$|R
5000|$|<b>Broadly</b> speaking, <b>general</b> {{equilibrium}} {{tries to}} give {{an understanding of the}} whole economy using a [...] "bottom-up" [...] approach, starting with individual markets and agents. (Macroeconomics, as developed by the Keynesian economists, focused on a [...] "top-down" [...] approach, where the analysis starts with larger aggregates, the [...] "big picture".) Therefore, general equilibrium theory has traditionally been classified as part of microeconomics.|$|R
50|$|Environmental Standards. These are {{centrally}} driven standards. A {{legally enforceable}} numerical limit {{is often used}} to determine the 'standard', but the term can be used more <b>broadly,</b> describing more <b>general</b> rules about acceptability.|$|R
40|$|Several privileged {{starting}} materials that allow {{access to a}} variety of different heterocyclic scaffolds through judicious choice of reaction conditions (and coupling partner) have emerged. This microreview focuses on the development and use of four of these starting material types in cascade reactions involving the palladium- or copper-catalysed formation of aromatic carbon-nitrogen, carbon-oxygen or carbon-sulfur bonds. Over the last decade, four precursor types have been established as <b>broadly</b> <b>general</b> substrates in heterocycle synthesis. By appropriate choice of catalyst and coupling partner, these four precursor types can be converted into a wide range of aromatic products. Copyright © 2013 WILEY-VCH Verlag GmbH and Co. KGaA, Weinheim...|$|E
40|$|Thomas Schelling, {{winner of}} the 2005 Nobel Memorial Prize in Economics, was a Founding Father of the Kennedy School of Government. He served the School warmly and well for many years. Schelling always served as an {{inspiration}} for his creativity, and as an exemplar {{for his ability to}} make his writings scholarly, <b>broadly</b> <b>general,</b> and of great policy import. His path breaking essays and books provide an anatomy of human behavior in the individual, the dyad, and the larger group. This essay was written as the preface for a forthcoming biography by Robert Dodge, titled The Strategist, The Life and Times of Thomas Schelling. ...|$|E
40|$|Packet {{acquisition}} for a time-frequency hopped asynchronous random {{multiple access}} (RMA) system is investigated. A novel analytical approach to performance evaluation is provided, which enables the waveform designer to quickly evaluate the network performance as various waveform design and RMA protocol parameters and are chosen. The {{results show that}} the network throughput is estimated and upper-bounded by the acquisition performance. The performance evaluation procedure is <b>broadly</b> <b>general,</b> allowing for variations in waveform parameters, network topology, network population and population heterogeneity (in, e. g., rate, transmit power, and medium access behavior). To illustrate the accuracy of the analytical results, simulations are performed over a wide range of waveform, RMA protocol and network parameters. United States. Dept. of the Air Force (FA 8721 - 05 -C- 0002...|$|E
5000|$|The Association for Core Texts and Courses (ACTC) is {{a global}} {{association}} {{of colleges and universities}} supporting the use of classic texts and core curricula in undergraduate education. [...] It is headquartered at Saint Mary's College of California. It is <b>broadly</b> associated with <b>general</b> education.|$|R
40|$|This thesis proposes {{some new}} model {{independent}} or nonparametric methods for estimating the dose-response curve and the effective dosage {{curve in the}} context of bioassay. The research problem is also of importance in environmental risk assessment and other areas of health sciences. It is shown in the thesis that our new nonparametric methods while bearing optimal asymptotic properties also exhibit strong finite sample performance. Although our specific emphasis is on bioassay and environmental risk assessment, the methodology developed in this dissertation applies <b>broadly</b> to <b>general</b> order restricted inference...|$|R
50|$|Lawyers in England and Wales {{are divided}} between solicitors and barristers, {{each of which}} have their own {{training}} requirements and customs of practice. Solicitors can be <b>broadly</b> described as <b>general</b> practitioner lawyers who have extensive direct access with clients, and barristers as lawyers who specialise in courtroom advocacy, drafting legal pleadings, and the giving of expert legal opinions.|$|R
40|$|In this paper, {{we report}} the {{metallization}} of a dsDNA template using a novel photography-derived two-step strategy in which dsDNA is first complexed with Ag(I) ions and then irradiated with UV light at 254 nm. The nucleobases act as light harvesters and sensitizers, triggering the photoreduction of the complexed silver ions. This process yields a silver nanoparticles blueprint along the DNA strand. The silver latent image is then developed by depositing metallic nickel through an electroless plating process. This photography-derived procedure generates very homogeneous and evenly distributed strings of silver-core/nickel-shell nanoparticles. Although still discontinuous, {{we believe that}} such chains can serve as the base for obtaining continuous metal nanowires. Furthermore, this process can most likely be extended to other plating metals, resulting in a <b>broadly</b> <b>general</b> procedure for metallizing DNA {{with a variety of}} different materials. Because of the intrinsic simplicity in using light as the key step, this methodology might be amenable to large-scale development, eventually leading to a very efficient molecular-photolithography process...|$|E
40|$|The {{demand for}} {{manpower}} resource by a hotel {{is derived from}} the public's demand for travel goods and services. In filling job vacancies, hotel managers are confronted with an array of internal constraints, industry-specific drivers as well as external economy-wide factors which influence the availability and cost of the labor input required. In this study a model incorporating relevant task and general environmental factors is used to measure their impact on hotel managers' decisions to raise or lower the level of employment in hotels. Industry-specific factors which were found to be statistically significant in influencing the demand for hotel staff included the number of hotel rooms, total revenue earned from room rentals and food and beverage sales, and revenue obtained from the sale of these items. More <b>broadly,</b> <b>general</b> environmental factors of importance were the production index and the gross domestic product in impacting on the number of hotel employees hired by hotel management. School of Hotel and Tourism Managemen...|$|E
40|$|Naturalized Epistemology {{might well}} have been called “Empiricized Epistemology ” instead because it had both <b>broadly</b> <b>general</b> {{epistemological}} motivations and specifically empiricist motivations. On the one hand, there is the empiricist imperative to naturalism: if rationality could be successfully characterized descriptively rather than normatively, then an empirical basis would ground (or at least provide some parameters for) the theorizing. On the other hand, there is the perennial epistemological problem of skepticism. Naturalized epistemology would, it was hoped, finally provide the answer to the skeptical challenge by seeing knowledge as one more natural phenomenon to be explained by empirical means in the vocabulary of the natural sciences. In practice, this meant breaking out of (or into) the justificatory circle by looking at the causes of knowledge. Menashe Schwed claims that the contemporary movements to naturalize epistemology fail on both counts: first, normativity is so deeply inherent in the epistemological project that it cannot, in the end, be eliminated or avoided. The desired sort of naturalistic account of justification cannot be achieved. Second, and consequent to the first, the specter of skepticism still haunts epistemology...|$|E
40|$|Each {{chapter in}} this volume {{describes}} in detail the application of one {{or a group of}} photosensitive molecules to biological research. In this chapter, we take up general prefatory questions: how to determine which molecules are appropriate to use, and what type of compound delivery and light-targeting apparatus for photoactivation is likely to give satisfactory spatial and temporal performance. We enumerate the advan-tages and disadvantages of currently available “caged ” and genetically encoded photosensitive molecules. We also compare current mature and emerging technologies for patterned light delivery, referring as much as possible to <b>broadly</b> applicable <b>general</b> principles. Our goal is to provide a comprehensive over-view with signposts to more detailed treatments...|$|R
40|$|I’m {{interested}} in how interactive tools can encourage users to go beyond a single default visualisation for a data set or simulation, ex-ploring and comparing alternatives {{in the hope of}} gaining a deeper understanding. Here I present two prototype interfaces that tackle this by reifying provenance: one gives the user explicit, instant ac-cess for revisiting parameter values that have been tried over the course of an exploration; the other ties provenance values to the visualisation marks themselves, to guide the user towards settings that lead to interesting alternative results. My hope is that the prin-ciples of these interfaces can be deployed more <b>broadly,</b> on <b>general</b> sensemaking tasks...|$|R
40|$|This {{research}} shows that consumers reward firms for extra effort. More specifically, {{a series of three}} laboratory experiments shows that when firms exert extra effort in making or displaying their products, consumers reward them by increasing their willingness to pay, store choice, and overall evaluations, even if the actual quality of the products is not improved. This rewarding process is defined <b>broadly</b> as <b>general</b> reciprocity. Consistent with attribution theory, the rewarding of generally directed effort is mediated by feelings of gratitude. When consumers infer that effort is motivated by persuasion, however, they no longer feel gratitude and do not reward high-effort firms. (c) 2005 by JOURNAL OF CONSUMER RESEARCH, Inc [...] ...|$|R
40|$|The {{spectral}} bound, s(a A + b V), of {{a combination}} of a resolvent positive linear operator A and an operator of multiplication V, was shown by Kato to be convex in b ∈ R. This is shown here, through an elementary lemma, to imply that s(a A + b V) is also convex in a > 0, and notably, ∂ s(a A + b V) / ∂ a <= s(A) when it exists. Diffusions typically have s(A) <= 0, so that for diffusions with spatially heterogeneous growth or decay rates, greater mixing reduces growth. Models of the evolution of dispersal in particular have found this result when A is a Laplacian or second-order elliptic operator, or a nonlocal diffusion operator, implying selection for reduced dispersal. These cases are shown here {{to be part of a}} single, <b>broadly</b> <b>general,</b> `reduction' phenomenon. Comment: 7 pages, 53 citations. v. 3 : added citations, corrections in introductory definitions. v. 2 : Revised abstract, more text, and details in new proof of Lindqvist's inequalit...|$|E
40|$|Abstract. A {{mathematical}} {{framework for}} Continuous Time Finance based on operator algebraic methods {{offers a new}} direct and entirely constructive perspective on the field. It also leads to new numerical analysis techniques which {{can take advantage of}} the emerging massively parallel GPU architectures which are uniquely suited to execute large matrix manipulations. This is partly a review paper as it covers and expands on the mathematical framework underlying a series of more applied articles. In addition, this article also presents a few key new theorems that make the treatment self-contained. Stochastic processes with continuous time and continuous space variables are defined constructively by establishing new convergence estimates for Markov chains on simplicial sequences. We emphasize high precision computability by numerical linear algebra methods as opposed to the ability of arriving to analytically closed form expressions in terms of special functions. Path dependent processes adapted to a given Markov filtration are associated to an operator algebra. If this algebra is commutative, the corresponding process is named Abelian, a concept which provides a far reaching extension of the notion of stochastic integral. We recover the classic Cameron-Dyson-Feynman-Girsanov-Ito-Kac-Martin theorem as a particular case of a <b>broadly</b> <b>general</b> block-diagonalization algorithm...|$|E
40|$|ABSTRACT. The {{original}} Studentization was {{the conversion}} of a sample mean departure into the familiar t-statistic, plus the derivation of the corresponding Stu-dent distribution function; the observed value of the distribution function is the observed p-value, as presented in an elemental form. We examine this process in a <b>broadly</b> <b>general</b> context: a null statistical model is available together with observed data; a statistic t(y) has been proposed as a plausible measure of {{the location of the}} data relative to what is expected under the null; a modified statistic, say t̃(y), is developed that is ancillary; the corresponding distribution function is determined, exactly or approximately; and the observed value of the distribution function is the p-value or percentage position of the data with respect to the model. Such p-values have had extensive coverage in the recent Bayesian literature, with many variations and some preference for two versions labelled pppost and pcpred. The bootstrap method also directly addresses this Studentization process. We use recent likelihood theory that gives a third order factorization of a reg-ular statistical model into a marginal density for a full dimensional ancillary an...|$|E
5000|$|Beginning in 2008, the Oz Club {{began to}} hold {{conventions}} with {{programs designed to}} appeal more <b>broadly</b> to the <b>general</b> public. They are frequently tied to geographic sites of special interest to Oz fans giving members a chance to visit these locations as a group. These national conventions will next be hosted in the Naperville, Illinois area from August 3-6, 2017. Information about all the conventions {{can be found at}} ozclub.org.|$|R
40|$|This {{dissertation}} discusses Stirling’s {{cycle and}} its contribution using different approaches. There are calculation of Schmidt’s theory and distinctiveness between ideal and real cycle described. Based on my previous research, this work provides a detailed summary of different methods {{which are used}} to stimulate Stirling’s cycles {{as well as the}} motor as a whole. Attention is particularly dedicated to current utilization of this machine which is not <b>broadly</b> known within <b>general</b> public...|$|R
40|$|Propofol (2, 6 -diisopropylphenol) is a <b>broadly</b> used <b>general</b> anesthetic. By {{combining}} spectroscopic {{techniques such}} as 1 - and 2 -color REMPI, UV/UV hole burning, infrared ion-dip spectroscopy (IRIDS) obtained under cooled and isolated conditions with high-level ab initio calculations, detailed information on the molecular structure of propofol and on its interactions with water can be obtained. Four isomers are found for the bare propofol, while only three are detected for the monohydrated species and two for propofol center dot(H 2 O) (2). The isopropyl groups do not completely block the OH solvation site, but reduce considerably {{the strength of the}} hydrogen bonds between propofol and water. Such results may explain the high mobility of propofol in the GABA(A) active site, where it cannot form a strong hydrogen bond with the tyrosine residue...|$|R
40|$|This paper {{provides}} a robust statistical approach to testing the unbiasedness hypothesis in forward exchange market efficiency studies. The methods we use {{allow us to}} work explicitly with levels rather than differenced data. They are statistically robust to data distributions with heavy tails, {{and they can be}} applied to data sets where the frequency of observation and the futures maturity do not coincide. In addition, our methods allow for stochastic trend nonstationarity and general forms of serial dependence. The methods are applied to daily data of spot exchange rates and forward exchange rates during the 1920 's, which marked the first episode of a <b>broadly</b> <b>general</b> floating exchange rate system. The tail behavior of the data is analyzed using an adaptive data-based method for estimating the tail slope of the density. The results confirm the need for the use of robust regression methods. We find cointegration between the forward rate and spot rate for the four currencies we consider (the Belgian and French francs, the Italian lira and the US dollar, all measured against the British pound), we find support for a stationary risk premium {{in the case of the}} Belgian franc, the Italian lira and the US dollar, and we find support for the simple market efficiency hypothesis (where the forward rate is an unbiased predictor of the future spot rate and there is a zero mean risk premium) in the case of the US dollar. ...|$|E
40|$|A {{mathematical}} {{framework for}} Continuous Time Finance based on operator algebraic methods oers a new direct and entirely constructive {{perspective on the}} field. It also leads to new numerical analysis techniques which {{can take advantage of}} the emerging massively parallel GPU architectures which are uniquely suited to execute large matrix manipulations. This is partly a review paper as it covers and expands on the mathematical framework underlying a series of more applied articles. In addition, this article also presents a few key new theorems that make the treatment self-contained. Stochastic processes with continuous time and continuous space variables are defined constructively by establishing new convergence estimates for Markov chains on simplicial sequences. We emphasize high precision computability by numerical linear algebra methods as opposed to the ability of arriving to analytically closed form expressions in terms of special functions. Path dependent processes adapted to a given Markov filtration are associated to an operator algebra. If this algebra is commutative, the corresponding process is named Abelian, a concept which provides a far reaching extension of the notion of stochastic integral. We recover the classic Cameron-Dyson-Feynman-Girsanov-Ito-Kac-Martin theorem as a particular case of a <b>broadly</b> <b>general</b> block-diagonalization algorithm. This technique has many applications ranging from the problem of pricing cliquets to target-redemption-notes and volatility derivatives. Non-Abelian processes are also relevant and appear in several important applications to for instance snowballs and soft calls. We show that in these cases one can eectively use block-factorization algorithms. Finally, we discuss the method of dynamic conditioning that allows one to dynamically correlate over possibly even hundreds of processes in a numerically noiseless framework while preserving marginal distributions. ...|$|E
40|$|International audienceThe {{original}} Studentization was {{the conversion}} of a sample mean departure into the familiar t-statistic, plus the derivation of the corresponding Student distribution function; the observed value of the distribution function is the observed p-value, as presented in an elemental form. We examine this process in a <b>broadly</b> <b>general</b> context: a null statistical model is available together with observed data; a statistic t(y) has been proposed as a plausible measure of {{the location of the}} data relative to what is expected under the null; a modified statistic, say ~t(y), is developed that is ancillary; the corresponding distribution function is determined, exactly or approximately; and the observed value of the distribution function is the p-value or percentage position of the data with respect to the model. Such p-values have had extensive coverage in the recent Bayesian literature, with many variations and some preference for two versions labelled pppost and pcpred. The bootstrap method also directly addresses this Studentization process. We use recent likelihood theory that gives a third order factorization of a regular statistical model into a marginal density for a full dimensional ancillary and a conditional density for the maximum likelihood variable. The full dimensional ancillary is shown to lead to an explicit determination of the Studentized version ~t(y) together with a highly accurate approximation to its distribution function; the observed value of the distribution function is the p-value and is available numerically by direct calculation or by Markov chain Monte Carlo or by other simulations. In this paper, for any given initial or trial test statistic proposed as a location indicator for a data point, we develop: an ancillary based p-value designated panc; a special version of the Bayesian pcpred; and a bootstrap based p-value designated pbs. We then show under moderate regularity that these are equivalent to the third order and have uniqueness as a determination of the statistical location of the data point, as of course derived from the initial location measure. We also show that these p-values have a uniform distribution to third order, as based on calculations in the moderate-deviations region. For implementation the Bayesian and likelihood procedures would perhaps require the same numerical computations, while the bootstrap would require a magnitude more in computation and would perhaps not be accessible. Examples are given to indicate the ease and exibility of the approac...|$|E
40|$|This {{paper is}} based on recent {{research}} into TAFE learning as experienced and expressed by students. The limitations of competency standards and national industry qualifications in particular will be addressed in this paper. Competency standards and national industry qualifications premised upon existing occupational categories are examined {{in terms of their}} capacity to equip TAFE graduates for changing employment patterns and social contexts. As participation in post compulsory education and training continues to grow there is an emerging requirement to move beyond choices in TAFE learning between a narrowed vocational learning and a <b>broadly</b> based <b>general</b> education. Whilst these two approaches offer some choice, increased participation in post compulsory education in Australia calls for more diverse approaches to vocational and technical learning. The diversity of the student populations demands a diversification of approaches to, and choices in, TAFE learning...|$|R
40|$|Extract] It's best, I think, {{to start}} with what we do know, rather than a deficit model of what we don't. What 'digital skills' and {{knowledge}} do you have? What is the extent of your 'digital practice'? Here are some suggestions for steps to take regardless of your feelings of competence {{in the world of}} IT - adopting a broad categorisation of entry level skill; consolidating skill; and mastery. My suggestions are somewhat <b>general,</b> <b>broadly</b> predicated on an assumption of a commitment to reflective practice...|$|R
5000|$|Bosley Crowther, {{critic for}} The New York Times, panned it, writing that [...] "If 'Love Is a Ball,' {{somebody}} fumbled." [...] He found it [...] "predictable nearly {{every step of}} the way" [...] and [...] "laboriously arch in tone, <b>broadly</b> played in <b>general</b> and directed with slapdash aimlessness by Mr. Swift." [...] He noted that the [...] "sun-kissed scenery, though, should set anyone drooling". He mentioned the [...] "determined, good-natured attitude of a game cast headed by Glenn Ford, Hope Lange and Charles Boyer", and in particular found Montalban and Jacobsson [...] "entirely disarming." ...|$|R
