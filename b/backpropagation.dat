4241|5|Public
25|$|A DNN can be discriminatively {{trained with}} the {{standard}} <b>backpropagation</b> algorithm.|$|E
25|$|The {{basics of}} {{continuous}} <b>backpropagation</b> were derived {{in the context}} of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969. In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to the modern version of <b>backpropagation</b> which is efficient even when the networks are sparse. In 1973, Dreyfus used <b>backpropagation</b> to adapt parameters of controllers in proportion to error gradients. In 1974, Werbos mentioned the possibility of applying this principle to ANNs, and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today. In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through <b>backpropagation.</b>|$|E
25|$|Alternatives to <b>backpropagation</b> include extreme {{learning}} machines, no-prop networks, training without backtracking, weightless networks. and non-connectionist neural networks.|$|E
25|$|<b>Backpropagation</b> is {{a method}} to {{calculate}} thegradientof theloss function(produces the cost associated with a given state) {{with respect to the}} weights in the ANN.|$|E
25|$|Today, neural {{networks}} are often {{trained by the}} <b>backpropagation</b> algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to {{neural networks}} by Paul Werbos.|$|E
25|$|To {{overcome}} this problem, Schmidhuber's multi-level hierarchy of networks (1992) pre-trained one level {{at a time}} by unsupervised learning, fine-tuned by <b>backpropagation.</b> Behnke (2003) relied only on {{the sign of the}} gradient (Rprop) on problems such as image reconstruction and face localization.|$|E
25|$|Most employ {{some form}} of {{gradient}} descent, using <b>backpropagation</b> to compute the actual gradients. This is done by simply taking the derivative of the cost function {{with respect to the}} network parameters and then changing those parameters in a gradient-related direction.|$|E
25|$|The {{vanishing}} gradient problem affects many-layered feedforward {{networks that}} use <b>backpropagation</b> and also recurrent neural networks. As errors propagate from layer to layer, they shrink exponentially {{with the number}} of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.|$|E
25|$|The {{original}} goal of {{the neural}} network approach was to solve problems {{in the same way}} that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as <b>backpropagation,</b> or passing information in the reverse direction and adjusting the network to reflect that information.|$|E
25|$|CNNs are {{suitable}} for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard <b>backpropagation.</b> CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Examples of applications in computer vision include DeepDream.|$|E
25|$|Deep {{learning}} often uses convolutional {{neural networks}} (CNNs), whose origins {{can be traced}} back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied <b>backpropagation</b> to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.|$|E
25|$|Schmidhuber {{notes that}} the {{resurgence}} of neural networks {{in the twenty-first century}} is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard <b>backpropagation</b> algorithm feasible for training networks that are several layers deeper than before. The use of parallel GPUs can reduce training times from months to days.|$|E
25|$|A a deep {{stacking}} network (DSN) (deep convex network) {{is based}} on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without <b>backpropagation</b> for the entire blocks.|$|E
25|$|A DBN {{can be used}} to generatively pre-train a DNN {{by using}} the learned DBN weights as the initial DNN weights. <b>Backpropagation</b> or other discriminative {{algorithms}} can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.|$|E
25|$|EEG {{recordings}} do {{not directly}} capture axonal action potentials. An action potential can be accurately {{represented as a}} current quadrupole, meaning that the resulting field decreases more rapidly than the ones produced by the current dipole of post-synaptic potentials. In addition, since EEGs represent averages of thousands of neurons, a large population of cells in synchronous activity is necessary to cause a significant deflection on the recordings. Action potentials are very fast and, as a consequence, the chances of field summation are slim. However, neural <b>backpropagation,</b> as a typically longer dendritic current dipole, can {{be picked up by}} EEG electrodes and is a reliable indication of the occurrence of neural output.|$|E
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised <b>backpropagation</b> for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.|$|E
2500|$|<b>Backpropagation</b> {{training}} algorithms {{fall into}} three categories: ...|$|E
2500|$|The weight updates of <b>backpropagation</b> {{can be done}} via {{stochastic}} {{gradient descent}} using the following equation: ...|$|E
2500|$|A key {{trigger for}} the renewed {{interest}} in neural networks and learning was Paul Werbos's (1975) <b>backpropagation</b> algorithm that effectively solved the exclusive-or problem and more generally accelerated the training of multi-layer networks.|$|E
2500|$|A {{commonly}} used cost function is the mean-squared error, which tries {{to minimize the}} average squared error between the network's output, , and the target value [...] over all the example pairs. Minimizing this cost function using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the <b>backpropagation</b> algorithm for training neural networks.|$|E
50|$|Almeida-Pineda {{recurrent}} <b>backpropagation</b> is {{an extension}} to the <b>backpropagation</b> algorithm that is applicable to recurrent neural networks. It {{is a type of}} supervised learning.|$|E
50|$|The {{phases of}} <b>back{{propagation}}</b> and propagation proceed in parallel. Nevertheless, in each branch (propagation track), the <b>backpropagation</b> starts after the propagation. Once the leave is {{reached in the}} exploration branch, the <b>backpropagation</b> process is triggered automatically by ascending the tree. In the <b>backpropagation</b> phase, {{the exploration of the}} tree goes up from the leaves to the root. Each neighborhood communicates its local minimum to its connections until reaching the root (ant-agent initiator) that marks {{the end of the process}} and selects the cluster corresponding to the global minimum.|$|E
50|$|Typically, {{stochastic}} {{gradient descent}} (SGD) {{is used to}} train the network. The gradient is computed using <b>backpropagation</b> through structure (BPTS), a variant of <b>backpropagation</b> through time used for recurrent neural networks.|$|E
5000|$|<b>Backpropagation</b> is a {{generalization}} {{of the delta}} rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. The <b>backpropagation</b> algorithm has been repeatedly rediscovered and is a special case of a more general technique called automatic differentiation in reverse accumulation mode. It {{is closely related to}} the Gauss-Newton algorithm, and is part of continuing research in neural <b>backpropagation.</b>|$|E
50|$|A DNN can be discriminatively {{trained with}} the {{standard}} <b>backpropagation</b> algorithm. <b>Backpropagation</b> is a method to calculate the gradient of the loss function (produces the cost associated with a given state) {{with respect to the}} weights in an ANN.|$|E
5000|$|... #Caption: Propagation and <b>backpropagation</b> {{mechanisms}} in PB-DNA ...|$|E
5000|$|Most employ {{some form}} of {{gradient}} descent, using <b>backpropagation</b> to compute the actual gradients. This is done by simply taking the derivative of the cost function {{with respect to the}} network parameters and then changing those parameters in a gradient-related direction. <b>Backpropagation</b> training algorithms fall into three categories: ...|$|E
50|$|Neural network <b>backpropagation,</b> η {{stands for}} the {{learning}} rate.|$|E
50|$|Here, ANN {{stands for}} the <b>backpropagation</b> {{artificial}} neural networks, SVM {{stands for the}} support vector machine, SOM for the self-organizing maps. The hybrid technology was developed for engineering applications. In this technology, elastic maps are used in combination with Principal Component Analysis (PCA), Independent Component Analysis (ICA) and <b>backpropagation</b> ANN.|$|E
5000|$|... #Caption: <b>Backpropagation</b> in {{the tree}} of local minima in PB-DNA ...|$|E
5000|$|... #Subtitle level 2: Artificial Neural Networks: Standard <b>Backpropagation</b> Networks and Their Training ...|$|E
50|$|While a backpropagating action {{potential}} can presumably cause {{changes in the}} weight of the presynaptic connections, there is no simple mechanism for an error signal to propagate through multiple layers of neurons, as in the computer <b>backpropagation</b> algorithm. However, simple linear topologies have shown that effective computation is possible through signal <b>backpropagation</b> in this biological sense.|$|E
5000|$|Paul J. Werbos (born 1947) is a {{scientist}} {{best known for}} his 1974 Harvard University Ph.D. thesis, which first described the process of training artificial neural networks through <b>backpropagation</b> of errors. The thesis, and some supplementary information, can be found in his book, The Roots of <b>Backpropagation</b> (...) [...] He also was a pioneer of recurrent neural networks.|$|E
50|$|This is {{the reason}} why <b>backpropagation</b> {{requires}} the activation function to be differentiable.|$|E
50|$|In 2014, <b>backpropagation</b> {{was used}} to train a deep neural network for speech recognition.|$|E
5000|$|<b>Backpropagation</b> can be {{used with}} any {{gradient-based}} optimizer, such as L-BFGS or truncated Newton.|$|E
