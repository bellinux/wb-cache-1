488|88|Public
2500|$|A common {{choice is}} a Gaussian kernel, {{which has a}} single {{parameter}} '. The best combination of C and [...] is often selected by a grid search with exponentially growing sequences of C and ', for example, [...] Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in <b>Bayesian</b> <b>optimization</b> {{can be used to}} select C and ' , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.|$|E
5000|$|... {{spearmint}} Spearmint is {{a package}} to perform <b>Bayesian</b> <b>optimization</b> of machine learning algorithms.|$|E
50|$|<b>Bayesian</b> <b>optimization</b> is a {{methodology}} {{for the global}} optimization of noisy black-box functions. Applied to hyperparameter optimization, <b>Bayesian</b> <b>optimization</b> consists of developing a statistical model of the function from hyperparameter values to the objective evaluated on a validation set. Intuitively, the methodology assumes {{that there is some}} smooth but noisy function that acts as a mapping from hyperparameters to the objective. In <b>Bayesian</b> <b>optimization,</b> one aims to gather observations in such a manner as to evaluate the machine learning model the least number of times while revealing as much information as possible about this function and, in particular, the location of the optimum. <b>Bayesian</b> <b>optimization</b> relies on assuming a very general prior over functions which when combined with observed hyperparameter values and corresponding outputs yields a distribution over functions. The methodology proceeds by iteratively picking hyperparameters to observe (experiments to run) in a manner that trades off exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters which are expected to have a good outcome). In practice, <b>Bayesian</b> <b>optimization</b> has been shown to obtain better results in fewer experiments than grid search and random search, due to the ability to reason about the quality of experiments before they are run.|$|E
5000|$|Auto-WEKA is a <b>Bayesian</b> {{hyperparameter}} <b>optimization</b> {{layer on}} top of WEKA.|$|R
5000|$|MOE MOE is a Python/C++/CUDA library {{implementing}} <b>Bayesian</b> Global <b>Optimization</b> using Gaussian Processes.|$|R
40|$|Abstract. Based on {{bayesian}} statistics {{theory and}} reliability-based optimization design, the research presents a new reliability-based optimization design approach that solves {{the form of}} finite test samples. In the article, the <b>bayesian</b> reliability-based <b>optimization</b> mathematical model is established and the <b>bayesian</b> reliability-based <b>optimization</b> approach of torsion bar is proposed. The method adopts a bayesian inference technique to estimate reliability, gives definition of bayesian reliability. The results illustrates the method presented is an efficient and practical reliability-based optimization approach of torsion bar...|$|R
5000|$|<b>Bayesian</b> <b>optimization,</b> a {{sequential}} design {{strategy for}} global optimization of black-box functions using Bayesian statistics ...|$|E
50|$|<b>Bayesian</b> <b>optimization</b> is a {{sequential}} design strategyfor global optimization of black-box {{functions that}} doesn't require derivatives.|$|E
5000|$|Bayesopt, an {{efficient}} implementation of <b>Bayesian</b> <b>optimization</b> in C/C++ with support for Python, Matlab and Octave.|$|E
40|$|In this paper, {{denoising}} on multicomponent {{images is}} performed. The presented procedure is a spatial waveletbased denoising techniques, based on <b>Bayesian</b> leastsquares <b>optimization</b> procedures, using a prior {{model for the}} wavelet coefficients that account for the intercorrelations between the multicomponent bands. The applied prior model for the multicomponent signal is a Gaussian Scale Mixture (GSM) model. The method is compared to single-band wavelet denoising and to multiband denoising using a Gaussian prior. Experiments on a Landsat multispectral remote sensing image are conducted. ...|$|R
40|$|We empirically {{evaluate}} a stochastic annealing strategy for <b>Bayesian</b> posterior <b>optimization</b> with variational inference. Variational inference is a deterministic approach to approximate posterior inference in Bayesian models {{in which a}} typically non-convex objective function is locally optimized over {{the parameters of the}} approximating distribution. We investigate an annealing method for optimizing this objective with the aim of finding a better local optimal solution and compare with deterministic annealing methods and no annealing. We show that stochastic annealing can provide clear improvement on the GMM and HMM, while performance on LDA tends to favor deterministic annealing methods...|$|R
40|$|Abstract. In this work, {{we present}} a {{generalisation}} to continuous domains of an optimization method based on evolutionary computation that applies Bayesian classifiers in the learning process. The main difference between other estimation of distribution algorithms (EDAs) and this new method –known as Evolutionary <b>Bayesian</b> Classifier-based <b>Optimization</b> Algorithms (EBCOAs) – {{is the way the}} fitness function is taken into account, as a new variable, to generate the probabilistic graphical model that will be applied for sampling the next population. We also present experimental results to compare performance of this new method with other methods of the evolutionary computation field like evolution strategies, and EDAs. Results obtained show that this new approach can at least obtain similar performance as these other paradigms 1. ...|$|R
50|$|Examples of {{acquisition}} functions include {{probability of}} improvement,expected improvement, Bayesian expected losses, upper confidence bounds (UCB), Thompson samplingand mixtures of these. They all trade-off exploration and exploitation {{so as to}} minimize the number of function queries. As such, <b>Bayesian</b> <b>optimization</b> is well suited for functions that are very expensive to evaluate.|$|E
50|$|Nando de Freitas is a Professor of Computer Science at the University of Oxford. He {{is also a}} Fellow of Linacre College, Oxford. De Freitas {{is noted}} as an {{authority}} {{in the field of}} machine learning, and in particular in the subfields of neural networks, Bayesian inference and <b>Bayesian</b> <b>optimization,</b> and deep learning.|$|E
50|$|The Data-based Online Nonlinear Extremumseeker (DONE) {{algorithm}} is a black-box optimization algorithm.DONE models the unknown cost function {{and attempts to}} find an optimum of the underlying function.The DONE {{algorithm is}} suitable for optimizing costly and noisy functions and does not require derivatives.An advantage of DONE over similar algorithms, such as <b>Bayesian</b> <b>Optimization,</b> is that the computational cost per iteration is independent {{of the number of}} function evaluations.|$|E
40|$|ABSTRACT: Bayesian {{estimation}} {{techniques are}} applied {{to the problem of}} time and frequency offset estimation for Global Positioning System receivers. The estimation technique employs Markov Chain Monte Carlo (MCMC) to estimate unknown system parameters, utilizing a novel, multi-dimensional, <b>Bayesian,</b> global <b>optimization</b> strategy for initializing a Metropolis-Hastings proposal distribution. The technique enables the design of a high performance multi-user GPS receiver, capable of overcoming the near-far problem when the relative signal power is on the order of 5 dB (single antenna element) and 20 dB (4 antenna element array) and providing dramatically improved performance over conventional matched filter techniques against interference and jamming when the relative jammer and satellite signal power is on the order of 20 dB (4 antenna element array) ...|$|R
40|$|We {{consider}} monotonic, {{multiple regression}} {{for a set}} of contiguous regions (lattice data). The regression functions permissibly vary between regions and exhibit geographical structure. We develop new Bayesian non-parametric methodology which allows for both continuous and discontinuous functional shapes and which are estimated using marked point processes and reversible jump Markov Chain Monte Carlo techniques. Geographical dependency is incorporated by a flexible prior distribution; the parametrisation allows the dependency to vary with functional level. The approach is tuned using <b>Bayesian</b> global <b>optimization</b> and cross-validation. Estimates enable variable selection, threshold detection and prediction as well as the extrapolation of the regression function. Performance and flexibility of our approach is illustrated by simulation studies and an application to a Norwegian insurance data set. Comment: 31 pages, 9 Figure...|$|R
40|$|Walras {{theory is}} well known and widely used in models of market economy [3]. Various {{iterative}} methods are developed {{to search for the}} equilibrium conditions. In this paper a search for the Nash equilibrium [2] is deЇned as a stochastic global optimization problem. The improved version of web-based optimization software is presented. The speciЇc property is simulation of stochastic customer arrivals and stochastic service times and application of <b>Bayesian</b> Approach to <b>optimization</b> [1]...|$|R
50|$|In the {{mathematical}} theory of probability, the Wiener process, named after Norbert Wiener, is a stochastic process used in modeling various phenomena, including Brownian motion and fluctuations in financial markets. A formula for the conditional probability {{distribution of the}} extremum of the Wiener process and a sketch of its proof appears in work of H. J. Kusher published in 1964. a detailed constructive proof appears in work of Dario Ballabio in 1978. This result was developed within a research project about <b>Bayesian</b> <b>optimization</b> algorithms.|$|E
5000|$|The {{effectiveness}} of SVM {{depends on the}} selection of kernel, the kernel's parameters, and soft margin parameter C.A common choice is a Gaussian kernel, which has a single parameter ''''. The best combination of C and [...] is often selected by a grid search with exponentially growing sequences of C and '''', for example, [...] Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in <b>Bayesian</b> <b>optimization</b> {{can be used to}} select C and '''' , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.|$|E
5000|$|In some global {{optimization}} {{problems the}} analytical {{definition of the}} objective function is unknown {{and it is only}} possible to get values at fixed points. There are objective functions in which the cost of an evaluation is very high, for example when the evaluation is the result of an experiment or a particularly onerous measurement. In these cases, the search of the global extremum (maximum or minimum) can be carried out using a methodology named [...] "Bayesian optimization", which tend to obtain a priori the best possible result with a predetermined number of evaluations. In summary it is assumed that outside the points in which it has already been evaluated, the objective function has a pattern which can be represented by a stochastic process with appropriate characteristics. The stochastic process is taken as a model of the objective function, assuming that the probability distribution of its extrema gives the best indication about extrema of the objective function. In the simplest case of the one-dimensional optimization, given that the objective function has been evaluated in a number of points, there is the problem to choose in which of the intervals thus identified is more appropriate to invest in a further evaluation. If a Wiener stochastic process is chosen {{as a model for the}} objective function, it is possible to calculate the probability distribution of the model extreme points inside each interval, conditioned by the known values at the interval boundaries. The comparison of the obtained distributions provides a criterion for selecting the interval in which the process should be iterated. The probability value of having identified the interval in which falls the global extremum point of the objective function can be used as a stopping criterion. <b>Bayesian</b> <b>optimization</b> is not an efficient method for the accurate search of local extrema so, once the search range has been restricted, depending on the characteristics of the problem, a specific local optimization method can be used.|$|E
40|$|This paper {{presents}} a nonlinear mixing model for hyperspectral image unmixing. The proposed model {{assumes that the}} pixel reflectances are nonlinear functions of pure spectral components contaminated by an additive white Gaussian noise. These nonlinear functions are approximated using polynomial functions leading to a polynomial postnonlinear mixing model. A <b>Bayesian</b> algorithm and <b>optimization</b> methods are proposed to estimate the parameters involved in the model. The performance of the unmixing strategies is evaluated by simulations conducted on synthetic and real data...|$|R
40|$|We {{deal with}} the {{efficient}} parallelization of <b>Bayesian</b> global <b>optimization</b> algorithms, and more specifically of those based on the expected improvement criterion and its variants. A closed form formula relying on multivariate Gaussian cumulative distribution functions is established for a generalized version of the multipoint expected improvement criterion. In turn, the latter relies on intermediate results that could be of independent interest concerning moments of truncated Gaussian vectors. The obtained expansion of the criterion enables studying its differentiability with respect to point batches and calculating the corresponding gradient in closed form. Furthermore, we derive fast numerical approximations of this gradient and propose efficient batch optimization strategies. Numerical experiments illustrate that the proposed approaches enable computational savings of between one and two order of magnitudes, hence enabling derivative-based batch-sequential acquisition function maximization to become a practically implementable and efficient standard...|$|R
40|$|We {{present an}} {{adaptive}} dose escalation scheme for cancer phase I clinical trials {{which is based}} on a parametric quantal response model. The dose escalation is Bayesian-feasible, Bayesian-optimal and consistent. It is designed to approach the maximum tolerated dose as fast as possible subject to the constraint that the predicted probability of assigning doses higher than the maximum tolerated dose is equal to a specified value. Dose escalation scheme Cancer phase I clinical trials Bayesian adaptive procedure Constrained <b>optimization</b> <b>Bayesian</b> feasible scheme Consistent escalation scheme...|$|R
40|$|<b>Bayesian</b> <b>optimization</b> forms {{a set of}} {{powerful}} tools that allows efficient blackbox optimization and has general applications in a large variety of fields. In this work we seek to advance <b>Bayesian</b> <b>optimization</b> both in the theoretical and the practical fronts as well as apply <b>Bayesian</b> <b>optimization</b> to novel and difficult problems in order to advance {{the state of the}} art. Chapter 1 gives a broad overview of <b>Bayesian</b> <b>optimization.</b> We start by covering the published applications of <b>Bayesian</b> <b>optimization.</b> The chapter then proceeds to introduce the essential ingredients of <b>Bayesian</b> <b>optimization</b> in depth. After going through some practical considerations, the theory and history of <b>Bayesian</b> <b>optimization,</b> we end the chapter with a discussion on the latest extensions and open problems. In Chapters 2 - 4, we solve three outstanding problems in the <b>Bayesian</b> <b>optimization</b> literature. Traditional <b>Bayesian</b> <b>optimization</b> approaches need to solve an auxiliary non-convex global optimization problem in the inner loop. The difficulties in solving this auxiliary optimization problem not only break the assumptions of most theoretical works in this area but also lead to computationally inefficient solutions. In Chapter 2, we propose the first algorithm in <b>Bayesian</b> <b>optimization</b> that does not need to solve auxiliary optimization problems and prove its convergence. In <b>Bayesian</b> <b>optimization,</b> it is often important to tune the hyper-parameters of the underlying Gaussian processes. There did not exist theoretical results that allow noisy observations {{and at the same time}} varying hyper-parameters. Chapter 3, proves the first such result. <b>Bayesian</b> <b>optimization</b> is very effective when the dimensionality of the problem is low. Scaling <b>Bayesian</b> <b>optimization</b> to high dimensionality, however, has been a long standing open problem of the field. In Chapter 4, we develop an algorithm that extends <b>Bayesian</b> <b>optimization</b> to very high dimensionalities where the underlying problems have low intrinsic dimensionality. We also prove theoretical guarantees of the proposed algorithm. In Chapter 5, we turn our attention to improving an essential component of Bayesian optimization: acquisition functions. Acquisition functions form a critical component of <b>Bayesian</b> <b>optimization</b> and yet there does not exist an optimal acquisition function that is easily computable. Instead of relying on one acquisition function, we develop a new information-theoretic portfolio of acquisition functions. We show empirically that our approach is more effective than any single acquisition function in the portfolio. Last but not least, in Chapter 6 we adapt <b>Bayesian</b> <b>optimization</b> to derive an adaptive Hamiltonian Monte Carlo sampler. Hamiltonian Monte Carlo is one of the most effective MCMC algorithms. It is, however, notoriously difficult to tune. In this chapter, we follow the approach of adapting Markov chains in order to improve their convergence where our adaptive strategy is based on <b>Bayesian</b> <b>optimization.</b> We provide theoretical analysis as well as a comprehensive set of experiments demonstrating the effectiveness of our proposed algorithm. </p...|$|E
40|$|<b>Bayesian</b> <b>optimization</b> {{has shown}} to be a {{fundamental}} global optimization algorithm in many applications: ranging from automatic machine learning, robotics, reinforcement learning, experimental design, simulations, etc. The most popular and effective <b>Bayesian</b> <b>optimization</b> relies on a surrogate model {{in the form of a}} Gaussian process due to its flexibility to represent a prior over function. However, many algorithms and setups relies on the stationarity assumption of the Gaussian process. In this paper, we present a novel nonstationary strategy for <b>Bayesian</b> <b>optimization</b> that is able to outperform the state of the art in <b>Bayesian</b> <b>optimization</b> both in stationary and nonstationary problems...|$|E
40|$|We {{present a}} {{tutorial}} on <b>Bayesian</b> <b>optimization,</b> {{a method of}} finding the maximum of expensive cost functions. <b>Bayesian</b> <b>optimization</b> employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which {{must take into account}} both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of <b>Bayesian</b> <b>optimization,</b> with experiments [...] -active user modelling with preferences, and hierarchical reinforcement learning [...] -and a discussion of {{the pros and cons of}} <b>Bayesian</b> <b>optimization</b> based on our experiences...|$|E
40|$|Abstract—This paper {{presents}} a nonlinear mixing model for hyperspectral image unmixing. The proposed model {{assumes that the}} pixel reflectances are nonlinear functions of pure spectral components contaminated by an additive white Gaussian noise. These nonlinear functions are approximated using polynomial functions leading to a polynomial postnonlinear mixing model. A <b>Bayesian</b> algorithm and <b>optimization</b> methods are proposed to estimate the parameters involved in the model. Theperformanceoftheunmixing strategies is evaluated by simulations conducted on synthetic and real data. Index Terms—Hyperspectral imagery, postnonlinear model, spectral unmixing (SU). I...|$|R
40|$|International audienceIn this paper, {{we present}} the {{application}} of a recently developed algorithm for <b>Bayesian</b> multi-objective <b>optimization</b> to the design of a commercial aircraft environment control system (ECS). In our model, the ECS is composed of two cross-flow heat exchangers, a centrifugal compressor and a radial turbine, the geometries of which are simultaneously optimized to achieve minimal weight and entropy generation of the system. While both objectives impact the overall performance of the aircraft, they are shown to be antagonistic and a set of trade-off design solutions is identified. The algorithm used for optimizing the system implements a Bayesian approach to the multi-objective optimization problem in the presence of non-linear constraints and the emphasis is on conducting the optimization using a limited number of system simulations. Noteworthy features of this particular application include a non-hypercubic design domain and the presence of hidden constraints due to simulation failures...|$|R
40|$|Models have {{parameters}} {{that need}} to be determined from experimental observations. The problem of determining these parameters is known as the inverse problem or the model calibration problem. Solving inverse problems can be ubiquitously difficult because when the models involved are computationally expensive, one can only make a limited number of simulations. This work addresses the issue of solving an inverse problem with a limited data budget. Towards this end, we pose the inverse problem as the problem of minimizing a loss function that measures the discrepancy between model predictions and experimental measurements. Then, we employ <b>Bayesian</b> global <b>optimization</b> (BGO) to actively select the most informative simulations until either the expected improvement falls below a user defined threshold or our computational budget has been exhausted. We apply our results to the problem of estimating the kinetic rate coefficients modeling the catalytic conversion of nitrate to nitrogen using real experimental data...|$|R
40|$|In typical {{applications}} of <b>Bayesian</b> <b>optimization,</b> minimal assumptions are {{made about the}} objective function being optimized. This is true even when researchers have prior information about {{the shape of the}} function with respect to one or more argument. We make the case that shape constraints are often appropriate in at least two important application areas of Bayesian optimization: (1) hyperparameter tuning of machine learning algorithms and (2) decision analysis with utility functions. We describe a methodology for incorporating a variety of shape constraints within the usual <b>Bayesian</b> <b>optimization</b> framework and present positive results from simple applications which suggest that <b>Bayesian</b> <b>optimization</b> with shape constraints is a promising topic for further research. Comment: NIPS 2016 <b>Bayesian</b> <b>Optimization</b> Worksho...|$|E
40|$|This paper {{presents}} a <b>Bayesian</b> <b>optimization</b> method with exponential convergence {{without the need}} of auxiliary optimization and without the delta-cover sampling. Most <b>Bayesian</b> <b>optimization</b> methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing <b>Bayesian</b> <b>optimization</b> method with exponential convergence requires access to the delta-cover sampling, which {{was considered to be}} impractical. Our approach eliminates both requirements and achieves an exponential convergence rate. Comment: In NIPS 2015 (Advances in Neural Information Processing Systems 2015...|$|E
30|$|GP-based <b>Bayesian</b> <b>optimization</b> {{is shown}} in Algorithm 1.|$|E
40|$|In this paper, {{we study}} {{denoising}} of multicomponent images. The presented procedures are spatial wavelet-based denoising techniques, based on <b>Bayesian</b> leastsquares <b>optimization</b> procedures, using prior {{models for the}} wavelet coefficients that account for {{the correlations between the}} spectral bands. We analyze three mixture priors: Gaussian scale mixture models, Bernoulli-Gaussian mixture models and Laplacian mixture models. These three prior models are studied within the same framework of least-squares optimization. The presented procedures are compared to Gaussian prior model and single-band denoising procedures. We analyze the suppression of non-correlated as well as correlated white Gaussian noise on multispectral and hyperspectral remote sensing data and Rician distributed noise on multiple images of within-modality magnetic resonance data. It is shown that a superior denoising performance is obtained when a) the interband covariances are fully accounted for and b) prior models are used that better approximate the marginal distributions of the wavelet coefficients...|$|R
40|$|The optimal {{portfolio}} selection {{problem has}} long been of interest to both academics and practitioners. A higher moments <b>Bayesian</b> portfolio <b>optimization</b> model can overcome the shortcomings of the traditional Markowitz approach and {{take into consideration the}} skewness of asset returns and parameter uncertainty. This paper presents a comparison between the simulated annealing and the nonlinear programming methods of <b>optimization</b> for the <b>Bayesian</b> portfolio selection problem in which the objective function includes the portfolio mean, variance and skewness. We make the comparison for a utility function that is easily optimized using both methods. In particular we maximize a cubic utility function, and our results show that to achieve the same level of accuracy, the CPU time for the nonlinear programming optimization will be shorter than for the simulated annealing algorithm. Though it is slower, the simulated annealing algorithm is still a viable option for this utility function. ...|$|R
40|$|In this {{position}} {{paper we discuss}} optimization in the HCI domain based on our experiences with Bayesian methods for modeling and optimization of audio systems, including challenges related to evaluating, designing, and optimizing such interfaces. We outline and demonstrate how a combined <b>Bayesian</b> modeling and <b>optimization</b> approach provides a flexible framework for integrating various user and content attributes, while also supporting model-based optimization of HCI systems. Finally, we discuss current and future research direction and applications, such as inferring user needs and optimizing interfaces for computer assisted teaching...|$|R
