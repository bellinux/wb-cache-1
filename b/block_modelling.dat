22|2995|Public
40|$|Key {{studies of}} social {{movement}} networks use <b>block</b> <b>modelling</b> to uncover movement network structures. Whilst {{it is promising to}} see mathematical sociology techniques applied to social movement studies, {{there are grounds}} for engendering a closer connection between these two fields of study. The mathematical sociology literature recommends, for example, that analyzed networks should be complete and relatively dense, some degree of deduction should be applied to select the ‘best’ model, levels of equivalence and / or error scores should be specified, and reliable and appropriate algorithms and levels of equivalence should be carefully selected. Some dilemmas involved in <b>block</b> <b>modelling</b> analysis are demonstrated through <b>block</b> <b>modelling</b> analysis of inter-organizational networking in Friends of the Earth International (FoEI) ...|$|E
40|$|Simulation {{of current}} {{interruption}} is currently performed with non-ideal switching devices for large power systems. Nevertheless, for small networks, non-ideal switching devices {{can be substituted}} by arc models. However, this substitution has {{a negative impact on}} the computation time. At the same time, these simulations are useful to design switchgear. Although these simulations are for large power systems cumbersome with traditional modelling methods, the <b>block</b> <b>modelling</b> method can handle arc models for any size of networks. The main advantage of applying the <b>block</b> <b>modelling</b> method is that the computation of the analytical Jacobian matrix is possible and cheap for any number of arc models. The computation time is smaller with this approach than with the traditional approach. Delft Institute of Applied MathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|Weak {{increases}} in classroom cohesion for three seventh-grade classes instructed by a mastery learning technique compared to three conventionally instructed classes are demonstrated using block analysis of variance, triadic census (balance theory), and <b>block</b> <b>modelling</b> (role theory) analytic techniques on repeated-measures sociometric rating eata. Each technique is described and clear examples of its use given. Since increased cohesion may have {{positive effects on}} student mental health, {{it is argued that}} researchers should routinely assess changes in classroom cohesion by these or other metliods during classroom interventions. (Authors) Documents acquired by ERIC incli. de Lany informal unpublished materials not available from ott,,er sources. ERIC makes every effort to obtain the best copy available. Nevertheless, items of margina...|$|E
50|$|The key {{difference}} between grid <b>models</b> and <b>block</b> <b>models</b> {{is that a}} gridded surface (e.g. a stratigraphic contact) cannot fold or wrap under itself whereas an isosurface within a <b>block</b> <b>model</b> can. Stated differently, when dealing with grids, there can only be one z-value for any given xy coordinate. On the other hand, when dealing with <b>block</b> <b>models,</b> there can only be one g-value for any given xyz coordinate. Another major difference is that gridding is computationally fast while <b>block</b> <b>modeling</b> can be very slow.|$|R
5000|$|The <b>block</b> <b>model</b> {{is created}} using geostatistics and the {{geological}} data gathered through drilling of the prospective ore zone. The <b>block</b> <b>model</b> {{is essentially a}} set of specifically sized [...] "blocks" [...] {{in the shape of}} the mineralized orebody. Although the blocks all have the same size, the characteristics of each block differ. The grade, density, rock type and confidence are all unique to each block within the entire <b>block</b> <b>model.</b> An example of a <b>block</b> <b>model</b> is shown on the right. Once the <b>block</b> <b>model</b> has been developed and analyzed, it is used to determine the ore resources and reserves (with project economics considerations) of the mineralized orebody. Mineral resources and reserves can be further classified depending on their geological confidence.|$|R
30|$|Finally, {{results from}} the three <b>block</b> <b>models</b> were {{combined}} in {{a new series of}} models by adding to Model 1 (‘dependent children’ covariate only): first variables from the sociodemographic <b>block</b> <b>model</b> (to give Model 2), then those from the psychosocial factors <b>block</b> <b>model</b> (to give Model 3), and finally those from the gambling <b>block</b> <b>model</b> (to give Model 4), with variables in each block being added together as a group. At each addition, any variables (apart from dependent children) that were found to have a Wald Chi Square test ≥  0.05 were removed one at a time according to the size of their p-value.|$|R
40|$|In {{this paper}} we propose {{methodology}} for inference of binary-valued adjacency matrices from various measures {{of the strength of}} association between pairs of network nodes, or more generally pairs of variables. This strength of association can be quantified by sample covariance and correlation matrices, and more generally by test-statistics and hypothesis test p-values from arbitrary distributions. Community detection methods such as <b>block</b> <b>modelling</b> typically require binary-valued adjacency matrices as a starting point. Hence, a main motivation for the methodology we propose is to obtain binary-valued adjacency matrices from such pairwise measures of strength of association between variables. The proposed methodology is applicable to large high-dimensional data-sets and is based on computationally efficient algorithms. We illustrate its utility in a range of contexts and data-sets...|$|E
40|$|This {{paper is}} {{original}} research that demonstrates new design possibilities for evaluation in the schematic phase of design {{through the use}} rapid prototyping {{as a tool of}} representation verses 2 D drawing. These program shapes are created from CAD files using a threedimensional printing and laser cutting CAM tools. This way of working is in response to two dimensional plan representation and evaluation (Mitchell 1976). This research combines the best of the visual aspects of plan representation and the formal representation of solid <b>block</b> <b>modelling.</b> The models in this paper demonstrate the buildingis physical scale of spaces, building use and overall form. Resulting models will demonstrate a new way of designing in CAD one that combined physical and visual ways or representation. ...|$|E
40|$|Reviewed {{simulation}} model of correlation-extreme navigation system (CASN) {{of the aircraft}} (A/C) using the mi- crowave radiation of the earth's surface, allowing to optimize system performance, ranging {{from the kind of}} used signal and the diagram of the radiometer to the expected positional accuracy of CASN. The developed model consists of the simula- tion of the radiometer; <b>block</b> <b>modelling</b> the radiating properties of the underlying surface, the unit of account of the influ- ence of the atmosphere; block kinematics modeling of the movement of the carrier, the characteristics of the antenna sys- tem and method of the review space; the unit for computing the coordinates. These model allows to estimate the infor- mation content and the stability of the radiation of the earth surface plot of the correction of the trajectory, the influence o...|$|E
50|$|Several {{variants}} {{of the model}} exist. One minor tweak allocates vertices to communities randomly, according to a categorical distribution, {{rather than in a}} fixed partition. More significant variants include the censored <b>block</b> <b>model</b> and the mixed-membership <b>block</b> <b>model.</b>|$|R
40|$|Abstract. The {{stochastic}} <b>block</b> <b>model</b> is {{a powerful}} tool for inferring community structure from network topology. However, it predicts a Pois-son degree distribution within each community, while most real-world networks have a heavy-tailed degree distribution. The degree-corrected <b>block</b> <b>model</b> can accommodate arbitrary degree distributions within com-munities. But since it takes the vertex degrees as parameters rather than generating them, it cannot use them to help it classify the vertices, and its natural generalization to directed graphs cannot even use the ori-entations of the edges. In this paper, we present variants of the <b>block</b> <b>model</b> with the best of both worlds: they can use vertex degrees and edge orientations in the classification process, while tolerating heavy-tailed degree distributions within communities. We show that for some networks, including synthetic networks and networks of word adjacencies in English text, these new <b>block</b> <b>models</b> achieve a higher accuracy than either standard or degree-corrected <b>block</b> <b>models...</b>|$|R
40|$|To {{capture the}} {{inherent}} geometric features of many community detection problems, we propose {{to use a}} new random graph model of communities that we call a Geometric <b>Block</b> <b>Model.</b> The geometric <b>block</b> <b>model</b> generalizes the random geometric graphs {{in the same way}} that the well-studied stochastic <b>block</b> <b>model</b> generalizes the Erdos-Renyi random graphs. It is also a natural extension of random community models inspired by the recent theoretical and practical advancement in community detection. While being a topic of fundamental theoretical interest, our main contribution is to show that many practical community structures are better explained by the geometric <b>block</b> <b>model.</b> We also show that a simple triangle-counting algorithm to detect communities in the geometric <b>block</b> <b>model</b> is near-optimal. Indeed, even in the regime where the average degree of the graph grows only logarithmically with the number of vertices (sparse-graph), we show that this algorithm performs extremely well, both theoretically and practically. In contrast, the triangle-counting algorithm is far from being optimum for the stochastic <b>block</b> <b>model.</b> We simulate our results on both real and synthetic datasets to show superior performance of both the new model as well as our algorithm...|$|R
40|$|International audienceWe {{develop a}} model in which {{interactions}} between nodes of a dynamic network are counted by non homogeneous Poisson processes. In a <b>block</b> <b>modelling</b> perspective, nodes belong to hidden clusters (whose number is unknown) and the intensity functions of the counting processes only depend on the clusters of nodes. In order to make inference tractable we move to discrete time by partitioning the entire time horizon in which interactions are observed in fixed-length time sub-intervals. First, we derive an exact integrated classification likelihood criterion and maximize it relying on a greedy search approach. This allows to estimate the memberships to clusters {{and the number of}} clusters simultaneously. Then a maximum-likelihood estimator is developed to estimate non parametrically the integrated intensities. We discuss the over-fitting problems of the model and propose a regularized version solving these issues. Experiments on real and simulated data are carried out in order to assess the proposed methodology...|$|E
40|$|Abstract Due to {{the strong}} {{polarisation}} of economic activities in space and rise in collaborative behaviour, increasing attention has recently been devoted {{to the relationship between}} geography and network formation. The studies conducted on this topic reveal a high variation in terms of methodologies. Putting special emphasis on R&D networks, the aim of this chapter is to review the different methods and assess their ability to address the issues raised by the relationship between network and space. We first discuss the different facets of the relationship between geography and networks. Then, we detail the methodological approaches and their capability to test each effect of geography on network formation. We argue that the effect of distance on dyads have received the major attention so far, but the development of <b>block</b> <b>modelling</b> and top-down approaches opens new research perspectives on how distance or location might affect formation of more complex structures. Moreover, recent improvement in temporal models also offer...|$|E
40|$|We {{develop a}} model in which {{interactions}} between nodes of a dynamic network are counted by non homogeneous Poisson processes. In a <b>block</b> <b>modelling</b> perspective, nodes belong to hidden clusters (whose number is unknown) and the intensity functions of the counting processes only depend on the clusters of nodes. In order to make inference tractable we move to discrete time by partitioning the entire time horizon in which interactions are observed in fixed-length time sub-intervals. First, we derive an exact integrated classification likelihood criterion and maximize it relying on a greedy search approach. This allows to estimate the memberships to clusters {{and the number of}} clusters simultaneously. Then a maximum-likelihood estimator is developed to estimate non parametrically the integrated intensities. We discuss the over-fitting problems of the model and propose a regularized version solving these issues. Experiments on real and simulated data are carried out in order to assess the proposed methodology...|$|E
50|$|The {{building}} <b>block</b> <b>model</b> is a {{tool for}} spreading or amortizing the expenditure of a regulated firm over time. The building <b>block</b> <b>model,</b> when applied correctly and consistently over time, ensures that the firm earns a revenue stream with a present value equal to {{the present value of}} its expenditure stream. Put another way, the building <b>block</b> <b>model</b> ensures that {{over the life of the}} firm, the cash-flow stream of the firm has a net present value equal to zero.|$|R
40|$|Analysis of the {{topology}} of a graph, {{regular or}} bipartite one, {{can be done}} by clustering for regular ones or co-clustering for bipartite ones. The Stochastic <b>Block</b> <b>Model</b> and the Latent <b>Block</b> <b>Model</b> are two models, which are very similar for respectively regular and bipartite graphs, based on probabilistic models. Initially developed for binary graphs, these models have been extended to valued networks with optional covariates on the edges. This paper present a implementation of a Variational EM algorithm for Stochastic <b>Block</b> <b>Model</b> and Latent <b>Block</b> <b>Model</b> for some common probability functions, Bernoulli, Gaussian and Poisson, without or with covariates, with some standard flavors, like multivariate extensions. This implementation allow automatic group number exploration and selection via the ICL criterion, and allow analyze networks with thousands of nodes in a reasonable amount of time...|$|R
50|$|The {{building}} <b>block</b> <b>model</b> {{is a form}} {{of public}} utility regulation that is common in Australia. Variants of the building <b>block</b> <b>model</b> are currently used in Australia in the regulation of electricity transmission and distribution, gas transmission and distribution, railways, postal services, urban water and sewerage services, irrigation infrastructure, and port access. The Australian Competition and Consumer Commission has stated that it intends to use a version of the building <b>block</b> <b>model</b> to determine indicative access prices for fixed-line telecommunications services. The building <b>block</b> <b>model</b> is so-called because the allowed revenue of the regulated firm is equal to the sum of underlying components or building blocks consisting of the return on capital, the return of capital (also known as depreciation), the operating expenditure, and various other components such as taxes and incentive mechanisms.|$|R
40|$|Since the new millennium, {{scholars}} have acclaimed a vigorous global justice movement (GJM). Many accounts have stressed the tolerant identities {{of those involved}} in this movement, and/or the movement’s horizontal decision-making structure. Consequently, formal organizations are often excluded from analysis, precluding the chance to systematically assess whether they display social movement modes of co-ordination. The article uses deductive <b>block</b> <b>modelling</b> and inferential statistics on survey data of a broad sample of 208 Western European global justice organizations to uncover their modes of coordination. I find that many organizations commonly considered integral to the GJM demonstrate organizational and coalitional modes of coordination, while formal organizations often engage in coalitional work. Organizations most densely networked, including some formal organizations, do have social movement modes of coordination: they identify with the GJM, display continuity in attendance at international protests/events, and have contentious relations with political institutions. In addition, I raise methodological considerations for future studies of social movement modes of coordination...|$|E
40|$|Due to {{the strong}} {{polarisation}} of economic activities in space and rise in collaborative behaviour, increasing attention has recently been devoted {{to the relationship between}} geography and network formation. The studies conducted on this topic reveal a high variation in terms of methodologies. Putting special emphasis on R&D networks, the aim of this chapter is to review the different methods and assess their ability to address the issues raised by the relationship between network and space. We first discuss the different facets of the relationship between geography and networks. Then, we detail the methodological approaches and their capability to test each effect of geography on network formation. We argue that the effect of distance on dyads have received the major attention so far, but the development of <b>block</b> <b>modelling</b> and top-down approaches opens new research perspectives on how distance or location might affect formation of more complex structures. Moreover, recent improvement in temporal models also offers also offers opportunities to better separate spatial effects from that of influence over time...|$|E
40|$|This thesis {{presents}} the limit analysis of masonry structures under the lateral exposures, that’s the most intentionally given sudden earthquakes {{and also to}} the associated lateral forces. The current study here in is concerned firstly on the parametric analysis of masonry walls to the lateral loads. Secondly, the analyses of representative existing masonry walls under lateral exposures are done. A simple user friendly but very important software, named Block, is used for this study. It {{is based on the}} rigid <b>block</b> <b>modelling</b> concept. In the parametric part, the basic and immediate first step, it is studied the relation of seismic safety factor with respect to the masonry wall shapes, masonry unit sizes, type of masonry bond, compressive stress and also the effect of overloading in which they are applied for a significant set of masonry two dimensional walls and their associated collapse mechanisms to lateral loads. The software gives the safety factor for a certain masonry structure {{as a function of the}} gravitational load of the masonry and the overloads on the masonry that are in the direction of gravity. With this, it was possible to know the relations among safety factor and other parameters...|$|E
40|$|We use the Stein-Chen {{method to}} obtain {{compound}} Poisson approximations for {{the distribution of}} the number of subgraphs in a generalised stochastic <b>block</b> <b>model</b> which are isomorphic to some fixed graph. This model generalises the classical stochastic <b>block</b> <b>model</b> to allow for the possibility of multiple edges between vertices. Both the cases that the fixed graph is a simple graph and that it has multiple edges are treated. The former results apply when the fixed graph {{is a member of the}} class of strictly balanced graphs and the latter results apply to a suitable generalisation of this class to graphs with multiple edges. We also consider a further generalisation of the model to pseudo-graphs, which may include self-loops as well as multiple edges, and establish a parameter regime in the multiple edge stochastic <b>block</b> <b>model</b> in which Poisson approximations are valid. The results are applied to obtain Poisson and compound Poisson approximations (in different regimes) for subgraph counts in the Poisson stochastic <b>block</b> <b>model</b> and degree corrected stochastic <b>block</b> <b>model</b> of Karrer and Newma...|$|R
5000|$|The {{building}} <b>blocks</b> <b>model</b> (an architectural meta-model for infrastructure) that...|$|R
5000|$|LOD 1: Buildings {{represented}} by <b>block</b> <b>models</b> (usually extruded footprints) ...|$|R
40|$|The {{purpose of}} this {{research}} was to examine the complexity of circumstances that result in deliberate self-poisoning cases. For the purposes of this paper, the cases were patients that presented for care and were admitted to the specialty hospital in Northwest of Iran. The research examined the problems preceding deliberate self-poisoning and the interrelations among them by applying network analysis methods. The network was scored for degrees of centrality and betweenness centrality. Structural analysis of network also was conducted using <b>block</b> <b>modelling.</b> The results showed that family conflicts had the highest score for degree of centrality among women, while the highest score for degree of centrality among men belonged to those dealing with drug addiction. Analysis for degree of betweenness centrality revealed that drug addiction had the highest score among men, whereas the highest score for women on betweenness centrality was related to physical illness. Structural analysis of the network showed differences in role that various problems played in intentional self-poisoning. The findings from this research can be used by public health authorities to create prevention programs that address the problems leading to deliberate self-poisoning...|$|E
40|$|International audienceTranspressive {{deformation}} at {{the northern}} Caribbean plate boundary is accommodated mostly by two major strikeslipfaults, but the amount and location of accommodation of the compressional component of deformation is still debated. We collected marine geophysical data including multi-beam bathymetry and multichannel seismic reflection profiles alongthis plate boundary around Hispaniola, in the Jamaica Passage and in the Gulf of Gonâve. The data set allows us to imagethe offshore active strike-slip faults {{as well as the}} compressional structures. We confirm that the Enriquillo-Plantain-GardenFault Zone (EPGFZ) in the Jamaica Passage has a primary strike-slip motion, as indicated by active left-lateral strike-sliprelatedstructures, i. e. : restraining bend, asymmetrical basin, en echelon pressures ridges and horsetail splay. Based ontopographic cross-sections across the EPGFZ, we image a very limited compressional component, if any, for at least thewestern part of the Jamaica Passage. Toward the east of the Jamaica Passage, the fault trace becomes more complex and weidentify adjacent compressional structures. In the Gulf of Gonâve, distributed folding and thrust faulting of the most recentsediments indicate active pervasive compressional tectonics. Estimates of shortening in the Jamaica Passage and in the Gulfof Gonâve indicate an increase of the compressional component of deformation towards the east, which nonetheless remainsvery small compared to that inferred from <b>block</b> <b>modelling</b> based on GPS measurement...|$|E
40|$|Individuals {{in human}} and animal populations are linked through dynamic contact {{networks}} with characteristic structural features that drive the epidemiology of directly transmissible infectious diseases. Using animal movement data from the British cattle industry as an example, this analysis explores whether disease dynamics can be altered by placing targeted restrictions on contact formation to reconfigure network topology. This was accomplished using a simple network generation algorithm that combined configuration wiring with stochastic <b>block</b> <b>modelling</b> techniques to preserve the weighted in- and out-degree of individual nodes (farms) as well as key {{demographic characteristics of the}} individual network connections (movement date, livestock market, and animal production type). We then tested a control strategy based on introducing additional constraints into the network generation algorithm to prevent farms with a high in-degree from selling cattle to farms with a high out-degree as these particular network connections are predicted to have a disproportionately strong role in spreading disease. Results from simple dynamic disease simulation models predicted significantly lower endemic disease prevalences on the trade restricted networks compared to the baseline generated networks. As expected, the relative magnitude of the predicted changes in endemic prevalence was greater for diseases with short infectious periods and low transmission probabilities. Overall, our study findings demonstrate that there is significant potential for controlling multiple infectious diseases simultaneously by manipulating networks to have more epidemiologically favourable topological configurations. Further {{research is needed to determine}} whether the economic and social benefits of controlling disease can justify the costs of restricting contact formation...|$|E
30|$|Two of {{the three}} cases were run using both the {{flexible}} <b>block</b> <b>model</b> (with Dan 3 D-Flex) and ignoring the initially coherent portion of motion (with Dan 3 D). By comparing {{the results of these}} cases, the necessity of using the flexible <b>block</b> <b>model</b> in accurately simulating rock avalanche motion can be assessed.|$|R
40|$|Estimating {{the number}} of {{communities}} {{is one of the}} fundamental problems in community detection. We re-examine the Bayesian paradigm for stochastic <b>block</b> <b>models</b> and propose a "corrected Bayesian information criterion",to determine {{the number of}} communities and show that the proposed estimator is consistent under mild conditions. The proposed criterion improves those used in Wang:Bickel: 2016 and Saldana:Yu:Feng: 2017 which tend to underestimate and overestimate the number of communities, respectively. Along the way, we establish the Wilks theorem for stochastic <b>block</b> <b>models.</b> Moreover, we show that, to obtain the consistency of model selection for stochastic <b>block</b> <b>models,</b> we need a so-called "consistency condition". We also provide sufficient conditions for both homogenous networks and non-homogenous networks. The results are further extended to degree corrected stochastic <b>block</b> <b>models.</b> Numerical studies demonstrate our theoretical results. Comment: 30 pages, update the simulations and add more result...|$|R
40|$|Community {{detection}} is {{a fundamental}} problem in network analysis, with applications in many diverse areas. The stochastic <b>block</b> <b>model</b> is a common tool for model-based community detection, and asymptotic tools for checking consistency of community detection under the <b>block</b> <b>model</b> have been recently developed. However, the <b>block</b> <b>model</b> is limited by its assumption that all nodes within a community are stochastically equivalent, and provides a poor fit to networks with hubs or highly varying node degrees within communities, which are common in practice. The degree-corrected stochastic <b>block</b> <b>model</b> was proposed to address this shortcoming and allows variation in node degrees within a community while preserving the overall block community structure. In this paper we establish general theory for checking consistency of community detection under the degree-corrected stochastic <b>block</b> <b>model</b> and compare several community detection criteria under both the standard and the degree-corrected models. We show which criteria are consistent under which models and constraints, as well as compare their relative performance in practice. We find that methods based on the degree-corrected <b>block</b> <b>model,</b> which includes the standard <b>block</b> <b>model</b> as a special case, are consistent under a wider class of models and that modularity-type methods require parameter constraints for consistency, whereas likelihood-based methods do not. On the other hand, in practice, the degree correction involves estimating many more parameters, and empirically we find it is only worth doing if the node degrees within communities are indeed highly variable. We illustrate the methods on simulated networks and on a network of political blogs. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL] With Correction...|$|R
40|$|This paper {{studies the}} Surpac and Whittle {{software}} and their application in designing an optimised pit. Exploration data from Mpeasem Gold Mining Project (MGMP) {{is used as}} the primary input for the work. The work entails: <b>block</b> <b>modelling</b> of MGMP deposit; pit optimisation; analysis of the Pit NPV’s sensitivity to changes in gold price and mining cost; and detailed pit design. The deposit has undergone intense weathering, forming an oxidised gold deposit up to about 50 m in depth. Sections through the deposit were {{used to create a}} solid model of the orebody, which was divided into blocks to form a block model comprising unit blocks measuring 20 m x 20 m x 10 m. The block model grade was estimated using the Inverse Distance Weighting (IDW) method, giving an average grade of 1. 533 g/t with 22. 79 Mt of ore. During the optimisation, a total of 82 optimal pit outlines were generated using the 3 D Lerchs-Grossmann algorithm. Pit 36 was chosen having the highest the Net Present Value (NPV) @ 10 % of $ 338. 60 million. The optimal pit had 21. 19 Mt of ore at an average grade of 1. 557 g/t. The NPV was very sensitive to gold price changes but marginally sensitive to mining cost changes. From the optimal pit, a detailed pit designed with circular ramp was selected over one with all-cut ramp since it had a higher expected revenue and lower stripping ratio. It is concluded that Surpac and Whittle software combine as a powerful tool for designing an optimal pit...|$|E
40|$|International audienceWhile the {{kinematics}} of Anatolia {{plate and}} the North Anatolian Fault System (NAFS) {{has been studied}} extensively, the slip rate and locking depth along the NAFS are usually assumed constant in the analyses {{due to the lack}} of sufficient data. This is also partly due to the reasonably good fit of Euler small circle and partly {{due to the lack of}} spatial resolution of observations to determine slip rates independently from locking depths. On the other hand, recent geodetic studies show a contrast for locking depth between Marmara and other parts of the NAFS, implying a non-uniform locking depth across the NAFS. In this study, we analyse new GPS data and homogenously combine available data sets covering the eastern part of the NAFS to form the most complete data set. In particular, we incorporate the first results of Turkish Real-Time Kinematic GPS Network (CORS-TR) into our data set. A detailed analysis of three profiles within the NAFS reveals an increase of locking depth in the middle profile to 19. 1 ± 3. 4 [*]km from 11. 9 ± 3. 5 [*]km in the easternmost profile while the slip rate is nearly constant (20 – 22 mm yr− 1), which implies a variation of strain rate of ∼ 100 nanostrain yr− 1. Assuming a constant locking depth throughout whole NAFS gives an average locking depth of 14. 3 ± 1. 7 [*]km. Our best estimates of slip rates in <b>block</b> <b>modelling</b> which takes the variation of locking depths into account are in the range between 22. 5 and 22. 8 mm yr− 1 over eastern part of the NAFS...|$|E
40|$|Anglo Platinum 2 ̆ 7 s Potgietersrust Platinums (PPRust) {{operates}} three open {{pits and}} plans to excavate 90 Mt of rock in 2007. With pit slopes expected to reach a height of 400 in, the geotechnical department has a key role to play. Over {{the last four years}} PPRust has successfully implemented new geotechnical strategies and tactics to reduce risk, improve safety and maximise profitability. A large database of core logging, face mapping and rock testing data has been assembled and used for failure analysis, geotechnical zoning and rock mass ratings. The data has also been used for optimising blast designs on a daily basis {{through the use of a}} geotechnical block model. This greatly improves blast fragmentation and therefore loading and milling efficiencies. The slope design process involves limit equilibrium analysis, numerical modelling, rock fall analysis and <b>block</b> <b>modelling.</b> Slope management includes comprehensive limit blasting and visual inspections, and state-of-the-art slope monitoring equipment, namely the GroundProbe radar, Riegl lasers and GcoMoS automated prism monitoring. The risk-consequence approach has been used in slope optimisation, where all the slope analyses, operational controls, costs of failure, economic analyses of various slope designs and fault tree analyses are used to determine the ideal slope designs. These activities added over 25 million of value to the operation and also ensured compliance with Anglo American 2 ̆ 7 s acceptable risk level. The successful mining of the optimised slopes is to due to the greatly improved geotechnical knowledge and slope management. PPRust 2 ̆ 7 s geotechnical work is used as a benchmark for Anglo American open pit operations. The focus of the paper will be on the value added to the operation by all aspects associated with slope management...|$|E
5000|$|... == Definition == The {{stochastic}} <b>block</b> <b>model</b> {{takes the}} following parameters: ...|$|R
5000|$|The Building <b>Blocks</b> <b>Model</b> dissects the {{infrastructure}} landscape from five directions: ...|$|R
5000|$|... 1995-97 2 Apartment <b>blocks,</b> <b>Model</b> {{single and}} double house; 3rd place ...|$|R
