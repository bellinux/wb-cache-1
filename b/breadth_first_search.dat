310|3376|Public
25|$|The {{algorithm}} terminates when no more augmenting paths {{are found}} in the <b>breadth</b> <b>first</b> <b>search</b> part of one of the phases.|$|E
25|$|The kth {{power of}} a graph with n {{vertices}} and m edges may be computed in time O(mn) by performing a <b>breadth</b> <b>first</b> <b>search</b> starting from each vertex to determine the distances to all other vertices. Alternatively, If A is an adjacency matrix for the graph, modified to have nonzero entries on its main diagonal, then the nonzero entries of A'k give the adjacency matrix of the kth power of the graph, from which it follows that constructing kth powers may be performed in {{an amount of time}} that is within a logarithmic factor of the time for matrix multiplication.|$|E
500|$|In any graph, {{directed}} or undirected, {{there is}} a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using <b>breadth</b> <b>first</b> <b>search</b> or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest [...] path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.|$|E
40|$|AbstractIn {{order to}} {{analysis}} car crash test in C-NCAP, an improved algorithm is given based on Apriori algorithm in this paper. The new algorithm is implemented with vertical data layout, <b>breadth</b> <b>first</b> <b>searching,</b> and intersecting. It {{takes advantage of}} the efficiency of vertical data layout and intersecting, and prunes candidate frequent item sets like Apriori. Finally, the new algorithm is applied in simulation of car crash test analysis system. The result shows that the relations will affect the C-NCAP test results, and it can provide a reference for the automotive design...|$|R
30|$|The main {{assumption}} for algorithms {{that use}} dominance {{as a basis}} is that either there are sparse connections or there is some overall hierarchical structure to the network. Network analysis in a static situation can reveal this type of information {{with many of the}} basic tools already established, such as depth <b>first</b> or <b>breadth</b> <b>first</b> <b>searches.</b> Dynamic networks, however present a complex field of parameters that can confound a single method of finding a solution. A dynamic network that is relatively stable might mimic a static environment; however edges can still be presented that could disrupt the assumptions of dominance or sparsity.|$|R
40|$|Assuming a {{suitably}} compressed input, we {{also show}} {{how to do}} depth [...] <b>first</b> and <b>breadth</b> [...] <b>first</b> <b>search</b> and how to compute strongly connected components and biconnected components in time 0 (nλ + n^ 2 /λ), and how to solve the single source shortest path problem with integer costs in the range [0 [...] C] in time 0 (n^ 2 (C) / n) ...|$|R
2500|$|Each phase {{consists}} of a single <b>breadth</b> <b>first</b> <b>search</b> and a single depth first search. Thus, a single phase may be implemented in [...] time.|$|E
2500|$|Since {{both this}} lexicographic <b>breadth</b> <b>first</b> <b>search</b> {{process and the}} process of testing whether an {{ordering}} is a perfect elimination ordering can be performed in linear time, it is possible to recognize chordal graphs in linear time. [...] The graph sandwich problem on chordal graphs is NP-complete ...|$|E
2500|$|For a graph that {{is already}} maximal planar it is {{possible}} to show a stronger construction of a simple cycle separator, a cycle of small length such that the inside and the outside of the cycle (in the unique planar embedding of the graph) each have at most 2n/3 vertices. [...] proves this (with a separator size of √8√n) by using the Lipton–Tarjan technique for a modified version of <b>breadth</b> <b>first</b> <b>search</b> in which the levels of the search form simple cycles.|$|E
40|$|We {{present some}} new results on {{distances}} in benzenoids. An {{algorithm is proposed}} which, for a given benzenoid system G bounded by a simple circuit Z with n vertices, computes the Wiener index of G in O(n) number of operations. We also show that benzenoid systems have a convenient dismantling scheme, which can be derived by applying the <b>breadth</b> [...] <b>first</b> <b>search</b> to their dual graphs. Our last result deals with the clustering problem of sets of atoms of benzenoids systems. We demonstrate how th...|$|R
40|$|Abstract. We have {{recently}} shown that classical planning {{problems can be}} characterized {{in terms of a}} width measure that is bounded and small for most planning benchmark domains when goals are re-stricted to single atoms. Two simple algorithms have been devised for exploiting this structure: Iterated Width (IW) for achieving atomic goals, that runs in time exponential in the problem width by per-forming a sequence of pruned <b>breadth</b> <b>first</b> <b>searches,</b> and Serialized IW (SIW) that uses IW in a greedy search for achieving conjunctive goals one goal at a time. While SIW does not use heuristic estimators of any sort, it manages to solve more problems than a Greedy BFS using a heuristic like hadd. Yet, it does not approach the performance of more recent planners like LAMA. In this short paper, we intro-duce two simple extension to IW and SIW that narrow the perfor-mance gap with state-of-the-art planners. The first involves changing the greedy search for achieving the goals one at a time, by a depth-first search that is able to backtrack. The second involves computing a relaxed plan once before going to the next subgoal for making the pruning in the breadth-first procedure less agressive, while keeping IW exponential in the width parameter. The empirical results are in-teresting as they follow from ideas that are very different from those used in current planners. ...|$|R
40|$|Malaria {{is one of}} the world’s {{most common}} and serious {{diseases}} causing death of about 3 million people each year. Its most severe occurrence is caused by the protozoan Plasmodium falciparum. Biomedical research could enable treating the disease by effectively and specifically targeting essential enzymes of this parasite. However, the parasite has developed resistance to existing drugsmaking it indispensable to discover new drugs. We have established a simple computational tool which analyses the topology of the metabolic network of P. falciparum to identify essential enzymes as possible drug targets. Weinvestigated the essentiality of a reaction in the metabolic network by deleting (knocking-out) such a reaction in silico. The algorithmselected neighbouring compounds of the investigated reaction that had to be produced by alternative biochemical pathways. Using <b>breadth</b> <b>first</b> <b>searches,</b> we tested qualitatively if these products could be generated by reactions that serve as potential deviations of the metabolic flux. With this we identified 70 essential reactions. Our results were compared with a comprehensive list of 38 targets of approved malaria drugs. When combining our approach with an in silico analysis performed recently [Yeh, I., Hanekamp, T., Tsoka, S., Karp, P. D., Altman, R. B., 2004. Computational analysis of Plasmodium falciparum metabolism: organizing genomic information to facilitate drug discovery. Genome Res. 14, 917 – 924] we could improve the precision of the prediction results. Finally we present a refined list of 22 new potential candidate targets for P. falciparum, half of which have reasonable evidence to be valid targets against micro-organisms and cancer...|$|R
2500|$|... {{suggest a}} {{simplified}} {{version of this}} approach: they augment the graph to be maximal planar and construct a <b>breadth</b> <b>first</b> <b>search</b> tree as before. Then, for each edge e that {{is not part of}} the tree, they form a cycle by combining e with the tree path that connects its endpoints. They then use as a separator the vertices of one of these cycles. Although this approach cannot be guaranteed to find a small separator for planar graphs of high diameter, their experiments indicate that it outperforms the Lipton–Tarjan and Djidjev breadth-first layering methods on many types of planar graph.|$|E
2500|$|The same proof idea holds more {{generally}} if u is any vertex, v is any vertex that is maximally far from u, and w is any neighbor of v that is maximally far from u. Further, {{the removal of}} v and w from the graph does not change {{any of the other}} distances from u. Therefore, the process of forming a matching by finding and removing pairs vw that are maximally far from u may be performed by a single postorder traversal of a <b>breadth</b> <b>first</b> <b>search</b> tree of the graph, rooted at u, in linear time. [...] provide an alternative linear-time algorithm based on depth-first search, as well as efficient parallel algorithms for the same problem.|$|E
2500|$|The {{reduction}} in the other direction, from triangle detection to median graph testing, is more involved and depends on the previous median graph recognition algorithm of , which tests several necessary conditions for median graphs in near-linear time. [...] The key new step involves using a <b>breadth</b> <b>first</b> <b>search</b> to partition the graph's vertices into levels according to their distances from some arbitrarily chosen root vertex, forming a graph from each level in which two vertices are adjacent if they share a common neighbor in the previous level, and searching for triangles in these graphs. [...] The median of any such triangle must be a common neighbor of the three triangle vertices; if this common neighbor does not exist, the graph is not a median graph. [...] If all triangles found in this way have medians, and the previous algorithm finds that the graph satisfies all the other conditions for being a median graph, then it must actually be a median graph. [...] This algorithm requires, not just the ability to test whether a triangle exists, but {{a list of all}} triangles in the level graph. [...] In arbitrary graphs, listing all triangles sometimes requires Ω(m3/2) time, as some graphs have that many triangles, however Hagauer et al. show that the number of triangles arising in the level graphs of their reduction is near-linear, allowing the Alon et al. fast matrix multiplication based technique for finding triangles to be used.|$|E
40|$|The {{contributions}} {{during the}} last few years on the structural theory of Petri nets can now be applied to formal verification. The structural theory provides methods to find efficient encoding schemes for symbolic representations of the reachable markings. It also provides approximations of the state space that allow to alleviate many bottlenecks in the calculation of the reachability set by <b>breadth</b> or depth <b>first</b> <b>search</b> algorithms. The paper reviews some of the results on the structural theory and explains how they can be incorporated in a model-checking veri#cation framework for concurrent systems...|$|R
40|$|Mining large graphs for {{information}} {{is becoming an}} increasingly important workload due to the plethora of graph structured data becoming available. An aspect of graph algorithms that has hitherto not received much interest {{is the effect of}} memory hierarchy on accesses. A typical system today has multiple levels in the memory hierarchy with differing units of locality; ranging across cache lines, TLB entries and DRAM pages. We postulate {{that it is possible to}} allocate graph structured data in main memory in a way as to improve the spatial locality of the data. Previous approaches to improving cache locality have focused only on a single unit of locality, either the cache line or virtual memory page. On the other hand cache oblivious algorithms can optimise layout for all levels of the memory hierarchy but unfortunately need to be specially designed for individual data structures. In this paper we explore hierarchical blocking as a technique for closing this gap. We require as input a specification of the units of locality in the memory hierarchy and lay out the input graph accordingly by copying its nodes using a hierarchy of <b>breadth</b> <b>first</b> <b>searches.</b> We start with a basic algorithm that is limited to trees and then extend it to arbitrary graphs. Our most efficient version requires only a constant amount of additional space. We have implemented versions of the algorithm in various environments: for C programs interfaced with macros, as an extension to the Boost object oriented graph library and finally as a modification to the traversal phase of the semispace garbage collector in the Jikes Java virtual machine. Our results show significant improvements in the access time to graphs of various structure. Comment: Work in progres...|$|R
50|$|Breadth-first search {{produces}} a so-called <b>breadth</b> <b>first</b> tree. You {{can see how}} a <b>breadth</b> <b>first</b> tree looks in the following example.|$|R
5000|$|This {{application}} {{was the original}} motivation that led [...] to develop the lexicographic <b>breadth</b> <b>first</b> <b>search</b> algorithm.|$|E
5000|$|... #Caption: The Petersen graph as a Moore graph. Any <b>breadth</b> <b>first</b> <b>search</b> tree has d(d-1)i {{vertices}} in its ith level.|$|E
50|$|The {{algorithm}} terminates when no more augmenting paths {{are found}} in the <b>breadth</b> <b>first</b> <b>search</b> part of one of the phases.|$|E
40|$|The {{problem of}} {{discovering}} frequent arrangements of temporal intervals is studied. It {{is assumed that}} the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine temporal arrangements of event intervals that appear frequently in the database. The motivation of this work is the observation that in practice most events are not instantaneous but occur {{over a period of}} time and different events may occur concurrently. Thus, there are many practical applications that require mining such temporal correlations between intervals including the linguistic analysis of annotated data from American Sign Language as well as network and biological data. Three efficient methods to find frequent arrangements of temporal intervals are described; the first two are tree-based and use <b>breadth</b> and depth <b>first</b> <b>search</b> to mine the set of frequent arrangements, whereas the third one is prefix-based. The above methods apply efficient pruning techniques that include a set of constraints that add user-controlled focus into the mining process. Moreover, based on the extracted patterns a standard method for mining association rules is employed that applies different interestingness measures to evaluate the significance of the discovere...|$|R
40|$|This paper {{presents}} and analysis the common existing sequential pattern mining algorithms. It presents a classifying study of sequential pattern-mining algorithms into five extensive classes. First, {{on the basis}} of Apriori-based algorithm, second on <b>Breadth</b> <b>First</b> Search-based strategy, third on Depth <b>First</b> <b>Search</b> strategy, fourth on sequential closed-pattern algorithm and five {{on the basis of}} incremental pattern mining algorithms. At the end, a comparative analysis is done on the basis of important key features supported by various algorithms. This study gives an enhancement in the understanding of the approaches of sequential pattern mining. Comment: 10 page...|$|R
40|$|In {{this paper}} we study {{a new problem}} in {{temporal}} pattern mining: discovering frequent arrangements of temporal intervals. We assume that the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine arrangements of event intervals that appear frequently in the database. There are many applications where these type of patterns can be useful, including data network, scientific, and financial applications. Efficient methods to find frequent arrangements of temporal intervals using both <b>breadth</b> <b>first</b> and depth <b>first</b> <b>search</b> techniques are described. The performance of the proposed algorithms is evaluated and compared with other approaches on real datasets (American Sign Language streams and network data) and large synthetic datasets...|$|R
50|$|Jarvis and Shier {{describe}} {{a very similar}} algorithm using <b>breadth</b> <b>first</b> <b>search</b> in place of depth-first search; the advantage of depth-first search is that the strong connectivity analysis {{can be incorporated into}} the same search.|$|E
5000|$|Each phase {{consists}} of a single <b>breadth</b> <b>first</b> <b>search</b> and a single depth first search. Thus, a single phase may be implemented in [...] time.Therefore, the first [...] phases, in a graph with [...] vertices and [...] edges, take time [...]|$|E
50|$|If {{you have}} only one (or a few) queries to make, {{it may be more}} {{efficient}} to forgo the use of more complex data structures and compute the reachability of the desired pair directly. This can be accomplished in linear time using algorithms such as <b>breadth</b> <b>first</b> <b>search</b> or iterative deepening depth-first search.|$|E
5000|$|It is {{straightforward}} {{to compute the}} connected components of a graph in linear time (in terms of the numbers of the vertices and edges of the graph) using either breadth-first search or depth-first search. In either case, a search that begins at some particular vertex v will find the entire connected component containing v (and no more) before returning. To find all the connected components of a graph, loop through its vertices, starting a new <b>breadth</b> <b>first</b> or depth <b>first</b> <b>search</b> whenever the loop reaches a vertex that has not already been included in a previously found connected component. [...] describe essentially this algorithm, and state that at that point it was [...] "well known".|$|R
5000|$|As {{a linear}} string {{notation}} based on depth <b>first</b> or <b>breadth</b> <b>first</b> traversal, such as: ...|$|R
5000|$|The level {{structure}} of a graph can be computed by a variant of <b>breadth</b> <b>first</b> search: ...|$|R
50|$|The {{characterization}} of squaregraphs {{in terms of}} distance from a root and links of vertices can be used together with <b>breadth</b> <b>first</b> <b>search</b> {{as part of a}} linear time algorithm for testing whether a given graph is a squaregraph, without any need to use the more complex linear-time algorithms for planarity testing of arbitrary graphs.|$|E
50|$|Since {{both this}} lexicographic <b>breadth</b> <b>first</b> <b>search</b> {{process and the}} process of testing whether an {{ordering}} is a perfect elimination ordering can be performed in linear time, it is possible to recognize chordal graphs in linear time. The graph sandwich problem on chordal graphs is NP-completewhereas the probe graph problem on chordal graphs has polynomial-time complexity.|$|E
50|$|In {{order to}} do that a linked list is formed that will keep the indexes of the pixels that are {{connected}} to each other, steps (2) and (3) below. The method of defining the linked list specifies the use of a depth or a <b>breadth</b> <b>first</b> <b>search.</b> For this particular application, there is no difference which strategy to use. The simplest kind of a last in first out queue implemented as a singly linked list will result in a depth first search strategy.|$|E
40|$|AbstractThe need of {{complete}} corpus nowadays is very crucial, especially for linguist. In order to assist linguist to construct corpus, {{a tool for}} collecting text in a specific language from the Internet is needed. This paper describes an approach to collecting Javanese and Sundanese text from the Internet. We have modified a focused crawler named WebSPHINX such {{that it can be}} useful for crawling the text. In order to determine which pages are crawled, the focused crawler needs a language classifier. In this research, we used the dictionary algorithm for classifying the text. In order to determine the next links to visit, we employed 2 crawling methods, i. e. <b>Breadth</b> <b>First</b> and By Page Length. The purpose of our research is to observe how the algorithm and the crawling methods perform to collect Javanese and Sundanese text from the Internet. Our experiments have shown that the dictionary algorithm classify the text based on the languages with average accuracy of 88, 64 % {{depending on the size of}} the documents being classified. The experiments also showed that in general the <b>Breadth</b> <b>First</b> method outperfoms the By Page Length method. In this research, we also campared the dictionary algorithm to the N-Gram algorithm when different crawling methods are employed. The experiments showed that the combination of <b>Breadth</b> <b>First</b> method and Dictionary algorithm generally outperforms other combinations. Therefore, we used the combination of <b>Breadth</b> <b>First</b> method and Dictionary algorithm for crawling the text and then constructing Javanese and Sundanese corpora...|$|R
40|$|This paper {{presents}} three minimum-area floorplanning algorithms {{that use}} flexible arbitrary rectilinear shapes {{for the standard}} cell regions in MBC designs. The first algorithm (pure HCST) introduces a grid traversal technique which guarantees a minimum-area floorplan. The second algorithm (Hybrid-BF) uses a combination of HCST and <b>Breadth</b> <b>First</b> (BF) traversals to give a practical solution that approximately places flexible blocks at specified locations called seeds. The third algorithm (Hybrid-MBF) improves on {{the shapes of the}} flexible blocks generated by Hybrid-BF by using a combination of HCST and a Modified <b>Breadth</b> <b>First</b> (MBF) traversal. All three algorithms are polynomial in the number of grid squares. Optimized implementations of Hybrid-BF and Hybrid-MBF required less than two seconds on a SUN SPARC station 10...|$|R
40|$|An {{association}} rule {{is a method}} {{to find out the}} frequent hidden relationship from a large amount of datasets in a database. Association analysis into existing database technology is very useful for indexing and query processing capabilities of database system and developing efficient and scalable mining algorithms as well as handling user specified or domain specific constraints and post processing the extracted patterns. In the present work, a methodology known as association analysis is presented which is very useful for discovery of interesting relationship hidden in large dataset, and an algorithm for generation of frequent data item set known as Apriori algorithm is used and validated the relations through Unified Modeling Language (UML). Authors used the lattice structure and also discussed the various {{association rule}}s for the frequent data itemset which is found by Apriori algorithm. The different strategies in generation and traversal are <b>breadth</b> <b>first</b> and depth <b>first</b> <b>search</b> traversal. These techniques provide different tradeoff in terms of the input and output memory and computational time requirements. The entire concept is implemented by considering a real case study of Vehicle Insurance Policy system (VIPS) in context of Indian scenario...|$|R
