240|70|Public
5000|$|Yann LeCun {{was born}} near Paris, France, in 1960. He {{received}} a Diplôme d'Ingénieur from the Ecole Superieure d'Ingénieur en Electrotechnique et Electronique (ESIEE), Paris in 1983, and a PhD in Computer Science from Université Pierre et Marie Curie in 1987 {{during which he}} proposed an early form of the <b>back-propagation</b> <b>learning</b> algorithm for neural networks.|$|E
30|$|We {{analyzed}} various <b>back-propagation</b> <b>learning</b> algorithms, {{which have}} an influence on system performance {{as well as the}} speed of learning.|$|E
30|$|In the {{proposed}} NFC structure, precondition parameters of membership layer {{have been trained}} in the simulation model. During the simulation studies, output parameters have been trained using <b>back-propagation</b> <b>learning</b> algorithm. These parameters are adapted until the desired performance is reached.|$|E
40|$|This paper proposes an {{alternating}} <b>back-propagation</b> algorithm for <b>learning</b> {{the generator}} network model. The {{model is a}} non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) <b>Learning</b> <b>back-propagation,</b> which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating <b>back-propagation</b> algorithm can <b>learn</b> realistic generator models of natural images, video sequences, and sounds. Moreover, {{it can also be}} used to learn from incomplete or indirect training data...|$|R
3000|$|In our studies, the {{learning}} algorithm based on error <b>back-propagation</b> with adaptive <b>learning</b> rate and momentum is applied, {{as used in}} [20, 21]. This learning algorithm automatically updates the weights and biases of the ANN for the following algorithm iteration ([...] [...]...|$|R
40|$|AbstractThis paper {{presents}} a computational framework, the Generic Programmable Neural Network (GPNN), for efficient implementation of <b>Back-Propagation</b> based neural <b>learning</b> algorithms running on multi-core machines. GPNN has three components: parallelization of neural learning, abstraction of network components, and compile-time generalization. Together these computational components make GPNN an efficient framework for fast implementation of <b>back-propagation</b> based neural <b>learning</b> algorithms, and provide flexibility and reusability for modifying neural network topologies. The GPNN {{was applied to}} four different neural <b>learning</b> algorithms: classic <b>back-propagation</b> (BP), quick propagation (QP), resilient propagation (RP) and Levenberg-Marquardt (LM) algorithm. Experiments were conducted {{to evaluate the effectiveness}} of GPNN, and results show that the neural learning algorithms implemented in GPNN are more efficient than their respective functions provided by Matlab...|$|R
40|$|AbstractWith the {{increasing}} use of large-scale grid-connected photovoltaic system, accurate forecast approach for the power output of photovoltaic system has become an important issue. In order to forecast the power output of a photovoltaic system at 24 -hour-ahead without any complex modeling and complicated calculation, an artificial neural network based approach is proposed in this paper. The improved <b>back-propagation</b> <b>learning</b> algorithm is adopted to overcome shortcomings of the standard <b>back-propagation</b> <b>learning</b> algorithm. Similar day selection algorithm based on forecast day information is proposed to improve forecast accuracy in different weather types. Forecasting results of a photovoltaic system show that the proposed approach has a great accuracy and efficiency for forecasting the power output of photovoltaic system...|$|E
40|$|In {{this paper}} we present the analog CMOS {{design of a}} multi-layer-perceptron network with on-chip by-pattern <b>back-propagation</b> <b>{{learning}}.</b> The learning algorithm {{is based on a}} local learning rate adaptation technique which makes the on-chip implementation more efficient in terms of convergence speed. Circuit simulation results validate the network behavio...|$|E
40|$|The {{goal of this}} {{dissertation}} is to use artificial {{neural networks}} to classify the writing system of previously unseen glyphs, e. g. Runes and Hieroglyphs. The arti-ficial neural networks shall be trained using both evolution and <b>back-propagation</b> <b>learning.</b> Combinations of these methods will be investigated, and compared to the pure methods...|$|E
40|$|Abstract. We {{construct}} a geometrical perspective {{to justify the}} slow learning period and fast learning period during training. We plot the error surfaces and the solution space on the input space for a single neuron with two inputs. We study various training paths on this space when we run the <b>back-propagation</b> (BP) <b>learning</b> algorithm [1]. We display {{the relation between the}} learning curve and the training path. We apply this study to correctly and efficiently operate the momentum method [2] to accelerate the training...|$|R
40|$|We {{have built}} an {{artificial}} neural network to analyze visual field maps. The program uses the <b>back-propagation</b> method of <b>learning</b> and has been trained with 62 different types of classical visual field defects. When tested against 18 unknowns, {{it was able to}} correctly classify 17 of them. Connectionist networks are thus shown to be capable of efficiently recognizing two-dimensional geometric patterns...|$|R
40|$|Despite {{the fact}} that many {{symbolic}} and connectionist (neural net) learning algorithms are addressing the same problem of learning from classified examples, very little Is known regarding their comparative strengths and weaknesses. This paper presents the results of experiments comparing the ID 3 symbolic learning algorithm with the perceptron and <b>back-propagation</b> connectionist <b>learning</b> algorithms on several large real-world data sets. The results show that ID 3 and perceptron run significantly faster than does backpropagation, both during learning and during classification of novel examples. However, the probability of correctly classifying new examples is about the same for the three systems. On noisy data sets there is some indication that backpropagation classifies more accurately. 1...|$|R
40|$|Bachelor's thesis {{studies the}} {{problems}} of neural networks and prediction of protein secondary structure. This thesis is focused on multilayer neural network with <b>back-propagation</b> <b>learning</b> algorithm and their use for prediction. It describes the infuence of network architecture and their parameters settings on the prediction results. Furthermore suitability of the network {{for this kind of}} prediction is discused...|$|E
40|$|The {{learning}} {{properties of}} a universal approximator, a normalized committee machine with adjustable biases, are studied for on-line <b>back-propagation</b> <b>learning.</b> Within a statistical mechanics frame-work, numerical studies show that this model has features which do not exist in previously studied two-layer network models with-out adjustable biases, e. g., attractive suboptimal symmetric phases even for realizable cases and noiseless data. ...|$|E
40|$|Combination of {{proposed}} modular, multilayer perceptron and algorithm for its operation recognizes new objects after relatively brief retraining sessions. (Perceptron is multilayer, feedforward {{artificial neural network}} fully connected and trained via <b>back-propagation</b> <b>learning</b> algorithm.) Knowledge pertaining to each object to be recognized resides in subnetwork of full network, therefore not necessary to retrain full network to recognize each new object...|$|E
40|$|In this study, the {{feed-forward}} (FF) {{neural networks}} (NNs) with <b>back-propagation</b> (BP) <b>learning</b> algorithm {{is used to}} estimate the ultimate tensile strength of unrefined Al-Zn-Mg-Cu alloys and refined the alloys by Al- 5 Ti- 1 B and Al- 5 Zr master alloys. The obtained mathematical formula is presented in great detail. The designed NN model shows good agreement with test results {{and can be used}} to predict the ultimate tensile strength of the alloys. Additionally, the effects of scandium (Sc) and carbon (C) rates are investigated by using the proposed equation. It was observed that the tensile properties of Al-Zn-Mg-Cu alloys improved with the addition of 0. 5 Sc and 0. 01 C wt. %...|$|R
40|$|This paper {{describes}} the design, training {{and testing of}} an artificial neural network for classification of normal and abnormal premature ventricular contraction (PVC) beats in ECG signal. To carry out the classification task, we use the <b>back-propagation</b> (BP) <b>learning</b> algorithm. Two feature selections types were investigated with aim of generating the most appropriate input vector for the artificial neural network classifier (ANNC). The first selected information of each ECG beat is stored as 33 -element vector; the second one is then reduced to a 10 dimensional vector using principal component analysis (P. C. A). The performance measures of the classifier will also be presented using as training and testing data sets from the MIT-BIH database...|$|R
40|$|An {{adaptive}} back-propagation {{algorithm is}} studied and compared with gradient descent (standard <b>back-propagation)</b> for on-line <b>learning</b> in two-layer neural networks with an arbitrary number of hidden units. Within a statistical mechanics framework, both numerical studies and a rigorous analysis {{show that the}} adaptive back-propagation method results in faster training by breaking the symmetry between hidden units more efficiently and by providing faster convergence to optimal generalization than gradient descent...|$|R
40|$|To {{construct}} of a multi-layer network in a FPGA, we discuss simplified network constructions. We reexamined neuron functions for <b>back-propagation</b> <b>learning.</b> We made some improvements for the functions, but couldn't achieve drastic reduction. As the results, we abandoned <b>back-propagation</b> <b>learning,</b> and proposed a new neural network, named as AND/OR-neural network, which {{is derived from}} the disjunctive normal-form of logical expressions. The network is defined in the binary logic only and has a conclusive learning. The hardware amounts are less than that of the original, and the network can be implemented in a small size FPGA. However, the AND/OR-neural network has not prediction ability. Therefore, to obtain the ability, we expand it to multi-valued type. The extension is done approximately by replacements of logical operators. We discussed the property. We implemented the multi-valued AND/OR-network in 20 K gates FPGA, and we could solve 7 th-dimensional exclusive-OR problem in order of microsecond. Keywords: Neural network, FPGA, HDL, conclusive learning, disjunctive normal form, multi-dimensiona...|$|E
30|$|As {{shown in}} Fig.  3, inputs of NFC were {{selected}} as the error and the change of error. Five membership functions are used for each input. In the proposed NFC structure, precondition parameters of membership layer have been trained in the simulation model. During the simulation studies, output parameters have been trained using <b>back-propagation</b> <b>learning</b> algorithm. These parameters are adapted until the desired performance is reached.|$|E
40|$|Abstract. We propose and {{evaluate}} a neural network approach to mine detection using Electromagnetic Induction (EMI) sensors {{which provides a}} robust non-parametric approach. In our approach, a neural network with the well-known <b>back-propagation</b> <b>learning</b> algorithm combines the S-Statistic with the δ-Technique to discriminate between non-mine patterns and mines. Experimental results show that this approach reduces false alarms substantially over using just the δ-Technique or the energy detector...|$|E
50|$|Matlab: The {{neural network}} toolbox has {{functionality}} designed {{to produce a}} time delay neural network give the step size of time delays and an optional training function. The default training algorithm is a Supervised <b>Learning</b> <b>back-propagation</b> algorithm that updates filter weights based on the Levenberg-Marquardt optimizations. The function is timedelaynet(delays, hidden_layers, train_fnc) and returns a time-delay neural network architecture that a user can train and provide inputs to.|$|R
40|$|In this paper, an {{enhanced}} hybrid method (EHM) is presented for the simulation of homogeneous non-Gaussian stochastic fields with prescribed target marginal distribution and spectral density function. The presented methodology constitutes an efficient blending of the Deodatis-Micaletti method with a neural network based function approximation. Precisely, the function fitting ability of neural networks {{based on the}} resilient <b>back-propagation</b> (Rprop) <b>learning</b> algorithm is employed to approximate the unknown underlying Gaussian spectrum. The resulting algorithm can be successfully applied for simulating narrow-banded fields with very large skewness {{at a fraction of}} the computing time required by the existing methods. Its computational efficiency is demonstrated in three numerical examples involving fields that follow the beta and lognormal distributions. (c) 2005 Elsevier B. V. All rights reserved...|$|R
40|$|AbstractThe {{objective}} of the present work was to use artificial neural network to study the quantitative structure-activity relationship (QSAR) of the protective effects of N-p-tolyl/phenylsulfonyl L-amino acid thiolester derivatives on anoxic damage of rat pheochromocytoma (PC 12) cells. Five molecular parameters of these target compounds, including heat of formation, total energy, dipole moment, {{the energy of the}} highest occupied molecular orbital and the energy of the lowest unoccupied molecular orbital, were calculated with the PM 6 semi-empirical quantum mechanical method. A multilayer feed-forward (MLFF) network with <b>back-propagation</b> (BP) <b>learning</b> was employed in the present work with the molecular parameters as inputs and neurotrophic activities as outputs. Results showed that the neural network can provide a good prediction of neurotrophic activity and may be useful for predicting the bioactivity of new compounds of similar class...|$|R
30|$|The {{framework}} {{employed for}} prediction entails two major blocks: the traffic estimator and its optimizer. The estimator structure is developed based on MLP NN with <b>back-propagation</b> <b>learning</b> algorithm. The optimizer used for setting the optimal set of variables {{is based on}} a specific kind of GAs called “non-dominated sorting genetic algorithm II” or briefly NSGA-II. This section gives a brief review on the properties of these two blocks.|$|E
40|$|This paper {{introduces}} Fuzzy ARTRON as a general-purpose classifier {{that can}} do high-quality classification in continuous, discrete, linear, or nonlinear domains. The topology of Fuzzy ARTRON contains a fuzzy ART network, on which a perceptron layer is superimposed. The learning algorithms involve unsupervised ART learning and supervised error <b>back-propagation</b> <b>learning.</b> The former is used to auto-construct proper clusters through the fuzzy ART self-construction ability. This improves the convergence rate and alleviates the local minima problem usually associated with the error <b>back-propagation</b> <b>learning</b> network. The latter is used to dynamically associate clusters with proper classes via connection weight adjustment. This improves the generalization ability so that Fuzzy ARTRON can successfully handle the linearly nonseparable problems usually associated with fuzzy ART and the weak generalization problem usually associated with fuzzy ARTMAP. Finally, Fuzzy ARTRON employs fuzzy hyperboxes to do clustering, which leads to better generalization performance compared to conventional hyperboxes. Computer simulations were conducted to evaluate the performance and applicability of Fuzzy ARTRON under continuous, discrete, linear, or nonlinear domains...|$|E
40|$|In {{this study}} Artificial Neural-networks is {{employed}} {{to predict the}} CO concentration during 2002 to 2004. The application of the multiple perceptron with <b>back-propagation</b> <b>learning</b> algorithm is reported in the prediction eleven sites at one site. The generalization ability of the model is confirmed by {{root mean square error}} and correlation between observed and predicted concentrations. The evaluation of model results shows that the degree of success in forecasting CO concentration is promising...|$|E
40|$|In this paper, {{the robust}} neuro-fuzzy {{networks}} (RNFNs) are {{proposed to improve}} the problems of neuro-fuzzy networks (NFNs) for modeling with out-liers. Firstly, the support vector regression (SVR) approach is applied to obtain the initial structure of RNFNs. Because of the SVR approach is equivalent to solving a linear constrained quadratic program-ming problem under the fixed structure of SVR, the RNFNs are easy to determine the parameters of promise parts and fuzzy singleton of consequence parts. Secondly, when the results of SVR are as initial structure of RNFNs, the annealing robust <b>back-propagation</b> (ARBP) <b>learning</b> algorithm used as the learning algorithm of RNFNs and applied to adjust the parameters of promise parts and fuzzy singleton of consequence parts in RNFNs. Simulation results are provided to show the validity and applicability of the proposed RNFNs...|$|R
40|$|Abstract. This paper {{deals with}} the {{parallel}} implementation of the <b>back-propagation</b> of errors <b>learning</b> algorithm. We propose two mapping schemes that allow to obtain two e cient parallel algorithms implemented on the Meiko CS- 2 MIMD parallel computer. The parallel algorithms, obtained from the sequential code by means of simple and well localised modi cations, {{are based on the}} use of a global operator whose straightforward hardware implementation could improve both performance and scalability of the proposed solutions. ...|$|R
40|$|Most {{applications}} of Neural Networks in Control Systems use {{a version of}} the BackPropagation algorithm for training. Learning in these networks is generally a slow and very time consuming process. Cascade-Correlation is a supervised learning algorithm that automatically determines the size and topology of the network and is quicker than <b>back-propagation</b> in <b>learning</b> for several benchmarks. We present modified versions of the Cascade-Correlation learning algorithm. This version is used to implement the inverse kinematic transformations of a robot arm controller with two and three degrees of freedom. This new version shows faster convergence than the original and scales better to bigger training sets and lower tolerances. Keywords: supervised learning, robot control, cascade-correlation. 1 Introduction Control Systems is one area for which neural networks are attractive because of their ability to learn, to approximate functions, to classify patterns and because of their potent [...] ...|$|R
30|$|Although machine {{learning}} techniques for facial recognition have provided decent results, these techniques do not perform well under unconstrained environments. This is mainly because {{machine learning}} approaches rely on hand-crafted features or representations selected by human experts that may work for one scenario and fail for other situations. On the other hand, deep learning (DL)-based approaches {{have proven to be}} most suitable as the representations and features are discovered automatically from data by the <b>back-propagation</b> <b>learning</b> technique.|$|E
40|$|Abstract — Tennis {{is one of}} {{the most}} popular sports in the world. Many {{researchers}} have studied in tennis model to find out whose player will be the winner of the match by using the statistical data. This paper proposes a powerful technique to predict the winner of the tennis match. The proposed method provides more accurate prediction results by using the statistical data and environmental data based on Multi-Layer Perceptron (MLP) with <b>back-propagation</b> <b>learning</b> algorithm...|$|E
40|$|Optimum {{design of}} singly and doubly {{reinforced}} beams with uniformly distributed and concentrated load {{has been done}} by incorporating actual self weight of beam, parabolic stress block, moment-equilibrium and serviceability constraints besides other constraints. Also, this design expertise has been incorporated into a genetically optimized artificial neural network based on steepest descent, adaptive and resilient <b>back-propagation</b> <b>learning</b> techniques. The initial solution for the optimization procedure has been obtained using limit state design as per IS: 456 - 2000...|$|E
40|$|Abstract — This paper {{presents}} an energy <b>back-propagation</b> algorithm (EBP). <b>Learning</b> and convergence {{processes of the}} standard backpropagation algorithm (SBP) {{are based on the}} energy function. The energy function is used with the convergence process to extract the nearest image for the unknown tested image. The EBP algorithm shows considerably better performance in terms of time of learning, time of convergence, and size of input image compared to the SBP algorithm. Index Terms — Artificial neural networks, backpropagation algorithm, energy function, pattern recognition. I...|$|R
40|$|Nonlinear system {{identification}} {{is becoming an}} important tool {{which can be used}} to improve control performance. This paper describes the application of adaptive neuro-fuzzy inference system (ANFIS) model for controlling a car. The vehicle must follow a predefined path by supervised <b>learning.</b> <b>Back-propagation</b> gradient descent method was performed to train the ANFIS system. The performance of the ANFIS model was evaluated in terms of training performance and classification accuracies and the results confirmed that the proposed ANFIS model has potential in controlling the non linear system...|$|R
40|$|Weintroduce {{adaptive}} optics as {{a technique to}} improve images taken by ground-based telescopes through a turbulent blurring atmosphere. Adaptive optics rapidly senses the wavefront distortion referenced to either a natural or laser guidestar, and then applies an equal but opposite pro#le to an adaptive mirror. In this paper, we summarize the application of neural networks in {{adaptive optics}}. First, we report previous work on employing multi-layer perceptron neural networks and <b>back-propagation</b> to <b>learn</b> how to sense and reconstruct the wavefront. Second, we showhow neural networks {{can be used to}} predict the wavefront, and compare the neural networks' predictivepower in the presence of noise to that of linear networks also trained with back-propagation. In our simulations, we #nd that the linear network predictors train faster, they havelower residual phase variance, and they are much more tolerant to noise than the non-linear neural network predictors, though both o#er improvementover [...] ...|$|R
