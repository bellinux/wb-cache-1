222|529|Public
2500|$|Lehmann–Scheffé theorem: a {{complete}} sufficient estimator is the <b>best</b> <b>estimator</b> of its expectation ...|$|E
2500|$|The estimators G1 for sample {{skewness}} and G2 for sample kurtosis {{are used}} by DAP/SAS, PSPP/SPSS, and Excel. [...] However, they are not used by BMDP and (according to [...] ) they were not used by MINITAB in 1998. Actually, Joanes [...] and Gill in their 1998 study [...] concluded that the skewness and kurtosis estimators used in BMDP and in MINITAB (at that time) had smaller variance and mean-squared error in normal samples, but the skewness and kurtosis estimators used in [...] DAP/SAS, PSPP/SPSS, namely G1 and G2, had smaller mean-squared error in samples from a very skewed distribution. [...] It {{is for this reason}} that we have spelled out [...] "sample skewness", etc., in the above formulas, to make it explicit that the user should choose the <b>best</b> <b>estimator</b> according to the problem at hand, as the <b>best</b> <b>estimator</b> for skewness and kurtosis depends on the amount of skewness (as shown by Joanes and Gill).|$|E
5000|$|Lehmann-Scheffé theorem: a {{complete}} sufficient estimator is the <b>best</b> <b>estimator</b> of its expectation ...|$|E
40|$|The {{canonical}} {{form for the}} comparison of certain linear estimators using Pitman's Measure of Closeness is generalized to the class of all linear estimators. Under the assumption of normality, the equivalence of Pitman-closest linear unbiased <b>estimators</b> and <b>best</b> linear unbiased <b>estimators</b> is shown. A sufficient condition is given for which the BLUE will be Pitman-closer than the <b>best</b> linear equivalent <b>estimator</b> (BLEE). Pitman's measure of closeness order statistics <b>best</b> linear unbiased <b>estimators</b> <b>best</b> linear equivariant <b>estimators...</b>|$|R
40|$|AbstractLet X 1,…,Xn (n> 1, p> 1) be {{independently}} and identically distributed normal p-vectors with mean μ and covariance matrix (μ′μC 2) I, where {{the coefficient of}} variation C is known. The authors have obtained the <b>best</b> equivariant <b>estimator</b> of μ under the loss function L(μd) =(μ−d) ′(μ−dμ′μ) They have compared the <b>best</b> equivariant <b>estimator</b> with 3 other wellknown equivariant estimators of μ and {{have shown that the}} <b>best</b> equivariant <b>estimator</b> is markedly superior to others when C→ 0...|$|R
5000|$|Further, {{while the}} {{corrected}} sample variance is the <b>best</b> unbiased <b>estimator</b> (minimum {{mean square error}} among unbiased estimators) of variance for Gaussian distributions, if the distribution is not Gaussian then even among unbiased <b>estimators,</b> the <b>best</b> unbiased <b>estimator</b> of the variance may not be ...|$|R
5000|$|The estimators G1 for sample {{skewness}} and G2 for sample kurtosis {{are used}} by DAP/SAS, PSPP/SPSS, and Excel. However, they are not used by BMDP and (according to [...] ) they were not used by MINITAB in 1998. Actually, Joanes and Gill in their 1998 study [...] concluded that the skewness and kurtosis estimators used in BMDP and in MINITAB (at that time) had smaller variance and mean-squared error in normal samples, but the skewness and kurtosis estimators used in DAP/SAS, PSPP/SPSS, namely G1 and G2, had smaller mean-squared error in samples from a very skewed distribution. It {{is for this reason}} that we have spelled out [...] "sample skewness", etc., in the above formulas, to make it explicit that the user should choose the <b>best</b> <b>estimator</b> according to the problem at hand, as the <b>best</b> <b>estimator</b> for skewness and kurtosis depends on the amount of skewness (as shown by Joanes and Gill).|$|E
5000|$|A {{real estate}} {{appraisal}} is {{like any other}} statistical sampling process. The comparables are the samples drawn and measured, and the outcome is an estimate of value—called an [...] "opinion of value" [...] in the terminology of real estate appraisal. In most statistical sampling processes, a single <b>best</b> <b>estimator</b> is sought. However, since real estate markets {{are known to be}} highly inefficient, and market transaction data is subject to significant error, the appraisal process generally relies on multiple simultaneous approaches to value, with a judgmental reconciliation as the final step to arrive at the appraiser's opinion. Thus, comparable data is used in all of the appraisal approaches.|$|E
50|$|If the {{experimental}} errors, , are uncorrelated, have {{a mean of}} zero and a constant variance, , the Gauss-Markov theorem states that the least-squares estimator, , has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical distribution function of the errors. In other words, the distribution function of the errors {{need not be a}} normal distribution. However, for some probability distributions, {{there is no guarantee that}} the least-squares solution is even possible given the observations; still, in such cases it is the <b>best</b> <b>estimator</b> that is both linear and unbiased.|$|E
40|$|In this contribution, we {{extend the}} {{existing}} theory of minimum {{mean squared error}} prediction (best prediction). This extention is motivated {{by the desire to}} {{be able to deal with}} models in which the parameter vectors have real-valued and/or integer-valued entries. New classes of predictors are introduced, based on the principle of equivariance. Equivariant prediction is developed for the real-parameter case, the integer-parameter case, and for the mixed integer/real case. The best predictors within these classes are identified, and they are shown to have a better performance than best linear (unbiased) prediction. This holds true for the mean squared error performance, as well as for the error variance performance. We show that, in the context of linear model prediction, best predictors and <b>best</b> <b>estimators</b> come in pairs. We take advantage of this property by also identifying the corresponding <b>best</b> <b>estimators.</b> All of the <b>best</b> equivariant <b>estimators</b> are shown to have a better precision than the <b>best</b> linear unbiased <b>estimator.</b> Although no restrictions are placed on the probability distributions of the random vectors, the Gaussian case is derived separately. The best predictors are also compared with least-squares predictors, in particular with the integer-based least-squares predictor introduced in Teunisse...|$|R
40|$|A purely frequentist {{development}} of James-Stein shrinkage estimators of the multivariate normal mean under quadratic loss functions is presented, {{which allows for}} an intuitive interpretation of these <b>estimators</b> as <b>best</b> <b>estimators</b> of <b>best</b> linear 'estimators' of the mean vector. Admissibility multivariate normal quadratic loss risk function shrinkage estimator...|$|R
40|$|Let X 1, [...] .,Xn (n> 1, p> 1) be {{independently}} and identically distributed normal p-vectors with mean [mu] and covariance matrix ([mu]'[mu]/C 2) I, where {{the coefficient of}} variation C is known. The authors have obtained the <b>best</b> equivariant <b>estimator</b> of [mu] under the loss function L([mu]d) =([mu]-d) '([mu]-d/[mu]'[mu]) They have compared the <b>best</b> equivariant <b>estimator</b> with 3 other wellknown equivariant estimators of [mu] and {{have shown that the}} <b>best</b> equivariant <b>estimator</b> is markedly superior to others when C [...] > 0. maximum likelihood estimator natural remanent magnetization equivariant estimator relative efficiency...|$|R
3000|$|... {{still remains}} the <b>best</b> <b>estimator</b> for small changes and shows {{acceptable}} performance {{in comparison with}} [...]...|$|E
40|$|It {{is known}} that the linear estimation, both with or without unbiasedness, may {{be reduced to a}} {{statistical}} game with a convex compact parameter set. Then all locally best estimators constitute a complete class and each locally <b>best</b> <b>estimator</b> being unique is admissible. However, if the considered locally <b>best</b> <b>estimator</b> is not unique, then all known sufficient conditions for the admissibility work very hard. We derive a simpler sufficient condition for the admissibility in a linear model with the natural parameter space...|$|E
40|$|Estimating {{distributions}} {{over large}} alphabets {{is a fundamental}} machine-learning tenet. Yet no method is known to estimate all distributions well. For example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, Jelinek-Mercer, and Good-Turing are not known to be near optimal for essentially any distribution. We describe the first universally near-optimal probability estimators. For every discrete distribution, they are provably nearly {{the best in the}} following two com-petitive ways. First they estimate every distribution nearly as well as the <b>best</b> <b>estimator</b> designed with prior knowledge of the distribution up to a permutation. Second, they estimate every distribution nearly as well as the <b>best</b> <b>estimator</b> de-signed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. Specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of Good-Turing estimator is always within KL divergence of (3 + o(1)) /n 1 / 3 from the <b>best</b> <b>estimator,</b> and that a more involved estimator is within Õ(min(k/n, 1 /√n)). Conversely, we show that any estimator must have a KL divergence ≥ Ω̃(min(k/n, 1 /n 2 / 3)) over the <b>best</b> <b>estimator</b> for the first comparison, and ≥ Ω̃(min(k/n, 1 /√n)) for the second...|$|E
40|$|We develop {{empirical}} <b>best</b> <b>estimators</b> {{for small}} area event rates {{based on the}} hierarchical Poisson model with log-normal mixing distribution, when the basic data consists of area level measurements. We derive an approximate expression to the mean squared error of the estimators and we provide a method for estimating this expression...|$|R
30|$|The most {{important}} point to emphasize about our {{approach is that}} it does not require any a priori knowledge of the (changing SNR) of any input signal. Therefore our approach is robust in a wide sense, weighting the <b>best</b> <b>estimators</b> at any given epoch to provide a consistently superior estimate to any single given technique.|$|R
40|$|Abstract. In Chen and Chiang [2] and Chen, Thompson and Hung [3], the {{symmetric}} {{trimmed mean}} has been shown, for various linear models, {{to have the}} efficiency of having asymptotic covariance matrices close to the Crámer-Rao lower bounds for some heavy tail error distributions. In this paper, we investigate some further theoretical results for this symmetric trimmed mean for the linear regression model. From the nonparametric point of view, we develop a robust version of the Gauss-Markov theorem for the problem of estimating regression parameter vector ¯ and parametric vector function C¯ where the <b>best</b> <b>estimators</b> are this trimmed mean and C multiplied by it, respectively. In addition, we show that these <b>best</b> <b>estimators</b> are the <b>best</b> Mallows-type bounded influence linear symmetric trimmed means. Finally, from the parametric aspect, we show that the symmetric trimmed mean is Rao 0 s first order efficient for a heavy tail error distribution. 1...|$|R
30|$|Open {{image in}} new window, after {{accumulation}} of infinitely many statistics, that estimator is called an unbiased estimator. Among such estimators, the one giving the least error {{is called the}} <b>best</b> <b>estimator.</b>|$|E
40|$|Abstract. In this paper, {{we propose}} {{a class of}} estimators for the {{population}} mean of a sensitive variable, taking account into a generic randomization scheme, under the simple random sampling with replacement (SRSWR), when the mean of a supplementary non-sensitive variable is known. The minimum attainable variance bound of the class is obtained and the <b>best</b> <b>estimator</b> is also defined. We prove that the <b>best</b> <b>estimator</b> acts as a regression estimator which {{is at least as}} efficient as the corresponding estimator without the auxiliary variable. A new measure of privacy protection is built, and some models can be compared from the perspective of efficiency and privacy protection...|$|E
40|$|AbstractThis paper {{deals with}} the {{asymptotic}} distribution of Wishart matrix and its application to the estimation of the population matrix parameter when the population eigenvalues are block-wise infinitely dispersed. We show that the appropriately normalized eigenvectors and eigenvalues asymptotically generate two Wishart matrices and one normally distributed random matrix, which are mutually independent. For a family of orthogonally equivariant estimators, we calculate the asymptotic risks {{with respect to the}} entropy or the quadratic loss function and derive the asymptotically <b>best</b> <b>estimator</b> among the family. We numerically show (1) the convergence in both the distributions and the risks are quick enough for a practical use, (2) the asymptotically <b>best</b> <b>estimator</b> is robust against the deviation of the population eigenvalues from the block-wise infinite dispersion...|$|E
30|$|Indeed, {{theoretical}} {{limitations for}} the <b>best</b> <b>estimators</b> exist and are {{given by the}} Cramér-Rao bound (CRB) which corresponds to the minimal error variance reachable with an unbiased estimator. This bound indicates that despite efforts to enhance the analysis methods, the maximal quality is bounded and can be insufficient for complex audio signals and demanding applications such as as active listening of music.|$|R
5000|$|The <b>best</b> {{invariant}} <b>estimator</b> [...] is the {{one that}} minimizesand this is Pitman's estimator (1939).|$|R
40|$|The {{application}} of the resampling-approach to the linear regression model analysis is considered. The mean square estimators {{are known to be}} the <b>best</b> <b>estimators</b> in case of regression model without nuisance observations. Alternatively, in the case of model with nuisance observations called disturbed model the classical approach gives bad, biased estimators. The considered numerical example shows that the resampling-approach gives comparably good results for disturbed models...|$|R
40|$|Given a {{previously}} unseen form that is morphologically n-ways ambiguous, {{what is the}} <b>best</b> <b>estimator</b> for the lexical prior probabilities for the various functions of the form? We argue that the <b>best</b> <b>estimator</b> is provided by computing the relative frequencies of the various functions among the hapax legomena — the forms that occur exactly once in a corpus. This result {{has important implications for}} the development of stochastic morphological taggers, especially when some initial hand-tagging of a corpus is required: For predicting lexical priors for very low-frequency morphologically ambiguous types (most of which would not occur in any given corpus) one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges. ...|$|E
40|$|This paper {{deals with}} the {{asymptotic}} distribution of Wishart matrix and its application to the estimation of the population matrix parameter when the population eigenvalues are block-wise infinitely dispersed. We show that the appropriately normalized eigenvectors and eigenvalues asymptotically generate two Wishart matrices and one normally distributed random matrix, which are mutually independent. For a family of orthogonally equivariant estimators, we calculate the asymptotic risks {{with respect to the}} entropy or the quadratic loss function and derive the asymptotically <b>best</b> <b>estimator</b> among the family. We numerically show (1) the convergence in both the distributions and the risks are quick enough for a practical use, (2) the asymptotically <b>best</b> <b>estimator</b> is robust against the deviation of the population eigenvalues from the block-wise infinite dispersion. Covariance matrix Wishart distribution Quadratic loss Stein's loss Asymptotic risk...|$|E
40|$|We {{study the}} {{performance}} of a large collection of block thresholding wavelet estimators, namely the Horizontal Block Thresholding family. In particular, we adopt a maxiset point of view, i. e. we are asking for the maximal functional space for a given estimator to converge in the L 2 −sense with a chosen rate of convergence. We provide sufficient conditions on the choices of rates and threshold values to ensure large maxisets. By deriving maxiset embeddings, we identify the <b>best</b> <b>estimator</b> of such a family, that is the one associated with the largest maxiset. As a particularity of this paper we propose a refined maxiset approach that models method-dependent threshold values. By a series of simulation studies, we confirm the good performance of the <b>best</b> <b>estimator</b> when comparing to the other members of its family...|$|E
40|$|In this paper, we {{consider}} GMM {{estimation of the}} regression and MRSAR models with SAR disturbances. We derive the <b>best</b> GMM <b>estimator</b> within the class of GMM estimators based on linear and quadratic moment conditions. The <b>best</b> GMM <b>estimator</b> has the merit of computational simplicity and asymptotic efficiency. It is asymptotically as efficient as the ML estimator under normality and asymptotically more efficient than the Gaussian QML estimator otherwise. Monte Carlo studies show that, with moderate-sized samples, the <b>best</b> GMM <b>estimator</b> has its biggest advantage when the disturbances are asymmetrically distributed. When the diagonal elements of the spatial weights matrix have enough variation, incorporating kurtosis of the disturbances in the moment functions will also be helpful. Spatial autoregressive models Spatial correlated disturbances GMM QMLE Efficiency...|$|R
3000|$|..., {{lower than}} the error commit when the median. But Figure 3 (d) shows that the <b>best</b> {{location}} <b>estimator</b> is the scale-W parameter ([...] [...]...|$|R
40|$|This paper, {{we studied}} {{the ability of}} geostatistical models (ordinary kriging (OK) and Inverse {{distance}} weighting (IDW)), adaptive neuro-fuzzy inference system (ANFIS) and Winter method for prediction of seasonality in prices of potatoes and onions in Iran over the seasonal period 1986 _ 2001. Results show that the <b>best</b> <b>estimators</b> in order are winter method, ANFIS and geostatistical methods. The results indicate that Winter and ANFIS had powerful results for prediction the prices while geostatistical models were not useful in this respect. ...|$|R
40|$|Our {{goal is to}} {{find the}} <b>best</b> <b>estimator</b> for the {{eligibility}} rate of the sampling units whose eligibility statuses are unknown. In this project, we focus on evaluating the survival analysis method proposed by Brick, Montaquila, and Scheuren (2000, 2002) in comparison to the method suggested by the Council of America...|$|E
3000|$|... where E[·] and V[·], respectively, are the {{expectation}} and variance operators and f(s;p) is the probability density function of s which {{depends on the}} p value. The inequality (3) means that the minimal error variance is bounded for the <b>best</b> <b>estimator.</b> Thus, if we aim at reaching a target variance V [...]...|$|E
40|$|International audienceWe {{study the}} maxiset {{performance}} of a large collection of block thresholding wavelet estimators, namely the horizontal block thresholding family. We provide sufficient conditions on the choices of rates and threshold values {{to ensure that the}} involved adaptive estimators obtain large maxisets. Moreover, we prove that any estimator of such a family reconstructs the Besov balls with a near-minimax optimal rate that can be faster than the one of any separable thresholding estimator. Then, we identify, in particular cases, the <b>best</b> <b>estimator</b> of such a family, that is, the one associated with the largest maxiset. As a particularity of this paper, we propose a refined approach that models method-dependent threshold values. By a series of simulation studies, we confirm the good performance of the <b>best</b> <b>estimator</b> by comparing it with the other members of its family...|$|E
40|$|Various wavelet-based estimators of {{self-similarity}} or long-range dependence {{scaling exponent}} are studied extensively. These estimators mainly include the (bi) orthogonal wavelet estimators and the Wavelet Transform Modulus Maxima (WTMM) estimator. This study focuses both on {{short and long}} time-series. In the framework of Fractional Auto-Regressive Integrated Moving Average (FARIMA) processes, we advocate the use of approximately adapted wavelet estimators. For these "ideal" processes, the scaling behavior actually extends down to the smallest scale, i. e., the sampling period of the time series, if an adapted decomposition is used. But in practical situations, there generally exists a cut-o# scale below which the scaling behavior no longer holds. We test the robustness of the set of wavelet-based estimators with respect to that cut-o# scale {{as well as to}} the specific density of the underlying law of the process. In all situations the WTMM estimator is shown to be the best or among the <b>best</b> <b>estimators</b> in terms of the mean square error. We also compare the wavelet estimators with the Detrended Fluctuation Analysis (DFA) estimator which was recently proved to be among the <b>best</b> <b>estimators</b> which are not wavelet-based estimators. The WTMM estimator turns out to be a very competitive estimator which can be further generalized to characterize multiscaling behavior...|$|R
50|$|For a Gaussian {{distribution}} {{this is the}} <b>best</b> unbiased <b>estimator</b> (that is, it has {{the lowest}} MSE among all unbiased estimators), but not, say, for a uniform distribution.|$|R
40|$|A novel {{approach}} to estimate (inverse) complex covariance matrices is proposed. By considering {{the class of}} unitary invariant estimators, the main challenge lies in estimating the underlying eigenvalues from sampled versions. By exploiting that {{the distribution of the}} sample eigenvalues can be derived in closed form, a Maximum A Posteriori (MAP) based scheme is then derived. The performance of the derived estimator is simulated and results indicate that the proposed scheme shows performance similar to one of the <b>best</b> <b>estimators</b> known to date. The main advantage lies in that the proposed solution only requires numerical optimization over a P-dimensional space where P is the size of the covariance matrix...|$|R
