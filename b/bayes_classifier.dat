1517|556|Public
25|$|In 2015, Zhou et al. {{suggested}} to apply naive <b>Bayes</b> <b>classifier</b> to detect pathological brains.|$|E
5000|$|Classification: Perceptron, SGD classifier, Naive <b>bayes</b> <b>classifier.</b>|$|E
5000|$|... on document-level: Searching, clustering, and Naive <b>Bayes</b> <b>classifier</b> ...|$|E
50|$|Additive {{smoothing}} {{is commonly}} {{a component of}} naive <b>Bayes</b> <b>classifiers.</b>|$|R
40|$|ABSTRACT. One-class Bayes {{learning}} such as one-class Naïve Bayes and one-class Bayesian Network employs Bayes {{learning to}} build a classifier on the positive class only for discriminating the positive class and the negative class. It {{has been applied to}} anomaly detection for identifying abnormal behaviors that deviate from normal behaviors. Because one-class <b>Bayes</b> <b>classifiers</b> can produce probability score, which can be used for defining anomaly score for anomaly detection, they are preferable in many practical applications as compared with other one-class learning techniques. However, previously proposed one-class <b>Bayes</b> <b>classifiers</b> might suffer from poor probability estimation when the negative training examples are unavailable. In this paper, we propose a new method to improve the probability estimation. The improved one-class <b>Bayes</b> <b>classifiers</b> can exhibits high performance as compared with previously proposed one-class <b>Bayes</b> <b>classifiers</b> according to our empirical results. 1...|$|R
40|$|We {{address the}} problem of {{efficiently}} learning Naive <b>Bayes</b> <b>classifiers</b> under classconditional classification noise (CCCN). Naive <b>Bayes</b> <b>classifiers</b> rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn Naive <b>Bayes</b> <b>classifiers</b> under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled. 1...|$|R
5000|$|Naive <b>Bayes</b> <b>classifier</b> with multinomial or multivariate Bernoulli event models.|$|E
50|$|In {{statistical}} classification the <b>Bayes</b> <b>classifier</b> {{minimizes the}} probability of misclassification.|$|E
5000|$|The multinomial naive <b>Bayes</b> <b>{{classifier}}</b> {{becomes a}} linear classifier when expressed in log-space: ...|$|E
5000|$|VFDTc (2006) extends VFDT for {{continuous}} data, concept drift, {{and application of}} Naive <b>Bayes</b> <b>classifiers</b> in the leaves.|$|R
50|$|Despite their naive {{design and}} {{apparently}} oversimplified assumptions, naive <b>Bayes</b> <b>classifiers</b> have worked quite well in many complex real-world situations. In 2004, {{an analysis of}} the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive <b>Bayes</b> <b>classifiers.</b> Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.|$|R
40|$|When {{classifying}} {{objects with}} Naïve <b>Bayes</b> <b>classifiers,</b> {{we are faced}} with {{the problem of how to}} handle continuous attributes. Common solutions to this problem are discretizing, or assuming the data to be normally distributed. In this paper we take a different approach and instead model the class-specific attribute distributions of Naïve <b>Bayes</b> <b>classifiers</b> with MDL-optimal histogram density functions. We present experimental results, comparing MDL-optimal histograms to Gaussian distributions and histograms learned with other methods. 1...|$|R
50|$|In 2015, Zhou et al. {{suggested}} to apply naive <b>Bayes</b> <b>classifier</b> to detect pathological brains.|$|E
50|$|Since the Naïve <b>Bayes</b> <b>classifier</b> {{is simple}} yet effective, {{it is usually}} used as a {{baseline}} method for comparison.|$|E
50|$|Contextual {{classification}} of image data {{is based on}} the Bayes minimum error classifier (also known as a naive <b>Bayes</b> <b>classifier).</b>|$|E
40|$|Abstract. Although {{at first}} sight {{probabilistic}} networks and fuzzy clustering seem to be disparate areas of research, a closer look reveals that they can both be seen as generalizations of naive <b>Bayes</b> <b>classifiers.</b> If all attributes are numeric (except the class attribute, of course), naive <b>Bayes</b> <b>classifiers</b> often assume an axis-parallel multidimensional normal distribution for each class as the underlying model. Probabilistic networks remove the requirement that the distributions must be axis-parallel by taking the covariance of the attributes into account, where this is necessary. Fuzzy clustering is an unsupervised method that tries to find general or axis-parallel distributions to cluster the data. Although {{it does not take}} into account the class information, it can be used to improve the result of naive <b>Bayes</b> <b>classifiers</b> and probabilistic networks by removing the restriction that there can be only one distribution per class. ...|$|R
50|$|Instead of {{decision}} trees, linear {{models have been}} proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive <b>Bayes</b> <b>classifiers.</b>|$|R
50|$|In machine learning, naive <b>Bayes</b> <b>classifiers</b> are {{a family}} of simple {{probabilistic}} classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.|$|R
5000|$|Given a {{collection}} [...] of labeled samples [...] and unlabeled samples , start by training a naive <b>Bayes</b> <b>classifier</b> on [...]|$|E
5000|$|The {{discussion}} {{so far has}} derived {{the independent}} feature model, that is, the naive Bayes probability model. The naive <b>Bayes</b> <b>classifier</b> combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; {{this is known as}} the maximum a posteriori or MAP decision rule. The corresponding classifier, a <b>Bayes</b> <b>classifier,</b> is the function that assigns a class label [...] for some [...] as follows: ...|$|E
50|$|Later {{supervised}} learning usually works much better when the raw input data is first translated {{into such a}} factorial code. For example, suppose the final goal is to classify images with highly redundant pixels. A naive <b>Bayes</b> <b>classifier</b> will assume the pixels are statistically independent random variables and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the naive <b>Bayes</b> <b>classifier</b> will achieve its optimal performance (compare Schmidhuber et al. 1996).|$|E
50|$|Naive <b>Bayes</b> <b>classifiers</b> are {{a popular}} {{statistical}} technique of e-mail filtering. They typically use {{bag of words}} features to identify spam e-mail, an approach commonly used in text classification.|$|R
5000|$|Unfortunately, the <b>Bayes</b> Optimal <b>Classifier</b> {{cannot be}} {{practically}} implemented for any {{but the most}} simple of problems. There are several reasons why the <b>Bayes</b> Optimal <b>Classifier</b> cannot be practically implemented: ...|$|R
40|$|Abstract — Although {{probabilistic}} {{networks and}} fuzzy clustering {{may seem to}} be disparate areas of research, they can both be seen as generalizations of naive <b>Bayes</b> <b>classifiers.</b> If all descriptive attributes are numeric, naive <b>Bayes</b> <b>classifiers</b> often assume an axis-parallel multidimensional normal distribution for each class. Probabilistic networks remove the requirement that the distributions must be axis-parallel by taking covariances into account where this is necessary. Fuzzy clustering tries to find general or axis-parallel distributions to cluster the data. Although it neglects the class information, {{it can be used to}} improve the result of the abovementioned methods by removing the restriction to only one distribution per class. I...|$|R
50|$|Classification: Building a {{model to}} assign items into {{different}} labeled groups. DAAL provides multiple algorithms in this area, including Naïve <b>Bayes</b> <b>classifier,</b> Support Vector Machine, and multi-class classifiers.|$|E
5000|$|Maximum {{conditional}} independence: if {{the hypothesis}} can be {{cast in a}} Bayesian framework, try to maximize conditional independence. This is the bias used in the Naive <b>Bayes</b> <b>classifier.</b>|$|E
5000|$|In practice, {{as in most}} of statistics, the {{difficulties}} and subtleties are associated with modeling the probability distributions effectively—in this case, [...] The <b>Bayes</b> <b>classifier</b> is a useful benchmark in statistical classification.|$|E
50|$|Naive <b>Bayes</b> <b>classifiers</b> work by {{correlating}} {{the use of}} tokens (typically words, {{or sometimes}} other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email {{is or is not}} spam.|$|R
40|$|The Tree Augmented Naïve <b>Bayes</b> (TAN) <b>classifier</b> relaxes the {{sweeping}} independence {{assumptions of the}} Naïve Bayes approach by taking account of conditional probabilities. It does this in a limited sense, by incorporating the conditional probability of each attribute given the class and (at most) one other attribute. The method of boosting has previously proven very effective in improving the performance of Naïve <b>Bayes</b> <b>classifiers</b> and in this paper, we investigate its effectiveness on application to the TAN classifier...|$|R
40|$|Naïve <b>Bayes</b> <b>classifiers,</b> {{a popular}} tool for {{predicting}} the labels of query instances, are typically {{learned from a}} training set. However, since many training sets contain noisy data, a classifier user {{may be reluctant to}} blindly trust a predicted label. We present a novel graphical explanation facility for Naïve <b>Bayes</b> <b>classifiers</b> that serves three purposes. First, it transparently explains the reasoning used by the classifier to foster user confidence in the prediction. Second, it enhances the user's understanding of the complex relationships between the features and the labels. Third, it can help the user to identify suspicious training data. We demonstrate these ideas in the context of our implemented web-based system, which uses examples from molecular biology. 1...|$|R
50|$|Despite {{the fact}} that the {{far-reaching}} independence assumptions are often inaccurate, the naive <b>Bayes</b> <b>classifier</b> has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the naive <b>Bayes</b> <b>classifier</b> will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the naive <b>Bayes</b> <b>classifier</b> are discussed in the literature cited below.|$|E
5000|$|Given a way {{to train}} a naive <b>Bayes</b> <b>classifier</b> from labeled data, it's {{possible}} to construct a semi-supervised training algorithm that can learn {{from a combination of}} labeled and unlabeled data by running the supervised learning algorithm in a loop: ...|$|E
50|$|In natural {{language}} processing, multinomial LR classifiers {{are commonly used}} {{as an alternative to}} naive Bayes classifiers because they do not assume statistical independence of the random variables (commonly known as features) that serve as predictors. However, learning in such a model is slower than for a naive <b>Bayes</b> <b>classifier,</b> and thus may not be appropriate given {{a very large number of}} classes to learn. In particular, learning in a Naive <b>Bayes</b> <b>classifier</b> is a simple matter of counting up the number of co-occurrences of features and classes, while in a maximum entropy classifier the weights, which are typically maximized using maximum a posteriori (MAP) estimation, must be learned using an iterative procedure; see #Estimating the coefficients.|$|E
40|$|Abstract. Recent work in {{supervised}} learning {{has shown that}} a surprisingly simple Bayesian <b>classifier</b> called naïve <b>Bayes</b> is competitive with {{state of the art}} classifiers. This simple approach stands from assumptions of conditional independence among features given the class. Improvements in accuracy of naïve Bayes has been demonstrated by a number of approaches, collectively named semi naïve <b>Bayes</b> <b>classifiers.</b> Semi naïve <b>Bayes</b> <b>classifiers</b> are usually based on the search of specific values or structures. The learning process of these classifiers is usually based on greedy search algorithms. In this paper we propose to learn these semi naïve Bayes structures through estimation of distribution algorithms, which are non-deterministic, stochastic heuristic search strategies. Experimental tests have been done with 21 data sets from the UCI repository...|$|R
50|$|Many {{networking}} and security companies claim {{to detect and}} control Skype's protocol for enterprise and carrier applications. While the specific detection methods used by these companies are often proprietary, Pearson's chi-squared test and stochastic characterization with Naive <b>Bayes</b> <b>classifiers</b> are two approaches that were published in 2007.|$|R
50|$|Naive <b>Bayes</b> <b>classifiers</b> {{are highly}} scalable, {{requiring}} {{a number of}} parameters linear {{in the number of}} variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.|$|R
