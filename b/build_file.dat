11|162|Public
2500|$|Additionally, the {{following}} special characters {{are not allowed}} in the first, fourth, fifth and eight character of a filename, as they conflict with the host command processor (HCP) and input sequence table <b>build</b> <b>file</b> names: ...|$|E
5000|$|One of {{the primary}} aims of Ant was to solve Make's {{portability}} problems. The first portability issue in a Makefile is that the actions required to create a target are specified as shell commands which are specific to the platform on which Make runs. Different platforms require different shell commands. Ant solves this problem by providing {{a large amount of}} built-in functionality that is designed to behave the same on all platforms. For example, in the sample [...] file above, the clean target deletes the [...] directory and everything in it. In a Makefile this would typically be done with the command: rm -rf classes/ is a Unix-specific command unavailable in some other environments. Microsoft Windows, for example, would use: rmdir /S /Q classesIn an Ant <b>build</b> <b>file</b> the same goal would be accomplished using a built-in command: ...|$|E
5000|$|These {{two forms}} of [...] {{directive}} can determine which header or source file to include in an implementation-defined way. In practice, what is usually done is that the angle-brackets form searches for source files in a standard system directory (or set of directories), and then searches for source files in local or project-specific paths (specified on the command line, in an environment variable, or in a Makefile or other <b>build</b> <b>file),</b> while the form with quotes does not search in a standard system directory, only searching in local or project-specific paths. In case there is no clash, the angle-brackets form {{can also be used}} to specify project-specific includes, but this is considered poor form. The fact that headers need not correspond to files is primarily an implementation technicality, and used to omit the [...]h extension in including C++ standard headers; in common use [...] "header" [...] means [...] "header file".|$|E
5000|$|In {{contrast}} to Make, Ninja lacks {{features such as}} string manipulation, as Ninja <b>build</b> <b>files</b> {{are not meant to}} be written by hand. Instead, a [...] "build generator" [...] should be used to generate Ninja <b>build</b> <b>files.</b> CMake, a popular build management software supports creating <b>build</b> <b>files</b> for Ninja.|$|R
5000|$|Product {{specifications}} with instant {{access to}} source code and <b>build</b> <b>files</b> ...|$|R
50|$|An {{element is}} {{anything}} that is checked into ClearCase. Here are some examples of elements: source <b>files,</b> directories, ant <b>build</b> <b>files,</b> make files, text files, and html files.|$|R
5000|$|Task-runners like gulp and grunt {{are built}} on node rather than npm because the basic npm scripts are inefficient when {{executing}} multiple tasks.Even though some developers prefer npm scripts {{because they can be}} simple and easy to implement, there are numerous ways where gulp and grunt seem to have an advantage over each other and the default provided scripts. [...] Grunt runs tasks by transforming files and saves as new ones in temporary folders and the output of one task is taken as input for another and so on until the output reaches the destination folder. This involves a lot of I/O calls and creation of many temporary files. Whereas gulp streams through the file system and does not require any of these temporary locations decreasing the number of I/O calls thus, improving performance. Grunt uses configuration files to perform tasks whereas gulp requires its <b>build</b> <b>file</b> to be coded. In grunt, each plugin needs to be configured to match its input location to the previous plugin’s output. In gulp, the plugins are automatically pipe-lined.|$|E
40|$|Despite {{their limited}} put/get interface, simple hosted storage {{services}} are becoming very popular. Many companies and individuals are {{using them to}} store and backup data. Recent work has demonstrated that it is even possible to <b>build</b> <b>file</b> systems on top of such abstractions. However, the available API lacks some features that could prove beneficiary in building distributed storage systems on top of such services. In this project I investigated extending the interface of Amazon’s S 3 storage service to allow for more powerful capabilities such as locking, multi-key transactions, and ensuring data integrity. ...|$|E
40|$|Research {{suggests}} that a well defined build process should be {{an essential part of}} any development project. Apache Ant is the most common build automation tool for the Java environment. However, present solutions to Ant IDE integration lack ways in which a developer can create or edit a <b>build</b> <b>file</b> by receiving structured support from a user friendly interface. This work describes, Ant’s Genie, a new application to fill that gap. Also described is the development process used which was based upon Extreme Programming. Ant’s Genie strives for user friendliness, has refactoring support and has support for Ant best practices. The tool has syntax highlighting and follows consistent style conventions...|$|E
50|$|Phing support (autocompletion, checks {{standard}} tags, properties, target names, path attribute {{values in}} <b>build</b> <b>files).</b>|$|R
50|$|<b>Build</b> <b>files</b> are {{provided}} for GCC makefiles, Visual Studio (2013 & 2015), Codelite, Code::Blocks and Xcode.|$|R
5000|$|It can {{generate}} automatic <b>build</b> <b>files</b> for Visual Studio, GNU Make, Xcode, Code::Blocks, CodeLite, SharpDevelop, and MonoDevelop.|$|R
40|$|Many {{scientific}} applications have intense computational and I/O requirements. Although multiprocessors {{have permitted}} astounding increases in computational performance, the formidable I/O {{needs of these}} applications cannot be met by current multiprocessors a their I/O subsystems. To prevent I/O subsystems from forever bottlenecking multiprocessors and limiting the range of feasible applications, new I/O subsystems must be designed. The successful design of computer systems (both hardware and software) depends on {{a thorough understanding of}} their intended use. A system designer optimizes the policies and mechanisms for the cases expected to most common in the user's workload. In the case of multiprocessor file systems, however, designers have been forced to <b>build</b> <b>file</b> systems based only on speculation about how they would be used, extrapolating from file-system characterizations of general-purpose workloads on uniprocessor and distributed systems or scientific workloads on vector supercomputers (see sidebar on related work). To help these system designers, in June 1993 we began the Charisma Project, so named because the project sought to characterize 1 / 0 in scientific multiprocessor applications from a variety of production parallel computing platforms and sites. The Charisma project is unique in recording individual read and write requests-in live, multiprogramming, parallel workloads (rather than from selected or nonparallel applications). In this article, we present the first results from the project: a characterization of the file-system workload an iPSC/ 860 multiprocessor running production, parallel scientific applications at NASA's Ames Research Center...|$|E
40|$|Today {{managing}} {{files in}} a server system {{has the same}} magnitude as managing the World Wide Web due to the dynamic nature of the file system. Even searching for files over the file system is time consuming because finding a file on hard disk is a long-running task. Every file on the disk has to be read with dangling pointers to files which no longer exist {{because they have been}} changed, moved or deleted. This makes the user frustrated. The Automatic file indexing framework facilitates users to resolve file names and locate documents stored in file repositories. The main design objective of the framework is to maintain sub-indexes at the folder level that have the full knowledge of the revisions that are made at the folder level automatically. This research proposes a framework that manages the creation and maintenance of the file index, with the use of Resources Description Framework (RDF) and retrieval using semantic query languages i. e. SPARQL. The sub-indexes are maintained hierarchically starting from the leaf node to the root node recursively. The proposed framework will monitor the file system continuously and update individual folder descriptors (sub-indexes) stored on each node as the file system changes making the cached indexes resilient to any file changes. The framework is resilient of file or folder name changes. Further, the study explores avenues to build an offline semantic index that can be used by the clients to perform distribute file search without performing the search on the server itself. This is viable since the framework uses semantic languages to describe and <b>build</b> <b>file</b> descriptors that can easily integrate semantic indexing and hence this makes the index readily available for the Web...|$|E
40|$|The Edinburgh Standard ML Library {{provides}} more than 200 functions {{on a range}} of types. It is written in portable Standard ML. Optimised versions exist for Standard ML of New Jersey, Poly/ML and Poplog ML. The library provides a consistent framework into which new entries can be incorporated easily. This report describes the basic framework of the library, and gives detailed documentation of all the current entries. It also gives some advice on how to write new entries. Contents 1 Introduction. 3 2 Installing the Library. 4 2. 1 Distribution. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 2. 2 Installation. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 2. 3 Building The Library. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 2. 4 Choice of <b>Build</b> <b>File.</b> : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 3 Using the Library. 6 3. 1 Getting Started. : : : : : : [...] ...|$|E
50|$|The build {{process with}} CMake {{takes place in}} two stages. First, {{standard}} <b>build</b> <b>files</b> are created from configuration files. Then the platform's native build tools are used for the actual building.|$|R
5000|$|The {{project was}} made {{available}} at sourceforge in August 2004 {{with the system}} analysis documents almost finished and some source-code implemented (infra-structure classes, <b>build</b> <b>files,</b> libs, etc.) [...] under LGPL license (totally free for any purposes).|$|R
50|$|Gradle has a {{very tight}} {{integration}} with Ant, and even treats Ant <b>build</b> <b>files</b> as scripts that could be directly imported while building. The example below shows a simplistic Ant target being incorporated as a Gradle task.|$|R
40|$|WELCOME TO Cogstack Introduction CogStack is a {{lightweight}} distributed, fault tolerant database processing architecture, {{intended to make}} NLP processing and preprocessing easier in resource constained environments. It makes use of the Spring Batch framework {{in order to provide}} a fully configurable pipeline with the goal of generating an annotated JSON that can be readily indexed into elasticsearch, or pushed back to a database. In the parlance of the batch processing domain language, it uses the partitioning concept to create 'partition step' metadata for a DB table. This metadata is persisted in the Spring database schema, whereafter each partition can then be executed locally or farmed out remotely via a JMS middleware server (only ActiveMQ is suported at this time). Remote worker JVMs then retrieve metadata descriptions of work units. The outcome of processing is then persisted in the database, allowing robust tracking and simple restart of failed partitions. Why does this project exist/ why is batch processing difficult? The CogStack is a range of technologies designed to to support modern, open source healthcare analytics within the NHS, and is chiefly comprised of the Elastic stack (elasticsearch, kibana etc), GATE, Bioyodie and Biolark (clinical natural language processing for entity extraction), OCR, clinical text de-identification, and Apache Tika for MS Office to text conversion. When processing very large datasets (10 s - 100 s of millions rows of data), it is likely that some rows will present certain difficulties for different processes. These problems are typically hard to predict - for example, some documents may have very long sentences, an unusual sequence of characters, or machine only content. Such circumstances can create a range of problems for NLP algorithms, and thus a fault tolerant batch frameworks are required to ensure robust, consistent processing. Installation We're not quite at a regular release cycle yet, so if you want a stable version, I suggest downloading v 1. 0. 0 from the release page. However, if you want more features and (potentially) fewer bugs, it's best to build from source on the master branch. To build from source: Install Tesseract and Imagemagick (can be installed but apt-get on Ubuntu) Run the following: gradlew clean build Quick Start Guide The absolute easiest way to get up and running with CogStack is to use Docker. Docker can provide lightweight virtualisation of a variety of microservices that CogStack makes use of. When coupled with the microservice orchestration docker compose technology, all of the components required to use CogStack can be set up with a few simple commands. First, ensure you have docker v 1. 13 or above installed. Elasticsearch in docker requires the following to be set on the host: sudo sysctl -w vm. max_map_count= 262144 Now you need to build the required docker containers. Fortunately, the gradle <b>build</b> <b>file</b> can do this for you. From the CogStack top level directory: gradlew buildSimpleContainers Assuming the containers have been built successfully, simply navigate to cd docker-cogstack/compose-ymls/simple/ And type docker-compose up All of the docker containers should be up and communicating with each other. You can view their status with docker ps -a That's it! "But that's what?", I hear you ask? The high level workflow of CogStack is as follows: Read a row of the table into the CogStack software Process the columns of the row with inbuilt Processors Construct a JSON that represents the table row and new data arising from the webservice Index the JSON into an elasticsearch cluster Visualise the results with Kibana To understand what's going on, we need to delve into what each of the components is doing. Let's start with the container called 'some-postgres'. Let's assume this is a database that contains a table that we want to process somehow. In fact this example database already contains some example data. If you have some database browsing software, {{you should be able to}} connect to it with the following JDBC confguration source. JdbcPath = jdbc:postgresql://localhost: 5432 /cogstack source. Driver = org. postgresql. Driver source. username = cogstack source. password = mysecretpassword You should see a table called 'tblinputdocs' in the 'cogstack' database with four lines of dummy data. This table is now constantly being scanned and indexed into elasticsearch. If you know how to use the Kibana tool, you can visualise the data in the cluster. Now bring the compose configuration down with from the same compose directory as before: docker-compose down This is the most basic configuration, and really doesn't do too much other than convert a database table/view into an elasticsearch index. For more advanced use cases/configurations, check out the integration test below. Integration Tests Although cogstack has unit tests where appropriate, the nature of the project is such that the real value fo testing comes from the integration tests. Consequently, cogstack has an extensive suite. To run the integration tests, ensure the required external services are available (which also give a good idea of how cogstack is configured). These services are Postgresql, Biolark, Bioyodie and Elasticsearch. The easiest way to get these going is with Docker. Once you have docker installed, cogstack handily will build the containers you need for you (apart from elasticsearch, where the official image will suffice). To build the containers: From the CogStack top level directory: gradlew buildAllContainers Note, Biolark and Bioyodie are external applications. Building their containers (and subsequently running their integration tests) may require you to meet their licencing conditions. Please check with Tudor Groza (Biolark) and Angus Roberts/Genevieve Gorrell if in doubt. Assuming the containers have been built successfully, navigate to cd docker-cogstack/compose-ymls/nlp/ And type docker-compose up to launch all of the external services. All being well, you should now be able to run the integration tests. Each of these demonstrate a different facet of cogstack's functionality. Each integration test follows the same pattern: Generate some dummy data for processing, by using an integration test execution listener Activate a configuration appropriate for the data and run cogstack Verify results All integration tests for Postgres can be run by using: gradlew postgresIntegTest Although if you're new to cogstack, you might find it more informative to run them individually, and inspect the results after each one. For example, to run a single test: gradlew -DpostgresIntegTest. single= -i postgresIntegTest Available classes for integration tests are in the package src/integration-test/java/uk/ac/kcl/it/postgres For example, to load the postgres database with some dummy word files into a database table called, process them with Tika, and load them into ElasticSearch index called and a postgres table called gradlew -DpostgresIntegTest. single=TikaWithoutScheduling -i postgresIntegTest then point your browser to localhost: 5601 A note on SQL Server Microsoft have recently made SQL Server available on linux, with a docker container available. This is good news, as most NHS Trusts use SQL Server for most of their systems. To run this container docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!) Password' -p 1433 : 1433 -d microsoft/mssql-server-linux [...] . noting their licence conditions. This container will then allow you to run the integration tests for SQL Server: gradlew sqlServergresIntegTest Single tests can be run in the same fashion as Postgres, substituting the syntax as appropriate (e. g.) gradlew -DsqlServerIntegTest. single=TikaWithoutScheduling -i sqlServerIntegTest A note on GATE Applications that require GATE generally need to be configured to point to the GATE installation directory (or they would need to include a rather large amount of plugins on their classpath). To do this in cogstack, set the appropriate properties as detailed in gate. *. Acceptance Tests The accompanying manuscript for this piece describes some artifically generated pseudo-documents containing misspellings and other string mutations in order to validate the de-identification algorithm without requiring access to real world data. These results can be replicated (subject to RNG) by using the acceptance test package. To reproduce the results described in the manuscript, simply run the following command: gradlew -DacceptTest. single=ElasticGazetteerAcceptanceTest -i acceptTest to reconfigure this test class for the different conditions described in the manuscript, you will need to alter the parameters inside the elasticgazetteer_test. properties file, which describes the potential options. For efficiency, it is recommended to do this from inside an IDE. Example usage in real world deployments The entire process is run through the command line, taking a path to a directory as a single argument. This directory should contain configuration files, (one complete one per spring batch job that you want to run simultaneously). These config files selectively activate Spring profiles as required to perform required data selection, processing and output writing steps. Examples of config file are in the exampleConfigs dir. Most are (hopefully) relatively self explanatory, or should be annotated to explain their meaning. example configs can be generated from the gradle task: gradlew writeExampleConfig The behaviour of cogstack is configured by activating a variety of spring profiles (again, in the config files - see examples) as required. Currently. the available profiles are inputs jdbc_in - Spring Batch's JdbcPagingItemReader for reading from a database table or view. Also requires a partitioning profile to be activated, to set a partitioning strategy. If you don't know what you're doing, just use the primaryKeyPartition profile. docmanReader - a custom reader for system that stores files in a file system, but holds their path in a database. Weird [...] . processes tika - process JDBC input with Tika. Extended with a custom PDF preprocessor to perform OCR on scanned PDF document. (requires ImageMagick and Tesseract on the PATH) gate - process JDBC input with a generic GATE app. dBLineFixer - process JDBC input with dBLineFixer (concatenates multi-row documents) basic - a job without a processing step, for simply writing JDBC input to elasticsearch deid - deidentify text with a GATE application (such as the Healtex texscrubber) or using the Cognition algorithm, which queries a database for identifiers and mask them in free text using Levenstein distance. webservice - send a document to a webservice (such as an NLP REST service, like bioyodie/biolark) for annotation. The response should be a JSON, so it can be mapped to Elasticsearch's 'nested' type. scaling localPartitioning - run all processes within the launching JVM remotePartitioning - send partitions to JMS middleware, to be picked up by remote hosts (see below) outputs elasticsearch - write to an elasticsearch cluster jdbc_out - write the generated JSON to a JDBC endpoint. Useful if the selected processes are particularly heavy (e. g. biolark), so that data can be reindexed without the need for reprocessing partitioning primaryKeyPartition - process all records based upon partitioning of the primary key primaryKeyAndTimeStampPartition - process all records based upon partitioning of the primary key and the timestamp, for finer control/ smaller batch sizes per job. Use the processingPeriod property to specify the number of milliseconds to 'scan' ahead for each job run Scheduling CogStack also offers a built in scheduler, to process changes in a database between job runs (requires a timestamp in the source database) useScheduling = true run intervals are handled with the following CRON like syntax scheduler. rate = "*/ 5 * * * * *" Logging support CogStack uses the SLF 4 J abstraction for logging, with logback as the concrete implementation. To name a logfile, simply add the -DLOG_FILE_NAME system flag when launching the JVM e. g. java -DLOG_FILE_NAME=aTestLog -DLOG_LEVEL=debug -jar cogstack- 0. 3. 0. jar /my/path/to/configs CogStack assumes the 'job repository' schema is already in place in the DB implementation of your choice (see spring batch docs for more details). The scripts to set this up for various vendors can be found here Scaling To add additional JVM processes, whether locally or remotely (via the magic of Spring Integration), just launch an instance with the same config files but with useScheduling = slave. You'll need an ActiveMQ server to co-ordinate the nodes (see config example for details) If a job fails, any uncompleted partitions will be picked up by the next run. If a Job ends up in an unknown state (e. g. due to hardware failure), the next run will mark it as abandonded and recommence from the last successful job it can find in the repository. JDBC output/reindexing Using the JDBC output profile, it is possible to generate a column of JSON strings back into a database. This is useful for reindexing large quantities of data without the need to re-process with the more computationally expensive item processors (e. g. OCR, biolark). To reindex, simply use the reindexColumn in the configuration file. Note, if you include other profiles, these will still run, but will not contribute to the final JSON, and are thus pointless. Therefore, only the 'basic' profile should be used when reindexing data. reindex = true #select the column name of jsons in the db table reindexField = sometext History This project is an update of an earlier KHP-Informatics project I was involved with called Cognition. Although Cognition had an excellent implementation of Levenstein distance for string substitution (thanks iemre!), the architecture of the code suffered some design flaws, such as an overly complex domain model and configuration, and lack of fault tolerance/job stop/start/retry logic. As such, it was somewhat difficult to work with in production, and hard to extend with new features. It was clear that there was the need for a proper batch processing framework. Enter Spring Batch and a completely rebuilt codebase, save a couple of classes from the original Cognition project. cogstack is used at King's College Hospital and the South London and Maudsley Hospital to feed Elasticsearch clusters for business intelligence and research use cases Some of the advancements in cogstack: A simple map, with a few pieces of database metadata for its domain model (essentially mapping a database row to a elasticsearch document, with the ability to embed nested types Complete, sensible coverage of stop, start, retry, abandon logic A custom socket timeout factory, to manage network failures, which can cause JDBC driver implementations to lock up, when the standard isn't fully implemented. Check out this blog post for info. The ability to run multiple batch jobs (i. e. process multiple database tables within a single JVM, each having its own Spring container Remote partitioning via an ActiveMQ JMS server, for complete scalability Built in job scheduler to enable near real time synchronisation with a database Questions? Want to help? Drop me a message: richgjackson@gmail. co...|$|E
50|$|CMake {{can locate}} executables, files, and libraries. These {{locations}} {{are stored in}} a cache, which can then be tailored before generating the target <b>build</b> <b>files.</b> The cache can be edited with a graphical editor which {{is included in the}} project.|$|R
40|$|Abstract—Software build system (e. g., make) {{plays an}} im-portant role in {{compiling}} human-readable source code into an executable program. One feature of build system such as make-based {{system is that}} it would use a <b>build</b> configuration <b>file</b> (e. g., Makefile) to record the dependencies among different target and source code files. However, sometimes important dependencies would be missed in a <b>build</b> configuration <b>file,</b> which would cause additional debugging effort to fix it. In this paper, we propose a novel algorithm named BuildPredictor to mine the missed de-pendncies. We first analyze dependencies in a <b>build</b> configuration <b>file</b> (e. g., Makefile), and establish a dependency graph which captures various dependencies in the <b>build</b> configuration <b>file.</b> Next, considering that a <b>build</b> configuration <b>file</b> is constructed based on the source code dependency relationship, we establish a code dependency graph (code graph). BuildPredictor is a com-posite model, which combines both dependency graph and code graph, to achieve a high prediction performance. We collected 7 <b>build</b> configuration <b>files</b> from various open source projects, which are Zlib, putty, vim, Apache Portable Runtime (APR), memcached, nginx, and Tengine, {{to evaluate the effectiveness of}} our algorithm. The experiment results show that compared with the state-of-the-art link prediction algorithms used by Xia et al., our BuildPredictor achieves the best performance in predicting the missed dependencies...|$|R
50|$|ITK {{uses the}} CMake (cross-platform make) build environment. CMake is an {{operating}} system and compiler independent build process that produces native <b>build</b> <b>files</b> {{appropriate to the}} OS and compiler that it is run with. On Unix CMake produces makefiles and on Windows CMake generates projects and workspaces.|$|R
40|$|Software {{development}} is a continuing, ongoing activity. With {{the exception of the}} most trivial of projects, once initial implementation and deployment of a software system is completed, the product must still be maintained. Artifacts such as source code, <b>build</b> <b>files,</b> and documentation need to be preserved to support these efforts...|$|R
40|$|Software build system (e. g., Make) {{plays an}} {{important}} role in compiling human-readable source code into an executable program. One feature of build system such as make-based system is that it would use a <b>build</b> configuration <b>file</b> (e. g., Make file) to record the dependencies among different target and source code files. However, sometimes important dependencies would be missed in a <b>build</b> configuration <b>file,</b> which would cause additional debugging effort to fix it. In this paper, we propose a novel algorithm named Build Predictor to mine the missed dependncies. We first analyze dependencies in a <b>build</b> configuration <b>file</b> (e. g., Make file), and establish a dependency graph which captures various dependencies in the <b>build</b> configuration <b>file.</b> Next, considering that a <b>build</b> configuration <b>file</b> is constructed based on the source code dependency relationship, we establish a code dependency graph (code graph). Build Predictor is a composite model, which combines both dependency graph and code graph, to achieve a high prediction performance. We collected 7 <b>build</b> configuration <b>files</b> from various open source projects, which are Zlib, putty, vim, Apache Portable Runtime (APR), memcached, nginx, and Tengine, to evaluate the effectiveness of our algorithm. The experiment results show that compared with the state-of-the-art link prediction algorithms used by Xia et al., our Build Predictor achieves the best performance in predicting the missed dependencies...|$|R
50|$|Ant, Maven, or Gradle {{can be used}} to <b>build</b> EAR <b>files.</b>|$|R
5000|$|Go {{back to the}} {{previous}} build (Windows 10): Windows 10 is an operating system for which Microsoft occasionally releases newer builds. In the event that installation of a new build of Windows 10 becomes problematic, this option allows the user to revert to an original build. It does not work if {{the previous}} <b>build's</b> <b>files</b> are deleted.|$|R
5000|$|The Rational Automation Framework {{software}} runs on a framework server, where it maintains configuration information {{that corresponds to}} product installations on one or more target systems. Target systems can be physical, virtualized, or in the cloud and can act as source systems, destination systems, or both. The product runs actions, actually elements within Apache Ant <b>build</b> <b>files,</b> to: ...|$|R
40|$|Build {{systems are}} an {{essential}} part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-) evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN <b>build</b> <b>files</b> and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0. 96 and 0. 98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73 % of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-) evolution of <b>build</b> <b>files</b> with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone <b>build</b> <b>files.</b> Comment: Accepted at the International Conference of Mining Software Repositories (MSR), 201...|$|R
5000|$|Google Web Toolkit (GWT [...] ), or GWT Web Toolkit, is an {{open source}} set of tools that allows web {{developers}} to create and maintain complex JavaScript front-end applications in Java. Other than a few native libraries, everything is Java source that can be built on any supported platform with the included GWT Ant <b>build</b> <b>files.</b> It is licensed under the Apache License version 2.0.|$|R
50|$|Gump {{can build}} shell script, Ant and Maven 1 projects, {{setting up the}} classpath appropriately. Ant and Maven 1 have special hooks built in them to give Gump {{complete}} control of the classpaths used to build and test the applications. This allows Gump to build the projects against the latest versions, even if the project's own <b>build</b> <b>files</b> have hard coded dependencies against static libraries in their own CVS or subversion repository.|$|R
50|$|A second {{portability}} {{issue is}} a result of the fact that the symbol used to delimit elements of file system directory path components differs from one platform to another. Unix uses a forward slash (/) to delimit components whereas Windows uses a backslash (\). Ant <b>build</b> <b>files</b> let authors choose their favorite convention: forward slash or backslash for directories; semicolon or colon for path separators. It converts each to the symbol appropriate to the platform on which it executes.|$|R
50|$|Each build project {{contains}} a single <b>build</b> graph (*.bg) <b>file</b> in the directory where Qbs stores the serialized representation of its internal build graph format. Deserializing this file and utilizing the cached information allows Qbs to very quickly resolve the project, leading to near-instant incremental builds. Deleting the <b>build</b> graph <b>file</b> causes a complete (clean) rebuild {{of the entire}} project. In contrast to make, Qbs does not rely on timestamps of <b>files</b> in the <b>build</b> directory, and tampering with its contents will not lead to targets being rebuilt unless the—check-outputs option is used. This is done for performance reasons because on some platforms (e.g. Windows), reading timestamps from the filesystem is very slow compared to reading them from the <b>build</b> graph <b>file.</b>|$|R
40|$|Abstract—With a {{large and}} rapidly {{changing}} codebase, Google software engineers are constantly paying interest on various forms of technical debt. Google engineers also make efforts to pay down that debt, whether through special Fixit days, or via dedicated teams, variously known as janitors, cultivators, or demolition experts. We describe several related efforts to measure and pay down technical debt found in Google’s <b>BUILD</b> <b>files</b> and associated dead code. We address debt found in dependency specifications, unbuildable targets, and unnecessary command line flags. These efforts often expose other forms of technical debt that must first be managed. Keywords-build system; technical debt; monolithic codebase; I...|$|R
50|$|As {{with most}} {{software}} tools, sbt has found advocates and critics. It is often compared against Maven, {{which is a}} standard build tool in the Java world. In particular, the domain-specific language used for sbt <b>build</b> <b>files</b> has attracted criticism as being cryptic compared to the pure declarative approach of Maven's XML files. Furthermore, an incompatible change in the file format and layout was introduced with the version jump from 0.7 to 0.10. Due to the maturity of Maven and sbt being rather young, {{it has also been}} said that Maven provides a greater number of plugins and that sbt's documentation is lacking, although others say that the quality of documentation is improving.|$|R
40|$|Compilation is an {{important}} step in building working software system. To compile large systems, typically build systems, such as make, are used. In this paper, we investigate a new research problem for <b>build</b> configuration <b>file</b> (e. g., Makefile) analysis: how to predict missed dependencies in a <b>build</b> configuration <b>file.</b> We refer to this problem as dependency mining. Based on a Makefile, we build a dependency graph capturing various relationships defined in the Makefile. By representing a Makefile as a dependency graph, we map the dependency mining problem to a link prediction problem, and leverage 9 state-of-the-art link prediction algorithms to solve it. We collected Makefiles from 7 open source projects {{to evaluate the effectiveness of}} the algorithms...|$|R
50|$|Grome 3.0 is {{currently}} licensed under two different prices, the same base version being offered {{to individuals and}} small developers under a reduced price. Professional companies benefit from premium support and dongle based protection. Normal <b>builds</b> offer <b>file</b> based activation per computer.|$|R
