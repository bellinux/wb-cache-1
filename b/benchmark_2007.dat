2|78|Public
25|$|Otinofski, Steven. Television. New York: Marshall Cavendish <b>Benchmark,</b> <b>2007.</b>|$|E
40|$|Abstract: The {{simplified}} {{spherical harmonics}} (SPn) approximation is implemented into the NIRFAST package. Preliminary studies are presented {{on a small}} 10 mm× 10 mm geometry and qualitative results are shown for SP 1, SP 3 and SP 5. Further work is underway to quantitatively test {{the accuracy of the}} SPN models using Monte Carlo data as a <b>benchmark.</b> <b>2007</b> Optical Society of America OCIS codes: (170. 3660) Light propagation in tissues; (170. 6960) Tomography 1...|$|E
50|$|In <b>2007</b> <b>Benchmark</b> Electronics {{acquired}} Pemstar Inc, {{a contract}} manufacturer.|$|R
50|$|In 2006, Tachyon was {{selected}} by the SPEC HPG {{for inclusion in the}} SPEC MPI <b>2007</b> <b>benchmark</b> suite.|$|R
50|$|In a <b>2007</b> <b>benchmark</b> of Ruby implementations, JRuby was {{faster than}} Ruby MRI 1.8 in some tests, but YARV {{outperformed}} both of them.|$|R
50|$|Lasky joined <b>Benchmark</b> in <b>2007</b> as {{a general}} partner. He focuses on {{identifying}} investment opportunities in mobile, games, digital privacy and identity, and online education. He has led the firm’s investments in and serves {{on the boards of}} Snapchat, CyanogenMod, Engine Yard, Gaia, Riot Games, PlayFab, Vivox, Grockit, Gaikai, RedRobot, Meteor Entertainment, thatgamecompany, and NaturalMotion.|$|R
50|$|Following {{the success}} of the first <b>benchmark</b> Disability Standard <b>2007</b> saw the {{introduction}} of the Chief Executives' Diamond Awards for outstanding performance and 116 organisations taking the opportunity to compare trends across a large group of UK employers and monitor the progress they had made on disability.|$|R
40|$|Abstract—In this paper, {{we present}} an image {{retrieval}} method based on feature tracking. Feature tracks are summa-rized into a compact discreet value {{and used for}} video indexing purpose. As opposed to existing space-time features, we do not make any assumption on the motion visible on the indexed videos. As a result, given an example query, our system is able to retrieve related videos from a large database. We evaluated our system with the copy detection <b>benchmark</b> MUSCLE-VCD- <b>2007.</b> We also ran retrieval experiment on hours of TV broadcast. Keywords-Video retrieval, Content based retrieval, Descrip-tor, Quantization...|$|R
40|$|We {{quantify}} the fiscal multipliers {{in response to}} the American Recovery and Reinvestment Act (ARRA) of 2009. We extend the <b>benchmark</b> Smets-Wouters (<b>2007)</b> New Keynesian model, allowing for credit-constrained households, the zero lower bound, government capital and distortionary taxation. The posterior yields modestly positive short-run multipliers around 0. 52 and modestly negative long-run multipliers around - 0. 42. The multiplier is sensitive to the fraction of transfers given to credit-constrained households, the duration of the zero lower bound and the capital. The stimulus results in negative welfare effects for unconstrained agents. The constrained agents gain, if they discount the future substantially. ...|$|R
40|$|We set out {{to model}} {{something}} of “the dynamism and vibrancy of the business environment ” (QAA Business and Management <b>benchmark</b> statement <b>2007)</b> and to develop an activity and context in which incoming Level 1 undergraduate business students embarked on their journey as business students and as creative potential professionals in a business context: “Self aware, confident, interested and excited. These people [creative potential professionals] perform higher, persist longer and demonstrate more creativity ” (Ryan and Deci, 2000, cited in Georgianna, 2007). The activity we developed was a business game: ‘Paper Chasers’. The students were to work i...|$|R
40|$|This {{spreadsheet}} contains {{data collected}} for Alliance's <b>2007</b> <b>Benchmarking</b> Report on bicycling and {{walking in the}} U. S. Tabs differentiate between data for the 50 states and the top 50 most-populous cities. Data collected here includes bicycling and walking modeshare, demographics, safety statistics, funding for bicycling and walking, policies {{at the state and}} local level, and more...|$|R
50|$|Using {{the same}} 5,000 square foot data center <b>benchmarked</b> in the <b>2007</b> model, Energy Logic 2.0 updates the ten {{prescribed}} actions to reflect current technologies and average equipment efficiency. As a result, the updated actions are forecast to yield energy savings up to 74 percent (reducing energy consumption from 1,543 kW to 408 kW {{in the model}} data center).|$|R
40|$|A {{story of}} {{students}} who have been involved as mentors in their HE courses and the wider workplace news Mary Wild tells us how students on Early Years Foundation Degree courses at Oxford Brookes University mentored undergraduate students, developing their skills of team working and leadership. Since the first Early Childhood Studies (ECS) degrees were developed in the 1990 s, the academic identity of the discipline has been fully established with the ratification of QAA <b>Benchmarks</b> in <b>2007.</b> Concurrently there has been growing recognition in UK governmental policy and research of the importance of a well educated Early Years workforce (Sylva et al., 2004). This ha...|$|R
40|$|Many {{researchers}} studying examination timetabling problems {{focus on}} either benchmark problems or problems from practice encountered in their institutions. Hyperheuristics are proposed as generic optimisation methods which explore the search space of heuristics rather than direct solutions. In the present study, {{the performance of}} tournament based hyperheuristics for the exam timetabling problem are investigated. The target instances include both the Toronto and ITC <b>2007</b> <b>benchmarks</b> and the examination timetabling problem at KAHO Sint-Lieven (Ghent, Belgium). The Toronto and ITC <b>2007</b> <b>benchmarks</b> are post-enrolment based examination timetabling problems, whereas the KAHO Sint-Lieven case is a curriculum-based examination timetabling problem. We drastically improve the previous (manually created) solution for the KAHO Sint-Lieven problem by generating a timetable that satisfies all the hard and soft constraints. We also make improvements on the best known results in the examination timetabling literature for seven out of thirteen instances for the Toronto benchmarks. The results are competitive {{with those of the}} finalists of the examination timetabling track of the International Timetabling Competition. status: publishe...|$|R
40|$|Graph {{coloring}} {{is one of}} {{the hardest}} combinatorial optimization problems for which a wide variety of algorithms has been proposed over the last 30 years. The problem is as follows: given a graph one has to assign a label to each vertex such that no monochromatic edge appears and the number of different labels used is minimized. In this paper we present a new heuristic for this problem which works with two different functionalities. One is defined by two greedy subroutines, the former being a greedy constructive one and the other a greedy modification one. The other functionality is a perturbation subroutine, which can produce also infeasible colorings, and the ability is then to retrieve feasible solutions. In our experimentation the proper tuning of this optimization scheme produced good results on known graph coloring <b>benchmarks.</b> (C) <b>2007</b> Elsevier B. V. All rights reserved...|$|R
40|$|In the 2008 QNDE {{ultrasonic}} benchmark session {{researchers from}} five different institutions {{around the world}} examined the influence that the curvature of a cylindrical fluid-solid interface has on the measured NDE immersion pulse-echo response of a flat-bottom hole (FBH) reflector. This was a repeat of a study conducted in the <b>2007</b> <b>benchmark</b> to try to determine the sources of differences seen in 2007 between model-based predictions and experiments. Here, we will summarize the results obtained in 2008 and analyze the model-based results and the experiments...|$|R
40|$|This paper {{presents}} an efficient formulation of a channel segmentation based approach to non-quasi-static modelling of the MOS transistor, {{in the context}} of a charge-based MOSFET model. In this minimal channel segmentation approach, only the essential charge equations are evaluated for each channel segment while other effects are handled at device level. As a result, simulation time is drastically reduced compared to a full channel segmentation approach. The model is validated versus measurement up to 10 GHz and passes relevant <b>benchmark</b> tests. (c) <b>2007</b> Elsevier Ltd. All rights reserved...|$|R
40|$|Two {{studies were}} part of the 2008 QNDE {{ultrasonic}} benchmark session. The first study considered the effects of the curvature of a fluid‐solid interface on the pulse‐echo immersion response of a flat‐bottom hole. This was a re‐examination of a problem considered in the <b>2007</b> <b>benchmark</b> but with a well‐characterized transducer and a different set of testing conditions. The second study considered the response of a series of side‐drilled holes at different depths. Here we will summarize the results obtained at the Center for NDE (CNDE), Iowa State University, for these two benchmark problems...|$|R
40|$|Novelty, {{coverage}} and balance are important requirements in topic-focused summarization, which {{to a large}} extent de-termine the quality of a summary. In this paper, we pro-pose a novel method that incorporates these requirements into a sentence ranking probability model. It differs from the existing methods in that the novelty, {{coverage and}} bal-ance requirements are all modeled w. r. t. a given topic, so that summaries are highly relevant to the topic {{and at the same time}} comply with topic-aware novelty, coverage and balance. Experimental results on the DUC 2005, 2006 and <b>2007</b> <b>benchmark</b> data sets demonstrate the effectiveness of our method...|$|R
40|$|We {{study the}} problem of rank aggregation: given a set of ranked lists, we want to form a {{consensus}} ranking. Furthermore, we {{consider the case of}} extreme lists: i. e., only the rank of the best or worst elements are known. We impute missing ranks by the average value and generalise Spearman’s ρ to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman’s ρ, which measures correlation between a set of ranked lists. Multivariate Spearman’s ρ is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best or worst elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman’s ρ. Finally, we demonstrate good performance on the rank aggregation <b>benchmarks</b> MQ <b>2007</b> and MQ 2008. 1...|$|R
40|$|ACM SIGIR; ACM SIGWEB; ACM SIGKDDNovelty, {{coverage}} and balance are important requirements in topic-focused summarization, which {{to a large}} extent determine the quality of a summary. In this paper, we propose a novel method that incorporates these requirements into a sentence ranking probability model. It differs from the existing methods in that the novelty, {{coverage and}} balance requirements are all modeled w. r. t. a given topic, so that summaries are highly relevant to the topic {{and at the same time}} comply with topic-aware novelty, coverage and balance. Experimental results on the DUC 2005, 2006 and <b>2007</b> <b>benchmark</b> data sets demonstrate the effectiveness of our method. © 2010 ACM...|$|R
40|$|In {{this paper}} we exploit image edges and {{segmentation}} maps to build features for object category recognition. We build a parametric line based image approximation {{to identify the}} dominant edge structures. Line ends are used as features described by histograms of gradient orientations. We then form descriptors based on connected line ends to incorporate weak topological constraints which improve their discriminative power. Using point pairs connected by an edge assures higher repeatability than a random pair of points or edges. The results are compared with state-of-the-art, and show significant improvement on challenging recognition <b>benchmark</b> Pascal VOC <b>2007.</b> Kernel based fusion is performed to emphasize the complementary nature of our descriptors {{with respect to the}} state-of-the-art features. 1...|$|R
40|$|The main agenda {{items for}} the Technical Committee (TC) meeting were: 1) {{reviewing}} Maryland’s estimate of the 2006 spring trophy fishery harvest (p. 2) and a proposal to eliminate the quota for the spring trophy fishery (p. 3); 2) reviewing a proposal from North Carolina to increase the AS/RR TAC by 50, 000 pounds (p. 5); 3) receiving a progress update from the Tagging Subcommittee (p. 7); 4) receiving a progress update from the Stock Assessment Subcommittee (p. 11); 5) and approving the Terms of Reference and timeline for the <b>2007</b> <b>benchmark</b> stock assessment (p. 13). The TC last met June 21, 2006. Note: Other than subject headings, underlining denotes tasks, bolding denotes TC consensus...|$|R
5000|$|Intel {{announced}} the iPSC/860 in 1990. The iPSC/860 consisted {{of up to}} 128 processing elements connected in a hypercube, each element consisting of an Intel i860 at 40-50 MHz or Intel 80386 microprocessor.Memory per node was increased to 8 MB and a similar Direct-Connect Module was used, which limited the size to 128 nodes.One customer was the Oak Ridge National Laboratory. The performance of the iPSC/860 was analyzed in several research projects. [...] The iPSC/860 was also the original development platform for the Tachyon parallel ray tracing engine that {{became part of the}} SPEC MPI <b>2007</b> <b>benchmark,</b> and is still widely used today.The iPSC line was superseded by a research project called the Touchstone Delta at the California Institute of Technology which evolved into the Intel Paragon.|$|R
40|$|For decades, {{personal}} bankruptcies {{increased in}} the U. S., either reflecting growing economic distress of families or a declining stigma associated with filing for bankruptcy. In {{a nod to the}} latter argument, the U. S. Congress passed the Bankruptcy Abuse Prevention and Consumer Prevention Act of 2005 (BAPCPA), after bankruptcies had grown to record high rates. The assumption was that with the new law many if not most bankruptcies would eventually disappear since they supposedly were the result of a “bankruptcy of convenience”. �The U. S. bankruptcy rate fell indeed sharply after the law went into effect, but increased quickly again afterwards. By the end of 2007, the U. S. bankruptcy rate exceeded all levels recorded during the 1980 s, and approached the levels prevalent during the early 1990 s. But it remains unclear how much of these changes resulted from BAPCPA and what was attributable to other factors. � In this Working Paper, the authors establish a benchmark level of the U. S. bankruptcy rates after 2005 that likely would have been observed if the law had not changed. They then compare the actual U. S. bankruptcy rate to the <b>benchmark</b> for <b>2007</b> to provide a sense of the effectiveness of BAPCPA. ...|$|R
40|$|This paper {{addresses}} the anomaly detection problem in large-scale data mining applications using residual subspace analysis. We are specifically concerned with {{situations where the}} full data cannot be practically obtained due to physical limitations such as low bandwidth, limited memory, storage, or computing power. Motivated by the recent compressed sensing (CS) theory, we suggest a framework wherein random projection {{can be used to}} obtained compressed data, addressing the scalability challenge. Our theoretical contribution shows that the spectral property of the CS data is approximately preserved under a such a projection and thus the performance of spectral-based methods for anomaly detection is almost equivalent to the case in which the raw data is completely available. Our second contribution is the construction of the framework to use this result and detect anomalies in the compressed data directly, thus circumventing the problems of data acquisition in large sensor networks. We have conducted extensive experiments to detect anomalies in network and surveillance applications on large datasets, including the <b>benchmark</b> PETS <b>2007</b> and 83 GB of real footage from three public train stations. Our results show that our proposed method is scalable, and importantly, its performance is comparable to conventional methods for anomaly detection when the complete data is available...|$|R
40|$|T he paper {{examines}} {{changes in}} poverty and inequality in Madagascar between the years 2001 and 2005. During this period Madagascar’s economic progress has been notable. Yet the record for poverty and living standards is mixed. Inequality has declined considerably, the depth of poverty has fallen by almost 25 percent, and income grew faster for the poor than the average. But poverty remains pervasive in Madagascar, with more than {{two thirds of the}} population below the poverty line. And though the incidence of poverty has barely changed, the number of the poor has increased by some two million individuals. Large disparities persist between urban and rural areas, as well as across provinces. Regression analysis shows that these disparities persist even after controlling {{for a wide range of}} socio-economic and demographic household characteristics. By matching household-level survey data from the Enquête Périodique auprès des Ménages to community-level census data we identify three factors that largely explain the provincial variation in poverty rates: (i) infrastructure, (ii) land tenure and cropping patterns, and (iii) climate shocks. As for the future, simulations for <b>benchmark</b> years <b>2007</b> and 2010 project incremental reductions in poverty rates on the order of 0. 5 - 2. 0 percent per yearas estimates of earnings functions, provide supporting evidence of these barriers...|$|R
40|$|Object {{detection}} when provided image-level labels {{instead of}} instance-level labels (i. e., bounding boxes) during training {{is an important}} problem in computer vision, since large scale image datasets with instance-level labels are extremely costly to obtain. In this paper, we address this challenging problem by developing an Expectation-Maximization (EM) based object detection method using deep convolutional neural networks (CNNs). Our method is applicable to both the weakly-supervised and semi-supervised settings. Extensive experiments on PASCAL VOC <b>2007</b> <b>benchmark</b> show that (1) in the weakly supervised setting, our method provides significant detection performance improvement over current state-of-the-art methods, (2) having access to {{a small number of}} strongly (instance-level) annotated images, our method can almost match the performace of the fully supervised Fast RCNN. We share our source code at [URL] 9 page...|$|R
40|$|International audienceWe {{propose a}} new model for object {{detection}} {{that is based on}} set representations of the contextual elements. In this formulation, relative spatial locations and relative scores between pairs of detections are considered as sets of unordered items. Directly training classification models on sets of unordered items, where each set can have varying cardinality can be difficult. In order to overcome this problem, we propose SetBoost, a discriminative learning algorithm for building set classifiers. The SetBoost classifiers are trained to rescore detected objects based on object-object and object-scene context. Our method is able to discover composite relationships, as well as intra-class and inter-class spatial relationships between objects. The experimental evidence shows that our set-based formulation performs comparable to or better than existing contextual methods on the SUN and the VOC <b>2007</b> <b>benchmark</b> datasets...|$|R
2500|$|Tom's Hardware {{published}} <b>benchmarks</b> in January <b>2007</b> {{that showed}} that Windows Vista executed typical applications more slowly than Windows XP with the same hardware configuration. [...] A subset of the benchmarks used were provided by Standard Performance Evaluation Corporation (or SPEC), who later stated that such [...] "results should not be compared to those generated while running Windows XP, even if testing is done with the same hardware configuration." [...] SPEC acknowledges that an apple-to-apples comparison cannot be made in cases {{such as the one}} done by Tom's Hardware, calling such studies [...] "invalid comparisons." [...] However, the Tom's Hardware report conceded that the SPECviewperf tests [...] "suffered heavily from the lack of support for the OpenGL graphics library under Windows Vista". [...] For this reason the report recommended against replacing Windows XP with Vista until manufacturers made these drivers available.|$|R
40|$|We present two {{approaches}} to extract regions from struc-tured edge detection. While the state-of-the-art algorithm based on globalized probability of boundary (gPb) gener-ates a hierarchical region tree, it entails significant com-putational load. In this work, we exploit an efficient al-gorithm for structured edge prediction to extract regions. To generate high quality regions, we develop a novel al-gorithm {{to link the}} structured edge and gPb hierarchical image segmentation framework with steerable filters. The extracted regions are grouped by the proposed hierarchi-cal grouping method to generate object proposals for effec-tive detection and recognition problems. We demonstrate the effectiveness of our region generation for image seg-mentation on the BSDS 500 database, and region generation for object proposals on the PASCAL VOC <b>2007</b> <b>benchmark</b> database. Experimental {{results show that the}} proposed al-gorithm achieves the comparable or superior quality to the state-of-the-art methods. 1...|$|R
40|$|International audienceThis paper {{addresses}} {{the problem of}} large scale image repre- sentation for object recognition and classification. Our work deals {{with the problem of}} optimizing the classification accu- racy and the dimensionality of the image representation. We propose to iteratively select sets of projections from an ex- ternal dataset, using Bagging and feature selection thanks to SVM normals. Features are selected using weights of SVM normals in orthogonalized sets of projections. The Bagging strategy is employed to improve the results and provide more stable selection. The overall algorithm linearly scales with the size of features, and thus is able to process the large state- of-the-art image representation. Given Spatial Fisher Vectors as input, our method consistently improves the classification accuracy for smaller vector dimensionality, as demonstrated by our results on the popular and challenging PASCAL VOC <b>2007</b> <b>benchmark...</b>|$|R
40|$|Many {{approaches}} to software verification are currently semi-automatic: a human must provide key logical insights — e. g., loop invariants, class invariants, and frame axioms that limit {{the scope of}} changes that must be analyzed. This paper describes a technique for automatically inferring frame axioms of procedures and loops using static analysis. The technique builds on a pointer analysis that generates limited information about all data structures in the heap. Our technique uses that information to over-approximate a potentially unbounded set of memory locations modified by each procedure/loop; this overapproximation is a candidate frame axiom. We have tested this approach on the buffer-overflow <b>benchmarks</b> from ASE <b>2007.</b> With manually provided specifications and invariants/axioms, our tool could verify/falsify 226 of the 289 benchmarks. With our automatically inferred frame axioms, the tool could verify/falsify 203 of the 289, demonstrating the effectiveness of our approach. 1...|$|R
40|$|Finance is an {{important}} subject in many accountancy and other undergraduate programmes. The technical competencies {{in this area are}} covered under the QAA <b>benchmark</b> in finance (<b>2007).</b> However, the <b>benchmark</b> does not rigidly lay down the curriculum and competencies it expects students to acquire; universities are free to teach the subject from a variety of perspectives. In this paper the subject specific knowledge and skills emphasised in finance subjects in accounting undergraduate programmes in the UK are examined. Learning outcomes from module handbooks/unit specifications from ten universities in the UK are used to gauge and analyse what cognitive skills and topics are emphasised. This research finds that universites should include higher level cognitive skills {{in order to meet the}} demands of the changing environment. It is also evident that funding and sources of capital is the most important topic in the curriculum...|$|R
40|$|This paper {{presents}} a numerical procedure for cohesive hydraulic fracture {{problems in a}} multiphase system. The transient problem of crack nucleation and/or advancement, with the ensuing topological changes, is solved by successive remeshing and projection of the field variables required in the time marching scheme. The projection is directly applied to the nodal vector of the previous step and is performed {{by means of a}} suitable mapping operator which acts on nodal forces and fluxes. This hence ensures 'a priori' the local fulfilment of the balance equations (equilibrium and mass conservation). The resulting procedure is computationally simple; however checks have to be made on its capability of conserving strain energy of the system. The latter property together with the accuracy of the solution is heuristically assessed on the basis of numerical <b>benchmarks.</b> Copyright (c) <b>2007</b> John Wiley & Sons, Ltd...|$|R
50|$|In 1995 the Federal Government {{set up an}} Information Society Working Group {{tasked with}} {{identifying}} the opportunities and threats posed {{by the development of}} the information society. In 1998 an IT-Cooperation Agreement was signed between the federal state and the various regions. In May 2003, the Austrian federal government launched an eGovernment initiative, the eGovernment Offensive, to co-ordinate all eGovernment activities in the country. The following year the short-term goal of the eGovernment Offensive - achieving a place in the EU's top 5 eGovernment leaders - was fulfilled, as Austria was ranked No. 4 in the annual eGovernment <b>benchmarking</b> survey. In <b>2007</b> according to the study The User Challenge - Benchmarking the Supply of Online Public Services, Austria is listed as the first EU member state to achieve a 100% fully online availability score for all services for citizens.|$|R
