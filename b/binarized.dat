616|178|Public
2500|$|Coined by Nancy Kendall, the [...] "sex {{education}} debates" [...] {{refers to}} the current <b>binarized</b> conversation surrounding sex education within the United States. The two sides, which supposedly exist in direct opposition to each other, are most commonly known as Abstinence-Only versus Comprehensive Sex Education. According to Kendall, this debate pertains mainly to which style of teaching is most “effective” and “appropriate” for adolescents in {{both private and public}} schools. The debate itself consists of each side continuously criticizing the other for not reducing rates of unplanned pregnancy, transmission of STIs, and for not postponing first sexual activity in students. These criticisms are generally dealt in the form of studies conducted or sponsored by Abstinence-Only or Comprehensive advocates, with the intent of once and for all convicting the other side of ineffectively educating.|$|E
5000|$|Stages are {{repeated}} for each bit (or [...] "bin") of the <b>binarized</b> symbol.|$|E
50|$|The first {{bit of the}} <b>binarized</b> {{codeword}} is bin 1; {{the second}} bit is bin 2; and so on.|$|E
3000|$|The {{final step}} of data {{preparation}} is to <b>binarize</b> the matrix, by identifying which purchases are significant {{and which are}} not. We cannot simply <b>binarize</b> the matrix considering the purchase presence/absence of a customer for a product. A matrix with a 1 if the customer [...]...|$|R
5000|$|... 1. <b>Binarize</b> {{the value}} MVDx, the motion vector {{difference}} in the x direction.|$|R
3000|$|The {{implemented}} SFS is {{a modification}} {{to the original}} SFS [29] as it <b>binarizes</b> the input feature vector and finds the highest contributing [...]...|$|R
5000|$|Object Attribute-based methods search {{a measure}} of {{similarity}} between the gray-level and the <b>binarized</b> images, such as fuzzy shape similarity, edge coincidence, etc.|$|E
5000|$|Entropy-based methods {{result in}} {{algorithms}} {{that use the}} entropy of the foreground and background regions, the cross-entropy between the original and <b>binarized</b> image, etc.|$|E
50|$|Despite Linaro's ARM {{focus the}} 96Boards {{specification}} is fairly relaxed about the processors {{that need to}} be supported which allows vendors to experiment with boards such as the Curious Curie which is based on the Intel Quark processor with an embedded <b>binarized</b> neural network (eBNN).|$|E
40|$|Abstract—In {{large-scale}} query-by-example retrieval, embedding image signatures in {{a binary}} space offers two benefits: data compression and search efficiency. While most embedding algorithms <b>binarize</b> both query and database signatures, {{it has been}} noted {{that this is not}} strictly a requirement. Indeed, asymmetric schemes which <b>binarize</b> the database signatures but not the query still enjoy the same two benefits but may provide superior accuracy. In this work, we propose two general asymmetric distances which are applicable {{to a wide variety of}} embedding techniques including Locality Sensitive Hashing (LSH), Locality Sensitiv...|$|R
40|$|Binarization of Character Images is very {{important}} preprocessing for character recoginition system. And to reduce processing time is {{very important}}. Our study is how to <b>binarize</b> character images which were taken with CCD camera. CCD camera has some advantages of such as its input area is free and also input speed is fast. But as weak point, it has shading for bad lighting conditions. Therefore, we studied how to <b>Binarize</b> character images with shading. We decided the threshold by segmentate the region and by the proposed simple method process time was reduced...|$|R
40|$|Abstract:- This paper {{presents}} a selective local thresholding method {{of a camera}} based document image for applying character segmentation. It {{is very difficult to}} <b>binarize</b> a camera based document image due to inconsistent lighting condition. It makes difficulties of character extraction and recognition. A selective local thresholding method in character region can reduce illumination effect and extract properly characters. Edge operator is used to detect some candidate character. The local thresholding method is applied in the selected candidate character region. Distribution of intensity value of pixels is used to <b>binarize</b> the character region. It is an efficient method to <b>binarize</b> camera image and to extract characters from background image. The proposed method can be implemented in mobile device because of high speed processing time and low memory requirement. To evaluate the proposed method, we have experimented with camera document image of the ETRI database. An encouraging results have been obtained. Key-Words:- Camera document recognition; Local thresholding; Character segmentatio...|$|R
5000|$|Context model selection: A [...] "context model" [...] is a {{probability}} model {{for one or}} more bins of the <b>binarized</b> symbol. This model may be chosen from a selection of available models depending on the statistics of recently coded data symbols. The context model stores the probability of each bin being [...] "1" [...] or [...] "0".|$|E
50|$|Beside {{being the}} first full {{featured}} Interactive Whiteboard software available in open source, Open-Sankoré brings two key innovations to the market. Firstly, its file format is non <b>binarized</b> {{and is based on}} the W3C web standard, therefore making it possible to be displayed in a modern web browser. This allows the content creators to distribute their lessons online without the need to install the software or a plugin. Second, the software can be extended using Apps which are written using the W3C widget standard. This allows the developers of the software to focus on the core functionality of Open-Sankoré while letting the community easily develops a wealth of more specific apps meeting their own needs.|$|E
5000|$|Coined by Nancy Kendall, the [...] "sex {{education}} debates" [...] {{refers to}} the current <b>binarized</b> conversation surrounding sex education within the United States. The two sides, which supposedly exist in direct opposition to each other, are most commonly known as Abstinence-Only versus Comprehensive Sex Education. According to Kendall, this debate pertains mainly to which style of teaching is most “effective” and “appropriate” for adolescents in {{both private and public}} schools. The debate itself consists of each side continuously criticizing the other for not reducing rates of unplanned pregnancy, transmission of STIs, and for not postponing first sexual activity in students. These criticisms are generally dealt in the form of studies conducted or sponsored by Abstinence-Only or Comprehensive advocates, with the intent of once and for all convicting the other side of ineffectively educating.|$|E
30|$|Nano-CT {{scanning}} {{can be used}} {{to photograph}} the true pore structure of tight sandstone. The image segmentation method based on experiment-measured porosity is established to <b>binarize</b> the digital image of tight sandstone.|$|R
40|$|Abstract. FCA-based {{classifiers}} {{can deal}} with nonbinary data represen-tation in different ways: use it directly or <b>binarize</b> it. Those algorithms that <b>binarize</b> data use metric information from the initial feature space only {{as a result of}} scaling (feature binarization procedure). Metric ap-proach in this area allows one significantly reducing classification refusals number and provides additional information which can be used for clas-sifier training. In this paper we propose an approach which generalizes some of existing FCA classification methods and allows one to modify them. Unlike other algorithms, the proposed classifier model uses initial metric information together with order object-attribute dependencies...|$|R
40|$|Abstract: In this paper, {{an image}} {{binarization}} method {{based on a}} new discriminant criterion is proposed. The criterion emphasizes much the homogeneity of the object gray level distribution and while intentionally de-emphasizes the heterogeneity of the background such that the new <b>binarizing</b> or thresholding method can overcome some shortcomings of famous Otsu’s method. Experimental results on the three real images show that compared to both Otsu’s and recent Kwon’s <b>binarizing</b> methods, the proposed method has not only visually better or comparable segmentation effect but also, more favorably, removal ability for noise. Keywords: Image binarization; Otsu’s method; discriminant analysis; threshold selection; image segmentation...|$|R
50|$|The {{framework}} {{proposed in}} this method by Soonmin Hwang et al., is split into four steps. First, {{the data from}} the camera and 3D lidar is input into to the system. Both inputs from lidar and camera are parallelly obtained and the color image from the camera is calibrated with the lidar. To improve the efficiency, horizontal 3D point sampling is applied as pre-processing. Second, the segmentation stage is where the entire 3D points are divided into several groups per the distance from the sensor and local planes from close plane to far plane are sequentially estimated. The local planes are estimated using statistical analysis. The group of points closer to the sensor are used to compute the initial plane. By using the current local plane, the next local plane is estimated by iterative update. The object proposals in the 2D image are used to separate foreground objects from background. For faster and accurate detection and tracking <b>Binarized</b> Normed Gradients for Objectness Estimation at 300fps is used. BING is a combination of normed gradient and its <b>binarized</b> version which speeds up the feature extraction and testing process, to estimate the objectness of an image window. This way the foreground and background objects are separated. To form objects after estimating the objectness of an image using BING, the 3D points are grouped or clustered. Clustering is done using DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm which could be robust due to its less-parametric characteristic. Using the clustered 3D points, i.e. 3D segment, more accurate region-of-interests (RoIs) are generated by projecting 3D points on the 2D image. The third step is detection, which is broadly divided into two parts. First is object detection in 2D image which is achieved using Fast R-CNN as this method doesn't need training and it also considers an image and several regions of interest. Second is object detection in 3D space which is done by using the spin image method. This method extracts local and global histograms to represent a certain object. To merge the results of 2D image and 3D space object detection, same 3D region is considered and two independent classifiers from 2D image and 3D space are applied to the considered region. Scores calibration is done to get a single confidence score from both detectors. This single score is obtained in the form of probability. The final step is tracking. This is done by associating moving objects in present and past frame. For object tracking, segment matching is adopted. Features such as mean, standard deviation, quantized color histograms, volume size and number of 3D points of a segment are computed. Euclidean distance is used to measure differences between segments. To judge the appearance and disappearance of an object, similar segments (obtained based on the Euclidean distance) from two different frames are taken and the physical distance and dissimilarity scores are calculated. If the scores go beyond a range for every segment in previous frame, the object being tracked is considered to have disappeared.|$|E
40|$|Gray-scale {{document}} {{images are}} <b>binarized</b> {{in order to}} perform optical character recognition (OCR). To perform this binarization, {{a variety of techniques}} have been proposed for performing threshold selection. Most of these are based on the intensity distribution of the image regions. However, when the spacing between two lines is very small, it is difficult to produce high quality characters using a fixed threshold. Also, with a fixed threshold, {{it is very hard to}} produce continuous lines when <b>binarized,</b> because the binarization introduces gaps within the line. This paper presents an adaptive threshold method based on gradient properties which handles the above problems. Our method consists of three steps. First we extract the pixels located at the boundary of a character and the background, then determine a threshold for each of the extracted pixels. After binarizing the boundary pixels, the remaining pixels are <b>binarized</b> based on the <b>binarized</b> boundary pixels. Our results show that an adaptive threshold produces higher quality <b>binarized</b> images than does the discriminant analysis method. ...|$|E
40|$|A {{necessary}} {{step for}} {{the recognition of}} scanned documents is binarization, which is essentially the segmentation of the document. In order to binarize a scanned document, we can find several algorithms in the literature. What is the best binarization result for a given document image? To answer this question, a user needs to check different binarization algorithms for suitability, since different algorithms may work better for different type of documents. Manually choosing the best from a set of <b>binarized</b> documents is time consuming. To automate {{the selection of the}} best segmented document, either we need to use ground-truth of the document or propose an evaluation metric. If ground-truth is available, then precision and recall can be used to choose the best <b>binarized</b> document. What is the case, when ground-truth is not available? Can we come up with a metric which evaluates these <b>binarized</b> documents? Hence, we propose a metric to evaluate <b>binarized</b> document images using eigen value decomposition. We have evaluated this measure on DIBCO and H-DIBCO datasets. The proposed method chooses the best <b>binarized</b> document that is close to the ground-truth of the document...|$|E
40|$|How to {{effectively}} approximate real-valued parameters with binary codes plays {{a central role}} in neural network binarization. In this work, we reveal an important fact that <b>binarizing</b> different layers has a widely-varied effect on the compression ratio of network and the loss of performance. Based on this fact, we propose a novel and flexible neural network binarization method by introducing the concept of layer-wise priority which <b>binarizes</b> parameters in inverse order of their layer depth. In each training step, our method selects a specific network layer, minimizes the discrepancy between the original real-valued weights and its binary approximations, and fine-tunes the whole network accordingly. During the iteration of the above process, it is significant that we can flexibly decide whether to <b>binarize</b> the remaining floating layers or not and explore a trade-off between the loss of performance and the compression ratio of model. The resulting binary network is applied for efficient pedestrian detection. Extensive experimental results on several benchmarks show that under the same compression ratio, our method achieves much lower miss rate and faster detection speed than the state-of-the-art neural network binarization method. Comment: 8 page...|$|R
50|$|Otsu's {{method is}} related to Fisher's linear discriminant, and was created to <b>binarize</b> the {{histogram}} of pixels in a grayscale image by optimally picking the black/white threshold that minimizes intra-class variance and maximizes inter-class variance within/between grayscales assigned to black and white pixel classes.|$|R
40|$|Binarization of grammars {{is crucial}} for {{improving}} the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored {{to a number of}} grammar formalisms by simply varying a formal parameter. We apply our algorithm to <b>binarizing</b> tree-to-string transducers used in syntax-based machine translation...|$|R
40|$|SIFT {{descriptors}} are broadly used {{in various}} emerging applications. In recent years, these descriptors were deployed in compressed and <b>binarized</b> forms {{due to the}} computational complexity, storage, security and privacy cost incurred by working on real data. At the same time, the theoretical analysis of SIFT feature performance in different applications remains an open issue {{due to the lack}} of accurate statistics of <b>binarized</b> SIFT descriptors. We address this problem and statistically analyse projected <b>binarized</b> SIFT descriptors in this paper. The methodology is based on dimensionality reduction using random projections with binarization. Furthermore, we investigate the statistical models of intra- and inter-descriptor dependencies for various distortions. Finally, we demonstrate a simple heuristic to distinguish between descriptors from identical but distorted images and descriptors from non identical images...|$|E
40|$|International audienceAs a very {{valuable}} cultural heritage, palm leaf manuscripts offer a new challenge in document analysis system due to the specific characteristics on physical support of the manuscript. With the aim of finding an optimal binarization method for palm leaf manuscript images, creating a new ground truth <b>binarized</b> image is a necessary step in document analysis of palm leaf manuscript. But, regarding to the human intervention in ground truthing process, an important remark about the subjectivity effect {{on the construction of}} ground truth <b>binarized</b> image has been analysed and reported. In this paper, we present an experiment in a real condition to analyse the existance of human subjectivity on the construction of ground truth <b>binarized</b> image of palm leaf manuscript images and to measure quantitatively the ground truth variability with several binarization evaluation metrics...|$|E
30|$|After edge thresholding, the {{morphological}} operators {{were employed}} to obtain single-pixel edge curves, resulting in the final <b>binarized</b> edge image shown in Figure  4 c.|$|E
30|$|Pre-processing: As the {{proposed}} method {{is based on}} features derived from individual characters, {{the characters in the}} tilt-corrected scanned image are first segmented. A threshold Th is generated to <b>binarize</b> the character image and used to simultaneously divide each character image into three parts: the text region, edge region, and background region.|$|R
40|$|Abstract: Quick Response Code {{has been}} widely used in the {{automatic}} identification field. This paper proposes a implementation of real-time Quick Response Code recognition using mobile, which is an efficient technology used for data transferring. An image processing system based on mobile is described {{to be able to}} <b>binarize,</b> locate, segment, and decode the QR Code...|$|R
40|$|For most deep {{learning}} algorithms training is notoriously time consuming. Since {{most of the}} computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that {{eliminates the need for}} most of these. Our method consists of two parts: First we stochastically <b>binarize</b> weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to <b>binarizing</b> the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR 10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks. Comment: Published as a conference paper at ICLR 2016. 9 pages, 3 figure...|$|R
30|$|Step 7 : Perform the {{statistics}} {{on the number of}} 0 and 1 pixels for the <b>binarized</b> image and store the statistical data as an array.|$|E
3000|$|Heuristic aspects: {{the aspect}} {{extraction}} technique {{described in the}} “Extracting aspects through heuristics” section, using the <b>binarized</b> sentiment approach in the items’ representation creation module; [...]...|$|E
30|$|Detection of {{vehicles}} on the road segments using image sequences is performed in the following way. First, two images with a short time lag usually few seconds (this value {{is derived from the}} constraints that a vehicle should not overlap with a previous vehicle and with itself, for more information see Sections  2.4. 3 and 2.4. 4) are selected, then the region of interest (tube) is defined based on the route middle and the change image is obtained with the MAD algorithm [14]. Finally, the obtained change image—chi squared image of MAD components—is <b>binarized</b> and denoised, e.g. by median filter. For an example of <b>binarized</b> images see the lower images in Fig.  5 (b–e). Now the vehicle density can be estimated for each road segment from the <b>binarized</b> image defined as the ratio of the number of white pixels to the total number of pixels in the road segment.|$|E
40|$|An {{original}} {{approach to}} <b>binarize</b> gray level images is proposed. A multi-scale algorithm {{based on a}} statistical test of homogeneity decides if a region belongs to the back-ground or not. At each iteration the image is smoothed with a nonlinear filter in order to remove the noise. Stable re-gions in scale space are used to automatically find a thresh-old from the intensity histogram. ...|$|R
40|$|In this research, {{we propose}} {{a method of}} {{improving}} on the accuracy of detecting nasal cavity location in far infrared images for non-contact measurement of human breathing. We found that although our previous method for far infrared imaging can detect regions that include nasal cavities well, several false alarms occur. In order to reduce false alarms, we propose to apply false alarm classification into our current method. O bject detection method based on a boosted cascade of Haar-like feature classifiers are applied to find the candidates of the region including nasal cavities. In false alarm classification, <b>binarize</b> process is employed to segment facial area and background strictly. Based on the result of <b>binarize</b> process, false alarm on background can be classified {{from the results of}} detection. 5, 100 FIR images are collected to train our nasal cavity detector; we evaluate the number of false alarms and detection failures. The results show that proposed method can reduce false alarm events. </span...|$|R
40|$|We {{present a}} new process of {{analyzing}} data to determine critical at-tributes in a classification problem. Our method allows for maximal error tolerance {{in the original}} data while producing a choice of simple Boolean formulas for classifying the data. We implement the method and test it on the Wisconsin Breast Cancer Database with classification accuracy over ninety percent. This process utilizes an existing dualiza-tion algorithm and the method of <b>binarizing</b> data with missing bits...|$|R
