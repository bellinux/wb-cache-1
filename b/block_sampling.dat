65|939|Public
5000|$|Note that {{a problem}} like this can be solved by <b>block</b> <b>sampling</b> the entire 100-bit vector at once. (This assumes that the 100-bit vector {{is part of a}} larger set of variables. If this vector is the only thing being sampled, then <b>block</b> <b>sampling</b> is {{equivalent}} to not doing Gibbs sampling at all, which by hypothesis would be difficult.) ...|$|E
50|$|The first {{exclusive}} {{exercise for}} enumeration of wild elephants in the ERs was conducted during February to May 2005. This exercise {{also sought to}} experiment with two sampling methods, viz. <b>Block</b> <b>sampling</b> and Line transect-Dung Count. PE arranged for training of trainers and also issued detailed guidelines to the chief wildlife wardens and the field coordinators. Total population of elephants in 2005 was nearly 21,200. The latest census carried out in 2012 put elephant numbers between 27,785 and 31,368.|$|E
50|$|This {{section is}} more {{specific}} {{with regard to}} how the ground is investigated using methods such as excavating or drilling. Frequency of sampling and testing can be decided with the following in mind, {{the determination of the}} character and structure of all the strata and ground water conditions, the determination of the properties of the strata and the use of special techniques should ‘normal’ techniques not give satisfactory results. Shallow trial pits go to a maximum depth of 4-5 metres, comprehensive records should include the location and orientation of the pit and the face logged. Samples should be taken as soon as the pit is opened and closed as soon as possible properly - there are however advantages to leaving them open for a time. Samples are taken from deep trial pits and shafts at certain sites if necessary and if below the water table can become a more complicated process. Boring augers are in common use. There are two types of rotary drilling, open hole drilling and core drilling. The selection of the type and method used can depend on ground conditions and time and cost constraints. Recovered cores should be maintained as near as possible to its natural state until it is stored. In most cases it is inevitably disturbed. Another method is wash boring which is most applicable to sands, silts, and clays. However, these are not representative of the character and consistency of the penetrated strata. Ground water conditions are determined from water level in boreholes and the use of standpipe, hydraulic, electrical and pneumatic piezometers. Water samples should be representative and stored in appropriate containers. Backfilling should be well compacted to obviate the flow of groundwater to any aquifer below and/or settlement. The use of cement based grout can be used - bentonite is also used to decrease shrinkage. Sampling quality can be classified to determine depending on their disturbance and other factors such as, wet or dry ground. Samplers should conform to the standard. Sampling takes different forms i.e. - continuous sampling, the sand and window sampler and <b>block</b> <b>sampling.</b> Due to the cost of sample acquisition, samples should be treated with great care. Good methods of handling and labelling should be established.|$|E
30|$|In this study, {{we report}} on {{geochemistry}} and rock magnetic properties of volcanic ash particles extracted from tephra-bearing ice <b>block</b> <b>samples</b> collected from Nansen Ice Field, Antarctica. The distribution of volcanic ash particles within tephra-bearing ice <b>block</b> <b>samples</b> was also measured with a microfocus X-ray computed tomography (X-ray CT) scanner and presented. Then, the tephra-bearing ice <b>block</b> <b>samples</b> were measured with a three-axis LTS-SQUID gradiometer developed for nondestructive evaluation. The extracted volcanic ash particles were used to construct a thin artificial ash layer of half-circular shape to imitate half-round ice cores and measured with the gradiometer.|$|R
40|$|General {{engineering}} {{properties of}} Leda clay {{have been recorded}} but this record is far from complete. Detailed studies of shear strength have been confined to samples from the Ottawa area. Earlier tests on tube samples indicated the need for fundamental studies on good undisturbed <b>block</b> <b>samples.</b> At every opportunity, therefore, <b>block</b> <b>samples</b> have been obtained from deep excavations. Peer reviewed: NoNRC publication: Ye...|$|R
40|$|The {{scanning}} thermogram of a <b>block</b> <b>sample</b> of a {{double-base propellant}} shows a shoulder around 200 °C {{which is not}} observed in a powder sample of the sample propellant. The heat of decomposition was also found to be different In the two cases. Product analysis and activation energy calculations show that nitroglycerine un dergoes decomposition in the <b>block</b> <b>sample,</b> whereas it vaporizes in the powder sample...|$|R
40|$|<b>Block</b> <b>sampling</b> of soils {{can produce}} {{samples of the}} highest quality. However during <b>block</b> <b>sampling</b> {{attention}} has {{to be given to}} a number of practical aspects to minimize disturbance to the material. Mechanisms by which disturbances can be introduced include strains imposed to samples during the act of sampling, swelling, stress relief and moisture content changes during storage. This paper discribes a number of techniques to obtain block samples. Techniques referred to include sampling the soil from test pits, auger holes and tunnels as well as down hole <b>block</b> <b>sampling</b> under high water table conditions. Themechanisms by which disturbance to samples can occur are discussed, and practical guidelines to minimize the level of disturbance are suggested...|$|E
40|$|The paper {{considers}} the <b>block</b> <b>sampling</b> method for long-range dependent processes. Our theory generalizes earlier ones by Hall, Jing and Lahiri (1998) on functionals of Gaussian processes and Nordman and Lahiri (2005) on linear processes. In particular, we allow nonlinear transforms of linear processes. Under suitable conditions on physical dependence measures, we prove {{the validity of}} the <b>block</b> <b>sampling</b> method. The problem of estimating the self-similar index is also studied...|$|E
40|$|The {{inference}} {{procedure for}} {{the mean of}} a stationary time series is usually quite different under various model assumptions because the partial sum process behaves differently {{depending on whether the}} time series is short or long-range dependent, or whether it has a light or heavy-tailed marginal distribution. In the current paper, we develop an asymptotic theory for the self-normalized <b>block</b> <b>sampling,</b> and prove that the corresponding <b>block</b> <b>sampling</b> method can provide a unified inference approach for the aforementioned different situations {{in the sense that it}} does not require the a priori estimation of auxiliary parameters. Monte Carlo simulations are presented to illustrate its finite-sample performance. The R function implementing the method is available from the authors. Comment: 32 pages, minor revisio...|$|E
40|$|Techniques {{required}} to successfully obtain downhole <b>block</b> <b>samples</b> of typical very soft high plasticity organic clay from Ireland are described. The vane shear {{strength of the}} material is as low as 4 kPa. These included using a sampler penetration rate three times faster than normally adopted. Comparisons are made between the results of laboratory tests on Sherbrooke <b>block</b> <b>samples,</b> on two fixed piston tube samplers and on a continuous sampler. In addition idealised tube sampling strains were imposed on <b>block</b> <b>sample</b> specimens prior to shearing (ISA approach). Both approaches confirmed that the material studied could not survive tube sampling undamaged, unlike {{the findings of a}} recent study in the Netherlands on Dutch organic soil. Tube sampling was found to have a more significant effect on triaxial test parameters that on those from the 1 D compression testing, where the behaviour of the <b>block</b> <b>sample</b> specimens and those from one of the two tube samplers were similar to in situ response. Increasing levels of disturbance were associated with progressively more dilatant behaviour...|$|R
30|$|To {{avoid the}} less-fired and/or unfired parts, we used only the {{surficial}} {{parts of the}} blocks (within 1.5  cm from the kiln floor) for experiments. For AFD measurements, each 1.5 -cm cubic specimen was cut from the <b>block</b> <b>samples</b> and then placed into a 7 -cm 3 plastic case with paper clay. For measurements of archeointensity and ThD, a 1.5 -cm 3 specimen cut from the <b>block</b> <b>samples</b> was placed into a silica glass case with glass wool and high-temperature cement. For the magnetic and mineralogical experiments, crushed specimens were prepared from the <b>block</b> <b>samples.</b> In the archeodirection measurements, the declination collection was set to be −[*] 7.3 °, based on the IGRF- 12 model (International Association of Geomagnetism and Aeronomy, Working Group V-MOD 2010).|$|R
30|$|Parts of both {{green and}} {{air-dried}} wood <b>block</b> <b>samples</b> were sterilized by two sterilization methods. One is autoclave sterilization treatment using the autoclave sterilizers system (HA- 300 MII, HIRAYAMA Manufacturing Co., Saitama, Japan). Ten {{pieces of wood}} <b>block</b> <b>samples</b> were enveloped in a self-sealing sterilization pouch (9  ×  23  cm, CROSSTEX, NY, USA), set in the chamber and then autoclave heated to 120 °C under a steaming condition. Following the autoclave treatment for 2  h, the pouch {{was removed from the}} chamber.|$|R
40|$|Block {{coordinate}} descent {{methods and}} stochastic subgradient {{methods have been}} extensively studied in optimization and machine learning. By combining randomized <b>block</b> <b>sampling</b> with stochastic subgradient methods based on dual averaging, we present stochastic block dual averaging (SBDA) [...] -a novel class of block subgradient methods for convex nonsmooth and stochastic optimization. SBDA requires only a block of subgradients and updates blocks of variables and hence has significantly lower iteration cost than traditional subgradient methods. We show that the SBDA-based methods exhibit the optimal convergence rate for convex nonsmooth stochastic optimization. More importantly, we introduce randomized stepsize rules and <b>block</b> <b>sampling</b> schemes that are adaptive to the block structures, which significantly improves the convergence rate w. r. t. the problem parameters. This is {{in sharp contrast to}} recent block subgradient methods applied to nonsmooth deterministic or stochastic optimization. For strongly convex objectives, we propose a new averaging scheme to make the regularized dual averaging method optimal, without having to resort to any accelerated schemes...|$|E
40|$|Sequential Monte Carlo (SMC) {{methods are}} a {{powerful}} set of simulation-based techniques for sampling sequentially from {{a sequence of}} complex probability distributions. These methods rely {{on a combination of}} importance sampling and resampling techniques. In a Markov chain Monte Carlo (MCMC) framework, <b>block</b> <b>sampling</b> strategies often perform much better than algorithms based on one-at-a-time sampling strategies if "good" proposal distributions to update blocks of variables can be designed. In an SMC framework, standard algorithms sequentially sample the variables one at a time whereas, like MCMC, the efficiency of algorithms could be improved significantly by using <b>block</b> <b>sampling</b> strategies. Unfortunately, a direct implementation of such strategies is impossible as it requires the knowledge of integrals which do not admit closed-form expressions. This article introduces a new methodology which by-passes this problem and is a natural extension of standard SMC methods. Applications to several sequential Bayesian inference problems demonstrate these methods. © 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America...|$|E
40|$|In {{this paper}} {{we present a}} nonparametric Bayesian {{approach}} for £tting unsmooth or highly oscillating functions in regression models with binary responses. The approach extends previous work by Lang et al. (2002) for Gaussian responses. Nonlinear functions are modelled by £rst or second order random walk priors with locally varying variances or smoothing parameters. Estimation is fully Bayesian and uses latent utility representations of binary regression models for ef£cient <b>block</b> <b>sampling</b> from the full conditionals of nonlinear functions. Key words: adaptive smoothing, forest health data, highly oscillating functions, MCMC, random walk priors, unsmooth functions, variable smoothing parameter. ...|$|E
40|$|Purpose: Turkey {{have the}} rich regions related to {{asbestos}} and erionite, and their inhabitants have been environmentally exposed to asbestos and erionite since childhood. The {{aim of this}} study was to investigate the p 53 tumour suppressor gene and ras oncogene mutations in patients with mesothelioma relation to asbestos and erionite exposure. Material and Methods: The Multiplex polymerase chain reaction (MPCR) assay was employed to examine mutations in the p 53 tumour suppressor gene and K-ras & N-ras proto-oncogene in 48 paraffin <b>block</b> <b>samples</b> from 41 patients with mesothelioma and malign mesothelioma relation to asbestos and erionite exposure. Results: No mutations in exon 2 - 4, exon 5 - 6, exon 7 - 9, and exon 10 - 11 of the p 53 tumour suppressor gene were determined in any of the paraffin <b>block</b> <b>samples</b> with mesothelioma and malign mesothelioma. Although no mutations were found in the exon 2 (K-ras) and exon 1 (N-ras) in the paraffin <b>block</b> <b>samples</b> analysed, mutations were found in the exon 1 (K-ras) in paraffin <b>block</b> <b>samples</b> of two patients with mesothelioma (4. 88 %). Conclusion: These results indicate the p 53 tumour suppressor gene and ras proto-oncogene mutations may not play a critical role in the induction of mesothelioma by asbestos and erionite in humans...|$|R
40|$|This report {{presents}} {{preliminary results}} of laboratory testing {{of a small}} <b>block</b> <b>sample</b> of Topopah Spring Tuff. This {{is the first in}} a series of tests on small <b>block</b> <b>samples.</b> The purpose of these tests is to investigate the thermal-mechanical, thermal-hydrological, and thermal-chemical response of the rock to conditions similar to the near-field environment (NFE) of a potential nuclear waste repository. This report presents preliminary results of deformation and elastic- wave velocity measurements on a 0. 5 -m scale block of Topopah Spring tuff tested in uniaxial compression and at temperatures to 85 {degrees}C...|$|R
30|$|It is {{important}} to note that comparison of different algorithms may not be always fair due to different system conditions applied to different algorithms. Thus, making trade-off decisions between different algorithms may well be intuitional. Many a time, the intuitional decision is quite obvious and hence rather easy to make. In our cases here, it is certainly unfair to compare the CRB for an ML estimator based on one training <b>block</b> <b>sample</b> with the estimator MSE using multiple training <b>block</b> <b>samples.</b> Nonetheless, the key point here is, by avoiding adaptive iterations, we can achieve considerable time-saving and substantial process simplification, meanwhile accomplish the purpose of lower estimator MSE. In fact, one can also go to great lengths to apply the ML algorithm to perform the usual joint frequency tracking and channel estimation but using multiple training <b>block</b> <b>samples.</b> This will bring the CRBs down a little further than our estimator MSEs. But, such processes would be overly complex and much time-consuming, and hence prohibitively expensive to implement.|$|R
40|$|In {{the line}} of {{research}} extending statistical parsing to more expressive grammar formalisms, we demonstrate {{for the first time}} the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel <b>block</b> <b>sampling</b> methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines. ...|$|E
40|$|Uncertainty {{quantification}} is {{very much}} needed to support decision making related to e. g. environmental impact assessment for waste disposal sites. A probabilistic result provides a much stronger basis for decision making compared to a single deterministic outcome. Accurate posterior exploration of high-dimensional and CPU-intensive models, which are often used for environmental impact assessment, is however a challenging task. To quantify the uncertainty associated with solute transport {{in the framework of}} a near surface radioactive waste disposal in Mol/Dessel, Belgium, we investigate combining the adaptive Metropolis (AM) McMC algorithm for updating the global model parameters, and adaptive spatial resampling (ASR) for updating of the spatially distributed model parameters, by <b>block</b> <b>sampling.</b> The forward model used is a groundwater flow model conditioned on borehole and direct push data, that accounts for non-stationary heterogeneity in hydraulic conductivity. The obtained flow solutions are used for solute transport simulations, and the results are compared with a different groundwater flow model parameterization, that makes use of homogeneous hydrogeological layers. Moreover, a number of simulations is performed to assess the effect of realistic dispersivity, which is derived from outcrop investigations. The obtained results indicate that the combination of AM and ASR using <b>block</b> <b>sampling</b> seems not to be very efficient for McMC sampling with the forward model used in this study. However, using the algorithm in optimization mode seems to work fine, and provides an alternate way for exploring the parameter space and the prediction uncertainty...|$|E
40|$|International audienceTo {{provide a}} {{complete}} local {{monitoring of the}} state of an unsaturated soil sample during triaxial testing, a local water content measurement device was adapted to a triaxial device comprising the measurement of local displacements (Hall effect transducers) and suction (High capacity transducer). Water content was locally monitored by means of a resistivity probe. The water content/resistivity calibration curves of an intact natural unsaturated loess from Northern France extracted by <b>block</b> <b>sampling</b> at two depths (1 and 3. 3 m) were carefully determined, showing good accuracy and repeatability. The validity of two models giving the resistivity of unsaturated soils with respect to their water content was examined...|$|E
40|$|A {{soil sample}} goes through stress changes {{during and after}} sampling. Sensitive clays are {{affected}} by sample disturbance and stress changes have a great effect on the quality. The reduction of in-situ total stresses to zero causes the soil sample to develop a negative pore pressure, which is {{also referred to as}} residual effective stresses. In an ideal situation, a <b>block</b> <b>sample</b> shall retain its residual effective stress during sampling and storage, which prevents it from swelling. To study this, an attempt was made to monitor the pore pressure variations inside a <b>block</b> <b>sample</b> of soft, sensitive, low-plasticity clay during and after sampling. The pore pressure was measured continuously during the storage period of 3 days and the results were compared with a similar work. The findings suggest that the residual effective stress in <b>block</b> <b>samples</b> may be reduced {{in a matter of minutes}} after sampling. Testing performed on reference samples corroborate these storage effects...|$|R
40|$|CAT (Computed Axial Tomography) {{has been}} {{sometimes}} {{used to obtain}} density maps of small core specimens of soft soil. Here it is applied {{to do the same}} with a Sherbrooke-type <b>block</b> <b>sample</b> recovered at depth from a silt layer. Sherbrooke <b>block</b> <b>samples</b> are considered as the one more closely related to the material 'in situ', because the sampling method excludes practically any other alteration process apart from deviatoric stress relief. Sherbrooke samples are large typically 25 cm in diameter and 35 in height. Because of this large specimen size the standard medical scanner used for the CAT test presented several artifacts (noise, rings, outliers) that made difficult the quantitative interpretation of the resulting images. The procedures employed to remove those artifacts from the images and obtain a map of density of the <b>block</b> <b>sample</b> are described here. Validation with independent laboratory measurements of density is shown to result in a good agreement...|$|R
3000|$|Collect N epochs in an ensemble. Let the epochs be {{processed}} as M <b>sample</b> <b>blocks.</b> The <b>sampling</b> frequency, f [...]...|$|R
40|$|Abstract: The {{widespread}} use of particle methods for addressing the filtering and smoothing problems in state-space models has, in recent years, been complemented {{by the development of}} particle Markov Chain Monte Carlo (PMCMC) methods. PMCMC uses particle filters within offline systems-identification settings. We develop a modified particle filter, based around <b>block</b> <b>sampling</b> and tempering, intended to improve their exploration of the state space and the associated estimation of the marginal likelihood. The aim is to develop particle methods with improved robustness properties, particularly for parameter values which are not able to explain observed data well, for use within PMCMC algorithms. The proposed strategies do not require a substantial analytic understanding of the model structure, unlike most techniques for improving particle-filter performance...|$|E
40|$|In this paper, {{we propose}} new nonparametric {{approach}} to network inference {{that may be}} viewed as a fusion of <b>block</b> <b>sampling</b> procedures for temporally and spatially dependent processes with the classical network methodology. We develop estimation and uncertainty quantification procedures for network mean degree using a "patchwork" sample and nonparametric bootstrap, under the assumption of unknown degree distribution. We investigate asymptotic properties of the proposed patchwork bootstrap procedure and present cross-validation methodology for selecting an optimal patch size. We validate the new patchwork bootstrap on simulated networks with short and long tailed mean degree distributions, and revisit the Erdos collaboration data to illustrate the proposed methodology. Comment: The paper has been withdrawn by the authors: a general revision of methodology is neede...|$|E
40|$|The {{widespread}} use of particle methods for addressing the filtering and smoothing problems in state-space models has, in recent years, been complemented {{by the development of}} particle Markov Chain Monte Carlo (PMCMC) methods. PMCMC uses particle filters within offline systems-identification settings. We develop a modified particle filter, based around <b>block</b> <b>sampling</b> and tempering, intended to improve their exploration of the state space and the associated estimation of the marginal likelihood. The aim is to develop particle methods with improved robustness properties, particularly for parameter values which are not able to explain observed data well, for use within PMCMC algorithms. The proposed strategies do not require a substantial analytic understanding of the model structure, unlike most techniques for improving particle-filter performance...|$|E
40|$|A {{possible}} {{penetration of}} contamination {{from the surface}} toward the inner part of cored ice and firn <b>block</b> <b>samples</b> was investigated for proper chemical analysis of Cl, SO_ 4,NO_ 3,NH_ 4,Na, K, Mg, Al, Fe, Ni, Cu, Zn and other elements. Generally, the contamination of the ice core sample remains within 10 mm under the surface. However, for the elements with intense contamination, a high concentration level in the surface layer tends to affect the level in the inner part. For the firn <b>block</b> <b>sample,</b> though the contamination by Cl, SO_ 4,and NO_ 3 remains within 20 mm of depth from the surface, NH_ 4,Na, K, Cu and Zn seem {{to pass through the}} surface layer but remain within 40 mm. A thermal knife was constructed, for use in the laboratory, of nichrome wire of 1 mm diameter covered with a platinum pipe, tested for cutting cored ice samples and was found satisfactory without significant contamination of the elements listed above. The use of a stainless steel hand saw was also found practical for firn <b>block</b> <b>samples...</b>|$|R
30|$|We sampled {{the entire}} 7  cm 3 cube (each side[*]=[*] 2.2  cm) from the <b>block</b> <b>samples</b> and {{measured}} their {{wet and dry}} bulk densities. The color of wet sediments in cubic samples is quantified by the L*, a*, and b* parameters measured by the Soil Color Reader SPAD- 503 instrument (Konica Minolta Sensing, Inc.). The a* and b* parameters specify the red (+) to green (−) and yellow (+) to blue (−) content, respectively, while L* represents lightness (0 [*]=[*]black, 100 [*]=[*]white). The loss on ignition (LOI) was conducted in each <b>block</b> <b>sample</b> following Bos et al. (2012) at 3 – 6  cm intervals, although this sampling was restricted to peat and peaty silt.|$|R
30|$|SQUID rock {{magnetometer}} (SRM- 760; 2 G Enterprises) {{was used}} to measure remanent magnetization of ice <b>block</b> <b>samples.</b> First, a <b>block</b> ice <b>sample</b> from each of the three sites was prepared and natural remanent magnetization (NRM) was measured by stepwise demagnetization with AC magnetic field of 0, 5, 10, 15, 20, 30, 40  mT. Second, anhysteretic remanent magnetization (ARM) was imparted in DC magnetic field of 50  mT and AC magnetic field of 80  mT, then the samples were stepwise demagnetized with AC magnetic field of 0, 5, 10, 15, 20, 30, 40  mT.|$|R
40|$|A {{sampling}} system {{was developed for}} monitoring population levels of woolly apple aphid, Eriosoma lanigerum (Hausmann), by counting colonies on half of each of 25 apple trees per 2 ha <b>block.</b> <b>Sampling</b> error was affected by whether the colonies were found in wounds or in leaf axils. Parasitism of colonies in leaf axils had a slight effect on sampling error. Sampling error for colonies in leaf axils was high at just over 40 %. However, decisions regarding intervention were not markedly compromised by simply classifying the 25 trees as infested or uninfested, as opposed to counting colonies in leaf axils. The presence-absence system greatly reduced the time spent monitoring E. lanigerum population levels, making it an attractive system for assessing woolly apple aphid infestations in commercial orchards. Articl...|$|E
40|$|We {{describe}} {{a framework for}} defining high-order image models {{that can be used}} in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for <b>block</b> <b>sampling</b> with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation. Comment: In NIPS 201...|$|E
40|$|An {{efficient}} simulation-based methodology {{is proposed}} for the rolling window estimation of state space models. Using {{the framework of the}} conditional sequential Monte Carlo update in the particle Markov chain Monte Carlo estimation, weighted particles are updated to learn and forget the information of new and old observations by the forward and backward <b>block</b> <b>sampling</b> with the particle simulation smoother. These particles are also propagated by the MCMC update step. Theoretical justifications are provided for the proposed estimation methodology. As a special case, we obtain a new sequential MCMC based on Particle Gibbs. It is a flexible method alternative to SMC$^ 2 $ that is based on Particle MH. The computational performance is evaluated in illustrative examples, showing that the posterior distributions of model parameters and marginal likelihoods are estimated with accuracy...|$|E
40|$|Given a white Gaussian {{noise signal}} on a {{sampling}} grid, its variance {{can be estimated}} from a small <b>block</b> <b>sample.</b> However, in natural images we observe {{the combination of the}} geometry of the scene being photographed and the added noise. In this case, estimating directly the standard deviation of the noise from <b>block</b> <b>samples</b> is not reliable since the measured standard deviation is not explained just by the noise but also by the geometry of the image. The Percentile method tries to estimate the standard deviation of the noise from blocks of a high-passed version of the image and a small p-percentile of these standard deviations. The idea behind is that edges and textures in a block of the image increase the observed standard deviation but they never make it decrease. Therefore, a small percentile (0. 5...|$|R
50|$|A <b>blocked</b> Gibbs <b>sampler</b> groups {{two or more}} {{variables}} {{together and}} samples from their joint distribution conditioned on all other variables, rather than sampling from each one individually. For example, in a hidden Markov model, a <b>blocked</b> Gibbs <b>sampler</b> might sample from all the latent variables making up the Markov chain in one go, using the forward-backward algorithm.|$|R
40|$|Laboratory {{compression}} {{tests have}} been conducted on small <b>block</b> <b>samples</b> of Topopah Spring Tuff to investigate the scale dependence of geomechanical properties. These tests are {{the first of a}} series of tests designed to study coupled processes in the near-field environment of a nuclear waste repository. This paper presents new deformation and elastic wave velocity data...|$|R
