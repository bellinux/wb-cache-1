31|43|Public
50|$|The figure below {{illustrates}} {{the relationship between}} the blank, the limit of detection (LOD), and the limit of quantification (LOQ) by showing the probability density function for normally distributed measurements at the blank, at the LOD defined as 3 * standard deviation of the blank, and at the LOQ defined as 10 * standard deviation of the blank. For a signal at the LOD, the alpha error (probability of false positive) is small (1%). However, the <b>beta</b> <b>error</b> (probability of a false negative) is 50% for a sample that has a concentration at the LOD (red line). This means a sample could contain an impurity at the LOD, but there is a 50% chance that a measurement would give a result less than the LOD. At the LOQ (blue line), there is minimal chance of a false negative.|$|E
40|$|Five adults {{completed}} this four-way randomized crossover study {{to compare the}} effects of oral treatment with ciprofloxacin, clarithromycin, and {{a combination of the}} two drugs on theophylline pharmacokinetics. The area under the concentration-time curve for theophylline during combination therapy was not different from that for ciprofloxacin alone. <b>Beta</b> <b>error</b> may explain this finding, but any real effect from combination treatment appears to be clinically unimportant...|$|E
40|$|Antonic et al. {{reported}} {{the findings of}} a randomized trial on vitamin C and atrial fibrillation (AF). They wrote that “the sample size estimation was {{based on the assumption that}} the incidence of AF after CABG lies at about 25 % and that the administration of ascorbic acid would result in a 20 % decrease of AF. With a power of 80 %, <b>beta</b> <b>error</b> of 0. 2 and alpha of 0. 05, about 50 patients were required in each group”. This sample size calculation does not seem to be correct...|$|E
40|$|Keywords: Covert {{channels}} in IP networks are investigated. The possibilities adversary needs to construct covert channels are given. Current methods of covert channels elimination, detection and capacity limitation are examined. Detection methods are compared using such criteria: alpha and <b>beta</b> <b>errors,</b> an ability of implementation...|$|R
3000|$|... {{which was}} {{examined}} in this retrospective study. The groups were comparable regarding their skeletal pattern, age, sex, and treatment time. The review of the confidence interval show, that {{a sufficient number of}} patients were evaluated. The significant differences are thereby supported by alpha and <b>beta</b> <b>errors.</b>|$|R
40|$|Covert timing {{channels}} became widespread {{with the}} increasing popularity of packet switching networks. Detection of these channels is the only approach to counter that {{does not lead to}} a decrease of channel’s capacity. Known methods of the IP covert timing channels detection were systematized. Detection methods based on the analysis of patterns in the distribution of inter-packet delays according to criteria such as alpha and <b>beta</b> <b>errors,</b> an ability of implementation were compared...|$|R
40|$|In {{most areas}} in life, it is {{difficult}} to work with populations and hence researchers work with samples. The calculation of the sample size needed depends on the data type and distribution. Elements include consideration of the alpha error, <b>beta</b> <b>error,</b> clinically meaningful difference, and the variability or standard deviation. The final number arrived at should be increased to include a safety margin and the dropout rate. Over and above this, sample size calculations must take into account all available data, funding, support facilities, and ethics of subjecting patients to research...|$|E
40|$|Reports of {{negative}} trials arc increasing in number as standard therapy for many gastrointestinal diseases is refined. The validity {{of a negative}} report depends {{on the number of}} patients in the trial, the alpha and bern error and the difference in efficacy which the trial is able to detect. The relationship between these parameters is discussed and a formula given for the calculation of trial size. All reports {{of negative}} trials should include not only the number of patients involved and the level of significance of the results but also the <b>beta</b> <b>error</b> and the detectable difference in efficacy of the treatments...|$|E
40|$|Thromboprophylaxis is a {{controversial}} and changing topic. Some have questioned its very need. Various objections {{have been raised}} as to its use including the relatively low incidence of symptomatic thromboembolic events, the risk of bleeding, the possibility of late infection, the reliance on surrogate end-points such as venography and finally ‘no evidence of effect’, when the <b>beta</b> <b>error</b> has been misinterpreted as ‘evidence of no effect’. However, in our cur-rent environment of risk management, {{it would be wise}} to remember that the weight of evidence supports the view that thrombo-embolism is a potentially serious complication and that on the balance of probability the ris...|$|E
40|$|Market risk {{management}} {{is one of}} the key factors to success in managing financial institutions. Underestimated risk can have desastrous consequences for individual companies and even whole economies, not least as could be seen during the recent crises. Overestimated risk, on the other side, may have negative effects on a company's capital requirements. Companies as well as national authorities thus have a strong interest in developing market risk models that correctly quantify certain key figures such as Value at Risk or Expected Shortfall. This paper presents several state of the art methods to evaluate the adequacy of almost any given market risk model. Existing models are enhanced by in-depth analysis and simulations of statistical properties revealing some previously unknown effects, most notably inconsistent behaviour of alpha and <b>beta</b> <b>errors.</b> Furthermore, some new market risk validation models are introduced. In the end, a simulation with various market patterns demonstrates strenghts and weaknesses of each of the models presented under realistic conditions...|$|R
40|$|Count Plus {{with the}} N 95 -Companion) were {{evaluated}} {{for their ability}} to identify wearers of respirators that do not provide adequate protection during a simulated workplace test. Thirty models of NIOSH-certified N 95 half-facepiece respirators (15 filtering-facepiece models and 15 elastomeric models) were tested by a panel of 25 subjects using each of the three fit testing methods. Fit testing results were compared to 5 th percentiles of simulated workplace protection factors. Alpha errors (the chance of failing a fit test in error) for all 30 respirators were 71 % for the Bitrex method, 68 % for the saccharin method, and 40 % for the Companion method. <b>Beta</b> <b>errors</b> (the chance of passing a fit test in error) for all 30 respirator models combined were 8 % for the Bitrex method, 8 % for the saccharin method, and 9 % for the Companion method. The three fit test methods had different error rate...|$|R
40|$|OBJECTIVE: To {{determine}} whether poor reporting of methods in randomised controlled trials reflects on poor methods. DESIGN: Observational study. SETTING: Reports of randomised controlled trials {{conducted by the}} Radiation Therapy Oncology Group since its establishment in 1968. PARTICIPANTS: The Radiation Therapy Oncology Group. Outcome measures Content of reports compared with the design features described in the protocols for all randomised controlled trials. RESULTS: The methodological quality of 56 randomised controlled trials was better than reported. Adequate allocation concealment was achieved in all trials but reported in only 42 % of papers. An intention to treat analysis was done in 83 % of trials but reported in only 69 % of papers. The sample size calculation was performed in 76 % of the studies, but reported in only 16 % of papers. End points were clearly defined and alpha and <b>beta</b> <b>errors</b> were prespecified in 76 % and 74 % of the trials, respectively, but only reported in 10 % of the papers. The one exception was the description of drop outs, where the frequency of reporting {{was similar to that}} contained in the original statistical files of the Radiation Therapy Oncology Group. CONCLUSIONS: The reporting of methodological aspects of randomised controlled trials does not necessarily reflect the conduct of the trial. Reviewing research protocols and contacting trialists for more information may improve quality assessment...|$|R
40|$|Clinical trials (CT) are {{the basis}} for {{generating}} evidence-basedrecommendations on therapy. Therefore, it is desirable that thelocal academic community increases its awareness and skills oncritical appraisal of CTs to judge their validity, weight their effectestimates and decide how applicable to practice a CT may become. This paper (first of two parts) outlines the more important concepts and characterisitics of CTs, while introducing some terms that areoften used in their reports. The following issues are discussed: a) The origins and consequences of alpha and <b>beta</b> <b>error</b> in CTs; b) The principle of randomization and how to assess the balance ofstudy groups at the baseline. c) Different ways by whichcomparisons between study groups {{can be set up}} as basis formaking inferences...|$|E
40|$|PURPOSE: To {{evaluate}} {{the efficacy of}} saline versus 10 units/ml heparin for peripheral i. v. flushes in neonates. DESIGN: A nonexperimental group design {{was used to compare}} the longevity of heparin and saline i. v. locks. A research utilization method was chosen to increase the study power while simultaneously implementing a practice change and evaluating the outcomes. Power analysis showed that a sample size of approximately 120 per group was needed to decrease the risk of <b>beta</b> <b>error</b> to 0. 1. SAMPLE: Subjects included neonates in the Special Care Nurseries at a Level III large midwestern university teaching hospital. Data were collected from a convenience sample of 123 neonates receiving 10 units/ml heparin flush into a peripheral i. v. Practice was then changed to preservative-free normal saline, and data collection continued for 117 neonates. MAIN OUTCOME VARIABLE: I. v. catheter longevity. RESULTS: There was no significant statistical difference in i. v. catheter longevity between i. v. locks flushed with 10 units/ml heparin and those flushed with normal saline. Patient weight accounted for a significant proportion of the variance in i. v. catheter life...|$|E
40|$|The AUP 24 {{audit risk}} model defines audit risk implicitly as the joint {{probability}} of three independent events: (i) a material error occurring in an account balance, (ii) that error not being corrected by internal control procedures, and (iii) the uncorrected balance being {{accepted by the}} auditor. A more apposite risk measure, relating to these same possible events, is the conditional probability of a material-error given that the stated balance has been subject to internal control procedures and accepted by the auditor. The two risk measures so defined are related {{by the laws of}} probability, through Bayes 2 ̆ 7 theorem specifically, but are not the same and exhibit no necessary correlation. Calculation of the conditional (2 ̆ 7 Bayesian 2 ̆ 7) risk measure requires consideration of both the type I (alpha) and type II (<b>beta)</b> <b>error</b> probabilities of the auditor 2 ̆ 7 s substantive test procedure. Unless both error characteristics are taken into account, {{it is not possible to}} interpret a test result (acceptance or rejection) in terms of the probability of the stated account balance being materially correct...|$|E
40|$|The Simon {{two stage}} design {{is a single}} arm study with an interim analysis. The main purpose of this design is to {{investigate}} whether an intervention works or not and to stop the study early for futility. Under the null hypothesis {{the probability of a}} success is p 0, this is usually taken as the probability of success for the current standard treatment. The probability of a success in this study, p, is tested using the null hypothesis H 0 :p=p 0 versus the alternative hypothesis H 1 :p>=p 1. The probability of success for the alternative hypothesis is fixed to be a pre-specified value p 1, where p 1 >p 0. The Simon two stage design consists of studying n 1 participants in a first stage and the study stops if there are r 1 or fewer responders to the intervention. If there are more than r 1 responders in the first stage then the study continues until n participants in total are studied. Then the null hypothesis is not rejected if there are r or fewer responders. Each design must satisfy the type 1 (alpha) and type 2 (<b>beta)</b> <b>errors.</b> The probability of not rejecting H 0 can be calculated conditional on any p and let this function be R(p). The design must therefore satisfy the constraints R(p 0) >= 1 -alpha and R(p 1) Simon's two stage design; optimal design; minimax; single-arm trial...|$|R
3000|$|The {{data were}} {{analyzed}} using the PASW (Predictive Analysis Software, version 19.0. Armonk, NY: IBM Corp.). All continuous variables were reported as mean (SD) whereas; the categorical variables were described using counts and proportions (%). Simple linear regression analysis was used to examine the possible association between HRQoL scores (i.e., PCS and MCS scores) and selected socio-demographic and clinical variables. Only the statistically significant variables in the univariate analysis were entered into a multiple linear regression analysis to predict the final determinants of HRQoL. Beta, 95  % confidence interval (CI) for <b>beta,</b> standard <b>error</b> and p value were reported for each variable. Similarly, logistic regression analysis was used to determine the independent factors associated with clinical depression (major depression and severe major depression). The variables which were statistically significant (i.e., p-value < 0.05) in the univariate analysis were entered into a multiple logistic regression analysis to predict the final independent factors. The adjusted odd ratios (AOR), 95  % confidence interval (CI), <b>beta,</b> standard <b>error</b> and p-value were reported for each predictor. The model fit was assessed by Chi square, degrees of freedom and p-value. Pseudo R square values were reported to provide information about the percentage of variance explained by the regression models. A p-value of < 0.05 was considered statistically significant. The internal consistency of SF- 36 v 2 and PHQ- 8 were assessed using Cronbach’s alpha coefficient (Santos 1999; Pallant 2013). An alpha value equal to or greater than [...]. 70 was considered acceptable (Alhabahba et al. 2006).|$|R
40|$|The {{statistical}} {{education of}} scientists emphasizes a flawed approach to data analysis {{that should have}} been discarded long ago. This defective method is statistical significance testing. It degrades quantitative findings into a qualitative decision about the data. Its underlying statistic, the P-value, conflates two important but distinct aspects of the data, effect size and precision [1]. It has produced countless misinterpretations of data that are often amusing for their folly, but also hair-raising in view of the serious consequences. Significance testing maintains its hold through brilliant marketing tactics—the appeal of having a ‘‘significant’’ result is nearly irresistible—and through a herd mentality. Novices quickly learn that significant findings are the key to publication and promotion, and that statistical significance is the mantra of many senior scientists who will judge their efforts. Stang et al. [2], in this issue of the journal, liken the grip of statistical significance testing on the biomedical sciences to tyranny, as did Loftus in the social sciences two decades ago [3]. The tyranny depends on collaborators to maintain its stranglehold. Some collude because they do not know better. Others do so because they lack the backbone to swim against the tide. Students of significance testing are warned about two types of errors, type I and II, also known as alpha and <b>beta</b> <b>errors.</b> A type I error is a false positive, rejecting a null hypothesis that is correct. A type II error is a false negative, a failure to reject a null hypothesis that is false. A large literature, much of it devoted to the topic of multiple comparisons, subgroup analysis, pre-specification of hypotheses, and related topics, are aimed at reducing type I errors [4]. This lopsided emphasis on type I errors comes a...|$|R
40|$|Section 303 (d) of the Clean Water Act {{requires}} {{states to}} establish a list of water bodies that do not meet water quality standards. State Water Resources Control Board staff recently proposed the binomial hypothesis test when deciding to list or delist a water body. The traditional binomial test effectively controls the alpha error rate (i. e., the chance of incorrectly rejecting a true null hypothesis) at or below the proposed nominal significance level of 10 %. Several authors, however, {{have suggested that the}} <b>beta</b> <b>error</b> rate (i. e., the chance of incorrectly failing to reject a false null hypothesis) should be considered when listing or delisting and that alpha and beta rates should be equally balanced, if possible. The methodology and probability equations used to derive sampling plans based on observed exceedances is reviewed, both for the proposed traditional binomial test and a binomial test based on a balanced error approach. Approximate error balancing provides an equitable way to decide whether a water body should be listed or delisted, as long as a sufficient number of samples are collected to keep the error rates below a moderate level...|$|E
40|$|AbstractObjectiveTo verify if the {{connection}} of electrodes for heart and transcutaneous oxygen monitoring interfere with the measurement of electrical bioimpedance in preterm newborns. MethodsThis was a prospective, blinded, controlled, cross-sectional, crossover study that assessed and compared paired measures of resistance (R) and reactance (Xc) by BIA, obtained with and without monitoring wires attached to the preterm newborn. The measurements were performed in immediate sequence, after randomization to {{the presence or absence}} of electrodes. The sample size calculated was 114 measurements or tests with monitoring wires and 114 without monitoring wires, considering for a difference between the averages of 0. 1 ohms, with an alpha error of 10 % and <b>beta</b> <b>error</b> of 20 %, with significance < 0. 05. ResultsNo differences were observed between the R (677. 37 ± 196. 07 vs. 677. 46 ± 194. 86) and Xc (31. 15 ± 9. 36 vs. 31. 01 ± 9. 56) values obtained with and without monitoring wires, respectively, with good correlation between them (R: 0. 997 and Xc: 0. 968). ConclusionThe presence of heart and/or transcutaneous oxygen monitoring wires connected to the preterm newborn did not affect the values of R or Xc measured by BIA, allowing them to be carried out in this population without risks...|$|E
40|$|Abstract Objective: To verify if the {{connection}} of electrodes for heart and transcutaneous oxygen monitoring interfere with the measurement of electrical bioimpedance in preterm newborns. Methods: This was a prospective, blinded, controlled, cross-sectional, crossover study that assessed and compared paired measures of resistance (R) and reactance (Xc) by BIA, obtained with and without monitoring wires attached to the preterm newborn. The measurements were performed in immediate sequence, after randomization to {{the presence or absence}} of electrodes. The sample size calculated was 114 measurements or tests with monitoring wires and 114 without monitoring wires, considering for a difference between the averages of 0. 1 ohms, with an alpha error of 10 % and <b>beta</b> <b>error</b> of 20 %, with significance < 0. 05. Results: No differences were observed between the R (677. 37 ± 196. 07 vs. 677. 46 ± 194. 86) and Xc (31. 15 ± 9. 36 vs. 31. 01 ± 9. 56) values obtained with and without monitoring wires, respectively, with good correlation between them (R: 0. 997 and Xc: 0. 968). Conclusion: The presence of heart and/or transcutaneous oxygen monitoring wires connected to the preterm newborn did not affect the values of R or Xc measured by BIA, allowing them to be carried out in this population without risks...|$|E
40|$|A {{covariance}} {{analysis was}} performed for a solar probe trajectory which encounters the sun at four solar radii. The unknown parameters in the analysis are the six initial cartesian coordinates for the probe, six initial cartesian coordinates for the earth, the astronomical unit, the solar gravitational quadrupole coefficient and two post Newtonian meters (<b>beta,</b> gamma). <b>Errors</b> in the unknown parameters were computed {{as a function of}} standard errors on the radio tracking data and on the nongravitational forces which act on the probe. Results were obtained for several tracking geometries and for several orbital inclinations to the ecliptic. The analysis shows that the principal scientific result from the radio tracking of a solar probe would be the determination of the quadrupole moment, which would place a constraint on models of the solar interior...|$|R
40|$|This paper {{develops}} {{a test of}} the asymptotic arbitrage pricing theory (APT) via the maximum squared Sharpe ratio of the factors extracted from individual stocks using the Connor-Korajczyk method. The test treats the beta pricing relation as approximate without predetermining the systematic factors, unlike the existing tests that take the relationship as exact and systematic factors as given. This paper also examines the magnitude of pricing errors bounded partly by the maximum squared Sharpe ratio. For most 60 -month subperiods of the sample, the hypothesis that the maximum squared Sharpe ratio for monthly returns is greater than 0. 25 can be rejected. Simulation indicates that the average pricing error in monthly returns is less than 0. 001. These results support the asymptotic APT. asymptotic APT, <b>beta</b> pricing <b>errors,</b> Sharpe ratio, Connor-Korajczyk method, eigenvalue, eigenvector...|$|R
40|$|This paper {{investigates the}} {{implications}} of time-varying betas in factor models for stock returns. It is shown that a single-factor model (SFMT) with autoregressive <b>betas</b> and homoscedastic <b>errors</b> (SFMT-AR) is capable of reproducing the most important stylized facts of stock returns. An empirical study on the major US stock market sectors shows that SFMT-AR outperforms, in terms of in-sample and out-of-sample performance, SFMT with constant betas and conditionally heteroscedastic (GARCH) errors, {{as well as two}} multivariate GARCH-type models...|$|R
40|$|Cefprozil (BMY- 28100) is a semisynthetic {{cephalosporin}} with broad-spectrum {{antibacterial activity}} and prolonged serum elimination half-life allowing for once-a-day oral administration. In vitro, cefprozil demonstrates excellent activity against Staphylococcus aureus, Streptococcus pyogenes, Haemophilus influenzae, and Moraxella catarrhalis. Cefprozil (500 mg once daily) {{was compared to}} cefaclor (250 mg three times daily) in an open, randomized, comparative trial {{for the treatment of}} acute group A beta-hemolytic streptococcal pharyngitis. Ninety-four patients were enrolled in this study; 53 patients were evaluable for clinical and bacteriological response assessment. Seventy-eight patients were evaluable for safety assessment. Three patients (all in the cefprozil treatment group) required disenrollment because of side effects, mainly nausea. Clinical and bacteriological responses were comparable for both study drugs. Leukopenia and nausea, the most common side effects observed, were more common in the cefprozil-treated group. Cefprozil appears to be an appropriate alternative to cefaclor for the treatment of acute group A beta-hemolytic streptococcal pharyngitis. However, because of the small number of patients eligible for efficacy assessment, a large type II (<b>beta)</b> <b>error</b> was expected in our study, which may have resulted in a potential failure to detect a difference between both treatment groups. A larger study would be required to determine the proper role of cefprozil in the treatment of group A beta-hemolytic streptococcal infections...|$|E
30|$|During {{the study}} design, authors {{assumed that the}} MM-C group would show {{inflammation}} and fibrosis scores of 3 and adhesion score of 4 in at least 10  % of rats. In comparison, sham group inflammation and fibrosis scores would be 3 and adhesion score would 4 in at least 60  % of rats. Thus, the sample size for attaining an alfa error of 0.05 and a <b>beta</b> <b>error</b> of 0.20 would require at least 10 rats per group. Accordingly, forty Wistar albino female rats were randomly and evenly assigned into four study groups. In order to elucidate the potential toxic effects of MM-C and decrease possible confounding factors, Group 1 (control) was created and we analyzed the basal values CBC counts and bone marrow morphologies. Except for Group 1, all other study groups were given a laparotomy with a 3  cm midline incision. Afterwards, the cecum was exteriorized with approximately 1  cm 2 of its antero-medial serosal layer denuded by brushing ten times with a sterile toothbrush. Group 2 (sham) was administered 5  ml of saline solution intraperitoneally. Group 3 was administered 1  mg/kg of MM-C in 5  ml of saline solution, intraperitoneally. In Group 4, a 1  ×  1  cm NH/CMC sheet was directly applied on the abrasion area. The cecum was {{then returned to the}} abdominal cavity and the abdomen was closed with continuous 4 / 0 silk sutures.|$|E
40|$|We {{report on}} an optical sensor system {{attached}} to a 4 kW fiber laser cutting machine to detect cutting interruptions. The sensor records the thermal radiation from the process zone with a modified ring mirror and optical filter arrangement, which is placed between the cutting head and the collimator. The process radiation is sensed by a Si and InGaAs diode combination with the detected signals being digitalized with 20 kHz. To demonstrate {{the function of the}} sensor, signals arising during fusion cutting of 1 mm stainless steel and mild steel with and without cutting interruptions are evaluated and typical signatures derived. In the recorded signals the piercing process, the laser switch on and switch off point and waiting period are clearly resolved. To identify the cutting interruption, the signals of both Si and InGaAs diodes are high pass filtered and the signal fluctuation ranges being subsequently calculated. Introducing a correction factor, we identify that only in case of a cutting interruption the fluctuation range of the Si diode exceeds the InGaAs diode. This characteristic signature was successfully used to detect 80 cutting interruptions of 83 incomplete cuts (alpha error 3. 6 %) and system recorded no cutting interruption from 110 faultless cuts (<b>beta</b> <b>error</b> of 0). This particularly high detection rate in combination with the easy integration of the sensor, highlight its potential for cutting interruption detection in industrial applications...|$|E
40|$|The LHC {{collimation}} system {{requires a}} high cleaning ef-ficiency {{in order to}} prevent magnet quenches due to regular beam diffusion. The cleaning efficiency is significantly re-duced due to imperfections of the collimator jaws and the machine optics. Tracking tools have been set up to predict the cleaning efficiency in presence of multiple imperfec-tions. The deterioration of cleaning efficiency is quanti-fied for different errors, including collimator surface non-flatness, collimator alignment <b>errors,</b> <b>beta</b> beating, orbit er-rors, non-linear field errors, and chromatic effects. ...|$|R
3000|$|... - <b>beta,</b> anderf- <b>error</b> function. The most {{significant}} value {{is in its}} generalization from discrete to continuous. In addition, we can move from the scope of natural integers to the set of real and noninteger values. Therefore, there exist conditions both for its graphical interpretation and a more concise analysis. For {{the development of the}} hypersphere function theory see Bishop and Whitlock [1], Collins [2], Conway and Sloane [3], Dodd and Coll [4], Hinton [5], Hocking and Young [6], Manning [7], Maunder [8], Neville [9], Rohrmann and Santos [10], Rucker [11], Maeda et al. [12], Sloane [13], Sommerville [14], Wells et al. [15] Nowadays, the research of hyperspherical functions is given both in Euclid's and Riemann's geometry and topology (Riemann's and Poincare's sphere) multidimensional potentials, theory of fluids, nuclear physics, hyperspherical black holes, and so forth.|$|R
40|$|This {{study was}} {{conducted}} at the Bebedouro Experimental Station in Petrolina-PE, Brazil, to evaluate the errors associated to {{the application of the}} Bowen ratio-energy balance in a 3 -years old vineyard (Vitis vinifera, L), grown in a trellis system, irrigated by dripping. The field measurements were taken during fruiting cycle (July to November, 2001), which was divided into eigth phenological stages. A micrometeorological tower was mounted in a grape-plants row in which sensors of net radiation, global solar radiation and wind speed were installed at about 1. 0 m above the canopy. Also in the tower, two psicometers were installed at two levels (0. 5 and 1. 8 m) above the vineyard canopy. Two soil heat flux plates were buried at 0. 02 m beneath the soil surface. All these sensors were connected to a Data logger 21 X of Campbell Scientific Inc., programmed for collecting data once every 5 seconds and storage averages for every 15 minutes. A comparative analysis were made among four Bowen ratio accepting/rejecting rules, according to the methodology proposed by Spano et al. (2000) : betar 1 - values of beta calculated by Bowen (1926) equation; betar 2 - values of beta as proposed by Verma et al. (1978) equation; betar 3 - exclusion of the beta values obtained as recommended by Unland et al. (1996) and betar 4 - exclusion of the beta values calculated as proposed by Bowen (1926), out of the interval (- 0. 7 < beta < 0. 7). Constacted that the Unland et al. (1996) and Soares (2003) accepting/rejection rules were better than that of Verma et al. (1978) for attenuating the advective effects on the calculations of the Bowen ratio. The comparison of betar 1 with betar 2 rules showed that the statistical errors reaching maximum values of 0. 015. When comparing betar 1 with betar 3 e betar 4, the <b>beta</b> <b>errors</b> reaching maximum values of 5. 80 and 3. 15, respectively...|$|R
40|$|Objectives: To {{evaluate}} whether three daily doses of GnRH agonist (Inj. Lupride 1 mg SC) administered 6 days after oocyte retrieval increases ongoing pregnancy rates following embryo transfer (ET) in cycles stimulated {{with the long}} GnRH agonist protocol. Settings And Design: Prospective randomized controlled study in a tertiary care center. Materials and Methods: Four hundred and twenty six women undergoing ET following controlled ovarian stimulation with a long GnRH agonist protocol were included. In addition to routine luteal-phase support (LPS) with progesterone, women were randomized to receive three 1 mg doses of Lupride 6 days after oocyte retrieval. Computer-generated randomization was done {{on the day of}} ET. Ongoing pregnancy rate beyond 20 th week of gestation was the primary outcome measure. The trial was powered to detect a 13 % absolute increase from an assumed 27 % ongoing pregnancy rate in the control group, with an alpha error level of 0. 05 and a <b>beta</b> <b>error</b> level of 0. 2. Results: There were 59 (27. 69 %) ongoing pregnancies in the GnRHa group, and 56 (26. 29 %) in the control group (P = 0. 827). Implantation, clinical pregnancy and multiple pregnancy rates were likewise similar in the GnRHa and placebo groups. Conclusions: Three 1 mg doses of Lupride administration 6 days after oocyte retrieval in the long protocol cycles does not result in an increase in ongoing pregnancy rates...|$|E
40|$|The {{transfusion}} provokes {{anxiety and}} this one compromises {{the improvement of the}} patient. Objetive: The study aims to evaluate whether a nursing intervention protocol-through oral and written submissions previous to the transfusion of packed red blood cells decreases anxiety levels in pretransfusion and postransfusion recipient patients through a randomized clinical trial. Methodology: Be conducted in patients over 18 years admitted in the Hospitable complex of Toledo, prescription transfusion of packed red blood cells. For an alpha error 0. 05, <b>beta</b> <b>error</b> of 0. 90, with an expected effect of 10 %, need 70 subjects in each group. The allocation to the intervention group and the control group was randomly made simple. The performance in the normal control group will be done in the hospital, patients receiving transfusion. As dependent variables evaluated:- The anxiety level pretransfusion and postransfusion. Using the questionnaire was validated by Spielberger (STAI). - The level of satisfaction perceived by the user on the information received prior to transfusion. By design developed for this study. Also recorded other control variables: sex, age, socio-cultural level, marital status, reason for transfusion, or no knowledge of the prescription of transfusion, incidents during transfusion. Scientific and sociosanitary relevancy of the study: The results will allow to know if the transfusion increases the anxiety and if an educational intervention nurse can diminish it; and to do the intervention before every transfusion...|$|E
30|$|However, {{the study}} {{has a number}} of limitations. Firstly, the aim {{of the study was to}} study the effects of steroids in sepsis; the study was {{conducted}} in an endotoxemic model on young and previously healthy animals limiting its validity for patients seen in intensive care. Nevertheless, endotoxemia induces the innate immune system, as does sepsis, and investigating the timing of steroid administration is difficult to do in the clinical situation. Also, these young and healthy animals may have different physiological response to the stress dose of hydrocortisone used in our protocol compared to patients in intensive care that are generally older and are more prone to have comorbidities. Secondly, although there was a total of 16 animals included in the study, there were only four animals in each group. Although this limits the power of the study, it has been taken into account, in the discussion of positive results, minimizing the effect of <b>beta</b> <b>error.</b> In addition, an obvious limitation of the study is the short observation period. Timing of hydrocortisone administration may be of limited impact on the systemic inflammatory response over a period of days, which is the natural course of sepsis. Finally, the correlation between neutrophil granulocytes and plasma NGAL is merely an association, and NGAL may have other sources; however, our findings are in line with the current literature suggesting neutrophil granulocytes to be the main source of NGAL in sepsis.|$|E
40|$|As a {{generalization}} of the accelerated failure time models, we consider parametric models of lifetime Y, where the conditional mean E(Y|X;beta) can depend nonlinearly on the covariates X and some parameters <b>beta.</b> The <b>error</b> distribution can be heteroscedastic and dependent on X. With observed data subject to right censoring, we propose regression analysis for beta based on Kaplan-Meier {{estimates of the}} means over several regions of X. Consistency and asymptotic distributional properties of the estimators are established under general conditions. A resulting estimator of beta is shown to be the sum of two possibly dependent asymptotic normal quantities, based on which conservative confidence intervals and tests are derived. Simulation studies are conducted to investigate {{the performance of the}} proposed estimator and to compare it with Buckley-Jame's method. To illustrate the methodology, we study an example with kidney transplant data, where a nonlinear relationship called "mixtures-of-experts", proposed in the neural networks literature, is used to model the relationship between the survival time and the age of the patients. ...|$|R
40|$|To assess {{risks of}} {{cultivation}} {{of genetically modified}} crops (GMCs) on non-target arthropods (NTAs), field tests are necessary to verify laboratory results and in situations where exposure pathways are very complex and cannot be reproduced in the laboratory. A central concern {{in the design of}} field trials for this purpose is whether the tests are capable of detecting differences in the abundance or activity of NTAs in a treated crop in comparison with a non-treated comparator plot. The detection capacity of a trial depends on the abundance and variability of the taxon, the values assumed for type I (alpha) and II (<b>beta)</b> <b>errors,</b> and the characteristics of the trial and statistical design. To determine the optimal trial layout and statistical analysis, 20 field trials carried out in Spain from 2000 to 2009 to assess risks of GMCs on NTAs were examined with alpha and beta set at 0. 05 and 0. 20, respectively. In this article we aim to determine the optimal number of sampling dates during a season, or longitudinal samples, in the design of field trials for assessing effects of GM maize on NTAs, and the ones that contribute most to achieving detectable treatment effects (d(c)) less than 50 % of the mean of the control. Detection capacities are a function of the number of individual samples taken during the season but a high number of samples is rarely justified because gains of repeated sampling can be relatively low. These gains depend primarily on field tests relative experimental variability in individual samplings (i. e. experimental variability relative to the mean of the control in each sampling date) which in turn depends on the sampling method (visual counts, pitfall traps or yellow sticky traps) and the density (or abundance) of the taxon in question. Taxa showing more density (or abundance) have less relative experimental variability. The smaller the experimental variability, the lower the profit of increasing the number of sampling dates. Sticky traps have a good effect detection capacity and need very few sampling dates, whereas visual counts and pitfall traps have a poorer effect detection capacity and need more individual samples to achieve d(c) values lower than 50 %. In maize field trials, it is recommended to concentrate sampling efforts in certain growth stages; the optimal ones for achieving an acceptable detection capacity are variable but, in general, samples {{in the first half of}} the season render better detection capacity than samples in the second half. Postprint (published version...|$|R
40|$|In {{the paper}} the Capital Asset Pricing Models of two Scandinavian Stock Markets are compared. With Finnish Stock data, a lower {{coefficient}} of determination is obtained than with Swedish Stock data. With Swedish data, the explanatory power of the squared <b>beta</b> and standard <b>error</b> components is markedly better. In {{so far as the}} sign of the regression coefficients is concerned, the Finnish models show a better correspondence with international evidence on the maifunctioning of the CAPM. With Swedish data, the coefficients are fairly close to those obtained with multiple factor models in the US-stock market. The finding suggests that the standard CAPM is unable to exhaustively represent the economic forces of capital asset pricing, especially in Sweden. CAPM Scandinavian stock markets...|$|R
