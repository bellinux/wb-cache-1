12|100|Public
5000|$|... : The <b>buffer</b> <b>address</b> is {{verified}} to be readable by {{the user}} mode caller.|$|E
5000|$|... 3270 {{displays}} and printers had a buffer containing one byte for every screen position. For example, a 3277 model 2 featured a screen size of 24 rows of 80 columns for a buffer size of 1920 bytes. Bytes were addressed {{from zero to}} the screen size minus one, in this example 1919. [...] "There is a fixed relationship between each ... buffer storage location and its position on the display screen." [...] Most orders started operation at the [...] "current" [...] <b>buffer</b> <b>address,</b> and executing an order or writing data would update this address. The <b>buffer</b> <b>address</b> could be set directly using the Set <b>Buffer</b> <b>Address</b> (SBA) order, often followed by Start Field. For a device with a 1920 character display a twelve bit address was sufficient. Later 3270s with larger screen sizes used fourteen or sixteen bits.|$|E
50|$|The 200-series IO {{instructions}} were a Peripheral Data Transfer (PDT) and a Peripheral Control and Branch (PCB) that explicitly implemented asynchronous IO. The PDT specified a device address, a <b>buffer</b> <b>address</b> and the transfer operation to be started, while the PCB specified a device address, {{and set the}} operating mode or tested {{the status of the}} device. Both used the format Op-code Address I/O unit address Variant.|$|E
40|$|An FPGA {{design of}} 4 K UHDTV (Ultra-high {{definition}} TV) H. 264 video decoder is proposed in this paper. The decoder {{is a complete}} one starting from bit stream input to decoding and final displaying, {{all of which are}} implemented on FPGA. Decoder engine that saves 51 % DRAM bandwidth and dis-play frame <b>buffer</b> <b>addressing</b> scheme that increases DRAM efficiency by 45 % are proposed. The proposed work is ca-pable of decoding and displaying...|$|R
5000|$|One {{processor}} sends {{messages to}} the other by writing the message into the pre-determined address and then sending an interrupt to signal the other processor that a new message is available. When transferring data buffers, only a pointer to a given buffer needs to be passed since the buffer resides in shared memory that is accessible to both the processors. ARM <b>buffer</b> <b>addresses</b> must be translated into physical addresses when being presented to the DSP, as the DSP {{does not have an}} MMU or a concept of virtual addressing.|$|R
5000|$|Atari BASIC uses a token {{structure}} to handle lexical processing for better performance and reduced memory size. The tokenizer converts lines {{using a small}} buffer in memory, and the program is stored as a parse tree. The token output <b>buffer</b> (<b>addressed</b> by a pointer at LOMEM [...] - [...] 80, 8116) is 256 bytes, and any tokenized statement larger than the buffer generates an error (14 [...] - [...] line too long). Indeed, the syntax checking described in the [...] "Program editing" [...] section is {{a side effect of}} converting each line into a tokenized form before it is stored.|$|R
5000|$|IDT 'HELLO' TITL 'HELLO - {{hello world}} program' * DXOP SVC,15 Define SVC TMLUNO EQU 0 Terminal LUNO * R0 EQU 0 R1 EQU 1 R2 EQU 2 R3 EQU 3 R4 EQU 4 R5 EQU 5 R6 EQU 6 R7 EQU 7 R8 EQU 8 R9 EQU 9 R10 EQU 10 R11 EQU 11 R12 EQU 12 R13 EQU 13 R14 EQU 14 R15 EQU 15 * DATA WP,ENTRY,0 * [...] * Workspace (On the 990 we can [...] "preload" [...] registers) * WP DATA 0 R0 DATA 0 R1 [...] DATA >1600 R2 - End of program SVC DATA >0000 R3 - Open I/O opcode DATA >0B00 R4 - Write I/O opcode DATA >0100 R5 - Close I/O opcode DATA STRING R6 - Message address DATA STRLEN R7 - Message length DATA 0 R8 DATA 0 R9 DATA 0 R10 DATA 0 R11 DATA 0 R12 DATA 0 R13 DATA 0 R14 DATA 0 R15 * [...] * Terminal SVC block * TRMSCB BYTE 0 SVC op code (0 = I/O) TRMERR BYTE 0 Error code TRMOPC BYTE 0 I/O OP CODE TRMLUN BYTE TMLUNO LUNO TRMFLG DATA 0 Flags TRMBUF DATA $-$ <b>Buffer</b> <b>address</b> [...] TRMLRL DATA $-$ Logical record length TRMCHC DATA $-$ Character count * [...] * Message * STRING TEXT 'Hello world!' BYTE >D,>A STRLEN EQU $-STRING EVEN PAGE * [...] * Main program entry * ENTRY MOVB R3,@TRMOPC Set open opcode in SCB SVC @TRMSCB Open {{terminal}} MOVB @TRMERR,R0 Check for error JNE EXIT MOVB R4,@TRMOPC Set write opcode MOV R6,@TRMBUF Set <b>buffer</b> <b>address</b> [...] MOV R7,@TRMLRL Set {{logical record length}} MOV R7,@TRMCHC and character count SVC @TRMSCB Write message MOVB @TRMERR,R0 Check for error JNE CLOSE CLOSE MOVB R5,@TRMOPC Set close opcode SVC @TRMSCB Close terminal EXIT SVC R2 Exit program * END ...|$|E
5000|$|In {{a typical}} {{application}} the host sends the terminal a preformatted panel containing both static data and fields into which data may be entered. The terminal operator keys data, such as updates in a database entry (or the device's buffer - 3270), into the appropriate fields. When entry is complete (or ENTER or PF key pressed on 3270's), {{a block of}} data, usually just the data entered by the operator (modified data), {{is sent to the}} host in one transmission. The 3270 terminal buffer (at the device) could be updated on a single character basis, if necessary, because of the existence of a [...] "set <b>buffer</b> <b>address</b> order" [...] (SBA), that usually preceded any data to be written/overwritten within the buffer. A complete buffer could also be read or replaced using the READ BUFFER command or WRITE Command (unformatted or formatted {{in the case of the}} 3270).|$|E
5000|$|I/O request packets (IRPs) are {{kernel mode}} {{structures}} {{that are used}} by Windows Driver Model (WDM) and Windows NT device drivers {{to communicate with each}} other and with the operating system. They are data structures that describe I/O requests, and can be equally well thought of as [...] "I/O request descriptors" [...] or similar. Rather than passing a large number of small arguments (such as <b>buffer</b> <b>address,</b> buffer size, I/O function type, etc.) to a driver, all of these parameters are passed via a single pointer to this persistent data structure. The IRP with all of its parameters can be put on a queue if the I/O request cannot be performed immediately. I/O completion is reported back to the I/O manager by passing its address to a routine for that purpose, IoCompleteRequest. The IRP may be repurposed as a special kernel APC object if such is required to report completion of the I/O to the requesting thread.|$|E
5000|$|Devices on a FireWire bus can {{communicate}} by {{direct memory access}} (DMA), where a device can use hardware to map internal memory to FireWire's [...] "Physical Memory Space". The SBP-2 (Serial Bus Protocol 2) used by FireWire disk drives uses this capability to minimize interrupts and buffer copies. In SBP-2, the initiator (controlling device) sends a request by remotely writing a command into a specified area of the target's FireWire address space. This command usually includes <b>buffer</b> <b>addresses</b> in the initiator's FireWire Physical Address Space, which the target is supposed to use for moving I/O data {{to and from the}} initiator.|$|R
40|$|Abstractâ€”The {{literature}} on queueing systems with finite <b>buffers</b> <b>addresses</b> mostly asymptotic performance metrics on an aggregate flow, and/or generally {{relies on a}} convenient, but provably inaccurate, approximation of the loss probability by the overflow probability in an infinite size <b>buffer.</b> This paper <b>addresses</b> non-asymptotic per-flow metrics in a multi-flow queue-ing system with finite buffer and FIFO scheduling. The analysis dispenses with the above approximation, and lends itself to several interesting insights {{on the impact of}} finite buffers on per-flow metrics. Counterintuitively, the per-flow delay distribution is not monotonous in the buffer size, and such an effect is especially visible in high burstiness regimes. Another observation is that buffer dimensioning becomes insensitive to the type of SLA constraint, e. g., fixed violation probability on either loss or delay, in high multiplexing regimes. In the particular case of aggregate scheduling, the results on the aggregate input flow significantly improve upon existing results by capturing the manifestation of bufferless multiplexing in regimes with many flows. I...|$|R
5000|$|Specialized {{instructions}} for modulo <b>addressing</b> in ring <b>buffers</b> and bit-reversed <b>addressing</b> mode for FFT cross-referencing ...|$|R
5000|$|IDT 'HELLO' TITL 'HELLO - {{hello world}} program' * R0 EQU 0 R1 EQU 1 R2 EQU 2 R3 EQU 3 R4 EQU 4 R5 EQU 5 R6 EQU 6 R7 EQU 7 R8 EQU 8 R9 EQU 9 R10 EQU 10 R11 EQU 11 R12 EQU 12 R13 EQU 13 R14 EQU 14 R15 EQU 15 * * Terminal CRU bits * TRMCRU EQU >0 Terminal device address XMIT EQU 8 DTR EQU 9 RTS EQU 10 WRQ EQU 11 RRQ EQU 12 NSF EQU 13 * PAGE * * Main program entry * ENTRY LWPI WP Load our {{workspace}} pointer BLWP @PRINT Call our print routine DATA STRING DATA STRLEN IDLE * WP BSS 32 Main program workspace * * Message * STRING TEXT 'Hello world!' BYTE >D,>A STRLEN EQU $-STRING EVEN PAGE * * Print a message * PRINT DATA PRWS,PRENT PRENT EQU $ MOV *R14+,R2 Get <b>buffer</b> <b>address</b> MOV *R14+,R1 Get message length SBO DTR Enable terminal ready SBO RTS PRI010 LDCR *R2+,8 Send out a character TB WRQ Wait until done JNE $-2 SBZ WRQ DEC R1 JGT PRI010 RTWP * PRWS DATA 0,0,0,0,0,0,0,0 DATA 0,0,0,0,TRMCRU,0,0,0 * END ENTRY ...|$|E
40|$|Time-shared {{interface}} speeds {{data processing}} in distributed computer network. Two-level high-speed scanning approach routes information to buffer, portion {{of which is}} reserved for series of "first-in, first-out" memory stacks. <b>Buffer</b> <b>address</b> structure and memory are protected from noise or failed components by error correcting code. System is applicable to any computer or processing language...|$|E
40|$|The {{proposed}} WIDAR correlator for the EVLA aims to use {{the method}} of `recirculation' to greatly increase the spectral resolution capabilities of the correlator at narrow bandwidths. This method involves buffering low sample rate data, and then bursting it at high data rates through the correlator with different relative station delays [...] -or <b>buffer</b> <b>address</b> offsets [...] -to effectively provide many more correlator lags for the hardware available. This memo investigates the basic recirculation architecture {{within the context of}} the proposed design, develops a <b>buffer</b> <b>address</b> offset algorithm, and discusses some fundamental limitations of the technique in terms of SNR degradation and integration time. Introduction In [1], the basic architecture of the proposed WIDAR correlator for the EVLA was presented. In that document so-called recirculation is used to provide very high spectral resolution when correlating narrow bandwidths. This capability was included in the design by request of NRAO at [...] ...|$|E
40|$|A {{controller}} for displaying restricted patterns inpassive matrix LCDs will be presented. Hardwarecomplexity and the computational time forgenerating column {{signals are}} reduced considerablyby using serial arithmetic directly from digitisedsamples and hence eliminating the frame <b>buffer</b> inmulti-line <b>addressing</b> techniques. QC 20140805 </p...|$|R
40|$|Cranium is a processor-network {{interface}} for an interconnection {{network based}} on adaptive packet routing. Adaptive networks relax the restriction that packet order is preserved; packets may {{be delivered to}} their destinations in an arbitrary sequence. Cranium uses two mechanisms: an automatic-receive interface for packet serialization and high performance, and a processor-initiated interface for flexibility. To minimize software overhead, Cranium is directly accessible by user-level programs. Protection for user-level message passing is implemented by mapping user-level handles into physical node identifiers and <b>buffer</b> <b>addresses.</b> 1 Introduction Scalable multicomputer architectures have been converging on a standard organization with four elements: a workstation microprocessor, main memory based on dynamic RAM, a point-to-point interconnection network and a processornetwork interface. Both the microprocessors and DRAM chips have become inexpensive and widely available. Multicomputer [...] ...|$|R
40|$|This survey compares and {{contrasts}} {{the memory}} management designs of six commercial microarchitectures {{in the context}} of today's operating system requirements, which include such features as multiple processes with address space protection, shared memory, large virtual address spaces, and fine-grained protection. Keywords: virtual memory, memory management units, translation lookaside <b>buffers,</b> <b>address</b> spaces, hardware support for operating system functions WORK-IN-PROGRESS March 20, 1997 5 : 26 pm 2 PART ONE: A Virtual Memory Primer Virtual memory supports the execution of processes only partially resident in memory; this allows operating systems to manage the physical memory effectively. In a virtual memory system, only the most-often used portions of a process's address space actually occupy physical memory; the rest of the address space is stored on disk until needed. Most processors support virtual memory through a hardware memory management unit (MMU) that translates virtual [...] ...|$|R
40|$|Buffer {{overflow}} mitigations generally {{fall into}} two categories: (i) eliminating the cause and (ii) alleviating the damage. In lecture, we saw memory-safe languages and proofs as examples for the first category. This question is about techniques in the second category. Several requirements must be met for a buffer overflow to succeed. Each requirement listed below can be combated with a different countermeasure. With each mitigation you discuss, think about {{where it can be}} implementedâ€”common targets include the compiler and the operating system (OS). Also discuss limitations, pitfalls, and costs of each mitigation. (a) The attacker needs to overwrite the return address on the stack to change the control flow. Is it possible to prevent this from happening or at least detect when it occurs? (b) The overwritten return address must point to a valid instruction sequence. The attacker often places the malicious code to execute in the vulnerable buffer. However, the <b>buffer</b> <b>address</b> must be known to set up the jump target correctly. One way to find out this address is to observe the program in a debugger. This works becaus...|$|E
40|$|In 2009, {{security}} researchers {{discovered a}} new, very powerful rootkit environment on x 86 platforms [1]. That environment {{is based on}} Intel's Active Management Technology (iAMT) [2], which is completely isolated from the host. One part of iAMT is implemented as an embedded my-controller in the platform's memory controller hub. That my-controller is called Manageability Engine (ME) and includes a processor (ARCtangent-A 4), read-only memory (ROM), {{static random access memory}} (SRAM) and direct memory access (DMA) engines. Furthermore, iAMT provides an isolated network channel (out-of-band (OOB) communication). To illustrate the power of the stealth enviroment, [1] called the iAMT environment in conjunction with rootkits "ring - 3 ", following the x 86 ring protection model. For our evaluation we implemented a prototype in form of a USB keyboard keystroke logger [3]. 1 Since we were unable to get an Intel developer board providing the "ring - 3 " environment, we had to use the exploit discovered by [1] to infiltrate our target platform. We monitor the keyboard buffer of the Linux based target platform via DMA. To find the physical address of the keyboard buffer we apply a search algorithm, that finds the USB product string and follows some pointers to the structure containing the <b>buffer</b> <b>address.</b> To exfiltrate captured keystrokes our prototype uses iAMT's OOB communication capabilities. [1] discussed countermeasures against "ring - 3 " rootkits, but they also provide approaches to defeat such countermeasures. Furthermore, it is doubtful if all the proposed countermeasures can be applied in practice. 2 The goal of our evaluation is to find a reliable detection mechanism for "ring - 3 " rootkits. We assume that we can provoke delays when accessing the same resources as our prototype. For example, our prototype has to scan the host memory to find certain data structures and it also has use the network interface card to send keystroke codes. Another possibility is to initiate various DMA transfers using multiple devices. Only one device can be the bus master at a certain point in time. The next step is to design an experimental set-up that allows the measurement of delays and finally derive a reliable detection mechanism for "ring - 3 " rootkits...|$|E
40|$|With the {{increasing}} uniprocessor and SMP computation power available today, interprocessor communication {{has become an}} important factor that limits the performance of cluster of workstations. Many factors including communication hardware overhead, communication software overhead, and the user environment overhead (multithreading, multiuser) affect {{the performance of the}} communication subsystems in such systems. A significant portion of the software communication overhead belongs to a number of message copying. Ideally, it is desirable to have a true zero-copy protocol where the message is moved directly from the send buffer in its user space to the receive buffer in the destination without any intermediate buffering. However, {{due to the fact that}} message -passing applications at the send side do not know the final receive <b>buffer</b> <b>addresses,</b> early arrival messages have to be buffered at a temporary area. In this paper, we show that there is a message reception communication locality in [...] ...|$|R
40|$|A feeding buffer {{is created}} by {{starting}} a project activity before its latest start time to provide adequate protection against project delay. Assuming linear costs for starting activities earlier and a linear project delay penalty, early optimization models for project <b>buffers</b> <b>addressed</b> simple project network structures and invoked the newsvendor model as an approximation. Recently, it had been shown that when the gating activities precede the only stochastic elements in a project, then there exists an exact generalization of the newsvendor model that characterizes the optimal feeding buffers. This insight led to an effective and efficient solution approach by simulation. We show that this result also holds when stochastic elements exist elsewhere within the project. Therefore, the same simulation approach applies. This yields practically optimal feeding buffers {{even when it is}} impossible to compute the completion time distribution. The same insights can be used for sequencing decisions and, with a slight enhancement, for optimal crashing decisions. The method works even when activities are statistically dependent...|$|R
40|$|Ultra-low power (ULP) {{biomedical}} implants and sensor nodes typically require small {{memories of}} a few kb, while previous work on reliable subthreshold (sub-VT) memories targets several hundreds of kb. Standard-cell based memories (SCMs) are a straightforward approach to realize robust sub- VT storage arrays and fill the gap of missing sub-VT memory compilers. This paper presents an ultra-low-leakage 4 kb SCM manufactured in 65 nm CMOS technology. To minimize leakage power during standby, a single custom-designed standard-cell (D- latch with 3 -state output <b>buffer)</b> <b>addressing</b> all major leakage contributors of SCMs is seamlessly integrated into the fully automated SCM compilation flow. Silicon measurements of a 4 kb SCM indicate a leakage power of 500 fW per stored bit (at a data-retention voltage of 220 mV) and a total energy of 14 fJ per accessed bit (at energy-minimum voltage of 500 mV), corresponding to the lowest values in 65 nm CMOS reported to date...|$|R
40|$|Abstract. Communication latencies {{have been}} {{identified}} as one of the performance limiting factors of message passing applications in clusters of workstations/multiprocessors. On the receiver side, message-copying operations contribute to these communication latencies. Recently, prediction of MPI messages has been proposed as part of the design of a zero message-copying mechanism. Until now, prediction was only evaluated for the next message. Predicting only the next message, however, may not be enough for real implementations, since messages do not arrive in the same order as they are requested. In this paper, we explore long-term prediction of MPI messages for the design of a zero message-copying mechanism. To achieve long-term prediction we evaluate two prediction schemes, the first based on graphs, and the second based on periodicity detection. Our experiments indicate that with both prediction schemes the <b>buffer</b> <b>addresses</b> and message sizes of several future MPI messages (up to + 10) can be predicted successfully. ...|$|R
40|$|In project {{scheduling}} or batch supply chain operations, a positive (negative) feeding buffer {{is created by}} starting an activity before (after) its expected latest start time. Positive feeding buffers provide protection against project tardiness. Assuming linear costs for starting activities earlier and a linear project tardiness penalty, early optimization models for project <b>buffers</b> <b>addressed</b> particular project network structures. By these models it can be shown that when the gating activities precede the only stochastic elements in a project, then there exists an exact generalization of the newsvendor optimal result that characterizes the optimal feeding buffers: the marginal cost of a buffer should match its criticality. This insight is associated with an effective and efficient solution approach by simulation. We show that this result also holds when stochastic elements exist anywhere else within the project and when activities are statistically correlated. Furthermore, the same simulation approach applies. This yields practically optimal feeding buffers {{even when it is}} impossible to compute the completion time distribution...|$|R
40|$|Data {{communications}} between producer instructions and consumer instructions through memory incur extra delays that degrade processor performance. In this paper, we {{introduce a new}} storage media with a novel addressing mechanism to avoid address calculations. Instead of a memory address, each load and store is assigned a signature for accessing the new storage. A signature consists {{of the color of}} the base register along with its displacement value. A unique color is assigned to a register whenever the register is updated. When two memory instructions have the same signature, they address to the same memory location. This memory signature can be formed early in the processor pipeline. A small Signature <b>Buffer,</b> <b>addressed</b> by the memory signature, can be established to permit stores and loads bypassing normal memory hierarchy for fast data communication. Performance evaluations based on an Alpha 21264 -like pipeline using SPEC 2000 integer benchmarks show that an IPC (Instruction-Per-Cycle) improvement of 13 - 18 % is possible using a small 8 -entry signature buffer. 1...|$|R
40|$|The cache {{directory}} is {{a unified}} control structure that maintains the contents {{and the state}} of a variety of onchip and off-chip cache structures, such as L 2 or L 3 caches, victim caches, prefetch buffers, stream buffers, and bypass <b>buffers.</b> <b>Address</b> tags with these individual cache structures are no longer needed. The cache directory provides a physical-to-physical address mapping, from the physical addresses issued by the processor (or by other devices) to a cache address space in which the aforementioned cache structures reside. The level of indirection afforded by this mapping provides a flat memory hierarchy, and avoids the need for inclusion between the different levels. It furthermore allows the storage for cache structures built out of the same technology (SRAM or DRAM) to be unified into a single area. Both of these advantages lead to a substantial reduction in complexity. Finally, the cache directory allows for more flexible placement policies within a cache structure and among [...] ...|$|R
5000|$|... #Caption: A {{block diagram}} of the {{architecture}} of the Z80 microprocessor, showing the arithmetic and logic section, register file, control logic section, and <b>buffers</b> to external <b>address</b> and data lines ...|$|R
40|$|Abstract. PCM {{can be used}} to {{overcome}} the capacity limit and energy issues of conventional DRAM-based main memory. This paper explores how the database buffer manager can deal with the write endurance problem, which is unique to PCM-based buffer pools and not considered by conventional buffer algorithms. We introduce a range of novel <b>buffer</b> algorithms <b>addressing</b> this problem, called wear-aware buffer algorithms, and study their behavior using trace-driven simulations. ...|$|R
50|$|Other {{divisions}} in Motorola developed components for the M6800 family. The Components Products Department designed the MC6870 two-phase clock IC, and the Memory Products group provided a full line of ROMs and RAMs. The CMOS group's MC14411 Bit Rate Generator provided a 75 to 9600 baud clock for the MC6850 serial interface. The <b>buffers</b> for <b>address</b> and data buses were standard Motorola products. Motorola could supply every IC, transistor, and diode necessary {{to build an}} MC6800-based computer.|$|R
5000|$|A {{partially}} standardized list {{of up to}} six parameters {{known as}} P1 through P6. The first two parameters typically specify the I/O <b>buffer</b> starting <b>address</b> (P1), and the I/O byte count (P2). The remaining parameters vary with the operation, and the particular device. For example, for a computer terminal, P3 might be the time {{to allow for the}} read to complete whereas, for a disk drive, it might be the starting block number of the transfer.|$|R
40|$|A {{controller}} for displaying restricted {{patterns in}} passive matrix LCDs will be presented. Hardware complexity and the computational time for generating column signals are reduced considerably by using serial arithmetic directly from digitised samples and hence eliminating the frame <b>buffer</b> in multi-line <b>addressing</b> techniques...|$|R
5000|$|The {{standard}} and classical way: Using <b>buffers</b> the SMF <b>address</b> space, {{together with a}} set of preallocated datasets (VSAM datasets) to use when a buffer fills up. The standard name for the datasets is SYS1.MANx, where x is a numerical suffix (starting from 0).|$|R
50|$|Buffering stores until {{retirement}} avoids WAW and WAR dependencies but {{introduces a}} new issue. Consider the following scenario: a store executes and <b>buffers</b> its <b>address</b> and {{data in the}} store queue. A few instructions later, a load executes that reads from the same memory address to which the store just wrote. If the load reads its data from the memory system, it will read an old value {{that would have been}} overwritten by the preceding store. The data obtained by the load will be incorrect.|$|R
50|$|Like Apple II, MPF II had two {{graphics}} buffers. However {{the second}} <b>buffer</b> was at <b>address</b> A000H while with Apple II {{it was at}} 4000H. The keyboard input was mapped to a different address than the Apple II making impossible to play Apple's games on the MPF II.|$|R
40|$|An {{experimental}} {{study was conducted}} using a network simulator to investigate the performance of packet communication networks as a function of: the network resource capacities (channels, buffers), the network load (number of virtual channels, virtual channel loads), protocols (flow control, congestion control, routing) and protocol parameters (virtual channel window size, input buffer limits). Performance characteristics are shown and the design of input buffer limits for network congestion control, virtual channel window size and nodal <b>buffer</b> capacity <b>addressed.</b> Network design strategies for the control of load fluctuations are proposed and discussed. 1...|$|R
