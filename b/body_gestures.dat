266|413|Public
5|$|It was {{originally}} revealed as Codename: Kingdoms during Microsoft's E3 2010 press conference, {{along with the}} announcement that the game was being developed by Crytek. During the Microsoft Press Conference at E3 2011, Ryse was announced as a Kinect-only title. The announcement entailed a prerendered trailer with minor gameplay footage. The gameplay footage featured players using their own <b>body</b> <b>gestures</b> to control the protagonist to fight against enemies, and perform actions like sword wielding, blocking attacks with a shield, and head-butting. The trailer served as a test for Crytek {{to see whether the}} general audience liked the Kinect features or not.|$|E
25|$|In this era, Sun Ra began {{conducting}} using {{hand and}} <b>body</b> <b>gestures.</b> This system inspired cornetist Butch Morris, who later developed his own more highly refined way to conduct improvisers.|$|E
25|$|Deathly Hallows Part 1 has side {{missions}} {{which use}} Kinect for the Xbox 360. The missions include battling against Death Eaters and Snatchers in environments from the game. The two-player missions are played in on-rails shooter mode, where the player casts spells through hand and <b>body</b> <b>gestures.</b> By progressing through the levels the player aims {{to achieve the}} highest score which is then posted on Xbox Live.|$|E
5000|$|<b>Body</b> <b>Gesture</b> (Elizabeth Leach Gallery, Portland, Oregon, United States) ...|$|R
5000|$|Games - mostly Kinect <b>body</b> <b>gesture</b> {{controlled}} games, {{and print}} {{a photo of}} the person and his/her scores ...|$|R
50|$|There {{are many}} {{proposed}} methods {{to detect the}} <b>body</b> <b>gesture.</b> Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts {{in order to obtain}} several important parameters, like palm position or joint angles. On the other hand, Appearance-based systems use images or videos to for direct interpretation. Hand gestures have been a common focus of <b>body</b> <b>gesture</b> detection, apparentness methods and 3-D modeling methods are traditionally used.|$|R
500|$|In casting Sing's love {{interest}} Fong, Chow {{stated that he}} wanted an innocent looking girl for the role. Television actress Eva Huang, in her film debut, was chosen from over 8,000 girls. When asked about his decision in casting her, Chow said that he [...] "just had a feeling about her" [...] and that he enjoyed working with new actors. She chose to have no dialogue in the film {{so that she could}} stand out only with her <b>body</b> <b>gestures.</b>|$|E
500|$|The {{team made}} use of Havok's Behavior toolset for {{character}} animation, which allows for a greater fluidity between the character's movements of walking, running and sprinting, and also increases {{the efficiency of the}} third-person camera option, which had been criticized in Oblivion. The toolset allows interactions between the player and NPCs to take place in real-time; in Oblivion, when the player went to interact with an NPC, time would freeze and the camera would zoom in on the NPC's face. In Skyrim, NPCs can move around and make <b>body</b> <b>gestures</b> while conversing with the player. Children are present in the game, and their presence is handled similarly to Fallout 3 in that they cannot be harmed by the player in any way, since depictions of violence involving children in video games is considered controversial. Skyrim makes use of the Radiant AI artificial intelligence system that was created for Oblivion, and it has been updated to allow NPCs to [...] "do what they want under extra parameters". The updated system allows for greater interaction between NPCs and their environments; NPCs can perform tasks such as farming, milling, and mining in the game world, and will react to each other.|$|E
500|$|Pip, a {{chipmunk}} who {{can talk}} in the 2D world of Andalasia, loses his ability to communicate through speech {{in the real world}} so he must rely heavily on facial and <b>body</b> <b>gestures.</b> This meant the animators had to display Pip's emotions through performance as well as making him appear like a real chipmunk. The team at Tippett began the process of animating Pip by observing live chipmunks which were filmed in motion from [...] "every conceivable angle", after which they created a photorealistic chipmunk through the use of 3D computer graphics software, Maya and Furrocious. When visual effects supervisor Thomas Schelesny showed the first animation of Pip to director Kevin Lima, he was surprised that he was a looking at a CG character and not reference footage. To enhance facial expressions, the modelers gave Pip eyebrows, which real chipmunks do not have. During the filming of scenes in which Pip appears, a number of ways were used to indicate the physical presence of Pip. On some occasions, a small stuffed chipmunk with a wire armature on the inside was placed in the scene. In other situations, a rod with a small marker on the end or a laser pointer would be used to show the actors and cinematographer where Pip is.|$|E
40|$|For the {{computer}} to interact intelligently with human users, computers {{should be able to}} recognize emotions, by analyzing the human’s affective state, physiology and behavior. In this paper, we present a survey of research conducted on face and <b>body</b> <b>gesture</b> and recognition. In order to make human-computer interfaces truly natural, we need to develop technology that tracks human movement, body behavior and facial expression, and interprets these movements in an affective way. Accordingly in this paper, we present a framework for a vision-based multimodal analyzer that combines face and <b>body</b> <b>gesture</b> and further discuss relevant issues...|$|R
50|$|Ngondek (sissy, effeminate) - in {{the manner}} of {{speaking}} and <b>body</b> <b>gesture,</b> popularized by LGBT community. Derived from bus kondektur (public bus attendant) that speaking fast on announcing the destinations while doing waving gesture.|$|R
40|$|The {{development}} status of human <b>body</b> motion <b>gesture</b> data fusion domestic and overseas has been analyzed. A triaxial accelerometer is adopted {{to develop a}} wearable human <b>body</b> motion <b>gesture</b> monitoring system aimed at old people healthcare. On {{the basis of a}} brief introduction of decision tree algorithm, the WEKA workbench is adopted to generate a human <b>body</b> motion <b>gesture</b> decision tree. At last, the classification quality of the decision tree has been validated through experiments. The experimental results show that the decision tree algorithm could reach an average predicting accuracy of 97. 5...|$|R
500|$|On its review, Monthly Film Bulletin {{qualified}} Presley's {{career as}} [...] "one {{of the most}} puzzling and less agreeable aspects of modern popular music". the review declared: [...] "Presley adopts a slurred and husky style of delivery {{and a series of}} grotesque <b>body</b> <b>gestures</b> to impose on his otherwise innocuous material a suggestive meaning.... in 'Loving You' he is allowed more scope and is at times both the cause and sum total of the film's somewhat doubtful entertainment value." [...] Down Beat opened its review mentioning the negative reception of Presley by the press, indicating that while other publications [...] "hotly despised" [...] him, Down Beat was [...] "prepared to dismiss him with a decimating round of punfire". [...] The reviewer, however noted that after watching Loving You, it was [...] "amiss to speak unkindly of [...] ", and that the film was [...] "a rather entertaining pic". It remarked the [...] "resourcefulness" [...] of Lizabeth Scott, the [...] "positive acting ability augmented by a fresh prettiness" [...] of Dolores Hart, and the [...] "witty lines and range of expressions" [...] delivered by Wendell Corey. The review favored Presley, describing his performance as [...] "an overpowering, if touchingly naive, celluloid sexuality." [...] It concluded: [...] "For all his high-voltage on-stage erotica... he plays the sullen country boy convincingly... evincing all the emotion of a well bred head of livestock." ...|$|E
5000|$|... 2011: <b>Body</b> <b>Gestures,</b> Herzliya Museum of Contemporary Art, Israel.|$|E
5000|$|... #Caption: Military air marshallers use {{hand and}} <b>body</b> <b>gestures</b> to direct flight {{operations}} aboard aircraft carriers.|$|E
40|$|We {{propose to}} develop a novel motion-driven interface, without having to hold {{additional}} devices such as Wii Remote. The <b>body</b> <b>gesture</b> can be first processed as silhouette, and a finite state machine follows to recognize the continuous motion as skeletons. Users can have more fun playing with avatars more naturally...|$|R
40|$|This report {{contains}} {{a description of}} the work done in MIRALab, University of Geneva, and LIG, EPF Lausanne, concerning the simulation of virtual dancers and <b>body</b> <b>gesture</b> recognition for the interaction avatar-autonomous. It summarizes the different steps needed to create and animate bodies in a virtual environment. Document ID eRENA-D 2. ...|$|R
40|$|Abstract: The {{development}} status of human <b>body</b> motion <b>gesture</b> data fusion domestic and overseas has been analyzed. A triaxial accelerometer is adopted {{to develop a}} wearable human <b>body</b> motion <b>gesture</b> monitoring system aimed at old people healthcare. On {{the basis of a}} brief introduction of decision tree algorithm, the WEKA workbench is adopted to generate a human <b>body</b> motion <b>gesture</b> decision tree. At last, the classification quality of the decision tree has been validated through experiments. The experimental results show that the decision tree algorithm could reach an average predicting accuracy of 97. 5 % with lower time cost...|$|R
50|$|Most of {{the major}} {{characters}} are portrayed as animals through facial and <b>body</b> <b>gestures,</b> {{as well as their}} speech.|$|E
5000|$|... 7) uses {{body and}} hand {{gestures}} to emphasize a point, or, never uses hand or <b>body</b> <b>gestures</b> to emphasize a point or makes inappropriate gestures.|$|E
50|$|Children with perceptual deficits do not {{perceive}} the environment appropriately and interpret interpersonal interactions inaccurately. They also have difficulty reading social cues, facial expressions and <b>body</b> <b>gestures.</b>|$|E
5000|$|Body {{language}} involves <b>body</b> posture, <b>gestures,</b> facial expressions, and eye movements.|$|R
40|$|Abstract. Several {{different}} {{techniques are}} described for turning a large wall into an interactive surface. These include capacitive sensing of arm and <b>body</b> <b>gesture,</b> scanning laser rangefinders for tracking hands above video walls, passive tracking of knocks on large plates of glass with distributed acoustic pickups, and {{the detection of}} touch via frustrated total internal reflection. ...|$|R
40|$|Abstract. In {{this paper}} we review the major {{approaches}} to multimodal human computer interaction {{from a computer}} vision perspective. In particular, we focus on <b>body,</b> <b>gesture,</b> gaze, and affective interaction (facial expression recognition, and emotion in audio). We discuss user and task modeling, and multimodal fusion, highlighting challenges, open issues, and emerging applications for Multimodal Human Computer Interaction (MMHCI) research. ...|$|R
50|$|In this era, Sun Ra began {{conducting}} using {{hand and}} <b>body</b> <b>gestures.</b> This system inspired cornetist Butch Morris, who later developed his own more highly refined way to conduct improvisers.|$|E
50|$|Professor Ray Birdwhistell {{was one of}} the {{earliest}} theorists of nonverbal communication. As an anthropologist, he created the term kinesics, and defined it as communication and perceived meaning from facial expressions and <b>body</b> <b>gestures.</b>|$|E
5000|$|The {{judges of}} the New York Dance and Performance Bessies awarded the 2003 prize, [...] "For an {{extraordinary}} symphony of upper <b>body</b> <b>gestures</b> performed in extrasensory collaboration in an ordinary setting made tense by the silent musical score, for an intimate production by an unlikely pair of average middle-aged white guys in chairs." ...|$|E
5000|$|Working {{directly}} with the future media labs at Microsoft that birthed the prototype technology {{that led to the}} Kinect and Hololens, his focus was on discovering new forms of “experiences” within emergent [...] "Natural User Interface" [...] (NUI) paradigms. First of kind advancements were made toward intuitive, multi modal, full <b>body</b> <b>gesture</b> based navigation of characters through spatially complex and virtually tactile(reactive) worlds.|$|R
40|$|Abstract. This paper {{presents}} {{an approach to}} automatic visual emotion recogni-tion from two modalities: expressive face and <b>body</b> <b>gesture.</b> Face and <b>body</b> movements are captured simultaneously using two separate cameras. For each face and body image sequence single “expressive ” frames are selected manually for analysis and recognition of emotions. Firstly, individual classifiers are trained from individual modalities for mono-modal emotion recognition. Sec-ondly, we fuse facial expression and affective <b>body</b> <b>gesture</b> information at the feature and at the decision-level. In the experiments performed, the emotion classification using the two modalities achieved a better recognition accuracy outperforming the classification using the individual facial modality. We further extend the affect analysis into a whole image sequence by a multi-frame post in-tegration approach over the single frame recognition results. In our experi-ments, the post integration based on the fusion of face and body has shown to be more accurate than the post integration based on the facial modality only. ...|$|R
40|$|In {{this paper}} we review the major {{approaches}} to Multimodal Human Computer Interaction, giving {{an overview of}} the field from a computer vision perspective. In particular, we focus on <b>body,</b> <b>gesture,</b> gaze, and affective interaction (facial expression recognition and emotion in audio). We discuss user and task modeling, and multimodal fusion, highlighting challenges, open issues, and emerging applications for Multimodal Human Computer Interaction (MMHCI) research...|$|R
50|$|In a job {{recruitment}} context, face-to-face {{interactions with}} company representatives, {{such as at}} career fairs, should be perceived by applicants as rich media. Career fairs allow instant {{feedback in the form}} of questions and answers and permit multiple cues including verbal messages and <b>body</b> <b>gestures</b> and can be tailored to each job seeker's interests and questions.|$|E
50|$|Lorentzen started dancing {{four years}} old in Stockholm. She studied dancing at the Opera Ballet School in Oslo, and {{became a member of}} the ballet {{ensemble}} of the Norwegian National Ballet in 1973, soloist 1977. With her background as a half Indonesian, half Norwegian, she is in contact with a different culture of human relations also through <b>body</b> <b>gestures.</b>|$|E
50|$|Sign Language {{may be used}} as {{a command}} method. This is the {{language}} of Deaf individuals who are in the deaf community. There are many people who are deaf who do not use sign language. This language is a visual language and does not require any vocal usage. Sign language uses facial and <b>body</b> <b>gestures</b> as well to distinguish meanings.|$|E
40|$|Nowadays {{controllers}} {{are one of}} main {{topic in}} engineering and researchers are to seek the produce simpler controller, so <b>Body</b> <b>Gesture</b> {{can be a good}} choice and it is one of popular way. This paper narrates the all proceeding of designing a system for control of a servo motor angle control using by <b>body</b> <b>gesture</b> of user. Hardware structure was wholly designed and implemented for this aim. Hardware structure was designed by an interface circuit that is based on microcontroller Atmega 8 for analyze the data and generate command also circumambient architecture for control movement of different angle of the servo motor. For operate the servo motor were used Kinect. Kinect is used for receive information from user and provide communication between user and computer then send information of angle of servo motor to microcontroller. For suitable interaction for controlling servo motor by user, was created Graphic User Interface in visual C#. The main objective of this paper was to servo motor controlled by users without any background. No training is needed to user for controlling servo motor like traditional controller...|$|R
40|$|Multimodal systems allow {{humans to}} {{interact}} with machines through multiple modalities such as speech, facial expression, gesture, and gaze. This paper presents a bimodal model of facial and upper-body gesture for affective HCI suitable {{for use in a}} vision-based multimodal system. What distinguishes the present study from its predecessors is that, this model combines Facial Action Units (FAUs) and Body Action Units (BAUs) to encode affective states. To our best knowledge there has been no attempt to combine face and <b>body</b> <b>gesture</b> for multimodal affect recognition yet...|$|R
40|$|Abstract—An {{expression}} can {{be approximated by}} {{a sequence}} of temporal segments called neutral, onset, offset and apex. However, {{it is not easy}} to accurately detect such temporal segments only based on facial features. Some researchers try to temporally segment expression phases with the help of <b>body</b> <b>gesture</b> analysis. The problem of this approach is that the expression temporal phases from face and gesture channels are not synchronized. Additionally, most previous work adopted facial key points tracking or body tracking to extract motion information, which is unreliable in practice due to illumination variations and occlusions. In this paper, we present a novel algorithm to overcome the above issues, in which two simple and robust features are designed to describe face and gesture information, i. e., motion area and neutral divergence features. Both features do not depend on motion tracking, and they can be easily calculated too. Moreover, it is different from previous work in that we integrate face and <b>body</b> <b>gesture</b> together in modeling the temporal dynamics through a single channel of sensorial source, so it avoids the unsynchronized issue between face and gesture channels. Extensive experimental results demonstrate the effectiveness of the proposed algorithm. Keywords-temporal segment; motion area; neutral divergence; I...|$|R
