199|258|Public
25|$|From this <b>bit</b> <b>vector</b> viewpoint, a {{concrete}} Boolean algebra {{can be defined}} equivalently as a nonempty set of bit vectors all of the same length (more generally, indexed by the same set) and closed under the <b>bit</b> <b>vector</b> operations of bitwise ∧, ∨, and ¬, as in 1010∧0110 = 0010, 1010∨0110 = 1110, and ¬1010 = 0101, the <b>bit</b> <b>vector</b> realizations of intersection, union, and complement respectively.|$|E
25|$|This {{observation}} is easily proved as follows. Certainly any law satisfied by all concrete Boolean algebras is satisfied by the prototypical one {{since it is}} concrete. Conversely any law that fails for some concrete Boolean algebra must have failed at a particular bit position, in which case that position by itself furnishes a one-bit counterexample to that law. Nondegeneracy ensures the existence {{of at least one}} bit position because there is only one empty <b>bit</b> <b>vector.</b>|$|E
2500|$|... 32 vector {{registers}} of 16 x 8-bit, 8 x 16-bit, 4 x 32-bit, and 2 x 64 <b>bit</b> <b>vector</b> elements ...|$|E
50|$|The pattern {{matching}} algorithms used by various TUTOR implementations varied in detail, but typically, each {{word in the}} input text and each word in the pattern were converted to <b>bit</b> <b>vectors.</b> To see whether a word of student input matched a word of the pattern, the Hamming distance between the two <b>bit</b> <b>vectors</b> {{was used as a}} measure of the degree of difference between the words. <b>Bit</b> <b>vectors</b> were typically 60 or 64 bits long, with fields for letter presence, letter pair presence, and the first letter. As a result, the number of one bits in the exclusive or of two such <b>bit</b> <b>vectors</b> approximated the extent of the phonetic difference between the corresponding words.|$|R
25|$|Naive {{set theory}} interprets Boolean {{operations}} as acting on subsets {{of a given}} set X. As we saw earlier this behavior exactly parallels the coordinate-wise combinations of <b>bit</b> <b>vectors,</b> with the union of two sets corresponding to the disjunction of two <b>bit</b> <b>vectors</b> and so on.|$|R
25|$|The set {0,1} and its Boolean {{operations}} as treated above {{can be understood}} as the special case of <b>bit</b> <b>vectors</b> of length one, which by the identification of <b>bit</b> <b>vectors</b> with subsets can also be understood as the two subsets of a one-element set. We call this the prototypical Boolean algebra, justified by the following observation.|$|R
2500|$|Example 2. All bit vectors {{of a given}} length form a Boolean algebra [...] "pointwise", {{meaning that}} any n-ary Boolean {{operation}} {{can be applied to}} n bit vectors one bit position at a time. For example, the ternary OR of three bit vectors each of length 4 is the <b>bit</b> <b>vector</b> of length 4 formed by oring the three bits {{in each of the four}} bit positions, thus 0100∨1000∨1001= 1101. Another example is the truth tables above for the n-ary operations, whose columns are all the bit vectors of length 2n and which therefore can be combined pointwise whence the n-ary operations form a Boolean algebra.|$|E
2500|$|Example 3. The [...] {{power set}} algebra, the set 2W of all subsets {{of a given}} set W. This is just Example 2 in disguise, with W serving to index the bit {{positions}}. Any subset X of W {{can be viewed as}} the <b>bit</b> <b>vector</b> having 1's in just those bit positions indexed by elements of X. Thus the all-zero vector is the empty subset of W while the all-ones vector is W itself, these being the constants 0 and 1 respectively of the power set algebra. The counterpart of disjunction x∨y is union X∪Y, while that of conjunction x∧y is intersection X∩Y. Negation ¬x becomes ~X, complement relative to W. There is also set difference X∖Y= X∩~Y, symmetric difference (X∖Y)∪(Y∖X), ternary union X∪Y∪Z, and so on. The atoms here are the singletons, those subsets with exactly one element.|$|E
50|$|In {{the full}} <b>bit</b> <b>vector</b> format, for each {{possible}} cache line in memory, a bit {{is used to}} track whether every individual processor has that line stored in its cache. The full <b>bit</b> <b>vector</b> format is the simplest structure to implement, but the least scalable. The SGI Origin 2000 uses a combination of full <b>bit</b> <b>vector</b> and coarse <b>bit</b> <b>vector</b> depending {{on the number of}} processors.|$|E
5000|$|In {{implementation}} of some succinct data structures like <b>bit</b> <b>vectors</b> and wavelet trees.|$|R
40|$|A {{common problem}} in {{computer}} science is how to efficiently store sets: when given a set, how do you store it {{so that you do}} not use much space and membership queries can be done quickly? One popular method is to use <b>bit</b> <b>vectors.</b> We use a model data structure that is a variant of <b>bit</b> <b>vectors,</b> the <b>bit</b> probe data structure, to demonstrate lower and upper bounds for <b>bit</b> <b>vectors</b> and similar structures. This thesis presents a survey of known results from literature, contributes some new proofs that have not appeared before, and gives complete proofs of known results that are missing from literature...|$|R
50|$|Hardware {{description}} languages such as VHDL, Verilog, and SystemVerilog natively support <b>bit</b> <b>vectors</b> {{as these}} are used to model storage elements like flip-flops, hardware busses and hardware signals in general. In hardware verification languages such as OpenVera, e and SystemVerilog, <b>bit</b> <b>vectors</b> are used to sample values from the hardware models, and to represent data that is transferred to hardware during simulations.|$|R
50|$|From this <b>bit</b> <b>vector</b> viewpoint, a {{concrete}} Boolean algebra {{can be defined}} equivalently as a nonempty set of bit vectors all of the same length (more generally, indexed by the same set) and closed under the <b>bit</b> <b>vector</b> operations of bitwise ∧, ∨, and ¬, as in 1010∧0110 = 0010, 1010∨0110 = 1110, and ¬1010 = 0101, the <b>bit</b> <b>vector</b> realizations of intersection, union, and complement respectively.|$|E
50|$|In {{this case}} the {{directory}} entry uses 1 bit {{for a group of}} processors for each cache line. For the same example as Full <b>Bit</b> <b>Vector</b> format if we consider 1 bit for 8 processors as a group, then the storage overhead will be 128/(32×8)=50%. This is a significant improvement over the Full <b>Bit</b> <b>Vector</b> format.|$|E
5000|$|If Jaccard or Tanimoto {{similarity}} {{is expressed}} over a <b>bit</b> <b>vector,</b> {{then it can}} be written as ...|$|E
40|$|<b>Bit</b> <b>vectors</b> and <b>bit</b> {{operations}} are proposed for efficient propositional inference. Bit arithmetic has efficient {{software and hardware}} implementations, which can be put to advantage in Boolean satisability procedures. Sets of variables are represented as <b>bit</b> <b>vectors</b> and formulæ as matrices. Symbolic {{operations are}} performed by bit arithmetic. As examples of inference done in this fashion, we describe ground resolution and ground completion...|$|R
5000|$|The Blitter in the Commodore Amiga is {{an early}} (circa 1985) {{graphics}} processor capable of combining three source streams of 16 component <b>bit</b> <b>vectors</b> in 256 ways to produce an output stream consisting of 16 component <b>bit</b> <b>vectors.</b> Total input stream bandwidth is up to 42 million bits per second. Output stream bandwidth is up to 28 million bits per second.|$|R
40|$|N-gram {{encoding}} for strings provides good inexact matching performance, but pays a {{large storage}} penalty. We describe {{a method for}} superimposed N-gram coding that provides more than a 40 - 60 % reduction in the storage space required over the typical N-gram implementation while giving good speed and matching performance. Our system uses <b>bit</b> <b>vectors</b> to represent the occurrence of sets of N-grams in lexicon strings. The space savings comes from the notion of superimposing sets of N-grams onto single <b>bit</b> <b>vectors.</b> We also discuss {{an extension of the}} superimposed coding idea which encodes every N-gram with an ensemble of <b>bit</b> <b>vectors</b> {{in such a way as}} to yield even greater space savings. 1...|$|R
5000|$|... 32 vector {{registers}} of 16 x 8-bit, 8 x 16-bit, 4 x 32-bit, and 2 x 64 <b>bit</b> <b>vector</b> elements ...|$|E
5000|$|Ada and VHDL enclose {{hexadecimal}} numerals in based [...] "numeric quotes": [...] For <b>bit</b> <b>vector</b> constants VHDL {{uses the}} notation [...]|$|E
50|$|System.CollectionsSpecialized: Defines {{specialized}} and strongly-typed collections; for example, a {{linked list}} dictionary, a <b>bit</b> <b>vector,</b> and collections that contain only strings.|$|E
40|$|Abstract — Mining {{frequent}} item sets in a {{data set}} {{is a significant}} problem of data mining {{that can only be}} solved in exponential time. For especially very large data sets, finding frequent item sets in practical run times is extremely important. A market basket data set can be represented by a set of <b>bit</b> <b>vectors,</b> which enables fast computation and low memory requirements. Space requirements can be reduced and better run time results can be obtained if <b>bit</b> <b>vectors</b> can be compressed, and binary operations can be applied on compressed <b>bit</b> <b>vectors.</b> In this paper, we study {{the advantages and disadvantages of}} using compression techniques for finding frequent item sets. Experimental evaluations clearly show that applying special purpose compression techniques has many benefits on a wide range of data sets. 1...|$|R
3000|$|... is the set {{containing}} the <b>bit</b> <b>vectors</b> {{corresponding to the}} Ncand candidates closer, in a Euclidean sense, to the received samples, i.e.|$|R
40|$|The {{study of}} {{compressed}} data structures strives to represent {{information on a}} computer concisely — using as little space as possible. Compressed <b>bit</b> <b>vectors</b> are the simplest compressed data structure. They are used {{as a basis for}} more complex data structures with applications in, for example, computational biology. Functional programming is a programming paradigm that represents computation using functions without side-effects (such as mutation). Data structures that are representable in and suitable for functional programming are termed functional data structures. Functional data structures are also persistent: operations on them do not destroy previous versions. This thesis provides implementations of functional compressed <b>bit</b> <b>vectors</b> in the purely functional programming language Haskell. The implemented structures are analyzed and benchmarked against established imperative (C++) implementations. Applications of compressed <b>bit</b> <b>vectors</b> are also surveyed. This includes compressed wavelet trees, an implementation of which is also presented...|$|R
5000|$|An {{array of}} [...] "base indexes", {{one for each}} {{consecutive}} subsequence of 64 bits in the <b>bit</b> <b>vector,</b> pointing to the first datum associated with a nonzero bit in that subsequence.|$|E
5000|$|This {{algorithm}} {{may also}} be used to eliminate duplicate keys, by replacing the [...] array with a <b>bit</b> <b>vector</b> that stores a [...] for a key that is present in the input and a [...] for a key that is not present. If additionally the items are the integer keys themselves, both second and third loops can be omitted entirely and the <b>bit</b> <b>vector</b> will itself serve as output, representing the values as offsets of the non- entries, added to the range's lowest value. Thus the keys are sorted and the duplicates are eliminated in this variant just by being placed into the bit array.|$|E
5000|$|An {{array of}} 16-bit words for each nonzero {{bit in the}} <b>bit</b> <b>vector.</b> Each datum either {{supplies}} an index that points to the second-level data structure object for the corresponding prefix, or supplies the routing information for that prefix directly.|$|E
5000|$|To generalize {{to where}} X and Y are not single bits, but instead <b>bit</b> <b>vectors</b> of length n, these 2&times;2 {{matrices}} {{are replaced by}} 2n&times;2n block matrices such as ...|$|R
30|$|Unlike {{optimization}} problems, {{where the}} possible {{solutions to the}} problem can be represented by vectors with real values, the candidate solutions to the feature selection problem are represented by <b>bit</b> <b>vectors.</b>|$|R
50|$|Another {{possibility}} {{is to use}} hash sets. Each document is represented by a hash table which contains every single term of that document. Since Hash-table size increases and decreases in real time with the addition and removal of terms, each document will occupy much less space in memory. However, {{it will have a}} slowdown in performance because the operations are more complex than with <b>bit</b> <b>vectors.</b> On the worst-case performance can degrade from O(n) to O(n2). On the average case, the performance slowdown will not be that much worse than <b>bit</b> <b>vectors</b> and the space usage is much more efficient.|$|R
5000|$|... {{where the}} same {{calculation}} {{is expressed in}} terms of vector scalar product and magnitude. This representation relies on the fact that, for a <b>bit</b> <b>vector</b> (where the value of each dimension is either 0 or 1) then [...] and [...]|$|E
50|$|The C-- type {{system is}} {{deliberately}} designed to reflect constraints imposed by hardware rather than conventions imposed by higher-level languages. In C-- a value {{stored in a}} register or memory may have only one type: <b>bit</b> <b>vector.</b> However, <b>bit</b> <b>vector</b> is a polymorphic type and may come in several widths, e.g., bits8, bits32, or bits64. In addition to the bit-vector type C-- also provides a Boolean type bool, which can be computed by expressions and used for control flow but cannot be stored in a register or in memory. As in an assembly language, any higher type discipline, such as distinctions between signed, unsigned, float, and pointer, is imposed by the C-- operators or other syntactic constructs in the language.|$|E
5000|$|At the transmitter, {{information}} bits are encoded. Encoding adds redundancy by mapping the {{information bits}} [...] to a longer <b>bit</b> <b>vector</b> - the code <b>bit</b> <b>vector</b> [...] The encoded bits [...] are then interleaved. Interleaving permutes {{the order of}} the code bits [...] resulting in bits [...] The main reason for doing this is to insulate the information bits from bursty noise. Next, the symbol mapper maps the bits [...] into complex symbols [...] These digital symbols are then converted into analog symbols with a D/A converter. Typically the signal is then up-converted to pass band frequencies by mixing it with a carrier signal. This is a necessary step for complex symbols. The signal is then ready to be transmitted through the channel.|$|E
50|$|That is, x is in {{language}} L if {{and only if}} there exist |r| binary vectors, where for all random <b>bit</b> <b>vectors</b> r, TM M accepts at least one random vector ⊕ ti.|$|R
40|$|Abstract — Instruction queues {{consume a}} {{significant}} amount of power in high-performance processors, primarily due to instruction wakeup logic access to the queue structures. The wakeup logic delay is also a critical timing parameter. This paper proposes a new queue organization using a small number of successor pointers plus a small number of dynamically allocated full successor <b>bit</b> <b>vectors</b> for cases with a larger number of successors. The details of the new organization are described and it is shown to achieve the performance of CAM-based or full dependency matrix organizations using just one pointer per instruction plus eight full <b>bit</b> <b>vectors.</b> Only two full <b>bit</b> <b>vectors</b> are needed when two successor pointers are stored per instruction. Finally, a design and pre-layout of all critical structures in 70 nm technology was performed for the proposed organization as well as for a CAM-based baseline. The new design is shown to use 1 / 2 to 1 / 5 th of the baseline instruction queue power, depending on queue size. It is also shown to use significantly less power than the full dependency matrix based design...|$|R
25|$|The {{atoms of}} such an algebra are the <b>bit</b> <b>vectors</b> {{containing}} exactly one 1. In general the atoms of a Boolean algebra are those elements x such that x∧y has only two possible values, x or 0.|$|R
