4789|1721|Public
25|$|Training a {{neural network}} model {{essentially}} means selecting one model from the set of allowed models (or, in a <b>Bayesian</b> <b>framework,</b> determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training {{neural network model}}s. Most {{of them can be}} viewed as a straightforward application of optimization theory and statistical estimation.|$|E
25|$|This {{contrasts}} with the Bayesian approach, which requires that the hypothesis be assigned a prior probability, which is revised {{in the light of}} the observed data to obtain the final probability of the hypothesis. Within the <b>Bayesian</b> <b>framework</b> there is no risk of error since hypotheses are not accepted or rejected; instead they are assigned probabilities.|$|E
25|$|Applications whose goal is {{to create}} a system that generalizes well to unseen examples, face the {{possibility}} of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (<b>Bayesian)</b> <b>framework,</b> where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the {{goal is to}} minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.|$|E
50|$|We can {{see that}} <b>Bayesian</b> {{evidence}} <b>framework</b> is a unified theory for learning the model and model selection.Kwok used the <b>Bayesian</b> evidence <b>framework</b> to interpret the formulation of SVM and model selection. And he also applied <b>Bayesian</b> evidence <b>framework</b> to support vector regression.|$|R
40|$|To achieve {{integrated}} segmentation {{and recognition}} in complex scenes, the model-based approach has widely been {{accepted as a}} promising paradigm. However, the performance is still far from satisfactory when the target object is highly deformed {{and the level of}} outlier contamination is high. In this paper, we first describe two <b>Bayesian</b> <b>frameworks,</b> one for classifying input patterns and another for detecting target patterns in complex scenes using deformable models. Then, we show that the two frameworks are similar to the forward-reverse setting of Hausdorff matching and that their matching and discriminating properties are complementary to each other. By properly combining the two frameworks, we propose a new matching scheme called bidirectional matching. This combined approach inherits the advantages of the two <b>Bayesian</b> <b>frameworks.</b> In particular, we have obtained encouraging empirical results on shape-based pattern extraction, using a subset of the CEDAR handwriting database containing handwritten words of highly varying shape. Index Terms: Model-based segmentation, deformable models, Bayesian inference, bidirectional matching, Hausdorff matching. An abridged version of this paper was published in [1]. Dr. Kwok-Wai Cheung is the correspondence author. 1...|$|R
5000|$|GeneRecon - {{software}} for the fine-scale mapping of linkage disequilibrium mapping of disease genes using coalescent theory based on an <b>Bayesian</b> MCMC <b>framework.</b>|$|R
25|$|Phylogenetic trees {{composed}} with a nontrivial number of input sequences {{are constructed using}} computational phylogenetics methods. Distance-matrix methods such as neighbor-joining or UPGMA, which calculate genetic distance from multiple sequence alignments, are simplest to implement, but do not invoke an evolutionary model. Many sequence alignment methods such as ClustalW also create trees by using the simpler algorithms (i.e. those based on distance) of tree construction. Maximum parsimony is another simple method of estimating phylogenetic trees, but implies an implicit model of evolution (i.e. parsimony). More advanced methods use the optimality criterion of maximum likelihood, often within a <b>Bayesian</b> <b>Framework,</b> and apply an explicit model of evolution to phylogenetic tree estimation. Identifying the optimal tree using many of these techniques is NP-hard, so heuristic search and optimization methods are used in combination with tree-scoring functions to identify a reasonably good tree that fits the data.|$|E
2500|$|The Bayesian {{analysis}} of genetic sequences may confer greater robustness to model misspecification. [...] MrBayes allows inference of ancestral states at ancestral nodes using the full hierarchical Bayesian approach. [...] The PREQUEL program {{distributed in the}} PHAST package performs comparative evolutionary genomics using ancestral sequence reconstruction. SIMMAP [...] stochastically maps mutations on phylogenies. BayesTraits analyses discrete or continuous characters in a <b>Bayesian</b> <b>framework</b> to evaluate models of evolution, reconstruct ancestral states, and detect correlated evolution between pairs of traits.|$|E
50|$|Lewis (2013) {{estimates}} {{by using the}} <b>Bayesian</b> <b>framework</b> that the equilibrium climate sensitivity is 1.6 K, with the likely range (90% confidence level) 1.2-2.2 K.|$|E
30|$|Restoration of blurred {{and noisy}} images is a {{classical}} problem arising in many applications, including astronomy, biomedical imaging, and computerized tomography [1]. This problem aims to invert the degradation {{because of a}} capture device, but the underlying process is mathematically ill posed and leads to a highly noise sensitive solution. A large number of techniques {{have been developed to}} cope with this issue, most of them under the regularization or the <b>Bayesian</b> <b>frameworks</b> (a complete review can be found in [2 – 4]).|$|R
40|$|Markov random fields (MRF's) {{have been}} widely used to model images in <b>Bayesian</b> <b>frameworks</b> for image {{reconstruction}} and restoration. Typically, these MRF models have parameters that allow the prior model to be adjusted for best performance. However, optimal estimation of these parameters (sometimes referred to as hyperparameters) is difficult in practice for two reasons: i) direct parameter estimation for MRF's is known to be mathematically and numerically challenging; ii) parameters can not be directly estimated because the true image cross section is unavailable...|$|R
40|$|A {{probabilistic}} approach to build models for paired comparison experiments {{based on the}} comparison of two Pareto variables is considered. Analysis of the proposed model is carried out in classical as well as <b>Bayesian</b> <b>frameworks.</b> Informative and uninformative priors are employed to accommodate the prior information. Simulation study is conducted to assess the suitablily and performance of the model under theoretical conditions. Appropriateness of fit of the is also carried out. Entire inferential procedure is illustrated by comparing certain cricket teams using real dataset. </p...|$|R
50|$|This table {{includes}} {{some of the}} most common phylogenetic software used for inferring phylogenies under a <b>Bayesian</b> <b>framework.</b> Some of them do not use exclusively Bayesian methods.|$|E
5000|$|Maximum {{conditional}} independence: if {{the hypothesis}} can be {{cast in a}} <b>Bayesian</b> <b>framework,</b> try to maximize conditional independence. This is the bias used in the Naive Bayes classifier.|$|E
5000|$|He {{has also}} done {{research}} into the perception of visual texture and orientation. Much of his work uses a normative <b>Bayesian</b> <b>framework</b> of perception, which posits that human behavior approximates Bayesian inference ...|$|E
50|$|G. Marrelec, P. Ciuciu, M. Pélégrini-Issac, H. Benali: Estimation of the hemodyamic {{response}} function in event-related functional MRI: directed acyclic graphs {{for a general}} <b>Bayesian</b> inference <b>framework.</b>|$|R
50|$|<b>Bayesian</b> <b>frameworks</b> are a form {{of maximum}} likelihood-based {{analyses}} and can be very effective in cross-species transmission studies. Bayesian inference of character evolution methods can account for phylogenetic tree uncertainty and more complex scenarios, with models such as the character diffusion model currently being developed {{for the study of}} CST in RNA viruses. A Bayesian statistical approach presents advantages over other analyses for tracking CST origins. Computational techniques allow integration over an unknown phylogeny, which cannot be directly observed, and unknown migration process, which is usually poorly understood.|$|R
40|$|Approximate Bayesian Gaussian process (GP) {{classification}} {{techniques are}} powerful nonparametric learning methods, similar in appearance and performance to Support Vector machines. Based on simple probabilistic models, they render interpretable results {{and can be}} embedded in <b>Bayesian</b> <b>frameworks</b> for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999), we prove distributionfree generalization error bounds {{for a wide range}} of approximate Bayesian GP classification techniques. We instantiate and test these bounds for two particular GPC techniques, including a sparse method which circumvents the unfavourable scaling of standard GP algorithms...|$|R
5000|$|The typical {{model for}} a {{computer}} code output is a Gaussian process. For notational simplicity, assume [...] is a scalar. Owing to the <b>Bayesian</b> <b>framework,</b> we fix our belief that the function [...] follows a Gaussian process, ...|$|E
5000|$|Balakrishnan S., Roy A., Ierapetritou M.G., Flach G.P. and Georgopoulos P.G. (2003). Uncertainty {{reduction}} and characterization for complex environmental fate and transport models: An empirical <b>Bayesian</b> <b>framework</b> incorporating the stochastic response surface method. Water Resources Research 39 (12): 1350.|$|E
5000|$|One {{issue with}} adjoint-based {{gradients}} in CFD {{is that they}} can be particularly noisy. [...] When derived in a <b>Bayesian</b> <b>framework,</b> GEK allows one to incorporate not only the gradient information, but also the uncertainty in that gradient information.|$|E
40|$|We {{propose a}} <b>Bayesian</b> {{evidence}} <b>framework</b> to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated {{on top of}} a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed <b>Bayesian</b> evidence <b>framework</b> also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our <b>Bayesian</b> evidence <b>framework</b> for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency. Comment: Appearing in CVPR- 2016 (oral presentation...|$|R
40|$|Building on the {{previous}} work of Deb, Munkin and Trivedi (2006), this paper develops an extended specification of a two-part model with endogenous regressors. The model is {{used to analyze the}} effect of alternative supplemental insurance choices on the prescription drug expenditure of the elderly, using a linked dataset based on the Medicare Current Beneficiary Survey (MCBS) data for 2003 - 2004. The econometric analysis is conducted in both the GMM and <b>Bayesian</b> <b>frameworks,</b> allowing us to compare the results and discuss the differences. We estimate the treatment effects for different counterfactuals and find significant evidence of endogeneity in plan choice...|$|R
40|$|We {{study and}} develop two <b>Bayesian</b> <b>frameworks</b> for {{supervised}} dimension reduction {{that apply to}} nonlinear manifolds: Bayesian mixtures of inverse regressions and gradient based methods. Formal probabilistic models with likelihoods and priors are given for both methods and efficient posterior estimates of the effective dimension reduction space and predictive factors {{can be obtained by}} a Gibbs sampling procedure. In the case of the gradient based methods estimates of conditional dependence between covariates predictive of the response can also be inferred. Relations to manifold learning and Bayesian factor models are made explicit. The utility of the approach is illustrated on simulated and real examples...|$|R
50|$|In a <b>Bayesian</b> <b>framework,</b> we use Bayes' Theorem {{to predict}} the Kriging mean and {{covariance}} conditional on the observations. When using GEK, the observations are usually {{the results of a}} number of computer simulations. GEK can be interpreted as a form of Gaussian process regression.|$|E
5000|$|Before {{show the}} <b>Bayesian</b> <b>framework,</b> the paper use {{an example to}} mention why we choose Bayesian instead of {{frequency}} approach, such that we can propose some posterior of prior distribution on the soft-label [...] We assume each [...] is drawn from a known Beta prior: ...|$|E
50|$|To {{address these}} disadvantages, {{non-linear}} estimators based on Bayesian theory have been developed. In the <b>Bayesian</b> <b>framework,</b> {{it has been}} recognized that a successful denoising algorithm can achieve both noise reduction and feature preservation if it employs an accurate statistical description of the signal and noise components.|$|E
40|$|The catnet {{package for}} R {{implements}} a categorical <b>Bayesian</b> network inference <b>framework.</b> <b>Bayesian</b> networks are graphical statistical models representing causal dependencies between random variables. A Bayesian network has two components: Directed Acyclic Graph (DAG) with nodes {{the variables of}} interest and a probability structure given {{as a set of}} conditional distributions, one for each node i...|$|R
40|$|Abstract. Simple {{analytic}} expressions {{are derived}} for the sensitivity curve of a pulsar timing array (PTA) to both a monochromatic source of gravitational waves and an isotropic stochastic background of gravitational waves. These derivations are performed in both frequentist and <b>Bayesian</b> <b>frameworks</b> {{and the results}} verified with numerical injections and recovery in mock PTA datasets. The consistency of the frequentist and Bayesian approaches is demonstrated {{and the results are}} used to emphasise the fact that the sensitivity curve of a PTA depends not only on the properties of the PTA itself, but also on the properties of the gravitational wave source being observed. 1...|$|R
40|$|Human {{intelligence}} has long inspired new benchmarks {{for research in}} artificial intelligence. However, recently, research in machine learning and AI has influenced research on children’s learning. In particular, <b>Bayesian</b> <b>frameworks</b> capture hallmarks of children’s causal reasoning: given causally ambiguous evidence, prior beliefs and data interact. However, we suggest that the rational frameworks that support rapid, accurate causal learning can actually lead children to generate and maintain incorrect beliefs. In this paper we present three studies demonstrating these surprising misunderstandings in children and show how these errors in fact reflect sophisticated inferences. Ambiguity and Rational Models Even if the causal structure of the world wer...|$|R
50|$|Aldrin et al. (2012) use {{a simple}} {{deterministic}} climate model, modelling yearly hemispheric surface temperature and global ocean heat content {{as a function}} of historical radiative forcing and combine it with an empirical, stochastic model. By using a <b>Bayesian</b> <b>framework</b> they estimate the equilibrium climate sensitivity to be 1.98 C.|$|E
50|$|Prediction is done using a {{completely}} <b>Bayesian</b> <b>framework.</b> The universal prior is calculated for all computable sequences—this is the universal {{a priori probability}} distribution;no computable hypothesis will have a zero probability. This means that Bayes rule of causation {{can be used in}} predicting the continuation of any particular computable sequence.|$|E
50|$|In {{contrast}} to other approaches, FABIA is a multiplicative model that assumes realistic non-Gaussian signal distributions with heavy tails. FABIA utilizes well understood model selection techniques like variational approaches and applies the <b>Bayesian</b> <b>framework.</b> The generative framework allows FABIA {{to determine the}} information content of each bicluster to separate spurious biclusters from true biclusters.|$|E
40|$|In the {{simultaneous}} estimation of {{a mean of}} a multivariate normal distribution, Charles Stein discovered the surprising decision-theoretic result that the usual maximum likelihood estimator is inadmissible with respect to quadratic loss in three or more dimensions. Since then, the researches on this Stein phenomenon have received considerable attention. This paper surveys the theoretical study of the Stein phenomenon. The minimaxity of the James-Stein estimator and its improvements are demonstrated instructively, and various extensions and developments in <b>Bayesian</b> <b>frameworks</b> and non-normal distributions are reviewed. The paper shortly refers to the Stein phenomenon in confidence sets {{and a series of}} decision-theoretic results in estimation of a covariance matrix. ...|$|R
40|$|This paper {{describes}} the mixtures-of-trees model, a probabilistic model for discrete multidimensional domains. Mixtures-of-trees generalize the probabilistic trees of Chow and Liu (1968) {{in a different}} and complementary direction to that of Bayesian networks. We present efficient algorithms for learning mixtures-of-trees models in maximum likelihood and <b>Bayesian</b> <b>frameworks.</b> We also discuss additional efficiencies that can be obtained when data are "sparse," and we present data structures and algorithms that exploit such sparseness. Experimental results demonstrate {{the performance of the}} model for both density estimation and classification. We also discuss the sense in which tree-based classifiers perform an implicit form of feature selection, and demonstrate a resulting insensitivity to irrelevant attributes...|$|R
5000|$|From a {{philosophical}} perspective, the loss {{function in a}} regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting [...] in place of , the likelihood function measures how likely the observations are from the model that {{was assumed to be}} true in the generative process. From a mathematical perspective, however, the formulations of the regularization and <b>Bayesian</b> <b>frameworks</b> make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions [...] that approximate the labels [...] as much as possible.|$|R
