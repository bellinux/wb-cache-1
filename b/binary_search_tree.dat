631|5849|Public
5|$|The Euclidean {{algorithm}} {{can be used}} {{to arrange}} the set of all positive rational numbers into an infinite <b>binary</b> <b>search</b> <b>tree,</b> called the Stern–Brocot tree.|$|E
5|$|There are {{numerous}} variations of binary search. In particular, fractional cascading speeds up binary {{searches for the}} same value in multiple arrays, efficiently solving a series of search problems in computational geometry and numerous other fields. Exponential search extends binary search to unbounded lists. The <b>binary</b> <b>search</b> <b>tree</b> and B-tree data structures are based on binary search.|$|E
5|$|A <b>binary</b> <b>search</b> <b>tree</b> is {{a binary}} tree data {{structure}} that works {{based on the}} principle of binary search. The records of the tree are arranged in sorted order, and each record in the tree can be searched using an algorithm similar to binary search, taking on average logarithmic time. Insertion and deletion also require on average logarithmic time in binary search trees. This can faster than the linear time insertion and deletion of sorted arrays, and binary trees retain the ability to perform all the operations possible on a sorted array, including range and approximate queries.|$|E
500|$|However, <b>binary</b> <b>search</b> {{is usually}} more {{efficient}} for <b>searching</b> as <b>binary</b> <b>search</b> <b>trees</b> {{will most likely}} be imperfectly balanced, resulting in slightly worse performance than <b>binary</b> <b>search.</b> This applies even to balanced <b>binary</b> <b>search</b> <b>trees,</b> <b>binary</b> <b>search</b> <b>trees</b> that balance their own nodes—as they rarely produce optimally-balanced trees—but to a lesser extent. Although unlikely, the tree may be severely imbalanced with few internal nodes with two children, resulting in the average and worst-case search time approaching [...] comparisons. <b>Binary</b> <b>search</b> <b>trees</b> take more space than sorted arrays.|$|R
40|$|We {{present a}} new linear time {{heuristic}} for constructing <b>binary</b> <b>search</b> <b>trees.</b> The {{analysis of the}} algorithm, by establishing an upper bound {{on the cost of}} the produced <b>binary</b> <b>search</b> <b>trees,</b> permits to derive a limitation on the cost of optimal <b>binary</b> <b>search</b> <b>trees.</b> The obtained upper bound improve on previous results...|$|R
5|$|<b>Binary</b> <b>search</b> <b>trees</b> lend {{themselves}} to fast searching in external memory stored in hard disks, as <b>binary</b> <b>search</b> <b>trees</b> can effectively be structured in filesystems. The B-tree generalizes this method of tree organization; B-trees are frequently used to organize long-term storage such as databases and filesystems.|$|R
25|$|In {{applications}} of <b>binary</b> <b>search</b> <b>tree</b> data structures, {{it is rare}} for the values in the tree to be inserted without deletion in a random order, limiting the direct {{applications of}} random binary trees. However, algorithm designers have devised data structures that allow insertions and deletions to be performed in a <b>binary</b> <b>search</b> <b>tree,</b> at each step maintaining as an invariant the property that {{the shape of the}} tree is a random variable with the same distribution as a random <b>binary</b> <b>search</b> <b>tree.</b>|$|E
25|$|In {{computer}} science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. Two different distributions are commonly used: binary trees formed by inserting nodes {{one at a}} time according to a random permutation, and binary trees chosen from a uniform discrete distribution in which all distinct trees are equally likely. It is also possible to form other distributions, for instance by repeated splitting. Adding and removing nodes directly in a random binary tree will in general disrupt its random structure, but the treap and related randomized <b>binary</b> <b>search</b> <b>tree</b> data structures use the principle of binary trees formed from a random permutation in order to maintain a balanced <b>binary</b> <b>search</b> <b>tree</b> dynamically as nodes are inserted and deleted.|$|E
25|$|Searching {{is similar}} to searching a <b>binary</b> <b>search</b> <b>tree.</b> Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range {{includes}} the search value. A subtree's range {{is defined by the}} values, or keys, contained in its parent node. These limiting values are also known as separation values.|$|E
5000|$|... #Caption: <b>Binary</b> <b>search</b> <b>trees</b> are <b>searched</b> {{using an}} {{algorithm}} similar to <b>binary</b> <b>search.</b>|$|R
40|$|AbstractWe {{present a}} {{detailed}} study of left–right-imbalance measures for random <b>binary</b> <b>search</b> <b>trees</b> under the random permutation model, i. e., where <b>binary</b> <b>search</b> <b>trees</b> are generated by random permutations of { 1, 2,…,n}. For random <b>binary</b> <b>search</b> <b>trees</b> of size n we study (i) {{the difference between the}} left and the right depth of a randomly chosen node, (ii) the difference between {{the left and the right}} depth of a specified node j=j(n), and (iii) the difference between the left and the right pathlength, and show for all three imbalance measures limiting distribution results...|$|R
50|$|Compared to hash tables, these {{structures}} have both advantages and weaknesses. The worst-case performance of self-balancing <b>binary</b> <b>search</b> <b>trees</b> is {{significantly better than}} that of a hash table, with a time complexity in big O notation of O(log n). This is in contrast to hash tables, whose worst-case performance involves all elements sharing a single bucket, resulting in O(n) time complexity. In addition, and like all <b>binary</b> <b>search</b> <b>trees,</b> self-balancing <b>binary</b> <b>search</b> <b>trees</b> keep their elements in order. Thus, traversing its elements follows a least-to-greatest pattern, whereas traversing a hash table can result in elements being in seemingly random order. However, hash tables have a much better average-case time complexity than self-balancing <b>binary</b> <b>search</b> <b>trees</b> of O(1), and their worst-case performance is highly unlikely when a good hash function is used.|$|R
25|$|Although it is {{a binary}} tree (each vertex has two children), the Calkin–Wilf tree {{is not a}} binary search tree: its inorder does not {{coincide}} with the sorted order of its vertices. However, it {{is closely related to}} a different <b>binary</b> <b>search</b> <b>tree</b> on the same set of vertices, the Stern–Brocot tree: the vertices at each level of the two trees coincide, and are related to each other by a bit-reversal permutation.|$|E
25|$|One case {{in which}} peek is not trivial is in an ordered list type (i.e., {{elements}} accessible in order) implemented by a self-balancing <b>binary</b> <b>search</b> <b>tree.</b> In this case find-min or find-max take O(log n) time, as does access to any other element. Making find-min or find-max take O(1) time {{can be done by}} caching the min or max values, but this adds overhead to the data structure and to the operations of adding or removing elements.|$|E
25|$|In {{computer}} science, a trie, {{also called}} digital tree and sometimes radix tree or prefix tree (as {{they can be}} searched by prefixes), {{is a kind of}} search tree—an ordered tree data structure that is used to store a dynamic set or associative array where the keys are usually strings. Unlike a <b>binary</b> <b>search</b> <b>tree,</b> no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are not necessarily associated with every node. Rather, values tend only to be associated with leaves, and with some inner nodes that correspond to keys of interest. For the space-optimized presentation of prefix tree, see compact prefix tree.|$|E
40|$|Abstract. We {{present a}} {{detailed}} study of left-right-imbalance measures for random <b>binary</b> <b>search</b> <b>trees</b> under the random permutation model, i. e., where <b>binary</b> <b>search</b> <b>trees</b> are generated by random permutations of { 1, 2, [...] ., n}. For random <b>binary</b> <b>search</b> <b>trees</b> of size n we study (i) {{the difference between the}} left and the right depth of a randomly chosen node, (ii) the difference between {{the left and the right}} depth of a specified node j = j(n), and (iii) the difference between the left and the right pathlength, and show for all three imbalance measures limiting distribution results. 1...|$|R
5000|$|A Note on the Horton-Strahler Number for Random <b>Binary</b> <b>Search</b> <b>Trees,</b> (1999) ...|$|R
40|$|We {{design an}} {{efficient}} sublinear time parallel construction of optimal <b>binary</b> <b>search</b> <b>trees.</b> The {{efficiency of the}} parallel algorithm corresponds to its total work (the product time Θ processors). Our algorithm works in O(n 1 log n) time with the total work O(n 2 + 2 ffl), for an arbitrarily small constant 0 ! ffl 1 2. This is optimal within a factor n 2 ffl {{with respect to the}} best known sequential algorithm given by Knuth, which needs only O(n 2) time due to a monotonicity property of optimal <b>binary</b> <b>search</b> <b>trees,</b> see [6]). It is unknown how to explore this property in an efficient NC construction of <b>binary</b> <b>search</b> <b>trees.</b> Here we show that it can be effectively used in sublinear time parallel computation. Our improvement also relies on the use (in independently processed small subcomputations) of the parallelism present in Knuth's algorithm. The best known sublinear time algorithms for the construction of <b>binary</b> <b>search</b> <b>trees</b> (as an instance of a more general pr [...] ...|$|R
25|$|For any set {{of numbers}} (or, more generally, values from some total order), one may form a <b>binary</b> <b>search</b> <b>tree</b> {{in which each}} number is {{inserted}} in sequence as a leaf of the tree, without changing {{the structure of the}} previously inserted numbers. The position into which each number should be inserted is uniquely determined by a binary search in the tree formed by the previous numbers. For instance, if the three numbers (1,3,2) are inserted into a tree in that sequence, the number 1 will sit {{at the root of the}} tree, the number 3 will be placed as its right child, and the number 2 as the left child of the number 3. There are six different permutations of the numbers (1,2,3), but only five trees may be constructed from them. That is because the permutations (2,1,3) and (2,3,1) form the same tree.|$|E
500|$|The binary {{logarithm}} {{also frequently}} {{appears in the}} analysis of algorithms, {{not only because of the}} frequent use of binary number arithmetic in algorithms, but also because binary logarithms occur {{in the analysis of}} algorithms based on two-way branching. [...] If a problem initially has [...] choices for its solution, and each iteration of the algorithm reduces the number of choices by a factor of two, then the number of iterations needed to select a single choice is again the integral part of [...] This idea is used in the analysis of several algorithms and data structures. For example, in binary search, the size of the problem to be solved is halved with each iteration, and therefore roughly [...] iterations are needed to obtain a problem of size , which is solved easily in constant time. Similarly, a perfectly balanced <b>binary</b> <b>search</b> <b>tree</b> containing [...] elements has height [...]|$|E
2500|$|If a given set of ordered {{numbers is}} {{assigned}} numeric priorities (distinct numbers unrelated to their values), these priorities {{may be used}} to construct a Cartesian tree for the numbers, a binary tree that has as its inorder traversal sequence the sorted sequence of the numbers and that is heap-ordered by priorities. Although more efficient construction algorithms are known, it is helpful to think of a Cartesian tree as being constructed by inserting the given numbers into a <b>binary</b> <b>search</b> <b>tree</b> in priority order. Thus, by choosing the priorities either to be a set of independent random real numbers in the unit interval, or by choosing them to be a random permutation of the numbers from [...] to [...] (where [...] is the number of nodes in the tree), and by maintaining the heap ordering property using tree rotations after any insertion or deletion of a node, it is possible to maintain a data structure that behaves like a random <b>binary</b> <b>search</b> <b>tree.</b> Such a data structure is known as a treap or a randomized <b>binary</b> <b>search</b> <b>tree.</b>|$|E
40|$|We {{develop a}} new class of weight {{balanced}} <b>binary</b> <b>search</b> <b>trees</b> called fi-balanced <b>binary</b> <b>search</b> <b>trees</b> (fi-BBSTs). fi-BBSTs are designed to have reduced internal path length. As a result, they are expected to exhibit good search time characteristics. Individual search, insert, and delete operations in an n node fi-BBST take O(log n) time for 0 ! fi p 2 Γ 1. Experimental results comparing the performance of fi-BBSTs, WB(ff) trees, AVL-trees, red/black trees, treaps, deterministic skip lists and skip lists are presented. Two simplified versions of fi-BBSTs are also developed. Keywords and Phrases. data structures, weight balanced <b>binary</b> <b>search</b> <b>trees</b> 1 Introduction A dictionary is a set of elements on which the operations of search, insert, and delete are performed. Many data structures have been proposed for the efficient representation of a dictionary [HORO 94]. These include direct addressing schemes such as hash tables and comparison schemes such as <b>binary</b> <b>search</b> <b>trees,</b> AVL-tr [...] ...|$|R
5000|$|The GHC {{implementation}} of Haskell provides a [...] module, which implements immutable sets using <b>binary</b> <b>search</b> <b>trees.</b>|$|R
5000|$|Ternary <b>search</b> <b>tree</b> {{three-way}} radix quicksort is isomorphic to {{this data}} {{structure in the}} same way that quicksort is isomorphic to <b>binary</b> <b>search</b> <b>trees</b> ...|$|R
2500|$|Although {{not as easy}} {{to analyze}} as the average path length, {{there has also been}} much {{research}} on determining the expectation (or high probability bounds) of the length of the longest path in a <b>binary</b> <b>search</b> <b>tree</b> generated from a random insertion order. It is now known that this length, for a tree with [...] nodes, is almost surely ...|$|E
2500|$|In many cases, the n · log n {{running time}} {{is simply the}} result of {{performing}} a Θ(log n) operation n times. [...] For example, binary tree sort creates a binary tree by inserting each element of the n-sized array one by one. [...] Since the insert operation on a self-balancing <b>binary</b> <b>search</b> <b>tree</b> takes O(log n) time, the entire algorithm takes O(n log n) time.|$|E
2500|$|In {{computer}} science, a B-tree is a self-balancing tree {{data structure}} that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a <b>binary</b> <b>search</b> <b>tree</b> in that a node {{can have more}} than two children [...] Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. B-trees are {{a good example of}} a data structure for external memory. [...] It is commonly used in databases and filesystems.|$|E
5000|$|OCaml's {{standard}} library {{contains a}} [...] module, which implements a functional set data structure using <b>binary</b> <b>search</b> <b>trees.</b>|$|R
40|$|AbstractWe {{consider}} distributional recursions {{which appear}} {{in the study of}} random <b>binary</b> <b>search</b> <b>trees</b> with monomials as toll functions. This extends classical parameters as the internal path length in <b>binary</b> <b>search</b> <b>trees.</b> As our main results we derive asymptotic expansions for the moments of the random variables under consideration as well as limit laws and properties of the densities of the limit distributions. The analysis is based on the contraction method...|$|R
5000|$|Because in {{the worst}} case this {{algorithm}} must search from {{the root of the}} tree to the leaf farthest from the root, the search operation takes time proportional to the tree's height (see tree terminology). On average, <b>binary</b> <b>search</b> <b>trees</b> with [...] nodes have [...] height. However, {{in the worst}} case, <b>binary</b> <b>search</b> <b>trees</b> can have [...] height, when the unbalanced tree resembles a linked list (degenerate tree).|$|R
2500|$|To each {{execution}} of quicksort corresponds the following <b>binary</b> <b>search</b> <b>tree</b> (BST): the initial pivot {{is the root}} node; the pivot of the left half {{is the root of}} the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the {{execution of}} quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted [...] form a random permutation.|$|E
2500|$|... {{generate}} random binary {{trees with}} [...] nodes by generating a real-valued random variable [...] {{in the unit}} interval , assigning the first [...] nodes (rounded down to an integer number of nodes) to the left subtree, the next node to the root, and the remaining nodes to the right subtree, and continuing recursively in each subtree. If [...] is chosen uniformly at random in the interval, {{the result is the}} same as the random <b>binary</b> <b>search</b> <b>tree</b> generated by a random permutation of the nodes, as any node is equally likely to be chosen as root; however, this formulation allows other distributions to be used instead. For instance, in the uniformly random binary tree model, once a root is fixed each of its two subtrees must also be uniformly random, so the uniformly random model may also be generated by a different choice of distribution for [...] As Devroye and Kruszewski show, by choosing a beta distribution on [...] and by using an appropriate choice of shape to draw each of the branches, the mathematical trees generated by this process can be used to create realistic-looking botanical trees.|$|E
50|$|In {{computer}} science, a self-balancing (or height-balanced) <b>binary</b> <b>search</b> <b>tree</b> is any node-based <b>binary</b> <b>search</b> <b>tree</b> {{that automatically}} keeps its height (maximal number of levels below the root) small {{in the face}} of arbitrary item insertions and deletions.|$|E
50|$|<b>Binary</b> <b>search</b> <b>trees</b> support {{three main}} operations: {{insertion}} of elements, deletion of elements, and lookup (checking whether a key is present).|$|R
50|$|In {{the context}} of <b>binary</b> <b>search</b> <b>trees</b> a total {{preorder}} is realized most flexibly {{by means of a}} three-way comparison subroutine.|$|R
40|$|Abstract:- Cerebellar Model Articulation Controllers (CMACs) are a biologically-inspired {{neural network}} system {{suitable}} for trajectory control. Traditionally, CMACs have been implemented using hash-coding for their memory allocation, requiring static allocation of fixed amounts of memory in advance to the training of the system. This paper presents a method for implementing CMACs using <b>Binary</b> <b>Search</b> <b>Trees</b> to provide dynamic memory allocation, allowing for lower memory usage without compromising the functionality of the CMAC. Key-Words:- CMAC, <b>binary</b> <b>search</b> <b>trees,</b> dynamic memory allocation, memory algorithm...|$|R
