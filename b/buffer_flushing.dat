3|35|Public
40|$|AbstractPerformance {{analysis}} of scientific parallel applications {{is essential to}} use High Performance Computing (HPC) infrastructures efficiently. Nevertheless, collecting detailed data of large-scale parallel programs and long-running applications is infeasible due to the huge amount of performance information generated. Even though there are no technological constraints in storing Terabytes of performance data, the constant flushing of such data to disk introduces a massive overhead into the application that makes the performance measurements worthless. This paper explores the use of Event flow graphs together with wavelet analysis and EZW-encoding to provide MPI event traces that are orders of magnitude smaller while preserving accurate information on timestamped events. Our mechanism compresses the performance data online while the application runs, thus, reducing the pressure put on the I/O system due to <b>buffer</b> <b>flushing.</b> As a result, we achieve lower application perturbation, reduced performance data output, and the possibility to monitor longer application runs...|$|E
40|$|Path-based {{techniques}} {{make the}} analysis of very large Markov models feasible by trading off high computational complexity for low space complexity. Often, a drawback in these techniques {{is that they have}} to evaluate many paths in order to compute reasonably tight bounds on the exact solutions of the models. In this paper, we present a path composition algorithm to speed up path evaluation significantly. It works by quickly composing subpaths that are precomputed locally at the component level. The algorithm is computationally efficient since individual subpaths are precomputed only once, and the results are reused many times in the computation of all composed paths. To the best of our knowledge, this work is the first to propose the idea of path composition for {{the analysis of}} Markov models. A practical implementation of the algorithm makes it feasible to solve even larger models, since it helps not only in evaluating more paths faster but also in computing long paths efficiently by composing them from short ones. In addition to presenting the algorithm, we demonstrate its application and evaluate its performance in computing the reliability and availability of a large distributed information service system in the presence of fault propagation and in computing the probabilities of buffer overflow and <b>buffer</b> <b>flushing</b> in a media multicast system with varying system configurations. 1...|$|E
40|$|Microtubules {{driven by}} kinesin motors have been {{utilised}} as "molecular shuttles" in microfluidic environments with potential applications in autonomous nanoscale manipulations such as capturing, separating, and/or concentrating biomolecules. However, the conventional flow cell-based assay has difficulty in separating bound target molecules from free ones even with <b>buffer</b> <b>flushing</b> because molecular manipulations by molecular shuttles {{take place on}} a glass surface and molecular binding occurs stochastically; this {{makes it difficult to}} determine whether molecules are carried by molecular shuttles or by diffusion. To address this issue, we developed a microtubule-based transport system between two compartments connected by a single-micrometre-scale channel array that forms dynamically via pneumatic actuation of a polydimethylsiloxane membrane. The device comprises three layers - a control channel layer (top), a microfluidic channel layer (middle), and a channel array layer (bottom) - that enable selective injection of assay solutions into a target compartment and dynamic formation of the microchannel array. The pneumatic channel also serves as a nitrogen supply path to the assay area, which reduces photobleaching of fluorescently labelled microtubules and deactivation of kinesin by oxygen radicals. The channel array suppresses cross-contamination of molecules caused by diffusion or pressure-driven flow between compartments, facilitating unidirectional transport of molecular shuttles from one compartment to another. The method demonstrates, for the first time, efficient and unidirectional microtubule transport by eliminating diffusion of target molecules on a chip and thus may constitute one of the key aspects of motor-driven nanosystems...|$|E
40|$|Rotating-tube {{electrophoresis}} apparatus employs rotating jet of eluting buffer {{to reduce}} effects of convection during separation. Designed {{for separation of}} microorganisms and biological species, system combines gravity/gradient compensating of lumen with <b>buffer</b> <b>flush</b> at fraction outlet to increase separation efficiency...|$|R
5000|$|Performance {{improvements}} in various areas, including <b>buffer</b> pool <b>flushing,</b> execution of {{certain types of}} SQL queries, and support for NUMA architectures ...|$|R
5000|$|Consider {{the cost}} of an {{insertion}}. Each message gets flushed [...] times, and {{the cost of}} a flush is [...] Therefore, {{the cost of an}} insertion is [...] Finally, note that the branching factor can vary, but for any branching factor , the cost of a flush is , thereby providing a smooth tradeoff between search cost, which depends on the depth of the search tree, and therefore the branching factor, versus the insertion time, which depends on the depth of the tree but more sensitively {{on the size of the}} <b>buffer</b> <b>flushes.</b>|$|R
40|$|AbstractHighly effcient {{encoding}} {{of event}} trace data is a quality feature of any event trace format. It not only enables measurements of long running applications but also reduces bias caused by intermediate memory <b>buffer</b> <b>flushes.</b> In this paper we present encoding techniques that will remarkably increase memory effciency without introducing overhead for the compression. We applied these techniques to the Open Trace Format 2, a state-of-the-art Open Source event trace data format and library {{used by the}} performance analysis tools Vampir, Scalasca, and Tau. In addition, we show that these encoding techniques are a basic step in achieving a complete in-memory event trace work flow...|$|R
40|$|We {{exploit the}} traffic shaping {{potential}} of network storage and improve energy efficiency for mobile devices {{through the creation}} of idle communication intervals. We model the activity patterns between the wired/wireless gateway and the wireless battery-powered receiver, and employ a rendezvous mechanism that utilizes periods of inactivity created by the traffic shaping function of the network. In case multiple receivers are simultaneously active, a scheduling algorithm limits overlaps of <b>buffer</b> <b>flushes.</b> Our scenarios are based on the DTN paradigm, however, our approach is not DTN-specific. The presented simulation study involves three main types of Internet traffic (i. e. file transfer, streaming and web browsing) and demonstrates that our proposed scheme achieves significant energy conservation for mobile receivers involving, under most circumstances, only mild performance cost...|$|R
50|$|Systems such as Windows NT and OS/2 {{are said}} to have cheap threads and {{expensive}} processes; in other operating systems there is not so great a difference except the cost of an address space switch which on some architectures (notably x86) results in a translation lookaside <b>buffer</b> (TLB) <b>flush.</b>|$|R
40|$|Part 3 : Energy EfficiencyInternational audienceWe {{exploit the}} traffic shaping {{potential}} of network storage and improve energy efficiency for mobile devices {{through the creation}} of idle communication intervals. We model the activity patterns between the WIRED/wireless gateway and the wireless battery-powered receiver, and employ a rendezvous mechanism that utilizes periods of inactivity created by the traffic shaping function of the network. In case multiple receivers are simultaneously active, a scheduling algorithm limits overlaps of <b>buffer</b> <b>flushes.</b> Our scenarios are based on the DTN paradigm, however, our approach is not DTN-specific. The presented simulation study involves three main types of Internet traffic (i. e. file transfer, streaming and web browsing) and demonstrates that our proposed scheme achieves significant energy conservation for mobile receivers involving, under most circumstances, only mild performance cost...|$|R
40|$|The Total Store Order memory {{model is}} widely {{implemented}} by modern multicore architectures such as x 86, where local buffers {{are used for}} optimisation, allowing limited forms of instruction reordering. The presence of buffers and hardware-controlled <b>buffer</b> <b>flushes</b> increases the level of non-determinism from the level specified by a program, complicating the already difficult task of concurrent programming. This paper presents a new notion of refinement for weak memory models, based on the observation that pending writes to a process’ local variables may be treated as if {{the effect of the}} update has already occurred in shared memory. We develop an interval-based model with algebraic rules for various programming constructs. In this framework, several decomposition rules for our new notion of refinement are developed. We apply our approach to verify the spinlock algorithm from the literature...|$|R
40|$|Abstract. Although event tracing of {{parallel}} applications offers highly detailed performance information, tracing on current leading edge systems {{may lead to}} unacceptable perturbation of the target program and unmanageably large trace files. High end systems of the near future promise even greater scalability challenges. Development of more scalable approaches requires a detailed understanding of the interactions between current approaches and high end runtime environments. In this paper we present the results of studies that examine several sources of overhead related to tracing: instrumentation, differing trace buffer sizes, periodic <b>buffer</b> <b>flushes</b> to disk, system changes, and increasing numbers of processors in the target application. As expected, the overhead of instrumentation correlates strongly {{with the number of}} events; however, our results indicate that the contribution of writing the trace buffer increases with increasing numbers of processors. We include evidence that the total overhead of tracing is sensitive to the underlying file system. ...|$|R
40|$|Abstract [...] Although event tracing of {{parallel}} applications offers highly detailed performance information, tracing on current leading edge systems {{may lead to}} unacceptable perturbation of the target program and unmanageably large trace files. High end systems of the near future promise even greater scalability challenges. In this work we identify and quantify the overheads of application tracing. We report results for two ASC Purple Benchmarks with different communication characteristics: SMG 2000, which exhibits an extremely high message rate, and SPhot, an embarrassingly parallel application with relatively little communication. We investigate several different sources of overhead related to tracing: instrumentation, differing trace buffer sizes, periodic <b>buffer</b> <b>flushes</b> to disk, system changes, and increasing numbers of processors in the target application. Our results show that tracing overhead is affected by differences in system software, {{as well as the}} choice of trace buffer size. As expected, the overhead of instrumentation correlates strongly with the number of events; however, our results indicate that the overhead of writing the trace buffer increases with increasing numbers of processors. 1...|$|R
5000|$|Fractal Trees nodes use {{a smaller}} {{branching}} factor, say, of [...] The {{depth of the}} tree is then , thereby matching the B-tree asymptotically. The remaining space in each node is used to buffer insertions, deletion and updates, which we refer to in aggregate as messages. When a buffer is full, it is flushed to the children in bulk. There are several choices for how the <b>buffers</b> are <b>flushed,</b> all leading to similar I/O complexity. Each message in a node <b>buffer</b> will be <b>flushed</b> to a particular child, as determined by its key. Suppose, for concreteness, that messages are flushed that are heading to the same child, and that among the [...] children, we pick {{the one with the}} most messages. Then there are at least [...] messages that can be flushed to the child. Each flush requires [...] flushes, and therefore the per-message cost of a flush is [...]|$|R
5000|$|When a [...] "non-local goto" [...] is {{executed}} via /, normal [...] "stack unwinding" [...] {{does not}} occur. Therefore any required cleanup actions will not occur either. This could include closing file descriptors, <b>flushing</b> <b>buffers,</b> or freeing heap-allocated memory.|$|R
40|$|This thesis {{contributes}} {{to the field of}} performance analysis in High Performance Computing with new concepts for in-memory event tracing. Event tracing records runtime events of an application and stores each with a precise time stamp and further relevant metrics. The high resolution and detailed information allows an in-depth analysis of the dynamic program behavior, interactions in parallel applications, and potential performance issues. For long-running and large-scale parallel applications, event-based tracing faces three challenges, yet unsolved: the number of resulting trace files limits scalability, the huge amounts of collected data overwhelm file systems and analysis capabilities, and the measurement bias, in particular, due to intermediate memory <b>buffer</b> <b>flushes</b> prevents a correct analysis. This thesis proposes concepts for an in-memory event tracing workflow. These concepts include new enhanced encoding techniques to increase memory efficiency and novel strategies for runtime event reduction to dynamically adapt trace size during runtime. An in-memory event tracing workflow based on these concepts meets all three challenges: First, it not only overcomes the scalability limitations due to the number of resulting trace files but eliminates the overhead of file system interaction altogether. Second, the enhanced encoding techniques and event reduction lead to remarkable smaller trace sizes. Finally, an in-memory event tracing workflow completely avoids intermediate memory <b>buffer</b> <b>flushes,</b> which minimizes measurement bias and allows a meaningful performance analysis. The concepts further include the Hierarchical Memory Buffer data structure, which incorporates a multi-dimensional, hierarchical ordering of events by common metrics, such as time stamp, calling context, event class, and function call duration. This hierarchical ordering allows a low-overhead event encoding, event reduction and event filtering, as well as new hierarchy-aided analysis requests. An experimental evaluation based on real-life applications and a detailed case study underline the capabilities of the concepts presented in this thesis. The new enhanced encoding techniques reduce memory allocation during runtime by a factor of 3. 3 to 7. 2, while at the same do not introduce any additional overhead. Furthermore, the combined concepts including the enhanced encoding techniques, event reduction, and a new filter based on function duration within the Hierarchical Memory Buffer remarkably reduce the resulting trace size up to three orders of magnitude and keep an entire measurement within a single fixed-size memory buffer, while still providing a coarse but meaningful analysis of the application. This thesis includes a discussion of the state-of-the-art and related work, a detailed presentation of the enhanced encoding techniques, the event reduction strategies, the Hierarchical Memory Buffer data structure, and a extensive experimental evaluation of all concepts...|$|R
40|$|One of {{the most}} urgent {{challenges}} in event based performance analysis is the enormous amount of collected data. Combining event tracing and periodic sampling has been a successful approach to allow a detailed event-based recording of MPI communication and a coarse recording of the remaining application with periodic sampling. In this paper, we present a novel approach to automatically adapt the sampling frequency during runtime to the given amount of buffer space, releasing users to find an appropriate sampling frequency themselves. This way, the entire measurement can be kept within a single memory buffer, which avoids disruptive intermediate memory <b>buffer</b> <b>flushes,</b> excessive data volumes, and measurement delays due to slow file system interaction. We describe our approach to sort and store samples based on their order of occurrence in an hierarchical array based on powers of two. Furthermore, we evaluate the feasibility {{as well as the}} overhead of the approach with the prototype implementation OTFX based on the Open Trace Format 2, a state-of-the-art Open Source event trace library used by the performance analysis tools Vampir, Scalasca, and Tau. This work is supported by the Spanish Ministry of Economy and Competitiveness under contract TIN 2015 - 65316 -P. Peer ReviewedPostprint (author's final draft...|$|R
50|$|On CPUs without Second Level Address Translation, {{installation}} of most WDDM accelerated graphics drivers on the primary OS {{will cause a}} dramatic drop in graphic performance. This occurs because the graphics drivers access memory in a pattern that causes the Translation lookaside <b>buffer</b> to be <b>flushed</b> frequently.|$|R
5000|$|MySQL Archive is {{a storage}} engine for the MySQL {{relational}} database management system. Users {{can use this}} analytic storage engine to create a table that is “archive” only. Data cannot be deleted from this table, only added. The Archive engine uses a compression strategy based on the zlib library and it packs the rows using a bit header to represent nulls and removes all whitespace for character type fields. When completed, the row is inserted into the compression <b>buffer</b> and <b>flushed</b> to disk by an explicit flush table, a read, or {{the closing of the}} table.|$|R
5000|$|Unix systems {{typically}} {{run some}} kind of flush or update daemon, which calls the sync function on a regular basis. On some systems, the cron daemon does this, and on Linux it's handled by the pdflush daemon. [...] <b>Buffers</b> are also <b>flushed</b> when filesystems are unmounted or remounted read-only, for example prior to system shutdown.|$|R
50|$|The {{fundamental}} {{problem that the}} dispose pattern aims to solve is that resources are expensive (for example, {{there may be a}} limit on the number of open files), and thus should be released promptly. Further, some finalization work is often needed, particularly for I/O, such as <b>flushing</b> <b>buffers</b> to ensure that all data is actually written.|$|R
40|$|GPRS {{extends the}} widely {{deployed}} GSM {{system with a}} more efficient wireless Internet access. In this paper we present measurements of GPRS traffic when preempted by circuit-switched calls. The results indicate that TCP performance is degraded more than necessary, as <b>buffered</b> data is <b>flushed</b> immediately when the GPRS traffic is preempted. The time required for error recovery is considerable also for very short preemption periods...|$|R
5000|$|The {{presence}} of an asterisk (...) prepended to the file type in a directory listing (for example, [...] ) indicates that the file was not properly closed after writing. When the drive is commanded to close a file that has been opened for writing, the associated <b>buffer</b> is <b>flushed</b> to the disk and the block availability map (BAM) is updated to accurately reflect which blocks have been used. If a program crash or other problem (such as the user removing the disk while a file is open) results in an [...] "orphan file", {{also referred to as}} a [...] "poison" [...] or [...] "splat" [...] file, <b>buffers</b> are not <b>flushed</b> and the BAM will not accurately reflect disk usage, putting the disk at risk of corruption. A poison file generally cannot be accessed (but can be opened in [...] "modify" [...] mode), and an attempt to use the DOS [...] command to delete the file may cause filesystem corruption, such as crosslinking. The only practical method of removing one of these files is by opening the file in [...] "modify" [...] mode (and fixing it), or by validating the disk (see the DOS [...] command below), the latter which rebuilds the BAM and removes poison file references from the directory. The infamous save-with-replace bug could result in creation of splat files.|$|R
5000|$|The RDBMS first writes all changes {{included}} in the transaction into the log buffer in the System Global Area (SGA). Using [...] memory in this way for the initial capture aims to reduce disk IO. Of course, when a transaction commits, the redo log <b>buffer</b> must be <b>flushed</b> to disk, because otherwise the recovery for that commit could not be guaranteed. The LGWR (Log Writer) process does that flushing.|$|R
50|$|The Compression Analysis Tool is a Windows {{application}} that enables end users to benchmark the performance characteristics of streaming implementations of LZF4, DEFLATE, ZLIB, GZIP, BZIP2 and LZMA {{using their own}} data. It produces measurements and charts with which users can compare the compression speed, decompression speed and compression ratio of the different compression methods and to examine how the compression level, <b>buffer</b> size and <b>flushing</b> operations affect the results.|$|R
50|$|RAII {{relies on}} object {{lifetime}} being deterministic; however, with automatic memory management, object lifetime {{is not a}} concern of the programmer: objects are destroyed at some point after {{they are no longer}} used, but when is abstracted. Indeed, lifetime is often not deterministic, though it may be, notably if reference counting is used. Indeed, in some cases {{there is no guarantee that}} objects will ever be finalized: when the program terminates, it may not finalize the objects, and instead just let the operating system reclaim memory; if finalization is required (e.g., to <b>flush</b> <b>buffers),</b> data loss can occur.|$|R
5000|$|For most file systems, {{a program}} terminates {{access to a}} file in a {{filesystem}} using the close system call. This <b>flushes</b> <b>buffers,</b> updates file metadata (which may include and end of file indicator in the data), de-allocates resources associated with the file (including the file descriptor) and updates the system wide table of files in use. Some languages maintain a structure of files opened by its run-time library and may close when the program terminates. Some operating systems will invoke the close if the program terminates. Some operating systems will invoke the close {{as part of an}} operating system recovery {{as a result of a}} system failure.|$|R
50|$|The most {{significant}} difference between HW1 and HW2 is {{in the way the}} calculator handles the display. In HW1 calculators there is a video buffer that stores all of the information that should be displayed on the screen, and every time the screen is refreshed the calculator accesses this <b>buffer</b> and <b>flushes</b> it to the display (direct memory access). In HW2 and later calculators, a region of memory is directly aliased to the display controller (memory-mapped I/O). This allows for slightly faster memory access, as the HW1's DMA controller used about 10% of the bus bandwidth. However, it interferes with a trick some programs use to implement grayscale graphics by rapidly switching between two or more displays (page-flipping). On the HW1, the DMA controller's base address can be changed (a single write into a memory-mapped hardware register) and the screen will automatically use a new section of memory {{at the beginning of the}} next frame. In HW2, the new page must be written to the screen by software. The effect of this is to cause increased flickering in grayscale mode, enough to make the 7-level grayscale supported on the HW1 unusable (although 4-level grayscale works on both calculators).|$|R
30|$|In {{contrast}} to the algorithm presented in Section 4 the AP does not work in a Stop and Wait manner, in which it stops all transmissions to a certain station upon frame failure until the frame is either received correctly or dropped. Rather, in our implementation it stores the un-acknowledged frame in the aforementioned buffer and continues sending subsequent frames to this user (i.e., selective repeat manner). Note that the unique sequential frame index included in each frame enables the support of such selective repeat mechanism and allows frame reordering. In order to avoid buffer overflows due to the selective repeat mechanism (which has an infinite window size), {{as soon as the}} buffer size crosses a threshold, all frames in the buffer are transmitted uncoded and the <b>buffer</b> is <b>flushed.</b> Additionally, for each un-acknowledged packet which is stored in the buffer, a time stamp is attached. A time-out mechanism is implemented such that the packet is retransmitted uncoded upon expiration of the time-out. It {{is important to note that}} in {{contrast to}} the algorithm presented earlier, no status packets are sent by the users to indicate which packets in their buffer are meant to the other user and were not acknowledged. In our implementation the AP assumes that each un-acknowledged packet is received by the other receiver, hence can be used for the coded packets. Consequently, the AP can send a coded packet that one or both receivers cannot really decode. Accordingly, as previously mentioned a retransmitted packet which cannot be decoded is lost. An enhancement in which a user periodically or upon request sends a status message which includes the unreceived packets can be easily implemented.|$|R
30|$|It is {{revealed}} in [21] that the wireless transmission has an interference range of four-hops. Thus, transmission conflicts in the intermediate nodes happen more frequently {{than those in the}} end nodes in the chain topology. The average queue length in Fig.  3 shows that, being in a situation with more severe interference, the second node in the chain topology has less chances to access the channel, and its <b>buffer</b> is quick <b>flushed</b> by incoming packets from the source node. In this sense, the failures and drops of RTSs in the first hop are mainly due to the aggressively sending in the semi-TCP source. Reserving the buffer of one packet in semi-TCP still results in a over-provisioned sending rate for the network. The aggressive sending makes the upstream nodes congested, leading to low transmission rates in the downstream links. This is confirmed by the curve of RTSC transmissions in Fig.  4, which shows that the source node sends the most RTSCs to broadcast its congestion state in its buffer 3.|$|R
40|$|The redo log plays a prime role in Oracle database’s core functionality. However it imposes disk i/o for the redo log’s {{inherent}} functionality, increased i/o can highly {{affect the}} Oracle performance. In this paper I am analyzing the performance {{issues related to}} the Oracle redo log, and solutions to address those issues, also I am covering how to minimize the overhead of redo logs and improve the overall database performance. Concept review The redo log buffer is a RAM area defined by the initialization parameter log_buffer that works to save changes to database, in case something fails and Oracle has to put it back into its original state (a “rollback”). When Oracle SQL updates a table, redo images are created and stored in the redo log buffer. Since RAM is faster than disk, this makes the storage of redo very fast. Oracle will eventually flush the redo log buffer to disk, called online redo log. This can happen in a number of special cases, but what’s really important is that Oracle guarantees that the redo log <b>buffer</b> will be <b>flushed</b> to disk after a commit operatio...|$|R
30|$|Following {{the layer}} {{formation}} in situ by QCM-D, Fig.  1 shows the mass uptake {{related to the}} changes in oscillation frequency of the silica substrates related to the absorbates getting attached from the laccase—maltodextrin suspension. As described in the experimental section, firstly the pure acetate <b>buffer</b> solution was <b>flushed</b> over the crystal surface (baseline), stage I, followed by a contact-period with the laccase-maltodextrin suspension, stage II. In stage II a decrease of the frequency indicates mass adsorption. A steady state was not achieved after {{one and a half}} hours of contact with the suspension. The slow decrease in the frequency was observed {{in the early stages of}} biofilm growth on the silica substrate and, later on, on top of the already formed biofilm as suggested by the results of AFM investigations presented in the next section. Such small growth rate might be considered an indicative of a low affinity of the biopolymers for adsorption to the surface. A steady state, stage III, was not achieved during the contact time, as it will be presented in further studies by the authors. In stage IV and V the surface was <b>flushed</b> with the <b>buffer</b> for 30  min and only low desorption of the biofilm in the early stage IV was detected. The insignificant extent of desorption of the surface is related to a surprisingly stable layer formation.|$|R
40|$|Target cells adsorbed in {{affinity}} sepn. processes need to {{be effectively}} recovered from the adsorption matrix once contaminating cell types have been removed. Although tangential flow {{can be used to}} generate detachment forces, the resultant diln. is excessive. In this study, three alternative detachment protocols have been assessed using a membrane-based adsorption surface. Back <b>flushing</b> <b>buffer</b> through the membrane to generate greater detachment forces was studied. However, at the max. back flush velocity obtainable less than 50 % of attached cells were removed and recovered cell concns. were low. Controlled transmembrane delivery of hydrochloric acid as a non-specific eluent was also investigated. While this allowed complete recovery of cells the effective acid concn. was excessive. The upstream diln. required to maintain cell viability again leads to considerable diln. Injection of air bubbles into the tangential flow stream to generate elevated detachment forces was found to be the most effective approach. Detachment was found to be a function of both no. and size of bubbles. Under optimal conditions, 80 % of attached cells can be removed to give a detached cell concn. of 108 cells ml- 1. This leads to an essentially reagentless detachment protocol allowing the development of an isocratic affinity cell separator where cells can be attached, washed, and detached in the same buffer. [on SciFinder (R) ...|$|R
40|$|Abstract—In this paper, we {{consider}} {{a network that}} supports multiple multicast flows with network coding. It is well-known that network coding {{can be used to}} increase the throughput of a delay-free network. However, as there are multiple multicast flows in the network, packets might be delayed due to contention. As packets from the same flow have to be synchronized for network coding, additional buffers, known as synchronization buffers in the literature, are required. In certain networks, such as multistage interconnection networks in switch fabrics [17], the size of internal buffers could be quite limited. One of the key contributions of the paper is to propose a packet scheduling algorithm so that synchronization buffers can be bounded by a finite constant that only depends on the maximum hop count of the flows. We also show that our scheduling algorithm does not cause any throughput degradation as it can stabilize the network for any admissible traffic. Moreover, our algorithm is universal and it does not require to know the rates of the flows in the network. The main idea of our algorithm is to introduce fictitious packets in the dynamic frame sizing algorithm recently proposed in [4], [19] for stabilizing switches and wired networks (without network coding). By so doing, packets that might be stalled in synchronization <b>buffers</b> can be <b>flushed</b> out. We show that the number of fictitious packets in each frame is also bounded by a finite constant that only depends on the maximum hop count...|$|R
40|$|The {{potential}} {{for using the}} crop maize (Zea mays) {{for the production of}} biogas in simple anaerobic leach beds was evaluated. The results showed that leach beds coupled to high-rate methanogenic reactors performed better than other systems on a specific methane yield per gram of substrate added basis while their performance on a volumetric gas yield basis was poorer. Initial experiments using a single stage digester showed rapid acidification due to the low buffering capacity of the system. To overcome this problem, leach beds were used as part of a two-phase system in which the intermediate metabolic products were flushed out and used as substrate for a second stage methanogenic reactor. Further experiments simulated the effect of a hydraulic flush in the leach bed using clean water as the flush liquid. Methane potential of the leachate was estimated based on the cumulative soluble chemical oxygen demand (SCOD) production. Under this operation mode, the effect of substrate to inoculum ratio, fresh substrate load (FSL), hydraulic retention time (HRT) and buffer and trace element addition was tested. The performance of the leach bed was found to be poor compared to conventional digesters where the methane yield is ~ 0. 35 l CH 4 g- 1 VSadded and volatile solids (VS) destruction is ~ 85 % and this was thought to be due to the low pH in the reactor. Increasing the FSL improved methane yield but the maximum value obtained was 0. 12 l CH 4 g- 1 VS. Decreasing the HRT allowed the leach bed to operate at a slightly higher pH. In this case, a volatile solids destruction of ~ 50 % and a methane yield of 0. 17 l CH 4 g- 1 VS was achieved using a HRT of 2. 6 days. The addition of buffer (NaHCO 3) to maintain pH ~ 6. 5 increased VS destruction to 89 % and methane yield to 0. 37 l CH 4 g- 1 VS at an HRT of 1. 5 days. This performance was similar at a HRT of 28 days despite the high VFA concentrations. Acid production increased with the addition of buffer as 75 - 97 % of SCOD was converted to this form. Buffering was also shown {{to increase the number of}} culturable anaerobic cellulolytic microorganisms. VS degradation and methane potential were further enhanced by the addition 0. 5 mg l- 1 of cobalt to the <b>buffered</b> <b>flush</b> medium giving an apparent VS destruction of 115 % and a methane yield of 0. 46 l CH 4 g- 1 VSadded, which indicated that a proportion of the inoculum was also degraded. In the final part of the research the leach beds were coupled to methanogenic reactors and trials were conducted using different feed cycle durations, in which all the digestate and leachate from the preceding run was used as inoculum, and only the solids destroyed were replaced with fresh feed material. The effect of the methanogenic reactor was multi-fold as it not only stripped out intermediate compounds, according to its primary design function, but also played an important role in stabilising pH, maintaining nutrients and retaining the microbial population in the system. The leach bed operated with a 7 -day feed cycle showed higher substrate degradation and was able to receive a higher OLR of 2. 4 gTS l- 1 reactor d- 1 than the 14 -day feed cycle at 1. 7 gTS l- 1 reactor d- 1 and the 28 -day feed cycle at 1. 3 gTS l- 1 reactor d- 1. This provided a higher volumetric methane yield the shorter the feed cycle, 0. 839 lCH 4 l- 1 d- 1, 0. 618 lCH 4 l- 1 d- 1 and 0. 482 lCH 4 l- 1 d- 1 in the 7 -day, 14 -day and 28 -day feed cycle, respectively. However, the specific methane yield obtained from the system was slightly higher in the 14 -day and 28 -day cycles at 0. 434 l CH 4 g- 1 VSadded while in the 7 -day cycle it was 0. 418 l CH 4 g- 1 VSadded. The retention of the digestate and leachate over successive cycles for a period of ~ 160 days appeared, however, to cause an accumulation of suspended solids (SS) and total and soluble COD in the leachate. This was especially the case in the higher loaded 7 -day feed cycle reactor and was probably the cause of the lower methane production. Initially the methanogenic reactors were responsible for most methane production but with progressive cycles the leach beds themselves became methanogenic and eventually accounted for more than 50 % of the methane generated in the system. The methanogenic and cellulolytic bacteria were shown to be present in the leachate from both reactors and suggested a synergy between them in exchange of microbial consortia. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Buffer caches in {{operating}} systems keep active file blocks in memory to reduce disk accesses. Related {{studies have focused}} on minimizing buffer misses and the resulting performance degradation. However, the side effects and performance implications of accessing the data in buffer caches (i. e. buffer cache hits) have been ignored. In this paper, we show that accessing buffer caches can cause serious performance degradation on multicores, particularly with shared last level caches (LLCs). There are two reasons for this problem. First, data objects in files normally have weaker localities than data objects in virtual memory spaces. Second, due to the shared structure of LLCs on multicore processors, an application accessing the data in a <b>buffer</b> cache may <b>flush</b> the to-be-reused data of its co-running applications from the shared LLC and significantly slow down these applications. The paper proposes a buffer cache design called Selected Region Mapping Buffer (SRM-buffer) for multicore systems to address effectively the cache pollution problem caused by OS buffer. SRM-buffer improves existing OS buffer management with an enhanced page allocation policy that carefully selects mapping physical pages upon buffer misses. For a sequence of blocks accessed by an application, SRMbuffer allocates physical pages that are mapped to a selected region consisting of a small portion of sets in the LLC. Thus, when these blocks are accessed, cache pollution is effectively limited within the small cache region. We have implemented a prototype of SRM-buffer into the Linux kernel, and tested it with extensive workloads. Performance evaluation shows SRM-buffer can improve system performance and decrease the execution times of workloads by up to 36 %...|$|R
