86|6520|Public
40|$|Pengembangan Multimedia Pembelajaran Interaktif dengan Metode Heuristik Tipe <b>Best</b> <b>First</b> <b>Search</b> untuk Mata Pelajaran Desain Grafis SMK” adalah penelitian dan pengembangan yang bertujuan untuk : 1) Mengetahui bagaimana mengembangkan Multimedia Pembelajaran Interaktif dengan Metode Heuristik Tipe <b>Best</b> <b>First</b> <b>Search</b> dan 2) Mengetahui apakah ada pengaruh Multimedia Pembelajaran Interaktif dengan Metode Heuristik Tipe <b>Best</b> <b>First</b> <b>Search</b> terhadap pemahaman siswa dalam Mata Pelajaran Desain Grafis. Hasil dari penelitian ini adalah : 1) Mengembangkan Multimedia Pembelajaran Interaktif Metode Heuristik Tipe <b>Best</b> <b>First</b> <b>Search</b> dilakukan dengan Metode Penelitian Research and Development yang terdiri dari identifikasi potensi masalah, pengumpulan data, desain produk, validasi desain, revisi desain, pengembangan, dan uji coba produk. Multimedia ini menggunakan model perancangan {{waterfall}} yang terdiri dari analisis, desain, kode, dan Tes. 2) Multimedia Pembelajaran Interaktif Metode Heurustik Tipe <b>Best</b> <b>First</b> <b>Search</b> berpengaruh terhadap pemahaman siswa, berdasarkan hasil penelitian terjadi peningkatan dari nilai pretest hingga nilai postest siswa dengan signifikansi indeks gain yang dihasilkan sebesar 0, 6 artinya penilaian indeks gain dalam pengaruhnya terhadap pembelajaran termasuk ke dalam kriteria “sedang”. Adapun hasil angket respon siswa terhadap multimedia menunjukan perolehan persentase sebesar 77, 76...|$|E
40|$|Plan Quality {{is defined}} as the sum of the costs {{associated}} with all the actions in a plan. This definition is incorporated into the Partial Order planner, UCPOP. For finding the optimal plans efficiently, he standard <b>Best</b> <b>First</b> <b>Search</b> strategy was compared against two versions of Iterative Deepening that were added to the UCPOP planner. Experiments over several problems in an extended version of blocks world domain and logistics domain show that while the first iterative deepening algorithm, based on the Depth First Search (ID-DEPTH-FS) yielded optimal plans with respect to number of actions in them, the second iterative deepening algorithm, based on the <b>best</b> <b>first</b> <b>search</b> (ID-BEST-FS) was both effective and efficient in finding optimal plans with respect to the total cost of all actions in it. Experiments on various ranking functions have shown that for standard <b>Best</b> <b>First</b> <b>Search</b> and its variants and for the the two iterative deepening algorithms added to UCPOP, ranking function based [...] ...|$|E
40|$|The HR program forms {{concepts}} and makes conjectures in domains of pure mathematics andusestheoremproverOTTERandmodel generatorMACEtoproveordisprovetheconjectures. HRmeasurespropertiesof{{concepts and}}assessesthetheoremsandproofsinvolving themtoestimatetheinterestingnessofeach concept and employ a <b>best</b> <b>first</b> <b>search.</b> This approachhasledHRtothediscoveryofinterestingnewmathematics and enables it to build theories from just the axioms of finite algebras...|$|E
40|$|The {{class of}} {{similarity}} based methods (SBM) covers most neural models {{and many other}} classifiers. Performance of such methods is significantly improved if irrelevant features are removed and feature weights introduced, scaling their influence on calculation of similarity. Several methods for feature selection and weighting are described. As {{an alternative to the}} global minimization procedures computationally efficient <b>best</b> [...] <b>first</b> <b>search</b> methods are advocated. Although these methods can be used with any SBM classifier they have been tested using the k-NN method, since it is relatively fast and for some databases gives excellent results. A few illustrative examples show significant improvements due to the feature weighting and selection. Introduction. M ANY neural, pattern recognition and machine learning methods developed in the past use explicitly or implicitly similarity measures during the training and classification process (although our focus here is on classification the sam [...] ...|$|R
40|$|This paper {{proposes a}} model of {{learning}} by discovery. The model consists of a program which discovers macro operators while conducting a <b>best</b> <b>first</b> heuristic <b>search</b> {{in the domain of}} puzzles. This work extends some recent work on permutation puzzles (Korf, 1982) and operator-decomposable puzzles (Korf, 1983), and is related to the earlier work on MACROPS (Fikes, Hart, and Nilsson, 1972). This work is part of a doctoral dissertation currently in progress at MIT, in which the model will be used to explore learning in conjunction with additional search paradigms and numerous alternative heuristics for macro generation and selection. The specific heuristic reported on here is that of using peaks of the evaluation function to segment the paths of the search tree in order to discover macros. The technique seems particularly valuable in difficult puzzles where only imperfect or approximate evaluation functions are available...|$|R
40|$|Program {{activity}} graphs (PAGs) can {{be constructed}} from timestamped traces of appropriate execution events. Information about the activities on the k longest execution paths is useful {{in the analysis of}} parallel program performance. In this paper, four algorithms for finding the near [...] critical paths of PAGs are compared, including a <b>best</b> [...] <b>first</b> <b>search</b> (BFS) algorithm that is worst [...] case asymptotically optimal in terms of both time and space. Results confirming the practical efficiency of the BFS algorithm are presented for five application programs. A framework for using the near [...] critical path information is also described. The framework includes statistical summaries and visualization capabilities that build upon the foundation of existing performance analysis tools. Within the framework, guidance is provided by the Maximum Benefit Metric, which uses near [...] critical path data to predict the maximum overall performance improvement that may be realized by optimizing particular critical p [...] ...|$|R
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaAs part of {{the ongoing}} Illinois Digital Library Initiative project, this {{research}} proposes an intelligent agent approach to Web searching. In this experiment, we developed two Web personal spiders based on <b>best</b> <b>first</b> <b>search</b> and genetic algorithm techniques, respectively. These personal spiders can dynamically take a userâ s selected starting homepages and search for the most closely related homepages in the Web, based on the links and keyword indexing. A graphical, dynamic, Java-based interface was developed and is available for Web access. A system architecture for implementing such an agent-based spider is presented, followed by detailed discussions of benchmark testing and user evaluation results. In benchmark testing, although the genetic algorithm spider did not outperform the <b>best</b> <b>first</b> <b>search</b> spider, we found both results to be comparable and complementary. In user evaluation, the genetic algorithm spider obtained significantly higher recall value {{than that of the}} <b>best</b> <b>first</b> <b>search</b> spider. However, their precision values were not statistically different. The mutation process introduced in genetic algorithm allows users to find other potential relevant homepages that cannot be explored via a conventional local search process. In addition, we found the Java-based interface to be a necessary component for design of a truly interactive and dynamic Web agent...|$|E
40|$|Greedy <b>Best</b> <b>First</b> <b>Search</b> (GBFS) is a {{powerful}} algorithm {{at the heart of}} many state-of-the-art satisficing planners. The Greedy <b>Best</b> <b>First</b> <b>Search</b> with Local Search (GBFS-LS) algo-rithm adds exploration using a local GBFS to a global GBFS. This substantially improves performance for domains that contain large uninformative heuristic regions (UHR), such as plateaus or local minima. This paper analyzes, quantifies and improves the performance of GBFS-LS. Planning problems with a mix of small and large UHRs are shown to be hard for GBFS but easy for GBFS-LS. In three standard IPC planning instances analyzed in detail, adding exploration using local GBFS gives more than three orders of magnitude speedup. As a second con-tribution, the detailed analysis leads to an improved GBFS-LS algorithm, which replaces larger-scale local GBFS explo-rations with a greater number of smaller explorations...|$|E
40|$|This paper {{presents}} an argumentation-based inter-preter for Golog programs. Traditional Golog inter-preters are {{not designed to}} find the most preferred executions of a program from the perspective of an agent. Existing techniques developed to discover these executions are limited in terms of how the preferences of an agent can be expressed, and the variety of preference types {{that can be used to}} guide search for a solution. The presented work combines the use of argumentation to compare executions rel-ative to a set of general comparison principles, and the theory behind <b>best</b> <b>first</b> <b>search</b> to reduce the cost of the search process. To the best of our knowledge this is the first work to integrate argumentation and the interpretation of Golog programs, and to use ar-gumentation as a tool for <b>best</b> <b>first</b> <b>search.</b> ...|$|E
50|$|The {{detection}} and description of local image features {{can help in}} object recognition. The SIFT features are local {{and based on the}} appearance of the object at particular interest points, and are invariant to image scale and rotation. They are also robust to changes in illumination, noise, and minor changes in viewpoint. In addition to these properties, they are highly distinctive, relatively easy to extract and allow for correct object identification with low probability of mismatch. They are relatively easy to match against a (large) database of local features but however the high dimensionality can be an issue, and generally probabilistic algorithms such as k-d trees with <b>best</b> bin <b>first</b> <b>search</b> are used. Object description by set of SIFT features is also robust to partial occlusion; as few as 3 SIFT features from an object are enough to compute its location and pose. Recognition can be performed in close-to-real time, at least for small databases and on modern computer hardware.|$|R
40|$|International audienceIn {{the design}} of exact methods for NP-hard machine {{scheduling}} problems, branch and bound algorithms have always been widely considered. In this work we revisit the classic search strategies for branch and bound schemes. We consider a systematic application of the well known dynamic programming dominance property for machine scheduling problems. Several conditions concerning {{the application of the}} proposed property with respect to <b>best</b> <b>first,</b> depth <b>first,</b> breadth <b>first</b> <b>search</b> strategies and problem characteristics are presented. Computational testing on single machine and flow shop problems validate in practice the efficiency of the considered approach and suggest that the traditional choice of depth <b>first</b> <b>search</b> with respect to <b>best</b> <b>first</b> and breadth first is strongly questionable...|$|R
2500|$|Each phase {{consists}} of a single breadth <b>first</b> <b>search</b> and a single depth <b>first</b> <b>search.</b> Thus, a single phase may be implemented in [...] time.|$|R
40|$|In this paper, {{the effect}} of {{heuristic}} graph search algorithms like best first and A* <b>best</b> <b>first</b> <b>search</b> on the offline browsing efficiency is studied. A web crawler based multithreaded web archiving system is designed using these heuristic graph search algorithms and the offline browsing efficiency of the web archiving system is estimated...|$|E
40|$|Routing and {{wavelength}} assignment (RWA) is {{a widely}} discussed design problem in the optical networks literature. Physical layer impairments (PLI) degrade the quality of transmission (QOT) of a propagating optical signal inside the optical fiber {{and they have a}} significant impact on the RWA process. 3 R regeneration, which is based on the expensive optical-to-electronic-to-optical (OEO) conversion technology, is a popularly used technique to restore the degraded QOT of an optical signal. In order to minimize both capital and operational costs, it is highly desirable to use a translucent optical network, in which the 3 R regenerators are sparsely yet strategically placed. This thesis presents a novel impairment-aware RWA approach, called <b>best</b> <b>first</b> <b>search</b> RWA (BFS-RWA), for dynamic connection requests, in a translucent optical network. BFS-RWA is based on the A* <b>best</b> <b>first</b> <b>search</b> algorithm and guarantees an optimal solution (i. e. using the least possible number of regenerators) ...|$|E
40|$|In {{this paper}} a new {{abstract}} machine model, the Stratified Recursive Backup machine model, is described. This machine {{model can be}} used to implement <b>best</b> <b>first</b> <b>search</b> algorithms efficiently. Two applications of <b>best</b> <b>first</b> <b>search,</b> a text layouting system and a natural language parser, are analyzed to provide an in-depth understanding of the Stratified Recursive Backup machine model. Dept. of Information Systems, Faculty of Mathematics and Computing Science, University of Nijmegen, Toernooiveld, NL- 6525 ED Nijmegen, The Netherlands, E-mail: paulj@cs. kun. nl Contents 1 Introduction 3 2 Generic framework for <b>Best</b> <b>First</b> <b>Search</b> 5 3 Stratified Recursive Backup 6 3. 1 Recursive Backup : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3. 1. 1 Domains : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3. 1. 2 Semantics : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 3. 1. 3 Example: Top-Down recognition : : : : : : : : : : : : : : : : [...] ...|$|E
50|$|A strong {{orientation}} {{of a given}} bridgeless undirected graph {{may be found in}} linear time by performing a depth <b>first</b> <b>search</b> of the graph, orienting all edges in the depth <b>first</b> <b>search</b> tree away from the tree root, and orienting all the remaining edges (which must necessarily connect an ancestor and a descendant in the depth <b>first</b> <b>search</b> tree) from the descendant to the ancestor. Although this algorithm is not suitable for parallel computers, due to the difficulty of performing depth <b>first</b> <b>search</b> on them, alternative algorithms are available that solve the problem efficiently in the parallel model. Parallel algorithms are also known for finding strongly connected orientations of mixed graphs.|$|R
5000|$|Each phase {{consists}} of a single breadth <b>first</b> <b>search</b> and a single depth <b>first</b> <b>search.</b> Thus, a single phase may be implemented in [...] time.Therefore, the first [...] phases, in a graph with [...] vertices and [...] edges, take time [...]|$|R
40|$|Abstract. We present {{algorithms}} to efficiently bound {{the depth}} of the state spaces explored by explicit state model checkers. Given a parameter k, our algorithms guarantee finding any violation of an invariant that is witnessed using a counterexample of length k or less from the initial state. Though depth bounding is natural with breadth <b>first</b> <b>search,</b> explicit state model checkers are unable to use breadth <b>first</b> <b>search</b> due to prohibitive space requirements, and use depth <b>first</b> <b>search</b> to explore large state spaces. Thus, we explore efficient ways to perform depth bounding with depth <b>first</b> <b>search.</b> We prove our algorithms sound (in the sense that they explore exactly all the states reachable within a depth bound), and show their effectiveness on large real-life models from Microsoft’s product groups. ...|$|R
40|$|Abstract — In this paper, {{the effect}} of {{heuristic}} graph search algorithms like best first and A * <b>best</b> <b>first</b> <b>search</b> on the offline browsing efficiency is studied. A web crawler based multithreaded web archiving system is designed using these heuristic graph search algorithms and the offline browsing efficiency of the web archiving system is estimated. Index Terms — Offline browsing efficiency, heuristic graph search algorithms, multithreaded, web archiving system. I...|$|E
40|$|We present {{algorithms}} {{for generating}} alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. Our algorithms use a <b>best</b> <b>first</b> <b>search</b> technique and report the solutions using an implicit representation ordered by cost. Experiments on randomly constructed AND/OR DAGs and problem domains including matrix chain multiplication, finding the secondary structure of RNA, etc, {{show that the}} proposed algorithms perform favorably to the existing approach {{in terms of time}} and space. ...|$|E
40|$|The HR program forms {{concepts}} and makes conjectures in domains of pure mathematics and uses theorem prover OTTER and model generator MACE to prove or disprove the conjectures. HR measures properties of {{concepts and}} assesses the theorems and proofs involving them {{to estimate the}} interestingness of each concept and employ a <b>best</b> <b>first</b> <b>search.</b> This approach has led HR {{to the discovery of}} interesting new mathematics and enables it to build theories from just the axioms of finite algebras...|$|E
50|$|Kosaraju's {{algorithm}} uses two passes {{of depth}} <b>first</b> <b>search.</b> The <b>first,</b> {{in the original}} graph, is used to choose {{the order in which}} the outer loop of the second depth <b>first</b> <b>search</b> tests vertices for having been visited already and recursively explores them if not. The second depth <b>first</b> <b>search</b> is on the transpose graph of the original graph, and each recursive exploration finds a single new strongly connected component. It is named after S. Rao Kosaraju, who described it (but did not publish his results) in 1978; Micha Sharir later published it in 1981.|$|R
50|$|There {{were two}} types of searching {{algorithms}} tried out for this implementation. There was the random search and the depth <b>first</b> <b>search.</b> A random search is where each of the agents go in any direction through the environment {{and try to find}} a pathway out. The depth <b>first</b> <b>search</b> is where agents follow one path as far as it can go then return and try another path if the path they traversed does not contain an exit. If was found that depth <b>first</b> <b>search</b> gave a speed up of 15 times versus a random search.|$|R
5000|$|Though Rafaela [...] was {{nominated}} for best female antagonist, <b>best</b> <b>first</b> actor, and <b>best</b> <b>first</b> actress, it did not win any awards.|$|R
40|$|We present {{algorithms}} {{for generating}} alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. The proposed algorithms use a <b>best</b> <b>first</b> <b>search</b> technique and report the solutions using an implicit representation ordered by cost. In this paper, we present {{two versions of}} the search algorithm [...] (a) an initial version of the <b>best</b> <b>first</b> <b>search</b> algorithm, ASG, which may present one solution more than once while generating the ordered solutions, and (b) another version, LASG, which avoids the construction of the duplicate solutions. The actual solutions can be reconstructed quickly from the implicit compact representation used. We have applied the methods on a few test domains, some of them are synthetic while the others are based on well known problems including the search space of the 5 -peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding secondary structure of RNA. Experimental results show the efficacy of the proposed algorithms over the existing approach. Our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition, where the AND/OR structure is widely used for representing problems...|$|E
40|$|In {{this paper}} {{a method for}} {{selecting}} features for Human Activity Recognition from sensors is presented. Using a large feature set that contains features that may describe the activities to recognize, <b>Best</b> <b>First</b> <b>Search</b> and Genetic Algorithms are employed to select the feature subset that maximizes the accuracy of a Hidden Markov Model generated from the subset. A comparative of the proposed techniques is presented to demonstrate their performance building Hidden Markov Models to classify different human activities using video sensors...|$|E
40|$|We {{present the}} first {{results of a}} new {{technique}} to bin Cone Beam projections without imposing any motion model. Such a technique is required for studying motion in regions of the body, such as the pelvis, where motion exists and is unpredictable. All motion information is obtained directly from the projections and the binning is performed through a type of <b>best</b> <b>first</b> <b>search</b> through the graph of possible complete assignments. Simplifying assumptions coupled with loss-less dimensional reduction using Principal Component Analysis, make the method tractable. ...|$|E
5000|$|... #Subtitle level 2: World’s <b>First</b> <b>Search</b> Engine for Golf Courses ...|$|R
5000|$|<b>First,</b> <b>search</b> {{for small}} prime factors of [...]We quickly find that ...|$|R
5000|$|Databases - Online articles, newspapers, maps, etc. (EBSCOhost and <b>First</b> <b>Search)</b> ...|$|R
40|$|We {{present a}} system for the {{animation}} of human hand that plays violin. Neural network controls the hand movement. We make use of an optimization method to generate the examples for the neural network training. The musical decision of which finger to use is automatically made by <b>best</b> <b>first</b> <b>search.</b> We will show that the movements of violinist’s hands are physically and musically feasible, and that the musical decisions {{are consistent with those}} recommended in the violin pedagogy. A description of system, the results of the decisions, and the animations are presented. 1...|$|E
40|$|In this paper, {{we propose}} a novel {{approach}} “Routing based on <b>Best</b> <b>First</b> <b>Search</b> Technique (RBFS) ” to improve the performance of Dynamic Source Routing (DSR) for mobile ad-hoc wireless networks. The proposed scheme tries to reduce broadcasting of Route Requests during routing. We investigated the QOS metrics namely Average jitter, Average end-to-end delay, Packet delivery ratio and Throughput in various simulation scenarios by varying network size. The simulation {{results show that the}} proposed approach achieves better performance than DSR when Average End-to-End delay and Average Jitter performance metrics are considered...|$|E
40|$|As World Wide Web (WWW) based Internet {{services}} {{become more}} popular, information overload also becomes a pressing research problem. Difficulties with searching on the Internet get worse {{as the amount}} of information that is available increases. A scalable approach to support Internet search is {{critical to the success of}} Internet services and other current or future national information infrastructure (NII) applications. A new approach to build an intelligent personal spider (agent), which is based on automatic textual analysis of Internet documents, is proposed. <b>Best</b> <b>first</b> <b>search</b> and genetic algorithm have been tested to develop the intelligent spider. These personal spiders are able to dynamically and intelligently analyze the contents of the users' selected homepages as the starting point to search for the most relevant homepages based on the links and indexing. An intelligent spider must have the capability to make adjustments according to progress of searching in order to be an intelligent agent. However, the current searching engines do not have communication between the users and the robots. The spider presented in the paper uses Java to develop the user interface such that the users can adjust the control parameters according to the progress and observe the intermediate results. The performances of the genetic algorithm based and <b>best</b> <b>first</b> <b>search</b> based spiders are also reported. published_or_final_versio...|$|E
50|$|Once a 2-SAT {{problem is}} reduced to a graph, then if a depth <b>first</b> <b>search</b> finds a {{strongly}} connected component with both phases of a variable, then the 2-SAT problem is not satisfiable. Likewise, if the depth <b>first</b> <b>search</b> does not find a strongly connected component with both phases of a variable, then the 2-SAT problem is satisfiable.|$|R
30|$|Active node {{for further}} {{branching}} is {{selected based on}} Depth <b>First</b> <b>Search</b> (DFS).|$|R
40|$|Slide puzzle {{is a game}} {{to compile}} pieces of a picture {{appropriate}} to its position. A piece can only be moved by screp {{it out to the}} empty space, {{and it is going to}} be a difficulty apart to finish the game. Slide puzzle needs an algorithm, way to search for which is used to get the game solution. In this paper is explained about applying of Breadth <b>First</b> <b>Search</b> and Depth <b>First</b> <b>Search</b> algorithm in the slide puzzle game by using Microsoft Visual Basic 6. 0. After that, it would be tested to finishing game manually (using people as player) and finishing game by using Breadth <b>First</b> <b>Search</b> and Depth <b>First</b> <b>search</b> algorithm (in this case computer). From the result of test, can be seen that searching by algorithm can be used to optimal of time and path solution in finishing slide puzzle which usually can’t be done if finishing game manually. ...|$|R
