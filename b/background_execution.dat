7|34|Public
5000|$|Batch {{services}} provide centralized <b>background</b> <b>execution</b> of DCL command files.|$|E
5000|$|The {{book was}} first {{published}} by Routledge & K. Paul in 1977 with the title Nemesis at Potsdam: the Anglo-Americans and the expulsion of the Germans: <b>background,</b> <b>execution,</b> consequences. It contained a preface by US Ambassador Robert Murphy, a participant at the Potsdam Conference and former political advisor of General Dwight D. Eisenhower during World War II and of General Lucius Clay during the American military government in Germany. Routledge published a 2nd edition in 1979. The third edition, published by University of Nebraska Press in 1989, the title Nemesis at Potsdam: the expulsion of the Germans from the East was published in 1979 by the University of Nebraska Press. [...] A 1998 edition was published by Picton Press, Rockland, Maine, 2003 296 pp[...]|$|E
40|$|This paper {{describes}} the <b>background,</b> <b>execution</b> and results of two large ergonomics projects in a Centralized Control Room (CCR) {{of a highly}} automated chemical process plant in Rotterdam. Special emphasis will {{be put on the}} extensive use of user participation techniques throughout all phases of these projects...|$|E
5000|$|Job scheduler, an {{enterprise}} software application {{in charge of}} unattended <b>background</b> <b>executions.</b>|$|R
50|$|Modern job schedulers, often termed {{workload}} automation, typically {{provide a}} {{graphical user interface}} and a single point of control for definition and monitoring of <b>background</b> <b>executions</b> in a distributed network of computers. Increasingly, job schedulers are required to orchestrate the integration of real-time business activities with traditional background IT processing across different operating system platforms and business application environments.|$|R
5000|$|... #Caption: Allegory: a Spanish soldier menaces the Dutch maiden with {{a dagger}} against a <b>background</b> of <b>executions</b> and battle, print from Johannes Gysius: Oorspronck ende voortgang der Neder-landtscher beroerten ende ellendicheden, 1616 ...|$|R
40|$|The {{application}} of microcomputer in {{measurement and control}} of biotechnological processes is gaining in popularity. In relation to this, an automation system was developed for the operation of a laboratory pilot scale stirred tank fermenter. The package employed a software interrupt function to create <b>background</b> <b>execution</b> for data acquisition and control operations whilst allowing other tasks being {{carried out in the}} foreground. The system was tested by controlling dissolved oxygen concentration employing a three term controller and good control operatis were achieved...|$|E
40|$|Software Engineers {{supporting}} a large software system often need {{to locate the}} code that performs a specific user feature. One method {{to solve this problem}} is software reconnaissance, which compares execution traces taken when the feature was active with <b>background</b> <b>execution</b> traces when it was not. Software components executed in the first set but not in the second tend {{to be involved in the}} feature of interest. The software reconnaissance method has been tried in a number of contexts and academic software tools, such as the Recon 3 toolset, are freely available. However companies might be more willing to apply this method if they could use commercial, industrial-strength tools, of known reliability. This report describes a study performed with Motorola, Inc. to see if Metrowerks CodeTEST and Klocwork inSight could be used for feature location. Both tools are currently in use in Motorola and are know to be robust and effective. CodeTEST is a dynamic analysis tool and can produce traces of execution, while inSight is a static analysis tool which allows browsing and architectural analysi...|$|E
40|$|Version 2. 0 of Toth's Materials Toolkit runs under Windows {{and prepares}} ASCII input files for popular ab initio {{packages}} such as ABINIT, VASP etc. Those packages, obtainable from their respective developers, may run in desktop or supercomputer setups with Linux or Windows operating systems. The Toolkit input is taken at will from a direct plug into CRYSTMET, with 93000 crystal-structure entries for metals and inorganic compounds, from CIF files of public-domain crystal-structure databases, or cut-and-paste from electronic journals followed by minimal free-format editing. The collection of fully general and highly graphical tools grouped on two command screens operates {{on the structure}} description stored in an editable ASCII screen. After the model has been searched, modified and evaluated in a few keystrokes with the above tools, its ASCII input files for a selection of ab initio packages are produced by selecting the meaningful flags and run options on a dialog. The tedious structure manipulation or decomposition into multiple simulations is performed in the <b>background.</b> <b>Execution</b> is followed by production of a plain-English job report. Four examples among the numerous possible applications of the Toolkit illustrate the fact that daunting topics, like the symmetry of chlorapatite, the voids and channels in the hydrogen-storage material EuNi 5, the energy per unit area of the contact plane for spinel twin in diamond, and the hardness of lonsdaleite versus diamond, are amenable to processing by materials scientists more versed in experiment than theory. Peer reviewed: YesNRC publication: Ye...|$|E
50|$|A job {{scheduler}} {{is a computer}} application for controlling unattended <b>background</b> program <b>execution</b> of jobs. This is commonly called batch scheduling, as execution of non-interactive jobs is often called batch processing, though traditional job and batch are distinguished and contrasted; see that page for details. Other synonyms include batch system, distributed resource management system (DRMS), distributed resource manager (DRM), and, commonly today, workload automation. The data structure of jobs to run {{is known as the}} job queue.|$|R
40|$|D. Phil. This study {{examines}} the <b>background,</b> the <b>execution</b> {{and the consequences}} of French agricultural development aid to thirteen former French colonies in sub-Saharan Africa. The period under analysis is 1960 to 1980. In a continent which experienced an overall downward trend in per capita agricultural production during this period, despite adequate natural resources and higher aid allocations than other parts of the Third World, it is felt that lessons can be learnt from French aid which was consistent and geographically concentrated. The accent of the study lies on improved understanding of French agricultural aid in francophone sub-Saharan Africa as a whole. It does not assess the impact of French aid on agricultural development at country level [...] ...|$|R
40|$|Automatic and {{unconstrained}} {{sign language}} recognition (SLR) in image sequences remains a challenging problem. The variety of signers, <b>backgrounds,</b> sign <b>executions</b> and signer positions makes {{the development of}} SLR systems very challenging. Current methods try to alleviate this complexity by extracting engineered features to detect hand shapes, hand trajectories and facial expressions as an intermediate step for SLR. Our goal is to approach SLR based on feature learning rather than feature engineering. We tackle SLR using the recent advances {{in the domain of}} deep learning with deep neural networks. The problem is approached by classifying isolated signs from the Corpus VGT (Flemish Sign Language Corpus) and the Corpus NGT (Dutch Sign Language Corpus). Furthermore, we investigate cross-domain feature learning to boost the performance to cope with the fewer Corpus VGT annotations...|$|R
40|$|This thesis {{describes}} the <b>background,</b> <b>execution</b> and {{results of a}} study of the feasibility of mixed-mode building cooling strategies involving radiant systems in California’s 16 climate zones. Informed by case studies, building modeling and evaluation literature, detailed climate studies, and past experience, the research team created a parametric building simulation model in EnergyPlus. The simulation model was used in conjunction with Adaptive and Predicted Mean Vote occupant comfort models to evaluate the energy and comfort performance of mixed-mode buildings with radiant cooling by simulating a range of mechanical systems, control strategies, and physical building characteristics in each climate. Energy performance was quantified as kBtu/ft 2 -yr and comfort was quantified using the percentage of occupant hours with more than 20 % of occupants predicted to be dissatisfied, also known as the exceedance percentage. The cooling strategies simulated performed particularly well in moderate coastal climates, but were also able to meet comfort criteria when gains were controlled through building shell improvements and efficient equipment operation. In several climates, the chilled mass of a floor slab charged overnight by water from a cooling tower was sufficient to preserve comfort throughout the day while using approximately 75 % less pump, fan, and chiller energy than a comparable conventional HVAC system. In cases where a cooling tower was insufficient, a chiller was used to improve overnight cooling or to support the all day operation of the slab. The examination of model sensitivity to inputs, and the evaluation of discomfort predicted by the Adaptive Comfort model vs. the Predicted Mean Vote model indicate that site context and occupant expectations will {{play a significant role in}} determining the feasibility of mixed-mode cooling strategies. Results are presented graphically to allow comparisons across and within climates and in the form of regional maps that illustrate the geography of potential feasibility of mixed-mode cooling systems. ...|$|E
40|$|Atmospherics/Weather Works is an {{interdisciplinary}} {{project in the}} sonification of storms and other meteorological events generated directly from data produced by a highly detailed and physically accurate model of weather systems used for research and forecasting. This paper discusses the <b>background,</b> conception, and <b>execution</b> {{of a series of}} sonifications of a historical hurricane and winter snowstorm that resulted in several performances, stereo recordings, a public multi-channel spatialized sound installation, and an online interactive sound listening environment. 1...|$|R
40|$|Presented at the 10 th International Conference on Auditory Display (ICAD 2004) Atmospherics/Weather Works is an {{interdisciplinary}} {{project in the}} sonification of storms and other meteorological events generated directly from data produced by a highly detailed and physically accurate model of weather systems used for research and forecasting. This paper discusses the <b>background,</b> conception, and <b>execution</b> {{of a series of}} sonifications of a historical hurricane and winter snowstorm that resulted in several performances, stereo recordings, a public multi-channel spatialized sound installation, and an online interactive sound listening environment...|$|R
40|$|Crowdsourcing is an {{emerging}} topic within software engineering research. This report presents the protocol for our {{case study of}} crowdsourcing at a multi-national company. The findings of the case study are presented in a paper in {{the proceedings of the}} 36 th International Conference on Software Engineering (2014) (see ref. [37]). This protocol presents additional details that provide more insight regarding the <b>background,</b> design and <b>execution</b> of our study. The research design can also be used for replicating the case study so as to be able to more easily compare different case studies...|$|R
40|$|This Technical Memorandum (TM) {{discusses}} the microgravity experiments carried {{out during the}} later missions of the Apollo program. Microgravity experiments {{took place during the}} Apollo 14, 16, and 17 missions and consisted of four experiments in various materials processing concentrations with two of the four experiments taking place over the course of two missions. Experiments consist of composite casting, electrophoresis, heat flow and convection, and liquid transfer. This TM {{discusses the}} <b>background,</b> the workup, <b>execution,</b> and results of each experiment. In addition, the historical significance of each experiment to future applications/NASA programs is discussed...|$|R
40|$|The {{execution}} {{model for}} mobile, dynamically-linked, object [...] oriented programs {{has evolved from}} fast interpretation to a mix of interpreted and dynamically compiled execution. The primary motivation for dynamic compilation is that compiled code executes significantly faster than interpreted code. However, dynamic compilation, which is performed while the application is running, introduces execution delay. In this paper we present two dynamic compilation techniques that enable high performance execution while reducing {{the effect of this}} compilation overhead. These techniques can be classified as: 1) decreasing the amount of compilation performed (Lazy Compilation), and 2) overlapping compilation with <b>execution</b> (<b>Background</b> Compilation). We firs...|$|R
40|$|Apostasy is a {{conversion}} of the original Muslim, abandoning the faith and following other religion than Islam. In Islamic law, the perpetrator of apostasy will receive punishment of execution according to the jurists’ consensus. The consensus is understood from some hadiths containing the command to execute the apostates {{and the fact of}} execution of apostates in the prophetic period of Muhammad saw. This article analyzes hadiths about the problematics of apostasy from the used terms, the background, and the contexts of the existance of command to execute apostates. The hadiths about apostasy were analyzed through socio-historical view of hadiths by the theory of compromise towards the seemingly contradictive hadiths. The result of analysis shows that the <b>background</b> of <b>execution</b> of apostates tended to the war situation in that period and the social crimes committed by the apostates, indicated by the existance of fact that the apostates committing religious crime, changing verses of al-Quran, received amnesty and were not executed. At present time, execution of apostates is a lively issue of debate. The result of analysis can provide other alternatives since apostate execution is not the only right way and it cannot be applied in any countries...|$|R
40|$|This is a {{critique}} of two articles by Professors Doob and Foltz describing the conception and execution of a group dynamics workshop for Belfast inhabitants {{which took place in}} Scotland in 1972. The theoretical <b>background,</b> practical <b>execution,</b> and subsequent social and political impact of the workshop are assessed. The authors consider that the goals of the exercise were ill-defined and mutually contradictory. Because of this, the workshop was ineffective in its own terms and harmful to many of the participants. In the last ten years, conflict and peace researchers have become more inclined to intervene in situations of violent social conflict. In many cases this intervention takes the form of some kind of &dquo;workshop&dquo; approach typically involving the &dquo;leaders&dquo; or &dquo;representatives&dquo; of conflicting parties and &dquo;experts&dquo; either in the field of social conflict or some aspect of human communications. Such workshops usually aim, through the use of psychodynamic techniques, to increase the level of trust and communica-tion between the warring factions, thus enabling them to come to more &dquo;reasonable&dquo; or &dquo;less destructive&dquo; appreciations of their opponents and the social structure in which they are all involved. On the basis of these new appreciations, the possibility of novel and mutually rewarding social innovation and action can be built. They are, all of them, exercises in serendipity...|$|R
40|$|Project (M. S., Computer Science) [...] California State University, Sacramento, 2012. Statement of Problem: Enterprise Job Schedulers and Rule Engines are an {{expensive}} class of software. According to an IDC report, {{the market for}} job scheduling software licenses in 2009 was 1. 5 billion USD. Over the past 25 years, since job schedulers appeared in the market, they compete on price, but not on solutions. Rule Engines are continuously evolving to solve complex problems for many software organizations. Today???s Job Schedulers and Rule Engines typically provide huge set of features with Graphical User Interfaces over a distributed network of computers to monitor the <b>background</b> <b>executions</b> and setup new rules or jobs. Job Schedulers and Rule Engines share a common goal in orchestrating the integration of real-time business activities over different platforms and application environments. The result of a job execution often needs to be validated against the most probable outcomes. The most probable outcomes {{can be stored in}} a Rule Database as understandable rules to support the Rule Engine as a Centralized Knowledge Base, increasing the ease of rules-invocation, consumption, portability and maintenance. An extensive application would be in an Enterprise-level Job Scheduler performing the ETL (Extract, Transform and Load) process across various database connection pools/multiple applications. As the Job Scheduler only involves in ETL, the data that is loaded needs to be validated against the most probable outcomes. Designing a set of rules to fire after each Load after a job fire validates the data before it is further used in the application. This provides reliable data for the business workflow enhancing the business process as a whole. Approach: FIT stands for Fully Integrated Tracking. FIT Scheduler is a lightweight open-source solution for organizations requiring client-server Enterprise Job Schedulers who cannot bear the over-priced licenses. FIT Scheduler uses Quartz Scheduling Framework [3] as back-end, Google Web Toolkit framework combined with Sencha???s Ext-GWT library for the web-based front-end and various other open-source projects like Jersey [6] implementation of Java API for RESTful Web-services, Groovy programming language, Apache POI ??? API for Microsoft Documents [11]. Conclusions Reached: It is a platform independent and vendor independent solution to perform the process of Extraction, Transformation and Data Load within an Application Server across multiple applications synchronously. It leverages a common framework to manage the connection pools to achieve data synchronization. Aimed at providing a configuration-driven approach to the end-user, the functionality of FIT Scheduler is highly extendable in future. It has built in features to overcome Daylight Saving Time, evaluate the data loaded using a custom-built rule engine, re-run failed jobs, launch new jobs, monitor the scheduling system, increase scheduler resources on the fly etc. It acquires various powerful features from Quartz like fail-over, load balancing, and clustering. Computer Scienc...|$|R
40|$|Software behavior, as {{the basis}} for {{evaluating}} softwares dynamic trustiness, has become a hot issue in worldwide information security area. To describe the software behavior, a model named Software Behavior Trace (SBT) is proposed which regards not only the software execution process but also the <b>execution</b> <b>background.</b> We also give series of rules to simplify the SBT and explore the usage of SBT in software's trustiness evaluation. SBT consists of operation trace and function trace and it characterizes the <b>execution</b> process and <b>background,</b> respectively and it is able to describer software behavior completely. After simplified, the time and space overheads of trustiness evaluation reduce a lot. In order to improve the accuracy of trustiness evaluation, we use evaluating strategy of determining multiple attributes weights by information entropy. Simulation experiments demonstrate that trustiness evaluation model based on simplified software behavior trace has improved evaluation effectively without performance degradation...|$|R
40|$|<b>Background</b> 1 3 <b>Execution</b> Model 1 3. 1 Model for the Algorithm................................. 1 3. 2 Execution {{model for}} the Platform for Implementation................. 2 3. 3 A Distributed Algorithm for the Shortest Path Problem................ 2 4 Specific Problem Statement 3 5 Solution Strategy 3 5. 1 Data Structures...................................... 3 6 Results 4 7 Conclusion 5 7. 1 Future Work........................................ 5 List of Figures 1 Data Structures for Graph Data............................. 3 2 Data Structures....................................... 4 3 A Directed Graph with Weighted Edges......................... 4 Distributed Computation on Graphs - Shortest Path Algorithm 575 A 1 Abstract The {{diffusing}} computation paradigm as proposed by Dijkstra and Schoelten is applied to solve a class of graph problems, notably the shortest path algorithm. Chandy and Misra suggested a distributed algorithm for computing the shortest paths from a single vertex to all other vertices, {{in the presence of}} negative cycles [1]...|$|R
40|$|Since 1942 the Dieppe Raid {{has been}} the subject of much {{controversy}} and debate concerning its political and military <b>background,</b> aims, plans, <b>execution</b> and supposed “lessons learned. ” Although historians have documented their arguments well, they have not examined accurately or in any detail the operations of The Calgary Regiment (Tank), 14 Canadian Army Tank Regiment (14 CATR), Canadian Armoured Corps. Some misunderstandings and myths concerning the tanks and men, their performance and conditions affecting their actions, must be dispelled. At this point it is worth noting that not only was 14 CATR the first Canadian armoured unit ever to go into action, it was the first time in history tanks were used in an amphibious landing, as well as the baptism of fire for the latest British equipment, such as the Tank Landing Craft (TLC), the new Churchill tank and its 6 -pounder gun...|$|R
40|$|Testing complex safety {{critical}} software always was difficult task. Development of automated techniques for error detection {{is even more}} difficult. Well known techniques for checking software are model checking static analysis and testing. Symbolic execution is a technique {{that is being used}} to improve security, to find bugs, and to help in debugging. A symbolic execution engine is basically an interpreter that figures out how to follow all paths in a program. It is a static code analysis technique. This work presents symbolic <b>execution</b> <b>background,</b> current state, analysis the possibilities of implementation on the. Net framework and platform. The work describes the master project – bug tracking software “Crunchbug” and the tool – Symex (symbolic execution engine) for. Net platform. Symex is white box model based automatic unit test generator and it is evaluated against two other tools – Microsoft Pex and framework that generates unit test inputs random. Detailed experiments made to cover symbolic execution possibilities with proprietary benchmarks and real code from the master project...|$|R
40|$|Idle {{resources}} can be exploited {{not only to}} run important local tasks such as data replication and virus checking, but also to make contributions to society by participating in open computing projects like SETI@home [2]. When executing background processes to utilize such valuable idle resources, we need to explicitly control them so that the user will not be discouraged from exploiting idle resources by foreground performance degradation. Unfortunately, common priority-based schedulers lack such explicit execution control. In addition, to encourage active use of idle resources, a mechanism for controlling background processes should not require modifications to the underlying operating system or user applications. If such modifications are required, the user {{may be reluctant to}} employ the mechanism. In this paper, we argue that we can reasonably detect resource contention between foreground and background processes and properly control <b>background</b> process <b>execution</b> at the user level. We infer the existence of resource contention from the approximated resource shares of background processes. Our approach takes advantage of dynamically instrumented probes, which are becoming increasingly popular, in estimating the resource shares. Also, it considers different resource types in combination and can handle varied workloads, including multiple background processes. We show that our system effectively avoids the performance degradation of foreground activities by suspending background processes in an appropriate fashion. Our system keeps the increase in foreground execution time due to background processes below 16. 9 %, or much lower in most of our experiments. Also, we extend our approach to address undesirable resource allocations to CPU-intensive processes that can occur in multiprocessor environments...|$|R
40|$|Abstract <b>Background</b> Repeated <b>execution</b> of a tactile task enhances task {{performance}}. In {{the present}} study we sought to improve tactile performance with unattended activation-based learning processes (i. e., focused stimulation of dermal receptors evoking neural coactivation (CA)). Previous {{studies show that the}} application of CA to a single finger reduced the stationary two-point discrimination threshold and significantly increased tactile acuity. These changes were accompanied by an expansion of the cortical finger representation in primary somatosensory cortex (SI). Here we investigated the effect of different types of multifinger CA on the tactile performance of each finger of the right hand. Results Synchronous and asynchronous CA was applied to all fingers of a subject's dominant hand. We evaluated changes in absolute touch thresholds, static two-point discrimination thresholds, and mislocalization of tactile stimuli to the fingertips. After synchronous CA, tactile acuity improved (i. e., discrimination thresholds decreased) and the frequency of mislocalization of tactile stimuli changed from directly neighboring fingers to more distant fingers. On the other hand, asynchronous CA did not significant improve tactile acuity. In fact, there was evidence of impaired tactile acuity. Multifinger CA with synchronous or asynchronous stimulation did not significantly alter absolute touch thresholds. Conclusion Our results demonstrate {{that it is possible to}} extend tactile CA to all fingers of a hand. The observed changes in mislocalization of tactile stimuli after synchronous CA indicate changes in the topography of the cortical hand representation. Although single-finger CA has been shown to improve tactile acuity, asynchronous CA of all fingers of the hand had the opposite effect, suggesting the need for synchrony in multifinger CA for improving tactile acuity. </p...|$|R
40|$|In the {{recovery}} of failed processes in a distributed program, causal logging schemes offer several benefits. These benefits include no rollback of unfailed processes and simple approaches to output commit. Unfortunately, previous approaches to {{the recovery}} of multiple simultaneous failures require that the distributed execution be blocked or that recovering processes coordinate. The latter requires assumptions which are not satisfactory. In this paper we present a solution that has neither of these drawbacks. Message logging is an important technique for recovering from failures in distributed programs. This technique logs {{the order in which}} messages are received. By assuming that receive ordering is the only source of non-determinism, execution is recoverable using this ordering. Pessimistic message logging [4, 11] forces a process to wait before sending any message while the message log is written to stable storage. Optimistic logging methods [9, 12, 13, 15] (and the similar sender based logging [8, 14]) assume failures are rare and therefore allow ordering information to be lost in a failure. (That is, a message is logged in the <b>background</b> while <b>execution</b> proceeds). Consequently, received messages and any sends that depend on them may not be recoverable. This may then require that unfailed processes roll back their execution as well. Causal message logging sends message receive ordering information with each message. This information includes receives and their causal history since the last send. The Manetho approach [6] uses this method. In family-based message logging (FBL) [2] causal history information for only K processes is included. This method then tolerates K simultaneous failures rather than all processes in the system (as with Manetho and the other logging methods.) The causal message logging approach offers advantage...|$|R
40|$|International audienceComponent-based systems (including {{distributed}} {{programs and}} multiagent systems) involve {{a lot of}} coordination. This coordination {{is done in the}} background, and is transparent to the operation of the system. The reason for this overhead is the interplay between concurrency and non-deterministic choice: processes alternate between progressing independently and coordinating with other processes, where coordination can involve multiple choices of the participating components. This kind of interactions appeared as early as some of the main communication-based programming languages, where overhead effort often causes a restriction on the possible coordination. With the goal of enhancing the efficiency of coordination for component-based systems, we propose here a method for coordination-based on the precalculation of the knowledge of processes and coordination agents. This knowledge can be used to lift part of the communication or synchronization that appears in the <b>background</b> of the <b>execution</b> to support the interaction. Our knowledge-based method is orthogonal to the actual algorithms or primitives that are used to guarantee the synchronization: it only removes messages conveying information that knowledge can infer...|$|R
40|$|<b>Background.</b> Quick step <b>execution</b> {{may prevent}} falls when balance is lost. Lateral steps often {{emerge as a}} conse-quence of frontal plane {{instability}} arising after the first rapid step. In this study, we suggest a new analysis, focusing on the variability of the frontal plane fluctuations of center of pressure (CoP), that is, mediolateral instability, and their changes over time during and immediately following rapid voluntary stepping in older and younger adults in single- and dual-task conditions. This {{may be useful in}} understanding age-related alterations in the locomotor control system. Methods. Seventeen older adults, who live independently in the community, and 16 younger adults performed rapid forward voluntary stepping under single- and dual-task conditions. The average mediolateral CoP fluctuations, that is, the average distance the CoP travels from side to side in the frontal plane over time, standard deviation, and the coefficient of variation of mediolateral CoP fluctuation were extracted and calculated from CoP data during and immediately following rapid voluntary stepping using a force plate. Results. We found an age-related increase in the coefficient of variation that represents the variability of frontal plane fluctuations and no significant differences in the average and standard deviations of frontal plane fluctuations. Cognitiv...|$|R
40|$|This cometary {{occultation}} observation from June 1983 {{remained to}} be formally reported due to other preoccupations of the authors. It {{was presented in}} seminars to colleagues at Ooty, Bangalore and elsewhere. We now write it up {{as we have been}} asked about it by various colleagues at various times, and feel we owe it to them to put it firmly on record. ] We planned and observed with Ooty Radio Telescope the occultation with Comet 1983 e Sugano-Saigusa-Fujikawa of the extragalactic radio source 2019 + 098 = 3 C 411. The results are presented formally for the first time, along with a brief account of other cometary occultations and general <b>background</b> of planning, <b>execution</b> and interpretation of such observations which will be useful for other future observers. The occultation occurred at 07 : 52 IST on 12 th June 1983. It amounted to 25 % peak to peak fluctuation in the flux density of the radio source. The rough predicted occultation time was 07 : 24 IST. We interpret the results after refining the occultation time to allow for various effects. Comment: 9 pages, 4 figures, 2 tables, paper to be enhanced shortl...|$|R
40|$|Abstract <b>Background</b> Abnormal <b>execution</b> {{of several}} {{movements}} {{in a sequence}} is a frequent finding in schizophrenia. Successful performance of such motor acts requires correct integration of cortico-subcortical processes, particularly those related to cerebellar functions. Abnormal connectivity between cortical and cerebellar regions with resulting cognitive dysmetria has been proposed as the core dysfunction behind many {{signs and symptoms of}} schizophrenia. The aim {{of the present study was}} to assess if these proposed abnormalities in connectivity are a unifying feature of schizophrenia, or, rather, reflect a specific symptom domain of a heterogeneous disease. We predicted that abnormal functional connectivity between the motor cortex and cerebellum would be linked with abnormal performance of movement sequencing. Methods We examined 24 schizophrenia patients (SCH) and 24 age-, sex-, and handedness-matched healthy controls (HC) using fMRI during a modified finger-tapping task. The ability to perform movement sequencing was tested using the Neurological Evaluation Scale (NES). The subjects were categorized into two groups, with (SQ+) and without (SQ-) movement sequencing abnormalities, according to the NES-SQ score. The effects of diagnosis and movement sequencing abnormalities on the functional connectivity parameters between the motor cortex and cerebellum (MC-CRBL) and the supplementary motor cortex and cerebellum (SMA-CRBL) activated during the motor task were analyzed. Results We found no effect of diagnosis on the functional connectivity measures. There was, however, a significant effect on the SQ group: SQ + patients showed a lower level of MC-CRBL connectivity than SQ- patients and healthy controls. Moreover, the level of MC-CRBL and SMA-CRBL negatively correlated with the magnitude of NES-SQ abnormalities, but with no other NES domain. Conclusions Abnormal cortico-cerebellar functional connectivity during the execution of a motor task is linked with movement sequencing abnormalities in schizophrenia, but not with the diagnosis of schizophrenia per se. It seems that specific patterns of inter-regional connectivity are linked with corresponding signs and symptoms of clinically heterogeneous conditions such as schizophrenia. </p...|$|R
40|$|Microsaccades are {{miniature}} saccades occurring once {{or twice}} per second during visual fixation. While microsaccades and saccades share similarities at the oculomotor level, the functional roles of microsaccades are still debated. In this study, we examined {{the hypothesis that the}} microsaccadic activity is affected by the type of noisy <b>background</b> during the <b>execution</b> of a particular discrimination task. Human subjects had to judge the orientation of a tilted stimulus embedded in static or dynamic backgrounds in a forced choice-task paradigm, as adapted from Rucci, Iovin, Poletti, and Santini (2007). Static backgrounds induced more microsaccades than dynamic ones only during the execution of the discrimination task. A directional bias of microsaccades, dictated by the stimulus orientation, was temporally coupled with this period of increased activity. Both microsaccade rates and orientations were comparable across background types after the response time although subjects maintained fixation {{until the end of the}} trial. This represents a background-specific modulation of the microsaccadic activity driven by attentional demands. The visual influence of microsaccades on discrimination performances was modeled at the retinal level for both types of backgrounds. A higher simulated microsaccadic activity was necessary for static backgrounds in order to achieve discrimination performance scores comparable to that of dynamic ones. Taken together, our experimental and theoretical findings further support the idea that microsaccades are under attentional control and represent an efficient sampling strategy allowing spatial information acquisition...|$|R
40|$|This thesis {{describes}} {{the design and}} implementation of a framework, PowerScan, which provides the ability to combine multiple tools {{in the analysis of}} a malware sample. The framework utilizes XML configuration in order to provide extendability so that new tools can be added post compilation without significant effort. The framework deals with three major types of malware analysis: 1. Surface scan of a sample with multiple on-demand anti-virus engines. 2. Execution of malware sample with real-time (on-access) anti-virus engines running in the <b>background.</b> 3. <b>Execution</b> of malware sample with different dynamic analysis solutions running. These tools may monitor the file system, registry, network or other aspects of the operating systems during execution. The reasoning behind each of these phases are: 1. Using multiple scanners increases the probability {{that at least one of}} the vendors has created a detection signature for the given malware. 2. Executing the sample ensures that the malware code sooner or later will be written to disk or memory. This should greatly enhance detection rate for samples obfuscated using packers with encryption or other techniques, as the code at some point must be deobfuscated before execution. Additionally, on-demand scanners might use more advanced (and resource consuming) techniques when monitoring files executed on the system. As for surface scanning, the odds of correctly identifying the malware also increases when using more scanners. 3. Although several good sandbox analysis tools exist, the solution presented here allows the malware analyst choose which analysis tools to use - and even use different tool for analyzing the same aspect of the execution. A thorough description of both design, implementation and testing is given in the report. In addition to the implementation of the PowerScan framework described above, the theory behind all involved components is presented. This includes description of the Microsoft Windows platform (which is used for executing malware in PowerScan, and the one definitely most targeted by malware at the time of writing), virtualization (which is used in the virtual machines), anti-virus technology, malware hiding techniques and more. Surveys of the usability of different anti-virus engines and dynamic analysis tools in the framework have been conducted and are presented in the appendices, together with a comprehensive user guide. </p...|$|R
40|$|Abstract: Web {{crawlers}} has dependably {{been the}} picked mode of data recovery (IR) frameworks. Clients {{are no more}} substance with issuing straightforward navigational inquiries. A complex question, for example, fly out game plan must {{be broken down into}} various mutually dependent steps over a time of time. Case in point, a client might first pursuit on conceivable objectives, timetable, occasions, and so on. In the wake of choosing when and where to go, the client might then hunt down the most suitable game plans for air tickets, rental autos, lodging, dinners, and so on. Each one stage obliges one or more inquiries, and each one inquiry brings about one or more clicks on applicable pages. Essential word based web crawlers can't help this sort of progressive questions. So we propose to utilize Random walk engendering routines that build client profile focused around his certifications from its client look history vaults. Joined together with click focuses driven click charts of client pursuit practices the IR framework can help complex questions for future appeals at diminished times. Irregular walk engendering over the question combination chart strategies help complex inquiry missions in IR frameworks at diminished times. For making the IR Systems viable and dynamic we likewise propose to utilize these pursuit missions as auto complete peculiarities in comparable inquiry proliferations. Biasing the positioning of indexed lists can likewise be given utilizing any positioning algorithms(top-k algorithms). supporting these systems yields dynamic execution in IR frameworks, by giving advanced client questioning <b>background.</b> A handy <b>execution</b> of the proposed framework approves our case...|$|R
40|$|Abstract <b>Background</b> Planning and <b>execution</b> of {{reaching}} requires {{a series of}} computational processes that involve localization of both the target and initial arm position, and the translation of this spatial information into appropriate motor commands that bring the hand to the target. We have investigated the effects of shifting the visual field on visuomotor control using a virtual visual environment {{in order to determine}} how changes in visuo-spatial relations alter motor planning during a reach. Methods Five healthy subjects were seated in front of an immersive, stereo virtual scene while reaching for a visual target that remained stationary in space or unpredictably shifted to a second position (either to the right or left of the first target) with different inter-stimulus intervals. Motion of the scene either matched the motion of their head or was rotated counter clockwise at 130 deg/s in the roll plane. Results Initial results suggested that both the temporal and spatial aspects {{of reaching}} were affected by a rolling visual field. Subjects were able to amend ongoing motion to match target position regardless of scene motion, but the presence of visual field motion produced significantly longer pauses during the reach movement when the target was shifted in space. In addition, terminal arm posture exhibited a drift in the direction opposite to the roll motion. Conclusion These findings suggest that roll motion of the visual field of view interfered with the ability to imultaneously process two consecutive stimuli. Observed changes in arm position following the termination of the reach suggest that subjects were compensating for a perceived change in their visual reference frame. </p...|$|R
