22|2218|Public
50|$|It {{can be used}} {{either through}} command line, text based or <b>batch</b> <b>processing</b> <b>mode.</b>|$|E
50|$|A {{common purpose}} of tag editors is to correct or update {{metadata}} and enable sorting and grouping of multimedia files, for example music collections. This often {{happens in a}} <b>batch</b> <b>processing</b> <b>mode</b> so that one doesn't have to manually edit every file on its own.|$|E
50|$|Usually, {{integration}} {{requires some}} kind of custom code. Users can connect to both relational and Versant databases using ORM products. They can load objects either from a relational database or Versant and then with some minor code implementation, disconnect those objects from the source and write them to a target. This {{can be used for}} import/export in a <b>batch</b> <b>processing</b> <b>mode</b> for integration with other database systems.|$|E
50|$|Although OCRFeeder is a GUI tool, it {{can also}} run in command line mode (as ocrfeeder-cli), which may be {{a useful tool for}} {{automatic}} document <b>batch</b> <b>processing.</b> In this <b>mode</b> OCRFeeder uses the default OCR engine, which the user can set in the application's preferences.|$|R
50|$|The ARSA system {{continuously}} collects xenon {{from the}} air in <b>batch</b> <b>mode,</b> <b>processing</b> approximately 48 m3 in an 8-hour period. The average amount of xenon collected in this period is approximately 2-cc. The xenon gas is then transferred into nuclear detection system consisting of a beta-gamma coincidence spectrometer.|$|R
40|$|The {{development}} and {{capabilities of the}} Montana geodata system are discussed. The system is entirely dependent on the state's central data processing facility which serves all agencies and is therefore restricted to <b>batch</b> <b>mode</b> <b>processing.</b> The computer graphics equipment is briefly described along with its application to state lands and township mapping {{and the production of}} water quality interval maps...|$|R
5000|$|Data set names (DSNs, {{mainframe}} {{term for}} filenames) are organized in a hierarchy whose levels are separated with dots, e.g. [...] "DEPT01.SYSTEM01.FILE01". Each {{level in the}} hierarchy can be up to eight characters long. The total filename length is a maximum of 44 characters including dots. By convention, the components separated by the dots are used to organize files similarly to directories in other operating systems. For example, there were utility programs that performed similar functions to those of Windows Explorer (but without the GUI and usually in <b>batch</b> <b>processing</b> <b>mode)</b> - adding, renaming or deleting new elements and reporting all {{the contents of a}} specified element. However, unlike in many other systems, these levels are not usually actual directories but just a naming convention (like the original Macintosh File System, where folder hierarchy was an illusion maintained by the Finder). TSO supports a default prefix for files (similar to a [...] "current directory" [...] concept), and RACF supports setting up access controls based on filename patterns, analogous to access controls on directories on other platforms.|$|E
40|$|IDENTIFIERS Batch Process; Conversational Mode The {{effect of}} {{different}} communicating modes to computers on students ' attitudes toward programing was studied. In a computer-related course, 13 students used <b>batch</b> <b>processing</b> <b>mode</b> {{to solve problems}} on the computer, while 12 other students used conversational mode to solve the same problems. It was found that those students accessing the computer in conversational mode developed {{more positive attitudes toward}} programing (as measured by the author's attitude test) than the students using batch processing. Also the grades obtained in the time-sharing environment did not seem to affect the students ' final attitudes towards programing. The major:Ay of two-year college computer centers operate under <b>batch</b> <b>processing</b> <b>mode.</b> This could be a factor in the high student attrition rate in introductory programing classes. To lower this rate, and t...|$|E
40|$|The systems {{organization}} of the SMART programs is discussed as im-plemented for operation in a <b>batch</b> <b>processing</b> <b>mode</b> oa the IBM 360 / 65. Covered in particular are the basic input and text analysis routines, the document clustering programs, the search routines and the feedback opera-tions. Sample computer output is shown in each case to illustrate the operations. 1...|$|E
5000|$|... 1970s was The Move to Online Information: During {{this time}} many {{institutions}} {{are making the}} move from <b>batch</b> <b>processing</b> to online <b>modes,</b> from mainframe computers to more modern computers. With the advancement of technology the traditional boundaries began to fade and library schools started to add [...] "information" [...] in the titles of their programs. ASIS sponsored a bicentennial conference which focused {{on the role of}} information in the country's development. The group also participated in the planning and implementation of the White House Conference on Library and Information Services. [...] ".|$|R
40|$|The present chapter surveys {{computational}} algorithms {{for solving}} the independent component analysis (ICA) problem. Most of these algorithms rely on gradient or Newton iterations for contrast function maximization, and can work either in <b>batch</b> or adaptive <b>processing</b> <b>mode.</b> After briefly summarizing the common tools employed in their design and analysis, the chapter reviews {{a variety of}} iterative techniques ranging from pioneering neural network approaches and relative (or natural) gradient methods to Newton-like fixed-point algorithms as well as methods based on some form of optimal step-size coefficient...|$|R
40|$|Version 1. 02, Y 2 K compliant, 29 June 1999 The ANTHRO {{software}} package {{was developed for}} calculating the WHO/CDC anthropometric indices with dBase files on IBM-compatible microcomputers. The indices can be calculated in a <b>batch</b> <b>processing</b> or calculator <b>mode.</b> ANTHRO also {{has the ability to}} perform some standard data analyses. Data can be imported from or exported from or to either fixed- column or comma-delimited files. anthro/anth_doc. pdfWhat is ANTHRO? [...] A general reminder on backing up files [...] Getting started [...] Limitations of NCHS/CDC/WHO reference [...] Setup [...] Main menu [...] Process dBase file [...] Standard analysis [...] Other [...] Other software for anthropometric calculations [...] Other information on the development, use, and interpretation of anthropometry data [...] Trouble shooting [...] History of ANTHRO [...] Reference...|$|R
30|$|To {{evaluate}} the ASR performance, a training set, a development test set (Dev.) {{and a final}} evaluation test set (Eval.) are provided. The SimData set consists of 1484 utterances from 10 speakers for Dev. and 2176 utterances from 28 speakers for Eval., respectively. The RealData set consists of 179 utterances from 5 speakers for Dev. and 372 utterances from 10 speakers for Eval., respectively. For a multi-condition training set with 7861 anechoic utterances from 92 speakers, 24 RIRs (cf. Fig. 3) and several types of stationary noise signals were recorded according to the 6 reverberant conditions mentioned above. Unlike for our workshop paper [12] that covered the 1 ch ASR task in both the full batch processing and the utterance-based <b>batch</b> <b>processing</b> <b>mode,</b> here {{we focus on the}} 1 ch scenarios only in the utterance-based <b>batch</b> <b>processing</b> <b>mode</b> for which each utterance is processed separately, since this provides the maximum potential for real-time applications.|$|E
30|$|Cong et al. [38] {{proposed}} a privacy-preserving public auditing system for data storage security in cloud computing. Specifically, the proposed system introduced the homomorphic linear authenticator with the random masking {{to prevent a}} TPA in accessing {{the contents of the}} customer’s outsourced data during the auditing process. The proposed system enables a TPA to perform multiple auditing tasks in a <b>batch</b> <b>processing</b> <b>mode</b> for improved efficiency.|$|E
40|$|A {{computer}} system uses several serial files. The files reside on a direct-access storage device in which storage space is limited. Records {{are added to}} the files either by jobs in <b>batch</b> <b>processing</b> <b>mode,</b> or by on-line transactions. Each transaction (or job) generates a demand vector which designates the space required in each file for record addition. Whenever one file runs out of space, the system must be reorganized. This paper considers several criteria for best allocating storage space to the files...|$|E
40|$|International Telemetering Conference Proceedings / October 25 - 28, 1993 / Riviera Hotel and Convention Center, Las Vegas, NevadaBoeing's Test Data Retrieval System {{not only}} {{acts as an}} {{interface}} between the Airborne Data Acquisition System and a mainframe computer but also does <b>batch</b> <b>mode</b> <b>processing</b> of data at faster than real time. Analysis engineers request time intervals and measurements of interest. Time intervals and measurements requested are acquired from the flight tape, converted to first order engineering units, and output to 3480 data cartridge tape for post processing. This allows all test data to be stored and only the data of interest to be processed at any given time...|$|R
50|$|<b>Batch</b> <b>{{processing}}</b> {{dates to}} the late 19th century, in the processing of data stored on decks of punch card by unit record equipment, specifically the tabulating machine by Herman Hollerith, used for the 1890 United States Census. This was the earliest use of a machine-readable medium for data, rather than for control (as in Jacquard looms; today control corresponds to code), and thus the earliest processing of machine-read data was <b>batch</b> <b>processing.</b> Each card stored a separate record of data with different fields: cards were processed by the machine one by one, {{all in the same}} way, as a <b>batch.</b> <b>Batch</b> <b>processing</b> continued to be the dominant <b>processing</b> <b>mode</b> on mainframe computers from the earliest days of electronic computing in the 1950s.|$|R
40|$|<b>Batch</b> <b>processing,</b> {{a primary}} <b>mode</b> of {{computing}} in mainframes and supercomputers, is becoming important for networked systems as the computing environments {{become more and}} more distributed. In this paper, we discuss the architectural and design considerations, and some important implementation issues of Lsbatch, a distributed batch system for large [...] scale, heterogeneous computer systems. Lsbatch supports batched submission and execution of parallel as well as sequential jobs in a system of up to several thousand hosts with possibly different architectures, Unix operating system varieties, and power. Implemented on top of the Utopia network operating system [5] as a distributed utility, Lsbatch takes advantage of the rich set of resource and load information services and efficient remote execution mechanisms of Utopia and provides scheduling algorithms to fully utilize the computing resources scattered around a distributed system. To the user, Lsbatch appears very much like a flexible sin [...] ...|$|R
30|$|Our {{previous}} {{contribution to}} the REVERB challenge [12] proposed a combined system including speech enhancement, robust feature extraction, acoustic model adaptation, posterior decoding, and word hypothesis fusion of multiple ASR systems for the REVERB single-channel (1 ch) ASR task. Compared to the REVERB challenge baseline results of the final evaluation test set, an absolute improvement of average word error rate (WER) of 12.43 % in the utterance-based <b>batch</b> <b>processing</b> <b>mode</b> and of 9.42 % in the full <b>batch</b> <b>processing</b> <b>mode</b> were achieved in [12]. For the single-channel scenario, the submitted system in [12] showed the best performance amongst all the submitted results which solely used the ASR back-end system based on the hidden Markov model toolkit (HTK) [13] that was provided as baseline system by the REVERB challenge. It should of course be noted, that by far better results were obtained with more advanced ASR back-end technologies, e.g., feature transformation/adaptation from the Kaldi toolkit [14] in [10], or deep neural networks (DNNs) in [11]. In general, a gap of 50 % relative difference w.r.t. WERs exists between the results using the provided baseline ASR back-end recognizer of the REVERB challenge and those using more advanced back-end recognizers. For instance, we achieved an average WER of 42.12 % in [12] with the real recording data in the utterance-based <b>batch</b> <b>processing</b> <b>mode,</b> while the best challenge result under this processing mode was 20.30 % by [11]. Such a significant boost also motivates the extensions of our work in this contribution by combining our front-end technologies with state-of-the-art ASR back-end strategies such as using DNNs to generate bottleneck (BN) features [15], and subspace Gaussian mixture models (SGMMs) [16], as well as DNN-based acoustic modeling [17]. Our proposed front-end is composed of two components. One is the speech enhancement system aiming at suppressing the interference signal components, i.e., the noise and late reverberation which significantly degrade ASR performance [3, 18]. The other component is the extraction of robust features [6] in adverse environments, {{which are based on}} findings in the auditory processing of mammals.|$|E
40|$|TARGET 2 is the RTGS {{system for}} the euro, offered by the Eurosystem. It {{is used for the}} {{settlement}} of central bank operations, large-value euro interbank transfers as well as other euro payments. It provides real-time processing, settlement in central bank money and immediate finality. A real-time gross settlement system, such as TARGET 2 is a payment system in which processing and settlement take place continuously ("in real time") rather than in <b>batch</b> <b>processing</b> <b>mode.</b> Like this, transactions can be settled with immediate finality. "Gross settlement" means that each transfer is settled individually rather than on a net basis...|$|E
40|$|Abstract. The most {{elementary}} knowledge {{available in}} connection with image motion computation can be informally expressed as “structures do not jump”. We present a novel PDE-based representation of image motion exploiting this knowledge. Our distributed-parameter approach takes into account spatial context, unlike Kalman filters applied to point features separately. It performs spatio-temporal regularization in a recursive online fashion, unlike previous variational approaches evaluating entire spatio-temporal image volumes in a <b>batch</b> <b>processing</b> <b>mode.</b> Deviations from the expected velocity distribution generate vector fields that may serve as attentional mechanism for a superordinate processing stage. We briefly speculate about relations of our approach to perceptual phenomena like motion aftereffects...|$|E
40|$|Aggregation in {{traditional}} database systems is performed in batch mode: a query is submitted, the system processes a {{large volume of}} data {{over a long period}} of time, and an accurate answer is returned. <b>Batch</b> <b>mode</b> <b>processing</b> has long been unacceptable to users. In this paper we describe the need for online aggregation processing, in which aggregation operators provide ongoing feedback, and are controllable during processing. We explore a number of issues, including both user interface needs and database technology required to support those needs. We describe new usability and performance goals for online aggregation processing, and present techniques for enhancing current relational database systems to support online aggregation. 1. Introduction Aggregation is an increasingly important operation in today's relational database systems. As data sets grow larger, and users (and their interfaces) become more sophisticated, there is an increasing emphasis on extracting not just specific [...] ...|$|R
40|$|Summingbird is an {{open-source}} domain-specific language {{implemented in}} Scala {{and designed to}} integrate online and batch MapReduce computations in a single framework. Summingbird programs are written using dataflow abstractions such as sources, sinks, and stores, and can run on different execution platforms: Hadoop for <b>batch</b> <b>processing</b> (via Scalding/Cascading) and Storm for online <b>processing.</b> Different execution <b>modes</b> require different bindings for the dataflow abstractions (e. g., HDFS files or message queues for the source) but do not require any changes to the program logic. Furthermore, Summingbird can operate in a hybrid <b>processing</b> <b>mode</b> that transparently integrates batch and online results to efficiently generate up-to-date aggregations over long time spans. The language was designed to improve developer productivity and address pain points in building analytics solutions at Twitter where often, the same code needs to be written twice (once for <b>batch</b> <b>processing</b> and again for online processing) and indefinitely maintained in parallel. Our key insight is that certain algebraic structures provide the theoretical foundation for integrating <b>batch</b> and online <b>processing</b> in a seamless fashion. This means that Summingbird imposes constraints on the types of aggregations that can be performed, although in practice we have not found these constraints to be overly restrictive for {{a broad range of}} analytics tasks at Twitter. 1...|$|R
40|$|This note {{describes}} a software utility, called X-batch which addresses two pressing issues typically faced by functional {{magnetic resonance imaging}} (fMRI) neuroimaging laboratories (1) analysis automation and (2) data management. The first issue is addressed by providing a simple <b>batch</b> <b>mode</b> <b>processing</b> tool for the popular SPM software package ([URL] ucl. ac. uk/spm/; Welcome Department of Imaging Neuroscience, London, UK). The second is addressed by transparently recording metadata describing all aspects of the batch job e. g., subject demographics, analysis parameters, locations and names of created files, date and time of analysis, and so on). These metadata are recorded as instances of an extended version of the Protégé-based Experiment Lab Book ontology created by the Dartmouth fMRI Data Center. The resulting instantiated ontology provides a detailed record of all fMRI analyses performed, and as such can be part of larger systems for neuroimaging data management, sharing, and visualization. The X-batch system is in use in our own fMRI research, and is available for download at [URL]...|$|R
40|$|The Model is {{described}} along with data preparation, determining model parameters, initializing and optimizing parameters (calibration) selecting control options and interpreting results. Some background information is included, and appendices contain a dictionary of variables, a source program listing, and flow charts. The model was operated on an IBM System/ 360 Model 44, using a model 2250 keyboard/graphics terminal for interactive operation. The {{model can be}} set up and operated in a <b>batch</b> <b>processing</b> <b>mode</b> on any System/ 360 or 370 that has the memory capacity. The model requires 210 K bytes of core storage, and the optimization program, OPSET (which was used previous to but not in this study), requires 240 K bytes. The data band for one small watershed requires approximately 32 tracks of disk storage...|$|E
40|$|Programs {{in basic}} FORTRAN 4 are described, which fall into three catagories: (1) {{interactive}} programs {{to be executed}} under time sharing (BTM); (2) non interactive programs which are executed in <b>batch</b> <b>processing</b> <b>mode</b> (BPM); and (3) large non interactive programs which require more memory than {{is available in the}} normal BPM/BTM operating system and must be run overnight on a special system called XRAY which releases about 45, 000 words of memory to the user. Programs in catagories (1) and (2) are stored as FORTRAN source files in the account FSNYDER. Programs in catagory (3) are stored in the XRAY system as load modules. The type of file in account FSNYDER is identified by the first two letters in the name...|$|E
40|$|Abstract—The {{competition}} among wireless data service providers brings in {{an option for}} the unsatisfied customers to switch their providers, which is called churning. The implementation of Wireless Local Number Portability (WLNP) is expected to further increase the churn rate (the probability of users switching the provider). However, the existing resource management algorithms for wireless networks fail to fully capture the far-reaching impact of this unforeseen competitiveness. From this perspective, we first formulate noncooperative games between the service providers and the users. A user’s decision to leave or join a provider {{is based on a}} finite set of strategies. A service provider can also construct its game strategy set so as to maximize their utility (revenue) considering the churn rate. Based on the game theoretic framework, we propose an integrated admission and rate control (ARC) framework for CDMA-based wireless data networks. The admission control is at the session (macro) level while the rate control is at the link layer packet (micro) level. Two admission control modes will be considered—one-by-one mode and <b>batch</b> <b>processing</b> <b>mode,</b> in which multiple users are admitted at a time. We show that: 1) for the one-by-one mode, the Nash equilibrium using pure strategy can be established for both under-loaded and fully-loaded systems and 2) for <b>batch</b> <b>processing</b> <b>mode,</b> there is either an equilibrium in pure strategy or a dominant strategy exists for the service provider. Therefore, the providers have clearly defined admission criteria as outcome of the game. Users are categorized into multiple classes and offered differentiated services based on the price they pay and the service degradation they can tolerate. We show that the proposed ARC framework significantly increases the provider’s revenue and also successfully offers differentiated QoS to the users. Index Terms—Wireless data networks, admission control, rate control, CDMA systems, noncooperative games. æ...|$|E
25|$|With the 1950s came {{increasing}} {{awareness of}} the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from <b>batch</b> <b>processing</b> to online <b>modes,</b> from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with library programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, {{as well as other}} professional programs, such as law and medicine in their curriculum. By the 1980s, large databases, such as Grateful Med at the National Library of Medicine, and user-oriented services such as Dialog and Compuserve, were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous special interest groups to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web.|$|R
50|$|Early {{mainframe}} computers (in the 1950s) were non-interactive, instead using <b>batch</b> <b>processing.</b> IBM's Job Control Language (JCL) is the {{archetype of}} languages {{used to control}} <b>batch</b> <b>processing.</b>|$|R
30|$|There {{are a few}} {{differences}} between utterance-based <b>batch</b> <b>processing</b> and full <b>batch</b> <b>processing.</b> In utterance-based <b>batch</b> <b>processing,</b> we normalize the features of each utterance to zero mean and compute a 100 -dimensional i-vector from this utterance. In full <b>batch</b> <b>processing,</b> we normalize the features of each speaker in a room to zero mean and compute a 100 -dimensional i-vector from this speaker in the room. In order to assign utterances in a room to speakers, we carry out speaker diarization using {{a modified version of}} the multi-stage segmentation and clustering system [42] as described before.|$|R
40|$|AbstractThe present paper {{reports on}} the use of {{modified}} activated carbon as an environmental friendly adsorbent, obtained from a green vegetable waste, for the removal of copper (II) from wastewaters. The activated carbon was prepared from green vegetable waste by KOH treatment for 24 h at 60 °C and thereafter evaluated by studying the effects of pH, contact time, dosage and initial concentration and optimized in the <b>batch</b> <b>processing</b> <b>mode.</b> The morphological and chemical changes in activated carbon were fully characterized by SEM, TGA, DSC, FTIR techniques. The Langmuir isotherm, yielded adsorption capacity of 75. 0 mgg− 1 for a pseudo-second-order model. Overall, these results suggest that green vegetable waste derived activated carbon as a low-cost adsorbent for the removal of copper (II) will be useful for future scale-up for the tertiary treatment of wastewater...|$|E
40|$|A {{variety of}} {{techniques}} in machine vision involve representation of objects by using their shape skeleton. Many algorithms {{have been proposed}} to date for obtaining the skeletal shape of digital images. The noise models predominantly used in these techniques are restricted to boundary noise. In particular, instances of noise occurring inside object regions and causing their non-contiguity are precluded. In this paper we present a method to obtain the skeletal shape of binary images {{in the presence of}} both boundary noise and noise occurring inside object regions. We propose to obtain the skeletal shape of such images by {{a modified version of the}} Kohonen self-organizing map, implemented in a <b>batch</b> <b>processing</b> <b>mode.</b> The modifications allow the map to adapt to the input shape distribution. At each iteration, a competitive Hebbian rule is used to progressively compute the Delaunay triangulation of the shape. Information from the triangulation augments the map topology to yield the final skele [...] ...|$|E
40|$|The U. S. Army Information Systems Engineering Command {{maintains}} over 100 standard Army {{management information}} systems. The need {{has been identified}} to modernize these systems. One way is to upgrade these systems from a COBOL, flat file, <b>batch</b> <b>processing</b> <b>mode</b> to systems written in Ada {{in order to increase}} functionality, maintainability and reusability [HOBB 90]. Through the SERC Design Metrics Research Project, we have developed a metrics approach for analyzing software designs which helps designers engineer quality into the design product. This paper discusses the calculation and analysis of our design metrics on the COBOL and Ada systems received from AIRMICS. This analysis was automated through the use of support tools that our research team developed for COBOL and through our Design Metric Analyzer (DMA) for Ada. The analyses of our metric results shed light on the design quality and design balance of the COBOL and Ada systems. Design Metrics The Design Metrics Research Team at Ba [...] ...|$|E
30|$|Another {{difference}} between utterance-based versus full <b>batch</b> <b>processing</b> {{is that we}} are able to decode with MLIFD features in full <b>batch</b> <b>processing.</b> The MLIFD features for an utterance are transformed using LDA + STC + FMLLR before input to the neural net. FMLLR transform per utterance resulted in significant increase in WER, and therefore, the MLIFD feature was not used in utterance-based <b>batch</b> <b>processing.</b> In full <b>batch</b> <b>processing,</b> the FMLLR is computed from all the utterances of a speaker in the room. In this scenario, MLIFD features gave very good results.|$|R
50|$|Memo-posting is a {{term used}} in {{traditional}} computerized banking environments where <b>batch</b> <b>processing</b> is employed. It represents temporary credit or debit transactions/entries made to an account for which the complete posting to update the balance will be done {{as part of the}} EOD(end-of-day) <b>batch</b> <b>processing.</b> The temporary transaction created as part of the memo-posting will be reversed/removed after the actual transaction is posted in <b>batch</b> <b>processing.</b>|$|R
30|$|Non-target {{screening}} analysis, {{combined with}} the integration of high-performance computing, becomes “ready to go” for environmental applications [30] and moves traditional exposure analysis to ‘big data’: the NORMAN ‘Digital Sample Freezing Platform (DSFP)’ is currently under development to host in a harmonized format full-scan high-resolution mass spectrometry (HR-MS) data, allowing for high-throughput processing (including retrospective analysis) of any environmental sample {{for a wide range}} (thousands) of pollutants. The concept of collaborating in one DSFP and sharing its ‘big data’ has been recently tested among a core group of NORMAN, with data sets obtained within the Joint Danube Survey 3 (surface water samples) [31] and the EU/UNDP EMBLAS project (marine water, sediment, and biota samples) [32]. Further improvement of functionalities of the DSFP (upload of raw mass chromatograms, visualisation of data, <b>batch</b> <b>mode</b> <b>processing,</b> use of MS–MS information, etc.), the extension of its functionalities for archiving and processing of gas chromatography–HR-MS data and testing of various options for archiving and processing of ‘big data’ at the wider European scale are planned for 2018 and beyond.|$|R
