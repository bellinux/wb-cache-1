32|0|Public
40|$|We {{consider}} {{the problem of}} determining robot manip-ulation plans when sensing and control uncertainties are specified as conditional probability densities. Traditional approaches are usually based on worst-case error anal-ysis in a methodology known as preimage <b>backchaining.</b> We have developed a general framework for determining sensor-based robot plans by blending ideas from stocbas-tic optimal control and dynamic game theory with tra-ditional preimage <b>backchaining</b> concepts. We argue that the consideration of a precise loss (or performance) func-tional is crucial to determining and evaluating manipu-lation plans in a probabilistic setting. We consequently introduce a stochastic, performance preimage that gen-eralizes previous preimage notions. We also present some optimal strategies for planar manipulation tasks that were computed by a dynamic programming-based algorithm. ...|$|E
40|$|A connectionist {{model for}} {{emergent}} planning behavior is proposed. The model demonstrates that a simple planning schema, acting {{in concert with}} two general purpose cognitive functionalities, namely, episodic memory and perception, can solve a restricted class of planning problems by <b>backchaining</b> from the goal to the current state...|$|E
40|$|Using some routine {{properties}} ([zu], [po]) of the Prawitz translation f from (sequent calculus) derivations to (natural) deductions, and restricting {{ourselves to}} the language of first-order hereditary Harrop formulae, we show (i) that f maps the simple uniform derivations of Miller et al onto the set of deductions in expanded normal form; and (ii) that f identifies two such derivations iff they differ only by the order in which conjunctions and universal formulae on the left are broken up, thus factoring through a bijection between the set of uniform proofs with <b>backchaining</b> and the set of deductions in expanded normal form. Thus, the logic programmer's restriction to the use of uniform proofs with <b>backchaining</b> is complete not merely w. r. t. derivability but also (in a bijective fashion) w. r. t. the construction of expanded normal deductions. (extended abstract, April 14, 1994. Caution: at present the results are not yet established to our satisfaction nor do we know the precise conte [...] ...|$|E
40|$|We {{introduce}} tools {{which help}} one to compose concurrent, hybrid control programs {{for a class}} of distributed robotic systems, assuming a palette of controllers for individual tasks is already constructed. These tools, which combine the <b>backchaining</b> of continuous robot behaviors with Petri nets, expand on successful work in sequential composition of robot behaviors. We apply these ideas to the design and verification of a robotic bucket brigade and to simple, distributed assembly tasks as found in automated factories...|$|E
40|$|A connectionist {{model for}} {{emergent}} planning behavior is proposed. The model demonstrates that a simple planning schema, acting {{in concert with}} two general purpose cognitive functionalities, namely, episodic memory and perception, can solve a restricted class of planning problems by <b>backchaining</b> from the goal to the current state. In spite of its simple structure, the schema can search for and execute plans involving multiple steps. We discuss how this simple model can be extended into a more powerful and expressive planning system by incorporating additional control and memory structures...|$|E
40|$|A {{method is}} {{outlined}} for automatically designing sensors from the specification of a robot 2 ̆ 7 s task, its actions, and its uncertainty in control. The sensors provide precisely the information {{required by the}} robot to perform its task, despite uncertainty in sensing and control. The key idea is to generate a strategy for a robot task by using a <b>backchaining</b> planner that assumes perfect sensing while taking careful account of control uncertainty. The resulting plan indirectly specifies a sensor that tells the robot when to execute which action. Although the planner assumes perfect sensing information, the sensor need not actually provide perfect information. The sensor provides only the information required for the plan to function correctly...|$|E
40|$|A {{prototype}} {{is described}} {{that can serve}} as a scientific-modeling software tool to facilitate the development of useful scientific models. The prototype is developed for applications to planetary modeling, and specific examples are given that relate to the atmosphere of Titan. The scientific modeling tool employs a high-level domain-specific modeling language, several data-display facilities, and a library of experimental datasets and scientific equations. The planetary modeling prototype links uncomputed physical variables to computed variables with computational transformations based on a <b>backchaining</b> procedure. The system - implemented in LISP with an object-oriented knowledge-representation tool - is run on a workstation that provides interface with several models. The prototype is expected to form the basis for a sophisticated modeling tool that can permit active experimentation...|$|E
40|$|This {{research}} {{proposes a}} method for automatically designing sensors from the specification of a robot 2 ̆ 7 s task, its actions, and its uncertainty in control. The sensors provide precisely the information required by the robot to perform its task, despite uncertainty in sensing and control. The key idea is to generate a strategy for a robot task by using a <b>backchaining</b> planner that assumes perfect sensing while taking careful account of control uncertainty. The resulting plan indirectly specifies a sensor that tells the robot when to execute which action. Although the planner assumes perfect sensing information, the sensor need not actually provide perfect information. Instead, the sensor provides only the information required for the plan to function correctly. This report is a revised version of a proposal currently submitted to NSF...|$|E
40|$|Subset-equational {{programming}} is a paradigm of programming with subset and equality assertions. The underlying computational model {{is based on}} innermost reduction of expressions and restricted associative-commutative (a-c) matching for iteration over setvalued terms, where [is the a-c constructor. Subset assertions incorporate a `collect-all' capability, so that the different subset assertions matching a goal expression and the different a-c matches with each subset assertion are all considered in defining the resulting set of the goal expression. We provide several examples to illustrate the paradigm, and also describe extensions to improve programming convenience: negation by failure, relative sets, and quantifiers. We also discuss the use of subset-equational programming for intelligent decision systems: the rule-based notation is well-suited for expressing domain knowledge and rules; subset assertions are especially appropriate in <b>backchaining</b> systems like MYCIN, which performs an [...] ...|$|E
40|$|A {{number of}} formalisms based on Linear Logic have been {{successfully}} employed for specifying concurrent systems. Nevertheless, {{there has been a}} much minor interest in providing an efficient implementation of them. An inconvenience of some of these approaches is the non-determinism generated by the resource management. One of the most expressive Linear Logic based formalisms is Forum, a specification logic providing both abstraction and concurrency into a declarative setting. In this paper, we define a lazy splitting system for Forum providing a nearly deterministic resource assignment. We also propose a new stack-based scope control mechanism, improving other proposals and having both a declarative and an operational reading. Keywords: Linear Logic, Implementation, <b>Backchaining,</b> Lazy Splitting 1 Introduction Linear Logic was introduced by Girard [3] to provide a logical framework for concurrency, where formulas are viewed as (possibly finite) resources and proofs as their consumers. I [...] ...|$|E
40|$|It is {{traditional}} wisdom {{that one should}} start from the goals when generating a plan in order to focus the plan generation process on potentially relevant actions. The graphplan system, however, {{which is the most}} efficient planning system nowadays, builds a "planning graph" in a forward-chaining manner. Although this strategy seems to work well, it may possibly lead to problems if the planning task description contains irrelevant information. Although some irrelevant information can be filtered out by graphplan, most cases of irrelevance are not noticed. In this paper, we analyze the effects arising from "irrelevant" information to planning task descriptions for different types of planners. Based on that, we propose a family of heuristics that select relevant information by minimizing the number of initial facts that are used when approximating a plan by <b>backchaining</b> from the goals ignoring any conflicts. These heuristics, although not solution-preserving, turn out to be v [...] ...|$|E
40|$|Achieving goals despite {{uncertainty}} {{in control and}} sensing may require robots to perform complicated motion planning and execution monitoring. This paper describes a reduced version of the general planning problem {{in the presence of}} uncertainty and a complete polynomial algorithm solving it. The planner computes a guaranteed plan (for given uncertainty bounds) by <b>backchaining</b> omnidirectional backprojections of the goal until one fully contains the set of possible initial positions of the robot. The algorithm assumes that landmarks are scattered across the workspace, that robot control and position sensing are perfect within the fields of influence of these landmarks (the regions in which the landmarks can be sensed by the robot), and that control is imperfect and sensing null outside these fields. The polynomiality and completeness of the algorithm derive from these simplifying assumptions, whose satisfaction may require the robot and/or its workspace to be specifically engineered. Thi [...] ...|$|E
40|$|In {{this paper}} we {{describe}} {{a part of}} the Why 2 -Atlas tutoring system that models students ’ reasoning in the domain of qualitative physics. The main goals of the model are (1) to evaluate correctness of the student’s essay, and, in case the essay contains errors, (2) to direct remedial tutoring actions according to plausible errors in the student’s reasoning. To meet these goals, a <b>backchaining</b> theorem prover generates a set of assumptions and a chain of reasoning (a proof) that plausibly led the student to write the observed essay. A proof can include correct as well as buggy reasoning steps and assumptions. After a proof is generated, it is analyzed for correctness and the analysis is used to generate appropriate feedback to the student. We describe the weighted abductive theorem proving framework, outline previous and upcoming evaluations and discuss possible future directions...|$|E
40|$|This thesis {{describes}} a diagnostic technique for explaining unanticipated modes of failure in continuous-variable systems. Previous approaches in model-based diagnosis have traditionally suffered from either a dependence on explicit fault models or {{a tendency to}} produce unintuitive results. This research aims at achieving the explanatory power of explicit fault models, without sacrificing the robustness of consistency-based diagnosis. The unique compositional nature of the process-centered models of Qualitative Process Theory makes the application of model-based diagnostic techniques both non-trivial and rewarding. Rather than relying on explicit fault models, this approach utilizes a general domain theory to model the broken device. Given a sufficiently broad domain theory, symptoms are {{explained in terms of}} a transformed physi-cal structure. Generative fault models replace explicit, pre-enumerated fault models, thereby increasing robustness for identifying novel faults. This approach combines the efficiency of the consistency-based approach with the explanatory power of abductive <b>backchaining.</b> Candi...|$|E
40|$|A {{generalization}} of Horn clauses to a higher-order logic is described and examined {{as a basis}} for logic programming. In qualitative terms, these higher-order Horn clauses are obtained from the first-order ones by replacing first-order terms with simply typed #-terms and by permitting quantification over all occurrences of function symbols and some occurrences of predicate symbols. Several proof-theoretic results concerning these extended clauses are presented. One result shows that although the substitutions for predicate variables can be quite complex in general, the substitutions necessary in the context of higher-order Horn clauses are tightly constrained. This observation is used to show that these higher-order formulas can specify computations in a fashion similar to first-order Horn clauses. A complete theorem proving procedure is also described for the extension. This procedure is obtained by interweaving higher-order unification with <b>backchaining</b> and goal reductions, and constitutes a higher-order {{generalization of}} SLD-resolution. These results have a practical realization in the higher-order logic programming language called λProlog...|$|E
40|$|AbstractSubset-equational {{programming}} is a paradigm of programming with subset and equality assertions. The underlying computational model {{is based on}} innermost reduction of expressions and restricted associative-commutative (a-c) matching for iteration over set-valued terms, where ∪ is the a-c constructor. Subset assertions incorporate a “collect-all” capability, so that the different subset assertions matching a goal expression and the different a-c matches with each subset assertion are all considered in defining the resulting set of the goal expression. We provide several examples to illustrate the paradigm, and also describe extensions to improve programming convenience: negation by failure, relative sets and quantities. We also discuss the use of subset-equational programming for intelligent decision systems: the rule-based notation is well-suited for expressing domain knowledge and rules; subset assertions are especially appropriate in <b>backchaining</b> systems like MYCIN, which performs an exhaustive depth-first consideration of subproblems before arriving at some decision; and restricted a-c matching is very convenient for querying attributes of objects in such systems, by relieving the concern for the next ordering of attributes...|$|E
40|$|Abstract. It is {{traditional}} wisdom {{that one should}} start from the goals when generating a plan in order to focus the plan generation process on potentially relevant actions. The graphplan system, however, {{which is the most}} ecient planning system nowadays, builds a graph" in a forward-chaining manner. Although this strategy seems to work well, it may possibly lead to problems if the planning task description contains irrelevant information. Although some irrelevant information can be ltered out by graphplan, most cases of irrelevance are not noticed. In this paper, we analyze the eects arising from " informa-tion to planning task descriptions for dierent types of planners. Based on that, we propose a family of heuristics that select relevant information by minimizing the number of initial facts that are used when approximat-ing a plan by <b>backchaining</b> from the goals ignoring any con icts. These heuristics, although not solution-preserving, turn out to be very useful for guiding the planning process, as shown by applying the heuristics to a large number of examples from the literature. ...|$|E
40|$|The most {{difficult}} tasks in expert system design are verification, validation and testing. Traditional techniques for these tasks require the knowledge engineer to woifc through {{the knowledge base}} and the human expert to run many test cases on the expert system. This consumes {{a great deal of}} time and does not guarantee finding idl mistakes. On the other hand, brute force enumeration of all inputs is an impossible technique for most systems. Therefore, we have developed a general purpose tool to help verify and validate knowledge bases with litfle human intervention. Our tool, named Validator, has four main components: (1) a Syntactic Error Checker, (2) a Debugger, (3) a Rules and Facts Validation Module, and (4) a Chaining Thread Tracer. It was designed for knowledge bases that use the M. 11 expert system shell; however, the principles should generalize to any rule-based, <b>backchaining</b> shells, i. e. MYCIN derived shells. 1 Contrary to popular belief in the AI community, M. I is still sold, supported and updated by Cimflex Teknowledge Inc. 37...|$|E
40|$|Abstract — Steerable needles {{composed}} of a highly flexible material and with a bevel tip offer greater mobility compared to rigid needles for minimally invasive medical procedures. In this paper, we apply sampling-based motion planning technique to explore motion planning for the steerable bevel-tip needle in 3 D environments with obstacles. Based on the Rapidly-exploring Random Trees (RRTs) method, we develop a motion planner to quickly build a tree to search the configuration space using a new exploring strategy, which generates new states using randomly sampled control space instead of the deterministically sampled one used in classic RRTs. Notice the fact that feasible paths might not be found for any given entry point and target configuration, we also address the feasible entry point planning problem to find feasible entry points in a specified entry zone for any given target configuration. To solve this problem, we developed a motion planning algorithm based on RRTs with <b>backchaining,</b> which grow backward from the target to explore the configuration space. Finally, simulation results with a approximated realistic prostate needle insertion environment demonstrate {{the performance of the}} proposed motion planner. I...|$|E
40|$|This paper {{provides}} {{an approach to}} integrating geometric motion planning with logical task planning for long-horizon tasks in domains with many objects. We propose a tight integration between the logical and geometric aspects of planning. We use a logical representation which includes entities that refer to poses, grasps, paths and regions, {{without the need for}} a priori discretization. Given this representation and some simple mechanisms for geometric inference, we characterize the pre-conditions and effects of robot actions in terms of these logical entities. We then reason about the interaction of the geometric and non-geometric aspects of our domains using the general-purpose mechanism of goal regression (also known as pre-image <b>backchaining).</b> We propose an aggressive mechanism for temporal hierarchical decomposition, which postpones the pre-conditions of actions to create an abstraction hierarchy that both limits the lengths of plans that need to be generated and limits the set of objects relevant to each plan. We describe an implementation of this planning method and demonstrate it in a simulated kitchen environment in which it solves problems that require approximately 100 individual pick or place operations for moving multiple objects in a complex domain. ...|$|E
40|$|We {{show how}} a {{procedure}} developed by Bledsoe for automatically nding substitution instances for set variables in higher-order logic {{can be adapted}} to provide increased automa-tion in proof search in the Calculus of Constructions (CC). Bledsoe's procedure operates on an extension of rst-order logic that allows existential quantication over set variables. This class of variables can also be identied in CC. The existence of a correspondence be-tween higher-order logic and higher-order type theories such as CC is well-known. CC can be viewed as an extension of higher-order logic where the basic terms of the language, the simply-typed -terms, are replaced with terms containing dependent types. We show how Bledsoe's techniques can be incorporated into a reformulation of a search procedure for CC given by Dowek and extended to handle terms with dependent types. We introduce a notion of search context for CC which allow us to separate the operations of assumption introduction and <b>backchaining.</b> Search contexts allow a smooth integration of the step which nds solu-tions to set variables. We discuss how the procedure can be restricted to obtain procedures for set variable instantiation in sublanguages of CC such as the Logical Framework (LF) and higher-order hereditary Harrop formulas (hohh). The latter serves as the logical foundation of the Prolog logic programming language. ...|$|E
40|$|Pigeons {{were trained}} on {{simultaneous}} red-green discrimination procedures with delayed reward and sequences of stimuli during the delay. In Experiment 1, three stimuli {{appeared during the}} 60 -second intervals between the correct responses and reward, and the incorrect responses and nonreward. The stimulus that immediately followed a correct response also preceded nonreward, and the stimulus that followed an incorrect response preceded reward. These stimuli were 10 or. 33 second in duration for different groups. Stimuli during {{the remainder of the}} delay interval differed following correct and incorrect responses. Group 10 initially persisted in the nonrewarded choice, but shifted to a preponderance of rewarded responses after further training. Group. 33 rapidly acquired the correct response. Similar results were obtained in Experiment 2 where delay intervals consisted of opposite sequences of two stimuli of equal duration and total delays were 6, 20, or 60 seconds. Early in training, generalization of differential conditioned-reinforcing properties from the conditions preceding reward and nonreward to postchoice conditions had a greater effect relative to <b>backchaining</b> than it did later. It was concluded that delayed-reward learning is best analyzed in terms of the conditioned-reinforcing value of the patterns of cues that follow immediately after rewarded and nonrewarded responses...|$|E
40|$|AbstractWe {{show how}} a {{procedure}} developed by Bledsoe for automatically finding substitution instances for set variables in higher-order logic {{can be adapted}} to provide increased automation in proof search in the Calculus of Constructions (CC). Bledsoe's procedure operates on an extension of first-order logic that allows existential quantification over set variables. This class of variables can also be identified in CC. The existence of a correspondence between higher-order logic and higher-order type theories such as CC is well-known. CC can be viewed as an extension of higher-order logic where the basic terms of the language, the simply-typed λ-terms, are replaced with terms containing dependent types. We show how Bledsoe's techniques can be incorporated into a reformulation of a search procedure for CC given by Dowek and extended to handle terms with dependent types. We introduce a notion of search context for CC which allows us to separate the operations of assumption introduction and <b>backchaining.</b> Search contexts allow a smooth integration of the step which finds solutions to set variables. We discuss how the procedure can be restricted to obtain procedures for set variable instantiation in sublanguages of CC such as the Logical Framework (LF) and higher-order hereditary Harrop formulas (hohh). The latter serves as the logical foundation of the λProlog logic programming language...|$|E
40|$|This paper {{explores the}} {{features}} of rhythm that make English difficult for speakers of Chinese and suggests some techniques for helring students speak English with an English rhythm. The focus is on information obtained during a workshop: the rhythmic differences between Chinse and English, word rhythm in English, and sentence rhythm in English. Not knowing the importance of syllable structure in English, many Chinese students ignore stress patterns of English words and often give each syllable the same strength, length, and pitch, resulting in poor pronunciation. Two suggested remedies are werd stress pattern building and stress production in terms of syllable length, strength, and pitch. Many activities to correct these problems are suggested, such as using rubber bands as visuals for syllable length, using musical instruments to indicate pitch change, and using <b>backchaining</b> to practice stress patterns of long words. Sentence rhythm poses two difficulties to many Chinese students: stress and syllable grouping. Two suggested remedies are sentence stress pattern building and rhytnmic grouping to practice pausing and linking of thought groups. Tools presented to help Chinese speakers learn English are designed for awareness building, ear training, and production. (Contains 10 references.) (Author/NAV) Reproductions supplied by EDRS are the best {{that can be made}} from the original document...|$|E
40|$|The {{logic of}} {{hereditary}} Harrop formulas (HH) has proven useful for specifying {{a wide range}} of formal systems that are commonly presented via syntax-directed rules that make use of contexts and side-conditions. The two-level logic approach, as implemented in the Abella theorem prover, embeds the HH specification logic within a rich reasoning logic that supports inductive and co-inductive definitions, an equality predicate, and generic quantification. Properties of the encoded systems can then be proved through the embedding, with special benefit being extracted from the transparent correspondence between HH derivations and those in the encoded formal systems. The versatility of HH relies on the free use of nested implications, leading to dynamically changing assumption sets in derivations. Realizing an induction principle in this situation is nontrivial and the original Abella system uses only a subset of HH for this reason. We develop a method here for supporting inductive reasoning over all of HH. Our approach relies on the ability to characterize dynamically changing contexts through finite inductive definitions, and on a modified encoding of <b>backchaining</b> for HH that allows these finite characterizations to be used in inductive arguments. We demonstrate the effectiveness of our approach through examples of formal reasoning on specifications with nested implications in an extended version of Abella. ...|$|E
40|$|International audienceHypothetical {{judgments}} go hand-in-hand with higher-order {{abstract syntax}} for meta-theoretic reasoning. Such judgments have {{two kinds of}} assumptions: those that are statically known from the specification, and the dynamic assumptions that result from building derivations out of the specification clauses. These dynamic assumptions often have a simple regular structure of repetitions of blocks of related assumptions, with each block generally involving one or several variables and their properties, that {{are added to the}} context in a single <b>backchaining</b> step. Reflecting on this regular structure can let us derive a number of structural properties about the elements of the context. We present an extension of the Abella theorem prover, which is based on a simply typed intuitionistic reasoning logic supporting (co-) inductive definitions and generic quantification. Dynamic contexts are repre-sented in Abella using lists of formulas for the assumptions and quantifier nesting for the variables, together with an inductively defined context relation that specifies their structure. We add a new mechanism for defining particular kinds of regular context relations, called schemas, and tacticals to derive theorems from these schemas as needed. Importantly, our extension leaves the trusted kernel of Abella unchanged. We show that these tacticals can eliminate many commonly encountered kinds of administrative lemmas that would otherwise have to be proven manually, which is a common source of complaints from Abella users...|$|E
40|$|Introduction: {{the active}} {{engagement}} of fathers in maternity care {{is associated with}} long-term health and social benefits for the mother, baby and family. The maternity care expectations and experiences of expectant and new fathers have received little attention to date. Aim: to identify and synthesise good quality qualitative research that explores the views and experiences of fathers who have encountered maternity care in high resource settings. Methods: based on a pre-determined search strategy, relevant databases were searched for papers published between January 1999 and January 2010. <b>Backchaining</b> of the reference lists in included papers was undertaken. Inclusion criteria: good quality qualitative research studies exploring fathers' involvement in maternity care through pregnancy, birth, and up to 6 months postnatally, that were undertaken in high resource countries. No language restrictions were imposed. Analytic strategy: the analysis {{was based on the}} metaethnographic techniques of Noblit and Hare (1988) as amended by Downe et al. (2007). Findings: from 856 hits 23 papers were included. The emerging themes were as follows: risk and uncertainty, exclusion, fear and frustration, the ideal and the reality, issues of support and experiencing transition. Synthesis: fathers feel themselves to be ‘partner and parent’ but their experience of maternity care services is as ‘not-patient and not-visitor’. This situates them in an interstitial and undefined space (both emotionally and physically) with the consequence that many feel excluded and fearful. Conclusions: fathers cannot support their partner effectively in achieving the ideal of transition to a successful pregnancy, joyful birth and positive parenthood experience unless they are themselves supported, included, and prepared for the reality of risk and uncertainty in pregnancy, labour and parenthood and for their role in this context...|$|E
40|$|Abstract: We {{present a}} new form of Herbrand's theorem which is {{centered}} around structures called expansion trees. Such trees contains substitution formulas and selected (critical) variables at various non-terminal nodes. These trees encode a shallow formula and a deep formula- the latter containing the formulas which label the terminal nodes of the expansion tree. If a certain relation among the selected variables of an expansion tree is acyclic and if the deep formula of the tree is tautologous, then we say that the expansion tree is a special kind of proof, called an ET-proof, of its shallow formula. Because ET-proofs are sufficiently simple and general (expansion trees are, in a sense, generalized formulas), they can be used in the context of not only first-order logic but also a version of higher-order logic which properly contains first-order logic. Since the computational logic literature has seldomly dealt with the nature of proofs in higher-order logic, our investigation of ET-proofs will be done entirely in this setting. It can be shown that a formula has an ET-proof if and only if that formula is a theorem of higher-order logic. Expansion trees have several pleasing practical and theoretical properties. To demonstrate this fact, we use ET-proofs to extend and complete Andrews ' procedure [41 for automatically constructing natural deductions proofs. We shall also show how to use a mating for an ET-proof's tautologous, deep formula to provide this procedure with the "look ahead " needed to determine if certain lines are unnecessary to prove other lines and when and how <b>backchaining</b> can be done. The resulting natural deduction proofs are generally much shorter and more readable than proofs build without using this mating information. This conversion process works without needing any search. Details omitted in this paper {{can be found in the}} author's dissertation [161...|$|E
40|$|Expansion {{trees are}} defined as generalizations of Herbrand {{instances}} for formulas in a nonextensional form of higher-order logic based on Church 2 ̆ 7 s simple theory of types. Such expansion trees can be defined {{with or without the}} use of skolem functions. These trees store substitution terms and either critical variables or skolem terms used to instantiate quantifiers in the original formula and those resulting from instantiations. An expansion tree is called an expansion tree proof (ET-proof) if it encodes a tautology, and, in the form not using skolem functions, an 2 ̆ 2 imbedding 2 ̆ 2 relation among the critical variables be acyclic. The relative completeness result for expansion tree proofs not using skolem functions, i. e. if A is provable in higher-order logic then A has such an expansion tree proof, is based on Andrews 2 ̆ 7 formulation of Takahasti 2 ̆ 7 s proof of the cut-elimination theorem for higher-order logic. If the occurrences of skolem functions in instantiation terms are restricted appropriately, the use of skolem functions in place of critical variables is equivalent to the requirement that the imbedding relation is acyclic. This fact not only resolves the open question of what is a sound definition of skolemization in higher-order logic but also provides a direct, syntactic proof of its correctness. Since subtrees of expansion trees are also expansion trees (or their dual) and expansion trees store substitution terms and critical variables explicitly, ET-proofs can be directly converted into sequential and natural deduction proofs. A naive translation will often produce proofs which contain a lot of redunancies and will often use implicational lines in an awkward fashion. An improved translation process is presented. This process will produce only focused proofs in which much of the redunancy has been eliminated and <b>backchaining</b> on implicational lines was automatically selected if it was applicable. The information necessary to construct focused proofs is provided by a certain connection scheme, called a mating,of the boolean atoms within the tautology encoded by an ET-proof...|$|E
40|$|Cataloged from PDF {{version of}} article. Thesis (Ph. D.) : Bilkent University, Department of Computer Engineering, İhsan Doğramacı Bilkent University, 2017. Includes bibliographical {{references}} (leaves 145 - 154). Linear Logic is a non-monotonic logic, with semantics that enforce single-use assumptions thereby allowing native and e cient encoding of domains with dynamic state. Robotic task planning {{is an important}} example for such domains, wherein both physical and informational components of a robot's state exhibit non-monotonic properties. We introduce two novel and e cient theorem provers for automated construction of proofs for an exponential multiplicative fragment of linear logic to encode deterministic STRIPS planning problems in general. The rst planner we introduce is Linear Planning Logic (LPL), {{which is based on}} the <b>backchaining</b> principle commonly used for constructing logic programming languages such as Prolog and Lolli, with a novel extension for LPL to handle program formulae with non-atomic conclusions. We demonstrate an experimental application of LPL {{in the context of a}} robotic task planner, implementing visually guided autonomous navigation for the RHex hexapod robot. The second planner we introduce is the Linear Logic Graph Planner (LinGraph), an automated planner for deterministic, concurrent domains, formulated as a graphbased theorem prover for a propositional fragment of intuitionistic linear logic. The new graph-based theorem prover we introduce in this context substantially improves planning performance by reducing proof permutations that are irrelevant to planning problems particularly in the presence of large numbers of objects and agents with identical properties (e. g. robots within a swarm, or parts in a large factory). We illustrate LinGraph's application for planning the actions of robots within a concurrent manufacturing domain and provide comparisons with four existing automated planners, BlackBox, Symba- 2, Metis and the Temporal Fast Downward (TFD), covering a wide range of state-of-the-art automated planning techniques and implementations that are well-known in the literature for their performance on various of problem types and domains. We show that even though LinGraph does not rely on any heuristics, it still outperforms these systems for concurrent domains with large numbers of identical objects and agents, nding feasible plans that they cannot identify. These gains persist even when existing methods on symmetry reduction and numerical uents are used, with LinGraph capable of handling problems with thousands of objects. Following these results, we also formally show that plan construction with LinGraph is equivalent to multiset rewriting systems, establishing a formal relation between LinGraph and intuitionistic linear logic. by Sıtar Kortik. Ph. D...|$|E

