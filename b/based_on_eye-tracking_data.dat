4|10000|Public
30|$|DeCarlo and Santella [13] {{proposed}} a stylization approach <b>based</b> <b>on</b> <b>eye-tracking</b> <b>data,</b> a robust {{variant of the}} Canny edge detector [40], and mean-shift image segmentation. A visual attention map is used to infer information about high-impact areas {{that are used to}} create stylized rendering with emphasis on the edges used in the composition of the final stylized image. According to the authors, despite some limitations of the edge detector, the generated edges added expressiveness to the final result.|$|E
40|$|This work {{presents}} a semantic level no-reference image sharpness/blurriness metric {{under the guidance}} of top-down & bottom-up saliency map, which is learned <b>based</b> <b>on</b> <b>eye-tracking</b> <b>data</b> by SVM. Unlike existing metrics focused on measuring the blurriness in vision level, our metric more concerns about the image content and human's intention. We integrate visual features, center priority, and semantic meaning from tag information to learn a top-down & bottom-up saliency model based on the eye-tracking data. Empirical validations on standard dataset demonstrate the effectiveness of the proposed model and metric. Department of ComputingRefereed conference pape...|$|E
40|$|AbstractThis paper proposes an {{interactive}} visualization system that supports tasks of monitoring multiple BBS threads regularly. A BBS is useful as means {{to know the}} latest information and wide range of opinions about the topics of interest. However, we must spend much time on reading large number of posts in a thread to recognize topics discussed in it. In addition, {{it is so difficult}} to read multiple threads simultaneously that we may miss valuable information. The proposed monitoring system resolves these problems by keyword-based visualization. Furthermore, this system realizes flexible monitoring reflecting user's interest by keyword tracking function for confirming the latest topics about specified keywords. This paper analyzes the behaviors of test participants using the prototype system <b>based</b> <b>on</b> <b>eye-tracking</b> <b>data...</b>|$|E
40|$|International audienceIn this article, we are {{interested}} in the computational modeling of visual attention. We report methods commonly used to assess the performance of these kinds of models. We survey {{the strengths and weaknesses of}} common assessment methods <b>based</b> <b>on</b> diachronic <b>eye-tracking</b> <b>data.</b> We then illustrate the use of some methods to benchmark computational models of visual attention...|$|R
40|$|International audienceVisual {{attention}} {{is one of}} the most important mechanisms in the human visual perception. Recently, its modeling becomes a principal requirement for the optimization of the image processing systems. Numerous algorithms have already been designed for 2 D saliency prediction. However, only few works can be found for 3 D content. In this study, we propose a saliency model for stereoscopic 3 D video. This algorithm extracts information from three dimensions of content, i. e. spatial, temporal and depth. This model benefits from the properties of interest points to be close to human fixations in order to build spatial salient features. Besides, as the perception of depth relies strongly on monocular cues, our model extracts the depth salient features using the pictorial depth sources. Since weights for fusion strategy are often selected in ad-hoc manner, in this work, we suggest to use a machine learning approach. The used artificial Neural Network allows to define adaptive weights <b>based</b> <b>on</b> the <b>eye-tracking</b> <b>data.</b> The results of the proposed algorithm are tested versus ground-truth information using the state-of-the-art techniques...|$|R
40|$|Reading {{experiments}} using naturalistic stimuli {{have shown}} unanticipated facilitations for completing center embeddings when frequency effects are factored out. To eliminate possible confounds due to surface structure, this paper introduces a processing model <b>based</b> <b>on</b> deep syntactic dependencies. Results <b>on</b> <b>eye-tracking</b> <b>data</b> indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings. ...|$|R
40|$|While {{over the}} last decades, much {{attention}} has been paid to the mental workload in the field of human computer interactions, there is still a lack of consensus concerning the factors that generate it as well as the measurement methods that could reflect workload variations. Based on the multifactorial Cognitive Load Theory (CLT), our study aims to provide some food for thought about the subjective and objective measurement that can be used to disentangle the intrinsic, extraneous, and germane load. The purpose is to provide insight into the way cognitive load can explain how users' cognitive resources are allocated in the use of hypermedia, such as an online newspaper. A two-phase experiment has been conducted on the information retention from online news stories. Phase 1 (92 participants) examined the influence of multimedia content on performance as well as the relationships between cognitive loads and cognitive absorption. In Phase 2 (36 participants), eye-tracking data were collected in order to provide reliable and objective measures. Results confirmed that performance in information retention was impacted by the presence of multimedia content such as animations and pictures. The higher number of fixations on these animations suggests that users' attention could have been attracted by them. Results showed the expected opposite relationships between germane and extraneous load, a positive association between germane load and cognitive absorption and a non-linear association between intrinsic and germane load. The trends <b>based</b> <b>on</b> <b>eye-tracking</b> <b>data</b> analysis provide some interesting findings about the relationship between longer fixations, shorter saccades and cognitive load. Some issues are raised about the respective contribution of mean pupil diameter and Index of Cognitive Activity. [URL] ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|The {{frequency}} {{of words and}} syntactic constructions has been observed to have a substantial effect on language processing. This begs {{the question of what}} causes certain constructions to be more or less frequent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions {{in such a way as}} to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated <b>on</b> <b>eye-tracking</b> <b>data</b> and the results are compared with predictions made by different theories of processing. ...|$|R
40|$|This short paper {{describes}} an update of A Simple Tool For Examining Fixations (ASTEF) developed for facilitating {{the examination of}} <b>eye-tracking</b> <b>data</b> and for computing a spatial statistics algorithm that has been validated {{as a measure of}} mental workload (namely, the Nearest Neighbor Index: NNI). The code is <b>based</b> <b>on</b> Matlab® 2013 a and is currently distributed on the web as an open-source project. This implementation of ASTEF got rid of many functionalities included in the previous version that are not needed anymore considering the large availability of commercial and open-source software solutions for eye-tracking. That makes it very easy to compute the NNI <b>on</b> <b>eye-tracking</b> <b>data</b> without the hassle of learning complicated tools. The software also features an export function for creating the time series of the NNI values computed on each minute of the recording. This feature is crucial given that the spatial distribution of fixations must be used to test hy-potheses about the time course of mental load...|$|R
30|$|We present EER-Tutor in the {{following}} section and discuss related work <b>on</b> using <b>eye-tracking</b> <b>data</b> in the ‘Related eye-tracking research’ section. ‘Methods’ section outlines the study we carried out, followed by a discussion of results in the ‘Analysing EER-Tutor logs’ and ‘Analysing eye-tracking data’ sections. Finally, we present conclusions and future research plans in the ‘Conclusions and future work’.|$|R
40|$|In {{this paper}} we {{describe}} research <b>on</b> using <b>eye-tracking</b> <b>data</b> for on-line assessment of user meta-cognitive behavior during the interaction with an intelligent learning environment. We describe the probabilistic user model that processes this information, and its formal evaluation. We show that adding eye-tracker information significantly improves the model accuracy on assessing user exploration and self-explanation behaviors...|$|R
40|$|Research {{in visual}} {{cognition}} {{has demonstrated that}} scene un-derstanding {{is influenced by the}} contextual properties of ob-jects, and a number of computational models have been pro-posed that capture specific context effects. However, a general model that predicts the fit of an arbitrary object with the con-text established {{by the rest of the}} scene is until now lacking. In this paper, we explain the contextual fit of objects in visual scenes using Bayesian topic models, which we induce from a database of annotated images. We evaluate our models firstly on synthetic object intrusion <b>data,</b> and then <b>on</b> <b>eye-tracking</b> <b>data</b> from a spot-the-difference task and from an object naming experiment. For the synthetic data, we find that our models are able to detect object intrusions accurately. For the <b>eye-tracking</b> <b>data,</b> we show that context scores derived from our models are associated with fixation latencies on target objects...|$|R
40|$|Analysis of eyetracking {{data can}} serve as an {{alternative}} method of evaluation when assessing the quality of computer-synthesized animations of American Sign Language (ASL), technology which can make information accessible {{to people who are}} deaf or hard-of-hearing, who may have lower levels of written language literacy. In this work, we build and evaluate the efficacy of descriptive models of subjective scores that native signers assign to ASL animations, <b>based</b> <b>on</b> <b>eye-tracking</b> metrics...|$|R
40|$|We {{present a}} {{cross-validation}} approach to assess animations of movement data. Specifically, we investigate if and how display design, data complexity and user background and training might influence participants’ decision-making with animated designs. Our triangulation approach is <b>based</b> <b>on</b> <b>eye-tracking</b> records, galvanic skin conductance responses, and electroencephalography data. We raise data analysis issues and data synchronization challenges for discussion at the workshop relating to data integration at various resolutions. With this empirical triangulation approach, {{we hope to}} better understand user decision-making with animated displays, and aim to develop sound animation design guidelines...|$|R
40|$|International audienceRetrieving {{specific}} {{categories of}} images among billions ofimages usually requires an annotation step. Unfortunately,keywords-based techniques {{suffer from the}} semantic gap ex-isting between a semantic concept and its digital representa-tion. Content Based Image Retrieval (CBIR) systems tacklethis issue simply considering semantic proximities can bemapped to similarities in the image space. Introducing rel-evance feedbacks involves the user in the task, but extendsthe annotation step. To reduce the annotation time, we want to prove that im-plicit relevance feedback can replace an explicit one. In thisstudy, we will evaluate the robustness of an implicit rele-vance feedback system only <b>based</b> <b>on</b> <b>eye-tracking</b> features(gaze-based interest estimator, GBIE). In [5], we showedthat our GBIE was representative for any set of users using“neutral images”. Here, we want {{to prove that it}} remainsvalid for more “subjective categories” such as food recipe...|$|R
40|$|In recent years, {{there has}} been {{substantial}} research on ex-ploring how AI can contribute to Human-Computer In-teraction by enabling an interface to understand a user’s needs and act accordingly. Understanding user needs is especially challenging when it involves assessing the user’s high-level mental states not easily reflected by in-terface actions. In this paper, we present our results <b>on</b> using <b>eye-tracking</b> <b>data</b> to model such mental states dur-ing interaction with adaptive educational software. We then discuss the implications of our research for Intelli-gent User Interfaces. Introduction 1 One of the main challenges in devising agents that can act intelligently is to endow them with the capability of un...|$|R
40|$|In this paper, we {{describe}} research <b>on</b> using <b>eye-tracking</b> <b>data</b> for on-line assessment of user meta-cognitive behavior during interaction with an environment for exploration-based learning. This work contributes to user modeling and intelligent interfaces research by extending existing research on eyetracking in HCI to on-line capturing of high-level user mental states for real-time interaction tailoring. We first describe the empirical work we did {{to understand the}} user meta-cognitive behaviors to be modeled. We then illustrate the probabilistic user model we designed to capture these behaviors {{with the help of}} on-line information on user attention patterns derived from <b>eye-tracking</b> <b>data.</b> Next, {{we describe}} the evaluation of this model, showing that gaze-tracking data can significantly improve model performance compared to lower level, timebased evidence. Finally, we discuss work we have done on using pupil-dilation information, also gathered through eyetracking data, to further improve model accuracy...|$|R
40|$|This {{doctoral}} thesis has signal processing of <b>eye-tracking</b> <b>data</b> as its main theme. An eye-tracker {{is a tool}} used for estimation of the point where one is looking. Automatic algorithms for classification {{of different types of}} eye movements, so called events, form the basis for relating the <b>eye-tracking</b> <b>data</b> to cognitive processes during, e. g., reading a text or watching a movie. The problems with the algorithms available today are that there are few algorithms that can handle detection of events during dynamic stimuli and that there is no standardized procedure for how to evaluate the algorithms. This thesis comprises an introduction and four papers describing methods for detection of the most common types of eye movements in <b>eye-tracking</b> <b>data</b> and strategies for evaluation of such methods. The most common types of eye movements are fixations, saccades, and smooth pursuit movements. In addition to these eye movements, the event post-saccadic oscillations, (PSO), is considered. The <b>eye-tracking</b> <b>data</b> in this thesis are recorded using both high- and low-speed eye-trackers. The first paper presents a method for detection of saccades and PSO. The saccades are detected using the acceleration signal and three specialized criteria <b>based</b> <b>on</b> directional information. In order to detect PSO, the interval after each saccade is modeled and the parameters of the model are used to determine whether PSO are present or not. The algorithm was evaluated by comparing the detection results to manual annotations and to the detection results of the most recent PSO detection algorithm. The results show that the algorithm is in good agreement with annotations, and has better performance than the compared algorithm. In the second paper, a method for separation of fixations and smooth pursuit movements is proposed. In the intervals between the detected saccades/PSO, the algorithm uses different spatial scales of the position signal in order to separate between the two types of eye movements. The algorithm is evaluated by computing five different performance measures, showing both general and detailed aspects of the discrimination performance. The performance of the algorithm is compared to the performance of a velocity and dispersion based algorithm, (I-VDT), to the performance of an algorithm <b>based</b> <b>on</b> principle component analysis, (I-PCA), and to manual annotations by two experts. The results show that the proposed algorithm performs considerably better than the compared algorithms. In the third paper, a method <b>based</b> <b>on</b> <b>eye-tracking</b> signals from both eyes is proposed for improved separation of fixations and smooth pursuit movements. The method utilizes directional clustering of the eye-tracking signals in combination with binary filters taking both temporal and spatial aspects of the eye-tracking signal into account. The performance of the method is evaluated using a novel evaluation strategy <b>based</b> <b>on</b> automatically detected moving objects in the video stimuli. The results show that the use of binocular information for separation of fixations and smooth pursuit movements is advantageous in static stimuli, without impairing the algorithm's ability to detect smooth pursuit movements in video and moving dot stimuli. The three first papers in this thesis are <b>based</b> <b>on</b> <b>eye-tracking</b> signals recorded using a stationary eye-tracker, while the fourth paper uses eye-tracking signals recorded using a mobile eye-tracker. In mobile eye-tracking, the user is allowed to move the head and the body, which affects the recorded data. In the fourth paper, a method for compensation of head movements using an inertial measurement unit, (IMU), combined with an event detector for lower sampling rate data is proposed. The event detection is performed by combining information from the eye-tracking signals with information about objects extracted from the scene video of the mobile eye-tracker. The results show that by introducing head movement compensation and information about detected objects in the scene video in the event detector, improved classification can be achieved. In summary, this thesis proposes an entire methodological framework for robust event detection which performs better than previous methods when analyzing eye-tracking signals recorded during dynamic stimuli, and also provides a methodology for performance evaluation of event detection algorithms...|$|R
40|$|Many cameras {{implement}} auto-focus functionality. However, {{they typically}} require {{the user to}} manually identify the location to be focused on. While such an approach works for temporally-sparse autofocusing functionality (e. g., photo shooting), it presents extreme usability problems when the focus must be quickly switched between multiple areas (and depths) of interest - e. g., in a gaze-based autofocus approach. This work introduces a novel, real-time auto-focus approach <b>based</b> <b>on</b> <b>eye-tracking,</b> which enables the user to shift the camera focus plane swiftly <b>based</b> solely <b>on</b> the gaze information. Moreover, the proposed approach builds a graph representation of the image to estimate depth plane surfaces and runs in real time (requiring ~ 20 ms on a single i 5 core), thus allowing for the depth map estimation to be performed dynamically. We evaluated our algorithm for gaze-based depth estimation against state-of-the-art approaches <b>based</b> <b>on</b> eight new data sets with flat, skewed, and round surfaces, as well as publicly available datasets...|$|R
40|$|International audienceAnaphora {{resolution}} a {{is complex}} problem, as {{it deals with}} syntax, semantics and discourse. The subject is well studied in field of psycholinguistics, where multiple preferences were discovered, and {{in the field of}} computational linguistics, where many systems have been developed to perform anaphora resolution in documents. Nevertheless, the work done in the two fields remains disconnected.   We investigate how we can bridge the gap by exploiting the options for making a cognitively plausible model for anaphora resolution. Such model can be beneficent for both fields as it can inspire computational linguistics with findings about how humans process anaphora, and help the psycholinguistic community developing large coverage, incremental models simulating the human processing of anaphora.   Inspired by the surprisal framework that uses incremental probabilistic parsing to predict processing cost coming from syntax for each word in a corpus, we focus on incremental probabilistic systems of anaphora resolution from the field of computational linguistics, for example the largely spread pair-wise model, and turn them into a cognitive model of processing of anaphora. We will present some preliminary results on the measures of processing cost of anaphora we developed and the perspective of evaluating the model <b>on</b> <b>eye-tracking</b> <b>data.</b>    ...|$|R
40|$|In {{the real}} world, people {{often have a}} habit tending to pay more {{attention}} to some things usually noteworthy, while ignore others. This phenomenon is associated with the top-down attention. Modeling this kind of attention has recently raised many interests in computer vision due to a wide range of practical applications. Majority of the existing models are <b>based</b> <b>on</b> <b>eye-tracking</b> or object detection. However, these methods may not apply to practical situations, because the eye movement data cannot be always recorded or there may be inscrutable objects to be handled in large-scale data sets. This paper proposes a Tag-Saliency model <b>based</b> <b>on</b> hierarchical image over-segmentation and auto-tagging, which can efficiently extract semantic information from large scale visual media data. Experimental results on a very challenging data set show that, the proposed Tag-Saliency model has the ability to locate the truly salient regions in a greater probability than other competitors. (C) 2013 Elsevier Inc. All rights reserved...|$|R
40|$|Abstract — In modern days, a large no of {{automobile}} accidents are caused due to driver fatigue. To {{address the problem}} we propose a vision-based real-time driver fatigue detection system <b>based</b> <b>on</b> <b>eye-tracking,</b> which is an active safety system. Eye tracking {{is one of the}} key technologies, for, future driver assistance systems since human eyes contain much information about the driver's condition such as gaze, attention level, and fatigue level. Face and eyes of the driver are first localized and then marked in every frame obtained from the video source. The eyes are tracked in real time using correlation function with an automatically generated online template. Additionally, driver’s distraction and conversations with passengers during driving can lead to serious results. A real-time vision-based model for monitoring driver’s unsafe states, including fatigue state is proposed. A time-based eye glance to mitigate driver distraction is proposed...|$|R
40|$|Part 2 : GamesInternational audienceImage-based {{rendering}} {{and reconstruction}} (IBR) approaches minimize time and costs to develop video-game assets, aiming to assist small game studios and indie game developers {{survive in the}} competitive video-game industry. To further investigate the interplay of IBR on developers’ efficiency, game performance, and players’ gaming experience we conducted two evaluation studies: a comparative, ecologically valid study with professional game developers who created games with and without an IBR-based game development pipeline, and a user study, <b>based</b> <b>on</b> <b>eye-tracking</b> and A/B testing, with gamers who played the developed games. The analysis of the results indicates that IBR tools provide a credible solution for creating low cost video game assets in short time, sacrificing game performance though. From a player’s perspective, we note that the IBR approach influenced players’ preference and gaming experience within contexts of varying levels of player’s visual intersections related to the IBR-created game assets...|$|R
40|$|Abstract—Driver fatigue is an {{important}} factor in the increasing number of road accidents. Dynamic template matching method was proposed to address the problem of real-time driver fatigue detection system <b>based</b> <b>on</b> <b>eye-tracking.</b> An effective vision based approach was used to analyze the driver’s eye state to detect fatigue. The driver fatigue system consists of Face detection, Eye detection, Eye tracking, and Fatigue detection. Initially frames are captured from a color video in a car dashboard and transformed from RGB into YCbCr color space to detect the driver’s face. Canny edge operator was used to estimating the eye region and the locations of eyes are extracted. The extracted eyes were considered as a template matching for eye tracking. Edge Map Overlapping (EMO) and Edge Pixel Count (EPC) matching function were used for eye tracking which is used to improve the matching accuracy. The pixel of eyeball was tracked from the eye regions which are used to determine the fatigue state of the driver...|$|R
40|$|Some chronic {{degenerative}} diseases lead {{to communication}} problems and individual confinement. The progressive and total loss of motor functions induces {{in these patients}} anatrhria, i. e., inability of using the normal Augmentative and Alternative Communication systems (AAC), and also the impossibility of interacting with their surrounding environment. In patients affected by ALS and MS, even if other communication forms are damaged or lost, the ability of controlling eye movements is typically maintained. The eye trackers are able to determine {{the direction of the}} user's gaze and to use it as an input channel. Communication instruments <b>based</b> <b>on</b> <b>eye-tracking</b> are for some patients a possible solution, both to have a useful communication with family members or at longer distance, using Internet, and for the possibility to control the home environment using a computer controlled by their eye movements. This book tackles, even if in partial and limited way, the communication and independence needs of severe motor disabled person...|$|R
40|$|Time lag {{between the}} source text input and the interpreter’s target text {{is known as}} ear-voice span or décalage. Recently, time lag has also been {{measured}} in written translation <b>based</b> <b>on</b> <b>eye-tracking</b> and key-logging (hence called eye-key span). Time lag provides insight into the temporal characteristics of simultaneity in interpreting, speed of translation and also into the cognitive load and cognitive processing involved in the translation/interpreting process. Ear-voice span/eye-key span (EVS/EKS) thus {{have the potential to}} become very valuable measures in translation/interpreting process-oriented research, but more needs to be known about their properties and the information this metric yields. Several exploratory analyses were carried out to compare methods used in previous research and to address the questions of EVS/EKS variability among participants, stability of EVS/EKS over the course of an individual’s performance, and variation in EVS/EKS at different points of measurement and across different tasks performed by the same person. The results indicate that EVS/EKS is indeed a sensitive measure useful for process-oriented research. Finally, some methodological challenges and procedures for both interpreting and translation are discussed. status: publishe...|$|R
40|$|Abstract — In this paper, {{we propose}} {{a method for}} {{automatics}} fatigue detection <b>based</b> <b>on</b> <b>eye-tracking.</b> In the present paper we use CCD camera for capturing image from drivers ‘face. The proposed method consists of four steps. In the first step we detect face area using YCbCr color-space, and then crop the image according to calculated maximum and minimum values of vertical and horizontal. <b>based</b> <b>on</b> the obtained results, generally eyes located in the two-fifth to three-fifth of the upper region of face, So we can crop image in this area, and estimate eyes region. In the third step determine exact location of eyes and track them whit use changing pixels of white to black. In the last step counter the number of white and black pixel. In the initial frame we posit driver is awaked, and consideration these number as threshold, so in another frame compare number of white and black pixel whit threshold and determine is driver sleepy or not [...] Index Terms — Introduction, previous worked, overal flowchart, face detection, eyes detection and localization, fatigue detection, experimental results, conclusion, references I...|$|R
40|$|OBJECTIVE: To {{introduce}} a new method of measuring sound localization ability <b>based</b> <b>on</b> <b>eye-tracking</b> and to test this method by analysing the influence of mild induced conductive hearing loss on sound localization. DESIGN: Sound signals were presented from different angles, and the participant's responses were measured using an eye-tracking device. For validation, a comparison of responses to visual stimuli was performed. To test the clinical application of this method, a mild conductive hearing loss was simulated, {{and the impact of}} this change on sound localization was measured. STUDY SAMPLE: Fifteen participants. RESULTS: The system provided repeatable measurements, and there was a good correlation of sound and visual signals. A large number of trials could be completed fairly rapidly. Following the induced conductive hearing loss, a decline of 5. 5 ° in the accuracy of sound localization in the horizontal plane was found towards the side of the non-impaired ear for frontal presentations. CONCLUSIONS: Quantifying sound localization by eye-tracking was found to be feasible, fast and accurate. A mild conductive hearing loss caused a slight degradation of sound localization accuracy within the 30 ° frontal sector, which is in good agreement with results found using methods requiring more extensive instrumentation...|$|R
40|$|This paper {{presents}} a renewed overview of photosensor oculography (PSOG), an <b>eye-tracking</b> technique <b>based</b> <b>on</b> {{the principle of}} using simple photosensors to measure the amount of reflected (usually infrared) light when the eye rotates. Photosensor oculography can provide measurements with high precision, low latency and reduced power consumption, and thus it appears as an attractive option for performing eye-tracking in the emerging head-mounted interaction devices, e. g. augmented and virtual reality (AR/VR) headsets. In our current work we employ an adjustable simulation framework as a common basis for performing an exploratory study of the eye-tracking behavior of different photosensor oculography designs. With the performed experiments we explore the effects from the variation of some basic parameters of the designs on the resulting accuracy and cross-talk, which are crucial characteristics for the seamless operation of human-computer interaction applications <b>based</b> <b>on</b> <b>eye-tracking.</b> Our experimental results reveal the design trade-offs {{that need to be}} adopted to tackle the competing conditions that lead to optimum performance of different eye-tracking characteristics. We also present the transformations that arise in the eye-tracking output when sensor shifts occur, and assess the resulting degradation in accuracy for different combinations of eye movements and sensor shifts. Comment: 12 pages, 18 figure...|$|R
40|$|Research on {{image quality}} {{assessment}} {{has shown the}} potential performance enhancement of adding visual attention in objective metrics. However, the use of attentionbased metrics in real-time applications is mainly limited by the complexity of modeling visual attention. Since most of the existing objective metrics are <b>based</b> <b>on</b> the luminance component of images only, we investigate whether also saliency can be modeled only with the luminance component. An eye-tracking experiment was carried out to subjectively assess the contribution of color in saliency. Additionally, two attention models well-known in literature were evaluated <b>on</b> predicting our <b>eye-tracking</b> <b>data.</b> Our experimental results show that human saliency is insensitive to color, but {{that this is not}} predicted by the attention models existing in literature. Index Terms — Visual attention, image quality assessment, objective metric, eye-tracking, salienc...|$|R
40|$|To date, {{few studies}} have {{investigated}} the eye movement patterns of individuals with glaucoma while they undertake everyday tasks in real-world settings. While some of these studies have reported possible compensatory gaze patterns in those with glaucoma who demonstrated good task performance despite their visual field loss, {{little is known about}} the complex interaction between field loss and visual scanning strategies and the impact on task performance and, consequently, on quality of life. We review existing approaches that have quantified the effect of glaucomatous visual field defects on the ability to undertake everyday activities through the use of eye movement analysis. Furthermore, we discuss current developments in eye-tracking technology and the potential for combining eye-tracking with virtual reality and advanced analytical approaches. Recent technological developments suggest that systems <b>based</b> <b>on</b> <b>eye-tracking</b> have the potential to assist individuals with glaucomatous loss to maintain or even improve their performance on everyday tasks and hence enhance their long-term quality of life. We discuss novel approaches for studying the visual search behavior of individuals with glaucoma that have the potential to assist individuals with glaucoma, through the use of personalized programs that take into consideration the individual characteristics of their remaining visual field and visual search behavior...|$|R
40|$|Inference by {{hypothetical}} {{drawing is}} a reasoning {{process in which}} imaginary drawings of diagrams are employed for prob-lem solving. We argue, in this paper, that inference by hy-pothetical drawing {{is one of the}} core cognitive mechanisms for realizing embodied spatial reasoning in higher cognition. <b>Based</b> <b>on</b> an <b>eye-tracking</b> experiment <b>on</b> logical inference tasks, we present evidence for the occurrence and the perva-siveness of inference by hypothetical drawing in diagrammatic reasoning. We provide its characterization as a mode of in-ference for higher cognition, which effectively utilizes inher-ent characteristics of external space and our capabilities for its manipulation...|$|R
40|$|Tourism {{communication}} {{has increasingly}} been enhanced by online multimodal resources, which provide {{wide range of}} information through invitingly interactive websites that function as hypertexts (Francesconi 2014, Fodde, Denti 2008; Maci 2012). Verbal texts and images should interact dynamically, since they pragmatically co-determine {{the meaning of the}} whole advert (Bateman 2014 : 32), as recent studies in informational architecture, <b>based</b> <b>on</b> <b>eye-tracking</b> methods, have shown. From a broad MCDA perspective (Black 2006, Kress 2010, van Leeuwen, 2013), which includes ecolinguistics, this study focused {{on one of the most}} engaging and iconic websites dedicated to the Blue Island Capri, www. capri. net/it/. We investigated its main lines of appeal (Dyer 1988), e. g. landscape, celebrities, cuisine, luxury, shopping, ancient historical places, and cultural memory (Assman 2008). Throughout capri. net’s pages, information and persuasion are deftly blended, through a hierarchy of foregrounding, although the interaction between the images and verbal texts is not entirely dynamic, as explained in the discussion. Another major finding of our investigation is the flattening of the diachronicity of the different topics: in capri. net’s easily scrolled curtains, the history of Tiberius, or the lives of famous writers, and the presence of trendy celebrities in well-known bars share an undifferentiated dimension. Thus, the ways of seeing of prospective tourists are virtually oriented to an a-chronically alluring and ‘unique’ Capri-lifestyle dimension...|$|R
40|$|Longitudinal growth {{patterns}} are routinely seen in medical studies where developments of individuals {{on one or}} more outcome variables are followed {{over a period of}} time. Many current methods for modeling growth presuppose a parametric relationship between the outcome and time (e. g., linear, quadratic); however, these relationships may not accurately capture growth over time. Functional mixed effects (FME) models provide flexibility in handling longitudinal data with nonparametric temporal trends because they allow the data to determine the shape of the curve. Although FME methods are well-developed for continuous, normally distributed outcome measures, nonparametric methods for handling categorical outcomes are limited. In this thesis, we propose a Bayesian hierarchical FME model to account for growth curves with non-Gaussian outcomes. In particular, we extend traditional FME models which assume normally distributed outcomes by modeling the probabilities associated with the binomially distributed outcomes and adding an additional level to the hierarchical model to correctly specify the outcomes as binomially distributed. We then extend the proposed binomial FME model to the multinomial setting where the outcomes consist of more than two nominal categories. Current modeling approaches include modeling each category of a multinomial outcome separately via linear and nonlinear mixed effects models; yet, these approaches ignore the inherent correlation among the categories of the outcome. Our model captures this correlation through a sequence of conditional binomial FME models which results in one model simultaneously estimating probabilities in all categories. Lastly, we extend our binomial FME model to address a common medical situation where multiple outcomes are measured on subjects over time and investigators are interested in simultaneously assessing the impact of all outcomes. We account for the relationship between outcomes by altering the correlation structure in the hierarchical model and simultaneously estimating the outcome curves. Our methods are assessed via simulation studies and real data analyses where we investigate the ability of the models to accurately predict the underlying growth trajectory of individuals and populations. Our applications include analyses of speech development data in adults and children with cochlear implants and analyses <b>on</b> <b>eye-tracking</b> <b>data</b> used to assess word processing in cochlear implant patients...|$|R
40|$|<b>Based</b> <b>on</b> 2 ̆ 2 {{ground truth}} 2 ̆ 2 <b>eye-tracking</b> <b>data,</b> earlier {{research}} [1] shows that adding natural scene saliency (NSS) can improve an objective metric's performance in predicting perceived image quality. To include NSS in a real-world implementation of an objective metric, a computational model instead of <b>eye-tracking</b> <b>data</b> is needed. Existing models of visual saliency are generally {{designed for a}} specific domain, and so, not applicable to image quality prediction. In this paper, we propose {{an efficient model for}} NSS, inspired by findings from our eye-tracking studies. Experimental results show that the proposed model sufficiently captures the saliency of the <b>eye-tracking</b> <b>data,</b> and applying the model to objective image quality metrics enhances their performance {{in the same manner as}} when including <b>eye-tracking</b> <b>data.</b> © 2012 IEEE...|$|R
40|$|Automated {{measurement}} of resolution acuity in infants using remote eye-tracking. Invest Ophthalmol Vis Sci. 2014; 55 : 8102 – 8110. DOI: 10. 1167 / iovs. 14 - 15108 PURPOSE. To validate a novel, automated test of infant resolution acuity <b>based</b> <b>on</b> remote <b>eye-tracking.</b> METHODS. Infants aged 2 to 12 months were tested binocularly {{using a new}} adaptive computerized test of infant vision using eye tracking (ACTIVE), and Keeler infant acuity cards (KIAC). The ACTIVE test ran automatically, using remote eye-tracking to assess whether the infant fixated a black-and-white grating of variable spatial frequency. Test-retest reliability was assessed by performing each test twice. Accuracy was assessed by comparing acuity measures across tests and with established age-norms, and by comparing low-contrast acuity estimates in adults with data reported previously. RESULTS. All infants completed the ACTIVE test at least once. Median test duration was 10...|$|R
