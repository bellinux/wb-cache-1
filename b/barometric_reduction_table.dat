0|95|Public
5000|$|Pub. 229 (formerly H.O. 229, Sight <b>Reduction</b> <b>Tables</b> for Marine Navigation, H.D. 605/NP 401 in the UK, 1945ish, 6 volumes. And the {{variant of}} HO-229: Sight <b>Reduction</b> <b>Tables</b> for Small Boat Navigation, known as Schlereth, 1983, 1 volume) ...|$|R
50|$|With {{the use of}} astral {{navigation}} for air navigation, faster methods {{needed to}} be developed and tables of precomputed triangles were developed. When using precomputed sight <b>reduction</b> <b>tables,</b> selection of the assumed position {{is one of the}} trickier steps for the fledgling navigator to master. Sight <b>reduction</b> <b>tables</b> provide solutions for navigation triangles of integral degree values. When using precomputed sight <b>reduction</b> <b>tables,</b> such as H.O. 229, the assumed position must be selected to yield integer degree values for LHA (local hour angle) and latitude. West longitudes are subtracted and east longitudes are added to GHA to derive LHA, so AP's must be selected accordingly. When using precomputed sight <b>reduction</b> <b>tables</b> each observation and each body will require a different assumed position.|$|R
5000|$|Pub. 249 (formerly H.O. 249, Sight <b>Reduction</b> <b>Tables</b> for Air Navigation, A.P. 3270 in the UK, 1947 - 53, 1+2 volumes) ...|$|R
5000|$|H.O. 211 (Dead Reckoning Altitude and Azimuth Table, {{known as}} Ageton, 1931, 36pg. And 2 {{variants}} of H.O. 211: Compact Sight <b>Reduction</b> <b>Table,</b> {{also known as}} Ageton - Bayless, 1980, 9+ pg. S-Table, also known as Pepperday, 1992, 9+ pg.) ...|$|R
2500|$|A truth <b>table</b> <b>{{reduction}}</b> or a weak truth <b>table</b> <b>reduction</b> must present {{all of its}} oracle queries at {{the same}} time. In a truth <b>table</b> <b>reduction,</b> the reduction also gives a boolean function (a truth table) which, when given {{the answers to the}} queries, will produce the final answer of the reduction. In a weak truth <b>table</b> <b>reduction,</b> the reduction uses the oracle answers as a basis for further computation depending on the given answers (but not using the oracle). Equivalently, a weak truth <b>table</b> <b>reduction</b> is one for which the use of the reduction is bounded by a computable function. For this reason, weak truth <b>table</b> <b>reductions</b> are sometimes called [...] "bounded Turing" [...] reductions.|$|R
50|$|Professional navigators {{are divided}} in usage between sight <b>reduction</b> <b>tables</b> {{on the one}} hand, and {{handheld}} computers or scientific calculators on the other. The methods are equally accurate. It is {{simply a matter of}} personal preference which method is used. An experienced navigator can reduce a sight from start to finish in about 5 minutes using nautical tables or a scientific calculator.|$|R
40|$|Abstract: Aiming at value reduction, {{a sort of}} RSVR {{algorithm}} {{was presented}} based on support in association rules via Apriori algorithm. A more effective <b>reduction</b> <b>table</b> {{can be obtained by}} deleting those rules with less support according to least support— minsup. The reduction feasibility of this algorithm was achieved by reducing the given decision table. Testing by UCI machine learning database and comparing this algorithm with least value reduction algorithm indicate the validity of RSVR algorithm...|$|R
40|$|If {{there is}} a sparse set hard for P under bounded truth <b>table</b> <b>reductions</b> computable in LOGSPACE or NC 2, then P = NC 2. We give {{the details of the}} proof to this theorem. 1 Introduction Recently a 1978 {{conjecture}} by Hartmanis [Har 78] was resolved [CS 95 a], following a breakthrough by [Ogi 95]. It was shown that there is no sparse set that is hard for P under logspace many-one reductions, unless P = LOGSPACE. Bounded truth <b>table</b> <b>reductions</b> are a natural extension of many-one reductions and it is natural to ask what consequences can be drawn assuming {{there is a}} sparse set hard for P under bounded truth <b>table</b> <b>reductions</b> computable in LOGSPACE. In this note we give the details of the proof of the theorem that if such a sparse set exists, then a very unlikely consequence follows, namely P = NC 2. This theorem is even valid for bounded truth <b>table</b> <b>reductions</b> computable in NC 2. The proof for the case of 1 -truth <b>table</b> <b>reductions,</b> which already generalizes the manyone reductions, has [...] ...|$|R
5000|$|Roughly, the {{latitude}} {{of a place}} on Earth is its angular distance north or south of the equator. Latitude is usually expressed in degrees (marked with °) ranging from 0° at the Equator to 90° at the North and South poles. [...] The latitude of the North Pole is 90° N, and {{the latitude}} of the South Pole is 90° S. Mariners calculated latitude in the Northern Hemisphere by sighting the North Star Polaris with a sextant and using sight <b>reduction</b> <b>tables</b> to correct for height of eye and atmospheric refraction. The height of Polaris in degrees above the horizon is the latitude of the observer, within a degree or so.|$|R
40|$|As the 750 kV {{substation}} has {{the characteristics}} like incomplete information, uncertain diagnosis result and dual protection configuration, a fault diagnosis method of substation with redundant protection configuration based on rough sets and grey relational analysis is proposed. In this method, the diagnosis decision table which {{takes advantage of}} information about wave-recording devices and two protective devices is constructed and simplified, and the minimal reduction {{can be obtained by}} using knowledge acquisition method based on rough set; Based on the point, the compar ative sequence and reference sequence is established. Through the use of gr e y relational analysis, the grey relational grade of attributes and failure rate of suspicious fault components is determined in the <b>reduction</b> <b>table,</b> and furthermore obtain a certain diagnosis result. The result shows that the proposed method is effective. </p...|$|R
50|$|Without {{applying}} SVD or {{some other}} method of dimension <b>reduction</b> the <b>table</b> of mortality data is a highly correlated multivariate data series; the complexity of these multidimensional time series makes such them almost impossible to forecast. SVD has become widely used {{as a method of}} dimension reduction in many disparate fields, including by Google in their page rank algorithm.|$|R
40|$|The new L-band 7 -feed-array at the 100 -m {{telescope}} in Effelsberg {{will be used}} to perform an unbiased fully sampled HI survey of the entire northern hemisphere observing the galactic and extragalactic sky using simultaneously two different backends. The integration time per position will be 10 min towards the SDSS area and 2 min for the remaining sky, thereby achieving a sensitivity competitive with the Arecibo ALFALFA and GALFA surveys but covering a much larger area of the sky. Both backends are FPGA-based digital fast fourier transform (DFFT) spectrometers, offering a superior dynamic range and temporal resolution. The latter is crucial for a sophisticated RFI mitigation scheme (Winkel et al. 2007) but produces huge amounts of data over the projected five years of observing. Consequently, we put considerable effort into efficient data <b>reduction.</b> <b>Table</b> 1 compares the survey parameters with several recent HI surveys. The data processing is organized making use of individual data reduction modules. Each module calculates correction factors/spectra that are store...|$|R
50|$|Navigation {{is the art}} {{and science}} of safely and {{efficiently}} directing the movements of a vessel from one point to another. Piloting uses water depth and visible references, while dead reckoning uses courses and distances from the last known position. More than just finding a vessel's present location, safe navigation includes predicting future location, route planning and collision avoidance. Nautical navigation in western nations, like air navigation, {{is based on the}} nautical mile. Navigation also includes electronics such as GPS and Loran (Long Range Navigation). Celestial navigation involves taking sights by sextant on the planets, moon, stars, sun and using the data with a nautical almanac and sight <b>reduction</b> <b>tables</b> to determine positions. Accurate time information is also needed. After nautical dusk, navigation at sea referencing the horizon is no longer possible, and after nautical dawn such navigation again becomes possible. Ice navigation involves navigating and operating a ship within sea ice conditions.|$|R
40|$|International audienceIn {{this paper}} a new method for fltering {{coherency}} matrices issued from Synthetic Aperture Radar (SAR) polarimetric interferometric data is presented. For each pixel of the interferogram, an adaptive neighborhood {{is determined by}} a region growing technique driven exclusively by the amplitude image information. All the available amplitude images of the interferometric couple are fused in the region growing process to ensure the stationarity hypothesis of the derived statistical population. In addition, for preserving local stationarity requirement of the interferogram, a phase compensation step is performed. Afterwards, all the pixels within the obtained adaptive neighborhood are complex averaged to yield the fltered values of the polarimetric and interferometric coherency matrices. The method has been tested on airborne high-resolution polarimetric interferometric SAR images (Oberpfaffenhofen area - German Space Agency). For comparison purposes, the standard phase compensated fixed multi-look flter and the linear adaptive coherence flter proposed by Lee at al. were also implemented. Both subjective and objective performance analysis, including coherence edge detection, ROC graph and bias <b>reduction</b> <b>tables,</b> recommends the proposed algorithm as a powerful post-processing POL-InSAR tool...|$|R
40|$|Recent {{demand on}} the {{measurement}} resolution of precise positioning {{comes up to}} tens of picometers. Some distinguished researches have been performed to measure the displacement in picometer order, however, few of them can verify the measurement performance as available tools in industry. This is {{not only because the}} picometer displacement is not yet required for industrial use, but also {{due to the lack of}} standard tools to verify such precise displacement. We proposed a displacement reduction mechanism for generating precise displacement using torsional leaf spring hinges (TLSHs) that consist of four leaf springs arranged radially. It has been demonstrated that a prototype of the reduction mechanism was able to provide one-nanometer displacement with 1 / 1000 reduction rate by a piezoelectric actuator. In order to clarify the potential of the reduction mechanism, a displacement <b>reduction</b> <b>table</b> that can be mounted on AFM stage was newly developed using TLSHs. This paper describes the design of the reduction mechanism and the sub-nanometer displacement performance of the table obtained from its dynamic and static characteristics measured by displacement sensors and from the AFM image...|$|R
30|$|This last {{equation}} {{has been}} used to obtain individually the complexity <b>reduction</b> ratios of <b>Tables</b> 7 and 8 in terms of arithmetic, read memory access and write memory access operations. Positive values correspond to a decreasing in complexity, meanwhile negative values correspond to a an increasing in complexity.|$|R
40|$|In a {{study of}} {{fourteen}} cases of polycythemia vera, Brown and Roth (1) found the serum calcium to be over 11. 0 mgm. per hundred cubic centimeters in every case. The actual range of serum calcium values was 11. 1 to 18. 1 mgm. per hundred cubic centimeters. The average value for the group was 14. 3 mgm. These authors noted a <b>reduction</b> in <b>TABLE</b> 1 Data on patients Case Sex Age Hemoglobin Red blood White blood Serum Spleennumber (Sahli) cells cells calcium palpable per cent miions mgm. per 100 cc...|$|R
40|$|Boolean logic {{minimization}} {{is traditionally}} used in logic synthesis tools running on powerful desktop computers. However, logic minimization {{has recently been}} proposed for dynamic use in embedded systems, including network route <b>table</b> <b>reduction,</b> network access control list <b>table</b> <b>reduction,</b> and dynamic hardware/software partitioning. These new uses require logic minimization to run dynamically {{as part of an}} embedded system's active operation. Performing such dynamic logic minimization onchip greatly reduces system complexity and security versus an approach that involves communication with a desktop logic minimizer. An on-chip minimizer must be exceptionally lean yet yield good enough results. Previous software-only on-chip minimizer results have been good, but we show that a codesigned minimizer can be much better, executing nearly 8 times faster and consuming nearly 60 % less energy, while yielding identical results...|$|R
40|$|At head of title: pt. I. United States Coast survey, Carlile P. Patterson, superintendent. Methods, {{discussions and}} results; pts. II-III: United States Coast and {{geodetic}} survey. Carlile P. Patterson, superintendent. Methods and results. Title of pts. I-II: Meteorological researches {{for the use}} of the coast pilot. pt. I. On the mechanics and the general motions of the atmosphere. [Appendix no. 20 [...] Report for 1875] [...] pt. II. On cyclones, tornadoes, and watersprouts. Appendix no. 10 [...] Report for 1878. [...] pt. III. <b>Barometric</b> hypsometry and <b>reduction</b> of the barometer to sea level. Appendix no. 10 [...] Report for 1881. Mode of access: Internet...|$|R
40|$|International audienceIn this paper, a {{new method}} to filter {{coherency}} matrices of polarimetric or interferometric data is presented. For each pixel, an adaptive neighborhood (AN) {{is determined by}} a region growing technique driven exclusively by the intensity image information. All the available intensity images of the polarimetric and interferometric terms are fused in the region growing process to ensure {{the validity of the}} stationarity assumption. Afterward, all the pixels within the obtained AN are used to yield the iltered values of the polarimetric and interferometric coherency matrices, which can be derived either by direct complex multilooking or from the locally linear minimum mean-squared error (LLMMSE) estimator. The entropy/alpha/anisotropy decomposition is then applied to the estimated polarimetric coherency matrices, and coherence optimization is performed on the estimated polarimetric and interferometric coherency matrices. Using this decomposition, unsupervised classification for land applications by an iterative algorithm based on a complexWishart density function is also applied. The method has been tested on airborne high-resolution polarimetric interferometric synthetic aperture radar (POL-InSAR) images (Oberpfaffenhofen area—German Space Agency). For comparison purposes, the two estimation techniques (complex multilooking and LLMMSE) were tested using three different spatial supports: a fix-sized symmetric neighborhood (boxcar filter), directional nonsymmetric windows, and the proposed AN. Subjective and objective performance analysis, including coherence edge detection, receiver operating characteristics plots, and bias <b>reduction</b> <b>tables,</b> recommends the proposed algorithm as an effective POL-InSAR postprocessing technique...|$|R
5000|$|Unfortunately, the Earth {{does not}} make a perfect {{circular}} orbit around the Sun. Due to the elliptical nature of the Earth’s orbit around the Sun, the speed of the Sun’s apparent orbit around the Earth varies throughout the year and that causes it to appear to speed up and slow down very slightly. Consequently, noon at the Prime Meridian is rarely if ever exactly at 1200 UTC, but rather it occurs some minutes and seconds before or after that time each day. This slight daily variation has been calculated and is listed for each day {{of the year in the}} Nautical Almanac under the title of Equation of time. This variation must be added to or subtracted from the UTC of local apparent noon to improve the accuracy of the calculation. Even with that, other factors, including the difficulty of determining the exact moment of local apparent noon due to the flattening of the Sun’s arc across the sky at its highest point, diminish the accuracy of determining longitude by chronometer as a method of celestial navigation. Accuracies of less than 10 nmi km error in position are difficult to achieve using the [...] "longitude by chronometer" [...] method. Other celestial navigation methods involving more extensive use of both the Nautical Almanac and sight <b>reduction</b> <b>tables</b> are used by navigators to achieve accuracies of one nautical mile (1.9 km) or less.|$|R
40|$|AbstractIn {{this paper}} {{we look at}} the {{phenomenon}} of autocomputability of infinite binary sequences. We define an appropriate mathematical model of this concept: the strongly autoreducible set. For this purpose, we need to introduce a natural restricted form of wtt-reduction between sets: minimal weak truth <b>table</b> <b>reduction.</b> Then we prove several basic facts about strongly autoreducible sets...|$|R
50|$|On September 17, 2012, Kelly {{released}} {{a white paper}} detailing the changes to the system {{after five years of}} use in the marketplace. The most notable difference {{between the first and second}} versions is the <b>reduction</b> of the <b>table</b> from 25 to 24 influence plays and from eight to seven subclasses, emphasized by a visual redesign.|$|R
40|$|At least 90 % of {{patients}} with Type 2 are hypertensive (> 130 / 80 mmHG) and have either low-density lipoprotein (LDL) cholesterol> 2. 5 mmol or a total cholesterol/high density lipopro-tein (HDL) > 4. 0 mmol. Furthermore, over 90 % have a body mass index (BMI) > 25 and a waist circumference of ≥ 102 cm. Most will satisfy definitions of the metabolic syndrome, and have a two- to fivefold risk of adverse cardiovascular outcomes. 1 Lorne’s Lament Lorne, 55, is new to your practice. He was diagnosed with diabetes three years ago. His current medications are ramipril, 10 mg/day, and glyburide, 5 mg twice/day. He is a lifetime non-smoker and drinks alcohol “socially”. Lorne rarely tests his glucose and does not own a home blood pressure monitor. He denies angina, heart attack, stroke, transient ischemic attack, or intermittent claudication. For more on Lorne, go to page 84. Is hypertension treatment effective? The Systolic Hypertension in the Elderly Program (SHEP) reported a 33 % reduction of stroke when patients with diabetes and isolated systolic hypertension were treated with chlorthalidone. 2 While a similar relative risk reduction was noted for those without diabetes, due to their higher baseline risk, patients with diabetes had a greater absolute risk <b>reduction.</b> <b>Tables</b> 1 and 2 list initial drug therapy for diabetes patients with hypertension and other treatment options. The Canadian Journal of CME / November 2004 83 At least 90 % ofpatients with Type 2 diabetes are hypertensive...|$|R
30|$|For mid-sized circuits, 2 {{the most}} compute-intensive {{part of the}} inner loop is the {{evaluation}} of the semiconductor models during the setup of the linear system (5), step (1.4) in Figure  1. Hence, other approaches target {{the evaluation of the}} semiconductor devices by either accelerated and parallelized evaluation [5, 25, 26] or by model order <b>reduction</b> such as <b>table</b> models [27] of the complex semiconductor models.|$|R
40|$|ABSTRACT: The {{traditional}} approach relying on sight <b>reduction</b> <b>tables,</b> a non-programmatic {{location of the}} position fix and an inadequate allowance for observation errors is still widely pursued and advocated. In the late 1970 s the programmatic Least Squares method (LSQ) was introduced which determines a random error fix (FixQ) for any multiple sights combination. B. D Yallop & C. Y Hohenkerk (1985) expanded LSQ to incorporate the computation of the random error margin of a fix. Several marketed PDA-based programs apply LSQ, but none have fully incorporated the random error margin {{as a guide for}} the navigator. All existing LSQ applications have two drawbacks. One is, all observation error is attributed to random sources, whereas the possibility of systematic error has in fact a long theoretical and practical background in celestial navigation. Systematic error represents a bias in statistical random error theory and can and should be allowed for. A major drawback is that existing LSQ program applications incorporate the running fix technique (RFT) traditionally applied in coastal navigation. It has no general validity in celestial navigation. The position circle of an earlier celestial sight can only be mathematically correctly transferred when its Geometric Position (GP) is transferred for the run data. A final aspect of reliability is the strategy adopted at the sight planning stage. At least during twilight observations, navigators should aim at getting three or four sights with a total azimuth angle> 180 o, with three successive subsights on each body. In such configurations FixQ and FixS will b...|$|R
40|$|This paper {{describes}} the collection, checking and homogenisation of a Canadian atmospheric surface pressure database. The {{object of the}} exercise {{was to create a}} database of monthly mean surface pressure for as many stations as possible across Canada as far back in time as possible. Data sources included the World Weather Records, Monthly Climatic Data for the World Bulletins, the Global Historical Climate Network and the electronic meteorological report archives of Environment Canada. Much of the earlier data was in paper form and had to be digitized by hand. Over 66, 000 individual mean monthly pressure values were obtained, with a missing value rate of 5. 9 %. The homogenisation procedures used were the Standard Normal Homogeneity Test (SNHT; Alexandersson and Moberg 1997) and Multiple Comparison Analysis (MCA; as used by Slonosky et al 1999). In addition, simple subtraction of sea-level pressure from station-level pressure revealed a major inhomogeneity which took place in 1977, when computer generated pressure <b>reduction</b> <b>tables</b> were used for the first time by the Meteorological Service of Canada, and when the meteorological reporting procedure was brought into alignment with the World Meteorological Organisation’s guidelines. As a result, the final homogenised database shows appreciable differences in trends compared to the unhomogenised series. The final database has been used by Slonosky & Graham (2003) in the statistical analysis of trends and variability of surface pressure across Canada during the 20 th century. Published in Proceedings of Fourth seminar for homogenisation and quality control in climatological databases. Budapest, Hungary. 6 - 10, October 2003...|$|R
40|$|The seven {{tables in}} this section {{describe}} different facets of the sentencing process. The tables provide definitions of basic sentencing terms in each state; powers and procedures of the courts in the sentencing process; the use of intermediate sanctions and sentencing guidelines; {{the consequences of a}} felon conviction, and provisions affecting sentence <b>reductions.</b> In <b>Table</b> 44, basic definitions are given for key terms used in the sentencing process. First, felony and misdemeanor offenses are described in terms of minimum and maximum sentence lengths, and for felonies in terms of possible fines. Next, the table defines the sentence enhancement provisions and mandatory minimums resulting from the use of deadly weapons {{in the course of a}} crime and from habitua...|$|R
40|$|The {{fuzziness}} and randomness {{of decision}} table affect hugely {{on the performance}} of knowledge acquisition in rough set. In order to reduce their influence, a novel reduction algorithm based on grey relational analysis is proposed. In the algorithm, every value of decision table is converted to the same domain. Moreover, on the basis of grey relational analysis, the grey relational matrix for the converted decision table is constructed to describe the equivalence relations between samples of decision table. Finally, the samples with the same similar level are adopted as the coarser granularity. The experiments fully show that the <b>reduction</b> decision <b>table</b> achieved almost the same recognition rate with less than one tenth of the former conditions. It fully shows the effectiveness of the algorithm...|$|R
40|$|In this study, the {{acquisition}} of rule-type knowledge from field inspection data on highway bridges is enhanced by introducing an improvement to a traditional data mining technique, i. e. applying the rough set theory to the traditional decision <b>table</b> <b>reduction</b> method. The new rough set theory approach helps in cases of exceptional and contradictory data, which in the traditional decision <b>table</b> <b>reduction</b> method are simply removed from analyses. Instead of automatically removing all apparently contradictory data cases, the new method determines whether the data really is contradictory and therefore must be removed or not. The new method is tested with real data on bridge members including girders and filled joints in bridges owned and managed by a highway corporation in Japan. There are, however, numerous inconsistent data in field data. A new method is therefore proposed {{to solve the problem}} of data loss. The new method reveals some generally unrecognized decision rules in addition to generally accepted knowledge. Finally, a computer programs is developed to perform calculation routines, and some field inspection data on highway bridges is used to show the applicability of the proposed method...|$|R
40|$|International audienceThis letter {{proposes a}} {{suboptimal}} {{implementation of a}} binary correlator suitable for detecting a known fixed pattern in a binary stream. The theoretical performances {{in terms of the}} probability of nondetection and the probability of false alarm are evaluated. These performances show that the degradations are negligible. Compared to a proprietary core provided by FPGA vendor, this implementation allows a 15 % look-up <b>table</b> <b>reduction,</b> a 30 % register reduction and up to a 30 % higher clock frequency in a FPGA...|$|R
40|$|Experimental {{aerodynamic}} investigations {{were conducted}} in a low speed wind tunnel on an 0. 0405 scale representation of the 89 A light weight Space Shuttle Orbiter to obtain pressure loads data {{in the presence of}} the ground for orbiter structural strength analysis. The model and the facility are described, and data <b>reduction</b> is outlined. <b>Tables</b> are included for data set/run number collation, data set/component collation, model component description, and pressure tap locations by series number. Tabulated force and pressure source data are presented...|$|R
40|$|This letter {{proposes a}} {{suboptimal}} {{implementation of a}} binary correlator suitable for detecting a known fixed pattern in a binary stream. The theoretical performances {{in terms of the}} probability of nondetection and the probability of false alarm are evaluated. These performances show that the degradations are negligible. Compared to a proprietary core provided by FPGA vendor, this implementation allows a 15 % look-up <b>table</b> <b>reduction,</b> a 30 % register reduction and up to a 30 % higher clock frequency in a FPGA. ...|$|R
40|$|Several {{surgical}} approaches {{could be}} used in hip arthroplasty or trauma surgery: anterior, anterolateral, lateral, posterior (with or without trochanterotomy), using or not an orthopedic <b>reduction</b> <b>table.</b> Subtrochanteric and extra-capsular trochanteric fractures (ECTF) are usually treated by internal fixation with mandatory restrictions on weight bearing. Specific complications have been widely described. Mechanical failures are particularly high in unstable fractures. Hip fractures are a major public health issue with a mortality rate of 12 %– 23 % at 1  year. An alternative option is to treat ECTF by total hip arthroplasty (THA) to prevent decubitus complications, to help rapid recovery, and to permit immediate weight bearing as well as quick rehabilitation. However, specific risks of THA have to be considered such as dislocation or cardiovascular failure. The classical approach (anterior or posterior) requires the opening of the joint and capsule, weakening hip stability and the repair of the great trochanter is sometimes hazardous. For 15  years, we have been treating unstable ECTF by THA with cementless stem, dual mobility cup (DMC), greater trochanter (GT) reattachment, and a new surgical approach preserving capsule, going through the fracture and avoiding joint dislocation. Bombaci first described a similar approach in 2008; our trans fractural digastric approach (medial gluteus and lateral vastus) is different. A coronal GT osteotomy is performed when there is no coronal fracture line. It allows easy access to the femoral neck and acetabulum. The THA is implanted without femoral internal rotation to avoid extra bone fragment displacement. With pre-operative planning, cup implantation is easy and stem positioning is adjusted referring {{to the top of the}} GT after trial reduction and preoperative planning. The longitudinal osteotomy and trochanteric fracture are repaired with wires and the digastric incision is closed. This variant of Bombaci approach could be use routinely for hemiarthroplasty or THA in the cases of unstable ECTF. It reduces complications usually linked to this procedure. Blood loss, operating time, and pain are limited, allowing fast recovery in order to decrease morbidity and mortality...|$|R
40|$|In {{this paper}} we analyze {{some of the}} main {{properties}} of a double base number system, using bases 2 and 3; in particular we emphasize the sparseness of the representation. A simple geometric interpretation allows an efficient implementation of the basic arithmetic operations and we introduce an index calculus for logarithmic-like arithmetic with considerable hardware <b>reductions</b> in look-up <b>table</b> size. We discuss the application of this new number system in the area of digital signal processing; we illustrate the discussion with examples of finite impulse response filtering...|$|R
40|$|This paper {{examines}} whether restaurant reservations {{should be}} locked to specific tables {{at the time}} the reservation is made, or whether the reservations should be pooled and assigned to tables in real-time. In two motivating studies, we find that {{there is a lack of}} consensus in the restaurant industry on handling reservations. Contrary to what might be expected based on research that shows the benefits of resource pooling in other contexts, a survey of 425 restaurants indicated that over 80 % lock reservations to tables. In two simulation studies, we determine that pooling reservations enables a 15 -minute <b>reduction</b> in <b>table</b> turn times more than 15 % of the time, which consequently increases service efficiency and enables a restaurant to serve more customers during peak periods. Pooling had the most consistent advantage with higher customer service levels, with larger restaurants, with customers who arrive late, and with larger variation in customer arrival time...|$|R
