0|36|Public
5000|$|Branch Delay Slot: Always {{fetch the}} {{instruction}} after the branch from the instruction cache, and always execute it, {{even if the}} branch is taken. Instead of taking an IPC penalty for some fraction of branches either taken (perhaps 60%) or not taken (perhaps 40%), <b>branch</b> delay <b>slots</b> take an IPC penalty for those branches into which the compiler could not schedule the <b>branch</b> delay <b>slot.</b> The SPARC, MIPS, and MC88K designers designed a <b>branch</b> delay <b>slot</b> into their ISAs.|$|R
50|$|When {{a branch}} {{instruction}} is involved, {{the location of}} the following delay slot instruction in the pipeline may be called a <b>branch</b> delay <b>slot.</b> <b>Branch</b> delay <b>slots</b> are found mainly in DSP architectures and older RISC architectures. MIPS, PA-RISC, ETRAX CRIS, SuperH, and SPARC are RISC architectures that each have a single branch delay slot; PowerPC, ARM, Alpha, and RISC-V do not have any. DSP architectures that each have a single <b>branch</b> delay <b>slot</b> include the VS DSP, µPD77230 and TMS320C3x. The SHARC DSP and MIPS-X use a double branch delay slot; such a processor will execute a pair of instructions following a branch instruction before the branch takes effect.|$|R
5000|$|The {{following}} {{example is}} MIPS I assembly code, showing both a load delay <b>slot</b> and a <b>branch</b> delay <b>slot.</b> lw v0,4(v1) # load word from address v1+4 into v0 nop # wasted load delay slot jr v0 # {{jump to the}} address specified by v0 nop # wasted <b>branch</b> delay <b>slot</b> ...|$|R
25|$|Many early RISC designs {{also shared}} the {{characteristic}} {{of having a}} <b>branch</b> delay <b>slot.</b> A <b>branch</b> delay <b>slot</b> is an instruction space immediately following a jump or branch. The instruction in this space is executed, {{whether or not the}} branch is taken (in other words the effect of the branch is delayed). This instruction keeps the ALU of the CPU busy for the extra time normally needed to perform a branch. Nowadays the <b>branch</b> delay <b>slot</b> is considered an unfortunate side effect of a particular strategy for implementing some RISC designs, and modern RISC designs generally do away with it (such as PowerPC and more recent versions of SPARC and MIPS).|$|R
40|$|Inline target insertion, a {{specific}} compiler and pipeline implementation method for delayed branches with squashing, is defined. The method {{is shown to}} offer two important features not discovered in previous studies. First, branches inserted into <b>branch</b> <b>slots</b> are correctly executed. Second, the execution returns correctly from interrupts or exceptions with only one program counter. These two features result in better performance and less software/hardware complexity than conventional delayed branching mechanisms...|$|R
50|$|The {{goal of a}} pipelined {{architecture}} is to complete an instruction every clock cycle. To maintain this rate, the pipeline must be full of instructions at all times. The <b>branch</b> delay <b>slot</b> is {{a side effect of}} {{pipelined architecture}}s due to the branch hazard, i.e. the fact that the branch would not be resolved until the instruction has worked its way through the pipeline. A simple design would insert stalls into the pipeline after a branch instruction until the new branch target address is computed and loaded into the program counter. Each cycle where a stall is inserted is considered one <b>branch</b> delay <b>slot.</b> A more sophisticated design would execute program instructions which are not dependent on the result of the branch instruction. This optimization can be performed in software at compile time by moving instructions into <b>branch</b> delay <b>slots</b> in the in-memory instruction stream, if the hardware supports this. Another side effect is that special handling is needed when managing breakpoints on instructions as well as stepping while debugging within <b>branch</b> delay <b>slot.</b>|$|R
40|$|The {{trend of}} deep {{pipelining}} and multiple instruction issue has made instruction sequencing an extremely critical issue. This paper defines Inline Target Insertion, a specific compiler and pipeline implementation method for Delayed Branches with Squashing. The method {{is shown to}} offer two important features not discovered in previous studies. First, branches inserted into <b>branch</b> <b>slots</b> are correctly executed. Second, the execution returns correctly from interrupts or exceptions with only one program counter. These two features make Inline Target Insertion a superior alternative (better performance and less software/hardware complexity) to the conventional delayed branching mechanisms...|$|R
50|$|Many RISC designs have {{included}} a <b>branch</b> delay <b>slot,</b> a position after a branch instruction {{that can be}} filled with an instruction which is executed {{whether or not the}} branch is taken. This feature can improve the performance of pipelined CPUs by absorbing some of the time wasted if a CPU mispredicts the operation of a conditional branch, and the CPU's pipeline stalls. RISC-V omits a <b>branch</b> delay <b>slot</b> because it complicates multicycle CPUs, superscalar CPUs, and long pipelines. Dynamic branch predictors have succeeded well enough to reduce the need for delayed branches.|$|R
2500|$|All MIPS I {{control flow}} {{instructions}} {{are followed by}} a <b>branch</b> delay <b>slot.</b> Unless the <b>branch</b> delay <b>slot</b> is filled by an instruction performing useful work, an nop is substituted. MIPS I branch instructions compare {{the contents of a}} GPR (rs) against zero or another GPR (rt) as signed integers and branch if the specified condition is true. Control is transferred to the address computed by shifting the 16-bit offset left by two bits, sign-extending the 18-bit result, and adding the 32-bit sign-extended result to the sum of the program counter (instruction address) and 810. Jumps have two versions: absolute and register-indirect. Absolute jumps ("Jump" [...] and [...] "Jump and Link") compute the address control is transferred to by shifting the 26-bit instr_index left by two bits and concatenating the 30-bit result with the two high-order bits of instruction in the <b>branch</b> delay <b>slot</b> address's. Register-indirect jumps transfer control to the instruction at the address sourced from a GPR (rs). The address sourced from the GPR must be word-aligned, else an exception is signaled after the instruction in the <b>branch</b> delay <b>slot</b> is executed. <b>Branch</b> and jump instructions that link (except for [...] "Jump and Link Register") save the return address to GPR 31. The [...] "Jump and Link Register" [...] instruction permits the return address to be saved to any writable GPR.|$|R
40|$|The {{performance}} of a superpipeline processor heavily relies on its branch performance. Traditional branch strategies used in pipelined processors are delayed branches and branch with squashing. Delayed branches use safe instructions to fill delay slots. However, for a deeply pipelined processor, a compiler {{may not be able}} to find sufficient safe instructions to fill the <b>branch</b> delay <b>slots.</b> <b>Branch</b> with squashing takes advantage of using instructions in target basic blocks to fill the <b>branch</b> delay <b>slots.</b> However, the penalty of branch misprediction is large in superpipelined processors. In this paper, we proposed a novel branch scheme, named branch with masked squashing, which takes advantage of both delayed branch and branch with squashing. The basic idea is to fill delay slots with safe instructions which may come from above or after the branch. For the remaining unfilled delay slots, we fill with instructions from the predicted target path. In the case of misprediction, only unsafe in [...] ...|$|R
50|$|The machine ran {{an early}} version of the Mach kernel for multi-processor support. The compilers were {{designed}} to keep the processors as full as possible by reducing the number of <b>branch</b> delay <b>slots,</b> and did a particularly good job of it.|$|R
25|$|MIPS II {{removed the}} load delay slot and added several sets of {{instructions}}. For shared-memory multiprocessing, the Synchronize Shared Memory, Load Linked Word, and Store Conditional Word instructions were added. A set of Trap-on-Condition instructions were added. These instructions caused an exception if the evaluated condition is true. All existing branch instructions were given branch-likely versions that executed the {{instruction in the}} <b>branch</b> delay <b>slot</b> only if the branch is taken. These instructions improve performance in certain cases by allowing useful instructions to fill the <b>branch</b> delay <b>slot.</b> Doubleword load and store instructions for COP1ndash&3 were added. Consistent with other memory access instructions, these loads and stores required the doubleword to be naturally aligned.|$|R
50|$|Both CPUs {{evaluate}} {{branches in}} the decode stage {{and have a}} single cycle instruction fetch. As a result, the branch target recurrence is two cycles long, and the machine always fetches the instruction immediately after any taken branch. Both architectures define <b>branch</b> delay <b>slots</b> in order to utilize these fetched instructions.|$|R
5000|$|Branch Likely: Always {{fetch the}} {{instruction}} after the branch from the instruction cache, but only execute {{it if the}} branch was taken. The compiler can always fill the <b>branch</b> delay <b>slot</b> on such a branch, and since branches are more often taken than not, such branches have a smaller IPC penalty than the previous kind.|$|R
50|$|Many {{processors}} include hardware {{support for}} breakpoints (typically instruction and data breakpoints). As an example, the x86 instruction set architecture provides hardware support for breakpoints with its x86 debug registers. Such hardware may include limitations, for example not allowing breakpoints on instructions located in <b>branch</b> delay <b>slots.</b> This kind of limitation is {{imposed by the}} microarchitecture of the processor and varies from processor to processor.|$|R
5000|$|Another {{technique}} is a <b>branch</b> delay <b>slot.</b> In this approach, one instruction after a branch is always executed. Therefore, the computer {{can use this}} instruction to do useful work whether or not its pipeline stalls. This approach was historically popular in RISC computers. In a family of compatible CPUs, it complicates multicycle CPUs (with no pipeline), faster CPUs with longer-than-expected pipelines, and superscalar CPUs (which can execute instructions out of order.) ...|$|R
5000|$|The first {{commercial}} RISC processors, the MIPS R2000 and R3000 and the earlier SPARC processors, do only trivial [...] "not-taken" [...] branch prediction. Because they use <b>branch</b> delay <b>slots,</b> fetched just one instruction per cycle, and execute in-order, {{there is no}} performance loss. The later, R4000 uses the same trivial [...] "not-taken" [...] branch prediction, and loses two cycles to each taken branch because the branch resolution recurrence is four cycles long.|$|R
40|$|Current {{schedule}}rs for acyclic regions schedule {{operations in}} dependence order and never revisit or undo a scheduling decision on any operation. In contrast, backtracking schedulers may unschedule already scheduled operations, {{in order to}} make space for the operation currently being scheduled. Backtracking schedulers have the potential for generating better schedules, e. g. more effectively filling <b>branch</b> delay <b>slots,</b> but are more compile-time intensive and therefore, not considered practical for production use. In this report, we first describe [...] ...|$|R
40|$|A flexible, parameterizable {{simulator}} of pipelined processors is presented. The simulator allows {{to configure}} several (micro-) architectural {{features such as}} the pipeline depth, the stage in which branch execution occurs, whether or not register file forwarding is performed, {{and the number of}} <b>branch</b> delay <b>slots.</b> We use the simulator to perform experiments with three synthetic benchmarks: vector addition, vector summation, and sum of absolute differences. These kernels are representative for data parallel loops, reduction operations, and benchmarks containing many hard to predict branches, respectively...|$|R
50|$|A NOP is most {{commonly}} used for timing purposes, to force memory alignment, to prevent hazards, to occupy a <b>branch</b> delay <b>slot,</b> to render void an existing instruction such as a jump, or as a place-holder {{to be replaced by}} active instructions later on in program development (or to replace removed instructions when reorganizing would be problematic or time-consuming). In some cases, a NOP can have minor side effects; for example, on the Motorola 68000 series of processors, the NOP opcode causes a synchronization of the pipeline.|$|R
50|$|The SPARC {{architecture}} {{was heavily}} influenced by the earlier RISC designs including the RISC I and II from the University of California, Berkeley and the IBM 801. These original RISC designs were minimalist, including as few features or op-codes as possible and aiming to execute instructions {{at a rate of}} almost one instruction per clock cycle. This made them similar to the MIPS architecture in many ways, including the lack of instructions such as multiply or divide. Another feature of SPARC influenced by this early RISC movement is the <b>branch</b> delay <b>slot.</b>|$|R
50|$|The ideal {{number of}} <b>branch</b> delay <b>slots</b> in a {{particular}} pipeline implementation is dictated {{by the number of}} pipeline stages, the presence of register forwarding, what stage of the pipeline the branch conditions are computed, whether or not a branch target buffer (BTB) is used and many other factors. Software compatibility requirements dictate that an architecture may not change the number of delay slots {{from one generation to the}} next. This inevitably requires that newer hardware implementations contain extra hardware to ensure that the architectural behavior is followed despite no longer being relevant.|$|R
40|$|A varietyof {{applications}} {{have arisen}} {{where it is}} worthwhile to apply code optimizations directly to the machine code (or assembly code) produced by a compiler. These include link-time whole-program analysis and optimization, code compression, binaryto -binary translation, and bit-transition reduction (f or power). Many, if not most, optimizations assume {{the presence of a}} control-flow graph (cfg). Compiled, scheduled code has properties that can make cfg construction more complex than it is inside a typical compiler. In this paper, we examine the problems of scheduled code on architectures that have multiple delay <b>slots.</b> In particular,if <b>branch</b> delay <b>slots</b> contain other <b>branches,</b> the classic algorithmsf or building a cfg produce incorrect results...|$|R
40|$|A {{variety of}} {{applications}} have arisen {{where it is}} worthwhile to apply code optimizations directly to the machine code (or assembly code) produced by a compiler. These include link-time whole-program analysis and optimization, code compression, binary- to-binary translation, and bit-transition reduction (for power). Many, if not most, optimizations assume {{the presence of a}} control-flow graph (cfg). Compiled, scheduled code has properties that can make cfg construction more complex than it is inside a typical compiler. In this paper, we examine the problems of scheduled code on architectures that have multiple delay slots. In particular, if <b>branch</b> delay <b>slots</b> contain other <b>branches,</b> the classic algorithms for building a cfg produce incorrect results. We explain the problem using two simple examples. We then present an algorithm for building correct cfgs from scheduled assembly code that includes <b>branches</b> in branch-delay <b>slots.</b> The algorithm works by building an approximate cfg and then refining it to reflect the actions of delayed branches. If all branches have explicit targets, the complexity of the refining step is linear with respect to the number of branches in the code. Analysis of the kind presented in this paper is a necessary first step for any system that analyzes or translates compiled, assembly-level code. We have implemented this algorithm in our power-consumption experiments based on the TMS 320 C 6200 architecture from Texas Instruments. The development of our algorithm was motivated by the output of TI’s compiler...|$|R
40|$|The Alpha AXP 64 -bit {{computer}} architecture {{is designed for}} high performance and longevity. Be-cause of the focus on multiple instruction issue, the architecture does not contain facilities such as <b>branch</b> delay <b>slots,</b> byte writes, and precise arith-metic exceptions. Because of the focus on multiple processors, the architecture does contain a careful shared-memory model, atomic-update primitive in-structions, and relaxed read/write ordering. The first implementation of the Alpha AXP architecture is the world’s fastest single-chip microprocessor. The DECchip 21064 runs multiple operating systems and runs native-compiled programs that were translated from the VAX and MIPS architectures. Thus in all these cases the Romans did what all wise princes ought to do; namely, not only to look to all present troubles, but also {{to those in the}} fu-ture, against which they provided with the utmos...|$|R
40|$|The {{performance}} of a Java Virtual Machine (JVM) interpreter running on a very long instruction word (VLIW) processor can be improved by means of pipelining. While one bytecode is in its execute stage, the next bytecode is in its decode stage, and the next bytecode is in its fetch stage. The paper describes how we implemented threading and pipelining by rewriting the source code of the interpreter and several modifications in the compiler. Experiments for evaluating the effectiveness of pipelining are described. Pipelining improves the execution speed of a threaded interpreter by 19. 4 % in terms of instruction count and 14. 4 % in terms of cycle count. Most of the simple bytecodes, like additions and multiplications, execute in four cycles. This number corresponds to the branch latency of our target VLIW processor. Thus most of the code of the interpreter is executed in <b>branch</b> delay <b>slots.</b> 1 Introduction One {{of the most important}} properties of the Java Platform is its abilit [...] ...|$|R
40|$|When {{integrating}} software threads {{together to}} boost {{performance on a}} processor with instruction-level parallel processing support, it is rarely clear which code regions should be aligned and integrated, and which regions should be left alone. This problem grows even worse on a modern VLIW DSP due to complicating factors in both the hardware and compiler: software pipelining, predication, <b>branch</b> delay <b>slots,</b> load delay slots and limited resources. As a result, finding an effective integration strategy requires extensive iteration through the integrate/compile/analyze sequence. In this paper we introduce methods to quantitatively estimate the performance benefit from the integration of multiple software threads. We use resource modeling, consider register pressure and compensate for compiler optimizations. This enables different scenarios to be compared and ranked. We then use these estimates to guide integration by concentrating on the most beneficial scenario. Information from each iteration of compilation is used to update the rankings of scenarios. We find that our modeling methods combined with limited compilation quickly find the best integration scenario without requiring exhaustive integration...|$|R
40|$|Increasing system {{complexity}} of SOC applications {{leads to an}} increased need of powerful embedded DSP processors. To fulfill the required computational bandwidth, state-of-the-art DSP processors allow executing several instructions in parallel and for reaching higher clock frequencies they {{increase the number of}} pipeline stages. However, deeply pipelined processors have drawbacks in the execution of branch instructions: branch delays. In average not more than two <b>branch</b> delay <b>slots</b> can be used, additional ones keep unused and decrease the overall system performance. Instead of compensating the drawback of branch delays (e. g. branch prediction circuits) it is possible {{to reduce the number of}} branch delays by reducing the number of branch instructions. Predicated execution (also guarded execution or conditional execution) can be used for implementing if-then-else constructs without using branch instructions. The drawback of traditional predicated execution is decreased code density. This paper introduces selective predicated execution based on FSEL which allows reducing the number of branch instructions without decreasing code density. Selective predicated execution based on FSEL is part of a project for a configurable DSP core. 1...|$|R
40|$|Instruction {{scheduling}} and register allocation play very {{important roles in}} compilation techniques for high-performance computers. In this thesis we study different analyses and transformations that may affect the performance of instruction {{scheduling and}} register allocation. An important technique for increasing code parallelism is alias analysis. Alias analysis can {{reduce the number of}} dependences in a Data Dependence Graph (DDG) by disambiguating address references. Our study shows that points-to analysis, the alias analysis method implemented in the McCAT compiler, can substantially increase parallelism in benchmarks with indirect memory references. In this thesis we also discuss the phase ordering problem of instruction scheduling and register allocation. An integrated approach [...] IPS (Integrated Prepass Scheduling) is implemented in the McCAT compiler. Experimental results show that IPS is an effective way to coordinate instruction scheduling and register allocation. Postpass scheduling and BDSF (<b>Branch</b> Delay <b>Slots</b> Filling) has been also implemented. Experiments were performed to measure the effectiveness of each scheduling method and combinations of the scheduling methods. These experiments show that IPS + postpass + BDSF is the best approach for the benchmarks studied...|$|R
40|$|Most {{software}} for embedded systems, including {{digital signal processing}} systems, is coded in assembly language. For both understanding the software and for reverse compiling it {{to a higher level}} language, we need to construct a control flow graph (CFG). However CFG construction is complicated by architectural features which include VLIW parallelism, predicated instructions and <b>branches</b> with delay <b>slots.</b> We describe an efficient algorithm {{for the construction of a}} CFG, where the parallelism has been eliminated, instructions are reordered and delay slots have been eliminated. The algorithm’s effectiveness has been demonstrated by its use in a reverse compiler for the Texas Instruments C 60 series of digital signal processors. ...|$|R
40|$|One of {{the main}} {{obstacles}} to exploiting the fine-grained parallelism that is available in general-purpose code is the frequency of branches that cause unpredictable changes in the control flow of a program at run-time. Whenever a branch is taken, a performance penalty may be incurred as the processor waits for instructions to be fetched from the branch target stream. RISC processors introduce a delayed-branch mechanism which defines <b>branch</b> delay <b>slots</b> into which code can be scheduled. This strategy allows the processor to be kept busy executing useful instructions while the change of control flow takes place. While the concept of delayed-branches can be readily extended to VLIW architectures, it is less clear {{how it should be}} incorporated in a superscalar architecture. This paper proposes a general branch-delay mechanism which is suitable for a range of code-compatible superscalar processors and which completely avoids the need to introduce NOPs into the code. This technique was developed {{as an integral part of}} the HSP superscalar project. HSP is a superscalar architecture currently being developed at the University of Hertfordshire with the aim of using compile-time instruction scheduling to achieve an order of magnitude speed-up over traditional RISC architectures for a suite of non-numeric benchmark programs...|$|R
40|$|Abstract A {{variety of}} {{applications}} have arisen where it isworthwhile to apply code optimizations {{directly to the}} machine code (or assembly code) produced bya compiler. These include link-time whole-program analysis and optimization, code compression, binary-to-binary translation, and bit-transition reduction (for power). Many, if not most, optimizations assumethe presence of a control-flow graph (cfg). Com-piled, scheduled code has properties that can make cfg construction more complex than it is inside a typ-ical compiler. In this paper, we examine the problems of scheduled code on architectures that have multipledelay slots. In particular, if <b>branch</b> delay <b>slots</b> contain other <b>branches,</b> the classic algorithms for building a cfg produce incorrect results. We explain the problem using two simple exam-ples. We then present an algorithm for building correct cfgs from scheduled assembly code that includesbranches in branch-delay slots. The algorithm works by building an approximate cfg and then refiningit to reflect the actions of delayed branches. If all branches have explicit targets, the complexity of therefining step is linear {{with respect to the}} number of branches in the code. Analysis of the kind presented in this paper is anecessary first step for any system that analyzes or translates compiled, assembly-level code. We have implemented this algorithm in ourpower-consumption experiments based on th...|$|R
40|$|For {{the second}} lab assignment, {{you are to}} write an RTL model of a {{two-stage}} pipelined SMIPSv 2 processor using Verilog and to synthesize your RTL model. After producing a working RTL model, you will attempt to optimize your design to increase performance and/or decrease area. The deliverables for this lab are (a) your optimized Verilog source {{and all of the}} scripts necessary to completely synthesize your RTL implementation checked into SVN, (b) two assembly test programs and two C benchmark programs to test your implementation, and (c) written answers to the critical questions given {{at the end of this}} document. The lab assignment is due at the start of class on Thursday, September 24. You must submit your written answers electronically by adding a directory titled writeup to your lab project directory (lab 2 /trunk/writeup). Electronic submissions must be in plain text or PDF format. You are encouraged to discuss your design with others in the class, but you must turn in your own work. The two-stage pipeline should perform instruction fetch in the first stage, while the second pipeline stage should do everything else including data memory access. Since SMIPS does not have a <b>branch</b> delay <b>slot,</b> you will need to handle branches carefully to ensure that incorrect instructions are not accidental executed. If you need to refresh your memory about pipelining and the MIPS instruction set, we recommen...|$|R

