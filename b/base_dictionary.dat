13|914|Public
2500|$|Ambiguity may be {{introduced}} in inflection – even if okurigana specify the {{reading in the}} <b>base</b> (<b>dictionary)</b> form of a verb, the inflected form may obscure it. For example, [...] i-ku [...] "go" [...] and [...] okona-u [...] "perform, carry out" [...] are distinct in dictionary form, but in past ("perfective") form become [...] i-tta [...] "went" [...] and [...] okona-tta [...] "performed, carried out" [...] – which reading to use must be deduced from context or furigana.|$|E
40|$|An {{efficient}} and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary {{is based on}} a sparsity model of the dictionary atoms over a <b>base</b> <b>dictionary,</b> and takes the form D = ΦA where Φ is a fixed <b>base</b> <b>dictionary</b> and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-{{efficient and}} costly to deploy. In this paper we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3 -D image denoising...|$|E
40|$|Abstract—An {{efficient}} and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary {{is based on}} a sparsity model of the dictionary atoms over a <b>base</b> <b>dictionary,</b> and takes the form D= 8 A, where 8 is a fixed <b>base</b> <b>dictionary</b> and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-{{efficient and}} costly to deploy. In this paper, we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3 -D image denoising. Index Terms—Computed tomography, dictionary learning, K-SVD, signal denoising, sparse coding, sparse representation. I...|$|E
40|$|AbstractCompression {{algorithm}} is what reduces the redundancy of data representation and decreases the data storage capacity. Data compression plays {{a vital role}} in reducing the communication cost making use of available bandwidth. The compressed data from the security aspect is transmitted through internet. It is, however very much vulnerable to a multitude of attacks. To propose a new <b>dictionary</b> <b>based</b> text compression technique for ASCII texts for the purpose of obtaining good performance on various document sizes. <b>Dictionary</b> <b>based</b> compression bits are hidden into the Lsb bit of audio signals and to calculate the signal to noise ratio (SNR). This audio Steganography is conducted for various compression algorithms with <b>dictionary</b> <b>based</b> compression. Audio Steganography <b>based</b> <b>dictionary</b> compression achieves better value of signal to noise ratio (SNR) ...|$|R
5000|$|When a class {{becomes a}} client to , the formal generic {{parameters}} are substituted with actual generic parameters in a generic derivation. In the following attribute declaration, [...] {{is to be}} used as a character string <b>based</b> <b>dictionary.</b> As such, both data and key formal generic parameters are substituted with actual generic parameters of type [...]|$|R
40|$|Tree and Trie are the Abstract Data Types (ADTs) {{that provide}} {{efficient}} implementation of ordered dictionary. The {{performance of a}} data structure will depend on hardware capabilities of the computing devices such as RAM size, Cache memory size and even {{the speed of the}} physical storage media. Hence, an application which will be running on real or virtualized hardware environment certainly will have restricted access to memory and other resources of the real hardware. Further, the time taken for any operation on a data structure rely on the data usage model and the most significant operations/tests are very much depend {{on the size of the}} “character payload objects ” which we use in dictionary like implementations. In this work, we do an analysis on the performance of Tree and Trie <b>based</b> <b>Dictionary</b> ADT Implementations with different data usage models. We consider data usage models such as a typical electronic dictionary with more than million of words or a typical electronic encyclopedia with large string data elements. In this work, we studied the performance of three popular Tree <b>based</b> <b>Dictionary</b> Implementations rbtree, googlebtree, stxbtree, and three Trie <b>based</b> <b>Dictionary</b> Implementations tommy-trie, tommy-trie-inplace, nedtrie under different hardware and software configurations. Among all, tommy trie is proved to be the best for character payload objects with 16 bytes and 4096 bytes. In some operations/tests googlebtree seems to be better. Our evaluation on different tree and trie structures shows tommy trie implementations perform well irrespective of size of application...|$|R
40|$|Abstract—We {{present a}} {{dictionary}} learning approach {{to compensate for}} the transformation of faces due to changes in view point, illumination, resolution, etc. The key idea of our approach is to force domain-invariant sparse coding, i. e., design a consistent sparse representation of the same face in different domains. In this way, classifiers trained on the sparse codes in the source domain consisting of frontal faces for example {{can be applied to the}} target domain (consisting of faces in different poses, illumination conditions, etc) without much loss in recognition accuracy. The approach is to first learn a domain <b>base</b> <b>dictionary,</b> and then describe each domain shift (identity, pose, illumination) using a sparse representation over the <b>base</b> <b>dictionary.</b> The dictionary adapted to each domain is expressed as sparse linear combinations of the <b>base</b> <b>dictionary.</b> In the context of face recognition, with the proposed compositional dictionary approach, a face image can be decomposed into sparse representations for a given subject, pose and illumination respectively. This approach has three advantages: first, the extracted sparse representation for a subject is consistent across domains and enables pose and illumination insensitive face recognition. Second, sparse representations for pose and illumination can subsequently be used to estimate the pose and illumination condition of a face image. Finally, by composing sparse representations for subject and the different domains, we can also perform pose alignment and illumination normalization. Extensive experiments using two public face datasets are presented to demonstrate the effectiveness of our approach for face recognition...|$|E
40|$|We {{present a}} {{dictionary}} learning approach {{to compensate for}} the transformation of faces due to changes in view point, illumination, resolution, etc. The key idea of our approach is to force domain-invariant sparse coding, i. e., design a consistent sparse representation of the same face in different domains. In this way, classifiers trained on the sparse codes in the source domain consisting of frontal faces for example {{can be applied to the}} target domain (consisting of faces in different poses, illumination conditions, etc) without much loss in recognition accuracy. The approach is to first learn a domain <b>base</b> <b>dictionary,</b> and then describe each domain shift (identity, pose, illumination) using a sparse representation over the <b>base</b> <b>dictionary.</b> The dictionary adapted to each domain is expressed as sparse linear combinations of the <b>base</b> <b>dictionary.</b> In the context of face recognition, with the proposed compositional dictionary approach, a face image can be decomposed into sparse representations for a given subject, pose and illumination respectively. This approach has three advantages: first, the extracted sparse representation for a subject is consistent across domains and enables pose and illumination insensitive face recognition. Second, sparse representations for pose and illumination can subsequently be used to estimate the pose and illumination condition of a face image. Finally, by composing sparse representations for subject and the different domains, we can also perform pose alignment and illumination normalization. Extensive experiments using two public face datasets are presented to demonstrate the effectiveness of our approach for face recognition. Comment: Transactions on Image Processing, 201...|$|E
40|$|In this paper, we {{describe}} {{the ways in which}} medical knowledge is encoded into the knowledge <b>base</b> <b>dictionary</b> of an expert system (Iliad) designed as a teaching and consulting tool. Starting with a basic hierarchy, attributes have been added to facilitate the entry and display of patient data and inferencing about diagnosis and optimal work-up of the patient. For inferencing, a table of inter-term relationships has been added to the dictionary and techniques for deriving partial information using frequencies of occurrence of parent and child nodes in the hierarchy have been incorporated in the program. The disease independent knowledge included in the dictionary component is necessary to support the expert system's ability to make medically relevant and common sense inferences and generate realistic patient cases...|$|E
5000|$|... 2005 — Armenian PowerSpell 2005 (only spelling, <b>based</b> on <b>dictionary</b> with 200,000 words) ...|$|R
5000|$|... 2000 — Armenian Orfo (ArmOrfo) 2000 (only spelling, <b>based</b> on <b>dictionary</b> with 130,000 words) ...|$|R
5000|$|The {{meaning of}} the ancient village name <b>based</b> on <b>dictionary</b> word is the following, ...|$|R
40|$|Abstract. Sparse {{decompression}} is a {{new theory}} for signal processing, having the advantage in that the <b>base</b> (<b>dictionary)</b> used in this theory is over-complete, and can reflect the nature of signa 1. So the sparse decompression of signal can get sparse representation, which {{is very important in}} data compression. In this paper, a novel ECG compression method for multi-channel ECG signals was introduced based on the Simultaneous Orthogonal Matching Pursuit (S-OMP). The proposed method decomposes multi-channel ECG signals simultaneously into different linear expansions of the same atoms that are selected from a redundant dictionary, which is constructed by Hermite fuctions and Gobar functions in order to the best match the characteristic of the ECG waveform. Compression performance has been tested using a subset of multi-channel ECG records from the St. -Petersburg Institute of Cardiological Technics database, the results demonstrate that much less atoms are selected to present signals and the compression ratio of Multi-channel ECG can achieve better performance in comparison to Simultaneous Matching Pursuit (SMP) ...|$|E
40|$|Discriminative {{dictionary}} learning, {{playing a}} critical role in sparse representation based classification, has led to state-of-the-art classification results. Among the existing discriminative dictionary learning methods, two different approaches, shared dictionary and class-specific dictionary, which associate each dictionary atom to all classes or a single class, have been studied. The shared dictionary is a compact method but with lack of discriminative information; the class-specific dictionary contains discriminative information but consists of redundant atoms among different class dictionaries. To combine the advantages of both methods, we propose a new weighted block dictionary learning method. This method introduces proto dictionary and class dictionary. The proto dictionary is a <b>base</b> <b>dictionary</b> without label information. The class dictionary is a class-specific dictionary, which is a weighted proto dictionary. The weight value indicates the contribution of each proto dictionary block when constructing a class dictionary. These weight values can be computed conveniently as they are designed to adapt sparse coefficients. Different class dictionaries have different weight vectors but share the same proto dictionary, which results in higher discriminative power and lower redundancy. Experimental results demonstrate that the proposed algorithm has better classification results compared with several dictionary learning algorithms...|$|E
40|$|Password {{authentication}} is {{the most}} widely used authentication mechanism, and it will still be with us for many years yet to come. It is effective, simple, and accurate, with no extra cost. The strength of password authentication relies on the strength of the passwords. Good (or strong) passwords are essential for high level security. End user education and computerized proactive password checking play vital roles in ensuring good passwords. However, both demand clear, simple, and concise rules on what a good password is. It is not hard to find guidelines and advices on good passwords; but it is not so easy to find a clear, simple, and concise rule to be used for end user education and computer programs for proactive password checking. In this paper, we develop a theoretic framework on measuring password quality – password quality indicator (PQI). A PQI of a password is a pair λ = (D, L), where D is the Levenshtein's edit distance of the password to the <b>base</b> <b>dictionary</b> words, and L is the effective password length. Based on PQI, we further simplify the rule for a good password to at least 8 characters long, with at least 3 special characters plus other alphanumeric characters. Key words...|$|E
40|$|The {{discipline}} where sentiment / opi-nion / emotion {{has been}} identified and classified in human written text is well known as sentiment analysis. A typical computational approach to sentiment analysis starts with prior polarity lex-icons where entries are tagged with their prior out of context polarity as human beings perceive using their cognitive knowledge. Till date, all re-search efforts found in sentiment lex-icon literature deal mostly with English texts. In this article, we propose mul-tiple computational techniques like, WordNet <b>based,</b> <b>dictionary</b> <b>based,</b> cor-pus based or generative approaches for generating SentiWordNet(s) for Indian languages. Currently, SentiWordNet(s) are being developed for three India...|$|R
40|$|Have {{been done}} by making of <b>based</b> physics <b>dictionary</b> {{interactive}} multimedia to increase learn physics can more easy and effective To make of <b>based</b> physics <b>dictionary</b> interactive multimedia method is used: analysing problem in physics, making scheme, making of application program use software Macromedia Flash MX and terminated by doing examination application program. This aplication is consist of three picture symbol, they are: movie clip, graphic and button. Movie clip is a moving picture, graphic is a static picture, and button is picture us as a button. This aplication control by mouse and keyboard From result of examination <b>based</b> physics <b>dictionary</b> interactive multimedia, can be obtained a physics term, explanation picture, and particle simulation INTISAR...|$|R
5000|$|... 2009 — Armenian PowerSpell 2009 (spelling, {{grammar and}} non-Armenian words checker, <b>based</b> on <b>dictionary</b> with 570,000 words) ...|$|R
40|$|Sparse {{representations}} {{has shown}} to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results {{in a wide variety of}} tasks. Yet, these methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the <b>base</b> <b>dictionary</b> within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call trainlets...|$|E
40|$|Dictionary {{learning}} is a challenge topic in many image processing areas. The basic goal is to learn a sparse representation from an overcomplete basis set. Due to combining the advantages of generic multiscale representations with learning based adaptivity, multiscale dictionary representation approaches have the power in capturing structural characteristics of natural images. However, existing multiscale learning approaches still suffer from three main weaknesses: inadaptability to diverse scales of image data, sensitivity to noise and outliers, difficulty to determine optimal dictionary structure. In this paper, we present a novel multiscale dictionary learning paradigm for sparse image representations based on an improved empirical mode decomposition. This powerful data-driven analysis tool for multi-dimensional signal can fully adaptively decompose the image into multiscale oscillating components according to intrinsic modes of data self. This treatment can obtain a robust and effective sparse representation, and meanwhile generates a raw <b>base</b> <b>dictionary</b> at multiple geometric scales and spatial frequency bands. This dictionary is refined by selecting optimal oscillating atoms based on frequency clustering. In order to further enhance sparsity and generalization, a tolerance dictionary is learned using a coherence regularized model. A fast proximal scheme is developed to optimize this model. The multiscale dictionary is considered {{as the product of}} oscillating dictionary and tolerance dictionary. Experimental results demonstrate that the proposed learning approach has the superior performance in sparse image representations as compared with several competing methods. We also show the promising results in image denoising application. Comment: to be published in Neurocomputin...|$|E
40|$|Dictionary Learning (DL) {{has seen}} {{widespread}} use in signal processing and machine learning. Given a data set, DL seeks {{to find a}} so-called ‘dictionary’ such that the data can be well represented by a sparse linear combination of dictionary elements. The representational power of DL may be extended {{by the use of}} kernel mappings, which implicitly map the data to some high dimensional feature space. In Kernel DL we wish to learn a dictionary in this underlying high-dimensional feature space, which can often model the data more accurately than learning in the original space. Kernel DL is more challenging than the linear case however since we no longer have access to the dictionary atoms directly – only their relationship to the data via the kernel matrix. One strategy is therefore to represent the dictionary as a linear combination of the input data whose coefficients can be learned during training [1], relying on the fact that any optimal dictionary lies in the span of the data. A difficulty in Kernel DL is that given a data set of size N, the full (N ×N) kernel matrix needs to be manipulated at each iteration and dealing with such a large dense matrix can be extremely slow for big datasets. Here, we impose an additional constraint of sparsity on the coefficients so that the learned dictionary is given by a sparse linear combination of the input data. This greatly speeds up learning, and furthermore the speed-up is greater for larger datasets and can be tuned via a dictionary-sparsity parameter. The proposed approach thus combines Kernel DL with the ‘double sparse’ DL model [2] in which the learned dictionary is given by a sparse reconstruction over some <b>base</b> <b>dictionary</b> (in this case, the data itself). We investigate the use of sparse Kernel DL as a feature learning step for a music transcription task and compare it to another Kernel DL approach based on the K-SVD algorithm [1] (which doesnt lead to sparse dictionaries in general), in terms of computation-time and performance. Initial experiments show that Sparse Kernel DL is significantly faster than the non-sparse Kernel DL approach (6 × to 8 × speed-up {{depending on the size of}} the training data and the sparsity level) while leading to similar performance...|$|E
50|$|There are {{differences}} in quality of hardware (hand held devices), software (presentation and performance), and dictionary content. Some hand helds are more robustly constructed than others, and the keyboards or touch screen input systems should be physically compared before purchase. The information on the GUI of computer <b>based</b> <b>dictionary</b> software ranges from complex and cluttered, to clear and easy-to-use with user definable preferences including font size and colour.|$|R
30|$|In the {{proposed}} method, similarity between concepts extracted by a network based measure. In the future, other approaches {{can be used}} for extracting semantic similarity, such as corpus <b>based</b> or <b>dictionary</b> <b>based</b> approach. Also we can add other measures for enhance accuracy of proposed method. Moreover, combine content analysis and social network analysis approaches can be done differently.|$|R
30|$|The {{article is}} {{organized}} as follows: Section 2 describes swallowing accelerometry and outlines {{the advantages of}} this approach for detecting swallowing difficulties. In Section 3, we describe the proposed approach for CS using the time-frequency <b>based</b> <b>dictionary</b> consisting of MDPSS bases. Section 4 reports the data analysis steps that we carried out to obtain the reported results, which are presented in Section 5 along with {{the discussion of the}} same results. The conclusions are drawn in Section 6.|$|R
50|$|This tool from Google is <b>based</b> on <b>dictionary</b> <b>based</b> phonetic {{transliteration}} approach. In {{contrast to}} older Indic typing tools, which work by transliterating under a particular scheme, Google transliterates by matching the Latin alphabet words with an inbuilt dictionary. Since users {{do not need}} to remember the transliteration scheme, the service is so easy that it is suitable for total beginners.|$|R
50|$|Machine {{translation}} can use {{a method}} <b>based</b> on <b>dictionary</b> entries, {{which means that the}} words will be translated as they are by a dictionary.|$|R
40|$|International audienceTo {{estimate}} geometrically regular {{images in}} the white noise model and obtain an adaptive near asymptotic minimaxity result, we consider a model selection based bandlet estimator. This bandlet estimator combines the best basis selection behaviour of the model selection and the approximation properties of the bandlet dictionary. We derive its near asymptotic minimaxity for geometrically regular images {{as an example of}} model selection with general <b>dictionary</b> of orthogonal <b>bases.</b> This paper is thus also a self contained tutorial on model selection with orthogonal <b>bases</b> <b>dictionary...</b>|$|R
5000|$|Chinese-English word <b>dictionary</b> (<b>based</b> on Paul Denisowski's CEDICT) ...|$|R
5000|$|Japanese-French <b>dictionary</b> (<b>based</b> on J.-M. Desperrier's Dico FJ) ...|$|R
5000|$|Simplified Chinese-English <b>dictionary</b> (<b>based</b> on Paul Denisowski's CEDICT) ...|$|R
30|$|WordNet or DBpedia {{are open}} {{vocabulary}} <b>based</b> <b>dictionaries</b> {{that may be}} used to find correlation between words. However, they mainly intend to provide a synonymous platform and other concepts that can be correlated and be used to describe content are not inferred from these systems. Attempts to use multimedia domain lists of words and dictionaries {{led to the creation of}} specific lists such as NUS-WIDE [38] a popular web image dataset extracted from Flickr and that includes approximately 260 k images with a manual annotation of 81 concept categories.|$|R
40|$|Both the Advanced Authoring Format (AAF) and the {{proposed}} Material eXchange Format (MXF) standard define a large set of highly structured metadata. This paper discusses {{a representation of the}} MXF/AAF metadata models using a set of XML Schema <b>based</b> <b>dictionaries.</b> MXF/AAF implementers can use these schemas for three purposes: a.) to standardise metadata handling in a human readable format, b.) to assist MXF standard maintenance and development, and c.) to automate the creation and maintenance of MXF/AAF encoding/decoding software. Examples of successful implementations of each application are given...|$|R
5000|$|In {{computer}} science, Iacono's working set structure [...] is {{a comparison}} <b>based</b> <b>dictionary.</b> It supports insertion, deletion and access operation {{to maintain a}} dynamic set of [...] elements. The working set of an item [...] is the set of elements that have been accessed in the structure {{since the last time}} that [...] was accessed (or inserted if it was never accessed).Inserting and deleting in the working set structure takes [...] time while accessing an element [...] takes [...] Here, [...] represents the size of the working set of [...]|$|R
40|$|Many {{compression}} {{techniques have}} been proposed to accommodate ever increasing sofmare pieces into restricted memory area in embedded systems. Recently. these techniques {{have been shown to}} improve other important design constrains like energv andperformance. This paper proposes a blended dictionarj model based on static/dynamic profiling that lead to best trade-offs on compression, performance and enera> savings. We also propose a new <b>dictionary</b> <b>based</b> code compression algorithm, independent of the cache organization and processor, to support OUT experiments. A mix of benchmarks from Mediahench and MiBench suites revels that compression ratios of 75 % can be obtained while decreasing bus accesses lo the cache by 3 l%for the Leon processor. These results approach simultaneously the best solutions of when using pure static or pure dynamic information <b>based</b> <b>dictionaries</b> techniques. 1...|$|R
5000|$|ABC (Alphabetically <b>Based</b> Computerized) Chinese-English <b>Dictionary</b> (colophon 1996: iv), ...|$|R
5000|$|Japanese-Russian <b>dictionary</b> (<b>based</b> on Oleg V. Volkov's Edict files) ...|$|R
