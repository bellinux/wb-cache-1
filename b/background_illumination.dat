279|237|Public
25|$|Increasing <b>background</b> <b>illumination</b> {{or using}} a pinhole to {{effectively}} decrease pupil diameter may allow {{a person to}} obtain {{a better view of}} his or her own floaters. The head may be tilted in such a way that one of the floaters drifts towards the central axis of the eye. In the sharpened image the fibrous elements are more conspicuous.|$|E
50|$|There {{are special}} {{versions}} for A0 double-sized boards, to make large drawings, or copying-boards with <b>background</b> <b>illumination,</b> which {{have all that}} is necessary to provide specific support.|$|E
5000|$|Different {{modulation}} frequencies: If {{the cameras}} modulate their light with different modulation frequencies, their light is {{collected in the}} other systems only as <b>background</b> <b>illumination</b> but does not disturb the distance measurement.|$|E
30|$|The OLVFOFR {{processed}} {{images have}} a mean brightness {{higher than the}} input but less than the results from AIEBHE and AGCCID. However, the latter two methods do not produce good qualitative images. Particularly, there are artifacts and uneven <b>background</b> <b>illuminations</b> being generated.|$|R
5000|$|For {{background}} on his reading Macdonald's monograph, Gabriel Miró: his private library and his literary <b>background.</b> provides <b>illumination.</b>|$|R
40|$|This paper {{discusses}} the technical challenges in maritime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames {{in a video}} are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant <b>backgrounds,</b> <b>illumination</b> effects, all contribute to the challenges as discussed here...|$|R
50|$|Note.— The two {{distances}} {{have different}} values in {{air of a}} given extinction coefficient, and the latter b) varies with the <b>background</b> <b>illumination.</b> The former a) {{is represented by the}} meteorological optical range (MOR).|$|E
50|$|With light adaptation, the eye has {{to quickly}} {{adapt to the}} <b>background</b> <b>illumination</b> {{to be able to}} {{distinguish}} objects in this background. The process for light adaptation occurs over a period of five minutes.|$|E
5000|$|... #Caption: The {{electronics}} for the Cyclops {{was contained}} on three small circuit boards. The two red LED's {{on the first}} board are the bias lights to provide <b>background</b> <b>illumination</b> of the sensor for low-light conditions.|$|E
40|$|This work {{presents}} a novel background-foreground classification technique based on adaptive non-parametric kernel estimation in a color-gradient space of components. By combining normalized color components with their gradients, shadows are efficiently suppressed from the results, while the luminance {{information in the}} moving objects is preserved. Moreover, a fast multi-region iterative tracking strategy applied over previously detected foreground regions allows to construct a robust foreground modeling, which combined with the background model increases noticeably the quality in the detections. The proposed strategy {{has been applied to}} different kind of sequences, obtaining satisfactory results in complex situations such as those given by dynamic <b>backgrounds,</b> <b>illumination</b> changes, shadows and multiple moving objects...|$|R
30|$|Statistical {{distortion}} {{video frame}} contains a video clip, {{and the client}} cannot normally play video clips, and there is significant <b>background</b> partial <b>illumination</b> changing video frames that are compared, obtained after the XOR video frame and the video frame reference for statistical failure value.|$|R
40|$|International audienceIn this paper, {{we address}} the {{difficult}} task of detecting and segmenting foreground moving objects in complex scenes. The sequences we consider exhibit highly dynamic <b>backgrounds,</b> <b>illumination</b> changes and low contrasts, and can have been shot by a moving camera. Three main steps compose the proposed method. First, a set of moving points is selected within a sub-grid of image pixels. A multi-cue descriptor is associated to each of these points. Clusters of points are then formed using a variable bandwidth mean shift technique with automatic bandwidth selection. Finally, segmentation of the object associated to a given cluster is performed using graph cuts. Experiments and comparisons to other motion detection methods on challenging sequences demonstrate the performance of the proposed method for video analysis in complex scenes...|$|R
50|$|In 1868, the Duke of Edinburgh visited Launceston {{where he}} planted two Oak {{trees and a}} massive celebration {{was held in the}} Square {{featuring}} a choir, candlelight and <b>background</b> <b>illumination</b> from the city's new town gas supply. The event saw the name changed from the former 'St John's Square' to 'Prince's Square' to commemorate the momentous event.|$|E
50|$|Increasing <b>background</b> <b>illumination</b> {{or using}} a pinhole to {{effectively}} decrease pupil diameter may allow {{a person to}} obtain {{a better view of}} his or her own floaters. The head may be tilted in such a way that one of the floaters drifts towards the central axis of the eye. In the sharpened image the fibrous elements are more conspicuous.|$|E
50|$|Both {{artificial}} {{and natural}} {{objects in the}} sky can {{be very difficult to}} detect using only the intensity of light. These objects include clouds, satellites, and aircraft. However, the polarization of these objects due to resonant scattering, emission, reflection, or other phenomena can differ from that of the <b>background</b> <b>illumination.</b> Thus they can be more easily detected by using polarization imaging. There is a wide range of remote sensing applications in which polarization is useful for detecting objects that are otherwise difficult to see.|$|E
40|$|For object {{recognition}} systems {{it is essential}} to have an internal representation of the object to be recognized. We introduce a system which learns such a representation from training images of an object class. Every image is preprocessed with banana wavelets. The result is a description of local areas of an image in terms of curved lines. These features are the input to a clustering algorithm which learns to seperate features specic for an object class from features generated accidentally by variations in <b>background</b> or <b>illumination.</b> This leads to a representation of an object class which can be visualized in form of a line drawing. The representation is sparse, for the most part free of redundancies and independent of varying <b>backgrounds</b> and <b>illuminations</b> in the training images. It comprises representative features only, and has already been utilized successfully for {{object recognition}} tasks. ...|$|R
40|$|In {{transparency}} {{perception the}} visual system assigns transmission-related attributes to transparent layers. Based on a filter model of perceptual transparency we investigate {{to what extent}} these attributes remain constant across changes of <b>background</b> and <b>illumination.</b> On a computational level, we used computer simulations to test how constant {{the parameters of the}} filter model remain under realistic changes in <b>background</b> reflectances and <b>illumination</b> and found almost complete constancy. This contrasts with systematic deviations from constancy found in cross-context matches of transparent filters. We show that these deviations are of a very regular nature and can be understood as a compromise between a proximal match of the mean stimulus color and complete constancy as predicted by the filter model. status: publishe...|$|R
40|$|This paper {{presents}} scale invariant face {{detection and}} classification methods which use spectral features extracted from Log-Polar image. Scale changes {{of a face}} in a image are represented as shift along the vertical axis in Log-Polar image. In order to make them robust to the scale changes of faces, spectral features are extracted from the each row of the Log-Polar image. Autocorrelations, Fourier power spectrum, and PARCOR coefficients are used as spectral features. Then these features are combined with simple classification methods based on the Linear Discriminant Analysis to realize scale invariant face detection and classification. The effectiveness of the proposed face detection method is confirmed by the experiment using the face images which are captured under the different scales, <b>backgrounds,</b> <b>illuminations,</b> and dates. We have also performed the experiments to evaluate the proposed face classification method using 2800 face images with 7 scales under 2 different backgrounds...|$|R
50|$|The {{darkness}} {{of the night sky}} {{is an important factor in}} optical astronomy. With the size of cities and human populated areas ever expanding, the amount of artificial light at night has also increased. These artificial lights produce a diffuse <b>background</b> <b>illumination</b> that makes observation of faint astronomical features very difficult without special filters. In a few locations such as the state of Arizona and in the United Kingdom, this has led to campaigns for the reduction of light pollution. The use of hoods around street lights not only improves the amount of light directed toward the ground, but also helps reduce the light directed toward the sky.|$|E
5000|$|David Bowie, as the {{character}} Major Tom, {{dressed in a}} spacesuit, and performing [...] "Space Oddity". This mimicked {{the section of the}} music video for the song, where a fully outfitted Major Tom can be seen spinning around in space, with a panicked Ground Control attempting to contact him. The lighting was set so that only the head of the animatronic was well-lit by a spotlight, with the body only lit by <b>background</b> <b>illumination.</b> This was intended to help hide the mechanics and rod that the figure was mounted on and which provided the spacemans rotation effect. As the song reached the lines [...] "Nothing I can do.. Nothing I can do", the spotlight and background lighting was switched off and replaced instantly by a projected film sequence of a spaceman receding away from the camera to infinity.|$|E
50|$|The {{normal human}} observer’s {{relative}} wavelength sensitivity {{will not change}} due to <b>background</b> <b>illumination</b> change under scotopic vision. The wavelength sensitivity {{is determined by the}} rhodopsin photopigment. This is a red pigment and can be seen at the back of eye of animals that have a white background to their eye called Tapetum lucidum. The pigment is not noticeable under photopic and mesopic conditions. The principle that the wavelength sensitivity does not change during scotopic vision led to the ability to detect two functional cone classes in individuals. If two cone classes are present, then their relative sensitivity will change the behavioral wavelength sensitivity. Therefore, experimentation can determine “the presence of two cone classes by measuring wavelength sensitivity on two different backgrounds and noting a change in the observer’s relative wavelength sensitivity.”For adaption to occur at very low levels, the human eye needs to have a large sample of light across the signal {{in order to get a}} reliable image. This leads to the human eye being unable to resolve high spatial frequencies in low light since the observer is spatially averaging the light signal.|$|E
30|$|An image {{processing}} procedure OLVFOFR, {{for the restoration}} of online video ferrography images against out-of-focus degradations, has been presented. The algorithm first extracts wear particle edges appearing in the image and then amplifies them in accordance to the optimum scale and shift profile that is generated depending on the pixel distance to the image center. The enhanced image is obtained by combining the amplified edges with the original image. Out-of-range pixel magnitudes are compressed using a hyperbolic function and further normalized to within the permitted magnitude bounds. Results have shown that the enhanced images are free of viewing artifacts, having even <b>background</b> <b>illuminations</b> and enhanced exposure of wear particles. These desirable characteristics are essential in online video ferrography analysis. From the test {{of a large number of}} real-world images, it is also found that optimal algorithmic parameters rest on closed ranges, and these values can be applied directly in the proposed algorithm for more efficient implementation.|$|R
40|$|Traditional {{projection}} algorithms {{have a bad}} {{adaptation to}} face image <b>background</b> and <b>illumination,</b> and are affected by eyebrow, bangs and eye wrinkle. Due to above problems, an algorithm of eye location based on image marking and curve blending is proposed in this paper. Firstly, according to the specialty of eye gray distribution and direction selectivity of certain operators, two image marking methods are defined to overcome the influence of <b>background</b> and <b>illumination.</b> On the basis of image marking, {{in order to make}} full use of strong change of eye gray and low gray value, adaptive weight projection curve blending method is proposed in this paper. In addition, we have analyzed the projection curve after blending to ensure horizontal and vertical positions, and then realized the eye accurate location. According to the experiments on YaleB and FERET face databases, our algorithm can overcome the influence of complex illumination etc, and the correct rate has achieved 99. 9 % when measure standard Derr< 0. 25...|$|R
40|$|This paper {{deals with}} an {{application}} of image sequence analysis. In particular, it addresses the problem of determining {{the number of people}} who get into and out of a train carriage when it's crowded and <b>background</b> and/or <b>illumination</b> might change. The proposed system analyses image sequences and processes them using an algorithm based on the use of several morphological tools and optical flow motion estimation...|$|R
50|$|The {{basic idea}} is that the {{illumination}} pattern is imaged onto a geometrically congruent cutoff pattern (essentially a multiplicity of knife edges) with focusing optics, while density gradients lying between the illumination pattern and the cutoff pattern are imaged, typically by a camera system. As in classical schlieren, the distortions produce regions of brightening or darkening corresponding to the position and direction of the distortion, because they redirect rays either away from or onto the opaque part of the cutoff pattern. While in classical schlieren, distortions over the whole beam path are visualized equally, in focusing schlieren, only distortions in the object field of the camera are clearly imaged. Distortions away from the object field become blurred, so this technique allows some degree of depth selection. It also has the advantage that a wide variety of illuminated backgrounds can be used, since collimation is not required. This allows construction of projection-based focusing schlieren systems which are much easier to build and align than classical schlieren systems. The requirement of collimated light in classical schlieren is often a substantial practical barrier for constructing large systems due to the need for the collimating optic to be {{the same size as the}} field of view. Focusing schlieren systems can use compact optics with a large <b>background</b> <b>illumination</b> pattern, which is particularly easy to produce with a projection system. For systems with large demagnification, the illumination pattern needs to be around twice the size of the field of view to allow for defocusing of the background pattern.|$|E
40|$|We {{investigate}} theoretically and experimentally one-dimensional bright photovoltaic solitons in LiNbO 3 :Fe crystal {{by use of}} the <b>background</b> <b>illumination.</b> We find that, in LiNbO 3 :Fe crystal, bright photovoltaic solitons can {{be obtained}} with <b>background</b> <b>illumination</b> for kappa> 1, where K is {{the ratio of the}} <b>background</b> <b>illumination</b> photovoltaic constant to the soliton optical beam&# 39;s photovoltaic constant. For kappa< 1, dark photovoltaic solitons are generated. On the other hand, our experiments show good agreement with theoretical prediction for the soliton existence curve in a special intensity ratio. Crown Copyright (c) 2008 Published by Elsevier GmbH. All rights reserved...|$|E
40|$|The {{observation}} of self-focusing and self-trapping in potassium lithium tantalate niobate (KLTN) crystals in a very-near-transition paraelectric phase without any external applied field {{and without any}} sort of artificial <b>background</b> <b>illumination</b> is reported. This behavior depends only on the temperature T of the sample and is independent on beam intensity and size. Finally, the effect of <b>background</b> <b>illumination</b> in KLTN is studied using a y polarized uniform beam of the same intensity as the focused beam. The presence of a <b>background</b> <b>illumination</b> increases the value of Ib and numerical integration decreases the value of γ at which quasi-soliton like propagation occurs, thus explaining the decreased focusing observed...|$|E
40|$|We {{address the}} problem of {{detecting}} and tracking people with a mobile robot. The need for following a person with a mobile robot arises in many different service robotic applications. The main problems of this task are realtime-constraints, a changing <b>background,</b> varying <b>illumination</b> conditions and a non-rigid shape of the person to be tracked. The presented system has been tested extensively on a mobile robot in our everyday office environment...|$|R
40|$|Abstract: The aim of {{this work}} is to {{simulate}} glaring headlights by measuring the effects of glare on human contrast perception and including the results in a driving simulator. As contrast perception is highly subjective, two psychophysical experiments were performed. For different glare intensities and durations, the recurring contrast perception of the subject was recorded. Further, varying <b>background</b> <b>illuminations</b> were incorporated. The results have been integrated into a driving simulation by adjusting the display contrast. Afterwards the implementation was evaluated in a field test. simulation. Such simulator studies offer the possibility to examine the driving behavior in critical traffic situations under controlled conditions. However, {{to be able to}} draw transferable conclusions from simulated test data on the driving behavior, the visual quality of the simulation has to be perceptually realistic. The modified night driving simulation provides a more realistic visualization and enables the analysis of critical traffic scenarios including headlight glare. This leads to a better transferability of driving simulator results to reality and enables a deeper research of the interaction of oncoming vehicles and advanced driver assistance systems...|$|R
40|$|Detecting and {{segmenting}} moving {{objects in}} dynamic scenes {{is a hard}} but essential task in {{a large number of}} applications such as surveillance. Most existing methods only give good results in the case of persistent or slowly changing background, or if both the objects and the background can be characterized by simple parametric motions. This paper aims at detecting and segmenting foreground moving objects in the absence of such constraints. The sequences we consider have highly dynamic <b>backgrounds,</b> <b>illumination</b> changes and low contrasts, and can have been shot by a moving camera. Three main steps compose the proposed method. First, moving points are selected within a sub-grid of image pixels. A descriptor is associated to each of these points. Clusters of points are then formed using a variable bandwidth mean shift with automatic bandwidth selection. Finally, segmentation of the object associated to a given cluster is performed using Graph cuts. Experiments and comparison to other motion detection methods on challenging sequences show the performance of the proposed method and its utility for video analysis in complex scenes...|$|R
40|$|Retinal action {{potentials}} {{were recorded}} at the corneas of light-adapted wolf spider eyes {{in response to}} large positive and negative step changes in <b>background</b> <b>illumination.</b> These incremental responses were superimposed upon the steady-state DC responses to the <b>background</b> <b>illumination.</b> Both positive and negative step responses had peaks which overshot the DC levels to which they decayed. The overshoot was greater for positive than for negative steps. Short term DC responses measured after one-half sec were larger for negative than for positive steps; these short-term DC responses were thus asymmetrical. However, responses to short positive and negative flashes were not asymmetrical; rather, they varied linearly with flash amplitude. Asymmetries were thus delayed in onset. The short-term DC responses {{were found to be}} different from the steady-state DC responses to maintained changes in <b>background</b> <b>illumination.</b> There was an approximately exponential decay or creep from the short-term to the steady-state DC responses. It is proposed that the dynamics of delayed asymmetries can explain the waveforms of the short-term transient responses...|$|E
3000|$|... is {{the maximum}} {{distance}} {{that can be}} measured without ambiguity because of phase superimposition. The main advantage of this method {{is the need for}} only one specific camera. Moreover, the high-energy light pulses are not very sensitive to the <b>background</b> <b>illumination.</b>|$|E
40|$|Assembly, used {{to achieve}} linear {{response}} in optical detection system, consists of tungsten lamp source, optical filters, fiber optics bundle, aperture mask, relay lens, and folding mirror. Tungsten lamp source provides sufficient <b>background</b> <b>illumination</b> to make input optical flux {{small compared to}} background...|$|E
40|$|Traditional {{particle}} filter cannot accommodate {{to the environment}} of <b>background</b> interferences, <b>illumination</b> variations and occlusions. This paper presents a face tracking method with fusion of color histogram, contour features and grey model based on {{particle filter}}. First, it brought in contour features as the main cue of multiple features when tracking the face without stable color histogram. Then, as prior information was neglected in traditional particle filter, this paper employed GM(1, 1) model to yield proposal distribution, such that the proposal distribution would bear a higher approximation to posterior probability. Finally, in the importance sampling step, sampling was corresponded to the particle weight in case of the particle degradation. The experiments show that our method outperformed the previous with more accuracy and flexibility, particularly under the condition of color <b>background</b> interferences, drastic <b>illumination</b> variations and complete occlusions. </em...|$|R
40|$|Abstract—This paper {{deals with}} an {{application}} of image sequence analysis. In particular, it addresses the problem of determining {{the number of people}} who get into and out of a train carriage when it’s crowded, and <b>background</b> and/or <b>illumination</b> changes. The proposed system analyzes image sequences and processes them using an algorithm based on the use of several morphological tools, which are presented in detail in the paper. Index Terms—Computer vision, morphological operations, people counting, real-time. I...|$|R
40|$|Although it {{has been}} widely {{discussed}} in video surveillance, background subtraction is still an open problem {{in the context of}} complex scenarios, e. g., dynamic <b>backgrounds,</b> <b>illumination</b> variations, and indistinct foreground objects. To address these challenges, we propose an effective background subtraction method by learning and maintaining an array of dynamic texture models within the spatio-temporal representations. At any location of the scene, we extract a sequence of regular video bricks, i. e. video volumes spanning over both spatial and temporal domain. The background modeling is thus posed as pursuing subspaces within the video bricks while adapting the scene variations. For each sequence of video bricks, we pursue the subspace by employing the ARMA (Auto Regressive Moving Average) Model that jointly characterizes the appearance consistency and temporal coherence of the observations. During online processing, we incrementally update the subspaces to cope with disturbances from foreground objects and scene changes. In the experiments, we validate the proposed method in several complex scenarios, and show superior performances over other state-of-the-art approaches of background subtraction. The empirical studies of parameter setting and component analysis are presented as well. Comment: 12 pages, 7 figure...|$|R
