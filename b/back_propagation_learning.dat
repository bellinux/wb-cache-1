141|3223|Public
40|$|Abstract — Cell {{assembly}} is one {{of explanations}} of information processing in the brain, in which an information is represented by a firing space pattern {{of a group of}} plural neurons. On the other hand, effectiveness of neural network has been confirmed in pattern recognition, system control, signal processing, and so on, since the <b>back</b> <b>propagation</b> <b>learning</b> was proposed. In this study, we propose a new network structure with chaotically-selected affordable neurons in the hidden layer of the feedforward neural network. Computer simulated results show that the proposed network exhibits a good performance for the <b>back</b> <b>propagation</b> <b>learning.</b> I...|$|E
40|$|Standard {{neural network}} based on general <b>back</b> <b>propagation</b> <b>learning</b> using delta method or {{gradient}} descent method has some great faults like poor optimization of error-weight objective function, low learning rate, instability. This paper introduces a hybrid supervised <b>back</b> <b>propagation</b> <b>learning</b> algorithm which uses trust-region method of unconstrained optimization of the error objective function by using quasi-newton method. This optimization {{leads to more}} accurate weight update system for minimizing the learning error during learning phase of multi-layer perceptron. [13][14][15] In this paper augmented line search is used for finding points which satisfies Wolfe condition. In this paper, This hybrid back propagation algorithm has strong global convergence properties & is robust & efficient in practice. Comment: Accepted for publish in 18 th December, 2012,International Journal of Computer Applications, Foundation of Computer Science, New York, US...|$|E
40|$|Most {{application}} {{work within}} neural computing continues to employ multi-layer perceptrons (MLP). Though many {{variations of the}} fully interconnected feed-forward MLP, and even more variations of the <b>back</b> <b>propagation</b> <b>learning</b> rule, exist; {{the first section of}} the paper attempts to highlight several properties of these standard networks. The second section outlines an application-namely the prediction of horse racing result...|$|E
5000|$|It usually forms part of {{a larger}} pattern {{recognition}} system. It has been implemented using a perceptron network whose connection weights were trained with <b>back</b> <b>propagation</b> (supervised <b>learning).</b>|$|R
40|$|This paper proposes {{the use of}} {{artificial}} neural networks to predict the flexural resistance and initial stiffness of beam-to-column steel joints using the <b>back</b> <b>propagation</b> supervised <b>learning</b> algorithm. Three types of steel beam-to-column joints were investigated: welded, endplate and bolted with top, seat and double web angles, respectively. The neural networks results proved {{to be consistent with}} experimental and design code reference values...|$|R
40|$|In this work, various {{neural network}} {{algorithms}} have been compared for function approximation problems. Multilayer Perceptron (MLP) structure with standard <b>back</b> <b>propagation,</b> MLP with fast <b>back</b> <b>propagation</b> (adaptive <b>learning</b> and momentum term added), MLP with Levenberg-Marquardt learning algorithms, Radial Basis Function (RBF) network structure trained by OLS algorithm and Conic Section Function Neural Network (CSFNN) with adaptive learning {{have been investigated}} for various functions. Results showed that the neural algorithms {{can be used for}} functional estimation as an alternative to classical methods...|$|R
40|$|Abstract. The fuzzy ARTMAP {{has been}} applied to the {{supervised}} classi ® cation of multi-spectral remotely-sensed images. This method is found to be more e � cient, in terms of classi ® cation accuracy, compared to the conventional maximum likelihood classi ® er and also multi-layer perceptron with <b>back</b> <b>propagation</b> <b>learning.</b> The results have been discussed. 1...|$|E
40|$|This chapter {{describes}} an analog VLSI {{implementation of a}} multilayer perceptron neural network with on-chip <b>back</b> <b>propagation</b> <b>learning.</b> Local adaptation of the learning rate offers fast convergence. Experimental results from a chip fabricated in the ATMEL ES 2 ECPD 07 0. 7 um CMOS process demonstrate learning of a 2 -input exclusive-or task in 32 ms...|$|E
30|$|In {{most of the}} {{reported}} research MLP <b>back</b> <b>propagation</b> <b>learning</b> and RBFN are employed on weather forecasting with less or more same input parameters. However, fusion of classifiers seems to be better option on weather forecasting as their efficiency is better than individual classifier. Finally, authors recommend using a large standard train set, fusion of several classifiers for precise weather forecasting.|$|E
40|$|In this study, {{performances}} {{comparison to}} discriminate five mental states of five {{artificial neural network}} (ANN) training methods were investigated. Wavelet Packet Transform (WPT) was used for feature extraction of the relevant frequency bands from raw electroencephalogram (EEG) signals. The five ANN training methods used were (a) Gradient Descent <b>Back</b> <b>Propagation</b> (b) Levenberg-Marquardt (c) Resilient <b>Back</b> <b>Propagation</b> (d) Conjugate <b>Learning</b> Gradient <b>Back</b> <b>Propagation</b> and (e) Gradient Descent <b>Back</b> <b>Propagation</b> with movementum...|$|R
40|$|In this paper, {{performance}} of three classifiers for classification of five mental tasks were investigated. Wavelet Packet Transform (WPT) {{was used for}} feature extraction of the relevant frequency bands from raw Electroencephalograph (EEG) signal. The three classifiers namely used were Multilayer <b>Back</b> <b>propagation</b> Neural Network, Support Vector Machine and Radial Basis Function Neural Network. In MLP-BP NN five training methods used were a) Gradient Descent <b>Back</b> <b>Propagation</b> b) Levenberg-Marquardt c) Resilient <b>Back</b> <b>Propagation</b> d) Conjugate <b>Learning</b> Gradient <b>Back</b> <b>Propagation</b> and e) Gradient Descent <b>Back</b> <b>Propagation</b> with movementum...|$|R
40|$|International audienceAlgorithmic Differentiation (AD) {{provides}} the analytic derivatives of functions given as programs. Adjoint AD, which computes gradients, {{is similar to}} <b>Back</b> <b>Propagation</b> for Machine <b>Learning.</b> AD researchers study strategies to overcome the difficulties of adjoint AD, {{to get closer to}} its theoretical efficiency. To promote fruitful exchanges between <b>Back</b> <b>Propagation</b> and adjoint AD, we present three of these strategies and give our view of their interest and current status...|$|R
40|$|The analog VLSI {{implementation}} of an on-chip learning neural network {{is discussed in}} this paper. The multi layer perceptron paradigm and <b>back</b> <b>propagation</b> <b>learning</b> rule have been mapped onto analog circuits. A local learning rate adaptation rule has been also considered to improve the training performance (i. e., fast convergence speed). Experimental results confirm the chip functionality and the soundness of our approac...|$|E
40|$|In {{this paper}} we present the analog CMOS {{architecture}} of a Multi Layer Perceptron network with onchip stochastic <b>Back</b> <b>Propagation</b> <b>learning.</b> The learning algorithm {{is based on}} a local learning rate adaptation technique which makes the on-chip implementation more ef®cient (i. e. fast convergence speed) with respect to similar architectures presented in the literature. Circuit simulation results on the XOR learning problem validate the network behavior...|$|E
40|$|This paper {{reports the}} results {{obtained}} from the application of artificial neural networks to the Australian wheat variety classification problem. A 'HyperSAB' network with a self adaptive acceleration strategy for the error <b>back</b> <b>propagation</b> <b>learning</b> has been developed. This {{has been applied to}} six different Australian wheat varieties with 200 samples in each case. The results indicate that the artificial neural network has some potential to be used as an identification tool in this problem...|$|E
40|$|In {{this paper}} {{performance}} of two classifiers based on Neural Network were investigated for classification of five mental tasks from raw Electroencephalograph (EEG) signal. Aim {{of this research}} was to improve brain computer interface (BCI) system applications. For this study, Wavelet packet transform (WPT) was used for feature extraction of the relevant frequency bands from raw electroencephalogram (EEG) signals. The two classifiers used were Radial Basis Function Neural Network (RBFNN) and Multilayer Perceptron <b>Back</b> <b>propagation</b> Neural Network(MLP-BP NN). In MLP-BP NN five training methods used were (a) Gradient Descent <b>Back</b> <b>Propagation</b> (b) Levenberg-Marquardt (c) Resilient <b>Back</b> <b>Propagation</b> (d) Conjugate <b>Learning</b> Gradient <b>Back</b> <b>Propagation</b> and (e) Gradient Descent <b>Back</b> <b>Propagation</b> with movementum...|$|R
40|$|Neural network are broadly used to {{approximate}} non-linear functions. However, {{it is difficult}} to decide an appropriate structure for a given problem. In this paper, “growing neural network ” is proposed as an extension of <b>Back</b> <b>Propagation</b> (BP) <b>learning.</b> The propagated error signal is diffused from a target neuron as a substance. The axon of a growing neuron grows according to the concentration gradient of the substance. In a simulation, it was examined that the simplest problems, “AND ” and “OR”, could be solved by the neural network and 2 -layer structure was properly obtained. 1...|$|R
40|$|Neural network (NNs) {{approach}} for controlling switching power converters {{has been investigated}} in this paper. Several NNs techniques including the <b>Back</b> <b>propagation</b> training algorithm, <b>Back</b> <b>propagation</b> with momentum, <b>Back</b> <b>propagation</b> with adaptive <b>learning</b> rate and the <b>Back</b> <b>propagation</b> with Levenberg Marquardt optimisation have been investigated and the most later method has been selected for identification and control of Quasi-Resonant Converters (QRCs). A neural network emulator is designed in this paper, which reproduce the converter dynamic behaviour with great accuracy. The NN controller is developed and applied to regulate the converter output voltage. Results obtained from the NNC proved the validity and fast dynamic response of the proposed controller. ...|$|R
40|$|Graduation date: 1990 In this thesis, the {{reduction}} of neural networks is studied. A new, largely automatic method is developed for reducing a neural network, and its capabilities are analyzed and compared with current methods. An example is presented that is irreducible via existing techniques, but is reducible by the new method. Factors affecting convergence to solutions using the <b>back</b> <b>propagation</b> <b>learning</b> method are discussed. In addition, the heuristic of adding hidden units to a neural network to obtain convergence is analyzed mathematically...|$|E
40|$|Abstract: Engineers often meet {{function}} regression {{problems in}} manufacturing. In this work, we use artifical neural network {{to solve this}} problem. We choose a typical function as the target function. Since the input value of the function is continuous, {{the output of the}} regression model should also have a continuous value range. We implement the feed forward neural network with <b>back</b> <b>propagation</b> <b>learning</b> algorithm, investigate different network parameters, and compare several different training algorithms. The performance assessment based on the test dataset is also discussed...|$|E
40|$|Abstract-The {{concept of}} {{contribution}} of agents {{has been introduced}} in the paper to handle impreciseness in problem domain. Initially, experts assign the agents to solve the tasks based on their qualifications while contribution of an agent {{with respect to a}} specific task is determined using belief measures. To solve an assigned task, agents communicate with others and the respondents are clustered using Kohonen’s network, resulting reallocation of agents. Progress of each task thus improves and using revised <b>back</b> <b>propagation</b> <b>learning</b> algorithm, approximate solution has been generated after one epoch. The system has been implemented using Java Socket programming technique in Linux platform...|$|E
40|$|This paper {{describes}} a robust and efficient method for rotation and location independent identification and localization of facial images using one modified Radial Basis Function Network (RBFN) which embeds a new Heuristic Based Clustering (HBC) and <b>Back</b> <b>Propagation</b> (BP) <b>learning.</b> HBC in RBFN determines the natural number of clusters or {{groups on the}} basis of „person-view‟. BP network learns to identify a „person ‟ irrespective of his view. The method successfully performs location invariant upright and rotated facial identification in different views and expressions with or without occlusion. The learning as well as identification with standard facial database is fast, efficient, effective and the accuracy as well as precision of the system with Holdout Method is moderate...|$|R
40|$|This {{research}} is focus on {{optical character recognition}} (OCR) for handwritten 5 basic consonant from Javanese character called "Aksara Jawa Nglegena" consists of "Ha", "Na", "Ca", "Ra", and "Ka". The pattern recognition method used is multi layer perceptron with <b>back</b> <b>propagation</b> as the <b>learning</b> algorithm. 75 samples {{was used as the}} learning data and 25 samples used for testing data. The final weight produced could recognize all the learning samples correctly and 56 % testing samples...|$|R
40|$|Abstract — Unlike many {{approaches}} to machine learning, human learning involves selective attention. When confronted by new things to learn, people can rapidly shift attention, thereby increasing speed of acquisition and decreasing interference with previous knowledge. The shift of attention is itself learned, so that attention is allocated to particular cues in particular contexts. While selective attention benefits acquisition, {{it can also}} lead to distortions of knowledge that are evident when the knowledge is transferred to novel situations. Several mathematical models have been designed to implement selective attention in learning; the models quantitatively fit human performance in many experiments. This presentation reviews various research projects of the author. Index Terms — Attention, <b>back</b> <b>propagation,</b> Bayesian <b>learning,</b> connectionist model, eye tracking, incremental learning...|$|R
40|$|In this paper, {{we created}} a dynamic {{function}} training rate for the <b>Back</b> <b>propagation</b> <b>learning</b> algorithm to avoid the local minimum and to speed up training. The Back propagation with dynamic training rate (BPDR) algorithm uses the sigmoid function. The 2 -dimensional XOR problem and iris data were used as benchmarks to test {{the effects of the}} dynamic training rate formulated in this paper. The results of these experiments demonstrate that the BPDR algorithm is advantageous with regards to both generalization performance and training speed. The stop training or limited error was determined by 1. 0 e-...|$|E
40|$|This paper {{proposes a}} neural network control scheme of a DC-DC Buck-Boost {{converter}} to produce variable DC voltage source {{that will be}} applied on DC motor drives. In this technique, a <b>back</b> <b>propagation</b> <b>learning</b> algorithm is derived. The controller is designed to track the output voltage of the DC-DC converter and to improve performance of the Buck-Boost converter during transient operations. Furthermore, to investigate {{the effectiveness of the}} proposed controller, some operations such as starting-up and reference voltage variations are verified. The numerical simulation results show that the proposed controller has a better performance compare to the conventional PI-Controller method...|$|E
40|$|Voltage {{stability}} {{problems have}} been one of the major concerns for electric utilities as a result of system heavy loading. As electric power systems are operated under increasingly stressed conditions, the ability to maintain voltage stability becomes a growing concern. This paper reports on an investigation on the application of artificial neural networks (ANNs) in voltage stability assessment. A multilayer feedforward ANN with error <b>back</b> <b>propagation</b> <b>learning</b> is proposed for calculation of voltage stability index (L). Extensive testing of the proposed ANN based approach indicates its viability for power system voltage assessment. Test results are presented on two sample power systems...|$|E
40|$|This note {{offers an}} {{extension}} of Tam and Kiang (Tam, K. Y., M. Y. Kiang. 1992. Management applications of neural networks: The case of bank failure predictions. Management Sci. 38 (7) 926 [...] 947.). First {{the weakness of the}} standard <b>back</b> <b>propagation</b> neural network <b>learning</b> algorithm is discussed, and then a warning is issued regarding applications of artificial neural networks in the management science field. Also suggested is a possible way of improving the performance of neural networks in managerial applications. neural networks, artificial intelligence, classification...|$|R
40|$|Abstract:-Handwriting {{recognition}} is having high demand in commercial & academics. In recent years {{lots of good}} {{work has been done}} on hand written digit recognition to improve accuracy. Handwritten digit recognition system needs larger dataset and long training time to improve accuracy & reduce error rate. Training of Neural Networks for large data sets is very time consuming task on CPU. Hence, in this paper we presented fast efficient artificial neural network for handwritten digit recognition on GPU to reduce training time. Standard <b>back</b> <b>propagation</b> (BP) <b>learning</b> algorithm with multilayer perceptron (MLP) classification is chosen for this task & implemented on GPU for parallel training. This paper focused on specific parallelization environment Compute Unified Device Architecture (CUDA) on a GPU hence effectively speedup training & reduce training time...|$|R
40|$|The {{present study}} assesses the {{technique}} of <b>back</b> <b>propagation</b> neural networks to appraise the average response time of B 2 C Electronic Commerce architecture. In order to delineate the response time, diverse array of user requests were engaged per unit time. Furthermore, engagement of <b>Back</b> <b>Propagation</b> Network <b>Learning</b> (BPNL) algorithm is used to summarize the average response time and augment {{the enactment of the}} system. The comprehensive study does the comparative investigation to express the average response time for ANN enabled and without-ANN-enabled algorithm. The objective was to plaid whether ANN enabled algorithm had any bearing on the overall performance of the system. For BPNL algorithm, learning of the responses for the user requests were steered for 7 repetitions and then thorough phases were accomplished to assess the response time. After each iterations, error rates were dogged and then feed forward and <b>back</b> <b>propagation</b> algorithm were used to improve the performance. The experimentation will find its prominence in imminent B 2 C Electronic Commerce system project and employment and will convey the outline for such investigation. Finally, the study expands the meticulous inferences of the study...|$|R
40|$|Abstract. BP {{neural network}} is a {{multilayer}} feed-forward neural network, it achieved from input to output arbitrary nonlinear mapping, and weights are adjusted {{by using the}} <b>back</b> <b>propagation</b> <b>learning</b> algorithm. Intrusion detection systems using the learning ability of neural network to extract the network data profile, and it also can use the neural network has the ability of self learning and parallel processing ability, {{through the construction of}} intelligent neural network classifier to identify abnormal, so as to achieve the purpose of detecting intrusion behavior. The paper proposes the development of intrusion detection system based on improved BP neural network. Experimental results show that the proposed algorithm has high efficiency...|$|E
40|$|This paper {{presents}} {{analysis of}} a modified Feed Forward Multilayer Perceptron (FMP) by inserting an ARMA (Auto Regressive Moving Average) model at each neuron (processor node) with the Backp ropagation learning algorithm. The stability analysis is presented to establish the convergence theory of the Back propagation algorithm based on the Lyapunov function. Furthermore, the analysis extends the <b>Back</b> <b>propagation</b> <b>learning</b> rule by introducing the adaptive learning factors. A range of possible learning factors {{is derived from the}} stability analysis. Performance of such network learning with adaptive learning factors is presented and demonstrates that the adaptive learning factor enhance the performance of training while avoiding oscillation phenomenon...|$|E
40|$|The goal of {{this paper}} is to {{introduce}} a new neural network architecture called Sigmoid Diagonal Recurrent Neural Network (SDRNN) to be used in the adaptive control of nonlinear dynamical systems. This is done by adding a sigmoid weight victor in the hidden layer neurons to adapt of the shape of the sigmoid function making their outputs not restricted to the sigmoid function output. Also, we introduce a dynamic <b>back</b> <b>propagation</b> <b>learning</b> algorithm to train the new proposed network parameters. The simulation results showed that the (SDRNN) is more efficient and accurate than the DRNN in both the identification and adaptive control of nonlinear dynamical systems...|$|E
40|$|Abstract — Stochastic {{resonance}} {{is observed}} when noise {{added to a}} system improves the systems performance in some ways. Using this concept, we apply noise to neural networks in order to observe their effect on network performance. In our previous research, we have proposed a new modified <b>back</b> <b>propagation</b> (BP) <b>learning</b> algorithm, in which chaotic noise is added into weight update process [1][2]. Computer simulation {{results show that the}} addition of chaotic noise during weight update process can give a faster learning performance and a better convergence rate compared to the standard BP algorithm. Hence, in this study, we extend our investigation by simulating our proposed network to learn sine wave function with different frequencies as one learning problem. We observe and analyze the network performance in order to confirm the generality of this work. I...|$|R
40|$|Ordinary {{data mining}} {{requires}} accurate input data, but privacy concerns may bar {{use of such}} techniques. Thus, privacy preserving data mining methods are needed, which can work well without opening the private data. Although, much {{work has been done}} on privacy preserving classification, {{to the best of our}} knowledge, there has not been a privacy preserving perceptron neural network learning algorithm that can work in the real world on distributed databases. To solve this problem, this study brings forward a privacy preserving <b>Back</b> <b>Propagation</b> (BP) <b>learning</b> algorithm for horizontally partitioned databases. In this algorithm, data nodes can privately exchange information that the original BP algorithm needs. This algorithm can obtain the same result as learning on global data using the BP algorithm without considering privacy protection and each data nodes is prevented from obtaining detailed data on other nodes in the learning process...|$|R
40|$|In this paper, a {{hardware}} {{implementation of a}} recurrent neural fuzzy network (RNFN) used for identification and prediction is proposed. A recurrent network {{is embedded in the}} RNFN by adding feedback connections in the second layer, where the feedback units act as memory elements. Although the <b>back</b> <b>propagation</b> (BP) <b>learning</b> algorithm is widely used in the RNFN, BP is too complicated to be implemented using hardware. However, we use the simultaneous perturbation method as a learning scheme for hardware implementation to overcome the above-mentioned problems. The hardware implementation of the RNFN uses random access memory (RAM), which stores all the parameters of a network. This design method reduces the number of logic gates used. The major findings of the experiment show that field programmable gate arrays (FPGA) implementation of the RNFN retains good performance in identification and prediction problems...|$|R
