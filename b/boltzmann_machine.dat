695|520|Public
25|$|Inspired by {{the success}} of Boltzmann {{machines}} based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum <b>Boltzmann</b> <b>machine</b> can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum <b>Boltzmann</b> <b>machine</b> has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.|$|E
25|$|The D-Wave 2X system hosted at NASA Ames Research Center {{has been}} {{recently}} {{used for the}} learning of a special class of restricted Boltzmann machines that {{can serve as a}} building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected <b>Boltzmann</b> <b>machine</b> to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.|$|E
25|$|Quantum {{annealing}} is not {{the only}} technology for sampling. In a prepare and measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted <b>Boltzmann</b> <b>machine,</b> and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.|$|E
50|$|Restricted <b>Boltzmann</b> <b>machines</b> are {{a special}} case of <b>Boltzmann</b> <b>machines</b> and Markov random fields.Their {{graphical}} model corresponds to that of factor analysis.|$|R
40|$|The {{promise of}} quantum neural nets, which utilize quantum effects to model complex data sets, has made their {{development}} an aspirational goal for quantum machine learning and quantum computing in general. Here we provide {{new methods of}} training quantum <b>Boltzmann</b> <b>machines,</b> which are a class of recurrent quantum neural network. Our work generalizes existing methods and provides new approaches for training quantum neural networks that compare favorably to existing methods. We further demonstrate that quantum <b>Boltzmann</b> <b>machines</b> enable a form of quantum state tomography that not only estimates a state but provides a perscription for generating copies of the reconstructed state. Classical <b>Boltzmann</b> <b>machines</b> are incapable of this. Finally we compare small non-stoquastic quantum <b>Boltzmann</b> <b>machines</b> to traditional <b>Boltzmann</b> <b>machines</b> for generative tasks and observe evidence that quantum models outperform their classical counterparts...|$|R
40|$|We {{show that}} deep narrow <b>Boltzmann</b> <b>machines</b> are {{universal}} approximators of probability distributions {{on the activities}} of their visible units, provided they have sufficiently many hidden layers, each containing {{the same number of}} units as the visible layer. We show that, within certain parameter domains, deep <b>Boltzmann</b> <b>machines</b> can be studied as feedforward networks. We provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow <b>Boltzmann</b> <b>machines</b> are at least as compact universal approximators as narrow sigmoid belief networks and restricted <b>Boltzmann</b> <b>machines,</b> with respect to the currently available bounds for those models. Comment: Published as a conference paper at ICLR 201...|$|R
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted <b>Boltzmann</b> <b>machine,</b> then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.|$|E
2500|$|Hinton et al. (2006) {{employed}} {{learning the}} distribution of a high-level representation using successive layers of binary or real-valued latent variables with a restricted <b>Boltzmann</b> <b>machine</b> to model each layer. Once sufficiently many layers have been learned, the deep architecture {{may be used as}} a generative model by reproducing the data when sampling down the model (an [...] "ancestral pass") from the top level feature activations [...] In 2012, Ng and Dean created a neural network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.|$|E
5000|$|A <b>Boltzmann</b> <b>machine,</b> like a Hopfield network, is {{a network}} of units with an [...] "energy" [...] defined for the overall network. Its units produce binary results. Unlike Hopfield nets, <b>Boltzmann</b> <b>machine</b> units are stochastic. The global energy, , in a <b>Boltzmann</b> <b>machine</b> is {{identical}} in form {{to that of a}} Hopfield network: ...|$|E
40|$|<b>Boltzmann</b> <b>Machines</b> are a {{powerful}} class of undirected graphical models. Originally proposed as artificial neural networks, {{they can be}} regarded as a type of Markov Random Field in which the connection weights between nodes are symmetric and learned from data. They are also closely related to recent models such as Markov logic networks and Conditional Random Fields. A major challenge for <b>Boltzmann</b> <b>machines</b> (as well as other graphical models) is speeding up learning for large-scale problems. The heart of the problem lies in efficiently and effectively approximating the partition function. In this paper, we propose a new efficient learning algorithm for <b>Boltzmann</b> <b>machines</b> that allows them to be applied to problems with large numbers of random variables. We introduce a new large-margin variational approximation to the partition function that allows <b>Boltzmann</b> <b>machines</b> to be trained using a support vector machine (SVM) style learning algorithm. For discriminative learning tasks, these large margin <b>Boltzmann</b> <b>machines</b> provide an alternative approach to structural SVMs. We show that these machines have low sample complexity and derive a generalization bound. Our results demonstrate that on multilabel classification problems, large margin <b>Boltzmann</b> <b>machines</b> achieve orders of magnitude faster performance than structural SVMs and also outperform structural SVMs on problems with large numbers of labels. ...|$|R
40|$|We {{show that}} deep narrow <b>Boltzmann</b> <b>machines</b> are {{universal}} approximators of probability distri-butions {{on the activities}} of their visible units, provided they have sufficiently many hidden layers, each containing {{the same number of}} units as the visible layer. Besides from this existence state-ment, we provide upper and lower bounds on the sufficient number of layers and parameters. These bounds show that deep narrow <b>Boltzmann</b> <b>machines</b> are at least as compact universal approximators as restricted <b>Boltzmann</b> <b>machines</b> and narrow sigmoid belief networks, with respect to the currently available bounds for those models...|$|R
40|$|We {{present and}} study {{learning}} rules for specafic models of <b>Boltzmann</b> <b>machines</b> which edend classical sequential and reversible synchronous models. 1 Introduction. <b>Boltzmann</b> <b>machines</b> are stochastic neural networks which associate to each input a probability distribution over the output. The neuronal interpretation {{comes from the}} fact that this probability distributio...|$|R
5000|$|... #Caption: [...] A {{graphical}} representation of an example <b>Boltzmann</b> <b>machine.</b> Each undirected edge represents dependency. In this example there are 3 hidden units and 4 visible units. This {{is not a}} restricted <b>Boltzmann</b> <b>machine.</b>|$|E
5000|$|... #Caption: [...] A {{graphical}} {{representation of a}} <b>Boltzmann</b> <b>machine</b> with a few weights labeled. Each undirected edge represents dependency and is weighted with weight [...] In this example there are 3 hidden units (blue) and 4 visible units (white). This is not a restricted <b>Boltzmann</b> <b>machine.</b>|$|E
50|$|The <b>Boltzmann</b> <b>machine</b> can {{be thought}} of as a noisy Hopfield network. It is one of the first neural {{networks}} to demonstrate learning of latent variables (hidden units). <b>Boltzmann</b> <b>machine</b> learning was at first slow to simulate, but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts.|$|E
5000|$|As {{their name}} implies, RBMs are {{a variant of}} <b>Boltzmann</b> <b>machines,</b> with the {{restriction}} that their neurons must form a bipartite graph: a pair of nodes {{from each of the}} two groups of units (commonly referred to as the [...] "visible" [...] and [...] "hidden" [...] units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, [...] "unrestricted" [...] <b>Boltzmann</b> <b>machines</b> may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of <b>Boltzmann</b> <b>machines,</b> in particular the gradient-based contrastive divergence algorithm.|$|R
40|$|We {{propose a}} relaxation-based {{approximate}} inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted <b>Boltzmann</b> <b>machines.</b> We also use our underlying sampler {{to estimate the}} log-partition function of restricted <b>Boltzmann</b> <b>machines</b> and compare against other sampling-based methods. Comment: ICLR 2014 workshop track submissio...|$|R
40|$|We {{introduce}} {{a large family}} of <b>Boltzmann</b> <b>machines</b> that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these <b>Boltzmann</b> <b>machines</b> exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results {{on the problems of}} N-bit parity and the detection of hidden symmetries. 1 Introduction <b>Boltzmann</b> <b>machines</b> (Ackley, Hinton, & Sejnowski, 1985) have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, <b>Boltzmann</b> <b>machines</b> [...] - as originally conceived [...] -also have some serious drawbacks [...] ...|$|R
50|$|The <b>Boltzmann</b> <b>machine</b> is a Monte Carlo {{version of}} the Hopfield network.|$|E
5000|$|A deep <b>Boltzmann</b> <b>machine</b> has a {{sequence}} of layers of hidden units.There are only connections between adjacent hidden layers, {{as well as between}} visible units and hidden units in the first hidden layer. The energy function of the system adds layer interaction terms to the energy function of general restricted <b>Boltzmann</b> <b>machine</b> and is defined by ...|$|E
5000|$|A {{restricted}} <b>Boltzmann</b> <b>machine</b> is a bipartite generative model specified over an undirected graph.|$|E
40|$|<b>Boltzmann</b> <b>machines</b> are physics {{informed}} generative {{models with}} wide applications in machine learning. They can learn the probability distribution from an input dataset and generate new samples accordingly. Applying {{them back to}} physics, the <b>Boltzmann</b> <b>machines</b> are ideal recommender systems to accelerate Monte Carlo simulation of physical systems due to their flexibility and effectiveness. More intriguingly, we show that the generative sampling of the <b>Boltzmann</b> <b>Machines</b> can even discover unknown cluster Monte Carlo algorithms. The creative power comes from the latent representation of the <b>Boltzmann</b> <b>machines,</b> which learn to mediate complex interactions and identify clusters of the physical system. We demonstrate these findings with concrete examples of the classical Ising model with and without four spin plaquette interactions. Our results endorse a fresh research paradigm where intelligent machines are designed to create or inspire human discovery of innovative algorithms. Comment: 4 pages, 4 figures, and half page appendi...|$|R
40|$|In {{this brief}} note, {{we show that}} chaotic <b>Boltzmann</b> <b>machines</b> truly yield samples from the {{probabilistic}} distribution of the corresponding <b>Boltzmann</b> <b>machines</b> if they are composed of only two elements. This note is an English translation (with slight modifications) of the article originally written in Japanese [H. Suzuki, Seisan Kenkyu 66 (2014), 315 - 316]. Comment: 5 pages, 1 figur...|$|R
40|$|This paper {{presents}} a novel Robust Deep Appearance Models {{to learn the}} non-linear correlation between shape and texture of face images. In this approach, two crucial components of face images, i. e. shape and texture, are represented by Deep <b>Boltzmann</b> <b>Machines</b> and Robust Deep <b>Boltzmann</b> <b>Machines</b> (RDBM), respectively. The RDBM, an alternative form of Robust <b>Boltzmann</b> <b>Machines,</b> can separate corrupted/occluded pixels in the texture modeling to achieve better reconstruction results. The two models are connected by Restricted <b>Boltzmann</b> <b>Machines</b> at the top layer to jointly learn and capture the variations of both facial shapes and appearances. This paper also introduces new fitting algorithms with occlusion awareness through the mask obtained from the RDBM reconstruction. The proposed approach is evaluated in various applications by using challenging face datasets, i. e. Labeled Face Parts in the Wild (LFPW), Helen, EURECOM and AR databases, to demonstrate its robustness and capabilities. Comment: 6 pages, 8 figures, submitted to ICPR 201...|$|R
5000|$|A <b>Boltzmann</b> <b>machine</b> {{is a type}} of {{stochastic}} recurrent {{neural network}} (and Markov Random Field).|$|E
5000|$|<b>Boltzmann</b> <b>machine</b> - like a Hopfield net but uses {{annealed}} Gibbs sampling {{instead of}} gradient descent ...|$|E
5000|$|Restricted <b>Boltzmann</b> <b>machine,</b> a type {{of neural}} net that is trained with a conceptually similar algorithm.|$|E
50|$|Noisy ReLUs {{have been}} used with some success in {{restricted}} <b>Boltzmann</b> <b>machines</b> for computer vision tasks.|$|R
5000|$|E. Aarts and J. Krost (1997); Simulated Annealing and <b>Boltzmann</b> <b>Machines,</b> John Wiley And Sons Publishers.|$|R
40|$|<b>Boltzmann</b> <b>machines</b> {{offer an}} {{exciting}} approach to connectionist networks. Salient features of these networks are their distributed internal representations {{and their use}} of massive parallelism. This paper reviews some of the achievements in the research on <b>Boltzmann</b> <b>machines</b> and discusses in particular two different fields of application, viz. (1) solving combinatorial optimization problems and (ii) carrying out learning tasks. Some open problems are also touched upon...|$|R
50|$|Multimodal deep Boltzmann {{machines}} is {{successfully used}} in classification and missing data retrieval. The classification accuracy of multimodal deep <b>Boltzmann</b> <b>machine</b> outperforms support vector machines, latent Dirichlet allocation and deep belief network, when models are tested on data with both image-text modalities or with single modality. Multimodal deep <b>Boltzmann</b> <b>machine</b> is {{also able to}} predict the missing modality given the observed ones with reasonably good precision.|$|E
5000|$|Gaussian-Bernoulli RBMs [...] are {{a variant}} of {{restricted}} <b>Boltzmann</b> <b>machine</b> used for modeling real-valued vectors such as pixel intensities. It is usually used to model the image data. The energy {{of the system of}} the Gaussian-Bernoulli RBM is defined as where [...] are the model parameters. The joint distribution is defined the same as the one in restricted <b>Boltzmann</b> <b>machine.</b> The conditional distributions now become ...|$|E
5000|$|... #Caption: Diagram of a {{restricted}} <b>Boltzmann</b> <b>machine</b> with three visible units and four hidden units (no bias units).|$|E
50|$|One {{example of}} a {{practical}} application of Restricted <b>Boltzmann</b> <b>machines</b> is the performance improvement of speech recognition software.|$|R
40|$|We {{present a}} new {{learning}} algorithm for <b>Boltzmann</b> <b>Machines</b> that contain {{many layers of}} hidden variables. Data-dependent statistics are estimated using a variational approximation that tends {{to focus on a}} single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn <b>Boltzmann</b> <b>Machines</b> with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer "pre-training" phase that initializes the weights sensibly. The pre-training also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that Deep <b>Boltzmann</b> <b>Machines</b> learn very good generative models of hand-written digits and 3 -D objects. We also show that the features discovered by Deep <b>Boltzmann</b> <b>Machines</b> are a very effective way to initialize the hidden layers of feed-forward neural nets which are then discriminatively fine-tuned...|$|R
40|$|An {{overview}} of the basic results in complexity theory of discrete neural computations is presented. Especially, the computational power and efficiency of single neurons, neural circuits, symmetric neural networks (Hopfield model), and of <b>Boltzmann</b> <b>machines</b> is investigated and characterized. Corresponding intractability results are mentioned as well. The evidence is presented why discrete neural networks (inclusively <b>Boltzmann</b> <b>machines)</b> {{are not to be}} expected to solve intractable problems more efficiently than other conventional models of computing...|$|R
