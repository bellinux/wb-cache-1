0|2290|Public
40|$|The {{status and}} {{performance}} of the LHCb detector during the physics LHC physics run is described. The LHCb detector {{has a number of}} notable features including: 12 micron resolution in the transverse plane on 30 - 35 track primary vertices, pion and kaon separation from 1 to 100 GeV, and 1 MHz full readout of all sub-systems. The detector is being operating above its design luminosity and is comprised of a silicon vertex detector, silicon and straw-tube tracking systems, ring imaging Cherenkov particle identification systems, electromagnetic and hadronic calorimetry, and muon systems. Hardware and software <b>based</b> <b>trigger</b> <b>levels</b> are utilised to efficiently select leptonically and hadronically decay beauty and charm hadrons. The alignment, tracking and particle identification performance will be discussed...|$|R
40|$|Since the restart of the LHC in November 2009, ATLAS has {{collected}} inelastic pp-collisions to perform first measurements on charged particle densities. These measurements {{will help to}} constrain various models describing phenomenologically soft parton interactions. Understanding the trigger efficiencies for different event types are therefore crucial to minimize any possible bias in the event selection. ATLAS uses two main minimum bias triggers, featuring complementary detector components and <b>trigger</b> <b>levels.</b> While a hardware <b>based</b> first <b>trigger</b> <b>level</b> situated in the forward regions with 2. 09 < |eta| < 3. 8 has been proven to select pp-collisions very efficiently, the Inner Detector <b>based</b> minimum bias <b>trigger</b> uses a random seed on filled bunches and central tracking detectors for the event selection. Both triggers were essential {{for the analysis of}} kinematic spectra of charged particles. Their performance and trigger efficiency measurements as well as studies on possible bias sources will be presented...|$|R
3000|$|... the {{specific}} short term operating strategies including pump schedules (when pumps are turned {{on or off}} <b>based</b> on time), <b>trigger</b> <b>levels</b> (water levels in tanks or other storages that determine when pumps or valves turn on or off), irrigation or demand schedules (for systems {{where they can be}} pre-determined), valve settings and operating rules, and pressure settings for pumps (to maintain the set pressure at a particular point).|$|R
40|$|The ATLAS {{experiment}} is collecting proton proton collisions to perform early physics measurements such as charged particle multiplicities. These measurements will constrain various phenomenological {{models in the}} LHC energy regime. Understanding the trigger efficiencies for different event types are therefore crucial to minimize any possible bias in the event selection. The ATLAS experiment uses two types of minimum bias trigger, providing a complementary detector components and <b>trigger</b> <b>levels.</b> While a hardware <b>based</b> first <b>trigger</b> <b>level</b> situated in the rather forward regions with 2. 2 < |eta| < 3. 8 has been proved to select very efficiently proton proton collisions, the Inner Detector <b>based</b> minimum bias <b>trigger</b> uses at random seed on filled bunches and central tracking detectors for the event selection. Both triggers were essential {{for the analysis of}} particle kinematic spectra. Their performance and efficiency measurements as well as studies on possible bias sources will be presented. We also highlight the evolution of these triggers in association with more complicated analyzes...|$|R
40|$|An {{approach}} {{for the control}} of a pumping plant feeding a tank at the inlet of a water distribution system is presented. The approach is aimed at minimizing the energy costs by maximizing pumping during off-peak electricity tariff periods. It is <b>based</b> on <b>trigger</b> <b>levels</b> which are variable during the day according to a prefixed pattern {{in order to ensure}} that the water level in the elevated tank is at its minimum and maximum values at the end of the peak and off-peak tariff periods, respectively. The pattern of the <b>trigger</b> <b>levels</b> is defined by solving a multi-objective problem aimed at minimizing the energy costs and the number of pump switches. The approach was applied to a couple of real cases with a single tank. The approach was compared with other methodologies typically used for pump control, i. e. fixed <b>trigger</b> <b>levels</b> and pump scheduling. The results show for the two particular cases that the proposed approach achieves energy costs that are lower than those obtainable by using fixed <b>trigger</b> <b>levels,</b> and comparable with those obtainable by using pump scheduling. This is based on achieving a similar number of pump switches...|$|R
40|$|A {{methodology}} {{for the control}} of a pumping plant feeding a tank is presented. This methodology is aimed at minimizing the energy costs by maximizing pumping during off-peak electricity tariff periods. It is <b>based</b> on <b>trigger</b> <b>levels</b> which are variable during the day according to a prefixed pattern {{in order to ensure}} that the water level in the elevated tank is at its minimum and maximum values at the end of the peak and off-peak tariff periods respectively. The pattern of the <b>trigger</b> <b>levels</b> is defined by solving a multi-objective problem aimed at minimizing the energy costs and the number of pump switches. The methodology was applied to the real case of a pumping plant feeding an elevated tank for daily balance which, in turn, feeds a small town in northern Italy; one week of hourly observed total consumptions was considered. This methodology was compared with other two methodologies typically used for pump control, i. e. pump scheduling and fixed <b>trigger</b> <b>levels.</b> The results show that the proposed methodology allows for achieving energy costs that are definitively lower than those obtainable by using fixed <b>trigger</b> <b>levels,</b> and comparable with those obtainable by using pump scheduling, being the number of pump switches the same. On the other hand, unlike the pump scheduling, the methodology presented does not require any water demand forecast and scheduling optimization to be repeated daily, thus representing an effective and efficient tool for pumping plant operation...|$|R
40|$|The ATLAS {{experiment}} at the High Luminosity LHC {{will face}} a fivefold {{increase in the number}} of interactions per bunch crossing relative to the ongoing Run 2. This will require a proportional improvement in rejection power at the earliest levels of the detector trigger system, while preserving good signal efficiency. One critical aspect of this improvement will be the implementation of precise track reconstruction, through which sharper trigger turn-on curves can be achieved, and b-tagging and tau-tagging techniques can in principle be implemented. The challenge of such a project comes in the development of a fast, custom electronic device integrated in the hardware <b>based</b> first <b>trigger</b> <b>level</b> of the experiment. This article will discuss the requirements, architecture and projected performance of the system in terms of tracking, timing and physics, based on detailed simulations. Studies are carried out using data from the strip subsystem only or both strip and pixel subsystems...|$|R
40|$|Partial {{discharge}} (PD) {{detection and}} location in cable systems {{is a valuable}} tool for the estimation of {{the condition of the}} system. Accurate localization of the PD origin, based on arrival times, is required for the identification and assessment of the defect. This paper evaluates different time-of-arrival algorithms to determine which method yields most accurate location under different circumstances. These methods are <b>based</b> on <b>trigger</b> <b>level,</b> Akaike Information Criterion (AIC), energy criterion, Gabor's signal centroid and phase in frequency domain. Several criteria are defined by which the algorithms are evaluated. These criteria include the sensitivity to the noise level, the sensitivity to the pulse shape and others. The methods are tested on a medium-voltage cable system by injecting PD pulses in a cable with a joint at a known location. Each algorithm is applied to the measured pulses and the resulting location is compared with the known location. From the result the methods using the energy criterion and the phase are preferred...|$|R
40|$|Part of {{the early}} physics program at the ATLAS {{experiment}} is to measure basic properties of proton proton collisions, such as charged particle multiplicities, which will constrain phenomenological models in the LHC energy regime. An inclusive and well understood trigger is crucial to minimize any possible bias in the event selection. The ATLAS experiment uses two complimentary types of minimum bias <b>triggers.</b> A hardware <b>based</b> first <b>level</b> <b>trigger</b> sensitive to the forward regions of 2. 2 ~$<~|eta|~<~$ 3. 8 has been proved to efficiently select proton proton collisions, while an Inner Detector <b>based</b> <b>trigger</b> seeded on randomly sampled filled bunches which selects events with hits in the tracker, has provided a useful control sample. The performance and efficiency measurements of these triggers and detectors will be presented...|$|R
40|$|Since March 2010, the ATLAS {{experiment}} {{has been}} recording collisions of the Large Hadron Collider (LHC) at {{a center of}} mass energy of 7 TeV. At low instantaneous luminosity, data were selected by the hardware <b>based</b> Level- 1 <b>trigger</b> and processed by the software <b>based</b> High <b>Level</b> <b>Trigger</b> (HLT) without active rejection; as the luminosity increased, the HLT rejection has been gradually activated. Since then, electrons from J/$psi$, bottom, charm, W and Z decays, prompt photons and a first sample of tau hadronic decays from W have been efficiently selected. This paper gives {{an overview of the}} implementation of the electron, photon and tau trigger trigger selection algorithms and of the first experience running these triggers online. The performance of the three <b>trigger</b> <b>levels</b> is discussed and a set of comparisons of the online discriminating variables with offline reconstruction is shown, as well as the comparison of data with the Monte Carlo simulation on which the current selection was tuned...|$|R
40|$|Accurate {{location}} {{of the origins of}} partial discharges in power cable systems, based on arrival times, is imperative for the identification and assessment of defects. This paper evaluates different time-of-arrival algorithms in order to determine which method yields most accurate location under different circumstances. These methods are <b>based</b> on <b>trigger</b> <b>level,</b> Akaike Information Criterion, energy criterion, Gabor’s signal centroid and phase in frequency domain. Several criteria are defined by which the algorithms are evaluated. These criteria include the sensitivity to noise, pulse shape and effect of load impedance. The sensitivity of the methods upon varying these quantities is evaluated analytically and by means of simulations. Further, the methods are tested on a medium-voltage cable system by injecting PD pulses in a cable with one joint. Each algorithm is applied to the measured pulses and the resulting location is compared with the known joint location. From the results the energy criterion method and the phase method show the best performance. Index Terms — Delay estimation, defect location, partial discharges, power cables, signal analysis. ...|$|R
40|$|A new {{methodology}} for the optimization {{of the daily}} operations of a pumping plant has been developed. Two major issues have been investigated in relation to pump operations; maximizing pumping during off-peak electricity tariff periods and minimizing the head against which water is pumped. The new approach {{presented in this paper}} incorporates the combination of both a pump schedule and <b>trigger</b> <b>levels</b> to control the system. Different <b>trigger</b> <b>levels</b> are used during different periods of the day, such that peak pumping and pumping head are minimized. Additionally, scheduling is used to maximize pumping in the off-peak period thereby ensuring that the maximum amount of water is available in elevated storage facilities at the start of peak tariff periods. A genetic algorithm that incorporates realvalue and integer decision variables has been developed to determine the best combinations of <b>trigger</b> <b>levels</b> and scheduling for a predicted demand flow for the next day. The methodology has been applied to the Murray Bridge Water Treatment plant in South Australia. The current operating controls <b>based</b> on fixed <b>trigger</b> <b>levels</b> in the upper tank at this plant have been compared with the new combined scheduling/trigger levels operating controls approach. Very promising results have been obtained, with a 20 % saving predicted on energy costs during low and moderate demands. Michael D. Kazantzis, Angus R. Simpson, David Kwong, and Shyh Min Ta...|$|R
40|$|The {{design of}} minimum bias {{triggers}} should {{allow for a}} highly efficient selection on pp-collisions, while minimising any possible bias in the event selection. In ATLAS two main minimum bias triggers have been developed using complementary technologies. A hardware <b>based</b> first <b>level</b> <b>trigger,</b> consisting of 32 plastic scintillators, has proven to efficienctly select pp-interactions. In particular during the start-up phase this trigger {{played a crucial role}} for the commissioning of the central trigger processor and detector sub-systems. A complementary selection is achieved by a multi-level minimum bias trigger, seeded off a random trigger on filled bunches. For the event selection at higher <b>trigger</b> <b>levels</b> a dedicated algorithm was developed, able to cope with around 86 millions of detector signals per bunch-crossing. We will present these trigger systems and their deployment online, highlighting their performance and trigger efficiencies. We outline as well the operation with increasing beam intensities and luminosities...|$|R
40|$|First {{collisions}} at {{a center}} of mass energy of 7 TeV at the Large Hadron Collider (LHC) were recorded by the ATLAS {{at the beginning of}} 2010. Data selected by the hardware based Level- 1 (L 1) trigger was processed online by the software <b>based</b> High <b>Level</b> <b>Trigger</b> (HLT) running without active rejection. This paper gives an overview of the calorimeter based performance of the electron and photon triggers during the initial 2010 running period; these trigger will {{play a key role in}} the physics analysis in ATLAS. Examples of comparisons of the three <b>trigger</b> <b>levels</b> with the offline reconstruction will be presented, such at the L 1 efficiency and the distributions of the calorimeter based selection variables used to distinguish between signal and background. This is an important step in the commissioning of the trigger system to ensure its correct functioning. Results from this first data are very encouraging. At the end a brief outlook on the commissioning steps and on the evolution of the e/$gamma$ trigger strategy with increasing luminosities will also be given...|$|R
40|$|Cosmics {{data are}} {{providing}} a valuable handle to optimize and commission the ATLAS detector before beam collissions. In this process the ATLAS Tau Trigger is also exercising and adjusting its different components, namely the hardware <b>based</b> first <b>level</b> <b>trigger,</b> {{and the second}} and third levels, implemented with software. In this contribution we summarize the performance at the different stages with cosmics events, and compare with Monte Carlo simulation and offline reconstructed muon candidates. We also describe the prospects for initial running with beam collisions, focusing on the commission of {{the second and third}} <b>level</b> tau <b>triggers</b> and the strategy to measure the first trigger efficiencies with data...|$|R
40|$|Events with muons in {{the final}} state are an impor- tant {{signature}} for many physics analyses in the harsh detector environment produced by collisions of high energy protons. The ATLAS experiment employs a multi-level trigger architecture that selects events in three sequential steps of increasing com- plexity and accuracy. The <b>Level</b> 1 <b>trigger</b> is implemented with custom built hardware to reduce the event rate from 40 MHz to 75 kHz. The software <b>based</b> higher <b>level</b> <b>triggers</b> refine the trigger decisions reducing the output rate down {{to the order of}} 100 Hz. This note presents the performance of the muon trigger evaluated with proton-proton collision data collected in 2011 at a centre-of-mass energy of 7 TeV...|$|R
40|$|A {{large scale}} slope {{instability}} developed at an operating mine over two years, {{resulting in a}} 4. 5 million tonne collapse in July 2004. During this period the Geotechnical personnel monitored and inspected the slope {{to ensure that the}} safety of personnel and equipment was not compromised. Monitoring of the slopes was done using visual inspections, conventional survey methods {{and the use of the}} Slope Stability Radar. The details of the observations and the monitoring results are described in this project, as well as the methods used to try to predict the onset of failure. The Slope Strain method of predicting failure is evaluated. An important part of the management of a failure is the control measures that are put in place. The control measures, and how they are escalated in reaction to an increasing risk, are discussed. Certain <b>trigger</b> <b>levels</b> were put in place. Due to location of mining at time of collapse the evacuation of personnel <b>based</b> on the <b>trigger</b> <b>levels</b> was not required. The effectiveness of the different <b>trigger</b> <b>levels</b> is evaluated. All slope deformation and slope failures behave differently. When no site specific historical data is available, the geotechnical practitioner relies on available literature to formulate guidelines and threshold levels for the monitoring, prediction of failure, and safe management of unstable slopes. The detailed case study described in this report is considered to be a valuable contribution to the literature in these fields. The data contained in the report have been presented in detail since they may be of value to other researchers and practitioners in the rock slope fiel...|$|R
40|$|Today's {{computing}} {{elements for}} software <b>based</b> high <b>level</b> <b>trigger</b> processing (HLT) {{are based on}} nodes with multiple cores. Using process based parallelization to filter particle collisions from the LHCb experiment on such nodes leads to expensive consumption of memory and hence significant cost increase. In the following an approach is presented to both minimize the resource consumption of the filter applications and to reduce the startup time. Described is the duplication of threads and the handling of files open in read-write mode when duplicating filter processes and the possibility to bootstrap the event filter applications directly from preconfigured checkpoint files. This led to a reduced memory consumption of roughly 60 % in the nodes of the LHCb HLT farm and an improved startup time of a factor 10...|$|R
40|$|Consistency of a {{database}} is {{as an important}} property that must be preserved at all times. In most OODB systems today, application code can directly access and alter both the data {{as well as the}} structure of the database. As a consequence application code can potentially violate the integrity of the database, in terms of the invariants of the data model, the user-specified application constraints, and even the referential integrity of the objects themselves. A common form of consistency management in most databases today is to encode constraints at the system level (e. g., foreign keys), or at the <b>trigger</b> <b>based</b> <b>level</b> (e. g., user constraints) and to perform transaction rollback on discovery of any violation of these constraints. However, for programs that alter the structure as well as the objects in {{a database}}, such as an extensible schema evolution program, roll-backs are expensive and add to the already astronomical cost of doing schema evolution. In this paper, pre-execution [...] ...|$|R
40|$|The ATLAS {{experiment}} at the LHC {{will face}} the challenge of selecting interesting candidate events in pp collisions at 14 TeV center of mass energy, while rejecting the enormous number of background events. The trigger system architecture is organized in three levels. From an interaction rate of 1 GHz the First <b>Level</b> <b>trigger,</b> hardware implemented, will reduce this rate to around ~ 100 kHz. Then the software <b>based</b> High <b>Level</b> <b>Trigger</b> (HLT), composed by the Second Level and the Event Filter reduces the rate to ~ 200 Hz. HLT is implemented on commercial CPUs using a framework built on the common ATLAS object oriented software architecture. Inclusive trigger selections are used to collect events for the ATLAS physics programme; final states with muons are crucial for Electroweak precision measurements as well as Higgs and SUSY searches. In this paper we will present the implementation of the muon slice, signal efficiencies, background rejection rates and system performances (execution time, [...] .) for online muon selection based on MonteCarlo simulations and results obtained on real events collected during cosmic data taking runs...|$|R
40|$|Abstract—After 2001 the {{upgraded}} ep collider HERA {{will provide}} an about five times higher luminosity for the two experiments H 1 and ZEUS. In order {{to cope with the}} expected higher event rates the H 1 collaboration is building a track <b>based</b> <b>trigger</b> system, the Fast Track Trigger (FTT). It will be integrated in the first three levels (L 1 –L 3) of the H 1 trigger scheme to provide higher selectivity for events with charged particles. The FTT will allow to reconstruct 3 -dimensional tracks in the central drift chamber down to 100 MeV/c within the L 2 latency of ∼ 23 µs. To reach the necessary momentum resolution of ∼ 5 % (at 1 GeV/c) sophisticated reconstruction algorithms have to be implemented using high density Field Programmable Gate Arrays (FPGA) and their embedded Content Addressable Memories (CAM). The final track parameter optimization will be done using non-iterative fits implemented in DSPs. While at the first <b>trigger</b> <b>level</b> rough track information will be provided, at L 2 tracks with high resolution are available to form trigger decisions on topological and other track based criteria like multiplicities and momenta. At the third <b>trigger</b> <b>level</b> a farm of commercial processor boards will be used to compute physics quantities such as invariant masses...|$|R
40|$|After 2001 the {{upgraded}} ep collider HERA {{will provide}} an about five times higher luminosity for the two experiments H 1 and ZEUS. In order {{to cope with the}} expected higher event rates the H 1 collaboration is building a track <b>based</b> <b>trigger</b> system, the Fast Track Trigger (FTT). It will be integrated in the first three levels (L 1 -L 3) of the H 1 trigger scheme to provide higher selectivity for events with charged particles. The FTT will allow to reconstruct 3 -dimensional tracks in the central drift chamber down to 100 MeV/c within the L 2 latency of ~ 23 mus. To reach the necessary momentum resolution of ~ 5 % (at 1 GeV/c) sophisticated reconstruction algorithms have to be implemented using high density Field Programmable Gate Arrays (FPGA) and their embedded Content Addressable Memories (CAM). The final track parameter optimization will be done using non-iterative fits implemented in DSPs. While at the first <b>trigger</b> <b>level</b> rough track information will be provided, at L 2 tracks with high resolution are available to form trigger decisions on topological and other track based criteria like multiplicities and momenta. At the third <b>trigger</b> <b>level</b> a farm of commercial processor boards will be used to compute physics quantities such as invariant masses. Comment: 6 pages, 7 figures, submitted to TN...|$|R
40|$|This work {{extends the}} single product, single {{facility}} and single warehouse Make-to-Stock (MTS) model (Zhao and Melamed 2004), by allowing subcontracting. In {{addition to the}} inventory <b>triggering</b> <b>level</b> employed in their model, a new subcontracting <b>triggering</b> <b>level</b> is used to manage the subcontracted amount. A stochastic fluid model (SFM) is used to model the MTS system with subcontracting. In the current model, the subcontractor is assumed capable of satisfying any difference between the demand and the inhouse production to keep the inventory at the subcontracting <b>triggering</b> <b>level.</b> The sensitivity of the inventory onhand, backorders and amounts subcontracted {{with respect to the}} two employed <b>triggering</b> <b>levels</b> are found using infinitesimal perturbation analysis (IPA). ...|$|R
50|$|A {{brown-out}} circuit {{monitors the}} VCC level during operation by {{comparing it to}} a fixed <b>trigger</b> <b>level.</b> When VCC drops below the <b>trigger</b> <b>level,</b> the brown-out reset is immediately activated. When VCC rises again, the MCU is restarted after a certain delay.|$|R
40|$|New trigger inputs for the ALICE Central Trigger Processor (CTP) are proposed. They {{are based}} on the use of Fast Multiplicity (FM) output signals {{generated}} by the ALICE Silicon Pixel Detector (SPD). These can be used for a multiplicity <b>based</b> centrality <b>trigger</b> and for a fast on-line computation of the primary vertex. A simple algorithm for primary vertex location at the <b>trigger</b> <b>level</b> is proposed. The precision that can be achieved with this method on centrality selection and primary vertex location, is discussed for interactions with different pseudo-rapidity density level. The feasibility of background rejection is also considered...|$|R
40|$|The term {{radiological}} <b>trigger</b> <b>level</b> (RTL) {{was used}} in previous technical memoranda, documents, and presentations associated with the Santa Susana Field Laboratory (SSFL) Area IV Radiological Study. Radiological <b>trigger</b> <b>levels</b> were used as a decision level (DL) value against which laboratory results may be compared {{to determine whether a}} predetermined actio...|$|R
40|$|Since {{the start}} of the LHC physics {{programme}} earlier this year, the ATLAS detector has been collecting proton-proton collisions at a 7 TeV center of mass Energy. As the LHC luminosity rises the ATLAS trigger system must become increasingly selective to reduce the event rate from a design bunch crossing rate of 40 MHz to about 200 Hz for recording. To achieve this the trigger algorithms must meet challenging requirements in terms of speed and selectivity. The <b>trigger</b> is hardware <b>based</b> at level- 1 and uses software algorithms running on a farm of commercial processors at the two higher <b>trigger</b> <b>levels.</b> The calorimeter-based software algorithms have been designed with a common part optimized for fast access to detector data and subsequent stages tailored for maximum selectivity for specific signatures such as electrons, photons, jets, taus and missing total energy. We present the physics performance achieved during 2010 data taking, highlighting the key performance aspects for the different signatures. Event features reconstructed by the Trigger are compared with offline reconstruction and with expectations from Monte Carlo simulations. Emphasis is given to distributions of calorimeter variables for energy determination and particle identification and the measured selection efficiency at the different <b>trigger</b> <b>levels.</b> Rate stability, processing time and data access performance during different periods of data-taking are also presente d. The results presented demonstrate that the calorimeter <b>based</b> <b>trigger</b> is functioning correctly and is effective in selecting data for the ATLAS physics programme...|$|R
3000|$|... the {{specific}} long term operating strategies including volumetric allocation {{of water from}} different alternative sources, <b>trigger</b> <b>levels</b> (for example in reservoirs) that determine allocations from different sources or water demand restriction levels, switch times between different operating regimes (for example between different <b>trigger</b> <b>level</b> sets for different seasons) and power source selection.|$|R
50|$|Data can be {{compared}} against Environmental Standards or site-specific <b>trigger</b> <b>levels.</b>|$|R
40|$|AbstractIn {{preparation}} for the high-luminosity phase of the Large Hadron Collider, ATLAS is planning a trigger upgrade that will enable the experiment to use tracking information already at the first <b>trigger</b> <b>level.</b> This will provide enhanced background rejection power at <b>trigger</b> <b>level</b> while preserving much needed flexibility for the trigger system. The status and current plans for the new ATLAS Level- 1 tracking trigger are presented...|$|R
30|$|Appendix {{shows that}} some EU MS use {{reporting}} criteria based on DRLs. DRL <b>trigger</b> <b>levels</b> are exposure parameters {{of a specific}} procedure exceeding a DRL by a multiplication factor (typically factor 2 – 5). In Appendix c, Germany uses <b>trigger</b> <b>levels</b> for the collective approach. If a single procedure exceeds the DRL by 200 %, the professional performing the study has to check whether the last 20 consecutive procedures exceeded the DRL by more than 100 %. Beyond their use for reporting of significant events, <b>trigger</b> <b>levels</b> {{should be used to}} establish local notification or alert values [13]. The DRL multiplication factors could be used also to derive absolute dose values tables.|$|R
50|$|<b>Bases</b> <b>trigger</b> the {{formation}} of a dimer 2,2,4,4-tetrakis-(trifluoromethyl)-1,3-dithietane. This includes amines.|$|R
40|$|International audienceElectron and photon {{triggers}} covering transverse energies from 5 GeV {{to several}} TeV {{are essential for}} signal selection {{in a wide variety}} of ATLAS physics analyses to study Standard Model processes and to search for new phenomena. Final states including leptons and photons had, for example, an important role in the discovery and measurement of the Higgs particle. Dedicated triggers are also used to collect data for calibration, efficiency and fake rate measurements. The ATLAS trigger system is divided in a hardware-based (Level 1) and a software <b>based</b> high <b>level</b> <b>trigger,</b> both of which were upgraded during the long shutdown of the LHC in preparation for data taking in 2015. The increasing luminosity and more challenging pile-up conditions as well as the planned higher center-of-mass energy demanded the optimisation of the trigger selections at each level to control the rates and keep efficiencies high. The evolution of the ATLAS electron and photon triggers and their performance will be presented, including initial results from the early days of the LHC Run 2 operation...|$|R
50|$|Interrupts may {{be either}} edge <b>triggered</b> or <b>level</b> <b>triggered.</b>|$|R
40|$|This article {{describes}} the Silicon Track Trigger (STT) which has been fully commissioned in 2004 at the DØ experiment. The STT allows to enrich already at the second <b>trigger</b> <b>level</b> the data sample with events containing B-mesons. The STT achieves this by providing within about 50 µs tracks with an impact parameter resolution of around 50 µm. The article shows preliminary results of the trigger performance and presents a fast b-identification algorithm for the second <b>trigger</b> <b>level.</b> 1...|$|R
30|$|The data {{communication}} paradigm can be event <b>based</b> (<b>triggered</b> by sensor nodes) or on demand (triggered by gateways).|$|R
