1098|632|Public
40|$|A novel semi-naive <b>Bayesian</b> <b>classifier</b> is {{introduced}} {{that is particularly}} suitable to data with many attributes. The naive <b>Bayesian</b> <b>classifier</b> is taken {{as a starting point}} and correlations are reduced through joining of highly correlated attributes. Our technique differs from related work in its use of kernel-functions that systematically include continuous attributes rather than relying on discretization as a preprocessing step. This retains distance information within the attribute domains and ensures that attributes are joined based on their correlation for the particular values of the test sample. We implement a kernel-based semi-naive <b>Bayesian</b> <b>classifier</b> using P-Trees and demonstrate that it generally outperforms the naive <b>Bayesian</b> <b>classifier</b> as well as a discrete semi-nave <b>Bayesian</b> <b>classifier...</b>|$|E
40|$|Abstract:- Segmentation of {{the various}} {{elements}} among the particles {{is very important to}} medical decision. Despite the significant advancement in this field due to various proposed approaches, medical image segmentation is still an unsolved problem. <b>Bayesian</b> <b>classifier</b> is an efficient method used to improve the medical image segmentation. In order to eliminate the background noises of images, we need preprocessing of images. After the preprocessing method, <b>Bayesian</b> <b>classifier</b> is used for classifying of particles in the image. <b>Bayesian</b> <b>classifier</b> is a powerful probabilistic graphical model that has been applied in computer vision. In this paper, we adapted some of the existing segmentation algorithms using <b>Bayesian</b> <b>classifier</b> and focused the effect of <b>Bayesian</b> <b>classifier</b> in segmentation algorithms...|$|E
40|$|The naive <b>Bayesian</b> <b>classifier</b> {{provides}} {{a very simple}} yet surprisingly accurate technique for machine learning. Some researchers have examined extensions to the naive <b>Bayesian</b> <b>classifier</b> that seek to further improve the accuracy. For example, a naive Bayesian tree approach generates a decision tree with one naive <b>Bayesian</b> <b>classifier</b> at each leaf. Another example is a constructive <b>Bayesian</b> <b>classifier</b> that eliminates attributes and constructs new attributes using Cartesian products of existing attributes. This paper proposes a simple, but effective approach for the same purpose. It generates a naive <b>Bayesian</b> <b>classifier</b> committee for a given classification task. Each {{member of the committee}} is a naive <b>Bayesian</b> <b>classifier</b> based on a subset of all the attributes available for the task. During the classification stage, the committee members vote to predict classes. Experiments across a wide variety of natural domains show that this method significantly increases the prediction accuracy of t [...] ...|$|E
40|$|Abstract—In this work, {{we study}} the {{application}} of <b>Bayesian</b> networks <b>classifiers</b> for gene expression data in three ways: first, we made an exhaustive state-of-art of <b>Bayesian</b> <b>classifiers</b> and <b>Bayesian</b> <b>classifiers</b> induced from microarray data. Second, we propose a preprocessing scheme for gene expression data, to induce <b>Bayesian</b> <b>classifiers.</b> Third, we evaluate different <b>Bayesian</b> <b>classifiers</b> {{for this kind of}} data, including the C-RPDAG classifier presented by the authors. Keywords-Machine learning, supervised classification, Bayesian network, gene expression data, DNA microarray...|$|R
40|$|In this study, both <b>Bayesian</b> <b>classifiers</b> {{and mutual}} {{information}} classifiers are examined for binary classifications {{with or without}} a reject option. The general decision rules in terms of distinctions on error types and reject types are derived for <b>Bayesian</b> <b>classifiers.</b> A formal analysis is conducted to reveal the parameter redundancy of cost terms when abstaining classifications are enforced. The redundancy implies an intrinsic problem of "non-consistency" for interpreting cost terms. If no data is given to the cost terms, we demonstrate the weakness of <b>Bayesian</b> <b>classifiers</b> in class-imbalanced classifications. On the contrary, mutual-information classifiers are able to provide an objective solution from the given data, which shows a reasonable balance among error types and reject types. Numerical examples of using two types of classifiers are given for confirming the theoretical differences, including the extremely-class-imbalanced cases. Finally, we briefly summarize the <b>Bayesian</b> <b>classifiers</b> and mutual-information classifiers in terms of their application advantages, respectively. Comment: (2 nd version: 19 pages, 5 figures, 7 tables. Theorems on <b>Bayesian</b> <b>classifiers</b> are extended to multiple variables. Appendix B for "Tighter bounds between the conditional entropy and Bayesian error in binary classifications" are added, in which Fano's bound is shown numerically to be very tight...|$|R
40|$|A popular {{method for}} {{creating}} an accurate classifier from {{a set of}} training data is to build several classifiers, and then to combine their predictions. The ensembles of simple <b>Bayesian</b> <b>classifiers</b> have traditionally not been a focus of research. One way to generate an ensemble of accurate and diverse simple <b>Bayesian</b> <b>classifiers</b> is to use different feature subsets generated with the random subspace method. In this case, the ensemble consists of multiple classifiers constructed by randomly selecting feature subsets, that is, classifiers constructed in randomly chosen subspaces. In this paper, we present an algorithm for building ensembles of simple <b>Bayesian</b> <b>classifiers</b> in random subspaces [...] ...|$|R
40|$|The Naïve <b>Bayesian</b> <b>Classifier</b> and an Augmented Naïve <b>Bayesian</b> <b>Classifier</b> {{are applied}} to human {{classification}} tasks. The Naïve <b>Bayesian</b> <b>Classifier</b> is augmented with feature construction using a Galois lattice. The best features, measured on their within- and between-category overlap, {{are added to the}} category’s concept description. The results show that space efficient concept descriptions can predict much {{of the variance in the}} classification phenomena...|$|E
40|$|Naive <b>Bayesian</b> <b>classifier</b> {{is one of}} the {{simplest}} yet surprisingly powerful technique to construct predictive model from classified data. Despite its naivety - assumption of attribute independence given the class - empirical results show that it performs surprisingly well in many domains containing clear attribute dependencies. In this work we theoretically, practically and graphically compare naive <b>Bayesian</b> <b>classifier</b> to logistic regression, a standard predictive modelling method from statistics. On the contrary to naive <b>Bayesian</b> <b>classifier</b> logistic regression makes no assumptions when constructing predictive model. We show that naive <b>Bayesian</b> <b>classifier</b> can be presented in an alternative mathematical form (log odds), which is comparable to logistic regression. We prove that methods are mathematically equivalent when attributes are conditionally independent. This logically implies of this that {{the differences between the two}} methods are a result of dependencies among attributes in the data. For visual presentation of naive <b>Bayesian</b> <b>classifier,</b> we develop a normalized naive Bayesian nomogram that is based on logistic regression nomogram. Additionally, we improve naive Bayesian nomogram so that it can depict both negative and positive influences of the values of attributes, and in contrast to logistic regression nomograms does not align the "base" values to the zero point. Another advantage over a nomogram for logistic regression is ability to handle unknown attribute values. We compare the two methods through visualization and study of predictive accuracy. Overall, experiments show very similar results, where logistic regression performs slightly better when learning on large data sets, and naive Bayesian classifiers turns to be better at smaller data sets. We summarize that naive <b>Bayesian</b> <b>classifier</b> performs similarly to logistic regression in most cases. Logistic regressions seems to be preferred to naive <b>Bayesian</b> <b>classifier</b> when learning on large data sets, leaving aside computational issues and matters such as handling missing data. Naive <b>Bayesian</b> <b>classifier</b> proves out to be successful at less deterministic learning problems. Also, when model understanding and graphical presentation of the model is important, naive <b>Bayesian</b> <b>classifier</b> is the better method...|$|E
40|$|This paper {{presents}} a statistical approach for font attribute recognition based on features extracted from projection profiles of text lines {{and using a}} <b>Bayesian</b> <b>classifier.</b> The presented features allow the discrimination of the font weight, slope and size. KEY WORDS Font recognition Projection profiles Discrimination power <b>Bayesian</b> <b>classifier...</b>|$|E
40|$|Naive Bayes is {{a simple}} <b>Bayesian</b> network <b>classifier</b> with strong {{independence}} assumptions among features. This classifier despite its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of naive Bayes may improve {{the performance of the}} resulting structure. Augmented <b>Bayesian</b> <b>Classifiers</b> relax the independence assumptions of naive Bayes by adding augmenting arcs that obey certain structural restrictions, between features of a naive Bayes classifier. In this paper we present algorithms for learning Augmented <b>Bayesian</b> <b>Classifiers</b> with respect to the Minimum Description Length (MDL) and Bayesian-Dirichlet (BD) metrics. Experimental results on the performance of these algorithms on various datasets selected from the UCI Machine Learning Repository are presented. Finally, a comparison of the learning rates and accuracies of various Augmented <b>Bayesian</b> <b>Classifiers</b> is presented on artificial data...|$|R
40|$|We {{investigate}} {{the use of}} Naive <b>Bayesian</b> <b>classifiers</b> for cor-related Gaussian feature spaces and derive error estimates for these classifiers. The error analysis is done by developing an exact expression for the error performance of a binary clas-sifier with Gaussian features while using any quadratic deci-sion boundary. Therefore, the analysis is not restricted to Naive <b>Bayesian</b> <b>classifiers</b> alone and can, for instance, be used to cal-culate the Bayes error performance. We compare the analyt-ical error rate to that obtained when Monte-Carlo simulations are performed for a 2 and 12 dimensional binary classification problem. Finally, we illustrate the robust performances ob-tained with Naive <b>Bayesian</b> <b>classifiers</b> (as apposed to a maxi-mum likelihood classifier) for high dimensional problems when data sparsity becomes an issue. 1...|$|R
30|$|Naive <b>Bayesian</b> <b>classifiers</b> [14] use Baye's theorem to {{classify}} the new instances of a data sample X. Each instance {{is a set}} of attribute values described by a vector, X[*]=[*](x 1, x 2,…, xn). Considering m classes, the sample X is assigned to the class Ci if and only if P(X | Ci) P(Ci)[*]>[*]P(X | Cj) P(Cj) for all i and j in (1, m) such that j < > i. The sample belongs to the class with maximum posterior probability for the sample. For categorical data, P(Xk | Ci) is calculated as the ratio of frequency of value Xk for attribute Ak and the total number of samples in the training set. For continuous valued attributes, Gaussian distribution can be assumed without loss of generality. In naive Bayesian approach, the attributes are assumed to be conditionally independent. In spite of this assumption, naive <b>Bayesian</b> <b>classifiers</b> give satisfactory results because focus is on identifying the classes for the instances, not the exact probabilities. Applications like spam mail classification and text classification can use naïve <b>Bayesian</b> <b>classifiers.</b> Theoretically, <b>Bayesian</b> <b>classifiers</b> are least prone to errors. The limitation is the requirement of the prior probabilities. The amount of probability information required is exponential in terms of number of attributes, number of classes, and the maximum cardinality of attributes. With increase in number of classes or attributes, the space and computational complexity of <b>Bayesian</b> <b>classifiers</b> increase exponentially.|$|R
30|$|A <b>Bayesian</b> <b>classifier</b> {{was used}} for {{classifying}} dynamic features.|$|E
40|$|AbstractThe {{problem of}} target {{classification}} is {{addressed in the}} Bayesian framework as {{an interpretation of the}} likelihood of Bayes’ theorem as a possibility. A better explanation and definition based on this perspective, as opposed to the conventional probability interpretation, is given for the uncertain mapping from the class space to the feature space. In this manuscript, we propose a new <b>Bayesian</b> <b>classifier</b> that can naturally combine both the probability and the possibility through a reinterpretation of Bayes’ theorem. An example of target classification using kinematic features demonstrates that the proposed <b>Bayesian</b> <b>classifier</b> outperforms the conventional <b>Bayesian</b> <b>classifier</b> and gives accurate classification results...|$|E
40|$|We {{predict the}} operon {{structure}} of the Bacillus subtilis genome using the average operon length, the distance between genes in base pairs, and the similarity in gene expression measured in time course and gene disruptant experiments. By expressing the operon prediction for each method as a Bayesian probability, {{we are able to}} combine the four prediction methods into a <b>Bayesian</b> <b>classifier</b> in a statistically rigorous manner. The discriminant value for the <b>Bayesian</b> <b>classifier</b> can be chosen by considering the associated cost of misclassifying an operon or a nonoperon gene pair. For equal costs, an overall accuracy of 88. 7 % was found in a leaveone-out analysis for the joint <b>Bayesian</b> <b>classifier,</b> whereas the individual information sources yielded accuracies of 58. 1 %, 83. 1 %, 77. 3 %, and 71. 8 % respectively. The predicted operon structure based on the joint <b>Bayesian</b> <b>classifier</b> is available from the DBTBS databas...|$|E
40|$|Abstract. In this paper, {{we review}} the {{induction}} of simple <b>Bayesian</b> <b>classifiers,</b> note {{some of their}} drawbacks, and describe a recursive algorithm that constructs a hierarchy of probabilistic concept descriptions. We posit that this approach should outperform the simpler scheme in domains that involve disjunctive concepts, since they violate the independence assumption on which the latter relies. To test this hypothesis, we report experimental studies with both natural and artificial domains. The results are mixed, but they are encouraging enough to recommend closer examination of recursive <b>Bayesian</b> <b>classifiers</b> in future work. 1...|$|R
40|$|In this paper, we will {{evaluate}} {{the power and}} usefulness of <b>Bayesian</b> network <b>classifiers</b> for credit scoring. Various types of <b>Bayesian</b> network <b>classifiers</b> will be evaluated and contrasted including unrestricted <b>Bayesian</b> network <b>classifiers</b> learnt using Markov Chain Monte Carlo (MCMC) search. The experiments {{will be carried out}} on three real life credit scoring data sets. It will be shown that MCMC <b>Bayesian</b> network <b>classifiers</b> have a very good performance and by using the Markov Blanket concept, a natural form of input selection is obtained, which results in parsimonious and powerful models for financial credit scoring...|$|R
40|$|For {{diseases}} {{of which the}} clinical diagnosis is uncertain, naive <b>Bayesian</b> <b>classifiers</b> can be of assistance to the veterinary practitioner. These simple probabilistic models {{have proven to be}} very powerful for solving classification problems in a variety of domains, but are not yet widely applied within the veterinary domain. In this paper, naive <b>Bayesian</b> <b>classifiers</b> and methods for their construction are reviewed. We demonstrate how to construct full and selective classifiers from a data set and how to build such classifiers from information in the literature. As a case study, naive <b>Bayesian</b> <b>classifiers</b> to discriminate between classical swine fever (CSF) -infected and non-infected pig herds were constructed from data collected during the 1997 / 1998 CSF epidemic in the Netherlands. The resulting classifiers were studied in terms of their accuracy and compared with the optimally efficient diagnostic rule that was reported earlier by Elbers et al. (2002). The classifiers were found to have accuracies within the range of 67 – 70 % and performed comparable to or even better than the diagnostic rule on the available data. In contrast with the diagnostic rule, the classifiers had the advantage of taking both the presence and the absence of particular clinical signs into account, which resulted in more discriminative power. These results indicate that naive <b>Bayesian</b> <b>classifiers</b> are promising tools for solving diagnostic problems in the veterinary field...|$|R
40|$|Naive Bayesian {{classifiers}} {{which make}} independence assumptions perform remarkably well on some data sets but poorly on others. We explore {{ways to improve}} the <b>Bayesian</b> <b>classifier</b> by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive <b>Bayesian</b> <b>classifier.</b> The domains on which the most improvement occurs are those domains on which the naive <b>Bayesian</b> <b>classifier</b> is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 1. 1 Introduction The <b>Bayesian</b> <b>classifier</b> (Duda & Hart, 1973) is a probabilistic method for classification. It can be used to determine the pro [...] ...|$|E
40|$|The <b>Bayesian</b> <b>classifier</b> is {{a simple}} {{approach}} to classification that produces results that are easy for people to interpret. In many cases, the <b>Bayesian</b> <b>classifier</b> {{is at least as}} accurate as much more sophisticated learning algorithms that produce results that are more difficult for people to interpret. To use numeric attributes with <b>Bayesian</b> <b>classifier</b> often requires the attribute values to be discretized into a number of intervals. We show that the discretization of numeric attributes is critical to successful application of the <b>Bayesian</b> <b>classifier</b> and propose a new method based on iterative improvement search. We compare this method to previous approaches and show that it results in significant reductions in misclassification error and costs on an industrial problem of troubleshooting the local loop in a telephone network. The approach can take prior knowledge into account by improving upon a user-provided set of boundary points, or can operate autonomously...|$|E
40|$|Abstract—A {{technique}} for accepting or rejecting {{the output of}} a <b>Bayesian</b> <b>classifier</b> is presented based on a metalevel <b>Bayesian</b> <b>classifier</b> using features of the probability distribution over the known classes calculated by the lower-level (object-level) classifier. The value of adding the metalevel classifier is determined by user utilities for the possible outcomes of the twolevel process. Keywords—classifier, novelty detection, metaclassification I...|$|E
40|$|Naive <b>Bayesian</b> <b>Classifiers,</b> which rely on {{independence}} hypotheses, {{together with}} a normality assumption to estimate densities for numerical data, {{are known for their}} simplicity and their effectiveness. However, estimating densities, even under the normality assumption, may be problematic in case of poor data. In such a situation, possibility distributions may provide a more faithful representation of these data. Naive Possibilistic Classifiers (NPC), based on possibility theory, have been recently proposed as a counterpart of <b>Bayesian</b> <b>classifiers</b> to deal with classification tasks. There are only few works that treat possibilistic classification and most of existing NPC deal only with categorical attributes. This work focuses on the estimation of possibility distributions for continuous data. In this paper we investigate two kinds of possibilistic classifiers. The first one is derived from classical or flexible <b>Bayesian</b> <b>classifiers</b> by applying a probability–possibility transformation to Gaussian distributions, which introduces some further tolerance in the description of classes. The second one is based on a direct interpretation of data in possibilistic formats that exploit an idea of proximity between data values in different ways, which provides a less constrained representation of them. We show that possibilistic classifiers have a better capability to detect new instances for which the classification is ambiguous than <b>Bayesian</b> <b>classifiers,</b> where probabilities may be poorly estimated and illusorily precise. Moreover, we propose, in this case, an hybrid possibilistic classification approach based on a nearest-neighbour heuristics to improve the accuracy of the proposed possibilistic classifiers when the available information is insufficient to choose between classes. Possibilistic classifiers are compared with classical or flexible <b>Bayesian</b> <b>classifiers</b> on a collection of benchmarks databases. The experiments reported show the interest of possibilistic classifiers. In particular, flexible possibilistic classifiers perform well for data agreeing with the normality assumption, while proximity-based possibilistic classifiers outperform others in the other cases. The hybrid possibilistic classification exhibits a good ability for improving accuracy...|$|R
40|$|International audienceThis {{study is}} an {{application}} of data fusion techniques, especially fuzzy theory, in determining oil producing zones through four nearby wells, located on an oil field in south west of Iran. Two fusing techniques, used here are based on Bayesian and fuzzy theories. At first, two <b>Bayesian</b> <b>classifiers</b> are being constructed by training in two different wells; then a fuzzy operator, called Sugeno discrete integral, is used to fuse outputs of two mentioned <b>Bayesian</b> <b>classifiers.</b> Finally, it is concluded that using fuzzy classifier fusion improves not only certainty and confidence of decision making, but also generalization ability of determining productive zones...|$|R
40|$|Recent work in <b>Bayesian</b> <b>classifiers</b> {{has shown}} that a better and more {{flexible}} representation of domain knowledge results in more accurate classifiers. We have recently examined {{a new type of}} <b>Bayesian</b> <b>classifiers</b> called Case-Based <b>Bayesian</b> Network (CBBN) <b>classifiers.</b> The basic idea is to partition the training data into semantically sound clusters. A local BN classifier is then learned independently from each cluster. Such a flexible organization of domain knowledge can represent dependency assertions among attributes more accurately and more relevantly than possible in traditional <b>Bayesian</b> <b>classifiers</b> (i. e., BN and BMN classifiers), hence improving classification accuracy. RBMNs also provide a more flexible representation scheme than BNs and generalize BMNs. Briefly, a RBMN is a Decision Tree (DT) with component BNs at the leaves. In this paper, we further explore our CBBN classifiers by comparing them to RBMN classifiers. RBMNs partition the data using a DT induction algorithm. By contrast, CBBNs rely on a flexible strategy for clustering that handles outliers, therefore, allowing more freedom to search for the best way to cluster the data and represent the knowledge. Our experimental results show that CBBN classifiers perform significantly better than RBMN classifiers...|$|R
40|$|Extensive {{research}} has been done on character recognition using the <b>Bayesian</b> <b>Classifier.</b> This paper discusses another approach to character recognition that combines Principal Component Analysis (PCA) and the <b>Bayesian</b> <b>Classifier.</b> PCA extracts the unique information from the feature set of the characters and creates a fewer number of features called 2 ̆ 2 principal components. 2 ̆ 2 Test results show that PCA not only reduces the feature set, but also improves the classification accuracy of the <b>Bayesian</b> <b>Classifier.</b> Character recognition is performed on many types of documents, such as postal documents, office documents, and newspapers. In this study, I apply the PCA techniques to 2 ̆ 2 real world 2 ̆ 2 newspaper characters. The effort is to improve the classification accuracy of 378, 000 newspaper characters by using PCA in addition to the <b>Bayesian</b> <b>Classifier.</b> Classifying newspaper characters is quite a challenge. Characters are of many different fonts, sizes, and shapes. Nevertheless, a classification accuracy of 60...|$|E
40|$|AbstractAnalyzed <b>Bayesian</b> <b>classifier</b> with string, n-gram and API as features, {{we found}} {{that it is very}} {{difficult}} to improve <b>Bayesian</b> <b>classifier</b> detection accuracy because selected features are not completely independent. In order to solve this problem, we propose a new improved choose features method which are most representative properties, and show that our method achieve high detection rates, even on completely new, previously unseen malicious executables...|$|E
40|$|AbstractIn {{this paper}} an {{intrusion}} detection system is developed using Bayesian probability. The system developed is a naive <b>Bayesian</b> <b>classifier</b> {{that is used to}} identify possible intrusions. The system is trained a priori using a subset of the KDD dataset. The trained classifier is then tested using a larger subset of KDD dataset. The <b>Bayesian</b> <b>classifier</b> was able to detect intrusion with a superior detection rate...|$|E
25|$|Lin, H., Koul, N., and Honavar, V. (2011). Learning Relational <b>Bayesian</b> <b>Classifiers</b> from RDF Data. In: Proceedings of the International Semantic Web Conference (ISWC 2011). Springer-Verlag Lecture Notes in Computer Science Vol. 7031 pp.389–404.|$|R
40|$|International audienceNaive <b>Bayesian</b> <b>Classifiers,</b> which rely on {{independence}} hypotheses, {{together with}} a normality assumption to estimate densities for numerical data, {{are known for their}} simplicity and their effectiveness. However, estimating densities, even under the normality assumption, may be problematic in case of poor data. In such a situation, possibility distributions may provide a more faithful representation of these data. Naive Possibilistic Classifiers (NPC), based on possibility theory, have been recently proposed as a counterpart of <b>Bayesian</b> <b>classifiers</b> to deal with classification tasks. There are only few works that treat possibilistic classification and most of existing NPC deal only with categorical attributes. This work focuses on the estimation of possibility distributions for continuous data. In this paper we investigate two kinds of possibilistic classifiers. The first one is derived from classical or flexible <b>Bayesian</b> <b>classifiers</b> by applying a probability–possibility transformation to Gaussian distributions, which introduces some further tolerance in the description of classes. The second one is based on a direct interpretation of data in possibilistic formats that exploit an idea of proximity between data values in different ways, which provides a less constrained representation of them. We show that possibilistic classifiers have a better capability to detect new instances for which the classification is ambiguous than <b>Bayesian</b> <b>classifiers,</b> where probabilities may be poorly estimated and illusorily precise. Moreover, we propose, in this case, an hybrid possibilistic classification approach based on a nearest-neighbour heuristics to improve the accuracy of the proposed possibilistic classifiers when the available information is insufficient to choose between classes. Possibilistic classifiers are compared with classical or flexible <b>Bayesian</b> <b>classifiers</b> on a collection of benchmarks databases. The experiments reported show the interest of possibilistic classifiers. In particular, flexible possibilistic classifiers perform well for data agreeing with the normality assumption, while proximity-based possibilistic classifiers outperform others in the other cases. The hybrid possibilistic classification exhibits a good ability for improving accuracy...|$|R
40|$|AbstractEmpirical {{evidence}} shows that naive <b>Bayesian</b> <b>classifiers</b> perform quite well compared to more sophisticated classifiers, even in view of inaccuracies in their parameters. In this paper, we {{study the effects of}} such parameter inaccuracies by investigating the sensitivity functions of a naive Bayesian network. We show that, {{as a consequence of the}} network’s independence properties, these sensitivity functions are highly constrained. We further investigate whether the patterns of sensitivity that follow from these functions support the observed robustness of naive <b>Bayesian</b> <b>classifiers.</b> In addition to standard sensitivities given available evidence, we also study the effect of parameter inaccuracies in view of scenarios of additional evidence. We show that standard sensitivity functions suffice to describe such scenario sensitivities...|$|R
40|$|The naive <b>Bayesian</b> <b>classifier</b> {{provides}} {{a simple and}} e#ective approach to classifier learning, but its attribute independence assumption is often violated in the real world. A number of approaches have sought to alleviate this problem. A Bayesian tree learning algorithm builds a decision tree, and generates a local naive <b>Bayesian</b> <b>classifier</b> at each leaf. The tests leading to a leaf can alleviate attribute inter-dependencies for the local naive <b>Bayesian</b> <b>classifier.</b> However, Bayesian tree learning still su#ers from the small disjunct problem of tree learning. While inferred Bayesian trees demonstrate low average prediction error rates, {{there is reason to}} believe that error rates will be higher for those leaves with few training examples. This paper proposes the application of lazy learning techniques to Bayesian tree induction and presents the resulting lazy Bayesian rule learning algorithm, called Lbr. This algorithm can be justified by a variant of Bayes theorem which supports a weaker conditional attribute independence assumption than is required by naive Bayes. For each test example, it builds a most appropriate rule with a local naive <b>Bayesian</b> <b>classifier</b> as its consequent. It is demonstrated that the computational requirements of Lbr are reasonable in a wide cross-section of natural domains. Experiments with these domains show that, on average, this new algorithm obtains lower error rates significantly more often than the reverse in comparison to a naive <b>Bayesian</b> <b>classifier,</b> C 4. 5, a Bayesian tree learning algorithm, a constructive <b>Bayesian</b> <b>classifier</b> that eliminates attributes and constructs new attributes using Cartesian products of existing nominal attributes, and a lazy decision tree learning algorithm. It also outperforms, although the result is not statisticall [...] ...|$|E
30|$|Greenspan et al. [19] used Gaussian mixture models (GMMs) for {{learning}} human skin {{color in the}} normalized rg chromaticity space. GMMs offer better generalization capabilities than the <b>Bayesian</b> <b>classifier,</b> and they were later exploited in many approaches to skin color modeling [20, 21]. In our recent survey [2], we demonstrated that GMMs outperform the <b>Bayesian</b> <b>classifier</b> for small training sets; however, for larger sets, the latter was more accurate.|$|E
40|$|AbstractService {{oriented}} computing {{has become}} the main stream research field nowadays. Meanwhile, machine learning is a promising AI technology which can enhance the performance of traditional algorithm. Therefore, aiming at solving service discovery problem, this paper imports <b>Bayesian</b> <b>classifier</b> to web service discovery framework, which can improve service querying speed. In this framework, services in service library become training set of <b>Bayesian</b> <b>classifier,</b> service query becomes a testing sample. Service matchmaking process can be executed in related service class, which has fewer services, thus can save time. Due to don’t know the class of service in training set, EM algorithm is used to estimate prior probability and likelihood functions. Experiment {{results show that the}} EM algorithm and <b>Bayesian</b> <b>classifier</b> supported method outperforms other methods in time complexity...|$|E
40|$|Abstract. Bayesian Networks are {{becoming}} increasingly popular within the Software Engineering research community as an effective method of analysing collected data. This paper deals with the creation {{and the use of}} Bayesian networks and <b>Bayesian</b> <b>classifiers</b> in project management. We illustrate this process with an example in the context of software estimation that uses the Maxwell’s dataset [17] (it is a subset of the Finnish dataset –STTF–). We highlight some of the difficulties and challenges of using Bayesian networks and <b>Bayesian</b> <b>classifiers.</b> We discuss how the Bayesian approach {{can be used as a}} viable technique in Software Engineering in general and for project management in particular; and also the challenges and the open issues. ...|$|R
40|$|Empirical {{evidence}} shows that naive <b>Bayesian</b> <b>classifiers</b> perform quite well compared to more sophisticated classifiers, even in view of inaccuracies in their parameters. In this paper, we {{study the effects of}} such parameter inaccuracies by investigating the sensitivity functions of a naive Bayesian network. We show that, {{as a consequence of the}} network’s independence properties, these sensitivity functions are highly constrained. We further investigate whether the patterns of sensitivity that follow from these functions support the observed robustness of naive <b>Bayesian</b> <b>classifiers.</b> In addition to standard sensitivities given available evidence, we also study the effect of parameter inaccuracies in view of scenarios of additional evidence. We show that standard sensitivity functions suffice to describe such scenario sensitivities...|$|R
40|$|The program PleioGRiP {{performs}} a genome-wide Bayesian model search to identify SNPs {{associated with a}} discrete phenotype, and uses SNPs ranked by Bayes factor to produce nested <b>Bayesian</b> <b>classifiers.</b> These classifiers {{can be used for}} genetic risk prediction, either selecting the classifier with optimal number of features, or using an ensemble of classifiers. In addition, PleioGRiP implements an extension to the Bayesian search and classification, and can search for pleiotropic relationships in which SNPs are simultaneously associated with two or more distinct phenotypes. These relationships can be used to generate connected <b>Bayesian</b> <b>classifiers</b> to predict the phenotype of interest either using genetic data alone, or in combination with the secondary phenotype(s). NIH/NHLBI R 21 HL 11423...|$|R
