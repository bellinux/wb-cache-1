44|41|Public
5000|$|As {{an example}} of this last point, {{consider}} the feature space shown to the right. The variables may each be regarded at two different resolutions. Variable [...] may be regarded at a high (quaternary) resolution wherein it takes on the four values [...] or at a lower (<b>binary)</b> <b>resolution</b> wherein it takes on the two values [...] Similarly, variable [...] may be regarded at a high (quaternary) resolution or at a lower (<b>binary)</b> <b>resolution,</b> where it takes on the values [...] or , respectively. It will be noted that at the high resolution, there are no detectable implications of the form , since every [...] is associated with more than one , and thus, for all , [...] However, at the low (binary) variable resolution, two bilateral implications become detectable: [...] and , since every [...] occurs iff [...] and [...] occurs iff [...] Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the higher quaternary variable resolution.|$|E
50|$|Vampire's kernel {{implements}} the calculi of ordered <b>binary</b> <b>resolution</b> and superposition {{for handling}} equality. The splitting rule and negative equality splitting can be simulated by {{the introduction of}} new predicate definitions and dynamic folding of such definitions. A DPLL-style algorithm splitting is also supported. A number of standard redundancy criteria and simplification techniques are used for pruning the search space: tautology deletion, subsumption resolution, rewriting by ordered unit equalities, basicness restrictions and irreducibility of substitution terms. The reduction ordering used is the standard Knuth-Bendix ordering.|$|E
40|$|Abstract. The rank/activity {{restriction}} on <b>binary</b> <b>resolution</b> is introduced. It accepts {{only a single}} derivation tree from a large equivalence class of such trees. The equivalence classes capture all trees that are {{the same size and}} differ only by reordering the resolution steps. A proof procedure that combines this restriction with the authors ’ minimal restriction of <b>binary</b> <b>resolution</b> computes each minimal <b>binary</b> <b>resolution</b> tree exactly once. ...|$|E
40|$|I. LINGELING Compared to {{the version}} {{submitted}} to the SAT competition 2011 and described in [1], we removed complicated algorithms and features, which did not really have any observable impact on the run-time for those benchmarks we tried. In particular, various versions of distillation inprocessors were removed. Regarding inprocessing [4], there are two new probing variants. One is called simple probing and tries to learn hyper <b>binary</b> <b>resolutions</b> eagerly. The other variant is based on tree-based look-ahead, which is a simplified version of the implementation in March [2]. These two techniques are complemented by gaussian elimination and a new congruence closure algorithm, which both use extracted gates to generate and propagate equivalences. We also switched to one merged inprocessing phase, calle...|$|R
40|$|Abstract — The paper {{researches}} {{the random}} contention {{system based on}} the <b>binary</b> tree conflict <b>resolution</b> algorithm to improve the throughput in-depth using the average cycle method, then gets the formulas of the systemic throughput and so on. The simulation results verify the correctness of the theory, meanwhile, gets some conclusions that the different arrival rate G is how to affect {{the main source of}} the throughput with variable packet length. It has some researching significance and improves the system performance. Keywords- the average cycle method; improved <b>binary</b> tree conflict <b>resolution</b> algorithm; throughput; discrete random contention system; arrival rat...|$|R
40|$|Abstract—Reducing {{the number}} of tag {{collisions}} {{is one of the}} most important issues in RFID systems, as collisions induce inefficiency. This paper presents a mechanism of grouping of tags via a bit mask, quick tag estimation by a pilot frame and near optimal <b>binary</b> tree-based collision <b>resolution</b> with a frame. Performance analysis and simulation results show that the proposed anti-collision algorithm consumes fewer time slots as compared to previous work, and approaches to the case with the optimal frame size using <b>binary</b> tree collision <b>resolution.</b> Index Terms—Anti-collision, collision threshold, pilot frame, RFID, tag estimation, tag identification. I...|$|R
40|$|The rank/activity {{restriction}} on <b>binary</b> <b>resolution</b> is introduced. It accepts {{only a single}} derivation tree from a large equivalence class of such trees. The equivalence classes capture all trees that are {{the same size and}} differ only by reordering the resolution steps. A proof procedure that combines this restriction with the authors' minimal restriction of <b>binary</b> <b>resolution</b> computes each minimal <b>binary</b> <b>resolution</b> tree exactly once. 1 Introduction A new restriction of <b>binary</b> <b>resolution</b> is proposed in this paper. The restriction is complete in a strong sense, in that every <b>binary</b> <b>resolution</b> proof, up to reordering the resolution steps, is allowed. On the other hand, the restriction prevents multiple versions of the same proof from being constructed. If a given proof is allowed, then no other proof that can be obtained from it by reordering the steps is allowed. Consider an automated reasoning procedure that takes a set of clauses as input, and resolves pairs of clauses containing com [...] ...|$|E
40|$|We {{study the}} {{tradeoff}} between inference and search in the Davis Putnam algorithm. We show that neighbour resolution, a restricted form of resolution applied during search, can be simulated by applying <b>binary</b> <b>resolution</b> before search. We compare experimentally {{the cost of}} the two different methods. Our results demonstrate that <b>binary</b> <b>resolution</b> during preprocessing is generally effective at reducing both the size of the search tree and the total search time. ...|$|E
40|$|A given <b>binary</b> <b>resolution</b> proof, {{represented}} as a binary tree, {{is said to be}} minimal if the resolutions cannot be reordered to generate an irregular proof. Minimality extends Tseitin's regularity restriction and still retains completeness. A linear time algorithm is introduced to decide whether a given proof is minimal. This algorithm can be used by a deduction system that avoids redundancy by retaining only minimal proofs, and thus lessens its reliance on subsumption, a more general but more expensive technique. Any irregular <b>binary</b> <b>resolution</b> tree is made smaller by an operation called surgery, which runs in time linear {{in the size of the}} tree. After surgery the result proved by the new tree is at least as general as the original result. Furthermore any non-minimal tree can be made irregular in linear time by an operation called splay. Thus a combination of splaying and surgery efficiently reduces a non-minimal tree to a minimal one. Finally, a close correspondence between clause [...] ...|$|E
40|$|We {{study how}} the angular {{resolution}} of LISA for merging massive black-hole binaries would be improved {{if we could}} observe multiple images (gravitational waves) due to strong gravitational lensing. As the correlation between fitting parameters is reduced by additional information of the second image, the error box in the sky could be significantly reduced. This improvement would be very helpful to specify the host galaxy of a <b>binary.</b> Angular <b>resolution</b> expected with multiple detectors is also discussed...|$|R
40|$|The Broad Band X Ray Telescope (BBXRT) {{is a new}} X ray {{spectrophotometer}} {{scheduled for}} launch Apr. 26, 1990, for a 10 day mission aboard the Space Shuttle Columbia. High quality spectra between 0. 3 and 12 keV of about 100 different sources are expected. BBXRT has the best energy resolution for examining features due to the K-shell of Fe, seen in the spectra of X ray <b>binaries.</b> The <b>resolution</b> is sufficient to make sensitive searches for broadened emission lines, to resolve multiple features, and to determine the ionization state of the emitting plasma by measuring line energies. Simulated spectra of several sources are presented...|$|R
40|$|This {{work was}} {{supported}} by the award of a Postgraduate Studentship from EPSRC (EP/K 503162 / 1) to TK. Two recognition-mediated reaction processes operating through a reactive <b>binary</b> complex drive <b>resolution</b> of a 24 -component dynamic covalent library, assembled from individual aldehydes and nucleophiles. The effectiveness of the library resolution and selective amplification of one recognition-enabled species over another is limited by the difference in the rates of the recognition-mediated reactive processes and strength of the recognition processes employed in the dynamic system. PostprintPeer reviewe...|$|R
40|$|Abstract—In this paper, an {{efficient}} voltage scalable switched capacitor converter (SCC) for 1. 1 V battery-powered system is presented. The SCC employs a <b>binary</b> <b>resolution</b> technique to step-down the input voltage {{to a range}} of voltages, while keeping the efficiency high. An optimization strategy for designing multitopology SCC is presented to improve the effectiveness of the circuit and to preserve efficiency over large load voltages. I...|$|E
40|$|A {{theorem proving}} {{procedure}} is described which combines {{the approach of}} locking resolution with that of rewriting systems. Indeed, both the <b>binary</b> <b>resolution</b> and a complete restriction of paramodulation are embodied by {{an extension of the}} rewriting operation called superposition. Experimental results are reported and compared with literature automated proofs. We describe in this paper a theorem-proving procedure for first-order logic with equality which combines the approach of locked resolutio...|$|E
40|$|Abstract We {{study the}} {{tradeoff}} between inference and search in the Davis Put-nam algorithm. We show that neighbour resolution, a restricted form of resolution applied during search, can be simulated by applying binary res-olution before search. We compare experimentally {{the cost of}} the two different methods. Our results demonstrate that <b>binary</b> <b>resolution</b> duringpreprocessing is generally e ffective at reducing both the size of the searchtree and the total search time...|$|E
40|$|We present {{numerical}} {{simulations of}} orbiting black holes for around twelve cycles, using a high-order multipatch approach. Unlike some other approaches, the computational speed scales almost perfectly {{for thousands of}} processors. Multipatch methods are an alternative to AMR (adaptive mesh refinement), with benefits of simplicity and better scaling for improving the resolution in the wave zone. The results presented here {{pave the way for}} multipatch evolutions of black hole-neutron star and neutron star-neutron star <b>binaries,</b> where high <b>resolution</b> grids are needed to resolve details of the matter flow...|$|R
40|$|Abstract. From {{the point}} of view to improve QoS of Ad Hoc network, this paper {{analyses}} the problem about mobile nodes in Ad Hoc network and decides to solve the problem by designing effective MAC protocol. Therefore, a novel random multi-access protocol using <b>Binary</b> Tree Collision <b>Resolution</b> for collision packets in Probability-Persistent Random Multi-Access (IBTCRPP-CSMA) is proposed. The throughput of IBTCRPP-CSMA and computer simulation experiments are obtained. Simulation results agree with theoretical analysis. By contrasts of throughputs among several CSMA protocols, IBTCRPP-CSMA has higher throughput and channel utilization, thereby ensuring the system’s high QoS...|$|R
40|$|Analysis results {{demonstrate}} that multiple sampling can achieve consistently higher signal-to-noise ratio at equal or higher dynamic range than using other image sensor dynamic range enhancement schemes such as well capacity adjusting. Implementing multiple sampling, however, requires much higher readout speeds {{than can be}} achieved using typical CMOS active pixel sensor (APS). This paper demonstrates, using a 640 222 512 CMOS image sensor with 8 -b bit-serial Nyquist rate analog-todigital converter (ADC) per 4 pixels, that pixel-level ADC enables a highly flexible and efficient implementation of multiple sampling to enhance dynamic range. Since pixel values are available to the ADC's at all times, the number and timing of the samples {{as well as the}} number of bits obtained from each sample can be freely selected and read out at fast SRAM speeds. By sampling at exponentially increasing exposure times, pixel values with <b>binary</b> floating-point <b>resolution</b> can be obtained. The 640 222 512 [...] ...|$|R
40|$|This paper {{focuses on}} {{developing}} efficient inference techniques for improving {{conjunctive normal form}} (CNF) Boolean satisfiability (SAT) solvers. We analyze a variant of hyper <b>binary</b> <b>resolution</b> from various perspectives: We show that it can simulate the circuit-level technique of structural hashing {{and how it can}} be realized efficiently using so called tree-based lookahead. Experiments show that our implementation improves the performance of state-of-the-art CNFlevel SAT techniques on combinational equivalent checking instances...|$|E
40|$|OTTER (Organized Techniques for Theorem-proving and Effective Research) is a resolution-style theorem-proving {{program for}} first-order logic with equality. OTTER {{includes}} the inference rules <b>binary</b> <b>resolution,</b> hyperresolution, UR-resolution, and binary paramodulation. Some of its other abilities and features are conversion from first-order formulas to clauses, {{forward and back}} subsumption, factoring, weighting, answer literals, term ordering, forward and back demodulation, evaluable functions and predicates, and Knuth-Bendix completion. OTTER is coded in C, is free, and is portable to {{many different kinds of}} computer...|$|E
40|$|We {{investigate}} {{the ability of}} two central encoding methods to propagate reachability and relevance information using resolution steps. More specifically, we compare the ability of unit-propagation and higher-order resolution steps to propagate reachability and relevance information {{in the context of}} the linear and GRAPHPLAN encoding schemes to the ability of a natural class of reachability and relevance algorithms that operate at the plan level. As a result of our observations and additional considerations, we experiment with a preprocessing step based on limited <b>binary</b> <b>resolution</b> that shows nice results. ...|$|E
40|$|In most mesosphere-stratosphere-troposphere (MST) {{applications}} pulsed radars are {{peak power}} limited and have excess average power capability. Short pulses {{are required for}} good range resolution but the problem of range biguity (signals received simultaneously {{from more than one}} altitude) sets a minimum limit on the interpulse period (IPP). Pulse compression is a echnique which allows more of the transmitter average power capacity to be used without scarificing range <b>resolution.</b> <b>Binary</b> phase coding methods for pulse compression are discussed. Many aspects of codes and decoding and their applications to MST experiments are addressed; this includes Barker codes and longer individual codes, and then complementary codes and other code sets. Software decoding, hardware decoders, and coherent integrators are also discussed...|$|R
40|$|We {{study the}} pairing of massive black holes {{embedded}} in a massive circum-nuclear, rotationally supported disc, until they form a close <b>binary.</b> Using high <b>resolution</b> SPH simulations, we follow the black hole dynamics, {{and in particular the}} eccentricity evolution, {{as a function of the}} composition in stars and gas of the disc. Binary-disc interaction always leads to orbital decay and, in case of co-rotating black holes, to orbit circularization. We present also a higher resolution simulation performed using the particle-splitting technique showing that the binary orbital decay is efficient down to a separation of ~ 0. 1 pc, comparable to our new resolution limit. We detail the gaseous mass profile bound to each black hole. Double nuclear activity is expected to occur on an estimated timescale of 10 Myr...|$|R
40|$|We {{present a}} case study of a early-type galaxy (ETG) hosting a kinematically {{distinct}} core (KDC) formed in a <b>binary</b> high <b>resolution</b> 1 : 1 spiral galaxy merger simulation. The runtime of the simulation is pushed up to 10 Gyr to follow the complete evolution of various physical properties. To investigate the origin of the KDC, the stellar component residing within the KDC is dissected, revealing that the rotational signal is purely generated by stars that belong to the KDC for at least 0. 5 Gyr and are newly formed during the merging process. Following the orientation of the total stellar angular momentum of the KDC, we show that it performs a motion comparable to the precession of a gyroscope in a gravitational potential. We draw the conclusion that the motion of the KDC is a superposition of an intrinsic rotation and a global precession that gets gradually damped over cosmic time. Finally, the stability of the KDC over the complete runtime of the simulation is investigated by tracing the evolution of the widely used λ_R parameter and the misalignment angle distribution. We find that the KDC is stable for about 3 Gyr after the merger and subsequently disperses completely on a timescale of 1. 5 Gyr. Comment: To appear in Galaxies, special issue "On the Origin (and Evolution) of Baryonic Galaxy Halos...|$|R
40|$|In this paper, {{we propose}} novel {{low-cost}} methods that combine static logic implications and <b>binary</b> <b>resolution</b> to significantly in-crease {{the number of}} non-trivial signal relations learned from the circuit. The proposed method first applies resolution techniques to learn new static single-node implications and then uses them to learn powerful multi-node implications. All the newly learned re-lations help in extracting more necessary assignments for a given fault, potentially increasing the chance for a conflict to occur among the necessary assignments. Experimental results on ISCAS 89 and ITC 99 benchmarks show that our method can identify significantly more untestable faults compared to existing non branch-and-bound based techniques...|$|E
40|$|Resolution reasoners, {{when applied}} to set theory problems, {{typically}} suffer from “lack of focus. ” MARS is a program that attempts to rectify this difficulty by exploiting the definition-like character of the set theory axioms. As {{in the case of}} its predecessor, SLIM, it employs a tableau proof procedure based on <b>binary</b> <b>resolution,</b> but MARS IS enhanced by an equality substitution rule and a device for introducing previously proved theorems as lemmas. MARS’s performance compares favorably with that of other existing automated reasoners for this domain. MARS finds proofs for many basic facts about functions, construed as sets of ordered pairs. MARS is being used to attack the homomorphism test problem, the theorem that the composition of two group homomorphisms is a group homomorphism...|$|E
40|$|A lot of {{the human}} ability to prove hard {{mathematical}} theorems can be ascribed to a domain-specific problem solving know-how. Such knowledge is intrinsically incomplete. In order to solve related problems, human mathematicians can go beyond their acquired knowledge by adapting their know-how. These two aspects, having rich experience and extending it by need, can be simulated in a proof planning framework. The domain-specific reasoning knowledge is represented in form of declarative planning operators, called methods. Since these are declarative, they can be mechanically adapted to new situations by socalled meta-methods. In this contribution we apply this framework to two prominent proofs in theorem proving. First, we present methods for proving the ground completeness of <b>binary</b> <b>resolution,</b> which essentially correspond to key lemmata, and then, we show how these methods can be reused for the proof of the ground completeness of lock resolution. Keywords: Proof planning, reuse, adapta [...] ...|$|E
40|$|Martin et al. {{showed that}} a {{substantially}} misaligned accretion disk around one component of a binary system can undergo global damped Kozai–Lidov (KL) oscillations. During these oscillations, the inclination and eccentricity of the disk are periodically exchanged. However, the robustness of this mechanism and its dependence on the system parameters were unexplored. In this paper, we use three-dimensional hydrodynamical simulations to analyze how various binary and disk parameters affect the KL mechanism in hydrodynamical disks. The simulations include the effect of gas pressure and viscosity, but ignore the effects of disk self-gravity. We describe results for different numerical <b>resolutions,</b> <b>binary</b> mass ratios and orbital eccentricities, initial disk sizes, initial disk surface density profiles, disk sound speeds, and disk viscosities. We show that the KL mechanism can operate {{for a wide range}} of binary-disk parameters. We discuss the applications of our results to astrophysical disks in various accreting systems...|$|R
40|$|This paper {{presents}} an approach for the effective combination of interpolation with binarization of gray level text images to reconstruct a high <b>resolution</b> <b>binary</b> image from a lower resolution gray level one. We study two nonlinear interpolative techniques for text image interpolation. These nonlinear interpolation methods map quantized low dimensional 2 Θ 2 image blocks to higher dimensional 4 Θ 4 (possibly binary) blocks using a table lookup operation. The first method performs interpolation of text images using context-based, nonlinear, interpolative, vector quantization (NLIVQ). This system has a simple training procedure and has performance (for gray-level high resolution images) that {{is comparable to}} our more sophisticated generalized interpolative VQ (GIVQ) approach, which is the second method. In it, we jointly optimize the quantizer and interpolator to find matched codebooks for the low and high resolution images. Then, to obtain the binary codebook that incorporates bin [...] ...|$|R
40|$|This paper mainly {{discusses}} the technique {{used to reduce}} sidelobes for polyphase pulse compression codes. A brief explanation is given on pulse compression and range <b>resolution.</b> <b>Binary,</b> ternary, quinquenary, multilevel and Barker codes are also discussed. The details of polyphase pulse compression code that comprises Frank code, Extended Frank code, P 3 and P 4 codes are described. The basic theory on autocorrelation and discrimination is explained briefly. Furthermore, sidelobe reduction techniques for polyphase pulse compression codes are discussed in this paper. The technique of least square amplitude and phase weighting is focussed on P 4, binary and ternary codes [...] Finally, comparison of mean square error is made between different choices like all sidelobes reduction, some sidelobes reduction etc. to conclude which type of choice; the reduction technique is achieving less error thus providing a good reduction in autocorrelation with maintaining the same mainlobe level...|$|R
40|$|One {{method of}} proving theorems in Horn clause {{theories}} is surface deduction (also {{known as the}} modification method). Surface deduction yields interpretations of unification failures in terms of residual hypotheses needed for unification to succeed. This suggests {{that it can be}} used for abductive reasoning with equality. In surface deduction the input clauses are first transformed to a flat form (involving no nested terms) and symmetrized (if necessary). They are then manipulated by <b>binary</b> <b>resolution,</b> a restricted version of factoring and compression. In this paper we partially characterize the deductive strength of surface deduction and show how it depends on the type of flattening used. This is used to show that some forms of surface deduction will yield all hypotheses preferred by parsimony when used as an abductive inference engine. The characterization of deductive strength suggests a new equational preference principle according to which honest explanations are preferred. In h [...] ...|$|E
30|$|The MIMOSA 26 sensors {{used for}} precise spatial {{measurements}} of particle trajectories are manufactured with the AMS 350 nm CMOS technology [7]. Each MIMOSA 26 sensor consists of pixels sized 18.4 μm × 18.4 μm, which {{are arranged in}} 1152 columns and 576 rows. This {{adds up to a}} total of about six hundred thousand readout channels per sensor, covering an active area of about 21.2 × 10.6 mm. The specifications of the MIMOSA 26 sensors quote a thickness of 50 μm. Measurements with a digital microscope reveal an average thickness over the six sensors used of (54.5 ± 3.6) μm. Free charge carriers produced in the underlying 20 μm high resistivity (about 400 Ωcm) epitaxial layer 1 are collected via drift (diffusion) in depleted (undepleted) regions. The <b>binary</b> <b>resolution</b> of 5.3 μm is improved by charge sharing, i.e. the collection of charge at adjacent pixels and subsequent calculation of the centre of gravity, as is shown in the “Track resolution studies” section.|$|E
40|$|Abstract — In this brief, an {{efficient}} voltage scalable switched capacitor converter (SCC) for 1. 1 V battery-powered digital system is presented. The SCC employs a <b>binary</b> <b>resolution</b> technique to preserve high efficiency at load voltages down to sub- 200 mV {{while keeping the}} efficiency high. The proposed converter can be configured into four topologies to support subthreshold output levels of 0. 18 – 0. 6 V. The converter is designed in a standard lowpower 40 -nm CMOS TSMC process. Simulation {{results show that the}} efficiency of the SCC can be improved by 10 %– 11 % in the vicinity of VDD = 200 mV as compared to one using a conventional approach. An optimization strategy for designing multi-topology SCC is presented to improve the effectiveness of the circuit and to preserve efficiency over large load voltages. Index Terms — Dc–dc power converter, design optimization, low-power electronics, subthreshold design, switched capacitor circuits. I...|$|E
40|$|Most CPU-based volume raycasting {{approaches}} achieve {{high performance}} by advanced memory layouts, space subdivision, and excessive pre-computing. Such approaches typically need {{an enormous amount}} of memory. They are limited to sizes which do not satisfy the medical data used in daily clinical routine. We present a new volume raycasting approach based on image-ordered raycasting with object-ordered processing, which is able to perform highquality rendering of very large medical data in real-time on commodity computers. For large medical data such as computed tomographic (CT) angiography run-offs (512 x 512 x 1202) we achieve rendering times up to 2. 5 fps on a commodity notebook. We achieve this by introducing a memory efficient acceleration technique for on-the-fly gradient estimation and a memory efficient hybrid removal and skipping technique of transparent regions. We employ quantized <b>binary</b> histograms, granular <b>resolution</b> octrees, and a cell invisibility cache. These acceleration structures require just a small extra storage of approximately 10 %...|$|R
40|$|Martin et al. (2014 b) {{showed that}} a {{substantially}} misaligned accretion disk around one component of a binary system can undergo global damped Kozai-Lidov oscillations. During these oscillations, the inclination and eccentricity of the disk are periodically exchanged. However, the robustness of this mechanism and its dependence on the system parameters were unexplored. In this paper, we use three-dimensional hydrodynamical simulations to analyze how various binary and disk parameters affect the Kozai-Lidov mechanism in hydrodynamical disks. The simulations include the effect of gas pressure and viscosity, but ignore the effects of disk self-gravity. We describe results for different numerical <b>resolutions,</b> <b>binary</b> mass ratios and orbital eccentricities, initial disk sizes, initial disk surface density profiles, disk sound speeds, and disk viscosities. We show that the Kozai-Lidov mechanism can operate {{for a wide range}} of binary-disk parameters. We discuss the applications of our results to astrophysical disks in various accreting systems. Comment: updated to match published versio...|$|R
40|$|Abstract. We {{study the}} {{evolution}} and emission of circumbinary disks around close classical T Tauri <b>binary</b> systems. High <b>resolution</b> numerical hydrodynamical simulations are employed to model a system {{consisting of a}} central eccentric binary star within an irradiated accretion disk. A detailed energy balance including viscous heating, radiative cooling and irradiation from the central star is applied to calculate accurately the emitted spectral energy distribution. Numerical simulations using two different methods, the previously developed Dual-Grid technique with a finite difference discretization, and the Smoothed Particle Hydrodynamics method are employed to compare the hydrodynamical features and strengthen our conclusions. Physical parameters of the setup are chosen to model the close systems of DQ Tau and AK Sco. Using the self-consistent models, {{we are able to}} fit the observed spectral energy distributions by constraining parameters such as disk mass, density profile and radial extension for those systems. We find that the incorporation of irradiation effects is necessary to obtain correct disk temperatures...|$|R
