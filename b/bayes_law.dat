16|6|Public
50|$|The {{principle}} of restricted choice is {{an application of}} <b>Bayes</b> <b>Law.</b> Increases and decreases in the probabilities of original lies of the opposing cards, as the play of the hand proceeds, are examples of Bayesian updating as evidence accumulates.|$|E
40|$|Financial models {{require an}} {{understanding}} of human behavior. Classical economics operated with the homo oeconomicus, who 1. has well-behaved preferences, and 2. acts rationally and updates beliefs according to <b>Bayes</b> <b>law.</b> Cognitive psychologists and behavioral economists challenge the homo oeconomicus view. The relevance of studying behavioral deviations from the homo oeconomicus (anomalies) stems from the limits of arbitrage literature. An anomaly is an (i) empirical fact that is (ii) inconsistent with theory. Anomalies {{can be found in}} the (i) real world (field data) or through (ii) experiments. What systematic errors do humans do? (for optical illusions: www. michaelbach. de/ot...|$|E
40|$|This paper {{describes}} {{a method for}} learning the joint probability distribution {{of a set of}} variables from a sample of instances from the domain. The method is based on a straightforward application of <b>Bayes</b> <b>Law</b> to the problem of estimating individual probabilities from a probability distribution. We use a maximum entropy distribution as an initial estimate and show how this estimate can be easily updated each time an additional example is observed. Although developed for the purpose of estimating the conditional probabilities required for Bayesian inference networks, this method can be adopted to simplify knowledge acquisition in any expert system that uses knowledge in the form of probabilities...|$|E
50|$|Schneps {{promotes}} {{public awareness}} {{of the importance of the}} proper use of mathematics and statistics in criminal proceedings. In addition to her book on the subject, she has written newspaper articles and she is a member of the <b>Bayes</b> and the <b>Law</b> International Consortium.|$|R
40|$|Although {{the last}} forty years has seen {{considerable}} {{growth in the}} use of statistics in legal proceedings, it is primarily classical statistical methods rather than Bayesian methods that have been used. Yet the Bayesian approach avoids many of the problems of classical statistics and is also well suited to a broader range of problems. This paper reviews the potential and actual use of <b>Bayes</b> in the <b>law</b> and explains the main reasons for its lack of impact on legal practice. These include misconceptions by the legal community, over-reliance {{on the use of the}} likelihood ratio and the lack of adoption of modern computational methods. We argue that Bayesian Networks (BNs), which automatically produce the necessary Bayesian calculations, provide an opportunity to address most concerns about using <b>Bayes</b> in the <b>law...</b>|$|R
40|$|Daniel Berger, "Bayes and the Law", Annual Review of Statistics and its Application, Vol. 3, March 2016. Although {{the last}} forty years has seen {{considerable}} {{growth in the}} use of statistics in legal proceedings, it is primarily classical statistical methods rather than Bayesian methods that have been used. Yet the Bayesian approach avoids many of the problems of classical statistics and is also well suited to a broader range of problems. This paper reviews the potential and actual use of <b>Bayes</b> in the <b>law</b> and explains the main reasons for its lack of impact on legal practice. These include misconceptions by the legal community about Bayes??? theorem, over-reliance {{on the use of the}} likelihood ratio and the lack of adoption of modern computational methods. We argue that Bayesian Networks (BNs), which automatically produce the necessary Bayesian calculations, provide an opportunity to address most concerns about using <b>Bayes</b> in the <b>law...</b>|$|R
40|$|<b>Bayes</b> <b>Law,</b> or {{the law of}} {{conditional}} probability, {{provides a}} natural inference framework for one who views hypothesis testing as parameter estimation. Heretofore a major difficul-ty in applying Bayesian ideas to psychological contexts has been the specification of an objective or public prior. This paper proposes a rule for selecting a prior hypothesis which is both unambiguous and hostile to the research hypothesis: choose the prior so that 1) its expectation is the conventional null value and 2) it has maximum probability of producing the obser-ved data. The rule is employed to develop {{a complete set of}} tests for nominal data, and both a one-sample and a two-sample test for difference of means. Numerical illustrations are include...|$|E
40|$|This article {{reviews the}} {{formulation}} {{and evolution of}} the Philadelphia National Bank anticompetitive presumption {{through the lens of}} decision theory and <b>Bayes</b> <b>Law.</b> It explains how the economic theory, empirical evidence and experience are used to determine a presumption and how that presumption interacts with the reliability of relevant evidence to rationally set the appropriate burden of production and burden of persuasion to rebut the presumption. The article applies this reasoning to merger presumptions. It also sketches out a number of non-market share structural factors that might be used to supplement or replace the current legal and enforcement presumptions for mergers. It also discusses the potential for conflicting presumptions and how such conflicts might best be resolved...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis report describes {{the testing of}} four Model Output Statistics prediction methods on simulated data fields {{for the purpose of}} determining their relative skills in forecasting a generic weather parameter (predictand). Of the four methods, three use <b>Bayes</b> <b>Law</b> of Inverse Probability to discriminate, while the other method uses conditional probability. The simulated data sets, models and observers necessary to accomplish this goal are created according to a uniquely developed simulation design. The results indicate that there is a definite difference in the ability of one of the four methods, namely the method using conditional probability, to forecast the weather parameter. Through the use of the Analysis of Variance (ANOVA) technique, this difference is found to be significant with respect to chance. [URL] United States Nav...|$|E
5000|$|With her mother, {{mathematician}} Leila Schneps, Colmez has co-authored Math on Trial: How Numbers Get Used and Abused in the Courtroom. This book, {{targeted for}} a general audience, uses ten historical legal cases to show how mathematics, especially statistics, can affect the outcome of criminal proceedings, especially when incorrectly applied or interpreted. While not written as a textbook, some reviewers have found it suitable for students, as {{an introduction to the}} topic and to [...] "get them thinking, talking and even arguing about the issues involved", with another agreeing that, [...] "they have struck the right balance of providing enough mathematics for the specialist to check out the details, but not so much as to overwhelm the general reader", and another finding the book suitable [...] "for parents trying to support teenagers in their studies of mathematics - or in fact, law". While most reviews are positive, there has been some criticism that the book over-simplifies the influence mathematics has in complex trial proceedings. One reviewer finds that, while the book's description of the weakness of the mathematics is valid, that it does not completely treat the role mathematics plays in complex modern legal proceedings, while another suggests the book attributes insufficient weight to the counterbalancing practice of lawyers attacking opposing evidence and experts with their own. In addition to the book, she has written guest columns in other publications on the same topic, {{and is a member of}} the <b>Bayes</b> and the <b>Law</b> International Consortium, which promotes improved understanding of the use of statistics in legal proceedings.|$|R
50|$|Fenton {{currently}} {{works on}} quantitative risk assessment. This typically involves analysing and predicting the probabilities of unknown events using Bayesian statistical methods including especially causal, probabilistic models (Bayesian networks). This type of reasoning enables improved assessment by taking account of both statistical data and also expert judgment. In April 2014 Fenton was awarded {{one of the}} prestigious European Research Council Advanced Grants to focus on these issues. Fenton's experience in risk assessment covers {{a wide range of}} application domains such as legal reasoning (he has been an expert witness in major criminal and civil cases), medical analytics, vehicle reliability, embedded software, transport systems, financial services, and football prediction. Fenton has a special interest in raising public awareness of the importance of probability theory and Bayesian reasoning in everyday life (including how to present such reasoning in simple lay terms) and he maintains a website dedicated to this and also a blog focusing on probability and the law. In March 2015 Norman presented the BBC documentary Climate Change by Numbers. Fenton has published 7 books and 230 referred articles and has provided consulting to many major companies world-wide. His 2012 book was the first to bring Bayesian networks to a general audience. Fenton's current projects are focused on using Bayesian methods for improved legal reasoning and improved medical decision making. Since June 2011 he has led an international consortium (<b>Bayes</b> and the <b>Law)</b> of statisticians, lawyers and forensic scientists working to improve the use of statistics in court. In 2016, he is leading a prestigious 6-month Programme on Probability and Statistics in Forensic Science at the Isaac Newton Institute for Mathematical Sciences, University of Cambridge. In addition to his research on risk assessment, Fenton is renowned for his work in software engineering (including pioneering work on software metrics); the third edition of his book “Software Metrics: A Rigorous and Practical Approach” was published in November 2014. The book {{is one of the most}} cited in software engineering (5040 citations, Google Scholar, Feb 2016).|$|R
40|$|Abstract—This paper {{addresses}} {{the problem of}} joint estima-tion {{of the state and}} parameters for a deterministic continuous time system, with discrete time observations, in which the parameter vector is constant but its value is not known, being a random variable with a known distribution. Along time, the uncertainty in the parameter induces uncertainty in the plant state. The joint probability density function (pdf) satisfies the Liouville partial differential equation that is a limit case of the Fokker-Planck equation for vanishing diffusion. The continuous-discrete filter proposed operates as follows: Between two consecutive output sampling time instants, the pdf is propagated by solving the Liouville equation for an augmented state and is then corrected by using the last observation and <b>Bayes</b> <b>law.</b> An application to state estimation of the neuromuscular blockade of patients subject to general anesthesia, where parameter uncertainty is due to inter-patient variability, is described...|$|E
40|$|The {{ability to}} {{identify}} {{the target of a}} common action is fundamental {{for the development of a}} multi-robot team able to interact with the environment. In most existing systems, the identification is carried on individually, based on either color coding, shape identification or complex vision systems. Those methods usually assume a broad point of view over the objects, which are observed in their entirety. This assumption is sometimes difficult to fulfil in practice, and in particular in swarm systems, constituted by a multitude of small robots with limited sensing and computational capabilities. In this paper, we propose a method for target identification with a heterogeneous swarm of low-informative spatially-distributed sensors employing a distributed version of the naive Bayes classifier. Despite limited individual sensing capabilities, the recursive application of the <b>Bayes</b> <b>law</b> allows the identification if the robots cooperate sharing the information that they are able to gather from their limited points of view. Simulation results show the effectiveness of this approach highlighting some properties of the developed algorithm...|$|E
40|$|Bayesian {{methods are}} finding {{increasing}} application and use in environmental modeling. <b>Bayes</b> <b>law</b> {{states that the}} posterior, P(θ|D) {{is proportional to the}} product of the prior, P(θ) and likelihood, L(θ|D), or in mathematical form, P(θ|D) / P(θ) L(θ|D). The main crux in the application of such methods relies in the definition of the likelihood function, L(θ|D) used to summarize the distance between the n model simulated values,D' and corresponding data, D. Under ideal conditions, the residuals exhibit normality and standard likelihood functions will suffice. Yet, in real-world modeling studies the residuals are dominated by model and input data errors with probabilistic properties that are not easy to capture in the construction of a likelihood function. Recent contributions therefore use latent variables to parameterize model input and structural errors and estimate these variables jointly with the model parameters, θ. We caution against this approach in the present thesis and demonstrate that the posterior values of the latent variables strongly depend on the (hydrologic) model structure being used. Although strong priors can be used to somewhat alleviate this problem, this requires explicit information about the size and space/time correlation of the input data errors. An alternative viewpoint emerges that model structural errors are relative and only meaningfully interpreted on a model comparative basis...|$|E
40|$|In a {{challenging}} and constantly changing world, {{students are required}} to develop advanced thinking skills such as critical systematic thinking, decision making and problem solving. This challenge requires developing critical thinking abilities which are essential in unfamiliar situations. A central component in current reforms in mathematics and science studies worldwide is the transition from the traditional dominant instruction which focuses on algorithmic cognitive skills towards higher order cognitive skills. The transition includes, a component of scientific inquiry, learning science from the student's personal, environmental and social contexts and the integration of critical thinking. The planning and implementation of learning strategies that encourage first order thinking among students is not a simple task. In an attempt to put the importance of this transition in mathematical education to a test, we propose a new method for mathematical instruction based on the infusion approach put forward by Swartz in 1992. In fact, the model is derived from two additional theories., that of Ennis (1989) and of Libermann and Tversky (2001). Union of the two latter is suggested by the infusion theory. The model consists of a learning unit (30 h hours) that focuses primarily on statistics every day life situations, and implemented in an interactive and supportive environment. It was applied to mathematically gifted youth of the Kidumatica project at Ben Gurion University. Among the instructed subjects were bidimensional charts, <b>Bayes</b> <b>law</b> and conditional probability; Critical thinking skills such as raising questions, seeking for alternatives and doubting were evaluated. We used Cornell tests (Ennis 1985) to confirm that our students developed critical thinking skills...|$|E
40|$|Multiple {{decision}} theory {{is concerned with}} those decision problems {{in which there are}} a finite number of possible decisions. The most widely used form of multiple {{decision theory}} argues that preferences among alternatives can be described by the maximization of the expected value of a numerical utility function, or equivalently, the minimization of the expected value of a loss function. Probability and statistics are usually heavily involved to represent the uncertainty of outcomes, and <b>Bayes</b> <b>Law</b> is frequently used to model the way in which new information is used to revise beliefs. ^ An important branch within multiple decision theory is ranking and selection: how to select a statistical model or population according to some pre-determined criteria; and once a selection is made, how to investigate its performance. This dissertation tries to resolve some ranking and selection problems. ^ First, a selection problem which originates from measurement error models is investigated in Chapter 2. A selection procedure is developed for selecting the treatment which has the largest regression slope, and the performance of the selection rule in terms of the probability of making a wrong decision is studied as well. In Chapter 3, a two-stage selection procedure for selecting the best Bernoulli population is studied using negative binomial sampling scheme under the Bayes and empirical Bayes framework. Then in Chapter 4, a problem for selecting the largest logistic population mean is investigated and the asymptotic optimality of the proposed selection rule is analyzed. In Chapter 5, isotonic subset selection procedures for gamma distribution family are investigated given prior information about the ordering. Chapter 6 is concerned with simultaneous selection and estimation procedures. ...|$|E
40|$|Mean-Variance optimal {{portfolios}} often {{tends to}} behave badly {{because of their}} sensitivity to movements in the variance-correlation matrix. Variance and correlation forecasting is notoriously difficult and the high sensitivity of the optimal solution to such inputs often results in extreme (corner) solutions. One possible interpretation of this phenomenon is the error-maximizing ten-dency of the optimal solution in that assets with positive pricing errors are significantly over-weighted versus those with negative errors, see Michaud (1989, 1998). Further, if there exists an asset with very low volatility rela-tive to other assets, a risk-minimizing procedure will tend to rely too much on that assets rather than diversifying across {{a wide range of}} holdings. The Black-Litterman (1992) model can help to construct stable mean-variance ef-ficient portfolios; the model was developed in Goldman Sachs in the early 90 s and provides a framework for combining subjective investors views with mar-ket (equilibrium) views. It then construct optimal portfolio weights based on a volatility/correlation matrix as in mean-variance analysis. 1. 1 Bayesian Updating In the Black-Litterman (1992) context we shall consider a framework to assess the joint likelihood of investors subjective views (or prior beliefs) and the empirical data (or model-based estimates). Therefore we can imagine that CAPM-implied equilibrium returns (based on data) can be synthesized with currently held opinions by the investment managers to form new opinions. This is a natural way of thinking since it is often the case that practitioners 2 exhibit the most strikingly different views on expected returns compared to the market consensus. Let us consider two possible events: A = expected return B = equilibrium return Using <b>Bayes</b> <b>Law</b> we can decompose the joint likelihood of A and B in the following way...|$|E
40|$|Accurate, {{reliable}} and skillful forecasting of key environmental {{variables such as}} soil moisture and snow are of paramount importance due to their strong influence on many water resources applications including flood control, agricultural production and effective water resources management which collectively control {{the behavior of the}} climate system. Soil moisture is a key state variable in land surface?atmosphere interactions affecting surface energy fluxes, runoff and the radiation balance. Snow processes also have a large influence on land-atmosphere energy exchanges due to snow high albedo, low thermal conductivity and considerable spatial and temporal variability resulting in the dramatic change on surface and ground temperature. Measurement of these two variables is possible through variety of methods using ground-based and remote sensing procedures. Remote sensing, however, holds great promise for soil moisture and snow measurements which have considerable spatial and temporal variability. Merging these measurements with hydrologic model outputs in a systematic and effective way results in an improvement of land surface model prediction. Data Assimilation provides a mechanism to combine these two sources of estimation. Much success has been attained in recent years in using data from passive microwave sensors and assimilating them into the models. This paper provides an overview of the remote sensing measurement techniques for soil moisture and snow data and describes the advances in data assimilation techniques through the ensemble filtering, mainly Ensemble Kalman filter (EnKF) and Particle filter (PF), for improving the model prediction and reducing the uncertainties involved in prediction process. It is believed that PF provides a complete representation of the probability distribution of state variables of interests (according to sequential <b>Bayes</b> <b>law)</b> and could be a strong alternative to EnKF which is subject to some limitations including the linear updating rule and assumption of jointly normal distribution of errors in state variables and observation...|$|E
40|$|THESIS 9799 Many {{problems}} in science require estimation and inference on systems that generate data over time. Such systems, quite common in statistical signal processing, {{time series analysis}} and econometrics, can be stated in a state-space form. Estimation is made {{on the state of}} the state-space model, using a sequence of noisy measurements made on the system. This difficult problem of estimating the parameters in real time, has generated a lot of interest in the statistical community, especially since the latter half of the last century. One area that is particularly important is the estimation of parameters which do not evolve over time. The parameters in the dynamic state-space model generally have a nonGaussian posterior distribution and holds a nonlinear relationship with the data. Sequential inference of these static parameters requires novel statistical techniques. Addressing the challenges of such a problem provides the focus for the research contributions presented in this thesis. A functional approximation update of the posterior distribution of the parameters is developed. The approximate posterior is explored at a sufficient number of points on a grid which is computed at good evaluation points. The grid is re-assessed at each time point for addition/reduction of grid points. <b>Bayes</b> <b>Law</b> and the structure of the state-space model are used to sequentially update the posterior density of the model parameters as new observations arrive. These approximations rely on already existing state estimation techniques such as the Kalman filter and its nonlinear extensions, as well as integrated nested Laplace approximation. However, the method is quite general and can be used for any existing state estimation algorithm. This new methodology of sequential updating makes the calculation of posterior both fast and accurate, while it can be applied to a wide class of models existing in literature. The above method is applied to three different state-space models namely, linear model with Gaussian errors, nonlinear model and model with non-Gaussian errors, and comparison with some other existing methods has been discussed...|$|E
40|$|In {{this thesis}} we address {{problems}} associated with financial modelling from a Bayesian point of view. Specifically, {{we look at the}} problem of calibrating financial models, measuring the model uncertainty of a claim and choosing an optimal hedging strategy. Throughout the study, the local volatility model is used as a working example to clarify the proposed methods. This thesis assumes a prior probability density for the unknown parameter in a model we try to calibrate. The prior probability density regularises the ill-posedness of the calibration problem. Further observations of market prices are used to update this prior, using <b>Bayes</b> <b>law,</b> and give a posterior probability density for the unknown model parameter. Resulting Bayes estimators are shown to be consistent for finite-dimensional model parameters. The posterior density is then used to compute the Bayesian model average price. In tests on local volatility models it is shown that this price is closer than the prices of comparable calibration methods to the price given by the true model. The second part of the thesis focuses on quantifying model uncertainty. Using the framework for market risk measures we propose axioms for new classes of model uncertainty measures. Similar to the market risk case, we prove representation theorems for coherent and convex model uncertainty measures. Example measures from the latter class are provided using the Bayesian posterior. These are used to value the model uncertainty for a range of financial contracts priced in the local volatility model. In the final part of the thesis we propose a method for selecting the model, from a set of candidate models, that optimises the hedging of a specified financial contract. In particular we choose the model whose corresponding price and hedge optimises some hedging performance indicator. The selection problem is solved using Bayesian loss functions to encapsulate the loss from using one model to price and hedge when the true model is a different model. Linkages are made with convex model uncertainty measures and traditional utility functions. Numerical experiments on a stochastic volatility model and the local volatility model show that the Bayesian strategy can outperform traditional strategies, especially for exotic options. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|The {{inherent}} nonlinear {{aspect of}} many practical systems and observation models is explicitly suggestive {{of the importance}} and necessity of considering the nonlinear behavior of such a system. The theoretical part of nonlinear systems and in particular nonlinear estimation and filtering has been developed through the decades of sixties and beginning part of seventies. The main obstacles in exact practical implementation of the theoretical results still remain unchallenged. But based {{on the possibility of}} realizing an adequate approximation criterion to feasibly utilize the scattered theory, this work has concentrated on least square approximation techniques as well as the Gauss-Hermite numerical integration method. The chosen trend to the estimation process in this work is a stochastic approach. In which by using a nonlinear state and observation equation and based on the <b>Bayes</b> <b>law</b> the conditional probability density is calculated. The derived recursive density equations are the pillar of the later calculations and have been used extensively thereafter. Primarily, this dissertation has been evolved around two prime purposes: (1) Considering the nonlinear aspects of image processing and detection in particular. (2) Developing a practical scheme to restore an observed image (scanned), by not ignoring the nonlinearities sustained in the system. The first intent has been fulfilled by the point that essentially main sources of nonlinearity in image processing are the scanning or recording mechanisms. The chosen observation models express the relationship between the input stimuli and the output response, and well indicate the nonlinear behaviour of the image detectors. Regarding the second goal, state of scanned image is calculated by a stochastic approach, where calculation of its conditional probability density is the prime objective of the task. One of the main difficulties in dealing with these equations is caused by their integral terms and lack of existence of an analytical approach to calculate them. This problem has been overcome by deployment of numerical integration formulas. Stochastic nonlinear filtering mechanisms being implemented in image processing, are a versatile and more general class of image processors which offer potential promise for image restoration and enhancement tasks. Particularly when the image processor is accounted for as a nonlinear entity. In such a case linear estimation methods are not likely to produce good results. However, stochastic restoration methods have been presented relatively on a limited scale, mainly due to the problems imposed by lack of proper statistical modeling techniques, as well as analytic manipulation, and implementation complexity...|$|E

