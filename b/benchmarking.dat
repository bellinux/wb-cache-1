10000|10000|Public
5|$|<b>Benchmarking</b> {{conducted}} by PassMark Software highlights the 2009 version's 52 second install time, 32 second scan time, and 7 MB memory utilization. Symantec funded the benchmark test and provided scripts used to benchmark each participating antivirus software. Tests {{were conducted in}} Windows Vista running on a dual core processor. PC Magazine found the suite added 15 seconds to the boot time, with a baseline of 60 seconds. Norton added less than 5 percent to {{the time it takes}} to complete file operations. 25 percent more time was taken to unzip and zip a set of files.|$|E
5|$|Improvements in {{structural}} alignment methods {{constitute an}} active area of research, and new or modified methods are often proposed that are claimed to offer advantages over the {{older and more}} widely distributed techniques. A recent example, TM-align, uses a novel method for weighting its distance matrix, to which standard dynamic programming is then applied. The weighting is proposed to accelerate the convergence of dynamic programming and correct for effects arising from alignment lengths. In a <b>benchmarking</b> study, TM-align {{has been reported to}} improve in both speed and accuracy over DALI and CE.|$|E
5|$|The {{government}} took responsibility for regulating {{the water and}} sanitation sector is shared between the Ministry of Health, Labor and Welfare in charge of water supply for domestic use; the Ministry of Land, Infrastructure, Transport and Tourism in charge of water resources development as well as sanitation; the Ministry of the Environment in charge of ambient water quality and environmental preservation; and the Ministry of Internal Affairs and Communications in charge of performance <b>benchmarking</b> of utilities.|$|E
50|$|The <b>benchmark</b> is a {{composite}} of results, or estimated results, from the SAP SD Standard Application Two-Tier <b>benchmark,</b> TPC-C, TPC-H, SPECjbb2005, SPECint, and SPECfp <b>benchmarks.</b> The <b>benchmark</b> was created in 2005, replacing an earlier <b>benchmark</b> called RPE.|$|R
40|$|<b>Benchmarks</b> are {{essential}} for computer architecture research and Performance evaluation. Constructing a good <b>benchmark</b> suite is, however non-trivial: it must be representative, show different types of behavior and the <b>benchmarks</b> should not be easily tweaked. This paper uses principal components analysis, a statistical data analysis technique, to detect differences in behavior between <b>benchmarks.</b> Two specific types of <b>benchmarks</b> are identified. Eccentric <b>benchmarks</b> have a behavior that differs significantly from the other <b>benchmarks.</b> They are useful to incorporate different types of behavior in a suite. Fragile <b>benchmarks</b> are weak benchmarks: their execution time is determined almost entirely by a single bottleneck. Removing that bottleneck reduces their execution time excessively. This paper argues that fragile <b>benchmarks</b> are not useful and shows {{how they can be}} detected by means of workload characterization techniques. These techniques are applied to the SPEC CPU 95 and CPU 2000 <b>benchmark</b> suites. It is shown that these suites contain both eccentric and fragile <b>benchmarks.</b> The notions of eccentric and fragile <b>benchmarks</b> are important when composing a <b>benchmark</b> suite and to guide the sub-setting of a <b>benchmark</b> suite...|$|R
50|$|The AIM Multiuser <b>Benchmark,</b> {{also called}} the AIM <b>Benchmark</b> Suite VII or AIM7, is a job {{throughput}} <b>benchmark</b> widely used by UNIX computer system vendors. Current research operating systems such as K42 use the reaim form of the <b>benchmark</b> for performance analysis.The AIM7 <b>benchmark</b> measures {{some of the same}} things as the SDET <b>benchmark.</b>|$|R
5|$|Hardware <b>benchmarking</b> {{conducted}} using Geekbench and GLBenchmark validates several claims that Apple included on their website and {{mentioned at the}} unveiling of the device, these include two times faster and two times the graphics performance. In the Geekbench overall hardware assessment, the iPhone 5 received a score that was approximately 2.5 {{times higher than the}} iPhone 4S. The benchmark {{conducted using}} GLBenchmark for the iPhone 5 returned a score that was 2 times better than the iPhone 4S. The result was however inconsistent as a 3D graphics benchmark assessment using Passmark returned a score that was only approximately 1.45 times better than the iPhone 4S. Battery life assessments conducted by AnandTech concluded that the battery life is shorter on the iPhone 5 than its predecessor when performing certain tasks, however when performing other tasks the iPhone 5 battery outlasts the iPhone 4S.|$|E
5|$|Similarly {{to other}} {{distributed}} computing projects, Folding@home quantitatively assesses user computing {{contributions to the}} project through a credit system. All units from a given protein project have uniform base credit, which is determined by <b>benchmarking</b> one or more work units from that project on an official reference machine before the project is released. Each user receives these base points for completing every work unit, though {{through the use of}} a passkey they can receive added bonus points for reliably and rapidly completing units which are more demanding computationally or have a greater scientific priority. Users may also receive credit for their work by clients on multiple machines. This point system attempts to align awarded credit with the value of the scientific results.|$|E
25|$|Many {{companies}} employ <b>benchmarking</b> {{to assess}} their CSR policy, implementation and effectiveness. <b>Benchmarking</b> involves reviewing competitor initiatives, as well as measuring and evaluating the impact that those policies have on society and the environment, and how others perceive competitor CSR strategy.|$|E
50|$|The {{most famous}} <b>benchmarks</b> are the SPECint and SPECfp <b>benchmarks</b> {{developed}} by Standard Performance Evaluation Corporation and the ConsumerMark <b>benchmark</b> {{developed by the}} Embedded Microprocessor <b>Benchmark</b> Consortium EEMBC.|$|R
40|$|This study {{examines}} whether firms just above and just below three earnings <b>benchmarks</b> (loss avoidance, earnings changes, and analyst forecast) have differing levels of discretionary accruals. If discretionary accruals are {{a measure of}} earnings management, then firms above (<b>benchmark</b> beaters) and firms below a <b>benchmark</b> should have differing levels of discretionary accruals. Dechow et al. (2003) investigate a similar question for the loss avoidance <b>benchmark</b> (earnings levels). They find that firms just above the loss avoidance <b>benchmark</b> do not have discretionary accruals that are significantly different than firms just below the <b>benchmark.</b> However, they do not consider firms just below the loss avoidance <b>benchmark</b> that might be using discretionary accruals to avoid missing an alternative <b>benchmark.</b> I find that after I remove firms with incentives to beat an alternative <b>benchmark</b> from the firms that just missed the loss avoidance <b>benchmark,</b> firms just above the <b>benchmark</b> have significantly higher discretionary accruals. I find similar results for the earnings changes and analyst forecast <b>benchmarks...</b>|$|R
5|$|Integer sorting {{provides}} {{one of the}} six <b>benchmarks</b> in the DARPA High Productivity Computing Systems Discrete Mathematics <b>benchmark</b> suite, and one {{of eleven}} <b>benchmarks</b> in the NAS Parallel <b>Benchmarks</b> suite.|$|R
25|$|Other Quantitative Data: include Geographic Data Collection, and <b>Benchmarking</b> Surveys.|$|E
25|$|In {{reaction}} to the liberalization debate the German Federal Parliament (Bundestag) passed a decision sponsored by the Green party and the Social-democrats (SPD) on sustainable {{water supply and sanitation}} (nachhaltige Wasserwirtschaft) in 2001. The decision rejected the liberalization of the water sector, but also called for the merging of smaller service providers, higher competitiveness and the general modernization of the sector, including through systematic performance <b>benchmarking.</b> In 2005, the six professional associations signed a declaration promoting <b>benchmarking,</b> based on a methodology developed by the International Water Association.|$|E
25|$|Hospitals {{and other}} {{healthcare}} facilities can upload the occupational injury data they already collect for analysis and <b>benchmarking</b> with other de-identified facilities, {{in order to}} identify and implement timely and targeted interventions.|$|E
40|$|Abstract. This paper {{presents}} an integrated database <b>benchmark</b> suite, which is named SIMS. SIMS offers generic <b>benchmarks,</b> custom <b>benchmarks,</b> and hy-brid <b>benchmarks</b> to users on a unified Web interface. Users can run <b>benchmarks</b> in realistic environments by performing the workload generation facility of SIMS, which generates composite workloads {{similar to those}} of the real world. Using SIMS, users can easily implement new custom and generic <b>benchmarks</b> in SIMS. An illustrative demonstration to add a new custom <b>benchmark</b> to SIMS is presented. ...|$|R
40|$|An {{abstract}} {{system of}} <b>benchmark</b> characteristics {{that makes it}} possible, {{in the beginning of}} the design stage, to design with <b>benchmark</b> performance in mind is presented. The <b>benchmark</b> characteristics for a set of commonly used <b>benchmarks</b> are then shown. The <b>benchmark</b> set used includes some <b>benchmarks</b> from the Systems Performance Evaluation Cooperative (SPEC). The SPEC programs are industry-standard applications that use specific inputs. Processor, memory-system, and operating-system characteristics are addressed...|$|R
40|$|This report {{presents}} potential screening <b>benchmarks</b> {{for protection}} of aquatic life form contaminants in water. Because {{there is no}} guidance for screening for <b>benchmarks,</b> a set of alternative <b>benchmarks</b> is presented herein. This report presents the alternative <b>benchmarks</b> for chemicals that have been detected on the Oak Ridge Reservation. It also presents the data {{used to calculate the}} <b>benchmarks</b> and the sources of the data. It compares the <b>benchmarks</b> and discusses their relative conservatism and utility. Also included is the updates of <b>benchmark</b> values where appropriate, new <b>benchmark</b> values, secondary sources are replaced by primary sources, and a more complete documentation of the sources and derivation of all values are presented...|$|R
25|$|<b>Benchmarking</b> {{has been}} {{undertaken}} {{for a long}} period by German utilities, but not in a comprehensive and systematic manner. In 1998 the Federal Ministry of Education and Research initiated a competition of ideas to reduce the costs of water supply together with the economic research institute RWI and 14 water utilities. It developed a set of criteria to assess strengths and weaknesses in the industry. Participating utilities say that they reduced their operating costs by about 5 percent after two to three years. The professional associations DVGW and DWA have jointly established a voluntary <b>benchmarking</b> system, which keeps individual company data confidential. The associations consider the system as being highly successful.|$|E
25|$|The {{search for}} best {{practices}} is also called <b>benchmarking.</b> This involves determining {{where you need}} to improve, finding an organization that is exceptional in this area, then studying the company and applying its best practices in your firm.|$|E
25|$|RUR {{is aimed}} to provide transparent, {{comprehensive}} analytical system for <b>benchmarking</b> and evaluation universities across the borders to the widest possible audience: students, analysts, decision-makers {{in the field}} of higher education development both at individual institutional and at the national level.|$|E
40|$|The {{purpose of}} this {{research}} report is to summarize the pricing performance of professional market advisory services for the 1995 - 2003 corn and soybean crops. First, advisory programs in corn do not consistently beat market <b>benchmarks,</b> but tend to consistently beat the farmer <b>benchmark.</b> Second, advisory programs in soybeans exhibit just the opposite pattern, consistently beating the market <b>benchmarks</b> but not the farmer <b>benchmark.</b> Third, in terms of 50 / 50 revenue, advisory programs show marginal consistency in beating both the market <b>benchmarks</b> and the farmer <b>benchmark.</b> So, the results provide mixed performance evidence with respect to both the market <b>benchmarks</b> and the farmer <b>benchmark.</b> Agricultural Finance,...|$|R
40|$|Abstract: <b>Benchmark</b> is {{a method}} of {{measuring}} performance, and we can obtain continuous per-formance improvement of programming algorithm through <b>Benchmark</b> validation. In order to solve large-scale linear programming problems, this paper proposes an integrated <b>Benchmark</b> validation which integrates theoretical <b>Benchmark</b> analysis with advance language-based <b>Benchmark.</b> Through the integrated <b>Benchmark</b> validation, we can continuously improve an optimizing algo-rithm, and validate whether the new optimizing algorithm achieves the performance objectives. The results of experiments show the proposed integrated <b>Benchmark</b> validation is an effective method for developing large-scale linear programming algorithms...|$|R
40|$|Vertical <b>benchmarks</b> {{are complex}} system designs {{represented}} at multiple levels of abstraction. More effective than componentbased CAD <b>benchmarks,</b> vertical <b>benchmarks</b> enable quantitative comparison of CAD techniques within or across design flows. This work describes {{the notion of}} vertical <b>benchmarks</b> and presents our <b>benchmark,</b> {{which is based on}} a commercial DSP, by comparing two alternative design flows. 2...|$|R
25|$|Services, {{available}} to government bodies and private developers, include measuring price movement, <b>benchmarking,</b> market research, statistical analysis, forecasting, and impact studies, relatng to construction, maintenance, rebuilding and insurance. BCIS International offers survey-based cost information for markets outside the UK.|$|E
25|$|The Division of Global Affairs (DGA) Ph.D. {{program at}} Rutgers University-Newark was ranked {{fifth in the}} nation in the <b>Benchmarking</b> Academic Excellence survey of Top Universities in Social and Behavioral Sciences Disciplines in the {{combined}} category of International Affairs and Development for 2006–07.|$|E
25|$|Since its inception, NETS has {{delivered}} its signature road safety campaign, Drive Safely Work Week (DSWW), through member companies and state partners. DSWW typically has an overarching theme, with specific topics or learning points delivered each {{day during the}} course of a week in October. As technology has evolved, the mode of delivery for DSWW materials has shifted from printed materials to electronic materials that may be downloaded from the Internet. Over the years, prevention of distracted driving has been a common theme for DSWW. In 2010 and 2011, NETS worked in partnership with the U.S. DOT to offer DSWW campaign materials on distracted driving free of charge, an arrangement that resulted in over 6,000 organizations downloading materials during 2010. One of NETS’s member services is the “Safety in Numbers” fleet safety <b>benchmarking</b> program, which provides international <b>benchmarking</b> services, with both qualitative and quantitative data from large companies representing various industry sectors. As a group, NETS member companies participating in the <b>benchmarking</b> program operate nearly half a million vehicles worldwide, with more than nine billion miles driven annually (Network of Employers for Traffic Safety, unpublished data).|$|E
50|$|The {{height of}} a <b>benchmark</b> is {{calculated}} relative to the heights of nearby <b>benchmarks</b> in a network extending from a fundamental <b>benchmark.</b> A fundamental <b>benchmark</b> is a point with a precisely known relationship to the level datum of the area, typically mean sea level. The position and height of each <b>benchmark</b> is shown on large-scale maps.|$|R
40|$|Unlike single-processor <b>benchmarks,</b> {{multiprocessor}} <b>benchmarks</b> {{can yield}} tens of numbers for each <b>benchmark</b> on each computer, as {{factors such as}} the number of processors and problem size are varied. A graphical display of performance surfaces therefore provides a satisfactory way of comparing results. The University of Southampton has developed the Graphical <b>Benchmark</b> Information Service (GBIS) on the World Wide Web (WWW) to display interactively graphs of user-selected <b>benchmark</b> results from the GENESIS and PARKBENCH <b>benchmark</b> suites...|$|R
40|$|The {{development}} of better CAD-tools for the layout, placement and routing of digital designs requires {{a huge amount}} of <b>benchmark</b> circuits to evaluate the new tools extensively. <b>Benchmarks</b> are also needed for the design of new computer hardware. Observing the lack of enough real <b>benchmark</b> designs for use in evaluation tools, one could consider to actually generate such <b>benchmarks.</b> In that case, {{it is very important that}} those generated <b>benchmarks</b> have the same characteristics as real designs. This paper describes and evaluates a new <b>benchmark</b> generation procedure that produces <b>benchmarks</b> with characteristics, similar to those of real <b>benchmark</b> designs. It will be shown that this new technique outperforms an existing method, presented by Darnauer and Dai [DD 96]...|$|R
25|$|Its {{functions}} are to regulate and supervise service providers, approve tariffs, establish norms, impose sanctions {{for violations of}} the law, and resolve user controversies and complaints. As part of its supervision activities SUNASS has established a <b>benchmarking</b> system to monitor the performance of service providers.|$|E
25|$|To perform {{stress testing}} and {{scenario}} analysis, the observed data {{needs to be}} altered, e.g. some payments delayed or removed. To analyze the levels of liquidity, initial liquidity levels are varied. System comparisons (<b>benchmarking)</b> or evaluations of new netting algorithms or rules are performed by running simulations with a fixed set of data and varying only the system setups.|$|E
25|$|The {{ability of}} Lead-Finder to {{estimate}} free energy of protein-ligand binding was benchmarked against {{the set of}} 330 diverse protein-ligand complexes, which is currently the most extensive <b>benchmarking</b> study of such kind. Lead-Finder demonstrated unique precision of binding energy prediction (RMSD = 1.5 kcal/mol) combined with high speed of calculations (less than one second per compound on average).|$|E
40|$|Software {{engineering}} frameworks {{tame the}} complexity of large collections of classes by identifying structural invariants, regularizing interfaces, and increasing sharing across the collection. We wish to appropriate these benefits for families of closely related <b>benchmarks,</b> say for evaluating query engine implementation strategies. We introduce {{the notion of a}} <b>benchmark</b> framework, an ecosystem of <b>benchmarks</b> that are related in semantically-rich ways and enabled by organizing principles. A <b>benchmark</b> framework is realized by iteratively changing one individual <b>benchmark</b> into another, say by modifying the data format, adding schema constraints, or instantiating a different workload. Paramount to our notion of <b>benchmark</b> frameworks are the ease of describing the differences between individual <b>benchmarks</b> and the utility of methods to validate the correctness of each <b>benchmark</b> component by exploiting the overarching ecosystem. As a detailed case study, we introduce τBench, a <b>benchmark</b> framework consisting of ten individual <b>benchmarks,</b> spanning XML, XQuery, XML Schema, and PSM, along with temporal extensions to each. A second case study examines the Mining Unstructured Data <b>benchmark</b> framework and a third examines the potential benefits of rendering the TPC family as a <b>benchmark</b> framework. Copyright c ○ 2012 Joh...|$|R
40|$|This paper {{proposes a}} class of {{financial}} market models with security price processes that exhibit intensity based jumps. Primary security account prices, when expressed in units of the <b>benchmark,</b> {{turn out to be}} local martingales. The <b>benchmark</b> is chosen to be the growth optimal portfolio. The proposed <b>benchmark</b> model excludes, so called, <b>benchmark</b> arbitrage but permits arbitrage amounts, which arise for <b>benchmarked</b> price processes that are strict local martingales. In the proposed framework, generally, an equivalent risk neutral measure does not exist. <b>Benchmarked</b> fair derivative prices are obtained as conditional expectations of future <b>benchmarked</b> prices under the real world probability measure...|$|R
40|$|The Dutch {{drinking}} water <b>benchmark</b> {{has been carried}} out every three years since 1997. In 2011, the <b>benchmark</b> was incorporated into the Drinking Water Act. As a result the <b>benchmark</b> switched from a voluntarily <b>benchmark</b> executed by the Association of Dutch water companies to a mandatory <b>benchmark</b> executed by the Human Environment and Transport Inspectorate. In addition, the {{drinking water}} companies have to hand in an improvement plan 6 months after the <b>benchmark</b> publication. Based on interviews with <b>benchmark</b> coordinators of drinking water companies the influence of the inclusion of the <b>benchmark</b> into the Drinking Water Act on the improvement driven part of the <b>benchmark</b> is studied. For this purpose, the following four issues are discussed: changes experienced {{as a consequence of the}} shifting from a voluntarily to a mandatory <b>benchmark,</b> influence of mandatory character of the <b>benchmark</b> on the drivers, identified by de Goede et al. (2016), the role of the improvement plans and a new instrument to stimulate improvement. The differences between the voluntarily and mandatory <b>benchmark</b> are identified: the <b>benchmark</b> switched from being only an improvement driven instrument to an instrument for accountability as well and the drinking water organizations lost control over the development of the <b>benchmark.</b> The influence of the mandatory character on the drivers for performance improvement have been determined: only the driver 'enhanced transparency' is (positively) influenced. In order for the improvement plans to be able to have a positive effect on the stimulation of improvement, a feedback system should be implemented and the publications of the <b>benchmark</b> and improvement plans have to be faster. The <b>benchmark</b> is still thought to be useful, although an adaptive <b>benchmark</b> could stimulate improvement again. Additional thesi...|$|R
