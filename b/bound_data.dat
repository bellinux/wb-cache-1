11|876|Public
5000|$|WPF has a {{flexible}} data binding system. UI elements can be populated and synchronized {{with data from}} an underlying data model. Rather than showing simple text for the <b>bound</b> <b>data</b> WPF can apply a Data Template (replaceable UI for [...]NET types) before rendering to the Visual Tree.|$|E
5000|$|Data Model: the {{description}} of the data types bound with elements of the interface. At runtime, modifying the state of an interactor will change also the value of the <b>bound</b> <b>data</b> element and vice versa, in order to describe dynamic UI changes (correlation between UI elements, conditional layout, conditional connections between presentations, input values format etc.). The data model is defined using the standard XML Schema Definition constructs.|$|E
50|$|From {{its defense}} and space businesses, Boeing rely on model-based systems {{engineering}} (MBSE), defining customer needs and functionality {{early in the}} aircraft design process with an interdisciplinary approach.A systems architecture model feeds and interacts with analytic and verification models, and helps define the product to <b>bound</b> <b>data</b> management and control cost and schedule, and the constraints, interfaces and requirements.The engine integration defines takeoff and climb capability, aircraft noise and ETOPS range circumference and engine failure altitude.|$|E
40|$|AbstractWe {{study the}} blow up or global {{existence}} of the solutions of the Cauchy problem for 2 × 2 one-dimensional first order semilinear strictly hyperbolic systems with homogeneous quadratic interaction. Two characterizations are obtained: global existence for locally <b>bounded</b> <b>data,</b> global existence for small <b>bounded</b> <b>data</b> with compact support...|$|R
40|$|Abstract—We {{develop a}} new linear {{estimator}} for estimating an unknown parameter vector x in a linear model {{in the presence of}} <b>bounded</b> <b>data</b> uncertainties. The estimator is designed to minimize the worst-case regret over all <b>bounded</b> <b>data</b> vectors, namely, the worst-case difference between the mean-squared error (MSE) attainable using a linear estimator that does not know the true parameters x and the optimal MSE attained using a linear estimator that knows x. We demonstrate through several examples that the minimax regret estimator can significantly increase the performance over the conventional least-squares estimator, as well as several other least-squares alternatives. Index Terms—Deterministic parameter estimation, linear estimation, mean squared error <b>bounded</b> <b>data</b> uncertainties estimation, minimax estimation, regret. I...|$|R
40|$|We {{develop a}} new linear {{estimator}} for estimating an unknown vector x in a linear model, {{in the presence of}} <b>bounded</b> <b>data</b> uncertainties. The estimator is designed to minimize the worst-case regret across all <b>bounded</b> <b>data</b> vectors, namely the worst-case difference between the MSE attainable using a linear estimator that does not know the true parameters x, and the optimal MSE attained using a linear estimator that knows x. We demonstrate through several examples that the minimax regret estimator can significantly increase the performance over the conventional least-squares estimator, as well as several other least-squares alternatives. 1...|$|R
40|$|Abstract. In this paper,we {{propose a}} {{pioneering}} work on designing and programming B&B algorithms on GPU. To {{the best of}} our knowledge, no contribution has been proposed to raise such challenge. We focus on the parallel evaluation of the bounds for the Flow-shop scheduling problem. To deal with thread divergence caused by the bounding operation, we investigate two software based approaches called thread data reordering and branch refactoring. Experiments reported that parallel evaluation of bounds speeds up execution up to 54. 5 times compared to a CPU version. Keywords: Branch and <b>Bound,</b> <b>Data</b> Parallelism, GPU Computing, Thread Divergence, Flow-shop Schedulin...|$|E
40|$|The paper {{addresses}} a novel method for realtime analysis of systems with cyclic data flows. The presented method {{is based on}} Network Calculus principles, where upper and lower flow and service constraint are used to <b>bound</b> <b>data</b> flows and processing resources. In acyclic systems flow constraints may be propagated through a system of service models along data flow paths, whereas service constraints are propagated along paths of priority order. For cyclic data flows however constraint propagation becomes equivalently cyclic leading to flow and service constraints implicitely given by a fix point equation in a space of constraint functions. In this paper a method denoted CyNC for obtaining a well defined solution to that problem is presented along with a theoretical justification of the method as well as comparative results for CyNC and alternative methods on a relevant example. The method is implemented in a prototype tool also denoted CyNC providing a graphical user interface for model specification based on the MATLAB/SimuLink framework...|$|E
40|$|When {{multiple}} outputs {{and multiple}} inputs are imprecise data such as bounded data, ordinal data or ratio <b>bound</b> <b>data,</b> the standard linear {{data envelopment analysis}} (DEA) model becomes a nonlinear and is called imprecise DEA (IDEA) which can either be converted into a linear program by scale transformations and variable alternations, or solved using the standard DEA model by converting imprecise data into a set of exact data. The current paper investigates the working mechanism of IDEA and shows alternative ways to convert ordinal data into a set of exact data. It is shown that (i) the original IDEA — multiplier IDEA (MIDEA) which is developed from the multiplier DEA model presents the best efficiency scenario, and (ii) the primal IDEA (PIDEA) which is developed from the primal DEA model presents the worst efficiency scenario. The nonlinear PIDEA can also be easily executed by the standard linear DEA models based upon a set of derived exact data whereas it cannot be converted into a linear program via scale transformations and variable alternations. Data envelopment analysis (DEA), efficiency, imprecise data...|$|E
40|$|For {{analyzing}} {{positive or}} <b>bounded</b> <b>data,</b> this paper suggests parametrically transformed nested error regression models (TNERM), which transform the data flexibly {{to follow the}} normal linear mixed regression. As useful transformations, we consider the dual power transformation for positive data and the dual power logistic transformation newly proposed for <b>bounded</b> <b>data.</b> We provide a procedure for estimating consistently {{the parameters of the}} proposed model and a predictor based on the consistent estimators. Then, in order to calibrate uncertainty of the transformed empirical best linear unbiased predictor, we derive both unconditional and conditional prediction intervals with second-order accuracy based on the parametric bootstrap method. The proposed methods are investigated through simulation and empirical studies...|$|R
40|$|Abstract: We {{propose a}} new {{algebraic}} framework for exception handling which is {{powerful enough to}} cope with many exception handling features such that recovery, implicit propagation of exceptions, etc. This formalism is capable of treating all the exceptional cases, including the fol-lowing ones: “intrinsic ” exceptions which {{are related to the}} underlying data structure (for instance, popping an emptstack or applying predecessor on zero for natural numbers), exceptions which are relied on “dynamic ” properties (as an acces to a non-initialized array cell) or else exceptions which are due to certain limitations (mainly <b>bounded</b> <b>data</b> structures). We show that within the already existing frameworks, the case of <b>bounded</b> <b>data</b> structures with certain recoveries of exceptional values remains unsolved. First, we justify the usefulness of “labelling ” some terms in order to easily specify exceptions without inconsistency, and we then define a general framework of label algebras which allows us to “type ” terms instead of values. Exception algebras and exception specifications are defined as a direct application of label algebras. Indeed, the usual inconsistency problems raised by exception handling are avoided by the possibility of labelling terms. As a conclusion, we also sketch out how far the application domain of label algebras seems to be much more general than exception handling. Key-words: Algebraic specifications of abstract data types, Error and exception handling, Exception recovery, <b>Bounded</b> <b>data</b> structures, Structured specifications. ...|$|R
40|$|Estimation of {{the maximum}} and minimum is {{considered}} in a random coefficient autoregressive model for <b>bounded</b> <b>data.</b> Limiting distributions and confidence intervals are obtained, for nonrandom sample sizes and also for a stopping rule designed to achieve sufficient precision of the estimates. Beta autoregressive process Strong mixing Asymptotic independence Precise estimation Stopping rule...|$|R
40|$|Interval {{arithmetic}} {{has been}} found to be useful in numerical analysis as an automatic means to <b>bound</b> <b>data,</b> truncation, and roundoff errors in computations. Now that the speed of microprogrammed interval arithmetic approaches that of standard floating-point operations, a wider range of application to engineering and other problems has become feasible. Since, in many practical situations, data are only known to lie within intervals and only ranges of values are sought as satisfactory answers, straightforward interval computation can yield the desired results. Examples of this type of application are worst-case analysis of the stability of structures and the performance of electrical circuits. The recently developed theory of integration of interval functions also bears directly on the problems of solution of integral equations and the minimization of functionals defined in terms of integrals. Since certain chaotic phenomena, such as catastrophes and turbulence, are difficult to describe by single-valued functions, the introduction of interval functions and the corresponding analysis may lead to simpler models which will yield results of accuracy satisfactory for practical purposes...|$|E
40|$|The {{purpose of}} this study is to {{describe}} how cultural identity perceptions of Latino high school seniors are influenced by participating in AVID or Upward Bound programs, specifically focusing on how identity perceptions are effected by each program's goals and future expectations for student achievement beyond high school. And each programs treatment of cultural heritage and community during implementation. Two data sets were surveyed, one for AVID and One group for Upward <b>Bound.</b> <b>Data</b> was collected using written surveys that asked students to comment on several dimensions of personal self perception and experiences in their respective program, either AVID or Upward Bound. Results were analyzed using four main themes: Life Goals, Perceptions of Self and Identity, Being Bilingual in School, Perceptions of Cultural Heritage. Student responses indicate that these programs are including issues of cultural identity in the curriculum by supporting native language, incorporating the cultural communities with parent involvement, and celebrating cultural heritage to some degree. Responses also indicate that program participation does effect perceptions of self as a student, life goals, and student views of their cultural community...|$|E
40|$|Real-Time Database Management Systems (DBMSs) {{attempt to}} reach the double {{objective}} of maintaining both database logical and temporal consistency while attempting to enhance concurrency in transaction execution. This objective being difficult to reach and since some real-time applications need not to hardly respect this objective, some proposed work attempts to bring new concepts for relaxing transactions and/or data properties in real-time context. In this paper, we first classify real-time applications according to data fluctuations and/or deadlines fluctuations. In these applications, the key problem is to efficiently schedule real-time transactions that allow to manipulate inaccurate data and/or that have deadline overtaken. We then introduce two notions to materialize these fluctuations and we show that multimedia applications are the best candidate {{to the use of}} these notions which are tightly connected to QoS. Then we propose transactions concurrency-control and scheduling protocols that allow to <b>bound</b> <b>data</b> imprecision while allowing some transactions to execute beyond their deadlines by a bounded time quantity. These algorithms are based on the extended Earliest Deadline algorithm which cooperates with the transaction manager {{in order to deal with}} transactions concurrency control. 1...|$|E
40|$|The authors {{study the}} {{asymptotic}} behaviour of solutions {{of the heat}} equation {{and a number of}} evolution equations using scaling techniques. It is proved that in the framework of <b>bounded</b> <b>data</b> stabilization need not occur and the general asymptotic behaviour is complex. This behaviour reflects for large times, even on compact sets, the complexity of the initial data at infinity...|$|R
40|$|This work {{focuses on}} the use of {{truncated}} Gaussian distributions as models for <b>bounded</b> <b>data</b> [...] measurements that are constrained to appear between fixed limits. We prove that the truncated Gaussian {{can be viewed as a}} maximum entropy distribution for truncated <b>bounded</b> <b>data,</b> when mean and covariance are given. We present the characteristic function for the truncated Gaussian; from this, we derive algorithms for calculation of mean, variance, summation, application of Bayes rule and filtering with truncated Gaussians. As an example of the power of our methods, we describe a derivation of the disparity constraint (used in computer vision) from our models. Our approach complements results in Statistics, but our proposal is not only to use the truncated Gaussian as a model for selected data; we propose to model measurements as fundamentally bounded in terms of truncated Gaussians. 1 Introduction This work presents a new class of statistical models that are well suited for several Robotics [...] ...|$|R
40|$|We obtain Schauder {{estimates}} {{for a class}} of concave fully nonlinear nonlocal parabolic equations of order σ∈ (0, 2) with rough and non-symmetric kernels. As a application, we prove that the solution to a translation invariant equation with merely <b>bounded</b> <b>data</b> is C^σ in x variable and Λ^ 1 in t variable, where Λ^ 1 is the Zygmund space. Comment: 41 pages, added Remark 1. 4 and the proof of Corollary 1. 3. Accepted in CVPD...|$|R
40|$|Abstract. Designing {{external}} memory data {{structures for}} string databases is of significant recent interest {{due to the}} proliferation of biological sequence data. The suffix tree is an important indexing structure that provides optimal algorithms for memory <b>bound</b> <b>data.</b> However, string Btrees provide the best known asymptotic performance in external memory for substring search and update operations. Work on external memory variants of suffix trees has largely focused on constructing suffix trees in external memory or layout schemes for suffix trees that preserve link locality. In this paper, we present a new suffix tree layout scheme for secondary storage and present construction, substring search, insertion and deletion algorithms that are competitive with the string B-tree. For a set of strings of total length n, a pattern p and disk blocks of size B, we provide a substring search algorithm that uses O(|p|/B +log B n) disk accesses. We present algorithms for insertion and deletion of all suffixes {{of a string of}} length m that take O(m log B (n + m)) and O(m log B n) disk accesses, respectively. Our results demonstrate that suffix trees can be directly used as efficient secondary storage data structures for string and sequence data. ...|$|E
40|$|INTRODUCTION: There is an {{increasing}} interest in recent developments in bioartificial and non-bioartificial devices, so called extracorporeal liver assist devices, which are now used widely not only to increase drug elimination, but also to enhance the removal of endogenous substances in acute liver failure. Most of the non-bioartificial techniques {{are based on the}} principle of albumin dialysis. The objective is to remove albumin-bound substances that could {{play a role in the}} pathophysiology of acute liver failure by dialysing blood against an albumin-containing solution across a high flux permeable membrane. The most widely used device is the Molecular Adsorbent Recirculating System (MARS™). METHODS: The relevant English and French literature was identified through Medline using the terms, 'molecular adsorbent recirculating system', 'MARS', 'acute liver failure', 'acute poisoning', 'intoxication'. This search identified 139 papers of which 48 reported on a toxic cause for the use of MARS™. Of these 48 papers, 39 specified the substance (eighteen different substances were identified); two papers reported on the same group of patients. BIOARTIFICIAL AND NON-BIOARTIFICIAL SYSTEMS: Bioartificial systems based on porcine hepatocytes incorporated in the extracorporeal circuit are no longer in use due to the possibility of porcine retroviral transmission to humans. Historically, experience with such devices was limited to a few cases of paracetamol poisoning. In contrast, an abundant literature exists for the non-bioartificial systems based on albumin dialysis. The MARS™ has been used more widely than other techniques, such as the one using fractionated plasma separation and adsorption (Prometheus™). All the extracorporeal liver assist devices are able to some extent to remove biological substances (ammonia, urea, creatinine, bilirubin, bile acids, amino acids, cytokines, vasoactive agents) but the real impact on the patient's clinical course has still to be determined. Improvement in cardiovascular or neurological dysfunction has been shown both in acute liver failure and acute-on-chronic liver failure but no impact on mortality has been reported. ACUTE POISONING WITH LIVER FAILURE: Randomized controlled trials are very limited in number and patients poisoned by paracetamol or Amanita phalloides are usually included for outcome analysis in larger groups of acute liver failure patients. Initial results look promising but should be confirmed. Beyond its effect in liver failure, MARS™ could also enhance the elimination of the drug or toxin responsible for the failure, as is described with paracetamol. ACUTE POISONING WITHOUT LIVER FAILURE: Extracorporeal liver assist devices have also been used to promote elimination of drugs that are highly protein <b>bound.</b> <b>Data</b> in various case reports confirm a high elimination of phenytoin, theophylline and diltiazem. However, definite conclusions on the toxicokinetic or clinical efficacy cannot be drawn. CONCLUSIONS: Despite the lack of large multicentre randomized trials on the use of MARS™ in patients with acute liver failure, the literature shows clinical and biological benefit from this technique. In drug or toxin-induced acute liver failure, such as paracetamol or mushroom poisoning, MARS™ has been used extensively, confirming in a non-randomized fashion, the positive effect observed in the larger population of acute liver failure patients. Furthermore, as MARS™ has been shown in experimental studies to remove protein-bound substances, it is potentially a promising treatment for patients with acute poisoning from drugs that have high protein-binding capacity and are metabolized by the liver, especially, if they develop liver failure concomitantly...|$|E
50|$|Apache Flink {{includes}} two core APIs: a DataStream API for bounded or unbounded streams {{of data and}} a DataSet API for <b>bounded</b> <b>data</b> sets. Flink also offers a Table API, which is a SQL-like expression language for relational stream and batch processing {{that can be easily}} embedded in Flink’s DataStream and DataSet APIs. The highest-level language supported by Flink is SQL, which is semantically similar to the Table API and represents programs as SQL query expressions.|$|R
40|$|Much of the {{programming}} done today requires data access. With {{large amounts of}} data being maintained in databases, access mechanisms that allow manipulation of this data are vital. Visual Basic is a popular programming package because of its user-friendly interface and data access features. Two popular data access mechanisms are <b>data</b> <b>bound</b> controls with <b>data</b> access objects, and remote data controls with remote data objects. The main difference between these methods are that <b>data</b> <b>bound</b> controls ar...|$|R
40|$|A new {{algorithm}} is developed, which guarantees the normalized {{bias in the}} weight vector due to persistent and <b>bounded</b> <b>data</b> perturbations to be bounded, Robustness analysis for this algorithm has been presented, An approximate recursive implementation is also proposed. It is termed as the robust recursive least squares (RRLS) algorithm since it resembles the RLS: algorithm in its structure and is robust with respect to persistent hounded data perturbation. Simulation results are presented to illustrate {{the efficacy of the}} RRLS algorithm...|$|R
40|$|Strip packing is a {{classical}} packing problem, where {{the goal is}} to pack a set of rectangular objects into a strip of a given width, while minimizing the total height of the packing. The problem has multiple applications, e. g. in scheduling and stock-cutting, and has been studied extensively. When the dimensions of objects are allowed to be exponential in the total input size, it is known that the problem cannot be approximated within a factor better than 3 / 2, unless P=NP. However, there was no corresponding lower bound for polynomially <b>bounded</b> input <b>data.</b> In fact, Nadiradze and Wiese [SODA 2016] have recently proposed a (1. 4 + ϵ) approximation algorithm for this variant, thus showing that strip packing with polynomially <b>bounded</b> <b>data</b> can be approximated better than when exponentially large values in the input data are allowed. Their result has subsequently been improved to a (4 / 3 + ϵ) approximation by two independent research groups [FSTTCS 2016, arXiv: 1610. 04430]. This raises a question whether strip packing with polynomially <b>bounded</b> input <b>data</b> admits a quasi-polynomial time approximation scheme, as is the case for related two-dimensional packing problems like maximum independent set of rectangles or two-dimensional knapsack. In this paper we answer this question in negative by proving that it is NP-hard to approximate strip packing within a factor better than 12 / 11, even when admitting only polynomially <b>bounded</b> input <b>data.</b> In particular, this shows that the strip packing problem admits no quasi-polynomial time approximation scheme, unless NP⊆DTIME(2 ^polylog(n)) ...|$|R
40|$|Data Envelopment Analysis (DEA) is a nonparametric {{method for}} {{identifying}} sources and estimating the mount of inefficiencies contained in {{inputs and outputs}} produced by Decision Making Units (DMUs). DEA requires that the data for all inputs and outputs should be known exactly, but under many qualifications, exact data are inadequate to model real-life situations. So these data may have different structures such as <b>bounded</b> <b>data,</b> interval data, and fuzzy data. Moreover, the main assumption in all DEA is that input and output values are positive, but we confront many cases that discount this condition producing negative data. The {{purpose of this paper}} is to compute efficiency for DMUs, which permits the presence of intervals which can take both negative and positive values...|$|R
40|$|Abstract: In the this paper, {{the authors}} propose to {{estimate}} the density of a targeted population with a weighted kernel density estimator (wKDE) based on a weighted sample. Bandwidth selection for wKDE is discussed. Three mean integrated squared error based bandwidth estimators are introduced and their performance is illustrated via Monte Carlo simulation. The least-squares cross-validation method and the adaptive weight kernel density estimator are also studied. The authors also consider the boundary problem for interval <b>bounded</b> <b>data</b> and apply the new method to a real data set subject to informative censoring...|$|R
40|$|Hierarchical {{representation}} of images {{is a crucial}} building principle of a system architecture that can cope with the complexity of visual perception. This paper presents an algorithm that builds a hierarchy which is flexible enough to provide a shift and scale invariant abstract {{representation of}} image content. The hierarchy is irregular {{in the sense that}} an element of the representation may have an arbitrary number of neighbors. At the same time, the representation can be described by a <b>bounded</b> <b>data</b> structure. Main properties of the algorithm with respect to visual perception are proved...|$|R
40|$|This paper {{builds on}} recent {{research}} {{that focuses on}} regression modeling of continuous <b>bounded</b> <b>data,</b> such as proportions measured on a continuous scale. Specifically, it deals with beta regression models with mixed effects from a Bayesian approach. We use a suitable parameterization of the beta law {{in terms of its}} mean and a precision parameter, and allow both parameters to be modeled through regression structures that may involve fixed and random effects. Specification of prior distributions is discussed, computational implementation via Gibbs sampling is provided, and illustrative examples are presented...|$|R
40|$|We {{discuss a}} {{numerical}} schema for solving the initial value {{problem for the}} Korteweg-de Vries equation in the soliton region {{which is based on}} a new method of evaluation of <b>bound</b> state <b>data.</b> Using a step-like approximation of the initial profile and a fragmentation principle for the scattering data, we obtain an explicit procedure for computing the <b>bound</b> state <b>data.</b> Our method demonstrates an improved accuracy on discontinuous initial data. We also discuss some generalizations of this algorithm and how it might be improved by using Haar and other wavelets...|$|R
40|$|The {{paper is}} {{concerned}} with the existence of solutions to an integro-differential problem arising in the neutron transport theory. By an anisotropic singular perturbations method we show that solutions of such a problem exist and are close to those of some nonlocal elliptic problem. The existence of the solutions of the nonlocal elliptic problem with <b>bounded</b> <b>data</b> is ensured by the Schauder fixed point theorem. Then an asymptotic method is applied in the general case. The limits of the solutions of the nonlocal elliptic problems are solutions of our integro-differential problem...|$|R
40|$|Regression {{function}} estimation {{from independent}} and identically distributed <b>bounded</b> <b>data</b> is considered. The L 2 error with integration {{with respect to}} the design measure is used as an error criterion. It is shown that the kernel regression estimate with an arbitrary random bandwidth is weakly and strongly consistent for all distributions whenever the random bandwidth is chosen from some deterministic interval whose upper and lower bounds satisfy the usual conditions used to prove consistency of the kernel estimate for deterministic bandwidths. Choosing discrete bandwidths by cross-validation allows to weaken the conditions on the bandwidths...|$|R
40|$|Abstract. We {{develop a}} new {{technique}} for proving cell-probe lower <b>bounds</b> on dynamic <b>data</b> structures. This enables us to prove Ω(lg n) bounds, breaking a long-standing barrier of Ω(lg n/lg lg n). We can also prove the first Ω(lgB n) lower bound in the external memory model, without assumptions on the data structure. We use our technique to prove better bounds for the partial-sums problem, dynamic connectivity and (by reductions) other dynamic graph problems. Our proofs are surprisingly simple and clean. The bounds we obtain are often optimal, and lead to a nearly complete understanding of the problems. We also present new matching upper bounds for the partial-sums problem. Key words. cell-probe complexity, lower <b>bounds,</b> <b>data</b> structures, dynamic graph problems, partial-sums problem AMS subject classification. 68 Q 1...|$|R
3000|$|... is a decreasing, positive, {{continuous}} function, {{then every}} nontrivial positive classical solution of (3) exists globally {{in time for}} any nontrivial positive <b>bounded</b> initial <b>data.</b> Furthermore, the solution [...]...|$|R
40|$|We {{show that}} entropy {{solutions}} to 1 dimensional scalar conservation laws for totally nonlinear fluxes and for arbitrary measurable <b>bounded</b> <b>data</b> have a structure {{similar to the}} one of BV maps without being always BV. The singular set-shock waves- of such solutions is contained in a countable union of C¹ curves and H¹ almost everywhere along these curves the solution has left and right approximate limits. The entropy production is concentrated on the shock waves and can be explicitly computed in terms of the approximate limits. The solution is approximately continuous H¹ almost everywhere outside this union of curves...|$|R
40|$|AbstractMin–Max {{optimization}} {{is often}} used for improving robustness in Model Predictive Control (MPC). An analogy to this optimization could be the BDU (<b>Bounded</b> <b>Data</b> Uncertainties) method, which is a regularization technique for least-squares problems {{that takes into account}} the uncertainty bounds. Stability of MPC can be achieved by using terminal constraints, such as in the CRHPC (Constrained Receding-Horizon Predictive Control) algorithm. By combining both BDU and CRHPC methods, a robust and stable MPC is obtained, which is the aim of this work. BDU also offers a guided method of tuning the empirically tuned penalization parameter for the control effort in MPC...|$|R
40|$|AbstractEstimation of {{regression}} functions {{from independent}} and identically distributed data is considered. The L 2 error with integration {{with respect to}} the design measure is used as an error criterion. Usually in the analysis of the rate of convergence of estimates besides smoothness assumptions on the regression function and moment conditions on Y also boundedness assumptions on X are made. In this article we consider partitioning and nearest neighbor estimates and show that by replacing the boundedness assumption on X by a proper moment condition the same rate of convergence can be shown as for <b>bounded</b> <b>data...</b>|$|R
