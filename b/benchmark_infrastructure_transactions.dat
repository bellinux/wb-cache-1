0|126|Public
5000|$|Cyril Cabanes: Director. Also, Vice-President, Head of <b>Infrastructure</b> <b>Transactions,</b> Asia-Pacific at Caisse de dépôt et {{placement}} du Québec (CDPQ) ...|$|R
5000|$|Significant energy <b>infrastructure</b> <b>transactions</b> include K-Electric, a {{vertically}} integrated power utility {{based in}} Pakistan. In 2015, Abraaj and India’s Aditya Birla Group announced a partnership to develop 1 GW of solar {{power in the}} country.|$|R
40|$|Background: Research {{work on the}} {{automatic}} extraction of information about mutations from texts is greatly hindered {{by the lack of}} consensus evaluation facilities and easy-to-use infrastructure for testing and benchmarking of mutation text mining systems. Results: We propose a community-oriented annotation and <b>benchmarking</b> <b>infrastructure</b> to support development, testing, benchmarking, and comparison of mutation text mining systems. The design is based on semantic standards, where RDF is used to represent the annotations, an OWL ontology provides an extensible schema for the data and SPARQL is used to compute various performance metrics, so that in many cases programming is not needed to analyze system results. While large benchmark corpora for biological entity and relation extraction are focused mostly on gene, proteins, diseases, and species, our <b>benchmarking</b> <b>infrastructure</b> fills the gap for mutation information. The core infrastructure comprises of: 1) an ontology for modelling annotations, 2) SPARQL queries for performance metrics computation, and 3) a sizeable collection of manually curated documents, that can minimally support mutation grounding and mutation impact extraction. Conclusion: This is the first example of <b>benchmarking</b> <b>infrastructure</b> for mutation text mining. It is designed for community uptake. ...|$|R
40|$|Linked data {{technologies}} provide advantages {{in terms of}} interoperability and integration, which, in certain cases, come {{at the cost of}} performance. The Web Observatory, a global Web Science research project, is providing a <b>benchmark</b> <b>infrastructure</b> to understand and address the challenges of analytics on distributed Linked Data infrastructures...|$|R
30|$|With the {{traditional}} data-management <b>infrastructure,</b> <b>transactions</b> between a party (e.g., {{an individual or}} organization) and others often are recorded and stored in data repositories maintained and controlled by the particular party or by central authorities (e.g., government agencies, banks). The integrity of the data, to a large extent, depend on the data management capability and functionality of the repository.|$|R
40|$|Appropriate {{benchmark}} {{design is}} a requirement for assessing and improving bioinformatics methods and tools, and needs of best practices to pass over from raw data to valuable knowledge for decision-making. In order to build a continuous automated <b>benchmarking</b> <b>infrastructure</b> which hosts different - emerging or existing - benchmark efforts, bioinformatics tools and data-types, {{it is crucial to}} discuss storage, analysis, comparison and sharing of large heterogeneous data sets. Finally, the most appropriate format to communicate results by either exposing them to third parties resources e. g. tools registries, and/or directly via a web-portal. The increasing complexity of the constantly growing body of biological data e. g. unstructured description of resources, non-standardized input and output formats, lack of appropriate metadata and/or deprecated software source codes, represents a tremendous challenge. Thus, we must develop new technical solutions or adapt existing ones to leverage existing data and to better prepare for future challenges around the scientific, technical and functional evaluation of bioinformatics methods and tools. Deliverable 2. 1. defines the data warehouse infrastructure needed to host different benchmark initiatives from a broad range of bioinformatics fields, provides a reference implementation, and oversee the overall ELIXIR Tools and Data Services Registry integration and operation. On this report we will examine the following aspects of a data warehouse: 	Background about the need of an automated <b>benchmark</b> <b>infrastructure.</b> 	An architecture overview with an emphasis in the data warehouse as central component. 	An explanation about the database warehouse design and implementation. 	A final review about future works {{in the context of the}} <b>benchmark</b> <b>infrastructure...</b>|$|R
50|$|The {{authority}} was formed of 1 July 2010 when it assumed {{the functions of}} Brisbane Water, a government business enterprise that was owned and managed by the Brisbane City Council, together with the merging of water assets from the four other member local government authorities. At the time, the 4.3 bn merger was the largest water transaction and second largest <b>infrastructure</b> <b>transaction</b> in Australian history.|$|R
40|$|After four {{successful}} JUnit tool competitions, {{we report}} on the achievements of a new Java Unit Testing Tool Competition. This 5 th contest introduces statistical analyses in the <b>benchmark</b> <b>infrastructure</b> and has been validated with significance against the results of the previous 4 th edition. Overall, the competition evaluates four automated JUnit testing tools taking as baseline human written test cases from real projects. The paper details the modifications performed to the methodology and provides full results of the competition...|$|R
40|$|The International Power Institute, in {{collaboration}} with American industries, seeks to address technical, political, economic and cultural issues of developing countries {{in the interest of}} facilitating profitable transactions in power related infrastructure projects. IPI works with universities, governments and commercial organizations to render project-specific recommendations for private-sector investment considerations. IPI also established the following goals: Facilitate electric power <b>infrastructure</b> <b>transactions</b> between developing countries and the US power industry; Collaborate with developing countries to identify development strategies to achieve energy stability; and Encourage market driven solutions and work collaboratively with other international trade energy, technology and banking organizations...|$|R
5000|$|... 20090112657 - Repository <b>infrastructure</b> {{to store}} <b>transaction</b> {{information}} for providing customer service ...|$|R
40|$|This {{paper is}} about the {{alignment}} of technology and modes of organization in infrastructures {{in the context of}} their reform. Since infrastructures are characterized by strong technical complementarities, we explore the resulting 'critical technical functions' that need to be performed in order to guarantee the expected technical performance of the system. We characterize 'critical transactions' as essential to provide adequate support to these functions. We distinguish various modes of organization that can effectively coordinate these critical transactions. We argue that the features of these transactions determine the alignment between organization and technology and should be taken explicitly into account when reforming <b>infrastructures.</b> <b>Transaction</b> costs; technology; institutional change; reforms of infrastructures...|$|R
40|$|Abstract — In {{this work}} {{we report on}} data {{gathered}} via a deployment of a monitoring and <b>benchmarking</b> <b>infrastructure</b> on two production grid platforms, TeraGrid and Geon. Our result show that these production grids are rather unavailable, with success rates for benchmark and application runs between 55 % and 80 %. We also found that performance fluctuation was in the 50 % range, expectedly mostly due to batch schedulers. We also investigate whether the execution time of a typical grid application can be predicated based on previous runs of simple benchmarks. Perhaps surprisingly, we find that application execution time can be predicted with a relative error as low as 9 %. I...|$|R
40|$|Over {{the last}} five years the VLSI Placement {{community}} achieved great strides in the understanding of placement problems, developed new high-performance algorithms, and achieved impressive empirical results. These advances have been supported by non-trivial <b>benchmarking</b> <b>infrastructure,</b> and future achievements are set to draw on benchmarking as well. In this paper we review motivations for benchmarking, especially for commercial EDA, analyze available benchmarks, and point out major pitfalls in benchmarking. Our empirical data offers perhaps the first comprehensive evaluation of several leading large-scale placers on multiple benchmark families. We outline major outstanding problems and discuss the future of placement benchmarking. Furthermore, we attempt to extrapolate our experience to circuit layout tasks beyond placement...|$|R
5000|$|The {{following}} aspects need to {{be addressed}} to offer a secure <b>infrastructure</b> for financial <b>transaction</b> over wireless network : ...|$|R
40|$|In {{this article}} we discuss the {{relationships}} between transportation infrastructure, firm location, agglomeration and regional development. We {{will argue that the}} spatial transaction costs faced by modern firms have changed over recent decades, and that this has changed the ways in which transportation infrastructure contributes to form location behaviour and regional economic development. Therefore, in order to analyse these issues, it is necessary to consider the spatial transaction costs faced by modern firms and to investigate the conditions under which reductions in these costs due to infrastructure improvements will allow firms to move. These complex relationships are seen to be mediated via different geography-firm-organisation structures and consideration of these is essential for any realistic evaluation of the role of transportation infrastructure. Copyright Springer-Verlag Berlin/Heidelberg 2003 Location theory, agglomeration, <b>infrastructure,</b> <b>transaction</b> costs,...|$|R
40|$|Benchmarking {{is a key}} {{activity}} in building and tuning data management systems, {{but the lack of}} reference workloads and a common platform makes it a time consuming and painful task. The need for such a tool is heightened with the advent of cloud computing— with its pay-per-use cost models, shared multi-tenant infrastructures, and lack of control on system configuration. Benchmarking is the only avenue for users to validate the quality of service they receive and to optimize their deployments for performance and resource utilization. In this talk, we present our experience in building several adhoc <b>benchmarking</b> <b>infrastructures</b> for various research projects targeting several OLTP DBMSs, ranging from traditional relational databases, main-memory distributed systems, and cloud-based scalable architectures. We also discuss our struggle to build meaningful micro-benchmarks and gather workloads representative o...|$|R
40|$|This study {{examines}} the portfolio-diversification benefits of listed infrastructure stocks. We employ three different definitions of listed infrastructure and tests of mean–variance spanning. The {{evidence shows that}} viewing infrastructure as an asset class is misguided. We employ different schemes of infrastructure asset selection (both traditional asset classes and factor exposures) and discover {{that they do not}} provide portfolio-diversification benefits to existing asset allocation choices. We also find that defining and selecting infrastructure investments by business model as opposed to industrial sectors can reveal a very different investment profile, albeit one that improves the mean–variance efficient frontier since the global financial crisis. This study provides new insights into defining and <b>benchmarking</b> <b>infrastructure</b> equity investments in general, as well as into the extent to which public markets can be used to proxy the risk-adjusted performance of privately held infrastructure investments...|$|R
50|$|Prior to {{joining the}} MPEA, Healey {{served as the}} Chief Executive Officer of Tur Partners LLC ("Tur") Tur, a firm founded by Chicagos former mayor Richard M. Daley. Healey also served as Principal in Charge of the Development Group for the John Buck Company, focused on growing the firms private real estate and <b>infrastructure</b> related <b>transactions.</b>|$|R
40|$|International audienceThis paper {{describes}} the AllWrite platform which innovates {{in the field}} of handwriting recognition system benchmarking. The AllWrite architecture provides the handwriting recognition community with a common <b>benchmarking</b> <b>infrastructure</b> designed (1) to host the training/test data sets together with their related ground truth, (2) to compute the metrics used for the objective performance evaluation, (3) to support any recognition system via an open and easy-to-use API, and (4) to render the evaluation outputs. Taking advantage of the RIMES project experience, AllWrite goes one step beyond, offering several advantages. It ensures an on-line, on-demand independent and fair evaluation. It solves the over-training problem. It makes automatic the system run and the result analysis phase hence saving a large amount of human resources. If AllWrite is dedicated to researchers, it is also well-suited for the on-line evaluation of any industrial system, the later remaining agnostic to and independent of the whole AllWrite software complexit...|$|R
40|$|Energy costs {{comprise}} a significant {{fraction of the}} total cost of ownership of a large supercomputer. As with performance, energy-efficiency is not an attribute of a compute resource alone; it is a function of a resource-workload combination. The operation mix and locality characteristics of the applications in the workload affect the energy consumption of the resource. Our experiments confirm that data locality is the primary source of variation in energy requirements. The major contributions of this work include a method for performing fine-grained power measurements on high performance computing (HPC) resources, a <b>benchmark</b> <b>infrastructure</b> that exercises specific portions of the node in order to characterize operation energy costs, and a method of combining application information with independent energy measurements in order to estimate the energy requirements for specific application-resource pairings. A verification study using the NAS parallel benchmarks and S 3 D shows that our model has an average prediction error of 7. 4 %...|$|R
50|$|As of June 2009, Marketcetera are in {{partnership}} with NYSE Technologies (the commercial technology division of NYSE Euronext) to provide a 'Software-as-a-Service' (SaaS) Trading Platform on NYSE Technologies’ SFTI (Secure Financial <b>Transaction</b> <b>Infrastructure)</b> Network.|$|R
40|$|This paper {{analyzes}} {{the effect of}} foreign aid on export diversification for a sample of developing countries while controlling {{for the effects of}} other factors that determine export diversification. We find that foreign aid not exceeding 20 % of a country's GDP significantly promotes export diversification, while foreign aid in excess of 20 % of GDP significantly impedes export diversification. The latter result corroborates evidence from related literature, which has shown that foreign aid can have an anti-export bias due to a Dutch disease effect. However, our results show that aid as a percent of GDP is below 20 % in most low-income countries. This implies that in many low-income countries, varying amounts of additional aid can be used to enhance export diversification without causing a Dutch disease effect. As in the previous literature, we find that the level of development, <b>infrastructure,</b> <b>transactions</b> costs, and natural resources significantly affect export diversification. Our results are robust to the use of two different export diversification measures and different sub-samples. foreign aid, export diversification, developing countries,...|$|R
40|$|In this paper, we {{consider}} {{a region that}} invests in infrastructure used by both local demand and through traffic. We then compare transport systems that have, for a given capacity, the same total infrastructure cost but vary {{in the proportion of}} fixed costs and variable capacity costs. We show that, compared to a <b>benchmark</b> <b>infrastructure</b> which has zero fixed costs, an infrastructure which has (ceteris paribus) a higher share of fixed costs leads to higher welfare for the regional government building it. Moreover, we find that, even for capacity characterized by very high shares of fixed costs, financing of infrastructure is generally not an important issue as long as regions are allowed to toll through traffic. Finally, we show that, compared to the benchmark technology without fixed costs, regions are more likely to make a particular transport investment if they can chose a technology with higher fixed costs. As a corollary, projects with a higher share of fixed costs do not necessarily require higher federal subsidies. Capacity cost structure Cost recovery Transport investment...|$|R
40|$|Sampling-based {{planning}} algorithms are {{the most}} common probabilistically complete algorithms and are widely used on many robot platforms. Within this class of algorithms, many variants have been proposed over the last 20 years, yet there is still no characterization of which algorithms are well-suited for which classes of problems. This has motivated us to develop a <b>benchmarking</b> <b>infrastructure</b> for motion planning algorithms. It consists of three main components. First, we have created an extensive benchmarking software framework that is included with the Open Motion Planning Library (OMPL), a C++ library that contains implementations of many sampling-based algorithms. Second, we have defined extensible formats for storing benchmark results. The formats are fairly straightforward so that other planning libraries could easily produce compatible output. Finally, we have created an interactive, versatile visualization tool for compact presentation of collected benchmark data. The tool and underlying database facilitate the analysis of performance across benchmark problems and planners. Comment: Submitted to IEEE Robotics & Automation Magazine (Special Issue on Replicable and Measurable Robotics Research), 201...|$|R
50|$|In April 2010, Thompson {{joined the}} {{investment}} banking firm of Siebert Brandford Shank & Co., a lead underwriter of and corporate municipal bonds, serving as Chief Administrative Officer and Managing Director. In November 2015, Thompson joined the firm's ownership group as an equity partner. It is now the largest majority- owned African-American, woman and Latino owned {{investment banking firm}} in the country, having handled more than $2 trillion in <b>infrastructure</b> financing <b>transactions.</b>|$|R
40|$|Resource sharing {{occurs when}} {{multiple}} active processes or software components compete for system resources, which influences the observed performance {{compared to an}} individual execution. Isolated benchmarking of durations of key operations for solving of performance prediction models may therefore yield imprecise results. Resource sharing also occurs between the measured code and the <b>benchmark</b> <b>infrastructure</b> for obtaining and storing samples, imposing an indirect overhead. This thesis quantifies the effects of sharing on performance for several resources that are often shared, namely the processor caches and the file systems. The highest possible performance impact of cache sharing is determined by synthetic benchmarks. Impact on practical code and its dependency {{on a number of}} factors such as cache trashing frequency and intensity are then determined by experiments with existing implementations of FFT and LZW algorithms and a video stream processing application. Effects of file system sharing are measured by experiments that read and write multiple files simultaneously. For both resources, situations with significant performance impact of sharing have been observed. Based on the results of the experiments, several suggestions for dealing with the overhead of performance monitoring infrastructure are [...] ...|$|R
40|$|Infrastructure enhances {{economic}} {{competitiveness and}} represents {{an important source}} of economic empowerment and revenue. At a global level there is increasing demand for infrastructure and as the demand and supply gap widens, the financing of infrastructure is becoming increasingly more complex. The desire to remain competitive in the global market has seen governments increasingly seeking to expand the role of the private sector in the financing and delivery of quality infrastructure resources. This study focuses on investors' attraction to infrastructure investment and the evolving role of the private sector in delivering infrastructure, bringing into perspective key vehicles in the financing of infrastructure resources across regions, countries and sectors. The uniqueness of the study stems from the timely assessment {{of the impact of the}} global financial crisis on <b>infrastructure</b> <b>transactions</b> and performance comparing infrastructure investment returns with other asset classes across global, European and UK markets. The study employs a quantitative research approach drawing time series data from two distinctive databases in order to fully explore infrastructure investments from both transaction based activities within the industry and the comparative performance of the asset class. Capital flows and trends in <b>infrastructure</b> <b>transactions</b> across geographical locations and sectors are examined and time series data used in the analysis of infrastructure returns over a ten year period (2001 - 2010). This facilitated the analyses of a broad range of listed infrastructure investment return characteristics. The data obtained were employed in the construction of efficient portfolio frontiers computed with the aid of an optimization tool. The study highlights the debt driven nature of greenfield projects offering investors a more attractive route into the infrastructure market. The oil & gas, power, transport and social infrastructure sectors attracted the highest investor funds within the infrastructure investment space. The role of the private sector within the infrastructure industry is evolving, as reflected in the growing acceptance of the PPP model and in the flexibility of options available across geographical markets. The impact of the global financial crisis (GFC) on infrastructure delivery from the transaction and performance perspectives was characterized by declining capital flows, resulting in a dwindling credit profile for the infrastructure markets. The resilience of infrastructure as a unique asset class was demonstrated during the GFC period by the superior performance shown by European generation utilities and UK infrastructure. The study shows that infrastructure plays a significant role within a mixed asset portfolio by enhancing diversification benefits; a unique investment strategy sought by investors to enhance investment performance. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|This paper {{constructs}} a two-region {{endogenous growth}} model where industrial location and public infrastructures {{play a key}} role. The model analyses the contribution {{of different types of}} public policies on growth, economic geography and spatial income distribution. It implies that an improvement in <b>infrastructures</b> that reduces <b>transaction</b> costs inside the poorest region decreases both the spatial concentration of industries, and the growth rate, and increases the income gap between the two regions. Conversely, an improvement in <b>infrastructure</b> facilitating <b>transactions</b> between regions has the reverse effect. In this sense, the paper highlights a trade-off between growth and the spatial distribution of economic activities. Contrary to transfers and traditional regional policies, it is shown that public policies that reduce the cost of innovation can attain the objectives of higher growth and more even spatial distribution of both income and economic activities. From that point of view, these policies seem preferable to the regional policies now implemented in Europe...|$|R
40|$|This project {{developed}} benchmarks and benchmarking {{procedures to}} indicate the current and future infrastructure requirements of the aged in regional centres. The <b>benchmarks</b> correlate the <b>infrastructure</b> requirements of the aged with demographic trends. The research also modelled aged infrastructure requirements within the broader context of the sustainability of cities...|$|R
40|$|Benchmarking is an {{essential}} aspect of any database management system (DBMS) effort. Despite several recent advancements, such as pre-configured cloud database images and database-as-a-service (DBaaS) offerings, the deployment of a comprehensive testing platform with a diverse set of datasets and workloads is still far from being trivial. In many cases, researchers and developers are limited to {{a small number of}} workloads to evaluate the performance characteristics of their work. This is {{due to the lack of}} a universal <b>benchmarking</b> <b>infrastructure,</b> and to the difficulty of gaining access to real data and workloads. This results in lots of unnecessary engineering efforts and makes the performance evaluation results difficult to compare. To remedy these problems, we present OLTP-Bench, an extensible “batteries included ” DBMS benchmarking testbed. The key contributions of OLTP-Bench are its ease of use and extensibility, support for tight control of transaction mixtures, request rates, and access distributions over time, as well as the ability to support all major DBMSs and DBaaS platforms. Moreover, it is bundled with fifteen workloads that all differ in complexity and system demands, including four synthetic workloads, eight workloads from popular benchmarks, and three workloads that are derived from real-world applications. We demonstrate through a comprehensive set of experiments conducted on popular DBMS and DBaaS offerings the different features provided by OLTP-Bench and the effectiveness of our testbed in characterizing the performance of database services. 1...|$|R
50|$|Alerian, {{based in}} Dallas, TX, is an {{independent}} company that provides transparent master limited partnerships (MLP) and energy <b>infrastructure</b> <b>benchmarks</b> and analytics. The flagship Alerian MLP Index (AMZ) is widely used by industry professionals to analyze relative performance. Its vision is to equip investors {{to make informed decisions}} about MLPs and energy infrastructure.|$|R
30|$|Electronic {{markets are}} often {{discussed}} {{in terms of}} their transformation power and can be considered a convergence of the market towards a perfect market [13]. With respect to higher accessibility, lower entry barriers, and their ubiquity, electronic markets carry a high advantage over traditional markets [31]. Given their higher transparency, electronic markets are usually attributed an improved allocation coordination [31]. These advancements give electronic markets an advantage over traditional forms of market organization. Especially transaction cost theory asserts that by implementing an electronic <b>infrastructure</b> the <b>transaction</b> costs become negligible, improving the competition and almost completing the conversion towards a perfect market [34].|$|R
40|$|This work {{analyzes}} and systematizes {{the effects}} that the increasing amount of cargo import and export of the European ports of Hamburg, Bremen, Bremerhaven and Koper have on the traffic in the hinterland <b>infrastructure.</b> Growing <b>transactions</b> at the ports, {{as well as the}} increase of cargo transport distribution has {{had a profound effect on}} the cargo traffic and infrastructure of Germany, Slovenia, Czech Republik, Slovak Republik, Hungary and Austria as well as other countries whose highways and rail tracks are being employed daily by transactions originating at these sea ports. ( [...] .) (author's abstract) Series: Schriftenreihe des Instituts für Transportwirtschaft und Logistik - Verkeh...|$|R
40|$|Early in 2016, an {{environmental}} scan {{was conducted by}} the Research Library Data Working Group for three purposes: 1.) Perform {{a survey of the}} data management landscape at Los Alamos National Laboratory in order to identify local gaps in data management services. 2.) Conduct {{an environmental}} scan of external institutions to <b>benchmark</b> budgets, <b>infrastructure,</b> and personnel dedicated to data management. 3.) Draft a research data infrastructure model that aligns with the current workflow and classification restrictions at Los Alamos National Laboratory. This report is a summary of those activities and the draft for a pilot data management project. Comment: 13 pages, repor...|$|R
50|$|There {{are still}} {{work to be}} done to build {{inclusive}} financial systems. This includes taking advantage of the technological advances in developing financial <b>infrastructure</b> to lower <b>transaction</b> costs, encouraging transparency, openness and competition to incentivize current institutions to expand service coverage, and enforcing prudential regulations in order to provide the private sector with the right incentives.|$|R
40|$|From {{news reports}} about {{companies}} attempting {{to reduce the}} impact of compromised supply chains, due to natural disasters, accidents or targeted attacks, or trying to avoid specific products or ingredients banned on moral grounds, {{it is apparent that}} many organizations have only rudimentary knowledge of the provenance of software, hardware, and other supplied items. Reasons for this situation include the difficulty and effort required to: build and maintain complete and accurate databases; obtain information on subcontractors down to the required level of detail; review, monitor and test products to ensure that they are genuine; encourage eradication of deficiencies, weaknesses, and vulnerabilities; ensure that changes are identified, reported, analyzed, and addressed; identify commonalities and common points of failure; introduce resiliency, redundancy, and backup within the supply chain; develop methods to simulate <b>infrastructures,</b> <b>transactions,</b> etc.; and bring together competitors to collaborate in exercising various scenarios. Thus, the question arises as to how to resolve these issues in an accurate, efficient, and cost-effective manner. Answering this question is our goal. Supply-chain models are generally substantially more intricate than the model developed for the US equities marketplace. However, the same approach works for developing and operating any complex industry-wide and sector-wide systems with many participants who want to keep proprietary information confidential but need to share information to facilitate a rich exercise experience for learning, training, and testing a variety of realistic scenarios. This paper describes a process for implementing such simulation-based exercises...|$|R
