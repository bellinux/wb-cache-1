40|0|Public
2500|$|<b>Backing-off</b> commences. This {{process is}} the {{unwinding}} of the several turns of the yarn, extending {{from the top of}} the cop in process of formation to the summit of the spindle. As this proceeds, the faller- wire, which is placed over and guides the threads upon the cop, is depressed the counter-faller at the same time rising, the slack unwound from the spindles is taken up, and the threads are prevented from running into snarls. <b>Backing-off</b> is completed.|$|E
50|$|For recovery, some systems simply cause senders to re-transmit {{collided}} data (perhaps with <b>backing-off</b> algorithms which {{reduce the}} sender's re-transmit rate when collisions keep occurring) or use Error Correction {{techniques such as}} FEC.|$|E
40|$|In {{this paper}} we propose to {{introduce}} <b>backing-off</b> in the acoustic contributions of the local distance functions used during Viterbi decoding as an operationalisation of missing feature theory for increased recognition robustness. Acoustic <b>backing-off</b> effectively removes the detrimental influence of outlier values from the local decisions in the Viterbi algorithm. It does so {{without the need for}} prior knowledge that specific features are missing. Acoustic <b>backing-off</b> avoids any kind of explicit outlier detection. This paper provides a proof of concept of acoustic <b>backing-off</b> in the context of connected digit recognition over the telephone, using artificial distortions of the acoustic observations. It is shown that the word error rate can be maintained at the level of 2 : 5 % obtained for undisturbed features, even in the case where a conventional local distance computation without <b>backing-off</b> leads to a word error rate ? 80 : 0 %. The approach appears {{to be able to handle}} up to four independe [...] ...|$|E
40|$|Acoustic <b>backing-off</b> was {{recently}} proposed as an operationalisation of missing feature theory for increased recognition robustness. Acoustic <b>backing-off</b> effectively removes the detrimental influence of outlier values {{from the local}} decisions in the Viterbi algorithm without any kind of explicit outlier detection. In the context of connected digit recognition over telephone lines, it is shown that with more than 30 % of the static mel-frequency cepstral coefficients disturbed, acoustic <b>backing-off</b> is capable of reducing the word error rate by one order of magnitude. Furthermore, our {{results indicate that the}} effectiveness of acoustic <b>backing-off</b> is optimal when dispersion of distortions due to acoustic feature transformations is minimal. 1. INTRODUCTION Recently, it was shown that missing feature theory can be used for improved robustness of automatic speech recognition (ASR) systems [1], [2]. According to missing feature theory, recognition performance in adverse conditions can be mai [...] ...|$|E
40|$|In this paper, {{we discuss}} {{acoustic}} <b>backing-off</b> {{as a method}} to improve automatic speech recognition robustness. Acoustic <b>backing-off</b> aims {{to achieve the same}} objective as the marginalization approach of Missing Feature Theory: The detrimental influence of outlier values is effectively removed from the local distance computation in the Viterbi algorithm. The proposed method is based on one of the principles of Robust Statistical Pattern Matching: During recognition the local distance function is modeled using a mixture of the distribution observed during training and a distribution describing observations not previously seen. In order to asses the effectiveness of the new method we used artificial distortions of the acoustic vectors in connected digit recognition over telephone lines. We found that acoustic <b>backing-off</b> is capable of restoring recognition performance almost to the level observed for the undisturbed features, even in cases where a conventional local distance function completely fails. These results show that recognition robustness can be improved using a marginalization approach where making the distinction between reliable and corrupted feature values is wired into the recognition process. In addition, the results show that application of acoustic <b>backing-off</b> is not limited to feature representations based on filter bank outputs...|$|E
40|$|The {{most popular}} tools for {{generating}} the gears are hobs, whose materials are the High Speed Steel (HSS) {{in the many}} case. Because the HSS contains such the rare components as W, Mo, Co and V, the material cost is expensive. A sequence for manufacturing the hobs consists of (1) hob-thread roughing, (2) flute milling (3) normalizing the body, (4) <b>backing-off</b> hob thread, (5) quenching and tempering and (6) grinding. Thus the sequence is very complicated and the machinability of HSS also is very poor due to the alloy elements above mentioned. Therefore the manufacture cost is higher too. In this paper the cost-down methods are developed in rough threading the large-sized hobs {{and the availability of}} these methods are checked by exprimenting with two typical end mills on the <b>backing-off</b> lathe. In order to decrease the rough machining times, the above operations (1) and (4) among the above operations are simultaneously worked on the <b>backing-off</b> lathe. Thus, in case of module 20 hob, the rough threading and backing time 33 hours required in the ordinary way can be shortened up to 24 hours. Through these results, it is cleared that the threading in backing with the tapered surfy end mill is hopeful for decreasing the manufacture prices...|$|E
40|$|In {{this work}} we present two new {{approaches}} to the well known <b>backing-off</b> method used for n-grams model smoothing. Both methods use contexts not usually addressed in language models. In the first method preceding history of a word is used, as well as posterior words (future context). In the second method unseen n-grams are estimated by log-linear interpolation between distance- 1 -(n − 1) grams. Kulback-Leibler distance maximization between true and estimated probability is used to obtain efficient recursion of interpolation parameters. In both cases significant improvements over traditional <b>backing-off</b> methods are obtained. We show perplexity results over spontaneous speech and read newspaper text. We also evaluate performance over an N-best re-scoring task using switchboard corpora. While paper discussion is limited to trigram models, {{it is not difficult}} to generalize results to n-grams. 1...|$|E
40|$|In {{this paper}} we {{investigate}} acoustic <b>backing-off</b> as an operationalization of Missing Feature Theory to increase recognition robustness in adverse acoustic conditions. Acoustic <b>backing-off</b> effectively removes the detrimental influence of outlier values {{from the local}} decisions in the Viterbi algorithm. It does so without prior knowledge about the specific feature vector elements which are unreliable; thus, the technique avoids the need for explicit outlier detection. From the theory underlying Missing Feature Theory it appears that acoustic feature representations which smear local spectro-temporal distortions over all feature vector elements are inherently unsuitable. Our experiments {{in the context of}} connected digit recognition over the telephone are presented that confirm this prediction. Our results show that feature representations which minimize distortion smearing are most suited to be used in combination with Missing Feature Theory. Using additive band limited noise as a distor [...] ...|$|E
40|$|Freely {{available}} {{software packages}} {{for the training}} of Conditional Random Fields, e. g. CRF++, do not support longer n-grams than bigram, which {{can be attributed to}} the fact that training CRFs in original notation has a polynomial computation time dependence on the target vocabulary size and an exponential dependence on the n-gram size. We transfer the <b>backing-off</b> idea from language models to CRFs. We realized the software with Finite State Transducers, where we modified the calculation of the posterior algorithm. To implement the <b>backing-off</b> scheme, we applied Failure-transitions(φ) known from Open-FST. Proof of concept is given on the semantic tagging task ME-DIA and on the grapheme-to-phoneme (G 2 P) conversion tasks NETtalk and Celex, showing that computational time increases much below the size of the target vocabulary and showing error rate reduction on the G 2 P tasks...|$|E
40|$|In this paper, {{we propose}} a new method for {{computing}} and applying language model look-ahead in a dynamic network decoder, exploiting the sparseness of <b>backing-off</b> n-gram language models. Only partial (sparse) look-ahead tables are computed, {{with a size}} {{that depends on the}} number of words that have an n-gram score in the language model for a specific context, rather than a constant, vocabulary dependent size. Since high order <b>backing-off</b> language models are inherently sparse, this mechanism reduces the runtime- and memory effort of computing the look-ahead tables by magnitudes. A modified decoding algorithm is required to apply these sparse LM look-ahead tables efficiently. We show that sparse LM look-ahead is much more efficient than the classical method, and that full n-gram look-ahead becomes favorable over lower order look-ahead even when many distinct LM contexts appear during decoding. Index Terms — speech, recognition, decoding, search, language model, look-ahea...|$|E
40|$|Passage {{retrieval}} is {{an essential}} part of question answering systems. In this paper we use statistical language models to perform this task. Previous work has shown that language modeling techniques provide better results for both, document and passage retrieval. The motivation behind this paper is to define new smoothing methods for passage retrieval in question answering systems. The final objective is to improve the quality of question answering systems to isolate the correct answer by choosing and evaluating the appropriate section of a document. In this work we use a three step approach. The first two steps are standard document and passage retrieval using the Lemur toolkit. As a novel contribution we propose as the third step a re-ranking using dedicated <b>backing-off</b> distributions. In particular <b>backing-off</b> from the passage-based language model to a language model trained on the document from which the passage is taken shows a significant improvement. For a TREC question answering task we can increase the mean average precision from 0. 127 t...|$|E
40|$|In {{this paper}} we {{investigate}} acoustic <b>backing-off</b> as an operationalization of Missing Feature Theory {{with the aim}} to increase recognition robustness. Acoustic <b>backing-off</b> effectively diminishes the detrimental influence of outlier values by using {{a new model of}} the probability density function of the feature values. The technique avoids the need for explicit outlier detection. Situations that are handled best by Missing Feature Theory are those where only part of the coefficients are disturbed {{and the rest of the}} vector is unaffected. Consequently, one may predict that acoustic feature representations that smear local spectrotemporal distortions over all feature vector elements are inherently less suitable for automatic speech recognition. Our experiments seem to confirm this prediction. Using additive band limited noise as a distortion and comparing four different types of feature representations, we found that the best recognition performance is obtained with recognizers that use acoustic backingoff and that operate on feature types that minimally smear the distortion...|$|E
40|$|This {{research}} aims {{to incorporate}} argument status-based modelling within an otherwise selectional constraint-based system of verb sense disambiguation, to capture e#ects such as underspecification, surface case alternation and semantic <b>backing-off.</b> The proposed implementation hinges around {{a description of}} the general behavioural characteristics of integral complements, complements, middles and adjuncts through a pre-determined weighting schema. On limited evaluation, the resultant system returned an accuracy of over 83 %, and was further shown to significantly outperform baseline methods...|$|E
40|$|In {{this paper}} we {{investigate}} statistical language models with avariable context length. For such models {{the number of}} relevantwords in a context is not fixed as in conventional M-gram models but depends on the context itself. We develop a measure {{for the quality of}} variable-length models and present a pruning algorithm for the creation of such models, based on this measure. Further we address the question how the use of a special <b>backing-off</b> distribution can improve the language models. Experiments were performed [...] ...|$|E
40|$|Abstract. In a {{previous}} work, a new probabilistic context-free grammar (PCFG) model for natural language parsing {{derived from a}} tree bank corpus has been introduced. The model estimates the probabilities according to a generalized k-grammar scheme for trees. It allows for faster parsing, decreases considerably the perplexity of the test samples and tends to give more structured and refined parses. However, it suffers from the problem of incomplete coverage. In this paper, we compare several smoothing techniques such as <b>backing-off</b> or interpolation {{that are used to}} avoid assigning zero probability to any sentence. ...|$|E
30|$|For instance, when a {{particular}} trigram {{does not occur}} (a certain number of times) in the data, the model may consider probabilities given by the lemma of the previous word L_t- 1 and the part-of-speech of the word before that P_t- 2, and that these probabilities should be combined in {{a particular}} way to estimate the overall probability of the current word W_t. Thus, even the simplest FLMs allow a large number of possible backoff paths, making directed backoff graphs as discussed in [8]. As we shall see, this may have significant advantages over simply <b>backing-off</b> down to the bigram or unigram level.|$|E
40|$|Abstract—Due {{to higher}} peak-to-average power ratio and higher power <b>backing-off</b> in OFDM {{relaying}} systems, a cyclic prefixed single-carrier cooperative diversity system is considered in this paper. Under the transmission power constraint, the joint optimal power allocation {{to the source}} and relay node is in-vestigated. For a two-hop decode-and-forward relaying protocol, the optimal power allocation is first obtained in the considered system, and then the opportunistic destination terminal selection is applied to improve the achievable average spectral efficiency (ASE). Based on the proposed QR decomposition (QRD) -based receiver in the destination terminal, closed-form expressions for the maximum achievable ASE are derived. Simulation results verify the derived closed-form analytical expressions. I...|$|E
40|$|In this paper, we {{describe}} some techniques to learn probabilistic k-testable tree models, a generalization {{of the well}} known k-gram models, {{that can be used}} to compress or classify structured data. These models are easy to infer from samples and allow for incremental updates. Moreover, as shown here, <b>backing-off</b> schemes can be defined to solve data sparseness, a problem that often arises when using trees to represent the data. These features make them suitable to compress structured data files at a better rate than string-based methods. The Spanish Comisión Interministerial de Ciencia y Tecnología through Grants TIC 2003 - 08681 -C 02 and TIC 2003 - 08496 -C 04...|$|E
40|$|An {{analysis}} method {{was developed to}} study the impact of training-test mismatch due {{to the presence of}} additive noise. The contributions of individual observation vector components to the emission cost are determined in the matched and mismatched condition and histograms are computed for these contributions in each condition. Subsequently, a measure of mismatch is defined based on differences between the two histograms. By means of two illustrative experiments it is shown to what extent this emission cost mismatch measure can be used to identify the features that cause the most important mismatch and how in certain cases this type of information may be helpful to increase recognition accuracy by applying acoustic <b>backing-off</b> to selected features only. Some limitations of the approach are also discussed...|$|E
40|$|This paper {{discusses}} {{various aspects}} of smoothing techniques in maximum entropy language modeling, a topic not sufficiently covered by previous publications. We show (1) that straightforward maximum entropy models with nested features, e. g. tri-, bi-, and unigrams, result in unsmoothed relative frequencies models; (2) that maximum entropy models with nested features and discounted feature counts approximate <b>backing-off</b> smoothed relative frequencies models with Kneser's advanced marginal back-off distribution; this explains some of the reported success of maximum entropy models in the past; (3) perplexity results for nested and non-nested features, e. g. trigrams and distance-trigrams, on a 4 -million word subset of the Wall Street Journal Corpus, showing that the smoothing method has more effect on the perplexity than the method to combine information...|$|E
30|$|While the PAPR problem, {{inevitable}} in the downlink, can be coped with by using highly linear (and thus expensive) HPAs for example, {{this is a}} much more sensitive issue in the uplink. Mobile users strive for good coverage and good autonomy handsets, but do not neglect the associated costs. On one hand, <b>backing-off</b> the uplink signal level to the linear region of the HPA would reduce the coverage. On the other hand, using highly linear HPAs would increase the handset cost. For these reasons, the uplink physical layer of LTE [8] was chosen to be a precoded OFDMA air interface, called single-carrier frequency division multiple access (SC-FDMA). The precoder is a discrete fourier transform (DFT), which restores the low envelope fluctuations of single-carrier (SC) systems [9, 10]. But SC-FDMA may lose its low-PAPR property in MIMO systems if no precaution is taken.|$|E
40|$|Factorization of {{statistical}} language models {{is the task}} that we resolve the most discriminative model into factored models and determine a new model by combining them so as to provide better estimate. Most of previous works mainly focus on factorizing models of sequential events, each of which allows only one factorization manner. To enable parallel factorization, which allows a model event to be resolved {{in more than one}} ways at the same time, we propose a general framework, where we adopt a backingoff lattice to reflect parallel factorizations and to define the paths along which a model is resolved into factored models, we use a mixture model to combine parallel paths in the lattice, and generalize Katz’s <b>backing-off</b> method to integrate all the mixture models got by traversing the entire lattice. Based on this framework, we formulate two types of model factorizations that are used in natural language modeling. ...|$|E
40|$|This paper {{studies the}} overall effect of {{language}} modeling on perplexity and word error rate, starting from a trigram model with a standard smoothing method up to complex state [...] of [...] the [...] art language models: (1) We compare different smoothing methods, namely linear vs. absolute discounting, interpolation vs. <b>backing-off,</b> and back-off functions based on relative frequencies vs. singleton events. (2) We show the effect of complex language model techniques by using distant-trigrams and automatically selected word classes and word phrases using a maximum likelihood criterion (i. e. minimum perplexity). (3) We show the overall gain of the combined application of the above techniques, as opposed to their separate assessment in past publications. (4) We give perplexity and word error rate results on the North American Business corpus (NAB) with a training text of about 240 million words and on the German Verbmobil corpus...|$|E
40|$|Nonlinearities can {{drastically}} {{degrade the}} performance of OFDM (orthogonal frequency division multiplexing) based optical wireless (OW) communication systems using intensity modulation (IM) of the optical carrier. The light emitting diode (LED) transfer function distorts the signal amplitude and forces the lower signal peaks to be clipped at the LED turn-on voltage (TOV). Additionally, the upper signal peaks can result in optical output degradation. The induced distortion can be controlled by optimizing the bias point (BP) of the LED and/or <b>backing-off</b> the signal power modulating the LED. In this paper, the obtained experimental results using a hardware demonstrator for OW OFDM transmission based on field-programmable gate array (FPGA) and off-the-shelf analog components are presented. The conducted measurements for the bit-error performance focus on determining the optimum BP and optimizing the OFDM signal amplitude to obtain best performance. In this context, the experimental bit-error ratio (BER) is obtained {{as a function of}} the LED BP and the RMS (root mean square) OFDM signal across the LED. © 2011 IEEE...|$|E
40|$|Abstract For {{a dynamic}} network based large {{vocabulary}} continuous speech recognizer, this paper proposes a fast language model (LM) look-ahead method using extended N-gram model. The extended N-gram model unifies the rep-resentations and score computations of the LM and the LM look-ahead tree, and thus greatly simplifies the decoder implementation and improves the LM look-ahead speed significantly, which makes higher-order LM look-ahead possible. The extended N-gram model is generated off-line before decoding starts. The generation procedure {{makes use of}} sparse-ness of <b>backing-off</b> N-gram models for efficient look-ahead score computation, and uses word-end node pushing and score quantitation to compact the model′s storage space. Experiments showed that with the same character error rate, the proposed method speeded up the overall recognition speed {{by a factor of}} 5 ∼ 9 than the traditional dynamic program-ming method which computes LM look-ahead scores on-line during the decoding process, and that using higher-order LM look-ahead algorithm can achieve a faster decoding speed and better accuracy than using the lower-order look-ahead ones...|$|E
40|$|Centralized Radio Access Network (C-RAN) {{is a new}} {{paradigm}} for wireless networks that centralizes the signal processing in a computing cloud, allowing commodity computational resources to be pooled. While C-RAN improves utilization and efficiency, the computational load occasionally exceeds the available resources, creating a computational outage. This paper provides a mathematical characterization of the computational outage probability for low-density parity check (LDPC) codes, a common class of error-correcting codes. For tractability, a binary erasures channel is assumed. Using the concept of density evolution, the computational demand is determined for a given ensemble of codes {{as a function of}} the erasure probability. The analysis reveals a trade-off: aggressively signaling at a high rate stresses the computing pool, while conservatively <b>backing-off</b> the rate can avoid computational outages. Motivated by this trade-off, an effective computationally aware scheduling algorithm is developed that balances demands for high throughput and low outage rates. Comment: Conference on Information Sciences and Systems (CISS) 2017, to appea...|$|E
40|$|This PhD thesis {{studies the}} overall effect of {{statistical}} language modeling on perplexity and word error rate in automatic speech recognition. A trigram language model with a standard smoothing method is extended by complex state-of-the-art language modeling techniques, including: the comparison of different smoothing methods, namely linear vs. absolute discounting, interpolation vs. <b>backing-off,</b> and back-off functions based on relative frequencies vs. singleton events, the effect of complex language model techniques by using distant trigrams and word classes, word phrases and varigrams that are automatically selected using a maximum likelihood criterion (i. e. minimum perplexity), the overall gain of the combined application of the above techniques, as opposed to their separate assessment in past publications, and perplexity results on the Wall Street Journal corpus and perplexity and word error rate results for both the North American Business corpus (NAB) with a training text of about 240 million words and the German Verbmobil corpus...|$|E
40|$|Stochastic Language Models (LMs) are key for {{achieving}} good performance in speech recognition systems. This {{is confirmed by}} the numerous LMs that have been proposed recently in the literature. This work compares three different LMs within the robot telecontrol speech understanding system developed at IRST. 1. 1 Introduction This work compares three different class-based bigram LMs inside an Automatic Speech Understanding (ASU) system developed at IRST, which provides a voice interface to a robot telecontrol station [1] 2. The LMs considered are the naive one, simply based on conditional frequencies, the recently proposed <b>backing-off</b> model by Placeway et al. [7] and the interpolated model by Derouault and Merialdo [5]. Robustness of each LM was evaluated against increasing sparseness of the training data, by augmenting the number of word classes assigned to the vocabulary. This issue is crucial for applications in which only small text corpora are available. Results are given in ter [...] ...|$|E
40|$|Abstract—Nonlinearities can {{drastically}} {{degrade the}} perfor-mance of OFDM (orthogonal frequency division multiplexing) based optical wireless (OW) communication systems using inten-sity modulation (IM) of the optical carrier. The {{light emitting diode}} (LED) transfer function distorts the signal amplitude and forces the lower signal peaks to be clipped at the LED turn-on voltage (TOV). Additionally, the upper signal peaks can result in optical output degradation. The induced distortion can be controlled by optimizing the bias point (BP) of the LED and/or <b>backing-off</b> the signal power modulating the LED. In this paper, the obtained experimental results using a hardware demonstrator for OW OFDM transmission based on field-programmable gate array (FPGA) and off-the-shelf analog components are presented. The conducted measurements for the bit-error performance focus on determining the optimum BP and optimizing the OFDM signal amplitude to obtain best performance. In this context, the experimental bit-error ratio (BER) is obtained {{as a function of}} the LED BP and the RMS (root mean square) OFDM signal across the LED. Index Terms—Optical wireless communication, OFDM, Non-linearity, LED...|$|E
40|$|Since MFCC was {{introduced}} by Davis and Mermelstein [5] and became the standard front-end, much effort has been taken to improve its efficiency in real-world environment, such as One main obstacle in speech recognition is what said “robustness”. This paper focus on one popular idea in cepstral mean subtraction (CMS) [6][7], distortion-constraint strategies including multi-band feature extraction and acoustic antagonizing speech system vulnerability-channel <b>backing-off</b> decoder [8][9], and alternative linear transforms normalization, and presents a new normalization algorithm-Multi-Layer Channel Normalization (MLCN), which exploits the recursive compensation progress in two domains- spectral domain and cepstral domain- to depress different noises, so that such as PCA and LDA [10][11], instead of normal Discrete Cosine Transform (DCT). Other types of front-ends as LSF [12] and PLP [13], which try to incorporate some {{of the features of}} the human auditory mechanism, have been suggested for robust the more robust speech representation is achieved. speech recognition. Combined with RASTA [14] processing, Experimental results of our gallina system demonstrate the validity of our new algorithm. these front-ends achieved better performance in noisy conditions compared with MFCC...|$|E
40|$|When {{transmitting}} signals, one of {{the most}} important issues is to keep the transmission errors as low as possible. Or in other words, to obtain a reliable transmission link, the bit-error-rate (BER) should be kept within certain limits. However, the probability of transmission errors strongly depends on the signal-to-noise ratio (SNR) of the transmitted signals. Hence, the power amplifier plays a key role in the sender part: the more power, the higher the SNR, the lower the probability of transmission errors. Unfortunately, this is a too simple vision. One should take care to keep the peak-to-average power ratio (PAPR) of the transmitted signal low in order not to push the power amplifier into its nonlinear operation region. Classical techniques use clipping or <b>backing-off</b> the input signal to reduce the PAPR of the transmitted signal. However, these techniques have a negative influence on the SNR and hence on the BER. In this paper, we present a technique to reduce the PAPR of the transmitted signals and hence to reduce the BER, by introducing errors into Orthogonal Frequency Division Multiplexing (OFDM) signals in a controlled way. Channel coding will be used to compensate for the introduced errors...|$|E
40|$|This paper {{deals with}} the {{estimation}} of powerful sta-tistical language models using a technique that scales from very small to very large amounts of domain-dependent data. We begin with an improved modeling of the grammar statistics, based {{on a combination of}} the <b>backing-off</b> tech-nique [6] and zero-frequency techniques [2, 91. These are extended to be more amenable to our particular system. Our resulting technique is greatly simplified, more robust, and gives improved recognition performance than either of the previous techniques. We then further attack the problem of robustness of a model based on a small training corpus by grouping words into obvious semantic classes. This significantly improves the robustness of the resulting statistical grammar. We also present a technique that allows the estimation of a high-order model on modest computation resources. This allows us to run a 4 -gram statistical model of a 50 million word corpus on a workstation of only modest ca-pability and cost. Finally, we discuss results from applying a 2 -gram sta-tistical language model integrated in the HMM search, ob-taining a list of the N-Best recognition results, and rescor-ing this list with a higher-order statistical model...|$|E
40|$|LFG-DOP is a powerful, {{hybrid model}} of {{language}} processing where the tree representations of Data-Oriented Parsing (DOP) are augmented with the functional representations of Lexical Functional Grammar (LFG). The {{result is a}} robust parsing model which generates linguistically informed output. However, difficulties arise in the accurate implementation of fragmentation and sampling in this model. Due to these unresolved issues, there is currently no satisfactory implementation of the LFG-DOP model. In this thesis, we propose a <b>backing-off</b> to Grammatical Feature-DOP (GF-DOP). The GF-DOP model differs from Tree-DOP and LFG-DOP in that the trees are annotated with selected features extracted from the f-structure, rather than explicitly linked to corresponding f-structure units. In this way, we rnake use of the irlformation available {{to us in the}} f-structure, while avoiding the problems inherent in the implementation of LFG-DOP. We aim {{to improve the quality of}} the parses generated by modeling additional functional and feature information. Experiments on the HomeCentre corpus have shown this model to be a valuable middleground between the two alternative models. GF-DOP has been shown to outperform the Tree-DOP model, as a result of its ability to identify and make use of grammatical features, while maintaining the integrity of the probability model...|$|E
40|$|For {{improved}} recognition robustness in mismatched training-test conditions, {{the application}} of key ideas from Missing Feature Theory and Robust Statistical Pattern Recognition {{in the framework of}} an otherwise conventional ASR system were investigated. To this end, both the type of features used to represent the speech signals and the algorithm used to compute the distance measure between an observed feature vector and a previously trained parametric model were studied. Two different types of feature representations were used: a type in which spectrally local distortions are smeared over the entire feature vector and a type in which distortions are only smeared over part of the feature vector. In addition, two different distance measures were investigated, viz., a conventional distance measure and a robust local distance function in the form of Acoustic <b>Backing-off.</b> The effects on recognition performance were studied for artificially created, band-limited noise and NOISEX noise added to the speech signals. The results for artificial band-limited noise indicate that a partially smearing feature transform is to be preferred over a fully smearing transform. In addition, for artificial band-limited noise a robust local distance function is to be preferred over the conventional distance measure as long as the distorted feature values are outliers with respect to the feature distribution observed during training. The experiments with NOISEX noise show that the combination of feature type and distance measure that is optimal for artificial, band-limited noise is also capable to improve recognition robustness for NOISEX noise, provided that the noise is band-limited...|$|E
40|$|Abstract — Due {{to highly}} dynamic nature of Mobile ad hoc {{networks}} (MANET), predictability {{and design of}} efficient protocols and methodology to handle congestion proves to be a tedious task. Since issues and architecture of mobile ad hoc networks are very much different from their counterparts, so are its congestion control strategies due to frequent changes in network’s topology. A noted congestion control mechanism is to notify source for the congestion in the network so that either it may pacify the transmission rate or look for an alternative option. It must be noted down that all the existing methodologies are capable to tell the source about the congestion problem as they use TCP. But in case of MANET, the packet losses due to link failure (due to its dynamic nature) are misinterpreted as packet losses due to congestion, and in the snapshot of a timeout, <b>backing-off</b> its RTO. This results in needless reduction of transmission speed due to which throughput of the whole network degrades. In this paper, we compare various congestion control mechanism used in MANET such as TCP Tahoe, TCP-Reno,TCP New Reno, TCP SACK, TCP FACK, TCP Vegas. Along with the above specified APCC, RED and strategical RED approach to handle congestion is also illustrated. The same is tried to resolve by using concept of explicit congestion Notification (ECN) which is an extension to transmission control protocol (TCP) and allows end to end notification of network congestion without dropping packets which is done conventionally in TCP/IP networks with a bit difference of additional bit and other methodologies available for the same have been discussed...|$|E
40|$|The {{emphasis}} on higher data rates, spectral efficiency and cost reduction has driven the field towards linear modulation {{techniques such as}} {{quadrature phase shift keying}} (QPSK), quadrature amplitude modulation (QAM), wideband code division multiple access (WCDMA), and orthogonal frequency division multiplexing (OFDM). The result is a complex signal with a non-constant envelope and a high peak-to-average power ratio. This characteristic makes these signals particularly sensitive to the intrinsic nonlinearity of the RF power amplifier (PA) in the transmitter. The nonlinearity will generate intermodulation (IMD) components, also referred to as out-of-band emission or spectral re-growth, which interfere with adjacent channels. Such distortion, or so called Adjacent Channel Interference (ACI), is strictly limited by FCC and ETSI regulations. Meanwhile, the nonlinearity also causes in-band distortion which degrades the bit error rate performance. Typically, the required linearity can be achieved either by reducing power efficiency or by using linearization techniques. For a Class-A PA, simply “backing off” the input power level can improve linearity; however, for high peak to average power ration (PAPR) signals, this normally reduces the power efficiency down to 10 % while increasing heat dissipation up to 90 %. When considering the vast number of base stations that wireless operators need to account for, increasing power consumption, or in other words, power back-off is not a viable tradeoff. Therefore, amplifier linearization has become an important technology and a desirable alternative to <b>backing-off</b> an amplifier in modern communications systems. In this work, a novel adaptive algorithm is presented for predistorter linearization of power amplifiers. This algorithm uses Pade-Chebyshev polynomials and a QR decomposition followed by back substitution to find the pre-distorter coefficients. This algorithm is implemented on a Field Programmable Gate Array (Stratix 1 S 80). The implementation provides improved linearization and also runs the algorithm fast enough so that the adaptive part can be done quickly. Yet another challenge was the integration of a transmitter, receiver and this adaptive algorithm into a single FPGA chip and its communication with a base station. The work thus presents a novel pre-distortion implementation technique using an FPGA and a soft processor (Nios 2) which provides significant intermodulation distortion suppression...|$|E
