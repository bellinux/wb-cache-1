176|904|Public
5000|$|Mark J. Polak, Sophia Zhou, Pentti M. Rautaharju, William W. Armstrong, Bernard R. Chaitman, Adaptive Logic Network Compared with <b>Backpropagation</b> <b>Network</b> in Automated Detection of Ischemia From Resting ECG, Computers in Cardiology, 217-220, 1995.|$|E
50|$|The basic {{components}} of artificial neural networks are nodes/units and weights. Nodes or units are simple processing elements, {{which can be}} considered artificial neurons. These units can act {{in a variety of}} ways. They can act like sensory neurons and collect inputs from the environment, they can act like motor neurons and sent and output, they can act like interneurons and relay information, or they may do all three functions. A <b>backpropagation</b> <b>network</b> is often a three-layer neural network that includes input nodes, hidden nodes, and output nodes (see Figure 1). The hidden nodes allow the network to be transformed into an internal representation, akin to a mental representation. These internal representations give the <b>backpropagation</b> <b>network</b> its ability to capture abstract relationships between different input patterns.|$|E
50|$|In {{order to}} {{understand}} the topic of catastrophic interference {{it is important to understand}} the components of an artificial neural network and, more specifically, the behaviour of a <b>backpropagation</b> <b>network.</b> The following account of neural networks is summarized from Rethinking Innateness: A Connectionist Perspective on Development by Elman et al. (1996).|$|E
40|$|Part 1 : GIS, GPS, RS and Precision FarmingInternational audienceStripe rust caused byPuccinia striiformis f. sp. tritici, is {{a devastating}} wheat {{disease in the}} world. The {{prediction}} of this disease {{is very important to}} make control strategies. In order to figure out suitable prediction methods based on neural networks that could provide accurate prediction information with high stability, the predictions of wheat stripe rust by using <b>backpropagation</b> <b>networks</b> with different transfer functions, training functions and learning functions, radial basis networks, generalized regression networks (GRNNs) and probabilistic neural networks (PNNs) were conducted in this study. The results indicated that suitable <b>backpropagation</b> <b>networks,</b> radial basis networks and GRNNs could be used for the prediction of wheat stripe rust. Good fitting accuracy and prediction accuracy could be obtained by using <b>backpropagation</b> <b>networks</b> with trainlm, trainrp or trainbfg as training function. Radial basis networks had more power than <b>backpropagation</b> <b>networks</b> and GRNNs to predict wheat stripe rust. GRNNs were easier to be used than <b>backpropagation</b> <b>networks.</b> New methods based on neural networks were provided for the prediction of wheat stripe rust...|$|R
40|$|Abstract. Stripe rust {{caused by}} Puccinia striiformis f. sp. tritici, is a devastating wheat {{disease in the}} world. The {{prediction}} of this disease {{is very important to}} make control strategies. In order to figure out suitable prediction methods based on neural networks that could provide accurate prediction information with high stability, the predictions of wheat stripe rust by using <b>backpropagation</b> <b>networks</b> with different transfer functions, training functions and learning functions, radial basis networks, generalized regression networks (GRNNs) and probabilistic neural networks (PNNs) were conducted in this study. The results indicated that suitable <b>backpropagation</b> <b>networks,</b> radial basis networks and GRNNs could be used for the prediction of wheat stripe rust. Good fitting accuracy and prediction accuracy could be obtained by using <b>backpropagation</b> <b>networks</b> with trainlm, trainrp or trainbfg as training function. Radial basis networks had more power than <b>backpropagation</b> <b>networks</b> and GRNNs to predict wheat stripe rust. GRNNs were easier to be used than <b>backpropagation</b> <b>networks.</b> New methods based on neural networks were provided for the prediction of wheat stripe rust...|$|R
5000|$|... #Subtitle level 2: Artificial Neural Networks: Standard <b>Backpropagation</b> <b>Networks</b> and Their Training ...|$|R
50|$|When {{tested on}} {{sequential}} learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard <b>backpropagation</b> <b>network.</b> This improvement was with both memory savings and exact recognition of old patterns. When the activation {{patterns of the}} pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.|$|E
50|$|When {{the novelty}} rule {{is used in}} a {{standard}} <b>backpropagation</b> <b>network</b> there is no, or lessened, forgetting of old items when new items are presented sequentially. However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer {{is identical to the}} input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as {{it would be impossible to}} calculate how much a new input differed from the old input.|$|E
50|$|In his {{tests of}} an 8-8-8 (input-hidden-output) node <b>backpropagation</b> <b>network</b> where one node was sharpened, French {{found that this}} {{sharpening}} paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning {{is a measure of}} memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.|$|E
5000|$|... {{interference}} was catastrophic in the <b>backpropagation</b> <b>networks</b> when {{learning was}} sequential but not concurrent ...|$|R
40|$|In this study, {{comparison}} {{of a system}} for introducing the signature with kohonen neural network method and feedforward <b>backpropagation</b> <b>networks</b> was done to know the effect of neighborhood size, alpha and desired error to the winner index output in kohonen networks. The study also was done to know the effect of learning rate, desired error and number of hidden layers in the feedforward <b>backpropagation</b> <b>networks</b> with binary input system that would be changed to the PCX grayscale 256 level format with 30 x 30 pixel resolution. The optimum parameters for kohonen networks obtained from this study are neighborhood size 12, alpha 0. 09 and desired error 0. 000001 with 50. 91 % chance to be succeed. For feedforward <b>backpropagation</b> <b>networks,</b> hidden layer 1 with 18 nodes, learning rate 0. 015, and desired error 0. 01 gave 43. 64 % chance to be succeed. Testing with tampered signature 45 % succeeded for kohonen networks and 55 % for feedforward <b>backpropagation</b> <b>networks...</b>|$|R
5000|$|Well-learned {{information}} was catastrophically forgotten as new {{information was}} learned in both {{small and large}} <b>backpropagation</b> <b>networks.</b>|$|R
50|$|The {{issue of}} catastrophic interference, comes about when {{learning}} is sequential. Sequential training involves the network learning an input-output pattern until the error is reduced below a specific criterion, then training the network on {{another set of}} input-output patterns. Specifically, a <b>backpropagation</b> <b>network</b> will forget information if it first learns input A and then next learns input B. It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns both inputs-output patterns at the same time, i.e. as AB. Weights are only changed when the network is being trained and not when the network is being tested on its response.|$|E
5000|$|Some {{researchers}} {{have argued that}} catastrophic interference {{is not an issue}} with the backpropagation model of human memory. For example, Mirman and Spivey (2001) found that humans show more interference when learning pattern-based information. Pattern-based learning is analogous to how a standard <b>backpropagation</b> <b>network</b> learns. Thus, they concluded that catastrophic interference is not limited to connectionist memory models but rather that it is a [...] "general product of pattern-based learning that occurs in humans as well" [...] (p. 272). [...] However, Musca, Rousset and Ans (2004) found contrasting results where retroactive interference was more pronounced in subjects who sequentially learned unstructured lists when controlling for methodological failure that occurred in the Mirman, D., & Spivey, M. (2001) study.|$|E
5000|$|Once {{the input}} {{has been sent}} to the hidden layer from the input layer, the hidden node may then send an output to the output layer. The output of any given node depends on the {{activation}} of that node and the response function of that node. In the case of a three-layer <b>backpropagation</b> <b>network,</b> the response function is a non-linear, logistic function. This function allows a node to behave in an all or none fashion towards high or low input values and in a more graded and sensitive fashion towards mid-ranged input values. It allows the nodes the result in more substantial changes in the network when the node activation is at the more extreme values. Transforming the net input into a net output that can be sent onto the output layer is calculated by: ...|$|E
40|$|This paper proposes {{technique}} of <b>backpropagation</b> neural <b>network</b> in power system protection scheme. The main objective {{this paper is}} perform protection system model to transmission line using technique <b>Backpropagation</b> Neural <b>Networks.</b> An improvement in performance to distance relay is expected after the backpropagation could acquire with different fault conditions. The implemented <b>Backpropagation</b> Neural <b>Network</b> should catch knowledge for the correct distance relay operation in appearance the different network conditions. In power system of three phase currents and voltages at fault location are used as inputs to <b>Backpropagation</b> Neural <b>Network</b> based on power system protection scheme. The <b>Backpropagation</b> Neural <b>Networks</b> are trained to address fault location. The reliability of proposed scheme investigated which using power system in transmission line by using Matlab...|$|R
40|$|Abstract. The paper {{presents}} {{the design of}} three types of neural networks with different features, including traditional <b>backpropagation</b> <b>networks,</b> {{radial basis function networks}} and counterpropagation <b>networks.</b> Traditional <b>backpropagation</b> <b>networks</b> require very complex training process before being applied for classification or approximation. Radial basis function networks simplify the training process by the specially organized 3 -layer architecture. Counterpropagation networks do not need training process at all and can be designed directly by extracting all the parameters from input data. Both design complexity and generalization ability of the three types of neural network architectures are compared, based on a digit image recognition problem...|$|R
40|$|The {{problem of}} {{cervical}} cancer diagnosis using neural networks from multisensor imagery is investigated. The radial basis function <b>networks</b> and error <b>backpropagation</b> <b>networks</b> {{are used for}} the diagnosis. The results of experiments are presented. ??????????? ???????? ??????????? ???? ????? ????? ? ??????? ????????? ????? ?? ??????????????????? ???????????. ??? ??????????? ???????????? ????????? ???? ? ???????? ???????????????? ?????? ? ???? ? ?????????? ???????. ????????? ?????????? ????????????????? ????????????...|$|R
50|$|In {{their second}} connectionist model, McCloskey and Cohen {{attempted}} {{to replicate the}} study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found {{that the amount of}} training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the <b>backpropagation</b> <b>network.</b> Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.|$|E
5000|$|To {{test their}} hypothesis, McRae and Hetherington (1993) {{compared}} {{the performance of}} a naïve and pre-trained auto-encoder <b>backpropagation</b> <b>network</b> on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like [...] "JEP" [...] and [...] "ZEP", was greater than for dissimilar CVCs, such as [...] "JEP" [...] and [...] "YUG". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.|$|E
50|$|Catastrophic interference, {{also known}} as catastrophic forgetting, is the {{tendency}} of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks {{are an important part}} of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard <b>backpropagation</b> <b>network</b> are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.|$|E
40|$|In {{this paper}} {{the process of}} forgetting, termed 'engram decay', in {{artificial}} neural networks is examined for three classes of networks: the self organizing map, fuzzy logic adaptive resonance theory and, maximally connected <b>backpropagation</b> <b>networks.</b> All networks were trained to categorize three varieties of iris. How iris categories are forgotten is shown to be strongly related to the distribution of iris categories in feature space...|$|R
50|$|Hybrid Kohonen SOM {{has been}} used in weather {{prediction}} and especially in forecasting stock prices, which has made a challenging task considerably easier. It is fast and efficient with less classification error, hence is a better predictor, when compared to Kohonen SOM and <b>backpropagation</b> <b>networks.</b>|$|R
50|$|MLPs were {{a popular}} machine {{learning}} solution in the 1980s, finding applications in diverse {{fields such as}} speech recognition, image recognition, and machine translation software, but thereafter faced strong competition from much simpler (and related) support vector machines. Interest in <b>backpropagation</b> <b>networks</b> returned due to the successes of deep learning.|$|R
5000|$|An {{important}} feature of neural networks {{is that they can}} learn. Simply put, this means that they can change their outputs when they are given new inputs. Backpropagation, specifically refers to how this the network is trained, i.e. how the network is told to learn. The way in which a <b>backpropagation</b> <b>network</b> learns, is through comparing the actual output to the desired output of the unit. The desired output is known as a 'teacher' and it can be the same as the input, {{as in the case of}} auto-associative/auto-encoder networks, or it can be completely different from the input. Either way, learning which requires a teacher is called supervised learning. The difference between these actual and desired output constitutes an error signal. This error signal is then fedback, or backpropagated, to the nodes in order to modify the weights in the neural network. Backpropagation first modifies the weights between output layer to the hidden layer, then next modifies the weights between the hidden units and the input units. The change in weights help to decrease the discrepancy between the actual and desired output. However, learning is typically incremental in these networks. This means that these networks will require a series of presentations of the same input before it can come up with the weight changes that will result in the desired output. The weights are usually set to random values for first learning trial and after many trials the weights become more able represent the desired output. The process of converging on an output is called settling. This kind of training is based on the error signal and backpropagation learning algorithm / delta rule: ...|$|E
5000|$|French (1997) {{proposed}} {{the idea of}} a pseudo-recurrent <b>backpropagation</b> <b>network</b> in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be [...] "brought back" [...] to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations {{is the only way to}} reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called pseudopatterns. These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows: ...|$|E
40|$|Computer {{system has}} been able to {{recognize}} writing as human brain does. The method mostly used for character recognition is the <b>backpropagation</b> <b>network.</b> <b>Backpropagation</b> <b>network</b> has been known for its accuracy because it allows itself to learn and improving itself thus it can achieve higher accuracy. On the other hand, backpropagation was less to be used because of its time length needed to train the network to achieve the best result possible. In this study, <b>backpropagation</b> <b>network</b> algorithm is combined with genetic algorithm to achieve both accuracy and training swiftness for recognizing alphabets. Genetic algorithm is used to define the best initial values for the network’s architecture and synapses ’ weight thus within a shorter period of time, the network could achieve the best accuracy. The optimized <b>backpropagation</b> <b>network</b> has better accuracy and less training time than the standard <b>backpropagation</b> <b>network.</b> The accuracy in recognizing character differ by 10, 77 %, with a success rate of 90, 77 % for the optimized backpropagation and 80 % accuracy for the standard <b>backpropagation</b> <b>network.</b> The training time needed for backpropagation learning phase improved significantly from 0...|$|E
40|$|<b>Backpropagation</b> neural <b>networks</b> {{have been}} applied to {{prediction}} and classification problems in many real world situations. However, a drawback {{of this type of}} neural network is that it requires a full set of input data, and real world data is seldom complete. We have investigated two ways of dealing with incomplete data, network reduction using multiple neural network classifiers and value substitution using estimated values from predictor networks, and compared their performance with an induction method. On a thyroid disease database collected in a clinical situation, we found that the network reduction method was superior. We conclude that network reduction can be a useful method for dealing with missing values in diagnostic systems based on <b>backpropagation</b> neural <b>networks.</b> Keywords: <b>backpropagation,</b> neural <b>networks,</b> decision support, thyroid disease, classification. Introduction <b>Backpropagation</b> neural <b>networks</b> {{have been applied}} to prediction and classification problems in many real [...] ...|$|R
40|$|This {{paper will}} discuss {{learning}} in hybrid models {{that goes beyond}} simple rule extraction from <b>backpropagation</b> <b>networks.</b> Although simple rule extraction has {{received a lot of}} research attention, to further develop hybrid learning models that include both symbolic and subsymbolic knowledge and that learn autonomously, it is necessary to study autonomous learning of both subsymbolic and symbolic knowledge in integrated architectures. This paper will describe knowledge extraction from neural reinforcement learning. It includes two approaches towards extracting plan knowledge: the extraction of explicit, symbolic rules from neural reinforcement learning, and the extraction of complete plans. This work points {{to the creation of a}} general framework for achieving the subsymbolic to symbolic transition in an integrated autonomous learning framework. 1 Introduction This paper will discuss learning in hybrid models that goes beyond simple rule extraction from <b>backpropagation</b> <b>networks.</b> Althou [...] ...|$|R
40|$|<b>Backpropagation</b> <b>networks</b> are {{studied in}} the context of the {{recognition}} of handwritten and machine printed characters and, specifically, the problem of the recognition of 'false-positive' patterns is investigated. The idea is proposed of integrating into the processing architecture an independent set of 'guard units' which act as basic matchers, rejecting patterns not belonging to the training classes...|$|R
40|$|<b>Backpropagation</b> <b>network</b> {{is able to}} {{deal with}} various types of data and also has the ability to model a complex {{decision}} system. However, some problem domains might involve a large amount of data. <b>Backpropagation</b> <b>network</b> with four input units and two hidden units for example required certain epochs, to create classification or prediction model. More input units or hidden units could increase the complexity of the model and also increase its computational complexity. In other words, additional input unit or hidden unit could increase the model complexity and increase training time. This is because a larger network is more difficult to train. Like human learning, a complex problem requires certain period of time to establish learning. In some studies, <b>backpropagation</b> <b>network</b> and in general neural network has been considered "not efficient". The network, appears very time consuming even for small network architectures (Sima, 1994; Sima, 1996). Sima (1996) highlighted that the failure of effort to speed up the algorithm is caused by the network's learning complexity problem. Therefore, in this study multi-backpropagation network approach is proposed as an alternative training method for <b>backpropagation</b> <b>network</b> to reduce the complexity...|$|E
40|$|Artificial neural {{networks}} model the circuitry of biological neurons. Inspired by the brain, they may share common properties with their biological counterparts, {{although they may}} be distinctly different. For example, the topology of the connections of neurons in an Adaline network (single-layered <b>backpropagation</b> <b>network)</b> {{will not be the}} same as those in a multi-layer <b>backpropagation</b> <b>network.</b> The size and number of layers in a neural network also influence its accuracy in its applications. In this project, different variations on {{neural networks}} will be tested and evaluated on their performance in recognizing handwritten characters...|$|E
40|$|A polar <b>backpropagation</b> <b>network</b> is {{developed}} for solving the polar classification problem. The architecture is a constrained version of general three-layer backpropagation, such that selected weights correspond to physical entities in the problem, namely, the coordinates of the pole. Heuristic {{knowledge of the}} problem is implicit in the network constraints. It is emphasized that the network is not as general as backpropagation, but is useful at solving a specific kind of problem which backpropagation can only partially explain. The polar <b>backpropagation</b> <b>network</b> is not suitable to general classification problems, but is useful at solving the polar classification problem because it locates the pole...|$|E
40|$|We study an {{interacting}} agent {{model of}} a game-theoretical economy. The agents play a minority-subsequently-majority game and they learn, using <b>backpropagation</b> <b>networks,</b> to obtain higher payoffs. We study the relevance of heterogeneity to performance, and how heterogeneity emerges. Comment: 7 pages, pdf, contribution to International conference on Econophysics, Bali 2002, to appear in proceedings in Physica...|$|R
40|$|Abstract. Despite some {{promising}} early approaches, {{neural networks}} have by now received comparatively little attention as a {{machine learning model}} for robust, corpus-based anaphor resolution. The work {{presented in this paper}} is intended to fill the apparent gap in research. Based on a hybrid algorithm that combines manually knowledge-engineered antecedent filtering rules with machine-learned preference criteria, it is investigated what can be achieved by employing <b>backpropagation</b> <b>networks</b> for the corpus-based acquisition of preference strategies for pronoun resolution. Thorough evaluation will be carried out, thus systematically addressing the numerous experimental degrees of freedom, among which are sources of evidence (features, feature vector signatures), training data generation settings, number of hidden layer nodes, and number of training epochs. According to the evaluation results, the neural network approach performs at least similar to a decision-tree-based ancestor system that employs the same general hybrid strategy. Key words: anaphor resolution, coreference resolution, machine learning, neural <b>networks,</b> <b>backpropagation</b> <b>networks,</b> decision trees, C 4. 5, information extraction, discourse, robust natural language processing...|$|R
40|$|In {{this topic}} {{research}} was provided about the <b>backpropagation</b> neural <b>network</b> to detect fault location in transmission line 150 kV between substation to substation. The distance relay {{is one of}} the good protective device and safety devices that often used on transmission line 150 kV. The disturbances in power system are used distance relay protection equipment in the transmission line. However, it needs more increasing large load and network systems are increasing complex. The protection system use the digital control, {{in order to avoid the}} error calculation of the distance relay impedance settings and spent time will be more efficient. Then <b>backpropagation</b> neural <b>network</b> is a computational model that uses the training process that can be used to solve the problem of work limitations of distance protection relays. The <b>backpropagation</b> neural <b>network</b> does not have limitations cause of the impedance range setting. If the output gives the wrong result, so the correct of the weights can be minimized and also the response of galat, the <b>backpropagation</b> neural <b>network</b> is expected to be closer to the correct value. In the end, <b>backpropagation</b> neural <b>network</b> modeling is expected to detect the fault location and identify operational output current circuit breaker was tripped it. The tests are performance with interconnected system 150 kV of Riau Region...|$|R
