592|10000|Public
6000|$|... "I {{think it}} was you, ancient <b>bag</b> <b>of</b> <b>words</b> and sweeper of paths for {{the feet of the}} great, who made a mock of me but now, when you thought that I fled before the horns of yonder man-eating bull--" [...] and he nodded towards the {{fragments}} of what once had been Rezu. [...] "Find now his axe and though I am weak and weary, I will wash away the insult with your blood." ...|$|E
5000|$|Limitations of <b>bag</b> <b>of</b> <b>words</b> model (BOW), where a text is {{represented}} as an unordered collection of words. To {{address some of}} the limitation of <b>bag</b> <b>of</b> <b>words</b> model (BOW), multi-gram dictionary can be used to find direct and indirect association as well as higher-order co-occurrences among terms.|$|E
5000|$|The Knitter's Gift: An Inspirational <b>Bag</b> <b>of</b> <b>Words,</b> Wisdom, and Craft, , Adams Media ...|$|E
40|$|Method for the {{automatic}} analysis {{of an image}} (1, 11, 12, 13) of a biological sample {{with respect to a}} pathological relevance, wherein f) local features of the image (1, 11, 12, 13) are aggregated to a global feature of the image (1, 11, 12, 13) using a <b>bag</b> <b>of</b> visual <b>word</b> approach, g) step a) is repeated at least two times using different methods resulting in at least two <b>bag</b> <b>of</b> <b>word</b> feature datasets,, h) computation of at least two similarity measures using the <b>bag</b> <b>of</b> <b>word</b> features obtained from a training image dataset and <b>bag</b> <b>of</b> <b>word</b> features from the image (1, 11, 12, 13) i) the image training dataset comprising a set <b>of</b> visual <b>words,</b> classifier parameters, including kernel weights and <b>bag</b> <b>of</b> <b>word</b> features from the training images, j); the computation of the at least two similarity measures is subject to an adaptive computation of kernel normalization parameters and / or kernel width parameters, f) for each image (1, 11, 12, 13) one score is computed depending on the classifier parameters and kernel weights and the at least two similarity measures, the at least one score being a measure of the certainty of one pathological category compared to the image training dataset, g) for each pixel of the image (1, 11, 12, 13) a pixel-wise score is computed using the classifier parameters, the kernel weights, the at least two similarity measures, the <b>bag</b> <b>of</b> <b>word</b> features <b>of</b> the image (1, 11, 12, 13), all the local features used in the computation <b>of</b> the <b>bag</b> <b>of</b> <b>word</b> features <b>of</b> the image (1, 11, 12, 13) and the pixels used in the computations of the local features, h); the pixel-wise score is stored as a heatmap dataset linking the pixels of the image (1, 11, 12, 13) to the pixel-wise scores...|$|R
40|$|International audienceMost of Information Retrieval Systems {{transform}} {{natural language}} users'queries into <b>bags</b> <b>of</b> <b>words</b> that are matched to documents also represented as <b>bags</b> <b>of</b> <b>words.</b> Through such process, {{the richness of}} the query is lost. In this paper we show that linguistic features of a query are good indicators to predict systems failure to answer it. The experiments are based on 42 systems or system variants and 50 TREC topics that consist of a descriptive part expressed in natural language...|$|R
40|$|Classifying tweets is an {{intrinsically}} hard task as tweets {{are short}} messages which makes traditional <b>bags</b> <b>of</b> <b>words</b> based approach ine cient. In fact, <b>bags</b> <b>of</b> <b>words</b> approaches ig- nores relationships between important terms {{that do not}} co-occur literally. In this paper we resort to word-word co-occurence informa- tion from a large corpus to expand the vocabulary of another corpus consisting of tweets. Our results show {{that we are able}} {{to reduce the number of}} erroneous classi cations by 14 % using co-occurence information...|$|R
5000|$|Weka. Weka is {{a popular}} data mining package for Java {{including}} WordVectors and <b>Bag</b> <b>Of</b> <b>Words</b> models.|$|E
50|$|Naive Bayes {{classifiers}} are {{a popular}} statistical technique of e-mail filtering. They typically use <b>bag</b> <b>of</b> <b>words</b> features to identify spam e-mail, an approach {{commonly used in}} text classification.|$|E
50|$|Each {{document}} {{is assumed to}} be characterized by a particular set of topics. This is akin to the standard <b>bag</b> <b>of</b> <b>words</b> model assumption, and makes the individual words exchangeable.|$|E
40|$|Most text {{categorization}} methods use {{the vector}} space model {{in combination with}} a representation of documents based on <b>bags</b> <b>of</b> <b>words.</b> As its name indicates, <b>bags</b> <b>of</b> <b>words</b> ignore possible structures in the text and only take into account isolated, unrelated words. Although this limitation is widely acknowledged, most previous at-tempts to extend the bag-of-words model with more advanced approaches failed to produce conclusive improvements. We propose a novel method that ex-tends the word-level representation to automatically extracted semantic and syntactic features. We investigated three extensions: word-sense information, subject–verb–object triples, and role-semantic predicate–argument tuples, all fitting within the vector space model. We computed their contribution to the categorization results on the Reuters corpus of newswires (RCV 1). We show that these three extensions, either taken individually or in combination, result in statistically significant improvements of the microaverage F 1 over a baseline using <b>bags</b> <b>of</b> <b>words.</b> We found that our best extended model that uses a combination of syntactic and semantic features reduces the error of the word-level baseline by up to 10 percent for the categories having more than 1, 000 documents in the training corpus. ∗Research done while at Lund University. ...|$|R
5000|$|Query {{expansion}}. A 2012 {{study by}} Zhao and Callan using expert created manual Conjunctive normal form queries {{has shown that}} searchonym expansion in the Boolean conjunctive normal form is much {{more effective than the}} traditional <b>bag</b> <b>of</b> <b>word</b> expansion e.g. Rocchio expansion.|$|R
40|$|Abstract. We {{present the}} {{experiment}} the LCI group has performed to prepare our submission to CLEF-IP Classification Track. In this preliminary experiment {{we used a}} part of the available target documents as test set and the rest as train set. We describe the systems AGFL used for extracting these triples and the LCS used for classification by the Winnow algorithm. We show that the use of linguistic triples in place <b>of</b> <b>bags</b> <b>of</b> <b>words</b> improves the accuracy, as well as using the names and addresses of the applicants. we found that using the complete descriptions as <b>bags</b> <b>of</b> <b>words</b> does not really perform better than using only abstracts and titles. Some simple mathematics show that the official measures are redundant and tha...|$|R
50|$|For {{convenience}} of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach {{is sometimes called}} bag of features and {{is analogous to the}} <b>bag</b> <b>of</b> <b>words</b> model and vector space model used in information retrieval for representation of documents.|$|E
50|$|The {{problem is}} very easy to formulate. E-mails should be classifiedinto one of two categories: non-spam or spam. The only {{available}} information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a <b>bag</b> <b>of</b> <b>words</b> model.|$|E
50|$|<b>Bag</b> <b>of</b> <b>words</b> {{analysis}} {{represent the}} adoption of vector space retrieval, a traditional IR concept, to the domain of plagiarism detection. Documents are represented as one or multiple vectors, e.g. for different document parts, which are used for pair wise similarity computations. Similarity computation may then rely on the traditional cosine similarity measure, or on more sophisticated similarity measures.|$|E
40|$|Abstract Background There are {{numerous}} definitions of General Practice and Family Medicine (GP/FM) and Primary Health Care (PHC), but {{the distinction between}} the two concepts is unclear. Aim To conduct a terminological analysis of a set of definitions of GP/FM and of PHC, to clarify what binds and what distinguishes these two concepts. Design The terms of 20 definitions were collected in two <b>bags</b> <b>of</b> <b>words</b> (one for GP/FM and one for PHC terms). A terminological analysis <b>of</b> these two <b>bags</b> <b>of</b> <b>words</b> was performed to prioritize the terms and analyze their world of reference. Methods The two collected <b>bags</b> <b>of</b> <b>words</b> were extracted with Vocabgrabber®, configured in two term butts using Wordle®, and further explored for similarities using Tropes®. The prioritized terms were analyzed using the Aristotelian approach to categorization of things. Results Although continuity of care (with person-centered approach and shared decision making) is the central issue of the two sets, the two sets of definitions differ greatly in content. The prioritized terms specific to GP/FM (community, medicine, responsibility, individual, problem, needs, [...] .) are different from prioritized terms specific to PHC (home, team, promotion, collaborator, engagement, neighborhood, medical center…). Conclusion Terminological analysis of the definitions for GP/FM and PHC shows two entities which are overlapping but distinct, necessitating a different taxonomic approach and different bibliographic search strategies. Peer reviewe...|$|R
30|$|This paper {{proposes a}} method to {{recognize}} scene categories using <b>bags</b> <b>of</b> visual <b>words</b> obtained by hierarchically partitioning into subregion the input images. Specifically, for each subregion the Textons distribution and {{the extension of the}} corresponding subregion are taken into account. The <b>bags</b> <b>of</b> visual <b>words</b> computed on the subregions are weighted and used to represent the whole scene. The classification of scenes is carried out by discriminative methods (i.e., SVM, KNN). A similarity measure based on Bhattacharyya coefficient is proposed to establish similarities between images, represented as hierarchy <b>of</b> <b>bags</b> <b>of</b> visual <b>words.</b> Experimental tests, using fifteen different scene categories, show that the proposed approach achieves good performances with respect to the state-of-the-art methods.|$|R
40|$|Text {{categorization}} algorithms usually represent documents as <b>bags</b> <b>of</b> <b>words</b> {{and consequently}} {{have to deal}} with huge numbers of features. Most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed...|$|R
50|$|In a 2005 {{paper by}} Fergus et al., pLSA (probabilistic latent {{semantic}} analysis) and extensions {{of this model}} were applied {{to the problem of}} object categorization from image search. pLSA was originally developed for document classification, but has since been applied to computer vision. It makes the assumption that images are documents that fit the <b>bag</b> <b>of</b> <b>words</b> model.|$|E
5000|$|Language {{models are}} used in {{information}} retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model [...] Commonly, the unigram language model is used for this purpose—otherwise known as the <b>bag</b> <b>of</b> <b>words</b> model.|$|E
50|$|In {{a typical}} {{document}} classification task, the input {{to the machine}} learning algorithm (both during learning and classification) is free text. From this, a <b>bag</b> <b>of</b> <b>words</b> (BOW) representation is constructed: the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) {{of each of the}} documents in both the training and test sets.|$|E
40|$|We {{submitted}} {{three runs}} of the Instance Search task. They {{are listed as}} follows: • UCSB UCR VCG 1 : This automatic run uses <b>Bag</b> <b>of</b> visual <b>word</b> with MSER+SIFT and Dense sift using svm classifier on chi-sq kernel. • UCSB UCR VCG 2 : This automatic run uses <b>Bag</b> <b>of</b> visual <b>word</b> with MSER+SIFT and dense Color-sift based based SVM classifier on chi-sq kernel • UCSB UCR VCG 3 : This automatic run uses <b>Bag</b> <b>of</b> visual <b>word</b> with MSER+SIFT and based svm classifier on histogram intersection kernel. Discriminative reranking using a SVM classifier signifi-cantly improved the results. Compared to Histogram Intersec-tion, Chi-square distance metric performed well on some <b>of</b> the queries. <b>Bag</b> <b>of</b> visual <b>word</b> model was effective in retrieving loca-tion based topics. However, with discriminative reranking(an offline query expansion) approach significantly improved the results. The discriminative classifiers learned from the internet images did not scale well on the test dataset. This poses a question on how to transfer model from one domain to another domain...|$|R
30|$|There is a {{very rich}} {{literature}} on image classification including methods based on <b>bag</b> <b>of</b> <b>word</b> [1, 2], Sparse representation [3 – 7], and Deep learning [8 – 10]. We {{should point out that}} nonlinear classifiers, including kernel based ones, have gained more attention due to their high performance compared to linear classifiers [5, 7, 9].|$|R
40|$|This paper {{presents}} a simple {{approach to the}} Wikipedia Question Answering pilot task in CLEF 2006. The approach ranks the snippets, retrieved using the Lucene search engine, {{by means of a}} similarity measure based on <b>bags</b> <b>of</b> <b>words</b> extracted from both the snippets and the articles in wikipedia. Our participation was in the monolingual English and Spanish tasks...|$|R
50|$|In {{computer}} vision, the bag-of-words model (BoW model) can {{be applied}} to image classification, by treating image features as words. In document classification, a <b>bag</b> <b>of</b> <b>words</b> is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary. In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.|$|E
5000|$|In a <b>bag</b> <b>of</b> <b>words</b> {{model of}} natural {{language}} processing and information retrieval, the data consists {{of the number of}} occurrences of each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Recent studies have proven that additive smoothing is more effective than other probability smoothing methods in several retrieval tasks such as language-model-based pseudo-relevance feedback and recommender systems.|$|E
5000|$|Most mildly {{context-sensitive}} grammar formalisms (in particular, LCFRS/MCFG) actually satisfy a stronger property than constant growth called semilinearity.A language is semilinear if its image under the Parikh-mapping (the mapping that [...] "forgets" [...] the relative {{position of the}} symbols in a string, effectively treating it as a <b>bag</b> <b>of</b> <b>words)</b> is a regular language.All semilinear languages are of constant growth, but not every language with constant growth is semilinear.|$|E
30|$|Multi-channel SIFT (mSIFT) [52]: mSIFT {{descriptor}} concatenates SIFTs {{from all}} channels in the <b>bag</b> <b>of</b> visual <b>words</b> (BOV) framework [52].|$|R
5000|$|And all images will be {{represented}} with this visual {{language as a}} collection <b>of</b> visual <b>words</b> (VW) or what can call it <b>bag</b> <b>of</b> visual <b>words</b> ...|$|R
40|$|Analyzing and {{classifying}} Human Epithelial type 2 (HEp- 2) cells using Indirect Immunofluorescence protocol {{has been}} the golden standard for detecting connective tissue diseases such as Rheumatoid Arthritis. However, this suffers from numerous shortcomings such as being subjective as well as time and labor intensive. Recently, several studies explore the advantages of artificial systems to automate the process, not only to reduce the test turn-around time but also to deliver more consistent results. In this paper, we extend the conventional <b>bag</b> <b>of</b> <b>word</b> models from Euclidean space to non-Euclidean Riemannian manifolds and utilize them to classify the HEp- 2 cells. The main motivation comes from the observation that HEp- 2 cells can be efficiently described by symmetric positive definite matrices which lie on a Riemannian manifold. With this motivation, we first discuss an intrinsic <b>bag</b> <b>of</b> Riemannian <b>words</b> model. We then propose Fisher tensors which can in turn encode additional information about {{the distribution of the}} signatures in a <b>bag</b> <b>of</b> <b>word</b> model. Experiments on two challenging HEp- 2 images datasets, namely ICPRContest and SNPHEp- 2 show that the proposed methods obtain notable improvements in discrimination accuracy, in comparison to baseline and several state-of-the-art methods. The proposed framework, while hand-crafted towards cell classification, is a generic framework for object recognition. This is supported by assessing the performance of our proposal on a challenging texture classification task...|$|R
50|$|Loop closure is {{the problem}} of {{recognizing}} a previously-visited location and updates the beliefs accordingly. This can be a problem because model or algorithm errors can assign low priors to the location. Typical loop closure methods apply a second algorithm to measure some type of sensor similarity, and re-set the location priors when a match is detected. For example, this can be done by storing and comparing <b>bag</b> <b>of</b> <b>words</b> vectors of SIFT features from each previously visited location.|$|E
5000|$|The HDP mixture {{model is}} a natural nonparametric {{generalization}} of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data. [...] Here each group is a document consisting of a <b>bag</b> <b>of</b> <b>words,</b> each cluster is a topic, and each document {{is a mixture of}} topics. The HDP is also a core component of the infinite hidden Markov model, which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.|$|E
50|$|Literal copies, aka {{copy and}} paste (c&p) plagiarism, or {{modestly}} disguised plagiarism cases can be detected with high accuracy by current external PDS if the source is accessible to the software. Especially substring matching procedures achieve a good performance for c&p plagiarism, since they commonly use lossless document models, such as suffix trees. The performance of systems using fingerprinting or <b>bag</b> <b>of</b> <b>words</b> analysis in detecting copies depends on the information loss incurred by the document model used. By applying flexible chunking and selection strategies, they are better capable of detecting moderate forms of disguised plagiarism when compared to substring matching procedures.|$|E
40|$|This paper {{proposes a}} method to {{recognize}} scene categories using <b>bags</b> <b>of</b> visual <b>words</b> obtained hierarchically partitioning into subregion the input images. Specifically, for each subregions the Texton histogram and {{the extension of the}} subregion is taken into account. The <b>bags</b> <b>of</b> visual <b>words,</b> obtained in this way, are weighted and used in a similarity measure during the categorization. Experimental tests using ten different scene categories show that the proposed approach achieves good performances with respect to {{the state of the art}} methods...|$|R
30|$|Images are {{informative}} {{in different}} aspects like color, shape and texture. Describing images with multiple features rather than a single feature, results in a more accurate classifier. For example, an approach is proposed in [11] which describes an image by means <b>of</b> multiple <b>bag</b> <b>of</b> <b>word</b> features and designs a classifier based on them. Also, some kernel based classifiers are proposed based on multiple features [12 – 16].|$|R
40|$|Abstract. This paper {{presents}} a simple {{approach to the}} Wikipedia Question Answering pilot task in CLEF 2006. The approach ranks the snippets, retrieved using the Lucene search engine, {{by means of a}} similarity measure based on <b>bags</b> <b>of</b> <b>words</b> extracted from both the snippets and the articles in wikipedia. Our participation was in the monolingual English and Spanish tasks. We obtained the best results in the Spanish one. ...|$|R
