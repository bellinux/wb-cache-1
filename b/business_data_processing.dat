85|10000|Public
25|$|In {{the first}} half of the 20th century, well before the {{computer}} era, <b>business</b> <b>data</b> <b>processing</b> was done using unit record equipment and punched cards, most commonly the 80-column variety employed by IBM, which dominated the industry. Many tricks were used to squeeze needed data into fixed-field 80-character records. Saving two digits for every date field was significant in this effort.|$|E
500|$|The COBOL {{community}} {{has always been}} isolated from the computer science community. No academic computer scientists participated {{in the design of}} COBOL: all of those on the committee came from commerce or government. [...] Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled. Jean Sammet attributed COBOL's unpopularity to an initial [...] "snob reaction" [...] due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for <b>business</b> <b>data</b> <b>processing.</b> The COBOL specification used a unique [...] "notation", or metalanguage, to define its syntax rather than the new Backus–Naur form because few committee members had heard of it. This resulted in [...] "severe" [...] criticism.|$|E
2500|$|The first Wang VS {{computer}} {{was introduced in}} 1977, {{about the same time}} as Digital Equipment Corporation's VAX, and continues to be in use 39 years later. Its instruction set was compatible with the IBM 360 series, but it did not run any IBM 360 system software. The VS operating system and all system software were built from the ground up to support interactive users as well as batch operations. The VS was aimed directly at the <b>business</b> <b>data</b> <b>processing</b> market in general, and IBM in particular. While many programming languages were available, the VS was typically programmed in COBOL. Other languages supported in the VS integrated development environment included Assembler, COBOL 74, COBOL 85, BASIC, Ada, RPG II, C, PL/I, FORTRAN, Glossary, MABASIC, SPEED II and Procedure (a scripting language). Pascal was also supported for I/O co-processor development. The Wang PACE (Professional Application Creation Environment) 4GL and database was used from the mid-1980s onward by customers and third party developers to build complex applications sometimes involving many thousands of screens, hundreds of distinct application modules, and serving many hundreds of users. Substantial vertical applications were developed for the Wang VS by third party software houses throughout the 1980s in COBOL, PACE, BASIC, PL/I and RPG II. The Wang OFFICE family of applications and Wang WP were both popular applications on the VS. Word Processing ran on the VS through services that emulated the OIS environment and downloaded the WP software as [...] "microcode" [...] (in Wang terminology) to VS workstations.|$|E
5000|$|<b>Business</b> Education (Accounting, <b>Data</b> <b>Processing,</b> Entrepreneurship, Finance, <b>Business,</b> Information and Communication Technology, Management, Marketing, and Secretarial) ...|$|R
50|$|Various {{constraints}} and influences {{will have an}} effect on data architecture design. These include enterprise requirements, technology drivers, economics, <b>business</b> policies and <b>data</b> <b>processing</b> needs.|$|R
40|$|Photograph {{used for}} {{a story in the}} Daily Oklahoman newspaper. Caption: "A year ago Jennifer Orcutt gave up a {{promising}} career as an emergency-room nurse to start her own <b>business,</b> Micro <b>Data</b> <b>Processing</b> - filling the need to microfilm and microfiche documents for companies too small to own their own filming equipment. ...|$|R
50|$|Optimization Techniques, <b>Business</b> <b>Data</b> <b>Processing,</b> Programming Techniques.|$|E
5000|$|<b>Business</b> <b>Data</b> <b>Processing</b> (Published By Devra Books, New Delhi, 2005) ...|$|E
5000|$|... {{by means}} of <b>business</b> <b>data</b> <b>processing</b> systems such as ERP systems.|$|E
50|$|The book {{critique}} on Business Insider {{comments on}} the contrast which exists between Google's advocacy of free access to information and intellectual property and their own policies related to disclosure of their <b>business</b> practices and <b>data</b> <b>processing</b> algorithms.|$|R
5000|$|Previously, {{he was the}} Chancellor of the California Community Colleges {{system from}} January 2004 through July 2007. [...] He left the system office in August 2007 {{to return to the}} Los Angeles Community College District as its chancellor, a post he {{previously}} held for six and a half years. Drummond has also been president of Eastern Washington University a post that he held for over 10 years. and professor of <b>business</b> and <b>data</b> <b>processing</b> at Chabot College in Hayward, California.|$|R
25|$|Academic {{departments}} at {{the time}} of the centennial were English, Mathematics, Physical Science (general science, biology, chemistry, physics), Social Science (geography, sociology, economics, history), Foreign Language (French, Latin, Spanish, German), Business Education (typing, shorthand, <b>business</b> law, salesmanship, <b>data</b> <b>processing),</b> Industrial Arts (mechanical drawing, metalwork, machinery), Home Economics, Music, Art, and Physical Education.|$|R
5000|$|Mihajlo Pupin Institute {{releases}} CER-12 {{computer system}} for <b>business</b> <b>data</b> <b>processing</b> in ERCs.|$|E
5000|$|University of Indianapolis {{programs}} in Business Administration and <b>Business</b> <b>Data</b> <b>Processing</b> are introduced.|$|E
5000|$|It {{has been}} widely used in <b>business</b> <b>data</b> <b>processing</b> and for system use for writing {{operating}} systems on certain platforms. Very complex and powerful systems have been built with PL/I: ...|$|E
5000|$|Technical Infrastructure Architecture : The IT {{infrastructure}} provides {{access to}} application systems and office automation tools used in performance of the business processes. The Corporation places high priority on maintaining a consistent, available, and reliable technical infrastructure. The Technical Architecture describes the underlying technology for the Corporation's <b>business,</b> <b>data,</b> and application <b>processing.</b> It includes the technologies used for communications, <b>data</b> storage, application <b>processing,</b> and computing platforms.|$|R
50|$|After World War II, IBM {{concentrated}} on electronic <b>data</b> <b>processing,</b> {{a significant departure from}} its previously very prosperous <b>business</b> of electromechanical <b>data</b> <b>processing.</b> IBM's engineers and workers in Endicott provided reliable and cost-effective computers to government agencies, banks, and large corporations in the 1950s. This information revolution transformed the American and world economies, and made IBM {{one of the}} world's most successful corporations of {{the second half of the}} 20th century.|$|R
50|$|The lower 3000 series used a 24-bit word size. They {{were based}} on the earlier CDC 924 - a 24-bit version of the CDC 1604. The first lower 3000 to be {{released}} was the CDC 3200 (May 1964), followed by the smaller CDC 3100 (February 1965), and the CDC 3300 (December 1965). The final machine in the series, the CDC 3500, was released in March 1967 and used integrated circuits instead of discrete components. The 3300 and 3500 had optional relocation capabilities, floating point arithmetic, and BDP (<b>Business</b> + <b>Data</b> <b>Processing)</b> instructions. These machines were targeted towards business and commercial computing.|$|R
5000|$|The Höhere Berufsfachschule für Wirtschaftsinformatik (HBFS-WI) {{located in}} Saarbrücken, Germany {{is a school}} {{providing}} higher vocational education in information technology and economics awarding the degree [...] "Staatlich geprüfte(r) Wirtschaftsinformatiker(in)" [...] (English: [...] "state-examined <b>business</b> <b>data</b> <b>processing</b> specialist") ...|$|E
50|$|Born in 1957, Bentley {{attended}} the Mountain Home High School in Arkansas. He {{studied at the}} Arkansas State University, where in 1979 he received his B.S. in <b>Business</b> <b>Data</b> <b>Processing,</b> and in 1981 his M.S. in Information Systems.|$|E
50|$|Computer system TIM-600 {{was used}} for <b>business</b> <b>data</b> <b>processing</b> in many offices in Serbia, for example: in public, health, and {{scientific}} organizations; for process automation in industrial production; in road traffic control; in some banks; for military and government's services, etc.|$|E
40|$|There are two obvious ways to map a two-dimension {{relational}} database table onto a one-dimensional storage interface: store the table row-by-row, or store the table column-by-column. Historically, database system implementations and research {{have focused on}} the row-by row data layout, since it performs best on the most common application for database systems: <b>business</b> transactional <b>data</b> <b>processing.</b> However, there are a set of emerging applications for database systems for which the row-by-row layout performs poorly. These applications are more analytical in nature, whose goal is to read through the data to gain new insight and use it to drive decision making and planning. In this dissertation, we study the problem of poor performance of row-by-row data layout for these emergin...|$|R
40|$|In {{recent years}} {{information}} and communication technologies have gained significant importance in the social sciences. Because there is such rapid growth of knowledge, methods and computer infrastructure, research can now seamlessly connect interdisciplinary fields such as <b>business</b> process management, <b>data</b> <b>processing</b> and mathematics. This study presents some of the latest results, practices and state-of-the-art approaches in network analysis, machine learning, data mining, data clustering and classifications in the contents of social sciences. It also covers various real-life examples such as...|$|R
50|$|The 1980s were a {{time when}} the {{hospital}} continued to grow. A surgical wing was added in 1980, the entire inside of the hospital received a facelift in 1982 and the <b>business</b> office and <b>data</b> <b>processing</b> center was remodeled to accommodate a new in-house computer system. In 1985, a wing of the hospital was converted into an Emergency Urgent Care facility, including new state-of-the-art treatment facilities. One year later, in 1986, HMH joined Adventist Health and a memorial chapel was added.|$|R
5000|$|The Pick {{operating}} system (often called just [...] "the Pick system" [...] or simply [...] "Pick") is a demand-paged, multiuser, virtual memory, time-sharing computer {{operating system}} based around a unique MultiValue database. Pick is used primarily for <b>business</b> <b>data</b> <b>processing.</b>|$|E
50|$|Dietz Computer Systems was a German {{minicomputer}} manufacturer {{with its}} main office in Mülheim an der Ruhr, Germany. The systems {{were used for}} industrial and <b>business</b> <b>data</b> <b>processing,</b> {{as well as for}} technical and scientific purposes. A popular computer-aided design software, Technovision, ran on the systems produced by Dietz.|$|E
5000|$|The system specifications, {{advanced}} for 1968 [...] - [...] {{five years}} {{before the advent of}} the first commercial personal computers [...] - [...] caused a lot of excitement in the computer industry. The System 21 was aimed, among others, at applications such as mathematical and statistical analysis, <b>business</b> <b>data</b> <b>processing,</b> data entry and media conversion, and educational/classroom use.|$|E
40|$|Abstract. Most {{commercial}} banks have put their <b>business</b> and <b>data</b> <b>processing</b> into centralized IT&Data centers. Fast business developments require IT centers to handle increasing number and volumes of batch processing; new challenges are {{how to make}} full use of available IT resources and have flexible configurations. Cloud Computation is changing the manner how information system architecture is to be redesigned to meet higher service level and IT cost constraints. The core features of IT resource and application virtualization in cloud computation could increase IT center performance and optimize resource allocation algorithms for batch processing in banking, e_commerce and other tense and large-scaled <b>data</b> <b>processing</b> industries. A batch processing optimization scheme is presented which consists in creating a dynamic model by dividing business process into parallel and independent tasks to whom are allocated adequate IT resources. An experimental environment based on Hadoop/MapReduce framework is set up to simulate {{the performance of the}} proposed scheme, the simulation results show inspiring advantages which encourage further research and application work...|$|R
40|$|Cloud {{computing}} {{has increasingly}} {{been used as}} a platform for running large <b>business</b> and <b>data</b> <b>processing</b> applications. Although clouds have become highly popular, when it comes to <b>data</b> <b>processing,</b> the cost of usage is not negligible. Conversely, Desktop Grids, have been used by a plethora of projects, taking advantage of the high number of resources provided for free by volunteers. Merging cloud computing and desktop grids into hybrid infrastructure can provide a feasible low-cost solution for big data analysis. Although frameworks like MapReduce have been conceived to exploit commodity hardware, their use on hybrid infrastructure poses some challenges due to large resource heterogeneity and high churn rate. This study introduces BIGhybrid a toolkit to simulate MapReduce on hybrid environments. The main goal is to provide a framework for developers and system designers to address the issues of hybrid MapReduce. In this paper, we describe the framework which simulates the assembly of two existing middleware: BitDew- MapReduce for Desktop Grids and Hadoop-BlobSeer for Cloud Computing. Experimental results included in this work demonstrate the feasibility of our approach...|$|R
40|$|Abstract—Cloud {{computing}} {{has increasingly}} {{been used as}} a platform for running large <b>business</b> and <b>data</b> <b>processing</b> applications. Although clouds have become extremely popular, when it comes to <b>data</b> <b>processing,</b> their use incurs high costs. Conversely, Desktop Grids, have been used {{in a wide range of}} projects, and are able to take advantage of the large number of resources provided by volunteers, free of charge. Merging cloud computing and desktop grids into a hybrid infrastructure can provide a feasible low-cost solution for big data analysis. Although frameworks like MapReduce have been devised to exploit commodity hardware, their use in a hybrid infrastructure raise some challenges due to their large resource heterogeneity and high churn rate. This study introduces BIGhybrid, a toolkit that is used to simulate MapReduce in hybrid environments. Its main goal is to provide a framework for developers and system designers that can enable them to address the issues of Hybrid MapReduce. In this paper, we describe the framework which simulates the assembly of two existing middleware: BitDew-MapReduce for Desktop Grids and Hadoop-BlobSeer for Cloud Computing. The experimental results that are included in this work demonstrate the feasibility of our approach. I...|$|R
50|$|In {{the first}} half of the 20th century, well before the {{computer}} era, <b>business</b> <b>data</b> <b>processing</b> was done using unit record equipment and punched cards, most commonly the 80-column variety employed by IBM, which dominated the industry. Many tricks were used to squeeze needed data into fixed-field 80-character records. Saving two digits for every date field was significant in this effort.|$|E
50|$|A {{post-secondary}} course targeted at developing skills in programming {{and the use}} of computer language in payrolls, inventories, and other similar business problems. It also includes planning tools such as PERT, CPM, Gantt charts, simulation and other techniques that are business related. Beyond the specialized computer and business education components, it also includes general education. Completion of this course leads to a Certificate in <b>Business</b> <b>Data</b> <b>Processing.</b>|$|E
50|$|In 1971, the School is {{renamed the}} School of Continuing Education (SCE). New diploma {{programs}} are created in <b>business,</b> <b>data</b> <b>processing,</b> computer technology, and systems analysis. The Institute for Paralegal Studies is created. The General Studies Program is instituted to offer {{two years of}} instruction to college-age students. A boom {{in the real estate}} market drives rapid expansion of the Real Estate Institute, which still thrives today.|$|E
40|$|As the Internet {{becomes the}} basis for {{electronic}} commerce, and as more <b>businesses</b> automate their <b>data</b> <b>processing</b> operations, the potential for unauthorized disclosure of sensitive data increases. On-line databases are becoming increasingly large and complex. Sensitive data is transmitted on communication lines and often stored off-line. As a result, the efficient, economical protection of enterprise-critical information is becoming increasingly important in many diverse application environments. The protection required to conduct commerce on the Internet, provide data confidentiality, and provide user authentication can be achieved only by cryptographic services and techniques. The high-speed, physically secure IBM S/ 390 ® CMOS Cryptographic Coprocessor for S/ 390 Parallel Enterprise Servers TM, together with th...|$|R
40|$|There are two obvious {{methods to}} map a two-dimension {{relational}} database table onto a one-dimensional storage interface: store the table row-by-row, or store the table column-by-column. Traditionally, database system implementations and research {{have focused on}} the row-by row data layout, since it performs best on the most common application for database systems: <b>business</b> transactional <b>data</b> <b>processing.</b> However, there are a set of emerging applications for database systems for which the row-by-row layout performs poorly. These applications are more analytical in nature, whose goal is to read through the data to gain new insight and use it to drive decision making and planning. In this paper, we study the poor performance of row-by-row data layout for these emerging applications, and evaluate the column-by-column data layout opportunity as a solution to this problem. The solution will be analyzed and represented by graph. At the end of the paper we will see the comparative performance of Oracle 10 g and MSSQLServer database...|$|R
40|$|There are two obvious ways to map a two-dimension {{relational}} database table onto a one-dimensional storage in-terface: store the table row-by-row, or store the table column-by-column. Historically, database system imple-mentations and research {{have focused on}} the row-by row data layout, since it performs best on the most common application for database systems: <b>business</b> transactional <b>data</b> <b>processing.</b> However, there are a set of emerging applications for database systems for which the row-by-row layout performs poorly. These applications are more analytical in nature, whose goal is to read through the data to gain new insight and use it to drive decision making and planning. In this dissertation, we study the problem of poor performance of row-by-row data layout for these emerging applications, and evaluate the column-by-column data layout opportunity as a solution to this problem. There have been a variety of proposals in the literature for how to build a database system on top of column-by-column layout...|$|R
