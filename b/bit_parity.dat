16|726|Public
5000|$|... where M = pq is {{the product}} of two large primes p and q. At each step of the algorithm, some output is derived from xn+1; the output is {{commonly}} either the <b>bit</b> <b>parity</b> of xn+1 or {{one or more of the}} least significant bits of xn+1.|$|E
50|$|As a 3838/1553B data word, the HS action word is {{preceded by}} the 3 bit-time data word sync field and {{followed}} by the single <b>bit</b> <b>parity</b> bit. As part of a 3838/1553B BC-RT transfer, it is preceded by a 3838/1553B command word, and should normally, i.e. if not broadcast, invalid, or illegal, elicit a 3838/1553B status word from the receiving RT.|$|E
40|$|In {{paper the}} {{algorithm}} of {{reception of the}} binomial modified codes is given. Parities for calculation of probability of a undetectable error are offered at use of such codes. The estimation of a noise-immunity of the binomial modified, equilibrium, cyclic codes and a code with <b>bit</b> <b>parity</b> for communication channels with independent distribution of errors is carried out. The carried out researches show that the binomial modified code provides high ability to detection of mistakes in communication channels {{with a high level}} of asymmetry. When you are citing the document, use the following link [URL]...|$|E
5000|$|... #Caption: Graphical {{depiction}} of the four data <b>bits</b> and three <b>parity</b> <b>bits</b> and which <b>parity</b> <b>bits</b> apply to which data bits ...|$|R
30|$|For m[*]=[*] 3, we have (7, 4, 3) Hamming error {{correcting}} codes. These Hamming codes encode 4 bits of {{data into}} 7 bit blocks (a Hamming code word). The extra 3 <b>bits</b> are <b>parity</b> <b>bits.</b> Each of the 3 <b>parity</b> <b>bits</b> is <b>parity</b> for 3 of the 4 data bits, and no 2 <b>parity</b> <b>bits</b> {{are for the}} same 3 data bits. All of the <b>parity</b> <b>bits</b> are even <b>parity.</b> The (7, 4, 3) Hamming error correcting code can correct 1 error {{in each of the}} Hamming codeword.|$|R
5000|$|There are two {{variants}} of parity bits: even <b>parity</b> <b>bit</b> and odd <b>parity</b> <b>bit.</b>|$|R
40|$|AbstractIn {{this paper}} we have {{presented}} a new symmetric encryption technique where a BCD converter, a four <b>bit</b> <b>parity</b> checker {{along with a}} sign function is used to generate the key sequence. In the next subsequent steps the input text and the key sequence are combined using the bitwise OR gate and NOT gate to produce the cipher text easily. The similar kinds of operations are performed in reverse succession to extract the original text from the cipher. Finally, by evaluating the obtained results against the AEC, DEC and BLOWFISH algorithms the competence of the proposed algorithm can be approximated...|$|E
40|$|This pap er {{presents}} exp erimental results ob-tained {{during the}} training of an analog hard-ware neural network. A simple genetic al-gorithm is used to optimize the synaptic weights. The parameter space of this algo-rithm has b een intensively scanned for two learning tasks (4 and 5 <b>bit</b> <b>parity).</b> The re-sults provide a quantitative insight into the interdep endencies of the evolution parame-ters and how the optimal settings are pre-determined by the learning problem. It is observed that p opulation sizes {{in the order of}} 15 in connection with mutation rates of ab out 1 % yield the b est p erformance of the trainin...|$|E
40|$|Network size {{of neural}} {{networks}} is {{highly dependent on}} activation functions. A trainable activation function has been proposed, which consists of a linear combination of some basic functions. The activation functions and the connection weights are simultaneously trained. An 8 <b>bit</b> <b>parity</b> problem can be solved by using a single output unit and no hidden unit. In this paper, we expand this model to multilayer neural networks. Furthermore, nonlinear functions are used at the unit inputs in order to realize more flexible transfer functions. The previous activation functions and the new nonlinear functions are also simultaneously trained. More complex pattern classification problems can be solved with {{a small number of}} units and fast convergence...|$|E
50|$|Shown {{are only}} 20 encoded <b>bits</b> (5 <b>parity,</b> 15 data) but the pattern {{continues}} indefinitely. The key thing about Hamming Codes {{that can be}} seen from visual inspection is that any given bit is included in a unique set of <b>parity</b> <b>bits.</b> To check for errors, check all of the <b>parity</b> <b>bits.</b> The pattern of errors, called the error syndrome, identifies the bit in error. If all <b>parity</b> <b>bits</b> are correct, there is no error. Otherwise, the sum of the positions of the erroneous <b>parity</b> <b>bits</b> identifies the erroneous bit. For example, if the <b>parity</b> <b>bits</b> in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one <b>parity</b> <b>bit</b> indicates an error, the <b>parity</b> <b>bit</b> itself is in error.|$|R
50|$|<b>Parity</b> <b>bit</b> {{checking}} is used occasionally for transmitting ASCII characters, {{which have}} 7 bits, leaving the 8th <b>bit</b> as a <b>parity</b> <b>bit.</b>|$|R
5000|$|... words: 6 chars (32 data <b>bits,</b> 1 <b>parity</b> <b>bit,</b> 3 {{end of file}} bits) ...|$|R
40|$|This {{research}} is proposed the watermarking scheme in tamper localization which {{is able to}} detect {{the location of the}} manipulated areas and verify the authentic by using the block average intensity. The watermark data for ROI will store in the region of RONI. The embedded watermark allows the image have been tamper can recover and check for the tampering detection. The usage of the block average intensity is one of the popular techniques due to its easy to implementation. In this research, the tamper localization will performed using block average intensity in the tamper watermarking scheme for medical image. The block average intensity and the sub-block average intensity will be used to find out the tamper area by using the authentication <b>bit,</b> <b>parity</b> check bit and average authentication bit...|$|E
40|$|Abstract—Back {{propagation}} {{is one of}} {{the well}} known training algorithms for multilayer perceptron. However the rate of convergence in back propagation learning tends to be relatively slow, which in turn makes it computationally excruciating. Over the last years many modifications have been proposed to improve the efficiency and convergence speed of the back propagation algorithm. The main emphasis of this paper is on investigating the performance of improved versions of back propagation algorithm in training the neural network. All of them are assessed on different training sets and a comparative analysis is made. Results of computer simulations with standard benchmark problems such as XOR, 3 <b>BIT</b> <b>PARITY,</b> MODIFIED XOR and IRIS are presented. The training performance of these algorithms is evaluated in terms of percentage of accuracy, and convergence speed. Index Terms—ANN, gain, momentum, error saturation...|$|E
40|$|In this thesis, {{we develop}} network {{information}} hiding methods {{which can be}} utilized to transmit external information between two network end-systems as stealthy as possible. Here, we propose three network information hiding methods namely, Packet Payload Byte Parity Based Steganography, Alternative Packet Payload <b>Bit</b> <b>Parity</b> Based Steganography and ﬁnally, the IP Identiﬁcation Based Steganographic method. In the ﬁrst and second methods, we exploit the parity of the payload to encode external information. We examine and analyze the packet lengths of normal trafﬁc {{to show that the}} methods can cope with trafﬁc anomaly detection methods and do not introduce noticeable overhead. Results suggest that the proposed methods achieve high undetectability at 1 bit per packet of channel capacity. On the other hand, we propose to utilize the IP identiﬁcation ﬁeld to establish steganographic communication in operating systems which manifest randomness in their IP identiﬁcation generation...|$|E
40|$|A channel {{encoding}} {{apparatus and}} method {{are provided in}} which part of the <b>parity</b> <b>bits</b> are set to erroneous <b>bits,</b> and full <b>parity</b> <b>bits</b> are created by correcting the erroneous bits using a channel decoding apparatus of a receiver in a communication system. In the channel encoding apparatus, in order to generate a coded bit stream by adding a <b>parity</b> <b>bit</b> stream to a message bit stream, a partial parity generator generates a partial <b>parity</b> <b>bit</b> stream {{as a part of}} the <b>parity</b> <b>bit</b> stream using the message bit stream, an erasure generator generates a bit stream having an erroneous value as the remaining part of the <b>parity</b> <b>bit</b> stream, and a decoder calculates the value of the <b>parity</b> <b>bit</b> stream by correcting the bit stream having the erroneous value using a parity-check matrix that determines the <b>parity</b> <b>bit</b> stream, the message bit stream, and the partial <b>parity</b> <b>bit</b> stream. Samsung Electronics Co., Ltd. Georgia Tech Research Corporatio...|$|R
5000|$|<b>Parity</b> <b>bit</b> 1 {{covers all}} bit {{positions}} {{which have the}} least significant bit set: <b>bit</b> 1 (the <b>parity</b> <b>bit</b> itself), 3, 5, 7, 9, etc.|$|R
5000|$|<b>Parity</b> <b>bit</b> 2 {{covers all}} bit {{positions}} {{which have the}} second least significant bit set: <b>bit</b> 2 (the <b>parity</b> <b>bit</b> itself), 3, 6, 7, 10, 11, etc.|$|R
40|$|This paper {{examines}} {{the effect of}} new technology trends such as less power consumption and smaller chip feature size on very large scale integration devices. The conventional way of detecting soft error is to save one parity bit with each message word and by checking the parity bit at the output we can detect the soft error. But in proposed method, an approach is provided which can be applied on previously saved data and afterwards error can be detected. In this method 64 bit data is saved in a VLSI chip and then 8 <b>bit</b> <b>parity</b> sequence of this data is generated. After sometimes due to some transient changes like alpha particles from package decay, cosmic rays emission and thermal neutrons, error may be generated in that saved data of 64 bit. The proposed method {{can be used to}} detect a single bit soft error from the saved data by using the parity detection method...|$|E
40|$|Abstract—This paper {{describes}} about novel key {{expansion and}} its inversion technique for private key cryptosystems. Our design uses (8, 4) Extended Hamming Code and its error control logic to produce memory efficient key schedule generation algorithm. A mathematical relationship between 4 bit word and its corresponding 4 <b>bit</b> <b>parity</b> bits is shown. Simplicity, symmetry elimination, diffusion and non-linearity {{of the proposed}} key expansion technique are described as the key schedule generation criteria. Proposed method removes the usage of S-box to reduce the working memory of the algorithm. High nonlinearity penetration of original input message bits is achieved by applying modulo 2 addition of code based key schedules for each round transformations. Security strength among these key schedules is achieved by intentional bit inversions among them with beyond the error correcting limitations of chosen code. Comparative results between proposed design and Rijndael algorithm is illustrated {{with the aid of}} Xilinx Simulation tool. This paper concludes that novel key generation technique by Error Control Algorithm of wireless communication channel is an alternative solution to the cryptosystems without S-box substitution and any lookup tables...|$|E
40|$|We {{present a}} {{technique}} for parallelizing {{the training of}} neural networks. Our technique is designed for parallelization on a cluster of workstations. To take advantage of parallelization on clusters, a solution must account for the higher network latencies and lower bandwidths of clusters as compared to custom parallel architectures. Parallelization approaches that may work well on special purpose parallel hardware, such as distributing the neurons of the neural network across processors, {{are not likely to}} work well on cluster systems because communication costs to process a single training pattern are too prohibitive. Our solution, Pattern Parallel Training, duplicates the full neural network at each cluster node. Each cooperating process in the cluster trains the neural network on a subset of the training set each epoch. We demonstrate the effectiveness of our approach by implementing and testing an MPI version of Pattern Parallel Training for the eight <b>bit</b> <b>parity</b> problem. Our results show a significant speed-up in training time as compared to sequential training. In addition, we analyze the communication costs of our technique and discuss which types of common neural network problems would benefit most from our approach...|$|E
50|$|This type of {{software}} {{is capable of}} emulating all serial port functionality, including Baud rate, data <b>bits,</b> <b>parity</b> <b>bits,</b> stop bits, etc. Additionally, it allows the data flow to be controlled, emulating all signal lines (DTR / DSR / CTS / RTS / DCD / RI) and customizing pinout.|$|R
40|$|This bachelor’s {{thesis is}} about two peripheries. First {{periphery}} creates from input parallel signals one output serial signal. This serial signal contains a start bit, the next are data <b>bits,</b> <b>parity</b> <b>bit</b> and stop bit or two stop bits. Data bits are variables. It is mean their count is set with two input signals called Dat 0 and Dat 1. We can secure data <b>bits</b> with <b>parity</b> <b>bit.</b> Of course we have choice between even <b>parity</b> <b>bit</b> or odd <b>parity</b> <b>bit.</b> After <b>parity</b> <b>bit</b> there is one stop bit or there are two stop bits. Second periphery realizes I 2 C bus. This communication is between two devices. First device is called master and creates the communication with second device called slave. For communication there are two bidirectional lines. The first line is called SDA, which is a serial data line and second line is a serial clock line called SCL. Communication begins with a start condition. That means line SDA go from high to low while SCL is high and communication is terminate with a stop condition. That means line SDA go from low to high while SCL is high. The peripheries are programming in VHDL language and implemented in FPGA device. After successful simulation in free software ISE WebPACK the peripheries was realized in the development board V 2 MB 1000 with device XC 2 V 1000...|$|R
50|$|The {{communications}} protocol {{used on a}} Wiegand interface {{is known as the}} Wiegand protocol. The original Wiegand format had one <b>parity</b> <b>bit,</b> 8 bits of facility code, 16 bits of ID code, and a trailing <b>parity</b> <b>bit</b> for a total of 26 <b>bits.</b> The first <b>parity</b> <b>bit</b> is calculated from the first 12 bits of the code and the trailing <b>parity</b> <b>bit</b> from the last 12 bits. However, many inconsistent implementations and extensions to the basic format exist.|$|R
40|$|Abstract: In this paper, we reinvestigate the {{solution}} for chaotic time series prediction problem using neural network approach. The {{nature of this}} problem is such that the data sequences are never repeated, but they are rather in chaotic region. However, these data sequences are correlated between past, present, and future data in high order. We use Cascade Error Projection (CEP) learning algorithm to capture the high order correlation {{between past and present}} data to predict a future data using limited weight quantization constraints. This will help to predict a future information that will provide us better estimation in time for intelligent control system. In our earlier work, {{it has been shown that}} CEP can suflciently learn 5 - 8 <b>bit</b> <b>parity</b> problem with 4 - or more bits, and color segmentation problem with 7 - or more bits of weight quantization. In this paper, we demonstrate that chaotic time series can be learned and generulized well with as low as 4 -bit weight quantization using round-offand truncation techniques. The results show that generalization feature will suffer less as more bit weight quantization is available and error surfaces with the round-off technique are more symmetric around zero than error surfaces with the truncation technique. This study suggests that CEP is a...|$|E
40|$|In this paper, we reinvestigate the {{solution}} for chaotic time series prediction problem using neural network approach. The {{nature of this}} problem is such that the data sequences are never repeated, but they are rather in chaotic region. However, these data sequences are correlated between past, present, and future data in high order. We use Cascade Error Projection (CEP) learning algorithm to capture the high order correlation {{between past and present}} data to predict a future data using limited weight quantization constraints. This will help to predict a future information that will provide us better estimation in time for intelligent control system. In our earlier work, {{it has been shown that}} CEP can sufficiently learn 5 - 8 <b>bit</b> <b>parity</b> problem with 4 - or more bits, and color segmentation problem with 7 - or more bits of weight quantization. In this paper, we demonstrate that chaotic time series can be learned and generalized well with as low as 4 -bit weight quantization using round-off and truncation techniques. The results show that generalization feature will suffer less as more bit weight quantization is available and error surfaces with the round-off technique are more symmetric around zero than error surfaces with the truncation technique. This study suggests that CEP is an implementable learning technique for hardware consideration...|$|E
40|$|Steganography {{technique}} is an approach of data obscuring which is invisible i. e. presence of hidden data is unknown to unintended users. The secret message {{is embedded in}} a cover medium which can be an image or text file and resultant cover object is transmitted over the untrusted channel. Steganography is gaining due significance due to the enormous increase in secret communication between potential computer users over the internet. It can also {{be defined as the}} study of invisible transmission that usually deals with the ways of hiding the existence of the communicated message. Generally data embedding is achieved in communication, image, text, voic e or multimedia content for copyright, military communication, authentication and many other purposes. Steganography is unlike cryptography in the way that cryptography obscures the contents of clandestine message whereas steganography is hiding the existence message. In this paper an overview of steganography has been elaborated. This paper studies and reviews various data obscuring techniques based on spatial domain image steganography like Least Significant Bit, Most Significant <b>Bit,</b> <b>Parity</b> check, Pixel value differencing. It elaborates an overview of steganography and illustrates the process of steganography. Various classifications of steganographic techniques are discussed. The image domain / spatial domain techniques are discussed in detail and a relative comparison is derived between these techniques on the basis of various parameters like robustness, image quality, an...|$|E
50|$|Virtual {{serial port}}s emulate all {{hardware}} serial port functionality, including baud rate, data <b>bits,</b> <b>parity</b> <b>bits,</b> stop bits, etc. Additionally, they allow controlling the data flow, emulating all signal lines (DTR, DSR, CTS, RTS, DCD, and RI) and customizing pinout. Virtual serial ports are common with Bluetooth {{and are the}} standard way of receiving data from Bluetooth-equipped GPS modules.|$|R
5000|$|Since {{the source}} is only 4 bits {{then there are}} only 16 {{possible}} transmitted words. Included is the eight-bit value if an extra <b>parity</b> <b>bit</b> is used (see Hamming(7,4) code with an additional <b>parity</b> <b>bit).</b> (The data bits are shown in blue; the <b>parity</b> <b>bits</b> are shown in red; and the extra <b>parity</b> <b>bit</b> shown in green.) ...|$|R
5000|$|Therefore, Triple DES uses a [...] "key bundle" [...] that {{comprises}} three DES keys, K1, K2 and K3, each of 56 <b>bits</b> (excluding <b>parity</b> <b>bits).</b> The {{encryption algorithm}} is: ...|$|R
40|$|Full version: Access {{restricted}} permanently due to 3 rd party copyright restrictions. Restriction set on 24 / 05 / 2017 by SC, Graduate schoolShingled Magnetic Recording (SMR) {{technology is}} important in the immediate need for expansion of magnetic hard disk beyond the limit of current disk technology. SMR provides a solution with the least change from current technology among contending technologies. Robust easy to implement Digital Signal Processing (DSP) techniques are needed to achieve the potentials of SMR. Current DSP techniques proposed border on the usage of Two Dimensional Magnetic Recording (TDMR) techniques in equalisation and detection, coupled with iterative error correction codes such as Low Density Parity Check (LDPC). Currently, Maximum Likelihood (ML) algorithms are normally used in TDMR detection. The shortcomings of the ML detections used is the exponential complexities with respect to the number of bits. Because of that, reducing the complexity of the processes in SMR Media is very important in order to actualise the deployment of this technology to personal computers in the near future. This research investigated means of reducing the complexities of equalisation and detection techniques. Linear equalisers were found to be adequate for low density situations. Combining ML detector across-track with linear equaliser along-track was found to provide low complexity, better performing alternative as compared to use of linear equaliser across track with ML along track. This is achieved if density is relaxed along track and compressed more across track. A gain of up to 10 dB was achieved. In a situation with high density in both dimensions, full two dimensional (2 D) detectors provide better performance. Low complexity full 2 D detector was formed by serially concatenating two ML detectors, one for each direction, instead of single 2 D ML detector used in other literature. This reduces complexity with respect to side interference from exponential to linear. The use of a single <b>bit</b> <b>parity</b> as run length limited code at the same time error correction code is also presented with a small gain of about 1 dB at BER of 10 ^- 5 recorded for the situation of high density. Emerging Markets Telecommunication Services Limited (Etisalat Nigeria) ...|$|E
3000|$|The {{binary data}} stream at the {{transmitter}} input of an OFDM system is encoded {{in order to}} generate a <b>parity</b> <b>bit</b> stream. These <b>parity</b> <b>bits</b> are used for the error correction on the receiver side (forward error correction - FEC). L [...]...|$|R
50|$|If a bit {{is present}} {{at a point}} {{otherwise}} dedicated to a <b>parity</b> <b>bit,</b> but is not used for parity, it may {{be referred to as}} a mark <b>parity</b> <b>bit</b> if the <b>parity</b> <b>bit</b> is always 1, or a space <b>parity</b> <b>bit</b> if the bit is always 0. In such cases where the value of the bit is constant, it may be called a stick <b>parity</b> <b>bit</b> even though its function {{has nothing to do with}} parity. The function of such bits varies with the system design, but examples of functions for such bits include timing management, or identification of a packet as being of data or address significance. If its actual bit value is irrelevant to its function, the bit amounts to a Don't-care term.|$|R
50|$|As a {{practical}} matter, the hardware {{that forms the}} accumulators is reused during the encoding process. That is, once a first set of <b>parity</b> <b>bits</b> are generated and the <b>parity</b> <b>bits</b> stored, the same accumulator hardware is used to generate a next set of <b>parity</b> <b>bits.</b>|$|R
50|$|The speed {{includes}} bits for framing (stop <b>bits,</b> <b>parity,</b> etc.) and so {{the effective}} data rate {{is lower than the}} bit transmission rate. For example, with 8-N-1 character framing only 80% of the bits are available for data (for every eight bits of data, two more framing bits are sent).|$|R
50|$|The {{addition}} of the fourth row effectively computes {{the sum of all}} the codeword <b>bits</b> (data and <b>parity)</b> as the fourth <b>parity</b> <b>bit.</b>|$|R
