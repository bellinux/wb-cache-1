9|8|Public
40|$|In this article, {{a method}} of {{predicting}} colour appearance (from colorimetric attributes to colour-appearance attributes, i. e., forward model) using an artificial neural network is presented. The neural network model developed is a multilayer feedforward neural network model for predicting colour appearance (FNNCAM for short). The model was trained by LUTCHI colour-appearance datasets. The Levenberg-Marquardt algorithm is incorporated into the <b>back-propagation</b> <b>procedure</b> to accelerate the training of FNNCAM and the Bayesian regularization method {{is applied to the}} training of neural networks to improve generalization. The results of FNNCAM obtained are quite promising. Institute of Textiles and ClothingDepartment of Computin...|$|E
40|$|Abstract. In Evolutionary Robotics, auto-teaching {{networks}}, {{neural networks}} that modify their own weights during the life-time of the robot, {{have been shown}} to be powerful architectures to develop adaptive controllers. Unfortunately, when run for a longer period of time than that used during evolution, the long-term behavior of such networks can become unpredictable. This paper gives an example of such dangerous behavior, and proposes an alternative solution based on anticipation: as in auto-teaching networks, a secondary network is evolved, but its outputs try to predict the next state of the robot sensors. The weights of the action network are adjusted using some <b>back-propagation</b> <b>procedure</b> based on the errors made by the anticipatory network. First results – in simulated environments – show a tremendous increase in robustness of the long-term behavior of the controller. ...|$|E
40|$|This paper {{presents}} {{a method for}} optimizing vibration absorbers on a flexible beam {{by means of a}} neural network procedure. Analytical results for the response of the beam with multi-absorbers have been obtained using a transfer matrix method. To achieve optimal control, a cost function composed of second-order nonlinear forms in terms of the displacement, strain, and frequencies was used. A method of minimizing the function was presented, in which input values, weights and output function were effectively controlled {{on the basis of the}} <b>back-propagation</b> <b>procedure</b> of the neural network method. Numerical calculations were carried out for an example problem. It is ascertained that the present method gives a better result than that obtained by the previously published method. The validity of the method and the analytical results was confirmed by experimental tests...|$|E
40|$|Reliability of tidal-level {{forecasting}} {{is essential}} for structure installation and human activities in the marine environment. This paper reports an application of the artificial neural network with <b>back-propagation</b> <b>procedures</b> for accurate forecast of tidal-level variations. Unlike the conventional harmonic analysis, this neural network model forecasts the time series of tidal levels directly using a learning process based {{on a set of}} previous data. Two sets of field data with diurnal and semidiurnal tide, respectively, were used to test the performance of the neural network model. Results indicate that the hourly tidal levels over a long duration can be efficiently predicted using only a very short-term hourly tidal record...|$|R
40|$|Abstract: This paper {{attempts}} {{to establish a}} fuzzy neural automatic incident detection (FNAID) algorithm, using <b>back-propagation</b> training <b>procedures.</b> A rolling training procedure continuously updating the traffic flow parameters is proposed to enhance the adaptability of FNAID to different traffic flow conditions. A real incident case is deliberately generated to calibrate the traffic simulator—Paramics. To validate the FNAID with and without rolling training procedure, the calibrated Paramics is used to simulate sufficient incident samples. The off-line tests and statistic tests conclude that under various traffic flow conditions, the FNAID with rolling training procedure has better detection performance than the one without rolling. Key Words: freeway incident detection algorithm, fuzzy neural network, rolling training procedure, traffic simulatio...|$|R
40|$|To {{illustrate}} {{the potential of}} multilayer neural networks for adaptive interfaces, we used a VPL DataGlove connected to a DECtalk speech synthesizer via five neural networks to implement a hand-gesture to speech system. Using minor variations of the standard <b>back-propagation</b> learning <b>procedure,</b> the complex mapping of hand movements to speech is learned using data obtained from a single "speaker" in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1 % of the time, and no word is produced about 5 % of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks {{can be used to}} develop the complex mappings required in a high bandwidth interface that adapts to the individual user. 3 This research was supported by the Institute for Robotics and Int [...] ...|$|R
40|$|This {{communication}} {{presents a}} microwave strategy for early breast cancer detection, which exploits magnetic nanoparticle (MNP) as contrast agent to artificially induce a magnetic anomaly localized into the cancer. The strategy estimates the anomaly by processing {{the difference of}} two data sets obtained owing to the capability of switching {{on and off the}} magnetic susceptibility by means of an applied static polarizing magnetic field. By doing so, data depending only on the magnetic anomaly are obtained and the imaging is faced as a linear inverse scattering problem. In particular, the <b>back-propagation</b> <b>procedure,</b> formulated for the magnetic case, is applied. Numerical examples provide a preliminary assessment of the achievable reconstruction capabilities and the robustness against measurement noise as well as the uncertainty on the electric scenario surrounding the MNP...|$|E
40|$|We {{present a}} novel {{deep neural network}} {{architecture}} for unsupervised subspace clustering. This architecture is built upon deep auto-encoders, which non-linearly map the input data into a latent space. Our key idea is to introduce a novel self-expressive layer between the encoder and the decoder to mimic the "self-expressiveness" property that has proven effective in traditional subspace clustering. Being differentiable, our new self-expressive layer provides a simple but effective way to learn pairwise affinities between all data points through a standard <b>back-propagation</b> <b>procedure.</b> Being nonlinear, our neural-network based method is able to cluster data points having complex (often nonlinear) structures. We further propose pre-training and fine-tuning strategies that let us effectively learn the parameters of our subspace clustering networks. Our experiments show that the proposed method significantly outperforms the state-of-the-art unsupervised subspace clustering methods. Comment: Accepted to NIPS' 1...|$|E
40|$|We {{propose a}} new machine {{learning}} paradigm called Graph Transformer Networks that extends {{the applicability of}} gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of <b>back-propagation</b> <b>procedure.</b> A complete check reading system based on these concept is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provides record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks a month. 1 Introduction The most common technique for building document processing systems is to partition the task into manageable subtasks, such as field detection, word segmentation, or character recognition, and to build a separate module for each one. Typically, each module is trained [...] ...|$|E
40|$|In recent years, {{there has}} been an {{increasing}} interest in the fusion of neural networks and fuzzy logic. Most of the existing fuzzy neural network (FNN) models have been proposed to implement different types of single-stage fuzzy reasoning mechanisms. Single-stage fuzzy reasoning, however, is only the most basic among a human being's various types of reasoning mechanisms. Syllogistic fuzzy reasoning, where the consequence of a rule in one reasoning stage is passed to the next stage as a fact, is essential to effectively build up a large scale system with high level intelligence. In {{view of the fact that}} the fusion of syllogistic fuzzy logic and neural networks has not been sufficiently studied, a new FNN model based on syllogistic fuzzy reasoning, termed cascaded FNN (CFNN), is proposed in this paper. From the stipulated input-output data pairs, the model can generate an appropriate syllogistic fuzzy rule set through structure (genetic) learning and parameter (<b>back-propagation)</b> learning <b>procedures</b> proposed in this paper. In addition, we particularly discuss and analyze the performance of the proposed model in terms of approximation ability and robustness as compared with single-stage FNN models. The effectiveness of the proposed CFNN model is demonstrated through simulating two benchmark problems in fuzzy control and nonlinear function approximation domain. Department of Computin...|$|R
40|$|In {{this paper}} we {{formulate}} a general class of neural network based filters, where each node is a morphological/rank operation. This {{type of system}} is computationally efficient since no multiplications are necessary. The introduction of such networks is partially motivated from observations that internal structures of a neuron can generate logic operations. An efficient adaptive optimal design procedure is proposed for these networks, based on the <b>back-propagation</b> algorithm. The <b>procedure</b> is optimal under the LMS criterion. Finally, experimental results are illustrated in problems of noise cancellation, encouraging {{the use of such}} class of systems and its training algorithm as important tools for nonlinear signal and image processing. 1. INTRODUCTION Adaptive filters and (artificial) neural networks (NNs) are closely related, and their adaptation/training can be studied under the same framework [1]. In this sense, a NN-filter is a multilayer feed-forward network with only one output, [...] ...|$|R
40|$|A {{method that}} allows us to give a {{different}} treatment to any neuron inside feedforward neural networks is presented. The algorithm has been implemented with two very different learning methods: a standard <b>Back-propagation</b> (BP) <b>procedure</b> and an evolutionary algorithm. First, we have demonstrated that the EA training method converges faster and gives more accurate results than BP. Then we have made a full analysis of the effects of turning off different combinations of neurons after the training phase. We demonstrate that EA is much more robust than BP for all the cases under study. Even in the case when two hidden neurons are lost, EA training is still able to give good average results. This difference implies that we must be very careful when pruning or redundancy effects are being studied since the network performance when losing neurons strongly depends on the training method. Moreover, the influence of the individual inputs will also depend on the training algorithm. Since EA keeps a good classification performance when units are lost, this method could {{be a good way to}} simulate biological learning systems since they must be robust against deficient neuron performance. Although biological systems are much more complex than the simulations shown in this article, we propose that a smart training strategy such as the one shown here could be considered as a first protection against the losing of a certain number of neurons...|$|R
40|$|Objective {{functions}} for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, {{we present a}} novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special purpose <b>back-propagation</b> <b>procedure,</b> peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate {{the superiority of the}} PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset. Comment: Published in ECCV 201...|$|E
40|$|The {{classification}} of small cell lung cancer (SCLC) and {{non-small cell lung cancer}} (NSCLC) can pose diagnostic problems due to inter-observer variability and other limitations of histopathology. There is {{an interest in}} developing classificatory models of lung neoplasms based on the analysis of multivariate molecular data with statistical methods and/or neural networks. DNA methylation levels at 20 loci were measured in 41 SCLC and 46 NSCLC cell lines with the quantitative real-time PCR method MethyLight. The data were analyzed with artificial neural networks (ANN) and linear discriminant analysis (LDA) to classify the cell lines into SCLC or into NSCLC. Models used either data from all 20 loci, or from five significant DNA methylation loci that were selected by a step-wise <b>back-propagation</b> <b>procedure</b> (PTGS 2, CALCA, MTHFR, ESR 1, and CDKN 2 A). The data were sorted randomly by cell line into 10 different data sets, each with training and testing subsets composed of 71 and 16 of the cases, respectively. Ten ANN models were trained using the 10 data sets: five using 20 variables, and five using the five variables selected by step-wise back-propagation. The ANN models with 20 input variables correctly classified 100 % of the cell lines, while the models with only five variables correctly classified 87 to 100 % of cases. For comparison, 10 different LDA models were trained and tested using the same data sets with either the original data or with logarithmically transformed data. Again, half of the models used all 20 variables while the others used only the five significant variables. LDA models provided correct classifications in 62. 5 % to 87. 5 % of cases. The classifications provided by all of the different models were compared with kappa statistics, yielding kappa values ranging from 0. 25 to 1. 0. We conclude that ANN models based on DNA methylation profiles can objectively classify SCLC and NSCLC cells lines with substantial to perfect concordance, while LDA models based on DNA methylation profiles provide poor to substantial concordance. Our work supports the promise of ANN analysis of DNA methylation data as a powerful approach for the development of automated methods for lung cancer classification...|$|E
40|$|Knowledge of the {{three-dimensional}} world {{is essential for}} many guidance and navigation applications. A sequence of images from an electro-optical sensor can be processed using optical flow algorithms to provide a sparse set of ranges {{as a function of}} azimuth and elevation. A natural way to enhance the range map is by interpolation. However, this should be undertaken with care since interpolation assumes continuity of range. The range is continuous in certain parts of the image and can jump at object boundaries. In such situations, the ability to detect homogeneous object regions by scene segmentation can be used to determine regions in the range map that can be enhanced by interpolation. The use of scalar features derived from the spatial gray-level dependence matrix for texture segmentation is explored. Thresholding of histograms of scalar texture features is done for several images to select scalar features which result in a meaningful segmentation of the images. Next, the selected scalar features are used with a neural net to automate the segmentation <b>procedure.</b> <b>Back-propagation</b> is used to train the feed forward neural network. The generalization of the network approach to subsequent images in the sequence is examined. It is shown that the use of multiple scalar features as input to the neural network result in a superior segmentation when compared with a single scalar feature. It is also shown that the scalar features, which are not useful individually, result in a good segmentation when used together. The methodology is applied to both indoor and outdoor images...|$|R
40|$|Restricted Boltzmann {{machines}} (RBMs) {{and their}} extensions, often called "deep-belief networks", are very powerful neural networks that have found widespread applicability {{in the fields}} of machine learning and big data. The standard way to training these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called "contrastive divergence", and additional supervised tuning via <b>back-propagation.</b> However, this <b>procedure</b> has been shown not to follow any gradient and can lead to suboptimal solutions. In this paper, we show a very efficient alternative to contrastive divergence by means of simulations of digital memcomputing machines (DMMs). We test our approach on pattern recognition using the standard MNIST data set of hand-written numbers. DMMs sample very effectively the vast phase space defined by the probability distribution of RBMs over the test sample inputs, and provide a very good approximation close to the optimum. This efficient search significantly reduces the number of generative pre-training iterations necessary to achieve a given level of accuracy in the MNIST data set, as well as a total performance gain over the traditional approaches. In fact, the acceleration of the pre-training achieved by simulating DMMs is comparable to, in number of iterations, the recently reported hardware application of the quantum annealing method on the same network and data set. Notably, however, DMMs perform far better than the reported quantum annealing results in terms of quality of the training. Our approach is agnostic about the connectivity of the network. Therefore, it can be extended to train full Boltzmann machines, and even deep networks at once. Comment: 6 pages, 4 figure...|$|R

