75|602|Public
5000|$|... {{the data}} {{transfer}} between host and USB-ICC using <b>bulk</b> <b>transfers</b> or control transfers; ...|$|E
50|$|The Uniform Commercial Code {{is another}} {{responsibility}} of the Business Filings Division, this Code conducts the laws of commercial transactions. This includes the sale of goods, commercial paper, bank deposits and collections, letters of credit, <b>bulk</b> <b>transfers,</b> bills of lading and investment securities.|$|E
5000|$|<b>Bulk</b> <b>transfers</b> tap {{the channel}} as {{bandwidth}} is available. Delivery is guaranteed, but neither transfer rate nor latency are, though the host can attempt to leverage pending transfers or endpoints. They {{are used for}} high-volume transfers exhibiting a sharp time-varying behavior. They use unidirectional pipes.|$|E
50|$|Along the {{southern}} boundary of North Albany lies a railroad owned by CSXT (the Chicago Line), {{which is also}} used by Amtrak. The former D&H Colonie Main Line (now owned by CP Rail) runs through North Albany, at Erie Street and Erie Boulevard sits a <b>bulk</b> <b>transfer</b> rail yard operated by <b>BULK</b> <b>Transfer</b> Services LLC.|$|R
5000|$|I/O {{processor}} interconnection: remote {{procedure call}} over a serial link, DMA controller for <b>bulk</b> <b>transfer</b> ...|$|R
50|$|The {{water is}} {{conveyed}} to the Coppermills Water Treatment Works for treatment, with the facility for <b>bulk</b> <b>transfer</b> to Essex and Suffolk Water.|$|R
50|$|All {{mainline}} rail {{operated by}} PBVR has 286,000 pound gross-weight-on-rail capability. PBVR can store up to 429 rail cars {{at any one}} time. Multi-modal warehouse and trans-load facilities are available. The multi-modal facility {{is located in the}} Gulf Coast Foreign Trade Zone (FTZ) #92 within the industrial park. A trans-loading site is available for other prime sites for <b>bulk</b> <b>transfers</b> and storage of product.|$|E
50|$|ISO/IEC 7816-12:2005 {{provides}} two protocols {{for control}} transfers. This {{is to support}} the protocol T=0 (version A) or to use the transfer on APDU level (version B). ISO/IEC 7816-12:2005 provides the state diagrams for the USB-ICC {{for each of the}} transfers (<b>bulk</b> <b>transfers,</b> control transfers version A and version B). Examples of possible sequences which the USB-ICC must be able to handle are given in an informative annex.|$|E
50|$|The USB module {{is fully}} {{compliant}} with the USB 2.0 specification and supports control, interrupt and <b>bulk</b> <b>transfers</b> at a data rate of 12 Mbps (full speed). The module supports USB suspend, resume and remote wake-up operations {{and can be}} configured for up to eight input and eight output endpoints. The module includes an integrated physical interface (PHY); a phase-locked loop (PLL) for USB clock generation; and a flexible power-supply system enabling bus-powered and self-powered devices.|$|E
5000|$|The <b>Bulk</b> <b>Transfer</b> Division {{designs and}} {{manufactures}} variations of rotary railcar dumpers, or [...] "wagon tipplers," [...] {{which are in}} operation worldwide. These include rotary dumpers, C-shaped rotary (CR) dumpers, closed and open-sided turnover dumpers, and single and multiple car dumpers. They also design and manufacture rail car moving devices such as train positioners, train indexing equipment, CUB and other support equipment. <b>Bulk</b> <b>Transfer</b> also offers material handling equipment such as barge unloaders with both grab and continuous bucket designs, and related specialty machinery.|$|R
50|$|A Haul road (also {{haulage road}} or haul track) {{is a term}} for roads {{designed}} for heavy or <b>bulk</b> <b>transfer</b> of materials by haul trucks in the mining industry.|$|R
40|$|Abstract — Sampled NetFlow {{data from}} a core Internet 2 router are {{analyzed}} to characterize the current use and performance of Internet 2, with particular emphasis on <b>bulk</b> TCP <b>transfers.</b> The distribution of throughput, transfer size, duration, and average packet size for 48, 301 observed bulk TCPs is presented. The top 10 % of <b>bulk</b> <b>transfer</b> TCPs achieved throughputs of 3. 9 Mbps or greater, while the top 1 % of <b>bulk</b> <b>transfer</b> TCPs achieved throughputs of 23 Mbps or greater. The median <b>bulk</b> <b>transfer</b> TCP throughput observed was 880 Kbps. Summaries of application and IP protocol mixes are presented. Popular applications included: NNTP (19. 3 % packets, 22. 8 % octets), active FTP (11. 9 % packets, 14. 2 % octets), HTTP (10. 5 % packets, 9. 5 % octets), and multicast (6. 6 % packets, 6. 2 % octets). Most used IP protocols were TCP (85. 6 % packets, 88. 7 % octets), UDP (12. 5 % packets, 10. 0 % octets), and ICMP (1. 4 % packets, 0. 8 % octets). Keywords — Internet 2, Abilene, OC- 48 c, backbone, TCP, throughput, NetFlow...|$|R
50|$|Although {{statutes}} vary, {{the legal}} requirements for a bulk sale generally {{apply to a}} sale of {{all or most of}} the materials, supplies or inventory of a business in a way not normally done in the ordinary course of the seller's business. <b>Bulk</b> <b>transfers</b> in the United States (U.S.) were generally governed by Article 6 of the Uniform Commercial Code (UCC). However, Article 6 has now been repealed by most states, in favor of revisions to other provisions of the UCC that apply to sales and secured transactions.|$|E
40|$|Abstract—The Internet is {{witnessing}} {{explosive growth}} in traffic, {{in large part}} due to <b>bulk</b> <b>transfers.</b> Delivering such traffic is expensive for ISPs because they pay other ISPs based on peak utilization. To limit costs, many ISPs are deploying ad-hoc traffic shaping policies that specifically target bulk flows. However, there is relatively little understanding today {{about the effectiveness of}} different shaping policies at reducing peak loads and what impact these policies have on the performance of <b>bulk</b> <b>transfers.</b> In this paper, we compare several traffic shaping policies with respect to (1) the achieved reduction in peak network traffic and (2) the resulting performance loss for <b>bulk</b> <b>transfers.</b> We identified a practical policy that achieves peak traffic reductions of up to 50 % with only limited performance loss for <b>bulk</b> <b>transfers.</b> However, we found that the same policy leads to large performance losses for <b>bulk</b> <b>transfers</b> when deployed by multiple ISPs along a networking path. Our analysis revealed that this is caused by certain TCP characteristics and differences in local peak utilization times. I...|$|E
40|$|This paper {{evaluates the}} {{suitability}} of Backward Explicit Congestion Notification (BECN) for IP networks. The BECN mechanism has previously been used in non-IP networks, {{but there has been}} limited experimental investigation into the application of the BECN scheme as congestion control mechanism in IP networks. In this paper, we consider an enhanced algorithm for BECN which uses Internet Control Message Protocol (ICMP) Source Quenches for backward congestion notification in IP networks and undertake comparative performance evaluation of Random Early Detection (RED), Explicit Congestion Notification (ECN) and our enhanced BECN mechanism using both longlived TCP <b>bulk</b> <b>transfers</b> and short-lived webtraffic workloads. Our results show that for webtraffic workloads, BECN offers only slight improvement in transfer delay while average goodput for <b>bulk</b> <b>transfers</b> is no worse than that of ECN. For paths that have a high bandwidth delay product our results show that not only can BECN offer significant improvement in average goodput for <b>bulk</b> <b>transfers</b> over the ECN mechanism, but packet drops and transfer delay for short-lived webtraffic connections are also comparatively reduced. Additional observations show that on such paths TCP (NewReno) with RED can offer higher goodput for <b>bulk</b> <b>transfers</b> compared to ECN...|$|E
40|$|Numerous {{experiments}} with a scaled pilot facility {{were carried out}} to compare the relative <b>bulk</b> <b>transfer</b> performance of three special devices for applications to drilling systems. The pipe diameter for bulk transportation was 3 in., which corresponds to around half of the actual system dimensions. Two different pressures, 3 and 4 bar, were considered to check the relative performance under different pressure conditions at a bulk storage tank. And to make a practical estimation method of the <b>bulk</b> <b>transfer</b> rate at the early design stages of the bulk handling system, a series of experiments were conducted for real scaled bulk handing systems of two drilling vessels. The pressure drops at each pipe element {{as well as the}} <b>bulk</b> <b>transfer</b> rates were measured under different operating conditions. Using the measured results, the friction factor for each pipe element was calculated and a procedure for transfer rate estimation was developed. Compared to the measured transfer rate results for other drilling vessels, the estimated transfer rates were within a maximum 15 % error bound...|$|R
40|$|Abstract—The {{steady-state}} {{performance of}} a <b>bulk</b> <b>transfer</b> TCP flow (i. e., a flow with {{a large amount of}} data to send, such as FTP transfers) may be characterized by the send rate, which is the amount of data sent by the sender in unit time. In this paper we develop a simple analytic characterization of the steady-state send rate as a function of loss rate and round trip time (RTT) for a <b>bulk</b> <b>transfer</b> TCP flow. Unlike the models in [7]–[9], and [12], our model captures not only the behavior of the fast retransmit mechanism but also the effect of the time-out mechanism. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more time-out events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP send rate and is accurate over a wider range of loss rates. We also present a simple extension of our model to compute the throughput of a <b>bulk</b> <b>transfer</b> TCP flow, which is defined as the amount of data received by the receiver in unit time. Index Terms—Empirical validation, modeling, retransmission timeouts, TCP...|$|R
40|$|The {{steady state}} {{performance}} of a <b>bulk</b> <b>transfer</b> TCP flow (i. e. a flow with {{a large amount of}} data to send, such as FTP transfers) may be characterized by three quantities. The first is the send rate, which is the amount of data sent by the sender in unit time. The second is the throughput, which is the amount of data received by the receiver in unit time. Note that the throughput will always be {{less than or equal to}} the send rate due to losses. Finally, the number of non-duplicate packets received by the receiver in unit time gives us the goodput of the connection. The goodput is always less than or equal to the throughput, since the receiver may receive two copies of the same packet due to retransmissions by the sender. In [9], we presented a simple model for predicting the steady state send rate of a <b>bulk</b> <b>transfer</b> TCP ow as a function of loss rate and round trip time. In this paper, we extend that work in two ways. First, we analyze the performance of <b>bulk</b> <b>transfer</b> TCP flows using more [...] ...|$|R
40|$|Abstract We {{characterize}} a TCP implementation by a function, {{called a}} profile, that expresses the instantaneous throughput {{at the source}} {{in terms of the}} instantaneous roundtrip time and instantaneous loss rate for <b>bulk</b> <b>transfers.</b> We empirically obtain profiles of several TCP implementations, accurately enough to distinguish not only the TCP version but also the implementation (BSD, Windows, etc) ...|$|E
40|$|Experimental data validating {{some of the}} {{proposed}} parallel computation models on the Intel Paragon is presented. This architecture {{is characterized by a}} large bandwidth and a relatively large startup cost of a message transmission, which makes it extremely important to employ <b>bulk</b> <b>transfers.</b> The models considered are the BSP model, in which it is assumed that all messages have a fixed short size, and the BPRAM, in which block transfers are rewarded. 1...|$|E
40|$|Abstract—We aim to broadly {{study the}} ways that modern {{applications}} use the underly-ing protocols and networks. Such an understanding is necessary when designing and op-timizing lower-layer protocols. Traditionally—as prior work shows—applications have been well represented as <b>bulk</b> <b>transfers,</b> often preceded by application-layer handshak-ing. Recent suggestions posit that application evolution has eclipsed this simple model, and a typical pattern is now a series of transactions over a single transport layer connec-tion. In this initial study we examine application transmission patterns via packet traces from two networks to better understand {{the ways that}} modern applications use TCP. ...|$|E
40|$|The {{steady-state}} {{performance of}} a <b>bulk</b> <b>transfer</b> TCP flow (i. e., a flow with {{a large amount of}} data to send, such as FTP transfers) may be characterized by the send rate, which is the amount of data sent by the sender in unit time. In this paper we develop a simple analytic characterization of the steady-state send rate as a function of loss rate and round trip time (RTT) for a <b>bulk</b> <b>transfer</b> TCP flow. Unlike the models in [7] [...] [9], and [12], our model captures not only the behavior of the fast retransmit mechanism but also the effect of the time-out mechanism. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more time-out events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP send rate and is accurate over a wider range of loss rates. We also present a simple extension of our model to compute the throughput of a <b>bulk</b> <b>transfer</b> TCP flow, which is defined as the amount of data received by the receiver in unit time...|$|R
40|$|More {{and more}} often people {{exchange}} geospatial information. This happens when a user gets data from producer. For a transfer of updated data a producer provides a new geographical dataset to users. Nowadays, this update transfer is usually a <b>bulk</b> <b>transfer.</b> Hence, transferred data describes {{a snapshot of the}} world. Successive changes undergone by features are not described. To improve the integration process, these modifications should be isolated in the transferred dataset. We study how metadata could be used to represent change in a <b>bulk</b> data <b>transfer.</b> We propose to use information about dating and lineage to detect altered features. Then, only these altered features are integrated in the user information system. For a <b>bulk</b> data <b>transfer,</b> this metadata improve the integration process. Moreover, we extend the use of metadata. It is used during the data transfer and it is not limited to a document role...|$|R
50|$|The Reliable Data Protocol (RDP) is {{a network}} {{transport}} protocol defined in RFC 908 and was updated in RFC 1151. It {{is meant to}} provide facilities for remote loading, debugging and <b>bulk</b> <b>transfer</b> of images and data. It {{should not be confused}} with RUDP.|$|R
40|$|The {{compatibility}} of <b>bulk</b> <b>transfers</b> of PNR data to Canadian {{law enforcement}} authorities with the EU Charter of Fundamental Rights requires clear and precise provisions as regard {{the scope of the}} Agreement, the way how the data will be used, the authorities and persons accessing the data, data retention periods, and onwards transfers. Such regimes should be accompanied by strict oversights mechanisms that ensure an independent control and provide sufficient and clear remedies to individuals. Sensitive data should be excluded from the scope of the Agreement. status: publishe...|$|E
40|$|Tor is {{vulnerable}} to network congestion and performance problems due to bulk data transfers. A large fraction of the available network capacity is consumed by {{a small percentage of}} Tor users, resulting in severe service degradation for the majority. Bulk users continuously drain relays of excess bandwidth, creating new network bottlenecks and exacerbating the effects of existing ones. While this problem may currently be attributed to rational users utilizing the network, it may also be exploited by a relatively low-resource adversary using similar techniques to contribute to a network denial of service (DoS) attack. Degraded service discourages the use of Tor, affecting both Tor’s client diversity and anonymity. Equipped with mechanisms from communication networks, we design and implement three Tor-specific algorithms that throttle <b>bulk</b> <b>transfers</b> to reduce network congestion and increase network responsiveness. Unlike existing techniques, our algorithms adapt to network dynamics using only information local to a relay. We experiment with full-network deployments of our algorithms under a range of light to heavy network loads. We find that throttling results in significant improvements to web client performance while mitigating the negative effects of <b>bulk</b> <b>transfers.</b> We also analyze how throttling affects anonymity and compare the security of our algorithms under adversarial attack. We find that throttling reduces information leakage compared to unthrottled Tor while improving anonymity against realistic adversaries. ...|$|E
3000|$|... 10 Béteille (2009) {{reports that}} half of the {{teachers}} she surveyed in three states agreed that transfers require connections and 30 percent believed {{that they would have to}} pay to obtain the post they wanted. Ramachandran et al. (2005) report that in 2005, the government in Rajasthan transferred 20, 000 teachers. To quote the authors: ‘Discussions with trade union leaders revealed that transfers and posting were big business in Rajasthan. Intensive lobbying followed <b>bulk</b> <b>transfers</b> and it was rumoured that political middlemen demanded Rs. 5, 000 to Rs. 25, 000 to cancel the transfer or ensure a good posting’.|$|E
40|$|Abstract [...] The {{steady-state}} {{performance of}} a <b>bulk</b> <b>transfer</b> TCP flow (i. e., a flow with {{a large amount of}} data to send, such as FTP transfers) may be characterized by the send rate, which is the amount of data sent by the sender in unit time. In this paper we develop a simple analytic characterization of the steady-state send rate as a function of loss rate and round trip time (RTT) for a <b>bulk</b> <b>transfer</b> TCP flow. Unlike the models in [7]-[9], and [12], our model captures not only the behavior of the fast retransmit mechanism but also the effect of the time-out mechanism. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more time-out events than fast retransmit events. Our measure-ments demonstrate that our model is able to more accurately predict TCP send rate and is accurate over a wider range of loss rates. We also present a simple extension of our model to compute the throughput of a <b>bulk</b> <b>transfer</b> TCP flow, which is defined as the amount of data received by the receiver in unit time. Index Terms [...] Empirical validation, modeling, retransmission timeouts, TCP...|$|R
40|$|GridSite has {{extended}} the industrystandard Apache webserver for use within Grid projects, both by adding support for Grid security credentials such as GSI and VOMS, {{and with the}} GridHTTP protocol for <b>bulk</b> file <b>transfer</b> via HTTP. We describe how GridHTTP combines the security model of X. 509 /HTTPS {{with the performance of}} Apache, in local and wide area <b>bulk</b> <b>transfer</b> applications. GridSite also supports file location within storage farms, and we explain how this has been implemented within Apache using the HTCP protocol, and the clientside commands and toolkit we have provided for applications...|$|R
40|$|This paper {{provides}} a preliminary {{assessment of the}} effectiveness of an application layer tool that measures the <b>Bulk</b> <b>Transfer</b> Capacity (BTC) of a network path. BTC is roughly defined as the throughput that a flow using standard congestion control techniques would obtain across a given network path at a given time. We utilize the NIMI mesh of measurement hosts to compare stock BSD TCP with a new BTC measurement tool, cap. While BTC tools have been around for some time, no systematic evaluation of their accuracy with respect to standard TCP congestion control across a wide variety of network paths has been conducted. The goal {{of this paper is to}} provide such an empirical evaluation of a BTC tool and therefore assess the reliability of the measurements obtained using BTC tools. Keywords [...] - TCP, <b>Bulk</b> <b>Transfer</b> Capacity, Congestion Control, Bandwidth Measurement I...|$|R
40|$|TCP over {{bandwidth}} asymmetric networks such as Cable TV, Asymmetric Digital Subscriber Loop (ADSL) and Wireless Networks exhibits different characteristics from TCP on symmetric links. A {{number of}} techniques {{have been proposed}} to address this problem. However previous research has been largely focused on <b>bulk</b> <b>transfers.</b> This paper investigates the effects of bandwidth asymmetry on Weblike short-lived transfers. A close-form prediction model is presented for TCP transfers over asymmetric links. The Web transfer model is then derived from it. Simulations based on ns- 2 show that the model can give predictions for TCP transfers {{with a high degree}} of accuracy...|$|E
40|$|Abstract — In {{this study}} we seek to broadly {{understand}} the ways that modern applications use the underlying protocols and networks. Such an understanding is necessary when designing and optimizing lower-layer protocols. Traditionally— as established in the literature—applications have been well represented as simple <b>bulk</b> <b>transfers,</b> often preceded by a bit of application-layer handshaking. Recently {{it has been suggested}} that applications have evolved past this simple model, and a typical pattern is now that of a series of transactions over a single transport layer connection. In this initial study we examine application transmission patterns via packet traces from two networks to better understand the ways that modern applications use TCP. 1...|$|E
40|$|Datacenter TCP (DCTCP) {{achieves}} low latencies {{for short}} flows while maintaining high throughputs for concur-rent <b>bulk</b> <b>transfers,</b> but requires changes to both end-points, which presents a deployment challenge. This paper presents extensions to DCTCP that enables one-sided de-ployment when peers implement standard TCP/ECN func-tionality. This makes DCTCP significantly easier to deploy incrementally. This paper also improves DCTCP in two-sided deployments by refining ECN processing and {{the calculation of}} the congestion estimate. A FreeBSD kernel implementation of these DCTCP improvements demon-strates better performance than the original DCTCP vari-ant, and validates that incremental one-sided deployments see benefits similar to those previously only achievable in two-sided deployments. ...|$|E
40|$|The {{granularity}} {{of sharing}} {{is one of}} the key components that affect the performance in distributed shared memory (DSM) systems. Providing only one or two fixed size granularities to the user may not result in an efficient use of resources. Providing an arbitrarily variable granularity increases hardware and/or software overheads. Moreover, its efficient implementation requires the user to provide some information on the granularity, sacrificing the programmability of the shared memory paradigm. In this paper, we present a new communication scheme, called Adaptive Granularity (AG). Adaptive Granularity is a communication scheme that effectively and transparently integrates <b>bulk</b> <b>transfer</b> into the shared memory paradigm. Adaptive Granularity provides a limited number of granularities, but efficient enough to achieve gains from <b>bulk</b> <b>transfer,</b> without sacrificing any programmability of the shared memory paradigm and requiring any additional hardware. An Adaptive Granularity protocol cons [...] ...|$|R
5000|$|<b>Bulk</b> milk <b>transfer</b> from Waingawa to Hawera via the Manawatu Gorge ...|$|R
40|$|The http {{protocol}} is {{a fundamental}} component of the current design of WWW. It provides a <b>bulk</b> <b>transfer</b> mode of operation, suitable for today's networks of limited bandwidth. The protocol for Hyper-G is similar, with important extensions for Hyper-G concepts of links, anchors, search and collections. We discuss a <b>bulk</b> <b>transfer</b> hyperdocument protocol for a distributed hypermedia system which permits an open document model architecture. This protocol is intended to support the hypermedia semantics of HyTime. We discuss {{the implications of these}} semantics on the protocol design, and compare this work with other approaches. We discuss implementation work in developing a distributed HyTime server. 1 Introduction A basic constraint in existing distributed hypermedia systems such as WWW and Hyper-G is that the document model is fixed. In the case of WWW, the native document model is HTML. In the case of Hyper-G, the native document model is HTF. Of course both systems support other document [...] ...|$|R
