101|16|Public
5000|$|In 1970, Pelletier {{developed}} a continuous price series which was trademarked [...] "Perpetual Contracts". It is a weighted average between two futures contracts. It provided a smooth series that was ideal for <b>back-testing</b> futures trading systems.|$|E
50|$|Although NumXL is {{intended}} as an analytical add-in for Excel, it extends Excel’s user-interface (UI) and offers many wizards, menus and toolbars to automate the mundane phases of time series analysis. The features include summary statistics, test of hypothesis, correlogram analysis, modeling, calibration, residuals diagnosis, <b>back-testing</b> and forecast.|$|E
50|$|Through <b>back-testing,</b> Piotroski {{found that}} buying the top {{stocks in the}} market {{according}} to his methodology and shorting those that got the worst scores {{would have resulted in}} 23% annualized gains from 1976 through 1996, more than double the S&P 500 broad market index return. His findings were made available to a wider audience via SmartMoney magazine and Bloomberg BusinessWeek.|$|E
50|$|Weston’s {{derivatives}} team of PhDs and Risk Management Executives reconstructs trading records, analyzes fund flow {{documentation and}} <b>back-tests</b> all margin calls, settlements, knock-outs, and re-sets. These investigations often uncover errors, or at worst, fraudulent {{intent on the}} part of the originating counterparties. These findings can then serve as the basis for an intensive public relations and litigation against the offending parties in order to force settlements.|$|R
40|$|Along {{with the}} {{increasing}} computing power, growing availability of various data streams, introduction of the electronic exchanges, decreasing trading costs and heating-up competition in financial investment industry, quantitative trading strategies or quantitative trading rules have been evolving rapidly in a few decades. They challenge the Efficient Market Hypothesis by trying to forecast future price movements of risky assets from the historical market information in algorithmic ways or in statistical ways. They try to find some patters or trends from the historical data {{and use them to}} beat the market benchmark. In this research, I introduce several quantitative trading strategies and investigate their performances empirically i. e. by executing <b>back-tests</b> assuming that the S&P 500 stock index is a risky asset to trade. The strategies utilize the historical data of the stock index itself, trading volume movement, risk-free rate movement and implied volatility movement in order to generate buy or sell trading signals. Then I attempt to articulate and decompose the source for successes of some strategies in the <b>back-tests</b> into several factors such as trend patterns or relationships between market information variables in intuitive way. Some strategies recorded higher performances than the benchmark in the <b>back-tests,</b> however it is still a problem how we can distinguish these winner strategies beforehand from the losers at the beginning of our investment horizon. Human discretion such as macro view on the future market trend is considered to still play an important role for quantitative trading to be successful in the long-run. by Masaharu Aiuchi. Thesis (M. B. A.) [...] Massachusetts Institute of Technology, Sloan School of Management, 2008. Includes bibliographical references (p. 277 - 280) ...|$|R
40|$|This article uses Extreme Value Theory (EVT) {{to measure}} extreme risk in futures {{contracts}} with diverging underlying assets. The approach {{provides a framework}} for analysing the distributional properties of extreme returns. EVT is statistically robust at estimating Value at Risk (VaR) for different asset classes and at very low probabilities. By way of contrast, the estimation bias by relying on the thin-tailed normal distribution for measuring extreme price movements is illustrated. <b>Back-tests</b> confirm the relative accuracy of the approaches. ...|$|R
5000|$|Piotroski {{is known}} in the {{investing}} world for an influential 2000 paper he wrote while at the University of Chicago, entitled Value Investing: The Use of Historical Financial Statement Information to Separate Winners from Losers. In the piece, Piotroski laid out a way (Piotroski F-Score) to buy and short stocks using several accounting-based criteria. His <b>back-testing</b> showed that the method would have produced returns well above the broader market averages over a two-decade period.|$|E
50|$|On January 8, 2014, John S. Tobey {{wrote an}} article in Forbes {{magazine}} where he criticized the Dogs. He said that it should use price weighting, instead of equal weighting, {{because that is what}} the DJIA uses. He said that, for the year 2013, using the price weighting the Dogs would have returned less, rather than more, than the DJIA. He suggested that it is too simple and it should use more factors such as dividend-payout ratio, growth of cash and earnings, price performance, and many other things. However, he did not say how to combine these into a strategy. He also criticized it for <b>back-testing</b> (although the method was published in 1991). He also said that the strategy would not work well for 2014.|$|E
50|$|In 1991, Omega Research {{released}} TradeStation and, {{three years}} later, struck a licensing deal with Dow Jones Telerate to offer TradeStation as a premium service to Telerate’s institutional clients worldwide. In 1997, Omega Research conducted an IPO and became {{listed on the}} Nasdaq National Market. The company launched an online version of its product in 1999. In 2001, the company converted itself from a trading software company to an online securities brokerage, and renamed itself “TradeStation.” The software’s <b>back-testing,</b> order-generation and trade execution capabilities were fully integrated for both securities and futures markets in 2003, and forex was added soon thereafter. In 2004 and 2005, TradeStation became a self-clearing equities and options firm. An entire industry eventually grew up around TradeStation’s software, including seminars, publications, and user networks.|$|E
40|$|Market {{liquidity}} risk, {{the difficulty}} or cost of trading assets in crises, {{has been recognized}} as {{an important factor in}} risk management. Literature has already proposed several models to include liquidity risk in the standard Value-at-Risk framework. While theoretical comparisons between those models have been conducted, their empirical performance has never been benchmarked. This paper performs comparative <b>back-tests</b> of daily risk forecasts for a large selection of traceable liquidity risk models. In a 5. 5 year stock sample we show which model provides most accurate results and provide detailed recommendations which model is most suitable in a specific situation. [...] asset liquidity,liquidity cost,price impact,Xetra liquidity measure (XLM),risk measurement,Value-at-Risk,market liquidity risk...|$|R
40|$|In this paper, {{parametric}}, nonparametric, and semi parametric {{models are}} applied to a hypothetical portfolio consisting a single asset - FTSE 100 Index, to assess their performance in the London stock market. In order to assess the performance of different approaches, the statistic features such as kurtosis, skewness and autocorrelation of daily return have been studied. In addition, this article analyzes {{the advantages and disadvantages}} of each model and implements <b>back-tests</b> to check the validation of them. The main finding of this article is that NGARCH and GARCH(1, 1) -t(d) model are the most accurate and reliable models to estimate Value at Risk in London Stock market...|$|R
40|$|We {{present a}} new Monte-Carlo {{methodology}} to forecast the crude oil production of Norway and the U. K. {{based on a}} two-step process, (i) the nonlinear extrapolation of the current/past performances of individual oil fields and (ii) a stochastic model of the frequency of future oil field discoveries. Compared with the standard methodology that tends to underestimate remaining oil reserves, our method gives a better description of future oil production, as validated by our <b>back-tests</b> starting in 2008. Specifically, we predict remaining reserves extractable until 2030 to be 188 +/- 10 million barrels for Norway and 98 +/- 10 million barrels for the UK, which are respectively 45 % and 66 % above the predictions using the standard methodology...|$|R
40|$|Financial {{markets are}} highly complex {{adaptive}} systems. This paper {{deals with the}} application of simulators in software architectures for <b>back-testing</b> and automating financial market trading strategies. It characterizes traits and problems of algorithmic trading and describes the established use of simulators in <b>back-testing</b> and automated trading. A new approach {{in the form of}} a hierarchical software architecture is introduced, containing simulators as integral parts in all layers, using them both during <b>back-testing</b> and automated trading. In addition to the software architecture the opening objects of investigation are outlined. Finally, the potential of generalizing the application domains of our approach beyond financial market trading strategies is pointed out...|$|E
40|$|Diploma {{thesis is}} focused on {{analysis}} and comparison using financial derivatives to hedge currency risk. The {{first part of the}} thesis describes instruments used for hedging: forex forwards, futures contracts and currency options. Those instruments are used for <b>back-testing</b> in analytical part, currency crosses used for <b>back-testing</b> are EUR/USD, EUR/GBP and GBP/USD. The main goal of this thesis is to evaluate the posibility of using them to hedge currency risk, comparison of their efectivity and application...|$|E
40|$|In this paper, we {{investigate}} {{the need to}} employ long-memory volatility models in terms of Value-at-Risk(VaR) estimation. We estimate the VaR of the KOSPI returns using long-memory volatility models such as FIGARCH and FIEGARCH; in addition, via <b>back-testing</b> we compare {{the performance of the}} ob-tained VaR with short memory processes such as GARCH and EGARCH. <b>Back-testing</b> says that there exists a long-memory property in the volatility process of KOSPI returns and that it is essential to emplo...|$|E
40|$|We {{document}} widespread {{changes to}} the historical I/B/E/S analyst stock recommendations database. Across seven I/B/E/S downloads, obtained between 2000 and 2007, we find that between 6, 580 (1. 6 %) and 97, 582 (21. 7 %) of matched observations are different from one download to the next. The changes include alterations of recommendations, additions and deletions of records, and removal of analyst names. These changes are nonrandom, clustering by analyst reputation, broker size and status, and recommendation boldness, and affect trading signal classifications and <b>back-tests</b> of three stylized facts: profitability of trading signals, profitability of consensus recommendation changes, and persistence in individual analyst stock-picking ability. Copyright (c) 2009 the American Finance Association. ...|$|R
40|$|Includes bibliographical references. Risk parity, a {{portfolio}} allocation technique {{based on the}} equalization of constituent risk contributions, has garnered significant attention in academic circles over the past decade. This study employs <b>back-tests</b> to explore the empirical performance of the approach relative to other prominent heuristic and risk based allocation techniques on South Africa's All Share Index (ALSI) and 12 auxiliary international equity indices. We find that the technique discharges its core risk contribution equalization objectives well in out of sample testing but appears to lag other risk based allocation techniques in terms of risk and return performance. We also establish links between the approaches' performance and leverage aversion theory and find some evidence that levels of market concentration may impact the performance of risk parity portfolios across equity indices...|$|R
40|$|Includes bibliographical references. A {{model with}} a {{stochastic}} interest rate process correlated to a stochastic volatility process is needed to accurately price long- dated contingent claims. Such a model should also price claims efficiently {{in order to allow}} for fast calibration. This dissertation explores the approximations for the characteristic function of the Heston-Hull&White model introduced by Grzelak and Oost- erlee (2011). Fourier-Cosine expansion pricing, due to Fang and Oosterlee (2008), is then used to price contingent claims under this model, which is implemented in MATLAB. We find that the model is efficient, accurate and has a relatively simple calibration procedure. In <b>back-tests,</b> it is determined that the Heston- Hull&White model produces better hedging profit and loss results than a Heston (1993) or a Black and Scholes (1973) model...|$|R
30|$|Risk {{measures}} {{include the}} standard deviation of daily returns and the “Highest Open Drawdown” (HOD), which is the maximum distance the equity line fell below the initial investment during the <b>back-testing</b> simulation.|$|E
40|$|A {{new method}} is {{proposed}} to estimate Value-at-Risk (VaR) by Monte Carlo simulation with optimal <b>back-testing</b> results. The Monte Carlo simulation is adjusted through an iterative process to accommodate recent shocks, thereby {{taking into account}} the latest market conditions. Empirical validation covering the current financial crisis shows that VaR estimation via the optimization process is relatively reliable and consistent, and generally outperforms the VaR generated by a simple Monte Carlo simulation. This is particularly true in cases when the out-of-sample evaluation sample spans a lengthy period, as the traditional method tends to underestimate the number of extreme shocks. Value-at-Risk Optimization <b>Back-testing</b> Monte Carlo simulation...|$|E
30|$|For {{testing the}} model effectiveness, an “out of sample” data subset {{was created to}} provide an {{independent}} <b>back-testing</b> view. In the training samples, we randomly selected 1500 out of 1665 fire samples and 330 out of 134, 091 non-fire samples, and in the testing samples, the unpicked 165 fire samples and 330 out of 134, 091 non-fire samples were used.|$|E
40|$|We {{document}} widespread ex post {{changes to}} the historical contents of the I/B/E/S analyst stock recommendations database. Across a sequence of seven downloads of the entire I/B/E/S recommendations database, obtained between 2000 and 2007, we find that between 6, 594 (1. 6 %) and 97, 579 (21. 7 %) of matched observations are different from one download to the next. The changes, which include alterations of recommendation levels, additions and deletions of records, and removal of analyst names, are non-random in nature: They cluster by analyst reputation, brokerage firm size and status, and recommendation boldness. The changes have a large and {{significant impact on the}} classification of trading signals and <b>back-tests</b> of three stylized facts: The profitability of trading signals, the profitability of changes in consensus recommendations, and persistence in individual analyst stock-picking ability. ...|$|R
40|$|Master's thesis in Industrial economicsValue at Risk (VaR) is an {{important}} calculation in risk management. It is a commonly used measure of risk in finance, and is used by corporations to estimate potential future loss. With a significance level, VaR gives the worst potential loss within a specific time period. VaR is easy to understand, and provides important information about risk. This thesis uses data from {{the oil and gas}} industry to compare different methods of calculating the VaR. The approaches compared are the non-parametric and the parametric methods, whereas the latter is calculated based on the simple standard deviation, EWMA and GARCH (1, 1). The thesis also studies the price fluctuations in the oil and gas market, which are mainly affected by changes in supply and demand. In the oil and gas market, the minimum price is set by the last supplier needed to fulfill the demand. Production will not be profitable for that supplier if the price is less than this. Expectations about the future have great influence on the price development. Geopolitical tensions, and other factors that could lead to reduced supply make the price increase. New discoveries, which lead to increased supply or reserves, tend to lower the price. As this thesis will show, the period from 2002 until the summer of 2008 was a period of steady growth for oil prices, whereas the prices for natural gas also started falling in 2008 due to the financial crisis. The normal and student-t distributions were assumed in the parametric approach, and were compared to a non-parametric approach, the historical simulation, as a benchmark. The Kupiec-test and the Christoffersen-test were both used to test the validity of the approaches. For all three time periods considered (250, 500, 1000 days), the non-parametric approach was without doubt the one that got accepted most by the <b>back-tests.</b> The VaR estimates for the 99 % confidence level were dominantly better than the ones for the 95 %, which the <b>back-tests</b> confirmed...|$|R
40|$|Portfolio {{management}} is {{the decision-making process}} of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices {{of a set of}} financial assets as its input, outputting portfolio weights of the set. The network is trained with 0. 7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10 -fold returns in 1. 8 months' periods. Some recently published portfolio selection strategies are also used to perform the same <b>back-tests,</b> whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets. Comment: accepted by Intelligent Systems Conference (IntelliSys) 201...|$|R
40|$|When <b>back-testing</b> the {{calibration}} {{quality of}} rating systems two-sided statistical tests can detect over- and underestimation of credit risk. Some users though, such as risk-averse investors and regulators, are primarily {{interested in the}} underestimation of risk only, and thus require one-sided tests. The established one-sided tests are multiple tests, which assess each rating class of the rating system separately and then combine the results to an overall assessment. However, these multiple tests may fail to detect underperformance of the whole rating system. Aiming to improve the overall assessment of rating systems, this paper presents a set of one-sided tests, which assess the performance of all rating classes jointly. These joint tests build on the method of Sterne [1954] for ranking possible outcomes by probability, which allows to extend <b>back-testing</b> to a setting of multiple rating classes. The new joint tests are compared to the most established one-sided multiple test and are further shown to outperform this benchmark in terms of power {{and size of the}} acceptance region...|$|E
40|$|Purpose The {{purpose of}} the paper is to back-test value-at-risk (VaR) models for {{conditional}} distributions belonging to a Generalized Hyperbolic (GH) family of Lévy processes - Variance Gamma, Normal Inverse Gaussian, Hyperbolic distribution and GH - and compare their risk-management features with a traditional unconditional extreme value (EV) approach using data from future contracts return data of S&amp;P 500, FTSE 100, DAX, HangSeng and Nikkei 225 indices. Design/methodology/approach The authors apply tail-based and Lévy-based calibration to estimate {{the parameters of the}} models as part of the initial data analysis. While the authors utilize the peaks-over-threshold approach for generalized Pareto distribution, the conditional maximum likelihood method is followed in case of Lévy models. As the Lévy models do not have closed form expressions for VaR, the authors follow a bootstrap method to determine the VaR and the confidence intervals. Finally, for <b>back-testing,</b> they use both static calibration (on the entire data) and dynamic calibration (on a four-year rolling window) to test the unconditional, independence and conditional coverage hypotheses implemented with 95 and 99 per cent VaRs. Findings Both EV and Lévy models provide the authors with a conservative proportion of violation for VaR forecasts. A model targeting tail or fitting the entire distribution has little effect on either VaR calculation or a VaR model&# 039;s <b>back-testing</b> performance. Originality/value To the best of the authors&# 039; knowledge, this is the first study to explore the <b>back-testing</b> performance of Lévy-based VaR models. The authors conduct various calibration and bootstrap techniques to test the unconditional, independence and conditional coverage hypotheses for the VaRs...|$|E
40|$|This diploma thesis {{deals with}} {{the design of a}} trading {{strategy}} and subsequent implementation of an automated trading system for the forex currency market. In this thesis, a "breakout" strategy with trade filtering based on moving average is created. Consequently, an automated trading system for the MetaTrader 4 platform is developed in MQL 4 language. This thesis also {{deals with the}} <b>back-testing</b> and optimization of the system in order to maximize the stability and profit...|$|E
40|$|Model {{risk in the}} {{estimation}} of value-at-risk is a challenging threat {{for the success of}} any financial investments. The degree of the model risk increases when {{the estimation}} process is constructed with a portfolio in the emerging markets. The proper model should both provide flexible joint distributions by splitting the marginality from the dependencies among the financial assets within the portfolio and also capture the non-linear behaviours and extremes in the returns arising from the special features of the emerging markets. In this paper, we use time-varying copula to estimate the value-at-risk of the portfolio comprised of the Bovespa and the IPC Mexico in equal and constant weights. The performance comparison of the copula model to the EWMA portfolio model made by the Christoffersen back-test shows that the copula model captures the extremes most successfully. The copula model, by estimating the portfolio value-at-risk with the least violation number in the <b>back-tests,</b> provides the investors to allocate the minimum regulatory capital requirement in accordance with the Basel II Accord. ...|$|R
40|$|Abstract__ is paper {{features}} {{an analysis of}} the effectiveness of a range of portfolio diversification strategies as applied to a set of daily arithmetically compounded returns on a set of ten market indices representing the major European markets for a nine year period from the beginning of 2005 to the end of 2013. The sample period, which incorporates the periods of both the Global Financial Crisis (GFC) and subsequent European Debt Crisis (EDC), is challenging one for the application of portfolio investment strategies. The analysis is undertaken via the examination of multiple investment strategies and a variety of hold-out periods and <b>back-tests.</b> We commence by using four two year estimation periods and subsequent one year investment hold out period, to analyse a naive 1 /N diversification strategy, and to contrast its effectiveness with Markowitz mean variance analysis with positive weights. Markowitz optimisation is then compared with various down-side investment opimisation strategies. We begin by comparing Markowitz with CVaR, and then proceed to evaluate the relative effectiveness of Markowitz with various draw-down strategies, utilising a series of backtests. Our results suggest that none of the more sophisticated optimisation strategies appear to dominate naive diversification...|$|R
40|$|Abstract: The {{objective}} {{of this paper is}} describe a new paradigm for the trading of equities. In our formulation, the control corresponds to a feedback law which modulates the amount invested I(t) in stock over time. The controller also includes a saturation limit Imax corresponding to a limit on the value at risk. The admissible stock price evolution p(t) over time is modelled as a family P of uncertain inputs against which we seek robust returns. Motivated by the fact that <b>back-testing</b> of candidate trading strategies involves significant cost and effort associated with computational simulation over sufficiently diverse markets, our paradigm involves the notion of synthetic prices and some idealizations involving the volatility of prices and trading liquidity. Our point of view is that a robust performance certification in this somewhat idealized market setting serves as a filter to determine if a trading strategy is worthy of the considerable time and expense associated with full-scale <b>back-testing.</b> The paper also includes a description of a so-called saturation reset controller. This controller is used to illustrate how the model works in practice and the attainment of robustness objectives over various sub-classes of P...|$|E
40|$|This paper proposes markovian {{models in}} {{portfolio}} theory and risk management. At first, we describe discrete time optimal allocation models. Then, {{we examine the}} investor’s optimal choices either when the returns are uniquely determined by their mean and variance or when they are modeled by a Markov chain. We subject these models to <b>back-testing</b> on out-of-sample data, {{in order to assess}} their forecasting ability. Finally, we propose some models to compute VaR and CVaR when the returns are modeled by a Markov chain. Key words...|$|E
40|$|We {{study the}} daily return {{distributions}} for 22 industry stock indexes on the Tai-wan Stock Exchange under the unconditional homoskedastic independent, identically distributed and the conditional heteroskedastic GARCH models. Two distribution hypotheses are tested: the Gaussian and the stable Paretian distributions. The {{performance of the}} stable Paretian distribution is better {{than that of the}} Gaussian distribution. A <b>back-testing</b> example is provided to give evidence on the superiority of the stable ARMA-GARCH to the normal ARMA-GARCH. Stable distributions, ARMA-GARCH, Heavy tails, Volatility clustering, Value at risk...|$|E
40|$|Pairs trading {{strategies}} aim {{to profit}} from temporary deviations in some underlying relationship between the prices of two stocks. The trader takes appropriate long and short positions in the two stocks and waits for their prices to revert back to the underlying relationship or even to deviate {{in the opposite direction}} from the current deviation, at which time the trader may exit at a profit. We formulate formal trading rules that implement pairs trading strategies and discuss their profitability and risk by means of <b>back-testing</b> on stocks listed on the Johannesburg Stock Exchange (JSE). [URL]...|$|E
40|$|In {{this paper}} we study the tail {{behaviour}} of eight major market indexes stratifying data {{according to the}} violation of a high threshold on the previous day. The distributional differences found can be exploited to improve VaR calculations in several settings, giving rise to what we call 'MCVaR'. We compare the performance of MCVaR with unconditioned VaR calculation methods and with GARCH VaR by means of several <b>back-testing</b> techniques that take into account not only the number of violations but also their magnitude and clustering. Value at Risk, Paretian tails, GARCH, Runs test, Back testing,...|$|E
40|$|This paper {{proposes a}} method of {{estimating}} Value at Risk (VaR) {{based on the assumption}} that the financial returns follow a switching regime ARCH model. We use the simple switching-regime model, the traditional GARCH(1, 1) model and the switching-regime ARCH model to do some empirical analysis and to calculate the VaR values under different confidence levels for Shanghai and Shenzhen Stock Index. The calculated VaR values are com- pared. The results of <b>back-testing</b> and the Proportion of Failure test show the VaR values calculated by the switching-regime ARCH model are preferred to other methods...|$|E
40|$|International audienceWe {{study in}} this article the problem of model risk in VaR {{computations}} and document a procedure for correcting the bias due to specification and estimation errors. This practical method consists of “learning from model mistakes”, since it dynamically relies on an adjustment of the VaR estimates – based on a <b>back-testing</b> framework – such as the frequency of past VaR exceptions always matches the expected probability. We finally show that integrating the model risk into the VaR computations implies a substantial minimum correction {{to the order of}} 10 – 40 % of VaR levels...|$|E
