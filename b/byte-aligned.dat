25|0|Public
5000|$|The {{intermediate}} {{interface between}} the two stages is made of a <b>byte-aligned</b> data stream {{of which there are}} two commands, a literal ("add") with length and data: ...|$|E
5000|$|Hashing the ASCII message [...] "Hello" [...] (hex: 0x48, 0x65, 0x6c, 0x6c, 0x6f) uses 6 message blocks. There are 5 {{blocks from}} the message, and {{since this is a}} <b>byte-aligned</b> input, there is 1 block for padding. The 512 bit hash value is: ...|$|E
50|$|Bit arrays {{are also}} a useful {{abstraction}} for examining streams of compressed data, which often contain elements that occupy portions of bytes or are not <b>byte-aligned.</b> For example, the compressed Huffman coding representation of a single 8-bit character can be anywhere from 1 to 255 bits long.|$|E
50|$|The {{instruction}} set {{was very much}} in the CISC model, with 2-operand instructions, memory-to-memory operations, flexible addressing modes, and variable-length <b>byte-aligned</b> instruction encoding. Addressing modes could involve up to two displacements and two memory indirections per operand as well as scaled indexing, making the longest conceivable instruction 23 bytes. The actual number of instructions was much lower than that of contemporary RISC processors.|$|E
50|$|A {{significant}} {{example of}} the use of tagged pointers is the Objective-C runtime on iOS 7 on ARM64, notably used on the iPhone 5S. In iOS 7, virtual addresses are 33 bits (<b>byte-aligned),</b> so word-aligned addresses only use 30 bits (3 least significant bits are 0), leaving 34 bits for tags. Objective-C class pointers are word-aligned, and the tag fields are used for many purposes, such as storing a reference count and whether the object has a destructor.|$|E
50|$|The second difference, {{which has}} the largest effect on {{the speed of the}} algorithm, is that the Adler sums are {{computed}} over 8-bit bytes rather than 16-bit words, resulting in twice the number of loop iterations. This results in the Adler-32 checksum taking between one-and-a-half to two times as long as Fletcher's checksum for 16-bit word aligned data. For <b>byte-aligned</b> data, Adler-32 is faster than a properly implemented Fletcher's checksum (e.g., one found in the Hierarchical Data Format).|$|E
5000|$|Software can {{compress}} each bitmap in a bitmap {{index to}} save spaces. There has been {{considerable amount of}} work on this subject.Though there are exceptions such as Roaring bitmaps, Bitmap compression algorithms typically employ run-length encoding, such as the <b>Byte-aligned</b> Bitmap Code, the Word-Aligned Hybrid code, the Partitioned Word-Aligned Hybrid (PWAH) compression, the Position List Word Aligned Hybrid, the Compressed Adaptive Index (COMPAX), Enhanced Word-Aligned Hybrid (EWAH) [...] and the COmpressed 'N' Composable Integer SEt. These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in bitwise operations without decompression. This gives them considerable advantages over generic compression techniques such as LZ77. BBC compression and its derivatives are used in a commercial database management system. BBC is effective in both reducing index sizes and maintaining query performance. BBC encodes the bitmaps in bytes, while WAH encodes in words, better matching current CPUs. [...] "On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC." [...] PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on logical operations. Similar considerations can be done for CONCISE [...] and Enhanced Word-Aligned Hybrid.|$|E
40|$|Compression reduces {{both the}} size of indexes and {{the time needed to}} {{evaluate}} queries. In this paper, we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms, considering two approaches to improving retrieval efficiency: better implementation and better choice of integer compression schemes. First, we propose several simple optimisations to well-known integer compression schemes, and show experimentally that these lead to significant reductions in time. Second, we explore the impact of choice of compression scheme on retrieval efficiency. In experiments on large collections of data, we show two surprising results: use of simple <b>byte-aligned</b> codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and, even when an index fits entirely in memory, <b>byte-aligned</b> codes result in faster query evaluation than does an uncompressed index, emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover, <b>byte-aligned</b> schemes have only a modest space overhead: the most compact schemes result in indexes that are around 10 % of {{the size of}} the collection, while a <b>byte-aligned</b> scheme is around 13 %. We conclude that fast <b>byte-aligned</b> codes should be used to store integers in inverted lists...|$|E
40|$|Abstract—Industry-standard {{lossless}} compression algorithms (such as LZW) are usually implemented {{so that they}} work on bytes as symbols. Experiments indicate that data for which bytes are not the natural choice of symbols compress poorly using these implementations, while algorithms working on a bit level perform reasonably on byte-based data {{in addition to having}} computational advantages resulting from operating on a small alphabet. In this correspondence, we offer an information-theoretic explanation to these experimental results by assessing the redundancy (which is approximated by the divergence rate of two source distributions) of a bit-based model when applied to a byte-based source. More specifically, we study the problem of approximating a block Markov source (our model for byte-based data) with higher order Markov sources (which model bit-based Markov encoders), and show that the divergence rate between a block Markov source and the best matching higher order Markov model for that source converges to zero exponentially fast as the memory of the model increases. This result is applied to obtain bounds on the redundancy of certain symbol-based universal codes when they are used for <b>byte-aligned</b> sources. Index Terms—Binary codes, block Markov sources, <b>byte-aligned</b> sources, higher order Markov modeling, lossless coding...|$|E
40|$|We {{present a}} {{comparison}} of two new word-aligned schemes with some schemes for compressing bitmap indexes, including the well-known <b>byte-aligned</b> bitmap code (BBC). On both synthetic data and real application data, the new wordaligned schemes use only 50 % more space, but perform logical operations on compressed data 12 times faster than BBC. The new schemes achieve this performance advantage by guaranteeing that during logical operations every machine instruction performs useful work on words rather than on bytes or bits as in BBC. 1...|$|E
40|$|Abstract. This paper {{introduces}} NORX, a novel authenticated encryp-tion scheme supporting arbitrary parallelism {{degree and}} based on ARX primitives, yet not using modular additions. NORX has a unique parallel architecture based on the monkeyDuplex construction, with an original domain separation scheme for a simple processing of header/payload-/trailer data. Furthermore, NORX specifies a dedicated datagram to facilitate interoperability and avoid users the trouble of defining cus-tom encoding and signalling. NORX was optimized for efficiency in both software and hardware, with a SIMD-friendly core, almost <b>byte-aligned</b> rotations, no secret-dependent memory lookups, and only bitwise oper-ations. On a Haswell processor, a serial version of NORX runs at 2. 51 cycles per byte. Simulations of a hardware architecture for 180 nm UMC ASIC give a throughput of approximately 10 Gbps at 125 MHz...|$|E
40|$|Abstract. Byte codes have {{a number}} of {{properties}} that make them attractive for practical compression systems: they are relatively easy to construct; they decode quickly; and they can be searched using standard <b>byte-aligned</b> string matching techniques. In this paper we describe a new type of byte code in which the first byte of each codeword completely specifies the number of bytes that comprise the suffix of the codeword. Our mechanism gives more flexible coding than previous constrained byte codes, and hence better compression. The structure of the code also suggests a heuristic approximation that allows savings {{to be made in the}} prelude that describes the code. We present experimental results that compare our new method with previous approaches to byte coding, in terms of both compression effectiveness and decoding throughput speeds. ...|$|E
40|$|It is well {{established}} that bitmap indices are effcient for read-only attributes with low attribute cardinalities. For an attribute with a high cardinality, {{the size of the}} bitmap index can be very large. To overcome this size problem, specialized compression schemes are used. Even though there are empirical evidences that some of these compression schemes work well, there has not been any systematic analysis of their effectiveness. In this paper, we systematically analyze the two most effcient bitmap compression techniques, the <b>Byte-aligned</b> Bitmap Code (BBC) and the Word-Aligned Hybrid (WAH) code. Our analyses show that both compression schemes can be optimal. We propose a novel strategy to select the appropriate algorithms so that this optimality is achieved in practice. In addition, our analyses and tests show that the compressed indices are relatively small compared with commonly used indices such as B-trees...|$|E
40|$|When {{using an}} out-of-core {{indexing}} method {{to answer a}} query, it is generally assumed that the I/O cost dominates the overall query response time. Because of this, most research on indexing methods concentrate on reduceing the sizes of indices. For bitmap indices, compression {{has been used for}} this purpose. However, in most cases, operations on these compressed bitmaps, mostly bitwise logical operations such as AND, OR, and NOT, spend more time in CPU than in I/O. To speedup these operations, a number of specialized bitmap compression schemes have been developed; the best known of which is the <b>byte-aligned</b> bitmap code (BBC). They are usually faster in performing logical operations than the general purpose compression schemes, but, the time spent in CPU still dominates the total query response time. To reduce the query response time, we designed a CPU-friendly scheme named the word-aligned hybrid (WAH) code...|$|E
40|$|Many {{database}} applications make {{extensive use}} of bitmap indexing schemes. In this paper, we study how to improve the efficiencies of these indexing schemes by proposing new compression schemes for the bitmaps. Most compression schemes are designed primarily to achieve good compression. During query processing they can be orders of magnitude slower than their uncompressed counterparts. The new schemes are designed to bridge this performance gap by reducing compression effectiveness and improving operation speed. In a number of tests on both synthetic data and real application data, {{we found that the}} new schemes significantly outperform the well-known compression schemes while using only modestly more space. For example, compared to the <b>Byte-aligned</b> Bitmap Code, the new schemes are 12 times faster and it uses only 50 percent more space. The new schemes use much less space(< 30 percent) than the uncompressed scheme and are faster in a majority of the test cases...|$|E
40|$|Video {{transmission}} {{over the}} multi-path fading wireless channel has {{to overcome the}} inherent vulnerability of compressed video to the channel errors. To effectively prevent the corruption of video stream and its propagation in spatial and temporal domain, proactive error controls are widely being deployed. Among possible candidates, turbo code is known to exhibit superior error correction performance over fading channel. Ordinary turbo codes, however, are not suitable to support the variable-size segment of the video stream. A version of turbo code, <b>byte-aligned</b> variablelength turbo code, is thus proposed and applied for the robust video transmission system. Protection performance of the proposed turbo code is evaluated by applying it to GOB-based variable-size ITU-T H. 263 + video packets, where the protection level is controlled based on the joint source-channel criteria. The resulting performance comparison with the conventional RCPC code clearly demonstrates {{the possibility of the}} proposed approach for the time-varying correlated Rayleigh-fading channel...|$|E
40|$|Industry-standard {{lossless}} compression algorithms (such as LZW) are usually implemented {{so that they}} work on bytes as symbols. Experiments indicate that data for which bytes are not the natural choice of symbols compress poorly using these implementations, while algorithms working on a bit level perform reasonably on byte-based data {{in addition to having}} compu-tational advantages resulting from operating on a small alphabet. In this paper, we offer an information-theoretic explanation to these experimental results by assessing the redundancy (which is approximated by the divergence rate of two source distributions) of a bit-based model when applied to a byte-based source. More specifically, we study the problem of approximat-ing a block Markov source with higher order Markov sources, and show that the divergence rate between a block Markov source and the best-matching higher order Markov model for that source converges to zero exponentially fast as the memory of the model increases. This result is applied to obtain bounds on the redundancy of certain symbol-based universal codes when they are used for <b>byte-aligned</b> sources...|$|E
40|$|Many {{science and}} {{engineering}} applications use high-resolution unstructured hexahedral meshes to model solid 3 D shapes for finite element simulations. These simulations frequently dump the mesh and associated fields to disk for subsequent analysis, which involves the transfer of huge volumes of data. To reduce requirements on disk space and bandwidth, we propose efficient schemes for lossless online compression of hexahedral mesh geometry and connectivity. Our approach is to use hash-based value predictors to transform the mesh connectivity list into a more compact <b>byte-aligned</b> stream of symbols that can then be efficiently compressed using conventional text compressors such as gzip. Our scheme is memory efficient, fast, and simple to implement, and yields 1 – 3 orders of magnitude reduction {{on a set of}} benchmark meshes. For geometry and field coding, we derive a set of local spectral predictors optimized for each possible configuration of previously encoded and thus available vertices within a hexahedron. Combined with lossless floating-point residual coding, this approach improves considerably upon prior predictive geometry coding schemes. 1...|$|E
40|$|In this paper, {{we study}} the e#ects of {{compression}} on bitmap indexes. The main operations on the bitmaps during query processing are bitwise logical operations such as AND,OR,NOT, etc. Using the general purpose compression schemes, such as gzip, the logical operations on the compressed bitmaps are much slower {{than on the}} uncompressed bitmaps. Specialized compression schemes, like the <b>byte-aligned</b> bitmap code (BBC), are usually faster in performing logical operations than the general purpose schemes, {{but in many cases}} they are still orders of magnitude slower than the uncompressed scheme. To make the compressed bitmap indexes operate more e#ciently, we designed a CPU-friendly scheme which we refer to as the word-aligned hybrid code (WAH). Tests on both synthetic and real application data show that the new scheme significantly outperforms well-known compression schemes at a modest increase in storage space. Compared to BBC, a scheme well-known for its operational e#ciency, WAH performs logical operations about 12 times faster and uses only 60 % more space. Compared to the uncompressed scheme, in most test cases WAH is faster while still using less space. We further verified with additional tests that the improvement in logical operation speed translates to similar improvement in query processing speed...|$|E
40|$|An index in a {{database}} {{system is a}} data structure that utilizes redundant information about the base data to speed up common searching and retrieval operations. Most commonly used indexes are variants of B-trees, such as B+-tree and B*-tree. FastBit implements a set of alternative indexes call compressed bitmap indexes. Compared with B-tree variants, these indexes provide very efficient searching and retrieval operations by sacrificing the efficiency of updating the indexes after the modification of an individual record. In addition to the well-known strengths of bitmap indexes, FastBit has a special strength stemming from the bitmap compression scheme used. The compression method is called the Word-Aligned Hybrid (WAH) code. It reduces the bitmap indexes to reasonable sizes {{and at the same}} time allows very efficient bitwise logical operations directly on the compressed bitmaps. Compared with the well-known compression methods such as LZ 77 and <b>Byte-aligned</b> Bitmap code (BBC), WAH sacrifices some space efficiency for a significant improvement in operational efficiency. Since the bitwise logical operations are the most important operations needed to answer queries, using WAH compression has been shown to answer queries significantly faster than using other compression schemes. Theoretical analyses showed that WAH compressed bitmap indexes are optimal for one-dimensional range queries. Only the most efficient indexing schemes such as B+-tree and B*-tree have this optimality property. However, bitmap indexes are superior because they can efficiently answer multi-dimensional range queries by combining the answers to one-dimensional queries...|$|E
40|$|It is well {{established}} that bitmap indices are e#cient for read-only attributes with {{a small number of}} distinct values. For an attribute with a large number of distinct values, the size of the bitmap index can be very large. To overcome this size problem, specialized compression schemes are used. Even though there is empirical evidence that some of these compression schemes work well, there has not been any systematic analysis of their e#ectiveness. In this paper, we analyze the time and space complexities of the two most e#cient bitmap compression techniques known, the <b>Byte-aligned</b> Bitmap Code (BBC) and the Word-Aligned Hybrid (WAH) code, and study their performance on high cardinality attributes. Our analyses indicate that both compression schemes are optimal in time. The time and space required to operate on two compressed bitmaps are proportional to the total size of the two bitmaps. We demonstrate further that an in-place OR algorithm can operate on a large number of sparse bitmaps in time linear in their total size. Our analyses also show that the compressed indices are relatively small compared with commonly used indices such as B-trees. Given these facts, we conclude that bitmap index is e#cient on attributes of low cardinalities as well as on those of high cardinalities. We also verify the analytical results with extensive tests, and identify an optimal way to combine di#erent options to achieve the best performance. The test results confirm the linearity in the total size of the compressed bitmaps, and that WAH outperforms BBC by about a factor of two...|$|E
40|$|When {{using an}} out-of-core {{indexing}} method {{to answer a}} query, it is generally assumed that the I/O cost dominates the overall query response time. Because of this, most research on indexing methods concentrate on reducing the sizes of indices. For bitmap indices, compression {{has been used for}} this purpose. However, in most cases, operations on these compressed bitmaps, mostly bitwise logical operations such as AND, OR, and NOT, spend more time in CPU than in I/O. To speedup these operations, a number of specialized bitmap compression schemes have been developed; the best known of which is the <b>byte-aligned</b> bitmap code (BBC). They are usually faster in performing logical operations than the general purpose compression schemes, but, the time spent in CPU still dominates the total query response time. To reduce the query response time, we designed a CPU-friendly scheme named the word-aligned hybrid (WAH) code. In this paper, we prove that the sizes of WAH compressed bitmap indices are about two words per row for large range of attributes. This size is smaller than typical sizes of commonly used indices, such as a B-tree. Therefore, WAH compressed indices are not only appropriate for low cardinality attributes but also for high cardinality attributes. In the worst case, the time to operate on compressed bitmaps is proportional to the total size of the bitmaps involved. The total size of the bitmaps required to answer a query on one attribute is proportional to the number of hits. These indicate that WAH compressed bitmap indices are optimal. To verify their effectiveness, we generated bitmap indices for four different datasets and measured the response time of many range queries. Tests confirm that sizes of compressed bitmap indices are indeed smaller than B-tree indices, and query processing with WAH compressed indices is much faster than with BBC compressed indices, projection indices and B-tree indices. In addition, we also verified that the average query response time is proportional to the index size. This indicates that the compressed bitmap indices are efficient for very large datasets...|$|E
40|$|Linux R ○ has {{supported}} {{a large number}} of SMP systems based on a variety of CPUs since the 2. 0 kernel. Linux has done an excellent job of abstracting away differences among these CPUs, even in kernel code. One important difference is how CPUs allow memory accesses to be reordered in SMP systems. SMMP Hardware Memory accesses are among the slowest of a CPU’s operations, {{due to the fact that}} Moore’s law has increased CPU instruction performance at a much greater rate than it has increased memory performance. This difference in performance increase means that memory operations have been getting increasingly expensive compared to simple register-to-register instructions. Modern CPUs sport increasingly large caches in order to reduce the overhead of these expensive memory accesses. These caches can be thought of as simple hardware hash table with fixed size buckets and no chaining, as shown in Figure 1. This cache has sixteen “lines ” and two “ways ” for a total of 32 “entries”, each entry containing a single 256 -byte “cache line”, which is a 256 <b>byte-aligned</b> block of memory. This cache line size is a little on the large size, but makes the hexadecimal arithmetic much simpler. In hardware parlance, this is a two-way set-associative cache, and is analogous to a software hash table with sixteen buckets, where each bucket’s hash chain is limited to at most two elements. Since this cache is implemented in hardware, the hash function is extremely simple: extract four bits from the memory address. In Figure 1, each box corresponds to a cache entry, which can contain a 256 -byte cache line. However, a cache entry can be empty, as indicated by the empty boxes in the figure. The rest of the boxes are flagged with the memory address of the cache line that they contain. Since the cache lines must be 256 -byte aligned, the low eight bits of each address are zero, and the choice of hardware hash function means that the next-higher four bits match the hash line number. The situation depicted in the figure might arise if the program’s code were located at address 0 x 43210 E 00 through 0 x 43210 EFF, and this program accessed data sequentially from 0 x 12345000 through 0 x 12345 EFF...|$|E

