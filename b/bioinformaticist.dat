9|32|Public
50|$|As {{the names}} suggest, the {{projects}} mentioned above use different programming languages. All of these APIs offer similar tools so on what criteria should one base their choice? For programmers who are experienced {{in only one}} of these languages, the choice is straightforward. However, for a well-rounded <b>bioinformaticist</b> who knows all of these languages and wants to choose the best language for a job, the choice can be made based on the following guidelines given by a software review done on the Bio* tool-kits.|$|E
40|$|Altmetrics" {{refers to}} methods of {{measuring}} scholarly impact using Web-based social media. Why does it matter? In many academic fields, attaining scholarly prestige means publishing research articles in important scholarly journals. However, {{many in the}} academic community consider a journal's prestige, which is determined by a metric calculated using the number of citations to the journal, to be a poor proxy {{for the quality of}} the individual author's work. At the same time, hiring and promotion committees {{are looking for ways to}} determine the impact of alternate formats now commonly used by researchers such as blogs, data sets, videos, and social media. The panelists all work with innovative new tools for assessing scholarly impact. They are: Jason Priem, Co-Founder, ImpactStory; Kristi Holmes, <b>Bioinformaticist,</b> Bernard Becker Medical Library, Washington University in St. Louis School of Medicine; and Caitlin Aptowicz Trasande, Head of Science Metrics, Digital Science...|$|E
40|$|What is Bioinformatics? Bioinformatics is {{the study}} of {{applying}} computational methods to large amount of biological information in order to facilitate discoveries in biology and medicine. A <b>bioinformaticist</b> is by nature a biologist, a biochemist, a biophysicist, a statistician, a computer scientist, and an engineer. He/she bridges the gaps between different fields in order to bring about a better understanding of biology. It’s not surprising to find that bioinformatics covers a wide array of topics. For example, bioinformatics includes sequence alignment, homology detection, Single nucleotide Polymorphism (SNP) detection and characterization, genetic element discovery (including genes, promoters, enhancers, repeats etc.), analysis of whole genomes, functional annotation of newly discovered genes and proteins, analysis of gene expression data generated by techniques such as DNA microarray, secondary and tertiary protein structure prediction, protein interactions at a molecular level as well as at a cellular level, small molecule docking, and modeling genetic and protein networks and pathways. To a large extent, the emerging fields of genomics and proteomics lie within bioinformatics. Indeed, this is a rapidly growing field with many interdisciplinary teams approaching biological and medica...|$|E
2500|$|Rutgers is {{also home}} to the RCSB Protein Data bank, 'an {{information}} portal to Biological Macromolecular Structures' cohosted with the San Diego Supercomputer Center. This database is the authoritative research tool for <b>bioinformaticists</b> using protein primary, secondary and tertiary structures worldwide.' ...|$|R
40|$|GermOnline {{provides}} information and microarray expression data for genes involved in mitosis and meiosis, gamete formation and germ line development across species. The database has been developed, {{and is being}} curated and updated, by life scientists in cooperation with <b>bioinformaticists.</b> Information is contributed through an online form using free text, images and the controlled vocabulary developed by the GeneOntology Consortium. Authors provide up to three references in support o...|$|R
5000|$|The term [...] "Paleogeoscience" [...] {{was coined}} by the Collaboration and Cyberinfrastructure for Paleogeosciences (C4P) {{research}} coordination network (RCN), a National Science Foundation EarthCube funded project intending to foster collaboration among paleogeoscientists, paleobiologists, <b>bioinformaticists,</b> stratigraphers, geochronologists, geographers, data scientists, and computer scientists with an aim to dramatically improve {{the application of}} modern data management approaches, data mining technologies, and computational methods to better analyze data within the paleogeosciences and other domains and disciplines.|$|R
40|$|Sean Eddy, {{telephone}} interview by Kathryn Maxson and Robert Cook-Deegan, conducted from Durham. NC, 15 March 2012. A noted <b>bioinformaticist,</b> Sean Eddy completed postdoctoral {{training at the}} Medical Research Council (MRC) Laboratory of Molecular Biology (LMB) in Cambridge from 1992 - 1995, where he worked with John Sulston and Richard Durbin. From 1995 through {{the completion of the}} HGP, he worked at Washington University School of Medicine and was an HHMI investigator there from 2000 - 2006. Though he did not participate directly in the Bermuda meetings, from his positions at two epicenters of the HGP he gained valuable perspectives on the Project. This transcript is available beginning 1 July 2014. Keywords: Human Genome Project, HGP, interview, Bermuda Principles, Bermuda Accord, International Strategy Meetings on Human Genome Sequencing, data sharing, science policy, genomics, genome, genome sequence, genetics, DNA sequence, DNA, deoxyribonucleic acid, NIH, National Institutes of Health, DOE, Department of Energy, Medical Research Council, MRC, Laboratory of Molecular Biology, LMB, Washington University School of Medicine, Howard Hughes Medical Institute, HHMI, Sean Eddy. This research was supported by the NHGRI-funded Duke Center for Public Genomics, P 50 HG 003391, with supplementary funding from the Ewing Marion Kauffman Foundation...|$|E
40|$|Bioinformatics is a {{necessary}} technology and tool, which current molecular biologists cannot do without {{in light of the}} genome data explosion and advent of fields such as genomics and proteomics. Yet, even with this in mind, there is still a limited pool of human resource to support and develop bioinformatics particularly in poorer developing nations. The highly multidisciplinary nature of this field can be an obstacle in converting just any molecular biologist into a proficient and capable <b>bioinformaticist.</b> Coupled with the high costs of software, hardware, technical maintenance and systems support; many developing and poor countries {{may not be able to}} fully realize their biotech potential either academically or economically due to the inability to fully utilize bioinformatics as a technology to support and co-develop with molecular biology oriented research programmes. Malaysia, a developing South East Asian nation took steps to counter these problems and develop an infrastructure for bioinformatics to support nationally important biotech research agendas, particularly government initiatives in genomics and proteomics, with the setting up of NBBnet - The National Biotechnology and Bioinformatics network. We further propose the setting up of similar models in other developing countries, an endeavour which we are willing to share our experiences and resources. We hope that these networks may in the future evolve into a super network of national or regional networks on biotechnology and bioinformatics...|$|E
40|$|ICB is {{organized}} {{to emphasize the}} unity of biological science across the size/complexity hierarchy. It focuses students on five unifying themes (“big ideas”) : information, evolu-tion, cells, homeostasis, and emergent properties. The text is divided into 10 major sections, five of which center on Vol. 14, 1 – 4, Fall 2015 Integrating Concepts in Biology (ICB) is the apt title of this groundbreaking electronic textbook (see Supplemental Ma-terial). The target audience is students seeking an introduc-tion to biology. It is structured to focus student attention on key concepts underlying biology {{at all levels of}} organization. In contrast to the current encyclopedic model of an introduc-tory textbook, this e-book makes effective use of electroni-cally linked text and online resources, selects examples to explore in depth, and offers students means to deepen their understanding. It explicitly uses current ideas about student learning, supported by evidence of best practices, to help stu-dents develop as biologically literate thinkers. The authors are colleagues at Davidson College: biologists A. Malcolm Campbell (cellular and molecular biology) and Christopher J. Paradise (ecology) and mathematician/computer scientist/ <b>bioinformaticist</b> Laurie J. Heyer. They share a common pas-sion for innovative pedagogy and multidisciplinarity, and ICB is their laudable effort to address many of the widely rec-ognized problems evident in introductory biology texts and introductory courses and explored in recent reports such a...|$|E
40|$|Current {{trends in}} the {{bioinformatics}} job market were assessed using a sample of 1, 996 online employment advertisements from the 6 -year period of January 2003 through December 2008. Job postings were classified by employer type, job role, and location, and a content analysis of the text of a subset of 404 of the posts was performed to identify detailed characteristics associated with master’s degree level positions, such as educational requirements and preferred scientific and technical skills. Consistent with previous studies, academic institutions, corporations, and research institutes are the primary employers of <b>bioinformaticists.</b> In the U. S. only three states, California, Maryland, and Massachusetts, provided the majority of opportunities. Graduates from all levels of education are needed in the field, although those with a bachelor’s degree in computer science or a Ph. D. in biology or bioinformatics are especially in demand. Perl programming is the most frequently requested skill across advertised positions, and experience using bioinformatics software tools (such as BLAST, CLUSTAL, and HMMER) is the bioinformatics skill in greatest demand. A small number of positions specifically requesting librarians and LIS-trained <b>bioinformaticists</b> was observed, but the significant bioinformatics activities in biomedical libraries over {{the same time period}} are not reflected in this snapshot of the bioinformatics workforce...|$|R
40|$|Large-scale {{computing}} often {{consists of}} many speculative tasks to test hypotheses, search for insights, and review potentially finished products. For example, speculative tasks are issued by <b>bioinformaticists</b> comparing DNA sequences and computer graphics artists adjusting scene properties. This paper promotes a new computing model for shared clusters and grids in which researchers and end-users exploring search spaces disclose sets of speculative tasks, request results as needed, and cancel unfinished tasks if early results suggest {{no need to}} continue. Doing so matches natural usage patterns, making users more effective, and also enables {{a new class of}} schedulers...|$|R
40|$|AbstractIntegrated genome {{databases}} – {{such as the}} UCSC, Ensembl and NCBI MapViewer databases – {{and their}} associated data querying and visualization interfaces (e. g. the genome browsers) have transformed the way that molecular biologists, geneticists and <b>bioinformaticists</b> analyze genomic data. Nevertheless, because {{of the complexity of}} these tools, many researchers take advantage of only a fraction of their capabilities. In this tutorial, using examples from medical genetics and alternative splicing, I describe some of the biological questions that can be addressed with these techniques. I also show why doing so typically is more effective than using alternative methods and indicate some of the resources available for learning more about the advanced capabilities of these powerful tools...|$|R
40|$|A b s t r a c t The rapid {{advances}} in high-throughput biotechnologies such as DNA microarrays and mass spectrometry have generated {{vast amounts of}} data ranging from gene expression to proteomics data. The large size and complexity involved in analyzing such data demand {{a significant amount of}} computing power. High-performance computation (HPC) is an attractive and increasingly affordable approach to help meet this challenge. There is a spectrum of techniques {{that can be used to}} achieve computational speedup with varying degrees of impact in terms of how drastic a change is required to allow the software to run on an HPC platform. This paper describes a high-productivity/low-maintenance (HP/LM) approach to HPC that is based on establishing a collaborative relationship between the <b>bioinformaticist</b> and HPC expert that respects the former’s codes and minimizes the latter’s efforts. The goal of this approach is to make it easy for bioinformatics researchers to continue to make iterative refinements to their programs, while still being able to take advantage of HPC. The paper describes our experience applying these HP/LM techniques in four bioinformatics case studies: (1) genome-wide sequence comparison using Blast, (2) identification of biomarkers based on statistical analysis of large mass spectrometry data sets, (3) complex genetic analysis involving ordinal phenotypes, (4) large-scale assessment of the effect of possible errors in analyzing microarray data. The case studies illustrate how the HP/LM approach can be applied to a range of representative bioinformatics applications and how the approach can lead to significant speedup of computationally intensive bioinformatics applications, whil...|$|E
40|$|Abstract Background Large-scale {{sequence}} {{comparison is}} {{a powerful tool for}} biological inference in modern molecular biology. Comparing new sequences to those in annotated databases is a useful source of functional and structural information about these sequences. Using software such as the basic local alignment search tool (BLAST) or HMMPFAM to identify statistically significant matches between newly sequenced segments of genetic material and those in databases is an important task for most molecular biologists. Searching algorithms are intrinsically slow and data-intensive, {{especially in light of the}} rapid growth of biological sequence databases due to the emergence of high throughput DNA sequencing techniques. Thus, traditional bioinformatics tools are impractical on PCs and even on dedicated UNIX servers. To take advantage of larger databases and more reliable methods, high performance computation becomes necessary. Results We describe the implementation of SS-Wrapper (Similarity Search Wrapper), a package of wrapper applications that can parallelize similarity search applications on a Linux cluster. Our wrapper utilizes a query segmentation-search (QS-search) approach to parallelize sequence database search applications. It takes into consideration load balancing between each node on the cluster to maximize resource usage. QS-search is designed to wrap many different search tools, such as BLAST and HMMPFAM using the same interface. This implementation does not alter the original program, so newly obtained programs and program updates should be accommodated easily. Benchmark experiments using QS-search to optimize BLAST and HMMPFAM showed that QS-search accelerated the performance of these programs almost linearly in proportion to the number of CPUs used. We have also implemented a wrapper that utilizes a database segmentation approach (DS-BLAST) that provides a complementary solution for BLAST searches when the database is too large to fit into the memory of a single node. Conclusions Used together, QS-search and DS-BLAST provide a flexible solution to adapt sequential similarity searching applications in high performance computing environments. Their ease of use and their ability to wrap a variety of database search programs provide an analytical architecture to assist both the seasoned <b>bioinformaticist</b> and the wet-bench biologist. </p...|$|E
40|$|Users often behave speculatively, {{submitting}} {{work that}} initially {{they do not}} know is needed. Farm computing often consists of single node speculative tasks issued by, e. g., <b>bioinformaticists</b> comparing dna sequences and computer graphics artists rendering scenes who wish to reduce their time waiting for needed tasks and the amount they will be charged for unneeded speculation. Existing schedulers are not e#ective for such behavior. Our `batchactive' scheduling exploits speculation: users submit explicitlylabeled batches of speculative tasks, interactively request outputs when ready to process them, and cancel tasks found not to be needed. Users are encouraged to participate by a new pricing mechanism charging for only requested tasks no matter what ran...|$|R
40|$|Accurate {{prediction}} of B-cell antigenic epitopes {{is important for}} immunologic research and medical applications, but compared with other bioinformatic problems, antigenic epitope prediction is more challenging because of the extreme variability of antigenic epitopes, where the paratope on the antibody binds specifically to a given epitope with high precision. In spite of the continuing efforts in the past decade, the problem remains unsolved and therefore still attracts {{a lot of attention}} from <b>bioinformaticists.</b> Recently, several discontinuous epitope prediction servers became available, and it is intriguing to review all existing methods and evaluate their performances on the same benchmark. In addition, these methods are also compared against common binding site prediction algorithms, since they have been frequently used as substitutes in the absence of good epitope prediction methods...|$|R
40|$|Bardsley, M. A. A {{proposed}} {{information resource}} for bioinformaticists: gene expression statistics. This paper briefly describes {{the field of}} bioinformatics and its complex information needs. The information {{in the field of}} bioinformatics is not yet standardized in many aspects and new information is being generated at an increasing rate. A common ontology is needed to facilitate communication between systems and scientists. Bioinformatic computer systems such as databases need to be constructed with standardization and integration in mind so as to facilitate data interchange. One information need of <b>bioinformaticists</b> that is being met unsatisfactorily is a tool that communicates gene expression statistics. This paper outlines the need and briefly proposes a solution. It is hoped that the ideas presented here will generate discussion and action...|$|R
40|$|<b>Bioinformaticists</b> use the Basic Local Alignment Search Tool (BLAST) to {{characterize}} an unknown sequence by comparing it against {{a database of}} known sequences, thus detecting evolutionary relationships and biological properties. mpiBLAST is a widely-used, high-performance, open-source parallelization of BLAST that runs on a computer cluster delivering super-linear speedups. However, the Achilles heel of mpiBLAST is its lack of modularity, adversely affecting maintainability and extensibility; an effective architectural refactoring will benefit both users and developers. This paper describes our experiences in the architectural refactoring of mpiBLAST into a modular, high-performance software package. Our evaluation of five component-oriented designs culminated in a design that enables modularity while retaining high-performance. Furthermore, we achieved this refactoring effectively and efficiently using eXtreme Programming techniques. These experiences will be of value to software engineers {{faced with the challenge}} of creating maintainable and extensible, high-performance, bioinformatics software...|$|R
40|$|Abstract: 2 ̆ 2 Large-scale {{computing}} often {{consists of}} many speculative tasks to test hypotheses, search for insights, and review potentially finished products. For example, speculative tasks are issued by <b>bioinformaticists</b> comparing DNA sequences and computer graphics artists adjusting scene properties. This paper promotes a new computing model for shared clusters and grids in which researchers and end-users exploring search spaces disclose sets of speculative tasks, request results as needed, and cancel unfinished tasks if early results suggest {{no need to}} continue. Doing so matches natural usage patterns, making users more effective, and also enables {{a new class of}} schedulers. In simulation, we demonstrate how batchactive schedulers significantly reduce user-observed response times relative to conventional models in which tasks are requested one at a time (interactively) or requested in batches without specifying which are speculative. Over a range of simulated user behavior, for 20...|$|R
40|$|The {{development}} of bioinformatics as an influential biological field should interest philosophers of biology and philosophers {{of science in}} general. Bioinformatics contributes significantly to the {{development of}} biological knowledge using a variety of scientific methods. Particular tools used by <b>bioinformaticists,</b> such as BLAST, phylogenetic tree creation software, and DNA microarrays, will be shown to utilize the scientific methods of extended cognition, analogical reasoning, and representations of mechanisms. Extended cognition is found in bioinformatics through the use of computer databases and algorithms in the representation and development of scientific theories in bioinformatics. Analogical reasoning is found in bioinformatics through particular analogical comparisons that are made between biological sequences and operations. Lastly, scientific theories that are created using certain bioinformatics tools are often representations of mechanisms. These methods are found in other scientific fields, but it will be shown that these methods are expanded in bioinformatics research through the use of computers to make the methods of analogical reasoning and representation of mechanisms more powerful...|$|R
40|$|Motivation: Bioinformatics {{researchers}} {{have a variety}} of program-ming languages and architectures at their disposal, and recent advances in graphics processing unit (GPU) computing have added a promising new option. However, many performance comparisons inate the actual advantages of GPU technology. In this study, we carry out a realistic performance evaluation of SNPrank, a network centrality algorithm that ranks single nucleotide polymorhisms (SNPs) based on their importance {{in the context of a}} phenotype-specic inter-action network. Our goal is to identify the best computational engine for the SNPrank web application and to provide a variety of well-tested implementations of SNPrank for <b>Bioinformaticists</b> to integrate into their research. Results: Using SNP data from the Wellcome Trust Case Control Consortium genome-wide association study of Bipolar Disorder, we compare multiple SNPrank implementations, including Python, Mat-lab, and Java as well as CPU vs GPU implementations. When compared with näve, single-threaded CPU implementations, the GPU yields a large improvement in the execution time. However, with comparable effort, multi-threaded CPU implementations negate the apparent advantage of GPU implementations. Availability: The SNPrank code is open source and available a...|$|R
40|$|Large-scale ad hoc {{analytics}} of {{genomic data}} is popular using the R-programming language supported by 671 software packages provided by Bioconductor. More recently, analytical jobs are benefitting from on-demand computing and storage, their scalability and their low maintenance cost, {{all of which}} are offered by the cloud. While Biologists and <b>Bioinformaticists</b> can take an analytical job and execute it on their personal workstations, it remains challenging to seamlessly execute the job on the cloud infrastructure without extensive knowledge of the cloud dashboard. How analytical jobs can not only with minimum effort be executed on the cloud, but also how both the resources and data required by the job can be managed is explored in this paper. An open-source light-weight framework for executing R-scripts using Bioconductor packages, referred to as `RBioCloud', is designed and developed. RBioCloud offers a set of simple command-line tools for managing the cloud resources, the data and the execution of the job. Three biological test cases validate the feasibility of RBioCloud. The framework is publicly available from [URL] Webpage: [URL]...|$|R
40|$|Systematic {{analyses}} are included as integral parts of bioinformatic analysis. The use of phenetic and phylogenetic trees {{in many of}} the newer areas of biology create a need for <b>bioinformaticists</b> to understand more completely the nuances of systematic analysis. Any description in comparative biology, universally begins with what information to use in the comparative endeavor. Phylogenetic approaches are no different. The diversity of approaches and phylogenetic questions in systematics have sometimes hindered a precise understanding of what primary data should be collected to perform such analyses. In addition, one should always {{keep in mind that the}} objective of systematic organization of entities in nature not only strives to organize those entities in an objective, repeatable and operational way, but also to organize the attributes of the entities in a similar hierarchical context. This paper attempts to describe characters as the basis of all comparative analysis, to describe the diverse kinds of primary data that exist today in biology, genomics, and bioinformatics, and to place these kinds of primary data in the context of the established approaches to tree building...|$|R
40|$|The {{exponential}} growth of biological data {{has given rise}} to new and difficult challenges. Because large data is often dealt with, it is inefficient to infer from each individual characteristics of a given dataset. <b>Bioinformaticists</b> are developing quantitative techniques to analyze and interpret key data properties. Graph algorithms can provide powerful and intuitive insight on such properties [1]. Using this approach, we collect biological data from transcriptomic and protein-protein interaction (PPI) sources. These data can be represented as a correlation matrix, where the rows are the vertices and the columns are the edges. We will analyze these graphs, and describe their differing structural characteristics. Materials and methods We are using a high throughput method for graphical exploration of genomic and proteomic data. Experimental datasets are extracted from the public databases Biomart and Gene Expression Omnibus (GEO) [2, 3]. R [4] and MATLAB are used to develop algorithms that compute and compare various structural characteristics. We specifically developed an in-house script used to output essential histograms and unweighted/ weighted edges. We are currently developing protocols to analyze the comparison of transcriptomes and PPI sources...|$|R
40|$|Abstract Background Integration and {{exploration}} of {{data obtained from}} genome wide monitoring technologies {{has become a major}} challenge for many <b>bioinformaticists</b> and biologists due to its heterogeneity and high dimensionality. A widely accepted approach to solve these issues has been the creation and use of controlled vocabularies (ontologies). Ontologies allow for the formalization of domain knowledge, which in turn enables generalization in the creation of querying interfaces {{as well as in the}} integration of heterogeneous data, providing both human and machine readable interfaces. Results We designed and implemented a software tool that allows investigators to create their own semantic model of an organism and to use it to dynamically integrate expression data obtained from DNA microarrays and other probe based technologies. The software provides tools to use the semantic model to postulate and validate of hypotheses on the spatial and temporal expression and function of genes. In order to illustrate the software's use and features, we used it to build a semantic model of rice (Oryza sativa) and integrated experimental data into it. Conclusion In this paper we describe the development and features of a flexible software application for dynamic gene expression data annotation, integration, {{and exploration}} called Orymold. Orymold is freely available for non-commercial users from [URL] </p...|$|R
40|$|ABSTRACT Our {{rapidly growing}} {{knowledge}} regarding genetic {{variation in the}} human genome offers great potential for understanding the genetic etiology of disease. This, in turn, could revolutionize detection, treatment, {{and in some cases}} prevention of disease. While genes for most of the rare monogenic diseases have already been discovered, most common diseases are complex traits, resulting from multiple gene-gene and gene-environment interactions. Detecting epistatic genetic interactions that predispose for disease is an important, but computationally daunting, task currently facing <b>bioinformaticists.</b> Here, we propose a new evolutionary approach that attempts to hill-climb from large sets of candidate epistatic genetic features to smaller sets, inspired by Kauffman’s “random chemistry” approach to detecting small auto-catalytic sets of molecules from within large sets. Although the algorithm is conceptually straightforward, its success hinges upon the creation of a fitness function able to discriminate large sets that contain subsets of interacting genetic features from those that don’t. Here, we employ an approximate and noisy fitness function based on the ReliefF data mining algorithm. We establish proof-of-concept using synthetic data sets, where individual features have no marginal effects. We show that the resulting algorithm can successfully detect epistatic pairs from up to 1000 candidate single nucleotide polymorphisms in time that is linear {{in the size of the}} initial set, although success rat...|$|R
40|$|GermOnline {{provides}} information and microarray expression data for genes involved in mitosis and meiosis, gamete formation and germ line development across species. The database has been developed, {{and is being}} curated and updated, by life scientists in cooperation with <b>bioinformaticists.</b> Information is contributed through an online form using free text, images and the controlled vocabulary developed by the GeneOntology Consortium. Authors provide up to three references in support of their contribution. The database is governed by an international board of scientists to ensure a standardized data format and the highest quality of GermOnline’s information content. Release 2. 0 provides exclusive access to microarray expression data from Saccharomyces cerevisiae and Rattus norvegicus, as well as curated information on ∼ 700 genes from various organisms. The locus report pages include links to external databases that contain relevant annotation, microarray expression and proteome data. Conversely, the Saccharomyces Genome Database (SGD), S. cerevisiae GeneDB and Swiss-Prot link to the budding yeast section of GermOnline from their respective locus pages. GermOnline, a fully operational prototype subject-oriented knowledgebase designed for community annotation and array data visualization, is accessible at [URL] The target audience includes researchers who work on mitotic cell division, meiosis, gametogenesis, germ line development, human reproductive health and comparative genomics...|$|R
40|$|Large-scale {{computing}} often {{consists of}} many speculative tasks to test hypotheses, search for insights, and review potentially finished products. For example, speculative tasks are issued by <b>bioinformaticists</b> comparing DNA sequences and computer graphics artists adjusting scene properties. This paper promotes a new computing model for shared clusters and grids in which researchers and end-users exploring search spaces disclose sets of speculative tasks, request results as needed, and cancel unfinished tasks if early results suggest {{no need to}} continue. Doing so matches natural usage patterns, making users more effective, and also enables {{a new class of}} schedulers. In simulation, we demonstrate how batchactive schedulers significantly reduce user-observed response times relative to conventional models in which tasks are requested one at a time (interactively) or requested in batches without specifying which are speculative. Over a range of simulated user behavior, for 20 % of our simulations, user-observed response time is at least two times better under a batchactive scheduler, and about 50 % better on average. Batchactive schedulers achieve such improvements by segregating tasks into two queues based on whether a task is speculative and scheduling these queues separately. Moreover, we show how user costs can be reduced under an incentive cost model of charging only for tasks whose results are requested...|$|R
40|$|The {{complexity}} of translation is a classical dilemma {{in the evolution}} of biological systems. Efficient translation requires coordination of complex, highly evolved RNAs and proteins; however, complex, highly evolved RNAs and proteins could not evolve without efficient translation system. At the heart of this complexity is the ribosome, itself a remarkably complex molecular machine. Our work illustrates the ribosome as deconstructed units of modification. Here we have deconstructed a segment of the ribosome to interacting RNA-protein units. L 23 interacts in vivo with both Domain III (DIII) and Domain IIIcore (DIIIcore) independently of the fully assembled ribosome. This suggests that DIIIcore represents the functional rRNA unit in DIII-L 23 interaction. Furthermore, L 23 peptide sustains binding function in vitro with both DIII and DIIIcore independently of any stabilizing effects from the globular domain of L 23. The ability of L 23 peptide to form a 1 : 1 complex with both DIII and DIIIcore suggests that L 23 peptide is the functional rProtein unit in DIII-L 23 interaction. We believe that our results will stimulate interest and discussions in the significance of 3 D architecture and units of evolution in the ribosome. The ubiquity of the ribosome in cellular life prognosticates that our results impact and appeal to biologists, chemists, <b>bioinformaticists,</b> as well as the general scientific community. MSCommittee Chair: Williams, Loren; Committee Member: Hud, Nicholas; Committee Member: Oyelere, Adegboyega; Committee Member: Wartell, Roge...|$|R
40|$|Metabolomics is the {{methodology}} that identifies and measures global pools of small molecules (of less than about 1, 000 [*]Da) of a biological sample, which are collectively called the metabolome. Metabolomics can therefore reveal the metabolic {{outcome of a}} genetic or environmental perturbation of a metabolic regulatory network, and thus provide insights into the structure and regulation of that network. Because of the chemical complexity of the metabolome and limitations associated with individual analytical platforms for determining the metabolome, it is currently difficult to capture the complete metabolome of an organism or tissue, which {{is in contrast to}} genomics and transcriptomics. This paper describes the analysis of Arabidopsis metabolomics data sets acquired by a consortium that includes five analytical laboratories, <b>bioinformaticists,</b> and biostatisticians, which aims to develop and validate metabolomics as a hypothesis-generating functional genomics tool. The consortium is determining the metabolomes of Arabidopsis T-DNA mutant stocks, grown in standardized controlled environment optimized to minimize environmental impacts on the metabolomes. Metabolomics data were generated with seven analytical platforms, and the combined data is being provided to the research community to formulate initial hypotheses about genes of unknown function (GUFs). A public database (www. PlantMetabolomics. org) has been developed to provide the scientific community with access to the data along with tools to allow for its interactive analysis. Exemplary datasets are discussed to validate the approach, which illustrate how initial hypotheses can be generated from the consortium-produced metabolomics data, integrated with prior knowledge to provide a testable hypothesis concerning the functionality of GUFs...|$|R
40|$|Background: Next-generation {{sequencing}} (NGS) is now {{a commonplace}} tool for molecular characterisation of virtually any species of interest. Despite the ever-increasing use of NGS in laboratories worldwide, analysis of whole genome re-sequencing (WGS) datasets {{from start to finish}} remains nontrivial due to the fragmented nature of NGS software and the lack of experienced <b>bioinformaticists</b> in many research teams. Findings. We describe SPANDx (Synergised Pipeline for Analysis of NGS Data in Linux), a new tool for high-throughput comparative analysis of haploid WGS datasets comprising one through thousands of genomes. SPANDx consolidates several well-validated, open-source packages into a single tool, mitigating the need to learn and manipulate individual NGS programs. SPANDx incorporates BWA for alignment of raw NGS reads against a reference genome or pan-genome, followed by data filtering, variant calling and annotation using Picard, GATK, SAMtools and SnpEff. BEDTools has also been included for genetic locus presence/absence (P/A) determination to easily visualise the core and accessory genomes. Additional SPANDx features include construction of error-corrected single-nucleotide polymorphism (SNP) and insertion-deletion matrices, and P/A matrices, to enable user-friendly visualisation of genetic variants. The SNP matrices generated using VCFtools and GATK are directly importable into PAUP∗, PHYLIP or RAxML for downstream phylogenetic analysis. SPANDx has been developed to handle NGS data from Illumina, Ion Personal Genome Machine (PGM) and 454 platforms, and we demonstrate that it has comparable performance across Illumina MiSeq/HiSeq 2000 and Ion PGM data. Conclusion: SPANDx is an all-in-one tool for comprehensive haploid WGS analysis. SPANDx is open source and is freely available at:. © 2014 Sarovich and Price; licensee BioMed Central Ltd...|$|R
40|$|BackgroundNext-generation {{sequencing}} (NGS) is now {{a commonplace}} tool for molecular characterisation of virtually any species of interest. Despite the ever-increasing use of NGS in laboratories worldwide, analysis of whole genome re-sequencing (WGS) datasets {{from start to finish}} remains nontrivial due to the fragmented nature of NGS software and the lack of experienced <b>bioinformaticists</b> in many research teams. FindingsWe describe SPANDx (Synergised Pipeline for Analysis of NGS Data in Linux), a new tool for high-throughput comparative analysis of haploid WGS datasets comprising one through thousands of genomes. SPANDx consolidates several well-validated, open-source packages into a single tool, mitigating the need to learn and manipulate individual NGS programs. SPANDx incorporates BWA for alignment of raw NGS reads against a reference genome or pan-genome, followed by data filtering, variant calling and annotation using Picard, GATK, SAMtools and SnpEff. BEDTools has also been included for genetic locus presence/absence (P/A) determination to easily visualise the core and accessory genomes. Additional SPANDx features include construction of error-corrected single-nucleotide polymorphism (SNP) and insertion-deletion matrices, and P/A matrices, to enable user-friendly visualisation of genetic variants. The SNP matrices generated using VCFtools and GATK are directly importable into PAUP*, PHYLIP or RAxML for downstream phylogenetic analysis. SPANDx has been developed to handle NGS data from Illumina, Ion Personal Genome Machine (PGM) and 454 platforms, and we demonstrate that it has comparable performance across Illumina MiSeq/HiSeq 2000 and Ion PGM data. ConclusionSPANDx is an all-in-one tool for comprehensive haploid WGS analysis. SPANDx is open source and is freely available at: [URL] webcite...|$|R
40|$|The {{pattern of}} linkage disequilibrium (LD) plays {{a central role}} in {{genome-wide}} association studies of identifying genetic variation responsible of common human diseases. A Single Nucleotide Polymorphism or SNP is a DNA sequence variation occurring when a single nucleotide in the genome differs between members of species. Recent studies show that the patterns of linkage disequilibrium observed in human chromosome reveal a block-like structure; the high LD regions are called haplotype blocks, and furthermore, a small subset of SNPs, called tag SNPs, is sufficient to capture the haplotype patterns in each haplotype block. Both Patil [18] and Zhang et al. [24] have proposed algorithms to partition haplotype sample into blocks fully under the circumstances of requiring minimal number of tag SNPs. However, when resources are limited, investigators and biologists {{may not be able to}} genotype all the tag SNPs and instead must restrict the number of tag SNPs used in their studies. In this paper, we examine several haplotype block diversity evaluation functions and propose dynamic programming algorithms for haplotype block partitioning with using the limited number of tag SNPs. We implement these algorithms and analyze the chromosome 21 haplotype data given by Patil et al. [18]. When the sample is partitioned into blocks fully, we identify a total of 2, 266 blocks and 3, 260 tag SNPs which is smaller than those identified by Zhang et al. [24]. We demonstrate that Zhang’s algorithm does not find the optimal solution due to ignoring the non-monotonic property of common haplotype evaluation function. The algorithms described have been implemented in the web-based system as the analysis tools for <b>bioinformaticists</b> and ∗ This work is supported by grants from the Taichun...|$|R
40|$|Chromosomal {{crossover}} is {{a biological}} mechanism to combine parental traits. It {{is perhaps the}} first mechanism ever taught in any introductory biology class. The formulation of crossover, and resulting recombination, came about 100 years after Mendel's famous experiments. To a great extent, this formulation {{is consistent with the}} basic genetic findings of Mendel. More importantly, it provides a mathematical insight for his two laws (and corrects them). From a mathematical perspective, and while it retains similarities, genetic recombination guarantees diversity so that we do not rapidly converge to the same being. It is this diversity that made the study of biology possible. In particular, the problem of genetic mapping and linkage-one of the first efforts towards a computational approach to biology-relies heavily on the mathematical foundation of crossover and recombination. Nevertheless, as students we often overlook the mathematics of these phenomena. Emphasizing the mathematical aspect of Mendel's laws through crossover and recombination will prepare the students to make an early realization that biology, in addition to being experimental, IS a computational science. This can serve as a first step towards a broader curricular transformation in teaching biological sciences. I will show that a simple and modern treatment of Mendel's laws using a Markov chain will make this step possible, and it will only require basic college-level probability and calculus. My personal teaching experience confirms that students WANT to know Markov chains because they hear about them from <b>bioinformaticists</b> all the time. This entire exposition is based on three homework problems that I designed for a course in computational biology. A typical reader is, therefore, an instructional staff member or a student in a computational field (e. g., computer science, mathematics, statistics, computational biology, bioinformatics). However, other students may easily follow by omitting the mathematically more elaborate parts. I kept those as separate sections in the exposition...|$|R
40|$|Strategies for {{discovering}} common molecular events among disparate diseases hold {{promise for}} improving understanding of disease etiology and expanding treatment options. One technique is to leverage curated datasets {{found in the}} public domain. The Comparative Toxicogenomics Database (CTD; [URL] manually curates chemical-gene, chemical-disease, and gene-disease interactions from the scientific literature. The use of official gene symbols in CTD interactions enables this information to be combined with the Gene Ontology (GO) file from NCBI Gene. By integrating these GO-gene annotations with CTD's gene-disease dataset, we produce 753, 000 inferences between 15, 700 GO terms and 4, 200 diseases, providing opportunities to explore presumptive molecular underpinnings of diseases and identify biological similarities. Through a variety of applications, we demonstrate the utility of this novel resource. As a proof-of-concept, we first analyze known repositioned drugs (e. g., raloxifene and sildenafil) and see that their target diseases have {{a greater degree of}} similarity when comparing GO terms vs. genes. Next, a computational analysis predicts seemingly non-intuitive diseases (e. g., stomach ulcers and atherosclerosis) as being similar to bipolar disorder, and these are validated in the literature as reported co-diseases. Additionally, we leverage other CTD content to develop testable hypotheses about thalidomide-gene networks to treat seemingly disparate diseases. Finally, we illustrate how CTD tools can rank a series of drugs as potential candidates for repositioning against B-cell chronic lymphocytic leukemia and predict cisplatin and the small molecule inhibitor JQ 1 as lead compounds. The CTD dataset is freely available for users to navigate pathologies within the context of extensive biological processes, molecular functions, and cellular components conferred by GO. This inference set should aid researchers, <b>bioinformaticists,</b> and pharmaceutical drug makers in finding commonalities in disease mechanisms, which in turn could help identify new therapeutics, new indications for existing pharmaceuticals, potential disease comorbidities, and alerts for side effects...|$|R
40|$|Microbial Informatics and Experimentation (MIE) is {{a journal}} about {{computers}} and microbes. We {{created a new}} journal {{to fill a gap}} {{for which there is no}} publication avenue that is particularly geared to computationally-oriented, strongly biologically motivated, pragmatic articles focused on microbes. On the one hand, the bioinformatics journals are generally very computer technical and unlikely to be read by the diverse community of microbiologists. There is also a strong emphasis in bioinformatics literature on human/mammalian systems, though this is a secondary issue. On the other hand, microbial informatics work has appeared in a variety of microbiological publications, but it is seldom a good fit there either, and methods that span diverse microbes have no obvious home. We envisage the readership being practising microbiologists who are interested in computational insights into their data, and <b>bioinformaticists</b> interested in the microbial species and systems that sustain life on our fragile planet, but that also can make us very ill. The 1918 Influenza (Spanish flu) had an estimated death toll of 50 - 100 million [1], more than either the first or second World Wars. However, without microbes (starting with the symbionts that became mitochondria and chloroplasts), life as we know it would not exist today. A further dimension is the context in which our work is now taking place: climate change and changing land-use patterns due to the expanding world population, which will increasingly impact all species, starting with microbes. Given the complexity of processes now being studied, computers have joined PCR machines, agar plates and restriction enzymes, as tools for understanding the species and systems we investigate. We see our topics of interest being as broad as the diversity of species we cover. A starting list of topic...|$|R
