2921|2530|Public
5|$|The score {{came under}} {{scrutiny}} from various soundtrack forums soon after being released. Spectral {{analysis of the}} content of the CD revealed certain frequency cutoff patterns around 16kHz, which are typical for lossy codecs. By analyzing the <b>block</b> <b>size</b> of these cutoffs, individuals at Hydrogenaudio were able to identify the lossy codec that was used before mastering the CD as MP3 with a sample rate of 48kHz. According to the aforementioned forums, Varèse's German subsidiary Colosseum Schallplatten acknowledged this as a mastering error, while Varèse Sarabande itself denied this. It is so far unclear if a remastered version with full frequency content will be released.|$|E
25|$|Blowfish's {{use of a}} 64-bit <b>block</b> <b>size</b> (as {{opposed to}} e.g. AES's 128-bit <b>block</b> <b>size)</b> makes it {{vulnerable}} to birthday attacks, particularly in contexts like HTTPS. In 2016, the SWEET32 attack demonstrated how to leverage birthday attacks to perform plaintext recovery (i.e. decrypting ciphertext) against ciphers with a 64-bit <b>block</b> <b>size</b> such as Blowfish.|$|E
25|$|City <b>block</b> <b>size</b> should {{increase}} from the current 1.5-2 to 9–15 ha.|$|E
30|$|In ME and mode decision, {{variable}} <b>block</b> <b>sizes</b> {{can reduce}} the prediction or residual error efficiently. For example, if a MB includes homogeneous regions with no or slow motion, {{it is appropriate to}} select large <b>block</b> <b>sizes</b> such as 16 × 16, since large <b>block</b> <b>sizes</b> can result in sufficiently small residual error. In contrast, large <b>block</b> <b>sizes</b> lead to large prediction error for a MB with fast motion or detailed regions. In this case, smaller <b>block</b> <b>sizes</b> such as 8 × 8, 8 × 4, 4 × 8, and 4 × 4 can be considered as the optimal mode. Accordingly, it is likely to select smaller <b>block</b> <b>sizes</b> when a MB includes detailed regions with fast motion.|$|R
5000|$|VMFS3 limits {{files to}} 262,144 (218) blocks, which {{translates}} to 256 GB for 1 MB <b>block</b> <b>sizes</b> (the default) up to 2 TB for 8 MB <b>block</b> <b>sizes.</b>|$|R
5000|$|Variable block-size motion {{compensation}} (VBSMC) with <b>block</b> <b>sizes</b> {{as large as}} 16×16 and as small as 4×4, enabling precise segmentation of moving regions. The supported luma prediction <b>block</b> <b>sizes</b> include 16×16, 16×8, 8×16, 8×8, 8×4, 4×8, and 4×4, many {{of which can be}} used together in a single macroblock. Chroma prediction <b>block</b> <b>sizes</b> are correspondingly smaller according to the chroma subsampling in use.|$|R
25|$|ZFS uses variable-sized blocks, with 128KB as {{the default}} size. Available {{features}} allow the administrator to tune the maximum <b>block</b> <b>size</b> which is used, as certain workloads do not perform well with large blocks. If data compression is enabled, variable block sizes are used. If a block {{can be compressed}} {{to fit into a}} smaller <b>block</b> <b>size,</b> the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).|$|E
25|$|Blowfish has a 64-bit <b>block</b> <b>size</b> and a {{variable}} key length from 32 bits up to 448 bits. It is a 16-round Feistel cipher and uses large key-dependent S-boxes. In structure it resembles CAST-128, which uses fixed S-boxes.|$|E
25|$|Suppose that {{skin cancer}} {{researchers}} want to test three different sunscreens. They coat two different sunscreens {{on the upper}} sides {{of the hands of}} a test person. After a UV radiation they record the skin irritation in terms of sunburn. The number of treatments is 3 (sunscreens) and the <b>block</b> <b>size</b> is 2 (hands per person).|$|E
30|$|The above {{condition}} {{is the same}} for 8 × 8 <b>block</b> <b>sizes</b> except for the value of the pre-defined threshold. The threshold values for 16 × 16 and 8 × 8 <b>block</b> <b>sizes</b> are 295 and 8, respectively.|$|R
3000|$|... [5 – 16] {{have been}} {{reported}} for H. 264 /AVC to reduce the encoder complexity. The main idea of algorithms is to eliminate unnecessary modes with different <b>block</b> <b>sizes</b> and to perform the RDO process for possible <b>block</b> <b>sizes</b> only. In some approaches [...]...|$|R
50|$|Floppy disks {{generally}} {{only used}} fixed <b>block</b> <b>sizes</b> but these sizes were {{a function of}} the host's OS and its interaction with its controller so that a particular type of media (e.g., 5¼-inch DSDD) would have different <b>block</b> <b>sizes</b> depending upon the host OS and controller.|$|R
25|$|Modern {{hard disk}} drives appear to their host {{controller}} as a contiguous set of logical blocks, and the gross drive capacity is calculated by multiplying {{the number of}} blocks by the <b>block</b> <b>size.</b> This information {{is available from the}} manufacturer's product specification, and from the drive itself through use of operating system functions that invoke low-level drive commands.|$|E
25|$|These {{innovations}} he {{based on}} functional grounds: the <b>block</b> <b>size,</b> {{to enable the}} creation of a quiet interior open space (60 m by 60 m) and allow ample sunlight and ventilation to its perimeter buildings; the rectilinear geometry, the wide streets and boulevards to sustain high mobility and the truncated corners to facilitate turning of carts and coaches and particularly vehicles on fixed rails.|$|E
25|$|In cryptography, Camellia is a {{symmetric}} key block cipher with a <b>block</b> <b>size</b> of 128 bits and key sizes of 128, 192 and 256 bits. It was jointly developed by Mitsubishi Electric and NTT of Japan. The cipher {{has been approved}} {{for use by the}} ISO/IEC, the European Union's NESSIE project and the Japanese CRYPTREC project. The cipher has security levels and processing abilities comparable to the Advanced Encryption Standard.|$|E
50|$|Optical discs {{generally}} {{only use}} fixed <b>block</b> <b>sizes.</b>|$|R
40|$|This paper gives a {{construction}} of group divisible designs on the binary extension fields with <b>block</b> <b>sizes</b> 3, 4, 5, 6, and 7, respectively, which is motivated from the decoding of binary quadratic residue codes. A conjecture is proposed for this construction of group divisible designs with larger <b>block</b> <b>sizes...</b>|$|R
3000|$|Despite this well-considered {{architecture}} and {{the analysis of}} many standards we cannot claim efficient interleaving for all possible permutations. The option of looking-up addresses would offer a 50 %-efficient baseline, but could be excessive in costs when a standard involves large <b>block</b> <b>sizes,</b> or many different <b>block</b> <b>sizes.</b> For a machine with [...]...|$|R
2500|$|Any <b>block</b> <b>size</b> {{could be}} {{specified}} {{up to the}} maximum track length. [...] Since the block headers occupied space, the usable capacity of the drive was dependent on the <b>block</b> <b>size.</b> [...] Blocks ("records" [...] in IBM's terminology) of 88, 96, 880 and 960 were often used because they related to the fixed <b>block</b> <b>size</b> of 80- and 96-character punch cards. The drive capacity was usually stated under conditions of full track record blocking. For example, the 100-megabyte 3336 disk pack only achieved that capacity with a full track <b>block</b> <b>size</b> of 13,030 bytes.|$|E
2500|$|... 32 {{pages of}} 512+16 bytes each for a <b>block</b> <b>size</b> (effective) of 16KB ...|$|E
2500|$|The Big Five studios could {{continue}} to block-book features, but the <b>block</b> <b>size</b> {{would be limited to}} five films; ...|$|E
40|$|In this paper, we are {{interested}} in minimizing the sum of <b>block</b> <b>sizes</b> in a pairwise balanced design, where there are some constraints on the <b>size</b> of one <b>block</b> or the <b>size</b> of the largest block. For every positive integers n,m, where m ≤ n, let S (n,m) be the smallest integer s for which there exists a PBD on n points whose largest <b>block</b> has <b>size</b> m and the sum of its <b>block</b> <b>sizes</b> is equal to s. Also, let S ′(n,m) be the smallest integer s for which there exists a PBD on n points which has a <b>block</b> of <b>size</b> m and the sum of it <b>block</b> <b>sizes</b> is equal to s. We prove some lower bounds for S (n,m) and S ′(n,m). Moreover, we apply these bounds to determine the asymptotic behaviour of the sigma clique partition number of the graph Kn −Km, Cocktail party graphs and complement of paths and cycles...|$|R
5000|$|A seek index with {{compressed}} and uncompressed <b>block</b> <b>sizes</b> {{in pairs}} ...|$|R
30|$|RQ codes offer better coding {{efficiency}} (very {{close to}} ideal codes) compared with R 10, which requires k[*]≥[*] 1000 [13]. This allows {{the use of}} a flexible range of source <b>block</b> <b>sizes,</b> i.e. in practice, using small <b>block</b> <b>sizes</b> is better for devices with limited power and processing capability such as smartphones or tablets.|$|R
2500|$|Its use of B+ tree. [...] With an LMDB {{instance}} {{being in}} shared {{memory and the}} B+ tree <b>block</b> <b>size</b> being set to the OS page size, access to an LMDB store is extremely memory efficient ...|$|E
2500|$|Patrick Geddes {{laid out}} the streets and decided on <b>block</b> <b>size</b> and utilisation. Geddes did not {{prescribe}} an architectural style for the buildings in the new city. But by 1933, many Jewish architects of the Bauhaus school in Germany, like Arieh Sharon, fled to the British Mandate of Palestine. Both the emigration of these Jewish architects and {{the closing of the}} Bauhaus school in Berlin were consequences of the rise to power of the Nazi [...] party in Germany in 1933.|$|E
2500|$|A reduced-round {{variant of}} Blowfish {{is known to}} be {{susceptible}} to known-plaintext attacks on reflectively weak keys. Blowfish implementations use 16 rounds of encryption, and are not susceptible to this attack. Blowfish users are encouraged by Bruce Schneier, Blowfish's creator, to use the more modern and computationally efficient alternative Twofish. He is quoted in 2007 as saying: [...] The FAQ for GnuPG (which features Blowfish as one of its algorithms) recommends that Blowfish should not be used to encrypt files that are larger than 4 Gb because of its small 64-bit <b>block</b> <b>size.</b>|$|E
5000|$|Optimize {{virtual machine}} I/O with {{adjustable}} volume, disk, file and <b>block</b> <b>sizes.</b>|$|R
5000|$|Simon {{supports}} the following combinations of <b>block</b> <b>sizes,</b> key sizes {{and number of}} rounds: ...|$|R
3000|$|... {{block from}} its neighbors [25, 26]. The mode index and the {{transform}} coefficients {{are critical for}} proper frame reconstruction at the decoder. In the case of P and B frames, the H. 264 /AVC standard allows the encoder the flexibility to choose among different reference frames and <b>block</b> <b>sizes</b> for motion prediction. In particular, the standard permits <b>block</b> <b>sizes</b> of [...]...|$|R
2500|$|Data {{is stored}} {{on a hard}} drive {{in a series of}} logical blocks. [...] Each block is delimited by markers {{identifying}} its start and end, error detecting and correcting information, and space between blocks to allow for minor timing variations. [...] These blocks often contained 512 bytes of usable data, but other sizes have been used. [...] As drive density increased, an initiative known as Advanced Format extended the <b>block</b> <b>size</b> to 4096 bytes of usable data, with a resulting significant reduction in the amount of disk space used for block headers, error checking data, and spacing.|$|E
2500|$|Like AAC and Ogg Vorbis, WMA was {{intended}} to address perceived deficiencies in the MP3 standard. Given their common design goals, the three formats ended up making similar design choices. All three are pure transform codecs. Furthermore, the MDCT implementation used in WMA is essentially a superset of those used in Ogg and AAC such that WMA iMDCT and windowing routines {{can be used to}} decode AAC and Ogg Vorbis almost unmodified. However, quantization and stereo coding is handled differently in each codec. The primary distinguishing trait of the WMA Standard format is its unique use of 5 different block sizes, compared to MP3, AAC, and Ogg Vorbis which each restrict files to just two sizes. [...] WMA Pro extends this by adding a 6th <b>block</b> <b>size</b> used at 88.2/96kHz sampling rate.|$|E
2500|$|Permuted-block {{randomization}} or blocked randomization: a [...] "block size" [...] and [...] "allocation ratio" [...] (number {{of subjects}} in one group versus the other group) are specified, and subjects are allocated randomly within each block. For example, a <b>block</b> <b>size</b> of 6 and an allocation ratio of 2:1 {{would lead to}} random assignment of 4 subjects to one group and 2 to the other. This type of randomization can be combined with [...] "stratified randomization", for example by center in a multicenter trial, to [...] "ensure good balance of participant characteristics in each group." [...] A special case of permuted-block randomization is random allocation, in which the entire sample is treated as one block. The major disadvantage of permuted-block randomization is {{that even if the}} block sizes are large and randomly varied, the procedure can lead to selection bias. Another disadvantage is that [...] "proper" [...] analysis of data from permuted-block-randomized RCTs requires stratification by blocks.|$|E
40|$|This paper {{presents}} {{a comparison between}} various High Efficiency Video Coding (HEVC) motion estimation configurations in terms of coding efficiency and memory cost in hardware. An HEVC motion estimation hardware model that is suitable to implement HEVC reference software (HM) search algorithm is created and memory area and data bandwidth requirements are calculated based on this model. 11 different motion estimation configurations are considered. Supporting smaller <b>block</b> <b>sizes</b> is shown to impose significant memory cost in hardware although the coding gain achieved through supporting them is relatively smaller. Hence, depending on target encoder specifications, the decision can be made not to support certain <b>block</b> <b>sizes.</b> Specifically, supporting only 64 x 64, 32 x 32 and 16 x 16 <b>block</b> <b>sizes</b> provide 3. 2 X on-chip memory area, 26 X on-chip bandwidth and 12. 5 X off-chip bandwidth savings at the expense of 12 % bit-rate increase when compared to the anchor configuration supporting all <b>block</b> <b>sizes.</b> Texas Instruments Incorporate...|$|R
50|$|Joan Daemen's 3-Way and BaseKing have unusual <b>block</b> <b>sizes</b> of 96 and 192 bits, respectively.|$|R
5000|$|Adaptive encoder {{selection}} {{between the}} 4×4 and 8×8 transform <b>block</b> <b>sizes</b> for the integer transform operation.|$|R
