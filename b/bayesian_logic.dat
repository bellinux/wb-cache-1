86|53|Public
50|$|One {{powerful}} paradigm {{that has}} been recently used for the representation of hypotheses in inductive programming (generally {{in the form of}} generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and <b>Bayesian</b> <b>logic</b> programming).|$|E
5000|$|It is {{designed}} to run as a co-processor with another controller (such as a CPU or an Tensilica/ARM core). It was developed as the culmination of DARPA's Analog Logic program [...] (although the GP5 chip architecture is digital, it could contemplate an entirely analog implementation of its <b>Bayesian</b> <b>logic</b> instruction set).|$|E
50|$|As a {{logic of}} {{induction}} {{rather than a}} theory of belief, Bayesian inference does not determine which beliefs are a priori rational, but rather determines how we should rationally change the beliefs we have when presented with evidence. We begin by committing to a prior probability for a hypothesis based on logic or previous experience, and when faced with evidence, we adjust {{the strength of our}} belief in that hypothesis in a precise manner using <b>Bayesian</b> <b>logic.</b>|$|E
40|$|Below we will {{consider}} the relations between inductive logic and statistics. More specifically, we will show that some concepts and methods of inductive logic may be applied in the rational reconstruction of several statistical notions and procedures and that, in addition, inductive logic suggests some new methods {{which can be used}} for different kinds of statistical inference. Although there are several approaches to inductive logic and statistics, here we will focus on some versions of the Bayesian approach and, thereby, on the relations between <b>Bayesian</b> inductive <b>logic</b> and <b>Bayesian</b> statistics. The paper is organized as follows. The subjects of inductive logic and statistics will be shortly illustrated in Section 1, where it will be suggested that statistics {{can be seen as a}} special field of inductive logic. Two important theories developed within <b>Bayesian</b> inductive <b>logic</b> are the theory of inductive probabilities, started by Rudolf Carnap in the forties of the past century, and the theory of confirmation: the conceptual relations between such theories and statistics will be considered in Section 2. A recent version of <b>Bayesian</b> inductive <b>logic,</b> proposed by Ilkka Niiniluoto and others, has been developed by using the notion of verisimilitude, introduced in philosophy of science by Karl Popper; the key ideas of the verisimilitudinarian version of <b>Bayesian</b> inductive <b>logic</b> will be illustrated in Section 3, where it will be argued that it provides useful conceptual tools for the analysis of some important kinds of statistical inference...|$|R
5000|$|Horacio Arló-Costa, Carnegie Mellon, Philosophy (<b>Bayesian</b> epistemology, epistemic <b>logic,</b> belief revision, conditionals, {{rational}} choice, normative {{and behavioral}} decision theory) ...|$|R
5000|$|International Patent (Publication Number WO 03/090466) for Improved TV Programme Selection (based on <b>Bayesian</b> Networks, Fuzzy <b>Logic</b> and an {{original}} approach to TV programme classification).|$|R
50|$|Stratocladistics is {{a method}} of making phylogenetic inferences using both {{geological}} and morphobiological data. It follows {{many of the same}} rules as cladistics, using <b>Bayesian</b> <b>logic</b> to quantify how good a phylogenetic hypothesis is in terms of debt and parsimony. However, in addition to the morphological debt that is used to determine phylogenetic dissimilarities in cladistics, there is also stratigraphic debt which adds the dimension of time to the equation.Although stratocladistics has been viewed with suspicion by some workers, it represents a total evidence approach that has some advantages over traditional cladistic approaches.|$|E
40|$|First order {{probabilistic}} logics {{combine a}} {{first order logic}} with a probabilistic knowledge representation. In this context, we introduce continuous <b>Bayesian</b> <b>logic</b> programs, which extend the recently introduced <b>Bayesian</b> <b>logic</b> programs to deal with continuous random variables. <b>Bayesian</b> <b>logic</b> programs tightly integrate definite logic programs with Bayesian networks. The resulting framework nicely seperates the qualitative (i. e. logical) component from the quantitative (i. e. the probabilistic) one. We also show how the quantitative component can be learned using a gradient-based maximum likelihood method...|$|E
40|$|<b>Bayesian</b> <b>logic</b> {{provides}} a rational model of probability judgments deviating {{from the standard}} extensional norm of extensional probability. It formalizes the general idea of an inductive pattern logic that may resolve paradoxes of inclusion. <b>Bayesian</b> <b>logic</b> predicts {{that it should be}} possible to generalize the phenomenon of frequency-based logical conjunction fallacies to a system of logical inclusion fallacies. In Experiment 1 quantitative conditions for conjunction fallacies and the role of negations are investigated. Experiment 2 {{provides a}} first test of the postulated more general system of logical inclusion fallacies. The results of both experiments confirmed the proposed pattern logic and its formalization as <b>Bayesian</b> <b>logic.</b> Other theories of the conjunction fallacy cannot readily explain this class of frequency-based and pattern-based inclusion fallacies. Whether there are simpler heuristics that may perhaps explain these data as well should be investigated in the future...|$|E
25|$|Intelligent control uses various AI {{computing}} approaches like neural networks, <b>Bayesian</b> probability, fuzzy <b>logic,</b> machine learning, {{evolutionary computation}} and genetic algorithms {{to control a}} dynamic system.|$|R
50|$|Intelligent {{control is}} a class of control {{techniques}} that use various artificial intelligence computing approaches like neural networks, <b>Bayesian</b> probability, fuzzy <b>logic,</b> machine learning, evolutionary computation and genetic algorithms.|$|R
50|$|Tools and {{techniques}} of automated reasoning include the classical logics and calculi, fuzzy <b>logic,</b> <b>Bayesian</b> inference, reasoning with maximal entropy {{and a large}} number of less formal ad hoc techniques.|$|R
40|$|Bayesian {{networks}} {{provide an}} elegant formalism for representing and reasoning about uncertainty using probability theory. They are a probabilistic extension of propositional logic and, hence, inherit {{some of the}} limitations of propositional logic, such as the difficulties to represent objects and relations. We introduce a generalization of Bayesian networks, called <b>Bayesian</b> <b>logic</b> programs, to overcome these limitations. In order to represent objects and relations it combines Bayesian networks with definite clause logic by establishing a one-to-one mapping between ground atoms and random variables. We show that <b>Bayesian</b> <b>logic</b> programs combine the advantages of both definite clause logic and Bayesian networks. This includes the separation of quantitative and qualitative aspects of the model. Furthermore, <b>Bayesian</b> <b>logic</b> programs generalize both Bayesian networks as well as logic programs. So, many ideas developed in both areas carry over. nrpages: 52 status: publishe...|$|E
40|$|Abstract We survey three {{applications}} of mathematical programming to reasoning under uncertainty: a) {{an application of}} linear programming to probabilistic logic; b) an application of nonlinear programming to <b>Bayesian</b> <b>logic,</b> a combination of Bayesian inference with probabilistic logic; and c) an application of integer programming to Dempster-Shafer theory, which is a method of combining evidence from different sources. 1 Introduction In recent years the methods of mathematical programming have been applied to reasoning under uncertainty. We will present the basic ideas behind three of these applications: that of linear programming to probabilistic logic, that of nonlinear programming to <b>Bayesian</b> <b>logic,</b> and that of integer programming to Dempster-Shafer theory. A mathematical programming approach not only provides a practical means of computing inferences, as in probabilistic logic and Dempster-Shafer theory, but it can suggest new types of logic for dealing with uncertainty, {{as in the case}} of <b>Bayesian</b> <b>logic...</b>|$|E
40|$|Various {{proposals}} for combining {{first order logic}} with Bayesian nets exist. Many of these {{are based on the}} so-called knowledge-based model construction method, e. g. probabilistic logic programs by Ngo and Haddawy, relational Bayesian nets by Jaeger and the more recent probabilistic relational models by Koller et. al. Upon a first investigation these frameworks seem different {{despite the fact that they}} attack essentially the same problem. The relationship among these approaches has so far not been studied. The main contribution of this paper is, that we clarify the relation among the three existing frameworks. This is achieved through the introduction of <b>Bayesian</b> <b>logic</b> programs, which serve as a common kernel to these first order Bayesian net approaches. <b>Bayesian</b> <b>logic</b> programs are not really new; they are basically a simplification and reformulation of Ngo's and Haddawy's probabilistic logic programs. However, <b>Bayesian</b> <b>logic</b> programs are sufficiently powerful to represent essent [...] ...|$|E
40|$|Abstract. We present <b>Bayesian</b> Description <b>Logics</b> (BDLs) : an exten-sion of Description Logics (DLs) with contextual probabilities {{encoded in}} a Bayesian network (BN). Classical DL {{reasoning}} tasks are extended to consider also the contextual and probabilistic information in BDLs. A complexity {{analysis of these}} problems shows that, for propositionally closed DLs, this extension comes without cost, while for tractable DLs the complexity {{is affected by the}} cost of reasoning in the BN. ...|$|R
50|$|Lataster {{identifies}} {{his main}} research interests as including philosophy of religion, Christian origins, <b>logic,</b> <b>Bayesian</b> reasoning, sustainability, and alternative god-concepts such as pantheism and pandeism. Lataster's PhD thesis will analyse arguments {{for the existence}} of God by theologians including William Lane Craig and Richard Swinburne.|$|R
40|$|Abstract:- This paper {{presents}} a consolidated {{view of the}} computational intelligence used in the natural language steganalysis. In {{order to understand the}} human intelligence on natural language, four major computational intelligence methods have been identified. They are <b>bayesian,</b> fuzzy <b>logic,</b> neural network, and genetic algorithm. This paper also {{presents a}} measurement tool to measure the natural language intelligent system properties based on steganalysis objectives. It can be learned that the more suitable intelligent systems to be applied in steganalysis domain properties are: neural network, genetic algorithm and fuzzy logic...|$|R
40|$|Abstract. Several models {{combining}} Bayesian networks with logic exist. The {{two most}} developed models are Probabilistic Relational Models (PRM's) and <b>Bayesian</b> <b>Logic</b> Programs (BLP's). While PRM's {{are easier to}} understand, BLP's are more expressive. However, we argue that BLP's do not always allow modeling problems intuitively. This motivates us to introduce Logical Bayesian Networks (LBN's). We argue that LBN's provide an expressive and intuitive modeling language due to explicitly distinguishing deterministic and probabilistic information and having multiple components. We briefly discuss perspectives for learning LBN's from data. Keywords: probabilistic-logical models, <b>Bayesian</b> <b>Logic</b> Programs, Bayesian network...|$|E
40|$|<b>Bayesian</b> <b>logic</b> {{programs}} tightly integrate definite logic {{programs with}} Bayesian networks {{in order to}} [...] . In this paper, we present results on combining Inductive Logic Programming with Bayesian networks to learn both the qualitative and the quantitative components of <b>Bayesian</b> <b>logic</b> programs from data. More precisely, we show how the qualitative components can be learned by combining the inductive logic programming setting learning from interpretations with score-based techniques for learning Bayesian networks. The estimation of the quantitative components is reduced to the corresponding problem of (dynamic) Bayesian network...|$|E
40|$|Various {{proposals}} for combining {{first order logic}} with Bayesian nets exist. We introduce the formalism of <b>Bayesian</b> <b>logic</b> programs, which is basically a simplification and reformulation of Ngo and Haddawys probabilistic logic programs. However, <b>Bayesian</b> <b>logic</b> programs are sufficiently powerful to represent essentially the same knowledge in a more elegant manner. The elegance {{is illustrated by the}} fact that they can represent both Bayesian nets and definite clause programs (as in "pure " Prolog) and that their kernel in Prolog is actually an adaptation of an usual Prolog meta-interpreter...|$|E
30|$|Knowledge-based {{approach}} uses {{various types}} of knowledge to predict user needs or suitable moment to push recommendations. The knowledge could be a set of logical rules (association rules/decision tree) assigned by experts or a set of previously stored data (cases/experiences) using machine learning techniques such as <b>Bayesian</b> network, fuzzy <b>logic,</b> etc.|$|R
40|$|C. Howson’s {{probabilistic}} logic as {{a comprehensive}} methodological account of scientific inference, which avoids Hume’s inductive skepticism, is discussed {{against the background}} of the latter’s quantitative theory of money. Hume’s theory leads to two causal accounts that may appear to be contradictory. As the more general one suggests neutrality of money, while the more descriptive attributes causal influence to specie-flow mechanism of money. The former is grounded by a counterfactual reasoning. The discussion of recent examples of bayesian counterfactual models leads to the conclusion that despite the possibility of a uniform account of Hume’s theory of money, it seems beyond the scope of the <b>Bayesian</b> probabilistic <b>logic</b> offered by Howson...|$|R
40|$|Abstract. The <b>Bayesian</b> Description <b>Logic</b> (BDL) BEL is a {{probabilistic}} DL, {{which extends}} the lightweight DL EL by defining a joint probability distribution over EL axioms {{with the help}} of a Bayesian network (BN). In the recent work, extensions of standard logical reasoning tasks in BEL are shown to be reducible to inferences in BNs. This work concentrates on a more general reasoning task, namely on conjunctive query answering in BEL where every query is associated to a probability leading to different reasoning problems. In particular, we study the probabilistic query entailment, top-k answers, and top-k con-texts as reasoning problems. Our complexity analysis suggests that all of these problems are tractable under certain assumptions. ...|$|R
40|$|The <b>Bayesian</b> <b>logic</b> is {{generally}} associated {{to the definition}} of a prior probabilistic law. Conditional algebra have been investigated by some authors though, but somehow the background framework is still probabilistic and the entire logic is not specified. In this paper, the definition of a Deterministic <b>Bayesian</b> <b>Logic</b> is proposed. This logic is completely independent of any notion of probability. The coherence of this logic is proven and various logical theorems are derived. It is shown that this logic is probabilizable and avoids the negative result of Lewis. At last the probabilistic Bayesian rule is recovered by posteriorly probabilizing our logic...|$|E
40|$|Recently, new {{representation}} languages that integrate {{first order}} logic with Bayesian networks have been developed. <b>Bayesian</b> <b>logic</b> programs are one of these languages. In this paper, we present results on combining Inductive Logic Programming (ILP) with Bayesian networks to learn both the qualitative and the quantitative components of <b>Bayesian</b> <b>logic</b> programs. More precisely, we show how to combine the ILP setting learning from interpretations with score-based techniques for learning Bayesian networks. Thus, the paper positively answers Koller and Pfeffer's question, whether techniques from ILP could help to learn the logical component of first order probabilistic models...|$|E
40|$|We survey three {{applications}} of mathematical programming to rea soning under uncertainty a {{an application of}} linear programming to probabilistic logic b an application of nonlinear programming to <b>Bayesian</b> <b>logic</b> a combination of Bayesian inference with probabilistic logic and c an application of integer programming to Dempster Shafer theory which is a method of combining evidence from dierent sources In recent years the methods of mathematical programming have been applied to reasoning under uncertainty We will present the basic ideas behind three of these applications that of linear programming to probabilistic logic that of nonlinear programming to <b>Bayesian</b> <b>logic</b> and that of integer progra...|$|E
40|$|Abstract- Uncertainty is a {{state of}} lack of certainty, where having {{incomplete}} knowledge can make it impossible to describe the outcome. There are many challenges that occur due to uncertainty. Uncertainty in events can cause losses of data, providing inaccurate data, noisy output. Thus the need to handle uncertainty is essential. This paper represents an outline of various methods through which the uncertainty can be analyzed and handled. Index Terms- <b>Bayesian</b> Network, fuzzy <b>logic,</b> probability, uncertainty U I...|$|R
5000|$|Edwin T. Jaynes {{proposed}} that probability {{could be considered}} as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science he developed this theory and proposed what he called “the robot,” which was nota physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of <b>logic.</b> <b>Bayesian</b> programming is a formal and concrete implementation of this [...] "robot".|$|R
5000|$|Systems such as Pei Wangs Non-Axiomatic Reasoning System (NARS) or Ben Goertzels Probabilistic Logic Networks (PLN) add an {{explicit}} confidence ranking, {{as well as}} a probability to atoms and sentences. The rules of deduction and induction incorporate this uncertainty, thus side-stepping difficulties in purely <b>Bayesian</b> approaches to <b>logic</b> (including Markov logic), while also avoiding the paradoxes of Dempster-Shafer theory. The implementation of PLN attempts to use and generalize algorithms from logic programming, subject to these extensions.|$|R
40|$|We {{describe}} how to combine probabilistic logic and Bayesian networks {{to obtain a}} new frame-work (2 ̆ 2 <b>Bayesian</b> <b>logic</b> 2 ̆ 2) for dealing with uncertainty and causal relationships in an expert system. Probabilistic logic, invented by Boole, is a technique for drawing inferences from uncertain propositions for {{which there are no}} independence assumptions. A Bayesian network is a 2 ̆ 2 belief net 2 ̆ 2 that can represent complex conditional independence assumptions. We show how to solve inference problems in <b>Bayesian</b> <b>logic</b> by applying Benders decomposition to a nonlinear programming formulation. We also show that the number of constraints grows only linearly with the problem size for a large class of networks...|$|E
40|$|Abstract. Across a {{wide range}} of domains, there is an urgent need for a wellfounded {{approach}} to incorporating uncertain and incomplete knowledge into formal domain ontologies. Although this subject is receiving increasing attention from ontology researchers, there is as yet no broad consensus on the definition of a probabilistic ontology and on the most suitable approach to extending current ontology languages to support uncertainty. This paper presents two contributions to developing a coherent framework for probabilistic ontologies: (1) a formal definition of a probabilistic ontology, and (2) an extension of the OWL Web Ontology Language that is consistent with our formal definition. This extension, PR-OWL, is based on Multi-Entity Bayesian Networks (MEBN), a first-order <b>Bayesian</b> <b>logic</b> that unifies Bayesian probability with First-Order Logic. As such, PR-OWL combines the full representation power of OWL with the flexibility and inferential power of <b>Bayesian</b> <b>logic...</b>|$|E
40|$|We survey three {{applications}} of mathematical programming to reasoning under uncertainty: a) {{an application of}} linear programming to probabilistic logic, b) an application of nonlinear programming to <b>Bayesian</b> <b>logic,</b> a combination of Bayesian inference with probabilistic logic and c) an application of integer programming to Dempster-Shafer theory, which is a method of combining evidence from diffierent source...|$|E
40|$|We {{present a}} {{language}} for representing context-sensitive temporal probabilistic knowledge. Context constraints allow inference {{to be focused}} on only the relevant portions of the probabilistic knowledge. We provide a declarative semantics for our language. We present a sound and complete algorithm for computing posterior probabilities of temporal queries, as well as an efficient implementation of the algorithm. Throughout we illustrate the approach with the problem of reasoning about the effects of medications and interventions on the state of a patient in cardiac arrest. We empirically evaluate the efficiency of our system by comparing its inference times on problems in this domain with those of standard Bayesian network representations of the problems. Keywords: Temporal probability models, <b>Bayesian</b> networks, <b>Logic</b> programming, Prognostic evaluation 1 Introduction For accurate medical diagnosis and prediction, it is often necessary to model a patient's condition over time. Because th [...] ...|$|R
40|$|Incidence Calculus [Bun 85] and Fagin-Halpern Structures [FH 89] are two {{alternative}} {{frameworks for}} uncertain reasoning in knowledgebased systems. In this paper {{we present a}} comparative review of these two frameworks, thus completing the series of papers started with [CdSB 90, CdS 93]. Keywords: representation languages for uncertain knowledge. 1 Introduction Incidence Calculus [Bun 85] was introduced in 1985 by Bundy as a framework to represent uncertainty in knowledge-based systems. It was presented {{as an extension of}} Nilsson's (<b>bayesian)</b> Probability <b>Logic</b> [Nil 86], which was in turn a reconstruction of Boole's work on logic and probabilities [Boo 54] (see also [Hai 86]). Incidence Calculus extends Nilsson's Probability Logic by relaxing the conditions to assign propositions to possible worlds, thus permitting that states of incomplete knowledge (of a certain form) be considered. The Dempster-Shafer Theory of Evidence [Sha 76] was also proposed as an extension of bayesian probabilities to [...] ...|$|R
40|$|Inductive logic admits {{a variety}} of {{semantics}} (Haenni et al. (2011) [7, Part 1]). This paper develops semantics based on the norms of Bayesian epistemology (Williamson, 2010 [16, Chapter 7]). Section 1 introduces the semantics and then, in Section 2, the paper explores methods for drawing inferences in the resulting logic and compares the methods of this paper with the methods of Barnett and Paris (2008) [2]. Section 3 then evaluates this <b>Bayesian</b> inductive <b>logic</b> {{in the light of}} four traditional critiques of inductive logic, arguing (i) that it is language independent in a key sense, (ii) that it admits connections with the Principle of Indifference but these connections do not lead to paradox, (iii) that it can capture the phenomenon of learning from experience, and (iv) that while the logic advocates scepticism with regard to some universal hypotheses, such scepticism is not problematic {{from the point of view}} of scientific theorising...|$|R
