943|628|Public
25|$|So {{the rule}} {{says that the}} {{posterior}} odds are the prior odds times the <b>Bayes</b> <b>factor,</b> or in other words, posterior is proportional to prior times likelihood.|$|E
25|$|One {{should also}} keep {{the purpose of}} the {{analysis}} in mind when choosing the prior distribution. In principle, uninformative and flat priors, that exaggerate our subjective ignorance about the parameters, may still yield reasonable parameter estimates. However, Bayes factors are highly sensitive to the prior distribution of parameters. Conclusions on model choice based on <b>Bayes</b> <b>factor</b> can be misleading unless the sensitivity of conclusions to the choice of priors is carefully considered.|$|E
2500|$|If {{the model}} priors are equal (...) , the <b>Bayes</b> <b>factor</b> equals the {{posterior}} ratio.|$|E
40|$|This article {{examines}} the consistency, interpretation and application of <b>Bayes</b> <b>factors</b> constructed from standard test statistics. Primary conclusions are that <b>Bayes</b> <b>factors</b> based on multinomial and normal test statistics are consistent for suitable choices of the hyperparameters used to specify alternative hypotheses, and that such constructions can be extended to obtain consistent <b>Bayes</b> <b>factors</b> based on likelihood ratio statistics. A connection between <b>Bayes</b> <b>factors</b> based on likelihood ratio statistics and the Bayesian information criterion is exposed, as is a connection between <b>Bayes</b> <b>factors</b> based on "F" statistics and parametric <b>Bayes</b> <b>factors</b> based on normal-inverse gamma models. Similarly, <b>Bayes</b> <b>factors</b> based on chi-squared statistics for multinomial data are shown to provide accurate approximations to <b>Bayes</b> <b>factors</b> based on multinomial/Dirichlet models. An illustration of how the simple form of these <b>Bayes</b> <b>factors</b> can be exploited to generate easily interpretable summaries of the experimental 'weight of evidence' is provided. Copyright (c) Board of the Foundation of the Scandinavian Journal of Statistics 2007. ...|$|R
40|$|Traditionally, {{the use of}} <b>Bayes</b> <b>factors</b> has {{required}} the specification of proper prior distributions on model parameters implicit to both null and alternative hypotheses. In this paper, I describe an approach to defining <b>Bayes</b> <b>factors</b> based on modeling test statistics. Because the distributions of test statistics do not depend on unknown model parameters, this approach eliminates the subjectivity normally associated with the definition of <b>Bayes</b> <b>factors.</b> For standard test statistics, including the _ 2, F, t and z statistics, the values of <b>Bayes</b> <b>factors</b> that result from this approach can be simply expressed in closed form. ...|$|R
40|$|The MCMC {{approach}} to calculating approximate <b>Bayes</b> <b>factors</b> is considered. The calculation, {{consisting of a}} log-likelihood, a prior, and a posterior, presents an excellent opportunity to observe directly the effects of priors on <b>Bayes</b> <b>factors.</b> Three empirical examples demonstrate that <b>Bayes</b> <b>factors</b> are sensitive {{to a combination of}} the prior variance and the difference in the number of parameters between the rival models. a I thank Susan Murphy for helpful discussions and Paul Huth, Christopher Gelpi, D...|$|R
2500|$|... {{is called}} the <b>Bayes</b> <b>factor</b> or {{likelihood}} ratio and the odds between two events is simply {{the ratio of the}} probabilities of the two events. Thus ...|$|E
2500|$|Initially, the car {{is equally}} likely to be behind {{any of the three}} doors: the odds on door 1, door 2, and door 3 are [...] This remains the case after the player has chosen door 1, by independence. According to Bayes' rule, the {{posterior}} odds on the location of the car, given that the host opens door 3, are equal to the prior odds multiplied by the <b>Bayes</b> <b>factor</b> or likelihood, which is, by definition, the probability of the new piece of information (host opens door 3) under each of the hypotheses considered (location of the car). Now, since the player initially chose door 1, the chance that the host opens door 3 is 50% if {{the car is}} behind door 1, 100% if the car is behind door 2, 0% if the car is behind door 3. Thus the <b>Bayes</b> <b>factor</b> consists of the ratios [...] or equivalently , while the prior odds were [...] Thus, the posterior odds become equal to the <b>Bayes</b> <b>factor</b> [...] Given that the host opened door 3, the probability that the car is behind door 3 is zero, and it is twice as likely to be behind door 2 than door 1.|$|E
2500|$|It {{has been}} shown that the {{combination}} of insufficient summary statistics and ABC for model selection can be problematic. Indeed, if one lets the <b>Bayes</b> <b>factor</b> based on the summary statistic [...] be denoted by , the relation between [...] and [...] takes the form: ...|$|E
40|$|This paper {{considers}} point {{null hypothesis}} testing when the sampling distribution {{belongs to a}} particular class, defined in Gleser & Hwang (1987). We discuss the drawbacks of frequentist and likelihood solutions and we show how proper Bayesian analysis encounters relatively similar difficulties. We explore the performance of several noninformative Bayesian approaches to testing, namely asymptotic approximations of <b>Bayes</b> <b>factors</b> and default <b>Bayes</b> <b>factors.</b> We argue that in a default Bayesian analysis of Fieller's problem {{the choice of the}} 'correct' prior distribution is crucial. Although standard and default <b>Bayes</b> <b>factors</b> based on Jeffreys' priors show, to a lesser extent, pathologies similar to those arising in a classical framework, default <b>Bayes</b> <b>factors</b> based on reference priors seem to correct the bias and provide sensible results in term of robustness and consistency...|$|R
40|$|We {{consider}} {{the problem of}} model choice for stochastic epidemic models given partial observation of a disease outbreak through time. Our main {{focus is on the}} use of <b>Bayes</b> <b>factors.</b> Although <b>Bayes</b> <b>factors</b> have appeared in the epidemic modelling literature before, they can be hard to compute and little attention has been given to fundamental questions concerning their utility. In this paper we derive analytic expressions for <b>Bayes</b> <b>factors</b> given complete observation through time, which suggest practical guidelines for model choice problems. We extend the power posterior method for computing <b>Bayes</b> <b>factors</b> so as to account for missing data and apply this approach to partially observed epidemics. For comparison, we also explore the use of a deviance information criterion for missing data scenarios. The methods are illustrated via examples involving both simulated and real data...|$|R
30|$|Second, by {{building}} upon Bayesian statistics, the significance (or decisiveness) of results in our approach {{is determined by}} <b>Bayes</b> <b>factors,</b> a Bayesian alternative to traditional p-value testing. Instead of just measuring evidence against one null hypothesis, <b>Bayes</b> <b>Factors</b> allow to directly gather evidence {{in favor of a}} hypothesis compared to another hypothesis, which is arguably more suitable for ranking.|$|R
2500|$|An {{alternative}} {{framework for}} statistical hypothesis testing is to specify {{a set of}} statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model. The most common selection techniques are based on either Akaike information criterion or <b>Bayes</b> <b>factor.</b>|$|E
2500|$|Once the {{posterior}} probabilities of {{models have been}} estimated, one can make full use of the techniques of Bayesian model comparison. For instance, to compare the relative plausibilities of two models [...] and , one can compute their posterior ratio, which {{is related to the}} <b>Bayes</b> <b>factor</b> : ...|$|E
2500|$|..... {{suppose that}} there are [...] objects that might be seen at any moment, of which [...] are ravens and [...] are black, and that the [...] objects each have {{probability}} [...] of being seen. Let [...] be the hypothesis {{that there are}} [...] non-black ravens, and suppose that the hypotheses [...] are initially equiprobable. Then, if we happen to see a black raven, the <b>Bayes</b> <b>factor</b> in favour of [...] is ...|$|E
50|$|<b>Bayes</b> <b>factors</b> {{are used}} to {{compensate}} for variables that cannot be calculated through conventional statistics; in this case, the variable created by the visual clues that Demkina might gather from observing a subject. The <b>Bayes</b> <b>factors</b> used by Hyman were calculated by professors Persi Diaconis and Susan Holmes of the Department of Statistics at Stanford University.|$|R
40|$|Hoijtink, van Kooten, and Hulsker (2016) outline a {{research}} agenda for Bayesian psychologists: evaluate {{and use the}} frequency properties of <b>Bayes</b> <b>factors.</b> Morey, Wagenmakers, and Rouder (2016) respond that <b>Bayes</b> <b>factors</b> calibrated using frequency properties should not be used. This paper contains the response of Hoijtink, van Kooten, and Hulsker to the criticism of Morey, Wagenmakers, and Rouder (2016) ...|$|R
2500|$|The {{computation}} of <b>Bayes</b> <b>factors</b> on [...] {{may therefore}} be misleading for model selection purposes, unless the ratio between the <b>Bayes</b> <b>factors</b> on [...] and [...] would be available, {{or at least}} could be approximated reasonably well. Alternatively, necessary and sufficient conditions on summary statistics for a consistent Bayesian model choice have recently been derived, which can provide useful guidance.|$|R
2500|$|Good's {{argument}} involves {{calculating the}} weight of evidence provided by the observation of a black raven or a white shoe {{in favor of the}} hypothesis that all the ravens in a collection of objects are black. The weight of evidence is the logarithm of the <b>Bayes</b> <b>factor,</b> which in this case is simply the factor by which the odds of the hypothesis changes when the observation is made. The argument goes as follows: ...|$|E
2500|$|It {{can be more}} {{intuitive}} {{to present}} this argument using Bayes' rule rather than Bayes' theorem. Having seen a black face we can rule out the white card. We {{are interested in the}} probability that the card is black given a black face is showing. Initially it is equally likely that the card is black and that it is mixed: the prior odds are 1:1. Given that it is black we are certain to see a black face, but given that it is mixed we are only 50% certain to see a black face. The ratio of these probabilities, called the likelihood ratio or <b>Bayes</b> <b>factor,</b> is 2:1. Bayes' rule says [...] "posterior odds equals prior odds times likelihood ratio". Since the prior odds are 1:1 the posterior odds equals the likelihood ratio, 2:1. It is now twice as likely that the card is black than that it is mixed.|$|E
2500|$|Richard Gill (2011) {{analyzes}} the likelihood for the host to open door 3 as follows. Given {{that the car}} is not behind door 1, it is equally likely that it is behind door 2 or 3. Therefore, the chance that the host opens door 3 is 50%. Given that the car is behind door 1, the chance that the host opens door 3 is also 50%, because, when the host has a choice, either choice is equally likely. Therefore, {{whether or not the}} car is behind door 1, the chance that the host opens door 3 is 50%. The information [...] "host opens door 3" [...] contributes a <b>Bayes</b> <b>factor</b> or likelihood ratio of , on whether or not the car is behind door 1. Initially, the odds against door 1 hiding the car were [...] Therefore, the posterior odds against door 1 hiding the car remain the same as the prior odds, [...]|$|E
40|$|Bayesian model {{selection}} poses {{two main}} challenges: the specification of parameter priors for all models, and the computation {{of the resulting}} <b>Bayes</b> <b>factors</b> between models. There is now a large literature on auto-matic and objective parameter priors in the linear model, which unburden the statistician from eliciting them manually {{in the absence of}} substantive prior information. One important class are g-priors, which were recently extended from linear to generalized linear models (GLMs). We show that the resulting <b>Bayes</b> <b>factors</b> can be approximated by test-based <b>Bayes</b> <b>factors</b> (Johnson, 2008) using the deviance statistics of the models. To estimate the hyperparameter g, we propose empirical and fully Bayes approaches and link the former to minimum <b>Bayes</b> <b>factors</b> and shrinkage estimates from the literature. Furthermore, we describe how to approximate the corres-ponding posterior distribution of the regression coefficients based on the standard output from a maximum likelihood analysis of the GLM con-sidered. We extend the proposed methodology to the Cox proportiona...|$|R
40|$|Abstract. â€”As larger, {{more complex}} data sets {{are being used}} to infer phylogenies, {{accuracy}} of these phylogenies increasingly requires models of evolution that accommodate heterogeneity in the processes of molecular evolution. We investigated the effect of improper data partitioning on phylogenetic accuracy, as well as the type I error rate and sensitivity of <b>Bayes</b> <b>factors,</b> a commonly used method for choosing among different partitioning strategies in Bayesian analyses. We also used <b>Bayes</b> <b>factors</b> to test empirical data for the need to divide data in a manner that has no expected biological meaning. Posterior probability estimates are misleading when an incorrect partitioning strategy is assumed. The error was greatest when the assumed model was underpartitioned. These results suggest that model partitioning is important for large data sets. <b>Bayes</b> <b>factors</b> performed well, giving a 5 % type I error rate, which is remarkably consistent with standard frequentist hypothesis tests. The sensitivity of <b>Bayes</b> <b>factors</b> was found to be quite high when the across-class model heterogeneity reflected that of empirical data. These results suggest that <b>Bayes</b> <b>factors</b> represent a robust method of choosing among partitioning strategies. Lastly, results of tests for the inclusion of unexpected divisions in empirical data mirrored the simulation results, although the outcome of such tests is highly dependent on accounting for rate variation among classes. We conclude by discussing other approaches for partitioning data, as well as other applications of <b>Bayes</b> <b>factors.</b> [Bayes factors; Bayesian phylogenetic inference; data partitioning; model choice; posterior probabilities. ] Maximum likelihood (ML) and Bayesian methods o...|$|R
40|$|This paper {{deals with}} the {{variable}} selection problem in linear regression models and its solution by means of <b>Bayes</b> <b>factors.</b> If substantive prior information is lacking or impractical to elicit, which {{is often the case}} in applications, objective <b>Bayes</b> <b>factors</b> come into play. These can be obtained by means of different methods, featuring Zellner-Siow priors, fractional <b>Bayes</b> <b>factors</b> and intrinsic priors. The paper reviews such methods and investigates their finite-sample ability to identify the simplest model supported by the data, introducing the notion of full discrimination power. The results obtained are relevant to structural learning of Gaussian DAG models, where large spaces of sets of recursive linear regressions are to be explored...|$|R
50|$|The most {{commonly}} used criteria are (i) the Akaike information criterion and (ii) the <b>Bayes</b> <b>factor</b> and/or the Bayesian information criterion (which to some extent approximates the <b>Bayes</b> <b>factor).</b>|$|E
5000|$|... #Subtitle level 3: <b>Bayes</b> <b>factor</b> with ABC and summary {{statistics}} ...|$|E
5000|$|If {{the model}} priors are equal (...) , the <b>Bayes</b> <b>factor</b> equals the {{posterior}} ratio.|$|E
40|$|Background: Allelic-loss studies record data on {{the loss}} of genetic {{material}} in tumor tissue relative to normal tissue at various loci along the genome. As the deletion of a tumor suppressor gene can lead to tumor development, one objective of these studies is to determine which, if any, chromosome arms harbor tumor suppressor genes. Results: We propose a large class of mixture models for describing the data, and we suggest using <b>Bayes</b> <b>factors</b> to select a reasonable model from the class in order to classify the chromosome arms. <b>Bayes</b> <b>factors</b> are especially useful {{in the case of}} testing that the number of components in a mixture model is n 0 versus n 1. In these cases, frequentist test statistics based on the likelihood ratio statistic have unknown distributions and are therefore not applicable. Our simulation study shows that <b>Bayes</b> <b>factors</b> favor the right model most of the time when tumor suppressor genes are present. When no tumor suppressor genes are present and background allelic-loss varies, the <b>Bayes</b> <b>factors</b> are often inconclusive, although this results in a markedly reduced false-positive rate compared to that of standard frequentist approaches. Application of our methods to three data sets of esophageal adenocarcinomas yields interesting differences from those results previously published. Conclusions: Our results indicate that <b>Bayes</b> <b>factors</b> are useful for analyzing allelic-loss data...|$|R
40|$|This article {{considers}} a Bayesian testing for cointegration rank, using an approach developed by Strachan and van Dijk (2007), {{that is based}} on Koop, Leon-Gonzalez, and Strachan (2006). The <b>Bayes</b> <b>factors</b> are calculated for selecting cointegrating rank. We calculate the <b>Bayes</b> <b>factors</b> using two methods - the Schwarz BIC approximation and Chib's (1995) algorithm for calculating the marginal likelihood. We run Monte Carlo simulations to compare the two methods. ...|$|R
40|$|A {{sensible}} Bayesian model selection or comparison strategy implies {{selecting the}} model {{with the highest}} posterior probability. While some improper priors have attractive properties such as, e. g., low frequentist risk, it is generally claimed that Bartlett's paradox implies that using improper priors for the parameters in alternative models results in <b>Bayes</b> <b>factors</b> that are not well defined, thus preventing model comparison in this case. In this paper we demonstrate this latter result is not generally true and expand the class of priors {{that may be used}} for computing posterior odds to include some improper priors. Our approach is to give a new representation of the issue of undefined <b>Bayes</b> <b>factors</b> and, from this representation, develop classes of improper priors from which well defined <b>Bayes</b> <b>factors</b> may be derived. This approach involves either augmenting or normalising the prior measure for the parameters. One of these classes of priors includes the well known and commonly employed shrinkage prior. Estimation of <b>Bayes</b> <b>factors</b> is demonstrated for a reduced rank model. Bayes factor;improper prior;marginal likelihood;shrinkage prior;measure...|$|R
50|$|Simply put, the <b>Bayes</b> <b>factor</b> is a {{ratio of}} the {{likelihood}} probability of two competing hypotheses, usually a null and an alternative.|$|E
50|$|Massaro, DW., Cohen, M.M., Campbell, C.S. and Rodriguez, T. <b>Bayes</b> <b>Factor</b> of Model Selection Validates FLMP. Psychonomic Bulletin and Review, 2001, 8, 1-17.|$|E
5000|$|... {{is called}} the <b>Bayes</b> <b>factor</b> or {{likelihood}} ratio and the odds between two events is simply {{the ratio of the}} probabilities of the two events. Thus ...|$|E
40|$|Bayesian model {{selection}} poses {{two main}} challenges: the specification of parameter priors for all models, and the computation {{of the resulting}} <b>Bayes</b> <b>factors</b> between models. There is now a large literature on automatic and objective parameter priors in the linear model. One important class are $g$-priors, which were recently extended from linear to generalized linear models (GLMs). We show that the resulting <b>Bayes</b> <b>factors</b> can be approximated by test-based <b>Bayes</b> <b>factors</b> (Johnson [Scand. J. Stat. 35 (2008) 354 - 368]) using the deviance statistics of the models. To estimate the hyperparameter $g$, we propose empirical and fully Bayes approaches and link the former to minimum <b>Bayes</b> <b>factors</b> and shrinkage estimates from the literature. Furthermore, we describe how to approximate the corresponding posterior distribution of the regression coefficients based on the standard GLM output. We illustrate the approach {{with the development of}} a clinical prediction model for 30 -day survival in the GUSTO-I trial using logistic regression. Comment: Published at [URL] in the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
30|$|However, {{models with}} {{different}} priors {{can be compared}} {{in terms of their}} predictive accuracy using such methods as <b>Bayes</b> <b>factors.</b>|$|R
40|$|A typical {{rule that}} has been used for the {{endorsement}} of new medications by the Food and Drug Administration is to have two trials, each convincing on its own, demonstrating effectiveness. "Convincing" may be subjectively interpreted, but the use of p-values and the focus on statistical significance (in particular with p <. 05 being coined significant) is pervasive in clinical research. Therefore, in this paper, we calculate with simulations what it means to have exactly two trials, each with p <. 05, in terms of the actual strength of evidence quantified by <b>Bayes</b> <b>factors.</b> Our results show that different cases where two trials have a p-value below. 05 have wildly differing <b>Bayes</b> <b>factors.</b> <b>Bayes</b> <b>factors</b> of at least 20 in favor of the alternative hypothesis are not necessarily achieved and they fail to be reached in a large proportion of cases, in particular when the true effect size is small (0. 2 standard deviations) or zero. In a non-trivial number of cases, evidence actually points to the null hypothesis, in particular when the true effect size is zero, when the number of trials is large, and when the number of participants in both groups is low. We recommend use of <b>Bayes</b> <b>factors</b> as a routine tool to assess endorsement of new medications, because <b>Bayes</b> <b>factors</b> consistently quantify strength of evidence. Use of p-values may lead to paradoxical and spurious decision-making regarding the use of new medications...|$|R
