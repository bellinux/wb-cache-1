9|48|Public
40|$|The {{potential}} {{for the use of}} DEA and simulation in a mutually supporting role in guiding operating units to improved performance is presented. An analysis following a three-stage process is suggested. Stage one involves obtaining the data for the DEA analysis. This can be sourced from historical data, simulated data or a combination of the two. Stage two involves the DEA analysis that identifies benchmark operating units. In the third stage simulation can now be used in order to offer practical guidance to operating units towards improved performance. This can be achieved by the use of sensitivity analysis of the <b>benchmark</b> <b>unit</b> using a simulation model to offer direct support as to the feasibility and efficiency of any variations in operating practices to be tested. Alternatively, the simulation {{can be used as a}} mechanism to transmit the practices of the <b>benchmark</b> <b>unit</b> to weaker performing units by building a simulation model of the weaker unit to the process design of the <b>benchmark</b> <b>unit.</b> The model can then compare performance of the current and benchmark process designs. Quantifying improvement in this way provides a useful driver to any process change initiative that is required to bring the performance of weaker units up to the best in class. © 2005 Operational Research Society Ltd. All rights reserved...|$|E
30|$|Further {{researchers}} can develop our {{model in the}} three-level or multi-level. Since the multi-level DEA models are NP-hard problem, we proposed to use heuristic model to solve them. Finally, we propose to further researcher to create a <b>benchmark</b> <b>unit</b> in bi-level DEA model and determine VRS in efficiency model.|$|E
40|$|The aim of {{this study}} is to {{investigate}} the effect of integer data in data envelopment analysis (DEA). The inputs and outputs in different types of DEA are considered to be continuous. In most application-oriented problems, some or all data are integers; and subsequently, the continuous condition of the values is omitted. For example, situations in which the inputs/outputs are representatives of the number of cars, people, etc. In fact, the <b>benchmark</b> <b>unit</b> is artificial and does not contain integer inputs/outputs after projection on the efficiency frontier. By rounding off the projection point, we may lose the feasibility or end up having inefficient DMU. In such cases, it is required to provide a <b>benchmark</b> <b>unit</b> such that the considered unit reaches the efficiency. In the present short communication, by proposing a novel algorithm, the projecting of an inefficient DMU is carried out in such a way that produced benchmarking takes values with fully integer inputs/outputs...|$|E
30|$|First we {{show that}} Model (5) is {{feasible}} for <b>benchmarking</b> of efficient <b>units.</b>|$|R
5000|$|CA Almaden [...] (second {{production}} <b>unit,</b> {{used for}} <b>benchmarking</b> all subsequent <b>units,</b> concrete tower, designated historic) ...|$|R
50|$|In 2005 Z/Yen was {{appointed}} by the Institute of Fundraising, with partner Business in the Community, to run the enquiry line and information centre for the Payroll Giving Centre programme funded by the Home Office.Z/Yen created its flagship Global Financial Centres Index for the City of London Corporation in 2007 (bringing it in-house in 2010). 2007 also saw Z/Yen sell its investment banking cost-per-trade <b>benchmarking</b> <b>unit</b> to Aon, and launch the London Accord. This initiative, {{with the help of}} over 50 financial service sector contributors, looks to provide an open-source research resource on finance and environmental, social and governance (ESG) issues. A year later, Z/Yen launched a way of measuring the immeasurable, the Global Intellectual Property Index, for the legal firm Taylor Wessing.|$|R
40|$|A robust {{controller}} with chattering {{is proposed}} {{based on a}} simplified model of an electronic throttle system. The chattering term provides robustness against un-modeled nonlinearities, e. g., limphome nonlinearity, parameter dispersion, and friction phenomena. As the simplified model of the throttle system {{can be seen to}} correspond to a horizontal one-degree-of-freedom robot manipulator, the proposed controller is based on a previously designed robust control for regulation of robot manipulators with friction. Moreover, this controller uses only position measurements as the throttle <b>benchmark</b> <b>unit</b> does not have velocity measurements. The conceived controller is then tested in the Benchmark throttle numerical platform offered by the host conference and their affiliates. Postprint (published version...|$|E
40|$|We {{consider}} the activity-based costing situation, in which {{for each of}} several compara-ble operational units, multiple cost drivers generate a single cost pool. Our study focuses on published data from a set of property tax collection offices, called rates departments, for the London metropolitan area. We define what may be called benchmark or most efficient costs per unit of driver. A principle of maximum performance efficiency is pro-posed, and an approach to estimating the <b>benchmark</b> <b>unit</b> costs is derived from this prin-ciple. A validation approach for this estimation method is developed {{in terms of what}} we call normal-like-or-better performance effectiveness. Application to longitudinal data on a single unit is briefly discussed. We also consider some implications for the more routine case when costs are disaggregated to subpools associated with individual cost drivers...|$|E
40|$|AbstractReductions {{of capital}} cost and energy {{consumption}} {{are the key}} challenges {{for the development of}} amine-based CO 2 capture technology from flue gases. Lipophilic amine solvents exhibit a thermomorphic phase transition upon heating, leading to auto extractive behaviour, which intensifies desorption at temperatures well below the boiling point of the aqueous solutions. Solvent screening experiments in a 100 mL bubble column with CO 2 absorption at a partial pressure of 19. 4 kPa and desorption with N 2 gas stripping or magnetic agitation demonstrate that those solvents have a regeneration temperature of less than 80  °C combined with a high net loading capacity of 3. 34  mol−CO 2 /L. This permits the use of low value and even waste heat for desorption purposes and can thus improves the process economics for CO 2 capture in industrial complexes. The new biphasic solvent system was evaluated in a newly constructed <b>benchmark</b> <b>unit,</b> which is comprised of an absorber column (2. 5 cm in diameter and 145 cm in height) filled with high efficiency structured packing and a 500 mL stirred-tank as regenerator. In order to observe the phase transition phenomenon, the absorber and regenerator were made of glass. Three solvent formulations were tested at various regeneration temperatures ranging from 50 to 95  °C. A maximum 100 % recovery was attained with certain solvent formulations at a regeneration temperature around 90  °C with total gas flow rate of 300 NL/hr (85 % N 2 and 15 % CO 2) ...|$|E
40|$|In contesting global today, {{supply chain}} contest together, not {{business}} marks, not {{companies and the}} other hand, the important and role of supply chain in living level of people and also, splendor of economic in our country consider to the companies more than over. Because, evaluation of performance have important for efficiency management in supply chain so that if supply chain have rest to consider, revisit, control and evaluation of performance then it possible that supply chain disappear. So, in this paper, it is aim that through using of DEA and modified Russell model, we do to evaluate in whole supply chain. The advantage of this method {{is not only to}} improve of performance of deficient supply chain, but also, <b>benchmarking</b> <b>units</b> provide to any deficient supply chain...|$|R
40|$|Introduction MPBench is a {{benchmark}} {{to evaluate the}} performance of MPI and PVM on MPP's and clusters of workstations. It uses a flexible and portable framework to allow benchmarking of any message passing layer with similar send and receive semantics. It generates two types of reports, consisting of the raw data files and Postscript graphs. No interpretation or {{analysis of the data}} is performed, it is left entirely up to the user. 2 How it works MPBench currently tests seven different MPI and PVM calls. MPI provides much richer functionality that PVM does, so some of the benchmarks have been implemented in terms of lower level PVM functions. The following functions are measured. <b>Benchmark</b> <b>Units</b> Num Procs Bandwidth Megabytes/sec 2, 3 Roundtrip Transactions/sec 2, 3 Application Latency Microseconds 2, 3 Broadcast Megabytes/sec 16 Reduce Megabytes/sec 16 AllReduce Megabytes/sec 16 AllReduce is a reduction operation in which all tasks rece...|$|R
30|$|In summary, it can {{be stated}} that under Model (5), some of the {{efficient}} artificial units can be obtained {{which could be used}} as the practical <b>benchmarks</b> of efficient <b>units.</b> If such artificial units are impossible, then the efficient unit is practically efficient too.|$|R
40|$|Abstract Background Assessment {{of human}} {{exposure}} to non-persistent pesticides such as pyrethroids is {{often based on}} urinary biomarker measurements. Urinary metabolite levels of these pesticides are usually reported in volume-weighted concentrations or creatinine-adjusted concentrations measured in spot urine samples. It is known that these units are subject to intra- and inter-individual variations. This research aimed at studying {{the impact of these}} variations on the assessment of pyrethroid absorbed doses at individual and population levels. Methods Using data obtained from various adult and infantile populations, the intra and inter-individual variability in the urinary flow rate and creatinine excretion rate was first estimated. Individual absorbed doses were then calculated using volume-weighted or creatinine-adjusted concentrations according to published approaches and compared to those estimated from the amounts of biomarkers excreted in 15 - or 24 -h urine collections, the latter serving as a <b>benchmark</b> <b>unit.</b> The effect of the units of measurements (volume-weighted or creatinine adjusted concentrations or 24 -h amounts) on results of the comparison of pyrethroid biomarker levels between two populations was also evaluated. Results Estimation of daily absorbed doses of permethrin from volume-weighted or creatinine-adjusted concentrations of biomarkers was found to potentially lead to substantial under or overestimation when compared to doses reconstructed directly from amounts excreted in urine during a given period of time (- 70 to + 573 % and - 83 to + 167 %, respectively). It was also shown that the variability in creatinine excretion rate and urinary flow rate may introduce a bias in the case of between population comparisons. Conclusion The unit chosen to express biomonitoring data may influence the validity of estimated individual absorbed dose as well as the outcome of between population comparisons. </p...|$|E
40|$|Metabolic rate is not {{routinely}} {{assessed in}} healthcare {{for the general}} population, {{nor is it a}} measure commonly recorded for in-patients (incorrect feeding can slow post-operation recovery rate). For the general community, this lack of knowledge prevents the accurate determination of calorific need and is a factor contributing towards the onset of an overweight and an increasingly obese population. In the UK alone, obesity costs the National Health Service a staggering £ 5 billion annually. In this thesis a novel low-cost hand-held breath analyser is presented in order to measure human energy expenditure (EE). A unique optical CO 2 sensor was developed, capable of sampling exhaled breath with a fast response time ~ 1 s and resilience to a humidity range of ~ 30 % to near saturated. The device was tested in a laboratory gas testing rig and a detection limit of ~ 25 ppm CO 2 was measured. A low power metal oxide sensor (~ 100 mW) was developed to detect volatile organic compounds (VOCs) in the breath, for disease detection and investigation of the variation of inter-individual metabolism processes. The device was sensitive to acetone (100 to 300 ppm, which is a biomarker for type-I diabetes). Other VOCs, such as NO 2 were tested (10 to 250 ppb). Further work includes investigating the inter-individual variance of metabolism processes, for which the metal oxide sensor would be well-suited. Software was developed to operate the gas testing rig and acquire sensor output data in real-time. An application was written for smartphones to enable EE measurements with the breath analyser, outside of a laboratory environment. Three hand-held analysers were constructed and tested with a trial of 10 subjects. A counterpart (<b>benchmark)</b> <b>unit</b> with medical grade commercial sensors (cost of ~£ 2500) and hospital respiratory rooms (reference) were included in the trial. The newly developed analysers improved upon the performance of the benchmark system (average EE measurement error + 2. 4 % compared to + 7. 9 %). The affordable device offered far greater accuracy than the traditional method often used by practitioners (predictive equations, error + 41. 4 %). It is proposed a set of periodic (hourly) breath measurements could be used to determine daily EE. The EE analyser and associated low-cost sensors developed in this work offer a potential solution to halt the growing cost of an obese population and provide point-of-care health management...|$|E
40|$|In today's {{competitive}} situation, with {{service industries}} sprouting at an incredible rate, managing service performance becomes an essential strategy for success and survival. The automotive service industry {{is one such}} industry where managing service performance becomes essential for successful operations. In automobile repair shops, the various dimensions considered for service performance include cost, time and service quality dimensions. In service quality measurement, vagueness and uncertainty lead to the insufficient and imprecise capture of the judgements of decision makers. Hence, in this paper, the authors use fuzzy Analytic Hierarchy Process (fuzzy AHP) for the accurate measurement of service quality. The Service Quality Measure (SQM) from fuzzy AHP, the cost dimensions (generated revenue and operating cost) and the time dimension (productive service time) are provided to the Data Envelopment Analysis (DEA) model to measure the efficiency of automobile repair shops. In addition, the results from DEA also provide the efficiency targets for all Decision-Making Units (DMUs) through a comparison with <b>benchmark</b> <b>units.</b> The integrated approach ensures the accurate measurement of service performance and provides efficient targets for each considered automobile unit. [Received 31 January 2008; Revised 09 June 2008; Accepted 19 December 2008]service performance management; service quality; fuzzy AHP; data envelopment analysis; DEA; automobile industry; automotive repair shops; analytical hierarchy process; decision making. ...|$|R
50|$|The JsPHP {{library is}} {{developed}} at www.jsphp.com, {{which provides a}} CMS and IDE for development and testing of the software. Of particular note are the built-in code editing, <b>unit</b> testing and <b>benchmarking</b> facilities. The <b>unit</b> testing facility is built on the QUnit library, part of the jQuery project.|$|R
40|$|This study {{investigates the}} {{incidence}} of per capita income convergence in CFA franc countries, using the total average, regional average (West African and Central African countries, separately) and per capita income of France, as <b>benchmarks.</b> Using <b>unit</b> root tests with single structural break, the findings illustrate no conditional convergence towards {{any of the three}} benchmarks. The findings further demonstrate that Benin satisfies the catch-up hypothesis towards the total average, while Burkina Faso satisfies the catch-up hypothesis towards the West African average and no country satisfies the catch-up hypothesis towards per capita income of France...|$|R
40|$|The {{results of}} a search for an excited bottom-quark b∗ in pp {{collisions}} at √s = 7 TeV, using 4. 7 fb^(− 1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b^∗-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton+jets final states, which are combined to set limits on b^∗-quark couplings {{for a range of}} b^∗-quark masses. For a <b>benchmark</b> with <b>unit</b> size chromomagnetic and Standard Model-like electroweak b^∗ couplings, b^∗ quarks with masses less than 870 GeV are excluded at the 95...|$|R
50|$|Benchmarking is {{sometimes}} referred to as 'post-stratification' because of its similarities to stratified sampling. The difference between the two is that in stratified sampling, we decide in advance how many units will be sampled from each stratum (equivalent to benchmarking cells); in <b>benchmarking,</b> we select <b>units</b> from the broader population, and the number chosen from each cell is a matter of chance.|$|R
40|$|AbstractThe {{results of}} a search for an excited bottom-quark b⁎ in pp {{collisions}} at s= 7 TeV, using 4. 7   fb− 1 of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b⁎-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton+jets final states, which are combined to set limits on b⁎-quark couplings {{for a range of}} b⁎-quark masses. For a <b>benchmark</b> with <b>unit</b> size chromomagnetic and Standard Model-like electroweak b⁎ couplings, b⁎ quarks with masses less than 870 GeV are excluded at the 95 % credibility level...|$|R
40|$|OBJECTIVE: To {{evaluate}} {{overall performance}} of English colorectal cancer surgical units identified as outliers {{for a single}} quality measure [...] 30 day inhospital mortality. DESIGN: 144, 542 patients that underwent primary major colorectal cancer resection between 2000 / 2001 and 2007 / 2008 in 149 English National Health Service units were included from hospital episodes statistics. Casemix adjusted funnel plots were constructed for 30 day inhospital mortality, length of stay, unplanned readmission within 28 days, reoperation, failure to rescue-surgical (FTR-S) and abdominoperineal excision (APE) rates. Institutional performance was evaluated across all other domains for institutions deemed outliers for 30 day mortality. Outliers were those that lay on or breached 3 SD control limits. 'Acceptable' performance was defined if units appeared under the upper 2 SD limit. RESULTS: 5 high mortality outlier (HMO) units and 15 low mortality outlier (LMO) units were identified. Of the five HMO units, two were substandard performance outliers (ie, above 3 SD) on another metric (both on high reoperation rates). A further two HMO institutions exceeded the second but not the third SD limits for substandard performance on other outcome metrics. One of the 15 LMO units exceeded 3 SD for substandard performance (APE rate). One LMO institution exceeded the second but not the third SD control limits for high reoperation rates. Institutional mortality correlated with FTR-S and reoperations (R= 0. 445, p< 0. 001 and R= 0. 191, p< 0. 020 respectively). CONCLUSIONS: Performance appraisal in colorectal surgery is complex and dependent on stakeholder perspective. <b>Benchmarking</b> <b>units</b> solely on a single performance measure is over simplistic and potentially hazardous. A global appraisal of institutional outcome is required to contextualise performance...|$|R
40|$|Nurses {{strive to}} reduce risk and ensure patient safety from falls {{in health care}} systems. Patients and their {{families}} are able to take {{a more active role in}} reducing falls. The focus of this article is on the use of bundled fall prevention interventions highlighted by a patient/family engagement educational video. The implementation of this quality improvement intervention across 2 different patient populations was successful in achieving <b>unit</b> <b>benchmarks...</b>|$|R
40|$|This is the {{accepted}} manuscript {{version of the}} paper. The final published version can be found at: [URL] specific effects are often used to estimate non-spatial efficiency. We extend such estimators to the case where there is spatial autoregressive dependence and introduce the concept of spillover efficiency. Intuitively, we present an approach to <b>benchmark</b> how successful <b>units</b> are at exporting and importing productive performance to and from other units...|$|R
40|$|The {{results of}} a search for an excited bottom-quark b * in pp {{collisions}} at s= 7 TeV, using 4. 7 fb - 1 of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b * -quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton+jets final states, which are combined to set limits on b * -quark couplings {{for a range of}} b * -quark masses. For a <b>benchmark</b> with <b>unit</b> size chromomagnetic and Standard Model-like electroweak b * couplings, b * quarks with masses less than 870 GeV are excluded at the 95 % credibility level. © 2013 CERN...|$|R
40|$|An {{extended}} quality-driven efficiency-adjusted {{data envelopment analysis}} (QE-DEA) {{method is}} developed to measure the performance of service units. Performance is measured based on efficiency and users’ satisfaction. The extended QE-DEA method identifies as <b>benchmarks</b> only <b>units</b> that are qualified both in efficiency and satisfaction and ensures {{that all of the}} units will be qualified in both dimensions of performance when their performance becomes maximal. If there are efficient units which fail to provide satisfactory services, an adjustment procedure is applied to their outputs before the assessment of the units’ performance. Optimal output targets that lead every unit to maximal performance are defined by the extended QE-DEA. The presented expression relaxes the main assumption of the original QE-DEA method that is the fixed weights between original and adjusted outputs. The extended expression is applied to fifty public one-stop shops. ...|$|R
40|$|We {{propose a}} <b>benchmark</b> for {{object-oriented}} <b>unit</b> testing, called the behavioural response. This is a normative set of state- and equivalence partition-based test cases. Metrics are then defined {{to measure the}} adequacy and effectiveness of a test set (with respect to the benchmark) and {{the efficiency of the}} testing method (with respect to the time invested). The metrics are applied to expert manual testing using JUnit, and semi-automated testing using JWalk, testing a standard suite of classes that mimic component evolution. © 2008 IEEE...|$|R
40|$|See {{paper for}} full list of authors - 11 pages plus author list (23 pages total), 12 figures, 2 tables, {{submitted}} to Phys. Lett. B, All figures including auxiliary {{figures are available}} at [URL] results of a search for an excited bottom-quark b* in pp collisions at sqrt(s) = 7 TeV, using 4. 7 /fb of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top-quark. The search is performed in the dilepton and lepton+jets final states, which are combined to set limits on b*-quark couplings {{for a range of}} b*-quark masses. For a <b>benchmark</b> with <b>unit</b> size chromomagnetic and Standard Model-like electroweak b* couplings, b*-quarks with masses less than 870 GeV are excluded at the 95 % credibility level...|$|R
30|$|The {{second reason}} for {{presenting}} a <b>benchmark</b> for efficient <b>units</b> is to stimulate motivation {{as well as}} competitiveness and a continual efficiency. Giving improvement suggestions to inefficient units {{in order to help}} them reach an efficient frontier, it is likely that efficient frontiers are improved by inefficient units, and if such a thing happens, efficient units that have had no improvement in their performance will become inefficient in the next period. In other words, {{there is no guarantee that}} the efficient unit of tth period can prove to be as efficient as it was before.|$|R
40|$|We {{argue that}} a {{capacity}} market is needed in most restructured electricity markets, and present a design that avoids problems found in the early capacity markets. The proposed market only rewards capacity that contributes to reliability as demonstrated by its performance during hours {{in which there is}} a shortage of operating reserves. The capacity price responds to market conditions, increasing when and where capacity is scarce and decreasing to zero when and where it is plentiful. Market power in the capacity market is addressed by basing the capacity price on actual capacity, rather than bid capacity, so generators cannot increase the capacity price by withholding supply. Actual peak energy rents (the short-run energy and reserve profits of a <b>benchmark</b> peaking <b>unit)</b> are subtracted from the capacity price. This allows the capacity market to more accurately control short-run profits and suppresses market power in the energy market. This design both avoids and hedges energy market risk, and by suppressing market power avoids regulatory risk. Risk reduction save...|$|R
40|$|This {{document}} {{contains the}} <b>benchmarking</b> and <b>unit</b> test {{results for the}} message-passing Goddard Earth Observing System General Circulation Model (MP GEOS GCM). The MP GEOS GCM is an atmospheric model which operates on uniform and stretched latitude-longitude grids and {{is closely related to}} the shared memory parallel GEOS GCM which is currently in production at the Data Assimilation Office. The unit tests give an indication that individual components on the MP GEOS GCM are performing as expected and that the results for a given processor configuration are reproducible. The benchmarks reflect the overall performance of the MP GEOS GCM and give an idea of how the application might perform in a production setting. This document also serves as a wrap-up of the parallelization of the GCM for the High Performance Computing and Communication (HPCC) initiative. iii Contents Abstract iii List of Figures v 1 Introduction 1 2 Unit Tests and Component Benchmarks 2 2. 1 Hermes Unit Tests [...] . ...|$|R
40|$|This paper {{demonstrates}} how an appropriate epsilon value can be selected when Kourosh and Arash Model (KAM) is applied {{in order to}} estimate production frontiers as well as simultaneously rank and <b>benchmark</b> Decision Making <b>Units</b> (DMUs). KAM was recently proposed to improve the capabilities of Data Envelopment Analysis (DEA) {{in order to measure}} the performance evaluation of homogenous DMUs inclusive real and integer values. The paper also illustrates that the selection of epsilon is logically allowed even if the DMUs’ data are exact. A non-linear programming model is also proposed to find the optimum value of score while a DMU is under evaluation...|$|R
40|$|A Capacity Market that Makes Sense," (with Steven Stoft) Electricity Journal, 18, 43 - 54, August/September 2005. We {{argue that}} a {{capacity}} market is needed in most restructured electricity markets, and present a design that avoids problems found in the early capacity markets. The proposed market only rewards capacity that contributes to reliability as demonstrated by its performance during hours {{in which there is}} a shortage of operating reserves. The capacity price responds to market conditions, increasing when and where capacity is scarce and decreasing to zero when and where it is plentiful. Market power in the capacity market is addressed by basing the capacity price on actual capacity, rather than bid capacity, so generators cannot increase the capacity price by withholding supply. Actual peak energy rents (the short-run energy and reserve profits of a <b>benchmark</b> peaking <b>unit)</b> are subtracted from the capacity price. This allows the capacity market to more accurately control short-run profits and suppresses market power in the energy market. This design both avoids and hedges energy market risk, and by suppressing market power avoids regulatory risk. Risk reduction saves consumers money as do the performance and investment incentives inherent in the pay-for-performance mechanism...|$|R
40|$|Conditional {{efficiency}} {{measures are}} very natural tools {{to capture the}} efficiency of firms facing heterogeneous environmental conditions. They are defined by the distance of a unit to {{the support of a}} conditional distribution, conditional to the level of these external factors. The traditional approach is to estimate nonparametrically this distribution and this requires the use of appropriate smoothing techniques. In this paper, we consider an alternative approach to estimate its support. We first assume flexible nonparametric location-scale models linking the inputs and the outputs to the environmental factors to eliminate in the inputs/outputs the dependence on Z. Then we use these “pre-whitened” inputs and outputs to define the optimal frontier function. This provides a “pure” measure of efficiency more reliable to produce rankings or <b>benchmarks</b> of <b>units</b> among themselves, since the influence of external factors has been eliminated. We estimate both the full frontier and its more robust version, the order-m frontier. The asymptotic properties are established. We can also recover the frontiers in the original inputs/outputs space and we give their asymptotic properties. The approach is illustrated with some selected simulated data but also with a real dataset from the bank industry...|$|R
40|$|The {{conventional}} {{measures of}} benchmarking focus {{mainly on the}} water produced or water delivered, and ignore the service quality, {{and as a result}} the 'low-cost and low-quality' utilities are rated as efficient <b>units.</b> <b>Benchmarking</b> must credit utilities for improvements in service delivery. This study measures the performance of 20 urban water utilities using data from an Asian Development Bank survey of Indian water utilities in 2005. It applies data envelopment analysis to measure the performance of utilities. The results reveal that incorporation of a quality dimension into the analysis significantly increases the average performance of utilities. The difference between conventional quantity-based measures and quality-adjusted estimates implies that there are significant opportunity costs of maintaining the quality of services in water delivery. ...|$|R
40|$|Background: The {{healthcare}} sector {{needs to}} deliver evidence-based care and be cost-effective. This can be monitored in part via a national quality registry containing individualized data concerning patient problems, medical interventions, outcomes of treatment, and patient-reported outcomes. With this aim, Web Rehab Sweden {{was launched in}} 1997 and has been available online since 2007. The aim {{of this paper is}} to discuss the design, some results, and possible use of such a registry. Methods: Data entered into the registry online since 2007 were used in this paper. The registry contains information from 7, 458 patients. Data from the first 3 years were used to show differences between genders and among diagnostic groups. Non-parametric statistics were used to analyse the differences between groups. Results: The registry coverage of the country is 95 %, and completeness is 81 %. Data from hospitals/units have been accessible to the general public since 2009, but no data from individuals can be accessed. Length of stay has varied over the years, becoming significantly shorter between 2007 and 2012. Conclusion: A quality registry presents an opportunity to improve rehabilitation processes at participating units, provides data for use in <b>benchmarking</b> between <b>units,</b> and enables hospital management to utilize resources wisely...|$|R
40|$|Academics and {{financial}} media are divided {{on whether the}} size of assets under management (AUM) influences returns. In an attempt to seek clarity in the local context, this study investigates {{the effect of the}} size of assets under management on alpha in South African equity mutual funds over the three-year period to 31 December 2015. The study is based on secondary quantitative data reported on the Bloomberg Professional service database that includes mutual fund and <b>benchmark</b> indices, <b>unit</b> prices and fund asset sizes. The research sample comprises 69 South African equity mutual funds that existed for the three-year period under review. In this study, the relationship between AUM size and alpha is examined using the cross-sectional regression approach. No evidence of a linear relationship between AUM size and alpha was observed in the analysis based on the sample data. This finding is consistent with the semi-strong form efficient market hypothesis that securities reflect publicly available information. This finding implies that the exponential growth in AUM experienced in South Africa {{over the past two decades}} has neither enhanced nor come at the cost of returns. There does not seem to be a size effect that new investors and fund managers should be aware of...|$|R
