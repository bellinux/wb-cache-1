50|114|Public
5000|$|... #Caption: Schematic {{overview}} {{of a deep}} <b>belief</b> <b>net.</b> Arrows represent directed connections in the graphical model that the net represents.|$|E
50|$|Deep{{learning}}4j is a {{deep learning}} programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep <b>belief</b> <b>net,</b> deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.|$|E
40|$|This paper {{introduces}} an algorithm for automatically {{acquiring the}} conceptual structure of each word from corpus. The {{concept of a}} word is defined within the probabilistic framework. A variation of <b>Belief</b> <b>Net</b> named as Collocation Map is used to compute the probabilities. The <b>Belief</b> <b>Net</b> captures the conditional independences of words, which is obtained from the cooccurrence relations. The computation in general Belief Nets {{is known to be}} NP-hard, so we 1 opted Gibbs sampling for the approximation of the probabilities...|$|E
40|$|This paper {{introduces}} fuzzy <b>belief</b> <b>nets</b> (FBN). The {{ability to}} invert arcs between nodes {{is key to}} solving <b>belief</b> <b>nets.</b> The inversion is accomplished by defining closeness measures which allow diagnostic reasoning from observed symptoms to cause of failures. The closeness measures are motivated by a Lukasiewicz operator which {{takes into account the}} distance from an observed symptom set to the modeled symptom set for all failure combinations. Hypothesized failures are then ranked according to maximum closeness measure and minimum cover, i. e., number of faults. Within the realm of fuzzy logic we show the graphical representation and solution of fuzzy <b>belief</b> <b>nets...</b>|$|R
40|$|A vine {{is a new}} {{graphical}} model for dependent random variables. Vines generalize the Markov trees often used in modeling multivariate distributions. They differ from Markov trees and Bayesian <b>belief</b> <b>nets</b> in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence. A general formula for the density of a vine dependent distribution is derived. This generalizes the well-known density formula for <b>belief</b> <b>nets</b> based on the decomposition of <b>belief</b> <b>nets</b> into cliques. Furthermore, the formula allows a simple proof of the Information Decomposition Theorem for a regular vine. The problem of (conditional) sampling is discussed, and Gibbs sampling is proposed to carry out sampling from conditional vine dependent distributions. The so-called lsquocanonical vinesrsquo built on highest degree trees offer the most efficient structure for Gibbs sampling...|$|R
40|$|A new {{graphical}} model, {{called a}} vine, for dependent random variables is introduced. Vines generalize the Markov trees {{often used in}} modelling high-dimensional distributions. They differ from Markov trees and Bayesian <b>belief</b> <b>nets</b> in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence...|$|R
40|$|The {{thesis is}} {{concerned}} with applying a method that uses graphical models to solve a reservoir simulation problem. The {{focus is on the}} graphical models called Bayesian belief nets. A Bayesian <b>belief</b> <b>net</b> based approach is compared with an already popular technique used in reservoir simulation, the ensemble Kalman filter. Risk and environmental modellingApplied mathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|Many belief nets {{are built}} by rst obtaining a {{structure}} (typically from an expert) then using a training sample to ll in the parameters. The answers to queries {{produced by the}} resulting <b>belief</b> <b>net</b> therefore depends on this sample. We investigate the task of computing the error-bars around these answers. We present both a Frequentist approach using condence intervals, where integration is done under the sampling distribution, and a Bayesian approach using credible regions, where integration is done under the posterior distribution. For each approach, we give two methods for computing the error-bars: an analytic method based on a linear-Normal approximation, and a Monte Carlo simulation method. We compare and contrast the two kinds of error-bars and the two methods for computing them, and we give empirical results for various <b>belief</b> <b>net</b> structures. 1 Introduction Belief nets are commonly used to model joint probability distributions in expert systems. A graph is used to r [...] ...|$|E
40|$|This report {{addresses}} {{the challenge of}} using auxiliary information I A to improve a given theory, encoded as a <b>belief</b> <b>net</b> BE. In contrast with many other "knowledge revision" systems, {{we deal with the}} situation where this I A may be imperfect, which means BE should not necessarily incorporate that information. Instead, we provide tools to help the expert decide how to use I A. After presenting objective criteria for measuring how much I A differs from BE, we discuss ways to evaluate whether this difference is statistically significant. We then provide tools to isolate the differences [...] - to tell the domain expert which parts of the <b>belief</b> <b>net</b> (e. g., which links, and/or which nodes) account for the discrepancy. Two of our tools involve techniques that are of independent interest: viz., the use of a noncentral Ø 2 -test to compute the relative likelihood of two similar belief nets, and a sensitivity analysis that provides the "error-bars" around the answers returned by a belief [...] ...|$|E
40|$|Deep <b>belief</b> <b>nets</b> {{have been}} {{successful}} in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep <b>belief</b> <b>nets</b> one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3 -way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model...|$|R
40|$|A new {{graphical}} model, {{called a}} vine, for dependent random variables is introduced. Vines generalize the Markov trees {{often used in}} modelling high-dimensional distributions. They differ from Markov trees and Bayesian <b>belief</b> <b>nets</b> in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence. Electrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Bayesian <b>belief</b> <b>nets</b> (BNs) {{are often}} used for {{classification}} tasks [...] - typically to return the most likely "class label" for each specified instance. Many BN-learners, however, attempt to find the BN that maximizes a different objective function (viz., likelihood, rather than classification accuracy), typically by first learning an appropriate graphical structure, then finding the maximal likelihood parameters for that structure...|$|R
40|$|In {{this paper}} we {{introduce}} {{a new class of}} image models, which we call dynamic trees or DTs. A dynamic tree model specifies a prior over a large number of trees, each one of which is a tree-structured <b>belief</b> <b>net</b> (TSBN). Experiments show that DTs are capable of generating images that are less blocky, and the models have better translation invariance properties than a fixed, "balanced" TSBN...|$|E
40|$|Learning the {{dependency}} {{structure of a}} (Bayesian) <b>belief</b> <b>net</b> involves a trade-o between simplicity and goodness of t to the training data. We describe {{the results of an}} empirical comparison of three standard model selection criteria | viz., a Minimum Description Length criterion (MDL), Akaike's Information Criterion (AIC) and a Cross-Validation criterion (XV) | applied to this problem. Our results suggest that AIC and XV are both good criteria for avoiding overtting, but MDL does not work well in this context. This report focuses on the challenge of learning the (Bayesian) <b>belief</b> <b>net</b> BN [Pea 88] that has minimum KL-divergence [KL 51] from the true distribution, D over a set of discrete variables X | i. e., the network that minimizes 1 info(BN; D) = X x PD (X = x) log PBN (X = x) from a xed training sample s drawn iid from D. As it is easy to nd the optimal parameter values (i. e., entries") for a given structure [CH 92, Hec 95], we focus further on sel [...] ...|$|E
40|$|Bayesian Belief Networks {{provide a}} principled, mathematically sound, and logically {{rational}} mechanism to represent student models. The <b>belief</b> <b>net</b> backbone structure proposed by Reye [14, 15] offers a {{practical way to}} represent and update Bayesian student models describing both cognitive and social aspects of the learner. Considering students as active participants in the modelling process, this paper explores visualization and inspectability issues of Bayesian student modelling. This paper also presents ViSMod (Visualization of Bayesian Student Models) an integrated tool to visualize and inspect distributed Bayesian student models...|$|E
40|$|Applying {{traditional}} {{collaborative filtering}} to digital publishing is challenging because user data is very sparse {{due to the}} high volume of documents relative {{to the number of}} users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep <b>belief</b> <b>nets</b> (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep <b>belief</b> <b>nets</b> toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model. Comment: Accepted to the ICML- 2014 Workshop on Knowledge-Powered Deep Learning for Text Minin...|$|R
50|$|Combining Deep <b>Belief</b> <b>Nets</b> {{with the}} Lambertian {{reflectance}} assumption, the model can learn good priors over the albedo from 2D images. Illumination variations {{can be explained}} by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition.|$|R
40|$|In {{this report}} I studied the {{performance}} of deep <b>belief</b> <b>nets</b> (DBNs) on semi-supervised learning problems, in which only {{a small proportion of}} data are la-beled. First the performance between DBNs and support vector machines (SVMs) are compared to investigate the advantage of deep models over shallow ones. I also explored the use of DBNs as pre-training for SVMs and feed-forward nets (FFNs). The experimental results show that DBN is able to yield state-of-art mod-eling power in semi-supervised learning. ...|$|R
40|$|We {{show how}} to use unlabeled data and a deep <b>belief</b> <b>net</b> (DBN) to learn a good {{covariance}} kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to {{the top layer of}} features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel. ...|$|E
40|$|Abstract—We {{present a}} novel natural-scene-statistics-based blind image quality {{assessment}} model that {{is created by}} training a deep <b>belief</b> <b>net</b> to discover good feature representations {{that are used to}} learn a regressor for quality prediction. The proposed deep model has an unsupervised pre-training stage followed by a supervised fine-tuning stage, enabling it to generalize over different distortion types, mixtures, and severities. We evaluated our new model on a recently created database of images afflicted by real distortions, and show that it outperforms current state-of-the-art blind image quality prediction models. Index Terms—Perceptual quality, deep belief nets, blind image quality assessment, natural scene statistics. I...|$|E
40|$|A BN 2 O {{network is}} a two level <b>belief</b> <b>net</b> {{in which the}} parent {{interactions}} are modeled using the noisy-or interaction model. In this paper we discuss application of the SPI local expression language to efficient inference in large BN 2 O networks. In particular, we {{show that there is}} significant structure, which can be exploited to improve over the Quickscore result. We further describe how symbolic techniques can provide information which can significantly reduce the computation required for computing all cause posterior marginals. Finally, we present a novel approximation technique with preliminary experimental results. Comment: Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI 1994...|$|E
40|$|This volume, {{like its}} predecessors, {{reflects}} {{the cutting edge}} of research on the automation of reasoning under uncertainty. A more pragmatic emphasis is evident, for although some papers address fundamental issues, the majority address practical issues. Topics include the relations between alternative formalisms (including possibilistic reasoning), Dempster-Shafer belief functions, non-monotonic reasoning, Bayesian and decision theoretic schemes, and new inference techniques for <b>belief</b> <b>nets.</b> New techniques are applied to important problems in medicine, vision, robotics, and natural language un...|$|R
40|$|The product {{expansion}} of conditional probabilities for <b>belief</b> <b>nets</b> is not maximum entropy. This appears to deny a desirable kind of assurance for the model. However, {{a kind of}} guarantee that is almost as strong as maximum entropy can be derived. Surprisingly, a variant model also exhibits the guarantee, and for many cases obtains a higher performance score than the product expansion. Comment: Appears in Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (UAI 1992...|$|R
40|$|Bayesian <b>belief</b> <b>nets</b> (BNs) {{are often}} used for {{classification}} tasks — typically to return the most likely “class label ” for each specified instance. Many BN-learners, however, attempt to find the BN that maximizes a different objective function (viz., likelihood, rather than classification accuracy), typically by first learning an appropriate graphical structure, then finding the maximal likelihood parameters for that structure. As these parameters may not maximize the classification accuracy, “discriminative learners ” follow the alternative approach of seeking the parameters that maximize conditional likelihood (CL), over the distribution of instances the BN will have to classify. This paper first formally specifies this task, and shows how it relates to logistic regression, which corresponds to finding the optimal CL parameters for a naïvebayes structure. After analyzing its inherent (sample and computational) complexity, we then present a general algorithm for this task, ELR, which applies to arbitrary BN structures and which works effectively even when given the incomplete training data. This paper presents empirical evidence that ELR works better than the standard “generative” approach {{in a variety of}} situations, especially in common situation where the BN-structure is incorrect. Keywords: (Bayesian) <b>belief</b> <b>nets,</b> Logistic regression, Classification, PAC-learning, Computational/sample complexity...|$|R
40|$|We {{propose a}} Deep <b>Belief</b> <b>Net</b> model for robust motion generation, which {{consists}} of two layers of Restricted Boltzmann Machines (RBMs). The lower layer has multiple RBMs for encoding real-valued spatial patterns of motion frames into compact representations. The upper layer has one conditional RBM for learning temporal constraints on transitions between those compact representations. This separation of spatial and temporal learning {{makes it possible to}} reproduce many attractive dynamical behaviors such as walking by a stable limit cycle, a gait transition by bifurcation, synchronization of limbs by phaselocking, and easy top-down control. We trained the model with human motion capture data and the results of motion generation are reported here...|$|E
40|$|We {{describe}} how to combine probabilistic logic and Bayesian networks {{to obtain a}} new frame-work (2 ̆ 2 Bayesian logic 2 ̆ 2) for dealing with uncertainty and causal relationships in an expert system. Probabilistic logic, invented by Boole, is a technique for drawing inferences from uncertain propositions for {{which there are no}} independence assumptions. A Bayesian network is a 2 ̆ 2 <b>belief</b> <b>net</b> 2 ̆ 2 that can represent complex conditional independence assumptions. We show how to solve inference problems in Bayesian logic by applying Benders decomposition to a nonlinear programming formulation. We also show that the number of constraints grows only linearly with the problem size for a large class of networks...|$|E
40|$|AbstractA Bayesian <b>belief</b> <b>net</b> is a {{factored}} representation for a {{joint probability}} distribution over {{a set of}} variables. This factoring is {{made possible by the}} conditional independence relationships among variables made evident in the sparseness of the graphical level of the net. There is, however, another source of factoring available which cannot be directly represented in this graphical structure. This source is the microstructure within an individual marginal or conditional distribution. We present a representation capable of making this intradistribution structure explicit, and an extension to the SPI algorithm capable of utilizing this structural information to improve the efficiency of inference. We discuss the expressivity of the local expression language, and present early experimental results showing the efficacy of the approach...|$|E
40|$|In recent years, {{with the}} rapid {{development}} of mobile Internet and its business applications, mobile advertising Click-Through Rate (CTR) estimation has become a hot research direction {{in the field of}} computational advertising, which is used to achieve accurate advertisement delivery for the best benefits in the three-side game between media, advertisers, and audiences. Current research on the estimation of CTR mainly uses the methods and models of machine learning, such as linear model or recommendation algorithms. However, most of these methods are insufficient to extract the data features and cannot reflect the nonlinear relationship between different features. In order to solve these problems, we propose a new model based on Deep <b>Belief</b> <b>Nets</b> to predict the CTR of mobile advertising, which combines together the powerful data representation and feature extraction capability of Deep <b>Belief</b> <b>Nets,</b> with the advantage of simplicity of traditional Logistic Regression models. Based on the training dataset with the information of over 40 million mobile advertisements during a period of 10 days, our experiments show that our new model has better estimation accuracy than the classic Logistic Regression (LR) model by 5. 57 % and Support Vector Regression (SVR) model by 5. 80 %...|$|R
40|$|In this paper, {{we address}} the problem for {{predicting}} cQA answer quality as a classification task. We propose a multimodal deep <b>belief</b> <b>nets</b> based approach that operates in two stages: First, the joint representation is learned by taking both textual and non-textual features into a deep learning network. Then, the joint representation learned by the network is used as input features for a linear classifier. Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach. ...|$|R
40|$|Learning sparse feature {{representations}} is {{a useful}} instrument for solving an unsupervised learning problem. In this paper, we present three labeled handwritten digit datasets, collectively called n-MNIST. Then, we propose a novel framework for the classification of handwritten digits that learns sparse representations using probabilistic quadtrees and Deep <b>Belief</b> <b>Nets.</b> On the MNIST and n-MNIST datasets, our framework shows promising results and significantly outperforms traditional Deep Belief Networks. Comment: Published in the European Symposium on Artificial Neural Networks, ESANN 201...|$|R
40|$|In this paper, {{instead of}} {{designing}} new fea-tures based on intuition, linguistic knowl-edge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model. Using the unsupervised pre-trained deep <b>belief</b> <b>net</b> (DBN) to ini-tialize DAE’s parameters {{and using the}} in-put original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsuper-vised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal compo-sition of more DAEs for large hidden lay-ers feature learning. On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant im-provements of 1. 34 / 2. 45 (IWSLT) and 0. 82 / 1. 52 (NIST) BLEU points over the unsupervised DBN features and the base-line features, respectively. ...|$|E
40|$|International audienceAutomatic facial {{expression}} recognition on 3 D face data {{is still a}} challenging problem. In this {{paper we propose a}} novel approach to perform expression recognition automatically and flexibly by combining a Bayesian <b>Belief</b> <b>Net</b> (BBN) and Statistical facial feature models (SFAM). A novel BBN is designed for the specific problem with our proposed parameter computing method. By learning global variations in face landmark configuration (morphology) and local ones in terms of texture and shape around landmarks, morphable Statistic Facial feAture Model (SFAM) allows not only to perform an automatic landmarking but also to compute the belief to feed the BBN. Tested on the public 3 D face expression database BU- 3 DFE, our automatic approach allows to recognize expressions successfully, reaching an average recognition rate over 82 %...|$|E
40|$|This paper {{explores the}} problem of object {{recognition}} from multiple observers. The basic idea is to overcome {{the limitations of the}} recognition module by integrating information from multiple sources. Each observer is capable of performing appearance-based object recognition, and through knowledge of their relative positions and orientations, the observerrs can coordinate their hypotheses to make object recognition more robust. A framework is proposed for appearance-based object recognition using Canny edge maps that are effectively normalized to be translation and scale invariant. Object matching is formulated as a non-parametric statistical similarity computation between two distribution functions, while information integration is performed in a Bayesian <b>belief</b> <b>net</b> framework. Such nets enable both a continuous and a cooperative consideration of recognition result. Experiments which are reported on two observers recognizing mobile robots show a significant improvent of the recognition results...|$|E
40|$|We {{introduce}} a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions {{can be used in}} shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep <b>belief</b> <b>nets.</b> ...|$|R
40|$|Graduation date: 1991 There {{are three}} {{families}} of exact methods used for probabilistic inference in <b>belief</b> <b>nets.</b> It {{is necessary to}} compare them and analyze the advantages and the disadvantages of each algorithm, and know the time cost of making inferences in a given belief network. This paper discusses {{the factors that influence}} the computation time of each algorithm, presents the predictive model of the time complexity for each algorithm and shows the statistical results of testing the algorithms with randomly generated belief networks...|$|R
40|$|Abstract In this report, {{first we}} give {{a survey of}} the work in plan {{recognition}} field, including the evolution of different approaches, their strength and weaknesses. Then we propose two decision-theoretic approaches to plan recognition problem, which explicitly take outcome utilities into consideration. One is an extension within the probabilistic reasoning framework, by adding utility nodes to <b>belief</b> <b>nets.</b> The other is based on maximizing the estimated expected utility of possible plan. Illustrative examples are given to explain the approaches. Finally, we compare the two approaches presented in the report and summarize the work...|$|R
