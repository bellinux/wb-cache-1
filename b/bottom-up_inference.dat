15|16|Public
25|$|A deep {{predictive}} coding network (DPCN) is a {{predictive coding}} scheme that uses top-down information to empirically adjust the priors {{needed for a}} <b>bottom-up</b> <b>inference</b> procedure {{by means of a}} deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.|$|E
40|$|Acetylcholine (ACh) {{has been}} implicated {{in a wide variety}} of tasks {{involving}} attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between <b>bottom-up</b> <b>inference,</b> influenced by input stimuli, and top-down inference, influenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model. ...|$|E
40|$|Abstract. Physicians {{and medical}} {{decision-support}} applications, such as for diagnosis, therapy, monitoring, quality assessment, and clinical research, reason about patients {{in terms of}} abstract, clinically meaningful concepts, typically over significant time periods. Clinical databases, however, store only raw, timestamped data. Thus, {{there is a need}} to bridge this gap. We introduce the Temporal Abstraction Language (TAR) which enables specification of abstract relations involving raw data and abstract concepts, and supports query answering. We characterize TAR knowledge bases that guarantee finite answer sets and shortly explain why a complete <b>bottom-up</b> <b>inference</b> mechanism terminates. The TAR language was implemented as the inference component termed ALMA in the distributed mediation system IDAN, which integrates a set of clinical databases and medical knowledge bases. Initial experiments with ALMA and IDAN on a large oncology-patients dataset are highly encouraging...|$|E
40|$|We {{present a}} <b>bottom-up</b> type <b>inference</b> {{algorithm}} for security types in Mobile Ambients. The algorithm, given an untyped process $P$, calculates the minimal set of constraints on security levels such {{that all the}} actions during a run of $P$ can be performed without violating the security level priorities. Our algorithm appears as a preliminary step {{in order to use}} type systems to ensure security properties in the web scenario...|$|R
40|$|We present two new {{algorithms}} {{for fast}} Bayesian Hierarchical Clustering on large data sets. Bayesian Hierarchical Clustering (BHC) [1] {{is a method}} for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. BHC has several advantages over traditional distancebased agglomerative clustering algorithms. It defines a probabilistic model of the data and uses Bayesian hypothesis testing to decide which merges are advantageous and to output the recommended depth of the tree. Moreover, the algorithm {{can be interpreted as}} a novel fast <b>bottom-up</b> approximate <b>inference</b> method for a Dirichlet process (i. e. countably infinite) mixture model (DPM). While the original BHC algorithm has O(n) computational complexity, the two new randomized algorithms are O(n log n) and O(n) ...|$|R
40|$|People {{routinely}} make sophisticated causal inferences unconsciously, effortlessly, {{and from}} very little data – often from {{just one or}} a few observations. We argue that these inferences can be explained as Bayesian computations over a hypothesis space of causal graphical models, shaped by strong top-down prior knowledge in the form of intuitive theories. We present two case studies of our approach, including quantitative models of human causal judgments and brief comparisons with traditional <b>bottom-up</b> models of <b>inference.</b> ...|$|R
40|$|The {{interpretation}} of complex scenes requires {{a large amount}} of prior knowledge and experience. To utilize prior knowledge in a computer vision or a decision support system for image interpretation, a probabilistic scene model for complex scenes is developed. In conjunction with a model of the observer’s characteristics (a human interpreter or a computer vision system), it is possible to support <b>bottom-up</b> <b>inference</b> from observations to interpretation as well as to focus the attention of the observer on the most promising classes of objects. The presented Bayesian approach allows rigorous formulation of uncertainty in the models and permits manifold inferences, such as the reasoning on unobserved object occurrences in the scene. Monte-Carlo methods for approximation of expectations from the posterior distribution are presented, permitting the efficient application even for high-dimensional models. The approach is illustrated on the {{interpretation of}} airfield scenes...|$|E
40|$|Variational Autoencoders are {{powerful}} models for unsupervised learning. However deep models with {{several layers of}} dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides {{state of the art}} predictive log-likelihood and tighter log-likelihood lower bound compared to the purely <b>bottom-up</b> <b>inference</b> in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers...|$|E
40|$|Facial {{behaviors}} represent {{activities of}} face or facial feature in spatial or temporal space, such as facial expressions, face pose, gaze, and furrow happenings. An automated system for facial behavior recognition is always desirable. However, it is a challenging task {{due to the}} richness and ambiguity in daily facial behaviors. This paper presents an efficient approach to real-world facial behavior recognition. With dynamic Bayesian network (DBN) technology and a general-purpose facial behavior description language (e. g., FACS), a task oriented framework is constructed to systematically represent facial behaviors of interest and the associated visual observations. Based on the task oriented DBN, we can integrate analysis results from previous times and prior knowledge of the application domain both spatially and temporally. With the top–down inference, the system can make dynamic and active selection among multiple visual channels. With the <b>bottom–up</b> <b>inference</b> from observed evidences, the current facial behavior can be classified with a desired confidence via belief propagation. We demonstrate the proposed task-oriented framework for monitoring driver vigilance. Experimental results demonstrate the validity and efficiency of our approach...|$|E
40|$|We present cTI, {{a system}} for <b>bottom-up</b> {{termination}} <b>inference.</b> Termination inference is a generalization of termination analysis /checking. Traditionally, a termination analyzer tries to prove that a given class of queries terminates. This class must be provided to the system, requiring user annotations. With termination inference such annotations are not necessary. Instead, all provably terminating classes to all related predicates are inferred at once. The architecture of cTI is discussed, highlighting several new aspects to termination analysis. The notion of termination neutral arguments is introduced, which helps to narrow down the actual arguments responsible for termination in a norm independent manner. We show how our approach can be adopted to realize an incremental system able to reuse previously inferred results, thereby allowing to use the system within a programming environment. Further we show how termination inference serves to tackle generalizations of th [...] ...|$|R
40|$|We {{present a}} novel {{algorithm}} for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic {{model of the}} data {{which can be used}} to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast <b>bottom-up</b> approximate <b>inference</b> method for a Dirichlet process (i. e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperparameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm. 1...|$|R
40|$|There {{has been}} much {{interest}} in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, highdimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (<b>bottom-up</b> and top-down) <b>inference</b> over full-sized images. ...|$|R
40|$|We {{now know}} that mid-level {{features}} can greatly enhance the performance of image learning, but how to automatically learn the image features efficiently and in an unsupervised manner is still an open question. In this paper, we present a very efficient mid-level feature learning approach (MidFea), which only involves simple operations such as $k$-means clustering, convolution, pooling, vector quantization and random projection. We explain why this simple method generates the desired features, and {{argue that there is}} no need to spend much time in learning low-level feature extractors. Furthermore, to boost the performance, we propose to model the neuron selectivity (NS) principle by building an additional layer over the mid-level features before feeding the features into the classifier. We show that the NS-layer learns category-specific neurons with both <b>bottom-up</b> <b>inference</b> and top-down analysis, and thus supports fast inference for a query image. We run extensive experiments on several public databases to demonstrate that our approach can achieve state-of-the-art performances for face recognition, gender classification, age estimation and object categorization. In particular, we demonstrate that our approach is more than an order of magnitude faster than some recently proposed sparse coding based methods. Comment: 19 pages, 14 figure...|$|E
40|$|Abstract. This paper {{addresses}} a new problem, that of multiscale activity recognition. Our {{goal is to}} detect and localize {{a wide range of}} activities, including individual actions and group activities, which may simultaneously co-occur in highresolution video. The video resolution allows for digital zoom-in (or zoom-out) for examining fine details (or coarser scales), as needed for recognition. The key challenge is how to avoid running a multitude of detectors at all spatiotemporal scales, and yet arrive at a holistically consistent video interpretation. To this end, we use a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. The AND-OR graph allows a principled formulation of efficient, cost-sensitive inference via an explore-exploit strategy. Our inference optimally schedules the following computational processes: 1) direct application of activity detectors – called α process; 2) <b>bottom-up</b> <b>inference</b> based on detecting activity parts – called β process; and 3) top-down inference based on detecting activity context – called γ process. The scheduling iteratively maximizes the log-posteriors of the resulting parse graphs. For evaluation, we have compiled and benchmarked a new dataset of high-resolution videos of group and individual activities co-occurring in a courtyard of the UCLA campus. ...|$|E
40|$|In this paper, a novel part-based {{pedestrian}} detection {{algorithm is}} proposed for complex traffic surveillance environments. To capture posture and articulation variations of pedestrians, we define a hierarchical grammar model with the and-or graphical structure to represent the decomposition of pedestrians. Thus, pedestrian detection is converted to a parsing problem. Next, we propose clustered poselet models, which use the affinity propagation clustering algorithm to automatically select representative pedestrian part patterns in keypoint space. Trained clustered poselets are utilized as the terminal part models in the grammar model. Finally, after all clustered poselet activations in the input image are detected, one <b>bottom-up</b> <b>inference</b> is performed to effectively search maximum a posteriori (MAP) solutions in the grammar model. Thus, consistent poselet activations are combined into pedestrian hypotheses, and their bounding boxes are predicted. Both appearance scores and geometry constraints among pedestrian parts are considered in inference. A series of experiments is conducted on images, both from the public TUD-Pedestrian data set and collected in real traffic crossing scenarios. The experimental results demonstrate that our algorithm outperforms other successful approaches with high reliability and robustness in complex environments...|$|E
40|$|In {{languages}} {{where the}} compiler performs no static type checks, many programs never go wrong, but {{the intended use}} of functions and component interfaces is often undocumented or appears only {{in the form of}} comments which cannot always be trusted. This often makes program maintenance problematic. We show {{that it is possible to}} reconstruct a significant portion of the type information which is implicit in a program, automatically annotate function interfaces, and detect definite type clashes without fundamental changes to the philosophy of the language or imposing a type system which unnecessarily rejects perfectly reasonable programs. To do so, we introduce the notion of success typings of functions. Unlike most static type systems, success typings incorporate subtyping and never disallow a use of a function that will not result in a type clash during runtime. Unlike most soft typing systems that have previously been proposed, success typings allow for compositional, <b>bottom-up</b> type <b>inference</b> which appears to scale well in practice. Moreover, by taking control-flow into account and exploiting properties of the language such as its module system, success typings can be refined and become accurate and precise. We demonstrate the power and practicality of the approach by applying it to Erlang. We report on our experiences from employing the type inference algorithm, without any guidance, on programs of significant size. Copyright © 2006 ACM...|$|R
40|$|Uncertainty {{in various}} forms plagues our {{interactions}} with the environment. In a Bayesian statistical framework, optimal inference and learning, based on im-perfect observation in changing contexts, require the representation and manip-ulation of different forms of uncertainty. We propose that the neuromodulatory systems such as acetylcholine (ACh) and norepinephrine (NE) {{play a major role}} in the brain’s implementation of these uncertainty computations. ACh and NE have long been supposed to be critically involved in cognitive processes such as attention and learning. However, there has been little consensus on their precise computational functions. We propose that acetylcholine reports expected uncer-tainty; norepinephrine signals unexpected uncertainty. The interaction between these formally distinct sorts of uncertainty is suggested as playing a important role in mediating the interaction between top-down and <b>bottom-up</b> processing in <b>inference</b> and learning. The generative models we use to describe probabilistic relationships in the en...|$|R
50|$|In the sequent {{calculus}} all inference {{rules have}} a purely <b>bottom-up</b> reading. <b>Inference</b> rules {{can apply to}} elements {{on both sides of}} the turnstile. (To differentiate from natural deduction, this article uses a double arrow ⇒ instead of the right tack ⊢ for sequents.) The introduction rules of natural deduction are viewed as right rules in the sequent calculus, and are structurally very similar. The elimination rules on the other hand turn into left rules in the sequent calculus. To give an example, consider disjunction; the right rules are familiar:On the left:Recall the ∨E rule of natural deduction in localised form:The proposition A ∨ B, which is the succedent of a premise in ∨E, turns into a hypothesis of the conclusion in the left rule ∨L. Thus, left rules can be seen as a sort of inverted elimination rule. This observation can be illustrated as follows:In the sequent calculus, the left and right rules are performed in lock-step until one reaches the initial sequent, which corresponds to the meeting point of elimination and introduction rules in natural deduction. These initial rules are superficially similar to the hypothesis rule of natural deduction, but in the sequent calculus they describe a transposition or a handshake of a left and a right proposition:The correspondence between the sequent calculus and natural deduction is a pair of soundness and completeness theorems, which are both provable by means of an inductive argument.|$|R
40|$|In this lecture {{we return}} to the view that a logic program is defined by a {{collection}} of inference rules for atomic propositions. But we now base the operational semantics on reasoning forward from facts, which are initially given as rules with no premisses. Every rule application potentially adds new facts. Whenever no more new facts can be generated we say forward reasoning saturates and we can answer questions about truth by examining the saturated database of facts. We illustrate bottom-up logic programming with several programs, including graph reachability, CKY parsing, and liveness analysis. 20. 1 <b>Bottom-Up</b> <b>Inference</b> We now return the very origins of logic programming as an operational interpretation of inference rules defining atomic predicates. As a reminder, consider the definition of even. even(z) evz even(N) even(s(s(N))) evss This works very well on queries such as even(s(s(s(s(z))))) (which succeeds) and even(s(s(s(z)))) (which fails). In fact, the operational reading of this program under goal-directed search constitutes a decision procedure for ground queries even(n). This specification makes little sense under an alternative interpretation where we eagerly apply the inference rules in the forward direction, from the premisses to the conclusion, until no new facts can be deduced. Th...|$|E
40|$|Fig. 1. Our {{algorithm}} recursively partitions the SLAM graph into a submap tree, and the optimization {{runs from}} the leaves to the root. Following the treemap visualization [1], each rectangle represents a submap, and the sub-rectangles represent the submaps in the child level. The red and green dots are robot poses and landmarks respectively. From left to right: 1). the finest level of submaps; 2). the coarsest level of submaps; 3). the optimized full map. Abstract — We propose a novel batch algorithm for SLAM problems that distributes the workload in a hierarchical way. We show that the original SLAM graph can be recursively partitioned into multiple-level submaps using the nested dissection algorithm, {{which leads to the}} cluster tree, a powerful graph representation. By employing the nested dissection algorithm, our algorithm greatly minimizes the dependencies between two subtrees, and the optimization of the original SLAM graph can be done using a <b>bottom-up</b> <b>inference</b> along the corresponding cluster tree. To speed up the computation, we also introduce a base node for each submap and use it to represent the rigid transformation of the submap in the global coordinate frame. As a result, the optimization moves the base nodes rather than the actual submap variables. We demonstrate that our algorithm is not only exact but also much faster than alternative approaches in both simulations and real-world experiments. I...|$|E
40|$|International audienceIn this paper, an {{original}} framework for grammar-based image understanding handling uncertainty is presented. The method takes as input an over-segmented image, every segment {{of which has}} been annotated during a ?rst stage of image classi?cation. Moreover, we assume that for every segment, the output class may be uncertain and represented by a belief function over all the possible classes. Production rules are also supposed to be provided by experts to de?ne the decomposition of a scene into objects, as well as the decomposition of every object into its components. The originality of our framework is to make it possible to deal with uncertainty in the decomposition, which is particularly useful when the relative frequencies of the production rules cannot be estimated properly. As in traditional visual grammar approaches, the goal is to build the ?parse graph? of a test image, which is its hierarchical decomposition from the scene, to objects and parts of objects while taking into account the spatial layout. In this paper, we show that the parse graph of an image can be modelled as an evidential network, and we detail a method to apply a <b>bottom-up</b> <b>inference</b> in this network. A consistency criterion is de?ned for any parse tree, and the search of the optimal interpretation of an image formulated as an optimization problem. The work was validated on real and publicly available urban driving scene data...|$|E
40|$|Humans acquire {{their most}} basic {{physical}} concepts early in development, but continue to enrich and expand their intu-itive physics throughout life {{as they are}} exposed to more and varied dynamical environments. We introduce a hierarchical Bayesian framework to explain how people can learn physi-cal theories across multiple timescales and levels of abstrac-tion. In contrast to previous Bayesian models of theory acqui-sition (Tenenbaum, Kemp, Griffiths, & Goodman, 2011), we work with more expressive probabilistic program representa-tions suitable for learning the forces and properties that govern how objects interact in dynamic scenes unfolding over time. We compare our model and human learners on a challeng-ing task of inferring novel physical laws in microworlds given short movies. People are generally able to perform this task and behave in line with model predictions. Yet they also make systematic errors suggestive of how a top-down Bayesian ap-proach to learning might be complemented by a more <b>bottom-up</b> feature-based approximate <b>inference</b> scheme, to best ex-plain theory learning at an algorithmic level...|$|R
40|$|Abstract This paper {{presents}} a numerical {{study of the}} <b>bottom-up</b> and top-down <b>inference</b> processes in hierarchical models using the And-Or graph as an example. Three infer-ence processes are identified for each node A in a recursively defined And-Or graph in which stochastic context sensitive image grammar is embedded: the α(A) process detects node A directly based on image features, the β(A) process com-putes node A by binding its child node(s) bottom-up and the γ (A) process predicts node A top-down from its par-ent node(s). All the three processes contribute to computing node A from images in complementary ways. The objective of our numerical study is to explore how much information each process contributes and how these processes should be integrated to improve performance. We study them in the task of object parsing using And-Or graph formulated un-der the Bayesian framework. Firstly, we isolate and train the α(A), β(A) and γ (A) processes separately by blocking the other two processes. Then, information contributions of each process are evaluated individually based on their dis-criminative power, compared with their respective human performance. Secondly, we integrate the three processes ex-plicitly for robust inference to improve performance an...|$|R
40|$|This paper {{presents}} a numerical {{study of the}} <b>bottom-up</b> and top-down <b>inference</b> processes in hierarchical models using the And-Or graph as an example. Three inference processes are identified for each node A in an And-Or graph: the α(A) process detects node A directly based on image features, the β(A) process computes node A by bottom-up binding of its child node(s) and the γ(A) process predicts node A top-down from its parent node(s). We isolate and train the α(A), β(A) and γ(A) processes separately through a blocking method. Information contributions of each process are evaluated individually based on their discriminative powers, compared with their respective human performance. Further more, we integrate the three processes explicitly for robust inference and propose a greedy pursuit algorithm for object parsing under the Bayesian framework. In experiments, we choose two hierarchical cases: one is junctions and rectangles in the low-to-middle-level vision {{and the other is}} human faces in the high-level vision, and observe that (1) the effectiveness of the α(A), β(A) and γ(A) processes depends on the scale and occlusion conditions, (2) the α(face) process is stronger than the α processes of its child and parent nodes, while β(junctions) and β(rectangle) work much better than their α processes, and (3) the integration of the three processes improves performance in ROC comparisons...|$|R
40|$|Large-scale mapping {{has become}} the key to {{numerous}} applications, e. g. simultaneous localization and mapping (SLAM) for autonomous robots. Despite {{of the success of}} many SLAM projects, there are still some challenging scenarios in which most of the current algorithms are not able to deliver an exact solution fast enough. One of these challenges is the size of SLAM problems, which has increased by several magnitudes over the last decade. Another challenge for SLAM problems is the large amount of noise baked in the measurements, which often yields poor initializations and slows or even fails the optimization. Urban 3 D reconstruction is another popular application for large-scale mapping and has received considerable attention recently from the computer vision community. High-quality 3 D models are useful in various successful cartographic and architectural applications, such as Google Earth or Microsoft Live Local. At the heart of urban reconstruction problems is structure from motion (SfM). Due to the wide availability of cameras, especially on handhold devices, SfM is becoming a more and more crucial technique to handle a large amount of images. In the thesis, I present a novel batch algorithm, namely Tectonic Smoothing and Mapping (TSAM). I will show that the original SLAM graph can be recursively partitioned into multiple-level submaps using the nested dissection algorithm, which leads to the cluster tree, a powerful graph representation. By employing the nested dissection algorithm, the algorithm greatly minimizes the dependencies between two subtrees, and the optimization of the original graph can be done using a <b>bottom-up</b> <b>inference</b> along the corresponding cluster tree. To speed up the computation, a base node is introduced for each submap and is used to represent the rigid transformation of the submap in the global coordinate frame. As a result, the optimization moves the base nodes rather than the actual submap variables. I will also show that TSAM can be successfully applied to the SfM problem as well, in which a hypergraph representation is employed to capture the pairwise constraints between cameras. The hierarchical partitioning based on the hypergraph not only yields a cluster tree as in the SLAM problem but also forces resulting submaps to be nonsingular. I will demonstrate the TSAM algorithm using various simulation and real-world data sets. Ph. D. Committee Chair: Frank Dellaert; Committee Member: Eric Johnson; Committee Member: Henrik Christensen; Committee Member: Irfan Essa; Committee Member: Marc Pollefey...|$|E
40|$|This paper {{presents}} a technique for the optimization of bound queries over disjunctive deductive databases with constraints. The proposed approach {{is an extension}} of the wellknown Magic-Set technique and is well-suited for being integrated in current <b>bottom-up</b> (stable) model <b>inference</b> engines. More specifically, {{it is based on the}} exploitation of binding propagation techniques which reduce the size of the data relevant to answer the query and, consequently, reduces both the complexity of computing a single model and the number of models to be considered. The motivation of this work stems from the observation that traditional binding propagation optimization techniques for bottom-up model generator systems, simulating the goal driven evaluation of top-down engines, are only suitable for positive (disjunctive) queries, while hard problems are expressed using unstratified negation. The main contribution of the paper consists in the extension of a previous technique, defined for positive disjunctive queries, to queries containing both disjunctive heads and constraints (a simple and expressive form of unstratified negation). As the usual way of expressing declaratively hard problems is based on the guess-and-check technique, where the guess part is expressed by means of disjunctive rules and the check part is expressed by means of constraints, the technique proposed here is highly relevant for the optimization of queries expressing hard problems. The value of the technique has been proved by several experiments. ...|$|R
40|$|To Appear in Theory and Practice of Logic Programming (TPLP) " This paper {{presents}} a technique for the optimization of bound queries over disjunctive deductive databases with constraints. The proposed approach {{is an extension}} of the well-known Magic-Set technique and is well-suited for being integrated in current <b>bottom-up</b> (stable) model <b>inference</b> engines. More specifically, {{it is based on the}} exploitation of binding propagation techniques which reduce the size of the data relevant to answer the query and, consequently, reduces both the complexity of computing a single model and the number of models to be considered. The motivation of this work stems from the observation that traditional binding propagation optimization techniques for bottom-up model generator systems, simulating the goal driven evaluation of top-down engines, are only suitable for positive (disjunctive) queries, while hard problems are expressed using unstratified negation. The main contribution of the paper consists in the extension of a previous technique, defined for positive disjunctive queries, to queries containing both disjunctive heads and constraints (a simple and expressive form of unstratified negation). As the usual way of expressing declaratively hard problems is based on the guess-and-check technique, where the guess part is expressed by means of disjunctive rules and the check part is expressed by means of constraints, the technique proposed here is highly relevant for the optimization of queries expressing hard problems. The value of the technique has been proved by several experiments. Comment: 35 page...|$|R
40|$|Computer {{vision has}} made {{significant}} progress in locating and recognizing objects in recent decades. However, {{beyond the scope of}} this “what is where” challenge, it lacks the abilities to understand scenes characterizing human visual experience. Comparing with human vision, what is missing in current computer vision? One answer is that human vision is not only for pattern recognition, but also supports a rich set of commonsense reasoning about object function, scene physics, social intentions etc [...] I build systems for real world applications and simultaneously pursuing a long-term goal of devising a unified framework that can make sense of an images and a scene by reasoning about the functional and physical mechanisms of objects in a 3 D world. By bridging advances spanning fields of stochastic learning, computer vision, cognitive science, my research tackles following challenges: (i) What is the visual representation? I develop stochastic grammar models to characterize spatiotemporal structures of visual scenes and events. The analogy of human natural language lays a foundation for representing both visual structure and abstract knowledge. I pose the scene understanding problem as parsing an image into a hierarchical structure of visual entities using the Stochastic Scene Grammar (SSG). With a set of production rules, the grammar enforces both structural regularity and flexibility of visual entities. Therefore, the algorithm is able to handle enormous number of configurations and large geometric variations for both indoor scenes and outdoor scenes. (ii) How to reason about the commonsense knowledge? I augment the commonsense knowledge about functionality, physical stability to the grammatical representation. The <b>bottom-up</b> and top-down <b>inference</b> algorithms are designed for finding a most plausible interpretation of visual stimuli. Functionality refers to the property of an object or scene, especially man-made ones, which has a practical use for which it was designed, and it's deeper than geometry and appearance and thus is a more invariant concept for scene understanding. We present a Stochastic Scene Grammar (SSG) as a hierarchical compositional representation which integrates functionality, geometry and appearance in a hierarchy. This represents a different philosophy that views vision tasks from the perspective of agents, that is, agents (humans, animals and robots) should perceive objects and scenes by reasoning their plausible functions. Physical stability assumption assumes objects in the static scene should be stable with respect to the gravity field. In other words, if any object is not stable on its own, it must be either grouped with neighbors or fixed to its supporting base. We pursue a physically stable scene understanding, namely ``a parse tree", by inferring object stability in the physical world. The assumption is applicable to general scene categories thus poses powerful constraints for physically plausible scene interpretation and understanding. (iii) How to acquire commonsense knowledge? I performed three case studies to acquire different kinds of commonsense knowledges: I teach the computer to learn affordance from observing human actions; to learn tool-use from single one-shot demonstration; and to infer containing relations by physical simulation without explicit training process. They provided some interesting perspectives on how to acquire and exploit commonsense knowledge. In general, the more prediction or simulation is performed, the less training data is needed. As a result, the acquired commonsense knowledge is more generalizable to new situations. Such sophisticated understanding of 3 D scenes enables computer vision to reason, predict, interact with the 3 D environment, as well as hold intelligent dialogues beyond visible spectrum...|$|R
40|$|In {{recent years}} {{a wide range}} of {{statistical}} models have been applied to vision related problems and have enjoyed much success. In a generative probabilistic model, the probability distributions of the observed images together with hidden variables describing the images are formulated (in a top-down fashion), and the visual perception and learning can be understood as an <b>inference</b> (<b>bottom-up)</b> operation that computes the posterior probabilities over the hidden variables, based on which model selection and parameter tuning, for example, can be carried out. A 'good' model requires a realistic probabilistic formulation that closely matches the statistics of the input data, and requires that the computation resulting from such formulation is tractable, and hopefully also biologically plausible. Those two requirements are not trivial. In factor analysis, for example, the observed image is expressed as a linear superposition of many basis functions. While the generation or synthesis of the image is immediate, the inference operation would typically require iterations if non-Gaussian prior is assumed or if direct matrix inversion is not allowed. If, on the other hand, the image is simply projected onto a set of filters, e. g., Gabor functions, then the probabilistic formulation is confounded, that is, it's not immediately clear how confident it would be to interpret a certain filter's response as the detection of a feature,e. g., an edge. In this talk, we present a generative probabilistic model that consists of a mixture of perturbed 2 D Gaussian Markov processes. (Because of this mixing, the resulting model is non-Gaussian.) In each Gaussian Markov process, the adjacent hidden nodes on a 2 D grid is coupled by some bond energy that resembles the energy prescribed in the "plate" model. This bond energy, however, can be subject to perturbation. Specifically, the 'bond' can be 'broken' or weakened. This is a manipulation on the inverse of the covariance matrix of the Gaussian process, instead of a constant amount of addition/subtraction to the covariance matrix {{as in the case of}} adding/removing a basis in the factor analysis. We show that the inference of the posterior probability of such perturbation amounts to the following computation: the input image is projected onto several receptive fields, and their outputs then go through a quadratic nonlinearity, subtract a threshold (controlled by the prior) and subsequently undergo a sigmoid function. Low-level features such as edges and bars of different scale and orientation can be obtained by suitable perturbations. Therefore the output of those feature detectors correspond to the data-likelihood given those components in our mixture model. We demonstrate how different features interact with each other: specifically, lateral inhibition and colinear facilitation. Also, we show that a contour can 'gate', or modify the extent of other feature detectors in its vicinity. Note that those phenomena fall directly from our probabilistic formulation; there are no heuristics involved. When we move beyond individual feature detectors and try to infer the posterior probability of contours, we will encounter the computation involving matrix inversion. We then show that there exists a family of effective preconditioners for different configurations of contours. In fact, those preconditioners are so good that the matrix inversion can be obtained in a single step! The posterior mean and covariance of the hidden nodes can therefore be easily obtained (in negligible time on a PC). In contrast, algorithms such as anisotropic diffusion or Graduated Non- Convexity would typically need many iterations of lateral propagation of information. In summary, apart from adapting a few parameters (e. g., noise level), the inference of our model can be carried out in predominantly feedforward, fan-in/fan-out type of computation, and seems biologically plausible...|$|R

