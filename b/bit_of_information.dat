515|10000|Public
5|$|Dausprungas is not {{mentioned}} anywhere else. However, {{it is known}} that Mindaugas had two nephews, Tautvilas and Gedvydas, who waged a war against their uncle. Since historians do not have any data on other brothers of Mindaugas, it is generally assumed that the two were sons of Dausprungas. During the civil war of 1249–1252 Tautvilas and Gedvydas asked Daniel of Galicia, their brother-in-law, for help. This <b>bit</b> <b>of</b> <b>information</b> indicates that they also had a sister. The sister was the second wife of Daniel and they did not have children. Dausprugas' wife must have been Duke of Samogitia Vykintas' sister since Vykintas was an uncle of Tautvilas and Gedvydas. It is believed that Gedvydas died in 1253 in a campaign against Bohemia, as it is the last message about him. Tautvilas was killed by his cousin Treniota in 1263. Some historians suggest that Tautvilas had a son, Constantine, who ruled Vitebsk, however others disagree and claim that his son might have been Aigust, who was sent by Novgorod to Pskov in 1271.|$|E
25|$|Flash memory stores {{information}} in {{an array of}} memory cells made from floating-gate transistors. In single-level cell (SLC) devices, each cell stores only one <b>bit</b> <b>of</b> <b>information.</b> Multi-level cell (MLC) devices, including triple-level cell (TLC) devices, can store more than one bit per cell.|$|E
25|$|If a {{compression}} {{scheme is}} lossless—that is, {{you can always}} recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, {{to have more than}} one <b>bit</b> <b>of</b> <b>information</b> per bit of message, but that any value less than one <b>bit</b> <b>of</b> <b>information</b> per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.|$|E
5000|$|Now an {{alphabet}} of 32 characters {{can carry}} 5 <b>bits</b> <b>of</b> <b>information</b> per character (as 32 = 25). In general the number <b>of</b> <b>bits</b> <b>of</b> <b>information</b> per character is , where N {{is the number}} of characters in the alphabet and [...] is the binary logarithm. So for English each character can convey [...] <b>bits</b> <b>of</b> <b>information.</b>|$|R
5000|$|Transmission {{of data to}} trains at 4,5 MHz at 50 kbit/s with 32 bit packets {{encoding}} 12 <b>bits</b> <b>of</b> <b>information</b> (EBICAB 700) or 255 bit packets encoding 180 <b>bits</b> <b>of</b> <b>information</b> (EBICAB 900), {{including the}} necessary synchronisation bits.|$|R
50|$|Each quinary digit has log25 (approx. 2.32) <b>bits</b> <b>of</b> <b>information.</b>|$|R
500|$|In the Middle Ages, {{chains of}} beacons were {{commonly}} used on hilltops {{as a means of}} relaying a signal. Beacon chains suffered the drawback that they could only pass a single <b>bit</b> <b>of</b> <b>information,</b> so the meaning of the message such as [...] "the enemy has been sighted" [...] had to be agreed upon in advance. One notable instance of their use was during the Spanish Armada, when a beacon chain relayed a signal from Plymouth to London.|$|E
500|$|Vanderbilt {{was drawn}} to the notion that Graysmith went from a {{cartoonist}} {{to one of the most}} significant investigators of the case. He pitched the story as: [...] "What if Garry Trudeau woke up one morning and tried to solve the Son of Sam"? As he worked on the script, he became friends with Graysmith. The filmmakers were able to get the cooperation of the Vallejo Police Department (one of the key investigators at the time) because they hoped that the film would inspire someone to come forward with a crucial <b>bit</b> <b>of</b> <b>information</b> that might help solve this decades-old cold case.|$|E
500|$|Armstrong {{expressed}} dismay at the then-upcoming presidential election. He felt {{confused by}} the country's culture war, noting the particular division among the general public on the Iraq War. Summing up his feelings {{in an interview at}} the time, he said, [...] "This war that's going on in Iraq [...] basically to build a pipeline and put up a fucking Wal-Mart." [...] Armstrong felt a duty to keep his sons away from violent images, including video games and news coverage of the war in Iraq and the 9/11 attacks. Armstrong noted divisions between America's [...] "television culture" [...] (which he said only cared about cable news) versus the world's view of America, which could be considered as careless warmongers. Dirnt felt similarly, especially so after viewing the 2004 documentary Fahrenheit 9/11. [...] "You don't have to analyze every <b>bit</b> <b>of</b> <b>information</b> in order to know that something's not fucking right, and it's time to make a change." [...] Cool hoped the record would influence young people to vote Bush out, or, as he put it, [...] "make the world a little more sane." [...] He had previously felt that it was not his place to [...] "preach" [...] to kids, but felt there was so much [...] "on the line" [...] in the 2004 election that he must.|$|E
50|$|Trivia {{refers to}} <b>bits</b> <b>of</b> <b>information,</b> often <b>of</b> little importance.|$|R
50|$|Special {{reconnaissance}} {{and intelligence}} gathering {{is intended to}} gather <b>information</b> <b>of</b> great tactical importance about the enemy´s activities, enemy personnel or other <b>bits</b> <b>of</b> <b>information</b> <b>of</b> operational significance.|$|R
50|$|Each base36 digit needs {{less than}} 6 <b>bits</b> <b>of</b> <b>information</b> to be represented.|$|R
500|$|Mendelson {{called the}} trailer [...] "a {{textbook}} case for unnecessary second pitches...Is there anyone {{out there who}} watched that first teaser back in December and said 'Hmm, it looks good and all, but I need more evidence'?" [...] He added that he was [...] "a little disappointed by the big Spider-Man reveal," [...] having previously wondered, after a Spider-Man character poster was not released with the others, if Marvel would have the [...] "courage" [...] to not include the character in any marketing materials before the film's release, letting the [...] "Peter Parker scenes the film has to offer be something that is a surprise for theatrical moviegoers and/or something that drives post-opening weekend buzz". Mendelson's colleague at Forbes Mark Hughes felt differently, noting that the trailer was targeting the general audience rather than just fans by giving [...] "us more explanation [...] of why a clearly major battle is raging between Captain America and Iron Man", and by including Spider-Man since [...] "there are plenty of surprises in these movies, and since we all already know Spider-Man is in Civil War, refusing to let us see him would frankly be a bit weird and pointless ... it's common for average filmgoers to hear some final <b>bit</b> <b>of</b> <b>information</b> or see some final image and feel compelled, inspired, or otherwise driven {{to go out to}} the movies that day. There are any number of factors that can come into play...and the world needed to see [...] because it’s the sort of value-added element that can make [...] difference". Graeme McMillan of The Hollywood Reporter felt that the Spider-Man reveal [...] "alone makes the trailer", describing it as feeling [...] "like the comic book character come to life. That awkward, voice-cracking, 'hey everyone' was as humble, playing-it-cool and cocky as he should be." ...|$|E
2500|$|However, if we {{know the}} coin is not fair, but comes up heads or tails with probabilities [...] and , where , then there is less {{uncertainty}}. Every time it is tossed, one side {{is more likely to}} come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full <b>bit</b> <b>of</b> <b>information.</b> For example, if =0.7, then ...|$|E
2500|$|The entropy of {{the unknown}} result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal {{probability}} 1/2). This is the situation of maximum uncertainty as it is most difficult to predict {{the outcome of the}} next toss; the result of each toss of the coin delivers one full <b>bit</b> <b>of</b> <b>information.</b> This is because ...|$|E
5000|$|Which <b>of</b> these <b>bits</b> <b>of</b> <b>information</b> {{should be}} {{included}} in the generated texts? ...|$|R
25|$|In this classification, 1973 Earth is a 0.7H civilization, {{with access}} to 1013 <b>bits</b> <b>of</b> <b>information.</b>|$|R
5000|$|The {{mind can}} be viewed as a device {{operating}} on <b>bits</b> <b>of</b> <b>information</b> according to formal rules.|$|R
2500|$|This {{differential}} equation {{leads to the}} solution [...] for any [...] Condition 2. leads to [...] and especially, [...] can be chosen on the form [...] with , which is equivalent to choosing a specific base for the logarithm. The different units of information (bits for , nats for the natural logarithm , bans for [...] and so on) are just constant multiples of each other. For instance, {{in case of a}} fair coin toss, heads provides [...] <b>bit</b> <b>of</b> <b>information,</b> which is approximately 0.693nbsp&nats or 0.301nbsp&decimal digits. Because of additivity, [...] tosses provide [...] bits of information, which is approximately [...] nats or [...] decimal digits.|$|E
2500|$|Cayce {{was quite}} unconvinced {{that he had}} been {{referring}} to the doctrine of reincarnation, and the best Lammers could offer was that the reading [...] "opens up the door" [...] and to go on to share his beliefs and knowledge with Cayce. Lammers had come to him with quite a <b>bit</b> <b>of</b> <b>information</b> of his own to share with Cayce and seemed intent upon convincing Cayce now that he felt the reading had confirmed his strongly-held beliefs. It should be noted, however, that 12 years earlier Cayce had briefly alluded to reincarnation. In reading 4841-1, given April 22, 1911, Cayce referred to the soul being [...] "transmigrated." [...] Because Cayce's readings were not systematically recorded until 1923, it is possible that he may have mentioned reincarnation in other earlier readings.|$|E
2500|$|Now {{consider}} {{the example of}} a coin toss. Assuming the probability of heads {{is the same as the}} probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time: if we have to choose, the best we can do is predict that the coin will come up heads, and this prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one <b>bit</b> <b>of</b> <b>information.</b> In contrast, a coin toss using a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, one binary-outcome with equiprobable values has a Shannon entropy of [...] bit. Similarly, one trit with equiprobable values contains [...] (about 1.58496) bits of information because it can have one of three values.|$|E
5000|$|A {{standard}} [...] sudoku {{has about}} 72.5 <b>bits</b> <b>of</b> <b>information</b> as calculated {{in the next}} section. Information after Shannon is the degree of randomness {{in a set of}} data. An ideal coin toss for example has an <b>information</b> <b>of</b> <b>bit.</b> To represent the outcome of 72 coin tosses 72 bits are necessary. One Sudoku contains therewith about the same information as 72 coin tosses or a sequence <b>of</b> 72 <b>bits.</b> A sequence <b>of</b> 81 random symbols [...] has [...] <b>bits</b> <b>of</b> <b>information.</b> One Sudoku code can be seen as 72.5 <b>bits</b> <b>of</b> <b>information</b> and 184.3 <b>bits</b> redundancy. Theoretically a string <b>of</b> 72 <b>bits</b> can be mapped to one sudoku that is sent over the channel as a string of 81 symbols. However, there is no linear function that maps a string to a sudoku code.|$|R
50|$|In {{all these}} standards, the CGMS-A {{information}} is only two out <b>of</b> many <b>bits</b> <b>of</b> <b>information</b> that are defined.|$|R
25|$|The {{psychological}} assumption: The {{mind can}} be viewed as a device operating on <b>bits</b> <b>of</b> <b>information</b> according to formal rules.|$|R
60|$|Ralph {{stared at}} this <b>bit</b> <b>of</b> <b>information.</b> Martin Thomas {{and the man}} he was after were most likely the same individual.|$|E
60|$|This <b>bit</b> <b>of</b> <b>information</b> was gratifying to me, and, {{without making}} any noise, I rolled {{back into the}} corner as far as possible.|$|E
60|$|When Miss Panney {{received}} this last <b>bit</b> <b>of</b> <b>information,</b> she gazed intently at Mrs. Drane {{and then at}} Ralph, after which she bade them good morning, and drove off.|$|E
5000|$|Assume {{that the}} {{combined}} system determined by two random variables X and Y has joint entropy , that is, we need [...] <b>bits</b> <b>of</b> <b>information</b> to describe its exact state. Now if we first learn {{the value of}} , we have gained [...] <b>bits</b> <b>of</b> <b>information.</b> Once [...] is known, we only need [...] bits to describe {{the state of the}} whole system. This quantity is exactly , which gives the chain rule of conditional entropy: ...|$|R
5000|$|In {{contrast}} to encoding combinations, this probability distribution usually varies in data compressors. For this purpose, Shannon entropy {{can be seen}} as weighted average: that symbol of probability [...] contains [...] <b>bits</b> <b>of</b> <b>information.</b> ANS encodes information into a single natural number , interpreted as containing [...] <b>bits</b> <b>of</b> <b>information.</b> Adding to it information from a symbol of probability , increases this informational content to [...] Hence, the new number containing both information should be [...]|$|R
5000|$|The Word Context Test {{measures}} verbal modality, deductive reasoning, integration <b>of</b> multiple <b>bits</b> <b>of</b> <b>information,</b> hypothesis testing, {{and flexibility}} of thinking ...|$|R
6000|$|... "So, when {{he comes}} back," [...] said Lawrence, [...] "he'll have a new <b>bit</b> <b>of</b> <b>information</b> to add to his stock on hand, which must be a very {{peculiar}} one, I fancy." ...|$|E
6000|$|... "A <b>bit</b> <b>of</b> <b>information</b> dating {{nearly ten}} years ago and written by one who perhaps knew more of the {{political}} intrigues of John and Beatrice Burrows than has ever come to your own knowledge." ...|$|E
6000|$|... "Up the river--at the 'Blue Boar', sir," [...] {{said the}} instructor, quite innocently--for {{it did not}} occur to him that this simple little <b>bit</b> <b>of</b> <b>information</b> was just so much incriminating {{evidence}} against Sheen.|$|E
5000|$|Similar {{analyses}} {{have been}} performed using the Kullback-Leibler divergence, a distance between two probability distributions {{defined in terms of}} entropy; the divergence of a distribution from uniform can be interpreted as the number <b>of</b> <b>bits</b> <b>of</b> <b>information</b> that can still be recovered about the initial state of the card deck. The results are qualitatively different: rather than having a sharp threshold between random and non-random at [...] shuffles, as occurs for total variation distance, the divergence decays more gradually, decreasing linearly as the number of shuffles ranges from zero to [...] (at which point the number <b>of</b> remaining <b>bits</b> <b>of</b> <b>information</b> is linear, smaller by a logarithmic factor than its initial value) and then decreasing exponentially until, after [...] shuffles, only a constant number <b>of</b> <b>bits</b> <b>of</b> <b>information</b> remain.|$|R
40|$|Nowadays, {{reversible}} computing is more fascinative {{research area}} to curtail power dissipation in comparison of conventional computing. In conventional computing, logic circuit dissipates more power by losing <b>bits</b> <b>of</b> <b>information.</b> Reversible computing recovers from losing <b>bits</b> <b>of</b> <b>information</b> through {{same number of}} output vector from same number of input vector and thus decreases the power dissipation. Since there are different cost considerations such as garbage outputs, gate count, quantum cost methods for specific cost reductions may be established...|$|R
50|$|In a synchsafe integer, {{the most}} {{significant}} <b>bit</b> <b>of</b> each byte is zero, making seven <b>bits</b> out <b>of</b> eight available. So, for example, a 32-bit synchsafe integer can only store 28 <b>bits</b> <b>of</b> <b>information.</b>|$|R
