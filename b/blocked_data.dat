30|3653|Public
50|$|In {{computing}} (specifically {{data transmission}} and data storage), a block, sometimes called a physical record, is {{a sequence of}} bytes or bits, usually containing some whole number of records, having a maximum length, a block size. Data thus structured {{are said to be}} blocked. The process of putting data into blocks is called blocking, while deblocking is the process of extracting data from blocks. <b>Blocked</b> <b>data</b> is normally stored in a data buffer and read or written a whole block at a time. Blocking reduces the overhead and speeds up the handling of the data-stream. For some devices such as magnetic tape and CKD disk devices blocking reduces the amount of external storage required for the data. Blocking is almost universally employed when storing data to 9-track magnetic tape, to NAND flash memory, and to rotating media such as floppy disks, hard disks, and optical discs.|$|E
5000|$|Those that oppose data {{discrimination}} {{say that}} it hurts {{the growth of the}} Internet, as well as the economy that is rooted into the depths of the Internet model. “Instead of promoting competition, such picking of winners and losers will stifle the investment needed to perpetuate the Internet's phenomenal growth, hurting the economy.“If, for example, telecommunication network operators <b>blocked</b> <b>data</b> packets of Voice-over-IP services that might substitute their own telephone services, this would not only discriminate against speciﬁc ﬁrms, but also reduce competition and economic welfare. Technically, this would not be a problem. Although data packets are homogeneous with respect to switching and transmission treatment, type, source, and destination can be revealed and data packets be handled differently if a network operator prefers to do so. Another problem is that the type of data that is given preferential treatment is up to the discretion of the ISP. This allows them to move data as they see fit, whether it be through a political, moral, any other such kind of [...] "lens". This goes against the first amendment, the freedom of speech because by stopping certain kinds of information from reaching the end user, they are censoring content. It is not the place of the ISP to censor content from the people.|$|E
40|$|Abstract. In {{this paper}} {{we show that}} the linear process {{bootstrap}} (LPB) and the autoregressive sieve bootstrap (AR sieve) fail in general for statistics whose large-sample distribution depends on higher order features of the dependence structure rather than just on autocovariances. We discuss why this is still the case under linearity {{if it does not}} come along with causality and invertibility with respect to an i. i. d. white noise. Inspired by the block-of-blocks bootstrap, in order to circumvent this non-validity we propose to apply the LPB and AR sieve not directly to the observations but to suitably <b>blocked</b> <b>data.</b> In a simulation study, we compare LPB, AR sieve and moving block bootstrap (MBB) applied directly and to <b>blocked</b> <b>data.</b> 1...|$|E
50|$|<b>DATA</b> <b>block</b> : The <b>DATA</b> <b>block</b> {{contains}} the <b>data</b> matrix (e.g. sequence alignment).|$|R
40|$|Abstract—Recently, several {{experimental}} {{studies have been}} conducted on <b>block</b> <b>data</b> layout in conjunction with tiling as a data transformation technique to improve cache performance. In this paper, we analyze cache and TLB performance of such alternate layouts (including <b>block</b> <b>data</b> layout and Morton layout) when used in conjunction with tiling. We derive a tight lower bound on TLB performance for standard matrix access patterns, and show that <b>block</b> <b>data</b> layout and Morton layout achieve this bound. To improve cache performance, <b>block</b> <b>data</b> layout is used in concert with tiling. Based on the cache and TLB performance analysis, we propose a <b>data</b> <b>block</b> size selection algorithm that finds a tight range for optimal block size. To validate our analysis, we conducted simulations and experiments using tiled matrix multiplication, LU decomposition, and Cholesky factorization. For matrix multiplication, simulation results using UltraSparc II parameters show that tiling and <b>block</b> <b>data</b> layout with a block size given by our block size selection algorithm, reduces up to 93 percent of TLB misses compared with other techniques (copying, padding, etc.). The total miss cost is reduced considerably. Experiments on several platforms (UltraSparc II and III, Alpha, and Pentium III) show that tiling with <b>block</b> <b>data</b> layout achieves up to 50 percent performance improvement over other techniques that use conventional layouts. Morton layout is also analyzed and compared with <b>block</b> <b>data</b> layout. Experimental results show that matrix multiplication using <b>block</b> <b>data</b> layout is up to 15 percent faster than that using Morton data layout. Index Terms—Block data layout, tiling, TLB misses, cache misses, memory hierarchy. ...|$|R
40|$|Recently, several {{experimental}} {{studies have been}} conducted on <b>block</b> <b>data</b> layout as a data transformation technique used in conjunction with tiling to improve cache performance. In this paper, we provide a theoretical analysis for the TLB and cache performance of <b>block</b> <b>data</b> layout. For standard matrix access patterns, we derive an asymptotic lower bound on the number of TLB misses for any data layout and show that <b>block</b> <b>data</b> layout achieves this bound. We show that <b>block</b> <b>data</b> layout improves TLB misses by a factor of O(B) compared with conventional data layouts, where B is the block size of <b>block</b> <b>data</b> layout. This reduction contributes to the improvement in memory hierarchy performance. Using our TLB and cache analysis, we also discuss the impact of block size on the overall memory hierarchy performance. These results are validated through simulations and experiments on state-of-the-art platforms. 1...|$|R
40|$|Abstract. Cache misses form a major {{bottleneck}} for memory-intensive applications, due to {{the significant}} latency of main memory accesses. Loop tiling, {{in conjunction with other}} program transformations, {{have been shown to be}} an effective approach to improving locality and cache exploitation, especially for dense matrix scientific computations. Beyond loop nest optimizations, data transformation techniques, and in particular <b>blocked</b> <b>data</b> layouts, have been used to boost the cache performance. The stability of performance improvements achieved are heavily dependent on the appropriate selection of tile sizes. In this paper, we investigate the memory performance of <b>blocked</b> <b>data</b> layouts, and provide a theoretical analysis for the multiple levels of memory hierarchy, when they are organized in a set associative fashion. According to this analysis, the optimal tile size that maximizes L 1 cache utilization, should completely fit in the L 1 cache, even for loop bodies that access more than just one array. Increased self- or/and cross-interference misses can be tolerated through prefetching. Such larger tiles also reduce mispredicted branches and, as a result, the lost CPU cycles that arise. Results are validated through actual benchmarksonanSMTplatform. ...|$|E
40|$|Cache misses form a major {{bottleneck}} for memory-intensive applications, due to {{the significant}} latency of main memory accesses. Loop tiling, {{in conjunction with other}} program transformations, {{have been shown to be}} an effective approach to improving locality and cache exploitation, especially for dense matrix scientific computations. Beyond loop nest optimizations, data transformation techniques, and in particular <b>blocked</b> <b>data</b> layouts, have been used to boost the cache performance. The stability of performance improvements achieved are heavily dependent on the appropriate selection of tile sizes. In this paper, we investigate the memory performance of <b>blocked</b> <b>data</b> layouts, and provide a theoretical analysis for the multiple levels of memory hierarchy, when they are organized in a set associative fashion. According to this analysis, the optimal tile size that maximizes LI cache utilization, should completely fit in the LI cache, even for loop bodies that access more than just one array. Increased self- or/and cross-interference misses can be tolerated through prefetching. Such larger tiles also reduce mispredicted branches and, as a result, the lost CPU cycles that arise. Results are validated through actual benchmarks on an SMT platform. © Springer-Verlag Berlin Heidelberg 2005...|$|E
40|$|Efficient use of {{the memory}} {{hierarchy}} is essential for good performance due to the ever increasing gap between processor and memory speed. Program transformations such as loop tiling {{have been shown to}} be an effective approach to improving locality and cache exploitation, especially for dense matrix scientific computations. In conjunction with tiling, several experimental studies have been conducted on <b>blocked</b> <b>data</b> layouts, as a data transformation technique used to boost the cache performance. The stability of the achieved performance improvements are heavily dependent on the appropriate selection of tile sizes, taking into account the actual layout of the arrays in memory. In this paper, we first provide a theoretical analysis for the cache and TLB performance of <b>blocked</b> <b>data</b> layouts. According to this analysis, the optimal tile size that maximizes L 1 cache utilization, should completely fit in the LI cache, to avoid any interference misses. We prove that when applying optimization techniques, such as register assignment, array alignment, prefetching and loop unrolling, tile sizes equal to L 1 capacity, offer better cache utilization, even for loop bodies that access more than just one array. Increased self-or/and cross-interference misses are now tolerated through prefetching. Such larger tiles also reduce lost CPU cycles due to less mispredicted branches. Results are validated through simulations and actual benchmarks on various modern platforms...|$|E
40|$|The {{fabrication}} of an updated <b>block</b> <b>data</b> composer and holographic storage array for a breadboard holographic read/write memory system is described. System considerations such as transform optics and controlled aberration lens design are described {{along with the}} <b>block</b> <b>data</b> composer, photoplastic recording materials, and material development...|$|R
5000|$|<b>Block</b> <b>Data</b> Group mirror large <b>blocks</b> of OPC <b>data</b> in an SQL {{database}} {{through the}} use of array read and writes.|$|R
30|$|Group all {{prediction}} <b>block</b> <b>data</b> in one data sub-bitstream.|$|R
40|$|Data {{storage and}} {{transfer}} operations {{at a new}} image processing facility are described. The equipment includes high density digital magnetic tape drives and specially designed controllers to provide an interface between the tape drives and computerized image processing systems. The controller performs the functions necessary to convert the continuous serial data stream from the tape drive to a word-parallel <b>blocked</b> <b>data</b> stream which then goes to the computer-based system. With regard to the tape packing density, 1. 8 times 10 to the tenth data bits are stored on a reel of one-inch tape. System components and their operation are surveyed, and studies on advanced storage techniques are summarized...|$|E
40|$|The suffix tree (or equivalently, the {{enhanced}} suffix array) provides efficient solutions to many problems involving pattern matching and pattern discovery in large strings, {{such as those}} arising in computational biology. Here we {{address the problem of}} arranging a suffix array on disk so that querying is fast in practice. We show that the combination of a small trie and a suffix array-like <b>blocked</b> <b>data</b> structure allows queries to be answered as much as three times faster than the best alternative disk-based suffix array arrangement. Construction of our data structure requires only modest processing time on top of that required to build the suffix tree, and requires negligible extra memory...|$|E
40|$|Modeling {{semantics}} {{based on}} dataflow graphs are used widely in design tools for {{digital signal processing}} (DSP). This paper develops efficient techniques for representing and manipulating blockbased operations in dataflow-based DSP design tools. In this context, a block refers to a finite-length sequence of data items, such as a sequence of speech samples, an image, {{or a group of}} video frames, as part of an enclosing data stream. We develop in this paper a meta-modeling technique called blocked dataflow (BLDF) for augmenting DSP design tools with more effective <b>blocked</b> <b>data</b> support in an efficient and general manner. We compare BLDF against alternative modeling approaches through a detailed case study of an MPEG 2 video encoder system...|$|E
50|$|<b>Block</b> <b>Data</b> Transfer: Provides a means {{transferring}} Block 1 and <b>Block</b> 2 <b>data</b> types as <b>block</b> transfers {{instead of}} point by point. In some situations this may reduce bandwidth requirements.|$|R
5000|$|... {{global data}} (replaces COMMON and <b>BLOCK</b> <b>DATA</b> from Fortran 77); ...|$|R
40|$|AbstractThe {{permeability}} is {{an important}} parameter {{for the evaluation of}} transport properties in many scientific and engineering fields. However, it is quite difficult to predict the unknown permeability information only by some sparse data in the process of interpolation. Any numerical modeling should incorporate all relevant information from different scale data including coarse scale support <b>data</b> (<b>block</b> <b>data)</b> and fine scale support data (point data) to improve interpolation accuracy. Block sequential simulation (BSSIM) is built on a combination of kriging using both point and <b>block</b> <b>data</b> with direct sequential simulation. Under the rather severe restriction, <b>block</b> <b>data</b> are linear averages of their constituent point values. This method allows reproducing both <b>block</b> and point <b>data</b> at their locations. The experimental results demonstrate that this method is practical...|$|R
40|$|Whenever {{distributed}} transaction processing in MANETs or other unreliable networks has to guarantee atomicity and isolation, {{a major challenge}} is how long-term blocking of resources can be avoided in case the mobile device looses connection to other participants of the transaction. We present a new technique for treating <b>blocked</b> <b>data</b> of transaction participants that wait for a coordinator’s commit decision. Our technique, Bi-State-Termination (BST), gives participants that have moved during transaction execution the possibility to continue transaction processing before they know the coordinator’s decision on transaction commit. The key idea of our technique is to consider both possible outcomes (commit and abort) of unknown transaction decisions. Within this paper, we describe a fast implementation of the fundamental relational database operations for a DBMS supporting the BST transaction synchronization protocol that avoids longterm transaction blocking...|$|E
40|$|A {{distributed}} algorithm {{with the}} same functionality as the single-processor level 3 BLAS operation GEMM, i. e., general matrix multiply and add, is presented. With the same functionality we mean the ability to perform GEMM operations on arbitrary subarrays of the matrices involved. The logical network is a 2 D square mesh with torus connectivity. The matrices involved are distributed with non-scattered <b>blocked</b> <b>data</b> distribution. The algorithm consists of two main parts, alignment and data movement of subarrays involved in the operation and a distributed blocked matrix multiplication algorithm on (sub) matrices using only a square submesh. Our general approach {{makes it possible to}} perform GEMM operations on non-overlapping submeshes simultaneously. 1 Introduction The level 3 Basic Linear Algebra Subprograms (BLAS) [6] were developed to exploit the memory hierarchy of modern advanced architectures, including vector and RISC-based processors and parallel computers. With optimized level 3 [...] ...|$|E
40|$|Several {{parallel}} algorithms for Fock matrix construction are described. The algorithms calculate {{only the}} unique integrals, distribute the Fock and density ma-trices over the processors of a massively parallel computer, use blocking techniques {{to construct the}} distributed data structures, and use clustering techniques on each processor to maximize data reuse. Algorithms based on both square and row blocked distributions of the Fock and density matrices are described and evaluated. Variants of the algorithms are discussed that use either triple-sort or canonical ordering of integrals, and dynamic or static task clustering schemes. The algorithms are shown to adapt to screening, with communication volume scaling down with computation costs. Modeling techniques are used to characterize algorithm performance. Given the characteristics of existing massively parallel computers, all the algorithms are shown to be highly ecient on problems of moderate size. The algorithms using the row <b>blocked</b> <b>data</b> distribution are the most ecient. ...|$|E
40|$|A <b>data</b> <b>block</b> has the {{following}} attributes: (a) A block code must be unique within the file containing the <b>data</b> <b>block.</b> (b) <b>Data</b> <b>blocks</b> {{may not be}} referenced from within a file [in contrast to save frames – see Section 2. 1. 3. 6 (d) ]. (c) The scope of data specified in a <b>data</b> <b>block</b> is the <b>data</b> <b>block.</b> The value of a data item is always associated with the <b>data</b> <b>block</b> {{in which it is}} specified. (d) Data specifications in a <b>data</b> <b>block</b> are unique, except they may be repeated within a save frame. Data specifications in a save frame are independent of the parent <b>data</b> <b>block</b> specifications. (e) If a data item is not specified in a given <b>data</b> <b>block,</b> the global value is assumed. If a global value is not specified, the value is unknown. 2. 1. 3. 8. Global bloc...|$|R
40|$|Abstract. Block-to-block and block-to-point kriging {{predictions}} {{based on}} <b>block</b> <b>data</b> are proposed. <b>Blocks</b> may be regular (mesh data) or of more general shapes. Under {{the assumptions of}} second-order stationarity and isotropicity, we show how to lessen the number of calculations of relevant block-to-block covariances. As illus-trations, a mesh data of population and a simulated <b>block</b> <b>data</b> on convex polygons are analyzed...|$|R
40|$|The UCLA Space Science Group has {{developed}} a fixed format intermediate data set called a <b>block</b> <b>data</b> set, {{which is designed to}} hold multiple segments of multicomponent sampled data series. The format is sufficiently general so that tensor functions of one or more independent variables can be stored in the form of virtual data. This makes it possible for the unit data records of the <b>block</b> <b>data</b> set to be arrays of a single dependent variable rather than discrete samples. The format is self-documenting with parameter, label and header records completely characterizing the contents of the file. The <b>block</b> <b>data</b> set has been applied to the filing of satellite data (of ATS- 6 among others) ...|$|R
40|$|Introduction As the bandwidth-distance product gets {{higher and}} higher in {{high-speed}} multi-gigabit networks, two issues, namely reducing latency and increasing link utilization, become increasingly important. In an alloptical network, once data enters the network, it is desirable to keep the data in the optical domain until it is delivered to its destination. With current optical technology, data cannot be buffered for an indefinite period {{without having to go}} through optical-to-electronic (O/E) and electronic-to-optical (E/O) conversions. A telland -go (TAG) [1, 2] protocol can reduce the pre-transmission delay associated with end-to-end reservations (e. g. in circuit switching), but if intermediate nodes do not buffer data, <b>blocked</b> <b>data</b> bursts are dropped, leading to a potentially high retransmission overhead when lossless communication is required. Deflection routing [2, 3] has been proposed as a way to reduce the retransmission overhead, but a data burst may have to be sent in seve...|$|E
40|$|We {{describe}} the design, implementation, {{and performance of}} a new parallel sparse Cholesky factorization code. The code uses a multifrontal factorization strategy. Operations on small dense submatrices are performed using new dense matrix subroutines {{that are part of}} the code, although the code can also use the blas and lapack. The new code is recursive at both the sparse and the dense levels, it uses a novel recursive data layout for dense submatrices, and it is parallelized using Cilk, an extension of C specifically designed to parallelize recursive codes. We demonstrate that the new code performs well and scales well on SMPs. In particular, on up to 16 processors, the code outperforms two state-of-the-art message-passing codes. The scalability and high performance that the code achieves imply that recursive schedules, <b>blocked</b> <b>data</b> layouts, and dynamic scheduling are effective in the implementation of sparse factorization codes...|$|E
40|$|We {{develop a}} drop-threshold {{incomplete}} Cholesky preconditioner which uses <b>blocked</b> <b>data</b> structures and computational kernels for improved performance on computers {{with one or}} more levels of cache memory. The techniques are similar to those used for Cholesky factorization in sparse direct solvers. We report on the performance of our preconditioned conjugate gradient solver on sparse linear systems from three application areas: structural mechanics, image analysis and materials modeling. On average, our blocked implementation (block-size 8) results in a speedup of over two. Keywords: sparse Cholesky, conjugate gradient, incomplete Cholesky, preconditioners. 1 Introduction Consider the solution of a linear system Ax = b when the matrix A is large, sparse, symmetric, and positive definite. Solution techniques can be broadly classified as either direct or iterative. Direct solvers are based on a sparse Cholesky factorization of A, while the conjugate gradient (CG) method of Hestenes [...] ...|$|E
40|$|The {{optimization}} of the BLAS is discussed, {{with examples}} {{given for the}} IBM superscalar RISC S/ 6000. The approach suggested is to use <b>block</b> <b>data</b> structures based on store-by-block schemes. We give results {{and analysis of the}} optimization of DGEMM. We also suggest how these results {{can be applied to the}} higher level factorizations and the other BLAS. Results are given to show the advantages of using <b>block</b> <b>data</b> structures...|$|R
5000|$|Do not retain {{full track}} data, card {{verification}} code or value (CAV2, CID, CVC2, CVV2), or PIN <b>block</b> <b>data.</b>|$|R
50|$|The BSSD {{comes under}} the {{umbrella}} of <b>block</b> <b>data</b> storage, as does the regular SSD and uses logical block addressing.|$|R
40|$|Taking Fitts’s {{law as a}} premise—that is, {{movement}} time is a linear function of an appropriate index of difficulty—we explore three issues related to the collection and reporting of these data {{from the perspective of}} application within human–computer interaction. The central question involved two design choices. Whether results obtained using blocked target conditions are representative of performance in situations in which, as is often the case, target conditions vary from movement to movement and how this difference depends on whether discrete or serial (continuous) movements are studied. Although varied target conditions led to longer movement times, the effect was additive, was surprisingly small, and did not depend on whether the movements were discrete or serial. This suggests that evaluating devices or designs using <b>blocked</b> <b>data</b> may be acceptable. With Zhai (2004) we argue against the practice of reporting throughput as a onedimensional summary for published comparisons of devices or designs.  Also questioned is whether analyses using an accuracy-adjusted index of difficulty are appropriate in all design applications...|$|E
40|$|Suffix {{trees and}} suffix arrays are {{important}} data structures for string processing, providing efficient solutions for many applications involving pattern matching. Recent work by Sinha et al. (SIGMOD 2008) addressed {{the problem of}} arranging a suffix array on disk so that querying is fast, and showed {{that the combination of}} a small trie and a suffix array-like <b>blocked</b> <b>data</b> structure allows queries to be answered many times faster than alternative disk-based suffix trees. A drawback of their LOF-SA structure, and common to all current disk resident suffix tree/array approaches, is that the space requirement of the data structure, though on disk, is large relative to the text - for the LOF-SA, 13 n bytes including the underlying n byte text. In this paper we explore techniques for reducing the space required by the LOF-SA. Experiments show these methods cut the data structure to nearly half its original size, without, for large strings that necessitate on-disk structures, any impact on search times...|$|E
40|$|Abstract — The {{increasing}} {{disparity between}} memory latency and processor speed {{is a critical}} bottleneck in achieving high performance. Recently, several {{studies have been conducted}} on <b>blocked</b> <b>data</b> layouts, in conjunction with loop tiling to improve locality of references. In this paper, we further reduce cache misses, restructuring the memory layout of multi-dimensional arrays, so that array elements are stored in a blocked way, exactly as they are swept by the tiled instruction stream. A straightforward way is presented to easily translate multi-dimensional indexing of arrays into their blocked memory layout using quick and simple binary-mask operations. Actual experimental results on three hardware platforms, using 5 different benchmarks with various array sizes, illustrate that execution time is greatly improved when combining tiled code with blocked array layouts and binary, mask-based translation functions for fast indexing. Finally, simulations verify that our enhanced performance is due to the considerable reduction of cache misses in all levels of memory hierarchy, and especially due to their concurrent minimization, for the same tile size...|$|E
5000|$|In the {{structure}} of BRS code, we divide the original <b>data</b> <b>blocks</b> into [...] blocks. They are [...] And encoding has been [...] <b>block</b> calibration <b>data</b> <b>blocks,</b> there are [...]|$|R
3000|$|... (f) is a 2 N × 1 vector of the k-th <b>block</b> <b>data,</b> and {{the length}} of vector in frequency-domain is twice than the length of vector in time-domain.|$|R
5000|$|... 1、Equally {{divides the}} {{original}} <b>data</b> <b>blocks</b> into [...] blocks, and each <b>block</b> of <b>data</b> has -bit data, recorded as ...|$|R
