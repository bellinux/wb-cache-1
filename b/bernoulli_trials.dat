278|81|Public
25|$|This {{result is}} easily {{generalized}} by substituting a letter such as t {{in the place}} of 49 to represent the observed number of 'successes' of our <b>Bernoulli</b> <b>trials,</b> and a letter such as n {{in the place of}} 80 to represent the number of <b>Bernoulli</b> <b>trials.</b> Exactly the same calculation yields the maximum likelihood estimator t/n for any sequence of n <b>Bernoulli</b> <b>trials</b> resulting in t 'successes'.|$|E
25|$|In other words, a Bernoulli {{process is}} a {{sequence}} of independent identically distributed <b>Bernoulli</b> <b>trials.</b>|$|E
25|$|The {{problem of}} {{determining}} the process, given only a limited sample of the <b>Bernoulli</b> <b>trials,</b> may be called the problem of checking whether a coin is fair.|$|E
2500|$|Bernoulli distribution, for {{the outcome}} of a single <b>Bernoulli</b> <b>trial</b> (e.g. success/failure, yes/no) ...|$|R
5000|$|... as the Jeffreys prior for the {{probability}} of success of a <b>Bernoulli</b> <b>trial.</b>|$|R
50|$|With {{respect to}} {{probability}} theory, Godwin's law becomes {{a special case}} of a <b>Bernoulli</b> <b>trial.</b>|$|R
25|$|The {{binomial}} distribution {{is a special}} case of the Poisson {{binomial distribution}}, or general binomial distribution, which is the distribution of a sum of n independent non-identical <b>Bernoulli</b> <b>trials</b> B(pi).|$|E
25|$|Two {{other common}} {{interpretation}}s of the values are true or false and yes or no. Under any interpretation of the two values, the individual variables X'i may be called <b>Bernoulli</b> <b>trials</b> with parameter p.|$|E
25|$|The Bernoulli {{distribution}} {{is a special}} case of the binomial distribution, where n=1. Symbolically, X~B(1,p) has the same meaning as X~B(p). Conversely, any binomial distribution, B(n,p), is {{the distribution of the}} sum of n <b>Bernoulli</b> <b>trials,</b> B(p), each with the same probability p.|$|E
5000|$|The model assumes that, for {{a binary}} outcome (<b>Bernoulli</b> <b>trial),</b> , and its {{associated}} vector of explanatory variables, , ...|$|R
5000|$|Let X be a <b>Bernoulli</b> <b>trial.</b> The Fisher {{information}} contained in X may be calculated to be ...|$|R
5000|$|... #Caption: Entropy of a <b>Bernoulli</b> <b>trial</b> as a {{function}} of binary outcome probability, called the binary entropy function.|$|R
25|$|For {{the special}} case where r is an integer, the {{negative}} binomial distribution {{is known as the}} Pascal distribution. It is the probability distribution of a certain number of failures and successes in a series of independent and identically distributed <b>Bernoulli</b> <b>trials.</b> For knbsp&+nbsp&r <b>Bernoulli</b> <b>trials</b> with success probability p, the negative binomial gives the probability of k successes and r failures, with a failure on the last trial. In other words, the {{negative binomial distribution}} is the probability distribution of the number of successes before the rth failure in a Bernoulli process, with probability p of successes on each trial. A Bernoulli process is a discrete time process, and so the number of trials, failures, and successes are integers.|$|E
25|$|In {{probability}} theory and statistics, the {{negative binomial distribution}} is a discrete probability distribution {{of the number of}} successes in a sequence of independent and identically distributed <b>Bernoulli</b> <b>trials</b> before a specified (non-random) number of failures (denoted r) occurs. For example, if we define a 1 as failure, all non-1s as successes, and we throw a dice repeatedly until the third time 1 appears (r = three failures), then the probability distribution of the number of non-1s that had appeared will be a negative binomial.|$|E
500|$|This is {{an example}} of a <b>Bernoulli</b> <b>trials</b> process. Each time the gambler plays the slots, there is a one in one million chance of winning. Playing one million times is {{modelled}} by the binomial distribution, which is closely related to the binomial theorem. The probability of winning [...] times out of a million trials is: ...|$|E
5000|$|Since a <b>Bernoulli</b> <b>trial</b> {{has only}} two {{possible}} outcomes, it can be framed as some [...] "yes or no" [...] question. For example: ...|$|R
40|$|We use the Bernoulli {{distribution}} {{when we have}} {{an experiment}} which can result {{in one of two}} outcomes – One outcome is labeled “success, ” and the other outcome is l b l d “f il ”a e e a ure – The probability of a success is denoted by p. The probability of a failure is then 1 – p • Such a trial is called a <b>Bernoulli</b> <b>trial</b> with success probability p Statistics-Berlin Chen 3 Examples 1. The simplest <b>Bernoulli</b> <b>trial</b> is the toss of a coin. The two outcomes are heads and tails. If we define heads to be the success outcome, then p is the probability that th i h d F f i i e co n comes up ea s. or a a r co n, p = f 2. Another <b>Bernoulli</b> <b>trial</b> is a selection o a component from a population of components, some of which are defective If we define “success ” to be a defective. component, then p is the proportion of defective components in the population Statistics-Berlin Chen 4 X ~ Bernoulli(p) • For any <b>Bernoulli</b> <b>trial,</b> we define a random variable X as follows: – If the experiment results in a success, then X = 1. Otherwise,...|$|R
5000|$|... {{where the}} {{independent}} random variables Xn are each equal to 0 or 1 with equal probabilities - this is a <b>Bernoulli</b> <b>trial</b> of each digit of the binary expansion.|$|R
500|$|The Bernoulli process, {{which can}} serve as a {{mathematical}} model for flipping a biased coin, is possibly the first stochastic process to have been studied. The process is [...] a sequence of independent <b>Bernoulli</b> <b>trials,</b> which are named after Jackob Bernoulli who used them to study games of chance, including probability problems proposed and studied earlier by Christiaan Hugens. Bernoulli's work, including the Bernoulli process, were published in his book Ars Conjectandi in 1713.|$|E
500|$|In this formula, E is the {{expected}} value, pi are the probabilities of attaining each value, and ai are the attainable values. Bernoulli normalizes {{the expected}} value by assuming that pi are the probabilities {{of all the}} disjoint outcomes of the value, hence implying that p0+p1+...+p'n=1. Another key theory developed in this part is the probability of achieving at least {{a certain number of}} successes from a number of binary events, today named <b>Bernoulli</b> <b>trials,</b> given that the probability of success in each event was the same. Bernoulli shows through mathematical induction that given a the number of favorable outcomes in each event, b the number of total outcomes in each event, d the desired number of successful outcomes, and e the number of events, the probability of at least d successes is ...|$|E
2500|$|... for {{any given}} random {{variable}} [...] out of the infinite sequence of <b>Bernoulli</b> <b>trials</b> that compose the Bernoulli process.|$|E
50|$|One {{would like}} to be able to {{interpret}} the return period in probabilistic models. The most logical interpretation for this is to take the return period as the counting rate in a Poisson distribution since it is the expectation value of the rate of occurrences. An alternative interpretation is to take it as the probability for a yearly <b>Bernoulli</b> <b>Trial</b> in the Binomial Distribution. This is disfavoured because each year does not represent an independent <b>Bernoulli</b> <b>trial</b> but is an arbitrary measure of time. This question is mainly academic as the results obtained will be similar under both the Poisson and Binomial interpretations.|$|R
50|$|The {{mathematical}} formalisation of the <b>Bernoulli</b> <b>trial</b> {{is known}} as the Bernoulli process. This article offers an elementary introduction to the concept, whereas the article on the Bernoulli process offers a more advanced treatment.|$|R
3000|$|Random {{sleeping}} strategy, {{we take it}} as a <b>Bernoulli</b> <b>trial,</b> that is each BS actives with probability q and {{sleeps with}} probability 1 −q independently. In detail, every MBS (or FAP) actives with probability q [...]...|$|R
2500|$|In {{the case}} n=1 (the case of <b>Bernoulli</b> <b>trials)</b> XY is non-zero only when both X and Y are one, and μ'X and μ'Y are {{equal to the}} two probabilities. Defining p'B as the {{probability}} of both happening at the same time, this gives ...|$|E
2500|$|The only memoryless {{discrete}} probability distributions are {{the geometric}} distributions, which feature {{the number of}} independent <b>Bernoulli</b> <b>trials</b> needed to get one [...] "success," [...] with a fixed probability p of [...] "success" [...] on each trial. In other words those are the distributions of waiting time in a Bernoulli process.|$|E
2500|$|... ƒ(k,nbsp&n,nbsp&p) is {{monotone}} increasing for knbsp&<nbsp&M and monotone decreasing for knbsp&>nbsp&M, {{with the}} exception of the case where (nnbsp&+nbsp&1)p is an integer. In this case, there are two values for which ƒ is maximal: (nnbsp&+nbsp&1)p and (nnbsp&+nbsp&1)pnbsp&−nbsp&1. M is the most probable (most likely) outcome of the <b>Bernoulli</b> <b>trials</b> and is called the mode. Note that the probability of it occurring can be fairly small.|$|E
50|$|In {{the theory}} of finite {{population}} sampling, Poisson sampling is a sampling process where each element {{of the population is}} subjected to an independent <b>Bernoulli</b> <b>trial</b> which determines whether the element becomes part of the sample.|$|R
5000|$|... #Caption: The entropy of a <b>Bernoulli</b> <b>trial</b> as a {{function}} of success probability, often called the binary entropy function, [...] The entropy is maximized at 1 bit per trial when the two possible outcomes are equally probable, as in an unbiased coin toss.|$|R
5000|$|The {{parameter}} M is {{a function}} of the corresponding Bernoulli process, which is parameterized by [...] the probability of success in a given <b>Bernoulli</b> <b>trial.</b> [...] is either the median of the distribution or the median +/− 1. It can be determined by these inequalities: ...|$|R
2500|$|Suppose {{there is}} a {{sequence}} of independent <b>Bernoulli</b> <b>trials.</b> Thus, each trial has two potential outcomes called [...] "success" [...] and [...] "failure". In each trial the probability of success is p and of failure is (1 − p). We are observing this sequence until a predefined number r of failures has occurred. Then the random number of successes we have seen, X, will have the negative binomial (or Pascal) distribution: ...|$|E
2500|$|Coins are popularly {{used as a}} sort of two-sided dice; {{in order}} to choose between two options with a random possibility, one choice will be labeled heads and the other tails, and a coin will be flipped or tossed to see whether the heads or tails side comes up on top – see coin flipping. Mathematically, this is known as a Bernoulli trial: a fair coin is defined to have the {{probability}} of heads (in the parlance of <b>Bernoulli</b> <b>trials,</b> a [...] "success") of exactly 0.5.|$|E
2500|$|Despite its name, the Poisson point {{process was}} neither {{discovered}} nor studied by the French mathematician Siméon Denis Poisson; {{the name is}} cited {{as an example of}} Stigler's law. [...] The name stems from its inherent relation to the Poisson distribution, derived by Poisson as a limiting case of the binomial distribution. This describes the probability of the sum of [...] <b>Bernoulli</b> <b>trials</b> with probability , often likened to the number of heads (or tails) after [...] biased flips of a coin with the probability of a head (or tail) occurring being [...] [...] For some positive constant , as [...] increases towards infinity and [...] decreases towards zero such that the product [...] is fixed, the Poisson distribution more closely approximates that of the binomial.|$|E
5000|$|A {{closely related}} model assumes that each i is {{associated}} {{not with a}} single <b>Bernoulli</b> <b>trial</b> but with ni independent identically distributed trials, where the observation Yi {{is the number of}} successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution: ...|$|R
40|$|We give {{conditions}} which, {{given two}} <b>Bernoulli</b> <b>trial</b> measures, {{determine whether there}} exists a homeomorphism of Cantor space which sends one measure to the other, answering a question of Oxtoby. We then provide examples, relating these results to the notions of good and refinable measures on Cantor space...|$|R
5000|$|Assuming {{that all}} members of the body vote {{independently}} (the votes are uncorrelated) and that the probability of each vote 'Yes' is equal to p = 1/2 one can estimate likelihood of such an event using the <b>Bernoulli</b> <b>trial.</b> The probability to obtain j votes 'Yes' out of 2j votes reads ...|$|R
