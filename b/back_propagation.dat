3255|51|Public
25|$|It {{is known}} that changes in {{dendritic}} excitability affect action potential <b>back</b> <b>propagation.</b> Action potentials begin near the axon hillock and propagate {{down the length of}} the axon, but they also propagate backward through the soma into the dendritic arbor. Active <b>back</b> <b>propagation</b> is dependent on ion channels and changing the densities or properties of these channels can influence the degree to which the signal is attenuated. Plasticity of back-propagation in the dendrites occurs in less than one minute and lasts longer than 25 minutes. <b>Back</b> <b>propagation</b> is a method of signaling to the synapses that an action potential was fired. This is important for spike-timing-dependent plasticity.|$|E
25|$|A {{fundamental}} objection is {{that they}} do not reflect how real neurons function. <b>Back</b> <b>propagation</b> is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.|$|E
50|$|It {{is known}} that changes in {{dendritic}} excitability affect action potential <b>back</b> <b>propagation.</b> Action potentials begin near the axon hillock and propagate {{down the length of}} the axon, but they also propagate backward through the soma into the dendritic arbor. Active <b>back</b> <b>propagation</b> is dependent on ion channels and changing the densities or properties of these channels can influence the degree to which the signal is attenuated. Plasticity of back-propagation in the dendrites occurs in less than one minute and lasts longer than 25 minutes. <b>Back</b> <b>propagation</b> is a method of signaling to the synapses that an action potential was fired. This is important for spike-timing-dependent plasticity.|$|E
30|$|An {{extended}} {{training set}} of 504 pictures per species was generated by rotating {{each of the}} seven pictures by 24 equidistant angles, as well as rescaling to 90, 100, and 110 % of the original size. Training was performed by (on average) 2000 forward passes per extended training set picture, with full error <b>back</b> <b>propagations.</b> Feed-forward and error-backward propagations employ well-known sigmoidal characteristics based on the logistic function. The target signals for the two output neurons were [1, 0] corresponding to E, and [0, 1] corresponding to P, respectively. The pillar tip positions in the SEM micrographs were detected by forward passing patches of 12  ×  12 pixels through the network. Pillar tip positions have been identified by finding the regions of maximum response of the second output neuron (Fig.  1 b).|$|R
40|$|We {{present a}} CMOS silicon chip that {{optically}} implements the <b>back</b> error <b>propagation</b> (BEP) algorithm [1] of a two layer neural network. The chip has eight units (or "neurons") on a area of approximately 2 x 2 mm. Each unit {{consists of a}} phototransistor as the detector, a modulator pad for light modulation, sample-and-hold circuits, and additional circuits necessary to perform the BEP algorithm...|$|R
50|$|The {{mechanical}} robustness compared to alumina {{is attributed to}} the displacive phase transformation of the metastable tetragonal ziroconia grains when the material is stressed. The stress concentration at a crack tip can cause a transformation from a tetragonal crystal structure to a monoclinic one, which has an associated volume expansion of ziroconia. This volume expansion effectively pushes <b>back</b> the <b>propagation</b> of the crack and results in higher toughness and strength.|$|R
5000|$|Artificial Neural {{networks}} library implements {{some common}} network architectures (multi-layer feed forward and distance networks) and learning algorithms (<b>back</b> <b>propagation,</b> delta rule, simple perceptron, evolutionary learning).|$|E
5000|$|It usually forms part of {{a larger}} pattern {{recognition}} system. It has been implemented using a perceptron network whose connection weights were trained with <b>back</b> <b>propagation</b> (supervised learning).|$|E
50|$|On each arc {{there is}} a {{statistical}} weight. Using <b>back</b> <b>propagation</b> the neural network learns the necessary pattern to recognize the prediction. It is trained by repeatedly exposing it to examples {{of the problem and}} learning the significance (weights) of the input nodes.|$|E
30|$|To {{construct}} the mapping, {{we used a}} feedforward neural network with variable structure, which is a network in which the structure is determined by minimizing the error on the training set. The training set is formed from the data registered during quiet periods. In this case, the trained neural network reproduced regular variations of the data being approximated, which is typical for quiet conditions. Network training {{was performed on the}} basis of the <b>back</b> error <b>propagation</b> algorithm (Haykin [1999]).|$|R
40|$|We propose local {{distributional}} smoothness (LDS), a new {{notion of}} smoothness for statistical model {{that can be}} used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and <b>back</b> <b>propagations.</b> When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets. Comment: Under review as a conference paper at ICLR 201...|$|R
40|$|A {{fuzzy logic}} system with center average defuzzifier, product-inference rule, nonsingleton fuzzifier and Gauss {{membership}} function is discussed. The fuzzy sets are initially defined by the cluster parameters from the Basic ISO-DATA algorithm on input space. The system is then trained via <b>back</b> error <b>propagation</b> algorithm so that the fuzzy sets are fine-tuned. The system is applied to functional EMG classification and compared with its ANN counterpart. It is superior to the latter {{in at least three}} points: higher recognition rate; insensitive to over-training; and more consistent outputs thus having higher reliability. published_or_final_versio...|$|R
50|$|The {{counterpropagation}} {{network is}} a hybrid network. It {{consists of an}} outstar network and a competitive filter network. It was developed in 1986 by Robert Hecht-Nielsen. It is guaranteed to find the correct weights, unlike regular <b>back</b> <b>propagation</b> networks that can become trapped in local minimums during training.|$|E
50|$|One of {{the well}} known machine {{learning}} technique is <b>Back</b> <b>Propagation</b> Algorithm. This mimics how humans learn from examples. The training patters are repeatedly presented to the network. The error is back propagated and the network weights are adjusted using gradient descent. The network converges through several hundreds of iterative computations.|$|E
50|$|In {{supervised}} learning models, there are tests {{that are needed}} to pass to reduce mistakes. Usually, when mistakes are encountered i.e. test output does not match test input, the algorithms use <b>back</b> <b>propagation</b> to fix mistakes. Whereas in un{{supervised learning}} models, the input is classified based on which problems need to be resolved.|$|E
40|$|An anti-Hebbian local {{learning}} algorithm for two-layer optical {{neural networks}} is introduced. With this learning rule, the weight update {{for a certain}} connection depends only on the input and output of that connection and a global, scalar error signal. Therefore the backpropagation of error signals through the network, {{as required by the}} commonly used <b>back</b> error <b>propagation</b> algorithm, is avoided. It still guarantees, however, that the synaptic weights are updated in the error descent direction. With the apparent advantage of simpler optical implementation this learning rule is also shown by simulations to be computationally effective...|$|R
40|$|The paper {{presents}} {{a new system}} identification methodology for industrial systems. Using the original Mamdani fuzzy rule based system (FRBS), an adaptive Mamdani fuzzy modeling (AMFM) is introduced in this paper. It differs from the original Mamdani FRBS in that it applies different membership functions and a defuzzification mechanism that is ‘differentiable’ {{with respect to the}} membership function parameters. The proposed system also includes a <b>back</b> error <b>propagation</b> (BEP) algorithm that is used to refine the fuzzy model. The efficacy of the proposed AMFM approach is demonstrated through the experimental trails from a compressor in an industrial gas turbine system...|$|R
40|$|The paper {{proposes a}} new Adaptive Mamdani Fuzzy Model (AMFM) based system {{modelling}} methodology that improves on traditional Mamdani fuzzy rule based system (FRBS) techniques {{through use of}} alternative membership functions and a defuzzification mechanism that is ‘differentiable’, allowing a <b>back</b> error <b>propagation</b> (BEP) algorithm to refine the initial fuzzy model. Moreover, a variational Bayesian (VB) method is applied to simplify the results via automatic selection {{of the number of}} input rules so that redundant rules can be removed for the initial modelling phase. The efficacy of the proposed VB modified AMFM (VB-AMFM) approach is demonstrated through experimental trials using measurements from a compressor in an industrial gas turbine (IGT) ...|$|R
50|$|When {{injected}} into mice it causes epileptiform behavior. This {{might be due}} to its effect on A-type K+ channels, which, like the Kv4.x, are involved in action potential <b>back</b> <b>propagation,</b> firing frequency, spike initiation and action potential waveform determination.Blocking of the hERG channel can cause drug-induced long QT syndrome, arrhythmias and ventricular fibrillation which can result in death.|$|E
5000|$|In 1997, the tLearn {{software}} was released to accompany a book. [...] This was {{a return to}} the idea of providing a small, user-friendly, simulator that was designed with the novice in mind. tLearn allowed basic feed forward networks, along with simple recurrent networks, both of which can be trained by the simple <b>back</b> <b>propagation</b> algorithm. tLearn has not been updated since 1999.|$|E
50|$|Faraday {{rotation}} is {{an example}} of non-reciprocal optical propagation. Unlike what happens in an optically active medium such as a sugar solution, reflecting a polarized beam back through the same Faraday medium does not undo the polarization change the beam underwent in its forward pass through the medium. This allows Faraday rotators to be used to construct devices such as optical isolators to prevent undesired <b>back</b> <b>propagation</b> of light from disrupting or damaging an optical system.|$|E
40|$|Abstract—The paper {{presents}} {{a new system}} identification methodology for industrial systems. Using the original Mamdani fuzzy rule based system (FRBS), an adaptive Mamdani fuzzy modeling (AMFM) is introduced in this paper. It differs from the original Mamdani FRBS in that it applies different membership functions and a defuzzification mechanism that is ‘differentiable’ {{with respect to the}} membership function parameters. The proposed system also includes a <b>back</b> error <b>propagation</b> (BEP) algorithm that is used to refine the fuzzy model. The efficacy of the proposed AMFM approach is demonstrated through the experimental trails from a compressor in an industrial gas turbine system. Keywords—Mamdani fuzzy rule based system; adaptive Mamdani fuzzy modeling; back error propagation; industrial gas turbine. I...|$|R
40|$|International audienceThe {{near-field}} (NF) {{regime is}} a highly challenging and rather unstudied area of electromagnetic (EM) fields due to the high mathematical complexity that it presents. Nevertheless, the evanescent fields, present within the near field, can offer extra flexibility and freedom in molding the close-to-the-antenna field. In this work we develop an algorithm capable to shape the EM fields according to specifications in 3 D regions at distances within the near-field and Fresnel region of the radiating device. The algorithm {{is based on a}} set theoretic approach and a frontand- <b>back</b> iterative <b>propagation</b> scheme, where the use of FFT is playing a key role in speeding up the algorithm's application. A prototype using a Radial Line Slot Antenna (RLSA) topology has been manufactured to prove the concept...|$|R
40|$|Abstract. Fiber optic sensors {{dispose of}} some {{advantages}} {{in the field}} of electrical current and magnetic field meas-urement, like large bandwidth, linearity, light transmission possibilities. Unfortunately, they suffer from some parasitic phenomena. The crucial issue is the presence of induced and latent linear birefringence, which is imposed by the fiber manufacture imperfections as well as mechanical stress by fiber bending. In order to the linear birefringence compensation a promising method was chosen for pulsed current sensor design. The method employs orthogonal polarization conjugation by the <b>back</b> direction <b>propagation</b> of the light wave in the fiber. The Jones calculus analysis presents its propriety. An experimental fiber optic current sensor has been designed and realized. The advantage of the proposed method was proved considering to the sensi-tivity improvement...|$|R
50|$|Jürgen Schmidhuber modifies error <b>back</b> <b>propagation</b> {{algorithm}} {{to change}} neural network weights {{in order to}} decrease the mismatch between anticipated states and states actually experienced in the future (Schmidhuber - Adaptive curiosity and adaptive confidence, 1991). He introduces the concept of curiosity for agents {{as a measure of}} the mismatch between expectations and future experienced reality. Agents able to monitor and control their own curiosity explore situations where they expect to engage with novel experiences and are generally able to deal with complex environments more than the others.|$|E
50|$|A {{fundamental}} objection is {{that they}} do not reflect how real neurons function. <b>Back</b> <b>propagation</b> is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.|$|E
50|$|The {{neocognitron}} is a hierarchical, multilayered {{network that}} was {{modeled after the}} visual cortex. It uses multiple types of units, (originally two, called simple and complex cells), as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using <b>back</b> <b>propagation</b> to achieve selective attention. It {{has been used for}} pattern recognition tasks and inspired convolutional neural networks.|$|E
40|$|A main {{criterion}} for {{the accuracy of}} solutions of Artificial Neural Networks (ANNs) for classification tasks is the architecture. In order to find problem-adapted topologies of ANNs, we adopted the evolutionary approach to ANN design by employing a Genetic Algorithm (GA) to evolve ANNs which are represented using a direct encoding method. The role of fitness functions used by the GA is investigated, especially, {{the impact of a}} fitness function expressing both, the learning error and a toplogy-dependent regularization term, is studied. A parallel system [...] the netGEN system [...] which has been implemented by the authors is generating problem [...] adapted Feed [...] Forward ANNs being trained by Error [...] <b>Back</b> [...] <b>Propagation.</b> Empirical results on a real world problem taken from the PROBEN 1 ANN benchmark suite are presented...|$|R
40|$|Independent {{component}} analysis (ICA), applied on low-frequency (LF) events recorded at Stromboli volcano, reveals that these signals {{are composed of}} three independent components in three different frequency bands (approximately 0. 8 – 1. 2, 2. 4 – 3. 0, 3. 2 – 4. 5 Hz, respectively). The first two frequency ranges {{are mainly composed of}} body waves coming from a direction in a range of [30 °, 30 °] around the crater area. This result is a clear indication that the entire signal, in these frequency bands, comes from the source area. In fact, any scattered wave should have different <b>back</b> azimuth <b>propagation,</b> due to the random distribution of scatterers’ location. However, the last independent component is dominated by other kinds of waves, coming from many different directions...|$|R
3000|$|... [...]). After binarization, {{the seeds}} are eroded {{with a small}} 5 × 5 kernel (line 7), which {{eliminates}} isolated positive pixels and shrinks the larger seeds. The shrinking is beneficial as the adapted seeds may be located at {{the boundaries of the}} skin regions. If the propagation is initiated from them, then some background pixels could be misclassified. Naturally, the shrinking eliminates some true-positive skin pixels, but then they are correctly adjoined <b>back</b> during the <b>propagation.</b>|$|R
50|$|The output {{targets in}} the {{response}} units (i.e., the examinee attributes) are compared to the pattern associated with each stimulus input or exemplar (i.e., the expected response patterns). The solution produced initially with the stimulus and association connection weights {{is likely to be}} discrepant resulting in a relatively large error. However, this discrepant result can be used to modify the connection weights thereby leading to a more accurate solution and a smaller error term. One popular approach for approximating the weights so the error term is minimized is with a learning algorithm called the generalized delta rule that is incorporated in a training procedure called <b>back</b> <b>propagation</b> of error.|$|E
5000|$|The interneurons in the {{external}} plexiform layer perform feedback inhibition on the mitral cells to control <b>back</b> <b>propagation.</b> They also participate in lateral inhibition of the mitral cells. This inhibition {{is an important part}} of olfaction as it aids in odor discrimination by decreasing firing in response to background odors and differentiating the responses of olfactory nerve inputs in the mitral cell layer. [...] Inhibition of the mitral cell layer by the other layers contributes to odor discrimination and higher level processing by modulating the output from the olfactory bulb. These hyperpolarizations during odor stimulation shape the responses of the mitral cells to make them more specific to an odor.|$|E
50|$|A {{key feature}} for TDNN’s are {{the ability to}} express a {{relation}} between inputs in time. This relation can {{be the result of}} a feature detector and is used within the TDNN to recognize patterns between the delayed inputs. One of the main advantages of neural networks is the lack of a dependence on prior knowledge to set up the banks of filters at each layer. However, this entails that the network must learn the optimal value for these filters through processing numerous training inputs. Supervised learning is generally the learning algorithm associated with TDNN’s due to its strength in pattern recognition and function approximation. Supervised learning is commonly implemented with a <b>back</b> <b>propagation</b> algorithm.|$|E
40|$|Aim at highly {{nonlinear}} {{mapping relationship}} between the laser processing parameters and the melting cell body’s transverse size，a method of reverse engineering laser melting parameters by <b>back</b> － <b>propagation</b> (BP) neural network was put forward． The model was constructed by BP neural network，and the prediction error was reduced to less than 3 % after training for many times． The DIEVAＲ die steel was melted by reverse engineering laser parameters，and {{the results show that}} the error was 1 ． 33 % between the transverse dimensions of the melting cell body and the expected，the expected precision can be met well． Thermal fatigue property of the melted and non － melted DIEVAＲ die steel has been studied． The analysis about cracks growth presents that thermal fatigue property of DIEVAＲ die steel melted by the reverse engineering parameters has been greatly improved． The melting cell body could block crack effectively...|$|R
40|$|This paper {{presents}} {{a new approach}} that uses neural networks to predict {{the performance of a}} number of dynamic decentralized load balancing strategies. A distributed multicomputer system using any distributed load balancing strategy is represented by a unified analytical queuing model. A large simulation data set is used to train a neural network using the <b>back</b> [...] <b>propagation</b> learning algorithm based on gradient descent. The performance model using the predicted data from the neural network produces the average response time of various load balancing algorithms under various system parameters. The validation and comparison with simulation data show that the neural network is very effective in predicting the performance of dynamic load balancing algorithms. Our work leads to interesting techniques for designing load balancing schemes (for large distributed systems) that are computationally very expensive to simulate. One of the important findings is that performance is affected least by t [...] ...|$|R
40|$|A main {{criterion}} for {{the accuracy of}} solutions of an Artificial Neural Network (ANN) for classification tasks is the architecture. In order to find problem-adapted topologies of ANNs, we adopted the evolutionary approach to ANN design by employing a Genetic Algorithm (GA) to evolve ANNs which are represented using a direct encoding method. As ANNs of low complexity show better generalization capabilities than more complex networks, we incorporated a regularization term into the fitness function which together with the problem representation is determining the preferred regions of the search space the GA will focus on. Especially, we investigated two different regularization terms proposed in literature and experimented with various degrees of impact on the fitness function. The parallel netGEN system which has been implemented by the authors is generating problem [...] adapted Feed [...] Forward ANNs being trained by Error [...] <b>Back</b> [...] <b>Propagation.</b> Empirical results on a real world problem taken from [...] ...|$|R
