6|941|Public
40|$|Dynamic {{programming}} algorithms guarantee to {{find the}} optimal alignment between two sequences. For {{more than a few}} sequences, exact algorithms <b>become</b> <b>computationally</b> <b>impractical,</b> and progressive algorithms iterating pairwise alignments are widely used. These heuristic methods have a serious drawback because pairwise algorithms do not differentiate insertions from deletions and end up penalizing single insertion events multiple times. Such an unrealistically high penalty for insertions typically results in overmatching of sequences and an underestimation of the number of insertion events. We describe a modification of the traditional alignment algorithm that can distinguish insertion from deletion and avoid repeated penalization of insertions and illustrate this method with a pair hidden Markov model that uses an evolutionary scoring function. In comparison with a traditional progressive alignment method, our algorithm infers a greater number of insertion events and creates gaps that are phylogenetically consistent but spatially less concentrated. Our results suggest that some insertion/deletion “hot spots” may actually be artifacts of traditional alignment algorithms...|$|E
40|$|The {{uncertainty}} often {{observed in}} experimental strengths of masonry constituents makes critical {{the selection of}} the appropriate inputs in finite element analysis of complex masonry buildings, as well as requires modelling the building ultimate load as a random variable. On the other hand, the utilization of expensive Monte Carlo simulations to estimate collapse load probability distributions may <b>become</b> <b>computationally</b> <b>impractical</b> when a single analysis of a complex building requires hours of computer calculations. To reduce the computational cost of Monte Carlo simulations, direct computer calculations can be replaced with inexpensive Response Surface (RS) models. This work investigates the use of RS models in Monte Carlo analysis of complex masonry buildings with random input parameters. The accuracy of the estimated RS models, as well as the good estimations of the collapse load cumulative distributions obtained via polynomial RS models, show how the proposed approach could be a useful tool in problems of technical interest...|$|E
40|$|A {{practical}} probabilistic data association filter {{is proposed}} for tracking multiple targets in clutter. The number of joint data association events increases combinatorially {{with the number}} of measurements and the number of targets, which may <b>become</b> <b>computationally</b> <b>impractical</b> for even small numbers of closely located targets in real target-tracking applications in heavily cluttered environments. In this paper, a Markov chain model is proposed to generate a set of feasible joint events (FJEs) for multiple target tracking that is used to approximate the multi-target data association probabilities and the probabilities of target existence of joint integrated probabilistic data association (JIPDA). A Markov chain with the transition probabilities obtained from the integrated probabilistic data association (IPDA) for single-target tracking is designed to generate a random sequence composed of the predetermined number of FJEs without incurring additional computational cost. The FJEs generated are adjusted for the multi-target tracking environment. A computationally tractable set of these random sequences is utilized to evaluate the track-to-measurement association probabilities such that the computational burden is substantially reduced compared to the JIPDA algorithm. By a series of simulations, the track confirmation rates and target retention statistics of the proposed algorithm are compared with the other existing algorithms including JIPDA to show the effectiveness of the proposed algorithm...|$|E
40|$|Optimal weapon target {{assignment}} problem involves NP-complete searching process and <b>becomes</b> <b>computationally</b> <b>impractical</b> {{as the number}} of weapons and targets increases. Existing approaches, therefore, only consider approximate method with heuristic searching scenarios, which are, however, no theoretical performance guarantee for the level of accuracy that the underlying algorithm may achieve. In this paper, the weapon {{target assignment}} problem is studied in the framework of combinatorial optimization theory. Following a previous work, an accelerated continuous greedy algorithm is proposed to address the underlying problem in polynomial time. The algorithm is proved to have the best guaranteed performance against optimal solution among the existing polynomial time methods...|$|R
40|$|Active sensing {{enables a}} sensor to {{optimize}} its tunings on-the-fly {{based on information}} obtained from previous measurements. When applied to networks of distributed sensors, however, active sensing <b>becomes</b> <b>computationally</b> <b>impractical</b> due to the combinatorial number of sensing configurations. To address this problem, we present an active decomposition and sensing (ADS) method that combines the advantages of classifier decomposition with those of active sensing. Namely, we use class posteriors to decompose the problem across the sensors in the network. Each sensor then applies active sensing to select the next tuning to solve its specific subproblem. As a result, the method scales linearly (rather than combinatorially) {{with the number of}} sensors. We validate ADS on a database of infrared absorption spectroscopy containing 50 chemicals. Our results show that active decomposition improves classification performance and reduces sensing costs when compared to using active sensing only at the node level. Index terms – Active sensing, chemical sensing, sensor networks. 1...|$|R
40|$|Recent {{research}} in multi-robot exploration and mapping {{has focused on}} sampling environmental fields, which are typically modeled using the Gaussian process (GP). Existing information-theoretic exploration strategies for learning GP-based environmental field maps adopt the non-Markovian problem structure and consequently scale poorly with the length of history of observations. Hence, it <b>becomes</b> <b>computationally</b> <b>impractical</b> to use these strategies for in situ, real-time active sampling. To ease this computational burden, this paper presents a Markov-based approach to efficient information-theoretic path planning for active sampling of GP-based fields. We analyze the time complexity of solving the Markov-based path planning problem, and demonstrate analytically that it scales better than that of deriving the non-Markovian strategies with increasing length of planning horizon. For a class of exploration tasks called the transect sampling task, we provide theoretical guarantees on the active sampling performance of our Markov-based policy, from which ideal environmental field conditions and sampling task settings can be established to limit its performance degradation due to violation of the Markov assumption. Empirical evaluation on real-world temperature and plankton density field data shows that our Markov-based policy can generally achieve active sampling performance {{comparable to that of}} the widely-used non-Markovian greedy policies under less favorable realistic field conditions and task settings while enjoying significant computational gain over them. Comment: 10 th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2011), Extended version with proofs, 11 page...|$|R
40|$|AbstractPattern {{classification}} methods assign {{an object}} {{to one of}} several predefined classes/categories based on features extracted from observed attributes of the object (pattern). When L discriminatory features for the pattern can be accurately determined, the pattern classification problem presents no difficulty. However, precise identification of the relevant features for a classification algorithm (classifier) {{to be able to}} categorize real world patterns without errors is generally infeasible. In this case, the pattern classification problem is often cast as devising a classifier that minimizes the misclassification rate. One way of doing this is to consider both the pattern attributes and its class label as random variables, estimate the posterior class probabilities for a given pattern and then assign the pattern to the class/category for which the posterior class probability value estimated is maximum. More often than not, the form of the posterior class probabilities is unknown. The so-called Parzen Window approach is widely employed to estimate class-conditional probability (class-specific probability) densities for a given pattern. These probability densities can then be utilized to estimate the appropriate posterior class probabilities for that pattern. However, the Parzen Window scheme can <b>become</b> <b>computationally</b> <b>impractical</b> when the size of the training dataset is {{in the tens of thousands}} and L is also large (a few hundred or more). Over the years, various schemes have been suggested to ameliorate the computational drawback of the Parzen Window approach, but the problem still remains outstanding and unresolved. In this paper, we revisit the Parzen Window technique and introduce a novel approach that may circumvent the aforementioned computational bottleneck. The current paper presents the mathematical aspect of our idea. Practical realizations of the proposed scheme will be given elsewhere...|$|E
40|$|The advancements in Whole Genome Sequencing (WGS) have {{increased}} the amount of genomic information available for epidemiological analyses. WGS opens many avenues for investigation into the tracking of pathogens, but the rapid advancements in WGS could soon lead to a situation where traditional analytical techniques might <b>become</b> <b>computationally</b> <b>impractical.</b> For example, the traditional method to determine the origin of an isolate is to use phylogenetic analyses. However, phylogenetic analyses become computationally prohibitive with larger datasets and are best for retrospective epidemiology. Therefore, I investigated if there might be less computationally demanding methods of analysing the same data to obtain similar conclusions. This thesis describes a proof-of-principle method for evaluating if such alternative analysis techniques might be viable. In this thesis Methicillin resistant Staphylococcus aureus (MRSA) was used, and single nucleotide polymorphism (SNP) and insertion/deletion (indel) genomic variation. I move away from whole genome analysis techniques, such as phylogenetic analysis, and instead focus on individual SNPs. I showed that genetic signals (such as SNPs and indels) can be utilised in novel ways to rapidly produce {{a summary of the}} possible geographic origin of an isolate with a minimal demand on computational power. The methods described could be added to the suite of analytical epidemiological tools and are a promising indication of the viability of developing cheap, rapid diagnostic tools to be implemented in healthcare institutions. Furthermore, the principles behind the development of the methods described in this thesis could have much wider applications than just MRSA. This implies that further work based on the principles described in this thesis on alternative pathogens could prove to be promising avenues of investigation...|$|E
40|$|We {{thank the}} BBSRC for funding this {{research}} through grant BB/I 00596 X/ 1. JBOM thanks the Scottish Universities Life Sciences Alliance (SULSA) for financial support. Pattern classiﬁcation methods assign an object {{to one of}} several predeﬁned classes/categories based on features extracted from observed attributes of the object (pattern). When L discriminatory features for the pattern can be accurately determined, the pattern classiﬁcation problem presents no difﬁculty. However, precise identiﬁcation of the relevant features for a classiﬁcation algorithm (classiﬁer) to able to categorize real world patterns without errors is generally infeasible. In this case, the pattern classiﬁcation problem is often cast as devising a classiﬁer that minimises the misclassiﬁcation rate. One way of doing this is to consider both the pattern attributes and its class label as random variables, estimate the posterior class probabilities for a given pattern and then assign the pattern to class/category for which the posterior class probability value estimated is maximum. More often than not, {{the form of the}} posterior class probabilities is unknown. The so-called Parzen Window approach is widely employed to estimate class-conditional probability (class-speciﬁc probability) densities a given pattern. These probability densities can then be utilised to estimate the appropriate posterior class probabilities for that pattern. However, the Parzen Window scheme can <b>become</b> <b>computationally</b> <b>impractical</b> when the size of the training dataset is {{in the tens of thousands}} and L is also large few hundred or more). Over the years, various schemes have been suggested to ameliorate the computational drawback of the Parzen Window approach, but the problem still remains outstanding and unresolved. In this paper, we revisit the Parzen Window technique and introduce a novel approach that may circumvent the aforementioned computational bottleneck. The current paper presents the mathematical aspect of our idea. Practical realizations of the proposed scheme will be given elsewhere. Publisher PDFPeer reviewe...|$|E
40|$|Exact optimal state {{estimation}} for discrete-time Boolean dy-namical systems may <b>become</b> <b>impractical</b> <b>computationally</b> if system dimensionality is large. In this paper, we consider a particle filtering approach {{to address this}} problem. The methodology is illustrated through application to state track-ing in high-dimensional Boolean network models. The re-sults show that the particle filter can be very accurate under a moderate number of particles. The impact of resampling on performance is also investigated...|$|R
30|$|Correct {{interpretation}} of measured planetary characteristics for interior models {{relies on the}} understanding of high-pressure and high-temperature phase mineralogy. With the use of physical and chemical laws which govern the crystal structure of minerals, e.g. thermodynamic relations and density functional theory, ambient-condition crystalline properties can be extrapolated to high pressures and high-pressure phases can be predicted (e.g. [15, 16]). However, such methods <b>become</b> <b>computationally</b> expensive and <b>impractical</b> for systems containing numerous elements, and have difficulties in predicting material properties in the high-temperature regime of deep planetary interiors.|$|R
40|$|Learned image {{features}} can provide great accuracy in many Computer Vision tasks. However, when the convolution filters used to learn {{image features}} are numerous and not separable, feature extraction <b>becomes</b> <b>computationally</b> de-manding and <b>impractical</b> {{to use in}} real-world situations. In this thesis work, a method for learning {{a small number of}} separable filters to approximate an arbitrary non-separable filter bank is developed. In this approach, separable filters are learned by grouping the arbitrary filters into a tensor and opti-mizing a tensor decomposition problem. The separable filter learning with tensor decomposition is general and can be applied to generic filter banks to reduce the computational burden of convolutions without a loss in perfor-mance. Moreover, the proposed approach is orders of magnitude faster than the approach of a very recent paper based on ` 1 -norm minimization [34]. Acknowledgements I would like to express my deepest gratitude to my supervisor, Prof. Pasca...|$|R
50|$|Every extensive-form {{game has}} an {{equivalent}} normal-form game, however the transformation to normal form {{may result in}} an exponential blowup {{in the size of}} the representation, making it <b>computationally</b> <b>impractical.</b>|$|R
5000|$|Bruce Schneier {{notes that}} even <b>computationally</b> <b>impractical</b> attacks can be {{considered}} breaks: [...] "Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions {{would be considered a}} break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised." ...|$|R
40|$|Abstract—This paper {{addresses}} the space–time code design for Rayleigh-fading channels using continuous phase modulation (CPM). General code construction is desirable {{due to the}} nonlinearity and inherent memory in CPM signals which make hand design or computer search <b>computationally</b> <b>impractical.</b> Several sufficient conditions for full rank are identified for CPM signaling schemes and for linear representations of CPM modulation. Simulation results verify the resulting performance. Index Terms—Continuous phase modulation (CPM), linear decomposition, rank criterion, Rayleigh fading, space–time code. I...|$|R
40|$|Total system {{performance}} assessment requires the explicit quantification {{of all the}} relevant processes and process interactions. However, process level descriptions of all the processes required {{for the evaluation of}} the total {{system performance}} is <b>computationally</b> <b>impractical,</b> thus requiring the abstraction of these process level models. In this paper the unsaturated flow abstraction methodology developed for the current iteration of the total system performance assessment for the potential repository at Yucca Mountain is presented along with a simple test problem...|$|R
40|$|Optimal channel {{allocation}} with {{message priority}} is formulated as a mathematical programming problem, {{and it is}} shown that the exact solution is <b>computationally</b> <b>impractical</b> because of the combinatorial nature of the problem. A heuristic algorithm that is computationally very efficient is then developed to determine the optimal number of channels allocated to each link and the maximum user input rates with message priority for packetized traffic. Message priority is considered in the input rate flow control strategy, and a discrete number of channels {{is used in the}} optimization process. link_to_subscribed_fulltex...|$|R
40|$|A {{method of}} vehicle routing and {{scheduling}} for an air based intraurban transportation system is developed. The maximization of {{level of service}} to passengers in a system operating under time varying demand is considered on both the optimal and heuristic levels. It is shown that while the determination of an optimal schedule is mathematically feasible, it is <b>computationally</b> <b>impractical.</b> Heuristic vehicle control algorithms are developed and tested using computer simulation. It is shown that, as compared to fixed routing strategies, dynamic vehicle routing strategies provide a greater level of service to passengers while substantially reducing the direct operating costs of the system. I...|$|R
40|$|Linear mixed {{models have}} {{attracted}} considerable recent attention {{as a powerful}} and effective tool for accounting for population stratification and relatedness in genetic association tests. However, existing methods for exact computation of standard test statistics are <b>computationally</b> <b>impractical</b> for even moderate-sized genome-wide association studies. To deal with this several approximate methods have been proposed. Here, we present an efficient exact method that makes these approximations unnecessary in many settings. This method is roughly n {{times faster than the}} widely-used exact method EMMA, where n is the sample size, making exact genome-wide association analysis computationally practical for large numbers of individuals...|$|R
40|$|The exact (Mie) {{theory for}} the {{scattering}} {{of a plane}} wave by a dielectric sphere is presented. Since this infinite series solution is <b>computationally</b> <b>impractical</b> for large spheres, another formulation is given {{in terms of an}} integral equation valid for a bounded, but otherwise general array of scatterers. This equation is applied to the scattering by a single sphere, and several methods are suggested for approximating the scattering cross section in closed form. A tensor scattering matrix is introduced, in terms of which some general scattering theorems are derived. The application of the formalism to multiple scattering is briefly considered...|$|R
5000|$|So {{here the}} number {{operator}} [...] is normal ordered {{in the usual}} sense used {{in the rest of}} the article yet its thermal expectation values are non-zero. Applying Wick's theorem and doing calculation with the usual normal ordering in this thermal context is possible but <b>computationally</b> <b>impractical.</b> The solution is to define a different ordering, such that the [...] and [...] are linear combinations of the original annihilation and creations operators. The combinations are chosen to ensure that the thermal expectation values of normal ordered products are always zero so the split chosen will depend on the temperature.|$|R
5000|$|The empty {{allocation}} {{is always}} EF. But {{if we want}} some efficiency in addition to EF, then the decision problem <b>becomes</b> <b>computationally</b> hard: ...|$|R
40|$|One of {{the most}} widely used methods for solving average cost MDP {{problems}} is the value iteration method. This method, however, is often <b>computationally</b> <b>impractical</b> and restricted in size of solvable MDP problems. We propose acceleration operators that improve the performance of the value iteration for average reward MDP models. These operators are based on two important properties of Markovian operator: contraction mapping and monotonicity. It is well known that the classical relative value iteration methods for average cost criteria MDP do not involve the max-norm contraction or monotonicity property. To overcome this difficulty we propose to combine acceleration operators with variants of value iteration for stochastic shortest path problems associated average reward problems. Comment: 34 pages, 3 figure...|$|R
40|$|This paper {{summarizes}} 10 {{years of}} NURC {{experience with the}} application of a state-of-the-art FE code [Zampolli et al., JASA 122, 1472 - 85 (2007) ] to propagation and scattering problems in ocean acoustics. We show benchmark results for low-frequency propagation in a range-dependent waveguide with an elastic bottom, and object scattering results for spheres and cylinders placed near the seafloor. Despite the computational burden associated with the FE method, its generality in treating propagation in layered fluid-elastic media of complex geometry without theoretical approximations makes it very attractive for acoustic benchmarking. And, as experience shows, what is <b>computationally</b> <b>impractical</b> today, will be easily done on a desktop computer 10 years from now. © 2010 American Institute of Physics...|$|R
5000|$|While GPS solved simple {{problems}} such as the Towers of Hanoi that could be sufficiently formalized, it could not solve any real-world problems because search was easily lost in the combinatorial explosion. Put another way, the number of [...] "walks" [...] through the inferential digraph <b>became</b> <b>computationally</b> untenable. (In practice, even a straightforward state space search such as the Towers of Hanoi can <b>become</b> <b>computationally</b> infeasible, albeit judicious prunings of the state space {{can be achieved by}} such elementary AI techniques as alpha-beta pruning and min-max.) ...|$|R
3000|$|Using {{the sparse}} channel model {{described}} in Section 2, {{we are able}} to incorporate prior information about channel sparsity into the path metric of the SA. However, the computation of m (b_ 1 ^n) [...] in (13) considers all possible realizations of the vector d. Without simplification, we would need to evaluate and sum i̇ 2 ^L_h exponential terms, which is <b>computationally</b> <b>impractical</b> for channels with long delay spread. For a given sparse channel, the sparsity vector d {{is just one of the}} i̇ 2 ^L_h possible binary vectors. Therefore, if the channel sparsity can be determined, the path metric can be computed for a unique vector d. In order to achieve this, we propose a computationally efficient sparsity estimation technique, described in the following section.|$|R
40|$|Optimal Bayesian multi-target {{filtering}} is, in general, <b>computationally</b> <b>impractical</b> {{owing to}} the high dimensionality of the multi-target state. The Probability Hypothesis Density (PHD) filter propagates the first moment of the multi-target posterior distribution. While this reduces the dimensionality of the problem, the PHD filter still involves intractable integrals in many cases of interest. Several authors have proposed Sequential Monte Carlo (SMC) implementations of the PHD filter. However, these implementations are {{the equivalent of the}} Bootstrap Particle Filter, and the latter is well known to be inefficient. Drawing on ideas from the Auxiliary Particle Filter (APF), we present a SMC implementation of the PHD filter which employs auxiliary variables to enhance its efficiency. Numerical examples are presented for two scenarios, including a challenging nonlinear observation model...|$|R
40|$|Abstract. There is {{increasing}} interest in applying spectral clustering (SC) algorithms to classification problems in medical imaging. These techniques model pair-wise voxel similarity relationships to generate feature eigenvectors which capture complex clustering structure but naı̈ve implementations are <b>computationally</b> <b>impractical</b> for real ap-plications. In previous work we described {{a more efficient}} approach to SC using stochastic sampling and sparse matrix methods. An alternative stochastic sampling approach, the Nyström Extension, solves a reduced eigenvector problem and applies a formal interpolation strategy to generate spectral features from the reduced eigenvector set. In this paper we show that these methods are distinct and have different properties. We compare and contrast SC using both approaches applied to classification problems in simulated and real medical images. ...|$|R
3000|$|As an aside, {{note that}} due to the product in the hypoexponential (see Eq.  2) {{determination}} of the probabilities for large N can <b>become</b> <b>computationally</b> intractable. For simulations larger than with [...]...|$|R
30|$|Maximum {{likelihood}} (ML) decoding is <b>computationally</b> <b>impractical</b> for SC-FDE {{systems for}} commonly used block sizes; thus, minimum {{mean square error}} (MMSE)-based equalization is commonly used. SC-FDE systems can also use a decision-feedback equalizer (DFE) to improve their error performance. A decision-feedback equalizer uses the previously detected symbols to reduce the postcursor intersymbol interference (ISI), which is left behind by the common MMSE equalizer. An efficient DFE structure for SC-FDE systems, using a linear frequency-domain feedforward filter and a time-domain feedback filter, was proposed in[3]. Since the feedforward filter realizes its operations in the frequency domain, its computational complexity is lower when compared to a purely time-domain DFE. In SC-FDE systems using a DFE, the effect that the error propagation can cause is limited to one symbol block, since the equalizer operates on a per-block basis.|$|R
40|$|Abstract — In this note, {{improvements}} to the non-local means image denoising method introduced in [2], [3] are presented. The original non-local means method replaces a noisy pixel by the weighted average of pixels with related surrounding neighborhoods. While producing state-of-the-art denoising results, this method is <b>computationally</b> <b>impractical.</b> In order to accelerate the algorithm, we introduce filters that eliminate unrelated neighborhoods from the weighted average. These filters are based on local average gray values and gradients, pre-classifying neighborhoods and thereby reducing the original quadratic complexity to a linear one and reducing the influence of less-related areas in the denoising of a given pixel. We present the underlying framework and experimental results for gray level and color images {{as well as for}} video. Index Terms- Image and video denoising, non-local neighborhood filters, contexts, computational complexity. I...|$|R
40|$|Optimal Bayesian multi-target {{filtering}} is, in general, <b>computationally</b> <b>impractical</b> due to {{the high}} dimensionality of the multi-target state. Recently Mahler, [9], introduced a filter which propagates the first moment of the multi-target posterior distribution, which he called the Probability Hypothesis Density (PHD) filter. While this reduces the dimensionality of the problem, the PHD filter still involves intractable integrals in many cases of interest. Several authors have proposed Sequential Monte Carlo (SMC) implementations of the PHD filter. However, these implementations are {{the equivalent of the}} Bootstrap Particle Filter, and the latter is well known to be inefficient. Drawing on ideas from the Auxiliary Particle Filter of Pitt and Shephard [10], we present a SMC implementation of the PHD filter which employs auxiliary variables to enhance its efficiency. Numerical examples are also presented. 1...|$|R
40|$|Harshman and Lundy (Comput Stat. Data AnaL 1994; 18 : 39 - 72) {{described}} an {{option in the}} Parafac algorithm for constraining one or more component matrices to have uncorrelated columns. In cases where an algorithm using this constraint {{is to be applied}} to a three-way array that has too many entries in one mode, the algorithm is <b>computationally</b> <b>impractical.</b> The present paper describes two modifications of their algorithm to handle the multiplications involving the large mode efficiently for cases where the product of the sizes of the other two modes is smaller than that of the larger mode. By means of a simulation study, it is demonstrated that these procedures (which are similarly efficient) are much more efficient than the original algorithm. Copyright (C) 2009 John Wiley & Sons, Ltd...|$|R
40|$|In this paper, Isomap and kernel Isomap {{are used}} to {{dramatically}} reduce the dimensionality of the output space to efficiently construct a Gaussian process emulator of parametrized partial differential equations. The output space consists of spatial or spatio-temporal fields that are functions of multiple input variables. For such problems, standard multi-output Gaussian process emulation strategies are <b>computationally</b> <b>impractical</b> and/or make restrictive assumptions regarding the correlation structure. The method we develop can be applied without modification to any problem involving vector-valued targets and vector-valued inputs. It also extends a method based on linear dimensionality reduction to response surfaces that cannot be described accurately by a linear subspace of the high dimensional output space. Comparisons to the linear method are made through examples that clearly demonstrate the advantages of nonlinear dimensionality reduction. ...|$|R
40|$|A {{statistical}} emulator of a high-fidelity {{computer model}} {{is based on}} the application of machine learning algorithms to input-output data generated by the model at selected design points. Applications include real-time control, design optimization and inverse parameter estimation. In many of these applications, the outputs are spatial or spatio-temporal fields. In such cases, standard emulation methods are <b>computationally</b> <b>impractical</b> due to the curse of dimensionality, or are limited in their applicability by simplifying assumptions in relation to the correlation structure. In this work, we combine linear and nonlinear dimensionality reduction with artificial neural networks to develop an efficient approach to emulating high-dimensional spatio-temporal models, without making ad hoc assumptions regarding correlations. The approach is tested on models of electromagnetic wave propagation. The necessity of nonlinear dimensionality reduction is highlighted...|$|R
30|$|Statistical {{inference}} {{provides an}} elegant framework {{to deal with}} this problem, but these methods are usually intended for single instrument f 0 estimation (typically piano), as exact inference often <b>becomes</b> <b>computationally</b> intractable for complex and very different sources.|$|R
