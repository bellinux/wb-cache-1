6|42|Public
5000|$|... <b>batch</b> <b>search</b> for {{multiple}} queries available in 'BatchSearch' mode, e.g., acetaminophen, isatin, aspirin ...|$|E
40|$|We {{propose a}} method called {{adaptive}} multiple feature subset (AMFES), which ranks and selects features {{at a reasonable}} computation cost. In the AMFES ranking procedure, we conduct an iterative process. At the initial stage, we compute features ’ strength (i. e., degree of usefulness) based on various subsets drawn from the pool of all features. We then rank features according to their strength thus derived. At each subsequent stage, we input half of features that were top-ranked from the previous stage. We then re-rank {{them in the same}} fashion as we do at the first stage. In the AMFES selection procedure, we conduct a sequential <b>batch</b> <b>search</b> to tremendously reduce the computation cost. Compared with a few other methods, we show by experiments that AMFES achieves higher or comparable test accuracy rates. When achieving comparable rates, AMFES selects a smaller number of features. We argue that the employment of multiple feature subsets can diminish the ill effect of feature correlation, which explains the advantage of AMFES over other methods on the experimental data sets...|$|E
40|$|Biological protein-protein {{interactions}} {{differ from}} the more general class of physical interactions: in a biological interaction, both proteins must be in their proper states (e. g. covalently modified state, conformational state, cellular location state, etc.). Also in every biological interaction, one or both interacting molecules undergo a transition to a new state. This regulation of protein states through protein-protein interactions underlies many dynamic biological processes inside cells. Therefore, understanding biological interactions requires information on protein states. Toward this goal, DIP (the Database of Interacting Proteins) has been expanded to LiveDIP, which describes protein interactions by protein states and state transitions. This additional level of characterization permits a more complete picture of the protein-protein interaction networks and is crucial to an integrated understanding of genome-scale biology. The search tools provided by LiveDIP, Pathfinder and <b>Batch</b> <b>Search,</b> allow users to assemble biological pathways from all the protein-protein interactions collated from the scientific literature in LiveDIP. Tools have also been developed to integrate the protein-protein interaction networks of LiveDIP with large-scale genomic data such as microarray data. An example of these tools applied to analyzing the pheromone response pathway in yeast suggests that the pathway functions {{in the context of}} a complex protein-protein interaction network. Seven out of the eleven proteins involved in signal transduction are under negative or positive regulation of up to five other proteins through biological protein-protein interactions. During pheromone response, the mRNA expression levels of thes...|$|E
40|$|A {{number of}} {{previous}} studies derived expressions for <b>batched</b> <b>searching</b> of sequential and tree-structured files {{on the assumption that}} all the keys in the batch exist in the file, i. e., all the searches are successful. Formulas for <b>batched</b> <b>searching</b> of sequential and tree-structured files are derived, but the assumption made is that either {{all or part of the}} keys i...|$|R
40|$|The {{performance}} of <b>batched</b> <b>search</b> {{when applied to}} index sequential files is studied. Analysis provides exact formulae {{for the cost of}} <b>searching</b> when <b>batching</b> is applied {{as a function of the}} magnitude of the query and time. The analysis takes into account (a) accesses to the overflow area only, and (b) accesses to both primary and overflow area, as well as whether the records of the query are (a) distinct, or (b) nondistinct. <b>Batched</b> <b>search</b> is compared with the simple on-line search and a proposal is stated concerning the reorganization of index sequential files...|$|R
40|$|The {{technique}} of <b>batching</b> <b>searches</b> has been {{ignored in the}} context of disk based online data retrieval systems. This paper suggests that batching be reconsidered for such systems since the potential reduct. ion in processor demand may actually reduce response time. An analysis with sample numerical results and algorithms is presented...|$|R
40|$|Abstract Background Public {{databases}} {{are crucial}} {{for analysis of}} high-dimensional gene and protein expression data. The Urologic Epithelial Stem Cells (UESC) database [URL] is a public database that contains gene and protein information for the major cell types of the prostate, prostate cancer cell lines, and a cancer cell type isolated from a primary tumor. Similarly, such information is available for urinary bladder cell types. Description Two major data types were archived in the database, protein abundance localization data from immunohistochemistry images, and transcript abundance data principally from DNA microarray analysis. Data results were organized in modules that were made to operate independently but built upon a core functionality. Gene array data and immunostaining images for human and mouse prostate and bladder were made available for interrogation. Data analysis capabilities include: (1) CD (cluster designation) cell surface protein data. For each cluster designation molecule, a data summary allows easy retrieval of images (at multiple magnifications). (2) Microarray data. Single gene or <b>batch</b> <b>search</b> can be initiated with Affymetrix Probeset ID, Gene Name, or Accession Number together with options of coalescing probesets and/or replicates. Conclusion Databases are invaluable for biomedical research, and their utility depends on data quality and user friendliness. UESC provides for database queries and tools to examine cell type-specific gene expression (normal vs. cancer), whereas most other databases contain only whole tissue expression datasets. The UESC database provides a valuable tool {{in the analysis of}} differential gene expression in prostate cancer genes in cancer progression. </p...|$|E
40|$|Background: Public {{databases}} {{are crucial}} {{for analysis of}} high-dimensional gene and protein expression data. The Urologic Epithelial Stem Cells (UESC) database [URL] is a public database that contains gene and protein information for the major cell types of the prostate, prostate cancer cell lines, and a cancer cell type isolated from a primary tumor. Similarly, such information is available for urinary bladder cell types. Description: Two major data types were archived in the database, protein abundance localization data from immunohistochemistry images, and transcript abundance data principally from DNA microarray analysis. Data results were organized in modules that were made to operate independently but built upon a core functionality. Gene array data and immunostaining images for human and mouse prostate and bladder were made available for interrogation. Data analysis capabilities include: (1) CD (cluster designation) cell surface protein data. For each cluster designation molecule, a data summary allows easy retrieval of images (at multiple magnifications). (2) Microarray data. Single gene or <b>batch</b> <b>search</b> can be initiated with Affymetrix Probeset ID, Gene Name, or Accession Number together with options of coalescing probesets and/or replicates. Conclusion: Databases are invaluable for biomedical research, and their utility depends on data quality and user friendliness. UESC provides for database queries and tools to examine cell typespecific gene expression (normal vs. cancer), whereas most other databases contain only whole tissue expression datasets. The UESC database provides a valuable tool {{in the analysis of}} differential gene expression in prostate cancer genes in cancer progression. This work was supported by grant 1 U 01 DK 63630 from NIDDK. Additional funding came from grants CA 85859, CA 98699 and CA 111244 from NCI...|$|E
40|$|Abstract Background Modern biology {{has shifted}} from "one gene" {{approaches}} to methods for genomic-scale analysis like microarray technology, which allow simultaneous measurement of thousands of genes. This has created a need for tools facilitating interpretation of biological data in "batch" mode. However, such tools often leave the investigator with large volumes of apparently unorganized information. To meet this interpretation challenge, gene-set, or cluster testing has become a popular analytical tool. Many gene-set testing methods and software packages are now available, most of which {{use a variety of}} statistical tests to assess the genes in a set for biological information. However, the field is still evolving, and there is a great need for "integrated" solutions. Results GeneTools is a web-service providing access to a database that brings together information from a broad range of resources. The annotation data are updated weekly, guaranteeing that users get data most recently available. Data submitted by the user are stored in the database, where it can easily be updated, shared between users and exported in various formats. GeneTools provides three different tools: i) NMC Annotation Tool, which offers annotations from several databases like UniGene, Entrez Gene, SwissProt and GeneOntology, in both single- and <b>batch</b> <b>search</b> mode. ii) GO Annotator Tool, where users can add new gene ontology (GO) annotations to genes of interest. These user defined GO annotations can be used in further analysis or exported for public distribution. iii) e GOn, a tool for visualization and statistical hypothesis testing of GO category representation. As the first GO tool, e GOn supports hypothesis testing for three different situations (master-target situation, mutually exclusive target-target situation and intersecting target-target situation). An important additional function is an evidence-code filter that allows users, to select the GO annotations for the analysis. Conclusion GeneTools is the first "all in one" annotation tool, providing users with a rapid extraction of highly relevant gene annotation data for e. g. thousands of genes or clones at once. It allows a user to define and archive new GO annotations and it supports hypothesis testing related to GO category representations. GeneTools is freely available through www. genetools. no</p...|$|E
40|$|In a {{previous}} study an ordered array o /N keys was considered {{and the problem of}} locating a batch ofM requested keys was investigated by assuming both batched sequential and <b>batched</b> binary <b>searching.</b> This paper introduces the idea of <b>batched</b> interpolation <b>search,</b> and two variations of the method are presented. Comparisons with the two previously defined methods are also made...|$|R
50|$|Online {{search is}} the process of interactively searching for and {{retrieving}} requested information via a computer from databases that are online. Interactive searches became possible in the 1980s with the advent of faster databases and smart terminals. In contrast, computerized <b>batch</b> <b>searching</b> was prevalent in the 1960s and 1970s. Today, searches through web search engines constitute the majority of online searches.|$|R
40|$|For <b>batched</b> <b>searching</b> of a B + -tree file, {{previous}} formulas which {{estimate the}} number of block accesses fail when the blocking factor at the leaf level is less than one. We overcome this weakness by developing both an exact formula and an approximate formula to estimate {{the number of}} block accesses in a batched searchofaB + -tree file where the blocking factor at the leaf level may be less than one...|$|R
40|$|In the TREC- 8 Interactive Track, {{our results}} {{indicated}} that the better performance obtained in <b>batch</b> <b>searching</b> evaluation do not translate into better performance by users in an instance recall task. This year we pursued this investigation further by performing the same experiments using the new questionanswering task adopted in the TREC- 9 Interactive Track. Our results once again show that better performance in <b>batch</b> <b>searching</b> evaluation does not translate into gains for real users. A continuing unanswered question in information retrieval (IR) research is whether <b>batch</b> and user <b>searching</b> evaluations give the same results. We explored this question in the TREC- 8 Interactive Track, where we found that the better results obtained in batch studies using the Okapi weighting scheme over the standard TFIDF approach did not accrue to real users for an instance recall task. [1] This work was limited by the small number of queries as well as the use of a single retrieval task, the recall of specific instances for a topic. Since the TREC- 9 Interactive Track would be using a different task- questionanswering- we decided to use the same research question again with this changed task. Although we would still have a small number of queries, it would provide another IR task to assess this research question. As with the TREC- 8 Interactive Track we performed three experiments. The first experiment was to identify an IR approach that achieved the best possible performance in the batch environment. In th...|$|R
40|$|Abstract. We propose {{two general}} {{heuristics}} to transform a <b>batch</b> Hillclimbing <b>search</b> into an incremental one. Then, we apply our heuristics to two Bayesian network structure learning algorithms and experimentally see that our incremental approach saves {{a significant amount}} of computing time while it yields similar networks than the batch algorithms. ...|$|R
40|$|Currently an {{abundance}} of historical manuscripts, journals, and scientific notes remain largely unaccessible in library archives. Manual transcription and publication of such documents is unlikely, and automatic transcription with high enough accuracy to support a traditional text search is difficult. In this work we describe a lexicon-free system for performing text queries on off-line printed and handwritten Arabic documents. Our segmentation-based approach utilizes gHMMs with a bigram letter transition model, and KPCA/LDA for letter discrimination. The segmentation stage is integrated with inference. We show that our method is robust to varying letter forms, ligatures, and overlaps. Additionally, we find that ignoring letters beyond the adjoining neighbors has little effect on inference and localization, {{which leads to a}} significant performance increase over standard dynamic programming. Finally, we discuss an extension to perform <b>batch</b> <b>searches</b> of large word lists for indexing purposes. 1...|$|R
40|$|Abstract Background Cancer is {{a disease}} of genome {{alterations}} that arise through the acquisition of multiple somatic DNA sequence mutations. Some of these mutations can be critical {{for the development of}} a tumor and can be useful to characterize tumor types or predict outcome. Description We have constructed an integrated biological information system termed the Roche Cancer Genome Database (RCGDB) combining different human mutation databases already publicly available. This data is further extended by hand-curated information from publications. The current version of the RCGDB provides a user-friendly graphical interface that gives access to the data in different ways: (1) Single interactive search by genes, samples, cell lines, diseases, as well as pathways, (2) <b>batch</b> <b>searches</b> for genes and cell lines, (3) customized searches for regularly occurring requests, and (4) an advanced query interface enabling the user to query for samples and mutations by various filter criteria. Conclusion The interfaces of the presented database enable the user to search and view mutations in an intuitive and straight-forward manner. The database is freely accessible at [URL]. </p...|$|R
40|$|Query {{processing}} is {{a crucial}} component of various application domains including information retrieval, database design and management, pattern recognition, robotics, and VLSI. Many of these applications involve data stored in a matrix satisfying a number of properties. One property that occurs time and again specifies that the rows and the columns of the matrix are independently sorted. It is customary to refer to such a matrix as sorted. An instance of the <b>Batched</b> <b>Searching</b> and Ranking problem, (BSR, for short) involves a sorted matrix A of items from a totally ordered universe, along with a collection Q of queries. Q is an arbitrary mix of the following query types: for a search query q j one is interested in an item of A that is closest to q j; for a rank query q j one {{is interested in the}} number of items of A that are strictly smaller than q j. The BSR problem asks for solving all queries in Q. In this work, we consider the BSR problem in the following context: the matrix A is p [...] ...|$|R
40|$|Background: Cancer is {{a disease}} of genome {{alterations}} that arise through the acquisition of multiple somatic DNA sequence mutations. Some of these mutations can be critical {{for the development of}} a tumor and can be useful to characterize tumor types or predict outcome. Description: We have constructed an integrated biological information system termed the Roche Cancer Genome Database (RCGDB) combining different human mutation databases already publicly available. This data is further extended by hand-curated information from publications. The current version of the RCGDB provides a user-friendly graphical interface that gives access to the data in different ways: (1) Single interactive search by genes, samples, cell lines, diseases, as well as pathways, (2) <b>batch</b> <b>searches</b> for genes and cell lines, (3) customized searches for regularly occurring requests, and (4) an advanced query interface enabling the user to query for samples and mutations by various filter criteria. Conclusion: The interfaces of the presented database enable the user to search and view mutations in an intuitive and straight-forward manner. The database is freely accessible a...|$|R
40|$|The author {{looks at}} {{the context of the}} {{articles}} published in World Patent Information's first volume, in 1979, in the light of subsequent developments over the ensuing 25 years. He provides examples of articles from 1979 and from recent years to illustrate his comments. Amongst themes that continue to be significant throughout this period, are patent classification schemes, patent information for techno-commercial intelligence and policy analysis, and the less than optimal take up of patent information by actual and potential users. By contrast, the author notes that microfilm, punch cards and <b>batch</b> computer <b>searching,</b> key themes of 1979, have given way to Internet technology. In conclusion he emphasises both the massive changes brought about by technology changes and the substantially unchanging situation in many other respects, for example in the major players in the provision of patent information databases and in the importance of effective patent searching. Historical context World Patent Information Patent classification schemes Patent searching Commercial intelligence Policy analysis Microfilm Punch cards <b>Batch</b> computer <b>searching</b> Internet technology Patent information providers...|$|R
25|$|The dbSNP can {{be searched}} using the Entrez SNP search tool (found at https://www.ncbi.nlm.nih.gov/projects/SNP/). A variety of queries {{can be used}} for searching: an ss number ID, a refSNP number ID, a gene name, an {{experimental}} method, a population class, a population detail, a publication, a marker, an allele, a chromosome, a base position, a heterozygosity range, a build number, or a strain. In addition, many results can be retrieved simultaneously using <b>batch</b> queries. <b>Searches</b> return refSNP number IDs that match the query term and a summary of the available information for that refSNP cluster.|$|R
5000|$|A [...] "front merge" [...] is an {{operation}} where the I/O Scheduler, seeking to condense (or [...] "merge") smaller requests into fewer (larger) operations, {{will take a}} new operation then examine the active batch and attempt to locate operations where the beginning sector is the same or immediately after another operation's beginning sector. A [...] "back merge" [...] is the opposite, where ending sectors in the active <b>batch</b> are <b>searched</b> for sectors that are either the same or immediately after the current operation's beginning sectors. Merging diverts operations from the current batch to the active one, decreasing [...] "fairness" [...] {{in order to increase}} throughput.|$|R
40|$|Abstract Background The Quail Genomics knowledgebase ([URL]) {{has been}} {{initiated}} {{to share and}} develop functional genomic data for Northern bobwhite (Colinus virginianus). This web-based platform {{has been designed to}} allow researchers to perform analysis and curate genomic information for this non-model species that has little supporting information in GenBank. Description A multi-tissue, normalized cDNA library generated for Northern bobwhite was sequenced using 454 Life Sciences next generation sequencing. The Quail Genomics knowledgebase represents the 478, 142 raw ESTs generated from the sequencing effort in addition to assembled nucleotide and protein sequences including 21, 980 unigenes annotated with meta-data. A normalized MySQL relational database was established to provide comprehensive search parameters where meta-data can be retrieved using functional and structural information annotation such as gene name, pathways and protein domain. Additionally, blast hit cutoff levels and microarray expression data are available for <b>batch</b> <b>searches.</b> A Gene Ontology (GO) browser from Amigo is locally hosted providing 8, 825 unigenes that are putative orthologs to chicken genes. In an effort to address over abundance of Northern bobwhite unigenes (71, 384) caused by non-overlapping contigs and singletons, we have built a pipeline that generates scaffolds/supercontigs by aligning partial sequence fragments against the indexed protein database of chicken to build longer sequences that can be visualized in a web browser. Conclusion Our effort provides a central repository for storage and a platform for functional interrogation of the Northern bobwhite sequences providing comprehensive GO annotations, meta-data and a scaffold building pipeline. The Quail Genomics knowledgebase will be integrated with Japanese quail (Coturnix coturnix) data in future builds and incorporate a broader platform for these avian species. </p...|$|R
40|$|The schema of the {{previously}} described Escherischia coli database coliBASE {{has been applied to}} a number of other bacterial taxa, under the collective name xBASE. The new databases include CampyDB for Campylobacter, Helicobacter and Wolinella; PseudoDB for pseudomonads; ClostriDB for clostridia; RhizoDB for Rhizobium and Sinorhizobium; and MycoDB, for Mycobacterium, Streptomyces and related organisms. The databases provide user friendly access to annotation and genome comparisons through a web-based graphical interface. Newly developed features include whole genome displays, ‘painting’ of genes according to properties such as GC content, a pattern search system to identify conserved motifs and <b>batch</b> BLAST <b>searching</b> of every protein encoded by a region. Examples of how the databases have been, and continue to be, used to generate hypotheses for subsequent laboratory investigation are presented. xBASE is available online at...|$|R
50|$|The general {{procedure}} is as follows: the parameterized search distribution {{is used to}} produce a <b>batch</b> of <b>search</b> points, and the fitness function is evaluated at each such point. The distribution’s parameters (which include strategy parameters) allow the algorithm to adaptively capture the (local) structure of the fitness function. For example, {{in the case of}} a Gaussian distribution, this comprises the mean and the covariance matrix. From the samples, NES estimates a search gradient on the parameters towards higher expected fitness. NES then performs a gradient ascent step along the natural gradient, a second order method which, unlike the plain gradient, renormalizes the update w.r.t. uncertainty. This step is crucial, since it prevents oscillations, premature convergence, and undesired effects stemming from a given parameterization. The entire process reiterates until a stopping criterion is met.|$|R
40|$|In {{this paper}} {{we present a}} case study from the {{lighting}} industry concerned with the scheduling {{of a set of}} job families each representing the production of a particular end-item in a given quantity. It is a job shop type problem, where each job family has a number of routing alternatives, and the solution has to respect batching and machine availability constraints. All jobs of the same job family have a common release date and a common due date, and they differ only in size. The objective is to minimize the total tardiness of the job families, rather than that of individual jobs. We propose a two-phase method based on solving a mixed-integer linear program and then improving the initial solution by tabu search. We evaluate our method on real-world as well as generated instances. Scheduling <b>Batching</b> Tabu <b>search</b> Mathematical programming...|$|R
40|$|Abstract Background Mass {{spectrometry}} {{has become}} the analytical method of choice in metabolomics research. The identification of unknown compounds is the main bottleneck. In addition to the precursor mass, tandem MS spectra carry informative fragment peaks, but the coverage of spectral libraries of measured reference compounds are far from covering the complete chemical space. Compound libraries such as PubChem or KEGG describe {{a larger number of}} compounds, which can be used to compare their in silico fragmentation with spectra of unknown metabolites. Results We created the MetFrag suite to obtain a candidate list from compound libraries based on the precursor mass, subsequently ranked by the agreement between measured and in silico fragments. In the evaluation MetFrag was able to rank most of the correct compounds within the top 3 candidates returned by an exact mass query in KEGG. Compared to a previously published study, MetFrag obtained better results than the commercial MassFrontier software. Especially for large compound libraries, the candidates with a good score show a high structural similarity or just different stereochemistry, a subsequent clustering based on chemical distances reduces this redundancy. The in silico fragmentation requires less than a second to process a molecule, and MetFrag performs a search in KEGG or PubChem on average within 30 to 300 seconds, respectively, on an average desktop PC. Conclusions We presented a method that is able to identify small molecules from tandem MS measurements, even without spectral reference data or a large set of fragmentation rules. With today's massive general purpose compound libraries we obtain dozens of very similar candidates, which still allows a confident estimate of the correct compound class. Our tool MetFrag improves the identification of unknown substances from tandem MS spectra and delivers better results than comparable commercial software. MetFrag is available through a web application, web services and as java library. The web frontend allows the end-user to analyse single spectra and browse the results, whereas the web service and console application are aimed to perform <b>batch</b> <b>searches</b> and evaluation. </p...|$|R
50|$|As such, {{it is an}} {{information}} aggregation, or integration approach - it provides single point access to many information resources, and typically returns the data in a standard or partially homogenized form. Other approaches include constructing an Enterprise data warehouse, Data lake, or Data hub. Federated Search queries many times in many ways (each source is queried separately) where other approaches import and transform data many times, typically in overnight <b>batch</b> processes. Federated <b>search</b> provides a real-time view of all sources (to the extent they are all online and available).|$|R
40|$|ABSTRACT In a <b>Batched</b> Colored Intersection <b>Searching</b> Problem (CI), one {{is given}} a set of n {{geometric}} objects (of a certain class). Each object is colored by one of c colors, and {{the goal is to}} report all pairs of colors (c 1, c 2) such that there are two objects, one colored c 1 and one colored c 2, that intersect each other. We also consider the bipartite version of the problem, where we are interested in intersections between objects of one class with objects of another class (e. g., points and halfspaces) ...|$|R
40|$|Abstract: In {{this paper}} we propose a {{decomposition}} approach that hierarchically integrates the <b>batching</b> and local <b>search</b> heuristics in manufacturing scheduling. The problem comprises two main interrelated stages embedded in any production-transportation supply chain, namely (i) scheduling processing of raw materials and robot's transportation operation within each individual cell, and (ii) scheduling of transportation and distribution of batches of semi-finished products between cells. Several efficient heuristic algorithms are proposed. This work has been motivated by a real-life problem of production planning for a CIM system for manufacturing of multi-component products served by robots...|$|R
40|$|TREC 2009 was {{the fourth}} year of the Legal Track, which focuses on {{evaluation}} of search technology for “discovery ” (i. e., responsive review) of electronically stored information in litigation and regulatory settings. The track included two tasks: an Interactive task (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback) and a <b>Batch</b> task (two-pass <b>search</b> in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass). This paper describes {{the design of the}} two tasks and presents the results. ...|$|R
40|$|In a <b>Batched</b> Colored Intersection <b>Searching</b> Problem (CI), one {{is given}} a set of n {{geometric}} objects (of a certain class). Each object is colored by one of c colors, and {{the goal is to}} report all pairs of colors (c 1, c 2) such that there are two objects, one colored c 1 and one colored c 2, that intersect each other. We also consider the bipartite version of the problem, where we are interested in intersections between objects of one class with objects of another class (e. g., points and halfspaces). In a Spars...|$|R
40|$|We {{start with}} {{a review of the}} pervasiveness of the nearest {{neighbor}} search problem and techniques used to solve it along with some experimental results. In the second chapter, we show reductions between two different classes of geo- metric proximity problems: the nearest neighbor problems to solve the Euclidean minimum spanning tree problem and the farthest neighbor problems to solve the k-centers problem. In the third chapter, we unify spatial partitioning trees un- der one framework the meta-tree. Finally, we propose a dual tree algorithm for Bichromatic Closest Pair and measure the complexity of <b>batch</b> nearest neighbor <b>search...</b>|$|R
40|$|We {{describe}} an algorithm for computing {{the intersection of}} n balls of equal radius in R³ which runs in time O(n lg² n). The algorithm can be parallelized so that the comparisons that involve the radius of the balls are performed in O(lg³ n) <b>batches.</b> Using parametric <b>search,</b> these algorithms are used to obtain an algorithm for computing the diameter {{of a set of}} n points in R³ (the maximum distance between any pair) which runs in time O(n lg^ 5 n). The algorithms are deterministic and elementary; this is in contrast with the running time O(n log n) in both cases that can be achieved using randomization [3], and the running times O(n lg n) and O(n lg³ n) using deterministic geometric sampling [2, 1]...|$|R
40|$|This {{paper is}} {{concerned}} with coordination aspects of supply chain management and, in particular, investigates setup coordination between two and three stages of a supply chain. The problem arises from a real application in the production chain of a kitchen furniture plant. In different stages of the plant, items are grouped according to different attributes. A setup is required in a stage when the new batch has a different level of attribute from the previous one. Two objectives are considered, i. e., minimizing {{the total number of}} setups and minimizing the maximum number of setups of the stages. The problem is to determine a sequence of <b>batches</b> in <b>search</b> for Pareto-optimal solutions with respect to the two objectives. Several metaheuristics, including genetic algorithm, simulated annealing, and iterated local search (ILS) have been proposed for the two-stage problem. In this paper, we develop a constructive heuristic, which combines a least flexibility first principle and a greedy search, for the two- and three-stage problems. Computational results show that the proposed heuristic performs significantly better than the genetic algorithm and simulated annealing. Although the proposed heuristic is inferior to the ILS, which employs two constructive initial solution heuristic, for the two-stage problem, it can be easily extended to the three-stage problem. Setup coordination Scheduling Multiple objectives Supply chain...|$|R
40|$|AbstractWe {{describe}} an algorithm for computing {{the intersection of}} n balls of equal radius in R 3 which runs in time O(nlg 2 n). The algorithm can be parallelized so that the comparisons that involve the radius of the balls are performed in O(lg 3 n) <b>batches.</b> Using parametric <b>search,</b> these algorithms are used to obtain an algorithm for computing the diameter {{of a set of}} n points in R 3 (the maximum distance between any pair) which runs in time O(n lg 5 n). The algorithms are deterministic and elementary; this is in contrast with the running time O(nlgn) in both cases that can be achieved using randomization (Clarkson and Shor, 1989), and the running times O(nlg n) and O(nlg 3 n) using deterministic geometric sampling (Brönnimann et al., 1993; Amato et al., 1994) ...|$|R
40|$|Consider {{a pair of}} plane straight-line graphs whose {{edges are}} colored red and blue, respectively, and let n be the total {{complexity}} of both graphs. We present a O(n log n) -time O(n) -space technique to preprocess such a pair of graphs, that enables efficient searches among the red-blue intersections along edges {{of one of the}} graphs. Our technique has a number of applications to geometric problems. This includes: (1) a solution to the <b>batched</b> red-blue <b>search</b> problem [Dehne et al. 2006] in O(n log n) queries to the oracle; (2) an algorithm to compute the maximum vertical distance between a pair of 3 D polyhedral terrains, one of which is convex, in O(n log n) time, where n is the total complexity of both terrains; (3) an algorithm to construct the Hausdorff Voronoi diagram of a family of point clusters in the plane in O((n+m) log 3 n) time and O(n + m) space, where n is the total number of points in all clusters and m is the number of crossings between all clusters; (4) an algorithm to construct the farthest-color Voronoi diagram of the corners of n disjoint axis-aligned rectangles in O(n log 2 n) time; (5) an algorithm to solve the stabbing circle problem for n parallel line segments in the plane in optimal O(n log n) time. All these results are new or improve on the best known algorithms. SCOPUS: cp. kinfo:eu-repo/semantics/publishe...|$|R
