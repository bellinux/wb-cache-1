8|21|Public
5000|$|Crypto-shredding is the {{practice}} of 'deleting' data by (only) deleting or overwriting the encryption keys.When a cryptographic disk erasure (or crypto erase) command is given (with proper authentication credentials), the drive self-generates a new media encryption key and goes into a 'new drive' state. Without the old key, the old data becomes irretrievable and therefore an efficient means of providing disk sanitization {{which can be a}} lengthy (and costly) process. For example, an unencrypted and unclassified computer hard drive that requires sanitizing to conform with Department of Defense Standards must be overwritten 3+ times; a one Terabyte Enterprise SATA3 disk would take many hours to complete this process. Although the use of faster solid-state drives (SSD) technologies improves this situation, the take up by enterprise has so far been slow. The problem will worsen as disk sizes increase every year. With encrypted drives a complete and secure data erasure action takes just a few milliseconds with a simple key change, so a drive can be safely repurposed very quickly. This sanitization activity is protected in SEDs by the drive's own key management system built into the firmware in order to prevent accidental data erasure with confirmation passwords and secure authentications related to the original key required. There is no way to retrieve data once erased in this way [...] - the keys are self generated randomly so there is no record of them anywhere. Protecting this data from accidental loss or theft is achieved through a consistent and comprehensive data <b>backup</b> <b>policy.</b>|$|E
40|$|AbstractThe {{present study}} {{discusses}} an optimal <b>backup</b> <b>policy</b> for a hard computer disk {{used for a}} personal computer or an engineering work station. The information stored in the hard disk is backed up at age T, where age refers to the elapsed time since the previous backup operation or the recovery from a hard disk failure, whichever occurred most recently. The existence of an optimal backup time that maximizes the availability is examined. A numerical example is also presented to illustrate the proposed <b>backup</b> <b>policy...</b>|$|E
40|$|Abstract. In {{this paper}} we define {{and address the}} problem of safe {{exploration}} in the context of reinforcement learning. Our notion of safety is concerned with states or transitions that can lead to damage and thus must be avoided. We introduce the concepts of a safety function for determining a state’s safety degree and that of a <b>backup</b> <b>policy</b> that is able to lead the controlled system from a critical state back to a safe one. Moreover, we present a level-based exploration scheme that is able to generate a comprehensive base of observations while adhering safety constraints. We evaluate our approach on a simplified simulation of a gas turbine. ...|$|E
5000|$|Review {{and update}} all systems-related {{operations}} procedures like <b>backup</b> <b>policies</b> and system monitoring ...|$|R
5000|$|Establish and {{maintain}} sound <b>backup</b> and recovery <b>policies</b> and procedures.|$|R
40|$|Controller {{applications}} in OpenFlow cannot be trivially aug-mented {{to support the}} various high availability (or failure recovery) models of server applications. Recent work on OpenFlow has largely assumed static replica configurations or has relied on controller developers to embed high avail-ability support in their design. Instead, we present Rule-Bricks, a system for flexibly embedding high availability support in existing OpenFlow policies. RuleBricks intro-duces three key primitives: drop, insert, and reduce. We de-scribe how these primitives can express various flow assign-ment and <b>backup</b> <b>policies,</b> demonstrating the one offered by the Chord protocol. We have implemented RuleBricks and the Chord assignment policy. Using simulation, we compare RuleBricks against a typical tree-based approach. We show that RuleBricks maintains linear scalability {{with the number of}} replicas on the Chord ring...|$|R
40|$|Abstract- With the {{increasing}} popularity of open-source platform Hadoop, the meteorological industry {{is available to}} create a Meteorological Cloud (MeteCloud) platform to store and deploy applications. In this paper, we propose an idea to build the MeteCloud platform for meteorological departments using Hadoop. We also present a <b>backup</b> <b>policy</b> for meteorological data. In addition, one kind of storage process of the meteorological A-format file is presented. Furthermore, we experiment with one-year historical data on the platform by varying many related parameters such {{as the number of}} nodes, meteorological records and fields. Finally, the proposed MeteCloud platform proves to be efficient and suitable for the storage of meteorological data...|$|E
40|$|Peer-to-Peer {{architecture}} is currently an attractive solution for facilitating {{the use of}} collaboration software without any server. Multicasting is usually proposed for group communication, such as IP multicasting, {{which is the most}} efficient way to disseminate collaboration data as it eliminates traffic redundancy and improves bandwidth utilization. But due to scalability and deployment issues, an alternative has been addressed to shift IP multicasting to Application Layer Multicasting (ALM) with the collaboration between the end hosts, the latter also known as P 2 P. This paper focuses on design and implementation of a hybrid Peer-to-Peer collaboration platform to share group data. Here, we present Fault Tolerant ALM (FT-ALM) architecture for content distribution. Multiple-server based FT-ALM improves cumulative end-to-end latency by reflecting physical network topology on to the overlay network structure. Resilience is ensured through <b>backup</b> <b>policy.</b> The effectiveness of FT-ALM is tested through extensive simulation. 1...|$|E
40|$|Abstract We are {{sometimes}} {{faced with the}} problem of searching for the actual failure time of a unit when it has failed until time t 0. This is called a backward time problem of how much time we go back to search for the failure time from time t 0, and is solved by using the reversed failure rate. This paper proposes an extended model of multiple backward trials in which we find the actual failure time from time t 0. Another problem is that when an operating unit fails, {{we have to go back}} to the newest checking time and reconstruct it as recovery techniques, which is called a <b>backup</b> <b>policy.</b> This paper takes up three backup policies where the unit is checked at periodic, successive and random working times, and discusses analytically their optimal policies. Finally, it would be important to keep a record of operational behavior of a unit when its failure is detected, which is called traceability. This paper compares two models with traceability and without traceability, and proposes three traceability models where the unit operates for a finite interval, and is checked at periodic and successive times. Optimal policies for each model are discussed by using backup techniques...|$|E
40|$|Exploring random {{maintenance}} models, {{this book}} provides {{an introduction to}} the implementation of random maintenance, and {{it is one of the}} first books to be written on this subject.   It aims to help readers learn new techniques for applying random policies to actual reliability models, and it provides new theoretical analyses of various models including classical replacement, preventive maintenance and inspection policies. These policies are applied to scheduling problems, <b>backup</b> <b>policies</b> of database systems, maintenance policies of cumulative damage models, and reliability of random redundant systems. Reliability theory is a major concern for engineers and managers, and in light of Japan’s recent earthquake, the reliability of large-scale systems has increased in importance. This also highlights the need for a new notion of maintenance and reliability theory, and how this can practically be applied to systems. Providing an essential guide for engineers and managers specializing in reliability maintenance and quality control, this book provides a useful resource for those with doubts carrying out maintenance of new and large systems. It is also intended for graduate students and researchers interested in operations research, statistics, industrial engineering and management science...|$|R
40|$|Objective: To {{explore the}} {{knowledge}} of {{a panel of experts}} to develop possible ways of minimising the risk of occupational violence towards remote area nurses. Design: The Delphi method using open-ended questionnaires and an online survey to measure support for suggested control measures. Setting: Remote area nursing posts across Australia. Participants: A panel of expert remote area nurses (n= 10) from geographically diverse regions. Main outcome measure: Identified and described measures with the potential {{to reduce the risk of}} violence. Results: A 2 ̆ 7 toolbox 2 ̆ 7 of strategies was suggested in recognition of the complex nature of occupational violence within the remote health context. Job-specific education included de-escalation techniques, risk assessment and cultural safety training. Professional support included access to counselling and debriefing services. Organisational responsibilities included: adequate staffing to provide <b>backup,</b> <b>policies</b> and procedures and action from management when hazards are identified. Community collaboration with the health service in developing orientation programs, safety plans and addressing violence within the community was also recommended. Conclusion: A variety of strategies were identified that could be used to reduce the risk of occupational violence towards remote health care staff. Further development and assessment of this 2 ̆ 7 toolbox 2 ̆ 7 of strategies is recommended to address the high incidence of violence towards remote health professionals in Australia and overseas...|$|R
5000|$|... 1. A Policy Framework on Disaster Management2. Donations and Importation of Relief Supplies Policy 1 {{approved}} 1149/963. Emergency Shelter Management Policy 1149/964. Emergency Housing Policy5. Hazard Mitigation Policy6. Mass Fatalities Policy7. Mass Crowd Events Policies and Guidelines8. Governmental Officers Security Travel Policy9. <b>Backup</b> Policy10. National <b>Policy</b> on Ambulance Operations11. National Incident Management System NIMS Policy ...|$|R
40|$|Web应用服务器为多层分布式企业级应用的开发、部署、集成、运行和维护提供了通用的基础设施。随着在网络计算环境下用户规模和数据量的增长，单个Web应用服务器往往不能满足性能、可靠性等方面的需求，需要借助集群技术以提高分布式系统的可用性、可扩展性和处理性能。可靠的集群通信是Web应用服务器信息交互的基础，然而目前应用服务器产品的集群通信系统普遍存在着自适应能力低、容错性较差、可重配能力不足等问题。本文围绕上述问题，探讨了Web应用服务器集群通信系统的设计与实现。首先，本文研究了维持集群结构一致性和正确性的关键问题。分析了Web应用服务器集群结构的应用需求，实现了自适应动态拓扑结构，在集群成员间维持拓扑视图的一致性，完成动态加入和优雅退出操作。提出了分布式对等管理机制，避免了性能和可用性瓶颈。利用管理服务器心跳信号侦测同名集群，通过管理服务器自动降级合并同名集群，保证集群的单一性。实现了失效监测链，在任意服务器失效情况下能够在有限时间内重组集群，同时在管理服务器失效后重新选举管理服务器，增强了集群的整体可用性。其次，本文研究了Web应用服务器集群消息传输的三类模型：PUB／SUB模型、恒定点一点传输模型和临时点一点传输模型，实现了基于集群协作的优化可靠多播，利用滑动窗口进行流控制。在局域网环境下扩展了GossiP算法，实现了分布式消息确认机制，同时依据动态拓扑视图的实时更新避免失效实例对消息的阻塞，在不可靠多播协议的基础上实现了消息的可靠传输。最后，本文描述了可灵活重配的集群通信系统CCSAS的具体实现。CCSAS利用XML结构化对象模型和Java虚拟机的运行时动态绑定机制，根据可配置组件参数和运行策略实现功能类实例的动态加载，赋予集群管理者灵活更改集群结构、备份策略、负载平衡机制等运行时规则的能力，实现了高度柔性的集群。论文的研究成果已应用于中科院软件所自主开发的基于J 2 EE规范的Web应用服务器OnceAs中。目前，OnceAs已成功用于电子政务、电子商务和ERP系统等领域。Web {{application}} server(WAS) {{provides a}} common infrastructure for the development, deployment, integration, running {{and maintenance of}} multi-tier distributed enterprise applications. With the increase of user number and data volume in network computing environment, a single WAS cannot always satisfy the need for performance and reliability. It is necessary to improve the availability, scalability and processing capability of WAS with clustering technology. Reliable cluster communication is the base of WAS information exchange. However, there still exist problems such as low adaptability, deficient fault tolerance and lack of configurability in most cluster communication systems of today's application servers. This thesis focuses on these problems and studies the design and implementation of web application server cluster communication system. Firstly, this thesis studies the key issues of maintaining the consistency and correctness of cluster structure. It analyzes the application requirements of WAS cluster structure and implements an adaptive dynamic topology. The consistency of topology views is kept around the cluster members and graceful joining and leaving operations are performed. This thesis brings about a distributed noncentral management mechanism to avoid performance and availability bottleneck. Homonymous clusters are detected through heartbeat signals and merged by automatic administrative server degradation to keep the uniqueness of clusters. A fail-detect chain is implemented to reorganize the cluster in a limited {{period of time in}} case that any number of servers would fail. And a new administrative server would be selected if the original failed, which greatly improves the whole availability of clusters. Secondly, this thesis studies three types of models of WAS cluster message transport: PUB/SUB model, constant peer-to-peer model and temporary peer-to-peer model. It implements the optimized multicast based on cluster cooperation and uses sliding windows to fulfill flow-control. Gossip algorithm is extended in the LAN environment to facilitate the distributed message acknowledgement. Real-time update of dynamic topology view is used to prevent failed instances to block message transfer. Thus, reliable message exchange is implemented on top of unreliable multicast protocols. At last, this thesis describes the implementation of the flexibly configurable cluster communication system - CCS AS. In CCS AS, XML structural object model and Java dynamic binding mechanism are used to dynamically load instances of functionality class according to the component parameters and runtime policies, which grants the administrator of cluster capabilities of flexibly changing runtime rules such as cluster structure, <b>backup</b> <b>policy</b> and load balancing method. A cluster with great flexibility is achieved in this way. The CCSAS cluster communication system has been integrated in the J 2 EE web application server OnceAS which is developed independently by the Institute of Software, Chinese Academy of Sciences. Now OnceAS has been widely used in such fields as E-Government, E-Commerce and ERR...|$|E
40|$|OBJECTIVES [...] To {{evaluate}} {{trends in}} referrals for emergency operations after {{percutaneous transluminal coronary angioplasty}} (PTCA) complications; to analyse {{morbidity and mortality}} and assess the influence of PTCA backup on elective surgery. DESIGN [...] A retrospective analysis of patients requiring emergency surgical revascularisation within 24 hours of percutaneous transluminal coronary angioplasty. PATIENTS [...] Between January 1980 and December 1990, 75 patients requiring emergency surgery within 24 hours of percutaneous transluminal coronary angioplasty. SETTING [...] A tertiary referral centre and postgraduate teaching hospital. RESULTS [...] 57 patients (76 %) were men, the mean age was 55 (range 29 - 73) years, and 30 (40 %) had had a previous myocardial infarction. Before PTCA, 68 (91 %) had severe angina, 59 (79 %) had multivessel disease, and six (8 %) had a left ventricular ejection fraction of less than 40 %. A mean of 2. 1 grafts (range one to five) were performed; the internal mammary artery was used in only one patient. The operative mortality was 9 % and inhospital mortality was 17 %. There was a need for cardiac massage until bypass was established in 19 patients (25 %) : this was the most important outcome determinant (P = 0. 0051) and was more common in those patients with multivessel disease (P = 0. 0449) and in women (P = 0. 0388). In 10 of the 19 cases a vacant operating theatre was unavailable, the operation being performed in the catheter laboratory or anaesthetic room. These 19 patients had an operative mortality of 32 % and inhospital mortality of 47 %, compared with 2 % and 7 % respectively for the 56 patients who awaited the next available operating theatre. Complications included myocardial infarction, 19 patients (25 %); arrhythmias, 10 patients (3 %); and gross neurological event, two patients (3 %). The mean intensive care unit stay was 2. 6 days (range 1 to 33 days) and the mean duration of hospital admission was 13 days (range 5 - 40 days). CONCLUSIONS [...] Patients undergoing emergency surgery after PTCA complications have a substantially increased inhospital mortality and morbidity. PTCA in this unit continues to require surgical cover. Delays in operating on stable patients in centres which operate a "next available theatre" <b>backup</b> <b>policy</b> may not differ from some units performing PTCA with offsite cover for PTCA complications. Particularly in the presence of multivessel disease, however, PTCA complications may be associated with the need for "crash" bypass and such patients are unlikely to survive hospital transfer. The proportion of patients requiring "crash" bypass has increased during the period reviewed because of the extent of disease in the emergency surgical group increased. These results indicate that surgery should not be denied to these patients...|$|E
40|$|There {{are several}} mobile {{password}} managers on the marked, {{where the most}} popular of these uses the classical solution for storage which requires both encryption and <b>backup</b> <b>policies.</b> If quantum computers become a reality, the security of encryption methods based on factoring primes or doing modular exponentiation is threatened. For threshold secret sharing schemes, an unauthorized set of shares of the secret provides no information about the secret. By this, one can say that secret sharing is information-theoretically secure, which means that it cannot be broken even when an attacker has unlimited computing power. In this thesis, the development of a password storage mobile application for Android is presented. The mobile application implements secret sharing for confidentiality and uses cloud storage services for storing the shares. A password is divided into three pieces, where two or more are needed to reconstruct the password. Together, this manifests itself as a (2, 3) threshold scheme. The cloud storage services implemented are Dropbox, Google Drive, and Microsoft OneDrive. User tests were conducted for testing the functionality and the user interface of the application. The result was a "PASSED" score on 98, 2 %, which indicated that the functionality performed better than expected and the alternative hypothesis H 1 was supported. The feedback from the test subjects stated that the application looked good and worked well, but some of the solutions could have been optimized in regards to the user interface...|$|R
40|$|Inflation {{targeting}} may do {{more harm}} than good if there is a substantial chance that the central bank cannot in fact control inflation. A prerequisite for central bank control of inflation is appropriate coordination with or <b>backup</b> by fiscal <b>policy,</b> and the nature of the required coordination will depend on whether and how central bank independence from the fiscal authority has been implemented...|$|R
25|$|Earl Weaver was the #1 ABC {{analyst in}} , {{but was also}} {{employed}} by the Baltimore Orioles as a consultant. At the time, ABC had a policy preventing an announcer who was employed by a team from working games involving that team. So whenever the Orioles were on the primary ABC game, Weaver worked the <b>backup</b> game. This <b>policy</b> forced Weaver to resign from the Orioles' consulting position in October {{in order to be}} able to work the World Series for ABC.|$|R
40|$|In this study, {{we present}} a series of {{well-known}} optimization methods to address two related decisions associated with the design of large-scale ambulance operations on highways: (1) The question of location, and (2) the issue of districting. As a result of computer storage and runtime constraints, previous approaches have only considered small-to-moderate scale problem scenarios, generally employing exact hypercube queuing models integrated into optimization procedures. We overcome these limitations here by embedding a fast and accurate hypercube approximation algorithm adapted for partial <b>backup</b> dispatch <b>policies</b> in single- and multi-start greedy heuristics. The proposed methods are tested on small-to-large-scale problems involving up to 100 ambulances. The results suggest that our approach is a viable alternative for the analysis and configuration of large-scale highway emergency medical systems, providing reasonable accuracy and affordable run times. Emergency medical systems Ambulance deployment Approximate hypercube queuing model Multi-start greedy heuristic Probabilistic location and districting problems...|$|R
40|$|A {{dissertation}} {{submitted to}} the Department of Computer Science and Engineering for the Degree of Master of Business of Administration in InformationIn the Journal of Information Law and Technology, Professor Gottschalk argued that law firms represent an industry that seems very well suited for knowledge management research [16]. The term Knowledge management in law firms (KM in law firms) can be defined as: The way in which lawyers optimize the relation between knowledge and knowledge processes {{with the help of}} Information Technology. Given the potential for improvement through the use of KM in law firms. I find it quite surprising that the popularity of knowledge management in legal circles stand at a very low level. I believe a contributing factor to this is the dearth of research and publications 011 Legal Knowledge Management by persons within and outside the legal industry-/ Knowledge Management (KM) usually focuses on the level of an organization, while (academic) lawyers arc known to be more people-oriented. This difference can beidentified as another factor to the lack of interest towards KM in law firms. / The majority of law firms overseas use Information Technology (IT) for Knowledge Management. In some firms, highly sophisticated analytical tools are used to make expert judgments. Moreover, new advance technology, such as. mobile technology is used for KM within law firms. In comparison to these changes, the Legal industry in Sri Lanka is at a ground-level in terms of IT. The primary aim of this research is to investigate the law firm industry in Sri Lanka. The detailed investigation includes the current "IT status" in these firms, for instance. Infrastructure, Software Usage (e. g [...] End-user tools. Who Knows What systems. What They Know Systems, and How They Think systems). Communications. <b>Backup</b> <b>Policies,</b> Virus Protection as well as the lawyer's IT literacy level. In terms of Knowledge Management the researcher is interested in identifying the knowledge management practices in these firms, for instance. Knowledge sharing attitudes. Knowledge Capture, and Disseminations. In addition to this, the researcher w ill focus on the possible relationship between Knowledge Sharing Perceptions and Reward attitudes for lawyers as well as the relationship between Knowledge Sharing and Information Technology. Finally, the researcher will identify the barriers to Knowledge Management and suggest recommendations to improve the use of Knowledge Management perspectives in Sri Lankan Law firms...|$|R
40|$|AbstractIn this decade, a {{hard disk}} {{has become an}} {{essential}} key component of a personal computer system. It preserves important information which is frequently updated. In case the hard disk fails, we may possibly lose such important information. This is called a hard disk failure. One of the simplest methods for coping with such a possibility of a hard disk failure is to periodically make {{a copy of the}} information to another secondary medium. This is called a backup operation. This study discusses an efficient <b>backup</b> warning <b>policy</b> which gives us a warning to back up files at the prespecified time Tw measured by the elapsed time since the previous backup operation or the recovery from a hard disk failure, whichever occurs first. For the purpose of determining the value of Tw, this study formulates the efficiency as a criterion, which is defined by the long-run average ratio of 1. (i) the time spent in processing jobs effectively in the sense that their accomplishments are successfully backed up, to 2. (ii) the total time spent in processing jobs ineffectively as well as effectively, and spent in backup or recovery operations. We then clarify the conditions under which an optimal warning time exists. A numerical example is also presented...|$|R
40|$|Cloud {{computing}} is {{an emerging}} paradigm used by an increasingly {{number of enterprises}} to support their business and promises to make the utility computing model fully realized by exploiting virtualization technologies. Free software is now mature not only to offer well-known server-side applications, but also to land on desktop computers. However, administering in a decentralized way {{a large amount of}} desktop computers represents a demanding issue: system updates, <b>backups,</b> access <b>policies,</b> etc. are hard tasks to be managed separately on each computer. This paper presents a general purpose architecture for building a reliable, scalable, flexible, and modular private cloud that exploits virtualization technologies at different levels. The architecture can be used to offer a variety of services that span from web applications and web services to soft real-time applications. To show the features of the proposed architecture, we also present the design and implementation over it of a Linux Terminal Server Project (LTSP) cluster that benefits from the underlying IaaS services offered by the private cloud. The cloud infrastructure, as well as the LTSP, have been implemented exclusively using free software and are now in a production state, being used by approximately 200 users for their everyday work. We hope that our description and design decisions can provide some guidance about designing an architecture for a cloud service provider...|$|R
50|$|Between his stints {{as manager}} Weaver {{served as a}} color {{commentator}} for ABC television, calling the 1983 World Series (which the Orioles won) along with Al Michaels and Howard Cosell. Weaver was the #1 ABC analyst in 1983 (replacing Don Drysdale, who moved over to secondary play-by-play for ABC), but was also employed by the Baltimore Orioles as a consultant. At the time, ABC had a policy preventing an announcer who was employed by a team from working games involving that team. So whenever the Orioles were on the primary ABC game, Weaver worked the <b>backup</b> game. This <b>policy</b> forced Weaver to resign from the Orioles consulting position in October {{in order to be}} able to work the World Series for ABC. Weaver later called the 1984 National League Championship Series (between the San Diego Padres and Chicago Cubs) for ABC alongside fellow hall of famers Reggie Jackson, who played for Weaver in 1976, and Don Drysdale.|$|R
40|$|ABSTRACT. Inflation {{targeting}} may do {{more harm}} than good if there is a substantial chance that the central bank cannot in fact control inflation. A prerequisite for central bank control of inflation is appropriate coordination with or <b>backup</b> by fiscal <b>policy,</b> and the nature of the required coordination will depend on whether and how central bank independence from the fiscal authority has been implemented. These considerations suggest that in those countries where inflation control has in the past been most difficult, inflation targeting may be least useful. Where inflation control has in the past been successful, the benefits of inflation targeting may have more to do with the associated changes in the policy process and in the central bank’s communication with the public than with the inflation target itself. I. THE TWO FACES OF INFLATION TARGETING Economists should recognize that they have a history of proposing simple “nominal anchor ” prescriptions for monetary policy that have eventually proved not to be very useful. If economists satisfy a demand for spurious technocratic solutions to the political and institutional pathologies that generate destructive episodes o...|$|R
40|$|Abstract — Overlay trees {{typically}} use directed {{trees as}} efficient structures for disseminating information, but their single-path structure means that just one node failure {{results in the}} disconnection of all descendants, possibly {{a significant portion of}} the graph. The addition of extra “backup ” links to a directed tree can provide alternate data paths that significantly reduce the number of nodes disconnected when some set of nodes are removed from the graph. We investigate several deterministic and randomized algorithms for adding such backup links to a directed tree and analyze the connectedness of the resulting graphs when nodes in the network fail with some random probability. We present closed-form approximations and simulation measurements for the connectivity of these augmented trees in networks ranging from hundreds to hundreds of thousands of nodes. We also identify and measure the costs of adding backup links, using simulations and real-world measurements from overlays constructed using PlanetLab latency data. We find that, with node failure rates up to 10 %, deterministic <b>backup</b> link selection <b>policies</b> offer comparable resiliency to random backup links with significantly lower overhead and resource usage. I...|$|R
40|$|Regional {{policy makers}} need {{regional}} economical data {{in order to}} define and <b>backup</b> their economical <b>policy</b> decisions. Regional input-output (IO) tables {{have proved to be}} useful in the policy making process, since the economic effects of policy decisions can be analysed in these models for the region as a whole. Unfortunately, the construction of regional IO tables on the basis of survey methods and other primary data collection methods is very costly and often incomplete. In this paper, we will discuss two techniques which can be applied to derive regional IO tables from national IO tables. In both methods, sectoral production specialization at the regional level is accounted for and affects the interindustrial structure of the region. The IO tables are constructed for 29 industrial sectors and 12 regions in the Netherlands. Policy makers, however, are not interested in the construction of regional IO tables themselves, but more in the economic indicators derived from them. Therefore, we present simple output- and employment-multipliers and employment- transformators derived from the IO tables and discuss some of the differences between them. A description of the economic performance of the Dutch regions is made by looking at the development of the economic indicators over a period of 12 years (1980 - 1992). ...|$|R
40|$|As {{health care}} professions, law {{enforcement}} agencies, governments, employers, {{insurance companies and}} others rush to utilise genetic information about individuals with whom they interact or have relationships with, conflict may arise between different stakeholders. In such circumstances, {{it may be the}} function of ethics to balance competing interests and activities, explore alternatives and options and protect the vulnerable from harm. Ethics may explore and advise on issues of the moral life, but sometimes it needs the <b>backup</b> of constructive <b>policy</b> developments and the investigative, protective and guiding forces of the law to achieve a more mutually beneficial outcome. This chapter on DNA kinship testing is an example of such merging of forces, namely ethics, policy and law to protect the vulnerable. It portrays the effects of unethical behaviour on three different stakeholders and makes some recommendations on how to re-establish and protect the moral life. Genetic kinship testing can create multiple vulnerabilities, all of which can be experienced on an individual, societal and cultural level. Parentage testing stands as an example where vulnerability to an individual is created by not observing proper consent procedures; genetic identity testing for immigration purposes discusses identify fraud and the potential harm to society; and, lastly, DNA kinship testing of Australia's Indigenous and Torres Strait Islander peoples demonstrates that it is appropriate to sometimes let the socio-cultural triumph over the biological...|$|R
40|$|Cloud Computing {{is a set}} of IT Services {{that are}} {{provided}} to a customer over a network and these services are delivered by third party provider who owns the infrastructure and reduce the burden at user’s end. Nowadays researchers devoted their work access control method to enhance the security on Cloud. RBAC is attractive access model because the number of roles is significantly less hence users can be easily classified according to their roles. The Role-based Access Control (RBAC) model provides efficient way to manage access to information while reducing the cost of security administration and complexity in large networked applications. This paper specify various policies in RBAC on clouds such as migration policy which helps the user to migrate the database schema and roles easily to the Cloud using XML with more security. Restriction policy provide the security enhancement in Role Based Access Model by restricting the number of transaction per user and if the number of transactions will increase the admin will come to know through its monitoring system that unauthorized access has been made and {{it would be easier to}} take action against such happening. This paper proposes <b>backup</b> and restoration <b>policy</b> in Role Based Access Model in which if the main cloud is crashed or not working properly then the backup and restoration facility will be available to avoid the lost of important data. In this case chances of loss of data are very less so enhance more security on Cloud Computing...|$|R
40|$|In {{order to}} save the {{switching}} ports {{and the cost of}} Multi-Granularity Optical Cross-Connect (MG-OXC), the waveband switching technique was proposed to groom multiple wavelength-level traffic to a few waveband tunnels to be switched by a few switching ports in MG-OXC. At the same time, protection for fibers is very important to ensure the service continuity since each wavelength carries a lot of traffic. Although existing works have addressed the waveband switching protection, most of them separately considered only the dedicated protection or shared protection in static demand scenario and did not deeply analyze and compare the dedicated protection and shared protection in dynamic demand scenario. Therefore, in this paper, we deeply study the consumptions of wavelengths in fibers and ports in MG-OXCs for Waveband Shared Protection (WSP) and Waveband Dedicated Protection (WDP) in dynamic demand scenario, and propose the port-cost calculation and update methods based on a new waveband layered auxiliary graph that is developed based on MG-OXC structure. In simulations, we compare WSP, WDP, traditional end-to-end waveband shared protection and traditional end-to-end waveband dedicated protection. Simulation results show that the shared protection has smaller port-cost, better wavelength utilization efficiency and lower blocking probability than the dedicated protection with the same waveband switching policy, and the sub-path waveband switching has bigger port-cost, better wavelength utilization efficiency and lower blocking probability than the end-to-end waveband switching with the same <b>backup</b> wavelength assignment <b>policy.</b> Department of Computin...|$|R

