31|14|Public
5000|$|Duplex {{perception}} {{refers to}} the linguistic phenomenon whereby [...] "part of the acoustic signal is used for both a speech and a nonspeech percept." [...] A listener is presented with two simultaneous, dichotic stimuli. One ear receives an isolated third-formant transition {{that sounds like a}} nonspeech chirp. At the same time the other ear receives a base syllable. This base syllable consists of the first two formants, complete with formant transitions, and the third formant without a transition. Normally, there would be peripheral masking in such a <b>binaural</b> <b>listening</b> task but this does not occur. Instead, the listener’s percept is duplex, that is, the completed syllable is perceived and the nonspeech chirp is heard at the same time. This is interpreted as being due to the existence of a special speech module.|$|E
40|$|The {{current study}} investigates how the spatial {{locations}} {{of a target}} and masker influence consonant identification in anechoic and reverberant space. Reverberation was expected ·to interfere with the task both directly by degrading consonant identification and indirectly by altering interaural cues and decreasing spatial unmasking. Performance was measured {{as a function of}} target-to-masker ratio (TMR) to obtain multiple points along the psychometric function. Results suggest that for consonant identification, there is little spatial unmasking; however, in reverberant environments, performance improves with <b>binaural</b> <b>listening</b> even when the target and masker give rise to roughly the same interaural cues. It is hypothesized that the time-varying changes in TMR at both ears that result from reverberation can lead to such <b>binaural</b> <b>listening</b> advantages. The behavioral results are discussed with respect to an acoustic analysis that quantifies the expected improvement of <b>binaural</b> <b>listening</b> over monaural listening using an "independent looks" approach. by Sasha Devore. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2005. Includes bibliographical references (leaves 66 - 69) ...|$|E
40|$|Considerable {{research}} has demonstrated an illusory causation effect in which visually salient people are perceived as more causal of events in a social inter-action than their nonsalient counterparts. The present studies extended this work {{to the realm of}} auditory salience. Two determinants of auditory salience were manipulated—the intensity of a speaker's voice and the sex of a speaker's voice. As predicted, subjects attended more to a 75 -dB (A) than to a 70 -dB voice on a <b>binaural</b> <b>listening</b> test, and subjects attributed more causality to an actor in a two-person conversation when his voice was 75 dB in intensity than when it was 70 dB. Contrary to expectation, subjects did not attend more to the voice whose sex matched their own on the <b>binaural</b> <b>listening</b> test. Rather, all subjects listened more to the actor with the male voice. Consistent with this tendency for the male voice to be more salient, subjects attributed more causality to an actor when the voice was-male than when it had been electronically converted to a female voice of the same intensity and intonation. Vocal salience also influenced subjects ' impressions of the actors, but it had no impact on recall of the actors&apos...|$|E
25|$|Binaural {{recordings}} use {{a different}} microphone technique to encode direction directly as phase, with very little amplitude difference below 2kHz, often using a dummy head, and can produce a surprisingly lifelike spatial impression through headphones. Commercial recordings almost always use stereo recording, rather than <b>binaural,</b> because loudspeaker <b>listening</b> has been more popular than headphone listening.|$|R
40|$|The {{simulation}} of spatialization {{is a matter}} of great concern. In this paper the auralization technique has been improved, in order to ensure a comprehensive evaluation of spaciousness by headphones <b>listening.</b> <b>Binaural</b> impulse responses have been calculated after the definition of proper reciprocal positions between the listener and the sound ray. A numerical code has been developed, implemented and tested. An example of binaural auralization is here presented...|$|R
40|$|This paper {{presents}} the results of an assessment of speech discrimination by hearing-impaired listeners (sensori-neural, conductive, and mixed groups) under <b>binaural</b> free-field <b>listening</b> in the presence of background noise. Subjects with pure-tone thresholds greater than 20 dB in 0. 5, 1. 0 and 2. 0 kHz were presented with a version of the W- 22 list of phonetically balanced words under three conditions: (1) 'quiet', with the chamber noise below 28 dB and speech at 60 dB; (2) at a constant S/N ratio of + 10 dB, and with a background white noise at 70 dB; and (3) same as condition (2), but with the background noise at 80 dB. The mean speech discrimination scores decreased significantly with noise in all groups. However, the decrease in binaural speech discrimination scores with an increase in hearing impairment was less for material presented under the noise conditions than for the material presented in quiet...|$|R
40|$|The goal of {{this study}} is to {{retrieve}} useful information about the reactions of listeners to different recording techniques, namely binaural and stereo. This has been done comparing different mixes of the same song. Each mix is obtained from stereophonic and binaural recordings, or processing proximity recordings with stereo panning, binaural synthesis or a hybrid approach. The comparison is made through listening tests with headphones and an analysis of subjects ’ reactions and meaningful subjective parameters ratings. [Keywords: Spatialization, <b>Binaural,</b> <b>Listening</b> Tests...|$|E
40|$|International audienceThis article {{presents}} a new processing method to design brain-computer interfaces (BCIs). It shows {{how to use}} the perturbations of the communication between different cortical areas due to a cognitive task. For this, the network of the cerebral connections is built from correlations between cortical areas at specific frequencies and is analyzed using graph theory. This allows us to describe the topological organisation of the networks using quantitative measures. This method is applied to an auditive steady-state evoked potentials experiment (dichotic <b>binaural</b> <b>listening)</b> and compared to a more classical method based on spectral filtering...|$|E
40|$|This {{study is}} {{concerned}} with the amplitude modulation (AM) detection thresholds for monaural and <b>binaural</b> <b>listening.</b> In the first experiment, using a Two-Alternative Forced-Choice (2 AFC) method with an adaptive procedure 2 -up 1 -down, the monaural and binaural AM detection thresholds were measured. Sinusoidal carrier at a frequency of 160, 500, 1000 or 4000 Hz was amplitude-modulated by a single sinusoidal modulator at a frequency of 4, 32, 64 or 128 Hz. Due to a significant intersubject scatter of the results it was impossible to estimate the difference between the thresholds determined for monaural and binaural presentation of the stimuli. Therefore, in the next experiment, psychometric functions for AM detection for both monaural and <b>binaural</b> <b>listening</b> were determined. This experiment was carried out for sinu-soidal carriers at frequencies of 5000, 2000 and 6000 Hz and for sinusoidal modulator at frequencies of 4, 64 and 128 Hz. The results of this experiment showed a statistically sig-nificant difference between slopes of the psychometric function (after the percent of correct responses was converted to the detectability, d′, domain) for monaural and binaural stimuli presentation. Assuming that the AM threshold coincided with d ′ = 1 it can be stated that monaural and binaural AM thresholds are significantly different. Key words: binaural and monaural thresholds, auditory filters, amplitude modulation, modu-lation filter bank. 1...|$|E
40|$|This {{data set}} {{contains}} stimuli and {{results from a}} listening paired-comparison preference test. The stimuli consisted always of the same acoustic scene with three individual sources, which had different degrees of impairments. All of the stimuli are created in order to <b>listen</b> <b>binaural</b> via headphones to them. They include already a headphone compensation filter for usage with AKG K 601 headphones. The stimuli were generated using a recording done by the FH Köln [1]. For details on the experiment see [2]. [1] [URL] [2] Raake, A., Wierstorf, H., Blauert J. (2014), “A case for TWO!EARS in audio quality assessment,” Forum Acusticu...|$|R
40|$|Full text: This paper {{describes}} {{a new approach}} to auditory diagnostics, {{which is one of the}} central themes of the EU-project HEARCOM. For this purpose we defined a so-called "Auditory Profile" that can be assessed for each individual listener using a standardized battery of audiological tests that - in addition to the pure-tone audiogram - focus on loudness perception, frequency resolution, temporal acuity, speech perception, <b>binaural</b> functioning, <b>listening</b> effort, subjective hearing abilities, and cognition. For the sake of testing time only summary tests are included from each of these areas, but the broad approach of characterizing auditory communication problems by means of standardized test is expected to have an added value above traditional testing in understanding the reasons for poor speech reception. The Auditory profile may also be relevant in the field of auditory rehabilitation and for design of acoustical environments. The results of an international 5 -center study (in 4 countries and in 4 languages) will be presented and the relevance of a broad but well-standardized approach will be discussed...|$|R
5000|$|This paper {{presents}} new {{binaural hearing}} threshold data obtained (a) by an earphone method over the frequency range 5-100 Hz and (b) by a whole body chamber method {{over the range}} 2-20 Hz. The results obtained are in excellent agreement with recent reported data. The <b>binaural</b> to monaural <b>listening</b> advantage appears to remain at 3 dB throughout the frequency range. A good approximation to the binaural threshold of hearing may be formed by lines from the point 92.0 dBSPL at 15.5 Hz with slopes of 12.3 dB/octave for frequencies below 15.5 Hz and 22.2 dB/octave above. Yeowart and Evans (1974) - Acoustical Society of America ...|$|R
40|$|A new {{objective}} {{measurement system}} to predict speech intelligibility in <b>binaural</b> <b>listening</b> conditions is proposed {{for use with}} nonlinear hearing devices. Digital processing inside such devices often involves nonlinear operations such as clipping, compression, and noise reduction algorithms. Standard objective measures such as the Articulation Indeix (AI), the Speech Intelligibility Index (SII) and the Speech Transmission Index (STI) {{have been developed for}} monaural listening. Binaural extensions of these measures have been proposed in the literature, essentially consisting of a binaural pre-processing stage followed by monaural intelligibility prediction using the better ear or the binaurally enhanced signal. In this work, a three-stage extension of the binaural SII approach is proposed that deals with nonlinear acoustic input signals. The reference-based model operates as follows: (1) a stage to deal with nonlinear processing based on a signal-separation model to recover estimates of speech, noise and distortion signals at the output of hearing devices; (2) a binaural processing stage using the Equalization-Cancellation (EC) model; and (3) a stage for intelligibility prediction using the SII or the short-time Extended SII (ESII). Multiple versions of the model have been developed and tested for use with hearing devices. A software simulator is used to perform hearing-device processing under various <b>binaural</b> <b>listening</b> conditions. Details of the modeling procedure are discussed along with an experimental framework for collecting subjective intelligibility data. In the absence of hearing-device processing, the model successfully predicts speech intelligibility in all spatial configurations considered. Varying levels of success were obtained using two simple distortion modeling approaches with different distortion mechanisms. Future refinements to the model are proposed based on the results discussed in this work...|$|E
40|$|The {{contribution}} {{presents an}} experiment {{to use the}} room acoustic parameter Direct-to-Reverberant-Energy-Ratio (DRR) to solve the room divergence effect in <b>binaural</b> <b>listening</b> via headphones. Perceived externalization of auditory events is decreased if acoustic divergence between the listening room and the resynthesized room occurs. The DRR is used to push the synthesis towards the listening room to increase externalization. The listeners adjust the DRR of the synthesis on the expected DRR of the listening room until congruence between synthesis and listening room is perceived. The {{results show that the}} DRR is a suitable acoustic parameter. The listeners are able to reliably adjust the DRR of the listening room only by their expectations and no explicit external reference. A subsequent quality test shows that the congruent DRR conditions have only a minor effect on the increase of externalization using divergent room conditions...|$|E
40|$|Methods for {{designing}} a <b>binaural</b> <b>listening</b> environment are presented. The designed system {{is capable of}} creating a natural and realistic spatial sound simulation in real time. The sound sources of the system may be either normal audio inputs from the AD converter or built-in model-based sound synthesizers. The output is directed to headphone, single-pair or multichannel loudspeaker reproduction. The system and the techniques presented here are intended for applications of virtual reality, multimedia, and music technology. 1. INTRODUCTION Auralization stands for binaural processing of sounds {{in such a way}} that an illusion of a three-dimensional sound space is created [1]. In this work, a binaural real-time room simulation system has been developed. The theory behind the development of an auralization environment relies on three building blocks:. room acoustics modeling. modeling of human spatial hearing. digital signal processing The real-time processing constraint places sever [...] ...|$|E
40|$|Binaural noise-reduction {{algorithms}} {{based on}} multi-channel Wiener filter (MWF) are promising techniques {{to be used}} in <b>binaural</b> assistive <b>listening</b> devices. The real-time implementation of the existing binaural MWF methods, however, involves challenges {{to increase the amount of}} noise reduction without imposing speech distortion, and at the same time preserving the binaural cues of both speech and noise components. Although significant efforts have been made in the literature, most developed methods so far have focused only on either the former or latter problem. This paper proposes an alternative binaural MWF algorithm that incorporates the non-stationarity of the signal components into the framework. The main objective is to design an algorithm that would be able to select the sources that are present in the environment. To achieve this, a modified speech presence probability (SPP) and a single-channel speech enhancement algorithm are utilized in the formulation. The resulting optimal filter also avoids the poor estimation of the second-order clean speech statistics, which is normally done by simple subtraction. Theoretical analysis and performance evaluation using realistic recorded data shows the advantage of the proposed method over the reference MWF solution in terms of the binaural cues preservation, as well as the noise reduction and speech distortion...|$|R
40|$|This paper focus {{mainly on}} {{comparing}} the localization {{performance in a}} gaming environment with a five channel surround sound system and with headphone reproduced <b>binaural</b> synthesis. A <b>listening</b> test was performed with a custom created simulation of a gaming environment with a visual environment where the test subject was only allowed to control the head movements. The environment was created with Unreal 3. The test measured the test subjects’ speed and accuracy in pinpoint the sound sources, with footsteps used as stimulus, placed at different positions spread out around the character played in a random order. The results indicate that it is significantly more difficult to locate sound sources with headphones than with loudspeakers in both time difference with a 0. 79 seconds time difference between the the two test’s medians and with a 11. 962 ° MAE {{difference between the two}} test’s mean values and are both statistically significant with a 95...|$|R
40|$|Helmholtz himself speculated about a {{role of the}} cochlea in the {{perception}} of musical dissonance. Here we indirectly investigated this issue, assessing the valence judgment of musical stimuli with variable consonance/dissonance and presented diotically (exactly the same dissonant signal was presented to both ears) or dichotically (a consonant signal was presented to each ear – both conso- nant signals were rhythmically identical but differed by a semitone in pitch). Differences in brain organisation underlying inter-subject differences in the percept of dichotically presented dissonance were determined with voxel-based morphometry. Behavioral results showed that diotic dissonant stimuli were perceived as more unpleasant than dichotically presented dissonance, indicating that interactions within the cochlea modulated the valence percept during dissonance. However, the behavioral data also suggested that the dissonance percept did not depend crucially on the cochlea, but also occurred as a result of <b>binaural</b> integration when <b>listening</b> to dichotic dissonance. These results also showed substantial between-participant variations in the valence response to dichotic dis- sonance. These differences were in a voxel-based morphometry analysis related to differences in gray matter density in the inferior colliculus, which strongly substantiated a key role of the inferior colliculus in consonance/dissonance representation in humans...|$|R
40|$|This paper {{addresses}} {{the problem of}} Target Activity Detection (TAD) for <b>binaural</b> <b>listening</b> devices. TAD denotes the problem of robustly detecting the activity of a target speaker in a harsh acoustic environment, which comprises interfering speakers and noise (cocktail party scenario). In previous work, {{it has been shown}} that employing a Feed-forward Neural Network (FNN) for detecting the target speaker activity is a promising approach to combine the advantage of different TAD features (used as network inputs). In this contribution, we exploit a larger context window for TAD and compare the performance of FNNs and Recurrent Neural Networks (RNNs) with an explicit focus on small network topologies as desirable for embedded acoustic signal processing systems. More specifically, the investigations include a comparison between three different types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent Units. The results indicate that all versions of RNNs outperform FNNs for the task of TAD...|$|E
40|$|Binaural beats (BBs) are an {{auditory}} illusion occurring {{when two}} tones of slightly different frequency are presented separately to each ear. BBs {{have been suggested}} to alter physiological and cognitive processes through synchronization of the brain hemispheres. To test this, we recorded electroencephalograms (EEG) at rest and while participants listened to BBs or a monaural control condition during which both tones were presented to both ears. We calculated for each condition the interhemispheric coherence, which expressed the synchrony between neural oscillations of both hemispheres. Compared to monaural beats and resting state, BBs enhanced interhemispheric coherence between the auditory cortices. Beat frequencies in the alpha (10  Hz) and theta (4  Hz) frequency range both increased interhemispheric coherence selectively at alpha frequencies. In a second experiment, we evaluated whether this coherence increase has a behavioral aftereffect on <b>binaural</b> <b>listening.</b> No effects were observed in a dichotic digit task performed immediately after BBs presentation. Our results suggest that BBs enhance alpha-band oscillation synchrony between the auditory cortices during auditory stimulation. This effect seems to reflect binaural integration rather than entrainment...|$|E
40|$|Arweiler and Buchholz [J. Acoust. Soc. Am. 130, 996 – 1005 (2011) ] showed that, {{while the}} energy of early {{reflections}} (ERs) in a room improves speech intelligibility, the benefit is smaller than that provided by {{the energy of}} the direct sound (DS). In terms of integration of ERs and DS, <b>binaural</b> <b>listening</b> did not provide a benefit from ERs apart from a binaural energy summation, such that monaural auditory processing could account for the data. However, a diffuse speech shaped noise (SSN) was used in the speech intelligibility experiments, which does not provide distinct binaural cues to the auditory system. In the present study, the monaural and binaural benefit from ERs for speech intelligibility was investigated using three directional maskers presented from 90 ° azimuth: a SSN, a multi-talker babble, and a reversed two-talker masker. For normal-hearing as well as hearing-impaired listeners, the directional and/or fluctuating (speech) maskers produced a similar benefit from ERs as obtained with the diffuse SSN, suggesting a monaural integration of the ERs and the DS for both types of maskers. 4 page(s...|$|E
40|$|The {{binaural}} {{synthesis of}} acoustical environments {{is based on}} binaural room impulse responses (BRIRs) measured with a dummy head for discrete head positions and angular resolutions of typically between 1 °and 15 °. The resolution of the BRIR grid defines {{the size of the}} BRIR database as well as the duration of its measurement. To determine the minimum grid resolution required for dynamic <b>binaural</b> synthesis a <b>listening</b> test was performed. Following an adaptive 3 AFC procedure, a spatial grid of BRIR data was gradually coarsened from a maximum resolution of 1 ° / 1 ° until audible artefacts were introduced. Thresholds of audibility were tested for a sound source located at 0 ° / 0 ° with dynamical auralization in two rotational degrees of freedom. The datasets used were acquired in an anechoic environment and in two rooms of different size and reverberation time. Pink noise and acoustical guitar were used as stimuli. A third octave band filter bank analysis of the data sets, using a 1 dB-deviation-in-a-band criterion for the audibility of spectral differences, was in good accordance with the listening test results...|$|R
40|$|Speech {{reception}} thresholds {{were measured}} in virtual rooms {{to investigate the}} influence of reverberation on speech intelligibility for spatially separated targets and interferers. The measurements were realized under headphones, using target sentences and noise or two-voice interferers. The room simulation allowed variation of the absorption coefficient of the room surfaces independently for target and interferer. The direct-to-reverberant ratio and interaural coherence of sources were also varied independently by considering <b>binaural</b> and diotic <b>listening.</b> The main effect of reverberation on the interferer was binaural and mediated by the coherence, in agreement with binaural unmasking theories. It appeared at lower reverberation levels than the effect of reverberation on the target, which was mainly monaural and associated with the direct-to-reverberant ratio, and {{could be explained by}} the loss of amplitude modulation in the reverberant speech signals. This effect was slightly smaller when listening binaurally. Reverberation might also be responsible for a disruption of the mechanism by which the auditory system exploits fundamental frequency differences to segregate competing voices, and a disruption of the “listening in the gaps” associated with speech interferers. These disruptions may explain an interaction observed between the effects of reverberation on the targets and two-voice interferers...|$|R
40|$|Thesis (MAud (Interdisciplinary Health Sciences. Speech-Language and Hearing Therapy) [...] Stellenbosch University, 2008. Individuals are {{increasingly}} undergoing bilateral cochlear implantation {{in an attempt}} to benefit from binaural hearing. The main aim {{of the present study was}} to compare the speech recognition of children fitted with bilateral cochlear implants, under <b>binaural</b> and monaural <b>listening</b> conditions, in quiet and in noise. Ten children, ranging in age from 5 years 7 months to 15 years 4 months, were tested using the Children’s Realistic Index for Speech Perception (CRISP). All the children were implanted with Nucleus multi-channel cochlear implant systems in sequential operations and used the ACE coding strategy bilaterally. The duration of cochlear implant use ranged from 4 years to 8 years 11 months for the first implant and 7 months to 3 years 5 months for the second implant. Each child was tested in eight listening conditions, which included testing in the presence and absence of competing speech. Performance with bilateral cochlear implants was not statistically better than performance with the first cochlear implant, for both quiet and noisy listening conditions. A ceiling effect may have resulted in the lack of a significant finding as the scores obtained during unilateral conditions were already close to maximum. A positive correlation between the length of use of the second cochlear implant and speech recognition performance was established. The results of the present study strongly indicated the need for testing paradigms to be devised which are more sensitive and representative of the complex auditory environments in which cochlear implant users communicate...|$|R
40|$|There {{are many}} {{proposals}} for binaural processing models and their applications {{for a hearing}} assistance system. These models attracts attention because of their directional selectivity in frontal direction of user. Not only binaural models but also any of two-element array system have a well know ambiguity in front-back discrimination which is called as ”front-back confusion” or called as ”cone of confusion” in psychoacoustics. Recently {{it was found that}} spectral cue of sound provides keys to solve this confusion in <b>binaural</b> <b>listening</b> condition and it was also reported the peaks and notches of spectral components play main role to estimate the vertical angle in sagittal coordinate. In this paper, a new method to estimate sound source direction on sagittal coordinate is proposed in order to solve the frontback confusion. It is implemented on an artificial neural network using interaural level and phase differences as input. The results of simulation using a set of Head Related Transfer Function of dummy head show that the averaged estimation error in quadrant segmentation is less than 1. 0 % for various types of sound...|$|E
40|$|Although several neuroimaging {{studies have}} {{reported}} pitch-evoked activations at the lateral end of Heschl's gyrus, it is still under debate whether these findings truly represent activity {{in relation to the}} perception of pitch or merely stimulus-related features of pitch-evoking sounds. We investigated this issue in a functional magnetic resonance imaging (fMRI) experiment using pure tones in noise and dichotic pitch sequences, which either contained a melody or a fixed pitch. Dichotic pitch evokes a sensation of pitch only in <b>binaural</b> <b>listening</b> conditions, while the monaural signal cannot be distinguished from random noise. Our data show similar neural activations for both tones in noise and dichotic pitch, which are perceptually similar, but physically different. Pitch-related activation was found at the lateral end of Heschl's gyrus in both hemispheres, providing new evidence for a general involvement of this region in pitch processing. In line with prior studies, we found melody-related activation in Planum temporale and Planum polare, but not in primary auditory areas. These results support the view of a general representation of pitch in auditory cortex, irrespective of the physical attributes of the pitch-evoking sound...|$|E
40|$|The {{perception}} of sound by human listeners in a listening space, {{such as a}} room or a concert hall is a complicated function {{of the type of}} source sound (speech, oration, concert, music and type), of the intended outcome (comprehension, comfort, enjoyment, localization), of the room (geometry, reflecting surfaces/absorbers, size) and the listener (HRTF, hearing acuity). Quite obviously, reducing this tremendous variability to a single standard is an extremely hard task. The ISO 3382 standard [1] specifies several performance space acoustic parameters (Part 1) and room acoustics parameters (Part 2) for this. These measurements include varying quantities (decay times, clarity, etc.) extracted in varying frequency bands, for both single microphone and binaural measurements. The binaural measurements attempt to account for the effects of the human head, torso and pinnae on the measurements, but do not account for inter-personal variations. To go beyond this standard, one needs to understand the impulse response for every source-receiver position combination of interest. In the case of <b>binaural</b> <b>listening,</b> this {{needs to be done in}} a way that particular individual head-related transfer functions can be employed. MEASUREMENTS USING ARRAYS OF CAMERAS AND MICROPHONES To obtain a finer understanding of the listening characteristics, individua...|$|E
40|$|The human {{auditory}} cortex plays {{a special}} role in speech recognition. It is therefore necessary {{to clarify the}} functional roles of individual auditory areas. We applied {{functional magnetic resonance imaging}} (fMRI) to examine cortical responses to speech sounds, which were presented under the dichotic and diotic (<b>binaural)</b> <b>listening</b> conditions. We found two different response patterns in multiple auditory areas and language-related areas. In the auditory cortex, the medial portion of the secondary auditory area (A 2), as well {{as a part of the}} planum temporale (PT) and the superior temporal gyrus and sulcus (ST), showed greater responses under the dichotic condition than under the diotic condition. This dichotic selectivity may reflect acoustic differences and attention-related factors such as spatial attention and selective attention to targets. In contrast, other parts of the auditory cortex showed comparable responses to the dichotic and diotic conditions. We found similar functional differentiation in the inferior frontal (IF) cortex. These results suggest that multiple auditory and language areas may play a pivotal role in integrating the functional differentiation for speech recognition. © 2000 Academic Press Key Words: speech recognition; functional magneti...|$|E
40|$|Auralizations {{are very}} {{useful in the}} design of {{performing}} arts spaces, where auralization is the process of rendering audible the sound field in a space, {{in such a way as}} to simulate the <b>binaural</b> <b>listening</b> experience at a given position in the modeled space. One of the fundamental modeling inputs to create auralizations is the source directivity. Standard methods involve inputting the measured source directivity, calculating the impulse response and convolving it with a single channel anechoic recording. An initial study was conducted using this method and the results showed significant differences in reverberation time and clarity index when using a directional versus omni-directional source. Further research was conducted focusing on an alternative method of modeling source directivity that involves multi-channel anechoic recordings to create auralizations. Subjective tests were conducted comparing auralizations made with one, four and thirteen channels, with three different instrument types and subjects rated differences in realism. An analysis of variance (ANOVA) was carried out to determine the effect of the number of channels and instrument on realism. The primary result from this study was that subjects rated the auralizations made with an increasing number of channels as sounding more realistic, indicating that when more accurate source directivity information is used a more realistic auralization is possible...|$|E
40|$|This and two {{accompanying}} articles [Breebaart et al., J. Acoust. Soc. Am. 110, 1074 – 1088 (2001); 110, 1105 – 1117 (2001) ] {{describe a}} computational {{model for the}} signal processing in the binaural auditory system. The model consists of several stages of monaural and binaural preprocessing combined with an optimal detector. In the present article the model is tested and validated by comparing its predictions with experimental data for binaural discrimination and masking conditions {{as a function of}} the spectral parameters of both masker and signal. For this purpose, the model is used as an artificial observer in a three-interval, forced-choice adaptive procedure. All model parameters were kept constant for all simulations described in this and the subsequent article. The effects of the following experimental parameters were investigated: center frequency of both masker and target, bandwidth of masker and target, the interaural phase relations of masker and target, and the level of the masker. Several phenomena that occur in <b>binaural</b> <b>listening</b> conditions can be accounted for. These include the wider effective binaural critical bandwidth observed in band-widening NoS conditions, the different masker-level dependence of binaural detection thresholds for narrow- and for wide-band maskers, the unification of IID and ITD sensitivity with binaural detection data, and the dependence of binaural thresholds on frequency...|$|E
40|$|The human {{auditory}} {{system is}} {{able to focus on}} one speech signal and ignore other speech signals in an auditory scene where several conversations are taking place. This ability of the human auditory system {{is referred to as the}} “cocktail-party effect”. This property of human hearing is partly made possible by <b>binaural</b> <b>listening.</b> Interaural time differences (ITDs) and interaural level differences (ILDs) between the ear input signals are the two most important binaural cues for localization of sound sources, i. e. the estimation of source azimuth angles. This paper proposes an implementation of a cocktail-party processor. The proposed cocktail-party processor carries out an auditory scene analysis by estimating the binaural cues corresponding to the directions of the sources. And next, as a function of these cues, suppresses components of signals arriving from non-desired directions, by speech enhancement techniques. The performance of the proposed algorithm is assessed in terms of directionality and speech quality. The proposed algorithm improves existing cocktail-party processors since it combines low computational complexity and efficient source separation. Moreover the advantage of this cocktailparty processor over conventional beam forming is that it enables a highly directional beam over a wide frequency range by using only two microphones. 1. 1. Overview 1...|$|E
40|$|Speech {{recognition}} with hearing protectors 1 The perceived {{negative influence}} of standard hearing protectors on communication {{is a common}} argument for not wearing them. Thus, ‘augmented ’ protectors {{have been developed to}} improve speech intelli-gibility. Nevertheless, their actual benefit remains a point of concern. In this paper, speech perception with active earplugs is compared to standard custom-made earplugs. The two types of active protectors in-cluded amplify the incoming sound respectively with a fixed level or to a user selected fraction of the maximum safe level. For the latter type, minimal and maximal amplification are selected. To compare speech intelligibility, 20 different speech-in-noise fragments are presented to 60 normal-hearing subjects and speech recognition is scored. The back-ground noise is selected from realistic industrial noise samples with different intensity, frequency and temporal characteristics. Statistical analyses suggest that the protectors ’ performance strongly depends on the noise condition. The active protectors with minimal amplification outclass the others for the most difficult and the easiest situations, but they also limit <b>binaural</b> <b>listening.</b> In other conditions, the passive protectors clearly surpass their active counterparts. Subsequently, test fragments are analyzed acoustically to clarify the results. This pro-vides useful information for developing prototypes, but also indicates that tests with human subjects remain essential...|$|E
40|$|The “cocktail party problem” was studied using virtual stimuli whose spatial {{locations}} {{were generated}} using anechoic head-related impulse {{responses from the}} AUDIS database [Blauert et al., J. Acoust. Soc. Am. 103, 3082 (1998) ]. Speech reception thresholds (SRTs) were measured for Harvard IEEE sentences presented from the front {{in the presence of}} one, two, or three interfering sources. Four types of interferer were used: (1) other sentences spoken by the same talker, (2) time-reversed sentences of the same talker, (3) speech-spectrum shaped noise, and (4) speech-spectrum shaped noise, modulated by the temporal envelope of the sentences. Each interferer was matched to the spectrum of the target talker. Interferers were placed in several spatial configurations, either coincident with or separated from the target. Binaural advantage was derived by subtracting SRTs from listening with the “better monaural ear” from those for <b>binaural</b> <b>listening.</b> For a single interferer, there was a binaural advantage of 2 – 4 dB for all interferer types. For two or three interferers, the advantage was 2 – 4 dB for noise and speech-modulated noise, and 6 – 7 dB for speech and time-reversed speech. These data suggest that the benefit of binaural hearing for speech intelligibility is especially pronounced when there are multiple voiced interferers at different locations from the target, regardless of spatial configuration; measurements with fewer or with other types of interferers can underestimate this benefit...|$|E
40|$|How do we {{recognize}} what {{one person is}} saying when others are speaking at the same time? This review summarizes widespread research in psychoacoustics, auditory scene analysis, and attention, all dealing with early processing and selection of speech, which has been stimulated by this question. Important effects occurring at the peripheral and brainstem levels are mutual masking of sounds and “unmasking” resulting from <b>binaural</b> <b>listening.</b> Psychoacoustic models have been developed that can predict these effects accurately, albeit using computational approaches rather than approximations of neural processing. Grouping—the segregation and streaming of sounds—represents a subsequent processing stage that interacts closely with attention. Sounds can be easily grouped—and subsequently selected—using primitive features such as spatial location and fundamental frequency. More complex processing is required when lexical, syntactic, or semantic information is used. Whereas {{it is now clear}} that such processing can take place preattentively, there also is evidence that the processing depth depends on the task-relevancy of the sound. This is consistent with the presence of a feedback loop in attentional control, triggering enhancement of to-be-selected input. Despite recent progress, there are still many unresolved issues: there is a need for integrative models that are neurophysiologically plausible, for research into grouping based on other than spatial or voice-related cues, for studies explicitly addressing endogenous and exogenous attention, for an explanation of the remarkable sluggishness of attention focused on dynamically changing sounds, and for research elucidating the distinction between binaural speech perception and sound localization...|$|E
40|$|Under <b>binaural</b> <b>listening</b> conditions, the {{detection}} of target signals within background masking noise is substantially improved when the interaural phase of the target {{differs from that of}} the masker. Neural correlates of this binaural masking level difference (BMLD) have been observed in the inferior colliculus and temporal cortex, but it is not known whether degeneration of the inferior colliculus would result in a reduction of the BMLD in humans. We used magnetoencephalography to examine the BMLD in 13 healthy adults and 13 patients with progressive supranuclear palsy (PSP). PSP is associated with severe atrophy of the upper brain stem, including the inferior colliculus, confirmed by voxel-based morphometry of structural MRI. Stimuli comprised in-phase sinusoidal tones presented to both ears at three levels (high, medium, and low) masked by in-phase noise, which rendered the low-level tone inaudible. Critically, the BMLD was measured using a low-level tone presented in opposite phase across ears, making it audible against the noise. The cortical waveforms from bilateral auditory sources revealed significantly larger N 1 m peaks for the out-of-phase low-level tone compared with the in-phase low-level tone, for both groups, indicating preservation of early cortical correlates of the BMLD in PSP. In PSP a significant delay was observed in the onset of the N 1 m deflection and the amplitude of the P 2 m was reduced, but these differences were not restricted to the BMLD condition. The results demonstrate that although PSP causes subtle auditory deficits, binaural processing can survive the presence of significant damage to the upper brain stem...|$|E
40|$|The Author(s) 2015. This {{article is}} {{published}} with open access at Springerlink. com Abstract How do we recognize what {{one person is}} saying when others are speaking at the same time? This review sum-marizes widespread research in psychoacoustics, auditory scene analysis, and attention, all dealing with early processing and selection of speech, which has been stimulated by this ques-tion. Important effects occurring at the peripheral and brainstem levels are mutual masking of sounds and Bunmasking ^ resulting from <b>binaural</b> <b>listening.</b> Psychoacoustic models have been de-veloped that can predict these effects accurately, albeit using computational approaches rather than approximations of neural processing. Grouping—the segregation and streaming of sounds—represents a subsequent processing stage that interacts closely with attention. Sounds can be easily grouped—and sub-sequently selected—using primitive features such as spatial lo-cation and fundamental frequency. More complex processing is required when lexical, syntactic, or semantic information is used. Whereas {{it is now clear}} that such processing can take place preattentively, there also is evidence that the processing depth depends on the task-relevancy of the sound. This is consistent with the presence of a feedback loop in attentional control, trig-gering enhancement of to-be-selected input. Despite recent progress, there are still many unresolved issues: there is a need for integrative models that are neurophysiologically plausible, for research into grouping based on other than spatial or voice-related cues, for studies explicitly addressing endogenous and exogenous attention, for an explanation of the remarkable sluggishness of attention focused on dynamically changing sounds, and for research elucidating the distinction between bin-aural speech perception and sound localization...|$|E
