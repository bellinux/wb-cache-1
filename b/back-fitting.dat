35|0|Public
50|$|There were 31 {{reportable}} {{events in}} the Swiss nuclear power plants during 2011. Of these, 30 were assigned to level 0 of the INES International Nuclear Event Scale and one was classified at level 1 on the scale. Seven events concerned the Beznau nuclear power plant, units 1 and 2; five events affected Gösgen, and there were eleven at the Leibstadt nuclear power plant, four at the Mühleberg nuclear power plant, three in the nuclear facilities of the Paul Scherrer Institute, {{and one in the}} research reactor of the Swiss Federal Institute of Technology, Lausanne. ENSI did not register any events at the Swiss national central interim storage facility for radioactive waste (ZWILAG) or in the research reactor at the University of Basel. The INES-1 event was reported as the consequence of a potential blockage of the water intake for the special emergency system during an extreme flood at the Mühleberg nuclear power plant. This finding prompted the operator of the power plant, BKW-FMB Energie AG, to shut the plant down prior to the date of the planned maintenance outage in order to carry out <b>back-fitting.</b>|$|E
5000|$|For {{the next}} ten years, the level of Thirlmere was twenty foot above that of the old lake: lowland {{pasturage}} was lost, but little housing. The straight level road on the east bank was favourably commented upon in accounts of cycling tours of the lakes, and it was widely thought - as James Lowther (MP for Penrith {{and the son of}} the Westmorland MP who had spoken against the 1878 Bill at its Second Reading) said at the Third Reading of a Welsh Private Bill - [...] "the beauty of Thirlmere had been improved" [...] by the scheme. Thirlmere water reached Manchester through a single 40-inch diameter cast iron pipe; due to leakage, only about 80% of the intended 10 million gallons a day supply reached Manchester. and, as early as May 1895 more than half the additional supply was accounted for by increased consumption. An Act of 1891 had allowed Manchester to specify water-closets for all new buildings and modification of existing houses; Manchester now encouraged <b>back-fitting</b> of water-closets, and reduced the additional charge for baths.The average daily consumption in 1899 was 32.5 million gallons a day, with 41 million gallons being consumed in a single day at the end of August In June 1900, Manchester Corporation accepted the recommendation of its Waterworks Committee that a second pipe be laid from Thirlmere; it insisted that despite any potential shortfall in water supply a 'water-closet' policy should be continued. The first section of the second pipe was laid at Troutbeck in October 1900. Hill noted that it would take three or four years to complete the second pipe, at the current rate of increase of consumption as soon as the second pipe was completed it would be time to start on the third.|$|E
40|$|Additive {{regression}} {{models have been}} shown to be useful in many situations. Numerical estimation of these models is usually done using the <b>back-fitting</b> technique. This iterative numerical procedure converges very fast but has the disadvantage of a complicated `hat matrix. ' This paper proposes an estimator with an explicit `hat matrix' which does not use <b>back-fitting.</b> The asymptotic normality of the estimator is proved. We also investigate a variable selection procedure using the proposed estimator and prove that asymptotically the procedure finds the correct variable set with probability 1. A simulation study is presented investigating the practical performance of the procedure...|$|E
40|$|The {{paper is}} {{concerned}} with the practical aspects of a unified approach to the identification and estimation of multiple-input, single-output (MISO) transfer function models for both continuous and discrete-time systems. The estimation algorithms considered in the paper are based on the Refined Instrumental Variable (RIV) approach to identification and estimation, where the MISO model denominator polynomials are normally constrained to be equal. Unconstrained RIV estimation presents a more difficult problem and it is necessary to exploit an iterative, <b>back-fitting</b> routine to handle this more general situation. The paper focuses on the practical realization of this <b>back-fitting</b> algorithm, including its initiation from either common denominator MISO or repeated SISO estimation. The rivcdd algorithm for continuous-time model estimation, as implemented in the CAPTAIN Toolbox for Matlab is then used in three practical examples: first, the modelling of solute transport and dispersion in a water body; secondly, modelling for two control problems, namely a pair of connected laboratory DC motors and a nonlinear wind turbine simulation...|$|E
40|$|AbstractThe {{procedures}} for estimations of the parametric component and the nonparametric component on generalized semi-varying coefficient models are developed as follows: The coefficient {{functions in the}} parametric component are estimated via local quasi-likelihood method and average method. After replacing the coefficients in the parametric component by the obtained estimators, quasi-likelihood method and <b>back-fitting</b> technical are employed to produce the estimators of the function coefficients in nonparametric part. For this procedure, the asymptotic normality of the estimations on parametric component and nonparametric component are investigated. Two simulated examples show that the procedure is effective...|$|E
40|$|An {{enhanced}} {{version of}} the Lee-Carter modelling approach to mortality forecasting, which has been extended to include an age modulated cohort index {{in addition to the}} standard age modulated period index, is described and tested for prediction robustness. Life expectancy and annuity value predictions, at pensioner ages and for various periods are compared, both with and without the age modulated cohort index, for the England & Wales male mortality experience. The simulation of prediction intervals for these indices of interest is discussed in detail. Mortality forecasting Age-period-cohort effects Forecast statistics <b>Back-fitting</b> Data truncation...|$|E
40|$|Despite their {{widespread}} {{use for the}} analysis of economic questions, a formal and systematic calibration methodology has not yet been developed for Auerbach-Kotlikoff (Auerbach and Kotlikoff 1987) overlapping generations (AK-OLG) models. Calibration as estimation in macroeconomics involves choosing free parameters by matching moments of simulated models with those of the data. This paper maps this approach into the framework of AK-OLG models. The paper further evaluates the <b>back-fitting</b> properties of three different versions of a prototype AK-OLG model along a number of dimensions of mostly US data for the time period 1960 - 2003. ...|$|E
40|$|Natural Science Foundation of Fujian Province of China [S 0750018]In {{the present}} article, we propose and study {{a new class}} of {{nonlinear}} autoregressive moving-average (ARMA) models, in which each moving-average (MA) coefficient is enlarged to an arbitrary univariate function. We first provide a sufficient condition for the existence of the stationary solution and further discuss the moment structure. We investigate the estimation method to the proposed models. The global estimates of parameters and local linear estimates of functional coefficients are obtained by using a <b>back-fitting</b> algorithm. For testing whether the functional coefficients are some specified parametric forms, a bootstrap test approach is provided. The proposed models are illustrated by both simulated and real data examples...|$|E
40|$|In this paper, the functional-coefficient {{partially}} {{linear regression}} (FCPLR) model is proposed by combining nonparametric and functional-coefficient regression (FCR) model. It includes the FCR {{model and the}} nonparametric regression (NPR) model as its special cases. It is also a generalization of the partially linear regression (PLR) model obtained by replacing the parameters in the PLR model with some functions of the covariates. The local linear technique and the integrated method are employed to give initial estimators of all functions in the FCPLR model. These initial estimators are asymptotically normal. The initial estimator of the constant part function shares the same bias as the local linear estimator of this function in the univariate nonparametric model, but the variance of the former is bigger {{than that of the}} latter. Similarly, initial estimators of every coefficient function share the same bias as the local linear estimates in the univariate FCR model, but the variance of the former is bigger than that of the latter. To decrease the variance of the initial estimates, a one-step <b>back-fitting</b> technique is used to obtain the improved estimators of all functions. The improved estimator of the constant part function has the same asymptotic normality property as the local linear nonparametric regression for univariate data. The improved estimators of the coefficient functions have the same asymptotic normality properties as the local linear estimates in FCR model. The bandwidths and the smoothing variables are selected by a data-driven method. Both simulated and real data examples related to nonlinear time series modeling are used to illustrate the applications of the FCPLR model. <b>Back-fitting</b> technique Functional-coefficient model Local linear polynomial technique Nonlinear time series...|$|E
40|$|In {{this paper}} we extend semiparametric mixed linear models with normal errors to {{elliptical}} errors {{in order to}} permit distributions with heavier and lighter tails than the normal ones. Penalized likelihood equations are applied to derive the maximum penalized likelihood estimates (MPLEs) which appear to be robust against outlying observations {{in the sense of}} the Mahalanobis distance. A reweighed iterative process based on the <b>back-fitting</b> method is proposed for the parameter estimation and the local influence curvatures are derived under some usual perturbation schemes to study the sensitivity of the MPLEs. Two motivating examples preliminarily analyzed under normal errors are reanalyzed considering some appropriate elliptical errors. The local influence approach is used to compare the sensitivity of the model estimates. CAPESCAPESCNPqCNPqFAPESP (Brazil) FAPESP, BrazilFONDECYT (Chile) FONDECYT-Chile [1070919...|$|E
40|$|Decomposition of {{absorption}} spectra using linear regression {{has been proposed}} for calculating concentrations of mixture compounds. The method is based on projecting the observed mixture spectrum onto the linear space generated by the reference spectra that correspond to the individual components comprising the mixture. The computed coefficients are then used as estimates for concentration of the components that comprise the mixture. Existence of unknown components in the mixture, however, introduces bias on the obtained concentration estimates. We extend the usual linear regression model to an additive semi-parametric model to take the unknown component into account, estimate the absorption profile of the unknown component, and obtain concentration estimates of the known compounds. A standard <b>back-fitting</b> method {{as well as a}} mean weighted least squares criterion are applied. The techniques are illustrated on simulated {{absorption spectra}}...|$|E
40|$|In {{the last}} decades, {{considerable}} {{attention has been}} paid to the collection of antimicrobial resistance data, with the aim of monitoring non-wild-type isolates. This monitoring is performed based on minimum inhibition concentration (MIC) values, which are collected through dilution experiments. We present a semi-parametric mixture model to estimate the entire MIC density on the continuous scale. The parametric first component is extended with a non-parametric second component and a new <b>back-fitting</b> algorithm, based on the Vertex Exchange Method, is proposed. Our data example shows how to estimate the MIC density for Escherichia coli tested for ampicillin and how to use this estimate for model-based classification. A simulation study was performed, showing the promising behavior of the new method, both in terms of density estimation as well as classification...|$|E
40|$|The {{relative}} {{merits of}} different parametric models for making life expectancy and annuity value predictions at both pensioner and adult ages are investigated. This study builds on current published research and considers recent model enhancements {{and the extent}} to which these enhancements address the deficiencies that have been identified of some of the models. The England & Wales male mortality experience is used to conduct detailed comparisons at pensioner ages, having first established a common basis for comparison across all models. The model comparison is then extended to include the England & Wales female experience and both the male and female USA mortality experiences over a wider age range, encompassing also the working ages. Mortality forecasting Binomial response models Age-period effects Age-period-cohort effects Forecast statistics Model and forecast comparison <b>Back-fitting...</b>|$|E
40|$|The paper {{outlines}} how improved {{estimates of}} time variable parameters in models of stochastic dynamic {{systems can be}} obtained using recursive filtering and fixed interval smoothing techniques, with the associated hyper-parameters optimized by maximum likelihood based on prediction error decomposition. It then shows how, by exploiting special data re-ordering and <b>back-fitting</b> procedures, similar recursive parameter estimation techniques can be utilized to estimate much more rapid State Dependent Parameter (SDP) variations. In this manner, {{it is possible to}} identify and estimate a widely applicable class of nonlinear stochastic systems, as illustrated by several examples that include simulated and real data from chaotic processes. Finally, the paper points out that such SDP models can form the basis for new methods of signal processing, automatic control and state estimation for nonlinear stochastic systems...|$|E
40|$|ABSTRACT: The K-stiffness Method is an empirically-developed {{working stress}} method used to compute {{reinforcement}} loads for the internal stability design of geosynthetic-reinforced soil walls under serviceability conditions. In this paper, additional data from Japanese case studies for five full-scale field and three full-scale laboratory geosynthetic-reinforced soil walls {{are added to}} the database that was used to calibrate the original K-stiffness Method. One more case study from an instrumented wall in the USA is also introduced. Measured loads are compared with predicted loads using the current AASHTO Simplified Method and {{a modified version of the}} K-stiffness Method that has been adjusted by <b>back-fitting</b> model parameters to the extended database. The AASHTO Simplified Method is shown to be excessively conservative (on average) with respect to accurate prediction of reinforcement loads and to correlate poorly with measured values. The modified K-stiffness Method is demonstrated by statistical analysis to give ratios (bias) of averag...|$|E
40|$|AbstractGeographically Weighted Regression (GWR) {{is a local}} {{technique}} that models spatially varying relationships, where Euclidean distance is traditionally used as default in its calibration. However, empirical work {{has shown that the}} use of non-Euclidean distance metrics in GWR can improve model performance, {{at least in terms of}} predictive fit. Furthermore, the relationships between the dependent and each independent variable may have their own distinctive response to the weighting computation, which is reflected by the choice of distance metric. Thus, we propose a <b>back-fitting</b> approach to calibrate a GWR model with parameter-specific distance metrics. To objectively evaluate this new approach, a simple simulation experiment is carried out that not only enables an assessment of prediction accuracy, but also parameter accuracy. The results show that the approach can provide both more accurate predictions and parameter estimates, than that found with standard GWR. Accurate localised parameter estimation is crucial to GWR's main use as a method to detect and assess relationship non-stationarity...|$|E
40|$|Spatiotemporal {{models for}} sulphur dioxide {{pollution}} over Europe are considered within an additive model framework. A suitable {{description of the}} spatiotemporal correlation structure of the data is constructed and incorporated {{in the analysis of}} the additive model, to ensure that standard errors and other forms of analysis reflect the form of variation that is exhibited by the data. To deal with the large sample size, an updating formula based on binning is derived to provide a computationally manageable implementation of the <b>back-fitting</b> algorithm. Interaction terms involving space, time and seasonal effects are also considered. This requires three-dimensional smoothing which is implemented by repeated application of lower dimensional marginal smoothing operations. The properties of this form of smoothing are examined and the estimators are shown to have first-order behaviour, inherited from the marginal operations, which is equivalent to the full multivariate versions. These models and methods are applied to the sulphur dioxide data, allowing detailed and informative descriptions of the spatiotemporal patterns to be create...|$|E
40|$|This study {{examines}} the load and resistance factor design calibration {{for the ultimate}} pullout limit state for steel strip reinforced soil walls (SSWs). The tensile loads and pullout capacities of reinforcement strips that were measured during full-scale tests are compared with predicted (nominal) values using analytical models recommended by the Public Works Research Center (PWRC) in Japan. Modified load and resistance models are also proposed in this study. The new load model preserves the general form of the current PWRC load equation. A new pullout capacity model is proposed that has {{the same number of}} coefficient terms as the current PWRC equation. The coefficients in both new equations are empirical and are selected by <b>back-fitting</b> to measured load and pullout capacity values to achieve a bias mean equal to one and a low coefficient of variation of bias values. Here, bias is the ratio of measured to predicted value. This study demonstrates the influence of model accuracy on the computed resistance factor in LRFD calibration of SSWs for the pullout limit state...|$|E
40|$|Changes in {{observational}} data {{over time}} can be severely distorted by errors in measurements, sampling, or reporting. Here, we show how smooth trends in vector time series can be separated from one or two abrupt level shifts that occur simultaneously in all coordinates. Trends are modelled nonparametrically, whereas abrupt changes {{and the impact of}} covariates are modelled parametrically. The model is estimated using a <b>back-fitting</b> algorithm in which estimation of smooth trends is alternated with estimation of regression coefficients for covariates and assessment of sudden level shifts. The proposed method is adaptive {{in the sense that the}} degree of smoothing over time and across coordinates is controlled by a roughness penalty and cross-validation procedure that automatically identifies the interdependence of the analysed data. Furthermore, it uses a resampling technique that can accommodate correlated error terms in the assessment of the uncertainty of both smooth trends and discontinuities. The method is applied to water quality data from Swedish national monitoring programmes to illustrate how known discontinuities can be quantified and how previously unrecognized discontinuities can be detected...|$|E
40|$|Regression splines are smooth, flexible, and parsimonious nonparametric {{function}} estimators. They {{are known}} to be sensitive to knot number and placement, but if assumptions such as monotonicity or convexity may be imposed on the regression function, the shaperestricted regression splines are robust to knot choices. Monotone regression splines were introduced by Ramsay [Statist. Sci. 3 (1998) 425 – 461], but were limited to quadratic and lower order. In this paper an algorithm for the cubic monotone case is proposed, and the method is extended to convex constraints and variants such as increasingconcave. The restricted versions have smaller squared error loss than the unrestricted splines, although they have the same convergence rates. The relatively small degrees of freedom of the model and the insensitivity of the fits to the knot choices allow for practical inference methods; the computational efficiency allows for <b>back-fitting</b> of additive models. Tests of constant versus increasing and linear versus convex regression function, when implemented with shape-restricted regression splines, have higher power than the standard version using ordinary shape-restricted regression. 1. Introduction. W...|$|E
40|$|This paper {{proposes a}} new nonparametric {{estimator}} for general regression functions with multiple regressors. The method used here {{is motivated by}} a remarkable result derived by Kolmogorov (1957) and later tightened by Lorentz (1966). In short, they show that any continuous function of multiple variables can be written as univariate functions. As it stands, this representation is difficult to estimate because of its lack of smoothness. Hence we propose to use a generalization of their representation that allows for the univariate functions to be differentiable. The model will be estimated using B-splines, which have excellent numerical properties. A crucial restriction in this representation {{is that some of}} the functions must be increasing. One of the main contributions of this paper is that we develop a method for imposing monotonicity on the cubic B-splines, a priori, such that the estimator is dense in the set of all monotonic cubic B-splines. A simulation experiment shows that the estimator works well when optimization is performed by using the <b>back-fitting</b> algorithm. The monotonic restriction has many other applications besides the one presented here, such as estimating a demand function. With only r + 2 more constraints, it is also possible to impose concavity. ...|$|E
40|$|Numerous {{approaches}} are {{proposed in the}} literature for non-stationarity marginal extreme value inference, including different model param-eterisations with respect to covariate, and different inference schemes. The objective {{of this article is}} to compare some of these procedures critically. We generate sample realisations from generalised Pareto distributions, the parameters of which are smooth functions of a single smooth periodic covariate, specified to reflect the characteristics of actual samples of significant wave height with direction considered in the literature in the recent past. We estimate extreme values models (1) using Constant, Fourier, B-spline and Gaussian Process parame-terisations for the functional forms of generalised Pareto shape and (adjusted) scale with respect to covariate and (2) maximum likelihood and Bayesian inference procedures. We evaluate the relative quality of inferences by estimating return value distributions for the response corresponding to a time period of 10 × the (assumed) period of the original sample, and compare estimated return values distributions with the truth using Kullback-Leibler, Cramer-von Mises and Kolmogorov-Smirnov statistics. We find that Spline and Gaussian Process param-eterisations estimated by maximum penalised likelihood using the <b>back-fitting</b> algorithm, or Monte Carlo Markov chain inference using the mMALA algorithm, perform equally well in terms of quality of inference and computational efficiency, and generally perform better than alternatives...|$|E
40|$|ABSTRACTA {{historical}} {{review of}} in-vessel melt retention (IVR) is given, {{which is a}} severe accident mitigation measure extensively applied in Generation III pressurized water reactors (PWRs). The idea of IVR actually originated from the <b>back-fitting</b> of the Generation II reactor Loviisa VVER- 440 in order {{to cope with the}} core-melt risk. It was then employed in the new deigns such as Westinghouse AP 1000, the Korean APR 1400 as well as Chinese advanced PWR designs HPR 1000 and CAP 1400. The most influential phenomena on the IVR strategy are in-vessel core melt evolution, the heat fluxes imposed on the vessel by the molten core, and the external cooling of the reactor pressure vessel (RPV). For in-vessel melt evolution, past focus has only been placed on the melt pool convection in the lower plenum of the RPV; however, through our review and analysis, we believe that other in-vessel phenomena, including core degradation and relocation, debris formation, and coolability and melt pool formation, may all contribute to the final state of the melt pool and its thermal loads on the lower head. By looking into previous research on relevant topics, we aim to identify the missing pieces in the picture. Based {{on the state of the}} art, we conclude by proposing future research needs...|$|E
40|$|At {{the end of}} 2008, the IAEA {{launched}} a new task on ¿Guidance for Designers and Operators and Measures to facilitate the implementation of Safeguards at Future Nuclear Cycle Facilities¿, contributed by EURATOM and other MS Support Programmes, whose goal is to formulate ¿safeguards by design¿, or SBD, Guidelines to designers and operators. SBD {{is a process that}} facilitates the implementation of international safeguards by taking into account requirements and guidelines very early in the design phase. To this scope, the legal framework and the interaction among the stake-holders need to be improved. The overall process can thus be made more effective and efficient without costly <b>back-fitting</b> and iterations. In this context, at the end of 2008, the IAEA {{launched a}} new task on ¿Guidance for Designers and Operators and Measures to facilitate the implementation of Safeguards at Future Nuclear Cycle Facilities¿, with contributions by EURATOM and other Member State Support Programmes (MSSP). A first set of high level guidelines of the IAEA Safeguards by Design series was drafted by EURATOM experts, and will be the basis for further improvements. This paper will develop on the contents of the document, as well as on methodological developments. Facility specific guidelines will {{have to be prepared to}} serve as reference for the design of new evolutionary and innovative facilities. All this will be achieved within useful deadlines with the contributions of other support programmes. JRC. DG. E. 9 -Nuclear security (Ispra...|$|E
40|$|Abstract. The paper {{describes}} a simple numerical FLAC model {{that was developed}} to simulate the dynamic response of two instrumented reduced-scale model reinforced soil walls constructed on a 1 -g shaking table. The models were 1 m high by 1. 4 m wide by 2. 4 m long and were constructed with a uniform size sand backfill, a polymeric geogrid reinforcement material with appropriately scaled stiffness, and a structural full-height rigid panel facing. The wall toe was constructed to simulate a perfectly hinged toe (i. e. toe allowed to rotate only) in one model and an idealized sliding toe (i. e. toe allowed to rotate and slide horizontally) in the other. Physical and numerical models were subjected to the same stepped amplitude sinusoidal base acceleration record. The material properties of the component materials (e. g. backfill and reinforcement) were determined from independent laboratory testing (reinforcement) and by <b>back-fitting</b> results of a numerical FLAC model for direct shear box testing to the corresponding physical test results. A simple elastic-plastic model with Mohr-Coulomb failure criterion for the sand was judged to give satisfactory agreement with measured wall results. The numerical results are also compared to closed-form solutions for reinforcement loads. In most cases predicted and closed-form solutions fall within the accuracy of measured loads based on ± 1 standard deviation applied to physical measurements. The paper summarizes important lessons learned and implications to the seismic design and performance of geosynthetic reinforced soil walls...|$|E
40|$|We {{develop a}} Bayesian “sum-of-trees ” model where each tree is {{constrained}} by a prior to be a weak leaner. Fitting and inference are accomplished via an iterative <b>back-fitting</b> MCMC algorithm. This model {{is motivated by}} ensemble methods in general, and boosting algorithms in particular. Like boosting, each weak learner (i. e., each weak tree) contributes a small amount to the overall model, and the training of a weak learner is conditional on the estimates for the other weak learners. The differences from boosting algorithms are just as striking as the similarities: BART is defined by a statistical model: a prior and a likelihood, while boosting is defined by an algorithm. MCMC is used both to fit the model and to quantify inferential uncertainty through the variation of the posterior draws. The BART modelling strategy can also be viewed {{in the context of}} Bayesian non-parametrics. The key idea is to use a model which is rich enough to respond to a variety of signal types, but constrained by the prior from overreacting to weak signals. The ensemble approach provides for a rich base model form which can expand as needed via the MCMC mechanism. The priors are formulated so as to be interpretable, relatively easy to specify, and provide results that are stable across a wide range of prior hyperparameter values. The MCMC algorithm, which exhibits fast burn-in and good mixing, can be readily used for model averaging and for uncertainty assessment...|$|E
40|$|AbstractIn this paper, the functional-coefficient {{partially}} {{linear regression}} (FCPLR) model is proposed by combining nonparametric and functional-coefficient regression (FCR) model. It includes the FCR {{model and the}} nonparametric regression (NPR) model as its special cases. It is also a generalization of the partially linear regression (PLR) model obtained by replacing the parameters in the PLR model with some functions of the covariates. The local linear technique and the integrated method are employed to give initial estimators of all functions in the FCPLR model. These initial estimators are asymptotically normal. The initial estimator of the constant part function shares the same bias as the local linear estimator of this function in the univariate nonparametric model, but the variance of the former is bigger {{than that of the}} latter. Similarly, initial estimators of every coefficient function share the same bias as the local linear estimates in the univariate FCR model, but the variance of the former is bigger than that of the latter. To decrease the variance of the initial estimates, a one-step <b>back-fitting</b> technique is used to obtain the improved estimators of all functions. The improved estimator of the constant part function has the same asymptotic normality property as the local linear nonparametric regression for univariate data. The improved estimators of the coefficient functions have the same asymptotic normality properties as the local linear estimates in FCR model. The bandwidths and the smoothing variables are selected by a data-driven method. Both simulated and real data examples related to nonlinear time series modeling are used to illustrate the applications of the FCPLR model...|$|E
40|$|The {{characteristics}} of extreme waves in hurricane dominated regions vary systematically {{with a number}} of covariates, including location and storm direction. Reliable estimation of design criteria requires incorporation of covariate effects within extreme value models. We present a spatio-directional model for extreme waves in the Gulf of Mexico, motivated by the non-homogeneous Poisson model for peaks over threshold. The model is applied to storm peak significant wave height HS for arbitrary geographic areas from the proprietary GOMOS hindcast for the US region of the Gulf of Mexico for the period 1900 - 2005. At each location, directional variability is modelled using a non-parametric directional location and scale; data are standardised (or ”whitened”) with respect to local directional location and scale to remove directional effects. For a suitable choice of threshold, the rate of occurrence of threshold exceedences of whitened storm peak HS with direction per location is modelled as a Poisson process. The size of threshold exceedences is modelled using a generalised Pareto form, the parameters of which vary smoothly in space, and are estimated within a roughness penalised likelihood framework using natural thin plate spline forms in two spatial dimensions. By re-parameterising the generalised Pareto model in terms of asymptotically independent parameters, an efficient <b>back-fitting</b> algorithm to estimate the natural thin plate spline model is achieved. The algorithm is motivated in an appendix. Design criteria, estimated by simulation, are illustrated for a typical neighbourhood of 1 Copyright c © 2009 by ASME 17 × 17 grid locations. Applications to large areas consisting of more than 2500 grid locations are outlined. ...|$|E
40|$|This paper {{proposes a}} new nonparametric {{estimator}} for general regression functions with multiple regressors. The method used here {{is motivated by}} a remarkable result derived by Kolmogorov (1957) and later tightened by Lorentz (1966). In short, any continuous function f(x_ 1, [...] .,x_d) has the representation G[a_ 1 P_ 1 (x_ 1) + [...] . + a_d P_ 1 (x_d) ] + [...] . + G[a_ 1 P_m(x_ 1) + [...] . + a_d P_m(x_d) ], m = 2 d+ 1, where G(.) is a continuous function, P_k(.), k= 1, [...] ., 2 d+ 1, is Lipschitz of order one and strictly increasing, and a_j, j= 1, [...] .,d, is some constant. Generalizing this result, we propose the following estimator, g_ 1 [a_ 1, 1 p_ 1 (x_ 1) + [...] . + a_d, 1 p_ 1 (x_d) ] + [...] . + g_m[a_ 1,d P_m(x_ 1) + [...] . + a_d,d p_m(x_d) ], where both g_k(.) and p_k(.) are twice continuously differentiable. These functions are estimated using regression cubic B-splines, which have excellent numerical properties. This problem has been previously intractable because there existed no method for imposing monotonicity on the p_k(.) 's, a priori, such that the estimator is dense in the set of all monotonic cubic B-splines. We derive a method that only requires 2 (r+ 1) + 1 restrictions, where r {{is the number of}} interior knots. Rates of convergence in L_ 2 are the same as the optimal rate for the one-dimensional case. A simulation experiment shows that the estimator works well when optimization is performed by using the <b>back-fitting</b> algorithm. The monotonic restriction has many other applications besides the one presented here, such as estimating a demand function. With only r+ 2 more constraints, it is also possible to impose concavity. ...|$|E
40|$|International audienceBackgroundThe goal of {{genome-wide}} prediction (GWP) is {{to predict}} phenotypes based on marker genotypes, often obtained through {{single nucleotide polymorphism}} (SNP) chips. The major problem with GWP is high-dimensional data from many thousands of SNPs scored on several thousands of individuals. A large number of methods {{have been developed for}} GWP, which are mostly parametric methods that assume statistical linearity and only additive genetic effects. The Bayesian additive regression trees (BART) method was recently proposed and is based on the sum of nonparametric regression trees with the priors being used to regularize the parameters. Each regression tree is based on a recursive binary partitioning of the predictor space that approximates an unknown function, which will automatically model nonlinearities within SNPs (dominance) and interactions between SNPs (epistasis). In this study, we introduced BART and compared its predictive performance with that of the LASSO, Bayesian LASSO (BLASSO), genomic best linear unbiased prediction (GBLUP), reproducing kernel Hilbert space (RKHS) regression and random forest (RF) methods. ResultsTests on the QTLMAS 2010 simulated data, which are mainly based on additive genetic effects, show that cross-validated optimization of BART provides a smaller prediction error than the RF, BLASSO, GBLUP and RKHS methods, and is almost as accurate as the LASSO method. If dominance and epistasis effects are added to the QTLMAS 2010 data, the accuracy of BART relative to the other methods was increased. We also showed that BART can produce importance measures on the SNPs through variable inclusion proportions. In evaluations using real data on pigs, the prediction error was smaller with BART than with the other methods. ConclusionsBART was shown to be an accurate method for GWP, in which the regression trees guarantee a very sparse representation of additive and complex non-additive genetic effects. Moreover, the Markov chain Monte Carlo algorithm with Bayesian <b>back-fitting</b> provides a computationally efficient procedure that is suitable for high-dimensional genomic data...|$|E
40|$|This paper {{proposes a}} nonparametric {{estimator}} for general regression functions with multiple regressors. The method used here {{is motivated by}} a remarkable result derived by Kolmogorov (1957) and later tightened by Lorentz (1966). In short, any continuous function f(x 1; : : :; x d) has the representation P 2 d+ 1 k= 1 g(1 OE k (x 1) + ΔΔΔ + d OE k (x d)), where g(Δ) is a continuous function, OE k (Δ), k = 1; : : :; 2 d + 1, is Lipschitz of order one and strictly increasing, and j, j = 1; : : :; d, is some constant. Extending this result {{to the case of}} smoother functions, we restrict f(Δ) to be of the form k= 1 g k (k; 1 OE k (x 1) + ΔΔΔ + k;d OE k (x d)), 1 t ! 1, where both g k (Δ) and OE k (Δ) are three times continuously differentiable and OE k (Δ) is nondecreasing. These functions are estimated using regression cubic B-splines, which have excellent numerical and approximating properties. One of the main contributions of this paper is that we develop a method for imposing monotonicity on the cubic B-splines, a priori, such that the estimator is dense in the set of all monotonic cubic B-splines. The method requires only 2 (r+ 1) + 1 restrictions per each OE k (Δ), where r is the number of interior knots. Rates of convergence in L 2 are the same as the optimal rate for the one-dimensional case. A simulation experiment shows that the estimator works well when t is small, f(Δ) does not belong to this class of linear superpositions, and optimization is performed using the <b>back-fitting</b> algorithm. The monotonic restriction has many other applications besides the one presented here, such as estimating a demand function. With only r + 2 more constraints, it is also possible to impose [...] ...|$|E
40|$|Extreme river flows {{can lead}} to {{inundation}} of floodplains, with consequent impacts for society, {{the environment and the}} economy. Flood risk estimates rely on river flow records, hence a good understanding of the patterns in river flow, and, in particular, in extreme river flow, is important to improve estimation of risk. In Scotland, a number of studies suggest a West to East rainfall gradient and increased variability in rainfall and river flow. This thesis presents and develops a number of statistical methods for analysis of different aspects of extreme river flows, namely the variability, temporal trend, seasonality and spatial dependence. The methods are applied to a large data set, provided by SEPA, of daily river flow records from 119 gauging stations across Scotland. The records range in length from 10 up to 80 years and are characterized by non-stationarity and long-range dependence. Examination of non-stationarity is done using wavelets. The results revealed significant changes in the variability of the seasonal pattern over the last 40 years, with periods of high and low variability associated with flood-rich and flood-poor periods respectively. Results from a wavelet coherency analysis suggest significant influence of large scale climatic indices (NAO, AMO) on river flow. A quantile regression model is then developed based on an additive regression framework using P-splines, where the parameters are fitted via weighted least squares. The proposed model includes a trend and seasonal component, estimated using the <b>back-fitting</b> algorithm. Incorporation of covariates and extension to higher dimension data sets is straightforward. The model is applied to a set of eight Scottish rivers to estimate the trend and seasonality in the 95 th quantile of river flow. The results suggest differences in the long term trend between the East and the West and a more variable seasonal pattern in the East. Two different approaches are then considered for modelling spatial extremes. The first approach consists of a conditional probability model and concentrates on small subsets of rivers. Then a spatial quantile regression model is developed, extending the temporal quantile model above to estimate a spatial surface using the tensor product of the marginal B-spline bases. Residual spatial correlation using a Gaussian correlation function is incorporated into standard error estimation. Results from the 95 th quantile fitted for individual months suggest changes in the spatial pattern of extreme river flow over time. The extension of the spatial quantile model to build a fully spatio-temporal model is briefly outlined and the main statistical issues identified...|$|E

