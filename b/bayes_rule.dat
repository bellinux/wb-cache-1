407|219|Public
25|$|Attack {{trees are}} related to the {{established}} fault tree formalism. Fault tree methodology employs boolean expressions to gate conditions when parent nodes are satisfied by leaf nodes. By including a priori probabilities with each node, it is possible to perform calculate probabilities with higher nodes using <b>Bayes</b> <b>Rule.</b> However, in reality accurate probability estimates are either unavailable or too expensive to gather. With respect to computer security with active participants (i.e., attackers), the probability distribution of events are probably not independent nor uniformly distributed, hence, naive Bayesian analysis is unsuitable.|$|E
2500|$|Initially GG, SS and GS {{are equally}} likely. Therefore, by <b>Bayes</b> <b>rule</b> the {{conditional}} {{probability that the}} chosen box is GG, given we have observed a gold coin, is: ...|$|E
5000|$|Then why is {{the notion}} of {{generalized}} <b>Bayes</b> <b>rule</b> an improvement? It is indeed equivalent to the notion of <b>Bayes</b> <b>rule</b> when a <b>Bayes</b> <b>rule</b> exists and all [...] have positive probability. However, no <b>Bayes</b> <b>rule</b> exists if the Bayes risk is infinite (for all [...] ). In this case it is still useful to define a generalized <b>Bayes</b> <b>rule</b> , which at least chooses a minimum-expected-loss action [...] for those [...] for which a finite-expected-loss action does exist. In addition, a generalized <b>Bayes</b> <b>rule</b> may be desirable because it must choose a minimum-expected-loss action [...] for every , whereas a <b>Bayes</b> <b>rule</b> would be allowed to deviate from this policy on a set [...] of measure 0 without affecting the Bayes risk.|$|E
50|$|Conversely, while <b>Bayes</b> <b>rules</b> {{with respect}} to proper priors are {{virtually}} always admissible, generalized <b>Bayes</b> <b>rules</b> corresponding to improper priors need not yield admissible procedures. Stein's example is one such famous situation.|$|R
40|$|Contract No. AF 49 (638) - 929 The {{problem of}} {{choosing}} {{the largest of}} n means is considered as a multiple decision problem which is generated from n component two-decision problems. With additive losses <b>Bayes</b> <b>rules</b> for the component problems yield <b>Bayes</b> <b>rules</b> for the multiple decision problem. Some properties of these <b>Bayes</b> <b>rules</b> are found. Also a conservative-near-Bayes rule is presented with tabled values {{for any number of}} means. Qualified requestors may obtain copies of this report from th...|$|R
5000|$|... #Subtitle level 3: Admissibility of (generalized) <b>Bayes</b> <b>rules</b> ...|$|R
5000|$|A {{decision}} rule [...] that minimizes [...] {{is called a}} <b>Bayes</b> <b>rule</b> with respect to [...] There {{may be more than}} one such <b>Bayes</b> <b>rule.</b> If the Bayes risk is infinite for all , then no <b>Bayes</b> <b>rule</b> is defined.|$|E
50|$|Note that no {{situation}} is possible where a nonrandomised <b>Bayes</b> <b>rule</b> {{does not exist}} but a randomised <b>Bayes</b> <b>rule</b> does. The existence of a randomised <b>Bayes</b> <b>rule</b> implies {{the existence of a}} nonrandomised <b>Bayes</b> <b>rule.</b> This is also true in the general case, even with infinite parameter space, infinite Bayes risk, and regardless of whether the infimum Bayes risk can be attained. This supports the intuitive notion that the statistician need not utilise randomisation to arrive at statistical decisions.|$|E
5000|$|If a <b>Bayes</b> <b>rule</b> {{is unique}} {{then it is}} admissible. For example, as stated above, under mean squared error (MSE) the <b>Bayes</b> <b>rule</b> is unique and {{therefore}} admissible.|$|E
5000|$|If θ {{belongs to}} a {{discrete}} set, then all <b>Bayes</b> <b>rules</b> are admissible.|$|R
5000|$|... #Caption: The <b>Bayes</b> <b>rules</b> are {{the set of}} {{decision}} rules of the form , [...]|$|R
50|$|<b>Bayes</b> <b>rules</b> having finite <b>Bayes</b> risk are {{typically}} admissible. The {{following are some}} specific examples of admissibility theorems.|$|R
5000|$|At first, {{this may}} appear rather {{different}} from the <b>Bayes</b> <b>rule</b> approach of the previous section, not a generalization. However, notice that the Bayes risk already averages over [...] in Bayesian fashion, and the Bayes risk may be recovered as the expectation over [...] of the expected loss (where [...] and [...] ). Roughly speaking, [...] minimizes this expectation of expected loss (i.e., is a <b>Bayes</b> <b>rule)</b> {{if and only if}} it minimizes the expected loss for each [...] separately (i.e., is a generalized <b>Bayes</b> <b>rule).</b>|$|E
5000|$|Having made {{explicit}} {{the expected}} loss for each given [...] separately, we can define a decision rule [...] by specifying for each [...] an action [...] that minimizes the expected loss. This {{is known as}} a generalized <b>Bayes</b> <b>rule</b> with respect to [...] There may be more than one generalized <b>Bayes</b> <b>rule,</b> since there may be multiple choices of [...] that achieve the same expected loss.|$|E
5000|$|The {{required}} conditionals can be correctly {{derived by}} inverting the available conditionals using <b>Bayes</b> <b>rule.</b> The inverted conditionals are obtained as follows: ...|$|E
5000|$|If θ {{belongs to}} a {{continuous}} (non-discrete set), and if the risk function R(θ,δ) is continuous in θ for every δ, then all <b>Bayes</b> <b>rules</b> are admissible.|$|R
40|$|In {{the problem}} of {{estimating}} a location parameter in any symmetric unimodal location parameter model, we demonstrate that <b>Bayes</b> <b>rules</b> with respect to squared error loss can be expanders for some priors that belong {{to the family of}} all symmetric priors. That generalizes the results obtained by DasGupta and Rubin for the one dimensional case. We also consider symmetric priors which either have an appropriate point mass at 0 or are unimodal, and prove that under these conditions all <b>Bayes</b> <b>rules</b> are shrinkers. Results of such nature are important, for example, in wavelet based function estimation and data denoising, where shrinkage of wavelet coefficients is associated with smoothing the data. We illustrate the results using Fiat stock market data. 1 Introduction DasGupta and Rubin (1993) have shown that in estimating the multivariate normal mean, <b>Bayes</b> <b>rules,</b> with respect to squared error loss and symmetric (about 0) priors, can be expanders only in one- and two-dimensional problems. They define the conditions under which a rule is an expander through the following definition...|$|R
40|$|We {{consider}} {{a problem of}} selecting a best of k one parameter exponential families with quadratic variance functions which {{is associated with the}} largest mean. It is shown that the minimax value under the " 0 - 1 " loss function is 1 - 1 /k. Also the <b>Bayes</b> <b>rules</b> are discussed under the " 0 - 1 " loss and the other loss functions. Natural exponential families with quadratic variance function Minimax value <b>Bayes</b> selection <b>rules</b> Selection...|$|R
5000|$|This {{allows us}} to {{calculate}} the new unnormalized probabilities state vector [...] through <b>Bayes</b> <b>rule,</b> weighting by the likelihood that each element of [...] generated event 1 as: ...|$|E
5000|$|The {{probability}} [...] that {{a document}} is relevant {{derives from the}} probability of relevance of the terms vector of that document [...] By using the <b>Bayes</b> <b>rule</b> we get: ...|$|E
5000|$|Initially GG, SS and GS {{are equally}} likely. Therefore, by <b>Bayes</b> <b>rule</b> the {{conditional}} {{probability that the}} chosen box is GG, given we have observed a gold coin, is: ...|$|E
40|$|A set of unlabelled items {{is used to}} {{establish}} a decision rule to classify defective items. The lifetime of an item has an exponential distribution. It is known that the <b>Bayes</b> decision <b>rule,</b> which classifies good and defective items, gives a minimum probability of misclassification. The <b>Bayes</b> decision <b>rule</b> needs to know the prior probability (defective percentage) and two mean lifetimes. In the set of unidentified samples, the defective percentage and two mean lifetimes are unknown. Hence, before {{we can use the}} <b>Bayes</b> decision <b>rule,</b> we have to estimate the three unknown parameters. In this study, a set of unlabelled samples is used to estimate the three unknown parameters. The <b>Bayes</b> decision <b>rule</b> with these estimated parameters is an empirical <b>Bayes</b> (EB) decision <b>rule.</b> A stochastic approximation procedure using the set of unidentified samples is established to estimate the three unknown parameters. When the size of unlabelled items increases, the estimates computed by the procedure converge to the real parameters and hence gradually adapt our EB decision rule to be a better classifier until it becomes the <b>Bayes</b> decision <b>rule.</b> The results of a Monte Carlo simulation study are presented to demonstrate the convergence of the correct classification rates made by the EB decision rule to the highest correct classification rates given by the <b>Bayes</b> decision <b>rule.</b> (C) 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved...|$|R
50|$|As nonrandomised {{alternatives}} always {{exist to}} randomised <b>Bayes</b> <b>rules,</b> randomisation {{is not needed}} in Bayesian statistics, although frequentist statistical theory sometimes {{requires the use of}} randomised rules to satisfy optimality conditions such as minimax, most notably when deriving confidence intervals and hypothesis tests about discrete probability distributions.|$|R
40|$|In this paper, we {{consider}} a multivariate hypergeometric population with k-category [pi] = [pi](N, M, s 1, [...] .,sk), where si, I = 1, [...] .,k, {{is the number}} of units in category [pi]i, with [Sigma]i- 1 k si = M, the total number of units in population [pi], and N {{is the number of}} units selected from population [pi]. Let s[1] = min 1 [less-than-or-equals, slant]i[less-than-or-equals, slant]k si, s[k] = max 1 [less-than-or-equals, slant]i[less-than-or-equals, slant]k si. A category associated with si = s[k] (or si = s[1]) is considered as the most (or the least) probable event. We are interested in selecting the most and the least probable event. It is assumed that the unknown parameters si, I = 1, [...] .,k, follow a multinomial prior distribution with unknown hyperparameters. Under this statistical framework, two empirical <b>Bayes</b> selection <b>rules</b> are studied according to the two different selection problems. It is shown that for each empirical <b>Bayes</b> selection <b>rule,</b> the corresponding <b>Bayes</b> risk tends to the minimum Bayes risk with a rate of convergence of order O(exp(-cn)) for some positive constant c, where the value of c varies depending on the rule and n is the number of accumulated past observations at hand. <b>Bayes</b> <b>rules</b> Empirical <b>Bayes</b> <b>rules</b> Least probable event Most probable event Rate of convergence...|$|R
50|$|Four {{principle}} inspirations for Solomonoff's algorithmic probability were: Occam's razor; Epicurus' {{principle of}} multiple explanations; modern computing theory (e.g. {{use of a}} universal Turing machine) and <b>Bayes</b> <b>rule</b> for prediction.|$|E
5000|$|A minimax <b>Bayes</b> <b>rule</b> is {{one that}} minimises the {{supremum}} risk [...] among all decision rules in [...] Sometimes, a randomised decision rule may perform better than all other nonrandomised decision rules in this regard.|$|E
5000|$|According to the {{complete}} class theorems, {{under mild conditions}} every admissible rule is a (generalized) <b>Bayes</b> <b>rule</b> (with respect to some prior —possibly an improper one—that favors distributions [...] where that rule achieves low risk). Thus, in frequentist decision theory it is sufficient to consider only (generalized) Bayes rules.|$|E
40|$|For classification, it {{is known}} that the <b>Bayes</b> {{decision}} <b>rule</b> is the best decision rule, which gives the minimum probability of misclassification. It is difficult to use the <b>Bayes</b> decision <b>rule,</b> since it contains unknown parameters from each class. In this study, a set of unidentied samples (patterns) is used to establish an optimal classifier such that (1) it only contains the observations of unclassified samples (testing samples), (2) no other classifier is strictly better than our optimal classifier, and (3) when the number of unidentifed samples increases, the recognition rate of our classifier converges to the rate of the <b>Bayes</b> decision <b>rule.</b> A Monte Carlo simulation study is presented to demonstrate the favorable recognition rates obtained from our optimal classifier, which quickly converge to the highest rates obtained from the real <b>Bayes</b> decision <b>rule,</b> where the parameters in each class are known...|$|R
50|$|As {{randomised}} <b>Bayes</b> <b>rules</b> {{always have}} nonrandomised alternatives, they are unnecessary in Bayesian statistics. However, in frequentist statistics, randomised rules are theoretically necessary under certain situations, and {{were thought to}} be useful in practice when they were first invented: Egon Pearson forecast that they 'will not meet with strong objection'. However, few statisticians actually implement them nowadays.|$|R
40|$|Inferences {{that arise}} from loss {{functions}} determined by the prior are considered and it is shown that these lead to limiting <b>Bayes</b> <b>rules</b> that are closely connected with likelihood. The procedures obtained via these loss functions are invariant under reparameterizations and are Bayesian unbiased or limits of Bayesian unbiased inferences. These inferences serve as well-supported alternatives to MAP-based inferences...|$|R
50|$|Prediction is done using a {{completely}} Bayesian framework. The universal prior is calculated for all computable sequences—this is the universal {{a priori probability}} distribution;no computable hypothesis will have a zero probability. This means that <b>Bayes</b> <b>rule</b> of causation {{can be used in}} predicting the continuation of any particular computable sequence.|$|E
5000|$|More important, it is {{sometimes}} convenient to use an improper prior [...] In this case, the Bayes risk is not even well-defined, {{nor is there any}} well-defined distribution over [...] However, the posterior —and hence the expected loss—may be well-defined for each , so that it is still possible to define a generalized <b>Bayes</b> <b>rule.</b>|$|E
50|$|Attack {{trees are}} related to the {{established}} fault tree formalism. Fault tree methodology employs boolean expressions to gate conditions when parent nodes are satisfied by leaf nodes. By including a priori probabilities with each node, it is possible to perform calculate probabilities with higher nodes using <b>Bayes</b> <b>Rule.</b> However, in reality accurate probability estimates are either unavailable or too expensive to gather. With respect to computer security with active participants (i.e., attackers), the probability distribution of events are probably not independent nor uniformly distributed, hence, naive Bayesian analysis is unsuitable.|$|E
40|$|This paper reports an {{experiment}} in which subjects are asked to assess probabilities for unknown events, with treatments that vary the extremity of the prior information. Probabilities are elicited using a Becker-DeGroot-Marshak procedure that {{does not depend on}} assumptions about risk aversion. The focus is on the pattern of biases in information processing. Laboratory experiments <b>Baye's</b> <b>rule</b> Probability weighting...|$|R
40|$|Abstract Background There is {{a growing}} trend towards the {{production}} of "hospital report-cards" in which hospitals with higher than acceptable mortality rates are identified. Several commentators have advocated {{for the use of}} Bayesian hierarchical models in provider profiling. Several researchers have shown that some degree of misclassification will result when hospital report cards are produced. The impact of misclassifying hospital performance can be quantified using different loss functions. Methods We propose several families of loss functions for hospital report cards and then develop <b>Bayes</b> <b>rules</b> for these families of loss functions. The resultant <b>Bayes</b> <b>rules</b> minimize the expected loss arising from misclassifying hospital performance. We develop <b>Bayes</b> <b>rules</b> for generalized 1 - 0 loss functions, generalized absolute error loss functions, and for generalized squared error loss functions. We then illustrate the application of these decision rules on a sample of 19, 757 patients hospitalized with an acute myocardial infarction at 163 hospitals. Results We found that the number of hospitals classified as having higher than acceptable mortality is affected by the relative penalty assigned to false negatives compared to false positives. However, the choice of loss function family had a lesser impact upon which hospitals were identified as having higher than acceptable mortality. Conclusion The design of hospital report cards can be placed in a decision-theoretic framework. This allows researchers to minimize costs arising from the misclassification of hospitals. The choice of loss function can affect the classification of a small number of hospitals. </p...|$|R
40|$|Includes bibliographical {{references}} (page 49) Pattern classification {{and recognition}} {{have been applied}} to solve {{a broad spectrum of}} problems in the field of engineering and science. The main concern in this project is the design and implementation of a particular kind of pattern classifier: N-class Bayes classifier. The necessary background in statistics and the detailed formulation of the <b>Bayes</b> <b>rules</b> are presented. Performance evaluation on the N-class Bayes classifier is accomplished by using the Nearest Neighbor methods to estimate the error bound of the <b>Bayes</b> <b>rules.</b> Several computer programs are developed to implement the N-class Bayes classifier. The data entry program in Appendix A is written for the purpose of storing sample data in a file rather than entering data interactively. The linear quadratic discriminant program in Appendix B performs Bayes classification on input data of n-dimensions, and the nearest neighbor classification program in Appendix C gives an indication of estimated performance of the Bayes classifier by calculating the misclassification error bounds...|$|R
