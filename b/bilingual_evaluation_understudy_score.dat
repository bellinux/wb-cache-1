1|42|Public
40|$|En col·laboració amb la Universitat de Barcelona (UB) i la Universitat Rovira i Virgili (URV) In recent years, Neural Machine Translation (NMT) has {{achieved}} state-of-the art performance in translating from a language; source language, to another; target language. However, {{many of the}} proposed methods use word embedding techniques to represent a sentence in the source or target language. Character embedding techniques for this task has been suggested to represent the words in a sentence better. Moreover, recent NMT models use attention mechanism where the most relevant words in a source sentence are used to generate a target word. The problem with {{this approach is that}} while some words are translated multiple times, some other words are not translated. To address this problem, coverage model has been integrated into NMT to keep track of already-translated words and focus on the untranslated ones. In this research, we present a new architecture in which we use character embedding for representing the source and target words, and also use coverage model to make certain that all words are translated. We compared our model with the previous models and our model shows comparable improvements. Our model achieves an improvement of 2. 87 BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy)</b> <b>score</b> over the baseline; attention model, for German-English translation, and 0. 34 BLEU score improvement for Catalan-Spanish translation...|$|E
50|$|Do {{repeated}} translations {{converge on}} a single expression in both languages? I.e. does the translation method show stationarity or produce a canonical form? Does the translation become stationary without losing the original meaning? This metric has been criticized as not being well correlated with BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy)</b> <b>scores.</b>|$|R
40|$|AbstractThe paper shows a {{statistical}} Sanskrit-Hindi Translator and analyzes the errors being {{generated by the}} system. The System is being trained simultaneously on the platform - the Microsoft Translator Hub (MTHub) and is intended only for simple Sanskrit prose texts. The training set includes 24 K parallel sentences and 25 k monolingual data with recent BLEU (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy)</b> <b>scores</b> of 41 and above. The paper discusses the errors analysis {{of the system and}} suggests possible solutions. Further, it also focuses on the evaluation of MTHub system with BLEU metrics. For developing MT systems, the parallel Sanskrit-Hindi text corpora has been collected or developed manually from the literature, health, news and tourism domains. The paper also discusses issues and challenges in the development of translation systems for languages like Sanskrit...|$|R
40|$|This paper proposes cross-lingual {{language}} modeling for transcribing source resourcepoor {{languages and}} translating them into target resource-rich languages if necessary. Our focus {{is to improve}} the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3. 4 % relative WER reduction in Cantonese transcription and 13. 3 % relative <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (BLEU) <b>score</b> improvement in Mandarin transcription compared with the system without reordering. ...|$|R
40|$|One of {{the major}} {{challenges}} {{in the era of}} big data use is how to ‘clean’ the vast amount of data, particularly from micro-blog websites like Twitter. Twitter messages, called tweets, are commonly written in ill-forms, including abbreviations, repeated characters, and misspelled words. These ‘noisy tweets’ require text normalisation techniques to detect and convert them into more accurate English sentences. There are several existing techniques proposed to solve these issues, however each technique possess some limitations and therefore cannot achieve good overall results. This paper aims to evaluate individual existing statistical normalisation methods and their possible combinations in order to find the best combination that can efficiently clean noisy tweets at the character-level, which contains abbreviations, repeated letters and misspelled words. Tested on our Twitter sample dataset, the best combination can achieve 88 % accuracy in the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) <b>score</b> and 7 % Word Error Rate (WER) score, both of which are considered better than the baseline model...|$|R
40|$|We {{show for}} the first time an {{end-to-end}} speech transcription and translation system with cross-lingual language modeling based on weighted finite-state transducers (WFSTs). The system can perform the decoding of the speech of a resource-poor source language to its transcription as well as into a resource-rich target language. The proposed cross-lingual language modeling approach uses phrase-level translation that includes phrase-level transduction and syntactic reordering. The phrase-level transduction is capable of performing n to m cross-lingual transduction instead of word-level transduction only allowing n to n transduction. The syntactic reordering serves to model the syntactic discrepancies between the resource-poor and resource-rich languages. As such, we can leverage the statistics from a resource-rich language to improve the language model of a resource-poor language in a truly cross-lingual language model. This cross-lingual language model can simultaneously improve the speech recognition performance of the resource-poor language and provide a translation of the resource-poor language to the resource-rich language. In this thesis, we focus on the recognition and translation of a non-standard Chinese language, Cantonese, which does not have a written form, to standard Chinese. The cross-lingual language model is trained from a large amount of resource-rich language (e. g. Mandarin) data and a small amount of resource-poor language (e. g. Cantonese) data, as well as some parallel data of the resource-poor and resource-rich languages. Evaluations on Cantonese speech recognition and Cantonese to standard Mandarin Chinese translation tasks show that our proposed cross-lingual language modeling improves the recognition and translation performance significantly, up to 12. 5 % relative word error rate (WER) reduction over the baseline language model interpolation, 6. 6 % relative WER reduction and 18. 5 % relative <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (BLEU) <b>score</b> improvement, compared to the best word-level transduction approach. This model can be further generalized to speech translation of any source and target language pairs via the transcription and translation framework...|$|R
40|$|Parallel bilingual corpora are {{important}} basic resources for statistical machine translation. Accurate alignment of textual elements (e. g. documents, paragraphs, sentences) {{in a parallel}} bilingual corpus is a crucial step for statistical machine translation. Rather than using sentence length, word co-occurrence, cognates, dictionaries or parts of speech, this thesis uses compression code lengths based on the Prediction by Partial Matching (PPM) compression algorithm to measure when two sentences are aligned for parallel Chinese-English corpora. PPM {{has been found to}} be an effective method as a measure of whether the information conveyed by the texts is similar at estimating the entropy of the text. Evaluation of the quality of sentence alignment is a way to measure the quality of a corpus. Evaluating parallel bilingual corpora is also an important process and usually the last step for parallel bilingual corpus creation. However, most statistics of parallel bilingual corpora are based on counts of characters, words, tokens, sentences or files. As there is a lack of advanced parallel <b>bilingual</b> corpus <b>evaluation</b> methods, this thesis adopts a new PPMbased method for parallel <b>bilingual</b> corpus <b>evaluation.</b> The method has been used to evaluate the quality of three existing parallel bilingual corpora|the DC Corpus, the Hong Kong Yearbook Corpus and the UN Corpus. The compression-based method has also been applied to the problem of the automatic creation of new parallel corpora. The quality of sentence alignment for automatically created parallel bilingual corpora is always lower than manually checked corpora. This thesis processed the Corpus of United Nations by using the PPM-based metric and sought the best code length threshold value that can be used for automatically determining satisfactory or unsatisfactory sentence alignment in terms of translation quality in the corpus. The thesis also collected bilingual textual elements from the web and improved the quality based on the threshold code length ratio of 1. 5. The approach has also been adapted to use as a method to perform translation system evaluation by comparing the compression code lengths of back translations at the sentence level. Compared to <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) <b>scores,</b> the back translation-based evaluation method was able to present differences at the sentence level between original sentences and their back translations more accurately when used to evaluate some common Chinese-English translation systems...|$|R
40|$|One of {{the major}} {{problems}} {{in the era of}} big data use is how to ‘clean’ the vast amount of data on the Internet, particularly data in the micro-blog website Twitter. Twitter enables people to connect with their friends, colleagues, or even new people who they have never met before. Twitter, one of the world’s biggest social media networks, has around 316 million users, and 500 million tweets posted per day (Twitter, 2016). Undoubtedly, social media networks create huge opportunities in helping businesses build relationships with customers, gain more insights into their customers, and deliver more value to them. Despite all the advantages of Twitter use, comments – called tweets - posted on social media networks may not be all that useful if they contain irrelevant and incomprehensible information, therefore making it difficult to analyse. Tweets are commonly written in ‘ill-forms’, such as abbreviations, repeated characters, and misspelled words. These ‘noisy tweets’ become text normalisation challenges in terms of selecting the proper methods to detect and convert them into the most accurate English sentences. There are several existing text cleaning techniques which are proposed to solve the issues, however they possess some limitations and still do not achieve good results overall. In this research, our aim is to propose the SNET, a statistical normalisation method for cleaning noisy tweets at character-level (which contain abbreviations, repeated letters, and misspelled words) that combines different techniques to achieve more accurate and clean data. To clean noisy tweets, existing techniques are evaluated in order to find the best solution by combining techniques so as to solve all problems with high accuracy. This research proposes that abbreviations are converted to their standard form by using abbreviations dictionary lookup, while repeated characters are normalised by the Natural Language Toolkit (NLTK) platform and a dictionary based approach. Besides the NLTK, the edit distance algorithm is also utilised as a means of solving misspelling problems, while “Enchant” dictionary can be used to access the spell checking library. Furthermore, existing models, such as a spell corrector, can be deployed for conversion purposes, while text cleanser is advanced as superior for comparing the SNET with a baseline model. With experiments on a Twitter sample dataset, our results show that the SNET satisfies 88 % accuracy in the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) <b>score</b> and 7 % in the word error rate (WER) score, both of which are better than the baseline model. Devising such a method to clean tweets can make a great contribution in terms of its adoption in brand sentiment analysis or opinion mining, political analysis, and other applications seeking to make sound predictions...|$|R
50|$|BLEU (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy)</b> is an {{algorithm}} {{for evaluating}} the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human. BLEU {{was one of the first}} metrics to achieve a high correlation with human judgments of quality, and remains one of the most popular automated and inexpensive metrics.|$|R
500|$|BLEU (<b>bilingual</b> <b>evaluation</b> <b>understudy)</b> is an {{algorithm}} {{for evaluating}} the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: [...] "the closer a machine translation is to a professional human translation, the better it is" [...] this is the central idea behind BLEU. BLEU {{was one of the first}} metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.|$|R
40|$|This paper evaluates {{different}} approaches on speech to sign language machine translation. The {{framework of the}} application focuses on assisting deaf people {{to apply for the}} passport or related information. In this context, the main aim is to automatically translate the spontaneous speech, uttered by an officer, into Spanish Sign Language (SSL). In order to get the best translation quality, three alternative techniques have been evaluated: a rule-based approach, a phrase-based statistical approach, and a approach that makes use of stochastic finite state transducers. The best speech translation experiments have reported a 32. 0 % SER (Sign Error Rate) and a 7. 1 BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy)</b> including speech recognition errors...|$|R
40|$|Our {{research}} {{extends the}} <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) <b>evaluation</b> technique for {{statistical machine translation}} {{to make it more}} adjustable and robust. We intend to adapt it to resemble human evaluation more. We perform experiments to evaluate the performance of our technique against the primary existing evaluation methods. We describe and show the improvements it makes over existing methods as well as correlation to them. When human translators translate a text, they often use synonyms, different word orders or style, and other similar variations. We propose an SMT evaluation technique that enhances the BLEU metric to consider variations such as those. Comment: machine translation evaluation, enchanced bleu. in Lecture Notes on Information Theory, ISSN: 2301 - 3788, 201...|$|R
40|$|Statistical Machine Translation has {{successfully}} {{been used for}} translation between many language pairs contributing to its popularity in recent years. It has however not been used for the English/Persian language pair. This paper presents the first such attempt and describes the problems faced in creating a corpus and building a base line system. Our experience {{with the construction of}} a parallel corpus during this ongoing study and the problems encountered especially with the process of alignment are discussed in this paper. The prototype constructed and its <b>evaluation</b> using the <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) is briefly described and results are analyzed. In the final part of the paper, conclusions are drawn and work planned for the future is discussed...|$|R
40|$|This paper proposes {{the use of}} Factored Translation Models (FTMs) for {{improving}} a Speech into Sign Language Translation System. These FTMs allow incorporating syntactic-semantic information during the translation process. This new information permits to reduce significantly the translation error rate. This paper also analyses different alternatives {{for dealing with the}} non-relevant words. The speech into sign language translation system has been developed and evaluated in a specific application domain: the renewal of Identity Documents and Driver’s License. The translation system uses a phrase-based translation system (Moses). The evaluation results reveal that the BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy)</b> has improved from 69. 1 % to 73. 9 % and the mSER (multiple references Sign Error Rate) has been reduced from 30. 6 % to 24. 8 %...|$|R
40|$|Report: 1 volume iii In this paper, I present Generation by Opportunistic Classification (GOC), a text {{generation}} system which produces summaries {{as an alternative}} form of an Internet search result. Unlike traditional approaches which use knowledge engineered rules, GOC predominantly uses machine learning techniques for its linguistic decisions. A key idea in this thesis {{is the use of}} opportunistic classification to resolve the issue of interdependency among the classification tasks. A series of heuristics is used to decide the order of classification, and a minimal set of knowledge engineered rules are deployed to improve the correctness of the generated text. Through the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) metric and human subject evaluations, I verified that the opportunistic classification formulation produces better text than other baseline systems. At the same time, I manage to verify that BLEU is able to correlate with human evaluators...|$|R
40|$|This paper {{describes}} and exemplifies {{an application}} of a Structured Exam Questions Test Bank and <b>Evaluation</b> Using Modified <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) Algorithm, a software system developed in pursuit of robust computerized marking of free-text answers to open-ended questions. It employs the Information System Development Research Methodology with modified BLEU Algorithm and Expert System for similar words. The system was developed to facilitate in managing and administrating structured questions for client/server architecture based on intranet. The system incorporates a number of processing modules specifically aim at providing an automated marking to reduce spelling errors, calculating scores, managing and administrating an exam. The system was trial-run {{by a group of}} students and lecturers, and modifications particularly on the interface have been modified and implemented. Problems and limitations discovered were then discussed and recommendations made to overcome the limitations for the future development of the research...|$|R
40|$|Computerized Evaluation of English Essays is {{performed}} using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Protégé, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology. Comment: 11 pages, 5 figures, journal, [URL] 201...|$|R
40|$|This study aims {{to compare}} the {{effectiveness}} of two popular machine translation systems (Google Translate and Babylon machine translation system) used to translate English sentences into Arabic relative {{to the effectiveness of}} English to Arabic human translation. There are many automatic methods used to evaluate different machine translators, one of these methods; <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU) method, which was adopted and implemented to achieve the main goal of this study. BLEU method is based on the assumptions of automated measures that depend on matching machine translators' output to human reference translations; the higher the score, the closer the translation to the human translation will be. Well known English sayings in addition to manually collected sentences from different Internet web sites were used for evaluation purposes. The results of this study have showed that Google machine translation system is better than Babylon machine translation system in terms of precision of translation from English to Arabic...|$|R
40|$|When two texts have an {{inclusion}} relation, {{the relationship}} between them is called entailment. The task of mechanically distinguishing such a relation is called recognising textual entailment (RTE), which is basically a kind of semantic analysis. A variety of methods have been proposed for RTE. However, when the previous methods were combined, the performances were not clear. So, we utilized each method as a feature of machine learning, in order to combine methods. We {{have dealt with the}} binary classification problem of two texts exhibiting inclusion, and proposed a method that uses machine learning to judge whether the two texts present the same content. We have built a program capable to perform entailment judgment on the basis of word overlap, i. e. the matching rate of the words in the two texts, mutual information, and similarity of the respective syntax trees (Subpath Set). Word overlap was calclated by utilizing <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU). Mutual information is based on co-occurrence frequency, and the Subpath Set was determined by using the Japanise WordNet. A Confidence-Weighted Score of 68. 6 % was obtained in the mutual information experiment on RTE. Mutual information and the use of three methods of SVM were shown to be effective. ...|$|R
40|$|This paper {{describes}} {{the development of}} and the first experiments in a Spanish to sign language translation system in a real domain. The developed system focuses on the sentences spoken by an official when assisting people applying for, or renewing their Identity Card. The system translates official explanations into Spanish Sign Language (LSE: Lengua de Signos Espan¿ola) for Deaf people. The translation system {{is made up of}} a speech recognizer (for decoding the spoken utterance into a word sequence), a natural language translator (for converting a word sequence into a sequence of signs belonging to the sign language), and a 3 D avatar animation module (for playing back the hand movements). Two proposals for natural language translation have been evaluated: a rule-based translation module (that computes sign confidence measures from the word confidence measures obtained in the speech recognition module) and a statistical translation module (in this case, parallel corpora were used for training the statistical model). The best configuration reported 31. 6 % SER (Sign Error Rate) and 0. 5780 BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy).</b> The paper also {{describes the}} eSIGN 3 D avatar animation module (considering the sign confidence), and the limitations found when implementing a strategy for reducing the delay between the spoken utterance and the sign sequence animation...|$|R
40|$|Machine Translation (MT) is exigent {{because it}} {{involves}} several thorny subtasks such as intrinsic language ambiguities, linguistic complexities and diversities between source and target language. Usually MT depends upon rules that provide linguistic information. At present, the corpus based MT approaches are used that include techniques like Example Based MT (EBMT) and Statistical MT (SMT). In addition to others, both of these corpus based techniques have different frameworks in the contemporary data-driven paradigm. SMT systems generate outputs using probabilities, whereas EBMT systems translate input text by matching examples from large amount of training data. Urdu MT is in its infancy with very limited availability of required data and computational resources. In this paper, we analyzed and evaluated the main MT techniques using qualitative as well as quantitative approaches. Strengths and weaknesses of each technique {{have been brought to}} light through special focus and discussion on examples from Urdu language MT literature. We evaluated the automated machine translated outputs using <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (BLEU). The EBMT approach produced the highest accuracy of 84. 21 % whereas the accuracy of the online SMT system is 62. 68 %. We found that BLUE scores of machine translated long Urdu sentences are low in comparison with long sentences. Similarly source text containing low frequency words affect the quality of Urdu machine translation negatively. Experiments and findings section of this paper explicate our reported results in detail. The paper concludes with proposal of future directions for research in Urdu machine translation...|$|R
40|$|This paper {{describes}} a preprocessing module {{for improving the}} performance of a Spanish into Spanish Sign Language (Lengua de Signos Española: LSE) translation system when dealing with sparse training data. This preprocessing module replaces Spanish words with associated tags. The list with Spanish words (vocabulary) and associated tags used by this module is computed automatically considering those signs that show the highest probability of being the translation of every Spanish word. This automatic tag extraction has been compared to a manual strategy achieving almost the same improvement. In this analysis, several alternatives for dealing with non-relevant words have been studied. Non-relevant words are Spanish words not assigned to any sign. The preprocessing module has been incorporated into two well-known statistical translation architectures: a phrasebased system and a Statistical Finite State Transducer (SFST). This system has been developed for a specific application domain: the renewal of Identity Documents and Driver’s License. In order to evaluate the system a parallel corpus made up of 4, 080 Spanish sentences and their LSE translation has been used. The evaluation results revealed a significant performance improvement when including this preprocessing module. In the phrase-based system, the proposed module has given rise to an increase in BLEU (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy)</b> from 73. 8 % to 81. 0 % and an increase in the human evaluation score from 0. 64 to 0. 83. In the case of SFST, BLEU increased from 70. 6 % to 78. 4 % and the human evaluation score from 0. 65 to 0. 82...|$|R
40|$|This is the author’s {{version of}} a work that was {{accepted}} for publication in Computer Speech and Language. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may {{have been made to}} this work since it was submitted for publication. A definitive version was subsequently published in Computer Speech and Language, 28, 3 (2015) DOI: 10. 1016 /j. csl. 2013. 10. 003 One of the aims of Assistive Technologies is to help people with disabilities to communicate with others and to provide means of access to information. As an aid to Deaf people, we present in this work a production-quality rule-based machine system for translating from Spanish to Spanish Sign Language (LSE) glosses, which is a necessary precursor to building a full machine translation system that eventually produces animation output. The system implements a transfer-based architecture from the syntactic functions of dependency analyses. A sketch of LSE is also presented. Several topics regarding translation to sign languages are addressed: the lexical gap, the bootstrapping of a bilingual lexicon, the generation of word order for topic-oriented languages, and the treatment of classifier predicates and classifier names. The system has been evaluated with an open-domain testbed, reporting a 0. 30 BLEU (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy)</b> and 42 % TER (Translation Error Rate). These results show consistent improvements over a statistical machine translation baseline, and some improvements over the same system preserving the word order in the source sentence. Finally, the linguistic analysis of errors has identified some differences due to a certain degree of structural variation in LSE...|$|R
40|$|The {{challenges}} of training {{generative adversarial network}} (GAN) to produce discrete tokens, have seen {{a considerable amount of}} work in the past year. However, the amount of successful work on applying deep generative models to text generation is limited, when compared to the visual domain. One of the reasons, is the challenge of passing the gradient, while keeping the network differentiable. The known effective models that generate text with GAN, extend the original framework proposed by Goodfellow et al. [2014], using reinforcement learning. We propose a novel approach that requires no modification to the training process introduced by Goodfellow et al. [2014], and in addition is able to produce meaningful text without any pre-training. Our approach {{is not limited to the}} textual domain, and could be able to solve a variety of problems related to generating sequences of discrete tokens. We show a proof of concept for an image captioning model that extends our text generator to perform controllable text generation. This approach provides a straightforward method for controlling text generation using images, or any other vector representing relevant information. Evaluating results produced by GANs is a common problem. The absence of real data for each corresponding sample generated from the network, makes the appliance of common evaluation methods difficult. As a solution to this challenge, we have developed an automatic evaluation method for text generative systems. This method combines the machine learning <b>evaluation</b> metric, <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (BLEU), with a set of interchangeable information retrieval techniques. This permits us to evaluate the semantic quality of our models, as well as comparing them to a baseline...|$|R
40|$|Evaluating a {{dialogue}} system {{is seen as}} a major challenge within the dialogue research community. Due to {{the very nature of the}} task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of <b>evaluation</b> <b>understudy</b> for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. ...|$|R
50|$|Born in Walthamstow, London, Hayes played youth {{football}} for Essex schoolboys before joining Arsenal {{as an apprentice}} in June 1982. A promising attacking player, either up front or on the left wing, Hayes impressed in the Arsenal youth and reserve teams, and made his first-team debut against Oxford United on 16 November 1985 {{at the age of}} 19. He played another 13 matches that seasons as Graham Rix's <b>understudy,</b> <b>scoring</b> three goals. Just weeks {{before the end of the}} season, Don Howe, the manager who had given Hayes his debut, stepped down as manager following reports that Arsenal had offered his job to Terry Venables, but when the new manager was announced a few weeks later it was George Graham who took over. And fortunately for Hayes, he remained in the new manager's plans.|$|R
40|$|We {{propose a}} {{multilingual}} unsupervised Word Sense Disambiguation (WSD) task for {{a sample of}} English nouns. Instead of providing manually sensetagged examples for each sense of a polysemous noun, our sense inventory is built up {{on the basis of}} the Europarl parallel corpus. The multilingual setup involves the translations of a given English polysemous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian. The task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns. For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning. Systems can participate in 5 <b>bilingual</b> <b>evaluation</b> subtasks (English- Dutch, English- German, etc.) and in a multilingual subtask covering all language pairs. As WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests. ...|$|R
50|$|Cohen led seven group {{trips to}} the Amazon Rainforest and wrote three college {{textbooks}} {{in the fields of}} <b>bilingual</b> education, school <b>evaluation,</b> and race relations. He was President of BCR&D, a large private research and development group that specialized in assessing public education programs.|$|R
40|$|This {{research}} investigates <b>bilingual</b> consumers' <b>evaluation</b> {{of brand}} name translations from logographic-Chinese to alphabetic-English language systems. The research examines four possible methods of translation - semantic, phonetic, phonosemantic and Hanyu Pinyin. Consumers' chronic differences in language proficiency levels {{and the presence}} of situational primes relating to phonological or semantic processing jointly influence preferences for the translation methods. In addition to findings consistent with the premise that phonological/semantic processing is effective in alphabetic/logographic languages, this research shows that consumers who are strong in Chinese and weak in English prefer Pinyin translations across all conditions. © 2010 Elsevier Inc. link_to_subscribed_fulltex...|$|R
40|$|This teacher's {{edition of}} {{training}} materials on bilingual program planning, implementation, and evaluation focuses on foundations for evaluating bilingual programs. The guide {{is part of}} a series directed at bilingual educators and intended for use in institutions of high,x education and inservice teacher education programs. Training ajectives, a pretest and posttest, suggested training procedures, and instructional activities and vaterials are included. Among the topics covered are (1) planning and organizing a <b>bilingual</b> program <b>evaluation,</b> (2) the evaluation team, (3) evaluation objectives, (4) establishing evaluation questions and methodologies, (5) collecting and analyzing evaluation evidence, and (6) using evaluation results. (RW) * Reproductions supplied by EDRS are the best that can be made * * from the original document. ...|$|R
40|$|Metadata onlyThis paper {{shows that}} aligned bilingual English-French corpora offer the {{advantage}} of improving the performance of anaphora resolution in both languages. It sets out a new ‘mutual enhancement’ methodology and discusses the results obtained from running the enhanced anaphora resolution algorithms for English and French on a <b>bilingual</b> corpus. The <b>evaluation</b> reports up to 4. 62 % improvement of the success rate for English and up to 5. 15 % for French...|$|R
40|$|Evaluation {{of machine}} {{translation}} {{is an important}} step towards improving MT. One way to evaluate the output of MT is to focus on different types of errors occurring in the translation hypotheses, and to think of possible solutions to fix those errors. An error categorization is a rather beneficent tool that makes it easy to analyze the translation errors and can also be utilized to manually generate post-editing rules to be applied automatically to the product of machine translation. In this work, we define a categorization for the errors occurring in Swedish [...] Persian machine translation by analyzing the errors that occur in three data-sets from two websites: 1177. se, and Linköping municipality. We define three types of monolingual reference free evaluation (MRF), and use two automatic metrics BLEU and TER, to conduct a <b>bilingual</b> <b>evaluation</b> for Swedish-Persian translation. Later on, based on the experience of working with the errors that occur in the corpora, we manually generate automatic post-editing (APE) rules and apply them to the product of machine translation. Three different sets of results are obtained: (1) The results of analyzing MT errors show that the three most common types of errors that occur in the translation hypotheses are mistranslated words, wrong word order, and extra prepositions. These types of errors are placed in semantic and syntactic categories respectively. (2) The results of comparing the correlation between the automatic and manual evaluation show a low correlation between the two evaluations. (3) Lastly, applying the APE rules to the product of machine translation gives an increase in BLEU score on the largest data-set while remaining almost unchanged on the other two data-sets. The results for TER show a better score on one data-set, while the scores on the two other data-sets remain unchanged...|$|R
40|$|Presentamos los primeros sistemas de traducción automática para inglés-español e inglés-euskara basados en tectogramática. A partir del modelo ya existente inglés-checo, describimos las herramientas para el análisis y síntesis, y los recursos para la trasferencia. La evaluación muestra el potencial de estos sistemas para adaptarse a nuevas lenguas y dominios. We {{present the}} first attempt to build machine {{translation}} systems for the English-Spanish and English-Basque language pairs following the tectogrammar approach. Based on the English-Czech system, we describe the language-specific tools added in the analysis and synthesis steps, and the resources for <b>bilingual</b> transfer. <b>Evaluation</b> shows the potential of these systems for new languages and domains. The research leading to these results has received funding from FP 7 -ICT- 2013 - 10 - 610516 (QTLeap project, qtleap. eu) ...|$|R
40|$|We {{describe}} our cross-lingual {{retrieval system}} for cross-lingual information retrieval task of NTCIR 2. The {{main part of}} our work is construction of bilingual dictionaries per academic field from a nonparallel bilingual corpus. EDICT is the startpoint of constructing our bilingual dictionaries. We merge compound word translation extracted from a bilingual corpus to the dictionaries. Disambiguation of our bilingual dictionaries is performed using the <b>bilingual</b> corpus. Experimental <b>evaluation</b> of our <b>bilingual</b> dictionaries and document retrieval is depicted...|$|R
40|$|A {{meta-analysis}} of {{a subset of}} the <b>bilingual</b> education <b>evaluation</b> literature failed in its attempt to compare meta-analysis and narrative review because the meta-analysis (a) included only one third of the studies in the narrative review, (b) addressed different questions than did the narrative review, and (c) redefined a study demon-strating the failure of bilingual education as a success. Moreover, the meta-analysis is not an appropriate analytic technique for this literature because (a) inappropriate statistics were applied to the data, (b) the bilingual education treatment was confounded with other treatments in several of the studies reviewed, and (c) the studies reviewed were not sufficiently homogeneous to support a meta-analysis. Finally, the author's conclusion that bilingual education is effective was not sup-ported because (a) the results of the analysis do not generalize, (b) inappropriate statistical analysis was used to arrive at this conclusion, and (c) the effects of confounded treatments were not considered. Willig (1985) 1 reports "a resynthesis of the same literature [reviewed by Baker & de Kanter, 1981, 19832] using the techniques of meta-analyses [to] provide a chec...|$|R
40|$|Blinding is a methodologic {{safeguard}} {{of treatment}} <b>evaluation,</b> yet severely <b>understudied</b> empirically. Mathieu et al. 's theoretical analysis (2014) provided an important message that blinding cannot eliminate potential for bias associated with belief about allocation in randomized controlled trial; {{just like the}} intent-to-treat principle does not guarantee unbiased estimation under noncompliance, the blinded randomized trial as a golden standard may produce bias. They showed possible biases but did not assess how large the bias could be in different scenarios. In this paper, we examined their findings, and numerically assessed and compared the bias in treatment effect parameters by simulation under frequently encountered blinding scenarios, aiming to identify the most ideal blinding scenarios in practice. We conclude that Random Guess and Wishful Thinking (e. g., participants tend to believe they received treatment) are the most ideal blinding scenarios, incurring minimal bias. We also find some evidence that imperfect or partial blinding can be better than no blinding...|$|R
