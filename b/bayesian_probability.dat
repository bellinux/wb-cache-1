683|1044|Public
25|$|Philosopher and theologian Richard Swinburne {{reaches the}} design {{conclusion}} using <b>Bayesian</b> <b>probability.</b>|$|E
25|$|The use of <b>Bayesian</b> <b>probability</b> {{raises the}} {{philosophical}} debate {{as to whether}} it can contribute valid justifications of belief.|$|E
25|$|Intelligent control uses various AI {{computing}} approaches like neural networks, <b>Bayesian</b> <b>probability,</b> fuzzy logic, machine learning, {{evolutionary computation}} and genetic algorithms {{to control a}} dynamic system.|$|E
5000|$|... #Subtitle level 2: Objective and {{subjective}} <b>Bayesian</b> <b>probabilities</b> ...|$|R
5000|$|QBism - a {{controversial}} application of <b>Bayesian</b> <b>probabilities</b> to quantum mechanics ...|$|R
50|$|The use of <b>Bayesian</b> <b>probabilities</b> as {{the basis}} of Bayesian {{inference}} has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.|$|R
25|$|Thomas Bayes {{attempted}} {{to provide a}} logic that could handle varying degrees of confidence; as such, <b>Bayesian</b> <b>probability</b> {{is an attempt to}} recast the representation of probabilistic statements as an expression of the degree of confidence by which the beliefs they express are held.|$|E
25|$|One of {{the many}} {{applications}} of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the <b>Bayesian</b> <b>probability</b> interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.|$|E
25|$|Carrier {{argues in}} his book On the Historicity of Jesus: Why We Might Have Reason for Doubt that there is {{insufficient}} <b>Bayesian</b> <b>probability,</b> that is evidence, {{to believe in the}} existence of Jesus. Furthermore, Carrier argues that the Jesus figure was probably originally known only through private revelations and hidden messages in scripture which were then crafted into a historical figure to communicate the claims of the gospels allegorically. These allegories then started to be believed as fact during the struggle for control of the Christian churches of the first century. He argues that the probability of Jesus' existence is somewhere in the range from 1/3 to 1/12000 depending on the estimates used for the computation.|$|E
50|$|<b>Bayesian</b> <b>probabilities</b> is a {{strategy}} to learn distributions over Bayesian programs. Gamalon, a machine learning company started by Ben Vigoda, invented the term as describing their framework for using Bayesian probabilistic programs to learn specialized probabilistic programs based on input data.|$|R
50|$|A decision-theoretic {{justification}} {{of the use}} of Bayesian inference (and hence of <b>Bayesian</b> <b>probabilities)</b> was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures. Conversely, every Bayesian procedure is admissible.|$|R
5000|$|Equation for f(m|n) {{looks like}} the Bayes formula for a posteriori probabilities; if l(n|m) in the result of {{learning}} become conditional likelihoods, f(m|n) become <b>Bayesian</b> <b>probabilities</b> for signal n originating from object m. The dynamic logic of the NMF is defined as follows: ...|$|R
25|$|Ramsey in {{his article}} disagrees with Keynes's {{approach}} as for him {{there is a difference}} between the notions of probability in physics and in logic. For Ramsey probability is not related to a disembodied body of knowledge but is related to the knowledge that each individual possesses alone. Thus personal beliefs that are formulated by this individual knowledge govern probabilities, leading to the notions of subjective probability and <b>Bayesian</b> <b>probability.</b> Consequently, subjective probabilities can be inferred by observing actions that reflect individuals' personal beliefs. Ramsey argued that the degree of probability that an individual attaches to a particular outcome can be measured by finding what odds the individual would accept when betting on that outcome.|$|E
25|$|Bayes’ theorem is {{fundamental}} to Bayesian inference. It is a subset of statistics, providing a mathematical framework for forming inferences through the concept of probability, in which evidence about the true {{state of the world}} is expressed in terms of degrees of belief through subjectively assessed numerical probabilities. Such a probability is known as a <b>Bayesian</b> <b>probability.</b> The fundamental ideas and concepts behind Bayes’ theorem, and its use within Bayesian inference, have been developed and added to over the past centuries by Thomas Bayes, Richard Price and Pierre Simon Laplace as well as numerous other mathematicians, statisticians and scientists. Bayesian inference has experienced spikes in popularity as it has been seen as vague and controversial by rival frequentist statisticians. In the past few decades Bayesian inference has become widespread in many scientific and social science fields such as marketing. Bayesian inference allows for decision making and market research evaluation under uncertainty and limited data.|$|E
500|$|Timescales for ice cores {{from the}} same {{hemisphere}} can usually be synchronised using layers that include material from volcanic events. [...] It {{is more difficult to}} connect the timescales in different hemispheres. [...] The Laschamp event, a geomagnetic reversal about 40,000 years ago, can be identified in cores; away from that point, measurements of gases such as [...] (methane) can be used to connect the chronology of a Greenland core (for example) with an Antarctic core. [...] In cases where volcanic tephra is interspersed with ice, it can be dated using argon/argon dating and hence provide fixed points for dating the ice. [...] Uranium decay has also been used to date ice cores. [...] Another approach is to use <b>Bayesian</b> <b>probability</b> techniques to find the optimal combination of multiple independent records. [...] This approach was developed in 2010 and has since been turned into a software tool, DatIce.|$|E
40|$|FIGURE 4. <b>Bayesian</b> {{posterior}} <b>probability</b> {{tree was}} reconstructed from 16 S ribosomal RNA mitochondrial gene sequences with Metaphrynella pollicaris, Metaphrynella sundana and Phrynella pulchra as outgroups. Maximum-likelihood tree produced near-identical topology. Two reliability indices are given on nodes: the <b>Bayesian</b> posterior <b>probabilities</b> / the maximum likelihood bootstrap percentages. Symbol (*) indicates nodes with good bootstrap supports for ML (> 80 %) inferences and <b>Bayesian</b> posterior <b>probabilities</b> (BPP> 95 %), and symbol (-) represents that node values {{are less than}} 60 %...|$|R
40|$|Figure 9 - Pachyseris and Agariciidae phylogenetic {{reconstruction}} {{inferred from}} Bayesian inference analysis of mitochondrial intergenic spacer between COI and 16 S-rRNA. Specimens identified as Pachyseris inattesa sp. n., Pachyseris rugosa and, Pachyseris speciosa are highlighted in pink, green and blue, respectively. Specimens of Leptoseris foliosa are indicated in bold. Uppercase letters A, B, and C delineate Pachyseris lineages. Clade numbers IV and VII are {{as reported by}} Fukami et al. (2008). Node values are Posterior <b>Bayesian</b> <b>probabilities</b> (> 0. 8), ML (> 80 %) bootstrap values, MP (> 50 %) bootstrap values. Posterior <b>Bayesian</b> <b>probabilities</b> below 0. 8, ML bootstrap values below 80 %, and MP bootstrap values below 50 % are indicated by a dash (-). Siderastrea radians was selected as outgroup...|$|R
40|$|Figure 1 - 50 % {{consensus}} tree {{produced by}} the Bayesian analysis of a concatenated matrix with three loci (nuLSU, mtSSU and RPB 1) with 2531 characters and highlighting the Arctomiaceae and the newly described Arctomia borbonica. Branches supported by MPBS and MLBS > 70 % and <b>Bayesian</b> posterior <b>probabilities</b> > 0. 95 are in black; those supported by MLBS > 70 % and <b>Bayesian</b> posterior <b>probabilities</b> > 0. 95 in dark grey and those only by <b>Bayesian</b> posterior <b>probabilities</b> > 0. 95 in light grey...|$|R
2500|$|Three corollaries {{are given}} for the sixth principle, which amount to <b>Bayesian</b> <b>probability.</b> Where event [...] {{exhausts}} the list of possible causes for event B, [...] Then ...|$|E
2500|$|<b>Bayesian</b> <b>probability</b> {{specifies}} {{that there}} is some prior probability. Bayesian statisticians can use both an objective and a subjective approach when interpreting the prior probability, which is then updated in light of new relevant information. The concept is a manipulation of conditional probabilities: ...|$|E
2500|$|The use of <b>Bayesian</b> <b>probability</b> {{involves}} specifying a prior probability. This may {{be obtained}} from consideration of whether the required prior probability is greater or lesser than a reference probability associated with an urn model or a thought experiment. [...] The issue is that for a given problem, multiple thought experiments could apply, and choosing one {{is a matter of}} judgement: different people may assign different prior probabilities, known as the reference class problem.|$|E
5000|$|Stephan Hartmann (<b>Bayesian</b> epistemology, <b>probability,</b> {{collective}} decision-making, etc.) ...|$|R
40|$|FIGURE 4. <b>Bayesian</b> {{posterior}} <b>probability</b> {{tree was}} reconstructed from 16 S ribosomal RNA mitochondrial gene sequences with Philautus aurifasciatus, Kurixalus eiffingeri and K. odontotarsus as outgroups. Maximum-likelihood tree produced nearidentical topology. Two reliability indices are given on nodes: the <b>Bayesian</b> posterior <b>probabilities</b> / the maximum likelihood bootstrap percentages...|$|R
40|$|FIGURE 20. Bayesian {{inference}} consensus tree of the D 3 - D 5 data set. Branch support> 50 of corresponding clades indicated as follow: The bold numbers {{refer to}} <b>bayesian</b> posterior <b>probabilities.</b> Numbers in italics are minimum-evolution bootstrap support values of corresponding clades. The inset displays the Maximum-likelihood tree {{resulting from the}} separate analysis of the Halichondriidae s. s. / Suberitidae clade (see text). The bold numbers refer to <b>bayesian</b> posterior <b>probabilities,</b> numbers in italics are minimum-evolution bootstrap support values, followed by <b>bayesian</b> posterior <b>probabilities</b> under secondary structure specific models (regular font) ...|$|R
2500|$|Many of {{the proponents}} of this {{resolution}} and variants of it have been advocates of <b>Bayesian</b> <b>probability,</b> {{and it is now}} commonly called the Bayesian Solution, although, as Chihara observes, [...] "there {{is no such thing as}} the Bayesian solution. There are many different 'solutions' that Bayesians have put forward using Bayesian techniques." [...] Noteworthy approaches using Bayesian techniques include Earman, Eells, Gibson, Hosiasson-Lindenbaum, Howson and Urbach, Mackie, and Hintikka, who claims that his approach is [...] "more Bayesian than the so-called 'Bayesian solution' of the same paradox". Bayesian approaches that make use of Carnap's theory of inductive inference include Humburg, Maher, and Fitelson et al. Vranas introduced the term [...] "Standard Bayesian Solution" [...] to avoid confusion.|$|E
2500|$|<b>Bayesian</b> <b>probability</b> {{is often}} {{found to be}} {{difficult}} when analysing and assessing probabilities due to its initial counter intuitive nature. Often when deciding between strategies based on a decision, they are interpreted as: [...] where there is evidence X that shows condition A might hold true, is misread by judging A’s likelihood by how well the evidence X matches A, but crucially without considering the prior frequency of A. In alignment with Falsification, which aims to question and falsify instead of prove hypotheses, where there is very strong evidence X, {{it does not necessarily}} mean there is a very high probability that A leads to B, but in fact should be interpreted as a very low probability of A not leading to B.|$|E
2500|$|Evidential probability, {{also called}} <b>Bayesian</b> <b>probability,</b> can be {{assigned}} to any statement whatsoever, even when no random process is involved, {{as a way to}} represent its subjective plausibility, or {{the degree to which the}} statement is supported by the available evidence. [...] On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. [...] The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap" [...] and [...] "probability2" [...] for evidential and physical probability, respectively.). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom).|$|E
40|$|Objectives. The purposeof this studywas to comparethe diagnosesof healedmyocardialinfarctionmadefromthe 12 -lead electrocardiogram(ECG) by artificialneuralnetworksand an experiencedelectrocardiographer. Background. Artificialneuralnetworkshaveprovedofvaluein patternrecognitiontasks. Studiesof theirutilityin ECGinter-pretationhaveshownperformancexceedingthatofconventional ECGinterpretationprograms. The latterpresentverbalstate-ments,oftenwith an indicationof the likelihoodfor {{a certain}} diagnosis,such as “possibleleft ventricularhypertrophy. ”A neuralnetworkpresentsits outputas a numericvaluebetweenO and 1; however,these values can be interpretedas <b>Bayesian</b> <b>probabilities.</b> Methods,Thestudywasbasedon 351 healthyvolunteersand 1, 313 patientswitha historyof chestpain whohad undergon...|$|R
40|$|One kind of {{deliberation}} {{involves an}} individual reassessing {{the strengths of}} her beliefs {{in the light of}} new evidence. Bayesian epistemology measures the strength to which one ought to believe a proposition by its probability relative to all available evidence, and thus provides a normative account of individual deliberation. This can be extended to an account of individual judgement by treating the act of judgement as a decision problem, amenable to the tools of decision theory. A normative account of public deliberation and judgement can be provided by merging the evidence of the individuals in question and calculating appropriate <b>Bayesian</b> <b>probabilities</b> and judgement thresholds relative to this merged evidence. But this formal epistemology for deliberation and judgement lacks substance without an account of how evidence can be merged. And in order to provide such an account, we need in turn an account of what the evidence is that grounds <b>Bayesian</b> <b>probabilities.</b> This paper attempts to tackle these tw...|$|R
40|$|Abstract. We {{present a}} novel {{approach}} to probabilistic description logic programs for the Semantic Web, which constitutes a tight combination of disjunctive logic programs under the answer set semantics with both description logics and <b>Bayesian</b> <b>probabilities.</b> The approach {{has a number of}} nice features. In particular, it allows for a natural probabilistic data integration, where probabilities over possible worlds may be used as trust, error, or mapping probabilities. Furthermore, it also provides a natural integration of a situation-calculus based language for reasoning about actions with both description logics and <b>Bayesian</b> <b>probabilities.</b> We show that consistency checking and query processing are decidable resp. computable, and that they can be reduced to consistency checking resp. cautious/brave reasoning in tightly integrated disjunctive description logic programs. We also analyze the complexity of consistency checking and query processing in probabilistic description logic programs in special cases. In particular, we present a special case of these problems with polynomial data complexity...|$|R
2500|$|Subjectivists assign numbers per {{subjective}} probability, i.e., as {{a degree}} of belief. The degree of belief has been interpreted as, [...] "the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E." [...] The most popular version of subjective probability is <b>Bayesian</b> <b>probability,</b> which includes expert knowledge as well as experimental data to produce probabilities. [...] The expert knowledge is represented by some (subjective) prior probability distribution. [...] These data are incorporated in a likelihood function. The product of the prior and the likelihood, normalized, results in a posterior probability distribution that incorporates all the information known to date. By Aumann's agreement theorem, Bayesian agents whose prior beliefs are similar {{will end up with}} similar posterior beliefs. However, sufficiently different priors can lead to different conclusions regardless of how much information the agents share.|$|E
2500|$|Most studies only {{sample part}} of a population, so results don't fully {{represent}} the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This {{does not imply that}} the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. [...] Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by [...] "probability", that is as a <b>Bayesian</b> <b>probability.</b>|$|E
5000|$|Coherence (philosophical {{gambling}} strategy), analogous {{concept in}} <b>Bayesian</b> <b>probability</b> ...|$|E
3000|$|The <b>Bayesian</b> outage <b>probability,</b> P̅_out, can be {{approximated}} in {{closed-form solution}} {{by the following}} expression: [...]...|$|R
40|$|Figure 4 - 50 % majority-rule {{consensus}} of <b>Bayesian</b> posterior <b>probabilities</b> resulting from {{analysis of the}} combined data. <b>Bayesian</b> posterior <b>probabilities</b> (PP) (> 0. 50) and bootstrap support (BS) (> 50) are noted. Yumtaax veracrucensis sp. n. = Yumtaax LCM; Yumtaax recticornis = Yumtaax recticornis OM; Yumtaax laticornis = Yumtaax CM; Yumtaax cameliae sp. n. = Yumtaax LM; Yumtaax jimenezi sp. n. = Yumtaax recticornis VM...|$|R
40|$|FIGURE 5. Relationships between Australian Indo-Pacific {{species of}} Kyphosus: A) Bayesian {{analysis}} of mtDNA cyt b, control region, partial 12 S, and tRNA-regions (tRNA-Thr, tRNA-Pro and tRNA-Phe), B) Bayesian analysis of nDNA RAG 1, RAG 2 and Tmo- 4 C 4. <b>Bayesian</b> posterior <b>probabilities</b> are given as percentages next to branches, all branches have a support of 100 % unless otherwise specified. Numbers above branches indicate <b>Bayesian</b> posterior <b>probabilities...</b>|$|R
