0|6197|Public
40|$|A {{new class}} of plug in {{classification}} techniques have recently been developed in the statistics literature. A plug in classification technique (PaCT) is a method that takes a standard classifier (such as LDA or nearest neighbors) and plugs it into an algorithm to produce a new classifier. The standard classifier {{is known as the}} Plug in Classifier (PiC). These methods often produce large improvements over using a single classifier. In this paper we investigate one of these methods and give some motivation for its success. 1 Introduction Dietterich and Bakiri (1995) suggested the following method, motivated by Error Correcting Coding Theory, for solving k class classification problems using binary classifiers. ffl Produce a k by n (n large) <b>binary</b> <b>coding</b> <b>matrix,</b> ie a matrix of zeros and ones. We will denote this matrix by Z, its i; jth component by Z ij, its ith row by Z i and its jth column by Z j. ffl Use the first column of the <b>coding</b> <b>matrix</b> (Z 1) to create two super groups [...] ...|$|R
40|$|Retrieving nearest neighbors across {{correlated}} data {{in multiple}} modalities, such as image-text pairs on Facebook and video-tag pairs on YouTube, {{has become a}} challenging task due to the huge amount of data. Multimodal hashing methods that embed data into <b>binary</b> <b>codes</b> can boost the retrieving speed and reduce storage requirement. As unsupervised multimodal hashing methods are usually inferior to supervised ones, while the supervised ones requires too much manually labeled data, the proposed method in this paper utilizes a part of labels to design a semi-supervised multimodal hashing method. It first computes the transformation matrices for data matrices and label matrix. Then, with these transformation matrices, fuzzy logic is introduced to estimate a label matrix for unlabeled data. Finally, it uses the estimated label matrix to learn hashing functions for data in each modality to generate a unified <b>binary</b> <b>code</b> <b>matrix.</b> Experiments show that the proposed semi-supervised method with 50 % labels can get a medium performance among the compared supervised ones and achieve an approximate performance to the best supervised method with 90 % labels. With only 10 % labels, the proposed method can still compete with the worst compared supervised one...|$|R
40|$|We {{quantify}} {{precisely the}} distribution of the output of a binary random number generator (RNG) after conditioning with a <b>binary</b> linear <b>code</b> generator <b>matrix</b> by showing the connection between the Walsh spectrum of the resulting random variable and the weight distribution of the code. Previously known bounds on the performance of linear <b>binary</b> <b>codes</b> as entropy extractors can be derived by considering generator matrices as a selector of a subset of that spectrum. We also extend this framework to the case of non-binary codes...|$|R
40|$|<b>Binary</b> <b>matrix</b> <b>codes</b> with {{restricted}} {{row and column}} weights are {{a desirable}} method of coded modulation for power line communication. In this work, we construct such <b>matrix</b> <b>codes</b> that are obtained as products of affine codes - cosets of <b>binary</b> linear <b>codes.</b> Additionally, the constructions have the property that they are systematic. Subsequently, we generalize our construction to irregular product of affine codes, where the component codes are affine codes of different rates. Comment: 13 pages, to appear in SIAM Journal on Discrete Mathematic...|$|R
40|$|Applications using face {{biometric}} are {{ubiquitous in}} various domains. We propose an efficient method using Discrete Wavelet Transform (DWT), Extended Directional <b>Binary</b> <b>codes</b> (EDBC), three <b>matrix</b> decompositions and Singular Value Decomposition (SVD) for face recognition. The combined effect of Schur, Hessenberg and QR matrix decompositions are utilized with existing algorithm. The discrimination power between two different persons is justified using Average Overall Deviation (AOD) parameter. Fused EDBC and SVD features are considered for performance calculation. City-block and Euclidean Distance (ED) measure {{is used for}} matching. Performance is improved on YALE, GTAV and ORL face databases compared with existing methods...|$|R
40|$|Subject of investigation: {{covering}} {{codes and}} testing methods. Purpose of the work: {{development of an}} algorithm for decoding codes which correct error bytes, development of a method for compact testing and formation of <b>binary</b> covering <b>codes.</b> Testing <b>matrixes</b> of the covering codes are designed. A new algorithm is developed for decoding, which permits correction of error bytes. Developed is a new cascade method of compact testing. The results {{can be used for}} the development of the computer hardware and software for increasing the reliability of the systemAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Simple memorizing tasks {{have been}} chosen such as a <b>binary</b> <b>code</b> on a <b>matrix.</b> After the {{establishment}} of an appropriate protocol, the codified matrices were individually presented to 150 university students who had to memorize them. A computer simulation for a similar task is available which uses a perceptron on which an algorithm was implemented allowing for some degree of globality (technically referred to as entropic nonextensivity within a current generalization of the usual, Boltzmann-Gibbs, statistical mechanics). Our main observation is that, for the very specific learning task on which we focus here, humans perform similarly to slightly nonextensive perceptrons. Comment: To appear in Complexus (Karger, Basel). 9 pages including 12 figure...|$|R
40|$|We {{consider}} ornamental {{sign language}} of first order where principles of sieve displacement, of asymmetric building blocks as base of ornament symmetry, color exchangeability and side equivalence principles work. The generic aspects of sieve and genesis of ornamental pattern and ornament sign in it are discussed. The hemiolia principle for ornamental genesis is introduced. The discoverer {{of most of}} these principles were artist Modris Tenisons [4, 5, 6, 7 (refs. 23, 24), 8 (ref. 65) ]. Here we apply systematical research using simplest mathematical arguments. We come to conclusions that mathematical argument in arising ornament is of much more significance than simply symmetries in it as in image. We are after to inquire how ornament arises from global aspects intertwinned with these local. We raise argument of sign’s origin from code rather from image, and its eventual impact on research of ornamental patterns, and on research of human prehension of sign and its connection with consciousness. Key words: <b>binary</b> <b>coding,</b> <b>binary</b> <b>matrices,</b> ornaments, asymmetry, sign coding, sieve in ornamental pattern, first order complexity, hemiolia principl...|$|R
40|$|Abstract—Single-tone {{frequency}} shift keying (FSK) modulation with permutation codes {{has been found to}} be useful in addressing the problem of narrowband noise disturbance in power line communications. However, this modulation scheme is restrictive since the number of frequencies used must be at least as large as the number of symbols in the permutation code. In this paper, we propose the use of multitone FSK and <b>binary</b> <b>matrix</b> <b>codes</b> to overcome this restriction. We construct infinite families of efficiently decodable <b>matrix</b> <b>codes</b> with rates and relative distances bounded away from zero, that uses only a logarithmic number of frequencies in the length of the code. Simulation results show that our multitone modulation scheme outperform single-tone modulation schemes. 1...|$|R
40|$|Abstract [...] Output Coding, {{in which}} a multiclass problem is {{decomposed}} into simpler binary sub-problems, has become a popular ensemble classifier approach. Originally the <b>code</b> <b>matrix</b> was chosen based on error-correcting principles, which led to Error-Correcting Output Coding (ECOC). However, various <b>code</b> <b>matrices</b> have been proposed for use with the Output Coding technique. In this paper we consider properties of the <b>code</b> <b>matrix</b> that affect performance, {{and with the help}} of synthetic and natural data, compare random, BCH and equidistant codes. Index Terms [...] Error Correcting Output <b>codes</b> (ECOC), <b>binary</b> <b>coding,</b> multiple classifiers, machine learning...|$|R
40|$|Abstract — This paper {{presents}} a novel image indexing and retrieval algorithm using Gaussian multi-resolution directional <b>binary</b> <b>code</b> (DBC) co-occurrence <b>matrix.</b> DBC histogram captures only the patterns distribution in a texture while the spatial {{correlation between the}} pair of patterns is gathered by DBC Co-occurrence. Multi-resolution texture decomposition and co-occurrence calculation has been efficiently used in the proposed method where multi-resolution texture images are computed using Gaussian filter for collection of DBCs from these particular textures. Eventually, feature vectors are constructed by making into play the co-occurrence matrix that exists between binary patterns. The retrieval results of the proposed method have been tested by conducting two experiments on Brodatz and MIT VisTex texture databases. The results after being investigated show a significant improvement {{in terms of their}} evaluation measures as compared to LBP, DBC and other transform domain features...|$|R
30|$|All the assays {{were carried}} out for three {{independent}} pools of each sample. Data were subjected {{to an analysis of}} variance (p ≤  0.05), and means were separated by Tukey test. Correlations between different parameters {{were carried out}} with Pearson test, by using the SPSS statistics 17.0 software. A principal component analysis (PCA), considering all the quantitative parameters, was carried out by using Past 3.0; the contribution of each parameter for the differentiation of samples was evaluated. For the different tissues (leaves, fruits, and calyces), the phenolic profiles were made up of all compounds present in the respective HPLC–DAD chromatogram. Each compound was treated as a single chemical character. A <b>binary</b> <b>matrix</b> <b>coded</b> by 1 (presence) or 0 (absence) formed by all individual samples vs. all resolved compounds for each type of tissue was subjected to a cluster analysis (paired group algorithm and Jaccard similarity measure) by using Past 3.0.|$|R
40|$|AbstractSome nontrivial {{properties}} of perfect <b>binary</b> <b>codes</b> are discussed. We consider some constructions of perfect <b>binary</b> <b>codes</b> {{with the purpose}} to outline bounds {{on the number of}} nonequivalent perfect <b>binary</b> <b>codes</b> and we present the best known lower and upper bounds on the number of different perfect <b>binary</b> <b>codes...</b>|$|R
40|$|The paper {{approaches}} the low-level {{details of the}} code generated by compilers whose format permits outside actions. <b>Binary</b> <b>code</b> modifications are manually done when the internal format is known and understood, or automatically by certain tools developed to process the <b>binary</b> <b>code.</b> The <b>binary</b> <b>code</b> instrumentation goals may be various from security increasing and bug fixing to development of malicious software. The paper highlights the <b>binary</b> <b>code</b> instrumentation techniques by code injection to increase the security and reliability of a software application. Also, the paper offers examples for <b>binary</b> <b>code</b> formats understanding and how the <b>binary</b> <b>code</b> injection may be applied...|$|R
40|$|<b>Binary</b> <b>coding</b> or hashing {{techniques}} are recognized to accomplish efficient near neighbor search, and have thus attracted broad {{interests in the}} recent vision and learning studies. However, such studies have rarely been dedicated to Maximum Inner Product Search (MIPS), which plays {{a critical role in}} various vision applications. In this paper, we investigate learning <b>binary</b> <b>codes</b> to exclusively handle the MIPS problem. Inspired by the latest advance in asymmetric hashing schemes, we propose an asymmetric <b>binary</b> <b>code</b> learning framework based on inner product fitting. Specifically, two sets of coding functions are learned such that the inner products between their generated <b>binary</b> <b>codes</b> can reveal the inner products between original data vectors. We also propose an alternative simpler objective which maximizes the correlations between the inner products of the produced <b>binary</b> <b>codes</b> and raw data vectors. In both objectives, the <b>binary</b> <b>codes</b> and coding functions are simultaneously learned without continuous relaxations, which is the key to achieving high-quality <b>binary</b> <b>codes.</b> We evaluate the proposed method, dubbed Asymmetric Inner-product <b>Binary</b> <b>Coding</b> (AIBC), relying on the two objectives on several large-scale image datasets. Both of them are superior to the state-of-the-art <b>binary</b> <b>coding</b> and hashing methods in performing MIPS tasks...|$|R
40|$|Different {{strategies}} for binary analysis {{are widely used}} in systems dealing with software maintenance and system security. <b>Binary</b> <b>code</b> is self-contained; though {{it is easy to}} execute, {{it is not easy to}} read and understand. Binary analysis tools are useful in software maintenance because the binary of software has all the information necessary to recover the source code. It is also incredibly important and sensitive in the domain of security. Malicious <b>binary</b> <b>code</b> can infect other applications, hide in their <b>binary</b> <b>code,</b> contaminate the whole system or travel through Internet and attack other systems. This makes it imperative for security personnel to scan and analyze <b>binary</b> <b>codes</b> with the aid of the <b>binary</b> <b>code</b> analysis tools. On the other hand, crackers can reverse engineer the <b>binary</b> <b>code</b> to assembly code in order to break the secrets embedded in the <b>binary</b> <b>code,</b> such as registration number, password or secret algorithms. This motivates researches to prevent malicious monitoring by <b>binary</b> <b>code</b> analysis tools. Evidently, binary analysis tools play an important doublesided role in security. This paper surveys <b>binary</b> <b>code</b> analysis from the most fundamental perspective views: the <b>binary</b> <b>code</b> formats, several of the most basic analysis tools, such as disassembler, debugger and the instrumentation tools based on them. The previous research on binary analysis are investigated and summarized and a new approach of analysis, disasembler-based binary interpreter, is proposed and discussed. 1...|$|R
30|$|Hinton et al. [34] {{describe}} a Deep Learning generative model {{to learn the}} <b>binary</b> <b>codes</b> for documents. The lowest layer of the Deep Learning network represents the word-count vector of the document which accounts as high-dimensional data, while the highest layer represents the learnt <b>binary</b> <b>code</b> of the document. Using 128 -bit codes, the authors demonstrate that the <b>binary</b> <b>codes</b> of the documents that are semantically similar lay relatively closer in the Hamming space. The <b>binary</b> <b>code</b> of the documents can then be used for information retrieval. For each query document, its Hamming distance compared to all other documents in the data is computed and the top D similar documents are retrieved. <b>Binary</b> <b>codes</b> require relatively little storage space, and in addition they allow relatively quicker searches by using algorithms such as fast-bit counting to compute the Hamming distance between two <b>binary</b> <b>codes.</b> The authors conclude that using these <b>binary</b> <b>codes</b> for document retrieval is more accurate and faster than semantic-based analysis.|$|R
40|$|In this paper, we {{introduce}} a novel criterion to rank puncturing patterns for rate-compatible LDPC codes. Specifically, based on Gaussian approximation density evolution, a cost function is devised {{to characterize the}} degree distribution of the punctured <b>code</b> <b>matrices,</b> which are derived from a mother <b>code</b> <b>matrix</b> by matrix transformation. This cost function allows us to effectively compare the expected performance of candidate puncturing patterns and to sort out good ones. Combined with well-designed search algorithms, the proposed criterion can be applied on both standardized Block-LDPC <b>codes</b> and generic <b>binary</b> LDPC <b>codes</b> to get good puncturing patterns with manageable complexity. Numerical simulation results verify {{the effectiveness of the}} proposed ranking criterion, and demonstrate that a series of good rate-compatible LDPC codes can be obtained by the proposed ranking criterion...|$|R
30|$|Instead {{of using}} a code with base 3 to encode the three states, LTP uses two <b>binary</b> <b>codes</b> {{representing}} the positive and the negative components of the ternary <b>code,</b> i.e., two <b>binary</b> <b>codes</b> coding for the two states {- 1, 1 }. These <b>binary</b> <b>codes</b> are collected in two separate histograms and, as a last step, the histograms are concatenated to form the LTP feature vector.|$|R
40|$|We {{examine the}} p-ary <b>codes</b> from {{incidence}} <b>matrices</b> of Paley graphs P (q) where q ≡ 1 (mod 4) {{is a prime}} power, and show that the codes are [q(q− 1) / 4, q − 1, (q − 1) / 2]_ 2 or [q(q − 1) / 4, q, (q − 1) / 2]p for p odd. By ﬁnding PD-sets we show that for q > 9 the p-ary codes, for any p, {{can be used for}} permutation decoding for full error-correction. The <b>binary</b> <b>code</b> from the line graph of P (q) is shown to be the same as the <b>binary</b> <b>code</b> from an incidence matrix for P (q) ...|$|R
40|$|Some {{mutually}} quasi-unbiased weighing matrices {{are constructed}} from <b>binary</b> <b>codes</b> satisfying certain conditions. Motivated by this, in this note, we study <b>binary</b> <b>codes</b> satisfying the conditions. The weight distributions of <b>binary</b> <b>codes</b> satisfying {{the conditions are}} determined. We also give a classification of <b>binary</b> <b>codes</b> of lengths $ 8, 16 $ and <b>binary</b> maximal <b>codes</b> of length $ 32 $ satisfying the conditions. As an application, sets of $ 8 $ mutually quasi-unbiased weighing matrices for parameters $(16, 16, 4, 64) $ and $ 4 $ mutually quasi-unbiased weighing matrices for parameters $(32, 32, 4, 256) $ are constructed for the first time. Comment: 14 page...|$|R
40|$|We give a {{classification}} of singly-even self-dual <b>binary</b> <b>codes</b> of length 32, by enumerating all neighbours {{of the known}} 85 doubly-even self-dual <b>binary</b> <b>codes</b> of length 32. There are 3, 210 singly-even self-dual <b>binary</b> <b>codes</b> of length 32 up to equivalence. This agrees in number with the enumeration by Bilous and van Rees, who enumer-ated these codes by a different method. ...|$|R
40|$|Binary {{encoding}} on high-dimensional {{data points}} has {{attracted much attention}} due to its computational and storage efficiency. While numerous efforts {{have been made to}} encode data points into <b>binary</b> <b>codes,</b> how to calculate the effective distance on <b>binary</b> <b>codes</b> to approximate the original distance is rarely addressed. In this paper, we propose an effective distance measurement for <b>binary</b> <b>code</b> ranking. In our approach, the <b>binary</b> <b>code</b> is firstly decomposed into multiple sub codes, each of which generates a query-dependent distance lookup table. Then the distance between the query and the <b>binary</b> <b>code</b> is constructed as the aggregation of the distances from all sub codes by looking up their respective tables. The entries of the lookup tables are optimized by minimizing the misalignment between the approximate distance and the original distance. Such a scheme is applied to both the symmetric distance and the asymmetric distance. Extensive experimental results show superior performance of the proposed approach over state-of-the-art methods on three real-world high-dimensional datasets for <b>binary</b> <b>code</b> ranking...|$|R
40|$|We {{explore the}} {{connection}} between simple polytopes and self-dual <b>binary</b> <b>codes</b> via the theory of small covers. We first show that a small cover M^n over a simple n-polytope P^n produces a self-dual code {{in the sense of}} Kreck-Puppe if and only if P^n is n-colorable and n is odd. Then we show how to describe such a self-dual <b>binary</b> <b>code</b> in terms of the combinatorial information of P^n. Moreover, we can define a family of <b>binary</b> <b>codes</b> B_k(P^n), 0 ≤ k≤ n, from an arbitrary simple n-polytope P^n. We will give some necessary and sufficient conditions for B_k(P^n) to be a self-dual code. A spinoff of our study of such <b>binary</b> <b>codes</b> gives some new ways to judge whether a simple n-polytope P^n is n-colorable in terms of the associated <b>binary</b> <b>codes</b> B_k(P^n). In addition, we prove that the minimum distance of the self-dual <b>binary</b> <b>code</b> obtained from a 3 -colorable simple 3 -polytope is always 4. Comment: 27 pages, 5 figure...|$|R
50|$|Build {{automation}} is {{the process}} of automating the creation of a software build and the associated processes including: compiling computer source <b>code</b> into <b>binary</b> <b>code,</b> packaging <b>binary</b> <b>code,</b> and running automated tests.|$|R
30|$|In our implementation, as {{described}} above, the interval coded with zero is half-bound while in [21] it is open. Instead {{of using a}} code with base 3 to encode the three states in Eq. 5, LTP uses two <b>binary</b> <b>codes</b> representing the positive and the negative components of the ternary <b>code,</b> i.e., two <b>binary</b> <b>codes</b> coding for the two states {− 1, 1 }. These <b>binary</b> <b>codes</b> are collected in two separate histograms, and as a last step, the histograms are concatenated to form the LTP feature vector [21].|$|R
40|$|Abstract—We {{investigate}} a <b>binary</b> <b>code,</b> which is implemented by serially concatenating a multiplexer, a multilevel delay proces-sor, and a signal mapper to a binary turbo encoder. To achieve improved convergence behavior, we modify the <b>binary</b> <b>code</b> by passing {{only a fraction}} of the bits in the turbo code through the multilevel delay processor and the signal mapper. Two decoding methods are discussed and their performances are evaluated. Index Terms—Turbo <b>codes,</b> concatenated <b>codes,</b> <b>binary</b> <b>codes.</b> I...|$|R
40|$|Hashing based {{methods have}} {{attracted}} considerable attention for efficient cross-modal retrieval on large-scale multimedia data. The core problem of cross-modal hashing {{is how to}} learn compact <b>binary</b> <b>codes</b> that construct the underlying correlations between heterogeneous features from different modalities. A majority of recent approaches aim at learning hash functions to preserve the pairwise similarities defined by given class labels. However, these methods fail to explicitly explore the discriminative property of class labels during hash function learning. In addition, they usually discard the discrete constraints imposed on the to-be-learned <b>binary</b> <b>codes,</b> and compromise to solve a relaxed problem with quantization to obtain the approximate binary solution. Therefore, the <b>binary</b> <b>codes</b> generated by these methods are suboptimal and less discriminative to different classes. To overcome these drawbacks, we propose a novel cross-modal hashing method, termed discrete cross-modal hashing (DCH), which directly learns discriminative <b>binary</b> <b>codes</b> while retaining the discrete constraints. Specifically, DCH learns modality-specific hash functions for generating unified <b>binary</b> <b>codes,</b> and these <b>binary</b> <b>codes</b> are viewed as representative features for discriminative classification with class labels. An effective discrete optimization algorithm is developed for DCH to jointly learn the modality-specific hash function and the unified <b>binary</b> <b>codes.</b> Extensive experiments on three benchmark data sets highlight the superiority of DCH under various cross-modal scenarios and show its state-of-the-art performance. </p...|$|R
40|$|Philosophiae Doctor - PhDIn this thesis, we {{describe}} linear codes over prime fields obtained from incidence designs of iterated line graphs of complete graphs Li(Kn) where i = 1, 2. In the binary case, results are extended to codes from neighbourhood designs {{of the line}} graphs Li+ 1 (Kn) using certain elementary relations. Codes from incidence designs of complete graphs, Kn, and neighbourhood designs of their line graphs, L 1 (Kn) (the so-called triangular graphs), have been considered elsewhere by others. We consider codes from incidence designs of L 1 (Kn) and L 2 (Kn), and neighbourhood designs of L 2 (Kn) and L 3 (Kn). In each case, basic parameters of the codes are determined. Further, we introduce a family of vertex-transitive graphs Γn that are embeddable into the strong product L 1 (Kn) ⊠  K 2, of triangular graphs and K 2, a class which at first sight may seem unnatural but, on closer look, is a repository of graphs rich with combinatorial structures. For instance, unlike most regular graphs considered here and elsewhere that only come with incidence and neighbourhood designs, Γn also has what we have termed as 6 -cycle designs. These are designs in which the point set contains vertices of the graph and every block contains vertices of a 6 -cycle in the graph. Also, <b>binary</b> <b>codes</b> from incidence <b>matrices</b> of these graphs have other minimum words in addition to incidence vectors of the blocks. In addition, these graphs have induced subgraphs isomorphic to the family Hn of complete porcupines (see Definition 4. 11). We describe <b>codes</b> from incidence <b>matrices</b> of Γn and Hn and determine their parameters. South Afric...|$|R
40|$|In this paper, the undetected error {{probability}} for large <b>binary</b> <b>codes</b> is studied. It is shown {{that if the}} size of the code is sufficiently large (for given length), then the code is good for error detection (in the technical sense). Index Terms- Undetected {{error probability}}, error detection, <b>binary</b> <b>codes,</b> good codes. ...|$|R
40|$|In this paper, we {{introduce}} a Graph based <b>Binary</b> <b>Code</b> Execution Path Exploration Platform. In the graph, a node {{is defined as}} a conditional branch instruction, and an edge is defined as the other instructions. We implemented prototype of the proposed method and works well on real <b>binary</b> <b>code.</b> Experimental results show proposed method correctly explores execution path of target <b>binary</b> <b>code.</b> We expect our method can help Software Assurance, Secure Programming, and Malware Analysis mor...|$|R
40|$|Binary hashing {{has been}} widely used for {{efficient}} simi-larity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional da-ta are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their <b>binary</b> <b>codes.</b> The Hamming dis-tance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the <b>binary</b> <b>codes</b> of hashing methods. By assigning different bit-level weights to different hash bits, the returned <b>binary</b> <b>codes</b> are ranked at a finer-grained <b>binary</b> <b>code</b> level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for <b>binary</b> <b>code</b> ranking. 1...|$|R
40|$|This paper tackles the {{efficiency}} problem of making recom-mendations {{in the context}} of large user and item spaces. In particular, we address the problem of learning <b>binary</b> <b>codes</b> for collaborative filtering, which enables us to effi-ciently make recommendations with time complexity that is independent {{of the total number of}} items. We propose to construct <b>binary</b> <b>codes</b> for users and items such that the preference of users over items can be accurately preserved by the Hamming distance between their respective <b>binary</b> <b>codes.</b> By using two loss functions measuring the degree of divergence between the training and predicted ratings, we formulate the problem of learning <b>binary</b> <b>codes</b> as a discrete optimization problem. Although this optimization problem is intractable in general, we develop effective relaxations that can be efficiently solved by existing methods. Moreover, we investigate two methods to obtain the <b>binary</b> <b>codes</b> from the relaxed solutions. Evaluations are conducted on three public-domain data sets and the results suggest that our pro-posed method outperforms several baseline alternatives...|$|R
50|$|Executor {{translates}} 68k big-endian <b>binary</b> <b>code</b> into x86 little-endian <b>binary</b> <b>code.</b> Executor {{can only}} run Macintosh {{programs designed to}} run on 68000-based Macintosh hardware. Executor can mimic either Macintosh System 7.0.0, or System 6.0.7 for older applications that are incompatible with System 7.0.0.|$|R
40|$|AbstractThe two {{concepts}} dual code and parity check matrix for a linear perfect 1 -error correcting <b>binary</b> <b>code</b> are generalized {{to the case}} of non-linear perfect codes. We show how this generalization can be used to enumerate some particular classes of perfect 1 -error correcting <b>binary</b> <b>codes.</b> We also use it to give an answer to a problem of Avgustinovich: whether or not the kernel of every perfect 1 -error correcting <b>binary</b> <b>code</b> is always contained in some Hamming code...|$|R
3000|$|Another way {{of making}} LBP {{rotation}} invariant is introduced in [9, 23]. The occurrences of rotation codes within the rotation groups are not summed, as in LBP_N,R^ri, but Fourier transformed and the resulting power spectrum is used as the feature vector. The descriptor is called LBP histogram Fourier features (LBP_N,R^HF). Since the Fourier features are computed on the global histogram of <b>binary</b> <b>codes</b> in the region/patch investigated, LBP_N,R^HF achieves rotation invariance globally, and hence, retains the relative distribution within rotation groups [23]. However, the LBP_N,R^HF descriptor in [9] only considers uniform <b>binary</b> <b>codes</b> (<b>binary</b> <b>codes</b> with at the most two transitions between 0 and 1). It was generalized in [10] to include all <b>binary</b> <b>codes,</b> uniform and non-uniform, and called LBP [...]...|$|R
