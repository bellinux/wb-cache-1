7|10000|Public
5000|$|Both {{types of}} access deal with records of a data set. Basic access methods {{read or write}} one {{physical}} record [...] - [...] block [...] - [...] at a time. Queued methods support internal <b>blocking</b> <b>of</b> <b>data</b> and also often read-ahead scheme. Queued access methods generally provide better performance, while basic methods provide more flexibility.|$|E
5000|$|... {{processing}} {{is necessary}} {{for the purposes of the}} legitimate interests pursued by the controller or by the third party or parties to whom the data are disclosed, except where such interests are overridden by the interests for fundamental rights and freedoms of the data subject. The data subject has the right to access all data processed about him. The data subject even has the right to demand the rectification, deletion or <b>blocking</b> <b>of</b> <b>data</b> that is incomplete, inaccurate or not being processed in compliance with the data protection rules. (art. 12) ...|$|E
40|$|The {{target of}} the thesis to reveal how much is {{possible}} {{to find out the}} truth about sensitive governmental topics from Russian mass media. In thesis will be compared results from Google and another search engine with results from another countries where that search engines is located. The thesis will be supported by citizen of Russian Federation and employee of anti propagandistic department in Brussels. Theoretical part of thesis will be dealing with censorship on the internet in general and ways how it could be realized. In thesis will be outline opportunities how censorship could be ignored. In practical part will be compared results from Russian search engine with Czech and another theirs American and European versions. Trough tracing will try to find out where exactly come about <b>blocking</b> <b>of</b> <b>data</b> retrieval and handling with results in case it will be manifeste...|$|E
50|$|Yet another {{solution}} to storing persistent data is to associate a name with each <b>block</b> <b>of</b> <b>data,</b> {{and use a}} distributed hash table to pseudo-randomly assign that name {{to one of the}} available servers, and then store that <b>block</b> <b>of</b> <b>data</b> in the assigned server.|$|R
50|$|ExOR is more {{efficient}} with large <b>blocks</b> <b>of</b> <b>data.</b> These give more chances for a batch to find alternative routes. However, the batchmaps get larger, too. So, <b>blocks</b> <b>of</b> <b>data</b> more than 100,000 bytes are broken into groups <b>of</b> <b>data</b> packets called batches. Smaller messages are just sent by the most reliable route.|$|R
25|$|This type of shellcode {{is similar}} to egg-hunt shellcode, but looks for {{multiple}} small <b>blocks</b> <b>of</b> <b>data</b> (eggs) and recombines them into one larger block (the omelette) that is subsequently executed. This is used when an attacker can only inject a number <b>of</b> small <b>blocks</b> <b>of</b> <b>data</b> into the process.|$|R
40|$|This study {{presents}} {{models for}} management of voice and data traffic and new algorithms, which use call admission control {{as well as}} buffer management to optimise the performance of single channel systems such as wireless local area networks {{in the presence of}} mobile stations. Unlike existing studies, the new approach queues incoming voice packets as well as data packets, and uses a new pre-emption algorithm {{in order to keep the}} response time of voice requests at certain levels while the <b>blocking</b> <b>of</b> <b>data</b> requests is minimised. A new performance metric is introduced to provide uncorrelated handling of integrated services. Queueing related issues such as overall queue capacity, individual capacities for voice and data requests, the probability of blocking, and effects of waiting time on overall quality of service are considered in detail. Analytical models are presented and the results obtained from the analytical models were validated using discrete event simulations...|$|E
40|$|Data mining {{services}} require accurate {{input data}} for their results to be meaningful, but privacy concerns may influence users to provide spurious information. The problem of privacy-preserving data mining has numerous applications in homeland security, medical database mining, and customer transaction analysis. The main {{feature of the}} most PPDM algorithms is that they usually modify the database through insertion of false information or through the <b>blocking</b> <b>of</b> <b>data</b> values in order to hide sensitive information. In this paper, we have incorporated the privacy preserving concept into the previously developed weighted utility mining approach. In this, we have presented an efficient algorithm for mining of privacy preserving high utility item sets by considering the sensitive item sets. The algorithm comprise of three major steps to attain the aim of our research includes, 1) Data sanitization, 2) Construction of sensitive utility FP-tree and, 3) Mining of sensitive utility item sets. The experimentation has carried out using real {{as well as the}} synthetic dataset and the performance of the proposed algorithm is evaluated with the aid of the evaluation metrics such as Miss cost and Database difference ratio...|$|E
40|$|Abstract—Mobile {{traffic demand}} varies {{significantly}} {{in time and}} space. Hence, wireless radio resources in hotspot areas and at peak traffic hours may be scarce. Consequently, special attention has {{to be paid to}} effects induced by admission control, i. e., <b>blocking</b> <b>of</b> <b>data</b> requests by base stations in case of high utilization or overload. Moreover, rising traffic demand requires denser deployments and frequency reuse one. Due to the resulting inter-cell interference, the base stations ’ utilizations have to be considered mutually dependent, which affects the admission control performance. In this paper, we extend a flow level model for elastic traffic, which explicitly takes into account the dynamic mutual inter-cell interference among base stations, by admission control. The model presented allows computing exact values for the average base station resource utilization, flow throughputs, and blocking probabilities. To analyze large networks containing many cells, we extend two approximation techniques, a state aggregation and an average interference approach, and compare them with the exact solution. Both techniques require far less computational effort and show remarkable accuracy. We believe that the extended flow level model is a positive step towards a more accurate, flexible, and holistic framework for network analysis and planning, and self-organizing network techniques. Index Terms—blocking probability; wireless network; inter-ference; admission control; flow level modeling; queuing theory I...|$|E
5000|$|... block ciphers, which encrypt <b>block</b> <b>of</b> <b>data</b> <b>of</b> fixed size, and ...|$|R
50|$|QSAM is—as {{its name}} says—queued, in this {{specific}} context meaning buffered with deblocking <b>of</b> reads and <b>blocking</b> <b>of</b> writes. It allows programs {{to read and}} write logical records within physical <b>blocks</b> <b>of</b> <b>data,</b> as opposed to the less advanced basic sequential access method (BSAM) which allows programs to access physical <b>blocks</b> <b>of</b> <b>data,</b> but provides no support for accessing logical records within blocks.|$|R
50|$|When an {{application}} sends data over an ATM connection using AAL5, the host delivers a <b>block</b> <b>of</b> <b>data</b> to the AAL5 interface. AAL5 generates a trailer, divides the information into 48-octet pieces, and transfers each piece across the ATM network {{in a single}} cell. On {{the receiving end of}} the connection, AAL5 reassembles incoming cells into a packet, checks the CRC to ensure that all pieces arrived correctly, and passes the resulting <b>block</b> <b>of</b> <b>data</b> to the host software. The process <b>of</b> dividing a <b>block</b> <b>of</b> <b>data</b> into cells and regrouping them is known as ATM segmentation and reassembly (SAR).|$|R
40|$|In {{order to}} cope with consistency-preserving {{operations}} (i. e. transactions) over distributed and heterogeneous database systems, all database systems involved must support a certain transaction protocol. Unfortunately, the ECMA-PCTE standard does not contain a protocol of this kind. The common protocol for distributed transactions is the two-phase commit. An increasingly accepted industrial standard for distributed transactions covering the two-phase commit is the XA specification by the X/Open group which is supported {{by most of the}} UNIX-database vendors. In this paper we propose a concept for an interface that fulfils the XA specification and can be implemented on top of a system conforming to the ECMA-PCTE standard. Recently, more and more (distributed) applications have been based on (transactions on) database systems, e. g. CAD/CAM systems or software engineering environments which are typical PCTE applications. The two-phase commit protocol is not always adequate for all these oft en long-lived kinds of transactions. A great number of non-standard transactions have been suggested to overcome problems like inefficient <b>blocking</b> <b>of</b> <b>data</b> or lack of cooperation between complex activities. However, currently available transaction managers just support the two-phase commit. In {{the second part of the}} paper we present a concept for a transaction system whichallows the processing of both standard and non-standard transactions (including nested transactions, SAGAS, split-and-joint transactions, and S-transactions) over different and distributed database systems...|$|E
5000|$|Early CD {{players were}} very basic in nature. A Laser tracks the <b>blocks</b> <b>of</b> <b>data</b> from {{the centre of}} the disc outwards, while the disc itself {{revolves}} at a variable speed between a starting speed of 495 RPM, and a minimum finishing speed of 212 RPM. Generally, one cycle constituted one <b>block</b> <b>of</b> data.If there is a faulty <b>block</b> <b>of</b> <b>data,</b> the player may do one of the following: ...|$|R
50|$|Often single {{transmission}} channels contain <b>blocks</b> <b>of</b> <b>data</b> at {{more than}} one of the rates.|$|R
25|$|The code values {{shown are}} packed into bytes {{which are then}} packed into <b>blocks</b> <b>of</b> up to 255 bytes. A <b>block</b> <b>of</b> image <b>data</b> begins with a byte that {{declares}} the number of bytes to follow. The last <b>block</b> <b>of</b> <b>data</b> for an image {{is marked by a}} zero block-length byte.|$|R
5000|$|The {{graphics}} {{processing unit}} (GPU), as a specialized computer processor, addresses the demands of real-time high-resolution 3D graphics compute-intensive tasks. By 2012, GPUs had evolved into highly parallel multi-core systems allowing very efficient manipulation <b>of</b> large <b>blocks</b> <b>of</b> <b>data.</b> This design {{is more effective than}} general-purpose central processing unit (CPUs) for algorithms in situations where processing large <b>blocks</b> <b>of</b> <b>data</b> is done in parallel, such as: ...|$|R
50|$|A <b>block</b> <b>of</b> <b>data</b> <b>of</b> size 2^(n+1)-1 {{always has}} one {{sub-block}} of size 2^n aligned on 2^n bytes.|$|R
50|$|Since {{the major}} {{internet}} protocol TCP sends a stream <b>of</b> <b>data,</b> ExOR uses local proxy data servers to accumulate <b>blocks</b> <b>of</b> <b>data.</b>|$|R
5000|$|... 1、Equally {{divides the}} {{original}} data blocks into [...] blocks, and each <b>block</b> <b>of</b> <b>data</b> has -bit data, recorded as ...|$|R
5000|$|Tiling or {{blocking}} - reorganizes a loop {{to iterate}} over <b>blocks</b> <b>of</b> <b>data</b> sized {{to fit in}} the cache.|$|R
50|$|Cylinder-head-sector (CHS) is {{an early}} method for giving {{addresses}} to each physical <b>block</b> <b>of</b> <b>data</b> on a hard disk drive.|$|R
50|$|It {{can mean}} {{to make a}} copy of a {{removable}} media (CD-ROM, DVD, etc.) directly, without first saving the source on an intermediate medium (a harddisk); for example, copying a CD-ROM from a CD-ROM drive to a CD-Writer drive. The copy process requires each <b>block</b> <b>of</b> <b>data</b> to be retrieved and immediately written to the destination, so that there is room in the working memory to retrieve the next <b>block</b> <b>of</b> <b>data.</b>|$|R
50|$|In {{order to}} encrypt or decrypt data, use the {{standard}} <b>block</b> cipher mode <b>of</b> operation on {{all but the}} last two <b>blocks</b> <b>of</b> <b>data.</b>|$|R
40|$|GSM ??????????, ??????????? ??? ????????? ?????????????The paper {{considers}} a <b>block</b> <b>of</b> <b>data</b> transmission and reception using GSM technologies, intended for mobile flaw detectors? ?????? ??????????????? ???? ??????-???????? ?????? ? ?????????????? GSM ??????????, ??????????????? ??? ????????? ????????????...|$|R
5000|$|This [...] "extra" [...] LRC word {{at the end}} <b>of</b> a <b>block</b> <b>of</b> <b>data</b> is {{very similar}} to {{checksum}} and CRC.|$|R
50|$|LZO {{supports}} overlapping {{compression and}} in-place decompression. As a block compression algorithm, it compresses and decompresses <b>blocks</b> <b>of</b> <b>data.</b> <b>Block</b> size {{must be the}} same for compression and decompression. LZO compresses a <b>block</b> <b>of</b> <b>data</b> into matches (a sliding dictionary) and runs of non-matching literals to produce good results on highly redundant data and deals acceptably with non-compressible data, only expanding incompressible data by a maximum of 1/64 of the original size when measured over a <b>block</b> size <b>of</b> at least 1 kB.|$|R
5000|$|BRIN operate by [...] "summarising" [...] large <b>blocks</b> <b>of</b> <b>data</b> into {{a compact}} form, {{which can be}} {{efficiently}} tested to exclude many of them from a database query, early on. These tests exclude a large <b>block</b> <b>of</b> <b>data</b> for each comparison. By reducing the data volume so early on, both by representing large blocks as small tuples, and by eliminating many blocks, BRIN substantially reduce the amount <b>of</b> detailed <b>data</b> that must be examined by the database node on a row-by-row basis.|$|R
5000|$|... #Caption: Diagram of a RAID 3 setup <b>of</b> six-byte <b>blocks</b> and two parity bytes, {{shown are}} two <b>blocks</b> <b>of</b> <b>data</b> in {{different}} colors.|$|R
50|$|A packet is a <b>block</b> <b>of</b> <b>data</b> with length {{that can}} vary between {{successive}} packets, ranging from 7to 65,542 bytes, including the packet header.|$|R
5000|$|Instead of {{providing}} a block-oriented interface that reads and writes fixed sized <b>blocks</b> <b>of</b> <b>data,</b> data is organized into flexible-sized data containers, called objects ...|$|R
5000|$|The {{authentication}} tag {{is constructed}} by feeding <b>blocks</b> <b>of</b> <b>data</b> into the GHASH function and encrypting the result. This GHASH function {{is defined by}} ...|$|R
5000|$|Writing {{many small}} <b>blocks</b> <b>of</b> <b>data</b> can even lead to {{negative}} compression rates, {{so it is}} essential for applications to use large write buffers.|$|R
25|$|Assemblers {{can be used}} to {{generate}} <b>blocks</b> <b>of</b> <b>data,</b> with no high-level language overhead, from formatted and commented source code, to be used by other code.|$|R
50|$|Space-time block codes (STBCs) act on a <b>block</b> <b>of</b> <b>data</b> at once (similarly {{to block}} codes) and also provide {{diversity}} gain but doesn't provide coding gain.|$|R
50|$|The trivial {{example of}} a large image of solid color {{demonstrates}} the variable-length LZW compression used in GIF files.The code values shown are packed into bytes which are then packed into <b>blocks</b> <b>of</b> up to 255 bytes. A <b>block</b> <b>of</b> image <b>data</b> begins with a byte that declares the number of bytes to follow. The last <b>block</b> <b>of</b> <b>data</b> for an image {{is marked by a}} zero block-length byte.|$|R
