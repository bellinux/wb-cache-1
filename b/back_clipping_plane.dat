0|156|Public
5000|$|Modelization {{tool set}} (<b>clipping</b> <b>plane,</b> 2D projection, {{measurement}} tool, colors, view shot,…) ...|$|R
50|$|<b>Clipping</b> <b>planes</b> {{are used}} in 3D {{computer}} graphics {{in order to prevent}} the renderer from calculating surfaces at an extreme distance from the viewer. The plane is perpendicular to the camera, a set distance away (the threshold), and occupies the entire viewport. Used in real-time rendering, <b>clipping</b> <b>planes</b> can help preserve processing for objects within clear sight.|$|R
5000|$|... some {{material}} parameters were removed, including back-face parameters and user defined <b>clip</b> <b>planes.</b>|$|R
50|$|Here n {{stands for}} normal {{of the current}} <b>clipping</b> <b>plane</b> (pointed away from interior).|$|R
5000|$|... #Caption: A view frustum, with near- and far- <b>clip</b> <b>planes.</b> Only {{the shaded}} volume is rendered.|$|R
40|$|Abstract. Several 3 D {{rendering}} {{techniques have}} been developed in which part of the final image {{is the result of}} rendering from a virtual camera whose position in the scene differs from that of the primary camera. In these situations, there is usually a planar surface, such as the reflecting plane of a mirror, that can be considered the physical boundary of the recursively rendered image. In order to avoid artifacts that can arise when rendered geometry penetrates the boundary plane {{from the perspective of the}} virtual camera, an additional <b>clipping</b> <b>plane</b> must be added to the standard six-sided view frustum. However, many 3 D graphics processors cannot support an extra <b>clipping</b> <b>plane</b> natively, or require that vertex and fragment shaders be augmented to explicitly perform the additional clipping operation. This paper discusses a technique that modifies the projection matrix in such a way that the conventional near plane of the view frustum is repositioned to serve as the generally oblique boundary <b>clipping</b> <b>plane.</b> Doing so avoids the performance penalty and burden of developing multiple shaders associated with user-defined <b>clipping</b> <b>planes</b> by keeping the total number of <b>clipping</b> <b>planes</b> at six. The near plane is moved without affecting the four side planes, but the conventional far plane is inescapably destroyed. We analyze the effect on the far plane as well as the related impact on depth buffer precision and present a method for constructing the optimal oblique view frustum. 1...|$|R
50|$|Now to find {{intersection}} point with the clipping window we calculate value of dot product. Let pE {{be a point}} on the <b>clipping</b> <b>plane</b> E.|$|R
50|$|Often, {{objects are}} so far away {{that they do not}} {{contribute}} significantly to the final image. These objects are thrown away if their screen projection is too small. See <b>Clipping</b> <b>plane.</b>|$|R
40|$|The {{concept of}} <b>clipping</b> <b>planes</b> {{is well known}} in {{computer}} graphics and can be used to create cut-away views. But clipping against just analytical defined planes is not always suitable for communicating every aspect of such visualization. For example, in hand-drawn technical illustrations, artists tend to communicate the difference between a cut and a model feature by using non-regular, sketchy cut lines instead of straight ones. To enable this functionality in computer graphics, this paper presents a technique for applying 2. 5 D clip-surfaces in real-time. Therefore, the <b>clip</b> <b>plane</b> equation is extended with an additional offset map, which can be represented by a texture map that contains height values. Clipping is then performed by varying the <b>clip</b> <b>plane</b> equation with respect to such an offset map. Further, a capping technique is proposed that enables the rendering of caps onto the clipped area to convey the impression of solid material. It avoids a re-meshing of a solid polygonal mesh after clipping is performed. Our approach is pixel precise, applicable in real-time, and takes fully advantage of graphics accelerators...|$|R
50|$|OpenGL ES 1.1 added {{features}} such as mandatory support for multitexture, better multitexture support (including combiners and dot product texture operations), automatic mipmap generation, vertex buffer objects, state queries, user <b>clip</b> <b>planes,</b> and greater control over point rendering.|$|R
40|$|ABSTRACT: This article {{presents}} a simple technique for splitting up a panoramic range image into {{a set of}} 2 [1 / 2]D representations. The proposed technique consists of three stages. First, a spherical dis-cretization map is generated. Second, main surface orientations are extracted together with their corresponding histogram of distances. Each one of these histograms is used to define {{the position of a}} pro-jection plane as well as two associated <b>clipping</b> <b>planes.</b> Finally, data points bounded by <b>clipping</b> <b>planes</b> are mapped onto the correspond-ing projection plane defining a classical 2 [1 / 2]D range image. This last stage—projection—is applied as many times as main orientations in the spherical discretization map. Experimental results with a pano...|$|R
40|$|Clipping is a fast, common {{technique}} for resolving occlusions. It only requires simple interaction, is easily understandable, and thus {{has been very}} popular for volume exploration. However, a drawback of clipping is that the technique indiscriminately cuts through features. Illustrators, for example, consider the structures {{in the vicinity of}} the cut when visualizing complex spatial data and make sure that smaller structures near the <b>clipping</b> <b>plane</b> are kept in the image and not cut into fragments. In this paper we present a new technique, which combines the simple clipping interaction with automated selective feature preservation using an elastic membrane. In order to prevent cutting objects near the <b>clipping</b> <b>plane,</b> the deformable membrane uses underlying data properties to adjust itself to salient structures. To achieve this behaviour, we translate data attributes into a potential field which acts on the membrane, thus moving the problem of deformation into the soft-body dynamics domain. This allows us to exploit existing GPU-based physics libraries which achieve interactive frame rates. For manual adjustment, the user can insert additional potential fields, as well as pinning the membrane to interesting areas. We demonstrate that our method can act as a flexible and non-invasive replacement of traditional <b>clipping</b> <b>planes...</b>|$|R
50|$|As the {{distance}} between near and far <b>clip</b> <b>planes</b> increases {{and in particular the}} near plane is selected near the eye, the greater the likelihood exists that z-fighting between primitives will occur. With large virtual environments inevitably there is an inherent conflict between the need to resolve visibility in {{the distance}} and in the foreground, so for example in a space flight simulator if you draw a distant galaxy to scale, you will not have the precision to resolve visibility on any cockpit geometry in the foreground (although even a numerical representation would present problems prior to z-buffered rendering). To mitigate these problems, z-buffer precision is weighted towards the near <b>clip</b> <b>plane,</b> {{but this is not the}} case with all visibility schemes and it is insufficient to eliminate all z-fighting issues.|$|R
50|$|In {{computer}} graphics, one of {{the most}} common matrices used for orthographic projection can be defined by a 6-tuple, (left, right, bottom, top, near, far), which defines the <b>clipping</b> <b>planes.</b> These planes form a box with the minimum corner at (left, bottom, -near) and the maximum corner at (right, top, -far).|$|R
5000|$|In the 1992 Roger Corman-produced film Munchie Strikes <b>Back,</b> <b>clips</b> {{from the}} film (including the {{helicopter}} chase) are used {{as part of a}} video game called Death Race 2000.|$|R
40|$|Visualization via direct volume {{rendering}} is {{a potentially}} very powerful technique for exploring and interacting with {{large amounts of}} scientific data. However, the available two-dimensional (2 D) interfaces make three-dimensional (3 D) manipulation with such data very difficult. Many usability problems during interaction in turn discourage {{the widespread use of}} volume rendering as a scientific tool. In this paper, we present a more in-depth investigation into one specific interface aspect, i. e., the positioning of a <b>clipping</b> <b>plane</b> within volume-rendered data. More specifically, we propose three different interface prototypes that have been realized with the help of wireless vision-based tracking. These three prototypes combine aspects of 2 D graphical user interfaces with 3 D tangible interaction devices. They allow to experience and compare different user interface strategies for performing the <b>clipping</b> <b>plane</b> interaction task. They also provide a basis for carrying out user evaluations in the near future...|$|R
40|$|Understanding big {{and complex}} {{scientific}} data {{is still an}} immature topic. It involves studying visualization methods to faithfully represent data, on the one hand, and designing interfaces that truly assist users with data analysis, on the other hand. In an earlier study, we developed guidelines for choosing display environment for four specific, but common, data analysis tasks: identification and judgment of the size, shape, density, and connectivity of objects in a volume. The results showed that using the fish tank virtual reality (VR) system was significantly more accurate at judging the shape, density, and connectivity of objects and significantly faster than the immersive Head-mounted display VR system. Based on those results, we asked the question {{whether or not the}} user performance could be further improved by adding tangible elements into the fish tank VR system. We propose several different interface prototypes of a <b>clipping</b> <b>plane</b> that have been realized with the help of wireless vision-based tracking. These prototypes allow to experience and evaluate those user interface strategies for performing the <b>clipping</b> <b>plane</b> function. An experimental study is carried out to quantitatively measure the added value of these tangible interfaces. The result shows that the inclusion of a tangible frame for controlling a virtual <b>clipping</b> <b>plane</b> and the correspondent 2 D intersection image into the basic fish tank VR system significantly improve the user performance for the shape, size and the connectivity task...|$|R
40|$|Medical {{illustrations}} {{have been}} used for a long time for teaching and communicating information for diagnosis or surgery planning. Illustrative visualization systems create methods and tools that adapt traditional illustration techniques to enhance the result of renderings. Clipping the volume is a popular operation in volume rendering for inspecting the inner parts, though it may remove some information of the context that is worth preserving. In this paper we present a new editing technique based on the use of <b>clipping</b> <b>planes,</b> direct structure extrusion, and illustrative methods, which preserves the context by adapting the extruded region to the structures of interest of the volumetric model. We will show that users may interactively modify the <b>clipping</b> <b>plane</b> and edit the structures to highlight, in order to easily create the desired result. Our approach works with segmented volume models and non-segmented ones. In the last case, a local segmentation is performed on-the-fly. We will demonstrate the efficiency and utility of our method. Peer ReviewedPostprint (published version...|$|R
5000|$|Approaching {{the distant}} plane to {{restrict}} scene such as far <b>clip</b> <b>plane,</b> thus increasing {{the accuracy of}} the depth buffer, or reducing the distance at which objects are visible in a scene. Increasing the number of bits allocated to the depth buffer, which is possible at the expense of memory for the stencil buffer. Staying away polygons, which could be inconsistent with the requirements elaborated scene.|$|R
50|$|Volumetric {{lighting}} requires two {{components: a}} light space shadow map, and a depth buffer. Starting at the near <b>clip</b> <b>plane</b> of the camera, {{the whole scene}} is traced and sampling values are accumulated into the input buffer. For each sample, it is determined if the sample is lit by the source of light being processed using the shadow map as a comparison. Only lit samples will affect final pixel color.|$|R
50|$|Conversely, {{sound quality}} for SNES games is more {{accurate}} as the SPC700 {{in the original}} SNES used a sample-driven mechanism similar to a type of instruction-driven MOD file. Reproducing the sound from these systems simply requires playing <b>back</b> <b>clips</b> of sounds with mathematically defined effects and loop points.|$|R
40|$|Abstract—We {{describe}} {{a series of}} experiments that compare 2 D displays, 3 D displays, and combined 2 D/ 3 D displays (orientation icon, ExoVis, and <b>clip</b> <b>planes)</b> for relative position estimation, orientation, and volume of interest tasks. Our results indicate that 3 D displays can be very effective for approximate navigation and relative positioning when appropriate cues, such as shadows, are present. However, 3 D displays are not effective for precise navigation and positioning except possibly in specific circumstances, for instance when good viewing angles or measurement tools are available. For precise tasks in other situations, orientation icon and ExoVis displays were better than strict 2 D or 3 D displays (displays consisting exclusively of 2 D or 3 D views). The combined displays had as good or better performance, inspired higher confidence, and allowed natural, integrated navigation. <b>Clip</b> <b>plane</b> displays were not effective for 3 D orientation because users could not easily view more than one 2 D slice {{at a time and}} had to frequently change the visibility of individual slices. Major factors contributing to display preference and usability were task characteristics, orientation cues, occlusion, and spatial proximity of views that were used together...|$|R
40|$|This work {{extends the}} metaphor of a see-through {{interface}} embodied in Magic LensesTM to 3 D environments. We present two new see-through visualization techniques: jlat lenses in 3 D and volumetric lenses. We discuss implementation concerns for platforms that have programmer accessible hardware <b>clipping</b> <b>planes</b> and show several examples of each visualization technique. We also examine composition of multiple lenses in 3 D environments, which strengthens the flat lens metaphor, but may have no meaningful semantics {{in the case of}} volumetric lenses...|$|R
50|$|The use of <b>clipping</b> <b>planes</b> {{can result}} in a detraction from the realism of a scene, as the viewer may notice that {{everything}} at the threshold is not rendered correctly or seems to (dis)appear spontaneously. The addition of fog—a variably transparent region of color or texture just before the clipping plane—can help soften the transition between what should be in plain sight and opaque, and what should be beyond notice and fully transparent, and therefore {{does not need to be}} rendered.|$|R
50|$|Final (or <b>back)</b> <b>clipping</b> is {{the most}} common type, in which the {{beginning}} of the prototype is retained. The unclipped original may be either a simple or a composite. Examples are: ad (advertisement), cable (cablegram), doc (doctor), exam (examination), fax (facsimile), gas (gasoline), gym (gymnastics, gymnasium), memo (memorandum), mutt (muttonhead), pub (public house), pop (popular music).|$|R
40|$|We compare 2 D/ 3 D {{combination}} displays to displays with 2 D and 3 D views alone. Combination displays {{we consider}} are: orientation icon (i. e., separate windows), in-place methods (e. g., <b>clip</b> and cutting <b>planes),</b> {{and a new}} method called ExoVis. We specifically analyze performance differences (i. e. time and accuracy) for 3 D orientation and relative position tasks. Empirical results show that 3 D displays are effective for approximate navigation and relative positioning whereas 2 D/ 3 D combination displays (orientation icon and ExoVis) are useful for precise orientation and position tasks. Combination 2 D/ 3 D displays had as good or better performance as 2 D displays. <b>Clip</b> <b>planes</b> were not effective for a 3 D orientation task, but may be useful when only one slice is needed...|$|R
40|$|Twenty-five years ago, Crow {{published}} the shadow volume approach for determining shadowed regions in a scene. A decade ago, Heidmann described a hardware-accelerated stencil buffer-based shadow volume algorithm. Unfortunately hardware-accelerated stenciled shadow volume techniques {{have not been}} widely adopted by 3 D games and applications {{due in large part}} to the lack of robustness of described techniques. This situation persists despite widely available hardware support. Specifically what has been lacking is a technique that robustly handles various "hard" situations created by near or far <b>plane</b> <b>clipping</b> of shadow volumes. We describe a robust, artifact-free technique for hardware-accelerated rendering of stenciled shadow volumes. Assuming existing hardware, we resolve the issues otherwise caused by shadow volume near and far <b>plane</b> <b>clipping</b> through a combination of (1) placing the conventional far <b>clip</b> <b>plane</b> "at infinity", (2) rasterization with infinite shadow volume polygons via homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth clamping, a new rasterization feature provided by NVIDIA's GeForce 3, preserves existing depth precision by not requiring the far plane to be placed at infinity. We also propose two-sided stencil testing to improve the efficiency of rendering stenciled shadow volumes. Comment: 8 pages, 5 figure...|$|R
40|$|We {{describe}} a CSG rendering algorithm that requires no {{evaluation of the}} CSG tree beyond normalization and pruning. It renders directly from the normalized CSG tree and primitives described (to the graphics system) by their facetted boundaries. It behaves correctly {{in the presence of}} user defined, "near" and "far" <b>clipping</b> <b>planes.</b> It has been implemented on standard graphics workstations using Iris GL and OpenGL graphics libraries. Modestly sized models can be evaluated and rendered at interactive (less than a second per frame) speeds. We have combined the algorithm with an existing B-rep based modeller to provide interactive rendering of incremental updates to large models...|$|R
5000|$|One {{method of}} {{speeding}} up the shadow volume geometry calculations is to utilize existing parts of the rendering pipeline {{to do some of}} the calculation. For instance, by using homogeneous coordinates, the w-coordinate may be set to zero to extend a point to infinity. This should be accompanied by a viewing frustum that has a far <b>clipping</b> <b>plane</b> that extends to infinity in order to accommodate those points, accomplished by using a specialized projection matrix. This technique reduces the accuracy of the depth buffer slightly, but the difference is usually negligible. See 2002 paper [...] "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated Rendering", C. Everitt and M. Kilgard, for a detailed implementation.|$|R
40|$|We {{present a}} system for interactively {{producing}} exploded views of 3 D architectural environments such as multi-story buildings. These exploded views allow viewers to simultaneously see {{the internal and external}} structures of such environments. To create an exploded view we analyze the geometry of the environment to locate individual stories. We then use <b>clipping</b> <b>planes</b> and multipass rendering to separately render each story of the environment in exploded form. Our system operates at the graphics driver level and therefore can be applied to existing OpenGL applications, such as first-person multiplayer video games, without modification. The resulting visualization allows users to understand the global structure of architectural environments and to observe the actions of dynamic characters and objects interacting within such environments...|$|R
40|$|We {{propose a}} simple {{modification}} {{to the classical}} polygon rasterization pipeline that enables exact, efficient raycasting of bounded implicit surfaces {{without the use of}} a global spatial data structure or bounding hierarchy. Our algorithm requires two descriptions for each object: a (possibly non-convex) polyhedral bounding volume, and an implicit equation (including, optionally, a number of <b>clipping</b> <b>planes).</b> Unlike conventional raycasters, the modified pipeline is unidirectional and operates in immediate mode, making hardware implementation feasible. We discuss an extension to the OpenGL state machine that enables immediate-mode raycasting while making no modification to OpenGL's architecture for high-performance polygon rendering. A software simulation of our algorithm generates scenes of visual fidelity equal to those produced by a conventional raycaster, and superior to those produced by a polygon rasterizer, significantly faster than either existing method alone...|$|R
40|$|One of {{the main}} {{obstacles}} in integrating 3 D volume visualization in the clinical workflow is the time-consuming process of adjusting parameters such as viewpoint, transfer functions, and <b>clipping</b> <b>planes</b> required to generate a diagnostically relevant image. Current applications therefore make scarce use of volume rendering and instead primarily employ 2 D views generated through standard techniques such as multi-planar reconstruction (MPR). However, in many cases 3 D renditions can supply additional useful information. This paper discusses ongoing work which aims to improve the integration of 3 D visualization into the diagnostic workflow by automatically generating meaningful renditions based on minimal user interaction. A method for automatically generating 3 D views for structures in 2 D slices based on a single picking interaction is presented. Index Terms — Volume visualization, viewpoint selection 1...|$|R
40|$|Volume Rendering {{applications}} require sophisticated {{user interaction}} for the definition and refinement of transfer functions. Traditional 2 D desktop user interface elements {{have been developed}} to solve this task, but such concepts do not map well to the interaction devices available in Virtual Reality environments. In this paper, we propose an intuitive user interface for Volume Rendering specifically designed for Virtual Reality environments. The proposed interface allows transfer function design and refinement based on intuitive two-handed operation of Wand-like controllers. Additional interaction modes such as navigation and <b>clip</b> <b>plane</b> manipulation are supported as well. The system is implemented using the Sony PlayStation Move controller system. This choice is based on controller device capabilities as well as application and environment constraints. Initial results document the potential of our approach...|$|R
50|$|He died May 14, 2016, {{when his}} {{single-engine}} <b>plane</b> <b>clipped</b> a cell telephone tower wire and crashed near West, Texas. He was 75.|$|R
40|$|We have {{exploited}} clipping {{in combination}} with multiresolution rendering to visualize large-scale threedimensional (3 D) scientific datasets. Our interactive clipping approach involves the dynamic manipulation of a <b>clip</b> <b>plane</b> to expose any cross-section of a given volume data and the subsequent adjustment of the clipped surface to the best view position. The data are rendered as stacks of 2 D textures at high-resolution (HR, the original resolution of input data/image) and low-resolution (LR, sampling at reduced-resolution). While clipping operations take place in LR to achieve an interactive frame rate, the best-view position also supports HR. Our approach thus enables a real-time exploration of any interior region of 3 D volumetric data at a desired resolution. Its successful demonstration {{has been carried out}} for two scientific datasets, which are the simulated electronic density distributions in a crystal and confocal microscopy images of tissues in a plan stem...|$|R
40|$|Volume {{rendering}} of 3 D anatomical and medical data would be valuable in medical diagnosis and surgical planning. In this paper, we investigated the visualization of the segmented heart {{obtained by the}} cross-sectional data from the Visible Human Project, and proposed an accelerated rendering method {{to speed up the}} original ray-casting rendering method. To provide a satisfactory visualization quality on the shape and boundary of cardiac tissues, we designed the transfer function and assigned an appreciate opacity and color value for each kind of tissue. In interactive visualization, we adopt the ray casting algorithm of volume rendering and modify it to accelerate the rendering speed. We also provide the fundamental rotation and zooming operations in the visualization platform, and further implement an interface to display three regular views of the volume data. Furthermore, a <b>clipping</b> <b>plane</b> tool is provided to crop the heart at arbitrary point of view. 1...|$|R
