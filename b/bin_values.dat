21|90|Public
50|$|Figure 2. Histogram of ResProx {{equivalent}} resolution for NMR {{models and}} experimental resolution for X-ray structures. 500 NMR ensembles and 500 X-ray structures {{were randomly selected}} from the PDB. Proteins were grouped in 0.25Å resolution bins. Resolution values on the X-axis indicate the upper limit of each resolution <b>bin.</b> <b>Values</b> for NMR structures and X-ray structures represent the number of structures in each resolution bin.|$|E
50|$|The SIFT-Rank {{descriptor}} {{was shown}} to improve {{the performance of the}} standard SIFT descriptor for affine feature matching. A SIFT-Rank descriptor is generated from a standard SIFT descriptor, by setting each histogram bin to its rank in a sorted array of bins. The Euclidean distance between SIFT-Rank descriptors is invariant to arbitrary monotonic changes in histogram <b>bin</b> <b>values,</b> and is related to Spearman's rank correlation coefficient.|$|E
3000|$|... 2 [*]=[*]. 29, t[98][*]=[*] 6.32, p[*]<[*]. 0001, d[*]=[*] 1.28). Finally, {{there was}} a modest {{interaction}} between the <b>bin</b> <b>values</b> of the more contemporary movies (1990 – 2015) and those of these midrange movies (1960 – 1985; F[1, 11, 996][*]=[*] 5.32, p[*]=[*]. 02). Thus, the patterns of the middle and right panels of Fig.  2 are a bit different.|$|E
50|$|Each tested die is {{assigned}} a <b>bin</b> <b>value,</b> {{depending on the}} result of the test. For example, a pass die {{is assigned}} a <b>bin</b> <b>value</b> of 1 for a good bin, bin 10 for an open circuit, and bin 11 for a short circuit. In the very early days of wafer test, the dies were put in different bins or buckets, depending on the test results.|$|R
5000|$|Probability update: The {{selected}} context {{model is}} updated {{based on the}} actual coded value (e.g. if the <b>bin</b> <b>value</b> was [...] "1", the frequency count of [...] "1"s is increased).|$|R
50|$|For {{each group}} of pixels taken from the same {{position}} from all input single-channel images, the function puts the histogram <b>bin</b> <b>value</b> to the destination image, where the coordinates of the bin {{are determined by the}} values of pixels in this input group. In terms of statistics, the value of each output image pixel characterizes the probability that the corresponding input pixel group belongs to the object whose histogram is used.|$|R
30|$|We fitted {{the median}} {{reception}} power curve, {{as it is}} more stable at lower reception rates. The average reception power curve suffers (<b>bin</b> <b>values</b> are too high) from incomplete data {{as soon as the}} reception rate sinks below 1.0. The median is technically accurate as long as reception rate is greater than 0.5. However, due to small-scale fading leading to variations and potential measurement inaccuracies around the reception threshold of the radio, median values also turn out to be slightly too high at reception rates close to 0.5. This is visible in plots. To prevent a negative influence on the fit, an exclusion criterion of reception-rate > 0.65 was selected.|$|E
40|$|Face {{detection}} is {{the first}} step in many visual processing systems like face recognition, emotion recognition and lip reading. In this paper, we propose a novel feature called Haar Local Binary Pattern (HLBP) feature for fast and reliable face detection, particularly in adverse imaging conditions. This binary feature compares <b>bin</b> <b>values</b> of Local Binary Pattern histograms calculated over two adjacent image subregions. These subregions are similar to those in the Haar masks, hence the name of the feature. They capture the region-specific variations of local texture patterns and are boosted using AdaBoost in a framework similar to that proposed by Viola and Jones. Preliminary results obtained on several standard databases show that it competes well with other face detection systems, especially in adverse illumination conditions. ...|$|E
30|$|The major {{objective}} {{of this paper is}} to propose a three-point interpolation-based estimator, which avoids the effect of non-monotonic mapping and further reduces the complexity of the MDCT domain frequency estimator to render a simple method for various applications. The contributions are summarized as follows: (i) derive an analytical expression of the MDCT of a single-tone sinusoid based on the sine window’s centered DFT (CDFT); (ii) propose an MDCT domain three-point interpolation-based low-complexity approach for the signal frequency estimation problem. The proposed algorithm estimates the frequency from three MDCT <b>bin</b> <b>values</b> with only simple calculations and is significantly less complex than the existing methods. The method is effective for the sine window case and exhibits an estimation error lower than 1  Hz when the signal-to-noise ratio (SNR) is above 20  dB.|$|E
30|$|Minimum allele {{frequency}} (MAF) for the 1713 SNPs panel in japonica and indica groups of our collection {{and for those}} from 3 KRGP were calculated using Tassel (Tassel 5.2. 26, Bradbury et al. 2007). In case of 3 KRGP varieties, MAF was set after removing third state of SNPs and imputing by fillin. MAF <b>bins</b> <b>values</b> for each group of varieties were calculated using hexbin R package. The number of bins was set to 50. Since we used MAF, the values of {{allele frequency}} did not exceed 0.5.|$|R
30|$|Ks histograms were {{generated}} by <b>binning</b> Ks <b>values</b> into 0.01 Ks unit bins, separating sites based on community metabolic type that migrations occurred between (between chemotrophic, between phototrophic, between phototrophic and chemotrophic). The 0 – 0.01 Ks histogram (Fig.  3 —inset) was generated by <b>binning</b> all Ks <b>values</b> < 0.01 into 0.002 Ks unit bins, also separated by {{the type of}} community the migrations occurred between. Ks counts were normalized by dividing observed counts {{by the total number of}} > 1, 000  bp cDNA sequences with each corresponding metagenome. Chemotrophic and phototrophic communities were normalized separately so that community types could be compared based on the fraction of cDNA migrations at a given Ks <b>value</b> (<b>bin).</b>|$|R
30|$|ALBPS {{descriptor}} {{is obtained}} by concatenating the P[*]+[*] 2 <b>bin</b> histogram <b>values</b> of the uniform LBP approach {{together with the}} P-dimensional standard deviation vector, yielding a descriptor of 2 P[*]+[*] 2 features with P[*]as {{the size of the}} neighbourhood.|$|R
40|$|We {{consider}} {{the problem of}} consistently estimating an unknown probability density function on a bounded interval from a sample of n independent and identically distributed univariate random variables. Adopting a Bayesian nonparametric approach, as first approximation, a hierarchical prior whose weak support comprises all absolutely continuous distribution functions, is considered that selects piecewise constant densities. The prior measure is constructed by putting a prior {{on the number of}} equal length bins and a Dirichtlet distribution on the <b>bin</b> <b>values.</b> Covergence rate for the Hellinger loss of the Bayes' estimator is deduced from the posterior rate, which is studied for various densities generating the data. This rate is comparable up to a logarithmic factor to that of the frequentist histogram estimator. Smoothing the Bayesian histogram, we get a continuous, piecewise linear competitor which possesses a faster rate of convergence...|$|E
40|$|PatentAn antenna {{receives}} an analog waveform and an analog signal {{indicative of the}} amplitude and frequency of the analog waveform. The analog signal is processed in a plurality of parallel digital processing channels each arranged to digitize the analog signal at a corresponding sampling frequency fsi to produce a plurality of digital signals. A discrete Fourier transform is applied {{to each of the}} digital signals output to produce a corresponding plurality of unique Fourier spectra of length mi=(fs;) (TL;) where TLi is the integration time for the discrete Fourier transform for each digital processing channel. The lengths of the Fourier spectra (m;) are selected to be pairwise relatively prime. The discrete Fourier transform encodes the signals in same form as the symmetrical number system (SNS). A SNS-todecimal algorithm is then applied to the detected <b>bin</b> <b>values</b> (a;) to determine the numerical value of the frequency f of the analog waveform. The receiver resolves all undersampiing ambiguities exactly, thereby relaxing the speed requirements on the digital section of the receiver...|$|E
40|$|AbstractIn {{content-based}} image retrieval (CBIR) application, a {{large amount}} of floating-point data is processed. Among various low-level features, color is an important feature and represented in the form of histogram. It is essential that features required to be coded {{in such a way that}} the storage space requirement is low and processing speed is high. In this paper, we propose an encoding approach using Golomb-Rice coding, which effectively codes the floating point <b>bin</b> <b>values</b> of the color histogram. The floating point values are converted into integer values using preprocessing steps. The encoded histogram is finally represented in the form of sparse matrix and XOR based bitwise comparison is used as similarity measure to calculate the distance between the encoded and query histogram in the feature space. Based on the number of 1 's, the retrieved list is ranked and the relevant images are presented. This approach is tested in CBIR application and the precision of retrieval is encouraging compared to the original color histogram and the average bit length is very low besides having fast retrieval time...|$|E
3000|$|From the {{examination}} of the clutter spectra, it is confirmed that the Doppler <b>bins</b> giving higher <b>values</b> of P [...]...|$|R
30|$|To draw Figs.  1 and 2, Swarm A {{residuals}} {{from each}} subset are grouped {{according to their}} SM latitude and longitude into 2 ° ×  2 ° <b>bins,</b> mapped <b>values</b> consisting of the averages {{of the values of}} dY and dH residuals, respectively, falling in each bin.|$|R
30|$|Step 1 For {{the echo}} signal in each range <b>bin,</b> {{calculate}} STFT <b>values</b> of mixed signals. By STFT, the mixed signals are {{demonstrated in the}} T-F domain.|$|R
40|$|We {{introduce}} {{and develop}} a new network-based and binless methodology to perform frequency analyses and produce histograms. In contrast with traditional frequency analysis techniques that use fixed intervals to <b>bin</b> <b>values,</b> we place a range ±ζ around each individual value in a data set and {{count the number of}} values within that range, which allows us to compare every single value of a data set with one another. In essence, the methodology is identical to the construction of a network, where two values are connected if they lie within a given a range (±ζ). The value with the highest degree (i. e., most connections) is therefore assimilated to the mode of the distribution. To select an optimal range, we look at the stability of the proportion of nodes in the largest cluster. The methodology is validated by sampling 12 typical distributions, and it is applied to a number of real-world data sets with both spatial and temporal components. The methodology can be applied to any data set and provides a robust means to uncover meaningful patterns and trends. A free python script and a tutorial are also made available to facilitate the application of the method...|$|E
40|$|We {{enhance the}} Syer & Tremaine made-to-measure (M 2 M) {{particle}} method of stellar dynamical modelling to model simultaneously both kinematic data and absorption line strength data thus creating a `chemo-M 2 M' modelling scheme. We apply the enhanced method to four galaxies (NGC 1248, NGC 3838, NGC 4452, NGC 4551) observed using the SAURON integral-field spectrograph {{as part of}} the ATLAS 3 D programme. We are able to reproduce successfully the 2 D line strength data achieving mean chi^ 2 per <b>bin</b> <b>values</b> of 1 with > 95 % of particles having converged weights. Because M 2 M uses a 3 D particle system, we are also able to examine the underlying 3 D line strength distributions. The extent to which these distributions are plausible representations of real galaxies requires further consideration. Overall we consider the modelling exercise to be a promising first step in developing a `chemo-M 2 M' modelling system and in understanding some of the issues to be addressed. Whilst the made-to-measure techniques developed have been applied to absorption line strength data, they are in fact general and may be of value in modelling other aspects of galaxies. Comment: 18 page...|$|E
30|$|Comparison {{results show}} that the {{adsorption}} energy of the B-e, A-j and B-j models is positive due to dissociation of the NH 2 NO 2 molecule {{on the surface of the}} nanosheet, whereas the E ads values of the other models are negative and these adsorbed models are exothermic. Also, the B-g model with E ads = − 10.48  kcal/mol is the most stable adsorption model (see Table  1). The obtained results demonstrate that the E ads of the NH 2 NO 2 /nanosheet system depends on C-replaced positions and NH 2 NO 2 molecule orientation. The C-replacing atoms increase the adsorption of the NH 2 NO 2 molecule on the surface of the nanosheet. These results confirm that C-replaced BN nanosheet is a good compound for adsorbing NH 2 NO 2 molecule. In all studied models (the A-a to B-j models), the basis set superposition error (BSSE) is in the range 0.0005 – 0.001  kcal/mol. In the A-a to B-j models, the binding energy (E <b>bin)</b> <b>values</b> are negative and the NH 2 NO 2 molecule is adsorbed on the surface of the BN nanosheet. The maximum binding energy is seen in the B-e, A-j and B-j models with − 173.22, − 214.22 and − 257.87  kcal/mol, respectively.|$|E
40|$|With {{the advent}} of new Multi-Electrode Arrays {{techniques}} (MEA), the simultaneous recording of the activity up to hundreds of neurons over a dense configuration supplies today a critical database to unravel the role of specific neural assemblies. Thus, the analysis of spike trains obtained from in vivo or in vitro experimental data requires suitable statistical models and computational tools. The EnaS software [7], developed by our team, offers new computational methods of spike train statistics, based on Gibbs distributions (in its more general sense, including, but not limited, to the Maximal Entropy- MaxEnt) and taking into account time constraints in neural networks (such as memory effects). It also offers several statistical model choices, some of these models already used in the community (such GLM [6] and the conditional Figure 1 The GUI of EnaS. This page allows displaying a spike-train, showing the firing rates and configuring the <b>binning</b> <b>value</b> (sampling rate) of the data. It also allows selecting a subset of neurons and sorting the neurons {{with respect to their}} activity...|$|R
40|$|One of {{the major}} goals of cosmological {{observations}} is to test theories of structure formation. The most straightforward way to carry out such tests is to compute the likelihood function L, the probability of getting the data given the theory. We write down this function for a general galaxy survey. The full likelihood function is very complex, depending {{on all of the}} $n$-point functions of the theory under consideration. Even in the simplest case, where only the two point function is non-vanishing (Gaussian perturbations), L cannot be calculated exactly, primarily because of the Poisson nature of the galaxy distribution. Here we expand L about the (trivial) zero correlation limit. As a first application, we take the <b>binned</b> <b>values</b> of the two point function as free parameters and show that L peaks at $(DD - DR + RR) /DD$. Using Monte Carlo techniques, we compare this estimator with the traditional $DD/DR$ and Landy & Szalay estimators. More generally, the success of this expansion should pave the way for further applications of the likelihood function. Comment: 23 pages, Late...|$|R
3000|$|... 2 are {{predefined}} thresholds. An {{image block}} {{is called a}} nontext block {{if there are no}} text edges in it. To compute the luminance variability score, a test image is partitioned into 8 × 8 blocks and the mean of each nontext block is calculated. We build a 256 -bin histogram of nontext block means over the test image. Luminance variability score is then defined as the number of <b>bins</b> whose <b>values</b> are greater than a predefined threshold η.|$|R
40|$|Large {{variations}} in image background may cause partial matching and normalization problems for histogram-based representations, i. e., the histograms {{of the same}} category may have bins which are significantly different, and normalization may produce large changes in the differences between corresponding bins. In this paper, we {{deal with this problem}} by using the ratios between <b>bin</b> <b>values</b> of histograms, rather than bin values' differences which are used in the traditional histogram distances. We propose a bin ratio-based histogram distance (BRD), which is an intra-cross-bin distance, in contrast with previous bin-to-bin distances and cross-bin distances. The BRD is robust to partial matching and histogram normalization, and captures correlations between bins with only a linear computational complexity. We combine the BRD with the ℓ 1 histogram distance and the χ 2 histogram distance to generate the ℓ 1 BRD and the χ 2 BRD, respectively. These combinations exploit and benefit from the robustness of the BRD under partial matching and the robustness of the ℓ 1 and χ 2 distances to small noise. We propose a method for assessing the robustness of histogram distances to partial matching. The BRDs and logistic regression-based histogram fusion are applied to image classification. The experimental results on synthetic data sets show the robustness of the BRDs to partial matching, and the experiments on seven benchmark data sets demonstrate promising results of the BRDs for image classification...|$|E
40|$|A {{sample of}} metal-poor subgiants has been {{observed}} with the UVES spectrograph at the Very Large Telescope and abundances of Li and Be have been determined. Typical signal-to-noise per spectral <b>bin</b> <b>values</b> for the co-added spectra are {{of the order of}} 500 for the Li I line (670. 78 nm) and 100 for the Be II doublet lines (313. 04 nm). The spectral analysis of the observations was carried out using the Uppsala suite of codes and MARCS (1 D-LTE) model atmospheres with stellar parameters from photometry, parallaxes, isochrones and Fe II lines. Abundance estimates of the light elements were corrected for departures from local thermodynamic equilibrium in the line formation. Effective temperatures and Li abundances seem to be correlated and Be abundances correlate with [O/H]. Standard models predict Li and Be abundances approximately one order of magnitude lower than main-sequence values which is in general agreement with the observations. On average, our observed depletions seem to be 0. 1 dex smaller and between 0. 2 and 0. 4 dex larger (depending on which reference is taken) than those predicted for Li and Be, respectively. This is not surprising since the initial Li abundance, as derived from main-sequence stars on the Spite plateau, may be systematically in error by 0. 1 dex or more, and uncertainties in the spectrum normalisation and continuum drawing may affect our Be abundances systematically. Comment: Accepted for publication in A&...|$|E
40|$|Abstract. This paper {{proposes that}} the study of Sturm {{sequences}} is invaluable in the numerical computation and theoretical derivation of eigenvalue distributions of random matrix ensembles. We first explore the use of Sturm sequences to efficiently compute histograms of eigenvalues for symmetric tridiagonal matrices and apply these ideas to random matrix ensembles such as the β-Hermite ensemble. Using our techniques, we reduce the time to compute a histogram of the eigenvalues of such a matrix from O(n 2 + m) to O(mn) time where n is the dimension of the matrix and m is the number of bins (with arbitrary bin centers and widths) desired in the histogram (m is usually much smaller than n). Second, we derive analytic formulas in terms of iterated multivariate integrals for the eigenvalue distribution and the largest eigenvalue distribution for arbitrary symmetric tridiagonal random matrix models. As an example of the utility of this approach, we give a derivation of both distributions for the β-Hermite random matrix ensemble (for general β). Third, we explore the relationship between the Sturm sequence of a random matrix and its shooting eigenvectors. We show using Sturm sequences that, assuming the eigenvector contains no zeros, the number of sign changes in a shooting eigenvector of parameter λ is equal to the number of eigenvalues greater than λ. Finally, we use the techniques presented in the first section to experimentally demonstrate a O(log n) growth relationship between the variance of histogram <b>bin</b> <b>values</b> and the order of the β-Hermite matrix ensemble. This paper is dedicated to the fond memory of James T. Albrecht 1...|$|E
40|$|In this paper, {{we present}} a new {{multimodal}} image registration method based on the a priori knowledge of the class label mappings between two segmented input images. A joint class histogram between the image pairs is estimated by assigning each <b>bin</b> <b>value</b> equal to {{the total number of}} occurrences of the corresponding class label pairs. The discrepancy between the observed and expected joint class histograms should be minimized when the transformation is optimal. Kullback-Leibler distance (KLD) is used to measure the difference between these two histograms. Based on the probing experimental results on a synthetic dataset as well as a pair of precisely registered 3 D clinical volumes, we showed that, with the knowledge of the expected joint class histogram, our method obtained longer capture range and fewer local optimal points as compared with the conventional Mutual Information (MI) based registration method. We also applied the proposed method to 2 D- 3 D rigid registration problems between DSA and MRA volumes. Based on manually selected markers, we found that the accuracies of our method and the MI-based method are comparable. Moreover, our method is more computationally efficient than the MI-based method...|$|R
40|$|The Lyman-α {{forest is}} a highly {{non-linear}} field {{with a lot of}} information available in the data beyond the power spectrum. The flux probability distribution function (PDF) {{has been used as a}} successful probe of small-scale physics. In this paper we argue that measuring coefficients of the Legendre polyonomial expansion of the PDF offers several advantages over measuring the <b>binned</b> <b>values</b> as is commonly done. In particular, n-th coefficient can be expressed as a linear combination of the first n moments, allowing these coefficients to be measured in the presence of noise and allowing a clear route for marginalisation over mean flux. Moreover, in the presence of noise, our numerical work shows that a finite number of coefficients are well measured with a very sharp transition into noise dominance. This compresses the available information into a small number of well-measured quantities. We find that the amount of recoverable information is a very non-linear function of spectral noise that strongly favors fewer quasars measured at better signal to noise. Comment: 12 pages, 6 figures; v 2 : version accepted by JCAP; numerical demonstration part completely rewritten with better insigh...|$|R
50|$|For high-cardinality columns, it {{is useful}} to <b>bin</b> the <b>values,</b> where each <b>bin</b> covers {{multiple}} <b>values</b> and build the bitmaps to represent the <b>values</b> in each <b>bin.</b> This approach reduces the number of bitmaps used regardless of encoding method. However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.|$|R
40|$|This {{research}} {{proposes a}} method to detect and classify the smoke particles of common household fires by analysing the image histogram features of smoke particles generated by Rayleigh scattered light. This research was motivated {{by the failure of}} commercially available photoelectric smoke detectors to detect smoke particles less than 100 nm in diameter, such as those in polyurethane (in furniture) fires, and the occurrence of false positives such as those caused by steam. Seven different types of particles (pinewood smoke, polyurethane smoke, steam, kerosene smoke, cotton wool smoke, cooking oil smoke and a test Smoke) were selected and exposed to a continuous spectrum of light in a closed particle chamber. A significant improvement over the common photoelectric smoke detectors was demonstrated by successfully detecting and classifying all test particles using colour histograms. As Rayleigh theory suggested, comparing the intensities of scattered light of different wavelengths is the best method to classify different sized particles. Existing histogram comparison methods based on histogram <b>bin</b> <b>values</b> failed to evaluate a relationship between the scattered intensities of individual red, green and blue laser beams with different sized particles due to the uneven particles movements inside the chamber. The current study proposes a new method to classify these nano-scale particles using the particle density independent intensity histograms feature; Maximum Value Index. When a Rayleigh scatter (particles that have the diameter which is less than one tenth of the incident wavelength) is exposed to a light with different wavelengths, the intensities of scattered light of each wavelength is unique according to the particle size and hence, a single unique maximum value index in the image intensity histogram can be detected. Each captured image in the video frame sequence was divided into its red, green and blue planes (single R, G, B channel arrays) and the particles were isolated using a modified frame difference method. Mean and the standard deviation of the Maximum Value Index of intensity histograms over predefined number of frames (N) were used to differentiate different types of particles. The proposed classification algorithm successfully classified all the monotype particles with 100 % accuracy when N ≥ 100. As expected, the classifier failed to distinguish wood smoke from other monotype particles due to the rapid variation of the maximum value index of the intensity histograms of the consecutive images of the image sequence since wood smoke is itself a complex composition of many monotype particles such as water vapour and resin smoke. The results suggest that the proposed algorithm may enable a smoke detector to be safer by detecting a wider range of fires and reduce false alarms such as those caused by steam...|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. There {{has been a significant}} number of security concerns in recent times; as a result, security cameras have been installed to monitor activities and to prevent crimes in most public places. These analysis are done either through video analytic or forensic analysis operations on human observations. To this end, within the research context of this thesis, a proactive machine vision based military recognition system has been developed to help monitor activities in the military environment. The proposed object detection, recognition and re-identification systems have been presented in this thesis. A novel technique for military personnel recognition is presented in this thesis. Initially the detected camouflaged personnel are segmented using a grabcut segmentation algorithm. Since in general a camouflaged personnel's uniform appears to be similar both at the top and the bottom of the body, an image patch is initially extracted from the segmented foreground image and used as the region of interest. Subsequently the colour and texture features are extracted from each patch and used for classification. A second approach for personnel recognition is proposed through the recognition of the badge on the cap of a military person. A feature matching metric based on the extracted Speed Up Robust Features (SURF) from the badge on a personnel's cap enabled the recognition of the personnel's arm of service. A state-of-the-art technique for recognising vehicle types irrespective of their view angle is also presented in this thesis. Vehicles are initially detected and segmented using a Gaussian Mixture Model (GMM) based foreground/background segmentation algorithm. A Canny Edge Detection (CED) stage, followed by morphological operations are used as pre-processing stage to help enhance foreground vehicular object detection and segmentation. Subsequently, Region, Histogram Oriented Gradient (HOG) and Local Binary Pattern (LBP) features are extracted from the refined foreground vehicle object and used as features for vehicle type recognition. Two different datasets with variant views of front/rear and angle are used and combined for testing the proposed technique. For night-time video analytics and forensics, the thesis presents a novel approach to pedestrian detection and vehicle type recognition. A novel feature acquisition technique named, CENTROG, is proposed for pedestrian detection and vehicle type recognition in this thesis. Thermal images containing pedestrians and vehicular objects are used to analyse the performance of the proposed algorithms. The video is initially segmented using a GMM based foreground object segmentation algorithm. A CED based pre-processing step is used to enhance segmentation accuracy prior using Census Transforms for initial feature extraction. HOG features are then extracted from the Census transformed images and used for detection and recognition respectively of human and vehicular objects in thermal images. Finally, a novel technique for people re-identification is proposed in this thesis based on using low-level colour features and mid-level attributes. The low-level colour histogram <b>bin</b> <b>values</b> were normalised to 0 and 1. A publicly available dataset (VIPeR) and a self constructed dataset have been used in the experiments conducted with 7 clothing attributes and low-level colour histogram features. These 7 attributes are detected using features extracted from 5 different regions of a detected human object using an SVM classifier. The low-level colour features were extracted from the regions of a detected human object. These 5 regions are obtained by human object segmentation and subsequent body part sub-division. People are re-identified by computing the Euclidean distance between a probe and the gallery image sets. The experiments conducted using SVM classifier and Euclidean distance has proven that the proposed techniques attained all of the aforementioned goals. The colour and texture features proposed for camouflage military personnel recognition surpasses the state-of-the-art methods. Similarly, experiments prove that combining features performed best when recognising vehicles in different views subsequent to initial training based on multi-views. In the same vein, the proposed CENTROG technique performed better than the state-of-the-art CENTRIST technique for both pedestrian detection and vehicle type recognition at night-time using thermal images. Finally, we show that the proposed 7 mid-level attributes and the low-level features results in improved performance accuracy for people re-identification...|$|E
40|$|Sherpa 4. 9. 0 This version fixes many bugs in the Python 3 support. Moreover, it {{includes}} a significant refactoring of the Fit and Stat classes {{that made it possible}} to fix several bugs related to the recent wstat implementation while making these classes more maintainable and extensible. Note that this version deprecates the use of load_table_model for XSPEC models. Sherpa/XSPEC users should use the new load_xstable_model function instead. Details Infrastructure and minor non-functional changes have been omitted. 242 Avoid use of inspect. getargspec in Python 3 Finish off the replacement of inspect. getargspec by inspect. signature. 263 List_data_ids() fails on py 3 with mixed id types (Fix # 262). Sherpa was sorting the list of dataset IDs in a non-python 3 compliant fashion, which resulted in issues when using strings and integers together as dataset IDs. This has now been fixed. 267 add wstat tests Add several regression tests for wstat. 282 Parallel_map not working on py 3 with numcores= 1 (Fix # 277). The utils function parallel_map failed on Python 3 when called with numcores= 1, i. e. on systems with only one processor/core. This has been fixed. 283 Sample flux and numpy deprecations (Fix # 273 and # 276). The sample_flux function was not working under Python 3 if the scales argument was provided. This has been fixed. Also, a DeprecationWarning was issued by numpy because during the sample_flux execution values were extracted from an array with non-integer indices. This has also been fixed. 284 String representation of data classes under py 3 (Fix # 275). Data classes DataPHA, DataARF, DataRMF, DataIMG, and DataIMGInt in sherpa. astro. data would throw an exception if users tried to print them as strings, under Python 3. This has been fixed. 287 Rewrite sherpa. stats. Stat. calc_stat and simplify sherpa. fit. Fit (fix # 227 # 248 # 289 # 292). In order to fix several issues related to the WStat support, and {{in order to make the}} code more maintainable, the sherpa. stats. Stat. calc_stat and sherpa. fit. Fit classes have gone through a round of refactoring. This fixes the following issues: # 227 Issues using wstat when grouping/filtering data; # 248 backscal column not treated properly for WStat; # 289 calc_stat does not error out if background subtracted data is used with Likelihood statistics; # 292 stat info does not include reduced stat/qval for wstat. 295 Fix display of pileup model in Python 3. 5 (Fix # 294). Fix display of instances of sherpa. astro. models. JDPileup so that, in Python 3. 5, they can be displayed after the model has been evaluated. 304 replace file -> open (Fix # 297). The save and restore functions used to use the file function which is not compatible with Python 3. This has now been fixed. 305 Fix python 3 issues with some session commands (Fix # 303). The set_xlog, set_ylog, and show_bkg_model functions were not compatible with Python 3. This has now been fixed (Issue # 303). 307 Move XSPEC table support to load_xstable_model and deprecate its support in load_table_model (Fix # 270). Add the load_xstable_model routine to the sherpa. astro. ui module, which supports loading XSPEC additive or multiplicative (atable and mtable) models. The support for these models is still available via load_table_model in this release, but it is deprecated. The read_xstable_model routine has been added to the sherpa. astro. xspec module. 312 Fix over-zealous code clean up in PR # 287 affecting sigmarej. Fits using the sigmarej iterated-fit method were broken if a filter had been applied to the data before the fit and there are any bins that get ignored at larger <b>bin</b> <b>values</b> than the filtered-out data. (This fixes a subtle regression introduced by # 287). 313 Allow sequence=None when using gridsearch and Python 3. 5 (Fix # 309). Allow the gridsearch optimiser to be used with the sequence option set to None for Python 3. 5. Caveats The requirements for Sherpa are to build with Python 2. 7 and 3. 5. There has been limited testing with Python 3. 6, for which we distribute conda binaries. If in doubt, please install Sherpa in 2. 7 or 3. 5 environments only. Support for Python versions 3. 3 and 3. 4 is possible but would require community support. It has been reported during testing that some versions of the matplotlib conda package do not install properly because of a pyqt v 5 dependency. If you encounter this issue, please pin down pyqt to version 4, e. g. conda install matplotlib pyqt= 4. The sherpatest package is not distributed as a conda package anymore. This will probably be true for the foreseeable future. The sherpatest package contains data and functional tests that relies on external datasets, so it allows users and developers to run the entire regression tests suite. If you want to install sherpatest, please use pip and github: $ pip install [URL] If you decide to run the full regression tests suite you should also have matplotlib installed. If matplotlib is not installed a test will run and fail rather than being skipped. This issue will be fixed in the next release...|$|E
3000|$|... (d) is the {{histogram}} of {{the known}} stable depths in the cross window around p, which {{was obtained from the}} first cost aggregation. The depth with the highest histogram <b>bin</b> with the <b>value</b> is selected as the most desirable disparity to fill the white hole.|$|R
50|$|Most of {{this article}} {{represents}} the most common view of measurement uncertainty, which assumes that random variables are proper mathematical models for uncertain quantities and simple probability distributions are sufficient for representing all forms of measurement uncertainties.In some situations, however, a mathematical interval rather than a probability distribution {{might be a better}} model of uncertainty. This may include situations involving periodic measurements, <b>binned</b> data <b>values,</b> censoring, detection limits, orplus-minus ranges of measurements where no particular probability distribution seems justified or where one cannot assume that the errors among individual measurements are completely independent.|$|R
30|$|Naive Bayes {{classifier}} (Tan et al. 2006) is a probabilistic classifier {{based on}} applying Bayes’ theorem. Naive Bayes assumes {{that all the}} attributes which {{will be used for}} classification are independent of each other. We used Naïve Bayes Classification to create four different models. In the first model we estimated the class labels for continuous attributes using probability distribution function (PDF), in the second model we used optimal equal <b>binning</b> width <b>value,</b> for the third model we over-sampled the data using SMOTE and for the fourth model we used both optimal equal width binning and SMOTE.|$|R
