28|5674|Public
2500|$|A blob (<b>binary</b> <b>large</b> <b>object)</b> is {{the content}} of a file. Blobs have no proper file name, time stamps, or other metadata. (A blob's name {{internally}} is a hash of its content.) ...|$|E
5000|$|Abstraction for <b>binary</b> <b>large</b> <b>object</b> (BLOB) and {{character}} large object (CLOB) handling ...|$|E
5000|$|... (or [...] ): <b>binary</b> <b>large</b> <b>object</b> with {{a maximum}} length n K | M | G | T [...]|$|E
50|$|SRB {{can store}} and {{retrieve}} data in archival storage {{systems such as}} the High Performance Storage System and SAM-FS, on disk file systems (Unix, Linux, or Windows), as <b>binary</b> <b>large</b> <b>objects</b> or tabular data in relational database management systems, and on tape libraries.|$|R
50|$|Database {{triggers}} in Drizzle {{are supported}} for DML, DDL, {{and a number}} of additional event-based operations in the server. The PrimeBase BLOB streaming system, which allows Drizzle to stream <b>binary</b> <b>large</b> <b>objects</b> (BLOBs) via HTTP, makes use of this system. All triggers for Drizzle currently must be written in C++.|$|R
50|$|RDM Server {{supports}} the following native data types: signed and unsigned 8-, 16-, 32- and 64-bit integers, UTF-8 and Unicode characters, floating point (32 and 64 bit), BLOBs (<b>binary</b> <b>large</b> <b>objects),</b> BCD (<b>Binary</b> Coded Decimal), date, time and timestamp. In addition it has native support for structs and multi-dimensional arrays {{based on the}} above list of base types.|$|R
5000|$|A blob (<b>binary</b> <b>large</b> <b>object)</b> is {{the content}} of a file. Blobs have no proper file name, time stamps, or other metadata. (A blob's name {{internally}} is a hash of its content.) ...|$|E
50|$|A <b>Binary</b> <b>Large</b> <b>OBject</b> (BLOB) is a {{collection}} of binary data stored as a single entity in a database management system. Blobs are typically images, audio or other multimedia objects, though sometimes binary executable code is stored as a blob. Database support for blobs is not universal.|$|E
50|$|Raster data {{is stored}} in various formats; from a {{standard}} file-based structure of TIFF, JPEG, etc. to <b>binary</b> <b>large</b> <b>object</b> (BLOB) data stored directly in a relational database management system (RDBMS) similar to other vector-based feature classes. Database storage, when properly indexed, typically allows for quicker retrieval of the raster data but can require storage of millions of significantly sized records.|$|E
40|$|Security of {{multimedia}} files attracts {{more and more}} attention and many encryption methods have been proposed in literature. However most cryptographic systems deal with multimedia files as <b>binary</b> <b>large</b> <b>objects,</b> without taking into consideration regions of semantic information. These regions may need better protection or can be the only regions that need protection, depending on the specific application. Towards thi...|$|R
50|$|In 2010, Metalogix {{acquired}} the StoragePoint product from BlueThread Technologies. StoragePoint is a SharePoint storage optimization solution that offloads unstructured SharePoint content data, known as <b>Binary</b> <b>Large</b> <b>Objects</b> (BLOBs) from SharePoint’s underlying SQL database to alternate tiers of storage. The company announced an integrated file share consolidation solution with StoragePoint and Content Matrix in July 2012. StoragePoint 4.0 {{was released in}} November 2012 with new SharePoint backup and restore features.|$|R
50|$|The SDF format design uses {{low-level}} storage {{components of}} SQLite using a flat <b>binary</b> serialization (<b>binary</b> <b>large</b> <b>objects).</b> However, the relational aspects are not present, thus the format cannot be opened with any software {{designed specifically for}} SQLite. The format supports multiple feature classes per file and multiple geometry properties per feature class. Each geometry property is indexed using an R-tree. It is optimized for fast spatial reading of large datasets in scenarios involving a single writer and multiple readers.|$|R
5000|$|Blobs were {{originally}} just big amorphous chunks of data invented by Jim Starkey at DEC, who describes them as [...] "the thing that ate Cincinnati, Cleveland, or whatever" [...] from [...] "the 1958 Steve McQueen movie", referring to The Blob. Later, Terry McKiever, a marketing person for Apollo, {{felt that it}} needed to be an acronym and invented the backronym Basic Large Object. Then Informix invented an alternative backronym, <b>Binary</b> <b>Large</b> <b>Object.</b>|$|E
50|$|Unstructured and semi-structured {{data have}} {{different}} meanings {{depending on their}} context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (<b>binary</b> <b>large</b> <b>object),</b> a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.|$|E
50|$|Binary objects (BO or <b>binary</b> <b>large</b> <b>object</b> = BLOB) are digital {{files from}} photos, graphics, films, sounds or any binary data in {{specific}} formats. There are {{many types of}} file converter {{and some of them}} are jpeg and gif as many people know it for. They have three steps is binary object that they work and they are: upload of binary object, metadata definition or import and data import data.Upload of binary objects can be standard formats. Image holds their data that they have been saved and they are usually separated of how the user uploads the image.Metadata definition or import locates the Binary object related to binary object as they have been organized. If they find a data that they are suitable in they will be ready for next process.Data import data sets title, principal investigator, method, and comment and references is needed in the process. Then the binary object can be added as pdf or txt file.|$|E
50|$|Column {{types of}} Long Text and Long <b>Binary</b> are <b>large</b> <b>binary</b> <b>objects.</b> They {{are stored in}} {{separate}} B+tree from the clustered index keyed by long value id and byte offset. ESE supports append, byte range overwrite, and set size for these columns. Also, ESE has a single instance store feature where multiple records may reference the same <b>large</b> <b>binary</b> <b>object,</b> as though each record had its own copy of the information, i.e. without inter-record locking conflicts. The maximum size of a Long Text or Long Binary column value is 2 GB.|$|R
50|$|A {{workable}} {{implementation of}} a time series database can be deployed in a conventional SQL-based relational database provided that the database software supports both <b>binary</b> <b>large</b> <b>objects</b> (BLOBs) and user-defined functions. SQL statements that operate on one or more time series quantities on the same row of a table or join can easily be written, as the user-defined time series functions operate comfortably inside of a SELECT statement. However, time series functionality such as a SUM function operating {{in the context of}} a GROUP BY clause cannot be easily achieved.|$|R
50|$|The data {{primitives}} {{are called}} 'components' {{and they are}} atomic. Components can be concatenated into short composites called 'Items' which are the unit of storage and retrieval. Higher-level structures that combine these Items include unlimited size records of an unlimited number of columns or attributes, with complex attribute values of unlimited size. Keys may be a composition of components. Attribute values can be ordered sets of composite components, character <b>large</b> <b>objects</b> (CLOB's), <b>binary</b> <b>large</b> <b>objects</b> (BLOB's), or unlimited sparse arrays. Other higher-level structures built of multiple Items include key/value associations like ordered maps, ordered sets, Entity-Attribute-Value nets of quadruples, trees, DAG's, taxonomies, or full-text indexes. Mixtures of these can occur along with custom client-defined structures.|$|R
5000|$|Paul Carpentier and Jan van Riel {{coined the}} term CAS while working at a company called FilePool in the late 1990s. FilePool was {{acquired}} in 2001 and became the underpinnings of the first commercially available CAS system, which was introduced as EMC's Centera platform. [...] The Centera CAS system consists {{of a series of}} networked nodes (1-U servers running Linux), divided between storage nodes and access nodes. The access nodes maintain a synchronized directory of content addresses, and the corresponding storage node where each address can be found. When a new data element, or blob (<b>Binary</b> <b>large</b> <b>object),</b> is added, the device calculates a hash of the content and returns this hash as the blob's content address. [...] As mentioned above, the hash is searched for to verify that identical content is not already present. If the content already exists, the device does not need to perform any additional steps; the content address already points to the proper content. Otherwise, the data is passed off to a storage node and written to the physical media.|$|E
50|$|Smaller DAM {{systems are}} easier to {{categorize}} as to content and usage since they normally operate in a particular operational context. This would hold true for systems attached to audio or video production systems. The key differentiators here are the type of decoders and I/O (input/output) used for the asset ingest, use and outgest. Since metadata describes the essence (and proxy copies), the metadata {{can serve as a}} guide to the playout decoders, transcoders, and channels as well as an input to access control rules. This means that the essence can be treated as a non-described storage object except when being accessed for viewing or editing. There is relevance to this when considering the overall design and use of larger implementations. The closer the asset is to the ingest/edit/playout tool, the greater the technical architecture needs to accommodate delivery requirements such as bandwidth, latency, capacity, access control, availability of resources, etc. The further the asset moves into a general storage architecture (e.g. hierarchical storage management HSM) the more it can be treated as a general blob (<b>binary</b> <b>large</b> <b>object)</b> that is typically held in the filesystem, not the database. The impact of this set of needs means that it is possible and reasonable to design larger systems using smaller, more expensive performance-systems {{at the edge of the}} network where the essence is being used in its intended form and less expensive systems further back for storage and archival. This type of design exemplifies Infrastructure Convergence Architecture, where the line-of-business operations technology and IT technologies depend on one another for functional and performance (non-functional) requirements.|$|E
40|$|The Sculptural User Installation: Social Sculpture as Tree-ed <b>Binary</b> <b>Large</b> <b>Object,</b> uses artist {{authored}} computer software, physical installation {{from building}} materials, automatic manufacture from 3 d computer files, video monitors that display interactive computer software, and a web component that is distributed among video monitors. This is an interactive, multi authored environment. These elements are combined {{as a large}} scale interactive, social, installation...|$|E
50|$|Ideally, these {{repositories}} {{are often}} natively implemented using specialized database algorithms. However, {{it is possible}} to store time series as <b>binary</b> <b>large</b> <b>objects</b> (BLOBs) in a relational database or by using a VLDB approach coupled with a pure star schema. Efficiency is often improved if time is treated as a discrete quantity rather than as a continuous mathematical dimension. Database joins across multiple time series data sets is only practical when the time tag associated with each data entry spans the same set of discrete times for all data sets across which the join is performed.|$|R
40|$|Many {{current and}} {{potential}} applications of database technology, e. g., geographical, medical, spatial, and multimedia applications, require efficient {{support for the}} management of data with new, complex data types. As a result, the major DBMS vendors are stepping beyond the support for uninterpreted <b>binary</b> <b>large</b> <b>objects,</b> termed BLOBs, and are beginning to offer extensibility features that allow external developers to extend the DBMS with, e. g., their own data types and accompanying access methods. Existing solutions include DB 2 extenders, Informix DataBlades, and Oracle cartridges. Extensible systems offer new and exciting opportunities for researchers and third-party developers alike. This pape...|$|R
40|$|This paper {{presents}} four implementations {{for support}} of <b>large</b> <b>objects</b> in POSTGRES. The four implementations offer {{varying levels of}} support for security, transactions, compression, and time travel. All are implemented using the POSTGRES abstract data type paradigm, support userdefined operators and functions, and allow file-oriented access to <b>large</b> <b>objects</b> in the database. The support for user-defined storage managers available in POSTGRES is also detailed. The performance of all four <b>large</b> <b>object</b> implementations on two different storage devices is presented. 1. Introduction There have been numerous implementations supporting <b>large</b> <b>objects</b> in database systems [BILI 92]. Typically, these implementations concentrate on low-level issues such as space management and layout of objects on storage media. Support for higher-level services is less uniform among existing systems. Commercial relational systems normally support BLOBs (<b>binary</b> <b>large</b> <b>objects),</b> and provide the capability to store an [...] ...|$|R
30|$|The last {{question}} of the questionnaire asks the user {{to take a picture}} of the mosquito encountered. By choosing yes, the phone camera opens. Ideally, the pictures taken by citizens could be used later by zoologists as a way to improve species distribution maps although preliminary tests were inconclusive. Pictures are usually large files, which could clutter our database. To reduce the size of images, these are modified locally using a BLOB (<b>Binary</b> <b>Large</b> <b>Object)</b> procedure. This procedure transforms the bitmap image taken with the camera into a bytes object. The BLOB is then sent directly to the server. Image quality is decreased to a quarter of the original picture.|$|E
40|$|In this thesis, we {{investigate}} {{the aspects of}} persistence, of the object-oriented approach and of multimedia data management. We {{show that there is}} currently no complete answer to the problems stated above. But partial solutions brought by relational database technology to the problem of efficient storage, combined with the ease of use of object-oriented techniques, allow us to implement multimedia applications without difficulty. Our work consists of an analysis of the requirements of multimedia data management. Based on this, we build a data model which allows us to take advantage of both the relational technology and the object-oriented paradigm. We then build an architecture for that system, based on the client-server model. Finally, as a validation for our data model and architecture, we present a prototype implementing our specifications, using a relational database management system and a sc BLOB (<b>binary</b> <b>large</b> <b>object)</b> storage manager. (Abstract shortened by UMI. ...|$|E
40|$|Most {{applications}} that access large data objects do so through file systems, but file systems provide an incomplete solution, as they maintain insufficient metadata {{and do not}} provide general purpose query engine. Storing large objects in a database addresses these problems, but, for {{applications that}} need to update object data, databases are inefficient as they do not provide direct access to data. Additionally, databases often relax the integrity and consistency constraints for large objects, as it the case with objects stored through the <b>Binary</b> <b>Large</b> <b>Object</b> (BLOB) data type. These shortcomings are exacerbated by multiple users or applications that wish to access large objects concurrently. We describe an architecture, based on the Datalink data type, in which large objects in a database are continuously available for read access and can be read and written through a file system interface. Additionally, this system does not relax version management, consistency and recoverability gua [...] ...|$|E
5000|$|DPAPI {{security}} relies {{upon the}} Windows operating system's {{ability to protect}} the Master Key and RSA private keys from compromise, which in most attack scenarios is most highly reliant on {{the security of the}} end user's credentials. A main encryption/decryption key is derived from user's password by PBKDF2 function. Particular data <b>binary</b> <b>large</b> <b>objects</b> can be encrypted in a way that salt is added and/or an external user-prompted password (aka [...] "Strong Key Protection") is required. The use of a salt is a per-implementation option - i.e. {{under the control of the}} application developer - and is not controllable by the end user or system administrator.|$|R
40|$|We present {{recommendations}} on Performance Management for databases supporting <b>Binary</b> <b>Large</b> <b>Objects</b> (BLOB) that, under {{a wide range}} of conditions, save both storage space and database transactions processing time. The research shows that for database applications where ad hoc retrieval queries prevail, storing the actual values of BLOBs in the database may be the best choice to achieve better performance, whereas storing BLOBs externally is the best approach where multiple Delete/Insert/Update operations on BLOBs dominate. Performance measurements are used to discover System Performance Bottlenecks and their resolution. We propose a strategy of archiving large data collections in order to reduce data management overhead in the Relational Database and maintain acceptable response time. ...|$|R
40|$|The Kepler Science Operations Center stores pixel {{values on}} {{approximately}} six million pixels collected every 30 -minutes, {{as well as}} data products that are generated {{as a result of}} running the Kepler science processing pipeline. The Kepler Database (Kepler DB) management system was created to act as the repository of this information. After one year of ight usage, Kepler DB is managing 3 TiB of data and is expected to grow to over 10 TiB {{over the course of the}} mission. Kepler DB is a non-relational, transactional database where data are represented as one dimensional arrays, sparse arrays or <b>binary</b> <b>large</b> <b>objects.</b> We will discuss Kepler DB's APIs, implementation, usage and deployment at the Kepler Science Operations Center...|$|R
40|$|Abstract. In this paper, we have {{designed}} and integrated an automatic optical inspection system, emphasizing on software {{implementation of the}} image processing, measurement and analysis utilities. As for the hardware equipments, we design an LED illumination unit and the custom-tailored machinery. By comparing the support functions of several main import brands of the optical inspection machine, we propose an optical inspecting procedure. By using the Windows-based user interface, we implement nine inspecting software tools, namely, the average gray level tool, the thresholding tool, the positioning tool, the edge detection tool, the <b>binary</b> <b>large</b> <b>object</b> (BLOB) tool, the template building tool, the smart matching tool, the inspection sequence tool, and the platform operation tool. All these tools {{can be used in}} an inspection with single operation and can also be arranged in a proper sequence of operations to fulfill a complicated inspection procedure. We use several part sample images with defects provided by the supplier to verify our fulfilled system...|$|E
40|$|This {{paper offers}} {{a review of}} {{technological}} developments regarding massive storage devices {{and the emergence of}} new streaming services for audio and video on-demand. While the capacity of storage devices has been increasing exponentially, the price of a stored bit has been falling at an even faster rate. Audio and video files can be stored on-line in personal computers and computer networks. Accessing this information requires new type of databases capable of handling special types of queries: access through annotations and metadata, access by similarity, and access by feature search. Databases with in-built streaming capabilities would be extremely useful for transmitting the information to the end user, while providing at the same time consistency checks, indexing, reporting, and querying features. They should also be fast and scalable. This paper reviews recent academic and industrial projects, and describes an implementation of a streaming video servlet based on the Oracle SQL database, its <b>Binary</b> <b>Large</b> <b>Object</b> storage data type, and the Java Database Connectivity interface. 1 Motivation: Th...|$|E
40|$|ACT/DB is a client—server {{database}} {{application for}} storing clinical trials and outcomes data, {{which is currently}} undergoing initial pilot use. It stores most of its data in entity—attribute—value form. Such data are segregated according to data type to allow indexing by value when possible, and <b>binary</b> <b>large</b> <b>object</b> data are managed {{in the same way}} as other data. ACT/DB lets an investigator design a study rapidly by defining the parameters (or attributes) that are to be gathered, as well as their logical grouping for purposes of display and data entry. ACT/DB generates customizable data entry. The data can be viewed through several standard reports as well as exported as text to external analysis programs. ACT/DB is designed to encourage reuse of parameters across multiple studies and has facilities for dictionary search and maintenance. It uses a Microsoft Access client running on Windows 95 machines, which communicates with an Oracle server running on a UNIX platform. ACT/DB is being used to manage the data for seven studies in its initial deployment...|$|E
40|$|International audienceThe {{increasingly}} growing {{data sets}} processed on HPC platforms raise major {{challenges for the}} underlying storage layer. A promising alternative to POSIX-IO-compliant file systems are simpler blobs (<b>binary</b> <b>large</b> <b>objects),</b> or object storage systems. They offer lower overhead and better performance {{at the cost of}} largely unused features such as file hierarchies or permissions. Similarly, blobs are increasingly considered for replacing distributed file systems for big data analytics or as a base for storage abstractions like key-value stores or time-series databases. This growing interest in such object storage on HPC and big data platforms raises the question: Are blobs the right level of abstraction to enable storage-based convergence between HPC and Big Data? In this paper we take a first step towards answering the question by analyzing the applicability of blobs for both platforms...|$|R
40|$|This paper {{presents}} the implementation {{and evaluation of}} a computer vision task on a Field Programmable Gate Array (FPGA). As an experimental approach for an application-specific image-processing problem, it provides results about gained performance and precision compared with similar solutions on General Purpose Processor (GPP) architectures. The problem of detecting <b>Binary</b> <b>Large</b> <b>OBjects</b> (BLOBs) in a continuous video stream and computation of their center points has been addressed. Most existing solutions are realized on GPP platforms, where resolution of image material and sequential processing define the performance barrier. FPGA based approaches perform implemented algorithms as fast as hardware circuits and in addition offer parallelization abilities. The evaluation compares precision and performance gain against similar approaches on GPP platforms. The paper discusses different concepts for BLOB detection and shows the implementation of one common method for BLOB detection, including design problems and performance evaluation...|$|R
40|$|Concurrent Big Data {{applications}} {{often require}} high-performance storage, {{as well as}} ACID (Atomicity, Consistency, Isolation, Durability) transaction support. Although blobs (<b>binary</b> <b>large</b> <b>objects)</b> are an increasingly popular storage model for such applications, state-of-the-art blob storage systems offer no transaction semantics. This demands users to coordinate data access carefully {{in order to avoid}} race conditions, inconsistent writes, overwrites and other problems that cause erratic behavior. We argue there is a gap between existing storage solutions and application requirements, which limits the design of transactionoriented applications. We introduce T¿yr, the first blob storage system to provide built-in, multiblob transactions, while retaining sequential consistency and high throughput under heavy access concurrency. T¿yr offers fine-grained random write access to data and in-place atomic operations. Large-scale experiments with a production application from CERN LHC show T¿yr throughput outperforming state-of-the-art solutions by more than 75 %...|$|R
