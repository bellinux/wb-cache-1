135|37|Public
2500|$|A single-level NOR flash cell in its default {{state is}} logically {{equivalent}} to a binary [...] "1" [...] value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the <b>bitline</b> voltage is pulled down. A NORflash cell can be programmed, or set to a binary [...] "0" [...] value, by the following procedure: ...|$|E
2500|$|The {{prefetch}} architecture {{takes advantage}} of the specific characteristics of memory accesses to DRAM. Typical DRAM memory operations involve three phases: <b>bitline</b> precharge, row access, column access. Row access is the heart of a read operation, as it involves the careful sensing of the tiny signals in DRAM memory cells; it is the slowest phase of memory operation. However, once a row is read, subsequent column accesses to that same row can be very quick, as the sense amplifiers also act as latches. For reference, a row of a 1 Gbit DDR3 device is 2,048 bits wide, so internally 2,048 bits are read into 2,048 separate sense amplifiers during the row access phase. [...] Row accesses might take 50 ns, depending on the speed of the DRAM, whereas column accesses off an open row are less than 10 ns.|$|E
50|$|The DRAM {{cells that}} are on the edges of the array do not have {{adjacent}} segments. Since the differential sense amplifiers require identical capacitance and <b>bitline</b> lengths from both segments, dummy <b>bitline</b> segments are provided. The advantage of the open <b>bitline</b> array is a smaller array area, although this advantage is slightly diminished by the dummy <b>bitline</b> segments. The disadvantage that caused the near disappearance of this architecture is the inherent vulnerability to noise, which affects the effectiveness of the differential sense amplifiers. Since each <b>bitline</b> segment does not have any spatial relationship to the other, it is likely that noise would affect only one of the two <b>bitline</b> segments.|$|E
5000|$|Write bit lines may be braided, so {{that they}} couple equally to the nearby read <b>bitlines.</b> Because write <b>bitlines</b> are full swing, they can cause {{significant}} disturbances on read <b>bitlines.</b>|$|R
40|$|Abstract — In a {{consumer}} electronic device, the embedded memories often consume {{a major portion}} of the total power. In this paper, we present a low-power SRAM design for a Viterbi decoder, featuring a quiet-bitline architecture with two techniques. Firstly, we use a one-side driving scheme for the write operation to prevent the excessive full-swing charging on the <b>bitlines.</b> Secondly, we use a precharge-free pulling scheme for the read operation so as to keep all <b>bitlines</b> at low voltages at all times. Silicon results shows that such architecture can lead to a significant 70 % power reduction over a self-designed baseline low-power SRAM macro. Index Terms — SRAM, embedded memory, low-power 1 I...|$|R
40|$|Limited memory {{bandwidth}} is {{a critical}} bottleneck in modern systems. 3 D-stacked DRAM enables higher bandwidth by leveraging wider Through-Silicon-Via (TSV) channels, but today's systems cannot fully exploit them due to the limited internal bandwidth of DRAM. DRAM reads a whole row simultaneously from the cell array to a row buffer, but can transfer {{only a fraction of}} the data from the row buffer to peripheral IO circuit, through a limited and expensive set of wires referred to as global <b>bitlines.</b> In presence of wider memory channels, the major bottleneck becomes the limited data transfer capacity through these global <b>bitlines.</b> Our goal in this work is to enable higher bandwidth in 3 D-stacked DRAM without the increased cost of adding more global <b>bitlines.</b> We instead exploit otherwise-idle resources, such as global <b>bitlines,</b> already existing within the multiple DRAM layers by accessing the layers simultaneously. Our architecture, Simultaneous Multi Layer Access (SMLA), provides higher bandwidth by aggregating the internal bandwidth of multiple layers and transferring the available data at a higher IO frequency. To implement SMLA, simultaneous data transfer from multiple layers through the same IO TSVs requires coordination between layers to avoid channel conflict. We first study coordination by static partitioning, which we call Dedicated-IO, that assigns groups of TSVs to each layer. We then provide a simple, yet sophisticated mechanism, called Cascaded-IO, which enables simultaneous access to each layer by time-multiplexing the IOs. By operating at a frequency proportional to the number of layers, SMLA provides a higher bandwidth (4 X for a four-layer stacked DRAM). Our evaluations show that SMLA provides significant performance improvement and energy reduction (55 %/ 18 % on average for multi-programmed workloads, respectively) over a baseline 3 D-stacked DRAM with very low area overhead...|$|R
50|$|The folded <b>bitline</b> array {{architecture}} routes bitlines {{in pairs}} throughout the array. The close {{proximity of the}} paired bitlines provide superior common-mode noise rejection characteristics over open <b>bitline</b> arrays. The folded <b>bitline</b> array architecture began appearing in DRAM ICs during the mid-1980s, beginning with the 256 Kbit generation. This architecture is favored in modern DRAM ICs for its superior noise immunity.|$|E
50|$|The {{horizontal}} wire, the wordline, {{is connected}} to the gate terminal of every access transistor in its row. The vertical <b>bitline</b> {{is connected to}} the source terminal of the transistors in its a column. The lengths of the wordlines and bitlines are limited. The wordline length is limited by the desired performance of the array, since propagation time of the signal that must transverse the wordline is determined by the RC time constant. The <b>bitline</b> length is limited by its capacitance (which increases with length), which must be kept within a range for proper sensing (as DRAMs operate by sensing the charge of the capacitor released onto the <b>bitline).</b> <b>Bitline</b> length is also limited by the amount of operating current the DRAM can draw and by how power can be dissipated, since these two characteristics are largely determined by the charging and discharging of the <b>bitline.</b>|$|E
50|$|The first {{generation}} (1 Kbit) DRAM ICs, {{up until the}} 64 Kbit generation (and some 256 Kbit generation devices) had open <b>bitline</b> array architectures. In these architectures, the bitlines are divided into multiple segments, and the differential sense amplifiers are placed in between <b>bitline</b> segments. Because the sense amplifiers are placed between <b>bitline</b> segments, to route their outputs outside the array, an additional layer of interconnect placed above those used to construct the wordlines and bitlines is required.|$|E
40|$|This paper proposes and investigates {{schemes for}} {{hardening}} the conventional CMOS cross-coupled DRAM sense amplifier to {{single event upset}} (SEU). These schemes, adapted from existing SRAM hardening techniques, are intended to harden the {{dynamic random access memory}} to bitline-mode errors during the sensing period. Simulation results indicate that a 9 kW L-resistor hardening scheme provides greater than 24 -fold improvement in critical charge over {{a significant part of the}} sensing period. Also proposed is a novel single event (SE) mirroring concept for SEU hardening of DRAMs. This concept has been implemented for hardening the <b>bitlines</b> to hits on diffusion regions connected to the lines during the highly susceptible highimpedance state of the <b>bitlines.</b> It is shown to result in over 26 -fold improvement in the level of critical charge using a 2 pF dynamic capacitive coupling...|$|R
40|$|A {{low-power}} cache {{has become}} essential in many applications, but cache accesses {{contribute significantly to}} a chip's total power consumption. because most bit values read from the cache are 0 S, the authors introduce a dynamic zero-sensitivity (DZS) scheme that reduces average cache power consumption by preventing <b>bitlines</b> from discharging in reading A 0...|$|R
40|$|Abstract. Memories are a {{main concern}} in {{low-power}} and high-speed designs. In a processor based SoC (System on Chip), they limit {{most of the}} time the speed and are the main part of the power consumption. For SRAM memories in 0. 25 μm, several improved low-power techniques have been applied, such as divided word lines at word level, physically split <b>bitlines</b> and a new asymmetrical RAM cell. Furthermore, to be capable of working at any supply voltage, and to take advantage of the new cell, sense amplifiers have been removed. This simple solution avoids noise problems, and in spite of the full swing on <b>bitlines,</b> the results show that power consumption performances are quite interesting: for a 8 Kbytes SRAM, at 0. 8 Volt, the access time is 74 ns while consuming 7 uA/MHz (at 2. 5 Volts, the access time is 3. 5 ns and the power consumption 22 uA/MHz). ...|$|R
50|$|The {{capacitor}} in the {{stacked capacitor}} scheme is constructed {{above the surface}} of the substrate. The capacitor is constructed from an oxide-nitride-oxide (ONO) dielectric sandwiched in between two layers of polysilicon plates (the top plate is shared by all DRAM cells in an IC), and its shape can be a rectangle, a cylinder, or some other more complex shape. There are two basic variations of the stacked capacitor, based on its location relative to the <b>bitline</b> - capacitor-over-bitline (COB) and capacitor-under-bitline (CUB). In a former variation, the capacitor is underneath the <b>bitline,</b> which is usually made of metal, and the <b>bitline</b> has a polysilicon contact that extends downwards to connect it to the access transistor's source terminal. In the latter variation, the capacitor is constructed above the <b>bitline,</b> which is almost always made of polysilicon, but is otherwise identical to the COB variation. The advantage the COB variant possesses is the ease of fabricating the contact between the <b>bitline</b> and the access transistor's source as it is physically close to the substrate surface. However, this requires the active area to be laid out at a 45-degree angle when viewed from above, which makes it difficult to ensure that the capacitor contact does not touch the <b>bitline.</b> CUB cells avoid this, but suffer from difficulties in inserting contacts in between bitlines, since the size of features this close to the surface are at or near the minimum feature size of the process technology (Kenner, pp. 33-42).|$|E
50|$|Sense {{amplifiers}} {{are required}} to read the state contained in the DRAM cells. When the access transistor is activated, the electrical charge in the capacitor is shared with the <b>bitline.</b> The bitline's capacitance {{is much greater than}} that of the capacitor (approximately ten times). Thus, the change in <b>bitline</b> voltage is minute. Sense amplifiers {{are required to}} resolve the voltage differential into the levels specified by the logic signaling system. Modern DRAMs use differential sense amplifiers, and are accompanied by requirements as to how the DRAM arrays are constructed. Differential sense amplifiers work by driving their outputs to opposing extremes based on the relative voltages on pairs of bitlines. The sense amplifiers function effectively and efficient only if the capacitance and voltages of these <b>bitline</b> pairs are closely matched. Besides ensuring that the lengths of the bitlines and the number of attached DRAM cells attached to them are equal, two basic architectures to array design have emerged to provide for the requirements of the sense amplifiers: open and folded <b>bitline</b> arrays.|$|E
50|$|The {{location}} where the <b>bitline</b> twists occupies additional area. To minimize area overhead, engineers select {{the simplest and}} most area-minimal twisting scheme that is able to reduce noise under the specified limit. As process technology improves to reduce minimum feature sizes, the signal to noise problem worsens, since coupling between adjacent metal wires is inversely proportional to their pitch. The array folding and <b>bitline</b> twisting schemes that are used must increase in complexity {{in order to maintain}} sufficient noise reduction. Schemes that have desirable noise immunity characteristics for a minimal impact in area is the topic of current research (Kenner, p. 37).|$|E
40|$|Leakage {{power is}} {{dominated}} by critical paths, and hence dynamic deactivation of fast transistors can yield large savings. We introduce metrics for comparing fine-grain dynamic deactivation techniques that include the effects of deactivation energy and startup latencies, as well as longterm leakage current. We present a new circuit-level technique for leakage current reduction, leakage-biased <b>bitlines,</b> that has low deactivation energy and fast wakeup times. We show how this technique can be applied at a fine grain within an active microprocessor, and how microarchitectural scheduling policies can improve its performance. Using leakage-biased <b>bitlines</b> to deactivate SRAM read paths within I-cache memories saves over 24 % of leakage energy and 22 % of total I-cache energy when using a 70 nm process. In the register file, fine-grained read port deactivation saves nearly 50 % of leakage energy and 22 % of total energy. Independently, turning off idle register file subbanks saves over 67 % of leakage energy (57 % total register file energy) with no loss in performance...|$|R
50|$|The usual layout {{convention}} {{is that a}} simple array is read out vertically. That is, a single word line, which runs horizontally, causes a row of bit cells to put their data on bit lines, which run vertically. Sense amps, which convert low-swing read <b>bitlines</b> into full-swing logic levels, are usually at the bottom (by convention). Larger register files are then sometimes constructed by tiling mirrored and rotated simple arrays.|$|R
40|$|Memories contain large arrays {{with high}} {{capacitance}} <b>bitlines</b> and IO lines. To reduce {{the power of}} memory accesses we limit the swings on these by controlling the time the lines are driven by using a replica feedback. The swings are set to 10 % of the supply {{over a wide range}} of process and operating conditions. Introduction Memory power is {{an important component of the}} power budget in today's electronic systems. This paper looks at circuit techniques that can be used to reduce the power requirements of a wide access width memory while having minimum effect on its access time. In CMOS memories the access path can be broken into two parts: from address to local wordline select, and from local wordline to sensed data. It is usually the latter half of the access (which involves driving the <b>bitlines</b> and local bus lines and sensing them) that consume the most power. These lines are heavily loaded and thus require a large amount of energy each time they are changed. While various techniques [...] ...|$|R
50|$|Advances {{in process}} {{technology}} {{could result in}} open <b>bitline</b> array architectures being favored if {{it is able to}} offer better long-term area efficiencies; since folded array architectures require increasingly complex folding schemes to match any advance in process technology. The relationship between process technology, array architecture, and area efficiency is an active area of research.|$|E
50|$|This {{architecture}} {{is referred to}} as folded because it takes its basis from the open array architecture {{from the perspective of the}} circuit schematic. The folded array architecture appears to remove DRAM cells in alternate pairs (because two DRAM cells share a single <b>bitline</b> contact) from a column, then move the DRAM cells from an adjacent column into the voids.|$|E
5000|$|A single-level NOR flash cell in its default {{state is}} logically {{equivalent}} to a binary [...] "1" [...] value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the <b>bitline</b> voltage is pulled down. A NOR flash cell can be programmed, or set to a binary [...] "0" [...] value, by the following procedure: ...|$|E
40|$|Three-dimensional (3 D) {{stacking}} using through silicon vias (TSVs) is {{a promising}} solution to provide low-latency and high-bandwidth DRAM access from microprocessors. The {{large number of}} TSVs implemented in 3 D DRAM circuits, however, are prone to open defects and coupling noises, leading to new test challenges. Through extensive simulation studies, this paper models the faulty behavior of TSV open defects occurred on the wordlines and the <b>bitlines</b> of 3 D DRAM circuits, which serves {{as the first step}} for efficient and effective test and diagnosis solutions for such defects. ...|$|R
40|$|We {{propose a}} new {{architecture}} for a dynamic random-access-memory (DRAM) capable of storing multiple values {{by using a}} single-electron transistor (SET). The gate of a SET {{is designed to be}} connected to a plurality of DRAM unit cells that are arrayed at intersections of word lines and <b>bitlines.</b> In this SET-DRAM hybrid scheme, the multiple switching characteristics of SET enables multiple value data stored in a DRAM unit cell, and this increases the storage functionality of the device. Moreover, since refreshing data requires only a small amount of SET driving current, this enables device operating with low standby power consumption. (C) 2016 Author(s) ...|$|R
40|$|In {{our work}} we present {{statistical}} methods and new memory array analysis approaches for decomposition {{and assessment of}} contributors to the Vth distribution widening. There, cell threshold voltage characteristics along <b>bitlines</b> and wordlines are considered as well as hidden systematic effects by convolutional analysis. Based on investigations on sub- 50 nm floating gate NAND memory arrays we demonstrate an analysis method to distinguish between different reasons for broadened distributions by means of memory map analysis algorithms and filters. The impact of systematic threshold voltage and cell current variation in memory arrays caused by intrinsic circuit properties will be discussed...|$|R
5000|$|In modern {{computer}} memory, a {{sense amplifier}} {{is one of}} the elements which make up the circuitry on a semiconductor memory chip (integrated circuit); the term itself dates back to the era of magnetic core memory. [...] A sense amplifier is part of the read circuitry that is used when data is read from the memory; its role is to sense the low power signals from a <b>bitline</b> that represents a data bit (1 or 0) stored in a memory cell, and amplify the small voltage swing to recognizable logic levels so the data can be interpreted properly by logic outside the memory.|$|E
50|$|During read accesses, the bit {{lines are}} {{actively}} driven {{high and low}} by the inverters in the SRAM cell. This improves SRAM bandwidth compared to DRAMs in a DRAM, the bit line is connected to storage capacitors and charge sharing causes the <b>bitline</b> to swing upwards or downwards. The symmetric structure of SRAMs also allows for differential signaling, which makes small voltage swings more easily detectable. Another difference with DRAM that contributes to making SRAM faster is that commercial chips accept all address bits at a time. By comparison, commodity DRAMs have the address multiplexed in two halves, i.e. higher bits followed by lower bits, over the same package pins {{in order to keep}} their size and cost down.|$|E
50|$|DRAM {{cells are}} {{laid out in a}} regular rectangular, grid-like pattern to {{facilitate}} their control and access via wordlines and bitlines. The physical layout of the DRAM cells in an array is typically designed so that two adjacent DRAM cells in a column share a single <b>bitline</b> contact to reduce their area. DRAM cell area is given as n F2, where n is a number derived from the DRAM cell design, and F is the smallest feature size of a given process technology. This scheme permits comparison of DRAM size over different process technology generations, as DRAM cell area scales at linear or near-linear rates over. The typical area for modern DRAM cells varies between 6-8 F2.|$|E
40|$|Clock {{frequency}} of a multi-ported, 256 X 32 h dynamic register file in a lOOnm technology is improved by 50 %, {{compared to the}} best dual-V, (DVT) design, using LBSF and SFN leakage-tolerant circuit techniques for LBL and GBL. Total transistor width of the full LBSF design is the smallest. I. Introductinn High performance microprocessor execution cores require multi-ported register tiles (RF) with single-cycle readlwrite. High fan-in or wide dynamic circuits are typically used for local (LBL) and global (GBL) <b>bitlines</b> to meet the aggressive delay and area requirements. However, noise tolerance of wide domino gates degrades rapidly with technology scaling as transistor subthreshold leakage increases exponentially [I]. Although circuit robustness can be recovered by upsizing th...|$|R
40|$|Power {{consumption}} and Static noise margin (SNM) {{are most important}} parameters for memory design. The main source of power consumption in SRAM cell is due to large voltage swing on the <b>bitlines</b> during write operation. To reduce the power {{consumption and}} enhance {{the performance of the}} SRAM cell, we propose a Low-power fast (LPF) SRAM cell. The cell is simulated in terms of power, delay and read stability. The simulated result shows that the read and write power of the proposed cell is reduced up to 33 % and 57. 12 % at 1. 2 V (in CMOS 0. 12 mu m technology) respectively compared to the 6 T cell. The read SNM of the LPF cell is 2 x times of the conventional cell...|$|R
40|$|DRAM vendors have {{traditionally}} optimized the cost-perbit metric, often making design decisions that incur energy penalties. A prime {{example is the}} overfetch feature in DRAM, where a single request activates thousands of <b>bitlines</b> in many DRAM chips, only to return a single cache line to the CPU. The focus on cost-per-bit is questionable in modern-day servers where operating costs can easily exceed the purchase cost. Modern technology trends are also placing very different demands on the memory system: (i) queuing delays are a significant component of memory access time, (ii) {{there is a high}} energy premium for the level of reliability expected for business-critical computing, and (iii) the memory access stream emerging from multi-core systems exhibits limited locality. All of these trends necessitate a...|$|R
5000|$|In 2008 {{concerns}} {{were raised in}} the book Wafer Level 3-D ICs Process Technology that non-scaling analog elements such as charge pumps and voltage regulators, and additional circuitry [...] "have allowed significant increases in bandwidth but they consume much more die area". Examples include CRC error-detection, on-die termination, burst hardware, programmable pipelines, low impedance, and increasing need for sense amps (attributed {{to a decline in}} bits per <b>bitline</b> due to low voltage). The authors noted that, as a result, the amount of die used for the memory array itself has declined over time from 70-78% with SDRAM and DDR1, to 47% for DDR2, to 38% for DDR3 and potentially to less than 30% for DDR4.|$|E
50|$|The {{prefetch}} architecture {{takes advantage}} of the specific characteristics of memory accesses to DRAM. Typical DRAM memory operations involve three phases: <b>bitline</b> precharge, row access, column access. Row access is the heart of a read operation, as it involves the careful sensing of the tiny signals in DRAM memory cells; it is the slowest phase of memory operation. However, once a row is read, subsequent column accesses to that same row can be very quick, as the sense amplifiers also act as latches. For reference, a row of a 1 Gbit DDR3 device is 2,048 bits wide, so internally 2,048 bits are read into 2,048 separate sense amplifiers during the row access phase. Row accesses might take 50 ns, depending on the speed of the DRAM, whereas column accesses off an open row are less than 10 ns.|$|E
5000|$|The {{data in a}} {{semiconductor}} memory chip is stored in tiny circuits called memory cells. Sense Amplifiers are primarily applied in Volatile memory cells. The memory cells are either SRAM or DRAM cells which are laid out in rows and columns on the chip. Each line is attached to each cell in the row. The lines which run along the rows are called wordlines which are activated by putting a voltage on it. The lines which run along the columns are called bit-line and two such complementary bitlines are attached to a sense amplifier {{at the edge of}} the array. Number of sense amplifiers are of that of the [...] "bitline' on the chip. Each cell lies at the intersection of a particular wordline and <b>bitline,</b> which can be used to [...] "address" [...] it. The data in the cells is read or written by the same bit-lines which run along the top of the rows and columns.|$|E
40|$|Data {{stability}} {{is a major}} concern in today's high performance memory circuits with deeply scaled transistors and power supply voltages. Multi-threshold-voltage (multi-V t) nine-transistor (9 T) Static Random Access Memory (SRAM) cells with enhanced data stability and superior overall electrical quality characteristics are presented in this paper. 9 T SRAM cells eliminate the data disturbance by isolating the <b>bitlines</b> from data storage nodes during read operations. The design tradeoffs of 9 T multi-V t SRAM cells are explored with a TSMC 65 nm CMOS technology that provides a rich set of device threshold voltage options. A triple-threshold-voltage 9 T memory cell offers up to 9. 7 X higher overall electrical quality as compared to a previously published single-threshold-voltage 9 T SRAM cell. © 2011 IEEE...|$|R
40|$|There {{is three}} type of low power {{technique}} discussedhere for Static random access memory. One is Quiet Bit linearchitecture {{in which the}} voltage of bit line stay as low aspossible. To prevent the excessive full-swing charging on thebitline one-side driving scheme for write operation is used andfor read precharge free-pulling scheme is used to keep all <b>bitlines</b> at low voltages at all times. Second is Body biastechnique which decreases the process variation on the SRAMcell and it can operate at 0. 3 and write margin is not degraded. Third is half –swing Pulse-mode techniques in which HalfswingPulse-mode gate family is used that uses reduced inputsignal swing without sacrificing performance and to save thepower, bit lines are operated from Vdd / 2 instead ofVdd...|$|R
5000|$|A SONOS {{memory array}} is {{constructed}} by fabricating {{a grid of}} SONOS transistors which are connected by horizontal and vertical control lines (wordlines and <b>bitlines)</b> to peripheral circuitry such as address decoders and sense amplifiers. After storing or erasing the cell, the controller can measure {{the state of the}} cell by passing a small voltage across the source-drain nodes; if current flows the cell must be in the [...] "no trapped electrons" [...] state, which is considered a logical [...] "1". If no current is seen the cell must be in the [...] "trapped electrons" [...] state, which is considered as [...] "0" [...] state. The needed voltages are normally about 2 V for the erased state, and around 4.5 V for the programmed state.|$|R
