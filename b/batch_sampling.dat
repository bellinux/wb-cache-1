29|623|Public
40|$|This paper {{investigates the}} {{estimation}} {{of the number of}} operating sensors in a wireless sensor network. The basic model considered is equivalent to an urn model with replacement. For this model, we propose an estimator based on the Good-Turing non-parametric estimator of the missing mass. We show how this estimator can be applied to other related problems like estimation of class histograms. It is also shown that the estimator proposed is robust to model changes; more exactly its performance degradation is small when it is applied to a <b>batch</b> <b>sampling</b> model that models a receiver with multiple packet reception (MPR) capabilities. A modified estimation method that takes into account the <b>batch</b> <b>sampling</b> model is given and its performance is discussed...|$|E
40|$|The Galileo Probe Mass Spectrometer (GPMS) is a Probe {{instrument}} {{designed to}} measure the chemical and isotopic composition including vertical variations of the constituents in the atmosphere of Jupiter. The measurement will be performed by in situ sampling of the ambient atmosphere in the pressure range from approximately 150 mbar to 20 bar. In addition <b>batch</b> <b>sampling</b> will be performed for noble gas composition measurement and isotopic ratio determination and for sensitivity enhancement of non-reactive trace gases...|$|E
40|$|Non-adaptive geostatistical designs (NAGD) offer {{standard}} ways {{of collecting}} and analysing geostatistical data in which sampling locations are fixed {{in advance of}} any data collection. In contrast, adaptive geostatistical designs (AGD) allow collection of exposure and outcome data over time to depend on information obtained from previous information to optimise data collection towards the analysis objective. AGDs are becoming more important in spatial mapping, particularly in poor resource settings where uniformly precise mapping may be unrealistically costly and priority is often to identify critical areas where interventions can have the most health impact. Two constructions are: singleton and batch adaptive sampling. In singleton sampling, locations x_i are chosen sequentially and at each stage, x_k+ 1 depends on data obtained at locations x_ 1, [...] ., x_k. In <b>batch</b> <b>sampling,</b> locations are chosen in batches of size b > 1, allowing new batch, {x_(k+ 1), [...] .,x_(k+b) }, to depend on data obtained at locations x_ 1, [...] ., x_kb. In most settings, <b>batch</b> <b>sampling</b> is more realistic than singleton sampling. We propose specific batch AGDs and assess their efficiency relative to their singleton adaptive and non-adaptive counterparts by using simulations. We show how we apply these findings to inform an AGD of a rolling Malaria Indicator Survey, part of a large-scale, five-year malaria transmission reduction project in Malawi. Comment: 18 pages, 4 figure...|$|E
5000|$|...Test Method F662-86 (1992)e1 Standard Test Method for Measurement of Particle Count and Size Distribution in <b>Batch</b> <b>Samples</b> for Filter Evaluation Using an Electrical Resistance Particle Counter ...|$|R
40|$|A new {{sequential}} sampling scheme is proposed in which, {{after an initial}} <b>batch</b> <b>sample,</b> sampling is continued in batches of data-dependent sizes (at most k such batches), and then one-at-a-time with a data-dependent stopping rule. This new scheme requires about the same sample size as the fully sequential Anscombe-Chow-Robbins (ACR) sampling scheme but substantially fewer sampling operations. The problem of constructing fixed-width confidence intervals for the mean of a normal population with unknown variance is used as an illustratio...|$|R
40|$|A novel optical {{measuring}} method is described {{which was used}} for the density measurement of model <b>samples</b> of glass <b>batch</b> during the melting process. It used a sample volume of some 1000 mm(3) which was sufficiently large to suppress single particle effects. Shape changes of <b>batch</b> <b>samples</b> during heating were taken into account by the optical method. Volumetric changes of soda-lime-silica glass batch were monitored between room temperature and 1100 degreesC. Initially a sintering stage was observed showing self similar reduction in the sample volume. Thereafter, the sample shape approached the equilibrium surface of drops. This was accompanied by a large increase in volume, which was attributed to the formation of gas bubbles within the <b>batch</b> <b>samples.</b> A strong influence of heating rate and composition on this volume increase was observed...|$|R
40|$|The {{chemical}} and isotopic {{composition of the}} Jupiter atmosphere's constituents, including their vertical variations, will be measured by the Galileo Probe Mass Spectrometer instrument through in situ sampling; <b>batch</b> <b>sampling</b> will also be undertaken for noble gas composition and isotopic ratio determinations. The instrument's gas-sampling system is connected to a quadrupole mass analyzer for molecular weight analysis. Threshold values are lowered through sample enrichment {{by a factor of}} 100 - 500 for stable hydrocarbons and by a factor of 10 for noble gases. The instrument follows a sampling sequence of 8192 steps, at a rate of 2 steps/sec...|$|E
40|$|We {{consider}} {{the design and}} analysis of algorithms to retrieve simple random samples from databases. Specifi-cally, we examine simple random sampling from B+ tree files. Existing methods of sampling from B+ trees, re-quire the use of auxiliary rank information in the nodes of the tree. Such modified B+ tree files are called “ranked B+ trees”. We compare sampling from ranked Bt tree files, with new acceptance/rejection (A/R) sam-pling methods which sample directly from standard B+ trees. Our new A/R sampling algorithm can easily be retrofit to existing DBMSs, and {{does not require the}} overhead of maintaining rank information. We consider both iterative and <b>batch</b> <b>sampling</b> methods. ...|$|E
40|$|In many {{statistical}} problems, the variances of {{the populations}} cannot {{be assumed to}} be equal. These inhomogeneity problems are often more difficult to handle than the corresponding homogeneity problems. In sequential estimation, this often means that only first order sequential procedures {{are available in the}} statistics literature for inhomogeneity problems. The {{purpose of this paper is}} to illustrate by using the classical Behrens–Fisher problem how to construct a second order sequential procedure using the <b>batch</b> <b>sampling</b> idea of Hall [Ann. Statist. 9 (1981) 1229 – 1238]; the cost of assuming variance inhomogeneity even when the two variances are equal turns out to be very limited. The approach of this paper can readily be applied to many other inhomogeneous problems. <br/...|$|E
30|$|The {{sequencing}} {{library was}} constructed using {{a modified version}} of the NEBNext Ultra DNA library Prep Kit (New England Biolabs, Ipswich, MA, USA) protocol. We constructed barcoded, paired-end libraries with an insert size of ~ 500 bp for each sample. Four samples were designed to be sequenced in each lane. Samples were randomly assigned to different sequence lanes. Extracted DNA (200 ng) was used for library construction for each sample. Samples were sequenced in two <b>batches.</b> <b>Samples</b> in the first <b>batch</b> (172 <b>samples)</b> were sent to BGI Shenzhen for sequencing. After quality determination, libraries passing quality control were sequenced with the Illumina HiSeq 2000 platform. The read length was set to 90 bp. Samples in the second <b>batch</b> (3 <b>samples)</b> were sequenced in Tsinghua University with the Illumina HiSeq 2500 platform. The read length was set to 100 bp.|$|R
40|$|Analytical methods {{validation}} is {{a mandatory}} step {{to evaluate the}} ability of developed methods to provide accurate results for their routine application. Validation usually involves validation standards or quality control samples that are prepared in placebo or reconstituted matrix made of a mixture of all the ingredients composing the drug product except the active substance or the analyte under investigation. However, {{one of the main}} concerns that can be made with this approach is that it may lack an important source of variability that come from the manufacturing process. The question that remains {{at the end of the}} validation step is about the transferability of the quantitative performance from validation standards to real authentic drug product samples. In this work, this topic is investigated through three case studies. Three analytical methods were validated using the commonly spiked placebo validation standards at several concentration levels as well as using samples coming from authentic <b>batch</b> <b>samples</b> (tablets and syrups). The results showed that, depending on the type of response function used as calibration curve, there were various degrees of differences in the results accuracy obtained with the two types of samples. Nonetheless the use of spiked placebo validation standards was showed to mimic relatively well the quantitative behaviour of the analytical methods with authentic <b>batch</b> <b>samples.</b> Adding these authentic <b>batch</b> <b>samples</b> into the validation design may help the analyst to select and confirm the most fit for purpose calibration curve and thus increase the accuracy and reliability of the results generated by the method in routine application. Peer reviewe...|$|R
5000|$|... track a <b>sample,</b> a <b>batch</b> of <b>samples,</b> or a [...] "lot" [...] of batches {{through its}} {{lifecycle}} ...|$|R
40|$|Large-scale data {{analytics}} frameworks are shifting towards shorter task durations and larger degrees of parallelism to provide low latency. However, scheduling highly parallel jobs that complete {{in hundreds of}} milliseconds poses a major scaling challenge for cluster schedulers, which will need to place millions of tasks per second on appropriate nodes while offering millisecondlevel latency and high availability. This paper presents a decentralized load balancing approach called <b>batch</b> <b>sampling,</b> based on a generalization {{of the power of}} two random choices, that performs within a few percent of an optimal centralized scheduler. We evaluate our approach through both analytical results and an implementation called Sparrow. Sparrow schedules tasks with less than 8 ms of overhead and features a design that is inherently fault-tolerant and scalable. ...|$|E
40|$|The {{evaluation}} of produced items {{at the time}} of delivery is, in practice, usually amended by at least one inspection at later time points. We extend the methodology of acceptance sampling for variables for arbitrary unknown distributions when additional sampling infor- mation is available to such settings. Based on appropriate approximations of the operating characteristic, we derive new acceptance sampling plans that control the overall operating characteristic. The results cover the case of independent sampling as well as the case of dependent sampling. In particular, we study a modified panel sampling design and the case of spatial <b>batch</b> <b>sampling.</b> The latter is advisable in photovoltaic field monitoring studies, since it allows to detect and analyze local clusters of degraded or damaged modules. Some finite sample properties are examined by a simulation study, focusing on the accuracy of estimation...|$|E
40|$|Abstract. Nowadays, concept {{detection}} from {{multimedia data}} is con-sidered as an emerging topic {{due to its}} applicability to various applica-tions in both academia and industry. However, there are some inevitable challenges including the high volume and variety of multimedia data {{as well as its}} skewed distribution. To cope with these challenges, in this paper, a novel framework is proposed to integrate two correlation-based methods, Feature-Correlation Maximum Spanning Tree (FC-MST) and Negative-based Sampling (NS), with a well-known deep learning algo-rithm called Convolutional Neural Network (CNN). First, FC-MST is introduced to select the most relevant low-level features, which are ex-tracted from multiple modalities, and to decide the input layer dimension of the CNN. Second, NS is adopted to improve the <b>batch</b> <b>sampling</b> in the CNN. Using NUS-WIDE image data set as a web-based applica-tion, the experimental results demonstrate the effectiveness of the pro-posed framework for semantic concept detection, comparing to other well-known classifiers...|$|E
40|$|Background: Establishing botanical {{extracts}} as globally-accepted polychemical {{medicines and}} a new paradigm for disease treatment, requires the development of high-level quality control metrics. Based on comprehensive chemical and biological fingerprints correlated with pharmacology, we propose a general approach called PhytomicsQC to botanical quality control. Methods: Incorporating the state-of-the-art analytical methodologies, PhytomicsQC was employed {{in this study and}} included the use of liquid chromatography/mass spectrometry (LC/MS) for chemical characterization and chemical fingerprinting, differential cellular gene expression for bioresponse fingerprinting and animal pharmacology for in vivo validation. A statistical pattern comparison method, Phytomics Similarity Index (PSI), based on intensities and intensity ratios, was used to determine the similarity of the chemical and bioresponse fingerprints among different manufactured batches. Results: Eighteen <b>batch</b> <b>samples</b> of Huangqin Tang (HQT) and its pharmaceutical grade version (PHY 906) were analyzed using the PhytomicsQC platform analysis. Comparative analysis of the <b>batch</b> <b>samples</b> with a clinically tested standardized batch obtained values of PSI similarity between 0. 67 and 0. 99. Conclusion: With rigorous quality control using analytically sensitive and comprehensive chemical and biologica...|$|R
40|$|Over {{the last}} three years the Centre for Soil Treatment has sampled 2570 soil {{stockpiles}} for the assessment of soil quality. For all stockpiles the same sampling strategy was used. 100 increments of approximately 180 grams were taken and collected into two <b>batch</b> <b>samples.</b> Appropriate sample pre-treatment and analyses resulted in two sets of analytical results per stockpile. These paired results were statistically interpreted in order to define the heterogeneity of the original soil stockpiles. By estimating this original heterogeneity, the effectiveness of the used sampling strategy could be determined. It is concluded that the chosen sampling strategy is adequate for the majority of soil stockpiles...|$|R
40|$|Batch {{cultures}} of Chinese hamster ovary cells expressing human interferon-gamma (IFN-gamma) were supplemented with the lipids dolichol or dolichyl phosphate. End of <b>batch</b> <b>samples</b> were analysed by micellar electrokinetic capillary electrophoresis to examine changes in glycosylation site occupancy. A {{detailed analysis of}} carbohydrate structures at each site was performed using matrix-assisted laser desorption/ionisation mass spectrometry. Cultures that had received a single supplement of dolichol or dolichyl phosphate {{at the start of}} culture exhibited a 16...|$|R
40|$|Abstract. One of {{the core}} {{applications}} of machine learning to knowledge discovery consists on building a function (a hypothesis) from a given amount of data (for instance a decision tree or a neural network) such {{that we can use}} it afterwards to predict new instances of the data. In this paper, we focus on a particular situation where we assume that the hypothesis we want to use for prediction is very simple, and thus, the hypotheses class is of feasible size. We study {{the problem of how to}} determine which of the hypotheses in the class is almost the best one. We present two on-line sampling algorithms for selecting hypotheses, give theoretical bounds on the number of necessary examples, and analize them exprimentally. We compare them with the simple <b>batch</b> <b>sampling</b> approach commonly used and show that in most of the situations our algorithms use much fewer number of examples. ...|$|E
40|$|The Huygens Probe of the Cassini Huygens Mission {{entered the}} {{atmosphere}} of the moon Titan on January 14, 2005. The GCMS was part of the instrument complement on the Probe to measure in situ the chemical composition of the atmosphere during the probe descent and to support the Aerosol Collector Pyrolyser (ACP) experiment by serving as detector for the pyrolization products. The GCMS employed a quadrupole mass filter with a secondary electron multiplier detection system and a gas sampling system providing continuous direct atmospheric composition measurements and <b>batch</b> <b>sampling</b> through three gas chromatographic (GC) columns. The mass spectrometer employed five electron impact ion sources with available electron energies of either 70 or 25 eV. Three ion sources served as detectors for the GC columns and two were dedicated to direct atmosphere sampling and ACP gas sampling, respectively. The GCMS gas inlet was heated to prevent condensation, and served to evaporate surface constituents after impact...|$|E
40|$|One of {{the core}} {{applications}} of machine learning to knowledge discovery consists on building a function (a hypothesis) from a given amount of data (for instance a decision tree or a neural network) such {{that we can use}} it afterwards to predict new instances of the data. In this paper, we focus on a particular situation where we assume that the hypothesis we want to use for prediction is very simple, and thus, the hypotheses class is of feasible size. We study {{the problem of how to}} determine which of the hypotheses in the class is almost the best one. We present two online sampling algorithms for selecting hypotheses, give theoretical bounds on the number of necessary examples, and analize them exprimentally. We compare them with the simple <b>batch</b> <b>sampling</b> approach commonly used and show that in most of the situations our algorithms use much fewer number of examples. 1 Introduction and Motivation The ubiquity of computers in business and commerce has lead to generation of huge quantitie [...] ...|$|E
40|$|Abstract Background Establishing botanical {{extracts}} as globally-accepted polychemical {{medicines and}} a new paradigm for disease treatment, requires the development of high-level quality control metrics. Based on comprehensive chemical and biological fingerprints correlated with pharmacology, we propose a general approach called PhytomicsQC to botanical quality control. Methods Incorporating the state-of-the-art analytical methodologies, PhytomicsQC was employed {{in this study and}} included the use of liquid chromatography/mass spectrometry (LC/MS) for chemical characterization and chemical fingerprinting, differential cellular gene expression for bioresponse fingerprinting and animal pharmacology for in vivo validation. A statistical pattern comparison method, Phytomics Similarity Index (PSI), based on intensities and intensity ratios, was used to determine the similarity of the chemical and bioresponse fingerprints among different manufactured batches. Results Eighteen <b>batch</b> <b>samples</b> of Huangqin Tang (HQT) and its pharmaceutical grade version (PHY 906) were analyzed using the PhytomicsQC platform analysis. Comparative analysis of the <b>batch</b> <b>samples</b> with a clinically tested standardized batch obtained values of PSI similarity between 0. 67 and 0. 99. Conclusion With rigorous quality control using analytically sensitive and comprehensive chemical and biological fingerprinting, botanical formulations manufactured under standardized manufacturing protocols can produce highly consistent batches of products. </p...|$|R
40|$|We analyze a batched {{variant of}} Stochastic Gradient Descent (SGD) with {{weighted}} sampling distribution for smooth and non-smooth objective functions. We show that by distributing the batches computationally, a significant speedup in the convergence rate is provably possible compared to either <b>batched</b> <b>sampling</b> or weighted sampling alone. We propose several computationally efficient schemes to approximate the optimal weights, and compute proposed sampling distributions explicitly {{for the least}} squares and hinge loss problems. We show both analytically and experimentally that substantial gains can be obtaine...|$|R
40|$|In {{this project}} a method is {{developed}} for the determination and quantification of the degradation products of the reaction of b-pinene with hydroxyl radicals. The study is carried out in a fast-flow reactor equipped with a specially designed microwave cavity (type Surfatron) allowing to operate at pressures up to 100 Torr. The semi-volatile products are collected on a liquid nitrogen trap (LN 2 trap) coated with a 2, 4 -DNPH solution and the <b>batch</b> <b>samples</b> are subsequently analyzed by HPLC-MS using APCI(-). In order to perform quantitative measurements the <b>batch</b> <b>samples</b> contained two internal standards: benzaldehyde- 2, 4 -DNPH and tolualdehyde- 2, 4 -DNPH. In the experiments carried out at 50 Torr and 100 Torr, HPLC-MS measurements showed that the semi-volatile products formaldehyde, nopinone, acetaldehyde, acetone, perilladehyde, myrtanal and trans- 3 -hydroxynopinone could be identified as oxidation products for the b-pinene /OH reaction, with nopinone and formaldehyde being the main products. Assuming that the oxidation products have the same collection efficiency on the LN 2 trap, one arrives at the following relative product yields (expressed in mole %) at 50 and 100 Torr respectively: 48 ± 2 and 43 ± 2 for formaldehyde; 1. 2 ± 0. 6 and 0. 6 ± 0. 4 for acetaldehyde; 20 ± 3 and 7. 8 ± 0. 5 for acetone and 31 ± 1 and 49...|$|R
40|$|Abstract. The Gas Chromatograph Mass Spectrometer (GCMS) on the Huygens Probe will meas-ure the {{chemical}} composition of Titan’s atmosphere from 170 km altitude (∼ 1 hPa) {{to the surface}} (∼ 1500 hPa) and determine the isotope ratios of the major gaseous constituents. The GCMS will also analyze gas samples from the Aerosol Collector Pyrolyser (ACP) and {{may be able to}} investigate the composition (including isotope ratios) of several candidate surface materials. The GCMS is a quadrupole mass filter with a secondary electron multiplier detection system and a gas sampling system providing continuous direct atmospheric composition measurements and <b>batch</b> <b>sampling</b> through three gas chromatographic (GC) columns. The mass spectrometer employs five ion sources sequentially feeding the mass analyzer. Three ion sources serve as detectors for the GC columns and two are dedicated to direct atmosphere sampling and ACP gas sampling re-spectively. The instrument is also equipped with a chemical scrubber cell for noble gas analysis and a sample enrichment cell for selective measurement of high boiling point carbon containin...|$|E
40|$|Ion Mobility Spectrometry (IMS) is {{currently}} being successfully applied {{to the problem of}} on-line trace detection of plastic and other explosives in airports and other facilities. The methods of sample retrieval primarily consist of <b>batch</b> <b>sampling</b> for particulate residue on a filter card for introduction into the IMS. The sample is desorbed into the IMS using air as the carrier and negative ions of the explosives are detected, some as an adduct with a reagent ion such as Cl(-). Based on studies and tests conducted by different airport authorities, this method seems to work well for low vapor pressure explosives such as RDX and PETN, as well as TNT that are highly adsorptive and can be found in nanogram quantities on contaminated surfaces. Recently, the changing terrorist threat and the adoption of new marking agents for plastic explosives has meant that the sample introduction and analysis capabilities of the IMS must be enhanced in order to keep up with other detector developments. The IMS has sufficient analytical resolution for a few threat compounds but the IMS Plasmogram becomes increasingly more difficult to interpret when the sample mixture gets more complex...|$|E
40|$|Karst {{aquifers}} provide 25 % of {{the overall}} drinking water resources to the world‟s population and sustain aquatic life in most fluvial systems, providing several ecological services to human beings, although, because of their complex links between surface and groundwater, {{turn out to be}} very vulnerable to contamination and pollution. This paper describes the preliminary findings from Radon- 222 activity concentration measurement data collected in streamflow and instream springs during monthly field campaigns in a typical Mediterranean karst river: the Bussento river (Campania region, Southern Italy). The general aim is to investigate the complex interactions and exchanges between streamflow and groundwater, at scales that are imperceptible to standard hydrological and hydraulic analyses. Experimental data about 222 Rn activity concentrations in streamflow and inflow spring waters, from selected sampling stations, have been acquired and managed by means of the Radon-in-Air analyzer, RAD 7, together with the Radon-in-water accessories, Radon Water Probe and RADH 2 O (DURRIDGE Co. Inc.), for continuous and <b>batch</b> <b>sampling</b> measurements, respectively. In addition, data about physical-chemical and streamflow rate have been, also, collected in-situ. During preliminary surveys, appropriate sampling procedures and measurement protocols have been tested, taking into account the different local hydrogeological and hydrological situations occurring along the Bussento river basin...|$|E
40|$|Medical devices {{manufactured}} for implantation into humans must {{be free of}} any contamination with viable bacteria. However, {{remnants of}} dead bacteria and bacterial components alone may induce an inflammatory immune response. Pyrogen tests for such inflammatory contaminations are generally performed either by determining the content of lipopolysaccharide in rinsing solutions of <b>batch</b> <b>samples</b> by limulus amoebocyte lysate assay, by injecting the rinsing solutions into rabbits or by implanting <b>batch</b> <b>samples</b> into rabbits and measuring change of body temperature. In this study, we show that the in vitro pyrogen test (IPT), which measures {{the release of the}} inflammatory cytokine IL- 1 b in fresh or cryopreserved human whole blood, can be used to assess the pyrogenic contamination of implantable medical devices. This test was used to check neurosurgical implants, namely aneurysm clips, as a proof of principle. Owing to the direct contact of the test material with the blood cells, this test does not require rinsing procedures, which have variable efficacy. The use of human blood ensures the detection of all substances that are pyrogenic for humans and reflects their relative potency. The safety of the products as delivered could be confirmed. The effects of sterilization and depyrogenization procedures on intentional pyrogenic contaminations of samples could be followed. This new application of the already internationally validated method promises to replace further rabbit pyrogen tests. It generates extremely sensitive results with an extended range of detectable pyrogenic contaminants. JRC. I. 2 -Validation of biomedical testing method...|$|R
40|$|Grain‐scale brittle {{fracture}} and grain rearrangement {{play an important}} role in controlling the compaction behavior of reservoir rocks during the early stages of burial. Therefore, the understanding of single‐grain failure is important. We performed constant displacement rate crushing tests carried out on selected, well‐rounded, single sand grains and on randomly sampled grains from different grain size (d) batches of pure quartz sand. Applying a Hertzian fracture mechanics model for grain crushing, the critical load at failure (Fc) data obtained for the selected grains were converted into an accurate estimate of the size of flaws associated with failure (cf). Similarly, the distributed Fc data obtained from the different <b>batch</b> <b>samples</b> were converted into distributions of grain failure stress. Weibull weakest link theory could not explain the observed grain failure behavior. On the contrary, the Hertzian grain failure criterion enabled the conversion of the distributed Fc data, for the <b>batch</b> <b>samples,</b> into distributions of cf, assuming spherical grains, or of “effective” radius of curvature (rg), characterizing contact surface asperities in the case of nonspherical grains. In contrast to the model of Zhang et al. (1990), our work shows that there is no clear physical basis for a grain size dependence of cf. However, since roundness data for dune sands exhibit a similar relation between rg and d, as seen in our grain size batches, it is inferred that the Hertzian fracture mechanics model assuming nonspherical grains with a distributed rg is the most physically reasonable model for grain failure...|$|R
40|$|In {{this paper}} a method is {{described}} for determining and quantifying the degradation {{products of the}} reaction of cr-pinene with hydroxyl radicals. The study is carried out in a fast-flow reactor equipped with a specially designed microwave cavity (type Surfatron) allowing to operate at pressures up to 100 Ton (1 Torr= 133. 322 Pa). The semi-volatile products are collected on a Liquid nitrogen trap (LN, trap) coated with a 2, 4 -dinitrophenylhydrazine (2, 4 -DNPH) solution and the <b>batch</b> <b>samples</b> are subsequently analyzed by HPLC. In order to perform quantitative measurements the <b>batch</b> <b>samples</b> contained two internal standards: benzaldehyde- 2, 4 -DNPH and tolualdehyde- 2, 4 -DNPH, In the experiments carried out at 50 Torr and 100 Ton, HPLC measurements showed that the semi-volatile products formaldehyde, acetaldehyde, acetone, campholenealdehyde and pinonaldehyde could be quantified as oxidation products for the alpha -pinene/OH reaction, with pinonaldehyde being the main product. Assuming that all these five oxidation products have the same collection efficiency on the LN, trap, one arrives at the following relative product yields (expressed in mol %) at 50 and 100 Ton, respectively: 9. 7 +/- 0. 7 and 6 +/- 5 for formaldehyde; 1. 1 +/- 0. 1 and 0. 9 +/- 0. 5 for acetaldehyde; 16 +/- 1 and 6 +/- 2 for acetone; 11 +/- 2 and 5. 5 +/- 0. 7 for campholenealdehyde; 63 +/- 3 and 82 +/- 7 for pinonaldehyde. (C) 2001 Elsevier Science B. V. All rights reserved. status: publishe...|$|R
40|$|In {{large storage}} systems, files are often coded across several servers to improve {{reliability}} and retrieval speed. We study load balancing under the <b>Batch</b> <b>Sampling</b> routing scheme for {{a network of}} n servers storing a set of files using the Maximum Distance Separable (MDS) code (cf. Li, Ramamoorthy, and Srikant (2016)). Specifically, each file is stored in equally sized pieces across L servers such that any k pieces can reconstruct the original file. When {{a request for a}} file is received, the dispatcher routes the job into the k-shortest queues among the L for which the corresponding server contains a piece of the file being requested. We establish a law of large numbers and a central limit theorem as the system becomes large (i. e. n→∞). For the central limit theorem, the limit process take values in ℓ_ 2, the space of square summable sequences. Due to the large size of such systems, a direct analysis of the n-server system is frequently intractable. The law of large numbers and diffusion approximations established in this work provide practical tools with which to perform such analysis. The Power-of-d routing scheme, also known as the supermarket model, is a special case of the model considered here. Comment: 40 page...|$|E
40|$|The {{effects of}} {{temperature}} and moisture content on the {{linear viscoelastic behavior}} of extruded amorphous potatostarch were studied using dynamic mechanical analysis (DMA) from the glassy to the rubbery state. Sinusoidal tensile tests were carried out in multifrequency mode {{in the range of}} 0. 1 – 40 Hz, with a temperature ramp of 20 to 135 °C. The water loss during DMA thermal scanning was evaluated by gravimetric analysis of <b>batch</b> <b>sampling</b> in parallel experiments that simulated temperature evolution. Loss and storage moduli were then corrected on real moisture content. Using the time temperature superposition principle, master curves were obtained for loss and storage moduli over 18 decades in frequency. The generalized Maxwell model, {{in the form of a}} Prony series obtained from master curve fitting, was used to predict the relaxation modulus. Results showed that the relaxationmodulus varied over a wide range, from 2650 MPa (t= 10 − 2 s, 20 °C, 8 % w. b.) to 0. 16 MPa (t= 105 s, 95 °C, 16 % w. b.). The obtained constitutive equation was effective to predict the linear viscoelasticity behavior of potato starch in a wide range of thermomechanical conditions, as shown by the results of simulations of tensile experiments using finite elements...|$|E
40|$|Mobile {{manipulation}} problems involving many {{objects are}} challenging to solve {{due to the}} high dimensionality and multi-modality of their hybrid configuration spaces. Planners that perform a purely geometric search are prohibitively slow for solving these problems because {{they are unable to}} factor the configuration space. Symbolic task planners can efficiently construct plans involving many variables but cannot represent the geometric and kinematic constraints required in manipulation. We present the FFRob algorithm for solving task and motion planning problems. First, we introduce Extended Action Specification (EAS) as a general purpose planning representation that supports arbitrary predicates as conditions. We adapt existing heuristic search ideas for solving strips planning problems, particularly delete-relaxations, to solve EAS problem instances. We then apply the EAS representation and planners to manipulation problems resulting in FFRob. FFRob iteratively discretizes task and motion planning problems using <b>batch</b> <b>sampling</b> of manipulation primitives and a multi-query roadmap structure that can be conditionalized to evaluate reachability under different placements of movable objects. This structure enables the EAS planner to efficiently compute heuristics that incorporate geometric and kinematic planning constraints to give a tight estimate of the distance to the goal. Additionally, we show FFRob is probabilistically complete and has finite expected runtime. Finally, we empirically demonstrate FFRob's effectiveness on complex and diverse task and motion planning tasks including rearrangement planning and navigation among movable objects...|$|E
40|$|AbstractRabies {{is still}} a cause of global concern, {{particularly}} in the developing countries. The treatment for post exposure prophylaxis {{is a combination of}} administration of vaccine and specific antiserum. Although equine antirabies immunoglobulin is available, the only approved method by World Health Organization for its testing is the Virus Neutralization Test (VNT), performed using mice. Since a large number of in-process samples are generated during antirabies immunoglobulin F(ab) 2 production., VNT is time consuming, utilizes a large number of mice, results are prone to errors; therefore cumbersome and costly for routine testing. Hence, there is an urgent need to develop an alternative test for screening large number of in-process samples. In the present study an attempt has been made to evaluate the possibility of applying Enzyme Linked Immunosorbent Assay (ELISA) for quantifying the samples, viz: Equine sera, Plasma, Purified bulk sera and Batch. A total of 4, 946 samples of sixty equines were monitored by indirect ELISA at an interval of two weeks for a period of five years. When seven equines of the above sixty were compared for VNT and indirect ELISA during their primary phase of immunization, a good correlation was observed (r = 1. 0). Based on this, the equines could be segregated into ‘Low’ responders (100 IU/ml). This segregation of equines based on their antibody titres proved to be very useful for initiating corrective remedial measures. On comparison of 14 Plasma samples, 31 Purified bulk sera <b>samples</b> and 17 <b>Batch</b> <b>samples</b> for VNT and indirect ELISA, a good correlation (r = 0. 82, 0. 923 & 0. 874 respectively) was observed. The sensitivity values observed were 100 %, 100 %, 90. 7 % & 90. 9 % and specificity values were 100 %, 100 %, 94. 7 % & 100 % for Equine sera, Plasma, Purified bulk sera and <b>Batch</b> <b>samples</b> respectively. The Cohen's Kappa index reflected a good agreement between the indirect ELISA test and the VNT test for Equine sera samples (1. 0), Plasma samples (1. 0), Purified bulk sera samples (0. 931) and <b>Batch</b> <b>samples</b> (0. 874). Thus, we conclude that indirect ELISA, which eliminates the use {{of a large number of}} mice, gives fast, yet reliable and reproducible results with less labor than the VNT test could be used as an optimum and ethical method for screening of antibody titers of all the in-process samples during manufacturing of antirabies serum...|$|R
30|$|Reagent blanks were {{prepared}} for each <b>batch</b> of <b>sample</b> and standard using equal amounts of all reagents used in the sample or standard preparation. The reagent blanks were used to correct for background absorption due to the reagent.|$|R
40|$|In {{monograph}} {{considered the}} methods and devices for testing of open micro-porosity of porcelain insulators. Analyzed the factors affecting the reliability of ultrasonic testing of porcelain insulators, and reasonable ways to reduce their impact. The proposed new criteria for culling of porcelain insulators {{on the results of}} ultrasonic testing based on an analysis of statistical parameters of samples measured values of the speed of ultrasound in <b>batch</b> <b>samples.</b> Described the testing system and specialized statistical diagnosis technical condition porcelain insulators, the use of which automates the process of monitoring and processing of experimental data. For scientific and technical personnel engaged in the development of diagnostic ultrasound systems, as well as for teachers and students of relevant specialties. ??????????? ?????? ? ???????? ???????? ???????? ???????????????? ?????????? ?????????? ??????????. ???????????????? ???????, ???????? ?? ????????????? ??????????????? ???????? ?????????? ?????????? ? ?????????? ???? ?????????? ?? ???????. ?????????? ????? ???????? ?????????? ?????????? ?????????? ?? ??????????? ??????????????? ????????, ?????????? ?? ??????? ?????????????? ?????????? ??????? ?????????? ???????? ???????? ??????????? ? ?????? ????????. ??????? ????????????? ???????? ???????? ? ?????????????????? ??????? ?????????????? ??????????? ???????????? ????????? ?????????? ??????????, ????????????? ??????? ????????? ???????????????? ??????? ???????? ? ????????? ????????????????? ??????. ??? ??????-??????????? ??????????, ???????????? ??????????? ?????????????? ?????? ???????????, ? ????? ??? ??????????????, ?????????? ? ????????? ??????????????? ??????????????...|$|R
