1283|4|Public
25|$|For efficiency, Prolog code is {{typically}} compiled to abstract machine code, often {{influenced by the}} register-based Warren Abstract Machine (WAM) instruction set. Some implementations employ abstract interpretation to derive type and mode information of predicates at compile time, or compile to real machine code for high performance. Devising efficient implementation methods for Prolog code is a field of active research in the logic programming community, and various other execution methods are employed in some implementations. These include clause <b>binarization</b> and stack-based virtual machines.|$|E
50|$|Image <b>binarization</b> is not required.|$|E
5000|$|System tools: These tools {{deal with}} <b>binarization,</b> synchronization, {{transport}} and storage of descriptors. It also deals with Intellectual Property protection.|$|E
50|$|He {{has engaged}} in {{mathematical}} fundamental research and its application concerning pattern recognition, image processing, multivariate analysis, artificial intelligence, and neurocomputing. Otsu's method, an image <b>binarization</b> technique, is still a standard technique widely used both in Japan and abroad.|$|E
50|$|Because {{the regions}} are defined {{exclusively}} by the intensity {{function in the}} region and the outer border, this leads to many key characteristics of the regions which make them useful. Over a large range of thresholds, the local <b>binarization</b> is stable in certain regions, and have the properties listed below.|$|E
50|$|There {{are many}} {{improvements}} focusing on different limitations for Otsu's method. One famous and effective way {{is known as}} two-dimensional Otsu's method. In this approach, the gray-level value of each pixel {{as well as the}} average value of its immediate neighborhood is studied so that the <b>binarization</b> results are greatly improved, especially for those images corrupted by noise.|$|E
50|$|Tesseract's output {{will have}} very poor quality if the input images are not preprocessed to suit it: Images (especially screenshots) must be scaled up {{such that the}} text {{x-height}} is at least 20 pixels, any rotation or skew must be corrected or no text will be recognized, low-frequency changes in brightness must be high-pass filtered, or Tesseract's <b>binarization</b> stage will destroy much of the page, and dark borders must be manually removed, or they will be misinterpreted as characters.|$|E
50|$|For efficiency, Prolog code is {{typically}} compiled to abstract machine code, often {{influenced by the}} register-based Warren Abstract Machine (WAM) instruction set. Some implementations employ abstract interpretation to derive type and mode information of predicates at compile time, or compile to real machine code for high performance. Devising efficient implementation methods for Prolog code is a field of active research in the logic programming community, and various other execution methods are employed in some implementations. These include clause <b>binarization</b> and stack-based virtual machines.|$|E
50|$|Due to its {{geographical}} roots, tropical music generally combines {{elements from}} European and African traditions. An {{example of this}} is the process of <b>binarization</b> of ternary rhythms brought from Africa, which took place originally in Cuba, later spreading {{throughout the rest of the}} Caribbean and Latin America. The presence of syncopated polyrhythms of African origin make most tropical music naturally dance-oriented. Tropical music instrumentation also includes both European (tres, piano, trumpet, timbales) and African-descended (congas, bongos, marimba) instruments. During the late 20th century, contemporary instruments such as synthesizers and drum machines were incorporated.|$|E
50|$|The {{purpose of}} {{preprocessing}} is to discard irrelevant {{information in the}} input data, that can negatively affect the recognition. This concerns speed and accuracy. Preprocessing usually consists of <b>binarization,</b> normalization, sampling, smoothing and denoising. The second step is feature extraction. Out of the two- or more-dimensional vector field received from the preprocessing algorithms, higher-dimensional data is extracted. The purpose of this step is to highlight important information for the recognition model. This data may include information like pen pressure, velocity or the changes of writing direction. The last big step is classification. In this step various models are used to map the extracted features to different classes and thus identifying the characters or words the features represent.|$|E
50|$|In this {{different}} form of clustering, each data object {{is allowed to}} be exclusively assigned to one and only one cluster, to be unassigned from all clusters, or to be simultaneously assigned to multiple clusters, in a completely tunable way. In some applications like gene clustering, this matches the biological reality {{that many of the}} genes considered for clustering in a particular gene discovery study might be irrelevant to the case of study in hand and should be ideally not assigned to any of the output clusters, moreover, any single gene can be participating in multiple processes and would be useful to be included in multiple clusters simultaneously. This has been proposed in the recent method of the <b>binarization</b> of consensus partition matrices (Bi-CoPaM) and is being used currently in the field of bioinformatics.|$|E
5000|$|The {{original}} algorithm of Matas et al. is [...] in {{the number}} [...] of pixels. It proceeds by first sorting the pixels by intensity. This would take [...] time, using BINSORT. After sorting, pixels are marked in the image, and the list of growing and merging connected components and their areas is maintained using the union-find algorithm. This would take [...] time. In practice these steps are very fast. During this process, the area of each connected component {{as a function of}} intensity is stored producing a data structure. A merge of two components is viewed as termination of existence of the smaller component and an insertion of all pixels of the smaller component into the larger one. In the extremal regions, the 'maximally stable' ones are those corresponding to thresholds where the relative area change as a function of relative change of threshold is at a local minimum, i.e. the MSER are the parts of the image where local <b>binarization</b> is stable over a large range of thresholds.|$|E
5000|$|... 6. Abu-Jamous et al.: They {{proposed}} their <b>binarization</b> {{of consensus}} partition matrices (Bi-CoPaM) method to enhance ensemble clustering in two major aspects. The {{first is to}} consider clustering {{the same set of}} objects by various clustering methods as well as by considering their features measured in multiple datasets; this seems perfectly relevant in the context of microarray gene expression clustering, which is the context they initially proposed the method in. The second aspect is the format of the final result; based on the consistency of inclusion of a data object in the same cluster by the multiple single clustering results, they allowed any single data object to have any of the three eventualities; to be exclusively assigned to one and only one cluster, to be unassigned from all clusters, or to be simultaneously assigned to multiple clusters at the same time. They made it possible to produce, in a perfectly tunable way, wide overlapping clusters, tight specific clusters, as well as complementary clusters. Therefore, they proposed their work as a new paradigm of clustering rather than merely a new ensemble clustering method.|$|E
50|$|Mainly in {{the context}} of gene clustering, the <b>binarization</b> of {{consensus}} partition matrices (Bi-CoPaM) was proposed by Abu-Jamous et al. as a method for consensus clustering. In contrast to other conventional clustering and ensemble clustering methods, Bi-CoPaM has the ability to combine the results of clustering the same set of genes from various microarray datasets and by using many clustering methods to produce one consensus result. Moreover, Bi-CoPaM relaxes conventional clustering constraints by allowing each gene to have any of the three possible eventualities - to be exclusively assigned to one and only one cluster (as any conventional clustering method does), to be simultaneously assigned to multiple clusters, or to be unassigned from all of the clusters. At the clusters level, clusters can be complementary (as in the case of conventional clustering), can be wide and overlapping, and can be tight and distinct while leaving many genes unassigned from all of them. The Bi-CoPaM method has not been designed to only allow for these three forms of clusters; it has also been provided with tuning parameters which can be used to tune the level of tightness and wideness of the clusters based on research requirements.|$|E
50|$|Throughout {{his long}} career as musicologist and researcher, Rolando Pérez has {{published}} the following books: ‘’La binarización de los ritmos ternarios africanos en América Latina’’ (Casa de las Américas, La Habana, 1987) and ‘’La música afromestiza mexicana’’ (Universidad Veracruzana, Xalapa (Veracruz), 1990). In {{the first of}} those books (which won the musicology award from Cuban cultural institution “Casa de las Américas” in 1982), he proposes a theory which is {{deeply rooted in the}} musicological problematic of Latin America and a methodology which could be developed in comparative studies created within the Continent. The text is comprised by three chapters in which the author begins by analyzing general aspects related to the presence of the African population in Latin America, his contributions to music and to the socio-historical context within which those processes have evolved. In the second chapter he describes in detail the characteristics of the African rhythm style and its fusion with the Hispanic style. In the third chapter he develops his own personal conclusions about what he describes as: “the <b>binarization</b> process of the African ternary rhythms”, their specific behavior and the consequences of this process for the cultural development of the musical culture in Cuba and Latin America. His reflections are illustrated with numerous musical examples. In his second book, Pérez Fernández intends to demonstrate the importance of the African contribution to the integration of the music of Mexico, as well as to provide facts that may support, within the musicological field, the conclusions of Mexican anthropologist Gonzalo Aguirre Beltrán in reference to the concept that the Mexican creole music is fundamentally a result of the cultural fusion between the Spanish and the African population.|$|E
40|$|Conventional <b>Binarization</b> methods try {{to obtain}} optimal results {{based on the}} single image only. They make {{distinct}} diversity of <b>binarization</b> quality sometimes even for images of the same documents. Using a <b>binarization</b> evaluation and feedback mechanism, this paper proposed a learning-based <b>binarization</b> method which can improve the <b>binarization</b> of same-type document, especially in the quality stability. It has a learning and a performing <b>binarization</b> stage. Learning stage obtains knowledge for <b>binarization</b> evaluation and optimization. In performing stage, the evaluation of <b>binarization</b> result is fed back to <b>binarization</b> in order to adjust <b>binarization</b> parameters, which will improve the <b>binarization.</b> Experiments validate the improvement. 1...|$|E
40|$|Document <b>binarization</b> is {{an active}} {{research}} area for many years. There are many difficulties associated with satisfactory <b>binarization</b> of document images and especially in cases of degraded historical documents. In this paper, we try {{to answer the question}} “how well an existing <b>binarization</b> algorithm can binarize a degraded document image? ” We propose a new technique for the validation of document <b>binarization</b> algorithms. Our method is simple in its implementation and can be performed on any <b>binarization</b> algorithm since it doesn’t require anything more than the <b>binarization</b> stage. Then we apply the proposed technique to 30 existing <b>binarization</b> algorithms. Experimental results and conclusions are presented. 1...|$|E
40|$|The article {{describes}} methods of <b>binarization</b> {{and their impact}} on the fractal dimension of the image surfaces of porous materials. Software implemented methods and algorithms for determining the <b>binarization</b> threshold <b>binarization,</b> calculated the fractal dimension of the binary image, and analyzes the values of the fractal dimension of natural coatings using different methods of <b>binarization</b> to obtain <b>binarization</b> threshold adaptation algorithm to control the properties of porous materials...|$|E
40|$|Abstract — A {{number of}} <b>binarization</b> {{techniques}} {{have been proposed}} {{in the past for}} automatic document processing. Although some studies have aimed to evaluate the performance of <b>binarization</b> algorithms, there is no automatic system that is capable of selecting the most appropriate method of <b>binarization.</b> While preprocessing techniques can be applied, <b>binarization</b> is essential to extract the objects in the first place before the characters can be separated for recognition. Although there are several commonly used <b>binarization</b> approaches, there is no single algorithm that is suitable for all images. Hence, {{there is a need to}} determine the optimal <b>binarization</b> algorithm for each image. The objective of this paper is to present a survey of the existing methods of <b>binarization</b> and evaluation measurement which have been developed recently. This will lead to the proposal and development of an approach for automatic selection of <b>binarization</b> techniques in handling historical document images. Keywords- <b>binarization,</b> image Segmentation, evaluation measurement, quantitative measurement I...|$|E
40|$|This paper {{proposes a}} new method for <b>binarization</b> of digital documents. The {{proposed}} approach performs <b>binarization</b> {{by using a}} heuristic algorithm with two different thresholds and {{the combination of the}} thresholded images. The method is suitable for <b>binarization</b> of complex background document images. In experiments, it obtained better results than classical techniques in the <b>binarization</b> of real bank checks. Index Terms — <b>Binarization,</b> Documents with complex background, Document image processing, Automatic ban...|$|E
40|$|Most of the {{document}} <b>binarization</b> techniques have many parameters that must initially be specified. Usually, document <b>binarization</b> evaluation is subjective and employs human observers for the estimation of the proper set of parameters, for {{each one of the}} <b>binarization</b> techniques. The selection of the proper values for these parameters is crucial for the final <b>binarization</b> result. However, there is no set of parameters that guarantees the best <b>binarization</b> result for all document images. Thus, the proper parameters must be adapted to each one of {{the document}} images. This paper proposes a new technique which permits the estimation of the proper parameters values for each one {{of the document}} <b>binarization</b> techniques. The proposed approach is based on a statistical performance analysis of a set of <b>binarization</b> results obtained by the application of a <b>binarization</b> technique, using different parameters values. Using the statistical performance analysis, the best document <b>binarization</b> result of a set of document <b>binarization</b> techniques can also be estimated. From several experimental results, we verify that the proposed evaluation technique is successfully applied to different types of document images. In addition, psycho-visual experiments show that the selection of the best <b>binarization</b> technique, obtained by the proposed approach, agrees in most of the cases, with the human perception ability...|$|E
40|$|Abstract: Document <b>binarization</b> is {{an active}} {{research}} area for many years. The choice of the most appropriate <b>binarization</b> algorithm for each case {{proved to be a}} very difficult procedure itself. In this paper, we propose a new technique for the validation of document <b>binarization</b> algorithms. Our method is simple in its implementation and can be performed on any <b>binarization</b> algorithm since it doesn’t require anything more than the <b>binarization</b> stage. As a demonstration of the proposed technique, we use the case of degraded historical documents. Then we apply the proposed technique to 30 <b>binarization</b> algorithms. Experimental results and conclusions are presented...|$|E
40|$|<b>Binarization</b> of a {{gray scale}} {{document}} image {{is one of}} the most important steps for automatic document processing. The paper presents a two-stage document image <b>binarization</b> approach. The approach applies a region based <b>binarization</b> technique first to the whole image and utilizes a neural network based <b>binarization</b> technique to those text blocks in which a good character segmentation cannot be achieved at the first stage. Experimental results on a number of document images show that our two-stage <b>binarization</b> approach performs better than other <b>binarization</b> techniques in terms of character segmentation quality and computing timeDepartment of Electronic and Information EngineeringRefereed conference pape...|$|E
40|$|Document image <b>binarization</b> {{is often}} a {{challenging}} task due to various forms of degradation. Although there exist several <b>binarization</b> techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image <b>binarization</b> algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed <b>binarization</b> technique is empirically demonstrated on the Document Image <b>Binarization</b> Competition (DIBCO) and the Handwritten Document Image <b>Binarization</b> Competition (H-DIBCO) datasets...|$|E
40|$|<b>Binarization</b> is {{essential}} for achieving polynomial time complexities in parsing and syntax-based machine translation. This paper presents a new <b>binarization</b> scheme, target-side <b>binarization,</b> and compares it with source-side and synchronous binarizations on both stringbased and tree-based systems using synchronous grammars. In particular, we demonstrate the effectiveness of targetside <b>binarization</b> on a large-scale tree-tostring translation system. ...|$|E
40|$|Image <b>binarization</b> is {{important}} step in the OCR (Optical Character Recognition). There are several methods used for image <b>binarization</b> recently, {{but there is no}} way to select single or best method which is used for all images. The main objective of this paper is to present the study on various existing <b>binarization</b> algorithms and compared their measurements. This paper will act as guide for fresher’s to start their work on <b>binarization.</b> Keywords: <b>Binarization,</b> Threshold value, SNR, OC...|$|E
40|$|A {{number of}} <b>binarization</b> {{techniques}} {{have been proposed}} {{in the past for}} automatic document processing. Although some studies have aimed to evaluate the performance of <b>binarization</b> algorithms, there is no automatic system that is capable of selecting the most appropriate method of <b>binarization.</b> While preprocessing techniques can be applied, <b>binarization</b> is essential to extract the objects in the first place before the characters can be separated for recognition. Although there are several commonly used <b>binarization</b> approaches, there is no single algorithm that is suitable for all images. Hence, {{there is a need to}} determine the optimal <b>binarization</b> algorithm for each image. The objective of this paper is to present a survey of the existing methods of <b>binarization</b> and evaluation measurement which have been developed recently. This will lead to the proposal and development of an approach for automatic selection of <b>binarization</b> techniques in handling historical document images...|$|E
40|$|Most of the {{existing}} document-binarization techniques deal with many parameters that require a priori setting of their values. Due to the unknown of the ground-truth images, the evaluation of document <b>binarization</b> techniques is subjective and employs human observers for the estimation of the appropriate parameter values. The selection of the appropriate values for these parameters is crucial and influences to the final <b>binarization.</b> However, there is no predetermined set of parameters that guarantees optimal <b>binarization</b> for all document images. This paper proposes a new technique that allows the estimation of proper parameters values for {{each one of the}} document <b>binarization</b> techniques. The proposed approach is based on a statistical performance analysis of a set of <b>binarization</b> results, which are obtained by applying various <b>binarization</b> techniques with different parameter values. The proposed statistical performance analysis can also depicts the best document <b>binarization</b> result obtained by a set of document <b>binarization</b> techniques...|$|E
40|$|Image <b>binarization</b> {{is still}} a {{relevant}} research area due to its wide range of applications {{in the field of}} document analysis and recognition. Accuracy of <b>binarization</b> methods affected by many factors such as shadows non-uniform illumination, low contrast, large signal-dependent noise etc. This paper provides a comprehensive survey of major <b>binarization</b> techniques. We also emphasis on the problems being encountered and the related issues in the research area of document image <b>binarization.</b> In addition, some important issues affecting the performance of image <b>binarization</b> methods are also discussed. This literature review suggests that designing a suitable document image <b>binarization</b> method is a prerequisite for a successful document image analysis and recognition...|$|E
40|$|<b>Binarization</b> of {{gray scale}} {{document}} images {{is one of}} the most important steps in automatic document image processing. In this paper, we present a two-stage document image <b>binarization</b> approach, which includes a top-down region-based <b>binarization</b> at the first stage and a neural network based <b>binarization</b> technique for the problematic blocks at the second stage after a feedback checking. Our two-stage approach is particularly effective for binarizing text images of highlighted or marked text. The region-based <b>binarization</b> method is fast and suitable for processing large document images. However, the block effect and regional edge noise are two unavoidable problems resulting in poor character segmentation and recognition. The neural network based classifier can achieve good performance in two-class classification problem such as the <b>binarization</b> of gray level document images. However, it is computationally costly. In our two-stage <b>binarization</b> approach, the feedback criteria are employed to keep the well binarized blocks from the first stage <b>binarization</b> and to re-binarize the problematic blocks at the second stage using the neural network binarizer to improve the character segmentation quality. Experimental results on a number of document images show that our two-stage <b>binarization</b> approach performs better than the single-stage <b>binarization</b> techniques tested in terms of character segmentation quality and computational cost. Department of Electronic and Information Engineerin...|$|E
40|$|Abstract. In {{this paper}} we present two methods of <b>binarization</b> of corneal endothelial images. The <b>binarization</b> {{is a first}} step of {{advanced}} image analy-sis. Images of corneal endothelial obtained by the specular microscopy have a poor dynamic range and they are usually non-uniformly illuminated. The <b>binarization</b> endothelial images is not trivial. Two <b>binarization</b> algorithms are proposed. The output images are presented. The quality of algorithms is discussed...|$|E
40|$|International audienceThe {{document}} <b>binarization</b> is {{a fundamental}} processing step toward Optical Character Recognition (OCR). It aims to separate the foreground text from the document background. In this article, we propose a novel <b>binarization</b> technique combining local and global approaches using the clustering algorithm Kmeans. The proposed Hybrid <b>Binarization,</b> based on Kmeans (HBK), performs a robust <b>binarization</b> on scanned documents. According to several experiments, we demonstrate that the HBK method improves the <b>binarization</b> quality while minimizing the amount of distortion. Moreover, it out-performs several well-known {{state of the art}} methods in the OCR evaluation...|$|E
40|$|The {{effects of}} {{different}} image pre-processing methods for document image <b>binarization</b> are explored. They are compared on five different <b>binarization</b> methods on images with bleed through and stains {{as well as}} on images with uniform background speckle. The <b>binarization</b> method is significant in the <b>binarization</b> accuracy, but the pre-processing also plays a significant role. The Total Variation method of pre-processing shows the best performance over a variety of pre-processing methods...|$|E
40|$|Image <b>binarization</b> {{has a large}} {{effect on}} the rest of the {{document}} image analysis processes in character recognition. Algorithm development is still a major focus of research. Evaluation of image <b>binarization</b> has been done by comparison of the result of OCR systems on images binarized by different methods. That has been criticized in that the <b>binarization</b> alone is not evaluated, but rather how it interacts with the downstream processes. Recently pixel accurate 2 ̆ 2 ground truth 2 ̆ 2 images have been introduced for use in <b>binarization</b> algorithm evaluation. This has been shown to be open to interpretation. The choice of <b>binarization</b> ground truth affects the <b>binarization</b> algorithm design, either directly if design is by automated algorithm trying to match the provided ground truth, or indirectly if human designers adjust their designs to perform better on the provided data. Three variations in pixel accurate ground truth were used to train a <b>binarization</b> classifier. The performance can vary significantly depending on choice of ground truth, which can influence <b>binarization</b> design choices...|$|E
40|$|Abstract—Document Image <b>Binarization</b> {{techniques}} {{have been studied}} for many years, and many practical <b>binarization</b> {{techniques have}} been developed and applied successfully on commercial document analysis systems. However, the current state-of-the-art methods, fail to produce good <b>binarization</b> results for many badly degraded document images. In this paper, we propose a self-training learning framework for document image <b>binarization.</b> Based on reported <b>binarization</b> methods, the proposed framework first divides document image pixels into three categories, namely, foreground pixels, background pixels and uncertain pixels. A classifier is then trained by learning from the document image pixels in the foreground and background categories. Finally, the uncertain pixels are classified using the learned pixel classifier. Extensive experiments have been conducted over the dataset that {{is used in the}} recent Document Image <b>Binarization</b> Contest(DIBCO) 2009. Experimental results show that our proposed framework significantly improves the performance of reported document image <b>binarization</b> methods. Keywords-document image binarization; image pixel classification; self-training learning framework; I...|$|E
40|$|Abstract — Historical and {{degraded}} documents have varying background due to {{poor and}} uneven illumination, ageing of paper etc. Global thresholding methods give poor <b>binarization</b> results for these documents. Local <b>binarization</b> methods which are adaptive give enhanced results. Many image <b>binarization</b> algorithms and techniques are {{proposed in the}} literature. In this paper a new <b>binarization</b> technique is proposed and is compared with few standard techniques. The varying background is modeled based on the observation that the text pixels constitute less compared to the background pixels in document images. This technique is faster due to fewer computations and shows improved binarized image compared to standard adaptive <b>binarization</b> methods...|$|E
