67|181|Public
50|$|An {{operational}} taxonomic unit, or an OTU, {{allows a}} microbiologist {{to define a}} bacterial taxa using defined similarity <b>bins</b> <b>based</b> on a gene of interest. In microbial ecology, the small subunit ribosomal RNA gene is generally used at a cut off of 97% similarity to define an OTU. In the most basic sense, the OTU represents a bacterial species.|$|E
5000|$|Being a {{constraint}} on x-bar theory, the criterion aims to parse out ill-formed sentences. Thus, {{if the number}} or categories of arguments in a sentence {{does not meet the}} theta-role assigner's requirement in any given sentence, that sentence will be deemed ungrammatical[...] In other words, theta-criterion sorts sentences into grammatical and ungrammatical <b>bins</b> <b>based</b> on c-selection and s-selection.|$|E
5000|$|If f has d one’s and N-d zero’s, then Eve creates all {{possible}} [...] and [...] {{in which they}} both have length [...] (e.g. [...] covers the [...] lowest coefficients of f and [...] the highest)with d/2 one’s. Then she computes [...] for all [...] and orders them in <b>bins</b> <b>based</b> on the first k coordinates. After that she computes all [...] and orders them in bins not only based on the first k coordinates, but also based on {{what happens if you}} add 1 to the first k coordinates. Then you check the bins that contain both [...] and [...] and see if the property [...] holds.|$|E
50|$|Similar to {{frequency}} binning, products {{may also}} be <b>binned</b> <b>based</b> upon the number of cores which are enabled. As with overclocking, some chips may have more cores than marketed. It may be possible for the end user to enable these cores.|$|R
40|$|Abstract—There are {{two common}} types of {{encoding}} paradigms in multiple descriptions (MD) coding: i) an approach based on conditional codebook generation, which was originally initiated by El-Gamal and Cover for the 2 channel setting and later extended {{to more than}} 2 channels by Venkataramani, Kramer and Goyal (VKG), ii) and an approach based on Slepian and Wolf’s random binning technique, proposed by Pradhan, Puri and Ramchandran (PPR) for L> 2 descriptions. It {{is well known that}} the achievable region due to PPR subsumes the VKG region for the symmetric Gaussian MD problem. Motivated by several practical advantages of random <b>binning</b> <b>based</b> methods over the conditional codebook encoding, this paper focuses on the following important questions: Does a random <b>binning</b> <b>based</b> scheme achieve the performance of conditional codebook encoding, even for the 2 descriptions scenario? Are random <b>binning</b> <b>based</b> approaches beneficial for settings that are not fully symmetric? This paper answers both these questions in the affirmative. Specifically, we propose a 2 descriptions coding scheme, <b>based</b> on random <b>binning,</b> which subsumes the currently known largest region for this problem due to Zhang and Berger. Moreover, we propose its extensions to L> 2 channels and derive the associated achievable regions. The proposed scheme enjoys the advantages of both encoding paradigms making it particularly useful when there is symmetry only within a subset of the descriptions. Index Terms—Multiple description coding, source coding, ratedistortion theory I...|$|R
40|$|Efficient use of {{available}} spectrum {{is of concern}} to future wireless network planners. Although global cooperation at access points (APs) maximizes sum rate, for large networks this assumption is too complex to implement. We partition the wireless network into localized jointly decoded cells implemented as fixed size clusters. This is an alternative model which is more practical and improves efficiency of current systems. Conventionally, frequency allocation using interference avoidance maximizes spectrum usage. However, with clusters, careful allocation of interference needs to be explored. This is done using flexible <b>bin</b> <b>based</b> frequency allocation and applying heuristic tools. This work is the first known attempt to analyze uplink capacity of <b>bin</b> <b>based</b> fixed cluster cellular systems using genetic algorithms. To implement this, we derive an expression for the uplink capacity of <b>bin</b> <b>based</b> fixed clusters. We then input this as a fitness function to a modified simple genetic algorithm to compute {{a good fit for}} our bin allocation problem. We deduce that for sparsely distributed APs and large cluster sizes, rates close to that of a joint processor are achievable. Moreover, decreasing AP density for small cluster sizes (inter cell distance greater than 5 km for 7 cell-cluster) has insignificant effect on sum rate performance. However, with a nominal {{increase in the number of}} bins available for transmission, for dense system, the per-cell sum rate of a clustered cellular system can reach close to that of a hyper receiver using a genetic algorithm...|$|R
5000|$|There {{is another}} mode of assembly, called [...] "selective assembly", which gives {{up some of}} the {{randomness}} capability in trade-off for other value. There are two main areas of application that benefit economically from selective assembly: when tolerance ranges are so tight that they cannot quite be held reliably (making the total randomness unavailable); and when tolerance ranges can be reliably held, but the fit and finish of the final assembly is being maximized by voluntarily giving {{up some of the}} randomness (which makes it available but not ideally desirable). In either case the principle of selective assembly is the same: parts are selected for mating, rather than being mated at random. As the parts are inspected, they are graded out into separate <b>bins</b> <b>based</b> on what end of the range they fall in (or violate). Falling within the high or low end of a range is usually called being heavy or light; violating the high or low end of a range is usually called being oversize or undersize. Examples are given below.|$|E
30|$|Bin packing [11] (BP), where hosting {{and network}} {{requirements}} are mapped using a Bin per type of media cloud resource. The Bin packing in [11] introduces {{a method for}} forming and classifying <b>Bins</b> <b>based</b> on the resources available. Using the information of Bin classification, the incoming requests are mapped accordingly. The pseudo code for mapping of incoming requests using Bin packing is shown in Algorithm 1.|$|E
30|$|For our analysis, we {{calculated}} entropy scores with the 10 % academic Sample API data. For the 10 % academic Sample API, Tweets in the millisecond range [657 – 756] are collected. First, milliseconds for all Tweets {{in this sample}} per user were extracted and grouped into 10 <b>bins</b> <b>based</b> on milliseconds, i.e. 657 – 666, 667 – 676, etc. Here, the first bin equals the 1 % Sample API. With the frequencies of Tweets in these 10 bins the entropy scores were calculated for {{every day of the}} 61 days observation period.|$|E
3000|$|... can be {{calculated}} from Eqs. (6), (13), and (14), respectively. We determined these values for both models using the nine data sets. Figure  6 shows the frequency distribution of microboudinaged tourmaline grains, which is characterized {{as the number of}} microboudinaged (grey bar) and intact grains (white bar) for each aspect ratio <b>bin.</b> <b>Based</b> on the maximum likelihood estimation applied to the data displayed in Fig.  6, we obtained the p [...]...|$|R
50|$|Shortly {{after the}} war, Mac Jauncy built a {{gristmill}} in the forks of Calimese Creek so {{people of the}} community could grind their own meal. This gristmill was powered by a single horse, and had a long sweep pole, which was common for sorghum mills in the area. Even though Mac Jauncy wasn't present to collect any fees, every customer would leave some of their meal in a <b>bin</b> <b>based</b> on the honor system.|$|R
50|$|There {{are also}} two {{licensed}} shows on Fun Kids, Beanotown based on The Beano and <b>Bin</b> Weevils <b>based</b> on the <b>Bin</b> Weevils online game.|$|R
30|$|Greedy node mapping {{combined}} with a K-shortest path algorithm for the link mapping phase (Multi-Site) [18]. The main difference between the greedy approach and Bin packing is that we classify <b>Bins</b> <b>based</b> on the resources. With the greedy approach, the model follows some of the well-known queue approaches such as First In First Out. We arrange the incoming requests in ascending order of their cost. Cost {{is defined by the}} function described in equation (2). The requests then are mapped to the MEC-DCs infrastructure. The pseudo-code for the resource allocation is shown in Algorithm 2.|$|E
40|$|We outline how redshift-space distortions (RSD) can be {{measured}} from the angular correlation function w(θ), of galaxies selected from photometric surveys. The natural degeneracy between RSD and galaxy bias can be minimized by comparing results from bins with top-hat galaxy selection in redshift, and <b>bins</b> <b>based</b> on the radial position of galaxy pair centres. This comparison {{can also be used}} to test the accuracy of the photometric redshifts. The presence of RSD will be clearly detectable with the next generation of photometric redshift surveys. We show that the Dark Energy Survey (DES) will be able to measure f(z) σ_ 8 (z) to a 1 σ accuracy of (17 × b) ...|$|E
40|$|We {{present a}} {{measurement}} of the electron charge asymmetry in pp¯→W+X→eν+X events at a center-of-mass energy of 1. 96 TeV, using data corresponding to 9. 7 fb− 1 of integrated luminosity collected with the D 0 detector at the Fermilab Tevatron Collider. The asymmetry is measured {{as a function of}} the electron pseudorapidity and is presented in five kinematic <b>bins</b> <b>based</b> on the electron transverse energy and the missing transverse energy in the event. The measured asymmetry is compared with next-to-leading-order predictions in perturbative quantum chromodynamics and provides accurate information for the determination of parton distribution functions of the proton. This is the most precise lepton charge asymmetry measurement to date...|$|E
30|$|A taxa summary {{summarizes}} the relative abundance of different taxonomic levels (from phylum to genus) among all samples {{based on an}} OTU table. Sequences are taxonomically <b>binned</b> <b>based</b> on the output of a local copy of the ribosomal database project (RDP) classifier. Normalized data are produced from the relative abundances of taxa present in each sample. Any unclear taxa are combined and named “other.” The results from the taxonomic binning of classified sequences are displayed as bar charts, which {{make it easier to}} convey the main compositions of the samples.|$|R
3000|$|... a) values (Yoshioka et al. 1987, 1988), {{the present}} study was {{designed}} to evaluate feasibility of the method for compound <b>binning</b> <b>based</b> on kinetic t 90 % (time for 10 % degradation) and t 98 % (time for 2 % loss of potency) values. Compound binning refers to the categorization of compounds into three groups: those which are highly stable and unlikely to present major challenges when it comes to drug stability (BIN 1), those which may be problematic with respect to chemical stability (BIN 2), or those which are chemically unstable and unlikely to have an adequate shelf-life (BIN 3).|$|R
30|$|In this paper, a new blind speech {{separation}} algorithm in {{the frequency}} domain was developed for the three-speaker environment. Since, {{the position of the}} sources are unknown, the SRP-PHAT localization is used for estimating the spatial location of all speakers in each frequency <b>bin.</b> <b>Based</b> on that information, an optimum beamformer is designed for each speech source to extract the desired signal. The permutation alignment is used before transforming the signals to the time domain. Simulation results show that the proposed blind speech separation algorithm offers a good interference suppression level whilst maintaining a low distortion level for the desired source.|$|R
40|$|Abstract—In {{this paper}} {{we present a}} method for {{estimating}} the bin-wise separation performance in convolutive blind source separation. A common way to separate convolutive mixtures is the transformation to the time-frequency domain and {{the separation of the}} single bins using an instantaneous ICA algorithm. This approach reduces the complexity but leads to the so-called permutation problem which has been widely studied. Another problem arises when the single bins are only poorly separable or even not separable at all. These bins can significantly reduce the overall performance. In this paper we propose a method for detecting such <b>bins</b> <b>based</b> on properties of the unmixing matrices. I...|$|E
40|$|International audienceThis paper {{deals with}} the problem of under-determined con- volutive blind source separation. We model the {{contribution}} of each source to all mixture channels in the time-frequency domain as a zero-mean Gaussian random variable whose covariance encodes the spatial properties of the source. We consider two covariance models and address the estimation of their parameters from the recorded mixture by a suitable initialization scheme followed by an iterative expectation- maximization (EM) procedure in each frequency bin. We then align the order of the estimated sources across all fre- quency <b>bins</b> <b>based</b> on their estimated directions of arrival (DOA). Experimental results over a stereo reverberant speech mixture show the effectiveness of the proposed approach...|$|E
40|$|Software & Copyright Submittal The tool used {{to analyze}} the {{progression}} of accidents in the DWPF is called an Accident Progression Event Tree (APET). The APET methodology groups analyzed progressions into a series of <b>bins,</b> <b>based</b> on similarities in their characteristics. DWPFASTXL is an Excel spreadsheet {{that can be used to}} calculate radiological source terms and consequences for these accident progression bins. This document presents the calculations used in version 2. 0 of the DWPFASTXL spreadsheet. This revision of DWPFASTXL has been written to complete the debugging of version 1. 0, and to reconfigure the spreadsheet to model the new bin attribute table developed for the latest revision of the DWPF safety analyse...|$|E
40|$|Include Shane's macros as 'legacy' for {{comparison}} Read L 1 TNtuples in python Transfer 1 Macro to python (makeJetResolutions) Benchmark legacy vs new Add histogram collections for easier creation & handling Added multidimensional dictionary based on defaultdict Added HistogramByPileUpCollection Automatic selection of PU <b>bin</b> <b>based</b> on pileup value. E. g. histograms[11] will fill the 2 nd bin if pileupBins=[0, 10, 20, 30, 999] Added ResolutionCollection Specialisation of HistogramByPileUpCollection Automatic selection of detector region based on cmsl 1 t. geometry Implement Ben's MET turnons to check if {{the package is}} going the right way Explore ways to recalculate MET Implement TurnOnCollection EfficiencyCollectio...|$|R
5000|$|... 2. Choose {{a context}} model for each bin. One of 3 models is {{selected}} for <b>bin</b> 1, <b>based</b> on previous coded MVD values. The L1 norm of two previously-coded values, ek, is calculated: ...|$|R
50|$|Statistical {{distributions}} reveal trends {{based on}} how numbers are distributed. Common examples include histograms and box-and-whisker plots, which convey statistical features such as mean, median, and outliers. In addition to these common infographics, alternatives include stem-and-leaf plots, Q-Q plots, scatter plot matrices (SPLOM) and parallel coordinates. For assessing a collection of numbers and focusing on frequency distribution, stem-and-leaf plots can be helpful. The numbers are <b>binned</b> <b>based</b> on the first significant digit, and within each stack <b>binned</b> again <b>based</b> on the second significant digit. On the other hand, Q-Q plots compare two probability distributions by graphing quantiles against each other. This allows the viewer {{to see if the}} plot values are similar and if the two are linearly related. SPLOM is a technique that represents the relationships among multiple variables. It uses multiple scatter plots to represent a pairwise relation among variables. Another statistical distribution approach to visualize multivariate data is parallel coordinates. Rather than graphing every pair of variables in two dimensions, the data is repeatedly plotted on a parallel axis and corresponding points are then connected with a line. The advantage of parallel coordinates is that they are relatively compact, allowing many variables to be shown simultaneously.|$|R
30|$|Dalal and Triggs {{introduce}} HOG descriptor {{which takes}} weighted votes {{depending on the}} gradient L 2 -norm for an orientated histogram channel [31]. HOG descriptor consists of several steps. The image is divided into small connected regions (e.g., 8 × 8 pixels) named as cells, and a histogram of gradient orientations is computed (e.g., using 1 D centered derivative mask [− 1, 0,+ 1]) for the pixels within each cell. Each cell is quantized into angular <b>bins</b> <b>based</b> on the gradient orientation. The pixels in each cell are used as a weighted gradient to the corresponding angular bin. The frequencies of histogram are also normalized using L 2 -norm to adapt with the variation of illumination. The final HOG descriptor is represented by combining these histograms.|$|E
40|$|This paper {{deals with}} the problem of under-determined convolutive blind source separation. We model the {{contribution}} of each source to all mixture channels in the time-frequency domain as a zero-mean Gaussian random variable whose covariance encodes the spatial properties of the source. We consider two covariance models and address the estimation of their parameters from the recorded mixture by a suitable initialization scheme followed by an iterative expectationmaximization (EM) procedure in each frequency bin. We then align the order of the estimated sources across all frequency <b>bins</b> <b>based</b> on their estimated directions of arrival (DOA). Experimental results over a stereo reverberant speech mixture show the effectiveness of the proposed approach. Index Terms — Convolutive blind source separation, under-determined mixtures, spatial covariance models, EM algorithm, permutation problem. 1...|$|E
40|$|This article {{addresses}} the modeling of reverberant recording environments {{in the context}} of under-determined convolutive blind source separation. We model the contribution of each source to all mixture channels in the time-frequency domain as a zero-mean Gaussian random variable whose covariance encodes the spatial characteristics of the source. We then consider four specific covariance models, including a full-rank unconstrained model. We derive a family of iterative expectationmaximization (EM) algorithms to estimate the parameters of each model and propose suitable procedures to initialize the parameters and to align the order of the estimated sources across all frequency <b>bins</b> <b>based</b> on their estimated directions of arrival (DOA). Experimental results over reverberant synthetic mixtures and live recordings of speech data show the effectiveness of the proposed approach...|$|E
40|$|A {{method for}} the {{efficient}} re-binning and shading based correction of intensity distributions {{of the images}} prior to normalized mutual information based registration is presented. Our intensity distribution re-binning method {{is based on the}} K-means clustering algorithm as opposed to the generally used equidistant binning method. K-means clustering is a binning method with a variable size for each bin which is adjusted to achieve a natural clustering. Furthermore, a shading correction method is applied to reduce the effect of intensity inhomogeneities in MR images. Registering clinical shading corrected MR images to PET images using our method shows that a significant reduction in computational time without loss of accuracy as compared to the standard equidistant <b>binning</b> <b>based</b> registration is possible. © Springer-Verlag Berlin Heidelberg 2003...|$|R
40|$|Abstract. Color histograms {{are widely}} used for visual {{tracking}} due to their robustness against object deformations. However, traditional histogram representation often suffers from problems of partial occlusion, background cluttering and other appearance corruptions. In this paper, we propose a probabilistic index histogram to improve the discriminative power of the histogram representation. With this modeling, an input frame is translated into an index map whose entries indicate indexes to a separate <b>bin.</b> <b>Based</b> on the index map, we introduce spatial information and the bin-ratio dissimilarity in histogram comparison. The proposed probabilistic indexing technique, together with the two robust measurements, greatly increases the discriminative power of the histogram representation. Both qualitative and quantitative evaluations show the robustness of the proposed approach against partial occlusion, noisy and clutter background. ...|$|R
40|$|In {{this paper}} the {{influence}} of intensity clustering and shading correction on mutual information based image registration is studied. Instead of the generally used equidistant re-binning, we use k-means clustering {{in order to achieve}} a more natural binning of the intensity distribution. Secondly, image inhomogeneities occurring notably in MR images can have adverse effects on the registration. We use a shading correction method in order to reduce these effects. The method is validated on clinical MR, CT and PET images, as well as synthetic MR images. It is shown that by employing clustering with inhomogeneity correction the number of misregistrations is reduced without loss of accuracy thus increasing robustness as compared to the standard non-inhomogeneity corrected and equidistant <b>binning</b> <b>based</b> registration. © 2005...|$|R
40|$|Low-energy pi+ (E 150 MeV) with intermediate-mass {{fragments}} {{were determined}} from the measured coincidence data. The deduced correlation functions 1 + R ≈ 1. 3 for inclusive event samples reflect the strong correlations {{evident from the}} common impact-parameter dependence of the considered multiplicities. For narrow impact-parameter <b>bins</b> (<b>based</b> on charged-particle multiplicity), the correlation functions are close to unity and do not indicate strong additional correlations. Only for pions at high particle multiplicities (central collisions) a weak anticorrelation is observed, probably due to a limited competition between these emissions. Overall, the {{results are consistent with}} the equilibrium assumption made in statistical multifragmentation scenarios. Predictions obtained with intranuclear cascade models coupled to the Statistical Multifragmentation Model are in good agreement with the experimental data. Comment: 9 pages, 11 figures, subm. to EPJ...|$|E
40|$|Fermilab BTeV Level 1 trigger {{system will}} be presented. The hash sorter {{examines}} track-segment data before the data are sent to a system comprised of 2500 Level 1 processors, and rearranges the data into <b>bins</b> <b>based</b> on the slope of track segments. We have found that by using the rearranged data, processing time is significantly reduced allowing {{the total number of}} processors required for the Level 1 trigger system to be reduced. The hash sorter can be implemented in an FPGA that is already included as part of the design of the trigger system. Hash sorting has potential applications in a broad area in trigger and DAQ systems. It is a simple O(n) process and is suitable for FPGA implementation. Several implementation strategies will also be discussed in this document...|$|E
40|$|See {{paper for}} full list of authors – 28 pages, 17 figures, Submitted to Phys. Rev. DInternational audienceWe present a {{measurement}} of the electron charge asymmetry in pp̅→ W+X → eν +X events at a center-of-mass energy of 1. 96 TeV, using data corresponding to 9. 7 fb^- 1 of integrated luminosity collected with the D 0 detector at the Fermilab Tevatron Collider. The asymmetry is measured {{as a function of}} the electron pseudorapidity and is presented in five kinematic <b>bins</b> <b>based</b> on the electron transverse energy and the missing transverse energy in the event. The measured asymmetry is compared with next-to-leading-order predictions in perturbative quantum chromodynamics and provides accurate information for the determination of parton distribution functions of the proton. This is the most precise lepton charge asymmetry measurement to date...|$|E
5000|$|... #Caption: Inside a {{recently}} started bokashi <b>bin.</b> The aerated <b>base</b> is just {{visible through the}} food scraps and bokashi bran.|$|R
40|$|Abstract—We study {{sequential}} coding of a Markov source pro-cess under {{error propagation}} constraints. The channel can erase up to B packets {{in a single}} burst, but reveals all other packets to the destination. The destination is required to reproduce all the (vector) source sequences sequentially, except those that occur in a window of length B+W following {{the start of the}} erasure burst. Our earlier work establishes upper and lower bounds on the compression rate as a function of B and W. In this work we show that for the class of symmetric sources, if we restrict to a memoryless encoding function, then a <b>binning</b> <b>based</b> scheme is optimal. Our converse involves a drawing connection between the sequential coding problem and a multiuser source coding problem called Zig-Zag source coding with side information. I...|$|R
40|$|Abstract — We {{present an}} {{approach}} for blind separation of acoustic sources produced from multiple speakers mixed in realistic room environments. We first transform recorded signals into the time-frequency domain to make mixing become instantaneous. We then separate the sources in each frequency <b>bin</b> <b>based</b> on an independent component analysis (ICA) algorithm. For the present paper, we choose the complex version of fixedpoint iteration (CFPI), i. e. the complex version of FastICA, as the algorithm. From the separated signals {{in the time}}-frequency domain, we reconstruct output-separated signals in the time domain. To solve the so-called permutation problem due to the indeterminacy of permutation in the standard ICA, we propose a method that applies a special property of the CFPI cost function. Generally, the cost function has several optimal points that correspond to the different permutations of th...|$|R
