9|29|Public
50|$|Many {{authors have}} {{augmented}} the <b>basic</b> <b>counter</b> machine model instruction set, {{with a more}} complex instructions, {{for this kind of}} studies.|$|E
50|$|Cordon {{and search}} is a {{military}} tactic to cordon off an area and search the premises for weapons or insurgents. It {{is one of the}} <b>basic</b> <b>counter</b> insurgency operations. Two types of cordon and search operations are cordon and knock and cordon and kick (or cordon and enter).|$|E
40|$|Abstract. We ask the {{question}} – how can websites and data aggregators continually release updated statistics, and meanwhile preserve each individual user’s privacy? Given a stream of 0 ’s and 1 ’s, we propose a differentially private continual counter that outputs at every time step the approximate number of 1 ’s seen thus far. Our counter construction has error that is only poly-log {{in the number of}} time steps. We can extend the <b>basic</b> <b>counter</b> construction to allow websites to continually give top...|$|E
50|$|Post-Turing machine—minimalist one-tape, two-direction, 1 symbol { blank, mark } Turing-like machine {{but with}} default {{sequential}} instruction execution {{in a manner}} similar to the <b>basic</b> 3-instruction <b>counter</b> machines.|$|R
5000|$|Cinco Teros - five strikes, {{refers to}} the five most <b>basic</b> strikes and <b>counters</b> ...|$|R
50|$|Whitcomb's {{education}} {{includes the}} Infantry Officer <b>Basic</b> Course, <b>Counter</b> Intelligence Officer Course, the Armor Advance Course, Command and General Staff College, and the U.S. Army War College. His civilian education includes a Bachelor of Arts Degree in History from the University of Virginia and a master's {{degree in education}} from California University of Pennsylvania.|$|R
40|$|We ask the {{question}} – how can websites and data aggregators continually release updated statistics, and meanwhile preserve each individual user’s privacy? Suppose we are given a stream of 0 ’s and 1 ’s. We propose a differentially private continual counter that outputs at every time step the approximate number of 1 ’s seen thus far. Our counter construction has error that is only poly-log {{in the number of}} time steps. We can extend the <b>basic</b> <b>counter</b> construction to allow websites to continually give top-k and hot items suggestions while preserving users ’ privacy. ...|$|E
40|$|Track CLNCS v. 6199 is the {{proceedings}} of ICALP 2010 We ask the question – how can websites and data aggregators continually release updated statistics, and meanwhile preserve each individual user’s privacy? Suppose we are given a stream of 0 ’s and 1 ’s. We propose a differentially private continual counter that outputs at every time step the approximate number of 1 ’s seen thus far. Our counter construction has error that is only poly-log {{in the number of}} time steps. We can extend the <b>basic</b> <b>counter</b> construction to allow websites to continually give top-k and hot items suggestions while preserving users’ privacy. postprintThe 37 th International Colloquium on Automata, Languages and Programming (ICALP) 2010, Bordeaux, France, 5 - 10 July 2010. In Lecture Notes in Computer Science, 2010, v. 6199 pt. 2, p. 405 - 41...|$|E
40|$|Why do {{students}} usually {{get into a}} muddle when analysing legal situations in Hohfeldian terms? What {{is the use of}} trying to straighten out the muddles, and of teaching Hohfeldian analysis at all? The short answer to the first question is that Hohfeld was clear-headed in applying his scheme, but because of his writing style and his odd views about definition was regrettably gnomic about the meaning and inter-relations of the terms of that scheme. The short answer to the second question is that clear-headed familiarity with Hohfeld 2 ̆ 7 s scheme can bring with it an awareness of the questions regularly begged when 2 ̆ 2 claims of right 2 ̆ 2 are raised in law, politics, and moral debate, and thus some some awareness of the ambiguity, evasion, and overkill afflicting the Western debate since 2 ̆ 2 rights 2 ̆ 2 became the <b>basic</b> <b>counter</b> in discourse...|$|E
50|$|The {{station was}} built in a unique design that {{incorporated}} large semicircular roofs as shelters for platforms with multi-tiered roofs {{for the rest of}} the building, similar to the adjoining Sri Petaling Line station. Unlike both the Komuter halt or the Sri Petaling Line station, however, the station provides support for disabled passengers, with elevators and inclinations up and down uneven walkways, alongside stairways and escalators in all relevant areas of the station. The station also features two island platforms along four tracks, and <b>basic</b> ticket <b>counters.</b>|$|R
50|$|The <b>basic</b> {{methods of}} <b>countering</b> IMINT are {{to know when}} the {{opponent}} will use imaging against one's own side, and interfering with the taking of images. In some situations, especially in free societies, it must be accepted that public buildings may always be subject to photography or other techniques.|$|R
40|$|The {{large signal}} {{bistable}} properties of negative resistance devices are described and the simulation of negative resistance devices using positive feedback amplifiers is discussed. The {{form of an}} ideal characteristic for "unique bit " ring counter applications is suggested and a practical circuit that fulfils the requirements is proposed. Circuit details of a <b>basic</b> ring <b>counter,</b> a slave ring counter, and a shift register are given and important design criteria are presented. A method of generating interlaced sequential waveforms is described for a system using time division sub-multiplexing of input data. BLDG. 81...|$|R
40|$|Dried varnish {{is rich in}} many ester moieties, {{which may}} {{be broken down into}} small, soluble {{compounds}} by esterase activity or alkaline hydrolysis. Two methods for varnish removal have been developed, including the treatment of either lipase or RbOH / PEG- 400 crown ether which allow aged oil varnishes or paint coverings to be removed or thinned. These techniques are designed to proceed in a controlled manner without damaging lower paint or base layers. Unfortunately, lipase did not react with the aged ester groups of dried linseed oil varnish. Surprisingly, the varnish came off in the presence of Tris buffer alone which, in addition, formed reactive metal complexes. A better choice was the use of high Mr alkali ion polyethylene glycol– 400 (PEG- 400) crown ether type chelates. PEG- 400 complexes alkali ions including rubidium and other alkaliions impeding the diffusion of their <b>basic</b> <b>counter</b> ions into lower varnish or paint layers. Possible migration of alkali metal ions into the paint layer during alkaline varnish removal was determined by labelling the cleansing solutions with 86 Rb. Fortunately, varnish is degraded on the surface only. Lower paint or varnish layers are not attacked even if chemically similar to the varnish or over painting to be removed as virtually no 86 Rb was detected on the paint surface...|$|E
40|$|In {{the early}} phases of nuclear research, {{scintillation}} counters proved valuable for detection of alpha particles. In this technique, as perfected by Rutherford and his co-workers, scintillations {{produced by the}} excitation of fluorescent and phosphorescent screens were observed visually. The method has been revived in modern form, principally by Kallman, {{with the use of}} photomultiplier tubes for the observation of light pulses, and large, transparent crystals for the conversion of as much of the energy of a penetrating particle into light as possible. A wide range of fluorescent materials has been tested by various workers. On the basis of reported efficiency and response time, the work at this laboratory was initiated using naphthalene. The first experimental arrangement consisted of a 1 P 21 P. M. tube mounted in a glass container stuffed with moth flakes and immersed in liquid air as shown in. With radium gamma ray source, a ratio of about 20 was observed between the counting rate of this set-up as compared with that of the photomultiplier surface alone exposed to the gamma radiation under otherwise identical conditions. As a consequence of the sensitivity (about 10 per cent gamma counting efficiency) of even this crude and inelegant device, as well as of its very simplicity, a program was undertaken with the aim of devising a rugged and adaptable <b>basic</b> <b>counter</b> unit. This program received added impetus when it was discovered that the much greater usable light output of anthracene made possible the elimination of the refrigerant. With respect to pulse width, it will be seen that the fortuitous condition exists whereby the minimum, independently observed pulse width coincides with the maximum resolution of simple, fairly conventional electronic recording equipment. Work performed at the University of California Radiation Laboratory. "Date Declassified: August 5, 1948. ""AECD- 2203. "Includes bibliographical references (p. 10). In the early phases of nuclear research, scintillation counters proved valuable for detection of alpha particles. In this technique, as perfected by Rutherford and his co-workers, scintillations produced by the excitation of fluorescent and phosphorescent screens were observed visually. The method has been revived in modern form, principally by Kallman, with the use of photomultiplier tubes for the observation of light pulses, and large, transparent crystals for the conversion of as much of the energy of a penetrating particle into light as possible. A wide range of fluorescent materials has been tested by various workers. On the basis of reported efficiency and response time, the work at this laboratory was initiated using naphthalene. The first experimental arrangement consisted of a 1 P 21 P. M. tube mounted in a glass container stuffed with moth flakes and immersed in liquid air as shown in. With radium gamma ray source, a ratio of about 20 was observed between the counting rate of this set-up as compared with that of the photomultiplier surface alone exposed to the gamma radiation under otherwise identical conditions. As a consequence of the sensitivity (about 10 per cent gamma counting efficiency) of even this crude and inelegant device, as well as of its very simplicity, a program was undertaken with the aim of devising a rugged and adaptable <b>basic</b> <b>counter</b> unit. This program received added impetus when it was discovered that the much greater usable light output of anthracene made possible the elimination of the refrigerant. With respect to pulse width, it will be seen that the fortuitous condition exists whereby the minimum, independently observed pulse width coincides with the maximum resolution of simple, fairly conventional electronic recording equipment. Mode of access: Internet...|$|E
50|$|<b>Basic</b> {{banknote}} <b>counters</b> {{provide a}} total {{count of the}} notes in the supply hopper. More advanced counters can identify different bill denominations to provide a total currency value of mixed banknotes, including those that are upside down. Some banknote counters can also detect counterfeit bills either magnetically and/or using blacklight. Blacklight (UV) based detectors exploit {{the fact that in}} many countries, real banknotes have fluorescent symbols on them that only show under a black light. Also, the paper used for printing money does not contain any of the brightening agents which make commercially available papers fluoresce under black light. Both features make counterfeit notes both easier to detect and more difficult to successfully produce.|$|R
40|$|A {{new class}} of exact vortex dipole {{solutions}} is derived for surface quasi-geostrophic (sQG) models. The analysis extends the two-dimensional barotropic modon to fully three-dimensional, continuously stratified flow. Dipole structures exist {{for a variety of}} sQG configurations. In particular, the <b>basic</b> dipole propagates <b>counter</b> to the phase speed of linear Rossby waves in the presence of uniform background gradients of the surface potential temperature or planetary vorticity, and reduces to the barotropic Lamb dipole in the limit of a thin sQG layer...|$|R
40|$|Robust and {{powerful}} software instrumentation tools are es-sential for dynamic program analysis {{tasks such as}} profiling, performance evaluation, and bug detection. Dynamic binary instrumentation (DBI) is a general purpose technique that eases the development of program analysis tools by facili-tating automatic low-level instrumentation. DBI-based pro-gram analysis can introduce high overhead and it is crucial for tool writers to minimize the cost. Analyzing the per-formance of instrumentation tools is challenging because most systems use a just-in-time compiler (JIT) to dynami-cally generate code. In this paper, we describe our method for analyzing the performance of instrumentation tools. The instrumented code is itself instrumented with <b>basic</b> block <b>counters.</b> We implement the profiler in Pin {{and use it to}} an-alyze the behavior of simple and complex instrumentation tools. The analysis yields several unexpected results about the dynamic behavior of instrumented programs. By exam-ining these results, we often find effective solutions to im-prove performance. 1...|$|R
40|$|Abstract—Debugging {{wireless}} {{sensor network}} (WSN) applica-tions has been complicated for multiple reasons, among which the lack of visibility {{is one of the}} most challenging. To address this issue, in this paper, we present a systematic approach to record and replay WSN applications at the granularity of instructions. This approach differs from previous ones in that it is purely software based, therefore, no additional hardware component is needed. Our key idea is to combine the static, structural information of the assembly-level code with their dynamic, run-time traces as measured by timestamps and <b>basic</b> block <b>counters,</b> so that we can faithfully infer and replay the actual execution paths of applications at instruction level in a post-mortem manner. The evaluation results show that this approach is feasible despite of the resource constraints of sensor nodes. We also provide two case studies to demonstrate that our instruction level record-and-replay approach can be used to: (1) discover randomness of EEPROM writing time; (2) localize stack smashing bugs in sensor network applications. I...|$|R
40|$|The work of {{intrusion}} detection (ID) in accomplishing network security is complex, requiring highly sought-after expertise. While limited automation exists, {{the role of}} human ID analysts remains crucial. This paper {{presents the results of}} an exploratory field study examining the role of expertise and collaboration in ID work. Through an analysis of the common and situated expertise required in ID work, our results <b>counter</b> <b>basic</b> assumptions about its individualistic character, revealing significant distributed collaboration. Current ID support tools provide no support for this collaborative problem solving. The results of this research highlight ID as an engaging CSCW work domain, one rich with organizational insights, design challenges, and practical import...|$|R
40|$|Abstract. This {{paper has}} {{proposed}} an improved frequency conversion equal precision frequency cycle measuring method {{based on the}} <b>basic</b> principles of <b>counter</b> cycle measuring method, {{and it can be}} used to measure the frequency conversion signal, whose pulse width is much different from repetition period or whose repetition period is random jittering. A high precision real time frequency measurement experiment is completed on CPLD based on the proposed theory and simulation and actual testing have been carried out. The result shows that the proposed system has some advantages of high measurement precision and great real-time performance, and can be widely applied in the field of high precision real time pulse width measurement with low cost...|$|R
5000|$|Control Track Pulse: Most are {{familiar}} with the digital [...] "counters" [...] on VHS recorders and camcorders, viewed via the onscreen display (OSD) and/or a dedicated LED display. These numbers are sometimes in real time format (hours:minutes:seconds), but are often only an ambiguous 4 digit sequential counter. These numbers advance up or down based on the machine counting a tape's control track pulses. This type of display is useful only as a simple and temporary reference, as it is very inaccurate, and the counter is reset to zero when a tape is inserted. A <b>basic</b> 4 digit <b>counter</b> is almost completely worthless, as their rate of advance was never standardized by manufacturers.|$|R
5|$|Supporters of the book's <b>basic</b> {{accuracy}} <b>countered</b> on {{the points}} raised by skeptics such as footbinding and the Great Wall of China. Historian Stephen G. Haw {{argued that the}} Great Walls were built to keep out northern invaders, whereas the ruling dynasty during Marco Polo's visit were those very northern invaders. They note that the Great Wall familiar to us today is a Ming structure built some two centuries after Marco Polo's travels; and that the Mongol rulers whom Polo served controlled territories both {{north and south of}} today's wall, and would have no reasons to maintain any fortifications that may have remained there from the earlier dynasties. Other Europeans who travelled to Khanbaliq during the Yuan Dynasty, such as Giovanni de' Marignolli and Odoric of Pordenone, said nothing about the wall either. The Muslim traveler Ibn Batutta, who asked about the wall when he visited China during the Yuan Dynasty, could find no one who had either seen it or knew of anyone who had seen it, suggesting that while ruins of the wall constructed in the earlier periods might have existed, they were not significant or noteworthy at that time.|$|R
40|$|The {{objectives}} of this module are to: # Become familiar with <b>basic</b> digital <b>counter</b> concepts # Test counter circuits using circuit simulation software # Build digital circuits using prototyping equipment To accomplish the objectives, {{we will give}} you a short overview of electrical and digital logic concepts. You will then build circuits described later in the handout using the MultiSim simulation program. A variety of circuits are described; however, you may not have time to complete the simulation of all of them. The simulation program we will be using is MultiSim 8 by Electronics Workbench. www. electronicsworkbench. com. It is a user-friendly program which a user can create and test circuits in a relatively short time. It has a variety of components listed in a series of menus. One of the initial difficulties is finding the component you want. The circuits we are going to build start on page 11. How to Find Components A list of component icons is shown along with the menu selections used to access them. The order of presentation {{is the same as the}} order in which you will be constructing the circuits. If your menu toolbar only shows icons without the text which identifies them, you need to turn on the text so the toolbar is as shown below...|$|R
40|$|Present {{investigation}} {{focuses on}} unsteady flow structures in end-view planes at {{the trailing edge of}} delta wing, X/C= 1. 0, where consequences of vortex bursting and stall phenomena {{vary according to}} angles of attack over the range of 25 ° ≤ α ≤ 35 ° and yaw angles, β over the range of 0 ° ≤ β ≤ 20 °. <b>Basic</b> features of <b>counter</b> rotating vortices in end-view planes of delta win with 70 ° sweep angle, Λ are examined both qualitatively and quantitatively using Rhodamine dye and the PIV system. In the light of present experiments it is seen that with increasing yaw angle, β symmetrical flow structure is disrupted continuously. Dispersed wind-ward side leading edge vortices cover a large part of flow domain, on the other hand, lee-ward side leading edge vortices cover {{only a small portion of}} flow domain...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of the Mobile Objects and Agents (MOA) project at the Open Group Research Institute. MOA was designed to support migration, communication and control of agents. It was implemented on top of the Java Virtual Machine, without any modifications to it. The initial project goals were to support communication across agent migration, as a means for collaborative work; and to provide extensive resource control, as a <b>basic</b> support for <b>countering</b> denial of service attacks. In the course of the project we added two further goals: compliance with the Java Beans component model which provides for additional configurability and customization of agent system and agent applications; and interoperability which allows cooperation with other agent systems. This paper analyzes the architecture of MOA, in particular the support for mobility, naming and locating, communication, and resource management. Object and component models of MOA ar [...] ...|$|R
40|$|Abstract- With the {{transition}} from buses to on-chip networks in SoCs the problem of deadlocks in on-chip interconnects arises. Deadlocks {{can be caused by}} routing cycles in the network, or by message dependencies, even if the network itself is actually free of routing cycles. Two <b>basic</b> approaches to <b>counter</b> message dependent deadlocks exist: deadlock avoidance, which is most popular in NoCs, and deadlock recovery, which has until now only been used in parallel computer networks. Deadlock recovery promises low buffer space requirements and does not impose restrictions on connections between individual communication partners. For this study, we have adapted a deadlock recovery scheme for the use in NoCs and compared it to strict ordering as a representative of deadlock avoidance in terms of throughput and buffer space. The results show significant buffer space savings for deadlock recovery, however, at the cost of reduced data throughput. Network-on-chip, message dependent deadlocks, deadlock recovery, deadlock avoidance, strict ordering I...|$|R
40|$|The {{objectives}} of this module are to: # Become familiar with <b>basic</b> digital <b>counter</b> concepts # Test counter circuits using circuit simulation software # Build digital circuits using prototyping equipment To accomplish the objectives, {{we will give}} you a short overview of electrical and digital logic concepts. You will then build circuits described later in the handout using the MultiSim simulation program. A variety of circuits are described; however, you may not have time to complete the simulation of all of them. The simulation program we will be using is MultiSim 8 by Electronics Workbench. www. electronicsworkbench. com. It is a user-friendly program which a user can create and test circuits in a relatively short time. It has a variety of components listed in a series of menus. One of the initial difficulties is finding the component you want. The circuits we are going to build start on page 11. Simulating Circuits The circuits which we will construct and simulate are digital. In a digital circuit we are only concerned if a signal is present or not. A switch {{is an example of a}} digital circuit. It is either on or off. Terms used for the presence of a signal are on, high, true, and one. Terms used for the absence of a signal are off, low, false and zero. Decimal Numbers in Computers Many applications require the use of a counter. The next circuit will count from 0 to 9 in binary. The binary number system uses base 2. Characters in the right column (sometimes labeled a) have the value 2 0. In the next column to the left (labeled b) they have the value 2 1. Th...|$|R
40|$|Since {{the dawn}} of life on Earth some four billions of years ago, gravity has been {{a more or less}} stable {{environmental}} factor thus influencing the phylogenetic develop-ment of all living organisms. On the one side, gravity represents a factor of physical restriction, which compelled the ancestors of all extant living beings to develop <b>basic</b> achievements to <b>counter</b> the gravitational force (e. g., elements of statics like any kind of skeleton- from actin to bone- to overcome gravity enforced size limits or to keep form). On the other side, already early forms of life possibly used gravity as an appro-priate cue for orientation and postural control, since it is continuously present and has a fixed direction. Due to such a thorough adaptation to the Earthly gravity vector, both orientation behavior as well as the ontogenetic development of animals is impaired, when they have to experience altered gravity (Dg; i. e. hyper- or microgravity). Nevertheless, ani-mals still can cope with Dg in a certain range based on their physiological plasticity, which varies among the different animal phyla...|$|R
40|$|Abstract. The {{number and}} {{diversity}} of personal electronic gadgets have been steadily increasing {{but there has been}} fairly little progress in secure pairing of such devices. The pairing challenge revolves around establish-ing on-the-fly secure communication without any trusted (on- or off-line) third parties between devices that have no prior association. One <b>basic</b> approach to <b>counter</b> Man-in-the-Middle (MiTM) attacks in such setting is to involve the user in the pairing process. Previous research yielded some interesting secure pairing techniques, some of which ask too much of the human user, while others assume availability of specialized equip-ment (e. g., wires, photo or video cameras) on personal devices. Further-more, all prior methods assumed an established insecure channel over a common digital (human-imperceptible) communication medium, such as infrared, 802. 11 or Bluetooth. In this paper we introduce a very simple technique called HAPADEP (Human-Assisted Pure Audio Device Pairing). HAPADEP uses the au-dio channel to exchange both data and verification information among devices without requiring any other means of common electronic commu-nication. Despite its simplicity, a number of interesting issues arise in the design of HAPADEP. We discuss design and implementation highlights as well as usability features and limitations...|$|R
40|$|To {{what extent}} has the counter-culture and the core values and attitudes it embodies, {{particularly}} {{with reference to}} work and leisure, become accepted in American society? This thesis addresses itself to this question; it asks whether {{some or all of}} these values have diffused into American life. The counter culture is a movement which has called into question the traditional core values in American society. The counter culture was a cultural movement, which was counter to, i. e. in opposition to, the prevailing cultural values of the times. The counter culture then, is a body of values and attitudes toward life. As a cultural movement, it encompasses a broad range of values, and involved a period of experimentation in which alternatives abounded. The author examines one core concept, freedom. This concept is <b>basic</b> to the <b>counter</b> culture in that it underlies and is advocated in many of the values that surfaced in the 1960 's. To a large extent, freedom came to be symbolically identified with the counter culture. Methodology includes a discussion of the major values as they appeared in the counter culture, in particular the reference to the values of main-stream middle class society...|$|R
40|$|The {{number and}} {{diversity}} of electronic gadgets has been steadily increasing and they are becoming indispensable {{to more and more}} professionals and non-professionals alike. At the same time, there has been fairly little progress in secure pairing of such devices. The pairing challenge revolves around establishing on-the-fly secure communication without any trusted (on- or off-line) third parties between devices that have no prior association. The main security issue is the danger of so-called Man-in-the-Middle (MiTM) attacks, whereby an adversary impersonates one of the devices by inserting itself into the pairing protocol. One <b>basic</b> approach to <b>countering</b> these MiTM attacks is to involve the user in the pairing process. Therein lies the usability challenge since it is natural to minimize user burden. Previous research yielded some interesting secure pairing techniques, some of which ask too much of the human user, while others assume availability of specialized equipment (e. g., wires, photo or video cameras) on devices. Furthermore, all prior methods assumed the existence of a common digital (humanimperceptible) communication medium, such as Infrared, 802. 11 or Bluetooth. In this paper we introduce a very simple technique calle...|$|R
40|$|This master’s thesis {{contains}} {{design of}} practice exercises for microcontroller MCF 52233 firm Freescale. Exercises {{are focused on}} working with <b>basic</b> peripheries as <b>counters</b> and timers, measurement with A/D converters, PWM modulation or I 2 C bus. Exercises are interested in evaluation of matrix keyboard and communication with alphanumeric display controller. To these exercises is made a preparation of external peripherals, which is connected with evaluation kit M 52233 DEMO. This preparation contains all necessary peripheries, which are used for practice exercises – buttons, LED diodes, potentiometer and I 2 C periphery. Individual periphery usually shares connection to microcontroller with another periphery. Therefore preparation contains switches, which are necessary to set up for correct functionality of the peripherals. The board of external peripheral also includes ports for connecting matrix keyboards and alphanumeric display. Connecting the display requires an external power supply. Work deals with implementation of laboratory exercises using a software firm Freescale. Development environment for programming and basic debugging is in program CodeWarrior. More advanced possibility of reading information from a running program in the microcontroller offers program FreeMaster, which {{is used in the}} exercise of AD converters. Each lab exercise contains an introduction, task, instructions and the flowchart of realized exercise. Instructions are written so that it contains important steps to properly manage assignments. Individual exercises are designed to gradually increasing complexity of exercises students obtain basic programming habits and manage the complex exercise...|$|R
40|$|Thesis (M. Ing. (Mechanical Engineering)) [...] North-West University, Potchefstroom Campus, 2005. This study {{investigates the}} {{applicability}} of the thermal-fluid network approach to the modelling of transient heat exchanger performance. Two different solution algorithms, namely the Implicit Pressure Correction Method (IPCM) and the Runge Kutta method with Trapezoidal Damping (RKTD) for the solution of the one-dimensional governing equations in thermal-fluid network problems are presented. The advantages and disadvantages of two types of numerical discretisation schemes used in thermal-fluid network problems are discussed and the discretised one-dimensional governing equations for the staggered grid discretisation scheme used in the IPCM and RKTD method is presented. The RKTD method is used as a time integration scheme for the generalised thermal-fluid network solver Xnet. Several test cases are introduced and the basic primitive elements available in Xnet are compared to the commercial thermal-fluid network code, Flownex (which uses the IPCM), for both steady-state and transient conditions. Two different network topologies are introduced for the discretisation of heat exchangers when a network approach is followed and the thermal-fluid network solver Xnet is applied to a <b>basic</b> parallel and <b>counter</b> flow configured pipe-in-pipe type heat exchanger to investigate the effect on the type of discretisation scheme used. The results obtained are compared to primitive element models in Flownex as well as the composite RX element in Flownex. The extent to which thermal-fluid network solvers are able to predict transient heat exchanger performance are further investigated by modelling a complex shell-and-tube heat exchanger using Xnet and comparing the steady-state and transient results to both a primitive element model in Flownex as well as the composite STX element in Flownex. This contributes to the validation of Flownex’s heat exchanger models by using a different approach than Flownex. The results showed that the explicit method used in Xnet is capable of solving large arbitrary structured thermal-fluid networks {{with a high level of}} accuracy. The result of Flownex compares very well with that of Xnet, which proves (verifies) that the solution algorithm is correctly implemented in both codes. Even though the explicit thermal-fluid network code, Xnet, can accurately predict fast transients, a drawback of this method is the large computational time required to simulate transient heat exchangers with large thermal masses. Master...|$|R
40|$|Organisational {{structure}} {{is synonymous with}} a firm’s knowledge – both today and in respect of a firm’s future knowledge stocks. For some, this may seem obvious, yet for most scholars (and practitioners) {{this is not the}} case, as structure – particularly where the boundaries of the firm lie and what they look like – rarely makes it into any knowledge management discussion. Yet what a firm does today, be it broad in its activities or highly focussed {{to the point of being}} a virtual organisation, is both a reflection of its existing capabilities/routines (which are based around knowledge) as well as determining its likely learning and transformational opportunities into the future. In addition, the permeability of any organisational boundary and the existence of any mechanism to maximise the inflows of new knowledge are fundamental to developing new or reconfiguring existing capabilities. This paper therefore addresses how knowledge and structure and inextricably linked and through the use of a case study illustrates how a public sector organisation has significantly rebuilt its capabilities by rethinking its organisational boundaries, both in terms of location and basic characteristics. The determination of organisational boundaries is a classic theme with theories being developed on the basis of tasks and activities (Katz & Kahn 1966; Lawrence & Lorsch 1967; Thompson 1967), to theories of economic organisation focused on property rights and transaction costs (Alchian & Demsetz 1972; Grossman & Hart 1986; Jensen & Meckling 1976; Williamson 1975), and strategic theories of resources, capabilities and knowledge (Barney 1995; Foss 2002; McGee 2003; Teece, Pisano & Shuen 1997). While these different theories each provide a different lens with which to understand how organisations structure themselves to create their boundaries, these theories tend to be weak in linking organisational boundaries to value creation (or competitive advantage). Furthermore, these theories say little about the nature of organisational boundaries beyond their <b>basic</b> location. To <b>counter</b> this perceived weakness, we draw primarily upon the knowledge based view of the firm which proffers an alternative explanation regarding organisational boundaries and the need for organisational alliances. The knowledge literature simultaneously provides an opportunity to investigate the nature of organisational boundaries in the context of alliances and knowledge transfer. Positing that organisational structure in terms of firm boundaries (location and permeability) fundamentally drive an organisation’s ability to engage in learning and knowledge transfer, we use a case study of Main Roads Western Australia (WA) to illustrate how rethinking their structural boundaries and the nature of these boundaries allowed for a rebuilding of key organisational capabilities...|$|R
40|$|Serbia {{is widely}} {{believed}} to be a country flooded with corruption. The question naturally arises as to what might be the causes of this unwelcome phenomenon. The basic stand extensively elaborated in this text is that the causes of the corruption are institutional, i. e. systemic. Without recourse to the systemic causes, the alternative would be that corruption arises from some special anthropological traits of this people. Upon rejecting such an explanation of corruptive practices, the institutional roots are further elaborated. Economic and other policies can also be conducive to corruption, but they are made in an environment structured in the spirit of public choice - policies themselves are motivated by the systemic peculiarities - so that the ultimate determinants are again to be sought in the institutional order of the given society. It might appear at first sight that corruption should be easy to eliminate, as institutions are man-made and not an unchangeable given coming from nature or some uncontrollable entity. However, institutional development is constrained by many factors - knowledge, competent people, information systems, real resources, and financial means - and it is argued that it can be as painful, uncertain, costly and slow as the economic development itself. The complexity of the corruption as an economic, social and even ethical phenomenon is demonstrated by the multiplicity of its definitions and by the conspicuous succession of alternative definitions in time. The problems of measurement are elaborated to some length. Corruption is illegal and therefore, unlike most economic aggregates, cannot be measured directly; it is the perception of the corruption rather than the corruption itself that is being measured in widely cited national and international surveys. The respondents are not centered exclusively at revealing the truth, but are subject {{to a wide variety of}} motivations. Some of them are motivated to overstate the extent of corruption, while others are interested in underestimating it. It seems that those bent on overstating it prevail most of the time. As a result, the picture is blurred and uncertain. The multiplicity and disturbing scope of consequences of corruption call for persistent activity aimed at its containing and, where possible, eliminating. As an activity running <b>counter</b> <b>basic</b> ethical principles, corruption is utterly bad and unacceptable by itself. Furthermore, it grossly distorts the prices of commodities and of the production factors and thus undermines the very functioning of the market. It introduces distortions into the choice of the projects, causes departure of the factor proportions from the optimal combination, shortens time horizons in the various spheres of the decision making, makes it impossible to achieve an efficient allocation of resources at various levels in the economic system, undermines credibility among economic actors and with respect to economic policies and distorts these policies themselves. Apart from any other determinants, corruption introduces the possibility of multiple equilibria into economic systems and, as a rule, makes it impossible for these systems to settle at the Pareto optimal points. A number of unequivocally established theoretical results are revealed next. These are presented according to whether they relate to the exogenous corruption, based on externally given rents, or to the endogenous corruption in which the rents depend on the actions and behavioural patterns of the decision makers. The two sets of the results are contraposed, compared and contrasted. Major contradictory corollaries are set out and the implications of their crosscurrents are drawn. The impact of alternative market structures on corruption is examined and the consequences of varying structures in the market for corruption itself are revealed. Particular reference is ascribed to the alleged superiority of the monopolistic market over the competitive market in the turnover of the corruptive services and the claimed superiority of the corruption over lobbying. These results are demonstrated to stem from the basic stand that corruption is exogenous and is fed from the independently given and fixed amount of rents. Turning this stand around and accepting the notion of endogenous corruption, generated by the market actors, makes it possible to reverse these somewhat bizarre and certainly counterintuitive conclusions...|$|R
40|$|This work {{contributes}} technical to {{the field}} of fair exchange protocols by proposing a new way to move safeguarded secrets between cryptographically secure endpoints excluding the possibility of duplication. After a brief introduction and presentation of an overview of the subject matter, the problem areas of creating a way to teleport secrets with the help of tamper-resistant hardware are defined. Objects like the CASTOR (a HSM element), eCoins (the secrets) and a copy-less transportation (the teleportation protocol) are introduced as key elements for the intended solution to build the proposed technical framework. With this in hand, its application could work as central exchange engine in the construction of a Digital Cash environment. Adding an ATM (the eMint) and a trust center (the CA) to the scenario taking the role of an optimistic Trusted Third Party (TTP) provides generatability (minting new eCoins) and revocability (blacklisting existing eCoins and eWallets) to the system. This work describes key elements of the proposed Digital Cash framework named fairCASH with the required accuracy tied and held together by defining assumptions, objectives, properties and specific objects. This then leads to a technological solution for building an electronic payment system based upon transferable eCoins. [chapter 2] Thematically this thesis starts providing a brief state-of-the art overview about how the field of mobile commerce gets involved by shifting more and more business from legacy methods to the Internet based ones. The need of an easy and safe way to endorse this transition with an adequate payment system is shown. The reader will find a first impression of the fairCASH architecture, infrastructure, and capabilities through the explanation of the main elements in its environment: eCoins based upon certificates, eWallets, and teleportation. In a first contribution overall guideline arguments are presented, why technology based developments are best suited to provide the tools for innovations like Digital Cash. The fundamental difference between token based blinding techniques widely used as a mean to protect ePayment systems against multi-spending and the proposed system by discussing the adversarial character for payment system users connected to that property is emphasized. Next, well-chosen system properties illustrating the real needs are presented. Here the interested reader will find a plurality of information around the fair exchange subject, starting with a formal definition. By following the existing literature, {{it is obvious that the}} exchange protocol has to be delayed fair, time lined, and effective to guarantee atomic parallelism. Naming the most important success factors for any Digital Cash system, this thesis goes on in its way to discuss functional must-have key elements including their meaning for a later protocol definition proposal. This chapter will be followed by a brief discussion of the characteristically metrics alongside with a comparison of Digital-Cash- and physical-token-currencies (by providing facts related to the €uro system), and finally a clear and pragmatic scenario definition including commented descriptions of a selection of contemporary (and failed) ePayment systems. The possibility of turning a technology into business is mostly an exciting case. This thesis looks and discusses questions with respect to the central business: • Can fairCASH be the base to offer essential customer value? • Will it be possible to find enough advantages compared to other cash services? • Can such a technology offer a significant level of user satisfaction? Payments are actuated by humans but effected and conducted by technical means. This thesis takes a look onto the technical environment about how the interaction takes place. This includes possible configurations about the eWallet operation methods for device discovery and pairing. The way this is done decides about the ability and performance of such a financial infrastructure for their usage not only on a PoS but also on payment gates for masses. [chapter 3] Next, the underlying formal communication message channel model is introduced, followed by the factual issue of packet loss and the ‘byzantine generals’ undecidability state, which has to be respected in the following. The knowledge of the “position of the value” (ownership) during the transmission under all circumstances is mission critical. Acceptance of the fact to live with an unreliable channel for exchanging messages between Alice and Bob has a great influence on how to deal with atomic commitments, a desired feature in the termination phase of the teleportation protocol. The effect of state inconsistency in two-party communication environments based upon atomic delivery as proposed in literature is reviewed and it is concluded that up today there is no standard way to solve the atomic commitment problem within the fairCASH environment by applying existing methods. [chapter 4] The previous discussion of different privacy levels and their meaning for anonymity concepts including unobservability, untraceability or unlinkability is pointing the way to a “delayed-true two-party fair exchange protocol”. Naming the requirements, this thesis proposes a general and modular definition for protocol assumptions, objectives and properties. Thereafter, a basic notation for the teleportation protocol is put on the table, following the usual methodology in consideration of the specific needs in the presented case. Describing the exchanged protocol messages, a complete definition about their structure and meanings is given. With that detailed step-by-step presentation and exhaustive discussion of all messages including their embedded elements, the reader is invited to follow Alice and Bob by logically stepping through the whole exchange process. Thereafter, the associated protocol states will be discussed and the analysis of achievable fairness, reasons why choosing special decisions in one case and in other cases being constrained by circumstances is presented. This leads to the solution about how to guarantee (by potentially recreating) true fairness through the usage of affidavits, proving a loss of value without exposing the system to new fraud threats, usable for automated reimbursements. Taking a closer look to the performance of the exchange, the forfeiture probability can be determined in the worst case scenario for transactions in the need of fairness recreation to an upper limit of 3 % (never expected to exceed 1 ‰ in practice) based upon the nature of the teleportation protocol itself and the effective pledged Quality of Service (QoS) defined by international standards and recommendations. A calculation of transactions times in different networks based on a previously done estimation of message sizes shows an asymptotic barrier of about one second, introduced by the assumed processing speed of the eWallet. At this instance, it should be pointed out, that data objects are playing an important role in the protocol presented. In combination with the transfer protocol itself, they provide and guarantee the proposed protocol properties including and conforming to an ITU-T X. 509 v 3 based PKI trust architecture. [chapter 5] Forgery and multi-spending are the <b>basic</b> threats to <b>counter</b> in any financial system based on token circulation. A short presentation of the potentially different (technical) risks introduced by such a system is discussed, concluding to the question of possibility “to cheat the fairCASH system without tampering the eWallet”. Goals are discussed as ranking criteria, what needs a protection, and which skills do attackers have. To counter and stabilize the presented fairCASH system, a basic security management model and questions about how to respond to attack scenarios are introduced. To make this more practical, known basic attacks are classified, ordered and grouped into different threat categories. [chapter 6] Protection is the first obligation in an unsafe environment. The degree of protection is crucial to make a system like fairCASH operational or dysfunctional. Given enough time and resources, every protection system can be broken. However, it is good news that the postulation of the requirement for tamper-proved hardware, which we all know does not exist, is not necessary. It is shown that one of the central points of interest is: How long can an eWallet remain in the open market before it gets cracked. This is also a function of the continuous technical (attack) progress in a world where yesterdays technologies become cheap and widely available to today’s attackers. This opens a race in which the manufaction of an eWallet starts on the leading edge position, while continuously dropping down to an imaginary bottom line, reaching the point in time being replaced by a new one including again the up-to-time defense technologies; an ongoing continuing cycle. As rule of thumb, it should be noticed that capabilities of silicon based HSMs should not be overestimated and capabilities of attackers must never be underestimated. It will be a bad design decision using just a few (or a single) system component(s) in a financial environment (e. g. eWallets) as sole barrier, standing between the hacker and the money. Nevertheless, the CASTOR is an essential part of the proposed design philosophy within the fairCASH system introducing countermeasures relating to possible eWallet integrity manipulation attacks. A good tamper-protected hardware with adequately chosen instruments against anticipated attacks joint risk management and safeguards against hypothetical losses on system level is a good starting point. This thesis references existing and introduces some new countermeasures like the presented TRUSTLET concept, contributing to the secure boot paradigm first presented by the AEGIS architecture [9] in combination with the Maze of Knossos first introduced with this thesis.|$|R
