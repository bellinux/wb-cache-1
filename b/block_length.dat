1580|992|Public
25|$|There is {{more than}} one upper bound on the {{achievable}} code rate of linear block codes for multiple phased-burst correction (MPBC). One such bound is constrained to a maximum correctable cyclic burst length within every subblock, or equivalently a constraint on the minimum error free length or gap within every phased-burst. This bound, when reduced to the special case of a bound for single burst correction, is the Abramson bound (a corollary of the Hamming bound for burst-error correction) when the cyclic burst length is less than half the <b>block</b> <b>length.</b>|$|E
2500|$|In the 1960s most {{disk drives}} used IBM's {{variable}} <b>block</b> <b>length</b> format (called Count Key Data or [...] "CKD").|$|E
2500|$|Let [...] and [...] be polynomials with degrees [...] and , {{representing}} {{bursts of}} length [...] and [...] respectively with [...] The integers [...] represent the starting {{positions of the}} bursts, and are less than the <b>block</b> <b>length</b> of the code. For contradiction sake, assume that [...] and [...] {{are in the same}} coset. Then, [...] is a valid codeword (since both terms are in the same coset). Without loss of generality, pick [...] By the division theorem we can write: [...] for integers [...] and [...] We rewrite the polynomial [...] as follows: ...|$|E
40|$|A single {{multiblock}} copolymer {{chain in}} poor solvent undergoes microphase separation {{within its own}} globule, driven by {{the same kind of}} forces operating in the bulk system. However, the necessity of packing a large AB interface into a small volume leads to novel convoluted geometries. Long <b>block</b> <b>lengths</b> form a double droplet. Very short <b>block</b> <b>lengths</b> exhibit bulk behavior, forming a lamellar globule. With intermediate <b>block</b> <b>lengths,</b> the AB interface buckles to form a hand shake or spiral dicluster. An order-disorder transition is reported for short <b>block</b> <b>lengths...</b>|$|R
50|$|While codes {{such as the}} LDPC are {{generally}} implemented on high-power processors, with long <b>block</b> <b>lengths,</b> there are also applications which use lower-power processors and short <b>block</b> <b>lengths</b> (1024).|$|R
40|$|It is {{well known}} that {{samplers}} are linear time varying systems, so in general, the commutativity of samplers does not hold. There are some existing results on the commutativity of conventional decimators and expanders, block samplers with the same integer <b>block</b> <b>lengths</b> but different integer sampling ratios, and block samplers with different integer <b>block</b> <b>lengths</b> and integer sampling ratios. This paper extends the existing results to a necessary and sufficient condition for the commutativity of block decimators and expanders with arbitrary rational sampling ratios and <b>block</b> <b>lengths.</b> Â© 2012 Elsevier Inc. All rights reserved...|$|R
2500|$|A simple {{example of}} a grid street pattern (see diagram) {{illustrates}} the progressive reduction in total street length (the sum of all individual street lengths) and the corresponding increase in <b>block</b> <b>length.</b> For a corresponding reduction of one, two, three and four streets within this [...] parcel, the street length is reduced from an original total of 12,600 to 7,680 linear feet, a 39% reduction. Simultaneously, block lengths increase from 200 × 200 feet to 1240 × 200 feet. When all five blocks have reached the ultimate size of [...] four street lengths out of total eight have been eliminated. Block lengths of 1000 feet or larger rarely appear in grid plans and are not recommended as they hinder pedestrian movement (Pedestrianism, below). From the pedestrian perspective, the smaller the block is, the easier the navigation and the more direct the route. Consequently, the finer grids are preferred.|$|E
50|$|Examples of doubly even {{codes are}} the {{extended}} binary Hamming code of <b>block</b> <b>length</b> 8 and the extended binary Golay code of <b>block</b> <b>length</b> 24. These two codes are, in addition, self-dual.|$|E
50|$|In {{information}} theory, {{the error}} exponent of a channel code or source code over the <b>block</b> <b>length</b> {{of the code}} is the logarithm of the error probability. For example, if the probability of error of a decoder drops as e−nα, where n is the <b>block</b> <b>length,</b> the error exponent is α. Many of the information-theoretic theorems are of asymptotic nature, for example, the channel coding theorem states that for any rate less than the channel capacity, the probability of the error of the channel code {{can be made to}} go to zero as the <b>block</b> <b>length</b> goes to infinity. In practical situations, there are limitations to the delay of the communication and the <b>block</b> <b>length</b> must be finite. Therefore, it is important to study how the probability of error drops as the <b>block</b> <b>length</b> go to infinity.|$|E
50|$|Automatic blocks: Two {{types of}} {{automatic}} block are in common use in France. BAL (Bloc Automatique Lumineux) {{is used on}} high traffic lines with <b>block</b> <b>lengths</b> of about 1500 m) and BAPR (Bloc automatique à permissivité restreinte) is used in low traffic areas with <b>block</b> <b>lengths</b> up to 15 km.|$|R
30|$|Better {{signalling}} systems, such as ERTMS, with shorter <b>block</b> <b>lengths.</b>|$|R
40|$|Several {{construction}} {{methods for}} regular and irregular low-density parity-check (LDPC) codes are investigated. The performance of these randomly and deterministically constructed codes [...] -in {{terms of the}} bit error rate [...] -is compared, where the <b>block</b> <b>lengths</b> of interest are between 10 ³ and 10 ^ 4. These <b>block</b> <b>lengths</b> are suitable for practical applications. Th...|$|R
5000|$|The {{advantage}} of choosing a primitive polynomial as the generator for a CRC code {{is that the}} resulting code has maximal total <b>block</b> <b>length</b> {{in the sense that}} all 1-bit errors within that <b>block</b> <b>length</b> have different remainders (also called syndromes) and therefore, since the remainder is a linear function of the block, the code can detect all 2-bit errors within that <b>block</b> <b>length.</b> If [...] is the degree of the primitive generator polynomial, then the maximal total <b>block</b> <b>length</b> is , and the associated code is able to detect any single-bit or double-bit errors. We can improve this situation. If we use the generator polynomial , where [...] is a primitive polynomial of degree , then the maximal total <b>block</b> <b>length</b> is , and the code is able to detect single, double, triple and any odd number of errors.|$|E
50|$|A natural {{concept for}} a {{decoding}} algorithm for concatenated codes is to ﬁrst decode the inner code {{and then the}} outer code. For the algorithm to be practical it must be polynomial-time in the final <b>block</b> <b>length.</b> Consider {{that there is a}} polynomial-time unique decoding algorithm for the outer code. Now {{we have to find a}} polynomial-time decoding algorithm for the inner code. It is understood that polynomial running time here means that running time is polynomial in the final <b>block</b> <b>length.</b> The main idea is that if the inner <b>block</b> <b>length</b> is selected to be logarithmic in the size of the outer code then the decoding algorithm for the inner code may run in exponential time of the inner <b>block</b> <b>length,</b> and we can thus use an exponential-time but optimal maximum likelihood decoder (MLD) for the inner code.|$|E
50|$|Now, {{the time}} {{complexity}} of {{the first step is}} O(N⋅exp(n)), where n = O(log(N)) is the inner <b>block</b> <b>length.</b> In other words, it is NO(1) (i.e., polynomial-time) in terms of the outer <b>block</b> <b>length</b> N. As the outer decoding algorithm in step two is assumed to run in polynomial time the {{complexity of the}} overall decoding algorithm is polynomial-time as well.|$|E
5000|$|... #Caption: Poloxamer 407 has {{approximate}} <b>block</b> <b>lengths</b> of a = 101 and b = 56.|$|R
3000|$|Below we {{exemplify the}} {{diversity}} versus channel estimation tradeoff for B-IFDMA, assuming different <b>block</b> <b>lengths</b> [...]...|$|R
50|$|The {{flexibility}} of the cipher, which includes its ability to support multiple key sizes and <b>block</b> <b>lengths.</b>|$|R
5000|$|In early {{versions}} of the OS (certainly before OS/360 R21.8) the <b>block</b> <b>length</b> must be in decreasing order, or the user must inspect each instance and append to the named DD statement the maximum <b>block</b> <b>length</b> found, as in, for example,//INPUT01 DD DSN=MYFILE01,DISP=SHR,BLKSIZE=800// DD DSN=JOESFILE,DISP=SHR (BLKSIZE assumed to be equal to or less than 800)// DD DSN=SUESFILE,DISP=SHR (BLKSIZE assumed to be equal to or less than 800) ...|$|E
50|$|The Reed-Solomon code is {{actually}} a family of codes, where every code is characterised by three parameters: an alphabet size q, a <b>block</b> <b>length</b> n, and a message length k, with k < n ≤ q. The set of alphabet symbols is interpreted as the finite field of order q, and thus, q {{has to be a}} prime power. In the most useful parameterizations of the Reed-Solomon code, the <b>block</b> <b>length</b> is usually some constant multiple of the message length, that is, the rate R = k/n is some constant, and furthermore, the <b>block</b> <b>length</b> is equal to or one less than the alphabet size, that is, n = q or n = q − 1.|$|E
5000|$|Ciphertext stealing, another {{approach}} {{to deal with}} messages that are not a multiple of the <b>block</b> <b>length</b> ...|$|E
40|$|It {{is often}} {{necessary}} to apply codes with short <b>block</b> <b>lengths</b> in many delay-sensitive applications. In this thesis, primarily driven by their complexity advantages, we investigate {{and explore the}} performance of low-density generator rnatrix (LDGM) codes [1] at short <b>block</b> <b>lengths.</b> We show that with proper constructions LDGM (codes may perform no worse than the state-of-the-art, low-density parity-check codes (LDPC) [2] but with lower complexity. Our construction of LDGM codes uses the Progressive Edge-Growth (PEG) algorithm [3], originally proposed for LDPC codes...|$|R
30|$|As an {{extension}} of this work, more illustrative examples can be developed with standard longer <b>block</b> <b>lengths</b> using special algorithms for generating the complete weight spectrum.|$|R
5000|$|... iBiquity {{initially}} tested PAC for the HD-Radio IBOC {{digital radio}} upgrade for FM and AM, but chose an MPEG4-derived codec, HE-AAC, instead. MPEG-2 AAC is substantially {{similar to the}} original AT&T PAC algorithm written by Johnston and Ferreira, including the specifics of stereo pair coding, bitstream sectioning, handling of 1 or 2 channels at a time, multiple codebooks responding to the same largest absolute value, and block switching triggers. The version of PAC tested for the MPEG-NBC (later to become AAC) trials used 1024/128 sample <b>block</b> <b>lengths,</b> rather than 512/128 sample <b>block</b> <b>lengths.</b>|$|R
5000|$|Theorem (List-Decoding Capacity). Let [...] and [...] The {{following}} two statements hold for large enough <b>block</b> <b>length</b> [...]|$|E
50|$|Convolutional {{codes are}} often {{described}} as continuous. However, {{it may also be}} said that convolutional codes have arbitrary <b>block</b> <b>length,</b> rather than being continuous, since most real-world convolutional encoding is performed on blocks of data. Convolutionally encoded block codes typically employ termination. The arbitrary <b>block</b> <b>length</b> of convolutional codes can also be contrasted to classic block codes, which generally have fixed block lengths that are determined by algebraic properties.|$|E
50|$|The noisy-channel coding theorem {{states that}} for any ε > 0 and for any {{transmission}} rate R {{less than the}} channel capacity C, there is an encoding and decoding scheme transmitting data at rate R whose error probability is less than ε, for a sufficiently large <b>block</b> <b>length.</b> Also, for any rate greater than the channel capacity, the probability of error at the receiver goes to one as the <b>block</b> <b>length</b> goes to infinity.|$|E
50|$|A {{backwards}} compatible extensions of XMODEM with 32k and 64k <b>block</b> <b>lengths</b> {{was created by}} Adontec for better performance on high-speed error free connections like ISDN or TCP/IP networks.|$|R
40|$|Coil-Liq. Cryst. diblock {{copolymers}} exhibit ordered morphologies in liq. crystal (LC) solvent {{that reflect}} the interplay between microphase segregation of the coil (LC phobic) block and the spontaneous anisotropy in the local chain conformation of the LC block. The self-assembly of "end-on" and "side-on" coil-LC diblock copolymers with near const. side group liq. crystal polymer (SGLCP) <b>block</b> <b>lengths</b> and increasing coil (polystyrene) <b>block</b> <b>lengths</b> (40 kg/mol to 120 kg/mol) was investigated using SANS and TEM. SANS measurements of dil. solns. of these coil-SGLCP diblocks in aligned nematic LC solvent reveal the formation of anisotropic, self-assembled micelles. Unstained and pos. -stained TEM enabled a direct correspondence between the structural features found in the anisotropic scattering and micelle sizes. Size scales of the morphol. features at both at the network (> 100 nm) and intrachain (5 - 100 nm) levels were examd. The shape and anisotropy of the micelle core appears to depend on both the SGLCP block and the polystyrene <b>block</b> <b>lengths...</b>|$|R
3000|$|... = 3. However, these {{architectures}} {{are dedicated}} to particular cases and other solutions were necessary to support other types of more demanding LDPC codes, namely those that have irregular nature and higher <b>block</b> <b>lengths.</b>|$|R
5000|$|Here {{relative}} {{distance is}} {{the ratio of}} minimum distance to <b>block</b> <b>length.</b> And [...] is the q-ary entropy function defined as follows: ...|$|E
5000|$|The outer code [...] {{have the}} {{relative}} distance [...] and <b>block</b> <b>length</b> of [...] The set of inner codes is the Wozencraft ensemble [...]|$|E
5000|$|Block codes work on fixed-size blocks (packets) of bits or {{symbols of}} {{predetermined}} size. Practical block codes can generally be hard-decoded in polynomial time to their <b>block</b> <b>length.</b>|$|E
40|$|Coil- Liq. Cryst. diblock {{copolymers}} exhibit ordered morphologies in liq. crystal (LC) solvent {{that reflect}} the interplay between microphase segregation of the coil (LC phobic) block and the spontaneous anisotropy in the local chain conformation of the LC block. The self- assembly of "end- on" and "side- on" coil- LC diblock copolymers with near const. side group liq. crystal polymer (SGLCP) <b>block</b> <b>lengths</b> and increasing coil (polystyrene) <b>block</b> <b>lengths</b> (40 kg /mol to 120 kg /mol) was investigated using SANS and TEM. SANS measurements of dil. solns. of these coil- SGLCP diblocks in aligned nematic LC solvent reveal the formation of anisotropic, self- assembled micelles. Unstained and pos. - stained TEM enabled a direct correspondence between the structural features found in the anisotropic scattering and micelle sizes. Size scales of the morphol. features at both at the network (> 100 nm) and intrachain (5 - 100 nm) levels were examd. The shape and anisotropy of the micelle core appears to depend on both the SGLCP block and the polystyrene <b>block</b> <b>lengths...</b>|$|R
40|$|This paper {{establishes}} the optimal bootstrap <b>block</b> <b>lengths</b> for coverage probabilities when the bootstrap {{is applied to}} covariance stationary ergodic dependent data. It is shown that the <b>block</b> <b>lengths</b> that minimize the error in coverage probabilities of one- and two-sided block bootstrap confidence intervals of normalized and studentized smooth functions of sample averages are proportional to $n^{ 1 / 4 }$. The minimum error rates in coverage probabilities of one- and two-sided block bootstrap confidence intervals are of order O($n^{- 3 / 2 }$) and O($n^{- 5 / 4 }$), respectively, for normalized and studentized statistics. This constitutes a refinement over the asymptotic confidence intervals. ...|$|R
40|$|This work {{investigates the}} use of a {{preventive}} lengthbased blocking strategy associated with local estimations. A logical process must <b>block</b> until the <b>length</b> of its input queue becomes greater than a threshold Lmin. By blocking one can reduce the number of time errors in order to allow the local estimations to handle more efficiently the remaining time errors. In a first time, we seek for an empirical optimal trade-off between the initial <b>blocking</b> <b>length,</b> the speedup relative to a conservative simulation and the accuracy of the results. Then an automatic tuning is performed to adjust the initial <b>blocking</b> <b>length</b> to a desired time error ratio. We apply this method to simulate an ATM network and the results show that the <b>length</b> <b>blocking</b> strategy succeed in reducing the number of causality violations with a very small blocking overhead. Also both accuracy and speedup can be achieved. 1. Introduction In most distributed simulations a real system is modeled by a collection of Logical Proce [...] ...|$|R
