221|929|Public
2500|$|More recent {{information}} on {{the membership of the}} Porcellian Club may be found in a 1994 Harvard Crimson article by Joseph Mathews. He writes, [...] "Prep school <b>background,</b> <b>region</b> and legacy status {{do not appear to be}} the sole determinants of membership they may once have been, but ... they remain factors." ...|$|E
5000|$|... = {{the input}} {{sequence}} (array/line values) from the signal region or <b>background</b> <b>region,</b> respectively.|$|E
5000|$|... = {{value of}} the ith line in the signal region or <b>background</b> <b>region,</b> respectively.|$|E
5000|$|Entropy-based methods {{result in}} {{algorithms}} {{that use the}} entropy of the foreground and <b>background</b> <b>regions,</b> the cross-entropy between the original and binarized image, etc.|$|R
40|$|Abstract. Scene {{understanding}} requires reasoning {{about both}} {{what we can}} see and what is occluded. We offer a simple and general approach to infer labels of occluded <b>background</b> <b>regions.</b> Our approach incorporates estimates of visible surrounding background, detected objects, and shape priors from transferred training regions. We demonstrate the ability to infer the labels of occluded <b>background</b> <b>regions</b> in both the outdoor StreetScenes dataset and an indoor scene dataset using the same approach. Our experiments show that our method outperforms competent baselines. ...|$|R
3000|$|The new frame {{referencing}} {{scheme is}} designed with clearer purpose: the immediate previous frame is meant for moving areas, and the McFIS is meant for <b>background</b> <b>regions.</b>|$|R
50|$|The RMS Noise {{is defined}} as the square root of the mean of variances from the <b>background</b> <b>region.</b>|$|E
5000|$|... #Caption: Half moon target - on {{the left}} {{the image of the}} <b>background</b> <b>region</b> and on the right an image of the signal region. Using {{specialized}} software, an operator arbitrarily defines an area of evaluation in both regions to be used in determining the signal transfer function.|$|E
5000|$|More recent {{information}} on {{the membership of the}} Porcellian Club may be found in a 1994 Harvard Crimson article by Joseph Mathews. He writes, [...] "Prep school <b>background,</b> <b>region</b> and legacy status {{do not appear to be}} the sole determinants of membership they may once have been, but ... they remain factors." ...|$|E
3000|$|... [...]) is {{automatically}} fulfilled. During the case studies, the speedfield F {{has been very}} simple, + 1 for the object region and − 1 for the <b>background</b> <b>regions.</b>|$|R
40|$|Abstract- A {{dynamic image}} fusion scheme for {{infrared}} and visible sequence based on region target detection is proposed in this paper. Target detection technique is employed to segment the source images into target and <b>background</b> <b>regions.</b> Different fusion rules are adopted respectively in target and <b>background</b> <b>regions.</b> A limitedly redundant discrete wavelet transform (LR DWT) method is introduced to achieve shift invariant multi-resolution representation of each source images. Fusion experiments on real world image sequences {{indicate that the}} proposed method is effective and efficient, which achieves better performance than the generic fusion method...|$|R
40|$|Our aim is {{to search}} for {{atmospheric}} g-mode oscillations in UV network, UV bright points and Uv <b>background</b> <b>regions.</b> We have analysed a 6 -hours of time sequence of ultraviolet (uv) images obtained on May 24, 2003 in 1600 A continuum under high spatial and temporal resolution from Transition Region and Coronal Explorer (TRACE). We have selected an isolated 15 uv bright points, 15 uv network elements and 15 uv <b>background</b> <b>regions</b> in a quiet region from the images for the detailed analysis. We derived the cumulative intensity values of these features. The light curves of all the features have been derived for the total duration of observations and done the power spectrum analysis using the time series data. We found that the uv bright points, the uv network and uv <b>background</b> <b>regions</b> will exhibit {{a longer period of}} intensity oscillations namely, 5. 5 hours, 4. 6 hours and 3. 4 hours respectively, in addition to the more familiar small scale intensity fluctuations. We suggest that the longer periods of oscillation may be related to solar atmospheric g-modes. Comment: 7 pages, 6 figure...|$|R
50|$|The RFB (VNC) {{protocol}} is odd {{when compared}} to other network graphics protocols, such as X11 and RDP, in that there is no provision for viewer-side caching of pixel data. While this makes the client easier to implement, there is a price to pay in terms of interactive response. For example, every re-exposure of a window or <b>background</b> <b>region</b> needs to have its (compressed) pixel data resent over the network. This effect is particularly noticeable for windows with complex or photo regions (such as a web browser window) that gets iconified and deiconified or re-exposed often.|$|E
50|$|The line data is {{gathered}} from the arbitrarily defined signal and background regions and input into an array (refer to image to the right). To calculate the average signal and background values, a second order polynomial is fitted to the array of line data and subtracted from the original array line data. This is done to remove any trends. Finding the mean of this data yields the average signal and background values. The net signal is calculated from the difference of the average signal and background values. The RMS or root mean square noise is defined from the <b>background</b> <b>region.</b> Finally, SNR is determined as {{the ratio of the}} net signal to the RMS noise.|$|E
5000|$|The {{manner of}} a character’s speech is to {{literature}} what an actor’s appearance and costume are to cinema. [...] In fiction, what a character says, {{as well as how}} he or she says it, makes a strong impression on the reader. [...] Each character should have his or her distinctive voice. [...] To differentiate characters in fiction, the writer must show them doing and saying things, but a character must be defined by more than one single topic of conversation or by the character’s accent. The character will have other interests or personality quirks as well. [...] Although individual temperament is the largest determinant of what a character says, {{it is not the only}} one. The writer can make the characters’ dialogue more realistic and interesting by considering several factors affecting how people speak: ethnicity, family <b>background,</b> <b>region,</b> gender, education, and circumstances. [...] Words characterize by their diction, cadence, complexity, and attitude. [...] Mannerisms and catch-phrases can help too. Considering the degree of formality in spoken language is also useful. Characters who spend a lot of their lives in a more formal setting often use a more formal language all the time, while others never do. [...] Tone of voice, volume, rate of delivery, vocabulary, inflection, emphasis, pitch, topics of conversation, idioms, colloquialisms, and figures of speech: all of these are expressions of who the character is on the inside. [...] A character’s manner of speech must grow from the inside out. The speaking is how his or her essential personality leaks out for the world to see; it is not the sum total of his or her personality.|$|E
5000|$|... #Caption: An {{operator}} arbitrarily {{defines a}} box {{area in the}} signal and <b>background</b> <b>regions</b> of a back-illuminated half moon or knife-edge test target. The data, (such as pixel intensity), is {{used to determine the}} average signal and background values.|$|R
40|$|Abstract. In this paper, {{we present}} a novel {{foreground}} extraction method that automatically identifies image regions corresponding to a common space region seen from multiple cameras. We assume that <b>background</b> <b>regions</b> present some color coherence in each image and we exploit the spatial consistency constraint that several image projections of the same space region must satisfy. Integrating both color and spatial consistency constraints allows to fully automatically segment foreground and <b>background</b> <b>regions</b> in multiple images. In contrast to standard background subtraction approaches, the proposed approach does not require any a priori knowledge on the background nor user interactions. We demonstrate {{the effectiveness of the}} method for multiple camera setups with experimental results on standard real data sets. ...|$|R
3000|$|... [*]dB. It {{means that}} the {{proposed}} SVA-based MVC scheme improves image quality of ROI up to 0.46 [*]dB; meanwhile, to improve compression ratio, the proposed SVA-based MVC scheme allocates fewer bits on the <b>background</b> <b>regions</b> and {{at the cost of}} its PSNR_ [...]...|$|R
30|$|Differentiate the {{foreground}} and <b>background</b> <b>region</b> of human boundary.|$|E
30|$|The SUVmax {{of the six}} hot spheres was {{determined}} from circular regions of interest (ROIs) placed on the centre slice of images of each sphere. The diameter of each ROI was {{equal to that of}} each hot sphere (10, 13, 17, 22, 28 and 37  mm). Twelve circular ROIs with a diameter of 10  mm were placed on the <b>background</b> <b>region</b> at the centre slice of the PET image; the average SUV was calculated from twelve ROIs as the SUVmean of the background. The acceptance criterion for the SUVmean of the <b>background</b> <b>region</b> was 1.00 [*]±[*] 0.05. Table  1 shows the average SUVmean of the <b>background</b> <b>region</b> after cross-calibration.|$|E
3000|$|... [...]. In the {{proposed}} MVC scheme, the image quality of ROIs {{is getting better}} than that of <b>background</b> <b>region,</b> that is, PSNR_ [...]...|$|E
3000|$|As show in Fig. 1, the {{proposed}} method separates the image {{into the dark}} and <b>background</b> <b>regions</b> using adaptively partitioned blocks based on the optimal threshold value computed by fuzzy C-means clustering (FCM). More specifically, {{the proposed}} method partitions the input image into non-overlapped blocks of size 64 × 64, and classifies them as dark, <b>background,</b> and ambiguous <b>regions</b> using the optimal threshold. The ambiguous blocks are further partitioned into four sub-blocks, which are then re-classified in the same manner. This partitioning process is repeated until {{the size of a}} block becomes 4 × 4 [...]. Finally, the detected block-based dark region is refined using the guided filter for removing block artifacts in the enhanced image region (He et al. 2010). The filtered dark region is enhanced by contrast stretching, and the final output is obtained by fusing the enhance dark and input <b>background</b> <b>regions.</b>|$|R
5000|$|Visual scene {{segmentation}} is a pre-attentive process where stimuli are grouped together into specific objects against a background. Figure and <b>background</b> <b>regions</b> {{of an image}} activate different processing centres: figures use the lateral occipital areas (which involve object processing) and background engages dorso-medial areas.|$|R
40|$|Abstract. Recently several {{methods for}} {{background}} subtraction from moving camera were proposed. They use bottom up cues to segment video frames into foreground and <b>background</b> <b>regions.</b> Due to {{this lack of}} explicit models, they can easily fail to detect a foreground object when such cues are ambiguous {{in certain parts of}} the video. This becomes even more challenging when videos need to be processed online. We present a method that enables learning of pixel-based models for foreground and <b>background</b> <b>regions</b> and, in addition, segments each frame in an online framework. The method uses long term trajectories along with a Bayesian ltering framework to estimate motion and appearance models. We compare our method to previous approaches and show results on challenging video sequences. ...|$|R
3000|$|..., of a {{frame is}} {{considered}} to be a potential <b>background</b> <b>region</b> and will be treated as a spatiotemporal neighbor of all other potential background regions in the clip for the purposes of trajectory-based merging. With this change of the spatiotemporal neighbor criteria, we are able to correctly segment the disconnected pieces of the background, while still enforcing connectivity of all other objects. Furthermore, after the trajectory-based merging is finished, any island regions (those with only one spatiotemporal neighbor which is not a potential <b>background</b> <b>region)</b> are merged into their surrounding object.|$|E
40|$|This work {{demonstrates}} {{how to improve}} the size distribution of silicon nanocrystals (Si-NCs) synthesized in a remote plasma, in which the flow dynamics and the particular chemistry initially resulted {{in the formation of}} small (2 - 10  nm) and large (50 - 120  nm) Si-NCs. Plasma consists of two regions: an axially expanding central plasma beam and a <b>background</b> <b>region</b> around the expansion. Continuum fluid dynamics simulations demonstrate that a significant mass flow occurs from the central beam to the <b>background</b> <b>region.</b> This mass flow can be gradually reduced upon confinement of the central beam, preventing the mass transport to the <b>background</b> <b>region.</b> Transmission electron microscopy and Raman spectroscopy analyses demonstrate that the volume fraction of large Si-NCs decreases from ∼ 77 % to below 45 % in parallel with the decrease of mass flow to the <b>background</b> <b>region</b> upon confinement, which indicates that large Si-NCs are synthesized in the background and small Si-NCs are synthesized in the central beam. Spatially resolved ion flux analyses demonstrate that the ions are localized in the central beam despite the mass flow to the background, indicating that the formation of small Si-NCs is governed by ion-assisted growth while the formation of large Si-NCs is governed by radical-neutral-assisted growth in the absence of ions. According to these observations, a better uniformity in the size distribution of Si-NCs can be obtained by creating a more uniform plasma flow and controlling the density of plasma species in the plasma...|$|E
30|$|From the experiment, {{the sum of}} the C 2 and C 3 {{components}} {{shows the}} difference distinctly between the lip region and the <b>background</b> <b>region.</b> Thus, we choose the two components to retain more lip details.|$|E
40|$|Abstract This paper {{proposes a}} {{selection}} method of foreground local features for generic object recognition in “bag of features”. Usually all local features detected from an given image are voted to a histogram of visual words in conventional bag-of-features method. But {{it may not}} be good choice because in the standard object recognition task, an image includes target <b>regions</b> and <b>background</b> <b>regions.</b> To distinguish the target from the background, a large number of visual-words are necessary because a variation of local features coming from the <b>background</b> <b>regions</b> is usually large. It is expected that the comparable classification performance will be achieved with a small number of visual words if such unimportant local features can be effectively removed. Although it is difficult to correctly classify all local features into the target and the background, the number of visual-words can be reduced by simply neglecting many of the local features obtained from the <b>background</b> <b>regions</b> which are easily classified by Support Vector Machine (SVM). Experimental results showed the proposed method outperformed the conventional bag-of-features representation with a fewer number of visual-words by neglecting background features by the kernel SVM. The classification performance with linear SVM was also better than the conventional bag-of-feature when the number of visual words was small. ...|$|R
3000|$|... [*]dB at {{the cost}} of {{imperceptible}} quality degradation at <b>background</b> <b>regions.</b> Additionally, PSNR_Y of ROI is better than that of background, which meets requirements of HVS. Moreover, the proposed MVC scheme can save over 20 % bit rate with imperceptible image quality degradation according to the evaluation of region selective image quality metrics.|$|R
3000|$|... [*]dB at {{the cost}} of {{indiscriminate}} image quality degradation in <b>background</b> <b>regions,</b> which is less conspicuous and sensitive to human visual system. It can be foreseen that the stereoscopic visual attention will play a more important role in the areas such as content-oriented three-dimensional video processing, video retrieval, and computer vision in future.|$|R
30|$|In {{order to}} {{selectively}} enhance the contrast with over-saturation problem, the dark backlit region is accurately detected using adaptively partitioned blocks. Contrast of the backlit region is then enhanced while preserving {{the brightness of}} the <b>background</b> <b>region.</b>|$|E
3000|$|In this section, {{characteristic}} {{errors of}} the RealSense R 200 structured light camera are investigated. Stereo block matching {{and the subsequent}} internal processing causes the speckle-like pattern that can be observed best in the planar <b>background</b> <b>region</b> of Fig.  4 [...]...|$|E
3000|$|... coefficient) {{belongs to}} some {{deterministic}} component region or to noise (or <b>background)</b> <b>region.</b> They {{pointed out that}} to perform such a decision one needs more than the energy level. The authors chose to utilize local statistical features so that these features aggregated in feature space.|$|E
40|$|In video {{communication}} applications at low bitrates, coding artifacts are especially disturbing in face areas. Encoding faces with higher quality than <b>background</b> <b>regions</b> requires an automatic face detection. This paper presents a robust face segmentation and tracking algorithm and investigates the application to region-of-interest coding with H. 263 + and object-based coding with MPEG- 4. ...|$|R
40|$|While current matting {{algorithms}} {{work very}} well for some natural images, their performance is questionable {{in the presence of}} sharp discontinuities in the foreground and <b>background</b> <b>regions.</b> To counter the above problem, we propose to use variational PDE based inpainting techniques within the matting problem, that are largely successful in inpainting geometric features into unknown regions...|$|R
40|$|International audienceSemantic gap is an {{important}} challenging problem in content-based image retrieval (CBIR) up to now. Bag-of-words (BOW) framework is a popular approach that tries to reduce the semantic gap in CBIR. In this paper, an approach integrating visual saliency model with BOW is proposed for semantic image retrieval. Images are firstly segmented into <b>background</b> <b>regions</b> and foreground objects by a visual saliency-based segmentation method. And then multi-features including Scale Invariant Feature Transform (SIFT) features packed in BOW are extracted from regions and objects respectively and fused considering different characteristics of <b>background</b> <b>regions</b> and foreground objects. Finally, a fusion of z-score normalized Chi-Square distance is adopted as the similarity measurement. This proposal has been implemented on two widely used benchmark databases and the results evaluated in terms of mean Average Precision (mAP) show that our proposal outperforms the referred state-of-the-art approaches. © 2017 Elsevier Inc...|$|R
