4|25|Public
5000|$|... • HTTPD: is a {{web server}} so remote clients can fetch output files of {{finished}} jobs from the 3G <b>Bridge</b> <b>server.</b> The main programming language is C++, however 3G Bridge supports implementing grid plugins in Java. Web service interfaces are implemented using gSOAP.|$|E
40|$|While cluster {{computing}} frameworks are continuously evolving {{to provide}} real-time data analysis capabilities, Apache Spark {{has managed to}} be at the forefront of big data analytics for being a unified framework for both, batch and stream data processing. There is also a renewed interest in Near Data Processing (NDP) due to technological advancement in the last decade. However, it is not known if NDP architectures can improve the performance of big data processing frameworks such as Apache Spark. In this paper, we build the case of NDP architecture comprising programmable logic based hybrid 2 D integrated processing-in-memory and instorage processing for Apache Spark, by extensive profiling of Apache Spark based workloads on Ivy <b>Bridge</b> <b>Server.</b> QC 20171124 </p...|$|E
40|$|Quantifying {{the power}} {{consumption}} of individual appli-cations co-running {{on a single}} server is a critical compo-nent for software-based power capping, scheduling, and provisioning techniques in modern datacenters. How-ever, {{with the proliferation of}} hyperthreading in the last few generations of server-grade processor designs, the challenge of accurately and dynamically performing this power attribution to individual threads has been signifi-cantly exacerbated. Due to the sharing of core-level re-sources such as functional units, prior techniques are not suitable to attribute the power consumption between hy-perthreads sharing a physical core. In this paper, we present a runtime mechanism that quantifies and attributes power consumption to individ-ual jobs at fine granularity. Specifically, we introduce a hyperthread-aware power model that differentiates be-tween the states when both hardware threads of a core are in use, and when only one thread is in use. By capturing these two different states, we are able to accurately at-tribute power to each logical CPU in modern servers. We conducted experiments with several Google production workloads on an Intel Sandy <b>Bridge</b> <b>server.</b> Compared to prior hyperthread-oblivious model, HaPPy is substan-tially more accurate, reducing the prediction error from 20. 5 % to 7. 5 % on average and from 31. 5 % to 9. 4 % in the worst case. ...|$|E
50|$|Many other clients, bots, <b>bridges,</b> <b>servers</b> {{and other}} {{implementations}} of the Matrix protocol exist too.|$|R
50|$|Connections are DTLS-encrypted and {{established}} point-to-point. Where a point-to-point connection is not possible, TURN relay servers {{are used to}} route the audio. TURN servers in the US, UK and Japan are available, with an independent backup system being maintained at ipdtl2.com. Connections can also be made through a special URL that allows users to access another account and connect with it. ipDTL uses a proprietary signaling method but also supports SIP for interoperability with other devices and applications such as Comrex Access and Media5-fone, and can transcode between Opus, G.722 and G.711. This also allows for Interoperability with legacy ISDN hardware via cloud based <b>bridging</b> <b>servers.</b>|$|R
40|$|Abstract — Naming and {{addressing}} are important issues for Next Generation Internet (NGI). In this paper, we discuss a new Mobility and Multihoming supporting Identifier Locator Split Architecture (MILSA). There are three main contributions of our solution. First, we separate trust relationships (realms) from connectivity (zones). A hierarchical identifier {{system for the}} realms and a Realm Zone <b>Bridging</b> <b>Server</b> (RZBS) infrastructure that performs the bridging function is introduced. Second, we separate the signaling and data plane functions to improve the performance and support mobility. Third, to provide transparency to the upper layer applications, identifier locator split happens in network layer. A Hierarchical URI-like Identifier (HUI) {{is used by the}} upper layers and is mapped to a locators set by HUI Mapping Sublayer (HMS) through interaction with RZBS infrastructure. Further scenarios description and analysis show the benefits of this scheme for routing scalability, mobility and multihoming...|$|R
40|$|Energy {{efficiency}} {{has been}} a daunting challenge for datacenters. The financial industry operates {{some of the largest}} datacenters in the world. With increasing energy costs and the financial services sector growth, emerging financial analytics workloads may incur extremely high operational costs, to meet their latency targets. Microservers have recently emerged as an alternative to high-end servers, promising scalable performance and low energy consumption in datacenters via scale-out. Unfortunately, stark differences in architectural features, form factor and design considerations make a fair comparison between servers and microservers exceptionally challenging. In this paper we present a rigorous methodology and new metrics for fair comparison of server and microserver platforms. We deploy our methodology and metrics to compare a microserver with ARM cores against two servers with x 86 cores, running the same real-time financial analytics workload. We define workload-specific but platform-independent performance metrics for platform comparison, targeting both datacenter operators and end users. Our methodology establishes that a server based the Xeon Phi processor delivers the highest performance and energy-efficiency. However, by scaling out energy-efficient microservers, we achieve competitive or better energy-efficiency than a power-equivalent server with two Sandy Bridge sockets despite the microserver's slower cores. Using a new iso-QoS (iso-Quality of Service) metric, we find that the ARM microserver scales enough to meet market throughput demand, i. e. a 100 % QoS in terms of timely option pricing, with as little as 55 % of the energy consumed by the Sandy <b>Bridge</b> <b>server...</b>|$|E
50|$|The {{computing}} segment provides timing products, PCI Express switching and <b>bridging</b> solutions, high-performance <b>server</b> memory interfaces, multi-port products, {{signal integrity}} products, and PC {{audio and video}} products. This segment’s computing products are designed for desktop, notebook, sub-notebook, storage, and server applications.|$|R
50|$|Multiple systems {{exist that}} allow Fibre Channel, SCSI and SAS devices to be {{attached}} to an IP network for use via iSCSI. They {{can be used to}} allow migration from older storage technologies, access to SANs from remote servers and the linking of SANs over IP networks. An iSCSI gateway <b>bridges</b> IP <b>servers</b> to Fibre Channel SANs. The TCP connection is terminated at the gateway, which is implemented on a Fibre Channel switch or as a standalone appliance.|$|R
50|$|A {{mobile app}} server is mobile {{middleware}} that makes back-end systems accessible to mobile application to support Mobile application development. Much like a web server that stores, processes and delivers web pages to clients, a mobile app <b>server</b> <b>bridges</b> the gap from existing infrastructure to mobile devices.|$|R
50|$|In 1981, Estrin co-founded Bridge Communications — {{a network}} router, <b>bridges,</b> and {{communications}} <b>servers</b> company that {{went public in}} 1985 and merged with 3Com in 1987. In 1988 she joined the founding team of Network Computing Devices (NCD) as Executive Vice President, later becoming President and CEO in 1993.|$|R
5000|$|Ports - Because {{the various}} {{protocols}} (file and printer <b>servers,</b> <b>bridge</b> discovery, and so forth) used defined port numbers, {{it was possible}} to for additional services such as BroadcastLoader, AppFS, a teletext server, and a range of chat programs and multiplayer games to coexist within the Econet system.|$|R
40|$|Today {{whole world}} is {{connected}} via internet for worldwide communication. Different network devices like router, switch, <b>bridge,</b> hub, <b>server,</b> multiplexer/Demultiplexer and modem are used for communication. To connect different networks only routers are used to provide routes for packets of data from one network to another. To communicate between different networks, IP routing is must. This paper helps you to configure IP routing and bring-up an interface of a router and also troubleshoot the network connectivity between IP devices. In this paper CLI commands are used for configuration...|$|R
50|$|Networking devices {{may include}} gateways, routers, network bridges, modems, {{wireless}} access points, networking cables, line drivers, switches, hubs, and repeaters; {{and may also}} include hybrid network devices such as multilayer switches, protocol converters, <b>bridge</b> routers, proxy <b>servers,</b> firewalls, network address translators, multiplexers, network interface controllers, wireless network interface controllers, ISDN terminal adapters and other related hardware.|$|R
40|$|Onion routing is a {{technique}} for anonymous and privacy preserving communication {{at the base of}} popular Internet anonymity tools such as Tor. In onion routing, traffic is relayed by a number of intermediary nodes (called relays) before it reaches the intended destination. To guarantee privacy and prevent tampering, each packet is encrypted multiple times in a layered manner, using the public keys of the relays. Therefore, this mechanism makes two important assumptions: first, that the relays are able to communicate with each other; second, that the user knows the list of available relays and their respective public keys. Tor implements therefore a distributed directory listing the relays and their keys. When a user is not able to communicate with relays directly, he has to use special <b>bridge</b> <b>servers</b> to connect to the onion network. This construction, however, does not work in a fully peer to peer setting, where each peer only knows a limited number of other peers and {{may not be able to}} communicate with some of them due, for instance, to NAT or firewalls. In this paper we propose a key management scheme for onion routing that overcomes these problems. The proposed solution does not need a directory system and does not imply knowledge of all active relays, while it guarantees the secure distribution of public keys. We also present an alternative strategy for building circuit of relays based on bloom filters. The proposed construction overcomes some of the structural inefficiencies of the Tor design, and opens the way for implementing onion routing over a true peer to peer overlay network...|$|R
50|$|Historically, AMD and Intel have {{switched}} places {{as the company}} with the fastest CPU several times. Intel currently leads on the desktop side of the computer CPU market, with their Sandy Bridge and Ivy <b>Bridge</b> series. In <b>servers,</b> AMD's new Opterons seem to have superior performance for their price point. This means that AMD are currently more competitive in low- to mid-end servers and workstations that more effectively use fewer cores and threads.|$|R
50|$|Bridges: A PCI Express bridge {{enables a}} system that uses PCI Express to work with devices that use other standards. This type of bridge {{facilitates}} connection back to conventional PCI for upgrading. Applications using these <b>bridge</b> devices include <b>servers,</b> storage host bus adapters, graphics, TV tuners and security systems. PLX also offers several legacy bridges that translate general purpose serial and parallel ports. PLX announced a controller that connects PCI Express to USB 3.0 in January 2011.|$|R
30|$|We {{describe}} {{the design and}} {{development of a new}} laboratory called VideoWeb to facilitate research in processing and understanding video in a wireless environment. While research into large-scale sensor networks has been carried out for various applications, the idea of massive video sensor networks consisting of cameras connected over a wireless network is largely new and relatively unexplored. The VideoWeb laboratory entails constructing a robust network architecture for a large number of components, including cameras, wireless routers and <b>bridges,</b> and video-processing <b>servers.</b> Hardware and equipment selection needs to take into account a number of factors, including durability, performance, and cost. In addition, VideoWeb requires a number of software applications including those for data recording, video analysis, camera control, event recognition, anomaly detection, and an integrated user interface.|$|R
40|$|This paper {{reports on}} recent work and {{directions}} in modern software architectures and their formal models {{with respect to}} software maintenance. Related earlier work, now entering practice, provides automatic creation of object structures for customer applications using such models and their algebra, and we will summarize that work. Our focus on maintenance intends to attack the most costly and frustrating aspect in dealing with large-scale software systems: keeping them up-to-date and responsive to user needs in changing environments. We introduce the concept of domain-specific mediators to partition the maintenance effort. Mediators are autonomous modules which create information objects out of source data. These modules are placed into an intermediate layer, <b>bridging</b> clients and <b>servers.</b> These mediators contain knowledge required {{to establish and maintain}} services in a coherent domain. A mediated architecture can reduce the cost growth of maintenance to a near-linear function of s [...] ...|$|R
40|$|This paper {{reports on}} recent work and {{directions}} in modern software architectures and their formal models {{with respect to}} software maintenance. The focus on maintenance attacks the most costly and frustrating aspect in dealing with large-scale software systems: keeping them up-to-date and responsive to user needs in changing environments. We employ mediators, autonomous modules which create information objects out of source data. These modules are placed into an intermediate layer, <b>bridging</b> clients and <b>servers.</b> A mediated architecture can reduce the cost growth of maintenance to a near-linear function of system size, whereas current system architectures have quadratic factors. Models provide the means for the maintainer to share knowledge with the customer. The customers can {{become involved in the}} maintenance of their task models without having to be familiar with the details of all the resources to be employed. These resources encompass the many kinds of databases that are becoming [...] ...|$|R
40|$|Abstract. Linked Data {{principles}} are increasingly employed to publish high-fidelity, heterogeneous statistical datasets in a distributed way. Currently, there exists no simple way for researchers, journalists and interested people to compare statistical data retrieved from different data stores on the Web. Given that the RDF Data Cube vocabulary {{is used to}} describe statistical data, its use {{makes it possible to}} discover and identify statistical data artifacts in a uniform way. In this article, the design and implementation of an application and service is presented, which utilizes federated SPARQL queries to gather statistical data from distributed data stores. The R language for statistical computing is employed to perform statistical analyses and visualizations. The Shiny application and <b>server</b> <b>bridges</b> the front-end Web user interface with R on the server-side in order to compare statistical macrodata, and stores analyses results in RDF for future research. As a result, distributed linked statistical data can be more easily explored and analysed...|$|R
40|$|While cluster {{computing}} frameworks are continuously evolving {{to provide}} real-time data analysis capabilities, Apache Spark {{has managed to}} be at the forefront of big data analytics. Recent studies propose scale-in clusters with in-storage processing devices to process big data analytics with Spark However the proposal is based solely on the memory bandwidth characterization of in-memory data analytics and also does not shed light on the specification of host CPU and memory. Through empirical evaluation of in-memory data analytics with Apache Spark on an Ivy <b>Bridge</b> dual socket <b>server,</b> we have found that (i) simultaneous multithreading is effective up to 6 cores (ii) data locality on NUMA nodes can improve the performance by 10 % on average, (iii) disabling next-line L 1 -D prefetchers can reduce the execution time by up to 14 %, (iv) DDR 3 operating at 1333 MT/s is suffcient and (v) multiple small executors can provide up to 36 % speedup over single large executor. Peer ReviewedPostprint (published version...|$|R
40|$|ABSTRACT The {{formation}} of disulphide bridges between cysteines {{plays an important}} role in protein folding, structure, function, and evolution. Here, we develop new methods for predicting disulphide bridges in proteins. We first build a large curated data set of proteins containing disulphide bridges to extract relevant statistics. We then use kernel methods to predict whether a given protein chain contains intrachain disulphide bridges or not, and recursive neural networks to predict the bonding probabilities of each pair of cysteines in the chain. These probabilities in turn lead to an accurate estimation of the total number of disulphide bridges and to a weighted graph matching problem that can be addressed efficiently to infer the global disulphide bridge connectivity pattern. This approach can be applied both in situations where the bonded state of each cysteine is known, or in ab initio mode where the state is unknown. Furthermore, it can easily cope with chains containing an arbitrary number of disulphide bridges, overcoming one of the major limitations of previous approaches. It can classify individual cysteine residues as bonded or nonbonded with 87 % specificity and 89 % sensitivity. The estimate for the total number of bridges in each chain is correct 71 % of the times, and within one from the true value over 94 % of the times. The prediction of the overall disulphide connectivity pattern is exact in about 51 % of the chains. In addition to using profiles in the input to leverage evolutionary information, including true (but not predicted) secondary structure and solvent accessibility information yields small but noticeable improvements. Finally, once the system is trained, predictions can be computed rapidly on a proteomic or protein-engineering scale. The disulphide <b>bridge</b> prediction <b>server</b> (DIpro), software, and datasets are available through www. igb. uci. edu/servers/pass. html...|$|R
40|$|This paper {{reports on}} recent work and {{directions}} in modern software architectures and their formal models {{with respect to}} software maintenance. Related earlier work, now entering practice, provides automatic creation of object structures for customer applications using such models and their algebra, and we will summarize that work. Our focus on maintenance intends to attack the most costly and frustrating aspect in dealing with large-scale software systems: keeping them up-to-date and responsive to user needs in changing environments. We introduce the concept of domain-speci c mediators to partition the maintenance e ort. Mediators are autonomous modules which create information objects out of source data. These modules are placed into an intermediate layer, <b>bridging</b> clients and <b>servers.</b> These mediators contain knowledge required {{to establish and maintain}} services in a coherent domain. A mediated architecture can reduce the cost growth of maintenance to a near-linear function of system size, whereas current system architectures have quadratic factors. The domain knowledge in a mediator de nes the terms and relationships among the source elements and desired information. It is represented as an ontology which models the domain. These models provide the means for the maintainer to share knowledge with the customer. We sketch a conservative algebra for interoperation among these models. The customers can become involved in the maintenance of their task models without having to be familiar with the details of all the resources to be employed. These resources encompass the many kinds of databases that are becoming available on our networks. The functionality of mediators will only be touched upon and referenced within this paper. Software maintenance is su ciently important towarrant our attention for a while. 1...|$|R
40|$|Simple Network Management Protocol (SNMP) is {{the most}} widely-used network {{management}} protocol on TCP/IP-based networks. The functionality of SNMP was enhanced {{with the publication of}} SNMPv 2. However, both these versions of SNMP lack security features, notably authentication and privacy, that are required to fully exploit SNMP. A recent set of RFCs, known collectively as SNMPv 3, correct this deficiency. This article outlines the overall network management framework defined in SNMPv 3, and then looks at the principal security facilities defined in SNMPv 3 : authentication, privacy, and access control. SURVEYS IEEE# COMMUNICATIONS www. comsoc. org/pubs/surveys IEEE Communications Surveys. [URL]. Fourth Quarter 1998. Vol. 1 No. 1 3 introduction document, Table 15. 1, as follows: "SNMPv 3 is SNMPv 2 plus security and administration. " The remainder of this article is organized as follows. The next section provides a brief introduction to the basic SNMP concepts. This is followed by a discussion of the SNMP architecture defined in RFC 2271. Next, the privacy and authentication facilities provided by the SNMPv 3 User Security Model (USM) are described. The next section discusses access control and the view-based access control model (VACM). An appendix provides a brief tutorial on cryptographic algorithms, including encryption and message authentication. BASIC SNMP CONCEPTS The basic idea of any network management system is that {{there are two types of}} systems in any networked configuration: agents and managers. Any node in the network that is to be managed, including PCs, workstations, <b>servers,</b> <b>bridges,</b> routers, and so on, includes an agent module. The agent is responsible for. Collecting and maintaining information about its [...] ...|$|R
40|$|This report {{presents}} the {{proposal for the}} constitution of a European platform consisting of the federation of real-time modelling and simulation facilities applied {{to the analysis of}} emerging electricity systems. Such a platform can be understood as a pan-European distributed laboratory aiming at making use of the best available relevant resources and knowledge for the sake of supporting industry and policy makers and conducting advanced scientific research. The report describes the need for such a platform, with reference to the current status of power systems; {{the state of the art}} of the relevant technologies; and the character and format that the platform might take. This integrated distributed laboratory will facilitate the modelling, testing and assessment of power systems beyond the capacities of each single entity, enabling remote access to software and equipment anywhere in the EU, by establishing a real-time interconnection to the available facilities and capabilities within the Member States. Such an infrastructure will support the remote testing of devices, enhance simulation capabilities for large multi-scale and multi-layer systems, while also achieving soft-sharing of expertise in a large knowledge-based virtual environment. Furthermore the platform should offer the possibility of keeping confidential all susceptible data/models/algorithms, enabling the participants to determine which specific data will be shared with other actors. This kind of simulation platform will benefit all actors that need to take decisions in the power system area. This includes national and local authorities, regulators, network operators and utilities, manufacturers, consumers/prosumers. The federation of labs is created through real-time remote access to high-performance computing, data infrastructure and hardware and software components (electrical, electronic, ICT) assured by the interconnection of different labs with a server-cloud architecture where the local computers or machines interact with other labs through dedicated VPN (Virtual Private Network) over the GEANT network (the pan-European research and education network that interconnects Europe's National Research and Education Networks). The local VPN <b>servers</b> <b>bridge</b> the local simulation platform at each site and the cloud ensuring the security of the data exchange while offering a better coordination of the communication and the multi-point connection. It is then possible the integration of the different sub-systems (distribution grid, transmission grid, generation, market, and consumer behaviour) with a holistic approac...|$|R
40|$|Side {{channels}} are channels of implicit information flow {{that can be}} used to find out information that is not allowed to flow through explicit channels. This thesis focuses on network side channels, where information flow occurs in the TCP/IP network stack implementations of operating systems. I will describe three new types of idle scans: a SYN backlog idle scan, a RST rate-limit idle scan, and a hybrid idle scan. Idle scans are special types of side channels that are designed to help someone performing a network measurement (typically an attacker or a researcher) to infer something about the network that they are not otherwise able to see from their vantage point. The thesis that this dissertation tests is this: because modern network stacks have shared resources, there is a wealth of information that can be inferred off-path by both attackers and Internet measurement researchers. With respect to attackers, no matter how carefully the security model is designed, the non-interference property is unlikely to hold, i. e., an attacker can easily find side channels of information flow to learn about the network from the perspective of the system remotely. One suggestion is that trust relationships for using resources be made explicit all the way down to IP layer with the goal of dividing resources and removing sharendess to prevent advanced network reconnaissance. With respect to Internet measurement researchers, in this dissertation I show that the information flow is rich enough to test connectivity between two arbitrary hosts on the Internet and even infer in which direction any blocking is occurring. To explore this thesis, I present three research efforts: [...] - First, I modeled a typical TCP/IP network stack. The building process for this modeling effort led to the discovery of two new idles scans: a SYN backlog idle scan and a RST rate-limited idle scan. The SYN backlog scan is particularly interesting because it does not require whoever is performing the measurements (i. e., the attacker or researcher) to send any packets to the victim (or target) at all. [...] - Second, I developed a hybrid idle scan that combines elements of the SYN backlog idle scan with Antirez's original IPID-based idle scan. This scan enables researchers to test whether two arbitrary machines in the world are able to communicate via TCP/IP, and, if not, in which direction the communication is being prevented. To test the efficacy of the hybrid idle scan, I tested three different kinds of <b>servers</b> (Tor <b>bridges,</b> Tor directory <b>servers,</b> and normal web servers) both inside and outside China. The results were congruent with published understandings of global Internet censorship, demonstrating that the hybrid idle scan is effective. [...] - Third, I applied the hybrid idle scan to the difficult problem of characterizing inconsistencies in the Great Firewall of China (GFW), which is the largest firewall in the world. This effort resolved many open questions about the GFW. The result of my dissertation work is an effective method for measuring Internet censorship around the world, without requiring any kind of distributed measurement platform or access to any of the machines that connectivity is tested to or from. Computer ScienceDoctoralUniversity of New Mexico. Dept. of Computer ScienceCrandall, JadidiahSavage, StefanFaloutsos, MichalisLane, Terra...|$|R
40|$|PosterClimate change {{impact is}} a {{transversal}} assessment in different studies. However, {{there is often}} a complex pathway usually requiring programming skills, from the need to the usage of climate data for different kind of modeling purposes. The CliPick tool provides an user-friendly interface to access climate datasets, that are used to supply climate scenarios for the International Panel on Climate Change (IPCC). The most common format used by climate modelers to exchange data is the network common data format (NETCDF) which is a binary file containing array-oriented scientific data that ease data exchange between climate modelers. Another common characteristic of climate datasets is their large size. Even with binary formats, the large areas (e. g. European scale, including oceans) and the time span (often 100 years in future) delivers an amount of information that needs to be mined to use for other modeling areas requiring simpler and easy access data. Clipick is a web based interface that uses the application programming interface of GoogleMaps and the javascript libraries of DOJO and JQuery as client side programming while PHP and Python languages are used at server side. AJAX, a programming technique <b>bridging</b> client and <b>server</b> sides, eases the communication between the user inputs and translates them into server requests to access the data. There are three steps to retrieve data from an end user point of view: 1) Provide the following details: a) time frame to retrieve data (between 1951 to 2100), b) time step of the data to be retrieved (daily or monthly), c) file format for use of the data (YieldSAFE (van der Werf et al. 2007) or HiSAFE models (Talbot 2011), d) dataset source (currently only HadRM 3 Q 0, but other may be added as needed); 2) Select an approximate location of the plot/stand to simulate, by moving an icon in the map through the GoogleMaps interface; 3) Data extraction. After the last step a link is built to download a text file with the data. Additional extractions can be executed as they will be queued. The climate scenario being used is one of several available in the ENSEMBLES dataset repository ([URL] The Hadley Center Regional Model 3 Q 0 (HadRM 3 Q 0) was the model initially chosen because Clipick was firstly developed in Portugal and, according to Soares et al. (2012), this is the most suited model for Portuguese context. However, this model seems to be suitable for other regions {{and the quality of the}} data is being verified by the AGFORWARD EU consortium partners. Nevertheless, Clipick is ready to add other ENSEMBLES datasets if needed. The scenario A 1 B is considered a moderate climate change scenario that is being widely used by the scientific community. Clipick accesses 8 climate variables across continental Europe (minimum, mean and maximum temperature, precipitation, radiation, minimum and maximum relative humidity and wind speed). Each variable is stored in 15 blocks of ten years each (1951 - 2100) and each block of ten years has about 450 MB (daily data). In total there are about 54 GB of data for a climate scenario, for the eight climate variables. The tool was built to be flexible regarding the addition of other climate variables and scenario datasets, in order to further supply different input formats to other modeling tools needing climate data. The tool is available @ [URL]...|$|R

