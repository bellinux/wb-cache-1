17|10000|Public
5000|$|The <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> is the {{difference}} between an estimator's expected value and the true value of the parameter being estimated.|$|E
5000|$|The {{jackknife}} technique {{can be used}} {{to estimate}} the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> calculated over the entire sample. Say [...] is the calculated estimator of the parameter of interest based on all [...] observations. Letwhere [...] is the estimate of interest based on the sample with the ith observation removed, and [...] is the average of these [...] "leave-one-out" [...] estimates.The jackknife estimate of the bias of [...] is given by: ...|$|E
5000|$|As a {{quantitative}} measure, the [...] "forecast bias" [...] {{can be specified}} as a probabilistic or statistical property of the forecast error. A typical measure of bias of forecasting procedure is the arithmetic mean or expected value of the forecast errors, but other measures of bias are possible. For example, a median-unbiased forecast would be one where half of the forecasts are too low and half too high: see <b>Bias</b> <b>of</b> <b>an</b> <b>estimator.</b>|$|E
3000|$|Windowing {{reduces the}} <b>bias</b> (the <b>bias</b> <b>of</b> <b>a</b> {{spectrum}} <b>estimator</b> θ {{is defined as}} the expected difference between the estimated value and the true value of the spectrum θ being estimated and is defined as bias(θ)=E[(θ-θ)] [...]), but it does not reduce the variance of the spectral estimate [14, 15]; therefore, the variance of the cepstral/filterbank features computed from this estimated spectrum remains large.|$|R
5000|$|In statistics, {{the bias}} (or <b>bias</b> function) <b>of</b> <b>an</b> <b>estimator</b> is the {{difference}} between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. Otherwise the estimator is said to be biased. In statistics, [...] "bias" [...] is <b>an</b> objective property <b>of</b> <b>an</b> <b>estimator,</b> and while not a desired property, it is not pejorative, unlike the ordinary English use of the term [...] "bias".|$|R
40|$|In this note, we {{introduce}} <b>a</b> simple nonparametric <b>estimator</b> for {{the slope}} <b>of</b> <b>a</b> monotone nondecreasing boundaries. The estimator {{may be used}} to correct the <b>bias</b> <b>of</b> <b>a</b> well-known <b>estimator</b> <b>of</b> <b>a</b> nondecreasing boundary, and also be used to give some important interpretations on economic systems. Our estimator is so simple that it does not involve any auxiliary smoothing parameter which usually appears in most of the nonparametric function estimation techniques and whose selection is often a hurdle for practical implementation of the nonparametric methods. Bias estimation Curve estimation Free disposal hull estimator Frontier estimation Productivity analysis...|$|R
40|$|Use of {{bootstrap}} {{for statistical}} {{problems related to}} estimation of parameters. Use of bootstrap for statistical problems related to estimation of parameters. This paper describes bootstrapping for the estimation of the standard error and the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> and for computing confidence intervals. The methods are illustrated by an example...|$|E
40|$|AbstractThe maximum {{asymptotic}} <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> is {{a global}} robustness measure of its performance. The projection median estimator for multivariate location shows a remarkable behavior regarding asymptotic bias. In this paper we consider a modification of the projection median estimator which renders an estimate with better bias performance for point mass contaminations (the worst situation for the projection median estimator). Moreover, it achieves the lowest bound for an equivariant estimate for point mass contaminations...|$|E
40|$|The maximum {{asymptotic}} <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> is {{a global}} robustness measure of its performance. The projection median estimator for multivariate location shows a remarkable behavior regarding asymptotic bias. In this paper we consider a modification of the projection median estimator which renders an estimate with better bias performance for point mass contaminations (the worst situation for the projection median estimator). Moreover, it achieves the lowest bound for an equivariant estimate for point mass contaminations. Projection estimates Maximum bias Robust estimates Multivariate location...|$|E
40|$|The {{traditional}} approach to obtain valid confidence intervals for nonparametric quantities is {{to select a}} smoothing parameter such that the <b>bias</b> <b>of</b> the estimator is negligible relative to its standard deviation. While this approach is apparently simple, it has two drawbacks: First, the question of optimal bandwidth selection is no longer well-defined, as {{it is not clear}} what ratio <b>of</b> <b>bias</b> to standard deviation should be considered negligible. Second, since the bandwidth choice necessarily deviates from the optimal (mean squares-minimizing) bandwidth, such a confidence interval is very inefficient. To address these issues, we construct valid confidence intervals that account for the presence <b>of</b> <b>a</b> nonnegligible <b>bias</b> and thus make it possible to perform inference with optimal mean squared error minimizing bandwidths. The key difficulty in achieving this involves finding a strict, yet feasible, bound on the <b>bias</b> <b>of</b> <b>a</b> nonparametric <b>estimator.</b> It is well-known that it is not possible to consistently estimate the pointwise <b>bias</b> <b>of</b> <b>an</b> optimal nonparametric <b>estimator</b> (for otherwise, one could subtract it and obtain a faster convergence rate violating Stone's bounds on optimal convergence rate). Nevertheless, we find that, under minimal primitive assumptions, it is possible to consistently estimate an upper bound on the magnitude <b>of</b> the <b>bias,</b> which is sufficient to deliver a valid confidence interval whose length decreases at the optimal rate and which does not contradict Stone's results...|$|R
40|$|Analytic {{evaluation}} of heteroskedasticity consistent covariance matrix estimates (HCCME) {{is difficult because}} {{of the complexity of}} the formulae currently available. We obtain new analytic formulae for the <b>bias</b> <b>of</b> <b>a</b> class <b>of</b> <b>estimators</b> of the covariance matrix <b>of</b> OLS in <b>a</b> standard linear regression model. These formulae provide substantial insight into the properties and performance characteristics of these estimators. In particular, we find <b>a</b> new <b>estimator</b> which minimizes the maximum possible bias and improves substantially on the standard Eicker-White estimate. ...|$|R
40|$|The <b>bias</b> bound {{function}} <b>of</b> <b>an</b> <b>estimator</b> is <b>an</b> important quantity {{in order}} to perform globally robust inference. We show how to evaluate the exact bias bound for the minimax estimator of the location parameter for <b>a</b> wide class <b>of</b> unimodal symmetric location and scale family. We show, by an example, how to obtain <b>an</b> upper bound <b>of</b> the <b>bias</b> bound for a unimodal asymmetric location and scale family. We provide the exact <b>bias</b> bound <b>of</b> the minimum distance/disparity <b>estimators</b> under <b>a</b> contamination neighborhood generated from the same distance...|$|R
40|$|In {{estimating}} insect {{density and}} impact, entomologists are understandably interested in accuracy of estimation, but they almost always {{are dealing with}} precision because of bias due to an invalid estimator, probability sampling, or nonsampling errors. Definitions related to statistical estimation are reviewed and the concepts of accuracy and precision examined. Interval estimation and optimum sample size determination related to accuracy and precision, using the concept of allowable error, are examined. Criteria for selecting the best estimator in tenns of accuracy and precision are presented, and the distortion of probability statements due to bias is discussed. Accuracy and precision are compared and contrasted using two examples: (I) estimating insect density and (2) estimating insect impact. Adjusted and more accurate estimators can be obtained if the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> can be estimated from a preliminary sample...|$|E
40|$|A bias is {{generally}} {{understood as a}} prejudice in the sense for having a predilection to one particular point of view or ideological perspective. Statistical bias is a quantifiable tendency of a statistical estimator to over or underestimate the quantity that is being estimated. In formal terms, if ˆ θ is an estimator of a parameter θ, then ˆ θ is biased if the expected value of θ, E (ˆ θ), is not equal to θ. Bias is a concern when any process {{is supposed to be}} objective and fair. Similarly statistical <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> should be of concern, as that bias can influence results and undermine the objectivity of conclusions. For example, systematic errors increase the likelihood of assigning statistical significance to chance events (Type I error). Given an hypothesis H 0 : ˆ θ = θ and the alternativ...|$|E
40|$|Maximum {{likelihood}} {{quantum state}} tomography yields estimators that are consistent, {{provided that the}} likelihood model is correct, but the maximum likelihood estimators may have bias for any finite data set. The <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> {{is the difference between}} the expected value of the estimate and the true value of the parameter being estimated. This paper investigates bias in the widely used maximum likelihood quantum state tomography. Our goal is to understand how the amount of bias depends on factors such as the purity of the true state, the number of measurements performed, and the number of different bases in which the system is measured. For that, we perform numerical experiments that simulate optical homodyne tomography under various conditions, perform tomography, and estimate bias in the purity of the estimated state. We find that estimates of higher purity states exhibit considerable bias, such that the estimates have lower purity than the true states. Comment: New version where we fixed error in evenly spaced phases. Expanded analysis of evenly spaced phases. 8 figures. Submitted to Phys. Rev. ...|$|E
40|$|Parametrically guided nonparametric {{estimation}} is {{an attractive}} method that allows to improve the <b>bias</b> <b>of</b> <b>a</b> nonparametric <b>estimator</b> by using <b>a</b> parametric pilot <b>estimator.</b> The aim of this dissertation is to generalize the parametrically guided nonparametric estimation to randomly right-censored data. The generalization is performed in three different contexts. First, based on the Kaplan-Meier (1958) estimator, we provide new parametrically guided kernel density and hazard rate estimators. Then, we investigate the parametrically guided local linear regression and the parametrically guided quasi-likelihood estimation using a synthetic data approach. The asymptotic properties of the new-guided estimators {{as well as their}} finite sample performance are investigated and compared with the corresponding unguided nonparametric estimators via numerical studies and applications to real data. The results confirm the bias reduction property and show that using an appropriate guide and the optimal bandwidth the guided estimators outperform the classical nonparametric estimators. (SC - Sciences) [...] UCL, 201...|$|R
40|$|Consider s+ 1 {{univariate}} normal populations {{with common}} variance [sigma] 2 and means [mu]i, i= 0, 1, [...] .,s, {{constrained by the}} tree-order restrictions [mu]i[greater-or-equal, slanted][mu] 0, i= 1, 2, [...] .,s. For certain sequences [mu] 0,[mu] 1, [...] . the maximum likelihood-based estimator (MLBE) of [mu] 0 diverges to -[infinity] as s [...] >[infinity] and its bias is unbounded. By contrast, the <b>bias</b> <b>of</b> <b>an</b> alternative <b>estimator</b> <b>of</b> [mu] 0 proposed by Cohen and Sackrowitz (J. Statist. Plan. Infer. 107 (2002) 89 - 101) remains bounded. In this note the <b>biases</b> <b>of</b> the MLBEs of the other components [mu] 1,[mu] 2, [...] . are studied and compared to the <b>biases</b> <b>of</b> the corresponding Cohen-Sackrowitz estimators (CSE). Unlike the MLBE of [mu] 0, the MLBEs of [mu]i for i[greater-or-equal, slanted] 1, are asymptotically unbiased in most cases. By contrast, the CSEs of [mu]i, i= 1, 2, [...] .,s more often have nonzero asymptotic bias. Order-restricted inference Tree order Maximum likelihood estimator Cohen-Sackrowitz estimator Bias...|$|R
40|$|A major {{difficulty}} {{in understanding the}} properties of variable bandwidth methods (Breiman, Meisel & Purcell, 1977; Abramson, 1982) is that extremely lengthy and complex algebra is needed to assess the influence <b>of</b> <b>bias.</b> Indeed, the complexity {{is so great that}} it has forced investigators to use computer algebraic manipulation to determine formulae for bias. In this note we completely eliminate these algebraic obstacles by presenting a simple easy-to-use formula which gives explicitly the <b>bias</b> <b>of</b> <b>a</b> variable bandwidth <b>estimator,</b> to arbitrarily high order, in very general problems. Some key words: Bias reduction; Density estimation; Nonparametric regression. 1...|$|R
40|$|Data such {{as income}} are {{frequently}} rounded. Rounding may {{be done to}} protect the confidentiality of records in a file, to enhance the readability of the data, or to simplify the data values under {{the notion that the}} digits subject to rounding are inconsequential. The rounding may not have any effect on the <b>bias</b> <b>of</b> <b>an</b> <b>estimator,</b> but it may have a large impact on variance. Integers can be expressed as X = qB + r, where q is the quotient, B is the base, and r is the remainder. B is a constant, but q and r are random variables. We will investigate four rules for rounding “r ” above and observe the effects of rounding on bias and variance. We will assume a uniform distribution for r, but no specific distributional assumption will be made for “q. ” When q = 0, we will show that the variance of the data after rounding is three times the variance before rounding. As the variance of q gets larger, the effect of rounding on the variance decreases...|$|E
40|$|We propose an {{adaptive}} data collection procedure for call prioritization {{in the context}} of computer-assisted telephone interview surveys. Our procedure is adaptive {{in the sense that the}} effort assigned to a sample unit may vary from one unit to another and may also vary during data collection. The goal of {{an adaptive}} procedure is usually to increase quality for a given cost or, alternatively, to reduce cost for a given quality. The quality criterion often considered in the literature is the nonresponse <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> that is not adjusted for nonresponse. Although the reduction of the nonresponse bias is a desirable goal, we argue that it is not a useful criterion to use at the data collection stage of a survey because the bias that can be removed at this stage through an adaptive collection procedure can also be removed at the estimation stage through appropriate nonresponse weight adjustments. Instead, we develop a procedure of call prioritization that, given the selected sample, attempts to minimize the conditional variance of a nonresponse-adjusted estimator subject to an overall budget constraint. We evaluate the performance of our procedure in a simulation study...|$|E
40|$|The <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> {{is defined}} as the {{difference}} of its expected value from the parameter to be estimated, where the expectation is with respect to the model. Loosely speaking, small bias reflects the desire that if an experiment is repeated indefinitely then the average of all the resultant estimates will be close to the parameter value that is estimated. The current paper is a review of the still-expanding repository of methods that have been developed to reduce bias in the estimation of parametric models. The review provides a unifying framework where all those methods are seen as attempts to approximate the solution of a simple estimating equation. Of particular focus is the maximum likelihood estimator, which despite being asymptotically unbiased under the usual regularity conditions, has finite-sample bias that can result in significant loss of performance of standard inferential procedures. An informal comparison of the methods is made revealing some useful practical side-effects in the estimation of popular models in practice including: i) shrinkage of the estimators in binomial and multinomial regression models that guarantees finiteness even in cases of data separation where the maximum likelihood estimator is infinite, and ii) inferential benefits for models that require the estimation of dispersion or precision parameters...|$|E
40|$|Local {{likelihood}} methods enjoy advantageous properties, such as {{good performance}} {{in the presence of}} edge effects, that are rarely found in other approaches to nonparametric density estimation. However, as we argue in this paper, standard kernel methods can have distinct advantages when edge effects are not present. We show that, whereas the integrated variances of the two methods are virtually identical, the integrated squared <b>bias</b> <b>of</b> <b>a</b> conventional kernel <b>estimator</b> is less than that <b>of</b> <b>a</b> local log-linear <b>estimator</b> by as much as <b>a</b> factor <b>of</b> 4. Moreover, the greatest bias improvements offered by kernel methods occur when they are needed most-i. e. when the effect <b>of</b> <b>bias</b> is particularly high. Similar comparisons can also be made when high degree local log-polynomial fits are assessed against high order kernel methods. For example, although (as is well known) high degree local polynomial fits offer potentially infinite efficiency gains relative to their kernel competitors, the converse is also true. Indeed, the asymptotic value of the integrated squared <b>bias</b> <b>of</b> <b>a</b> local log-quadratic <b>estimator</b> can exceed any given constant multiple of that for the competing kernel method. In all cases the densities that suffer problems in the context of local log-likelihood methods can be chosen to be symmetric, either unimodal or bimodal, either infinitely or compactly supported, and to have arbitrarily many derivatives as functions on the real line. They are not pathological. However, our results reveal quantitative differences between global performances of local log-polynomial estimators applied to unimodal or multimodal distributions. Copyright 2002 Royal Statistical Society. ...|$|R
40|$|We {{introduce}} two {{new methods}} for estimating the Marginal Data Density (MDD) from the Gibbs output, {{which are based}} on exploiting the analytical tractability condition. Such a condition requires that some parameter blocks can be analytically integrated out from the conditional posterior densities. Our estimators are applicable to densely parameterized time series models such as VARs or DFMs. An empirical application to six-variate VAR models shows that the <b>bias</b> <b>of</b> <b>a</b> fully computational <b>estimator</b> is sufficiently large to distort the implied model rankings. One estimator is fast enough to make multiple computations of MDDs in densely parameterized models feasible. Marginal likelihood, Gibbs sampler, time series econometrics, Bayesian econometrics, reciprocal importance sampling...|$|R
40|$|Causal {{inference}} on <b>a</b> population <b>of</b> units connected {{through a}} network often presents technical challenges, including how {{to account for}} interference. In the presence of local interference, for instance, potential outcomes <b>of</b> <b>a</b> unit depend on its treatment {{as well as on}} the treatments of other local units, such as its neighbors according to the network. In observational studies, a further complication is that the typical unconfoundedness assumption must be extended - say, to include the treatment of neighbors, and individual and neighborhood covariates - to guarantee identification and valid inference. Here, we derive analytical expressions for the <b>bias</b> <b>of</b> <b>a</b> naive <b>estimator</b> that wrongly assumes away interference. The bias depends on the level of interference but also on the degree of association between individual and neighborhood treatments. We propose an extended unconfoundedness assumption that accounts for interference, and we develop new covariate-adjustment methods that lead to valid estimates of treatment and interference effects in observational studies on networks. Valid estimation is based on a generalized propensity score that balances individual and neighborhood covariates across units under different levels of individual treatment and of exposure to neighbors' treatment. We illustrate how the proposed approach improves over alternative methods in a simulation study, leveraging Add-Health network and covariate data...|$|R
40|$|In this thesis, {{a general}} {{approach}} for correcting the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> and for obtaining {{estimates of the}} accuracy of the bias-corrected estimator is proposed. The method, entitled extended bootstrap bias correction (EBS), is based on the bootstrap resampling technique and attempts to identify the functional relationship between the estimates obtained from the original and bootstrap samples and the true parameter values, drawn from a plausible parameter space. The bootstrap samples are used for studying the behaviour of the bias and, consequently, for the bias correction itself. The EBS approach is assessed by extensive Monte Carlo studies in three different applications of multilevel analysis of survey data. First, the proposed EBS method is applied to bias adjustment of unweighted and probability weighted estimators of two-level model parameters under informative sampling designs with small sample sizes. Second, the EBS approach is considered for estimating the mean squared error (MSE) of predictors of small area means under the area level Fay-Herriot model for different distributions of the model error terms. Finally, the EBS procedure is applied to MSE estimation of predictors of small area proportions under a unit level generalized linear mixed model. The general conclusion emerging from this thesis is that the EBS approach is effective in providing bias corrected estimators in all the three cases considered. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Let A be action {{space and}} a ∈ A be an action. For example, in {{estimation}} problems, A is the set of real numbers and a is a number, say a = 2 is adopted as an estimator of θ ∈ Θ. In other words, inference maker “took ” action a = 2 in estimating θ. The action, {{as a function of}} observations is called rule. For example, the rule is a(X 1, [...] ., Xn) = ¯ X. Often, the rules are denoted by δ(X). No action can be taken without potential losses. Statisticians are pessimistic creatures that replaced nicely coined term utility to a more loss, although, for all practical purposes, the loss is negative utility. The loss function is denoted by L(θ, a) and represents the loss for a decision maker (statistician) if he takes an action a ∈ A, and the real state of nature is θ ∈ Θ. The loss function usually satisfies the following properties, L(a, a) = 0 and L(a, θ) is nondecreasing function of |a − θ|. Examples are squared error loss (SEL) L(θ, a) = (θ − a) 2, absolute loss, L(θ, a) = |θ − a|, the 0 - 1 loss, L(θ, a) = 1 (|a − θ |> m), etc. The most common for estimation problems and mathematically easiest to work with is the SEL. The expected SEL (frequentist risk) is linked with variance and <b>bias</b> <b>of</b> <b>an</b> <b>estimator,</b> E X|θ (θ − δ(X)) 2 = V ar(δ(X)) + [bias(δ(X)) ] 2...|$|E
30|$|However, the {{original}} algorithm proposed in [36] has a drawback {{in that the}} bias of estimates is too large owing to the noise correlation between the regressor and regressand in the weighted least-squares (WLS) formulation and some nonlinear operations. Especially when the noise level is high or the localization geometry is not good enough, the bias becomes large and seriously affects the localization performance. Moreover, in some modern applications, we can obtain multiple independent measurements {{in a short time}} period. The localization performance can be improved by averaging these estimates from multiple independent measurements. Nevertheless, this operation only reduces the variance and not the bias. In tracking applications, the bias problem remains because the measurements made at different instants are coherent [37]. It is therefore necessary to reduce the bias to improve the localization performance. Over the years, many studies have reduced the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> using TDOAs [38 – 45]. Two methods of reducing the bias of the closed-form solution using TDOAs have been proposed [38], but the receiver position errors and synchronization clock bias were not taken into account. One study [39] proposed a bias-reduced method for a two-sensor (or two-receiver) positioning system based on TDOA and AOA measurements in the presence of sensor errors. The simulation validates the availability of the proposed method. Moreover, an improved algebraic solution employing new stage- 2 processing for the TDOA with sensor position errors has been proposed [40]. Simulation results show lower estimation bias; however, this method only improves the stage- 2 processing, and the bias introduced in stage 1 needs to be further reduced.|$|E
40|$|AbstractIn some {{applications}} of kernel density estimation the data {{may have a}} highly non-uniform distribution and be confined to a compact region. Standard fixed bandwidth density estimates can struggle {{to cope with the}} spatially variable smoothing requirements, and will be subject to excessive bias at the boundary of the region. While adaptive kernel estimators can address the first of these issues, the study of boundary kernel methods has been restricted to the fixed bandwidth context. We propose a new linear boundary kernel which reduces the asymptotic order <b>of</b> the <b>bias</b> <b>of</b> <b>an</b> adaptive density <b>estimator</b> at the boundary, and is simple to implement even on an irregular boundary. The properties of this adaptive boundary kernel are examined theoretically. In particular, we demonstrate that the asymptotic performance of the density estimator is maintained when the adaptive bandwidth is defined in terms <b>of</b> <b>a</b> pilot estimate rather than the true underlying density. We examine the performance for finite sample sizes numerically through analysis of simulated and real data sets...|$|R
40|$|In some {{applications}} of kernel density estimation the data {{may have a}} highly non-uniform distribution and be confined to a compact region. Standard fixed bandwidth density estimates can struggle {{to cope with the}} spatially variable smoothing requirements, and will be subject to excessive bias at the boundary of the region. While adaptive kernel estimators can address the first of these issues, the study of boundary kernel methods has been restricted to the fixed bandwidth context. We propose a new linear boundary kernel which reduces the asymptotic order <b>of</b> the <b>bias</b> <b>of</b> <b>an</b> adaptive density <b>estimator</b> at the boundary, and is simple to implement even on an irregular boundary. The properties of this adaptive boundary kernel are examined theoretically. In particular, we demonstrate that the asymptotic performance of the density estimator is maintained when the adaptive bandwidth is defined in terms <b>of</b> <b>a</b> pilot estimate rather than the true underlying density. We examine the performance for finite sample sizes numerically through analysis of simulated and real data sets. Adaptive smoothing Boundary bias Edge effects Kernel estimator Variable bandwidth...|$|R
40|$|Suppose we have <b>a</b> {{parametric}} family <b>of</b> probability distributions with a {{likelihood function}} f(x, θ) for one observation, where f(x, θ) is a {{probability mass function}} for a discrete distribution or a probability density function for a continuous distribution. Let Eθ denote expectation, and Pθ probability, when θ is the true value of the parameter. Let X = (X 1, [...] ., Xn) be <b>a</b> vector <b>of</b> i. i. d. observations with distribution Pθ. Suppose g = g(θ) is <b>a</b> real-valued function <b>of</b> the parameter θ. One criterion for choosing <b>an</b> <b>estimator</b> T = T(X) of g(θ) is to minimize the mean-squared error (MSE) Eθ((T(X) − g(θ)) 2). Recall that T is called <b>an</b> unbiased <b>estimator</b> <b>of</b> g(θ) if EθT(X) = g(θ) for all θ. More generally, the <b>bias</b> <b>of</b> T as <b>an</b> <b>estimator</b> <b>of</b> g(θ) is defined by bT(θ) : = bT,g(θ) : = EθT − g(θ) for all θ. Thus T is unbiased as <b>an</b> <b>estimator</b> <b>of</b> g(θ) {{if and only if}} bT(θ) = 0 for all θ. If Eθ(T 2) < + ∞ for all θ, let Varθ(T) be the variance of T for the given θ, which equals Eθ(T 2) − (EθT) 2. The MSE equals the variance plus the bias squared, as follows: Theorem. For any statistic T(X) such that Eθ(T 2) < ∞ for all θ and any real-valued function g(θ), the mean-square error <b>of</b> T as <b>an</b> <b>estimator</b> <b>of</b> g is given b...|$|R
40|$|Let Xj be a {{sequence}} of independent, identically distributed random variables taking value in some space X, all with pdf f(x|θ) for some uncertain parameter θ ∈ Θ, and let Tn(X 1, [...] ., Xn) be {{a sequence}} of estimators of θ — i. e., of functions Tn: X n → Θ intended to satisfy Tn(X) ≈ θ. An example {{to keep in mind}} would be X ∼ No(θ, 1) with θ ∈ Θ = R, and Tn(X) = X n = (X 1 + · · · + Xn) /n. Let us now explore different ways of making the intention “Tn(X) ≈ θ ” more precise. 1. 1. Bias The <b>Bias</b> <b>of</b> <b>an</b> <b>estimator</b> Tn(x) is simply the expected difference, βn(θ) : = E[Tn(X) − θ | θ]. An estimator is called unbiased if βn ≡ 0, i. e., if E[Tn(X) | θ] ≡ θ for all n ∈ N and all θ ∈ Θ, and an estimator sequence is called asymptotically unbiased if βn → 0 as n → ∞, i. e., lim n→ ∞ E[Tn(X) | θ] = θ. An unbiased estimator will satisfy the goal “Tn(X) ≈ θ ” {{in the sense that the}} average value of Tn(X) over many replications will be θ. This will offer little or no comfort to someone using the estimator only once, since the possibility remains that perhaps [Tn(X) −θ] will be hugely positive with high probability, and hugely negative with high probability, with large deviations 1 that average out to zero in the end. Asymptotic unbiasedness offers even less comfort, but its absence would be alarming. What is needed are bounds or conditions on how large |Tn(X) − θ | can be. Most of the criteria below depend, in one way or another, on the quadrati...|$|E
40|$|This {{dissertation}} {{consists of}} three projects in matched case-control studies. In the first project, we employ a general bias preventive approach developed by Firth (1993) to handle the <b>bias</b> <b>of</b> <b>an</b> <b>estimator</b> of the log-odds ratio parameter in conditional logistic regression by solving a modified score equation. The resultant estimator not only reduces bias but also can prevent producing infinite value. Furthermore, we propose a method to calculate the standard error of the resultant estimator. A closed form expression for the estimator of the log-odds ratio parameter is derived {{in the case of}} a dichotomous exposure variable. Finite sample properties of the estimator are investigated via a simulation study. Finally, we apply the method to analyze a matched case-control data from a low-birth-weight study. In the second project of this dissertation, we propose a score typed test for checking adequacy of a functional form of a covariate of interest in matched case-control studies by using penalized regression splines to approximate an unknown function. The asymptotic distribution of the test statistics under the null model is a linear combination of several chi-square random variables. We also derive the asymptotic distribution of the test statistic when the alternative model holds. Through a simulation study we assess and compare the finite sample properties of the proposed test with that of Arbogast and Lin (2004). To illustrate the usefulness of the method, we apply the proposed test to a matched case-control data constructed from the breast cancer data of the SEER study. Usually a logistic model is needed to associate the risk of the disease with the covariates of interests. However, this logistic model may not be appropriate in some instances. In the last project, we adopt idea to matched case-control studies and derive an information matrix based test for testing overall model adequacy and investigate the properties against the cumulative residual based test in Arbogast and Lin (2004) via a simulation study. The proposed method is less time consuming and has comparative power for small parameters. It is suitable to explore the overall model fitting...|$|E
40|$|The {{bootstrap}} is a resampling {{method for}} statistical inference. It {{is commonly used}} to estimate confidence intervals, {{but it can also}} be used to estimate <b>bias</b> and variance <b>of</b> <b>an</b> <b>estimator</b> or calibrate hypothesis tests. <b>A</b> short <b>of</b> papers illustrative of the diversity of recent environmentric applications of the bootstrap includes toxicology [2], fisheries surveys [27], groundwater and air polution modelling [1, 4], chemometrics [35], hydrology [14], phylogenetics [23], spatial point patterns [33], ecological indices [9], and multivariate summarization [24, 38]. The literature on the bootstrap is extensive. Book length treatments of the concepts, applications, and theory of the bootstrap range in content from those that emphasize applications [19], to comprehensive treatments [13, 5, 3], to those that emphasize theory [11, 15, 18, 28]. Major review papers on the bootstrap and its applications include [12, 8, 37, 7]. Papers describing the bootstrap and demonstrating its use to non statisticians have been published in many different journals. Extensive bibliographies, listing applications, are included in [19] and [3]. This article can not duplicate the comprehensive coverage found in these books and papers. Instead, I will illustrate bootstrap concepts using a simple example, describe different types of bootstraps and some of their theoretical and practical properties, discuss computation and other details, and indicate extensions that are especially appropriate for environmentric data. The methods will be illustrated using data on heavy meta...|$|R
40|$|Graduation date: 1993 Methodologies {{for data}} reduction, modeling, and {{classification}} of grouped response curves are explored. In particular, the thesis {{focuses on the}} analysis <b>of</b> <b>a</b> collection <b>of</b> highly correlated, highly dimensional response-curve data of spectral reflectance curves of wood surface features. In the analysis, questions about the application of cross-validation estimation of discriminant function error rates for data that has been previously transformed by principal component analysis arise. Performing cross-validation requires re-calculating the principal component transformation and discriminant functions of the training sets, a very lengthy process. A more efficient approach of carrying out the cross-validation calculations, plus the alternative of estimating error rates without the re-calculation of the principal component decomposition, are studied to address questions about the cross-validation procedure. If populations are assumed to have common covariance structures, the pooled covariance matrix can be decomposed for the principal component transformation. The leave-one-out cross-validation procedure results in a rank-one update in the pooled covariance matrix for each observation left out. Algorithms {{have been developed for}} calculating the updated eigenstructure under rank-one updates and they {{can be applied to the}} orthogonal decomposition of the pooled covariance matrix. Use of these algorithms results in much faster computation of error rates, especially when the number of variables is large. The <b>bias</b> and variance <b>of</b> <b>an</b> <b>estimator</b> that performs leave-one-out cross-validation directly on the principal component scores (without re-computation of the principal component transformation for each observation) is also investigated...|$|R
40|$|We propose two nonparametric {{transition}} density-based speci {{tests for}} continuous-time diffusion models. In contrast to marginal density {{as used in}} the literature, transition density can capture the full dynamics <b>of</b> <b>a</b> diffusion process, and in particular, can distinguish processes with the same marginal density but different transition densities. To address {{the concerns of the}} sample performance of nonparametric methods in the literature, we introduce an appropriate data transformation and correct the boundary <b>bias</b> <b>of</b> kernel <b>estimators.</b> <b>As</b> a result, our tests are robust to persistent dependence in data and provide reliable inferences for sample sizes often encountered in empirical. Simulation studies show that our tests have reasonable size and good power against <b>a</b> variety <b>of</b> alternatives in samples even for data with highly persistent dependence. Besides the single-factor diffusion models, our tests can be applied to <b>a</b> broad class <b>of</b> dynamic economic models, such as discrete time series models, time-inhomogeneous diffusion models, stochastic volatility models, jump-diffusion models, and multi-factor term structure models. When applied to daily Eurodollar interest rates, our tests overwhelmingly reject some popular spot rate models, including those with nonlinear drifts that some existing tests can not reject after correcting size distortions. We that models with nonlinear drifts do not signi improve the goodness-of-, and the main source of model inadequacy seems to be the violation of the Markov assumption. We also that GARCH, regime switching and jump diffusion models perform signi better than single-factor diffusion models, although they are far from being adequate to fully capture the interest rate dynamics. Our study shows that nonparametric methods are a reliable and powerful tool for analyzing data...|$|R
40|$|The number <b>of</b> {{topics that}} <b>a</b> test {{collection}} contains {{has a direct}} impact on how well the evaluation results reflect the true performance of systems. However, large collections can be prohibitively expensive, so researchers are bound to balance reliability and cost. This issue arises when researchers have an existing collection and they would like to know how much they can trust their results, and also when they are building a new collection and they would like to know how many topics it should contain before they can trust the results. Several measures have been proposed in the literature to quantify the accuracy <b>of</b> <b>a</b> collection to estimate the true scores, as well as different ways to estimate the expected accuracy of hypothetical collections with <b>a</b> certain number <b>of</b> topics. We can find ad-hoc measures such as Kendall tau correlation and swap rates, and statistical measures such as statistical power and indexes from generalizability theory. Each measure focuses on different aspects <b>of</b> evaluation, has <b>a</b> different theoretical basis, and makes <b>a</b> number <b>of</b> assumptions that are not met in practice, such as normality of distributions, homoscedasticity, uncorrelated effects and random sampling. However, how good these estimates are in practice remains a largely open question. In this paper we first compare measures and estimators of test collection accuracy and propose unbiased statistical estimators of the Kendall tau and tau AP correlation coefficients. Second, we detail a method for stochastic simulation of evaluation results under different statistical assumptions, which can be used for <b>a</b> variety <b>of</b> evaluation research where we need to know the true scores of systems. Third, through large-scale simulation from TREC data, we analyze the <b>bias</b> <b>of</b> <b>a</b> range <b>of</b> <b>estimators</b> of test collection accuracy. Fourth, we analyze the robustness to statistical assumptions of these estimators, in order to understand what aspects <b>of</b> <b>an</b> evaluation are affected by what assumptions and guide in the development of new collections and new measures. All the results in this paper are fully reproducible with data and code available online. This work was supported by an A 4 U postdoctoral grant, a Juan de la Cierva postdoctoral fellowship and the Spanish Government (HAR 2011 - 27540). I am very thankful to Mónica Marrero, the anonymous reviewers and the editors for their help in making this paper. Thanks also to Rafa Nadal for convincing El Gran Guasch to stop shouting “!‘La Décima!” [...] . that was definitely it...|$|R
