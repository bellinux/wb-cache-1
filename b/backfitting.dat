222|0|Public
5000|$|The <b>backfitting</b> {{algorithm}} is then: [...] Initialize , Do until [...] converge: For each predictor j: (a) [...] (<b>backfitting</b> step) (b) [...] (mean centering of estimated function) ...|$|E
5000|$|The {{original}} GAM fitting method {{estimated the}} smooth {{components of the}} model using non-parametric smoothers (for example smoothing splines or local linear regression smoothers) via the <b>backfitting</b> algorithm. <b>Backfitting</b> works by iterative smoothing of partial residuals and provides a very general modular estimation method capable of using {{a wide variety of}} smoothing methods to estimate the [...] terms. A disadvantage of <b>backfitting</b> is {{that it is difficult to}} integrate with the estimation of the degree of smoothness of the model terms, so that in practice the user must set these, or select between a modest set of pre-defined smoothing levels.|$|E
5000|$|As well, the {{solution}} {{found by the}} <b>backfitting</b> procedure is non-unique. If [...] is a vector such that [...] from above, then if [...] is a solution then so is [...] is also a solution for any [...] A modification of the <b>backfitting</b> algorithm involving projections onto the eigenspace of S can remedy this problem.|$|E
5000|$|In general, {{estimation}} in FGAM requires combining IWLS with <b>backfitting.</b> However, if {{the expansion}} coefficients are obtained as functional principal components, then {{in some cases}} (e.g. Gaussian predictor function [...] ), they will be independent in which case <b>backfitting</b> is not needed, and one can use popular smoothing methods for estimating the unknown parameter functions [...]|$|E
50|$|In statistics, the <b>backfitting</b> {{algorithm}} {{is a simple}} iterative procedure used to fit a generalized additive model. It was introduced in 1985 by Leo Breiman and Jerome Friedman along with generalized additive models. In most cases, the <b>backfitting</b> {{algorithm is}} equivalent to the Gauss - Seidel method algorithm for solving a certain linear system of equations.|$|E
5000|$|For the two {{dimensional}} case, we can formulate the <b>backfitting</b> algorithm explicitly. We have: ...|$|E
5000|$|We can {{modify the}} <b>backfitting</b> {{algorithm}} {{to make it}} easier to provide a unique solution. Let [...] be the space spanned by all the eigenvectors of Si that correspond to eigenvalue 1. Then any b satisfying [...] has [...] and [...] Now if we take [...] to be a matrix that projects orthogonally onto , we get the following modified <b>backfitting</b> algorithm: ...|$|E
5000|$|If we denote [...] as the {{estimate}} of [...] in the ith updating step, the <b>backfitting</b> steps are ...|$|E
50|$|Which is the {{standard}} formulation of a Generalized Additive Model. It was then shown that the <b>backfitting</b> algorithm will always converge for these functions.|$|E
50|$|Looking at the {{abbreviated}} form {{it is easy}} to see the <b>backfitting</b> algorithm as {{equivalent to}} the Gauss - Seidel method for linear smoothing operators S.|$|E
5000|$|Aside: Previously-fitted pairs can be readjusted {{after new}} fit-pairs are {{determined}} by an algorithm known as [...] <b>backfitting,</b> which entails reconsidering a previous pair, recalculating the residual given how other pairs have changed, refitting to account for that new information, and then cycling through all fit-pairs this way until parameters converge. This process typically results in a model that performs better with fewer fit-pairs, though it takes longer to train, and it is usually possible {{to achieve the same}} performance by skipping <b>backfitting</b> and simply adding more fits to the model (increasing r).|$|E
5000|$|Initialize ,, [...] Do until [...] converge: Regress [...] {{onto the}} space , setting [...] For each {{predictor}} j: Apply <b>backfitting</b> update to [...] using the smoothing operator , yielding new estimates for ...|$|E
50|$|Currently, work {{is being}} done to {{estimate}} VGAMs using P-splines of Eilers and Marx (1996).This allows for several advantages over using smoothing splines and vector <b>backfitting,</b> such as theability to perform automatic smoothing parameter selection easier.|$|E
5000|$|Where , [...] and [...] The {{functions}} [...] {{are unknown}} smooth functions fit from the data. Fitting the AM (i.e. the functions [...] ) {{can be done}} using the <b>backfitting</b> algorithm proposed by Andreas Buja, Trevor Hastie and Robert Tibshirani (1989).|$|E
5000|$|An exact {{solution}} {{of this is}} infeasible to calculate for large np, so the iterative technique of <b>backfitting</b> is used. We take initial guesses [...] and update each [...] in turn to be the smoothed fit for the residuals of all the others: ...|$|E
50|$|Unfortunately, {{though the}} Kolmogorov-Arnold {{representation}} theorem asserts {{the existence of}} a function of this form, it gives no mechanism whereby one could be constructed. Certain constructive proofs exist, but they tend to require highly complicated (i.e. fractal) functions, and thus are not suitable for modeling approaches. It is not clear that any step-wise (i.e. <b>backfitting</b> algorithm) approach could even approximate a solution. Therefore, the Generalized Additive Model drops the outer sum, and demands instead that the function belong to a simpler class.|$|E
5000|$|Boosting {{is a form}} {{of linear}} {{regression}} in which the features of each sample [...] are the outputs of some weak learner [...] applied to [...] Specifically, in the case where all weak learners are known a priori, AdaBoost corresponds to a single iteration of the <b>backfitting</b> algorithm in which the smoothing splines are the minimizers of , that is: [...] fits an exponential cost function and is linear with respect to the observation. Thus, boosting is seen to be a specific type of linear regression.|$|E
5000|$|For {{steepest descent}} {{versions}} of AdaBoost, where [...] is chosen at each layer t to minimize test error, the next layer added {{is said to}} be maximally independent of layer t: it is unlikely that a weak learner t+1 will be chosen that is similar to learner t. However, there remains the possibility that t+1 produces similar information to some other earlier layer. Totally corrective algorithms, such as LPBoost, optimize the value of every coefficient after each step, such that new layers added are always maximally independent of every previous layer. This can be accomplished by <b>backfitting,</b> linear programming or some other method.|$|E
5000|$|For {{the output}} of the {{computer}} graphics include factory only a character generator, the text or graphic symbols (in [...] "quasi Graphics") with a 32 × 32 characters with 8 × 8 pixels can represent. The needed for this to ROM located fixed character set contains 96 alphanumeric and control characters and graphic symbols 146. 12 A pixel graphics mode ("graphic") is by default not available, but can be supplemented in DIY. For instructions were published in various magazines and books until the early 1990s. The black-and-white image is output through the coaxial RF antenna port on a standard TV, the <b>backfitting</b> of color representation are possible. The operation of the keyboard and the control of the tape recorder via the built-in computers input and output port U855-PIO (English Parallel Input Output).|$|E
40|$|In this paper, {{we study}} the {{ordinary}} <b>backfitting</b> and smooth <b>backfitting</b> as methods of fitting additive quantile models. We show that these <b>backfitting</b> quantile estimators are asymptotically {{equivalent to the}} corresponding <b>backfitting</b> estimators of the additive components in a specially-designed additive mean regression model. This implies that the theoretical properties of the <b>backfitting</b> quantile estimators are not unlike those of <b>backfitting</b> mean regression estimators. We also assess the finite sample properties of the two <b>backfitting</b> quantile estimators. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL] With Correction...|$|E
40|$|We {{introduce}} a new algorithm, called adaptive sparse <b>backfitting</b> algorithm, for solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric, non-negative definite smoothers. Unlike the previous sparse <b>backfitting</b> algorithm, our method is essentially a block coordinate descent algorithm that guarantees to converge to the optimal solution. It bridges {{the gap between the}} population <b>backfitting</b> algorithm and that of the data version. We also prove variable selection consistency under suitable conditions. Numerical studies on both synthesis and real data are conducted to show that adaptive sparse <b>backfitting</b> algorithm outperforms previous sparse <b>backfitting</b> algorithm in fitting and predicting high dimensional nonparametric models. Comment: This is a term project report and has been withdrawn by the authors; arXiv admin note: author list has been modified due to misrepresentation of authorshi...|$|E
40|$|In {{this paper}} a new smooth <b>backfitting</b> {{estimate}} is proposed for additive regression models. The estimate has the simple structure of Nadaraya [...] Watson smooth <b>backfitting</b> {{but at the}} same time achieves the oracle property of local linear smooth <b>backfitting.</b> Each component is estimated with the same asymptotic accuracy as if the other components were known. Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|A spatial-temporal {{model is}} postulated, {{possibly}} allowing for spatial units of irregular shapes. An estimation procedure that imbeds the Cochranne-Orcutt procedure into the <b>backfitting</b> algorithm is proposed. Using agricultural data, the method yields superior forecast to some common approaches. Spatial-temporal model <b>Backfitting</b> algorithm Cochranne-Orcutt procedure Mixed models...|$|E
40|$|AbstractAdditive models {{based on}} <b>backfitting</b> estimators {{are among the}} most {{important}} recent contributions to modern statistical modelling. However, the statistical properties of <b>backfitting</b> estimators have received relatively little attention. Recently, J. -D. Opsomer and D. Ruppert (1997,Ann. Statist. 25, 186 - 211; 1998,J. Amer. Statist. Assoc. 93, 605 – 619) and J. -D. Opsomer (1997, preprint 96 - 12, Department of statistics, Iowa State University) derived their mean squared error properties in the case of local polynomial smoothers. In this paper the asymptotic distributional behaviour of <b>backfitting</b> estimators is investigated...|$|E
40|$|We {{examine and}} compare the finite sample {{performance}} of the competing <b>backfitting</b> and integration methods for estimating additive nonparametric regression using simulated data. Although, the asymptotic properties of the integration estimator, {{and to some extent}} the <b>backfitting</b> method too, are well understood, its small sample properties are not well investigated. Apart from some small experiments in the above cited papers, there is little hard evidence concerning the exact distribution of the estimates. It is our purpose to provide an extensive finite sample comparison between the <b>backfitting</b> procedure and the integration procedure using simulated data...|$|E
40|$|This paper {{proposes a}} nonparametric FPE-like {{procedure}} {{based on the}} smooth <b>backfitting</b> estimator when the additive structure is a priori known. This procedure {{can be expected to}} perform well because of its well-known finite sample performance of the smooth <b>backfitting</b> estimator. Consistency of our procedure is established under very general conditions, including heteroskedasticity...|$|E
40|$|Additive models {{based on}} <b>backfitting</b> estimators {{are among the}} most {{important}} recent contributions to modern statistical modelling. However, the statistical properties of <b>backfitting</b> estimators have received relatively little attention. Recently, J. -D. Opsomer and D. Ruppert (1997,Ann. Statist. 25, 186 - 211; 1998,J. Amer. Statist. Assoc. 93, 605 - 619) and J. -D. Opsomer (1997, preprint 96 - 12, Department of statistics, Iowa State University) derived their mean squared error properties in the case of local polynomial smoothers. In this paper the asymptotic distributional behaviour of <b>backfitting</b> estimators is investigated. additive models kernel smoothing limiting distribution regression functionals. ...|$|E
40|$|We propose general {{procedures}} for posterior sampling from additive and generalized additive models, with applications to non-parametric, semi-parametric and mixed models. One chooses a linear operator S j for each predictor, and the algorithm requires only {{the application of}} S j and S 1 = 2 j. Both of these can be done applied efficiently (O(n) operations) for many popular operators. The procedure is a stochastic generalization of the <b>backfitting</b> algorithm for fitting additive models. Keywords: additive models, <b>backfitting,</b> Bayes, Gibbs sampling, random effects, Metropolis-Hastings procedure 1 Introduction In this paper we propose {{general procedures}} for posterior sampling from additive and generalized additive models. The main idea evolves from the close relationship between the <b>backfitting</b> algorithm for fitting additive models, and the Gibbs sampler for drawing realizations from a posterior distribution. In a nutshell: ffl <b>Backfitting</b> cycles around and replaces each current funct [...] ...|$|E
40|$|Smooth <b>backfitting</b> {{has proven}} to {{have a number of}} {{theoretical}} and practical advantages in structured regression. Smooth <b>backfitting</b> projects the data down onto the structured space of interest providing a direct link between data and estimator. This paper introduces the ideas of smooth <b>backfitting</b> to survival analysis in a proportional hazard model, where we assume an underlying conditional hazard with multiplicative components. We develop asymptotic theory for the estimator and we use the smooth backfitter in a practical application, where we extend recent advances of in-sample forecasting methodology by allowing more information to be incorporated, while still obeying the structured requirements of in-sample forecasting...|$|E
40|$|We {{consider}} {{the problem of}} estimating an additive regression function in an inverse regres- sion model with a convolution type operator. A smooth <b>backfitting</b> procedure is developed and asymptotic normality of the resulting estimator is established. Compared to other meth- ods for the estimation in additive models the new approach neither requires observations on a regular grid nor the estimation of the joint density of the predictor. It is also demonstrated {{by means of a}} simulation study that the <b>backfitting</b> estimator outperforms the marginal in- tegration method at least by a factor two with respect to the integrated mean squared error criterion. Comment: Keywords: inverse regression; additive models; curse of dimensionality; smooth <b>backfitting</b> Mathematical subject classification: Primary: 62 G 20; Secondary 15 A 29 Pages: 26 Figures:...|$|E
40|$|The {{additive}} model {{is one of}} the most popular semiparametric models. The <b>backfitting</b> estimation (Buja, Hastie and Tibshirani, 1989, Ann. Statist.) for the model is intuitively easy to understand and theoretically most efficient (Opsomer and Ruppert, 1997, Ann. Statist.); its implementation is equivalent to solving simple linear equations. However, convergence of the algorithm is very difficult to investigate and is still unsolved. For bivariate {{additive model}}s, Opsomer and Ruppert (1997, Ann. Statist.) proved the convergence under a very strong condition and conjectured that a much weaker condition is sufficient. In this short note, we show that a weak condition can guarantee the convergence of the <b>backfitting</b> estimation algorithm when the Nadaraya-Watson kernel smoothing is used. Key words: additive model; <b>backfitting</b> algorithm; convergence of algorithm; kernel smoothing. ...|$|E
40|$|The smooth <b>backfitting</b> {{introduced}} by Mammen, Linton and Nielsen [Ann. Statist. 27 (1999) 1443 – 1490] is a promising technique to fit additive regression models {{and is known}} to achieve the oracle efficiency bound. In this paper, we propose and discuss three fully automated bandwidth selection methods for smooth <b>backfitting</b> in additive models. The first one is a penalized least squares approach {{which is based on}} higher-order stochastic expansions for the residual sums of squares of the smooth <b>backfitting</b> estimates. The other two are plug-in bandwidth selectors which rely on approximations of the average squared errors and whose utility is restricted to local linear fitting. The large sample properties of these bandwidth selection methods are given. Their finite sample properties are also compared through simulation experiments. 1. Introduction. Nonparametri...|$|E
30|$|A {{generalized}} kernel nonparametric estimation can {{be given}} using smooth <b>backfitting</b> for the functions m_ 1,...,m_q (see again the above mentioned papers).|$|E
30|$|Every instant t, φ_κ(Z_t) is {{estimated}} with the smooth <b>backfitting</b> technique independently {{for each of}} r components using the data (Y_l,Z_l-κ), l=κ+ 1,...,T.|$|E
40|$|Estimating all {{parameters}} in a multiparameter response {{model as}} smooth functions of an explanatory variable {{is very similar}} to estimating the different components of an additive model for the response mean. It is shown that, in a general estimating framework, local polynomial <b>backfitting</b> estimators in an additive one-parameter model do not work optimally. For a multiparameter model, however, a <b>backfitting</b> algorithm can be defined that leads to local polynomial estimators that do have optimal properties. (C) 2000 Elsevier Science B. V. All rights reserved. status: publishe...|$|E
40|$|In {{this paper}} we relate <b>backfitting</b> to the {{standard}} iterative procedures, i. e. Jacobi and Gauss-Seidel, developed for solving linear equation systems with non-singular system matrices. Aspects of improving the performance of these methods through relaxation are also considered. When fitting additive regression models non-parametrically by linear scatterplot smoothers, we are confronted with a singular system matrix of a certain block structure. <b>Backfitting</b> commonly applied, although not designed for this situation, is examined. A simulation experiment is carried out applying cubic smoothing splines in a simple additive model. The behaviour of the iterative procedures is studied by comparing the obtained results with those from a non-iterative Tichonow method. The Jacobi iteration {{turns out to be}} superior to the other procedures when speed and precision are considered simultaneously. Relaxation is of minor importance in Jacobi iteration. 1 Introduction The term <b>backfitting</b> is due to [...] ...|$|E
