8|29|Public
40|$|This paper {{describes}} a GA for job shop scheduling problems. Using the Giffler and Thompson algorithm, we created two new operators, THX crossover and mutation, which better transmit temporal {{relationships in the}} schedule. The approach produced excellent results on standard <b>benchmark</b> <b>job</b> shop scheduling problems. We further tested many models and scales of parallel GAs {{in the context of}} job shop scheduling problems. In our experiments, the hybrid model consisting of coarse-grain GAs connected in a fine-grain-GA-style topology performed best, appearing to integrate successfully the advantages of coarse-grain and fine-grain GAs. ...|$|E
40|$|Grid {{computing}} is a {{frame work}} that shares data, storage, computing across heterogeneous and distributed locations {{to meet the}} current and growing computational demands. Thispaper proposes a novel evolutionary optimization approachusing fuzzy Teaching Learning Based Optimization (TLBO) for resource scheduling in computational grids. The fuzzy TLBOgeneratesan efficient schedule to complete the jobs within a minimum period of time. The performance of the proposed fuzzy based TLBOalgorithm evaluate with various other nature heuristic algorithms, GeneticAlgorithm (GA), Simulated Annealing (SA), Differential Evolution, and fuzzy PSO. Experimental results have shown the efficiency and prominence of new proposed algorithm in producing optimal solutions for the selected <b>benchmark</b> <b>job</b> scheduling problems compared to other algorithms...|$|E
40|$|In this thesis, the Systematic Local Search, {{a hybrid}} search method {{previously}} developed in [18] for constraint satisfaction problems, is extended for optimization problems {{especially in the}} constraint-based scheduling domain. Also in [10], a novel nogood definition (a nogood {{is a set of}} variable assignments that is precluded from any solution to the problem) is proposed as to induce nogoods on the precedence relations of the critical path and a proof demonstrates that this nogood definition captures the optimization criterion and is sound with respect to resolution in search. We evaluate the effectiveness of this extension on the <b>benchmark</b> <b>job</b> shop scheduling problems. Experimental results show that Systematic Local Search outperforms conventional heuristic search methods such as simulated annealing and tabu search and compares favourably with methods designed specifically for the job shop scheduling problems...|$|E
50|$|The Tax side of Croner Group {{primarily}} {{concentrates on}} offering Fee Protection Insurance: a scheme {{that covers the}} clients of an accountancy practice for any professional fees that result from an investigation by HM Revenue and Customs. Croner Reward experts can help companies {{with a range of}} pay-related matters, including salary <b>benchmarking,</b> <b>job</b> evaluation, and gender pay reporting.|$|R
5000|$|Job analysis, which {{pinpoints}} or estimates skill <b>benchmarks</b> {{for specific}} <b>job</b> positions that individuals must meet through testing ...|$|R
40|$|Dynamic load-balancing {{strategies}} for distributed systems seek to improve average completion time of inde-pendent tasks by migrating each incoming task {{to the site}} where {{it is expected to}} finish the fastest: usually the site having the smallest load index. SMALL is an off-line learning system for developing configuration-specific load-balancing strategies; it learns new load indices as well as tunes the parameters of given migration policies. Using a dynamic workload generator, anumber of typical systemwide load patterns are first recorded; the completion times of several <b>benchmark</b> <b>jobs</b> are then measured at each site, under each of the recorded load patterns. These measure-ments are used to simultaneously train comparator neural networks, one per site. The comparators collectively model a set of perfect load indices in that they seek to rank, at arrival time, the possible destinations for an incoming task by their (not yet known) respective completion times. The numerous parameters of the decentralized dynamic load-balancing policy are then tuned using a genetic algorithm. We present experimental results for a mix of scien-tific and interactive workloads on Sun workstations connected by Ethernet. The policies tuned by SMALL are shown to intelligently and effectively exploit idle resources. 1...|$|R
40|$|Finding a good join {{order is}} crucial for query performance. In this paper, we {{introduce}} the Join Order <b>Benchmark</b> (<b>JOB)</b> and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure {{the impact of the}} cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates...|$|E
40|$|The {{artificial}} {{bee colony}} {{has the advantage}} of employing fewer control parameters compared with other population-based optimization algorithms. In this paper a binary artificial bee colony (BABC) algorithm is developed for binary integer job scheduling problems in grid computing. We further propose an efficient binary artificial bee colony extension of BABC that incorporates a flexible ranking strategy (FRS) to improve the balance between exploration and exploitation. The FRS is introduced to generate and use new solutions for diversified search in early generations and to speed up convergence in latter generations. Two variants are introduced to minimize the makepsan. In the first a fixed number of best solutions is employed with the FRS while in the second the number of the best solutions is reduced with each new generation. Simulation results for <b>benchmark</b> <b>job</b> scheduling problems show that the performance of our proposed methods is better than those alternatives such as genetic algorithms, simulated annealing and particle swarm optimization. Web of Science 17588286...|$|E
40|$|Decomposition {{procedure}} {{is a common}} approach to handle computational complexity caused by much larger sized production scheduling problems. However, the overall problem should be decomposed on which criterion to extend {{the applicability of the}} decomposition approach and improve its computational efficiency and resultant effectiveness is still a challenging task in this field. In this paper, in order to improve decomposition approach’s efficiency and effectiveness to obtain optimal solution within reasonable time limitations for job shop scheduling problems, a new decomposition approach which has a machine-based divided criterion focused on the longest active chain is proposed. That means the machines processing operations on the identified longest active chain of the whole problem will be classified as one focused cell (decomposed sub-problem). The whole schedule is improved by redefining, solving and adjusting the focused cell schedule which is updated iteratively with the whole schedule’s longest active chain during this dynamic procedure. The proposed approach is tested on makespan minimum <b>benchmark</b> <b>job</b> shop scheduling problems. The computational results show that our approach is competent to generate high-quality solutions efficiently. Keywords...|$|E
40|$|The {{current state}} of {{practice}} in supercomputer resource allocation places jobs from different users on disjoint nodes {{both in terms of}} time and space. While this approach largely guarantees that jobs from different users do not degrade one another’s performance, it does so at high cost to system throughput and energy efficiency. This focused study presents job striping, a technique that significantly increases performance over the current allocation mechanism by colocating pairs of jobs from different users on a shared set of nodes. To evaluate the potential of job striping in large scale environments, the experiments are run at the scale of 128 nodes on the state-of-the-art Gordon supercomputer. Across all pairings of 1024 process NAS parallel <b>benchmarks,</b> <b>job</b> striping increases mean throughput by 26 % and mean energy efficiency by 22 %. On pairings of the real applications GTC, LAMMPS, and MILC at equal scale, job striping improves average throughput by 12 % and mean energy efficiency by 11 %. In addition, the study provides a simple set of heuristics for avoiding low performing application pairs. Copyright c ○ 2012 John Wiley & Sons, Ltd...|$|R
40|$|This book {{covers a}} broad {{spectrum}} from job evaluation systems and how they help staff to understand different <b>job</b> <b>benchmarks,</b> {{to a range of}} financial incentive schemes and other benefits which are important to employees - helping you to build loyalty, motivation and productivity. The author highlights the pitfalls of some schemes, using real case studies and offers advice and guidance on packages that work...|$|R
40|$|This report uses {{detailed}} analysis of the National Aged Care Workforce Census and Survey, 2012, to identify <b>benchmarks</b> of <b>job</b> quality for Personal Care Attendants (PCAs) and Community Care Workers (CCWs). The purpose in identifying these <b>job</b> quality <b>benchmarks</b> is two-fold. Firstly, they allow different organisations within the aged care sector to assess themselves against industry-wide job quality norms. Secondly, they provide the basis for the development of organisation-specific benchmarks by the three aged care industry partners involved in the Quality Care Quality Jobs Project, and will inform the development of measures for interventions designed to improve both job and care quality. A key objective of this project is to deliver outcomes that have a positive and tangible impact for aged care workers and services...|$|R
40|$|Ant colony {{optimisation}} (ACO), {{a constructive}} metaheuristic {{inspired by the}} foraging behaviour of ants, has frequently been applied to shop scheduling problems such as the job shop, in which a collection of operations (grouped into jobs) must be scheduled for processing on different machines. In typical ACO applications solutions are generated by constructing a permutation of the operations, from which a deterministic algorithm can generate the actual schedule. An alternative approach is to assign each machine {{one of a number}} of alternative dispatching rules to determine its individual processing order. This representation creates a substantially smaller search space biased towards good solutions. A previous study compared the two alternatives applied to a complex real-world instance and found that the new approach produced better solutions more quickly than the original. This paper considers its application to a wider set of standard <b>benchmark</b> <b>job</b> shop instances. More detailed analysis of the resultant search space reveals that, while it focuses on a smaller region of good solutions, it also excludes the optimal solution. Nevertheless, comparison of the performance of ACO algorithms using the different solution representations shows that, using this solution space, ACO can find better solutions than with the typical representation. Hence, it may offer a promising alternative for quickly generating good solutions to seed a local search procedure which can take those solutions to optimality...|$|E
40|$|Introduction: Maximum oxygen {{consumption}} {{is acceptable to}} measure the fit between {{a person with a}} job. The aim {{of this study was to}} determine maximal oxygen uptake career as a medical emergency and use it as a tool for selecting students. Materials and Methods: This cross-sectional descriptive study among emergency medical personnel (n = 6) and emergency medical students (n = 56) was conducted in 1395 in Qazvin University of Medical Sciences. For estimate maximal {{oxygen consumption}} personnel, The most difficult task (cardiopulmonary resuscitation - lung) identification, simulation and was measured by meta-Max device German cortex. Students also completed the Physical Activity Readiness – Questionnaire (PAR-Q) and demographic characteristics were evaluated through treadmill (Gorkin) and Step (Queen's College) tests. Then compare the maximal oxygen uptake individual and job, were use to comment about the suitability and the true selection of students for the jobs of medical emergencies. Results: Maximal oxygen uptake in emergency medicine job in the most difficult task (cardiopulmonary resuscitation - lung) using the experimental approach by Metamax, 2. 52 liters per minute (Equivalent to - 0. 0113 in standardization) respectively. Average maximum oxygen consumption students by Gorkin and Queen's College tests obtained respectively 4. 17 and 3. 36 liters per minute. Weight, height and body mass index were identified as factors influencing the maximum oxygen consumption (p< 0. 05). Also were obtained positive and significant correlation between the above-mentioned tests (r= 0. 66, P< 0. 05). 41. 1 percent of medical emergencies students by step test and 48. 2 percent by treadmill test are not chosen appropiate based on criteria oxygen consumption emergency medicine job. Discussion and conclusion: Based on compared maximum oxygen consumption students with the <b>benchmark</b> <b>jobs,</b> two-third of emergency medical fitness of heart - lung are not appropriate And to achieve the appropriate level of oxygen consumption, exercise programs are needed. Considering the correlation between the two tests and ease of use Gorkin and Queen's College Step Test This test can be used to replace treadmill tests. key words: Students, Medical emergency, maximum oxygen consumptio...|$|R
40|$|The Flexible Job Shop Scheduling Problem (FJSP) is a {{generalization}} {{of the classical}} Job Shop Problem in which each operation must be processed on a given machine chosen among a finite sub-set of candidate machines. The aim is to find an allocation for each operation and to define the se-quence of operations on each machine so that the resulting schedule has a minimal completion time. We propose {{a variant of the}} climbing discrepancy search approach for solving this problem. Experiments have been performed on well-known <b>benchmarks</b> for flexible <b>job</b> shop scheduling...|$|R
40|$|Both the {{comparable}} worth controversy {{as well as}} the current federal posture, which emphasizes the importance of basing employment tests on information that is directly related to job content, have led employers to inspect their wage allocation systems. In the present study, 66 professional jobs were evaluated with both a priori point factor and policy capturing structured methods of job evaluation. The intent was to determine which method yielded the more equitable salary hierarchy, and to test for ways to measure and adjust for sex-based wage discrimination. Regression model terms in addition to factors related to job content included a measure of seniority, a supply and demand proxy, a measure of occupational prestige, and an index of the percent female of each occupation. Additionally, several salary measures were evaluated for their appropriateness as criterion measures. ^ Contrary to the commonly accepted notion of a 2 ̆ 2 universal 2 ̆ 2 set of factors, different factors were found across the two approaches to job evaluation. The policy capturing structured approach allowed for the identification of a new factor, measuring technical processing activities, which appears to reflect the blue collar component of white collar jobs. The policy capturing approach was thus determined to be the more flexible approach for job analysis purposes (the identification of job requirements). The a priori approach was seen as superior for purposes of job evaluation (salary setting). This determination was founded upon evidence suggesting that female incumbents rated themselves lower than their male counterparts rated themselves in evaluating their job requirements, and that a priori job ratings were found to be more veridical estimates of job duties since they reflected the mean of four raters 2 ̆ 7 evaluations. ^ In constructing the salary schedule, it was recommended: that incumbent salaries rather than market salaries be used as the criterion measure; that the function relating supply and demand to market salary be empirically determined and used to adjust salaries predicted from job content information; and that, where <b>benchmark</b> <b>jobs</b> are chosen to construct the salary hierarchy, these jobs be neither male nor female dominated in the marketplace. ...|$|R
40|$|In {{this paper}} we propose a {{flexible}} model for scheduling problems, which allows the modeling of systems with complex alternative behaviour. This model could for example facilitate the step from process planning model to optimization model. We show how automatic constraint generation can be performed for both Constraint Programming and Mixed Integer Linear Programming (MILP) models. Also, for the MILP case, a new formulation for mutual exclusion of resources is proposed. This new formulation works well for proving optimality in systems with multiple capacity resources. Some <b>benchmarks</b> for such <b>job</b> shop scheduling problems as well as systems with a large number of alternatives are also presented...|$|R
40|$|Explains {{revisions}} to workforce jobs, originally intended for release last December, now published with the March LMS First ReleaseThis article explains the revisions {{made to the}} workforce jobs (WFJ) series, released on 14 March 2007, in the Labour Market Statistics First Release. It was originally intended to release these revisions in December 2006 but further quality assurance was required. This quality assurance has now been concluded and a Review of Workforce <b>Jobs</b> <b>Benchmarking</b> has been published separately, which includes a comparison of annual growth in jobs {{as measured by the}} revised WFJ series and the Labour Force Survey. Economic & Labour Market Review (2007) 1, 56 – 59; doi: 10. 1057 /palgrave. elmr. 1410076...|$|R
40|$|In {{this paper}} {{we present a}} {{framework}} for developing an intelligent job management and scheduling system that utilizes application specific <b>benchmarks</b> to mould <b>jobs</b> onto available resources. In an attempt to achieve the seemingly irreconcilable goals of maximum usage and minimum turnaround time this research aims to adapt an open-framework benchmarking scheme to supply information to a mouldable job scheduler. In a green IT obsessed world, hardware efficiency and usage of computer systems becomes essential. With an average computer rack consuming between 7 and 25 kW {{it is essential that}} resources be utilized in the most optimum way possible. Currently the batch schedulers employed to manage these multi-user multi-application environments are nothing more than match making and service level agreement (SLA) enforcing tools. These managemen...|$|R
40|$|Title from PDF {{of title}} page viewed October 30, 2017 Dissertation advisor: Deep MedhiVitaIncludes bibliographical {{references}} (pages 122 - 135) Thesis (Ph. D.) [...] School of Computing and Engineering. University of Missouri [...] Kansas City, 2017 This dissertation investigates improvement in application performance. For applications, we consider two classes: Hadoop MapReduce and video streaming. The Hadoop MapReduce (M/R) framework {{has become the}} de facto standard for Big Data analytics. However, the lack of network-awareness of the default MapReduce resource manager in a traditional IP network can cause unbalanced job scheduling and network bottlenecks; such factors can eventually {{lead to an increase}} in the Hadoop MapReduce job completion time. Dynamic Video streaming over the HTTP (MPEG-DASH) is becoming the defacto dominating transport for today’s video applications. It has been implemented in today’s major media carriers such as Youtube and Netﬂix. It enables new video applications to fully utilize the existing physical IP network infrastructure. For new 3 D immersive medias such as Virtual Reality and 360 -degree videos are drawing great attentions from both consumers and researchers in recent years. One of the biggest challenges in streaming such 3 D media is the high band width demands and video quality. A new Tile-based video is introduced in both video codec and streaming layer to reduce the transferred media size. In this dissertation, we propose a Software-Deﬁned Network (SDN) approach in an Application-Aware Network (AAN) platform. We ﬁrst present an architecture for our approach and then show how this architecture can be applied to two aforementioned application areas. Our approach provides both underlying network functions and application level forwarding logics for Hadoop MapReduce and video streaming. By incorporating a comprehensive view of the network, the SDN controller can optimize MapReduce work loads and DASH ﬂows for videos by application-aware trafﬁc reroute. We quantify the improvement for both Hadoop and MPEG-DASH in terms of job completion time and user’s quality of experience (QoE), respectively. Based on our experiments, we observed that our AAN platform for Hadoop MapReduce job optimization offer a signiﬁcant improvement compared to a static, traditional IP network environment by reducing job run time by 16 % to 300 % for various MapReduce <b>benchmark</b> <b>jobs.</b> As for MPEG-DASH based video streaming, we can increase user perceived video bitrate by 100 %. Introduction [...] Research survey [...] Proposed architecture [...] AAN-SDN for Hadoop [...] Study of User QoE Improvement for Dynamic Adaptive Streaming over HTTP (MPEG-DASH) [...] AAN-SDN For MPEG-DASH [...] Conclusion [...] Appendix A. Mininet Topology Source Code For DASH Setup [...] Appendix B. Hadoop Installation Source Code [...] Appendix C. Openvswitch Installation Source Code [...] Appendix D. HiBench Installation Guid...|$|R
40|$|The Flexible Job Shop {{scheduling}} Problem (FJSP) is a {{generalization of}} the classical Job Shop Problem in which each operation must be processed on a given machine chosen among a finite subset of candidate machines. The aim is to find an allocation for each operation and to define the sequence of operations on each machine so that the resulting schedule has a minimal completion time. We propose {{a variant of the}} Climbing Discrepancy Search approach for solving this problem. We also present various neighborhood structures related to assignment and sequencing problems. We report the results of extensive computational experiments carried out on well-known <b>benchmarks</b> for flexible <b>job</b> shop scheduling. The results demonstrate that the proposed approach outperforms the best-known algorithms for the FJSP on some types of benchmarks and remains comparable with them on other ones...|$|R
40|$|This study investigates, how the {{management}} of a jobcentre responds strategic to institutional pressures in the competition reform. I have chosen only to analyze three New Public Management inspired management thools in the competition reform: Benchmarking, target management and performance based pay. The possible strategic responds to {{the management}} tools are according to Christine Oliver acquiesce, compromise, avoid, defy and manipulate. The management responds positively {{to the use of}} benchmarking, but only one of the managers uses <b>benchmarking</b> in the <b>job</b> centre. The management is active in the use of target management, but two of the managers use the manipulate- and compromise strategic. Only one of the managers has implemented performance based pay in his department. The two other managers do not believe that performance based payment will increase the efficiency in the job centre and as a result they do not use it...|$|R
40|$|A recent {{adaptive}} algorithm, named Ant System, {{is introduced}} {{and used to}} solve the problem of job shop scheduling. The algorithm was first introduced by Dorigo, Maniezzo and Colorni in 1991 and is derived from the foraging and recruiting behaviour observed in an ant colony. It can be applied to combinatorial optimisation problems. This paper outlines the algorithm's implementation and performance when applied to job shop scheduling. The algorithm parameter settings seem to play a crucial role in its efficiency and determine the quality of solutions. In this paper we present some statistic analysis for parameter tuning and we compare the quality of obtained solutions for well-known <b>benchmark</b> problems in <b>job</b> shop scheduling. 1. Introduction The study from biology of an ant colony shows that its behaviour is highly structured. Knowing that a single ant has limited capacities (i. e., a single ant is not capable of communicating directly with other ants about past experiences), it is curi [...] ...|$|R
40|$|Due to {{copyright}} restrictions, {{the access}} to {{the full text of}} this article is only available via subscription. The flexible job shop scheduling problem (FJSP) is a generalization of the classical job shop problem in which each operation must be processed on a given machine chosen among a finite subset of candidate machines. The aim is to find an allocation for each operation and to define the sequence of operations on each machine, so that the resulting schedule has a minimal completion time. We propose a variant of the climbing discrepancy search approach for solving this problem. We also present various neighborhood structures related to assignment and sequencing problems. We report the results of extensive computational experiments carried out on well-known <b>benchmarks</b> for flexible <b>job</b> shop scheduling. The results demonstrate that the proposed approach outperforms the best-known algorithms for the FJSP on some types of benchmarks and remains comparable with them on other ones. Fatimah Alnijris Research Chair for Advanced Manufacturing Technolog...|$|R
40|$|In this study, {{a general}} {{framework}} is proposed that combines the distinctive features of three well-known approaches: the adaptive memory programming, the simulated annealing, and the tabu search methods. Four variants of a heuristic {{based on this}} framework are developed and presented. The performance of the proposed methods is evaluated and compared with a conventional simulated annealing approach using <b>benchmark</b> problems for <b>job</b> shop scheduling. The unique feature of the proposed framework {{is the use of}} two short-term memories. The first memory temporarily prevents further changes in the configuration of a provisional solution by maintaining the presence of good elements of such solutions. The purpose of the second memory is to keep track of good solutions found during an iteration, so that the best of these can be used as the starting point in a subsequent iteration. Our computational results for the job shop scheduling problem clearly indicate that the proposed methods significantly outperform the conventional simulated annealing...|$|R
40|$|The paper investigates labor {{reallocation}} across main {{economic sectors}} between 1989 and 2007 in the CEE 2 countries, now {{all members of}} the EU, using a methodology presented in Jackman and Puna (1997). Defining a series of indices aimed at capturing the speed, magnitude and efficiency of employment reallocation, the work assesses {{the extent to which these}} countries have succeeded in converging towards distributions of sectoral employment similar to those in the old EU members. The work shows that, overall, the CEE countries have made progress towards reallocating jobs from the oversized labor intensive sectors, characteristic of the early years of transition, such as agriculture and heavy industries, towards the services sector. However, convergence has been relatively slow and its pace has been different from country to country. Bulgaria emerges as the country where the fastest restructuring has taken place, and in the right direction. Romania, in particular, appears to have made least progress, although it is also moving in the right direction. The still large agricultural sector, which continued to hire around 30 % of the occupied population in 2007, remains an area which will require further and massive restructuring. As of 2007, in the case of Romania, around 40 % of the jobs expected to be created in the growing sectors, computed by <b>benchmarking</b> actual <b>job</b> destruction and job creation against the comparator economy, have occurred. The figure increases to over 50 %, when the distortive effect of agriculture is removed. At the same time, over 90 % of the job destruction and creation took place in the appropriate direction, towards the comparator EU employment distribution. labor reallocation, labor market convergence, job creation, job destruction...|$|R
40|$|Paper {{presented}} at the XXXIII IAHS World Congress on Housing, 27 - 30 September 2005,"Transforming Housing Environments through Design", University of Pretoria. The aim {{of this paper is}} to present housing processes and roles and competencies of housing managers in developing countries. The competencies will be indicated for one sub-sector on the different managerial levels. The roles and competencies are based upon a newly developed generic housing management model. The model was developed with due cognisance of the contemporary international housing guidelines, the developing country context and the national housing policies and legislation. The housing management model, as well as the processes, roles and competencies were developed out of a literature study and empirical studies including workshops and meetings with stakeholder representatives, analyses of job descriptions of housing management practitioners and postal surveys throughout the housing sector. The model can be used to support professionalisation processes. The paper should contribute towards a clearer understanding of the housing processes, housing management, the roles that housing managers should fulfil and the competencies that they should possess. Therefore the information can be used by practitioners as a <b>benchmark</b> for <b>job</b> descriptions, housing management practice and performance management. Housing educators can use the information to generate educational outcomes for their qualifications and programmes. Authors of papers in the proceedings and CD-ROM ceded copyright to the IAHS and UP. Authors furthermore declare that papers are their original work, not previously published and take responsibility for copyrighted excerpts from other works, included in their papers with due acknowledgment in the written manuscript. Furthermore, that papers describe genuine research or review work, contain no defamatory or unlawful statements and do not infringe the rights of others. The IAHS and UP may assign any or all of its rights and obligations under this agreement...|$|R
40|$|BACKGROUND: In {{order to}} {{maintain}} the most comprehensive structural annotation databases we must carry out regular updates for each proteome using the latest profile-profile fold recognition methods. The ability to carry out these updates on demand is necessary {{to keep pace with the}} regular updates of sequence and structure databases. Providing the highest quality structural models requires the most intensive profile-profile fold recognition methods running with the very latest available sequence databases and fold libraries. However, running these methods on such a regular basis for every sequenced proteome requires large amounts of processing power. In this paper we describe and <b>benchmark</b> the JYDE (<b>Job</b> Yield Distribution Environment) system, which is a meta-scheduler designed to work above cluster schedulers, such as Sun Grid Engine (SGE) or Condor. We demonstrate the ability of JYDE to distribute the load of genomic-scale fold recognition across multiple independent Grid domains. We use the most recent profile-profile version of our mGenTHREADER software in order to annotate the latest version of the Human proteome against the latest sequence and structure databases in as short a time as possible. RESULTS: We show that our JYDE system is able to scale to large numbers of intensive fold recognition jobs running across several independent computer clusters. Using our JYDE system we have been able to annotate 99. 9...|$|R
40|$|Abstract—In this paper, {{we present}} our effort towards compre-hensive traffic {{forecasting}} for big data applications using external, light-weighted file system monitoring. Our idea {{is motivated by}} the key observations that rich traffic demand information already exists in the log and meta-data files of many big data applications, and that such information can be readily extracted through run-time file system monitoring. As the first step, we use Hadoop as a concrete example to explore our methodology and develop a system called HadoopWatch to predict traffic demands of Hadoop applications. We further implement HadoopWatch in a small-scale testbed with 10 physical servers and 30 virtual machines. Our experiments over a series of MapReduce applications demonstrate that HadoopWatch can forecast the traffic demand with almost 100 % accuracy and time advance. Furthermore, it makes no mod-ification on the Hadoop framework, and introduces little overhead to the application performance. Finally, to showcase the utility of accurate traffic prediction made by HadoopWatch, we design and implement a simple HadoopWatch-enabled network optimiza-tion module into the HadoopWatch controller, and with realistic Hadoop <b>job</b> <b>benchmarks</b> we find that even a simple algorithm can leverage the forecasting results provided by HadoopWatch to significantly improve the Hadoop job completion time by up to 14. 72 %. Index Terms—Cloud computing, data center networks, traffic prediction, Hadoop. I...|$|R
40|$|This paper {{proposes a}} swarm {{intelligence}} approach {{based on a}} disjunctive graph model in order to schedule a manufacturing system with resource flexibility and separable setup times. Resource flexibility assigns each operation {{to one of the}} alternative resources (assigning sub-problem) and, consequently, arranges the operation in the right sequence of the assigned resource (sequencing sub-problem) in order to minimize the makespan. Resource flexibility is mandatory for rescheduling a manufacturing system after unforeseen events which modify resource availability. The proposed method considers parallel (related) machines and enforces in a single step both the assigning and sequencing sub-problems. A neighboring function on the disjunctive graph is enhanced by means of a reinforced relation-learning model of pheromone involving more effective machine-sequence constraints and a dynamic visibility function. It also considers the overlap between the jobs feeding and the machine (anticipatory) setup times. It involves separable sequence-independent and dependent setup phases. The algorithm performance is evaluated by modifying the well-known <b>benchmark</b> problems for <b>JOB</b> shop scheduling. Comparison with other systems and lower bounds of benchmark problems has been performed. Statistical tests highlight how the approach is very promising. The performance achieved when the system addresses the complete problem is quite close to that obtained {{in the case of the}} classical job-shop problem. This fact makes the system effective in coping with the exponential complexity especially for sequence dependent setup times...|$|R
40|$|University of Minnesota Ph. D. dissertation. July 2012. Major: Anthropology. Advisor: Gloria Goodwin Raheja. 1 {{computer}} file (PDF); iv, 237 pages, appendices A-D. Since India's economic liberalization in the 1980 s, corporations in the U. S. and Europe have been outsourcing service and computer programming jobs to urban centers in India such as Hyderabad. In this period, numerous Indian national {{as well as}} international processes have gone into making Hyderabad a "global city," where information technology (IT) jobs in multinational corporations provide new kinds of cultural capital and prestige that are shaping global Indian middle class identities. In this dissertation, I critically analyze how global neoliberal discourses encounter established, local practices, changing the previous calculus of social relations as well as refashioning particular meanings of the "global. " IT professionals have to adapt quickly {{to take advantage of}} opportunities in the new economy, while also conforming to social <b>benchmarks</b> of <b>job</b> security set by previous generations. IT professionals have found ways to "brand" themselves and their careers to find a more solid foothold in a transient, transnational job sector. The process of branding involves specific kinds of soft skill training, resume building, networking, and practices outside of the professional space to be recognized as a "quality IT professional. " New urban spaces of consumption such as malls, theme parks, and consumer showrooms have become iconic sites of global consumerism that seek to cater to these global, IT professionals. The significance of these landscapes is dependent on everyday, repetitive actions and narratives about consumption that highlight the city's present international role. Consumer practices play a dual role, at once the site of claiming to be globally Indian and the site of accusatory assertions of the loss of Indian traditional culture and the incursion of Western frivolity. Instead of looking at "traditional" and "Western" as opposing influences, I investigate how these concepts are produced through consumer practices and narratives of consumption. Furthermore, processes of professionalization and consumerism are incorporated into a global, modern, Indian middle class and the politics of exclusion that they deploy; a politics that recognizes some as being in synch with global and national growth, and renders large sections of the population invisible or outside of the citizenry of the Indian nation...|$|R
40|$|To {{effectively}} {{manage and}} control the execution of production process, a correct scheduling activity must be performed. In any manufacturing environment, resources utilization, production rate, customer service level can be switched across the definition of suitable jobs’ sequence and tasks’ allocation. Being a NP complete and highly constrained problem, {{the resolution of the}} Job Shop Scheduling Problem (JSSP) is recognized as a key point to the factory optimization process. In recent years, a great number of multi-objective meta-heuristics has been proposed to evaluate the quality of a scheduling solution and obtain sets of compromising solutions. Powerful methods for running these kinds of optimization problems have been inspired by research on evolutionary theory and swarm intelligence approach. The cooperative behaviour that emerges from the organization of multi agent systems is the inspiring source of the two implemented approaches. The pursuit of optimal solution, on both <b>benchmark</b> and real-world <b>job</b> shop problem, has been successful tested for Genetic Algorithms (GA) and Ant Colony Optimization (ACO) techniques. This work starts with analysis on optimization methods for JSSP. Across the implementation of a new Genetic Algorithm and an improved model based on ant’s way, the performance of the two meta-heuristic approaches has been evaluated and compared. Similarity/dissimilarity of evolutionary and swarm intelligent approaches has pointed out. The logic, the parameters, the representation schemes and operators used in these two approaches have been widely discussed during this paper. A guide to the implementation of GAs and ACO approach to JSSP was performed...|$|R
40|$|In {{this paper}} {{we present a}} {{framework}} for developing an intelligent job management and scheduling system that utilizes application specific <b>benchmarks</b> to mould <b>jobs</b> onto available resources. In an attempt to achieve the seemingly irreconcilable goals of maximum usage and minimum turnaround time this research aims to adapt an open-framework benchmarking scheme to supply information to a mouldable job scheduler. In a green IT obsessed world, hardware efficiency and usage of computer systems becomes essential. With an average computer rack consuming between 7 and 25 kW {{it is essential that}} resources be utilized in the most optimum way possible. Currently the batch schedulers employed to manage these multi-user multi-application environments are nothing more than match making and service level agreement (SLA) enforcing tools. These management systems rely on user prescribed parameters that can lead to over or under booking of compute resources. System administrators strive to get maximum “usage efficiency” from the systems by manual fine-tuning and restricting queues. Existing mouldable scheduling strategies utilize scalability characteristics, which are inherently 2 dimensional and cannot provide predictable scheduling information. In this paper we have considered existing benchmarking schemes and tools, schedulers and scheduling strategies, and elastic computational environments. We are proposing a novel job management system that will extract performance characteristics of an application, with an associated dataset and workload, to devise optimal resource allocations and scheduling decisions. As we move towards an era where on-demand computing becomes the fifth utility, the end product from this research will cope with elastic computational environments...|$|R
40|$|Abstract Background In {{order to}} {{maintain}} the most comprehensive structural annotation databases we must carry out regular updates for each proteome using the latest profile-profile fold recognition methods. The ability to carry out these updates on demand is necessary {{to keep pace with the}} regular updates of sequence and structure databases. Providing the highest quality structural models requires the most intensive profile-profile fold recognition methods running with the very latest available sequence databases and fold libraries. However, running these methods on such a regular basis for every sequenced proteome requires large amounts of processing power. In this paper we describe and <b>benchmark</b> the JYDE (<b>Job</b> Yield Distribution Environment) system, which is a meta-scheduler designed to work above cluster schedulers, such as Sun Grid Engine (SGE) or Condor. We demonstrate the ability of JYDE to distribute the load of genomic-scale fold recognition across multiple independent Grid domains. We use the most recent profile-profile version of our mGenTHREADER software in order to annotate the latest version of the Human proteome against the latest sequence and structure databases in as short a time as possible. Results We show that our JYDE system is able to scale to large numbers of intensive fold recognition jobs running across several independent computer clusters. Using our JYDE system we have been able to annotate 99. 9 % of the protein sequences within the Human proteome in less than 24 hours, by harnessing over 500 CPUs from 3 independent Grid domains. Conclusion This study clearly demonstrates the feasibility of carrying out on demand high quality structural annotations for the proteomes of major eukaryotic organisms. Specifically, we have shown that it is now possible to provide complete regular updates of profile-profile based fold recognition models for entire eukaryotic proteomes, through the use of Grid middleware such as JYDE. </p...|$|R
40|$|Recent {{research}} shows that observed labor market flows can be explained in search and matching models only by assuming either implausibly large productivity shocks (Hall 2003) or an excessively high degree of real wage rigidity even for new hires (Shimer 2003). If {{this is not the}} case, the incentive to create new jobs in a boom is weakened since workers share the returns with employers. Fewer vacancies are opened, and unemployment falls by less than is evident from the data. However, this argument relies on treating the vacancy-unemployment ratio as the relevant measure of labor market tightness, which, in turn, is a central determinant of wages in the Nash bargaining approach. We argue in this paper that on-the-job search by workers expands the pool of searchers that firms can recruit from. Since job-to-job movements are highly procyclical, so is the number of on-the-job searchers. The bargaining power of incumbent workers and new hires therefore rises by much less in a boom than would be suggested by the standard vacancy-unemployment ratio alone. Instead, the ratio of vacancies to unemployed and employed searchers is the correct measure of labor market tightness. We quantitatively assess the role of on-the-job search in a fully specified, real DSGE model with endogenous labor market flows. There are two types of jobs, good and bad, which differ by creation cost and productivity. Vacant good jobs can be filled with employed and unemployed job seekers, while bad jobs only recruit from the unemployed. We consider alternative assumptions for the bargaining process, which is {{complicated by the fact that}} employed searchers have a fallback option that differs from the value of unemployment. The critical issue here is how firms bargain with employed workers, and what fraction of the rents from a new job workers can extract. We show that on-the-job search greatly reduces the volatility of true labor market tightness, compared to a <b>benchmark</b> model without <b>job</b> search. Therefore, wages are also much less volatile. At the same time, the predicted variation of the vacancy-unemployment ratio is much higher, and thus closer to the data. We are thus able to replicate the observed labor market dynamics in a full general equilibrium context. We also extend our real framework by including money and a nominal rigidity. In monetary business cycle models with sticky prices, real labor costs enter the price setting of firms. Hence, the cyclicality of real wages as determined by bargaining between workers and firms has implications for inflation dynamics. The less cyclical real wages are, the lower we can expect inflation volatility to be. We show that on-the-job search plays a crucial role in dampening fluctuations in real wages, thereby helping to resolve the challenges to the search and matching framework of labor market dynamics posed by Shimer (2003). Search, Nash Bargaining, Real Wage Rigidity, Labor Market Dynamics, New Keynesian...|$|R

