12|9|Public
25|$|Each {{data section}} is 2 <b>byte</b> <b>aligned.</b> If {{it would end}} on an odd offset, a newline ('\n', 0x0A) is used as filler.|$|E
50|$|The {{alternate}} wording b-bit aligned designates a b/8 <b>byte</b> <b>aligned</b> address (ex. 64-bit aligned is 8 bytes aligned).|$|E
50|$|Each {{data section}} is 2 <b>byte</b> <b>aligned.</b> If {{it would end}} on an odd offset, a newline ('\n', 0x0A) is used as filler.|$|E
5000|$|Shared {{memory is}} {{declared}} in the PTX file via {{lines at the}} start of the form:.shared [...]align 8 [...]b8 pbatch_cache15744; // define 15744 <b>bytes,</b> <b>aligned</b> to an 8-byte boundary ...|$|R
50|$|Each screen {{character}} is actually represented by two <b>bytes</b> <b>aligned</b> as a 16-bit word accessible by the CPU {{in a single}} operation. The lower, or character, byte is the actual code point for the current character set, and the higher, or attribute, byte is a bit field used to select various video attributes such as color, blinking, character set, and so forth. This byte-pair scheme is among the features that the VGA inherited from the EGA, CGA, and ultimately from the MDA.|$|R
5000|$|The {{following}} formulas {{provide the}} number of padding <b>bytes</b> required to <b>align</b> {{the start of a}} data structure (where mod is the modulo operator): padding = (align - (offset mod align)) mod align aligned = (offset + padding) = (offset + (align - (offset mod align)) mod align ...|$|R
50|$|The iAPX 432 {{instruction}} had variable length, and peculiarly were bit {{rather than}} <b>byte</b> <b>aligned,</b> with an instruction taking between 6 and 321 bits.|$|E
50|$|Since 10*1 padding always adds {{at least}} two bits, in <b>byte</b> <b>aligned</b> {{libraries}} there are always six unused zero bits. Therefore, these appended extra bits never make the padded message longer.|$|E
50|$|This {{structure}} {{was designed for}} any digital audio or metadata {{that is to be}} synchronized with a video frame. SDI transports every eight bits in a 10 bit aligned packet, unlike MXF which is <b>byte</b> <b>aligned</b> and the ancillary flag bytes are replaced by 128 bit header.|$|E
50|$|A {{memory address}} a, {{is said to}} be n-byte aligned when a is a {{multiple}} of n bytes (where n is a power of 2). In this context a byte is the smallest unit of memory access, i.e. each memory address specifies a different <b>byte.</b> An n-byte <b>aligned</b> address would have a minimum of log2(n) least-significant zeros when expressed in binary.|$|R
50|$|There are two {{variations}} of packed encoding rules: unaligned and aligned. With the unaligned encoding, the bits {{are packed with}} no regard for octet (<b>byte)</b> boundaries. With <b>aligned</b> encoding, certain types of data structures are aligned on octet boundaries, meaning {{there may be some}} number of wasted padding bits. Unaligned encoding uses the least number of bits, but presumably at some cost in processing time.|$|R
40|$|The {{three generations}} of postings list {{compression}} strategies (Var-iable <b>Byte</b> Encoding, Word <b>Aligned</b> Codes, and SIMD Codecs) are examined {{in order to test}} whether or not each truly represented a generational change – they do. Some weaknesses of the current SIMD-based schemes are identified and a new scheme, QMX, is introduced to address both space and decoding inefficiencies. Improvements are examined on multiple architectures and it is shown that different SSE implementations (Intel and AMD) per-form differently...|$|R
50|$|The {{simplest}} cache is {{a virtually}} indexed direct-mapped cache. The virtual address is calculated with an adder, the relevant {{portion of the}} address extracted and used to index an SRAM, which returns the loaded data. The data is <b>byte</b> <b>aligned</b> in a byte shifter, and from there is bypassed to the next operation. There {{is no need for}} any tag checking in the inner loop — in fact, the tags need not even be read. Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit. On a miss, the cache is updated with the requested cache line and the pipeline is restarted.|$|E
50|$|This {{structure}} {{was designed for}} any digital audio or metadata {{that is to be}} synchronized with a video frame. SDI transports every eight bits in a 10 bit aligned packet, unlike MXF which is <b>byte</b> <b>aligned</b> and the ancillary flag bytes are replaced by 128 bit header. If the cdp_timecode_added is true, then a five byte SMPTE timecode section is inserted before the cdp_data_section. If the cdp_service_info_added is true, then a two byte header and seven bytes per service listing of caption services is inserted after the cdp_data_section. The cdp_framing_rate can be set to the following enumerations: 1 for 24000/1001, 2 for 24, 3 for 25, 4 for 30000/1001, 5 for 30, 6 for 50, 7 for 60000/1001 and 8 for 60 frames per second.|$|E
50|$|A blitter is a circuit, {{sometimes}} as a coprocessor or a logic block on a microprocessor, {{dedicated to the}} rapid movement and modification of data within a computer's memory. A blitter can copy large quantities of data from one memory area to another relatively quickly, and in parallel with the CPU, while freeing up the CPU's more complex capabilities for other operations. A typical use for a blitter is the movement of a bitmap, such as windows and fonts in a graphical user interface or images and backgrounds in a 2D computer game. The name comes from the bit blit operation of the 1973 Xerox Alto, which stands for bit-block transfer. A blit operation {{is more than a}} memory copy, because it can involve data that's not <b>byte</b> <b>aligned</b> (hence the bit in bit blit), handling transparent pixels (pixels which should not overwrite the destination data), and various ways of combining the source and destination data.|$|E
50|$|This {{means that}} unaligned PER data is {{essentially}} an ordered stream of bits, and not an ordered stream of <b>bytes</b> like with <b>aligned</b> PER, {{and that it will}} be a bit more complex to decode by software on usual processors because it will require additional contextual bit-shifting and masking and not direct byte addressing (but the same remark would be true with modern processors and memory/storage units whose minimum addressable unit is larger than 1 octet). However modern processors and signal processors include hardware support for fast internal decoding of bit streams with automatic handling of computing units that are crossing the boundaries of addressable storage units (this is needed for efficient processing in data codecs for compression/decompression or with some encryption/decryption algorithms).|$|R
50|$|In 1994 Jordan Hargraphix Software {{released}} SVGA BGI drivers version 5.5 {{that are}} compatible with some SVGA hardware like ATI or Cirrus Logic cards and VESA VBE-compatible cards. Also there are tweaked VGA drivers for non-standard graphic modes supported by VGA by writing directly into its registers, protected mode driver versions for Turbo Pascal 7.0 and mouse driver (actually cursor handler for unsupported video modes by standard mouse drivers). These drivers were shareware and buying them let receiving their source code and technical support; now {{they are no longer}} supported and come as abandonware. Main bugs are lack of <b>aligning</b> <b>bytes</b> support in VESA TrueColor modes (so TrueColor driver is not suitable for Nvidia graphic cards) and video memory bank switching bug in mouse driver (since real mode addressing space is 1 megabyte, but some video modes require up to 4 megabytes of memory, it is split into 64 kilobyte banks).|$|R
50|$|Since {{the codes}} emitted {{typically}} do not fall on byte boundaries, the encoder and decoder must {{agree on how}} codes are packed into bytes. The two common methods are LSB-first ("least significant bit first") and MSB-first ("most significant bit first"). In LSB-first packing, the first code is aligned so that the least significant bit of the code falls in the least significant bit of the first stream byte, and if the code has more than 8 bits, the high-order bits left over are aligned with the least significant bits of the next byte; further codes are packed with LSB going into the least significant bit not yet used in the current stream byte, proceeding into further bytes as necessary. MSB-first packing aligns the first code so that its most significant bit falls in the MSB of the first stream <b>byte,</b> with overflow <b>aligned</b> with the MSB of the next byte; further codes are written with MSB going into the most significant bit not yet used in the current stream byte.|$|R
40|$|Variable-length splittable {{codes are}} derived from {{encoding}} sequences of ordered integer pairs, {{where one of the}} pair's components is upper bounded by some constant, and the other one is any positive integer. Each pair is encoded by the concatenation of two fixed independent prefix encoding functions applied to the corresponding components of a pair. The codeword of such a sequence of pairs consists of the sequential concatenation of corresponding pair's encodings. We call such codes splittable. We show that Fibonacci codes of higher orders and codes with multiple delimiters of the form 011 [...] . 10 are splittable. Completeness and universality of multi-delimiter codes are proved. Encoding of integers by multi-delimiter codes is considered in detail. For these codes, a fast <b>byte</b> <b>aligned</b> decoding algorithm is constructed. The comparative compression performance of Fibonacci codes and different multi-delimiter codes is presented. By many useful properties, multi-delimiter codes are superior to Fibonacci codes...|$|E
30|$|For {{polyphase}} filtering, we {{load the}} values column-wise, but we operate row-wise. Thus, when loading {{the data from}} GM to SM, the data is not coalesced properly, and an additional step is necessary to enforce further coalescing in the IP operation. A kernel is applied to shuffle the data to pre-position the data prior to polyphase filtering. A pseudocode specification of this data shuffling process is shown in Algorithm ??. It {{is important to note}} that we read the data in a linear fashion initially, to enforce caching on the read operation, then we write back to GM in polyphase decomposed fashion. This reduces the latency slightly compared to reading the data in polyphase decomposition manner first, and then writing it back linearly. Following this operation, the polyphase filter kernel is called. This kernel now reads the data linearly in a coalesced manner to SM. Since we instantiate threads that are multiples of a warp, the access pattern is <b>byte</b> <b>aligned</b> and linearly read, which further enhances the efficiency of the GPU implementation.|$|E
40|$|Abstract. Bitmap indices {{have gained}} wide {{acceptance}} in data ware-house applications and are an ecient access method for querying {{large amounts of}} read-only data. The main trend in bitmap index research focuses on typical business applications based on discrete attribute val-ues. However, scientic data that is mostly characterised by non-discrete attributes cannot be queried eciently by currently supported access methods. In our previous work [13] we introduced a novel bitmap algorithm called GenericRangeEval for eciently querying scientic data. We evaluated our approach based primarily on uniformly distributed and independent data. In this paper we analyse the behaviour of our bitmap index algo-rithm against various queries based on dierent data distributions. We have implemented an improved version {{of one of the}} most cited bitmap compression algorithms called <b>Byte</b> <b>Aligned</b> Bitmap Compression and adapted it to our bitmap indices. To prove the eciency of our access method, we carried out high-dimensional queries against real data taken from two dierent scientic applications, namely High Energy Physics and Astronomy. The results clearly show that depending on the underly-ing data distribution and the query access patterns, our proposed bitmap indices can signicantly improve the response time of high-dimensional queries when compared to conventional access methods. ...|$|E

