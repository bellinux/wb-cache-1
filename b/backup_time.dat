24|86|Public
50|$|A {{database}} {{with multiple}} locations {{has the advantage}} of allowing parallel backups, and hence it can potentially reduce the <b>backup</b> <b>time.</b>|$|E
50|$|Alexander's {{career began}} at South Charleston High School, {{where he was}} the high school {{football}} national player of the year his senior season as picked by Parade Magazine. He then attended West Virginia over USC, Penn State, Ohio State and Maryland. On his freshman campaign, Alexander shared <b>backup</b> <b>time</b> with Walter Easley, behind starting back Dave Riley. Alexander rushed for 426 yards and a touchdown his freshman year.|$|E
5000|$|Maintaining {{accurate}} time {{is important}} in computer systems. For example, in a transaction-processing system the recovery process reconstructs the transaction data from log files. If time stamps are used for transaction-data logging, and the time stamps of two related transactions are transposed from the actual sequence, then {{the reconstruction of the}} transaction database may not match the state before the recovery process. Server Time Protocol (STP) can be used to provide a single time source between multiple servers. Based on Network Time Protocol concepts, one of the System z servers is designated by the HMC as the primary time source (Stratum 1). It then sends timing signals to the Stratum 2 servers through use of coupling links. The Stratum 2 servers in turn send timing signals to the Stratum 3 servers. To provide availability, one of the servers can be designated as a <b>backup</b> <b>time</b> source, and a third server can be designated as an “Arbiter” to assist the <b>Backup</b> <b>Time</b> Server in determining if it should take the role of the Primary during exception conditions.|$|E
5000|$|Archiving of {{historical}} data: Uncommonly used data are moved into other databases or backed up tables. This keeps tables small and also improves <b>backup</b> <b>times.</b>|$|R
50|$|Multiplexing {{can reduce}} <b>backup</b> <b>times</b> when backing up data from non-solid state sources {{containing}} millions of small or highly fragmented files, which require very {{large amounts of}} head-seeking using traditional mechanical hard drives, and which significantly slow down the backup process.|$|R
50|$|Advantages of {{the hard}} link method are that {{it is easy to}} look at {{snapshots}} of the system at different times and also easy to remove old snapshots compared to incremental backup methods which save difference information between consecutive <b>backup</b> <b>times.</b> However, a drawback of Back In Time {{is that it does not}} allow for compression, and requires file systems that support hard links on the backup location.|$|R
50|$|A {{differential}} backup is a cumulative backup of all changes made {{since the last}} full backup, i.e., the differences since the last full backup. The advantage to this is the quicker recovery time, requiring only a full backup and the last {{differential backup}} to restore the entire data repository. The disadvantage is that for each day elapsed since the last full backup, more data needs to be backed up, especially if {{a significant proportion of}} the data has changed, thus increasing <b>backup</b> <b>time</b> as compared to the incremental backup method.|$|E
40|$|Abstract — Analyzing and {{managing}} {{large amounts of}} unstructured information is a high priority task for many companies. For implementing content management solutions, companies need a comprehensive view of their unstructured data. In order to provide {{a new level of}} intelligence and control over data resident within the enterprise, one needs to build a chain of tools and automated processes that enable the evaluation, analysis, and visibility into information assets and their dynamics during the information life-cycle. We propose a novel framework to utilize the existing backup infrastructure by integrating additional content analysis routines and extracting already available filesystem metadata over time. This is used to perform data analysis and trending required for adding performance optimization and selfmanagement capabilities to backup and information management tasks. Backup management faces serious challenges on its own: processing ever increasing amount of data while meeting the timing constraints of backup windows could require adaptive changes in backup scheduling routines. We revisit a traditional backup job scheduling and demonstrate that random job scheduling may lead to inefficient backup processing and an increased <b>backup</b> <b>time.</b> In this work, we use a historic information about the object backup processing time and suggest an additional job scheduling, and automated parameter tuning which may significantly optimize the overall <b>backup</b> <b>time.</b> Under this scheduling, called LBF, the longest backups (the objects with longest <b>backup</b> <b>time)</b> are scheduled first. We evaluate the performance benefits of the introduced scheduling using a realistic workload collected from the seven backup servers at HP Labs. Significant reduction of the <b>backup</b> <b>time</b> (up to 30 %) and improved quality of service can be achieved under the proposed job assignment policy. I...|$|E
40|$|In {{this work}} I have been briefed by {{hardware}} realization of CLINICOM. After description of this {{hospital information system}} I concerned with important attributes of the server. Next the UPS has been described with its kinds and parameters. Consequently I was getting acquainted with methods of measuring of power consumption of computers and description of instrument Energy Logger. At next I concerned with load measuring of the server NIS CLINICOM UBMI with instrument Energy Logger 3500 {{and in the next}} section of this part I have calculated theoretic <b>backup</b> <b>time.</b> Sequentially all gauging have been compared with real gauging of <b>backup</b> <b>time.</b> At the end I concerned with methods realization of automated sending of warning message about electric feed problems to administrator and every logged users of the hospital information system...|$|E
50|$|Fossil <b>backup,</b> night <b>time</b> preservation, {{and morning}} pre-heating, is {{provided}} by natural gas and provides up to 2% of total output.|$|R
5000|$|Certain {{models of}} the drives offered a tape backup option called [...] "Mirror" [...] to make hard disk backups using a VCR, which was itself a {{relatively}} new technology. A standalone version of [...] "Mirror" [...] was also made available. Data was backed up at roughly one megabyte per minute which resulted in five or ten-minute <b>backup</b> <b>times.</b> Tapes could hold up to 73MB. Even though Corvus had a patent on this technology, several other computer companies later used this technique.|$|R
50|$|There are {{technical}} {{advantages to}} backup-to-disk technology. One {{of the main}} advantages is {{the speed at which}} backups can be performed to the disk appliance. Backing up data to a backup-to-disk technology can be up to four times faster than traditional SCSI tape devices. While the new Serial Attached SCSI (SAS) connected tape drives are faster than the original tape drives, the disk appliance is still faster than most tape technologies. These faster <b>backup</b> <b>times</b> lead to shorter backup windows allowing the technology to backup the data while in a smaller amount of time thus increasing the window for processing which is also a benefit for the business.|$|R
40|$|AbstractThe {{present study}} {{discusses}} an optimal backup policy for a hard computer disk {{used for a}} personal computer or an engineering work station. The information stored in the hard disk is backed up at age T, where age refers to the elapsed time since the previous backup operation or the recovery from a hard disk failure, whichever occurred most recently. The existence of an optimal <b>backup</b> <b>time</b> that maximizes the availability is examined. A numerical example is also presented to illustrate the proposed backup policy...|$|E
40|$|International audience—Data grows at the {{impressive}} rate of 50 % per year, and 75 % {{of the digital}} world is a copy 1 ! Although keeping multiple copies of data is necessary to guarantee their availability and long term durability, in many situations the amount of data redundancy is immoderate. By keeping a single copy of repeated data, data deduplication is considered {{as one of the}} most promising solutions to reduce the storage costs, and improve users experience by saving network bandwidth and reducing <b>backup</b> <b>time.</b> However, this solution must now solve many security issues to be completely satisfying. In this paper we target the attacks from malicious clients that are based on the manipulation of data identifiers and those based on <b>backup</b> <b>time</b> and network traffic observation. We present a deduplication scheme mixing an intra-and an inter-user deduplication in order to build a storage system that is secure against the aforementioned type of attacks by controlling the correspondence between files and their identifiers, and making the inter-user deduplication unnoticeable to clients using deduplication proxies. Our method provides global storage space savings, per-client bandwidth network savings between clients and deduplication proxies, and global network bandwidth savings between deduplication proxies and the storage server. The evaluation of our solution compared to a classic system shows that the overhead introduced by our scheme is mostly due to data encryption which is necessary to ensure confidentiality...|$|E
40|$|International audienceIn {{this paper}} {{we present a}} {{strategy}} to combine supercapacitors with the battery in a 500 kVA rated uninterruptible power supply. First the sizing of the supercapacitors and battery {{taking into account the}} UPS specifications is presented. The validation of the supercapacitor and the battery models is carried by using MATLAB/SIMULINK. Second, the power sharing system between the supercapacitors and the battery is presented. The supercapacitors are added as higher energy storage to overcome the full load power during short time grid failures and to boost peak power during <b>backup</b> <b>time</b> of few minutes. The UPS waveforms obtained by simulation are presented and analyzed...|$|E
50|$|Backups can be {{compressed}} and encrypted with up to 256-bit AES. Cloud backups are deduplicated {{in order}} to further reduce <b>backup</b> size, transfer <b>times,</b> and storage costs.|$|R
3000|$|... 3.) Cloud {{infrastructure}} {{monitoring and}} audit With the presented SAaaS the security {{state of the}} entire cloud environment, especially the cloud management system will be monitored. Of interest are customer data and data path, administrative actions concerning customer’s instances (e.g., patch management), incident response <b>time,</b> <b>backup</b> restore <b>time,</b> etc. [...]. This way cross-customer monitoring {{is used by the}} cloud provider as well as a 3 rd parties, like a security service provider to ensure the overall cloud security state. Standardized interfaces enable security audits of a cloud infrastructure, which can lead to a cloud security certification. This addresses the presented cloud security problem 2) Missing security monitoring in cloud infrastructure and helps to bring assessable security features to cloud computing.|$|R
50|$|In 1962, he {{was named}} a starter in a linebacker corps that {{included}} Bill Pellington and Don Shinnick. He was mostly a <b>backup</b> during his <b>time</b> with the Colts.|$|R
40|$|International audienceIn this paper, a {{power sharing}} system between the battery and the supercapacitors in a 500 kVA rated {{uninterruptible}} power supply is presented. Compared to typical batteries, the supercapacitors can be charged and discharged very rapidly and can supply high current which are important characteristics of an UPS. Supercapacitors are then added as higher energy storage to overcome the full load power during short time grid failures and to boost peak power during <b>backup</b> <b>time</b> of few minutes. The high power demands applied to the battery are then smoothed {{by the use of}} a low-pass filter. The potential reduction of the battery stresses is subsequently studied. Mathematical models for battery and supercapacitors pack are set up and validated through experimental and manufacturer data. The UPS waveforms obtained by simulation with MATLAB/SIMULINK are presented and analyzed...|$|E
40|$|Abstract. A {{main memory}} system employs a main memory {{rather than a}} disk as a primary storage and {{efficiently}} supports various real time applications that require high performance. The time to recover the system from failure needs to be shortened for real time service, and fast index reconstruction is an essential step for data recovery. In this paper, we present a snappy B+-Tree reconstruc-tion algorithm called Max-PL. The basic Max-PL (called Max) stores the max keys of the leaf nodes at <b>backup</b> <b>time</b> and reconstructs the B+-Tree index struc-ture using the pre-stored max keys at restoration time. Max-PL employs a paral-lelism to Max {{in order to improve}} the performance. We analyze the time com-plexity of the algorithm, and perform the experimental evaluation to compare its performance with others. Using Max-PL, we achieve a speedup of 2 over Batch Construction and 6. 7 over B+-tree Insertion at least. ...|$|E
40|$|Abstract: This paper {{presents}} {{a study of}} the reduction in battery stresses by using Super capacitors (SCs) in UPS. We aim at investigating the optimal Super capacitors-battery combination versus the SCs cost. This investigation is threefold; first, super capacitors and battery models developed using MATLAB / Simulink are presented and validated. Second, the architecture and the simulation of the designed system that combines the SCs and the battery are shown. The Super capacitors are used as high-power storage devices to smooth the peak power applied to the battery during <b>backup</b> <b>time</b> and to deliver full power during short grid outages. By charging the SCs through the battery at a suitable rate, all impulse power demands would be satisfied by the Super capacitors. Third, extensive simulations are carried out to determine the gain in battery RMS current, the gain in energy losses, the energy efficiency and the elimination rate of surge load power...|$|E
50|$|While Carole King is {{the lead}} singer of this song in the 1974 version, her daughters Louise Goffin and Sherry Goffin sang <b>backup.</b> At the <b>time</b> they were children.|$|R
50|$|The Doctor V64 {{could be}} used to read the data from a game {{cartridge}} and transfer the data to a PC via the parallel port. This allowed developers and homebrew programmers to upload their game images to the Doctor V64 without having to create a CD <b>backup</b> each <b>time.</b> It also allowed users to upload game images taken from the Internet.|$|R
50|$|Arriving at Pierce's hideout, Liz {{states that}} she can't {{transfer}} to another police station {{to get away}} from the police chief because the police chief is her mother. Monica and Liz are lured into a trap by Pierce, but they outsmart him where Liz actually called for <b>backup</b> this <b>time.</b> After being tasered by Liz, Pierce is arrested by the police who arrived with Ed.|$|R
40|$|The {{telecommunications}} {{backup power}} market {{is an attractive}} early-revenue opportunity for fuel cell technology. Fuel cells offer numerous advantages over batteries and diesel generators in distributed telecom applications where extended <b>backup</b> <b>time</b> duration is required. China is an attractive geographic segment within this market, given {{it is home to}} the world 2 ̆ 7 s largest telecom industry and has one of the fastest network infrastructure expansion rates. For Ballard to capitalize on this opportunity, it needs to develop an entry strategy that addresses specific marketing and commercialization challenges pertaining to this segment. This report provides a background on telecom backup power, an overview of doing business in China, an analysis of the telecommunications power market in China, as well as an assessment of Ballard 2 ̆ 7 s challenges and key success factors. It also provides recommendations for a partnership strategy, including a selection criteria matrix and options overview, which may help Ballard succeed in this market...|$|E
40|$|The {{purpose of}} this study was to {{investigate}} how well the computer hardware/software industry was meeting the needs of the retail pharmacist. The needs were determined by a survey of 1000 Indiana pharmacists. A reply rate of 22 % revealed that the most important problems pharmacists were facing with their computer systems were slow access of the data, the length of <b>backup</b> <b>time,</b> no drug interaction check, and no multitasking. Hardware and software means of meeting these problems were studied. Also the currently available systems were evaluated in terms of these problems. It was found that while most systems were adequately meeting some of these problems no system was addressing all of them. Some of the systems were multitasking but were much too expensive for the small pharmacy. A system can be designed that meets all of these needs without neglecting the basic needs of pharmacists and at a very reasonable cost. Thesis (M. A. ...|$|E
40|$|University of Minnesota M. S. E. C. E. thesis. May 2015. Major: Electrical Engineering. Advisor: David Lilja. 1 {{computer}} file (PDF); viii, 58 pages. Recently {{the amount of}} data generated and stored on computers has seen outrageous growth and the trend will only continue. With the 24 hour global business structure being {{the way it is}} now, backup windows are shrinking and/or data is expected to be available at all times. Because of this, having effective and efficient data protection has become increasingly important. It is therefore necessary to move past the outdated static backup configurations and adopt intelligent dynamic backup systems. With that in mind we introduce the Affinity dynamic backup scheduling algorithm. Using a dynamic backup simulator we examine this algorithm as well as others and examine the performance trade-offs between these algorithms. Using this algorithm we have seen incremental improvements in the three primary metrics of Storage Throughput Utilization, Storage Distribution and <b>Backup</b> <b>Time</b> Consistency. With the insight gained from our simulation we discuss the benefits and trade-offs of dynamic scheduling algorithms and also dive into ideas and changes important to the future of backup systems...|$|E
50|$|Stanley {{returned}} to the San Jose SaberCats in 2015, where he again served as <b>backup</b> (this <b>time</b> to Erik Meyer). An injury to Meyer forced Stanley to once again become the team's starter. He {{returned to}} the role of backup following Meyer's return. In his second season, Stanley threw a total of twenty touchdown passes (and no interceptions) while winning every game in which he started.|$|R
5000|$|Off-Line Backup: Off-Line Backup allows {{along with}} {{and as part}} of the online backup {{solution}} to cover daily <b>backups</b> in <b>time</b> when network connection is down. At this <b>time</b> the remote <b>backup</b> software must perform backup onto a local media device like a tape drive, a disk or another server. The minute network connection is restored remote backup software will update the remote datacenter with the changes coming out of the off-line backup media [...]|$|R
5|$|Lunney worked backup on Gemini 3, {{taking charge}} of the newly {{established}} Mission Control Center in Houston, {{at a time when}} flights were still controlled from Cape Canaveral in Florida. On Gemini 4, he again was working <b>backup,</b> this <b>time</b> in Florida, supporting the first mission that was controlled entirely from Houston. After spending some time on unmanned testing for the Apollo program, he returned to work as a flight director on Gemini 9, 10, 11 and 12.|$|R
40|$|International audienceThis study {{presents}} {{a study of}} the reduction in battery stresses by using supercapacitors (SCs) in a 500 -kVA rated UPS. We aim at investigating the optimal supercapacitors-battery combination versus the SCs cost. This investigation is threefold; first, supercapacitors and battery models developed using MATLAB/Simulink are presented and validated. Second, the architecture and the simulation of the designed system that combines the SCs and the battery are shown. The supercapacitors are used as high-power storage devices to smooth the peak power applied to the battery during <b>backup</b> <b>time</b> and to deliver full power during short grid outages. By charging the SCs through the battery at a suitable rate, all impulse power demands would be satisfied by the supercapacitors. Third, extensive simulations are carried out to determine the gain in battery RMS current, the gain in energy losses, the energy efficiency and the elimination rate of surge load power. These four performance parameters are determined by simulation and then analyzed. The influence of the SCs recharge on the performance indicators is highlighted. A thorough analysis involving optimal study proposes to draw the optimal SCs number and filter constant from the variation of the aforementioned parameters versus the cost of the SCs...|$|E
40|$|Abstract — The {{amount of}} stored data in {{enterprise}} Data Centers quadruples every 18 months. This trend presents {{a serious challenge}} for backup management and sets new requirements for performance efficiency of traditional backup and archival tools. In this work, we discuss potential performance shortcomings of the existing backup solutions. During a backup session a predefined set of objects (client filesystems) should be backed up. Traditionally, no information on the expected duration and throughput requirements of different backup jobs is provided. This may lead to an inefficient job schedule and the increased backup session time. We analyze historic data on backup processing from eight backup servers in HP Labs, and introduce two additional metrics associated with each backup job, called job duration and job throughput. Our goal is to use this additional information for automated design of a backup schedule that minimizes the overall completion time for a given set of backup jobs. This problem can be formulated as a resource constrained scheduling problem which {{is known to be}} NP-complete. Instead, we propose an efficient heuristics for building an optimized job schedule, called FlexLBF. The new job schedule provides a significant reduction in the <b>backup</b> <b>time</b> (up to 50 %) and reduced resource usage (up to 2 - 3 times). Moreover, we design a simulation-based tool that aims to automate parameter tuning for avoiding manual configuration by system administrators while helping them to achieve nearly optimal performance. I...|$|E
40|$|Abstract—Many {{industries}} {{experience an}} explosion in digital content. This explosion of electronic documents, along with new regulations and document retention rules, sets new requirements for performance efficiency of traditional data protection and archival tools. During a backup session a predefined set of objects (client filesystems) should be backed up. Traditionally, no information on the expected duration and throughput requirements of different backup jobs is provided. This {{may lead to a}} suboptimal job schedule that results in the increased backup session time. In thiswork,wecharacterizeeachbackupjobviatwometrics,called job duration and job throughput. These metrics are derived from collected historic information about backup jobs during previous backup sessions. Our goal is to automate the design of a backup schedule that minimizes the overall completion time for a given set of backup jobs. This problem can be formulated as a resource constrained scheduling problem where a set of n jobs should be scheduled on m machines with given capacities. We provide an integer programming (IP) formulation of this problem and use available IP-solvers for finding an optimized schedule, called binpacking schedule. Performance benefits of the new bin-packing scheduleareevaluatedviaabroadvarietyofrealisticexperiments using backup processing data from six backup servers in HP Labs. The new bin-packing job schedule significantly optimizes the backup session time (20 %- 60 % of <b>backup</b> <b>time</b> reduction). HP Data Protector (DP) is HP’s enterprise backup offering and it can directly benefit from the designed technique. Moreover, significantlyreducedbackupsessiontimesguaranteeanimproved resource/power usage of the overall backup solution. I...|$|E
50|$|By the 1999-2000 season Mercer {{had found}} himself down the pecking order at Saltergate, but on 26 October 1999 League One rivals Bristol City paid Chesterfield £300,000 for Mercer's services. In {{his first season}} there, Billy made 32 {{appearances}} in all competitions, but again found himself as <b>backup,</b> this <b>time</b> to Steve Phillips. Mercer would not play another game for the Robins, but remained on their books until he retired from professional football on 15 January 2003.|$|R
50|$|Lunney worked backup on Gemini 3, {{taking charge}} of the newly {{established}} Mission Control Center in Houston, {{at a time when}} flights were still controlled from Cape Canaveral in Florida. On Gemini 4, he again was working <b>backup,</b> this <b>time</b> in Florida, supporting the first mission that was controlled entirely from Houston. After spending some time on unmanned testing for the Apollo program, he returned to work as a flight director on Gemini 9, 10, 11 and 12.|$|R
50|$|Folsom {{was mostly}} a <b>backup</b> during his <b>time</b> with the team, until 1988, {{when he started}} 4 games in place of an injured Doug Cosbie. The next year, he started 16 games after Cosbie left the Cowboys via Plan B free agency.|$|R
