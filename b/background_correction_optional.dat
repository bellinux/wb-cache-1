0|597|Public
50|$|In HR-CS AAS <b>background</b> <b>correction</b> {{is carried}} out mathematically in the {{software}} using information from detector pixels that are not used for measuring atomic absorption; hence, in contrast to LS AAS, no additional components are required for <b>background</b> <b>correction.</b>|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 126 p. This project involves the development and use of an acousto-optic <b>background</b> <b>correction</b> system for atomic emission spectroscopy. Preliminary development was done with a flint glass acousto-optic device mounted in a Rowland circle polychromator. This <b>background</b> <b>correction</b> system was characterized using a hydrogen lamp as a radiation source for acousto-optic diffraction with acousto-optic <b>background</b> <b>correction.</b> The system was then characterized using a microwave-induced plasma as a radiation source using fast acousto-optic <b>background</b> <b>correction</b> in the analysis of hydrogen gas. The acousto-optic device was then mounted in a 0. 5 m scanning monochromator, where this <b>background</b> <b>correction</b> system was characterized for acousto-optic diffraction. Also, background-corrected atomic emission spectroscopy was accomplished using a microwave-induced plasma as a radiation source. An ultrasonic nebulizer was used for sample introduction in the analysis of sodium and scandium solutions. The <b>background</b> <b>correction</b> system was then used with an electrothermal vaporization apparatus for analyte introduction into the microwave-induced plasma in the analysis of sodium, scandium and chromium solutions...|$|R
40|$|Noise removal is {{considered}} a primary and inevitable step for <b>background</b> <b>correction</b> in experimentally obtained Raman spectra. Employing an appropriate algorithm for a smoothing-free <b>background</b> <b>correction</b> technique not only increases the speed but also eliminates unwanted errors from the smoothing algorithms. Herein, we show a new smoothing-free method for <b>background</b> <b>correction,</b> which we developed by merging continuous wavelet transform and signal removal method, which in combination, {{could be applied to}} noisy signals without smoothing. We used wavelet transformation for suppressing the side effects of noise and eliminating peaks from the spectrum, thereby providing spectral sections purely related to the background {{to be used in the}} <b>background</b> <b>correction</b> process. We applied a range of statistical analyses to test the performance of this algorithm, wherein a low deviation in <b>background</b> <b>correction</b> procedure was observed. Additionally, when we tested this algorithm for experimentally obtained real Raman spectra, it showed good capability to correct background of noisy signals without the requirement of a smoothing process...|$|R
5000|$|... 4th edition <b>corrections</b> and <b>optional</b> {{rules for}} World in Flames ...|$|R
5000|$|... #Subtitle level 2: Background {{absorption}} and <b>background</b> <b>correction</b> ...|$|R
5000|$|... #Subtitle level 4: <b>Background</b> <b>{{correction}}</b> using correction pixels ...|$|R
40|$|<b>Background</b> <b>correction</b> is an {{essential}} part in LIBS signal analysis. The interpolation method of <b>background</b> <b>correction</b> has major drawbacks. This paper introduces an interpolation method to overcome the shortcomings of linear and cubic spline interpolation methods. Finally, we compare different interpolation methods to verify the proposed interpolation method...|$|R
5000|$|... #Subtitle level 3: <b>Background</b> <b>correction</b> {{techniques}} in HR-CS AAS ...|$|R
5000|$|... #Subtitle level 4: <b>Background</b> <b>correction</b> using a least-squares {{algorithm}} ...|$|R
40|$|Motivation: Illumina BeadArray {{technology}} includes {{negative control}} features that allow a precise {{estimation of the}} background noise. As {{an alternative to the}} background subtraction proposed in BeadStudio which leads to an important loss of information by generating negative values, a <b>background</b> <b>correction</b> method modeling the observed intensities as the sum of the exponentially distributed signal and normally distributed noise has been developed. Nevertheless, Wang and Ye (2011) display a kernel-based estimator of the signal distribution on Illumina BeadArrays and suggest that a gamma distribution would represent a better modeling of the signal density. Hence, the normal-exponential modeling may not be appropriate for Illumina data and <b>background</b> <b>corrections</b> derived from this model may lead to wrong estimation. Results: We propose a more flexible modeling based on a gamma distributed signal and a normal distributed background noise and develop the associated <b>background</b> <b>correction.</b> Our model proves to be markedly more accurate to model Illumina BeadArrays: on the one hand, this model offers a more correct fit of the observed intensities. On the other hand, the comparison of the operating characteristics of several <b>background</b> <b>correction</b> procedures on spike-in and on normal-gamma simulated data shows high similarities, reinforcing the validation of the normal-gamma modeling. The performance of the <b>background</b> <b>corrections</b> based on the normal-gamma and normal-exponential models are compared on two dilution data sets. Surprisingly, we observe that the implementation of a more accurate parametrisation in the model-based <b>background</b> <b>correction</b> does not increase the sensitivity. These results may be explained by the operating characteristics of the estimators: the normal-gamma <b>background</b> <b>correction</b> offers an improvement in terms of bias, but at the cost of a loss in precision...|$|R
40|$|International audienceBACKGROUND:Illumina BeadArray {{technology}} includes {{non specific}} negative control features that allow a precise {{estimation of the}} background noise. As {{an alternative to the}} background subtraction proposed in BeadStudio which leads to an important loss of information by generating negative values, a <b>background</b> <b>correction</b> method modeling the observed intensities as the sum of the exponentially distributed signal and normally distributed noise has been developed. Nevertheless, Wang and Ye (2012) display a kernel-based estimator of the signal distribution on Illumina BeadArrays and suggest that a gamma distribution would represent a better modeling of the signal density. Hence, the normal-exponential modeling may not be appropriate for Illumina data and <b>background</b> <b>corrections</b> derived from this model may lead to wrong estimation. RESULTS:We propose a more flexible modeling based on a gamma distributed signal and a normal distributed background noise and develop the associated <b>background</b> <b>correction,</b> implemented in the R-package NormalGamma. Our model proves to be markedly more accurate to model Illumina BeadArrays: on the one hand, it is shown on two types of Illumina BeadChips that this model offers a more correct fit of the observed intensities. On the other hand, the comparison of the operating characteristics of several <b>background</b> <b>correction</b> procedures on spike-in and on normal-gamma simulated data shows high similarities, reinforcing the validation of the normal-gamma modeling. The performance of the <b>background</b> <b>corrections</b> based on the normal-gamma and normal-exponential models are compared on two dilution data sets, through testing procedures which represent various experimental designs. Surprisingly, we observe that the implementation of a more accurate parametrisation in the model-based <b>background</b> <b>correction</b> does not increase the sensitivity. These results may be explained by the operating characteristics of the estimators: the normal-gamma <b>background</b> <b>correction</b> offers an improvement in terms of bias, but at the cost of a loss in precision. CONCLUSIONS:This paper addresses the lack of fit of the usual normal-exponential model by proposing a more flexible parametrisation of the signal distribution as well as the associated <b>background</b> <b>correction.</b> This new model proves to be considerably more accurate for Illumina microarrays, but the improvement in terms of modeling does not lead to a higher sensitivity in differential analysis. Nevertheless, this realistic modeling makes way for future investigations, in particular to examine the characteristics of pre-processing strategies...|$|R
40|$|In two con gurations, a {{solid-state}} acousto-optic (AO) de ector or modulator {{is mounted}} in a 0. 5 m monochromator for <b>background</b> <b>correction</b> with {{inductively coupled plasma}} atomic emission spectrometry (ICP-AES). A fused silica acousto-optic modulator (AOM) {{is used in the}} ultraviolet (UV) spectral region applications while a glass AO de ector (AOD) is used for the visible (VIS) region. The system provides rapid sequential observation of adjacent on- and off-line wavelengths for <b>background</b> <b>correction.</b> Seventeen elements are examined using pneumatic nebulization (PN) and electrothermal vaporization (ETV) sample introduction. Calibration plots were obtained with each sample introduction technique. Potable water and vitamin tablets were analyzed. Flame atomic absorption (FAA) was used to verify the accuracy of the AO <b>background</b> <b>correction</b> system...|$|R
40|$|Background: Illumina BeadArray {{technology}} includes {{non specific}} negative control features that allow a precise {{estimation of the}} background noise. As {{an alternative to the}} background subtraction proposed in BeadStudio which leads to an important loss of information by generating negative values, a <b>background</b> <b>correction</b> method modeling the observed intensities as the sum of the exponentially distributed signal and normally distributed noise has been developed. Nevertheless, Wang and Ye (2012) display a kernel-based estimator of the signal distribution on Illumina BeadArrays and suggest that a gamma distribution would represent a better modeling of the signal density. Hence, the normal-exponential modeling may not be appropriate for Illumina data and <b>background</b> <b>corrections</b> derived from this model may lead to wrong estimation. [br/] Results: We propose a more flexible modeling based on a gamma distributed signal and a normal distributed background noise and develop the associated <b>background</b> <b>correction,</b> implemented in the R-package NormalGamma. Our model proves to be markedly more accurate to model Illumina BeadArrays: on the one hand, it is shown on two types of Illumina BeadChips that this model offers a more correct fit of the observed intensities. On the other hand, the comparison of the operating characteristics of several <b>background</b> <b>correction</b> procedures on spike-in and on normal-gamma simulated data shows high similarities, reinforcing the validation of the normal-gamma modeling. The performance of the <b>background</b> <b>corrections</b> based on the normal-gamma and normal-exponential models are compared on two dilution data sets, through testing procedures which represent various experimental designs. Surprisingly, we observe that the implementation of a more accurate parametrisation in the model-based <b>background</b> <b>correction</b> does not increase the sensitivity. These results may be explained by the operating characteristics of the estimators: the normal-gamma <b>background</b> <b>correction</b> offers an improvement in terms of bias, but at the cost of a loss in precision. [br/] Conclusions: This paper addresses the lack of fit of the usual normal-exponential model by proposing a more flexible parametrisation of the signal distribution as well as the associated <b>background</b> <b>correction.</b> This new model proves to be considerably more accurate for Illumina microarrays, but the improvement in terms of modeling does not lead to a higher sensitivity in differential analysis. Nevertheless, this realistic modeling makes way for future investigations, in particular to examine the characteristics of pre-processing strategies...|$|R
40|$|Methods for {{identifying}} differential expression were compared on time series microarray data from artificial gene networks. Identifying differential expression {{was dependent on}} normalization and whether the background was removed. Loess after <b>background</b> <b>correction</b> improved results for most methods. On data without <b>background</b> <b>correction</b> median centering improved performance. We recommend Cui and Churchill’s ANOVA variants on background subtracted data and Efron and Tibshirani’s Empirical Bayes Wilcoxon Rank Sum test when the background cannot be removed...|$|R
40|$|X-ray Talbot-Lau {{interferometer}} {{has been}} used widely to conduct phase contrast imaging with a conventional low-brilliance x-ray source. Typically, in this technique, <b>background</b> <b>correction</b> has to be performed {{in order to obtain}} the pure signal of the sample under inspection. In this study, we reported on a research on the <b>background</b> <b>correction</b> strategies within this technique, especially we introduced a new phase unwrapping solution for one conventional <b>background</b> <b>correction</b> method, the key point of this new solution is changing the initial phase of each pixel by a cyclic shift operation on the raw images collected in phase stepping scan. Experimental result and numerical analysis showed that the new phase unwrapping algorithm could successfully subtract contribution of the system's background without error. Moreover, a potential advantage of this phase unwrapping strategy is that its effective phase measuring range could be tuned flexibly in some degree for example to be (-pi+ 3, pi+ 3], thus it would find usage in certain case because measuring range of the currently widely used <b>background</b> <b>correction</b> method is fixed to be (-pi, pi]. Comment: 23 pages, 8 figures, english polishe...|$|R
40|$|In {{near-infrared}} (NIR) {{analysis of}} plant extracts, excessive background often exists in near-infrared spectra. The detection of active constituents {{is difficult because}} of excessive <b>background,</b> and <b>correction</b> of this problem remains difficult. In this work, the orthogonal signal correction (OSC) method was used to correct excessive background. The method was also compared with several classical <b>background</b> <b>correction</b> methods, such as offset correction, multiplicative scatter correction (MSC), standard normal variate (SNV) transformation, de-trending (DT), first derivative, second derivative and wavelet methods. A simulated dataset and a real NIR spectral dataset were {{used to test the}} efficiency of different <b>background</b> <b>correction</b> methods. The results showed that OSC is the only effective method for correcting excessive background...|$|R
40|$|Despite the {{tremendous}} growth of microarray usage in scientific studies, {{there is a}} lack of standards for <b>background</b> <b>correction</b> methodologies, especially in single-color microarray platforms. Traditional background subtraction methods often generate negative signals and thus cause large amounts of data loss. Hence, some researchers prefer to avoid <b>background</b> <b>corrections,</b> which typically result in the underestimation of differential expression. Here, by utilizing nonspecific negative control features integrated into Illumina whole genome expression arrays, we have developed a method of model-based <b>background</b> <b>correction</b> for BeadArrays (MBCB). We compared the MBCB with a method adapted from the Affymetrix robust multi-array analysis algorithm and with no background subtraction, using a mouse acute myeloid leukemia (AML) dataset. We demonstrated that differential expression ratios obtained by using the MBCB had the best correlation with quantitative RT–PCR. MBCB also achieved better sensitivity in detecting differentially expressed genes with biological significance. For example, we demonstrated that the differential regulation of Tnfr 2, Ikk and NF-kappaB, the death receptor pathway, in the AML samples, could only be detected by using data after MBCB implementation. We conclude that MBCB is a robust <b>background</b> <b>correction</b> method that will lead to more precise determination of gene expression and better biological interpretation of Illumina BeadArray data...|$|R
40|$|The aim of {{this study}} was to {{estimate}} the reproducibility and accuracy of 99 mTc-mercaptoacetyltriglycine (MAG 3) relative percentage uptake. Methods: Reproducibility was evaluated on healthy volunteers who were submitted twice to a 99 mTc-MAG 3 renographic study, which used different uptake algorithms, different <b>background</b> <b>corrections</b> and different time intervals. Accuracy was evaluated in a group of patients with symmetrical or asymmetrical relative renal function, who underwent both 99 mTc- dimercaptosuccinic acid (DMSA) and 99 mTc-MAG 3 studies, using the DMSA relative percentage uptake as a reference. Results and Conclusion: The methods that combined the best reproducibility and accuracy for estimating 99 mTc-MAG 3 left-to-right uptake ratio were the integral method, with subrenal or perirenal <b>background</b> <b>correction,</b> and the Patlak-Rutland plot. The use of the integral method without <b>background</b> <b>correction</b> introduced a systematic bias, whereas the slope method resulted in high variability. Therefore these methods cannot be recommended. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
2500|$|... signal {{intensity}} of the area between spots. A variety of tools for <b>background</b> <b>correction</b> and further analysis are available from TIGR, Agilent (GeneSpring), and Ocimum Bio Solutions (Genowiz).|$|R
40|$|Abstract. According to {{the uneven}} {{illumination}} or the very complex background cases, global threshold value method can't correctly to binary particle image; this paper puts forward {{a kind of}} <b>background</b> <b>correction</b> method and the OTSU for particle image binary method. It's used <b>background</b> <b>correction</b> method to eliminate the influence of uneven illumination, and the OTSU for binary particle image. Combining with the above methods are tested, and the result shows that, after the background is corrected, we segment the background brightness particle image; the OTSU can obtain ideal image segmentation effect. ...|$|R
40|$|International audienceIn {{order to}} vertify the {{feasibility}} of the near-infrared to detect the veterinary drug in honey, studying the impact of different <b>background</b> <b>correction</b> methods for linear modeling and nonlinear modeling. Use PLSR to establish the linear model between near-infrared spectroscopy and to be measured, and LSSVR to establish the nonlinear model between near-infrared spectroscopy and to be measured. At the same time, we used Wavelet transform, Multiplicative Scatter Correction, Standard Normalized Variate, First derivative transform, Second derivative transform, Orthogonal Signal <b>Correction</b> and other <b>background</b> <b>correction</b> methods to improve {{the feasibility of}} the near-infrared detection...|$|R
40|$|Abstract—Quantitative {{confocal}} microscopy is {{a powerful}} analytical tool used to visualize the associations between cellular processes and anatomical struc-tures. In our biological experiments, we use quantitative confocal microscopy to study the association of three cellular components: binding proteins, recep-tors, and organelles. We propose an automated method that will (1) reduce the time consuming effort of manual <b>background</b> <b>correction</b> and (2) compute numerical coefficients to associate cellular process with structure. The project is implemented, end-to-end, in Python. Pure Python is used for managing file access, input parameters, and initial processing of the repository of 933 images. NumPy is used to apply manual <b>background</b> <b>correction,</b> to compute the automated <b>background</b> <b>corrections,</b> and to calculate the domain specific coefficients. We visualize the raw intensity values and computed coefficient values with Tufte-style panel plots created in matplotlib. A longer term goal of this work is to explore plausible extensions of our automated methods to triple-label coefficients. Index Terms—confocal microscopy, immunofluorescence, thresholding, colo-calization coefficient...|$|R
40|$|AbstractMethods for {{identifying}} differentially expressed genes were compared on time-series microarray data simulated from artificial gene networks. Select methods were further analyzed on existing immune response data of Boldrick et al. (2002, Proc. Natl. Acad. Sci. USA 99, 972 – 977). Based on the simulations, we recommend the ANOVA variants of Cui and Churchill. Efron and Tibshirani’s empirical Bayes Wilcoxon rank sum test is recommended when the background cannot be effectively corrected. Our proposed GSVD-based differential expression method {{was shown to}} detect subtle changes. ANOVA combined with GSVD was consistent on background-normalized simulation data. GSVD with empirical Bayes was consistent without <b>background</b> <b>correction.</b> Based on the Boldrick et al. data, ANOVA is best suited to detect changes in temporal data, while GSVD and empirical Bayes effectively detect individual spikes or overall shifts, respectively. For methods tested on simulation data, lowess after <b>background</b> <b>correction</b> improved results. On simulation data without <b>background</b> <b>correction,</b> lowess decreased performance compared to median centering...|$|R
30|$|SUVmax are {{commonly}} used for threshold-dependent volume definition in a clinical setting. If the delineation is strictly based on SUVmax (i.e., relative threshold without <b>background</b> <b>correction),</b> these differences would also implicate corresponding MTV deviations as reported previously [12]. However, we refrained from volumetric analyses as such delineation methods may not reflect the clinical practice where more sophisticated algorithms with <b>background</b> <b>correction</b> or manual MTV delineation are required - especially in hepatic lesions [14, 25, 26]. Thus, the actual MTV deviations may be lower and less dependent on the TBR than the current results on SUVmax deviations suggest.|$|R
3000|$|B-carrier was {{administered}} {{because of the}} slight genotoxicity of the drug (Table  1). Therefore, for <b>background</b> <b>correction,</b> we used the net MN frequency to exclude {{the effects of the}} genotoxicity of the [...]...|$|R
40|$|We {{performed}} a prospective study {{to establish the}} optimal <b>background</b> <b>correction</b> algorithm for the determination of differential renal function (DRF), using Tc- 99 (m) -diethylenetriamine pentaacetate (Tc- 99 (m) -DTPA) {{in the presence of}} unilateral hydronephrosis, with 24 h Tc- 99 (m) -dimercaptosuccinic acid (Tc- 99 (m) -DMSA) uptake as the `gold standard'. From September 1996 to June 1997, 12 males and 4 females (mean age 10 years, range 1 month to 72 years), presenting with unilateral hydronephrosis, were studied. All patients underwent both DTPA renography and quantitative DMSA scintigraphy within 24 h. In all patients, using a surface method, the DRF of the obstructed kidney was determined using infrarenal, suprarenal and perirenal <b>background</b> <b>correction,</b> time intervals of 60 - 180 s (t(1)), 120 - 180 s (t(2)) and 80 - 140 s (t(3)), and the application or non-application of a Rutland-Patlak correction (RPC). In the absence of RPC, for all three types of <b>background</b> <b>correction,</b> no difference in DTPA DRF for any of the three time intervals was noted; higher DTPA DRF values were found (mean +/- S. D. : overestimates of 7. 8 +/- 24. 4 %, 6. 5 +/- 9. 5 % and 3. 3 +/- 14. 9 % for suprarenal, infrarenal and perirenal <b>background</b> <b>correction,</b> respectively). Application of RPC resulted in an overall decrease in both the mean and standard deviation values, which was most pronounced with infrarenal background correction: - 0. 38 +/- 6. 5 % for t 1, 0. 31 +/- 6. 3 % for t(2) and - 1. 3 +/- 6. 9 % for t(3) (t(1) vs t(2) P = 0. 06; t(3) vs t(1) or t(2), P = 0. 04). Our results suggest that infrarenal <b>background</b> <b>correction</b> using t(1) or t(2) and RPC is the best algorithm for DRF estimation using Tc- 99 (m) -DTPA renography. ((C) 1998 Chapman & Hall Ltd.). status: publishe...|$|R
40|$|Abstract Background The {{standard}} approach for preprocessing spotted microarray data is to subtract the local background intensity from the spot foreground intensity, {{to perform a}} log 2 transformation and to normalize the data with a global median or a lowess normalization. Although well motivated, {{standard approach}}es for <b>background</b> <b>correction</b> and for transformation have been widely criticized because they produce high variance at low intensities. Whereas various alternatives to the standard <b>background</b> <b>correction</b> methods and to log 2 transformation were proposed, impacts of both successive preprocessing steps were not compared in an objective way. Results In this study, we assessed the impact of eight preprocessing methods combining four <b>background</b> <b>correction</b> methods and two transformations (the log 2 and the glog), by {{using data from the}} MAQC study. The current results indicate that most preprocessing methods produce fold-change compression at low intensities. Fold-change compression was minimized using the Standard and the Edwards <b>background</b> <b>correction</b> methods coupled with a log 2 transformation. The drawback of both methods is a high variance at low intensities which consequently produced poor estimations of the p-values. On the other hand, effective stabilization of the variance as well as better estimations of the p-values were observed after the glog transformation. Conclusion As both fold-change magnitudes and p-values are important in the context of microarray class comparison studies, we therefore recommend to combine the Edwards correction with a hybrid transformation method that uses the log 2 transformation to estimate fold-change magnitudes and the glog transformation to estimate p-values. </p...|$|R
30|$|The {{heavy metal}} {{concentration}} {{present in the}} solid waste, leachate and soil was analyzed using atomic absorption spectrophotometer (AAS) supplied by Thermo Fisher Scientific, USA with D 2 <b>background</b> <b>correction</b> lamp. Acetylene–air flame was used for higher concentrations (mg/l).|$|R
40|$|<b>Background</b> <b>correction</b> is a {{major problem}} in {{recording}} the Laser Induced Breakdown Spectroscopy(LIBS),which is an important pre-processing technique used to separate signals. For this purpose,the algorithm can automatically correct varying continuum background based on a penalized least squares method [...] ...|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 224 p. A solid-state acousto-optic deflector (AOD) or modulator (AOM) was mounted in a 0. 5 m monochromator with inductively coupled plasma atomic emission spectrometry (ICP-AES) <b>background</b> <b>correction.</b> The fused silica AOM was used for ultraviolet (UV) spectral region applications while the glass AOD was used for the visible (VIS) region. The system provided rapid sequential observation of adjacent on and off-line wavelengths for <b>background</b> <b>correction.</b> Acousto-optic (AO) <b>background</b> <b>correction</b> was successfully employed for six elements in the visible region of the spectrum. Both pneumatice nebulization (PN) and electrothermal vaporization (ETV) sample introduction methods were investigated. The linearities of the calibration plots were good for both PN and ETV. The r 2 values for PN ranged from 0. 991 to 0. 999993 and from 0. 990 to 0. 99997 for ETV. Detection limits were calculated for the elements with each sample introduction technique. Detection limits using PN ranged from 2 to 20 ppm and the ETV detection limits ranged from 3 to 100 ng. AO <b>background</b> <b>correction</b> was successfully employed with ICP for fifteen elements in the UV region of the spectrum. Both PN and ETV sample introduction methods were investigated. The linearities of the calibration plots were good for both PN and ETV. The r 2 values for PN ranged from 0. 997 to 0. 99995 and from 0. 992 to 0. 999997 for ETV. Detection limits were calculated for the elements with each sample introduction technique. The PN detection limits ranged from 0. 3 to 8 ppm and the ETV detection limits ranged from 1 to 65 ng. Finally, potable water and vitamin tablets were analyzed for Ca, Cu, Fe, Mg, Mn, and Zn with the flint glass AOD and the fused silica AOM. Flame atomic absorption (FAA) was used to verify the accuracy of the AO <b>background</b> <b>correction</b> system. Overall, the ICP and FAA methods yielded similar mean concentrations for both the water samples and the vitamin samples. These values agreed on average within +/- 13 % for all samples and elements analyzed...|$|R
30|$|The {{effect of}} {{increasing}} activity from hot regions on adjacent lesion quantification, {{as well as}} the improvement brought about by the recently proposed <b>background</b> <b>correction</b> technique has been extensively studied in this work. This study shows that lesions relatively close to hot regions (within 15 – 20 [*]mm) are greatly affected by the spill-in effect, causing reduced visibility and activity overestimation of lesions. This effect is more pronounced in SUVmax than SUVmean and reduces over iteration, but it is further aggravated by the use of filter. However, improved quantification and better lesion detectability were achieved with the recently proposed <b>background</b> <b>correction</b> technique irrespective of the lesion size, lesion distance from the hot region, the activity in the hot region or application of post-filter.|$|R
30|$|Due {{to various}} {{instrumental}} and scattering effects, an actual FTIR spectrum gets superimposed {{on top of}} a background. Similar to Raman spectroscopy, the background in FTIR signals consists of broad bands (low frequency regions). These <b>background</b> <b>correction</b> methods could be as simple as subtracting an offset (DC shift) or removing a piecewise constructed baseline by selecting a few points and joining those points through straight lines [44]. Various complex background correction/baseline removal techniques explained earlier for Raman spectroscopy can also be used for FTIR spectra, but some of those techniques are more suitable than others. Such techniques include polynomial fitting and differentiation based on SG filters [32]. Generally, lower order polynomials (second or third order) are well suitable for FTIR spectroscopy. In fact, this technique can be combined with certain normalization techniques such as Multiplicative Scatter correction (MSC) to perform <b>background</b> <b>correction</b> and normalization simultaneously. More details about this technique are given below. As second order derivatives can efficiently remove the background present in FTIR spectra, SG-based second order differentiation techniques are very popular for <b>background</b> <b>correction</b> of FTIR spectra [45]. As the SG filter is a nonlinear weighted smoothing function, it makes sure that high frequency noise amplified during second order differentiation is well suppressed.|$|R
40|$|Abstract Background DNA {{methylation}} plays a {{very important}} role in the silencing of tumor suppressor genes in various tumor types. In order to gain a genome-wide understanding of how changes in methylation affect tumor growth, the differential methylation hybridization (DMH) protocol has been developed and large amounts of DMH microarray data have been generated. However, it is still unclear how to preprocess this type of microarray data and how different <b>background</b> <b>correction</b> and normalization methods used for two-color gene expression arrays perform for the methylation microarray data. In this paper, we demonstrate our discovery of a set of internal control probes that have log ratios (M) theoretically equal to zero according to this DMH protocol. With the aid of this set of control probes, we propose two LOESS (or LOWESS, locally weighted scatter-plot smoothing) normalization methods that are novel and unique for DMH microarray data. Combining with other normalization methods (global LOESS and no normalization), we compare four normalization methods. In addition, we compare five different <b>background</b> <b>correction</b> methods. Results We study 20 different preprocessing methods, which are the combination of five <b>background</b> <b>correction</b> methods and four normalization methods. In order to compare these 20 methods, we evaluate their performance of identifying known methylated and un-methylated housekeeping genes based on two statistics. Comparison details are illustrated using breast cancer cell line and ovarian cancer patient methylation microarray data. Our comparison results show that different <b>background</b> <b>correction</b> methods perform similarly; however, four normalization methods perform very differently. In particular, all three different LOESS normalization methods perform better than the one without any normalization. Conclusions It is necessary to do within-array normalization, and the two LOESS normalization methods based on specific DMH internal control probes produce more stable and relatively better results than the global LOESS normalization method. </p...|$|R
50|$|Deuterium HCL or even {{hydrogen}} HCL and deuterium {{discharge lamps}} {{are used in}} LS AAS for <b>background</b> <b>correction</b> purposes. The radiation intensity emitted by these lamps decreases significantly with increasing wavelength, {{so that they can}} be only used in the wavelength range between 190 and about 320 nm.|$|R
30|$|The spill-in {{effect is}} {{dependent}} on the activity in the hot region, lesion size and location, as well as post-filtering; and this is more evident in SUVmax than SUVmean. However, the recently proposed <b>background</b> <b>correction</b> method facilitates stability in quantification and enhances the contrast in lesions with low uptake.|$|R
50|$|Oshlack's {{research}} has focused on methods for the analysis of genome expression, notably computational and statistical analysis of transcriptome data. This includes work on <b>background</b> <b>correction</b> methods for two-colour microarrays, normalisation of microarray data, comparative analysis of RNAseq data and normalisation of methylation patterns in human DNA beadchip data.|$|R
