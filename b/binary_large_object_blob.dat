10|5734|Public
5000|$|Abstraction for <b>binary</b> <b>large</b> <b>object</b> (<b>BLOB)</b> and {{character}} large object (CLOB) handling ...|$|E
50|$|A <b>Binary</b> <b>Large</b> <b>OBject</b> (<b>BLOB)</b> is a {{collection}} of binary data stored as a single entity in a database management system. Blobs are typically images, audio or other multimedia objects, though sometimes binary executable code is stored as a blob. Database support for blobs is not universal.|$|E
50|$|Raster data {{is stored}} in various formats; from a {{standard}} file-based structure of TIFF, JPEG, etc. to <b>binary</b> <b>large</b> <b>object</b> (<b>BLOB)</b> data stored directly in a relational database management system (RDBMS) similar to other vector-based feature classes. Database storage, when properly indexed, typically allows for quicker retrieval of the raster data but can require storage of millions of significantly sized records.|$|E
50|$|Database {{triggers}} in Drizzle {{are supported}} for DML, DDL, {{and a number}} of additional event-based operations in the server. The PrimeBase BLOB streaming system, which allows Drizzle to stream <b>binary</b> <b>large</b> <b>objects</b> (<b>BLOBs)</b> via HTTP, makes use of this system. All triggers for Drizzle currently must be written in C++.|$|R
50|$|In 2010, Metalogix {{acquired}} the StoragePoint product from BlueThread Technologies. StoragePoint is a SharePoint storage optimization solution that offloads unstructured SharePoint content data, known as <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOBs)</b> from SharePoint’s underlying SQL database to alternate tiers of storage. The company announced an integrated file share consolidation solution with StoragePoint and Content Matrix in July 2012. StoragePoint 4.0 {{was released in}} November 2012 with new SharePoint backup and restore features.|$|R
50|$|A {{workable}} {{implementation of}} a time series database can be deployed in a conventional SQL-based relational database provided that the database software supports both <b>binary</b> <b>large</b> <b>objects</b> (<b>BLOBs)</b> and user-defined functions. SQL statements that operate on one or more time series quantities on the same row of a table or join can easily be written, as the user-defined time series functions operate comfortably inside of a SELECT statement. However, time series functionality such as a SUM function operating {{in the context of}} a GROUP BY clause cannot be easily achieved.|$|R
40|$|Most {{applications}} that access large data objects do so through file systems, but file systems provide an incomplete solution, as they maintain insufficient metadata {{and do not}} provide general purpose query engine. Storing large objects in a database addresses these problems, but, for {{applications that}} need to update object data, databases are inefficient as they do not provide direct access to data. Additionally, databases often relax the integrity and consistency constraints for large objects, as it the case with objects stored through the <b>Binary</b> <b>Large</b> <b>Object</b> (<b>BLOB)</b> data type. These shortcomings are exacerbated by multiple users or applications that wish to access large objects concurrently. We describe an architecture, based on the Datalink data type, in which large objects in a database are continuously available for read access and can be read and written through a file system interface. Additionally, this system does not relax version management, consistency and recoverability gua [...] ...|$|E
40|$|Abstract. In this paper, we have {{designed}} and integrated an automatic optical inspection system, emphasizing on software {{implementation of the}} image processing, measurement and analysis utilities. As for the hardware equipments, we design an LED illumination unit and the custom-tailored machinery. By comparing the support functions of several main import brands of the optical inspection machine, we propose an optical inspecting procedure. By using the Windows-based user interface, we implement nine inspecting software tools, namely, the average gray level tool, the thresholding tool, the positioning tool, the edge detection tool, the <b>binary</b> <b>large</b> <b>object</b> (<b>BLOB)</b> tool, the template building tool, the smart matching tool, the inspection sequence tool, and the platform operation tool. All these tools {{can be used in}} an inspection with single operation and can also be arranged in a proper sequence of operations to fulfill a complicated inspection procedure. We use several part sample images with defects provided by the supplier to verify our fulfilled system...|$|E
40|$|Similarity search, or query by content, is an {{important}} operation for time series databases. While the research community has been quite active in this area, we are yet to see full support for these operations and data from commercial Database Management Systems (DBMS). In this work we explore how efficiently can similarity search algorithms be implemented inside a DBMS with User Defined Functions (UDFs). We concentrate our work on querying Electrocardiograms (ECG), a biosignal and {{a particular type of}} time series data. Given the lack of support for time series as an standard data type, we identify two alternatives for managing a database of ECG signals using a DBMS, namely using either references to flat files on the operating system, or using <b>Binary</b> <b>Large</b> <b>Object</b> (<b>BLOB)</b> attributes. Our experiments show a significant overhead in the total elapsed time while doing similarity search on signals stored as BLOB. On the other hand, querying signals stored as files using UDFs is as competitive as using an ad-hoc implementation of the query by content algorithm running as a stand-alone application on the operating system...|$|E
50|$|Ideally, these {{repositories}} {{are often}} natively implemented using specialized database algorithms. However, {{it is possible}} to store time series as <b>binary</b> <b>large</b> <b>objects</b> (<b>BLOBs)</b> in a relational database or by using a VLDB approach coupled with a pure star schema. Efficiency is often improved if time is treated as a discrete quantity rather than as a continuous mathematical dimension. Database joins across multiple time series data sets is only practical when the time tag associated with each data entry spans the same set of discrete times for all data sets across which the join is performed.|$|R
50|$|The data {{primitives}} {{are called}} 'components' {{and they are}} atomic. Components can be concatenated into short composites called 'Items' which are the unit of storage and retrieval. Higher-level structures that combine these Items include unlimited size records of an unlimited number of columns or attributes, with complex attribute values of unlimited size. Keys may be a composition of components. Attribute values can be ordered sets of composite components, character <b>large</b> <b>objects</b> (CLOB's), <b>binary</b> <b>large</b> <b>objects</b> (<b>BLOB's),</b> or unlimited sparse arrays. Other higher-level structures built of multiple Items include key/value associations like ordered maps, ordered sets, Entity-Attribute-Value nets of quadruples, trees, DAG's, taxonomies, or full-text indexes. Mixtures of these can occur along with custom client-defined structures.|$|R
40|$|We present {{recommendations}} on Performance Management for databases supporting <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOB)</b> that, under {{a wide range}} of conditions, save both storage space and database transactions processing time. The research shows that for database applications where ad hoc retrieval queries prevail, storing the actual values of BLOBs in the database may be the best choice to achieve better performance, whereas storing BLOBs externally is the best approach where multiple Delete/Insert/Update operations on BLOBs dominate. Performance measurements are used to discover System Performance Bottlenecks and their resolution. We propose a strategy of archiving large data collections in order to reduce data management overhead in the Relational Database and maintain acceptable response time. ...|$|R
40|$|We {{describe}} a code services retrieval system tool designed and implemented within a grid test-bed environment. Among {{the problems and}} challenges for code services systems included are: how to store; and, how to query, access and build the code services efficiently {{within the context of}} distributed storage sites. This thesis proposes a solution that integrates object-relational database technology, the Java JINI concept and JDBC technologies together to establish a centralized object-relational database based code services retrieval system tool for software reuse. In this thesis, we focus on how to store the code services and find which system architecture is suitable for code services retrieval system. We design and implement generic user-defined data types, including Character Large Object (CLOB) and <b>Binary</b> <b>Large</b> <b>Object</b> (<b>BLOB)</b> to store source codes and corresponding binary codes respectively. We use pure Java client-side JDBC to provide access to our new data types (CLOB and BLOB). The code services stored in the centralized object-relational database can be accessed remotely across networks. The portability of the centralized database is enhanced through use of Java. This system is access-effective. Paper copy at Leddy Library: Theses 2 ̆ 6 Major Papers - Basement, West Bldg. / Call Number: Thesis 2002. Z 53. Source: Masters Abstracts International, Volume: 41 - 04, page: 1125. Adviser: Robert D. Kent. Thesis (M. Sc.) [...] University of Windsor (Canada), 2002...|$|E
40|$|Material {{scientists}} regularly {{acquire and}} analyze infrared images of deforming objects in their material tensile deformation, crack propagation, and fracture toughness tests. Although {{there are many}} image processing packages, {{none of them are}} available in a database integrated fashion. Material scientists typically select a set of image files satisfying given constraints by browsing a file directory/catalog, and then perform simple but labor-intensive contentquerying on the images. These simple queries take days to answer. A more serious problem is that material scientists are not equipped with the flexibility to query images across different time snapshots or materials to validate their research hypotheses. In this paper, we report about our work on helping material scientists accelerate their work. Specifically, we propose a database approach {{to solve the problem of}} storing images and querying the content of images. In particular, we (1) proposed to use map algebra operations to compose image content querying needed by material scientists; (2) developed an SQL integrated image data cartridge which implements a core set of map algebra operations needed by material scientists; (3) analyzed the query processing and evaluation challenges; and (4) empirically evaluated the performance of three approaches, namely a multi-dimensional array based approach, a relational table based approach, and a <b>binary</b> <b>large</b> <b>object</b> (<b>BLOB)</b> based approach on bulk loading and typical material science queries using both real and synthetic data. ...|$|E
40|$|Database systems {{dealing with}} textual {{contents}} {{have been in}} use for a long time. A database management system (DBMS) allows convenient and efficient storage and retrieval of {{a huge amount of}} data. Traditional databases are designed for handling alphanumeric data efficiently, but fail to manage complex data like audio and/or video. One dimensional audio data and two dimensional image data can be stored {{in the form of a}} <b>binary</b> <b>large</b> <b>object</b> (<b>BLOB)</b> with no emphasis on the contents. Textual information can be attached to BLOBs for retrieval, but mere a textual information is insufficient for describing the rich contents of data. So there is a need to extend the capabilities of such information management system to handle both audio and visual data. Contents of such data items can be extracted in the form of features which can be used for distinction amongst the instances of these data types. This paper describes how the relational data model can be extended to retrieve face images and audio data in the form of utterances of alphabets. Face images are characterized by sizes of different objects, e. g. nose, lips and the inter-object distances. The audio data is characterized by pitch, formants and LPC coefficients. The purpose of the paper is to develop an automated system for human identification based on audio-visual querying. The system allows the query to be partly audio, partly visual and textual...|$|E
40|$|This paper {{presents}} the implementation {{and evaluation of}} a computer vision task on a Field Programmable Gate Array (FPGA). As an experimental approach for an application-specific image-processing problem, it provides results about gained performance and precision compared with similar solutions on General Purpose Processor (GPP) architectures. The problem of detecting <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOBs)</b> in a continuous video stream and computation of their center points has been addressed. Most existing solutions are realized on GPP platforms, where resolution of image material and sequential processing define the performance barrier. FPGA based approaches perform implemented algorithms as fast as hardware circuits and in addition offer parallelization abilities. The evaluation compares precision and performance gain against similar approaches on GPP platforms. The paper discusses different concepts for BLOB detection and shows the implementation of one common method for BLOB detection, including design problems and performance evaluation...|$|R
40|$|A virtual {{instrumentation}} (VI) {{system called}} VI localized corrosion image analyzer (LCIA) based on LabVIEW 2010 was developed allowing rapid automatic and subjective error-free {{determination of the}} pits number on large sized corroded specimens. The VI LCIA controls synchronously the digital microscope image taking and its analysis, finally resulting in a map file containing the coordinates of the detected probable pits containing zones on the investigated specimen. The pits area, traverse length, and density are also determined by the VI using <b>binary</b> <b>large</b> <b>objects</b> (<b>blobs)</b> analysis. The resulting map file can be used further by a scanning vibrating electrode technique (SVET) system for rapid (one pass) “true/false” SVET check of the probable zones only passing through the pit’s centers avoiding thus the entire specimen scan. A complete SVET scan over the already proved “true” zones could determine the corrosion rate {{in any of the}} zones...|$|R
40|$|Most of the {{database}} management systems deal with <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOBs).</b> Progressively however, multimedia data types {{are introduced to}} deal explicitly with still pictures, cartographic images and more recently video and audio. Nevertheless, limiting oneself to introducing these new data types is not a sufficient condition for an easy modelling of multimedia schemas integrating the multimedia information. In this paper, we propose a library of abstract classes to model these types of multimedia objects in a coherent, reusable and extensible framework, the structure of {{which is based on}} a clear taxonomy of media and their subsequent combination to obtain complex multimedia objects. An originality of this work is that time {{is considered one of the}} base media, in the same way than pictures or video! The aim was to introduce this hierarchy into an objectoriented database management system in order to ease the design and development of multimedia applications. Another important con [...] ...|$|R
40|$|Poster for Canadian Society of Microbiologists Annual Conference 2017. Amplytica: Bringing {{microbial}} ecology to the cloud. Abstract The Amplytica Cloud Platform (ACP) is a software system for building large scale bioinformatics applications on commercial cloud computing infrastructure {{with a focus}} on {{microbial ecology}} workloads. It makes use of emerging open source cloud technologies such as Docker, Salt-Cloud, RabbitMQ, <b>Binary</b> <b>Large</b> <b>Object</b> (<b>BLOB)</b> stores such as Amazon S 3 and cloud databases such as Heroku PostgreSQL. The platform is designed to be distributed across many servers which {{do not need to be}} in a cluster or even on the same cloud provider. Components can even be hosted on-site to utilize existing hardware or for data security purposes. They are also stateless and no sequence data is stored inside them allowing for failure at any time with minimal data loss. Since the system is distributed, components can be turned on and off on demand allowing end users to pay for only individual bioinformatics processing jobs rather than for longterm servers. In a microbial ecology context, ACP processing components wrap the QIIME microbial ecology pipeline allowing it to cluster OTUs in a closed, or in the future, open-reference guided fashion on high-RAM cloud virtual machines. The platform is targeted towards high throughput applications such as bioreactor monitoring and optimization, personalized medicine and sequencing centres with the ability to process, store and organized hundreds of samples per month. The platform also has facilities for sequence quality filtering and quality control, compressed storage of sequence data, project and sample management and metadata capture and integration. Unlike other competing platforms, ACP can be hosted by Canadian cloud providers or private cloud and will support for multiple sequencer vendors in the near future...|$|E
40|$|This paper {{presents}} a multimedia DBMS with powerfull capabilities for video data management. The video model {{is part of}} the STORM model which allows to store and to query multimedia presentations in an object oriented database framework. We propose a library of class which provides user with reusable components to create multimedia applications including video objects. We also propose some extensions to the standardized query language OQL in order {{to take into account the}} video structure and the temporal aspects of such data. These extensions allow to take into account temporal and structural aspects of video objects. Keywords: Object-Oriented DBMS, Multimedia, Video, Query languages, Indexation, GEP. 1 Introduction Database systems allow to manage large volume of data and most of commercial products either relationnal (e. g. Oracle) or object (e. g. ObjectStore) allow to store multimedia data as <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOB)</b> which are unstructured data. However, they do not capture the c [...] ...|$|R
5000|$|Supported datatypes: 8, 16, 32, and 64 bit signed or {{unsigned}} integers, float, double, decimal (BCD), {{fixed or}} variable-length character or wide character, <b>binary</b> or character <b>large</b> <b>objects</b> (<b>blobs),</b> date, time, timestamp, guid/uuid, and db_addr (database address—aka, rowid).|$|R
40|$|Data Types ADTs Type constructors for row {{types and}} {{reference}} types Type constructors for collection types (sets, lists and multi sets) Support for <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOBs)</b> and Character <b>Large</b> <b>Objects</b> (CLOBs) User defined functions and procedures 2. 2. 1 User-Defined Types, ADTs The application programmer is {{now able to}} make definitions of Abstract Data Types (ADTs) that encapsulate attributes and operations in a single entity. Operations are implemented as procedures, which are called routines in SQL 3. In addition there is support for inheritance, in fact we have multiple inheritance. 2. 2. 2 Row Types and Reference Types A row type is a sequence of field name/data type pairs that are equal to a table definition. We say that two row types are equal if: CENTRE FOR OBJECT TECHNOLOGY COT/ 4 - 02 -V 1. 1 Page 33 o f 120 Both rows have {{the same number of}} fields Every pair of fields at the same position have compatible types A name row type is simply a row type with a name assi [...] ...|$|R
40|$|Originally <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOBs)</b> in {{databases}} {{were conceived}} {{as a means to}} cap-ture any large data (whatever large meant at the time of writing) which, for whatever reason, cannot or should not be modeled relationally. Today we find images, movies, XML, format-ted documents, and many more data types stored in database BLOBs. A particular chal-lenge obviously is moving such large units of data as fast as possible, hence performance benchmarks are of interest. However, while extensive evaluations have been undertaken for a variety of SQL work-loads, BLOBs have not been the target of thorough benchmarking up to now. TPC and SPC- 2 standards do not address BLOB benchmarking either. We present a comparative BLOB benchmark of the leading commercial and open-source systems available under Unix/Linux. Commercial DBMSs are anonymised, open-source DBMSs benchmarked are PostgreSQL and MySQL. Measurements show large differences between the systems under test, depending on various parameters. A surprising result is that overall the open-source DBMSs in most situations outperform commercial systems if configured wisely. ACM Categories and Subject Descriptor...|$|R
40|$|Copyright © 2013 Rogelio Ramos et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A virtual instrumentation (VI) system called VI localized corrosion image analyzer (LCIA) based on LabVIEW 2010 was developed allowing rapid automatic and subjective error-free determination of the pits number on large sized corroded specimens. The VI LCIA controls synchronously the digital microscope image taking and its analysis, finally resulting in a map file containing the coordinates of the detected probable pits containing zones on the investigated specimen. The pits area, traverse length, and density are also determined by the VI using <b>binary</b> <b>large</b> <b>objects</b> (<b>blobs)</b> analysis. The resulting map file can be used further by a scanning vibrating electrode technique (SVET) system for rapid (one pass) “true/false ” SVET check of the probable zones only passing through the pit’s centers avoiding thus the entire specimen scan. A complete SVET scan over the already proved “true ” zones could determine the corrosion rate {{in any of the}} zones. 1...|$|R
40|$|Many {{current and}} {{potential}} applications of database technology, e. g., geographical, medical, spatial, and multimedia applications, require efficient {{support for the}} management of data with new, complex data types. As a result, the major DBMS vendors are stepping beyond the support for uninterpreted <b>binary</b> <b>large</b> <b>objects,</b> termed <b>BLOBs,</b> and are beginning to offer extensibility features that allow external developers to extend the DBMS with, e. g., their own data types and accompanying access methods. Existing solutions include DB 2 extenders, Informix DataBlades, and Oracle cartridges. Extensible systems offer new and exciting opportunities for researchers and third-party developers alike. This pape...|$|R
40|$|To improve {{data quality}} and save cost, {{clinical}} trials are nowadays performed using electronic data capture systems (EDCS) providing electronic case report forms (eCRF) instead of paper-based CRFs. However, such EDCS are insufficiently {{integrated into the}} medical workflow and lack in interfacing with other study-related systems. In addition, most EDCS are unable to handle image and biosignal data, although electrocardiography (EGC, as example for one-dimensional (1 D) data), ultrasound (2 D data), or magnetic resonance imaging (3 D data) have been established as surrogate endpoints in clinical trials. In this paper, an integrated workflow based on OpenClinica, one of the world’s largest EDCS, is presented. Our approach consists of three components for (i) sharing of study metadata, (ii) integration of large volume data into eCRFs, and (iii) automatic image and biosignal analysis. In all components, metadata is transferred between systems using web services and JavaScript, and <b>binary</b> <b>large</b> <b>objects</b> (<b>BLOBs)</b> are sent via the secure file transfer protocol and hypertext transfer protocol. We applied the close-looped workflow in a multicenter study, where long term (7 days/ 24 h) Holter ECG monitoring is acquired on subjects with diabetes. Study metadata is automatically transferred into OpenClinica, the 4 GB BLOBs are seamlessly integrated into the eCRF, automatically processed, {{and the results of}} signal analysis are written back into the eCRF immediately...|$|R
40|$|Abstract: Problem statement: OGSA-DAI {{middleware}} allows data {{resources to}} be federated and accessed via web services {{on the web}} or within grids or clouds. It provides a client API for writing programs that access the exposed databases. Migrating existing applications to the new technology and using a new API to access the data of DBMS with BLOB is difficult and discouraging. A JDBC Driver is a much convenient alternative to existing mechanism and provides an extension to OGSA-DAI middleware and allows applications to use databases exposed in a grid through the OGSA-DAI 3. 0. However, the driver does not support <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOB).</b> Approach: The driver is enhanced to support BLOB using the OGSA-DAI Client API. It transforms the JDBC calls into an OGSA-DAI workflow request and sends it to the server using Web Services (WS). The client API of OGSA-DAI uses activities that are connected to form a workflow and executed using a pipeline. This workflow mechanism is embedded into the driver. The WS container dispatches the request to the OGSA-DAI middleware for processing {{and the result is}} then transformed back to an instance of ResultSet implementation using the OGSA-DAI Client API, before it is returned to the user. Results: Test on handling of BLOBs (images, flash files and videos) ranging from size 1 KB to size 2 GB were carried out on Oracle, MySQL and PostgreSQL databases using our enhanced JDBC driver and i...|$|R
40|$|Problem statement: OGSA-DAI {{middleware}} allows data {{resources to}} be federated and&# 13; accessed via web services {{on the web}} or within grids or clouds. It provides a client API for writing&# 13; programs that access the exposed databases. Migrating existing applications to the new technology and&# 13; using a new API to access the data of DBMS with BLOB is difficult and discouraging. A JDBC Driver&# 13; is a much convenient alternative to existing mechanism and provides an extension to OGSA-DAI&# 13; middleware and allows applications to use databases exposed in a grid through the OGSA-DAI 3. 0. &# 13; However, the driver does not support <b>Binary</b> <b>Large</b> <b>Objects</b> (<b>BLOB).</b> Approach: The driver is&# 13; enhanced to support BLOB using the OGSA-DAI Client API. It transforms the JDBC calls into an&# 13; OGSA-DAI workflow request and sends it to the server using Web Services (WS). The client API of&# 13; OGSA-DAI uses activities that are connected to form a workflow and executed using a pipeline. This&# 13; workflow mechanism is embedded into the driver. The WS container dispatches the request to the&# 13; OGSA-DAI middleware for processing {{and the result is}} then transformed back to an instance of&# 13; ResultSet implementation using the OGSA-DAI Client API, before it is returned to the user. Results:&# 13; Test on handling of BLOBs (images, flash files and videos) ranging from size 1 KB to size 2 GB were&# 13; carried out on Oracle, MySQL and PostgreSQL databases using our enhanced JDBC driver and it&# 13; performed well. Conclusion: The enhanced JDBC driver now can offer users, with no experience in&# 13; Grid computing specifically on OGSA-DAI, the possibility to give their applications the ability to&# 13; access databases exposed on the grid with minimal effort. </div...|$|R
40|$|This report {{presents}} the implementation {{and evaluation of}} a computer vision task on a Field Programmable Gate Array (FPGA). As an experimental approach for an application-specific image-processing problem it provides reliable results to measure gained performance and precision compared with similar solutions on General Purpose Processor (GPP) architectures. The project addresses the problem of detecting <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOBs)</b> in a continuous video stream. For this problem {{a number of different}} solutions exist. But most of these are realized on GPP platforms, where resolution and processing speed define the performance barrier. With the opportunity of parallelization and performance abilities like in hardware, the application of FPGAs become interesting. This work belongs to the MI 6 project from the Computer Vision research group of the University of Applied Sciences Bonn-Rhein-Sieg. It address the detection of the users position and orientation in relation to the virtual environment in an Immersion Square. The goal is to develop a light emitting device, that points from the user towards the point of interest on the projection screen. The projected light dots are used to represent the user in the virtual environment. By detecting the light dots with video cameras, the idea is to interface the position and orientation of the relative position of the user interface. Fort that the laser dots need to be arranged in a unique pattern, which requires at least five points. [29] For a reliable estimation a robust computation of the BLOB's center-points is necessary. This project has covered the development of a BLOB detection system on a FPGA platform. It detects binary spatially extended objects in a continuous video stream and computes their center points. The results are displayed to the user and where validated for their ground truth. The evaluation compares precision and performance gain against similar approaches on GPP platforms...|$|R
40|$|In {{order to}} better support current and new applications, the major DBMS vendors are {{stepping}} beyond uninterpreted <b>binary</b> <b>large</b> <b>objects,</b> termed <b>BLOBs,</b> and are beginning to offer extensibility features that allow external developers to extend the DBMS with, e. g., their own data types and accompanying access methods. Existing solutions include DB 2 extenders, Informix DataBlades, and Oracle cartridges. Extensible systems offer new and exciting opportunities for researchers and third-party developers alike. This paper reports on an implementation of an Informix DataBlade for the GR-tree, a new R-tree based index. This effort represents a stress test of the perhaps currently most extensible DBMS, in that the new DataBlade aims to achieve better performance, not just to add functionality. The paper provides guidelines for how to create an access method DataBlade, describes the sometimes surprising challenges that must be negotiated during DataBlade development, and evaluates the extensibility of the Informix Dynamic Server...|$|R
40|$|This report {{presents}} the implementation {{and evaluation of}} a computer vision problem on a Field Programmable Gate Array (FPGA). This work is based upon [5] where the feasibility of application specific image processing algorithms on a FPGA platform have been evaluated by experimental approaches. The results and conclusions of that previous work builds {{the starting point for}} the work, described in this report. The project results show considerable improvement of previous implementations in processing performance and precision. Different algorithms for detecting <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOBs)</b> more precisely have been implemented. In addition, the set of input devices for acquiring image data has been extended by a Charge-Coupled Device (CCD) camera. The main goal of the designed system is to detect BLOBs in continuous video image material and compute their center points. This work belongs to the MI 6 project from the Computer Vision research group of the University of Applied Sciences Bonn-Rhein-Sieg 1. The intent is the invention of a passive tracking device for an immersive environment to improve user interaction and system usability. Therefore the detection of the users position and orientation in relation to the projection surface is required. For a reliable estimation a robust and fast computation of the BLOB's center-points is necessary. This project has covered the development of a BLOB detection system on an Altera DE 2 Development and Education Board with a Cyclone II FPGA. It detects binary spatially extended objects in image material and computes their center points. Two different sources have been applied to provide image material for the processing. First, an analog composite video input, which can be attached to any compatible video device. Second, a five megapixel CCD camera, which is attached to the DE 2 board. The results are transmitted on the serial interface of the DE 2 board to a PC for validation of their ground truth and further processing. The evaluation compares precision and performance gain dependent on the applied computation methods and the input device, which is providing the image material...|$|R
50|$|Binary <b>objects</b> (BO or <b>binary</b> <b>large</b> <b>object</b> = <b>BLOB)</b> are digital {{files from}} photos, graphics, films, sounds or any binary data in {{specific}} formats. There are {{many types of}} file converter {{and some of them}} are jpeg and gif as many people know it for. They have three steps is binary object that they work and they are: upload of binary object, metadata definition or import and data import data.Upload of binary objects can be standard formats. Image holds their data that they have been saved and they are usually separated of how the user uploads the image.Metadata definition or import locates the Binary object related to binary object as they have been organized. If they find a data that they are suitable in they will be ready for next process.Data import data sets title, principal investigator, method, and comment and references is needed in the process. Then the binary object can be added as pdf or txt file.|$|R
40|$|International audienceTo {{accommodate}} {{the needs of}} large-scale distributed P 2 P systems, scalable data management strategies are required, allowing appli cations to efficiently cope with continuously growing, highly dis tributed data. This paper addresses the problem of efficiently stor ing and accessing very <b>large</b> <b>binary</b> data <b>objects</b> (<b>blobs).</b> It proposesan efficient versioning scheme allowing {{a large number of}} clients to concurrently read, write and append data to huge blobs that are fragmented and distributed at a very large scale. Scalability under heavy concurrency is achieved thanks to an original metadata scheme, based on a distributed segment tree built on top of a Distributed Hash Table (DHT). Our approach has been implemented and experimented within our BlobSeer prototype on the Grid' 5000 testbed, using up to 175 nodes...|$|R
30|$|Large file {{dissemination}} is {{a prospective}} trend in vehicular networks. For example, in [14], the authors study querying multimedia data such as video and voice clips in hybrid vehicular networks which consist of vehicles that {{are capable of}} both infrastructure-less short-range communication and infrastructure communication. Also, in [15], the authors study querying <b>binary</b> <b>large</b> <b>objects</b> (<b>blobs)</b> such as video and voice clips in a network of vehicles communicating wirelessly. They focus on the efficient query of the content while none of them considered the data dissemination problem. Ref. [16] uses dynamic network topology graph to model the content downloading problem in vehicular networks. However, it needs the preemptive knowledge of vehicular trajectories and perfect scheduling of data transmissions, which is impractical in highly dynamical VANET since DNTG will become very complicated. Ref. [17, 18] prove that vehicle mobility follows certain patterns and can be used to predict the encountering of vehicles. One way to support large data chunk is to split, such as SADF [19], it is an automatic data packet dividing algorithm. To improve the delivery ratio, it cuts the large file into small segments according to the network quality and duration of contacts. In [20], large files are divided into data chunks. However, the relay-to-relay transmission is not considered. The carrier vehicle can only deliver the data chunk to the downloader directly. Another way is to add more storage, METhoD [21] implements a platform for distributing multimedia contents in delay-tolerant networks. It does not give solution on how to prevent memory overflow but adding a lot of external storage to help the big data application. Abdelmoumen et al. [22] analyze the adverse effect brought by the insufficiency of nodes’ storage. By adding some fixed nodes with large storage space, the problem can be solve to some extent. However, in many cases, the data file is not allowed to be portioned and additional infrastructure is costly. Again, improving the data exchange efficiency would be another option. Zhao et al. [23] turn the problem of global optimizing of forwarding utility into the local optimizing of forwarding utility upon nodes’ encounter. The proposed cooperative forwarding is modeled as a 0 − 1 knapsack problem and solved by a greedy algorithm. In [24], the authors propose a dynamic segmented network coding scheme to efficiently exploit the transmission opportunity that is scarce in DTNs. In particular, they adopt a dynamic segment size control mechanism, which makes the segmentation adapt to the dynamics of the network.|$|R
40|$|This thesis work {{presents}} the implementation and validation of image processing problems in hardware {{to estimate the}} performance and precision gain. It compares the implementation for the addressed problem on a Field Programmable Gate Array (FPGA) with a software implementation for a General Purpose Processor (GPP) architecture. For both solutions the implementation costs for their development is an important aspect in the validation. The analysis of the flexibility and extendability {{that can be achieved}} by a modular implementation for the FPGA design was another major aspect. This work is based upon approaches from previous work, which included the detection of <b>Binary</b> <b>Large</b> <b>OBjects</b> (<b>BLOBs)</b> in static images and continuous video streams [13, 15]. One addressed problem of this work is the tracking of the detected BLOBs in continuous image material. This has been implemented for the FPGA platform and the GPP architecture. Both approaches have been compared with respect to performance and precision. This research project is motivated by the MI 6 project of the Computer Vision research group, which is located at the Bonn-Rhein-Sieg University of Applied Sciences. The intent of the MI 6 project is the tracking of a user in an immersive environment. The proposed solution is to attach a light emitting device to the user for tracking the created light dots on the projection surface of the immersive environment. Having the center points of those light dots would allow the estimation of the user’s position and orientation. One major issue that makes Computer Vision problems computationally expensive is the high amount of data that has to be processed in real-time. Therefore, one major target for the implementation was to get a processing speed of more than 30 frames per second. This would allow the system to realize feedback to the user in a response time which is faster than the human visual perception. One problem that comes with the idea of using a light emitting device to represent the user, is the precision error. Dependent on the resolution of the tracked projection surface of the immersive environment, a pixel might have a size in cm 2. Having a precision error of only a few pixels, might lead to an offset in the estimated user’s position of several cm. In this research work the development and validation of a detection and tracking system for BLOBs on a Cyclone II FPGA from Altera has been realized. The system supports different input devices for the image acquisition and can perform detection and tracking for five to eight BLOBs. A further extension of the design has been evaluated and is possible with some constraints. Additional modules for compressing the image data based on run-length encoding and sub-pixel precision for the computed BLOB center-points have been designed. For the comparison of the FPGA approach for BLOB tracking a similar implementation in software using a multi-threaded approach has been realized. The system can transmit the detection or tracking results on two available communication interfaces, USB and RS 232. The analysis of the hardware solution showed a similar precision for the BLOB detection and tracking as the software approach. One problem is the strong increase of the allocated resources when extending the system to process more BLOBs. With one of the applied target platforms, the DE 2 - 70 board from Altera, the BLOB detection could be extended to process up to thirty BLOBs. The implementation of the tracking approach in hardware required much more effort than the software solution. The design of high level problems in hardware for this case are more expensive than the software implementation. The search and match steps in the tracking approach could be realized more efficiently and reliably in software. The additional pre-processing modules for sub-pixel precision and run-length-encoding helped to increase the system’s performance and precision...|$|R
5000|$|... (or [...] ): <b>binary</b> <b>large</b> <b>object</b> with {{a maximum}} length n K | M | G | T [...]|$|R
