2816|1551|Public
25|$|For small molecules, several <b>benchmark</b> <b>data</b> {{sets for}} docking and virtual {{screening}} exist e.g. Astex Diverse Set consisting {{of high quality}} protein−ligand X-ray crystal structures or the Directory of Useful Decoys (DUD) for evaluation of virtual screening performance.|$|E
25|$|First {{steps to}} {{bringing}} together various approaches—learning, lexical, knowledge-based, etc.—were {{taken in the}} 2004 AAAI Spring Symposium where linguists, computer scientists, and other interested researchers first aligned interests and proposed shared tasks and <b>benchmark</b> <b>data</b> sets for the systematic computational research on affect, appeal, subjectivity, and sentiment in text.|$|E
5000|$|... 5. {{collecting}} <b>benchmark</b> <b>data</b> on neighborhood social processes." ...|$|E
40|$|Background: New, silicon-based multielectrodes {{comprising}} {{hundreds or}} more electrode contacts offer {{the possibility to}} record spike trains from thousands of neurons simultaneously. This potential cannot be realized unless accurate, reliable automated methods for spike sorting are developed, in turn requiring <b>benchmarking</b> <b>data</b> sets with known ground-truth spike times. New method: We here present a general simulation tool for computing <b>benchmarking</b> <b>data</b> for evaluation of spike-sorting algorithms entitled ViSAPy (Virtual Spiking Activity in Python). The tool {{is based on a}} well-established biophysical forward-modeling scheme and is implemented as a Python package built on top of the neuronal simulator NEURON and the Python tool LFPy. Results: ViSAPy allows for arbitrary combinations of multicompartmental neuron models and geometries of recording multielectrodes. Three example <b>benchmarking</b> <b>data</b> sets are generated, i. e., tetrode and polytrode data mimicking in vivo cortical recordings and microelectrode array (MEA) recordings of in vitro activity in salamander retinas. The synthesized example <b>benchmarking</b> <b>data</b> mimics salient features of typical experimental recordings, for example, spike waveforms depending on interspike interval. Comparison with existing methods: ViSAPy goes beyond existing methods as it includes biologically realistic model noise, synaptic activation by recurrent spiking networks, finite-sized electrode contacts, and allows for inhomogeneous electrical conductivities. ViSAPy is optimized to allow for generation of long time series of <b>benchmarking</b> <b>data,</b> spanning minutes of biological time, by parallel execution on multi-core computers. Conclusion: ViSAPy is an open-ended tool as it can be generalized to produce <b>benchmarking</b> <b>data</b> or arbitrary recording-electrode geometries and with various levels of complexity...|$|R
50|$|Various {{consulting}} and public accounting firms perform research on audit committees, to provide <b>benchmarking</b> <b>data.</b>|$|R
5000|$|IDS HR in Practice {{includes}} <b>benchmarking</b> <b>data</b> on a {{wide range}} of employee benefits and allowances, including: ...|$|R
5000|$|Anomaly {{detection}} <b>benchmark</b> <b>data</b> {{repository of}} the Ludwig-Maximilians-Universität München; Mirror at University of São Paulo.|$|E
50|$|Benchmarks are {{designed}} to mimic {{a particular type of}} workload on a component or system. The computer programs used for compiling some of the <b>benchmark</b> <b>data</b> in this section may not have been fully optimized, and the relevance of the data is disputed. The most accurate benchmarks are those that are customized to your particular situation. Other people's <b>benchmark</b> <b>data</b> may have some value to others, but proper interpretation brings many challenges. The Computer Language Benchmarks Game site warns against over-generalizing from <b>benchmark</b> <b>data,</b> but contains a large number of micro-benchmarks of reader-contributed code snippets, with an interface that generates various charts and tables comparing specific programming languages and types of tests.|$|E
5000|$|... #Caption: Figure 2. Seismic <b>benchmark</b> <b>data</b> model. Green graph is undamped and red graph is damped impulse {{response}} Q=50 ...|$|E
50|$|TIPA for ITIL {{does not}} allow making an {{assessment}} of organisational maturity. Moreover, no <b>benchmarking</b> <b>data</b> is available so far.|$|R
5000|$|Various {{consulting}} and public accounting firms perform research on audit committees, to provide <b>benchmarking</b> <b>data.</b> Some results are identified below: ...|$|R
50|$|Drawn {{from the}} CGS, the annual Giving in Numbers report {{provides}} thorough analysis and comprehensive <b>benchmarking</b> <b>data</b> for corporate philanthropy professionals seeking {{to assess the}} scope of their contributions initiatives.|$|R
5000|$|SMEAR is a <b>benchmark</b> <b>data</b> stream {{with a lot}} {{of missing}} values. Environment {{observation}} data over 7 years. Predict cloudiness. Access ...|$|E
50|$|Revisit {{areas and}} sites first {{explored}} by traverses in the 1960s, for detection of possible changes {{and to establish}} <b>benchmark</b> <b>data</b> sets for future research efforts.|$|E
50|$|As {{mentioned}} above, at {{the same}} time we start discussing the design and application of an inverse Q-filter, we need to specify a mathematical Q-model that can compute <b>benchmark</b> <b>data</b> similar to Wang’s <b>benchmark</b> <b>data</b> that can be used for inversion later. Then we must regard the forward Q-filtering process as our solution of (1.2) which means we must compute the inversion of (1.2) as a Q-forward filtering process. Wang showed how easily this could be done by simply changing the sign before γ and Q.|$|E
5000|$|Benchmarking is {{not easy}} and often {{involves}} several iterative rounds in order to arrive at predictable, useful conclusions. Interpretation of <b>benchmarking</b> <b>data</b> is also extraordinarily difficult. Here is a partial list of common challenges: ...|$|R
50|$|GPOs {{submit that}} their {{services}} allow for improved operating margins for healthcare providers, and that members enjoy value added benefits like clinical support, <b>benchmarking</b> <b>data,</b> supply chain support and comprehensive portfolios {{of products and}} services to address specific needs.|$|R
40|$|Finance is a {{very broad}} field where the {{uncertainty}} plays a central role and every financial operator {{have to deal with}} it. In this paper we propose a new method for a trend prediction on financial time series combining a Linear Piecewise Regression with a granular computing framework. A set of parameters control the behavior of the whole system, thus making their fine tuning a critical optimization task. To this aim in this paper we employ an evolutionary optimization algorithm to tackle this crucial phase. We tested our system on both synthetic <b>benchmarking</b> <b>data</b> and on real financial time series. Our tests show very good classification results on <b>benchmarking</b> <b>data.</b> Results on real data, although not completely satisfactory, are encouraging, suggesting further developments...|$|R
50|$|For small molecules, several <b>benchmark</b> <b>data</b> {{sets for}} docking and virtual {{screening}} exist e.g. Astex Diverse Set consisting {{of high quality}} protein−ligand X-ray crystal structures or the Directory of Useful Decoys (DUD) for evaluation of virtual screening performance.|$|E
50|$|Globally buy-side asset {{managers}} {{are looking at}} increased regulation in regards to their use of financial index and <b>benchmark</b> <b>data,</b> including adhering to IOSCO Principles. This {{is expected to increase}} cost and time for the buy-side in managing their data.|$|E
50|$|Conveniently, the perturbational {{approach}} may {{be extended to}} new asset types without requiring any new pricing code or types of data, and it also works for benchmark sectors {{as well as individual}} securities, which is useful if <b>benchmark</b> <b>data</b> is only available at sector level.|$|E
30|$|In this subsection, we {{describe}} the data sets that we use for our <b>benchmarking.</b> We use <b>data</b> sets {{from a variety of}} different mathematical and scientific areas and applications. In each case, when possible, we use data sets that have already been studied using PH. Our list of data sets is far from complete; we view this list as an initial step towards building a comprehensive collection of <b>benchmarking</b> <b>data</b> sets for PH.|$|R
40|$|Abstract In {{this paper}} we {{describe}} PAVER 2. 0, an environment (i. e. a process and {{a suite of}} tools supporting that process) for the automated performance anal-ysis of <b>benchmarking</b> <b>data.</b> This new environment improves on its predecessor by addressing some of the shortcomings of the original PAVER [6] and extending its capabilities. The changes serve to further the original goals of PAVER (automa-tion of the visualization and summarization of <b>benchmarking</b> <b>data)</b> while making the environment more accessible {{for the use of}} and modification by the entire community of potential users. In particular, we have targeted the end-users of optimization software, as they are best able to make the many subjective choices necessary to produce impactful results when benchmarking optimization software. We illustrate with some sample analyses conducted via PAVER 2. 0...|$|R
40|$|Abstract Objective Quality {{improvement}} {{initiatives in}} emergency medicine (EM) often {{suffer from a}} lack of <b>benchmarking</b> <b>data</b> on the quality of care. The objectives of this study were twofold: 1. To assess the feasibility of collecting <b>benchmarking</b> <b>data</b> from different Swedish emergency departments (EDs) and 2. To evaluate patient throughput times and inflow patterns. Method We compared patient inflow patterns, total lengths of patient stay (LOS) and times to first physician at six Swedish university hospital EDs in 2009. Study data were retrieved from the hospitals' computerized information systems during single on-site visits to each participating hospital. Results All EDs provided throughput times and patient presentation data without significant problems. In all EDs, Monday was the busiest day and the fewest patients presented on Saturday. All EDs had a large increase in patient inflow before noon with a slow decline over the rest of the 24 h, and this peak and decline was especially pronounced in elderly patients. The average LOS was 4 h of which 2 h was spent waiting for the first physician. These throughput times showed a considerable diurnal variation in all EDs, with the longest times occurring 6 - 7 am and in the late afternoon. Conclusion These results demonstrate the feasibility of collecting <b>benchmarking</b> <b>data</b> on quality of care targets within Swedish EM, and form the basis for ANSWER, A National SWedish Emergency Registry. </p...|$|R
5000|$|Some Sort Benchmark {{entrants}} use {{a variation}} on radix sort for {{the first phase of}} sorting: they separate data into one of many [...] "bins" [...] based on the beginning of its value. Sort <b>Benchmark</b> <b>data</b> is random and especially well-suited to this optimization.|$|E
5000|$|... {{for use as}} {{the primary}} input to methods for {{estimating}} project effort. The ability to measure a software size from its requirements is especially valuable early {{in the life of}} a software project. <b>Benchmark</b> <b>data</b> to support COSMIC-based project estimating can be obtained from www.isbsg.org http://www.isbsg.org ...|$|E
50|$|First {{steps to}} {{bringing}} together various approaches—learning, lexical, knowledge-based, etc.—were {{taken in the}} 2004 AAAI Spring Symposium where linguists, computer scientists, and other interested researchers first aligned interests and proposed shared tasks and <b>benchmark</b> <b>data</b> sets for the systematic computational research on affect, appeal, subjectivity, and sentiment in text.|$|E
40|$|The {{proposed}} paper 1 {{will challenge}} {{the notion that}} speed of query execution is the appropriate critical metric in benchmarking decision support systems and will propose a new method for <b>benchmarking</b> <b>data</b> warehouses based on a novel notion of fitness, defined as an empirical approximation {{of the extent to}} which the data in a warehous...|$|R
40|$|INTRODUCTION: The {{hospital}} {{billing system}} {{is usually the}} source for reporting activity counts used in benchmarking efforts. Because billing {{is associated with a}} specific procedure, <b>benchmarking</b> <b>data</b> are often reported as procedure-days, procedure-shifts, or procedure-hours. Normalizing (usually to procedure-days) is required when comparing <b>data</b> for <b>benchmarking</b> purposes. For an institution that uses hourly billing, simply dividing procedure-hours by 24 (or procedure-shifts b...|$|R
50|$|The National Business Research Institute, {{abbreviated}} NBRI, is {{a global}} survey research and consulting organization founded in 1982 and headquartered in Plano, Texas, United States. The firm is known for scientific psychological research, <b>benchmarking</b> <b>data,</b> and root cause analyses, specifically {{in the area of}} organizational assessment. The company conducts customer and employee surveys, market research, and political research related to organizational behavior.|$|R
50|$|While the {{geometric}} intuition of LOF {{is only applicable}} to low-dimensional vector spaces, the algorithm can be applied in any context a dissimilarity function can be defined. It has experimentally been shown to work very well in numerous setups, often outperforming the competitors, for example in network intrusion detection and on processed classification <b>benchmark</b> <b>data.</b>|$|E
50|$|In 2008, Lipman Hearne {{worked with}} the American Marketing Association to {{complete}} The State of Nonprofit Marketing: A Report on Priorities, Spending, Measurement and the Challenges Ahead. In this survey, Lipman Hearne and the AMA set out to provide the nonprofit industry with <b>benchmark</b> <b>data</b> on challenges, priorities, budgets, staffing, strategies, and spending to guide decision-making about marketing at nonprofit organizations.|$|E
50|$|APX Group (APX) is {{an energy}} {{exchange}} operating the spot markets for {{electricity in the}} Netherlands, the United Kingdom, and Belgium. Established in 1999, APX provides exchange trading, central clearing and settlement, and data distribution services as well as <b>benchmark</b> <b>data</b> and industry indices. APX has over 180 members from more than 15 countries. In 2014, a total volume of 92 TWh of energy was traded or cleared by APX.|$|E
5000|$|Modular {{function}} deployment uses QFD {{to establish}} customer requirements {{and to identify}} important design requirements with a special emphasis on modularity. There are three main differences to QFD as applied in modular function deployment compared to house of quality: The <b>benchmarking</b> <b>data</b> is mostly gone; the checkboxes and crosses have been replaced with circles, and the triangular [...] "roof" [...] is missing.|$|R
50|$|The Collective Optimization Database {{is an open}} {{repository}} {{to enable}} sharing of <b>benchmarks,</b> <b>data</b> sets and optimization cases from the community, provide web services and plugins to analyze optimization data and predict program transformations or better hardware designs for multi-objective optimizations based on statistical and machine learning techniques provided there is enough information collected in the repository from multiple users.|$|R
40|$|Motivation: Computational {{methods are}} {{essential}} to extract actionable information from raw sequencing data, and to thus fulfill the promise of next-generation sequencing technology. Unfortunately, computational tools developed to call variants from human sequencing data disagree on many of their predictions, and current methods to evaluate accuracy and computational performance are ad-hoc and incomplete. Agreement on benchmarking variant calling methods would stimulate development of genomic processing tools and facilitate communication among researchers. Results: We propose SMaSH, a benchmarking methodology for evaluating human genome variant calling algorithms. We generate synthetic datasets, organize and interpret {{a wide range of}} existing <b>benchmarking</b> <b>data</b> for real genomes, and propose a set of accuracy and computational performance metrics for evaluating variant calling methods on this <b>benchmarking</b> <b>data.</b> Moreover, we illustrate the utility of SMaSH to evaluate the performance of some leading single nucleotide polymorphism (SNP), indel, and structural variant calling algorithms. Availability: We provide free and open access online to the SMaSH toolkit, along with detailed documentation, at smash. cs. berkeley. edu...|$|R
