18|15|Public
50|$|In telecommunications, <b>bit</b> <b>inversion</b> {{means the}} {{changing}} of the state of a bit to the opposite state, i.e. {{the changing of}} a 0 bit to 1 or of a 1 bit to 0. It also refers to the changing of a state representing a given bit to the opposite state.|$|E
50|$|There {{exists a}} class of {{universal}} computers with one instruction based on bit manipulation such as bit copying or <b>bit</b> <b>inversion.</b> Since their memory model {{is the same as}} the memory structure used in real computers, those bit manipulation machines are equivalent to real computers rather than to Turing machines.|$|E
50|$|G.711 {{is unclear}} {{about how to}} code the values at the limit of a range (e.g. whether +31 codes to 0xEF or 0xF0).However, G.191 {{provides}} example code in the C language for a μ-law encoder. The difference between {{the positive and negative}} ranges, e.g. the negative range corresponding to +30 to +1 is −31 to −2. This is accounted for by the use of 1's complement (simple <b>bit</b> <b>inversion)</b> rather than 2's complement to convert a negative value to a positive value during encoding.|$|E
40|$|A {{technique}} is presented for correction of an (n,k) cyclic block code {{subjected to a}} noise disturbance consisting of an arbitrary number of both bit deletions and <b>bit</b> <b>inversions</b> contained within a single error burst. Following the procedure described by Meggitt (1961), the correction of a b-bit burst is attempted by first loading the initial error syndrome into an (n-k) order feedback shift register with taps selected for the code's generating polynomial; the register is then successively shifted one bit position {{in the direction of}} lower order. The discussion covers burst correction with m-bit deletion, burst/deletion decoder implementation, false correction probability, and bit slippage involving bit insertions. The principal elements of the burst/deletion correction decoder are presented in schematic form...|$|R
40|$|In this paper, {{we examine}} the digital {{hardware}} design and implementation of a novel compact block cipher, referred to as PUFFIN, that is suitable for embedded applications. An implementation of PUFFIN targeted to ASIC technology is considered. The proposed block cipher is designed to have a 64 -bit block size, a 128 -bit key, and is capable of both encryption and decryption operations. The cipher structure {{is based on the}} following features: a simple encryption process composed of permutations and substitutions based on 4 × 4 S-boxes, an identical datapath for both encryption and decryption facilitated by involutional operations, and a straightforward on-the-fly subkey generation composed of only a permutation and <b>bit</b> <b>inversions.</b> PUFFIN is found to perform well for implementations based on 0. 18 -micron CMOS technology. In comparison to other lightweight ciphers, PUFFIN has preferred features, low hardware complexity, and good throughput. 1...|$|R
40|$|The error {{performance}} of the Tunstall code is analysed, and a relevant metric defined. Several parameters which can be varied in the encoding process are considered, {{with the objective of}} minimising the effect of errors without sacrificing compression. The results obtained show that the error performance may be significantly improved without loss of compression. 1 Introduction Data compression techniques seek to reduce redundancy in the source message, thereby encoding the same amount of information in a smaller number of bits. It is thus natural to expect that an error in a single bit of a compressed stream will result in a larger number of errors in the decompressed stream [1]. The Tunstall source coding algorithm [2] is a variable-to-fixed length encoding scheme. In general, over most practical channels, errors in the compressed stream are restricted to <b>bit</b> <b>inversions</b> only. Thus, since all codewords in a Tunstall code have a fixed length, an error in the compressed stream wil [...] ...|$|R
5000|$|Where [...] is {{the sign}} bit, [...] is its inverse (i.e. {{positive}} values are encoded with MSB = [...] = 1), and bits marked [...] are discarded. Note {{that the first}} column of the table uses different representation of negative values than the third column. So for example, input decimal value −21 is represented in binary after <b>bit</b> <b>inversion</b> as 1000000010100, which maps to 00001010 (according to the first row of the table). When decoding, this maps back to 1000000010101, which is interpreted as output value −21 in decimal. Input value +52 (0000000110100 in binary) maps to 10011010 (according to the second row), which maps back to 0000000110101 (+53 in decimal).|$|E
40|$|This paper {{presents}} {{a method of}} error-correcting coding of digital information. Feature of this method is the treatment of cases of inversion and skip bits caused by {{a violation of the}} synchronization of the receiving and transmitting device or other factors. The article gives a brief overview of the features, characteristics, and modern methods of construction LDPC and convolutional codes, as well as considers a general model of the communication channel, taking into account the probability of bits inversion, deletion and insertion. The proposed coding scheme is based on a combination of LDPC coding and convolution coding. A comparative analysis of the proposed combined coding scheme and a coding scheme containing only LDPC coder is performed. Both of the two schemes have the same coding rate. Experiments were carried out on two models of communication channels at different probability values of <b>bit</b> <b>inversion</b> and deletion. The first model allows only random <b>bit</b> <b>inversion,</b> while the other allows both random <b>bit</b> <b>inversion</b> and deletion. In the experiments research and analysis of the delay decoding of convolutional coder is performed and the results of these experimental studies demonstrate the feasibility of planted coding scheme to improve the efficiency of data recovery that is transmitted over a communication channel with noises which allow random <b>bit</b> <b>inversion</b> and deletion without decreasing the coding rate...|$|E
40|$|In this paper, we {{analyze the}} error {{recovery}} performance of variable length codes (VLCs) transmitted over binary symmetric channel (BSC). Simple expressions for the exact mean symbol error rate (MSER) and the exact variance of symbol error rate (VSER) for any crossover probability p, are presented. We also {{prove that the}} mean error propagation length (MEPL) derived for single <b>bit</b> <b>inversion</b> error case is a scaled value of MSER when p, tends to zero. Comparisons with simulations demonstrate {{the accuracy of the}} MSER and VSER expressions...|$|E
40|$|Abstract—This paper {{describes}} about novel key {{expansion and}} its inversion technique for private key cryptosystems. Our design uses (8, 4) Extended Hamming Code and its error control logic to produce memory efficient key schedule generation algorithm. A mathematical relationship between 4 bit word and its corresponding 4 bit parity bits is shown. Simplicity, symmetry elimination, diffusion and non-linearity {{of the proposed}} key expansion technique are described as the key schedule generation criteria. Proposed method removes the usage of S-box to reduce the working memory of the algorithm. High nonlinearity penetration of original input message bits is achieved by applying modulo 2 addition of code based key schedules for each round transformations. Security strength among these key schedules is achieved by intentional <b>bit</b> <b>inversions</b> among them with beyond the error correcting limitations of chosen code. Comparative results between proposed design and Rijndael algorithm is illustrated {{with the aid of}} Xilinx Simulation tool. This paper concludes that novel key generation technique by Error Control Algorithm of wireless communication channel is an alternative solution to the cryptosystems without S-box substitution and any lookup tables...|$|R
40|$|In {{this work}} {{the problem of}} Single Event Upset (SEU) is {{considered}} in a recent analog technology: The Field Programmable Analog Arrays (FPAAs). Some FPAA models are based on SRAM memory cells to implement the user programmability, which makes this kind of device vulnerable to SEU when employed in applications susceptible to the incidence of radiation. In the former part of this work some fault injection experiments are made in order to investigate the effects of SEU in the SRAM blocks of a commercial FPAA. For this purpose, single <b>bit</b> <b>inversions</b> are injected in the FPAA programming bitstream, when an oscillator module is programmed. In a second moment, a self-checking scheme using the studied FPAA is proposed. This scheme, which is built from the FPAA programming resources, is able to restore the original programming data if an error is detected. Fault injection is also performed to investigate {{the reliability of the}} proposed scheme when the bitstream section which controls the checker blocks is corrupted due to a SEU...|$|R
5000|$|... 8B/12B first sends an {{odd parity}} bit, {{followed}} by 8 bits (least-significant bit first), {{followed by an}} <b>inversion</b> <b>bit,</b> followed by a 1 (which is the start bit), and a 0 which is the stop bit.|$|R
40|$|Several {{methods for}} {{increasing}} bit transition densities in a data stream are summarized, discussed in detail, and compared against constraints {{imposed by the}} 2 MHz data link {{of the space shuttle}} high rate multiplexer unit. These methods include use of alternate pulse code modulation waveforms, data stream modification by insertion, alternate <b>bit</b> <b>inversion,</b> differential encoding, error encoding, and use of bit scramblers. The psuedo-random cover sequence generator was chosen for application to the 2 MHz data link of the space shuttle high rate multiplexer unit. This method is fully analyzed and a design implementation proposed...|$|E
40|$|Power {{dissipation}} is a {{major issue}} with testing of designs having full scan architectures. The proposed scan technique minimizes toggle activity while scanning in test patterns. The method uses <b>bit</b> <b>inversion</b> technique to avoid toggles in scan flip-flops. The setup is dynamically configurable to one among the logic reversal structure and traditional scan while shift-in/shift-out of test patterns. Experimental {{results indicate that the}} average toggle activity is minimized substantially compared to California Scan architecture. It has features of full diagnosability of single stuck-at faults along the scan chain path...|$|E
40|$|This thesis {{presents}} a new multiple-valued encoder with re-configurable radix. The proposed circuits utilize serial cyclic D/A conversion and semi floatinggate (SFG) inverters for compact design {{and a high}} functional capacity per device. A re-configurable radix is not supported by existing SFG inverter based multiple-valued encoders which make use of parallel binary weight D/A conversion. The study covers least significant bit-first (LSB), least significant bit-first with alternate <b>bit</b> <b>inversion</b> (LSB ABI) and most significant bit-first (MSB) digital input codes. The serial cyclic D/A converters with LSB and LSB ABI input codes are implemented in a double-poly 0. 35 um AMS process. Measured results are provided and analyzed using standard static D/A converter performance measures. Circuits are tested using the practical radices 4, 8 and 16. Experimental results demonstrate that serial cyclic D/A converters using SFG inverters are feasible. Compared to related work on cyclic D/A conversion, the proposed circuits feature both a reduced number of devices {{and a reduction in}} the required die area. Several new techniques are identified for extending the resolution beyond radix 4, 8 and 16 MVL applications. This includes an error correction algorithm called least significant bit-first with alternate <b>bit</b> <b>inversion</b> (LSB ABI), a sample and hold clock scheme and a Dual Data-Rate (DDR) mode of D/A converter operation. The techniques are implemented on a chip and measured results are provided. The thesis also includes simulation work on several new SFG based circuits. A ternary serial D/A converter, a MSB-first serial D/A converter and a multiple-valued frequency divider which features re-configurable modulus...|$|E
5000|$|R̅, X̅ and B̅ <b>bits</b> are <b>inversion</b> of the REX prefix's R, X and B bits; these {{provide a}} fourth (high) bit for {{register}} index fields (ModRM reg, SIB index, and ModRM r/m; SIB base; or opcode reg fields, respectively) allowing access to 16 instead of 8 registers. The W bit {{is equivalent to}} the REX prefix's W bit, and specifies a 64-bit operand; for non-integer instructions, it is a general opcode extension bit.|$|R
40|$|FPGAs {{can be an}} {{attractive}} solution for many safety critical applications due to their reconfigurability, short time-to-market and low-cost for low-volume production characteristics. However, they are highly susceptible to transient faults (e. g. ionizing radiation). Spontaneous <b>bit</b> <b>inversions</b> can occur in RAM memories, which serve for data or program code storage in FPGA based embedded systems. This can be clearly unacceptable for many safety critical applications (e. g. applications compliant with SIL 3 and SIL 4 levels of IEC 61508 standard). This document is an introduction to a fault-tolerant RAM memory system for the use in FPGA based embedded systems. The custom component is accommodated on an embedded bus {{and can be used}} for high level program data or program code (data and instructions) storage. The memory content is protected against single bit errors by means of a SEC-DED code. Additionally, the technique of probabilistic scrubbing is applied to further improve the reliability. The memory system is incorporated within the development flow of the Xilinx development software tools. As a real time operating system, FreeRTOS has been introduced in the development flow and run from the memory component...|$|R
40|$|We give {{deterministic}} {{polynomial time}} algorithms for two different decision version the modular inversion hidden number problem introduced by D. Boneh, S. Halevi and N. A. Howgrave-Graham in 2001. For example, {{for one of}} our algorithms {{we need to be}} given about 1 / 2 of the <b>bits</b> of each <b>inversion,</b> while for the computational version the best known algorithm requires about 2 / 3 of the bits and is probabilistic. 8 page(s...|$|R
40|$|International Telemetering Conference Proceedings / October 14 - 16, 1975 / Sheraton Inn, Silver Spring, MarylandConventional {{burst error}} {{correction}} techniques for (n,k) cyclic block codes cannot {{cope with the}} presence of bit slippages that frequently occur in conjunction with burst errors of the <b>bit</b> <b>inversion</b> variety. A technique is described to enable the correction of an (n,k) cyclic code subjected to a noise disturbance consisting of an arbitrary number of both bit deletions and bit inversions contained within a single error burst. An efficient implementation of a Burst/Deletion Correction Decoder is presented. Although bit insertion correction is conceptually similar to that of bit deletions, the decoder implementation for combined insertion and inversion correction within a burst is much more cumbersome. The probabilities of false correction are analyzed...|$|E
40|$|Maxted et al. in 1985 gave a {{conjecture}} stating that, for a Geometric source, {{the stable}} code {{has the best}} error recovery performance for the case of <b>bit</b> <b>inversion</b> among all Huffman codes for this source, while the unstable code has the worst error recovery performance. This conjecture was extended by Swaszek et al. ten years later, but without proof, to sources with certain probability mass function. In this paper, we prove the correctness of the extended version of this conjecture. Our proof provides a novel mathematical technique for proving the optimality of variable length code {{in the sense of}} error recovery capability. Furthermore, our result offers some insight into the working mechanism of the suffix condition that has been widely used by many heuristic algorithms to find error-resilient codes. © 2007 IEEE...|$|E
40|$|Abstract. We find symmetries for {{constraints}} that model the nonlinearity {{requirements of a}} discrete function f: Z 2 n → Z 2 m(n> m). Such constraints are very important, as the functions are employed in generating deterministic but difficult-to-analyze permutations used in symmetric cryptographic systems. There, such functions {{are referred to as}} Substitution Boxes (S-boxes). The nonlinearity is a complex requirement that has been traditionally formulated using a set of criteria (that we interpret as new constraints). Most of these constraints are found to exhibit symmetries that can be exploited for reducing the size of the search space, and for efficiently generating new solutions. Among discovered symmetries, a <b>bit</b> <b>inversion</b> symmetry (a special case of the value reversal symmetry) and a rotational symmetry (a special case of variable symmetry) are foundtoapplytoallstudiednonlinearityconstraintswithoutaffectingtheirsecuritymetric,andquadrupletheefficiencyofsolvers. Theoreticalandexperimental results onsymmetryare reported. ...|$|E
40|$|Abstract — In this paper, {{we propose}} a new hybrid {{automatic}} repeat request (H-ARQ) scheme in MIMO. The proposed scheme performs bit-level exchanges and modifications every retransmission and covers all combinations of number of antennas {{and number of}} retransmissions in M-QAM. In order to obtain a better bitwise mapping scheme, we define two independent bitwise suboperations, which are called <b>bit</b> swapping and <b>inversion</b> (BSI) and <b>bit</b> shifting between antennas (BSA), and derive sets for BSI and BSA. The performance of the proposed scheme is evaluated through link simulations with a 3 GPP long-term evolution (LTE) specification and is compared to previous H-ARQ schemes in MIMO. The {{results show that the}} proposed bitwise mapping scheme provides approximately 2 dB gain over the conventional scheme even at a high mobile speed. I...|$|R
40|$|Genetic Algorithms (GA) and Evolutionary Programming(EP) {{are both}} search {{techniques}} predicated upon simulating {{some of the}} processes outlined in evolutionary theories. Both techniques claim the capacity to find global minima (or alternately maxima) in large search spaces. Genetic Algorithms employ sexual reproduction among fit parents using genotypic operators such as crossover, <b>bit</b> mutation, and <b>inversion.</b> On the other hand, Evolutionary Programming espouses a form of asexual phenotypic reproduction whereby each candidate parent undergoes a zero-mean Gaussian mutation. Previous {{studies have shown that}} EP is capable of producing more precise solutions than GAs for a number of simple functions. This paper investigates another class of problem spaces for which GA outperforms EP, specifically problems where portions of a given solution have a greatly varying impact on the overall score. A simple representative problem is tested against typical EP and GA implementations with special attention paid to testing the effects of search parameters on both EP and GA performance. Finally, the relative performanc...|$|R
40|$|ABSTRACT: In {{piezoelectric}} energy harvesting, the harvesting efficiency can {{be greatly}} improved with the switching interface called synchronized switch harvesting on inductor. However, we found from experiments that, the voltage across the piezoelectric element reverses a little <b>bit</b> after every <b>inversion.</b> This reversion, although {{small compared to}} the inversion, weakens the inversion effect produced by the switching inductive shortcut, and therefore decreases the voltage magnitude {{as well as the}} harvesting efficiency a lot. This phenomenon can also be found from the experimental waveforms in the previous studies; but it has never been pointed out. In these studies, the analytical results, which used the measured effective inversion factor in calculation, agreed with experiments; yet, the origin of the reversion after every switching action, as well as the quantitative relation between the ideal and effective inversion factors are of interest. In this article, the phenomenon on voltage reversion after every inversion is first described. Based on the experimental results, it is analyzed that this reversion is caused by the transducer internal loss. A revised model con-sidering the influence of the internal energy loss is introduced. With this model, not only the reason, which causes the reversion after every inversion, can be explained; but also the quantitative relation between the ideal and effective inversion factors can be obtained. Experiments are also conducted to validate the revised model...|$|R
40|$|While Phase Change Memory (PCM) holds a {{great promise}} as a {{complement}} or even replacement of DRAM-based memory and flash-based storage, it must effectively overcome its limit on write endurance to be a reliable device {{for an extended}} period of intensive use. The limited write endurance can lead to permanent stuck-at faults after a certain number of writes, which causes some memory cells permanently stuck at either ’ 0 ’ or ’ 1 ’. State-of-the-art solutions apply a <b>bit</b> <b>inversion</b> technique on selected bit groups of a data block after its partitioning. The effectiveness of this approach hinges on how a data block is partitioned into bit groups. While all existing solutions can separate faults into different groups for error correction, they are inadequate on three fundamental capabilities desired for any partitio...|$|E
30|$|Holzer et al. {{processed}} grayscale {{images with}} a diamond-shaped SE, while Genovese et al. processed binary images with a rectangular SE, where both implementations directly mapped a fixed morphological operation with no reconfigurable SE options. Deforge et al. and Bartovsky et al. demonstrate rectangular- and convex-shaped SE of arbitrary size, while Deforge et al. provide no documentation on the designs' programmable options after synthesis, and Bartovsky et al. support reprogrammable rectangular dimensions, however {{were limited to}} rectangular-shaped SE. The binary morphology proposed by Genovese et al. maps directly to digital logic gates implementing the dilation and a selectable <b>bit</b> <b>inversion</b> to obtain both dilation and erosion results from the same architecture, while in contrast, efficient grayscale morphology duality architecture has not been attempted. Only Deforge et al. addressed sampling outside the image boundary with introduction of an extra memory line to flush the pipeline when required.|$|E
40|$|The error {{recovery}} capability of variable length code (VLC) {{has been considered}} as an important performance and design criterion {{in addition to its}} coding efficiency. However, almost all of the existing methods for evaluating the {{error recovery}} capability of VLC assume that the transmission fault is a random single <b>bit</b> <b>inversion.</b> In this paper, we consider a more generalized problem of precisely evaluating the error recovery capability of VLC in the case that the encoded bit stream is transmitted over a BSC with arbitrary crossover probability. By making use of the Perron-Frobenius Theorem, we derive a very simple expression for the exact mean error propagation rate (MEPR), and show that the variance of error propagation rate (VEPR) is zero. We also prove that in the regime of very low crossover probability, the mean error propagation length (MEPL) derived for single inversion error case approaches a scaled value of the MEPR. Furthermore, we briefly discuss the problem of evaluating the error detection capability of non-exhaustive code over BSC...|$|E
40|$|With {{the rise}} of mobile (cellphones, tablets, notebooks, etc.) and {{broadband}} wireline communications (Fiber to the Home), there are increasing demands being placed on transmitters for moving data from device to device and around the world. Digital and analog fiber-optic communications have been the key technology to meet this challenge, ushering in ubiquitous Internet and cable TV over the past 20 years. At the physical layer, high-volume low-cost manufacturing of semiconductor optoelectronic devices has played an integral role in allowing for deployment of high-speed communication links. In particular, vertical cavity surface emitting lasers (VCSEL) have revolutionized short reach communications and are poised to enter more markets due to their low cost, small size, and performance. However, VCSELs have disadvantages such as limited modulation performance and large frequency chirp which limits fiber transmission speed and distance, key parameters for many fiber-optic communication systems. Optical injection locking is one method to overcome these limitations without re-engineering the VCSEL at the device level. By locking the frequency and phase of the VCSEL by the direct injection of light from another laser oscillator, improved device performance is achieved in a post-fabrication method. In this dissertation, optical injection locking of VCSELs is investigated from an applications perspective. Optical injection locking of VCSELs {{can be used as}} a pathway to reduce complexity, cost, and size of both digital and analog fiber-optic communications. On the digital front, reduction of frequency chirp via <b>bit</b> pattern <b>inversion</b> for large-signal modulation is experimentally demonstrated showing up to 10 times reduction in frequency chirp and over 90 times increase in fiber transmission distance. Based on these results, a new reflection-based interferometric model for optical injection locking was established to explain this phenomenon. On the analog side, the resonance frequency enhancement was exploited for millimeter-wave radio over fiber communications. Experimental demonstration of 4 Gb/s data transmission over 20 km of fiber and 3 m of wireless transmission at a 60 GHz carrier frequency was achieved. Additionally, optical injection of multi-transverse mode (MM) VCSELs was investigated showing record resonance frequency enhancement of > 54 GHz and 3 - dB bandwidth of 38 GHz. Besides these applications, a number of other intriguing applications are also discussed, including an optoelectronic oscillator (OEO) and wavelength-division multiplexed passive optical networks (WDM-PON). Finally, the future of optical injection locking and its direction going forward will be discussed...|$|R
40|$|Mobile {{applications}} such as tablets pack increasingly more processing capability comparable to workstations or laptops but can do little for cooling or extending the battery life in their form factors. SRAMs account for a large fraction of chip area and are critical in this context. Recent work has focused on voltage scaling in SRAMs, which is an effective way of achieving energy efficiency [1, 2]. These conventional SRAMs are mostly general-purpose {{in the sense that}} they are designed without considering the specific features of the data they will store. However, application-specific features such as statistics of storage data can be exploited and incorporated into the transistor-level design to provide a new dimension towards achieving the next level of energy savings in addition to the savings provided through voltage scaling. The work in [3] is an example where an <b>inversion</b> <b>bit</b> is added for each word to reduce read-bitline (RBL) transitions in an 8 T-cell-based design with a single-ended read port. Similarly, the work in [4] stores only the LSBs of each word in 6 T SRAMs where occasional bit-errors at low voltages are tolerable for its application. In this work, we focus on video; however, the ideas can be generalized to different applications. In video encoders, pixel processing is performed over large partitions of image frames (e. g., 192 × 192 pixels), which are stored in on-chip SRAMs and accessed frequently. Image frames generally consist of smooth backgrounds or large objects where the intensity of pixels is spatially correlated. For the video image frame in Fig. 18. 2. 1, the deviation of each pixel's intensity from its block average for a 16 × 16 block shows that 76 % of pixels lie within 3 LSB of the average. This additional information can be used to design an SRAM where correlation of data is used to reduce bitline activity factor which, for an 8 T SRAM in a 65 nm low-power CMOS process, accounts for ~ 50 % of total energy consumption during read a- cesses at 0. 6 V. In this work, we present a prediction-based reduced-bitline-switching-activity (PB-RBSA) scheme along with a hierarchical sensing network with statistical sense-amplifier gating to exploit the correlation of storage data. Reduction of switching activity on the bitlines and in the sensing network of the memory provide up to 1. 9 × reduction in energy/access. Texas Instruments Incorporate...|$|R
40|$|The paper {{presents}} the results of research that is aimed to improve the reliability of transmission of telemetry information (TMI) through a communication channel with noise from the object of telemeasurements to the telemetry system for collecting and processing data. It considers the case where the quality of received information changes over time, due to movement of the object relative to the receiving station, or other factors that cause changes in the characteristics of noise in the channel, up to the total loss due to some temporary sites. To improve the reliability of transmission and ensure continuous communication with the object, it is proposed to use a multi-channel system to record the TMI. This system consists of several telemetry stations, which simultaneously register data stream transmitted from the telemetry object. The multichannel system generates a single stream of TMI for the user at the output. The stream comprises the most reliable pieces of information, being received at all inputs of the system. The paper investigates the task of constructing a multi-channel registration scheme for telemetry information (TMI) to provide a simultaneous reception of the telemeasurement data by multiple telemetry stations and to form a single TMI stream containing the most reliable pieces of received data on the basis of quality analysis of information being received. In a multichannel registering system of TMI there are three main factors affecting the quality of the output of a single stream of information: 1) quality of the method used for protecting against errors during transmission over the communication channel with noise; 2) efficiency of the synchronization process of telemetry frames in the received flow of information; 3) efficiency of the applied criteria to form a single output stream from multiple input streams coming from different stations in the discussed multichannel registering system of TMI. In the paper, in practical implementation of the multi-channel registering system of TMI, additional effect obtained from applying a method of error-correcting coding TMI correcting omissions and <b>inversion</b> <b>bits</b> [1], is applied, as well as the effect of applying the criteria for the choice of parameters of TMI frame synchronizer [2]. This article {{presents the}} necessary and effective criteria for constructing the single output stream of information and to assess the quality of the output stream in various realizations of the multi-channel registering system of TMI. The paper discusses two options for building a multi-channel recording system. The first variant of the system does not use additional methods of error-correcting coding during transmission. The second option for constructing a multi-channel system is based {{on the use of the}} developed combination of convolutional codes and low-density parity-check (LDPC) (error-correcting coding method presented in [1]). The paper presents selection criteria of the most significant pieces of TMI input streams and a comparative analysis of the effectiveness of the proposed implementations. It gives a comparative assessment of the effectiveness of the proposed methods for constructing a multichannel recording system of TMI according to the following parameters: 1) bit error rate in the output TMI frame; 2) the percentage of fully reconstructed output frames; 3) the gain defined as the ratio of bit error rate in output TMI output frame for the systems under comparison. </p...|$|R
40|$|A novel {{implementation}} of code based cryptography (Cryptocoding) technique for multi-layer key distribution scheme is presented. VLSI chip {{is designed for}} storing information on generation of round keys. New algorithm is developed for reduced key size with optimal performance. Error Control Algorithm is employed for both generation of round keys and diffusion of non-linearity among them. Two new functions for <b>bit</b> <b>inversion</b> and its reversal are developed for cryptocoding. Probability of retrieving original key from any other round keys is reduced by diffusing nonlinear selective bit inversions on round keys. Randomized selective bit inversions are done on equal length of key bits by Round Constant Feedback Shift Register within the error correction limits of chosen code. Complexity of retrieving the original key from any other round keys is increased by optimal hardware usage. Proposed design is simulated and synthesized using VHDL coding for Spartan 3 E FPGA and results are shown. Comparative analysis is done between 128 bit Advanced Encryption Standard round keys and proposed round keys for showing security strength of proposed algorithm. This paper concludes that chip based multi-layer key distribution of proposed algorithm is an enhanced solution to the existing threats on cryptography algorithms...|$|E
30|$|Mutation {{is carried}} out with a single parent and plays an {{important}} role in increasing the population diversity. Various mutation operators have been developed for different solution representations: <b>bit</b> <b>inversion</b> mutation for binary coding (Holland 1992); swap, insertion, inversion and displacement for permutation coding (Larrañaga et al. 1999); Gaussian mutation (Sarangi et al. 2015), polynomial and power mutation for real coding (Deb and Deb 2012; Deep and Thakur 2007). Some mutation operators are problem-dependent, such as greedy sub tour mutation for traveling salesman problems (Albayrak and Allahverdi 2011) and energy mutation for multicast routing problems (Karthikeyan et al. 2013). Some studies suggest a mutation-combination (Deep and Mebrahtu 2011) or self-adaptive mutation operators (Hong et al. 2000; Mc Ginley et al. 2011; Serpell and Smith 2010). The performance of different mutation operators highly depends on the parameter choice of genetic algorithms (Brizuela and Aceves 2003; Osaba et al. 2014; Wang and Zhang 2006) and the type of problems (Hasan and Saleh 2011; Karthikeyan et al. 2013). Most of related work studied problems without cooperative tasks, such as traveling salesman problems (Albayrak and Allahverdi 2011; Deep and Mebrahtu 2011) and flow shop scheduling (Nearchou 2004; Wang and Zhang 2006). In this paper, the performance of mutation operators will be analyzed when solving multi-robot task allocation problems without or with cooperative tasks.|$|E
40|$|With the {{widespread}} use of multimedia content, the multimedia security and digital rights management (DRM) issues have become increasingly important. A straightforward way to protect multimedia data is to use the traditional cryptographic algorithms such as AES and RC 4 to encrypt the whole bit stream. Nevertheless, the encryption of multimedia files has to be carried out judiciously. On one hand, ciphering the complete compressed file may result in excessive computational burden and power consumption at the decoder and perhaps even the server/encoder/transcoder. While this may not be critical for PC-based systems, it could lead to excessive battery drain on a mobile device. Even more importantly, multimedia compressed files typically exhibit well-defined hierarchical structure (e. g. in the H. 264, MPEG- 4, H. 263, MPEG- 2, AVS, mp 3, AAC, JPEG, JPEG 2000 formats) that can be exploited in several useful ways, e. g., for scalability, transcoding, and rate shaping. However, these structures are not recognizable in the ciphertext, and hence, are wasted. In this thesis, we address the problem of joint compression and encryption by introducing randomness into the entropy coder, including Huffman coder, arithmetic coder, and Lempel-Ziv-Welch coder. These entropy coders have been widely utilized in the state-of-the-art text/image/video coding standards, e. g., TIFF, JPEG and H. 264. First, we study the error recovery of variable length codes (VLCs), which lays the theoretical foundation of many multimedia encryption schemes based on Huffman-like coding. More specifically, we extend the problem of precisely evaluating the error recovery capability of VLC to the binary symmetric channel (BSC) case. Two criteria to evaluate the error recovery performance of VLCs in this case are proposed, together with their explicit expressions. In addition, we establish a linkage between our proposed results with the previous results assuming single <b>bit</b> <b>inversion</b> error. We also address an over twenty-year old conjecture in information theory society, which states that a certain class of codes have the optimal error recovery performance among all the codes having the same coding efficiency, provided that a constraint on the probability mass function is satisfied. We solve this conjecture by providing a rigorous proof. To the best of our knowledge, {{it is the first time}} that the optimality of VLCs in terms of error recovery performance can be proved. In addition, this proof could help us gain more insight into the working mechanism of the suffix condition widely used in many heuristic methods for finding the error-resilient codes. Then, the practical multimedia encryption schemes using Huffman-like coding are investigated. We present a chosen-plaintext attack to break the Multiple Huffman Table (MHT) schemes. Based on the cryptanalysis results, we suggest an improved version of MHT by incorporating with a stream cipher. We also propose two criteria to select Huffman tables to achieve high level of security with small number of Huffman tables. These results could be readily extended to the Exp-Golomb coding, where the alphabet size is conceptually infinite. We also investigate the lightweight multimedia encryption schemes using arithmetic coding, which is capable of offering higher coding efficiency compared with a Huffman coding. We give an adaptive chosen-ciphertext attack with linear complexity to break the recently proposed secure arithmetic coding, which is an advanced version of the interval splitting arithmetic coding. We then design a system that can resist all the existing attacks and can be conveniently incorporated with the context-based coding. Finally, we address the problem of designing a secure joint compression-encryption scheme based on Lempel-Ziv-Welch coding, which is a universal source coding technique. We show that the proposed secure Lempel-Ziv-Welch algorithm with random dictionary permutation and insertion can provide high level of security without coding efficiency loss...|$|E

