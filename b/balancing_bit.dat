1|25|Public
40|$|Cryptography algorithms, such as Advanced Encryption Standard (AES) algorithm, are {{responsible}} for keeping confidential and critical data secure using a secret key to access that data. Today, due to the integration of digital technology into all disciplines, personal information, government, financial, and military information is stored using cryptographic algorithms to prevent unauthorised access. When cryptographic algorithms are executed on either circuit based implementations or software based implementations, using non-computational emanated information, such as power dissipation, elapsed time, electromagnetic radiation, faulty ciphertext, and cache content (which {{are referred to as}} side-channels) the secret key can be deduced. Such attacks are referred to as side-channel attacks. Most devices, armed with cryptographic algorithms, use modes of operations to remove data dependencies. In this dissertation, first, the resistance of modes of operations of AES is tested against power based side-channel analysis attacks. The minimal number of power traces to break each mode is calculated with a 99. 99 % confidence interval. This analysis is the first comprehensive analytical study of power based side-channel analysis resistance and the comparison of the levels of resistance offered by the differing modes of AES. In order to mitigate power based side channel attacks, countermeasures must be deployed. <b>Balancing</b> <b>bit</b> flips {{has been shown to be}} an excellent solution against power analysis attacks where the data and the complement of the data are processed simultaneously to cancel out (balance) the data dependent power dissipations. A novel algorithmic circuit balancing technique, QuadSeal, which balances both static power and the dynamic power is proposed as the main contribution of this thesis. QuadSeal uses four algorithmically modified circuits. The mathematical proof of the QuadSeal countermeasure is presented, and AES is balanced as an example with a random input swapping methodology to resist variability effects. Having a 6. 5 x area overhead and 4 × power overhead, QuadSeal-AES is the smallest complete balancing countermeasure against power based side-channel analysis attacks. Unlike, all the circuitry balancing countermeasures proposed in the literature, QuadSeal can turn off additional circuitry to save power consumption or perform parallel encryptions to increase throughput when the security is not essential. Only a few countermeasures offer protection against multiple side-channel leakages. QuadSeal countermeasure was tested against fault injection attacks. First, a mathematical proof of the fault injection attack resistance of QuadSeal is presented, and it is proven that QuadSeal offers protection against fault injection attacks, but not detection. Therefore, a dual mode circuit (referred to as C-FIA circuit) which can detect and correct fault injections is proposed to hone the resistance against fault injection attacks. The only possible way to break the security of C-FIA circuit is by injecting identical faults into all four circuits which has not been possible thus far. Pre-charge stage is essential in all previously proposed balancing countermeasures, where during the pre-charge stage the registers and logic are initialised to ‘ 0 ’ to achieve a constant number of bit transitions. An extension of QuadSeal, NORA balancing methodology, is proposed as the last contribution of the thesis, in which the pre-charge stage is not needed. The mathematical proof of NORA is presented for a general register, and then the two AES implementations (using distributed Random Access Memory - RAM and block RAM) are proposed. The security of NORA is tested against 600, 000 encryptions using both multi-bit and mono-bit attack models. The information leakage is also presented. NORA is the only balancing countermeasure which does not need pre-charge stage to maintain constant power dissipation...|$|E
50|$|Balance {{factors can}} be kept {{up-to-date}} by knowing the previous balance factors and the change in height - {{it is not necessary}} to know the absolute height. For holding the AVL <b>balance</b> information, two <b>bits</b> per node are sufficient.|$|R
40|$|Abstract. No one {{can deny}} the {{significance}} of wireless communication in our life, but there is always noise {{that can lead to}} some mistakes in transmitting signal. Facing the noise, we should realize that we cannot eliminate completely. Therefore, we should estimate the signal-to-noise ratio so that we can <b>balance</b> the <b>Bit</b> Error Rate and the power, which can save more energy. Here is a simple way to estimate the signal-to-noise ratio based on Bernoulli's Law of Large Numbers...|$|R
30|$|The {{resulting}} spectrally flattened {{signal is}} analyzed for its temporal structure. This structure is analyzed over several frames simultaneously {{in order to}} obtain a good <b>balance</b> between required <b>bit</b> rate and modeling capability. The envelope modeling is done by linear prediction in the frequency domain [48, 49].|$|R
30|$|The {{design of}} the quantizers was made by {{applying}} the LBG algorithm [33] on the (voiced or unvoiced) training corpus described in Section 4.1, using the perceptual weighted Euclidian distance between LSF vectors proposed in [28]. The two/three-stage quantizers are obtained as follows. The LBG algorithm is first used to design the first codebook block. Then, the difference between each LSF vector of the training corpus and its associated codeword is calculated. The overall resulting set of vectors {{is used as a}} new training corpus for the {{design of the}} next block, again with the LBG algorithm. The decoding of a quantized LSF vector is made by adding the outputs of the different blocks. For resolutions ranging from 20 to 24, two-stage quantizers were designed, with a <b>balanced</b> <b>bit</b> allocation between stages, that is, 10 - 10, 11 - 11, and 12 - 12. For resolutions within the range 26 – 36, a third stage was added with 2 to 12 [*]bits. This is because computational considerations limit the resolution of each block to 12 [*]bits. Note that the ms structure does not guarantee that the quantized LSF vector is correctly conditioned (i.e., in some cases, LSF pairs can be too close to each other or even permuted). Therefore, a regularization procedure was added to ensure correct sorting and a minimal distance of 50 [*]Hz between LSFs.|$|R
30|$|In this work, we have {{presented}} a dynamic buffer-aided DSTC scheme for cooperative DS-CDMA systems with different relay pair selection techniques. With {{the help of}} the dynamic buffers, this approach effectively improves the transmission performance and help to achieve a good <b>balance</b> between <b>bit</b> error rate (BER) and delay. We have developed algorithms for relay-pair selection based on an exhaustive search and on a greedy approach. A dynamic buffer design has also been devised to improve the performance of buffer-aided schemes. Simulation results show that the performance of the proposed scheme and algorithms can offer good gains as compared to previously reported techniques.|$|R
5000|$|It was {{reported}} {{there was a}} filmed but unused scene of a fight between April, Vernon Fenwick, Karai, and The Foot in downtown Manhattan that was cut from the finished film. Will Arnett, who played Vernon, said his character [...] "comes out just {{at the right moment}} and hits Karai with the gurney and knocks her off <b>balance</b> a little <b>bit.</b> Then April kind of finishes her off." ...|$|R
5000|$|A {{finished}} reining horse {{could be}} controlled and directed with minute movements of the fingers of the left hand, which hovered above the saddle horn. (Compare to the grazing-bit style of Western riding developed in Texas, where reins are split between the fingers and the hand moves {{in front of the}} saddle, controlling the horse by neck reining.) Because of the potential severity of the spade bit, chains added {{to the ends of the}} reins to <b>balance</b> the <b>bit</b> in the horse's mouth, and knotted and braided rawhide reins which prevented the reins from swinging unnecessarily, even at a lope, the [...] "made" [...] reining horse seemed to run, stop, spin and handle a cow on its own, with little communication from its rider.|$|R
30|$|It {{is helpful}} to control and <b>balance</b> the <b>bit</b> errors {{with the aid of}} a new bit geometry. The {{proposed}} method uses isosceles triangular-shaped quantization (TSQ) of the DCT domain [6] to correctly distribute the error rates. As a result, the proposed algorithm can reduce the probability of decoding failure, and the target bitrate can be gradually adjusted. In other words, error rates can be counterbalanced by combining lower frequency components (which have lower error probabilities) and higher frequency components (which have higher error probabilities) of a block. Therefore, the error rate of a quantized block as determined by the proposed TSQ is not as high as that of the LSB bitplanes or the high-frequency components. The proposed method can also control target bitrates by adjusting quantization parameters and error rates.|$|R
40|$|A vector {{enhancement}} of Said and Pearlman's Set Partitioning in Hierarchical Trees (SPIHT) methodology, named VSPIHT, {{has recently been}} proposed for embedded wavelet image compression. A major advantage of vector based embedded coding with fixed length VQs over scalar embedded coding, is its superior robustness to noise. In this work we show that vector set partitioning can effectively alter the <b>balance</b> of <b>bits</b> in the bit stream so that significantly fewer critical bits carrying significance information is transmitted, thereby improving inherent noise resilience. Additionally, the degradation in reconstruction quality caused by errors in non-critical quantization information, can be reduced by appropriate VQ indexing, or designing channel optimized VQs for the successive refinement systems. For very noisy channels unequal error protection to the critical and non critical bits with either block codes or convolution codes are used. Extensive simulation results are presented. 1. INTRODUC [...] ...|$|R
5000|$|Karai plays {{only a minor}} role in the film, [...] {{which came}} as a {{disappointment}} for some reviewers and commentators. [...] There was a filmed but unused scene of a fight between April O'Neil, Vernon Fenwick, Karai, and The Foot in downtown Manhattan that was cut from the finished film. Will Arnett, who played Vernon, said that in this scene his character [...] "comes out just at the right moment and hits Karai with the gurney and knocks her off <b>balance</b> a little <b>bit.</b> Then April kind of finishes her off." ...|$|R
40|$|This paper {{presents}} {{a new approach}} to designing pseudorandom number generators based on cellular automata. Current cellular automata designs either focus on i) ensuring desirable sequence properties such as maximum length period, <b>balanced</b> distribution of <b>bits</b> and uniform distribution of n-bit tuples etc. or ii) ensuring the generated sequences pass stringent randomness tests. In this work, important design patterns are first identified from the latter approach and then incorporated into cellular automata such that the desirable sequence properties are preserved like in the former approach. Preliminary experiment results show that the new cellular automata designed have potential in passing all DIEHARD tests...|$|R
50|$|In coding theory, a {{balanced}} code is a binary {{forward error correction}} code for which each codeword contains {{an equal number of}} zero and one <b>bits.</b> <b>Balanced</b> codes have been introduced by Donald Knuth; they are a subset of so-called unordered codes, which are codes having the property that the positions of ones in a codeword are never a subset of the positions of the ones in another codeword. Like all unordered codes, balanced codes are suitable for the detection of all unidirectional errors in an encoded message. Balanced codes allow for particularly efficient decoding, which can be carried out in parallel.|$|R
40|$|DFD {{have to be}} {{transmitted}} to the receiver, a well designed A new motion field representation based on the boundary- video coder should <b>balance</b> the <b>bits</b> used in these two control vector (BCV) scheme for video coding is examined in parts. Other factors of consideration in video coder design this work. With this scheme, the motion field is characterized include computational cost, hardware complexity and the {{by a set of}} control vectors and boundary functions. The control domain of applicability. vectors are associated with the center points of blocks to control We can roughly classify existing motion field representathe overall motion behavior. We use the boundary functions tion into block-based, pel-based, and model-based categoto specify the continuity of the motion field across adjacent ries. The block-based representation has been widely used blocks. For BCV-based motion field estimation, an optimization framework based on the Markov random field model and maxi- and adopted by several standards such as H. 261 [16] and mum a posteriori (MAP) criterion is used. The new scheme MPEG [6]. It divides an image frame into nonoverlapping effectively represents complex motions such as translation, ro- blocks, and represents the motion field in each block with tation, zooming, and deformation and does not require complex a translation vector. This representation is generally appli...|$|R
40|$|This is {{a conference}} paper [© IEEE]. It is also {{available}} from: [URL] Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must {{be obtained from the}} IEEE. In order to obtain a good <b>balance</b> of <b>bit</b> error rate (BER) across channels, the geometric mean decomposition (GMD) is introduced to replace the singular value decomposition (SVD) for precoding in the downlink of a multiuser multistream multiple-input multiple-output (MIMO) system. By combining GMD with a block diagonalization method, we obtain two kinds of precoding schemes: iterative nullspace-directed GMD and non-iterative nullspace-directed GMD. Considering their respective advantages and disadvantages, a mixed nullspace-directed GMD is proposed to solve the convergence related problems of the iterative method. Furthermore, the computational complexity of the mixed scheme is similar to the iterative scheme under the same conditions. The simulation results show that the average BER performance of the block diagonalization method based on GMD is better than the same method based on SVD, and the mixed nullspace-directed GMD outperforms the iterative nullspace-directed GMD and the non-iterative nullspace-directed GMD...|$|R
40|$|The ongoing MPEG {{standardization}} of Compact Descriptors for Visual Search (CDVS) focuses on image search for mobile applications {{and in that}} process, the extraction of local descriptors constitutes an important step. These local descriptors extracted from an image are further aggregated into global descriptors that are used for efficient retrieval of matching images from a database for a given query image. Current CDVS Test Model (TM) implements the global descriptor using the uncompressed Scale Invariant Feature Transform (SIFT) points. At the mobile devices, the global descriptor (GD) is computed as the quantized Fisher Vector of up to 300 SIFT points w. r. t a SIFT space Gaussian Mixture Model (GMM). It is noted that such an approach requires significant overhead in communication to transmit the global descriptor, especially at low bit rate. Hence, we propose an alternative and efficient way to re-construct the global descriptor from the local descriptors at the server side. The difference between the reconstructed GD and the original GD, are then selectively coded to strike a <b>balance</b> between <b>bit</b> rate cost and performance. The experiments on CDVS datasets shows around 0. 5 % increase in true positive rate and 1 % decrease in false positive rate. Index Terms — compact visual descriptor, global descriptor, mobile visual searc...|$|R
2500|$|Soon {{after her}} {{election}} to Parliament, Hanson's book, Pauline Hanson – the truth : on Asian immigration, the Aboriginal question, the gun debate {{and the future}} of Australia, was published. In it she makes claims of Aboriginal cannibalism, in particular that Aboriginal women ate their babies and tribes cannibalised their members. Hanson told the media that the reason for these claims of cannibalism was to [...] "demonstrate the savagery of Aboriginal society". David Ettridge, the One Nation party director, said that the book's claims were intended to correct [...] "misconceptions" [...] about Aboriginal history. These alleged misconceptions were said to be relevant to modern-day Aboriginal welfare funding. He asserted that [...] "the suggestion that we should be feeling some concern for modern day Aborigines for suffering in the past is <b>balanced</b> a <b>bit</b> by the alternative view of whether you can feel sympathy for people who eat their babies". The book predicted that in 2050 Australia would have a lesbian president of Chinese-Indian background called Poona Li Hung who would be a cyborg. In 2004, Hanson said that the book was [...] "written by some other people who actually put my name to it" [...] and that while she held the copyright over The Truth, she was unaware that much of the material was being published under her name.|$|R
40|$|In this paper, {{we present}} a full-static carry-skip adder {{designed}} to achieve low power dissipation and high-performance operation. To reduce the adder’s delay and power consumption, the adder is divided into variable-sized blocks that balance the inputs to the carry chain. The optimum block sizes for minimizing the critical path delay with complementary carry generation are achieved. Within blocks, highly optimized carry look-ahead logic, which computes block generate and block propagate signals, is used to further decrease delay. The adder architecture decreases power consumption by {{reducing the number of}} logic levels, glitches, and transistors. To achieve <b>balanced</b> delay, input <b>bits</b> are grouped unevenly in the carry chain. This grouping reduces active power by minimizing extraneous glitches and transitions. The adder has been implemented in 130 nm CMOS technology. At 1. 2 V and 25 C, typical performance is 1. 086 GHz and power dissipation normalized to 600 MHz operation is 0. 786 mW. 1...|$|R
5000|$|The race {{restarted}} on lap 210. During {{the green}} flag cycle of stops on lap 266, Logano was tagged for his crew being {{over the wall}} too soon {{and was forced to}} serve a pass-through penalty. Logano said after the race that his team, and others, are [...] "trying to make pit stops so fast and you’re gonna push everything to the edge. I guess we jumped off the wall a little bit too soon. I haven’t seen it, but unfortunately, that kind of made us make a green flag pit stop, which is really hard to overcome. Overall, {{we were able to get}} our lap back by racing up there, which was kind of cool. We didn’t have to take a lucky dog or any of that. We actually raced back to the lead lap, but we lost the <b>balance</b> a little <b>bit</b> on the last run and I couldn’t make much time once we got going." ...|$|R
5000|$|In late 2009, Raja agreed {{terms to}} work with actor Vijay, and spent ten months writing a script based on a story written by {{director}} Thirupathisamy in the early 2000s. Thirupathisamy {{had gone on to}} make the story into a Telugu film titled Azad (2000), while also began pre-production work to remake it in Tamil as Velan with Vijay and Priyanka Chopra in 2001, before he died later that year. Raja subsequently bought the remake rights and re-worked the script of Azad to suit Tamil audiences and signed on Genelia D'Souza and Hansika Motwani to play other lead roles. The film, which was retitled Velayudham, became Raja's first Tamil venture which did not include his brother in the cast and while writing the script, Raja analysed Vijay's popularity amongst children and women audience to insert certain scenes into the script. The film told the tale of a milkman who turns into a vigilante, with a human interest story as a backdrop. The film opened to positive reviews and became the third highest grossing Tamil film of 2011. A critic from Behindwoods.com noted [...] "Raja has done a fine job of mixing all the elements that go towards making a mass hero film, though he has lost the <b>balance</b> a <b>bit</b> in the last hour", while Rediff.com noted it is [...] "a masala entertainer that doesn't require you to tax your brain cells". In early 2012, Raja began pre-production on remaking the Tamil film Ramanaa (2002) into Hindi with Akshay Kumar and Tamannaah in the lead roles. He later opted out of the film after it went through delays and was not involved in the subsequent remake titled Gabbar Is Back (2015). Raja then briefly forayed into acting and played the father of quadruplets in the comedy thriller, Enna Satham Indha Neram (2014), directed by his friend, Guru Ramesh. Raja shot for the film for six days in Chennai but has not acted in any further venture, after the film had a low-key opening at the box office.|$|R
40|$|ABSTRACT — In this paper, {{we present}} {{the design of}} a low power carry-skip adder with fast {{saturation}} that achieves low power dissipation and high-performance operation. To reduce the adder’s delay and power consumption, the adder is divided into variable-sized blocks that balance the inputs to the carry chain. The optimum block sizes for minimizing the critical path delay with complementary carry generation are achieved. Within blocks, highly optimized carry-lookahead logic, which computes block generate and block propagate signals, is used to further decrease delay. The adder architecture decreases power consumption by reducing the number of logic levels, switching glitches, and transistors. To achieve <b>balanced</b> delay, input <b>bits</b> are grouped unevenly in the carry chain. This grouping reduces active power by minimizing extraneous glitching and transitions. The design has been simulated in both 0. 13 u and 90 nm CMOS technology. Worst case performance is 830 MHz and 1. 2 GHz, respectively. Worst case power dissipation normalized to 600 MHz operation is 0. 4 mW and 0. 25 mW, respectively. Extensions to the carry-skip adder allow it to quickly perform saturating addition, which is useful in a variety of digital signal processing and multimedia applications. Index Term—Low Power, Computer Arithmetic, Carry Skip Adders. 1...|$|R
5000|$|The cheek-to-shank angle also varies, {{with some}} {{straight}} up and down, others with the shanks curving backward. Some shanks have a dramatic S-curve. Cheek angle influences the angle at which the bit engages and thus way the horse carries its head. Therefore, the type of shank needs to be considered according {{to the use of}} the horse. Horses that maintain a more vertical head position, such as dressage horses and western horses trained in the [...] "straight up" [...] or Vaquero tradition generally use a curb bit with straighter shanks. Those that have a nose-out head position when working, such as cutting and roping horses, more commonly use a more curved shank. Shanks on certain western bits that curve back are sometimes called a [...] "grazing bit." [...] Though a horse should never be allowed to graze in a bridle, the term came from the mistaken notion that the turned-back shank was to allow the horse to eat with a bridle. In reality, the design simply allowed the horse to comfortably travel with its nose well ahead of the vertical. An S curve in a shank does not have a major effect on the angle at which the rein engages, but may alter the <b>balance</b> of the <b>bit</b> at the point the lever arm joins the mouthpiece.|$|R
40|$|International audiencePDC {{drill bit}} {{performances}} in hard rock {{has been greatly}} improved during the last decades by innovations in PDC wear, impact resistance and better vibrations understanding. The bit design is generally done by <b>balancing</b> the <b>bit,</b> distributing uniform wear along the profile and achieving high drillability and steerability. To obtain required drilling performances, drill bit designer adjust features such as profile shape, gage and mainly cutter characteristics (shape, type and orientation). Cutter rock interaction model became a critical feature in the design process. But previously used models considered only three forces on a cutter based on the cutter-rock contact area : drag force, normal force and side force. Such models are no longer valid {{with the introduction of}} PDC cutters with chamfer and special shape. This paper presents a new cutter rock interaction model including some several improvements. It is based on the presence of a build-up edge of crushed materials on the cutting face often described in the literature. In addition, the chamfer, which significantly affects bit Rate Of Penetration (ROP), is taken into account (shape and size). Forces applied {{on the back of the}} cutter and due to the rock deformation and back flow of crushed materials are considered in the model. Finally, results of numerous single cutter tests (under atmospheric and confining pressure) are presented and compared to the new cutter rock interaction model predictions. An analysis of the influence of the PDC characteristics (shape, size, chamfer, back and side rake angles, [...] ) is presented. The model has been applied to optimize the cutting efficiency and bit steerability and some design rules are given to minimize the specific energy and maximize the rate of penetration. Finally, full scale laboratory drilling tests and field results indicate that the use of accurate cutter rock interaction model can help the drill bit designer to find the best drill bit for a specific application. Standard laboratory full scale drilling procedures have been developed. The tests have shown that drillability, stability, steerability and wear can be improved and controlled by acting on the cutter characteristics, cutter setup, trimmer characteristics and gage type...|$|R
40|$|This thesis investigates {{advanced}} {{signal processing}} concepts and their application to geometric processing and transformations {{of images and}} volumes. In the first part, we discuss the class of transformations that project volume data onto a plane using parallel line integrals; {{it is called the}} X-ray transform. In computer tomography (CT) the problem is to reconstruct the volume from these projections. We consider a basic setup with parallel projection and a geometry model in which the CT scanner rotates around one main axis of the volume. In this case, the problem is separable and reduces to the reconstruction of parallel images (slices of the volume). Each image is reconstructible from a series of 1 D projections taken at different angular positions. The standard reconstruction algorithm is the filtered back-projection (FBP). We propose an alternative discretization of the Radon transform and of its inverse that is based on least-squares approximation and the convolution of splines. It improves the quality of the transform significantly. Next we discuss volume rendering based on the X-ray transform. The volume is represented by a multiresolution wavelet decomposition. The wavelets are projected onto an adaptive multiresolution 2 D grid. The multiresolution grid allows to speed up the rendering process especially at coarse scales. In {{the second part of the}} thesis, we discuss transformations that warp images. In computer graphics, this is called texture mapping. Simple warps, such as shear, rotation, or zoom, can be computed by least-squares sampling, e. g. again with convolutions of splines. For more general warps there is not an easy continuous solution, if an analytical one exists at all. After a review of existing texture mapping methods, we propose a novel recursive one, which minimizes information loss. For this purpose, the texture is reconstructed from the mapped image and compared to the original texture. This algorithm outperforms the existing methods in terms of quality, but its complexity can be very high. A multiresolution version of the algorithm allows to keep the storage requirements and computational complexity within acceptable range. Fast transmission of textures and 3 D models over communication links requires low bitrate and progressive compression. We can achieve very low bitrate if we code textures only with the information necessary for a given view of the 3 D scene. If the view changes, the missing information will be transmitted. This results in a progressive bitstream for an animated 3 D scene. In contrast to recursive texture mapping, we do not back-project the texture, but come up with a heuristic that predicts the information loss. This concept is called view-dependent scalability and we show how to apply it on DCT-based (as part of MPEG- 4) and wavelet-based coders. Last, we inspect the question of how to <b>balance</b> the <b>bit</b> budget of a jointly coded mesh and texture for the progressive and view-dependent transmission of a 3 D model. By exhaustive search, we find the rate-distortion optimal path. By marginal analysis we find a close solution but at much lower costs (only two evaluated frames per step as compared to a full search) ...|$|R
40|$|Another {{objective}} of this work is to study the perceptual optimized video object coding. Since MPEG- 4 treats a scene as a composition of video objects that are separately encoded and decoded, such a flexible video coding framework {{makes it possible to}} code different video objects with different priorities. It is necessary to analyze the priorities of video objects according to their intrinsic properties and psycho-visual characteristics such that the bit budget can be distributed properly to video objects to improve the perceptual quality of the compressed video. An object-level visual attention model is developed to automatically obtain the visual attention information of video objects. The visual attention values of video objects are calculated and incorporated in the newly developed dynamic bit allocation mechanism to improve the objective quality of the high priority objects such that the perceptual quality of the overall picture can be maximized. As strict rate control algorithms used in video coding sacrifice the quality consistency, the rate distortion tradeoff is important to achieve a <b>balance</b> between the <b>bit</b> rate and quality. A novel separable rate distortion modeling method is proposed to analyze the rate distortion characteristics of the color video signal. This method provides higher estimation accuracy when compared to the non-separable modeling method. To achieve rate distortion tradeoff in H. 264 /AVC, a new control strategy is presented. The feedback from the encoder buffer is analyzed by a control-theoretic adaptation approach to avoid buffer overflow and underflow. A novel rate distortion tradeoff controller is designed by considering both the quality variation and buffer fluctuation. Smooth video quality is achieved and the relevant constraints are satisfied. Due to the unique features of the video object coding such as both texture and shape introducing distortions and video objects being of arbitrarily shapes, the rate distortion analysis and optimization strategies are different from the traditional rectangular frame-based techniques. Two new rate distortion modeling methods are proposed for the shape coding. The first one is a linear rate distortion modeling method. The computational complexity is low and the estimation is accurate. To further improve the modeling performance, a novel statistical learning based method is proposed by incorporating shape features to provide rate distortion analysis for the shape coding. Therefore, a joint texture-shape rate distortion modeling approach is derived by integrating the texture and shape rate distortion models. The new joint texture-shape distortion models provide the basis for optimal bit allocation for the video object coding to minimize the coding distortion with the bit rate constraint and stabilize the buffer fullness. The major contribution of this optimal bit allocation scheme is to provide a unified solution for the following two problems: how to allocate bits between the texture and shape and how to distribute the hit budget for multiple video objects, simultaneously. This thesis addresses rate distortion analysis, optimization, and control problems in video coding. These rate distortion issues not only provide the theoretical background but also are concerned with the practical design for video coding systems. The main {{objective of}} this thesis is to consider the problems associated with analyzing the rate distortion characteristics of the video source and providing optimal solutions or tradeoffs for the rate and distortion in video coding systems. More specifically this thesis focuses on both the object-based video coding system, MPEG- 4, and the rectangular frame-based video coding system, H. 264 /AVC. Chen, Zhenzhong. "July 2007. "Adviser: King Ngi Ngan. Source: Dissertation Abstracts International, Volume: 69 - 02, Section: B, page: 1194. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2007. Includes bibliographical references (p. 225 - 247). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstract in English and Chinese. School code: 1307...|$|R

