172|114|Public
60|$|As I was {{ignorant of}} the month in which Sir Percival was born, I began my <b>backward</b> <b>search</b> with {{the early part of}} the year. The register-book was of the old-fashioned kind, the entries being all made on blank pages in manuscript, and the {{divisions}} which separated them being indicated by ink lines drawn across the page at the close of each entry.|$|E
5000|$|Forward and <b>backward</b> <b>search</b> {{to switch}} between the sources and the PDF ...|$|E
5000|$|In {{order to}} {{demonstrate}} that this algorithm retrieves shortest paths, consider it by contradiction: let's assume (for forward search, identical thing stands for <b>backward</b> <b>search)</b> that there exists a path that is shorter than the one we found with this algorithm: ...|$|E
60|$|Stood {{the tall}} Archangel {{weighing}} All man's dreaming, doing, saying, All the failure and the pain, All the triumph and the gain, In the unimagined years, Full of hopes, more full of tears, Since old Adam's hopeless eyes <b>Backward</b> <b>searched</b> for Paradise, And, instead, the flame-blade saw Of inexorable Law.|$|R
5000|$|... <b>backward</b> {{chaining}} <b>search,</b> possibly {{enhanced by}} the use of state constraints (see STRIPS, graphplan) ...|$|R
40|$|Abstract. One of {{the most}} {{relevant}} succinct suffix array proposals in the literature is the Compressed Suffix Array (CSA) of Sadakane [ISAAC 2000]. The CSA needs n(H 0 + O(log log σ)) bits of space, where n is the text size, σ is the alphabet size, and H 0 the zero-order entropy of the text. The number of occurrences of a pattern of length m can be computed in O(m log n) time. Most notably, the CSA does not need the text separately available to operate. The CSA simulates a binary search over the suffix array, where the query is compared against text substrings. These are extracted from the same CSA by following irregular access patterns over the structure. Sadakane [SODA 2002] has proposed using <b>backward</b> <b>searching</b> on the CSA in similar fashion as the FM-index of Ferragina and Manzini [FOCS 2000]. He {{has shown that the}} CSA can be searched in O(m) time whenever σ = O(polylog(n)). In this paper we consider some other consequences of <b>backward</b> <b>searching</b> applied to CSA. The most remarkable one is that we do not need, unlike all previous proposals, any complicated sub-linear structures based on the four-Russians technique (such as constant time rank and select queries on bit arrays). We show that sampling and compression are enough to achieve O(m log n) query time using less space than the original structure. It is also possible to trade structure space for search time. Furthermore, the regular access pattern of <b>backward</b> <b>searching</b> permits an efficient secondary memory implementation, so that the search can be done with O(m log B n) disk accesses, being B the disk block size. Finally, it permits a distributed implementation with optimal speedup and negligible communication effort...|$|R
50|$|The {{simplest}} classical planning(see Automated Planning) algorithms are {{state space}} search algorithms. Theseare search algorithms in which the search space is {{a subset of the}} state space: Eachnode corresponds to a state of the world, each arc corresponds to a state transition,and the current plan corresponds to the current path in the search space.Forward Search and <b>Backward</b> <b>Search</b> are two of main samples of state space planning.|$|E
5000|$|The Reverse Hierarchy Theory (RHT), {{proposed}} by Ahissar & Hochstein, aims to link between learning dynamics and specificity {{and the underlying}} neuronal sites. RHT proposes that naïve performance is based on responses at high-level cortical areas, where crude, categorical level representations of the environment are represented. Hence initial learning stages involve understanding global aspects of the task. Subsequent practice may yield better perceptual resolution {{as a consequence of}} accessing lower-level information via the feedback connections going from high to low levels. Accessing the relevant low-level representations requires a <b>backward</b> <b>search</b> during which informative input populations of neurons in the low level are allocated. Hence, subsequent learning and its specificity reflect the resolution of lower levels. RHT thus proposes that initial performance is limited by the high-level resolution whereas post-training performance is limited by the resolution at low levels. Since high-level representations of different individuals differ due to their prior experience, their initial learning patterns may differ. Several imaging studies are in line with this interpretation, finding that initial performance is correlated with average (BOLD) responses at higher-level areas whereas subsequent performance is more correlated with activity at lower-level areas. RHT proposes that modifications at low levels will occur only when the <b>backward</b> <b>search</b> (from high to low levels of processing) is successful. Such success requires that the <b>backward</b> <b>search</b> will [...] "know" [...] which neurons in the lower level are informative. This [...] "knowledge" [...] is gained by training repeatedly on a limited set of stimuli, such that the same lower-level neuronal populations are informative during several trials. Recent studies found that mixing a broad range of stimuli may also yield effective learning if these stimuli are clearly perceived as different, or are explicitly tagged as different. These findings further support the requirement for top-down guidance in order to obtain effective learning.|$|E
50|$|Jack Scantlin of Scantlin Electronics, Inc. (SEI) {{developed}} the Quotron I system, {{consisting of a}} magnetic tape storage unit that could be sited at a brokerage and Desk Units with a keyboard and printer. The storage unit recorded {{the data from the}} ticker line. Brokers could enter the stock symbol on a desk unit. This triggered a <b>backward</b> <b>search</b> on the magnetic tape (which continued recording incoming ticker data). When a transaction was located, the price was sent to the desk unit, which printed it on a tape.|$|E
3000|$|... is found, and its {{motion vector}} is calculated. Once the motion vector is obtained, the {{interpolated}} frame can {{be filled by}} simply using bidirectional motion compensation. Due to block-matching techniques not being ideal, forward and <b>backward</b> <b>searches</b> usually do not produce the same results, {{and they need to}} be averaged. This scheme holds as long as the block has constant velocity. However, when there is large or asymmetric motion, MCTI fails to generate a good SI estimate.|$|R
40|$|Figure 1 : Novel views {{synthesized}} {{using different}} approaches. Since {{the scene is}} sparsely sampled using 16 images, the color interpolation-based approach only keeps part of the scene (background wall) in focus {{and the rest is}} blurry. With additional estimated disparity information, the forward warping-based approach gives shaper result, but artifacts show up due to errors in the disparity maps. Using the same set of disparity maps, our approach yields better result. This paper presents a backward disparity-based rendering algorithm, which runs at real-time speed on programmable graphics hardware. The algorithm requires only a handful of image samples of the scene and estimated noisy disparity maps, whereas most existing techniques need either dense samples or accurate depth information. To color a given pixel in the novel view, a <b>backward</b> <b>searching</b> process is conducted to find the corresponding pixels from the closest four reference images. The use of <b>backward</b> <b>searching</b> process makes the algorithm more robust to errors in estimated disparity maps than existing forward warping-based approaches. In addition, since the computations for different pixels are independent, they can be performed in parallel on the Graphics Processing Units of modern graphics hardware. Experiment results demonstrate that our algorithm can synthesize accurate novel views for dynamic real scenes at a high frame rate...|$|R
40|$|Abstract. In this {{contribution}} {{we study}} how to adapt <b>Backward</b> Plan <b>search</b> to the Logics of Communication and Change (LCC). These are dynamic epistemic logics with common knowledge modeling {{the way in}} which announcements, sensing and world-changing actions modify the beliefs of agents or the world itself. The proposed LCC planning system greatly expands the social complexity of scenarios involving cognitive agents that can be solved. For example, goals or plans may consist of a certain distribution of beliefs and ignorance among agents. Our results include: soundness and completeness of <b>backward</b> plan <b>search</b> (under BFS), both for deterministic planning and strong non-deterministic planning. ...|$|R
50|$|In the {{mathematical}} optimization method of dynamic programming, backward induction {{is one of}} the main methods for solving the Bellman equation. In game theory, backward induction is a method used to compute subgame perfect equilibria in sequential games. The only difference is that optimization involves just one decision maker, who chooses what to do at each point of time, whereas game theory analyzes how the decisions of several players interact. That is, by anticipating what the last player will do in each situation, it is possible to determine what the second-to-last player will do, and so on. In the related fields of automated planning and scheduling and automated theorem proving, the method is called <b>backward</b> <b>search</b> or backward chaining. In chess it is called retrograde analysis.|$|E
40|$|Heuristic search with reachability-based {{heuristics}} {{is arguably}} the most successful paradigm in Automated Planning to date. In its earlier stages of development, heuristic search was proposed as both forward and <b>backward</b> <b>search.</b> Due to the disadvantages of <b>backward</b> <b>search,</b> in the last decade researchers focused mainly on forward search, and <b>backward</b> <b>search</b> was abandoned for the most part as a valid alternative. In the last years, important advancements regarding both the theoretical understanding and the performance of heuristic search have been achieved, applied mainly to forward search planners. In this work we revisit regression in planning with reachabilitybased heuristics, trying to extrapolate to <b>backward</b> <b>search</b> current lines of research that were not as well understood as they are now. ...|$|E
3000|$|<b>Backward</b> <b>search</b> accessibility: By the Property 1 of Definition 1, if {{the latest}} {{released}} group search key is [...]...|$|E
40|$|This thesis {{presents}} {{new methods}} for exploring data using visualization techniques. The {{first part of}} the thesis develops a procedure for visualizing the sampling variability of a plot. The motivation behind this development is that reporting a single plot of a sample of data without a description of its sampling variability can be uninformative and misleading {{in the same way that}} reporting a sample mean without a confidence interval can be. Next, the thesis develops a method for simplifying large scatter plot matrices, using similar techniques as the above procedure. The second part of the thesis introduces a new diagnostic method for regression called <b>backward</b> selection <b>search.</b> <b>Backward</b> selection <b>search</b> identifies a relevant feature set and a set of influential observations with good accuracy, given the difficulty of the problem, and additionally provides a description, in the form of a set of plots, of how the regression inferences would be affected with other model choices, which are close to optimal. This description is useful, because an observation, that one analyst identifies as an outlier, could be identified as the most important observation in the data set by another analyst. The key idea behind <b>backward</b> selection <b>search</b> has implications for methodology improvements beyond the realm of visualization. This is described following the presentation of <b>backward</b> selection <b>search.</b> Real and simulated examples, provided throughout the thesis, demonstrate that the methods developed in the {{first part of the}} thesis will improve the effectiveness and validity of data visualization, while the methods developed in the second half of the thesis will improve analysts' abilities to select robust models. by Rajiv Menjoge. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Sloan School of Management, Operations Research Center, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 97 - 103) ...|$|R
50|$|On the asteroid, {{the real}} Koenig regains consciousness. He {{slips into the}} {{discarded}} jacket, realising it is <b>backward.</b> <b>Searching</b> the labyrinth for an exit, he hears the high-pitched keening—then a voice, informing him he is a prisoner of Kalthon. It relates how its civilisation was destroyed when a black sun formed in this galaxy and began absorbing all energy from the nearby solar systems. Before the end, a 'seed' was prepared and launched into space. This asteroid, the Heart of Kalthon, contains the remnants of their world suspended in microcosm—cities, people, animal and vegetable life—awaiting regeneration. With the energy about to be stolen from the Alphans, that process will commence. Programmed to survive, the intelligence is completely amoral.|$|R
5000|$|Although in the {{description}} of the generic push-relabel algorithm above, [...] is set to zero for each node u other than [...] and [...] at the beginning, it is preferable to perform a <b>backward</b> breadth-first <b>search</b> from [...] to compute exact labels.|$|R
30|$|In our {{proposed}} Modified bidirectional BFS algorithm {{shown in}} Table 1, user has {{to enter the}} intermediate entity along with the source and destination entities. The algorithm returns the semantic association paths only if both forward and <b>backward</b> <b>search</b> {{meet in the middle}} and path pass through the user-specific intermediate entity. We have assigned different colours to identify the examined entity for both forward and <b>backward</b> <b>search.</b> When the forward search meets the entity which is already examined by the <b>backward</b> <b>search,</b> we can say that these two searches meet in the middle. When we examine the entity, we set the flag value as 1, if it meets the user-specific intermediate entity. The flag value has to reset as 0, when the new search starts. It helps to identify whether the path passes through the user-specific intermediate entity or not.|$|E
40|$|Keyword {{search is}} one of the most {{friendly}} and intuitive information retrieval methods. Using the keyword search to get the connected subgraph has a lot of application in the graph-based cognitive computation, and it is a basic technology. This paper focuses on the top-k keyword searching over graphs. We implemented a keyword search algorithm which applies the <b>backward</b> <b>search</b> idea. The algorithm locates the keyword vertices firstly, and then applies <b>backward</b> <b>search</b> to find rooted trees that contain query keywords. The experiment shows that query time is affected by the iteration number of the algorithm...|$|E
40|$|We {{will discuss}} the {{shortcomings}} of known variable and value ordering strategies for Graphplan's <b>backward</b> <b>search</b> phase, and propose a novel strategy {{that is based on}} a notion of the difficulty of achieving the corresponding subgoal. The difficulty of achievement is quantified in terms {{of the structure of the}} planning graph itself [...] specifically, the earliest level of the planning-graph at which that subgoal appears. We will present empirical results showing the surprising effectiveness of this simple heuristic on benchmark problems. We will end by contrasting the way distance-based heuristics are used in Graphplan and state-search planners like UNPOP, HSP and HSP-R. 1 Introduction It has been known for sometime now that the <b>backward</b> <b>search</b> of Graphplan algorithm can be seen as solving a (dynamic) CSP problem [8; 17]. Given this relation, the order in which the <b>backward</b> <b>search</b> considers goals for expansion [...] the socalled "variable ordering heuristic", and the order in whic [...] ...|$|E
40|$|This paper {{introduces}} a general <b>backward</b> proof <b>search</b> strategy for multiplicative additive linear logic. This strategy, {{which is based}} on Isabelle's basic tactics and tacticals, has been implemented and appears to be rather efficient. Its efficiency derives from several heuristics that we introduce in the paper. We prove that these heuristics preserve completeness...|$|R
40|$|In this paper, {{a review}} {{of the state of the}} art of theory {{associated}} with the young and quickly evolving Design Science Research (DSR) paradigm is presented. The core of the review consists of a structured literature search covering the senior scholars’ basket of eight from 1977 until the end of 2016, which resulted in data set of 196 sources. An iterative, selective coding of the title and abstracts revealed four major grounded clusters (138 papers). Three clusters (93 papers) were selected for co-citation analysis and augmented with additional forward and <b>backward</b> <b>searches.</b> The co-citation analysis affords an objective look at the current state of theory use in DSR and allows for the systematic identification of research opportunities. Altogether, the paper presents a multi-grounded DSR approach to literature reviews and contributes a reliable platform for further analysis and development of the DSR paradigm...|$|R
40|$|In {{content-based}} image retrieval, precision {{is usually}} {{regarded as the}} top metric used for performance measurement. With image databases reaching {{hundreds of millions of}} records, it is apparent that many retrieval strategies will not scale. Data representation and organization has to be better understood. This paper focuses on: (a) feature selection and optimal representation of features and (b) multidimensional tree indexing structure. The paper proposes the use of a forward and conditional <b>backward</b> <b>searching</b> feature selection algorithm. The data is then put through a minimum description length based optimal non-uniform bit allocation algorithm {{to reduce the size of}} the stored data, while preserving the structure of the data. The results of our experiments show that the proposed feature selection process with a minimum description length based non-uniform bit allocation method gives a system that improves retrieval time and precision...|$|R
40|$|A feature {{selection}} procedure {{is used to}} successively remove features one-by-one from a statistical classifier by an iterative <b>backward</b> <b>search.</b> Each classifier uses a smaller subset of features than the classifier in the previous iteration. The classifiers are subsequently combined into a cascade. Each classifier in the cascade should classify cases to which a reliable class label can be assigned. Other cases should be propagated to the next classifier which uses also {{the value of a}} new feature. Experiments demonstrate the feasibility of building cascades of classifiers (neural networks for prediction of atrial fibrillation (FA)) using a <b>backward</b> <b>search</b> scheme for {{feature selection}}. < 3 1999 Elsevier Science B. V. All rights reserved...|$|E
40|$|We {{present a}} method for {{exploiting}} the symmetry in the planning graph structure and certain redundancies inherent in the Graphplan algorithm, so as to improve its <b>backward</b> <b>search.</b> The main insight underlying our method is that due to these features the <b>backward</b> <b>search</b> conducted at level k + 1 of the graph is essentially a replay of the search conducted at the previous level k with certain welldefined extensions. Our method consists of maintaining a pilot explanation structure capturing the failures encountered at previous levels of the search, and using it in an intelligent way to guide the search at the newer levels. The standard EBL and DDB techniques can be employed to contro...|$|E
40|$|We {{present a}} method for {{accelerating}} explicit-state <b>backward</b> <b>search</b> algorithms for systems of arbitrarily many finite-state threads. Our method statically analyzes the program executed by the threads {{for the existence of}} simple loops. We show how such loops can be collapsed without approximation into Presburger arithmetic constraints that symbolically summarize the effect of executing the <b>backward</b> <b>search</b> algorithm along the loop in the multi-threaded program. As a result, the subsequent explicit-state search does not need to explore the summarized part of the state space. The combination of concrete and symbolic exploration gives our algorithm a concolic flavor. We demonstrate the power of this method for proving and refuting safety properties of unbounded-thread programs. Comment: arXiv admin note: substantial text overlap with arXiv: 1505. 0263...|$|E
3000|$|... aOur {{literature}} review {{was based on}} a Web of Science search using the terms “time preference(s)”, “behavio(u)r”, “smoking”, “obesity” and “sex”. Several other papers were identified by performing <b>backward</b> literature <b>searches</b> on the papers from our initial list. We are confident that all studies of particular importance in the subject area of our paper were identified and included in the paper.|$|R
40|$|Background: There {{has been}} a {{significant}} increase in the availability of online programs for alcohol problems. A systematic review of the research evidence underpinning these programs is timely. Objectives: Our objective was to review the efficacy of online interventions for alcohol misuse. Systematic searches of Medline, PsycINFO, Web of Science, and Scopus were conducted for English abstracts (excluding dissertations) published from 1998 onward. Search terms were: (1) Internet, Web*; (2) online, computer*; (3) alcohol*; and (4) E, trial*, random* (where * denotes a wildcard). Forward and <b>backward</b> <b>searches</b> from identified papers were also conducted. Articles were included if (1) the primary intervention was delivered and accessed via the Internet, (2) the intervention focused on moderating or stopping alcohol consumption, and (3) the study was a randomized controlled trial of an alcohol-related screen, assessment, or intervention. Results: The literature search initially yielded 31 randomized controlled trials (RCTs), 17 of which met inclusion criteria. Of these 17 studies, 12 (70. 6...|$|R
40|$|Context: In-store food {{marketing}} {{can influence}} food-purchasing behaviors and warrants increased attention given the dramatic rise in obesity. Descriptive and {{experimental studies of}} key marketing components have been conducted by consumer scientists, marketing researchers, and public health experts. This review synthesizes research and publications from industry and academic sources and provides direction for developing and evaluating promising interventions. Evidence acquisition: Literature sources for the review were English-language articles published from 1995 to 2010, identifıed from multidisciplinary <b>search</b> indexes, <b>backward</b> <b>searches</b> of cited articles, review articles, industry reports, and online sources. Only articles that focused on physical grocery stores and food products were included. Data collection occurred in 2010 and 2011. Evidence synthesis: Articles were classifıed in the categories of product, price, placement, and promotion and divided into controlled laboratory experiments, observation, and fıeld experiments; 125 primary peer-reviewed articles met the inclusion criteria. Narrative synthesis methods were used. Key fındings were synthesized by category of focus and study design...|$|R
40|$|Although {{the deep}} {{affinity}} between Graphplan's <b>backward</b> <b>search,</b> {{and the process}} of solving constraint satisfaction problems has been noted earlier, these relations have hither-to been primarily used to adapt CSP search techniques into the <b>backward</b> <b>search</b> phase of Graphplan. This paper describes GP-CSP, a system that does planning by automatically converting Graphplan's planning graph into a CSP encoding, and solving the CSP encoding using standard CSP solvers. Our comprehensive empirical evaluation of GP-CSP demonstrates that it is quite competitive with both standard Graphplan and Blackbox system, which compiles planning graphs into SAT encodings. We discuss the many advantages offered by focusing on CSP encodings rather than SAT encodings, including the fact that by exploiting implicit constraint representations, GP-CSP tends to be less susceptible to memory blow-up associated with methods that compile planning problems into SAT encodings. Our work is inspired by t [...] ...|$|E
40|$|The {{requirements}} analysis of critical software components often involves {{a search for}} hazardous states and failure modes. This paper describes the integration of a forward search for consequences of reaching these forbidden modes with a <b>backward</b> <b>search</b> for contributing causes. Results are reported from two projects in which the integrated search method was {{used to analyze the}} requirements of critical spacecraft software. The search process was found to be successful in identifying some ambiguous, inconsistent, and missing requirements. More importantly, it identified four significant, unresolved requirements issues involving complex system interfaces and unanticipated dependencies. The results suggest that recent efforts by researchers to integrate forward and <b>backward</b> <b>search</b> have merit. 1. INTRODUCTION There are software programs onboard spacecraft that must autonomously detect, identify, and oversee the recovery of the spacecraft from faults during flight. Since these faults can t [...] ...|$|E
40|$|We {{provide a}} {{reconstruction}} of Blum and Furst's Graphplan algorithm, {{and use the}} reconstruction to extend and improve the original algorithm in several ways. In our reconstruction, the process of growing the planning-graph and inferring mutex relations corr esponds to doing forward state-space refinement over disjunctively represented plans. The <b>backward</b> <b>search</b> phase of Graphplan corresponds to solving a binary dynamic constraint satisfaction problem. Our reconstruction sheds light on the sources of strength of Graphplan. We also use the reconstruction to explain how Graphplan can be made goaldirected, {{how it can be}} extended to handle actions with conditional effects, and how backward state-space refinement can be generalized to apply to disjunctive plans. Finally, we discuss how the <b>backward</b> <b>search</b> phase of Graphplan can be improved by applying techniques from CSP literature, and by teasing apart planning and scheduling (resource allocation) phases in Graphplan. 1. Introduction Blum [...] ...|$|E
40|$|We {{consider}} the modal logics KT and S 4, the tense logic K t, and the fragment IPC (^;!) of intuitionistic logic. For these logics <b>backward</b> proof <b>search</b> {{in the standard}} sequent or tableau calculi does not terminate in general. In terms of the respective Kripke semantics, there are several kinds of non-termination: loops inside a world (KT), innite resp. looping branches (S 4, IPC (^;!)), and innite branching degree (K t). We give uniform sequent-based calculi that contain specically tailored loop-checks such that the eciency of proof search is not deteriorated. Moreover all these loop-checks are easy to implement and can be combined with optimizations. Note that our calculus for S 4 is not a pure contraction-free sequent calculus, but this (theoretical) defect does not hinder its application in practice. 1 Introduction For many non-classical propositional logics, <b>backward</b> proof <b>search</b> in the usual sequent calculi does not terminate in general. For all the logics we consider in th [...] ...|$|R
40|$|Transient {{stability}} constrained optimal {{power flow}} (TSCOPF) {{is an important}} tool for power system planning and operation. In the paper, a discontinuous non-convex TSCOPF model with many minima is proposed to fully consider the generation valve-point effects and prohibited operation zones (POZs). Though the model is hard, it is general and flexible not only to accommodate any complex system components and discrete control variables, but also to be solved with any {{state of the art}} heuristic solution methods. Furthermore, an improved Group Search Optimization (IGSO) algorithm based on <b>backward</b> <b>searching</b> strategy, Cauchy mutation and inheritance operator is tailored to solve this challenging discontinuous non-convex problem. Four comprehensive case studies on the WSCC 9 -bus system, New England 39 -bus system, and IEEE 145 -bus system have confirmed the validity of proposed discontinuous non-convex TSCOPF model and the outperformance of IGSO solution method compared with seven representative artificial intelligence algorithms. Department of Electrical Engineerin...|$|R
40|$|The {{original}} publication {{is available}} at www. springerlink. comThe multi-stage coordinated attack (MSCA) bring many challenges to the security analysts due to their special temporal an spacial characteristics. This paper presents a two-sided model, Janus, to characterize and analyze the the behavior of attacker and defender in MSCA. Their behavior is firstly formulated as Multi-agent Partially Observable Markov Decision Process (MPO-MDP), an ANTS algorithm is then developed {{from the perspective of}} attacker to approximately search attack schemes with the minimum cost, and another <b>backward</b> <b>searching</b> algorithm APD-BS is designed from the defender’s standpoint to seek the pivots of attack schemes in order to effectively countermine them by removing those key observations associated with the system state estimates. Two case studies are conducted to show the application of our models and algorithms to practical scenarios, some preliminary analysis are also given to validate their performance and advantages. Zonghua Zhang, Pin-Han Ho, Xiaodong Lin and Hong She...|$|R
