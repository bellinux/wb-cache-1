0|827|Public
5000|$|The empty {{allocation}} {{is always}} EF. But {{if we want}} some efficiency in addition to EF, then the decision problem <b>becomes</b> <b>computationally</b> hard: ...|$|R
5000|$|While GPS solved simple {{problems}} such as the Towers of Hanoi that could be sufficiently formalized, it could not solve any real-world problems because search was easily lost in the combinatorial explosion. Put another way, the number of [...] "walks" [...] through the inferential digraph <b>became</b> <b>computationally</b> untenable. (In practice, even a straightforward state space search such as the Towers of Hanoi can <b>become</b> <b>computationally</b> infeasible, albeit judicious prunings of the state space {{can be achieved by}} such elementary AI techniques as alpha-beta pruning and min-max.) ...|$|R
3000|$|As an aside, {{note that}} due to the product in the hypoexponential (see Eq.  2) {{determination}} of the probabilities for large N can <b>become</b> <b>computationally</b> intractable. For simulations larger than with [...]...|$|R
30|$|Statistical {{inference}} {{provides an}} elegant framework {{to deal with}} this problem, but these methods are usually intended for single instrument f 0 estimation (typically piano), as exact inference often <b>becomes</b> <b>computationally</b> intractable for complex and very different sources.|$|R
50|$|However, this quickly <b>becomes</b> <b>computationally</b> prohibitive: {{if there}} are 100 binary variables, then one needs to sum over 299 ≈ 6.338 × 1029 {{possible}} values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.|$|R
40|$|When {{parallel}} architectures, parallel databases, {{and wireless}} networks are scaled to larger configurations, sequential simulations <b>become</b> <b>computationally</b> intractable. UCLA researchers have addressed this {{problem with a}} simulation environment that supports a diverse set of algorithms and architectures and provides a visual programming interface...|$|R
30|$|Finding {{the best}} {{rotations}} and permutation requires a search over i̇Q^N_U angle vectors θ and over NU! row permutations of H, which <b>becomes</b> <b>computationally</b> prohibitive {{for a large}} number of users. Therefore, some reduced-complexity searches will be explored, at the expense of a performance penalty.|$|R
50|$|To {{avoid this}} attack, the output {{length of the}} hash {{function}} used for a signature scheme can be chosen large enough so that the birthday attack <b>becomes</b> <b>computationally</b> infeasible, i.e. about twice as many bits as are needed to prevent an ordinary brute-force attack.|$|R
30|$|For {{the fuzzy}} logic, {{especially}} type 2 fuzzy logic system, reports {{have shown that}} it easily <b>becomes</b> <b>computationally</b> intensive (Abe 2004), hence requiring more time for execution when applied on high-dimensional data (Mendel 2003), and performs poorly when applied on datasets of small size (Mendel 2003; Pratama et al. 2012).|$|R
5000|$|... {{where the}} sum ranges over all {{outcomes}} as likely as, or less likely than, that observed. In practice this <b>becomes</b> <b>computationally</b> onerous as [...] and [...] increase {{so it is}} probably only worth using exact tests for small samples. For larger samples, asymptotic approximations are accurate enough and easier to calculate.|$|R
30|$|It can be {{seen from}} (23) that the {{recursive}} algorithm does not <b>become</b> <b>computationally</b> less complex. However, it does require lesser memory compared to the original algorithm of [11] and the result improves with {{an increase in the number}} of data blocks processed. The performance almost matches that of the algorithm of [11].|$|R
40|$|We {{study the}} optimal {{strategy}} for selling multiple items {{in a setting}} where bidders can bid for individual items and for subsets of items. There is a large literature examining search problems with a single good, but few papers that generalize the problem to vector offers for subsets of items. One of the chal-lenges in extending to multiple goods is that, as the state space expands, the problem can <b>become</b> <b>computationally</b> intractable. We show how to solve the dy-namic optimization problem so that it <b>becomes</b> <b>computationally</b> feasible {{when the number of}} items is not too large. We also consider special cases of the model, including an “additive ” case and a “single item ” case, and present sev-eral intuitive structural results about the optimal policy and value functions for these specialized cases. Finally, we consider extensions to a stopping rule problem...|$|R
25|$|Tissues may {{be assumed}} to be in series, where {{dissolved}} gas must diffuse through one tissue to reach the next, which has different solubility properties, in parallel, where diffusion {{into and out of}} each tissue is considered to be independent of the others, and as combinations of series and parallel tissues, which <b>becomes</b> <b>computationally</b> complex.|$|R
30|$|Our results clearly {{indicate}} that the convergence of the realisation of the sum-over-trips framework by either the four-classes or the length-priority method strongly depends on a dendritic geometry. For real morphologies, the number of trips required quickly becomes very large {{to the point where}} guaranteeing convergence to within some small error threshold may <b>become</b> <b>computationally</b> expensive.|$|R
40|$|The {{possibility}} {{of reducing the}} sampling point density in the numerical evaluation of radiation integrals is discussed by resorting to asymptotic high-frequency technique concepts. It is shown that the numerical evaluation of the radiation integrals <b>becomes</b> <b>computationally</b> more efficient by introducing an adaptive sampling. By this approach the number of sampling points results drastically smaller than the standard Nyquist sampling rate...|$|R
5000|$|LpO {{cross-validation}} requires {{training and}} validating the model [...] times, where n {{is the number}} of observations in the original sample, and where [...] is the binomial coefficient. For p > 1 and for even moderately large n, LpO CV can <b>become</b> <b>computationally</b> infeasible. For example, with n = 100 and p = 30 = 30 percent of 100 (as suggested above), ...|$|R
3000|$|... [...]. Therefore, {{to get a}} {{sufficiently}} small discretization error, one must choose very small time-steps, {{which means that the}} method <b>becomes</b> <b>computationally</b> expensive and also causes a stronger increase of round-off errors. However, there exists stable time-integration methods of arbitrary high order. They are of implicit Runge-Kutta quadrature type (see e.g. [1 – 5]), and belong to the class of A-stable methods, i.e. the eigenvalues [...]...|$|R
30|$|In (A type) and (B type), i.e., n= 4 and n= 7, (2) {{formulated}} by {{the conversion of}} H-representation is faster than (3), whereas in (C type), (3) is approximately 2 times faster than (2). In fact, as we can expect form Table 1, the number of linear inequalities in (2) considerably increases. Consequently, the evaluation of computed solutions at each iteration <b>becomes</b> <b>computationally</b> intensive.|$|R
3000|$|For g > 20, the {{stochastic}} blockmodel’s {{likelihood function}} <b>becomes</b> <b>computationally</b> intractable. For these networks, maximum simulated likelihood (MSL) as {{implemented in the}} id software system [60] is used to fit the parameters. This implementation of MSL requires a distance metric between two networks. For this purpose, the metric of [64] is employed because its run-time is linear in g. The marginal, multivariate distribution of Y [...]...|$|R
40|$|Computational {{complexity}} {{results are}} obtained for decentralized discrete-event system problems. These results generalize the earlier work of Tsitsiklis, who showed that for centralized supervisory control problems (under partial observation), solution existence is decidable in polynomial {{time for a}} special type of problem but <b>becomes</b> <b>computationally</b> intractable for the general class. As {{in the case of}} centralized control, there is no polynomial-time algorithm for producing supervisor solutions...|$|R
40|$|In {{this paper}} we propose an {{adaptive}} integration algorithm, based on asymptotic high-frequency concepts, for the numerical evaluation of surface radiation integrals. It is shown that the numerical evaluation of radiation integrals <b>becomes</b> <b>computationally</b> more efficient by introducing an adaptive sampling. By this approach the number of sampling points {{is found to be}} drastically smaller than that resulting from a standard Nyquist sampling rate...|$|R
40|$|Two {{fundamental}} problems- {{scheduling and}} routing- present {{themselves in the}} networking field. Although some ”easy” routing algorithms based upon single constraint optimization exist, we will see that routing with multiple constraints <b>becomes</b> <b>computationally</b> difficult. In order to quantify their intractability, {{a brief overview of}} complexity theory will first be presented and then applied to routing algorithms. Issues surrounding optical routing and QoS-aware routing conclude the lecture. I...|$|R
40|$|Models for {{long-term}} planning {{often lead to}} infinite horizon stochastic pro-grams that offer significant challenges for computation. Finite-horizon approxi-mations are often used in these cases but they may also <b>become</b> <b>computationally</b> difficult. In this paper, we directly solve for stationary solutions of infinite hori-zon stochastic programs. We show that a successive linear approximation method converges to an optimal stationary solution for the case with convex objective, linear dynamics, and feasible continuation...|$|R
50|$|Since the {{mathematical}} problems stemming from nontrivial fleet scheduling easily <b>become</b> <b>computationally</b> unsolvable, the PONTIFEX idea consisted in a seamless merge of algorithms and heuristic knowledge embedded in rules. The system, based on domain knowledge collected from airliners Alitalia, KLM, Swissair, and TAP Portugal, was first adopted by Swissair and Alitalia {{in the late}} 1980s, then also by the Italian railroad national operator, for their cargo division. It is still in use today (2008).|$|R
40|$|We {{present a}} study of {{deformed}} nuclei {{in the framework of}} the sdg interacting boson model utilizing both numerical diagonalization and analytical $ 1 /N$ expansion techniques. The focus is on description of high-spin states which have recently <b>become</b> <b>computationally</b> accessible through the use of computer algebra in the $ 1 /N$ expansion formalism. A systematic study is made of high-spin states in rare-earth and actinide nuclei. Comment: 38 pages, latex, 27 postscript figures available upon reques...|$|R
40|$|In {{this paper}} we present an analytical, parameter-free, Petrov-Galerkin method that gives stable {{solutions}} of convection dominated boundary-value problems. We {{call it the}} Best Approximation Weighted Residuals (BAWR) method since it gives the best approximation in the norm induced by the inner-product used to build the weighted-residuals approximation. The method computes the optimal weighting functions by solving suitable adjoint problems. Moreover, through a localization technique it <b>becomes</b> <b>computationally</b> efficient without loosing accuracy. The analysis is confirmed by numerical results...|$|R
40|$|Wavelength Division Multiplexed {{switching}} {{networks are}} considered {{as an important}} candidate for the future transport networks. As the size of network increases conventional methods used in teletraffic theory to model these networks <b>become</b> <b>computationally</b> difficult to handle as the state space grows exponentially. In this research we have applied overflow analysis to model these networks. Our results show that moment analyses using equivalent random theory (ERT) results in accurate approximations for the modeling of WDM switching networks...|$|R
30|$|If {{we were to}} {{use this}} {{analysis}} approach for design, we could, in principle, “run the forward problem” many times, for many candidate morphologies, and invoke a variety of optimization methods to determine “what the picture should look like”. Although for simple single-physics problems such an approach might be feasible; such an approach for a multi-physics analysis would be akin to the many body problem (which often involves additional nonlinear response associated with coupling, etc.), which would quickly <b>become</b> <b>computationally</b> intractable and unpredictable.|$|R
40|$|The {{simultaneous}} {{adjustment of}} very large nets of overlapping plates covering the celestial sphere <b>becomes</b> <b>computationally</b> feasible {{by virtue of}} a twofold process that generates a system of normal equations having a bordered-banded coefficient matrix, and solves such a system in a highly efficient manner. Numerical results suggest that when a well constructed spherical net is subjected to a rigorous, simultaneous adjustment, the exercise of independently established control points is neither required for determinancy nor for production of accurate results...|$|R
40|$|Representing the {{explicit}} state space of performance models has inherent difficulties. Just as the state-space explosion effects functional correctness evaluation, {{so it can}} also be easily a problem in performance models. In particular, classical Markov chain analysis of any variety requires exploration of the global state space and, even for a simple system, this quickly <b>becomes</b> <b>computationally</b> infeasible. Fluid and mean-field analysis techniques attempt to side-step the state-space explosion and provide a computationally cheap way of analysing certain features of Markov chains. ...|$|R
30|$|Correct {{interpretation}} of measured planetary characteristics for interior models {{relies on the}} understanding of high-pressure and high-temperature phase mineralogy. With the use of physical and chemical laws which govern the crystal structure of minerals, e.g. thermodynamic relations and density functional theory, ambient-condition crystalline properties can be extrapolated to high pressures and high-pressure phases can be predicted (e.g. [15, 16]). However, such methods <b>become</b> <b>computationally</b> expensive and impractical for systems containing numerous elements, and have difficulties in predicting material properties in the high-temperature regime of deep planetary interiors.|$|R
40|$|Influence {{diagrams}} {{provide a}} compact way to represent problems {{of decision making}} under uncertainty. As the number of variables in the problem increases, computing exact expectations and making optimal decisions <b>becomes</b> <b>computationally</b> intractable. A new method of action selection is presented, based on variational approximate inference. A policy is approximated where high-probability actions under the policy have high utility. Actions are then selected which have high probability under the approximating policy. The variational action selection method is shown to compare favorably to greedy and sampling-based action selection...|$|R
40|$|Partial wave {{analysis}} is a core tool in hadron spectroscopy. With the high statistics data available at facilities {{such as the}} Beijing Spectrometer III, this procedure <b>becomes</b> <b>computationally</b> very expensive. We have successfully implemented a framework for performing partial wave analysis on graphics processors. We discuss the implementation, the parallel computing frameworks employed and the performance achieved, {{with a focus on}} the recent transition to the OpenCL framework. Comment: 6 pages, 2 figures, prepared for the proceedings of Computing in High Energy Physics (CHEP) 201...|$|R
40|$|In {{order to}} study 4 -body atomic {{collisions}} such as excitation-ionization, transfer with target excitation, and double electron capture, {{the calculation of}} a nine-dimensional numerical integral is often required. This calculation can <b>become</b> <b>computationally</b> expensive, especially when calculating fully differential cross sections (FDCS), where the positions and momenta of all the particles are known. We have developed a new technique for calculating FDCS using fewer computing hours, but more memory. This new technique allows for much more efficient calculations {{and the use of}} many fewer resources...|$|R
40|$|We {{establish}} simple {{analytical and}} numerical methods for propagating stochastic price processes backwards in time, step by step, {{to the initial}} value while satisfying all cross-sectional and serial requirements. This proves useful in dealing with complex path-dependent options with American triggers, where storing {{the history of the}} underlying can <b>become</b> <b>computationally</b> onerous. Examples involving the Wiener, Ornstein-Uhlenbeck, Clark, and Cox-Ingersoll-Ross processes illustrate our techniques. In particular, we examine the use of the “just-in-time” method for the pricing of mortgages with embedded prepayment and default options...|$|R
40|$|The identiﬁcation of signiﬁcant {{changes in}} systemresource {{behaviors}} is mandatory for an efﬁcient managementof data centers. As {{the dimension of}} modern data centersincreases, the evaluation of state change detections throughtraditional algorithms <b>becomes</b> <b>computationally</b> intractable. We propose a novel approach that characterizes the statisticalproperties of the resource measures coming from systemmonitors, classiﬁes them, and signals a change only whenthere is modiﬁcation of the resource classiﬁcation. This methoddiminishes the computational complexity and reaches the samedetection accuracy of traditional approaches as demonstratedby several results obtained in real enterprise data centers...|$|R
