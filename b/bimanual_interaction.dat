75|21|Public
40|$|The availability, and popularity, {{of touch}} screen tablets is {{drastically}} increasing with over 30 % of internet users now owning one. However {{the lack of}} <b>bimanual</b> <b>interaction</b> in touch screen tablets is presenting product designers with serious challenges. Several {{attempts have been made}} to facilitate <b>bimanual</b> <b>interaction</b> in such products but results are not comparable to that of their non-mobile cousins, e. g. laptops. This paper presents the finding of a group collaboration aimed at prototyping a mobile touch screen device which supports <b>bimanual</b> <b>interaction</b> during internet browser navigation through rear mounted inputs. The researchers found it problematic to add basic bimanual interactions for internet browser navigation to the rear of a prototype mobile touch screen device due to issues regarding grip type, finger movement and hand position. This paper concludes that in order to achieve <b>bimanual</b> <b>interaction</b> researchers need to return to basics and consider how to free the hand and fingers from current constraints...|$|E
40|$|In {{everyday}} life people skillfully use both hands in complex {{tasks such as}} driving a car or drawing a picture. However, when attempting tasks on a computer, we are normally restricted to using our dominant hand for direct manipulation. <b>Bimanual</b> <b>interaction</b> {{is the study of}} how systems can be de- veloped to allow users to take advantage of their capacity for skilled <b>bimanual</b> <b>interaction.</b> Four design principles for developing <b>bimanual</b> <b>interaction</b> systems are distilled from a review of the current research. Principle One discusses the importance of understanding how people perform bimanual actions. Principle Two discusses what type of devices and actions should be used. Principle Three describes how <b>bimanual</b> <b>interaction</b> techniques can be used to eliminate modes. Principle Four discusses how <b>bimanual</b> <b>interaction</b> techniques can be used to increase usable screen space and reduce time to target and attention switching. The principles are used to develop two systems (Bi-DM and Bi-Draw). These systems are evaluated with their equivalent traditional unimanual sys- tems (Uni-DM and Uni-Draw). Bi-DM was slightly faster than Uni-DM though the di erence was just outside the 95 % signi cance level. Bi-Draw was signi cantly slower than Uni-Draw. The users were required to complete the tasks approximately. An experiment with an expert user of Bi-Draw showed that similar times to Uni-Draw can be achieved. These results disagreed with prior work that showed <b>bimanual</b> <b>interaction</b> systems are more e cient than their equivalent unimanual versions. A third experiment based on earlier work was conducted. The users were required to complete tasks with a high accuracy level. Comparing the results from this experiment to the one on which it was based indicates that the low quality of the non-preferred mouse was probably responsible for the poor performance of our bimanual systems. Other likely factors detrimentally a ecting the bimanual results include the low accuracy required for completing the tasks and the short training periods...|$|E
40|$|Figure 1. <b>Bimanual</b> <b>interaction</b> with BiPad: a) {{navigating}} a PDF, b) {{shifting to}} uppercase, c) zooming on a map. The non-dominant support hand can tap, make gestures or perform chords, thus modifying interaction {{by the dominant}} hand. Despite the demonstrated benefits of <b>bimanual</b> <b>interaction,</b> most tablets use just one hand for interaction, to free the other for support. In a preliminary study, we identified five holds that permit simultaneous support and interaction, and noted that users frequently change position to combat fatigue. We then designed the BiTouch design space, which introduces a support function in the kinematic chain model for interacting with hand-held tablets, and developed BiPad, a toolkit for creating bimanual tablet interaction with the thumb or the fingers of the supporting hand. We ran a controlled experiment to explore how tablet orientation and hand position affect three novel techniques: bimanual taps, gestures and chords. Bimanual taps outperformed our one-handed control condition in both landscape and portrait orientations; bimanual chords and gestures in portrait mode only; and thumbs outperformed fingers, but were more tiring and less stable. Together, BiTouch and BiPad offer new opportunities for designing <b>bimanual</b> <b>interaction</b> on hand-held tablets...|$|E
40|$|Stacking {{physical}} documents {{is one of}} {{the main}} forms of spatio-temporal organization of information. We present DisplayStacks, a system that enables physical stacking of digital documents via piles of flexible E Ink displays. With a conductive dot pattern sensor attached to the flexible display, we dynamically track the position and orientation of these displays in relation to one another. We introduce mechanisms for interacting with these physical stacks for access and manipulation of information using asymmetric <b>bimanual</b> <b>interactions,</b> such as providing contextual overviews. Initial user experiences indicate a preference for linear overlaps as a stacking configuration...|$|R
40|$|In {{this paper}} we {{introduce}} a novel device, called iObject, which {{is equipped with}} tactile and motion tracking sensors that allow {{for the evaluation of}} human and robot grasping and manipulation actions. Contact location and contact force, object acceleration in space (6 D) and orientation relative to the earth (3 D magnetometer) are measured and transmitted wirelessly over a Bluetooth connection. By allowing human-human, human-robot and robot-robot comparisons to be made, iObject is a versatile tool for studying manual interaction. To demonstrate the efficiency and flexibility of iObject for the study of <b>bimanual</b> <b>interactions,</b> we report on a physiological experiment and evaluate the main parameters of the considered dual-handed manipulation task. ...|$|R
40|$|Locomotion in {{vertebrates}} and invertebrates has a {{long history}} in research as the most prominent example of interlimb coordination. However, the evolution towards upright stance and gait has paved the way for a bewildering variety of functions in which the upper limbs interact with each other in a context-specific manner. The neural basis of these <b>bimanual</b> <b>interactions</b> has been investigated in recent years on different scales, ranging from the single-cell level to the analysis of neuronal assemblies. Although the prevailing viewpoint has been to assign bimanual coordination to a single brain locus, more recent evidence points to a distributed network that governs the processes of neural synchronization and desynchronization that underlie the rich variety of coordinated functions. The distributed nature of this network accounts for disruptions of interlimb coordination across various movement disorders. status: publishe...|$|R
40|$|In {{this paper}} {{we present a}} {{comparative}} study of free-hand pointing, an absolute remote pointing device. Unimanual and <b>bimanual</b> <b>interaction</b> were tested {{as well as the}} static reference system (spatial coordinates are fixed in the space in front of the TV) and novel body-aligned reference system (coordinates are bound to the current position of the user). We conducted a point-and-click experiment with 12 participants. We have identified the preferred interaction areas for left- and right-handed users in terms of hand preference and preferred spatial areas of the interaction. In <b>bimanual</b> <b>interaction,</b> the users relied more on dominant hand, switching hands only when necessary. Even though the remote pointing device was faster than the free-hand pointing, it was less accepted probably due to its low precision...|$|E
40|$|This paper {{focuses on}} the {{evaluation}} of virtual reality (VR) interaction techniques for exploration of data warehouse (DW). The experimental DW involves hierarchical levels and contains information about customers profiles and related purchase items. A user study {{has been carried out}} to compare two navigation and selection techniques. Sixteen volunteers were instructed to explore the DW and look for information using the interaction techniques, involving either a single WiimoteTM (monomanual) or both WiimoteTM and NunchuckTM (bimanual). Results indicated that the <b>bimanual</b> <b>interaction</b> technique is more efficient in terms of speed and error rate. Moreover, most of the participants preferred the <b>bimanual</b> <b>interaction</b> technique and found it more appropriate for the exploration task. We also observed that males were faster and made less errors than females for both interaction techniques...|$|E
40|$|Touchpad and {{touchscreen}} interaction {{using multiple}} fingers {{is emerging as}} a valuable form of high-degree-of-freedom input. While <b>bimanual</b> <b>interaction</b> has been extensively stud-ied, touchpad interaction using multiple fingers of the same hand is not yet well understood. We describe two experi-ments on user perception and control of multi-touch inter-action using one and two hands. The first experiment ad-dresses how to maintain perceptual-motor compatibility in multi-touch interaction, while the second measures the sep-arability of control of degrees-of- freedom in the hands and fingers. Results indicate that two-touch interaction using two hands is compatible with control of two points, while two-touch interaction using one hand is compatible with control of a position, orientation, and hand-span. A slight advan-tage is found for two hands in separating the control of two positions. Author Keywords Multi-touch input, <b>bimanual</b> <b>interaction,</b> high-degree-of...|$|E
40|$|We {{explore a}} novel {{interaction}} technique using force-feedback devices for creative manipulation of 3 D models that combines interactive mesh deformation with painting a deformation property map. Painting a deformation map permits {{the artist to}} easily assign different values of “malle-ability ” to specific areas of the model in order to locally manipulate the deformation behavior during interactive sculpting. The technique uses Phantom force-feedback de-vices for direct 3 D interactions with the 3 D object and to provide the artist with immediate haptic and visual feed-back. Using a second haptic device allows us to explore complex <b>bimanual</b> <b>interactions</b> in which both hands con-tribute to the sculpting process. One example of this biman-ual interaction is painting the malleability value onto the model with one hand while simultaneously deforming it with the second hand. These complex multi-sensory interac-tions are an important step towards empowering digital 3 D artists with efficient and easy-to-use 3 D tools. Author Keywords Digital 3 D shapes, geometric modeling, real-time, biman...|$|R
40|$|We have {{implemented}} an augmented reality videoconferencing system that inserts virtual graphics overlays into the live video stream of remote conference participants. The virtual objects are manipulated using a novel <b>interaction</b> technique cascading <b>bimanual</b> tangible <b>interaction</b> and eye tracking. User studies prove that our user interface enriches remote collaboration by offering hitherto unexplored ways for collaborative object manipulation such as gaze controlled raypicking of remote physical and virtual objects...|$|R
40|$|MouseLight is a spatially-aware aware {{standalone}} mobile projector {{with the}} form factor {{of a mouse}} {{that can be used}} in combination with digital pens on paper. By interacting with the projector and the pen bimanually, users can visualize and modify the virtually augmented ugmented contents on top of the paper, and seamlessly transition between virtual and physical information. We present a high fidelity hardware prototype of the system and demonstrate a set of novel interactions specifically tailored to the unique properties of MouseLight. MouseLight differentiates itself from related systems such as PenLight in two aspects. First, MouseLight presents a rich set of <b>bimanual</b> <b>interactions</b> inspired by the ToolGlass interaction metaphor, but applied to physical paper. Secondly, our system explores novel displaced interactions, that take advantage of the independent input and output that is spatially aware of the underneath paper. These properties enable users to issue remote commands such as copy and paste or search. We also report rt on a preliminary evaluation of the system, which produced encouraging observations and feedback...|$|R
40|$|Precise {{alignment}} of graphical objects and creating proper layouts {{is crucial in}} many domains, such as graphic design or graph editing. In this paper we are presenting a multi-touch alignment guide for interactive displays. It allows adjusting the alignment and spacing of graphical objects by multi-touch input and <b>bimanual</b> <b>interaction.</b> ACM Classification: H 5. 2 [Information interfaces an...|$|E
40|$|I present mouse-based, symmetric, <b>bimanual</b> <b>interaction</b> {{techniques}} {{as a solution}} to both the lack of spatial input and the lack of natural interaction techniques for direct manipulation in desktop interfaces. I outline the techniques I have implemented and tested thus far and the techniques and interfaces yet to be developed as part of my doctoral thesis...|$|E
40|$|International audienceDespite the {{demonstrated}} {{benefits of}} <b>bimanual</b> <b>interaction,</b> most tablets use just one hand for interaction, {{to free the}} other for support. In a preliminary study, we identified five holds that permit simultaneous support and interaction, and noted that users frequently change position to combat fatigue. We then designed the BiTouch design space, which introduces a support function in the kinematic chain model for interacting with hand-held tablets, and developed BiPad, a toolkit for creating bimanual tablet interaction with the thumb or the fingers of the supporting hand. We ran a controlled experiment to explore how tablet orientation and hand position affect three novel techniques: bimanual taps, gestures and chords. Bimanual taps outperformed our one-handed control condition in both landscape and portrait orientations; bimanual chords and gestures in portrait mode only; and thumbs outperformed fingers, but were more tiring and less stable. Together, BiTouch and BiPad offer new opportunities for designing <b>bimanual</b> <b>interaction</b> on hand-held tablets...|$|E
40|$|The {{virtual studio}} {{is a form}} of Mixed Reality {{environment}} for creating television programmes, where the (real) actor appears to exist within an entirely virtual set. The work presented in this thesis evaluates the routes required towards developing a virtual studio that extends from current architectures in allowing realistic interactions between the actor and the virtual set in real-time. The methodologies and framework presented in this thesis is intended to support future work in this domain. Heuristic investigation is offered as a framework to analyse and provide the requirements for developing interaction within a virtual studio. In this framework a group of experts participate in case study scenarios to generate a list of requirements that guide future development of the technology. It is also concluded that this method could be used in a cyclical manner to further refine systems postdevelopment. This leads to the development of three key areas. Firstly a feedback system is presented, which tracks actor head motion within the studio and provides dynamic visual feedback relative to their current gaze location. Secondly a real-time actor/virtual set occlusion system that uses skeletal tracking data and depth information to change the relative location of virtual set elements dynamically is developed. Finally an interaction system is presented that facilitates real-time interaction between an actor and the virtual set objects, providing both single handed and <b>bimanual</b> <b>interactions.</b> Evaluation of this system highlights some common errors in mixed reality interaction, notably those arising from inaccurate hand placement when actors perform <b>bimanual</b> <b>interactions.</b> A novel two stage framework is presented that measures the magnitude of the errors in actor hand placement, and also, the perceived fidelity of the interaction from a third person viewer. The first stage of this framework quantifies the actor motion errors while completing a series of interaction tasks under varying controls. The second stage uses examples of these errors to measure the perceptual tolerance of a third person when viewing interaction errors in the end broadcast. The results from this two stage evaluation lead to the development of three methods for mitigating the actor errors, with each evaluated against its ability to aid in the visual fidelity of the interaction. It was discovered that the adapting the size of the virtual object was effective in improving the quality of the interaction, whereas adapting the colour of any exposed background did not have any apparent effects. Finally a set of guidelines based on these findings is provided to recommend appropriate solutions that can be applied for allowing interaction within live virtual studio environments that can easily be adapted for other mixed reality systems...|$|R
40|$|Research {{has shown}} that <b>bimanual</b> <b>interactions</b> occur in tasks such as {{asymmetric}} reaching (Kelso et al, 1979), tapping rhythmically (Helmuth & Ivry, 1996) and orthogonal movements (Franz & Ramachandran, 1998). We conducted an experimental study, using a within-subjects design, to investigate coupling effects in real, imagined and phantom limb movement, {{as well as the}} impact of virtual visual feedback. In asymmetric reaching tasks we found a significant effect of real movement (F[1, 20] = 6. 225, p 0. 006), because of the high level of overlap in the data. In rhythmic tapping tasks, we did not find the effect described by Helmuth and Ivry in either real or imagined movement in the controls (p> 0. 05). However, the amputee participants showed a clear bimanual advantage (F[1, 33] = 19. 531, p 0. 05). Virtual visual feedback improved control of phantom movement, which had some impact on the coupling effects. Handedness of the participants may have {{had an impact on the}} results, and perhaps masked the interactions that occur. The impact of handedness on imagery should be carefully considered in further research. The findings suggest a complex picture of the relationships between real, imagined and phantom limb movements. They create an opening for further research into the subjective nature of imagined and phantom movements, which may grant us a better understanding of the phantom limb experience and improve our ability to treat problems that occur here...|$|R
40|$|Abstract—This paper {{presents}} a new library called MHaptic for <b>bimanual</b> haptic <b>interaction</b> within generic virtual environments. It has been {{specifically designed to}} work with a Haptic Workstation TM. MHaptic provides tools for accelerated development of virtual environment applications with haptic feedback like device calibration, user comfort improvements and access to low level parameters. Due to its integration with the Ageia PhysX library, it facilitates the dynamic animation of virtual objects. A realistic hand model based on mass-spring systems allows natural and intuitive manipulation. MHaptic is complemented by an authoring tool that associates information required for haptic feedback to existing virtual environments. The combination of the library and the authoring tool creates a framework for easy development of complex VR haptic applications. I...|$|R
40|$|International audienceVery-high-resolution wall-sized {{displays}} {{offer new}} opportunities for interacting with large data sets. While pointing on this type of display has been studied extensively, higher-level, more complex tasks such as pan-zoom navigation have received little attention. It thus remains unclear which techniques are best suited to perform multiscale navigation in these environments. Building upon empirical data gathered from studies of pan-and-zoom on desktop computers and studies of remote pointing, we identified three key factors {{for the design of}} mid-air pan-and-zoom techniques: uni- vs. <b>bimanual</b> <b>interaction,</b> linear vs. circular movements, and level of guidance to accomplish the gestures in mid-air. After an extensive phase of iterative design and pilot testing, we ran a controlled experiment aimed at better understanding the influence of these factors on task performance. Significant effects were obtained for all three factors: <b>bimanual</b> <b>interaction,</b> linear gestures and a high level of guidance resulted in significantly improved performance. Moreover, the interaction effects among some of the dimensions suggest possible combinations for more complex, real-world tasks...|$|E
40|$|International audienceSPad {{is a new}} <b>bimanual</b> <b>interaction</b> {{technique}} {{designed to}} improve productivity on multi-touch tablets: the user activates quasimodes with the thumb of the non-dominant hand while holding the device with that hand and interacts with the content with the dominant hand. The paper describes the design of SPad and a tablet application that demonstrates how it enables faster, more direct and more powerful interaction without increasing complexity...|$|E
40|$|Multi-touch {{surfaces}} and tabletops present new challenges and possibilities for text input. By basing designs on established theoretical models of <b>bimanual</b> <b>interaction,</b> {{it is possible}} to evaluate the best choice of bimanual technique for a novel form of text input. As a first step, we propose an asymmetric bimanual text entry method for the purpose of evaluation. Early results indicate that text entry performance improves more quickly using the novel method, while overall speed is very similar...|$|E
40|$|A {{hallmark}} of the age-related neural reorganization is that old versus young adults execute typical motor tasks by a more diffuse neural activation pattern including stronger ipsilateral activation during unilateral tasks. Whether such changes in neural activation are present already at middle age and affect <b>bimanual</b> <b>interactions</b> is unknown. We compared the amount of associated activity, i. e., muscle activity and force produced by the non-task hand and motor evoked potentials (MEPs) produced by magnetic brain stimulation between young (mean 24 years, n = 10) and middle-aged (mean 50 years, n = 10) subjects during brief unilateral (seven levels of % maximal voluntary contractions, MVCs) and bilateral contractions (4 × 7 levels of % MVC combinations), and during a 120 -s-long MVC of sustained unilateral index finger abduction. During the force production, the excitability of the ipsilateral (iM 1) or contralateral primary motor cortex (cM 1) was assessed. The associated activity in the “resting” hand was ~ 2 -fold higher in middle-aged (28 % of MVC) versus young adults (11 % of MVC) during brief unilateral MVCs. After controlling for the background muscle activity, MEPs in iM 1 were similar {{in the two groups}} during brief unilateral contractions. Only at low (bilateral) forces, MEPs evoked in cM 1 were 30 % higher in the middle-aged versus young adults. At the start of the sustained contraction, the associated activity was higher in the middle-aged versus young subjects and increased progressively in both groups (30 versus 15 % MVC at 120 s, respectively). MEPs were greater {{at the start of the}} sustained contraction in middle-aged subjects but increased further during the contraction only in young adults. Under these experimental conditions, the data provide evidence for the reorganization of neural control of unilateral force production as early as age 50. Future studies will determine if the altered neural control of such inter-manual interactions are of functional significance...|$|R
40|$|International audienceWhen {{interacting}} with virtual objects through haptic devices, {{most of the}} time only one hand is involved. However, the increase of computational power, along with the decrease of device costs, allow more and more the use of dual haptic devices. The field which encompasses all studies of the haptic interaction with either remote or virtual environments using both hands of the same person is referred to as bimanual haptics. It differs from the common unimanual haptic field notably due to specificities of the human bimanual haptic system, e. g., the dominance of the hands, their differences in perception and their interactions at a cognitive level. These specificities call for adapted solutions in terms of hardware and software when applying the use of two hands to computer haptics. This paper reviews {{the state of the art}} on bimanual haptics, encompassing the human factors in <b>bimanual</b> haptic <b>interaction,</b> the currently available bimanual haptic devices, the software solutions for two-handed haptic interaction, and the existing interaction techniques...|$|R
40|$|In {{this paper}} {{we present a}} {{simulator}} for two-handed haptic interaction. As an application example, we chose a medical scenario that requires simultaneous interaction with a hand and a needle on a simulated patient. The system combines <b>bimanual</b> haptic <b>interaction</b> with a physics-based soft tissue simulation. To our knowledge the combination of finite element methods for the simulation of deformable objects with haptic rendering is seldom addressed, especially with two haptic devices in a non-trivial scenario. Challenges are to find a balance between real-time constraints and high computational demands for fidelity in simulation and to synchronize data between system components. The system has been successfully implemented and tested on two different hardware platforms: one mobile on a laptop and another stationary on a semi-immersive VR system. These two platforms have been chosen to demonstrate scaleability in terms of fidelity and costs. To compare performance and estimate latency, we measured timings of update loops and logged event-based timings of several components in the software...|$|R
40|$|Abstract. This paper {{presents}} {{two studies}} investigating {{the use of}} novel modalities for bimanual vertical scrolling on tablet devices. Several <b>bimanual</b> <b>interaction</b> techniques are presented, {{using a combination of}} physical dial, touch and pressure input, which split the control of scrolling speed and scrolling direction across two hands. The new interaction techniques are compared to equivalent unimanual techniques in a controlled linear targeting task. The results suggest that participants can select targets significantly faster and with a lower subjective workload using the bimanual techniques...|$|E
40|$|Part 1 : Long and Short PapersInternational audienceThis paper {{presents}} {{two studies}} investigating {{the use of}} novel modalities for bimanual vertical scrolling on tablet devices. Several <b>bimanual</b> <b>interaction</b> techniques are presented, {{using a combination of}} physical dial, touch and pressure input, which split the control of scrolling speed and scrolling direction across two hands. The new interaction techniques are compared to equivalent unimanual techniques in a controlled linear targeting task. The results suggest that participants can select targets significantly faster and with a lower subjective workload using the bimanual techniques...|$|E
40|$|Abstract. Two-handed {{interaction}} {{is a very}} common paradigm that is gaining popularity {{in the fields of}} medical tele-operation, gaming, and large-scale design. In this paper, we validate Guiard’s theory of bimanual control for the tasks of navigation and selection. We present the related literature and the theoretical models that motivate the research, in particular Guiard’s theory of bimanual control. Two experiments are designed to verify and establish the relationship between navigation and selection in <b>bimanual</b> <b>interaction</b> based on Guiard’s theory. The contributions assist interaction designers in developing adequate tools for bimanual operation...|$|E
40|$|In this work, {{we present}} a haptic {{training}} simulator for a maxillofacial procedure comprising the controlled breaking of the lower mandible. To our knowledge the haptic simulation of fracture is seldom addressed, especially when a realistic breaking behavior is required. Our system combines <b>bimanual</b> haptic <b>interaction</b> with a simulation of the bone based on well-founded methods from fracture mechanics. The system resolves the conflict between simulation complexity and haptic real-time constraints by employing a dedicated multi-rate simulation and a special solving strategy for the occurring mechanical equations. Furthermore, we present remeshing-free methods for collision detection and visualization which are tailored for an efficient treatment of the topological changes induced by the fracture. The methods have been successfully implemented and tested in a simulator prototype using real pathological data and a semi-immersive VR-system with two haptic devices. We evaluated the computational efficiency of our methods and show that a stable and responsive haptic simulation of the fracturing has been achieved...|$|R
40|$|We extend Forlines et al. 's idea {{of mixed}} {{absolute}} and relative "HybridPointing" to large multitouch displays. In our <b>bimanual</b> "CursorTap" <b>interaction</b> technique, one hand triggers a kinaesthetic relative pointing mode {{while the other}} controls a distant cursor similar to a large touchpad. A controlled experiment compares CursorTap to standard absolute touch input, and a "Drag" technique, where all display content may be dragged to the user. Our results show CursorTap is fastest when accessing distant targets and returning to nearby targets, a common usage scenario. Overall, the measured selection times across distances is nearly flat for CursorTap, but linearly increases for the absolute and Drag techniques. As further validation, a second study explores how CursorTap is used in a more open setting. We developed a two-person game to create opportunities to use CursorTap under cooperative and competitive settings. The results demonstrate a willingness to use CursorTap, and reveal situations and input modalities where usage is most likely...|$|R
40|$|Information {{visualisation}} (infovis) {{tools are}} integral {{for the analysis}} of large abstract data, where interactive processes are adopted to explore data, investigate hypotheses and detect patterns. New technologies exist beyond post-windows, icons, menus and pointing (WIMP), such as tangible user interfaces (TUIs). TUIs expand on the affordance of physical objects and surfaces to better exploit motor and perceptual abilities and allow for the direct manipulation of data. TUIs have rarely been studied in the field of infovis. The overall aim of this thesis is to design, develop and evaluate a TUI for infovis, using expression quantitative trait loci (eQTL) as a case study. The research began with eliciting eQTL analysis requirements that identified high- level tasks and themes for quantitative genetic and eQTL that were explored in a graphical prototype. The main contributions of this thesis are as follows. First, a rich set of interface design options for touch and an interactive surface with exclusively tangible objects were explored for the infovis case study. This work includes characterising touch and tangible interactions to understand how best to use them at various levels of metaphoric representation and embodiment. These design were then compared to identify a set of options for a TUI that exploits the advantages of touch and tangible interaction. Existing research shows computer vision commonly utilised as the TUI technology of choice. This thesis contributes a rigorous technical evaluation of another promising technology, micro-controllers and sensors, as well as computer vision. However the findings showed that some sensors used with micro-controllers are lacking in capability, so computer vision was adopted {{for the development of the}} TUI. The majority of TUIs for infovis are presented as technical developments or design case studies, but lack formal evaluation. The last contribution of this thesis is a quantitative and qualitative comparison of the TUI and touch UI for the infovis case study. Participants adopted more effective strategies to explore patterns and performed fewer unnecessary analyses with the TUI, which led to significantly faster performance. Contrary to common belief <b>bimanual</b> <b>interactions</b> were infrequently used for both interfaces, while epistemic actions were strongly promoted for the TUI and contributed to participants’ efficient exploration strategies...|$|R
40|$|Abstract: Experimental {{studies of}} spatial input devices {{have focused on}} demonstrating either the superiority of 3 D input devices over 2 D input devices, or the superiority of <b>bimanual</b> <b>interaction</b> over unimanual interaction. In this paper, we argue that hybrid {{interfaces}} that combine a 3 D input device with a 2 D input device have received little attention up to now and are potentially very useful. We demonstrate {{by means of an}} experimental evaluation that working with hybrid interfaces can indeed provide superior performance compared to strictly 3 D and 2 D interfaces...|$|E
40|$|International audienceWe {{present a}} semi-immersive {{environment}} for conceptual design where virtual mockups are obtained from gestures {{we aim to}} get closer to the way people conceive, create and manipulate three-dimensional shapes. We developed on-and-above-the-surface interaction techniques based on asymmetric <b>bimanual</b> <b>interaction</b> for creating and editing 3 D models in a stereoscopic environment. Our approach combines hand and nger tracking in the space on and above a multitouch surface. This combination brings forth an alternative design environment where users can seamlessly switch between interacting on the surface or in the space above it to leverage the bene t of both interaction spaces...|$|E
40|$|Dual-arm robots provide {{efficient}} {{approach for}} automated execution of complex assembly operations. With bimanual-manipulation, a dual-arm robot can simultaneously control relative motion and interaction of assembly counterparts in a dexterous human-like manner. This requires, however, sophisticated programming and control algorithms for arms cooperation. This paper addresses {{the development of}} an advanced industrial dual-arm robot system with novel capabilities, such as easy and rapid commissioning, compliance control of <b>bimanual</b> <b>interaction</b> in all assembly process phases, as well as intuitive planning and programming. The robot can be leased and easily integrated in assembly environment sharing the same workspace with human workers...|$|E
40|$|Skillful {{manipulation}} of objects often requires the spatio-temporal coordination of both hands and, {{at the same}} time, the compensation of environmental forces. In bimanual coordination, movements of the two hands may be coupled because each hand needs to compensate the forces generated by the other hand or by an object operated by both hands (dynamic coupling), or because the two hands share the same workspace (spatial coupling). We examined how spatial coupling influences bimanual coordination, {{by looking at the}} adaptation of velocity-dependent force fields during a task in which the two hands simultaneously perform center-out reaching movements with the same initial position and the same targets, equally spaced on a circle. Subjects were randomly allocated to two groups, which differed in terms of the force fields they were exposed to: in one group (CW–CW), force fields had equal clockwise orientations in both hands; in the other group (CCW–CW), they had opposite orientations. In both groups, in randomly selected trials (catch trials) of the adaptation phase, the force fields were unexpectedly removed. Adaptation was quantified in terms of the changes of directional error for both hand trajectories. Bimanual coordination was quantified in terms of inter-limb longitudinal and sideways displacements, in force field and in catch trials. Experimental results indicate that both arms could simultaneously adapt to the two force fields. However, in the CCW–CW group, adaptation was incomplete for the movements from the central position to the more distant targets with respect to the body. In addition, in this group the left hand systematically leads in the movements toward targets on the left of the starting position, whereas the right hand leads in the movements to targets on the right. We show that these effects are due to a gradual sideways shift of the hands, so that during movements the left hand tends to consistently remain at the left of the right hand. These findings can be interpreted in terms of a neural mechanism of bimanual coordination/interaction, triggered by the force field adaptation process but largely independent from it, which opposes movements that may lead to the crossing of the hands. In conclusion, our results reveal a concurrent interplay of two task-dependent modules of motor-cognitive processing: an adaptive control module and a ‘protective’ module that opposes potentially ‘dangerous’ (or cognitively costly) <b>bimanual</b> <b>interactions...</b>|$|R
40|$|We {{present a}} virtual reality {{platform}} which addresses and integrates {{some of the}} currently challenging research topics {{in the field of}} virtual assembly: realistic and practical scenarios with several complex geometries, <b>bimanual</b> six-DoF haptic <b>interaction</b> for hands and arms, and intuitive navigation in large workspaces. We put an especial focus on our collision computation framework, which is able to display stiff and stable forces in 1 kHz using a combination of penalty- and constraint-based haptic rendering methods. Interaction with multiple arbitrary geometries is supported in realtime simulations, as well as several interfaces, allowing for collaborative training experiences. Performance results for an exemplary car assembly sequence which show the readiness of the system are provided...|$|R
40|$|We {{present the}} theory and mixed methods {{approach}} for analyzing how children’s hands can help them think during interaction. The methodology was developed for a study comparing indirect with direct input methods for object manipulation activities in digitally supported problem solving. We propose a classification scheme based on the notions of complementary and epistemic actions in spatial problem solving. In order to overcome inequities when comparing mouse input with the multi-access, bimanual input, we develop a series of relative measures based on our classification scheme. This methodology is applicable {{to a range of}} computationally augmented activities involving object manipulation. Author Keywords Input methods, tangible computing, embodied <b>interaction,</b> <b>bimanual</b> manipulation, video analysis, methodology. ACM Classification Keywords H 5. 2. User Interfaces: Evaluation/methodology...|$|R
