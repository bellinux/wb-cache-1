7|341|Public
40|$|DIMM), a CPU-DRAM {{interface}} {{that uses}} multiwavelength optical interconnects. We show that OCDIMM is more scalable and offers higher bandwidth and lower latency than FBDIMM (Fully-Buffered DIMM), a state-of-the-art electrical alternative. Though OCDIMM is more power efficient than FBDIMM, {{we show that}} ultimately the total power consumption in the memory subsystem is a key impediment to scalability and thus to achieving truly <b>balanced</b> <b>computing</b> systems in the terascale era. I...|$|E
40|$|Four {{decades ago}} Amdahl {{proposed}} {{a set of}} rules of thumb for computer architects that have withstood the test of time. One such rule of thumb is that a <b>balanced</b> <b>computing</b> system should be capable of providing one byte of memory and one byte per second of memory bandwidth for each instruction per second of computation. Building <b>balanced</b> <b>computing</b> systems in the multicore era with hundreds of processing cores per die is challenging because of the pin limitations and poor scalability of bandwidth and memory capacity with off-chip electrical interconnects between the CPU and memory subsystem. We propose using Wavelength Division Multiplexing (WDM) -based optical interconnects between the CPU and the memory subsystem to overcome the problems of pin limitations and provide both high bandwidth and high memory capacity simultaneously. We make use of concepts studied widely by long distance optical networks such as dynamic wavelength allocation and bandwidth management in our design. The main contributions of this thesis are (a) A prototype design of an optical interconnect for CPU-DRAM interface without any modifications to commodity DRAM devices. (b) A frame-based protocol for interfacing CPU and DRAM with WDM-based optical interconnects. (c) Algorithms for dynamically allocating optical resources for better utilization and more concurrent operations. We show that significant improvements in memory bandwidth and memory capacity can be achieved, by exploiting the wavelength domain concurrency offered by WDM-based interconnects. Design and Evaluation of an Optical CPU-DRA...|$|E
40|$|Distributed {{computation}} {{and storage}} {{have been widely}} used for processing of big data sets. For many big data problems, with the size of data growing rapidly, the distribution of computing tasks and related data can affect the performance of the computing system greatly. In this paper, a distributed computing framework is presented for high performance computing of All-to-All Comparison Problems. A data distribution strategy is embedded in the framework for reduced storage space and <b>balanced</b> <b>computing</b> load. Experiments are conducted to demonstrate the effectiveness of the developed approach. They have shown that about 88 % of the ideal performance capacity have be achieved in multiple machines through using the approach presented in this paper...|$|E
30|$|The {{analytical}} precision for {{the chemical}} variables {{was determined by}} the ionic <b>balance</b> <b>computed</b> between cations (Ca 2 +, Mg 2 +, Na+ and K+) and anions (HCO 3 −, Cl−, SO 4 2 −, NO 3 − and F−) as 100  × (cations − anions)/(cations + anions). This is within the acceptable limit of ±  5  % (Deutsch 1997).|$|R
50|$|A {{service is}} not {{available}} if it cannot service all the requests being placed on it. The “scale-out” property of a system refers {{to the ability to}} create multiple copies of a subsystem to address increasing demand, and to efficiently distribute incoming work to these copies (Load <b>balancing</b> (<b>computing))</b> preferably without shutting down the system. High availability software should enable scale-out without interrupting service.|$|R
50|$|The entire loop nest touches about 2*N**2 array elements, and {{performs}} about 5*T*N**2 floating-point operations.Thus, {{the overall}} <b>compute</b> <b>balance</b> (ratio of floating-point computations to floating-point memory cells used) of this entire loop nest is about 5T/2.When the <b>compute</b> <b>balance</b> {{is a function}} of problem size, as it is here, the code is said to have scalable compute balance.Here, we could achieve any <b>compute</b> <b>balance</b> we desire by simply choosing a large enough T.|$|R
40|$|Presented is a {{speculative}} server blade architecture {{called a}} FlashBlade that combines 100 x I/O performance in both latency and bandwidth with <b>balanced</b> <b>computing.</b> The blade {{consists of a}} standard multi-core CPU with attached DRAM. It uses a fast interconnect, such as Intel’s QuickPath, to communicate with a FPGA router called the X 1. This router handles traffic to the C 1 complexes and off-blade. Each C 1 complex is a System on a Chip with Package on Package DRAM, connected to local flash memory. There are numerous complexes, giving tremendous I/O performance and computational balance. A large design space of parameters such as flash size, number of complexes, and link bandwidth between each C 1 and the X 1 is available for power and performance optimization. A single blade server constructed from these blades, just 12. 25 inches high and drawing about 10 KW, could support {{a few hundred thousand}} basic web searches a second on 1 billion pages. It could also provide triple store performance 100 x greater than achievable now for datasets of 6 TB and scales to petabyte datasets although at somewhat reduced performance; with numerous applications to defense, commerce, and science...|$|E
40|$|Abstract—A {{considerable}} {{portion of}} a microprocessor chip is dedicated to cache memory. However, not all applications need all the cache storage all the time, especially the computing bandwidth-limited applications. In addition, some applications have large embedded computations with a regular structure. Such applications {{may be able to}} use additional computing resources. If the unused portion of the cache could serve these computation needs, the on-chip resources would be utilized more efficiently. This presents an opportunity to explore the reconfiguration of a part of the cache memory for computing. Thus, we propose adap-tive <b>balanced</b> <b>computing</b> (ABC) —dynamic resource configuration on demand from application—between memory and computing resources. In this paper, we present a cache architecture to convert a cache into a computing unit for either of the following two struc-tured computations: finite impulse response and discrete/inverse discrete cosine transform. In order to convert a cache memory to a function unit, we include additional logic to embed multibit output lookup tables into the cache structure. The experimental results show that the reconfigurable module improves the execution time of applications with a large number of data elements by a factor as high as 50 and 60. Index Terms—Cache memory, reconfigurable computing. I...|$|E
40|$|The Stockpile Stewardship Program (SSP) is a single, highly {{integrated}} technical {{program for}} maintaining the safety and reliability of the U. S. nuclear stockpile. The SSP uses past nuclear test data along with current and future nonnuclear test data, computational modeling and simulation, and experimental facilities to advance understanding of nuclear weapons. It includes stockpile surveillance, experimental research, development and engineering programs, and an appropriately scaled production capability to support stockpile requirements. This integrated national program requires the continued use of current facilities and programs along with new experimental facilities and computational enhancements to support these programs. The Advanced Simulation and Computing Program (ASC) is a cornerstone of the SSP, providing simulation capabilities and computational resources to support the annual stockpile assessment and certification, to study advanced nuclear-weapons design and manufacturing processes, to analyze accident scenarios and weapons aging, and to provide the tools to enable Stockpile Life Extension Programs (SLEPs) and the resolution of Significant Finding Investigations (SFIs). This requires a balanced resource, including technical staff, hardware, simulation software, and computer science solutions. In its first decade, the ASC strategy focused on demonstrating simulation capabilities of unprecedented scale in three spatial dimensions. In its second decade, ASC is focused on increasing its predictive capabilities in a three-dimensional simulation environment while maintaining the support to the SSP. The program continues to improve its unique tools for solving progressively more difficult stockpile problems (focused on sufficient resolution, dimensionality and scientific details); to quantify critical margins and uncertainties (QMU); and to resolve increasingly difficult analyses needed for the SSP. Moreover, ASC has restructured its business model from one that was very successful in delivering an initial capability to one that is integrated and focused on requirements driven products that address long-standing technical questions related to enhanced predictive capability in the simulation tools. ASC must continue to meet three objectives: Objective 1. Robust Tools [...] Develop robust models, codes, and computational techniques to support stockpile needs such as refurbishments, SFIs, LEPs, annual assessments, and evolving future requirements. Objective 2. Prediction through Simulation [...] Deliver validated physics and engineering tools to enable simulations of nuclear-weapons performances {{in a variety of}} operational environments and physical regimes and to enable risk informed decisions about the performance, safety, and reliability of the stockpile. Objective 3. Balanced Operational Infrastructure [...] Implement a <b>balanced</b> <b>computing</b> platform acquisition strategy and operational infrastructure to meet Directed Stockpile Work (DSW) and SSP needs for capacity and high-end simulation capabilities...|$|E
50|$|The <b>balance</b> is <b>computed</b> {{after all}} profits or losses have been {{allocated}} {{in accordance with}} the partnership agreement, and the books closed.|$|R
30|$|Cation–anion <b>balance</b> was <b>computed,</b> {{taking their}} {{concentrations}} in milliequivalents per liter (meq/L), which is observed within {{the limit of}} ± 5  % (Domenico and Schwartz 1990).|$|R
40|$|This {{paper will}} discuss the entire process of {{acquiring}} and deploying Hopper from the first vendor market surveys to providing 3. 8 million hours of production cycles per day for NERSC users. Installing the latest system at NERSC has been both a logistical and technical adventure. <b>Balancing</b> <b>compute</b> requirements with power, cooling, and space limitations drove the initial choice and configuration of the XE 6, {{and a number of}} first-of- a-kind features implemented in collaboration with Cray have resulted in a high performance, usable, and reliable system...|$|R
40|$|The {{general-purpose}} computing processor {{performs a}} wide range of functions. Although the performance of general-purpose processors has been steadily increasing, certain software technologies like multimedia and digital signal processing applications demand ever more computing power. Reconfigurable computing has emerged to combine the versatility of general-purpose processors with the customization ability of ASICs. The basic premise of reconfigurability is to provide better performance and higher computing density than fixed configuration processors. Most of the research in reconfigurable computing is dedicated to on-chip functional logic. If computing resources are adaptable to the computing requirement, the maximum performance can be achieved. To overcome the gap between processor and memory technology, the size of on-chip cache memory has been consistently increasing. The larger cache memory capacity, though beneficial in general, does not guarantee a higher performance for all the applications as they may not utilize all of the cache efficiently. To utilize on-chip resources effectively and to accelerate the performance of multimedia applications specifically, we propose a new architecture [...] -Adaptive <b>Balanced</b> <b>Computing</b> (ABC). ABC uses dynamic resource configuration of on-chip cache memory by integrating Reconfigurable Functional Caches (RFC). RFC can work as a conventional cache or as a specialized computing unit when necessary. In order to convert a cache memory to a computing unit, we include additional logic to embed multi-bit output LUTs into the cache structure. We add the reconfigurability of cache memory to a conventional processor with minimal modification to the load/store microarchitecture and with minimal compiler assistance. ABC architecture utilizes resources more efficiently by reconfiguring the cache memory to computing units dynamically. The area penalty for this reconfiguration is about 50 [...] 60 % of the memory cell cache array-only area with faster cache access time. In a base array cache (parallel decoding caches), the area penalty is 10 [...] 20 % of the data array with 1 [...] 2 % increase in the cache access time. However, we save 27 % for FIR and 44 % for DCT/IDCT in area with respect to memory cell array cache and about 80 % for both applications with respect to base array cache if we were to implement all these units separately (such as ASICs). The simulations with multimedia and DSP applications (DCT/IDCT and FIR/IIR) show that the resource configuration with the RFC speedups ranging from 1. 04 X to 3. 94 X in overall applications and from 2. 61 X to 27. 4 X in the core computations. The simulations with various parameters indicate that the impact of reconfiguration can be minimized if an appropriate cache organization is selected...|$|E
30|$|Daily fluid balance was {{calculated}} by subtracting the urinary output from the fluid intake (including both IV and enteral fluid administration); each day cumulative fluid <b>balance</b> was <b>computed</b> {{by the addition of}} daily fluid balances.|$|R
5000|$|Schedule CCR - Consolidated Capital Requirements - <b>Balances</b> {{necessary}} to <b>compute</b> the OTS minimum capital requirement ...|$|R
40|$|A FETI-like Domain {{decomposition}} {{dedicated to}} Contact Dynamics is considered here, especially concerning "corner grain" treatment {{and its influence}} on the numerical simulation. Moreover, parallel implementation of Non-Smooth Contact Domain Decomposition (NSCDD) using MPI library is studied as regards of load <b>balancing</b> and <b>computing</b> performances, leading to motivate time homogenization techniques...|$|R
40|$|We {{present the}} first {{simultaneous}} measurements of HOx, NOx, and Clx radicals {{in the middle}} stratosphere obtained during a balloon flight at 34 N in September 1989, along with calculations from a photochemical model, to show that NOx catalytic cycles dominate loss of ozone (O 3) for altitudes between 24 and 38 km; the observed abundance of ClO is lower than that expected for altitudes above 30 km {{on the basis of}} models using recommended rates and cross sections, reducing the relative importance of the Clx catalytic cycles for loss of O 3; and removal rates of O 3 derived from observed concentrations of rate limiting HOx, NOx, and Clx radicals <b>balance</b> <b>computed</b> production rates for altitudes between 32 and 38 km, a region where ozone is expected to be regulated primarily by photochemical processes. 1 1...|$|R
50|$|Computer usage {{management}} method, unlike content filters, {{is focused}} on empowering the parents to <b>balance</b> the <b>computing</b> environment for children by regulating gaming. The main idea of these applications is to allow parents to enforce learning component into the computing time of children, where children must earn gaming time while working through educational contents.|$|R
30|$|Every 12  h, gas {{exchange}} (arterial and mixed venous blood), MAP, CVP, mean PAP, PCWP and CO were measured. Stroke volume (SV), systemic vascular resistance (SVR), pulmonary vascular resistance (PVR), venous admixture, venous-to-arterial partial pressure carbon dioxide difference (Pv-aCO 2) and fluid <b>balance</b> were <b>computed.</b> As previously reported [8], we computed the inotropic score and the vasopressor dependency index.|$|R
30|$|The {{performance}} of load <b>balancing</b> is <b>computed</b> using three ways of information managements: the default {{one that is}} created by caching recent information, the second version where load balancing agent alters local cache by updating it with accurate information (reinforcement), and the last version where resource discovery agent filters cached entries according to their activeness values (information preference).|$|R
40|$|Summary: A FETI-like Domain {{decomposition}} {{dedicated to}} Contact Dynamics is considered here, especially concerning“corner grain” treatment {{and its influence}} on the numerical simulation. Moreover, parallel implementation of Non-Smooth Contact Domain Decomposition (NSCDD) using MPI library is studied as regards of load <b>balancing</b> and <b>computing</b> performances, leading to motivate time homogenization techniques. hal- 00776135, version 1 - 15 Jan 201...|$|R
40|$|International audiencePhysical {{modeling}} {{sound synthesis}} methods {{are known to}} be on the one hand potentially interesting in term of sound richness and probability, {{but on the other hand}} expansive to develop and compute. This paper is dedicated to mass-interaction methods using explicit time. By studying theoretically a specific model, we define a new basic modelization toolkit using non-linear stiffness and we show how it is possible to reach a <b>balance</b> between <b>computing</b> speed and sound realism. Physical modeling sound synthesis methods {{are known to be}} on the one hand potentially interesting in term of sound richness and probability, but on the other hand expansive to develop and compute. This paper is dedicated to mass-interaction methods using explicit time. By studying theoretically a specific model, we define a new basic modelization toolkit using non-linear stiffness and we show how it is possible to reach a <b>balance</b> between <b>computing</b> speed and sound realism...|$|R
40|$|In this paper, we {{introduce}} a new efficient O(n log n) graph search based bottom-up clustering algorithm named ESC (Edge Separability based Clustering). Unlike existing bottom-up algorithms {{that are based on}} local connectivity information of the netlist, ESC exploits more global connectivity information "edge separability" to guide clustering process while carefully monitoring cluster area <b>balance.</b> <b>Computing</b> the edge separability for a given edge e = (x; y) in an edge weighted undirected graph G(V; E; s; w) is equivalent to finding the x-y mincut. Then, we show that a simple and efficient algorithm CAPFOREST [14] can be used to provide a good estimation of edge separability for all edges in G without using any network flow computation. Related experiments based on large scale ISPD 98 [1] benchmark circuits confirm that exploiting edge separability yields better quality partitioning solution compared to various bottom-up clustering algorithms proposed in the literature including Absorpt [...] ...|$|R
40|$|With the {{development}} of multi-/many-core processors, applications need to be written as parallel programs to improve execution efficiency. For data-intensive applications that use multiple threads to read/write files simultaneously, an I/O sub-system can easily become a bottleneck when too many {{of these types of}} threads exist; on the contrary, too few threads will cause insufficient resource utilization and hurt performance. Therefore, programmers must pay much attention to parallelism control to find the appropriate number of I/O threads for an application. This paper proposes a parallelism control mechanism named IOPA that can adjust the parallelism of applications to adapt to the I/O capability of a system and <b>balance</b> <b>computing</b> resources and I/O bandwidth. The programming interface of IOPA is also provided to programmers to simplify parallel programming. IOPA is evaluated using multiple applications with both solid state and hard disk drives. The results show that the parallel applications using IOPA can achieve higher efficiency than those with a fixed number of threads...|$|R
40|$|International audienceThe {{influence}} of meteorological variables on snow/ice melting has been analyzed for two very contrasting months, in summer 2006, on Glacier de Saint-Sorlin, French Alps. July 2006 was the warmest July since 1950, and August 2006 was the coldest August since 1979. The total energy available for melting {{was just over}} half as much in August as in July, due to a sharp decrease in net shortwave radiation and in turbulent flux. This decrease of net shortwave radiation was mainly controlled by a strong increase in albedo responsible for an increase of reflected shortwave radiation, {{as well as by}} a reduction in incident shortwave radiation. During the two months, net longwave radiation remained almost unchanged. The mass <b>balance</b> <b>computed</b> from energy-balance modelling or with a degree-day approach was in good agreement with measured mass balance. Differences were attributed to space and time surface aspect variations which mainly controlled the observed mass balance...|$|R
40|$|Cloud {{computing}} is {{deployed in}} the data centre where physical machine are virtualized. Cloud computing being the new technology has both advantages and disadvantages, {{one of the issues}} which cloud computing faces is load balancing. More than one virtual machine runs above the Virtualization. Load <b>balancing</b> in cloud <b>computing</b> is emerging topic which needs to be researched and study. The data centre is built with lots of systems where balancing is not an easy task especially for cloud computing. Most of the research is done in distributed environments. Using of semi-distributed load <b>balancing</b> in cloud <b>computing</b> is not discussed in any literature, wherever distributed load <b>balancing</b> on cloud <b>computing</b> is already in the list. By using the method of semidistributed load balancing we can design a new algorithm for the cloud computing. This paper proposed to design a better load balance for the cloud computing which can be applied in every central node of the cluster...|$|R
40|$|In this paper, we {{introduce}} a new efficient graph search based bottom-up clustering algorithm, named ESC (Edge Separability based Clustering), that adopts the concept of edge separability. Unlike existing bottom-up algorithms {{that are based on}} local connectivity information of the netlist, ESC exploits more global connectivity information to guide clustering process while carefully monitoring cluster area <b>balance.</b> <b>Computing</b> the edge separability for a given edge e = (x; y) in an edge weighted undirected graph G = (V; E; w) is equivalent to finding the x-y mincut. Thus, direct computation of edge separability for all edges in G requires max-flow computation for jEj times, which is extremely time-consuming, even for moderate size graphs with a few thousand vertices. However, we show in this paper that (i) a simple and efficient O(jV j log jV j) time algorithm, named CAPFOREST [13], can be used to provide a good estimation of edge separability for all edges in G without using any [...] ...|$|R
40|$|The {{influence}} of meteorological variables on snow/ice melting has been analyzed for two very contrasting months, in summer 2006, on Glacier de Saint-Sorlin, French Alps. July 2006 was the warmest July since 1950, and August 2006 was the coldest August since 1979. The total energy available for melting {{was just over}} half as much in August as in July, due to a sharp decrease in net shortwave radiation and in turbulent flux. This decrease of net shortwave radiation was mainly controlled by a strong increase in albedo responsible for an increase of reflected shortwave radiation, {{as well as by}} a reduction in incident shortwave radiation. During the two months, net longwave radiation remained almost unchanged. The mass <b>balance</b> <b>computed</b> from energy-balance modelling or with a degree-day approach was in good agreement with measured mass balance. Differences were attributed to space and time surface aspect variations which mainly controlled the observed mass balance...|$|R
5000|$|If the {{respective}} impedances {{of the branches}} of the hybrid that are connected to the conjugate sides of the hybrid are known, hybrid <b>balance</b> may be <b>computed</b> by the formula for return loss.|$|R
30|$|Integration {{of primary}} and {{secondary}} devices are impelled {{by the development of}} smart grid. Based on the ideology of {{primary and secondary}} device integration, an all-in-one device and its concept, configuration and function are proposed. Then, an all-in-one device framework of distributed substation area protection is proposed. Point-to-point model and virtual local area network (VLAN) based network-to-network model are adopted in this framework. The dual redundancy on hardware and software level is realized by modularization and protection information mirroring storage/overwriting. Considering the fact that the sampling, logic judgment and information sharing processes of protection may fail in atrocious conditions, a distributed cooperative all-in-one device cluster based substation area joint defensive protection strategy is proposed. The adaptability of sectional component failure in secondary system is strengthened by reusing module function, setting value mirroring storage and dynamic load <b>balancing</b> <b>computing</b> ability to constitute strong intelligent outdoor secondary device integrated network. Finally, the simulation examples based on EPOCHS which consider various system failure conditions are presented to verify the validity and rationality of the proposed architecture and strategy.|$|R
40|$|The Adda Ticino basin is a densely-populated area, {{dependent}} massively on groundwater resources {{not only}} for industrial uses but also for agricultural and domestic ones. A large scale hydrogeological assessment study over 8000 Km 2 area northwest of Lombardy Region was done in reason to implement a groundwater flow model. The main objective was to give a quantitative estimate of groundwater budget into the Adda-Ticino basin and to deliver a tool to support Public Authorities in regional groundwater management required by the European Directive 2000 / 60 /CE. The approach consists of improving the hydrogeological conceptual model and better define hydraulic properties of the aquifer units and all components in the basin influencing the groundwater flow (rivers, wells withdrawal). Then the collected GIS information were added into the numerical model Modflow. Regional steady-state flow was calibrated under current stress conditions with an inverse calibration code (Doherty, 2005) mainly by changing hydraulic conductivity and vertical recharge inflow. The water mass <b>balance</b> <b>computed</b> shows a total flow of 3279 Mm 3 /y. Vertical recharge provides 65...|$|R
40|$|Summary. In {{parallel}} simulations, partitioning and load-balancing algorithms {{compute the}} distribution of application data and work to processors. The effectiveness of this distribution greatly influences {{the performance of a}} parallel simulation. Decompositions that balance processor loads while keeping the application’s communication costs low are preferred. Although a wide variety of partitioning and load-balancing algorithms have been developed, their effectiveness depends on the characteristics of the application using them. In this chapter, we review several partitioning algorithms, along with their strengths and weaknesses for various PDE applications. We also discuss current efforts toward improving partitioning algorithms for future applications and architectures. The distribution of data among cooperating processes is a key factor in the efficiency of parallel solution procedures for partial differential equations (PDEs). This distribution requires a data-partitioning procedure and distributed data structures to realize and use the decomposition. In applications with constant workloads, a static partition (or static load <b>balance),</b> <b>computed</b> in a serial or parallel pre-processing step, can be used throughout the computation. Other applications, such as adaptive finit...|$|R
40|$|A {{practical}} {{user guidance}} of Runge-Kutta (R-K) integration method with {{the context of}} non-linear time dependent finite element analysis (FEA) was proposed in this paper. Following the literature review of different integration method within the finite element analysis framework, detailed numerical experiments were conducted {{to find out the}} right <b>balance</b> between <b>computing</b> accuracy and efficiency. It contributes to knowledge to the numerical analysis software development in general and specific to computational creep damage mechanics...|$|R
40|$|We {{presented}} a parallel tabu search (PTS) algorithm for the traveling salesman problem (TSP), which is NP-hard. To parallelize tabu search (TS) algorithm efficiently, the search space decomposition based on partition principle {{was used to}} <b>balance</b> the <b>computing</b> load, while exploitation in subspace had been boosted by an adaptive search strategy of intensification and diversification. Numerical results illustrated this algorithm was efficient and easy to implement. Key words: Parallel Tabu Search, Meta-heuristic, TS...|$|R
30|$|Severity {{of illness}} on ICU {{admission}} {{was described by}} an averaged simplified acute physiology (SAPS II) score [24], acute physiology and chronic health evaluation (APACHE II) score [25], and SOFA score [26]. Daily fluid balance was calculated by subtracting the fluid output (diuresis, ultrafiltration volume in case of CRRT, and any loss from drainage tubes) from the fluid intake (IV and enteral fluid administration); each day the cumulative fluid <b>balance</b> was <b>computed</b> {{by the addition of}} daily fluid balances.|$|R
40|$|An {{optimization}} {{procedure is}} presented for combined power plant heat recovery boilers (HRBs), {{which are the}} components with the greatest single impact on plant cost and efficiency. A calculation code was developed to solve the thermodynamic <b>balance</b> and <b>compute</b> the exergetic losses in a single-pressure HRB. The code was tested on three working fluids (water, ammonia, and a mixture of both). Results show that maximum energetic efficiency and maximum output are achieved separately, using different design parameter...|$|R
