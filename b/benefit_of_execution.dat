1|10000|Public
40|$|This paper {{focuses on}} {{managing}} {{the cost of}} deliberation before action. In many problems, {{the overall quality of}} the solution reflects costs incurred and resources consumed in deliberation as well as the cost and <b>benefit</b> <b>of</b> <b>execution,</b> when both the resource consumption in deliberation phase, and the costs in deliberation and execution are uncertain and may be described by probability distribution functions. A feasible (in terms of resource consumption) strategy that minimizes the expected total cost is termed computationally-optimal. For a situation with several independent, uninterruptible methods to solve the problem, we develop a pseudopolynomial-time algorithm to construct generate-and-test computationally optimal strategy. We show this strategy-construction problem to be NP-complete, and apply Bellman's Optimality Principle to solve it efficiently. Comment: Appears in Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence (UAI 1990...|$|E
40|$|This {{paper will}} {{describe}} the <b>benefits</b> <b>of</b> <b>execution</b> <b>of</b> the Asteroid Redirect Mission {{as an early}} mission in deep space, demonstrating solar electric propulsion, deep space robotics, ground and on-board navigation, docking, and EVA. The paper will also discuss how staging in trans-lunar space and the elements associated with this mission are excellent building blocks for subsequent deep space missions to Mars or other destinations...|$|R
5000|$|Documented <b>benefits</b> <b>of</b> process {{development}} <b>execution</b> systems include: ...|$|R
30|$|Finally, the <b>benefits</b> <b>of</b> {{cooperative}} <b>execution</b> {{using the}} CPU and Intel Phi 7120 P are assessed in Section 6.7. This evaluation has not included the Intel Phi 7250 {{because it is}} a standalone bootable device and, as such, it is not attached to a CPU.|$|R
40|$|A deep-space {{mission has}} been {{proposed}} to identify and redirect an asteroid to a distant retrograde orbit around the moon, and explore it by sending a crew using the Space Launch System and the Orion spacecraft. The Asteroid Redirect Crewed Mission (ARCM), which represents the third segment of the Asteroid Redirect Mission (ARM), could be performed on EM- 3 or EM- 4 depending on asteroid return date. Recent NASA studies have raised questions on how we could progress from current Human Space Flight (HSF) efforts to longer term human exploration of Mars. This paper will describe the <b>benefits</b> <b>of</b> <b>execution</b> <b>of</b> the ARM as the initial stepping stone towards Mars exploration, and how the capabilities required to send humans to Mars could be built upon those developed for the asteroid mission. A series of potential interim missions aimed at developing such capabilities will be described, and the feasibility of such mission manifest will be discussed. Options for the asteroid crewed mission will also be addressed, including crew size and mission duration...|$|R
40|$|Mobile agent {{systems are}} a {{powerful}} approach to develop distributed applications since they migrate to hosts {{on which they}} {{have the resources to}} execute individual tasks. Existing mobile agent systems require detailed knowledge about these hosts at the time of coding. This assumption is not acceptable in a dynamic environment like a peer-to-peer network, where hosts and, as a consequence, also agents become repeatedly connected and disconnected. To this end, we propose a predicatebased approach allowing the specification of hosts an agent has to migrate to. With this highly flexible approach, termed P 2 PMobileAgents, we combine the <b>benefits</b> <b>of</b> <b>execution</b> location transparency with those of code mobility. Similarly, also the recipients of messages can be specified by predicates, e. g. for synchronisation purposes. For providing meta information about agents and hosts we use XML documents. ...|$|R
40|$|Parallelizing {{a program}} can greatly speed it up. However, the {{synchronization}} primitives {{needed to protect}} shared resources result in contention, overhead, and added complexity. These costs can overwhelm the performance <b>benefits</b> <b>of</b> parallel <b>execution.</b> Since the only reason {{to go to the}} trouble of parallelizing a program is to gain performance, it is necessary to understand the performance implications of synchronization primitives in addition to their correctness, liveness, and safety properties...|$|R
40|$|This {{dissertation}} explores high-performance complexity-efficient processors {{focusing on}} VLIW processors. Complexity efficiency is a qualitative characteristic that describes {{a system where}} performance has not reached the point of diminishing returns. Using the techniques described in this dissertation, simple statically-scheduled very-long-instructionword (VLIW) processors can be efficient architectures for exploiting instruction-level parallelism and can effectively {{address the needs of}} general purpose computing. We studied the ability <b>of</b> dynamic <b>execution</b> to exploit instruction-level parallelism in dynamic VLIW processors. Unlike previous studies, this study explores the <b>benefits</b> <b>of</b> dynamic <b>execution</b> on an instruction stream with explicit instruction-level parallelism. Dynamic execution is thus applied to problems that compilers have difficulty solving rather than to those problems that compilers readily solve reducing the need for complex and costly hardware. In addition to present [...] ...|$|R
40|$|The {{focus of}} this paper is on a {{specific}} component of the capital punishment debate: delays in capital punishment. Although the legal justifications for delays in capital punishment cases, such as the right of the writ of habeas corpus, are well known, the analysis of the economic costs and <b>benefits</b> <b>of</b> delaying <b>executions</b> is limited. Fluctuations in the political consensus regarding capital punishment and resulting changes in the imposition of the death penalty in practice suggest that such costs and benefits are important to policymakers. The employment of option value theory is useful when considering the implementation of the death penalty because it involves an action that has uncertain net benefits and that is irreversible. Punishment...|$|R
50|$|The PPU is an In-Order processor, but it {{has some}} unique traits which allow it to achieve some <b>benefits</b> <b>of</b> Out-of-Order <b>execution</b> without {{expensive}} re-ordering hardware. Upon reaching an L1 cache miss - it can execute past the cache miss, stopping only when an instruction is actually dependent on a load. It can send up to 8 load instructions to the L2 cache out-of-order. It has an instruction delay pipe - a side path that allows it to execute instructions that would normally cause pipeline stalls without holding {{up the rest of}} the pipeline. The instruction delay pipeline is used for the Out-Of-Order Load/Stores: cache misses are put there while it moves on.|$|R
40|$|One can {{effectively}} utilize predicated execution to improve branch handling in instruction-level parallel processors. Although the potential <b>benefits</b> <b>of</b> predicated <b>execution</b> are high, the tradeoffs {{involved in the}} design of an instruction set to support predicated execution can be difficult. On one end of the design spectrum, architectural support for full predicated execution requires increasing the number of source operands for all instructions. Full predicate support provides for the most flexibility and the largest potential performance improvements. On the other end, partial predicated execution support, such as conditional moves, requires very little change to existing architectures. This paper presents a preliminary study to qualitatively and quantitatively address the <b>benefit</b> <b>of</b> full and partial predicated execution support. With our current compiler technology, we show that the compiler can use both partial and full predication to achieve speedup in large control-intensive pro [...] ...|$|R
40|$|The C++ {{language}} disposes of a two layer execution model: {{the static}} <b>execution</b> <b>of</b> metaprograms, and the dynamic <b>execution</b> <b>of</b> resulting programs. The Expression Templates technique <b>benefits</b> <b>of</b> this dual <b>execution</b> model, through {{the construction of}} C++ types expressing simple arithmetic formulas. Our intent is to extend this technique to a whole programming language. The Tiger language is a small, imperative language with types, variables, arrays, records, control flow structures and nested functions. First, we show how to express a Tiger program as a C++ type. The second step concerns operational analysis which is done through the use of metaprograms. Finally an implementation of our Tiger evaluator is proposed...|$|R
40|$|Although {{systems with}} {{hardware}} support for fine-grained execution migration {{are becoming a}} reality, no concrete execution model or compiler exist for these machines. This limits the complexity of software that can be written for these machines, and therefore also the scope of studies for which these machines can be used. In this thesis, we define a productive programming model for an execution migration platform by exposing migration {{as a set of}} interfaces usable with the C programming language via a custom optimizing compiler. We employ hardware-software co-design to describe a stack core architecture with support for partial context migration in order to simplify the compiler problem and improve compiler efficiency. We also consider instruction encoding in abstract terms to establish a baseline comparison of encoded instruction density to an ideal upper bound. The stack-based execution migration platform offers a new and unexplored cost model, which leads us to reevaluate the trade-offs associated with compilation for these architectures, and to explore novel algorithms, or novel applications of existing optimizations. Throughout this work, we attempt to gain a deep understanding of the costs and <b>benefits</b> <b>of</b> <b>execution</b> migration by aggressive design space exploration. We use the insight gained to better inform the the problem of compiling to this unorthodox architecture, and design the compiler, a library of optimized parallel primitives, and a set of compiler optimization passes to best reflect and utilize the underlying hardware. by Ilia Andreevich Lebedev. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (pages 137 - 141) ...|$|R
50|$|PyPy is an {{alternate}} {{implementation of the}} Python programming language written in Python. Specifically, its interpreter is written in RPython (a subset of Python). In contrast, the standard reference implementation of Python is written in C (known as CPython). The implementation of the interpreter in high level Python, over a low-level implementation in C, enables quick experimentation of new language features. This is shown to have <b>benefits</b> in areas <b>of</b> <b>execution</b> speed, memory usage, sandboxing etc., in certain use cases. The self-hosting nature of PyPy {{is reflected in the}} project's logo, which depicts a snake swallowing its own tail in an ouroboros.|$|R
40|$|In {{language}} {{systems that}} support separate compilation, the header files are internalized {{over and over}} again when the source files that depend on them are compiled. Making a compiler a longlived server eliminates such redundant processing of header files, thus reducing the compilation time. Modern JVM implementations interleave <b>execution</b> with compilation <b>of</b> “hot” methods to achieve reasonable performance. Since compilation overhead impacts the <b>execution</b> time <b>of</b> the application and induces run-time pauses, it is better to offload compilation onto a compilation server. Compilation server is the server which compiles and optimizes Java byte codes on behalf of its clients. It provides the <b>benefit</b> <b>of</b> lower <b>execution</b> and pause times due to reducing the overhead of optimization. Compilation server is able to handle more than 50 concurrent clients while still allowing them to outperform best performing adaptive configuration...|$|R
40|$|Semiconductor {{technological}} {{advances in the}} recent years have led to the inclusion <b>of</b> multiple CPU <b>execution</b> cores in a single processor package. This processor architecture is known as Multi-core (MC) or Chip Multi Processing (CMP). Any application which is well optimized and scales with SMP will take immediate <b>benefit</b> <b>of</b> the multiple <b>execution</b> cores, provided by the Multi-core architecture. Even if the application is single-threaded, multi-tasking environment will take advantage <b>of</b> these multiple <b>execution</b> cores. 2. 6 Linux kernels (which have better SMP scalability compared to 2. 4 kernels) take instant advantage of the MC architecture. MC also brings in performance optimization opportunities which will further enhance the performance. This paper captures the recent enhancements to the 2. 6 Linux Kernel which better support and enhance the performance of Multi-core capable platforms. 1...|$|R
40|$|Message-Driven Programming style {{avoids the}} use of {{blocking}} receives and allows overlap of computation and communication by scheduling processes (or objects) depending on availability of messages. Charm is a parallel programming system that uses message-driven execution to exhibit latency-tolerance. Charm supports objects whose methods can be triggered by remote objects asynchronously. Such asynchronous invocation capability endows Charm programs to tolerate communication latencies in an adaptive manner. However, many parallel objectbased applications require the object to coordinate the sequencing <b>of</b> the <b>execution</b> <b>of</b> their methods. Structured Dagger is a coordination language built on top of Charm that supports such applications by facilitating a clear expression {{of the flow of}} control within the object without losing the performance <b>benefits</b> <b>of</b> adaptive message-driven <b>execution.</b> 1 Introduction One of the daunting tasks for parallel programmers is to tolerate message l [...] ...|$|R
40|$|Component {{middleware}} {{is popular}} for enterprise distributed systems {{because it provides}} effective reuse of the core intellectual property (i. e., the “business logic”). Component-based enterprise distributed real-time and embedded (DRE) systems, however, incur new integration problems associated with component configuration and deployment. New research is therefore needed to minimize {{the gap between the}} development and deployment/configuration of components, so that deployment and configuration strategies can be evaluated well before system integration. This paper uses an industrial case study from the domain of shipboard computing to show how system execution modeling tools can provide software and system engineers with quantitative estimates of system bottlenecks and performance characteristics to help evaluate the performance of componentbased enterprise DRE systems and reduce time/effort in the integration phase. The results from our case study show the <b>benefits</b> <b>of</b> system <b>execution</b> modeling tools and pinpoint where more work is needed. 1...|$|R
40|$|The {{only reason}} to {{parallelize}} a {{program is to}} gain performance. However, the synchronization primitives needed in parallel programs can consume execessive memory bandwidths, can be subject to memory latencies, consume excessive memory, and result in unfair access or even starvation. These problems can overwhelm the performance <b>benefits</b> <b>of</b> parallel <b>execution.</b> Therefore, {{it is necessary to}} understand these performance implications of synchronization primitives in addition to their correctness, liveness, and safety properties. This paper presents a pattern language to assist you in selecting synchronization primitives for parallel programs. This pattern language assumes you have already chosen a locking design, perhaps by using a locking design pattern language [McK 95 b]. 1 Overview A lock-based parallel program uses synchronization primitives to define critical sections of code in which only one CPU at a time may execute concurrently. For example, Figure 1 presents a fragment of parall [...] ...|$|R
40|$|Parallel {{execution}} is a {{very efficient}} means of processing vast amounts of data in {{a small amount of}} time. Creating parallel applications has never been easy, and requires much knowledge of the task and the execution environment used to execute parallel processes. The process of creating parallel applications can be made easier through using a compiler that automatically parallelises a supplied application. Executing the parallel application is also simplified when a well designed execution environment is used. Such an execution environment provides very powerful operations to the programmer transparently. Combining both a parallelising compiler and execution environment and providing a fully automated parallelisation and execution tool is the aim of this research. The advantage of using such a fully automated tool is that the user does not need to provide any additional input to gain the <b>benefits</b> <b>of</b> parallel <b>execution.</b> This report shows the tool and how it transparently supports the programmer creating parallel applications and supports their execution. <br /...|$|R
40|$|Scaling {{iterative}} graph processing {{applications to}} large graphs {{is an important}} problem. Performance is critical, as data scientists need to execute graph programs many times with varying parameters. The need for a high-level, high-performance programming model has inspired much research on graph programming frameworks. In this paper, we show that the important class of computationally light graph applications – applications that perform little computation per vertex – has severe scalability problems across multiple cores as these applications hit an early “memory wall ” that limits their speedup. We propose a novel block-oriented computation model, in which computation is iterated locally over blocks of highly connected nodes, significantly improving the amount of computation per cache miss. Following this model, we describe the design and implementation of a block-aware graph processing runtime that keeps the familiar vertex-centric programming paradigm while reaping the <b>benefits</b> <b>of</b> block-oriented <b>execution.</b> Our experiments show that block-oriented execution significantly improves the performance of our framework for several graph applications. 1...|$|R
40|$|We {{present a}} parallelizing {{compiler}} for lazy functional programs that uses strictness analysis {{to detect the}} implicit parallelism within programs. It generates an intermediate functional program, where a special syntactic construct `letpar', which is semantically equivalent to the well-known let-construct, is used to indicate subexpressions for which a parallel execution is allowed. Only for sufficiently complex expressions a parallelization will be worthwhile. For small expressions the communication overhead may outweigh the <b>benefits</b> <b>of</b> the parallel <b>execution.</b> Therefore, the parallelizing compiler uses some heuristics to estimate the complexity of expressions. The distributed implementation of parallelized functional programs described in [Loogen et al. 89] enabled us to investigate the impact of various parallelization strategies on the runtimes and speedups. The strategy, which only allows the parallel <b>execution</b> <b>of</b> non-predefined function calls in strict positions, shows t [...] ...|$|R
40|$|This paper {{presents}} a formalization of {{a subset of}} the BPMN 2. 0 execution semantics in terms of graph rewrite rules. The formalization is supported by graph rewrite tools and implemented in one of these tools, called GrGen. The <b>benefit</b> <b>of</b> formalizing the <b>execution</b> semantics by means of graph rewrite rules {{is that there is a}} strong relation between the execution semantics rules that are informally specified in the BPMN 2. 0 standard and their formalization. This makes it easy to validate the formalization. Having a formalized and implemented execution semantics supports simulation, animation and <b>execution</b> <b>of</b> BPMN 2. 0 models. In particular this paper explains how to use the formal execution semantics to verify workflow engines and service orchestration and choreography engines that use BPMN 2. 0 for modeling the processes that they execute...|$|R
40|$|Sensitivity Analysis (SA) {{is a novel}} {{compiler}} {{technique that}} complements, and integrates with, static automatic parallelization analysis for the cases when relevant program behavior is input sensitive. In this paper we show how SA can extract all the input dependent, statically unavailable, conditions for which loops can be dynamically parallelized. SA generates a sequence of sufficient conditions which, when evaluated dynamically in order of their complexity, can each validate the dynamic parallel <b>execution</b> <b>of</b> the corresponding loop. For example, SA can first attempt to validate parallelization by checking simple conditions related to loop bounds. If such simple conditions cannot be met, then validating dynamic parallelization may require evaluating conditions related to the entire memory reference trace of a loop, thus decreasing the <b>benefits</b> <b>of</b> parallel <b>execution.</b> We have implemented Sensitivity Analysis in the Polaris compiler and evaluated its performance using 22 industry standard benchmark codes running on two multicore systems. In most cases we have obtained speedups superior to the Intel Ifort compiler because with SA we could complement static analysis with minimum cost dynamic analysis and extract most of the available coarse grained parallelism...|$|R
40|$|This paper {{explores the}} {{potential}} of a program representation called the program dependence graph (PDG) for representing and exposing programs' hierarchical control dependence structure. It presents several extensions to current PDG designs, including a node labeling scheme that simplifies and generalizes PDG traversal. A post-pass PDG-based tool called PEDIGREE has been implemented. It is used to generate and analyze the PDGs for several benchmarks, including the SPEC 92 suite. In particular, initial results characterize the control dependence structure of these programs to {{provide insight into the}} scheduling <b>benefits</b> <b>of</b> employing speculative <b>execution,</b> and exploiting control equivalence information. Some <b>of</b> the <b>benefits</b> <b>of</b> using the PDG instead of the CFG are demonstrated. Our ultimate aim is to use this tool for exploiting multi-grained parallelism. 1. Introduction A program representation called the program dependence graph (PDG) [6] explicitly represents all control dependences [...] ...|$|R
40|$|Abstract — Reconfigurable Arrays {{combine the}} <b>benefit</b> <b>of</b> spa-tial <b>execution,</b> typical <b>of</b> {{hardware}} solutions, {{with that of}} programmability, present in microprocessors. When mapping software applications (or parts of them) onto hardware, however, FPGAs often provide more flexibility than is needed, and do not implement coarser-level operations efficiently. Therefore, Coarse Grained Reconfigurable Arrays (CGRAs) have been proposed to this aim. While most CGRA designs feature an array cell {{of the order of}} an ALU, this paper proposes a new kind of coarse grained array, called EGRA (Expression-Grained Reconfigurable Array), featuring a cell composed of a cluster of ALUs with flexible interconnect. The EGRA attempts to further close the performance gap between reconfigurable and hardwired logic by implementing an arithmetic/logic expression per cell, rather than a single operation. A mapping methodology is proposed that can retargetably compile to a family of EGRAs, therefore enabling architectural exploration of the granularity of the proposed cell. Performance results on a number of embedded applications show that EGRAs {{can be used as a}} reconfigurable fabric for customizable processors, outperforming more traditional CGRA designs. I...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedComputer Science courses often include laboratory exercises {{to make sure}} certain concepts are experienced hands-on by the students. These courses sometimes are taken by {{a large number of}} students and each assignment needs to be graded. Instructors or teaching assistants responsible for grading assignments are presented with the tedious task of verifying students' work. Besides making sure that each student performs the assignment correctly, the assignment grader may also be concerned that students do not cheat on the assignment by copying and submitting work from other students. The objective of this thesis is to investigate and develop a framework for Linux-based cybersecurity laboratory exercises performed on individual student computers. The purpose of the framework is to provide the designer of laboratory exercises with tools to parameterize labs for each student, and automate some aspects of the grading of laboratory exercises. A prototype of this framework was implemented by making use of the Linux Containers, which provide an additional <b>benefit</b> <b>of</b> standardizing <b>execution</b> environments utilized by students and instructors. Civilian, Department of the Nav...|$|R
40|$|Abstract. This paper {{presents}} a formalization of {{a subset of}} the BPMN 2. 0 execution semantics in terms of graph rewrite rules. The formalization is supported by graph rewrite tools and implemented in one of these tools, called GrGen. The <b>benefit</b> <b>of</b> formalizing the <b>execution</b> semantics by means of graph rewrite rules {{is that there is a}} strong relation between the execution semantics rules that are informally specified in the BPMN 2. 0 standard and their formalization. This makes it easy to validate the formalization. Having a formalized and implemented execution semantics supports simulation, animation and <b>execution</b> <b>of</b> BPMN 2. 0 models. In particular this paper explains how to use the formal execution semantics to verify workflow engines and service orchestration and choreography engines that use BPMN 2. 0 for modeling the processes that they execute. ...|$|R
40|$|The {{subject of}} this thesis is to provide {{theoretical}} and knowledge basis needed for creation of communication campaign and to analyze preparation of specific communication campaign created by Vodafone Czech Republic a. s. {{in a time of}} financial crisis together with analysis of market environment and company's position at that time. The thesis depicts in detail subsequent steps of campaign preparation, from goals definition, choice <b>of</b> right <b>benefits</b> to <b>execution</b> <b>of</b> the campaign. It also evaluates the impacts of the campaign on the position of the company from the market and customer perspective...|$|R
500|$|Section 31 {{eliminated}} the <b>benefit</b> <b>of</b> clergy for capital crimes. Section 33 designated the means <b>of</b> <b>execution</b> as [...] "hanging [...] [...] [...] {{by the neck}} until dead." ...|$|R
40|$|Power {{consumption}} {{has become}} a major concern, both for processor design with high clock rates and embedded systems that rely on batteries to operate. Recent support for dynamic frequency and voltage scaling (DVS) in contemporary architectures allows software to affect power consumption by varying both execution frequency and supply voltage on the fly. However, processors generally enter a sleep state while transitioning between frequencies/voltages. In the following, we describe an experimental framework for studying DVS for a processor that continues to execute during frequency/voltage transitions. For this purpose, we developed an infrastructure for investigating hard real-time DVS schemes on the IBM PowerPC 405 LP. Task scheduling was performed using four earliest-deadline-first (EDF) DVS schemes, including our feedback real-time DVS algorithm that, prior to this work, had only been evaluated in simulation. Voltage and current of the processor core were depicted through an oscilloscope, and the energy consumption was assessed through a data acquisition board. Measurements indicate a considerable potential for real-time DVS scheduling algorithms to lower energy consumption up to 54 % over na ve DVS schemes. The <b>benefits</b> <b>of</b> continued <b>execution</b> during frequency/voltage switching provide up to 5 % energy savings for frequent switches...|$|R
40|$|International audienceData {{locality}} optimization is {{a well-known}} goal when handling programs that must run {{as fast as possible}} or use a minimum amount of energy. However, usual techniques never address the significant impact of numerous stalled processor cycles that may occur when consecutive load and store instructions are accessing the same memory location. We show that two versions of the same program may exhibit similar memory performance, while performing very differently regarding their <b>execution</b> times because <b>of</b> the stalled processor cycles generated by many pipeline hazards. We propose a new programming structure called ''xfor'', enabling the explicit control of the way data locality is optimized in a program and thus, to control the amount of stalled processor cycles. We show the <b>benefits</b> <b>of</b> xfor regarding <b>execution</b> time and energy saving...|$|R
40|$|Processing-in-memory (PIM) {{architectures}} {{have seen}} an increase in popularity recently, as the high internal bandwidth available within 3 D-stacked memory provides greater incentive to move some computation into the logic layer of the memory. To maintain program correctness, the portions of a program that are executed in memory must remain coherent with the portions of the program that continue to execute within the processor. Unfortunately, PIM architectures cannot use traditional approaches to cache coherence due to the high off-chip traffic consumed by coherence messages, which, as we illustrate in this work, can undo the <b>benefits</b> <b>of</b> PIM <b>execution</b> for many data-intensive applications. We propose LazyPIM, a new hardware cache coherence mechanism designed specifically for PIM. Prior approaches for coherence in PIM are ill-suited to applications that share {{a large amount of}} data between the processor and the PIM logic. LazyPIM uses a combination of speculative cache coherence and compressed coherence signatures to greatly reduce the overhead of keeping PIM coherent with the processor, even when a large amount of sharing exists. We find that LazyPIM improves average performance across a range of data-intensive PIM applications by 19. 6 %, reduces off-chip traffic by 30. 9 %, and reduces energy consumption by 18. 0 %, over the best prior approaches to PIM coherence...|$|R
40|$|The {{research}} {{contained in}} this thesis was performed in order to model the external compression axisymmetric inlet portion of a supersonic jet engine {{in a way that}} captures the effects of a full 3 -D CFD model while maintaining the quickness of a lower dimensional model. This was accomplished by first creating high fidelity 3 -D and 2 -D models with the CFD code PHASTA. These models were used as base models to both verify and drive the creation of the lower dimensional model. The lower dimensional 1 -D model, created in MATLAB, was developed by piecing together established methods with novel ones. In particular, a new approach was developed in order to properly model the dynamics of the inherently three dimensional external compression flow field. With comparison to the higher order PHASTA models, the lower order model proved capable of accurately modeling both the steady state and dynamic response of the the external compression supersonic inlet. This was accomplished approximately 13, 000 times more efficiently than using the higher order CFD models. The results of this research provided a lower dimensional supersonic inlet model that maintains the dynamic accuracy of a higher order CFD model while exhibiting the <b>benefit</b> <b>of</b> quick <b>execution</b> time and adaptability allowed by its simpler construction...|$|R
40|$|Malaysia once {{replicated}} Bangladesh’s trail-blazer Grameen Bank {{to establish}} its Amanah Ikhtiar model of Islamic banking for the poor. Now, {{it may also}} learn some useful things from Bangladesh’s Islamic Green Banking. In this light, this study analysed the bankers’ perceptions of the green banking concept, its <b>benefits,</b> its complexities <b>of</b> <b>execution,</b> and its possible relationship with Islamic banking. This is an empirical study of 48 Islamic bankers from 21 branches of seven Islamic banks located in Dhaka, Bangladesh employing structured questionnaires. Data purification and analysis were carried out through a software SPSS version 1; and Henry Garrett’s ranking technique was used to gauge the most influential factors. The analysis revealed that respective bankers perceived green banking as an environmental banking that helped {{to protect the environment}} despite its high adoption cost. Also, green banking and Islamic banking were perceived to be compatible and interlinked as Islam supports resource savings, cleanliness, ethics and social responsibility all of which are also the mottos of Green Banking...|$|R
40|$|A {{parallel}} execution {{model for}} the update process of temporal databases is introduced in this paper, based on temporal parallelism and temporal independence. The parallel approach improves the throughput of massive and complex updates of a multi-version schema system. The notion of temporal agents {{and its effect on}} parallelism is discussed, as well as different transaction modes. Several simulation results that present the <b>benefits</b> <b>of</b> the parallel <b>execution</b> model are introduced and discussed. keywords: temporal databases, parallel processing, schema versioning. The work of this author was supported by the H. Kieval Research Fund. 1 Introduction and motivation Current studies of many contemporary applications in areas such as decision support systems and decision analysis systems show the necessity of representing a time dependent information and of update operations that refer to past and future information [31]. These requirements is partially satisfied by the infrastructure that [...] ...|$|R
