88|268|Public
25|$|A single success/failure {{experiment}} is also called a <b>Bernoulli</b> <b>trial</b> or Bernoulli experiment and {{a sequence of}} outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution {{is the basis for}} the popular binomial test of statistical significance.|$|E
25|$|The {{mathematical}} abstraction of {{the statistics}} of coin flipping {{is described by}} means of the Bernoulli process; a single flip of a coin is a <b>Bernoulli</b> <b>trial.</b> In the study of statistics, coin-flipping plays the role of being an introductory example of the complexities of statistics. A commonly treated textbook topic is that of checking if a coin is fair.|$|E
500|$|One of the {{simplest}} stochastic processes is the Bernoulli process, which is a sequence of independent and identically distributed (iid) random variables, where each random variable takes either the value one with probability, say, [...] and value zero with probability [...] This process can be likened to somebody flipping a coin, where the probability of obtaining a head is [...] and its value is one, while {{the value of a}} tail is zero. In other words, a Bernoulli process is a sequence of iid Bernoulli random variables, where each coin flip is a <b>Bernoulli</b> <b>trial.</b>|$|E
25|$|This {{result is}} easily {{generalized}} by substituting a letter such as t {{in the place}} of 49 to represent the observed number of 'successes' of our <b>Bernoulli</b> <b>trials,</b> and a letter such as n {{in the place of}} 80 to represent the number of <b>Bernoulli</b> <b>trials.</b> Exactly the same calculation yields the maximum likelihood estimator t/n for any sequence of n <b>Bernoulli</b> <b>trials</b> resulting in t 'successes'.|$|R
50|$|<b>Bernoulli</b> <b>trials</b> {{may also}} lead to {{negative}} binomial distributions (which {{count the number of}} successes in a series of repeated <b>Bernoulli</b> <b>trials</b> until a specified number of failures are seen), as well as various other distributions.|$|R
50|$|Because Fisher {{information}} is additive, the Fisher {{information contained in}} n independent <b>Bernoulli</b> <b>trials</b> is thereforeThis is the reciprocal of the variance of {{the mean number of}} successes in n <b>Bernoulli</b> <b>trials,</b> so in this case, the Cram√©r-Rao bound is an equality.|$|R
2500|$|Bernoulli distribution, for {{the outcome}} of a single <b>Bernoulli</b> <b>trial</b> (e.g. success/failure, yes/no) ...|$|E
2500|$|In {{probability}} and statistics, a Bernoulli {{process is}} a finite or infinite sequence of binary random variables, {{so it is a}} discrete-time stochastic process that takes only two values, canonically 0 and1. The component Bernoulli variables X'i are identically distributed and independent. [...] Prosaically, a Bernoulli {{process is a}} repeated coin flipping, possibly with an unfair coin (but with consistent unfairness). [...] Every variable X'i in the sequence is associated with a <b>Bernoulli</b> <b>trial</b> or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided die); this generalization is known as the Bernoulli scheme.|$|E
2500|$|The rate of {{an event}} {{is related to the}} {{probability}} {{of an event}} occurring in some small subinterval (of time, space or otherwise). In the case of the Poisson distribution, one assumes that there exists a small enough subinterval for which the probability of an event occurring twice is [...] "negligible". With this assumption one can derive the Poisson distribution from the Binomial one, given only the information of expected number of total events in the whole interval. Let this total number be [...] Divide the whole interval into [...] subintervals [...] of equal size, such that [...] > [...] (since we are interested in only very small portions of the interval this assumption is meaningful). This means that the expected number of events in an interval [...] for each [...] is equal to [...] Now we assume that the occurrence of an event in the whole interval {{can be seen as a}} <b>Bernoulli</b> <b>trial,</b> where the [...] trial corresponds to looking whether an event happens at the subinterval [...] with probability [...] The expected number of total events in [...] such trials would be , the expected number of total events in the whole interval. Hence for each subdivision of the interval we have approximated the occurrence of the event as a Bernoulli process of the form [...] As we have noted before we want to consider only very small subintervals. Therefore, we take the limit as [...] goes to infinity.|$|E
40|$|AbstractBernoulli ("Ars Conjectandi," Basle, 1713) {{proved the}} {{first limit theorem}} of law of large numbers which {{provided}} the foundation of probability and statistical theory. However, the problem of <b>Bernoulli</b> <b>trials</b> is still unsettled (e. g., see Hacking, "The Emergence of Probability," Cambridge Univ. Press, Cambridge, 1975). It is from different interpretations {{of the relationship between}} the <b>Bernoulli</b> <b>trials</b> and relative frequency that we have different schools of probability theories (e. g., see Cox (Amer. J. Phys. 14, No. 1 (1946), 1 - 13) and Fine (IEEE Trans. Inform. TheoryIT- 16, No. 3 (1970), 251 - 257)). In this paper we give a new treatment of the <b>Bernoulli</b> <b>trials</b> based on fuzzy measure, and we interpret the <b>Bernoulli</b> <b>trials</b> through the interaction of probability and possibility measures...|$|R
40|$|General inequalities for the {{distribution}} of the longest success run in <b>Bernoulli</b> <b>trials</b> are found through a direct combinatorial analysis. Their employment allows the derivation of a new lower bound which can be used in the achievement of theoretical results. <b>Bernoulli</b> <b>trials</b> Longest success run Lower bound Asymptotic behavior...|$|R
40|$|The {{waiting time}} {{problems}} introduced by Ebneshahrashoob and Sobel (1990) for independent trials are generalized to Markov correlated <b>Bernoulli</b> <b>trials.</b> A new waiting time problem arising due to mixed quotas is also discussed. A learning model {{is used as}} illustration. Frequency quota later waiting time Markovian <b>Bernoulli</b> <b>trials</b> run quota sooner waiting time...|$|R
5000|$|Bernoulli distribution, for {{the outcome}} of a single <b>Bernoulli</b> <b>trial</b> (e.g. success/failure, yes/no) ...|$|E
5000|$|... as the Jeffreys prior for the {{probability}} of success of a <b>Bernoulli</b> <b>trial.</b>|$|E
50|$|With {{respect to}} {{probability}} theory, Godwin's law becomes {{a special case}} of a <b>Bernoulli</b> <b>trial.</b>|$|E
40|$|New simple {{formulae}} {{for some}} probability distributions of success runs in <b>Bernoulli</b> <b>trials</b> are found {{by using the}} classical definition of run. These expressions contain only one summation of ordinary binomial coefficients and thus allow a faster and efficient computation. <b>Bernoulli</b> <b>trials</b> Number of success runs Longest success run Discrete distributions of order k...|$|R
5000|$|... #Subtitle level 3: Related to <b>Bernoulli</b> <b>trials</b> (yes/no events, with a given probability) ...|$|R
5000|$|Similarly, for the Binomial {{distribution}} with n <b>Bernoulli</b> <b>trials,</b> it can {{be shown}} that ...|$|R
5000|$|The model assumes that, for {{a binary}} outcome (<b>Bernoulli</b> <b>trial),</b> , and its {{associated}} vector of explanatory variables, , ...|$|E
5000|$|Let X be a <b>Bernoulli</b> <b>trial.</b> The Fisher {{information}} contained in X may be calculated to be ...|$|E
5000|$|... #Caption: Entropy of a <b>Bernoulli</b> <b>trial</b> as a {{function}} of binary outcome probability, called the binary entropy function.|$|E
40|$|The {{probability}} {{distribution of the}} numbeer of success runs of length k (>/ 1) in n ([greater-or-equal, slanted] 1) <b>Bernoulli</b> <b>trials</b> is obtained. It is noted that this distribution is a binomial distribution of order k, and several open problems pertaining to it are stated. Let Sn and Ln, respectively, denote {{the number of successes}} and the length of the longest success run in the n <b>Bernoulli</b> <b>trials.</b> A formula is derived for the probability P(Ln [less-than-or-equals, slant] k Sn = r) (0 [less-than-or-equals, slant] k [less-than-or-equals, slant] r [less-than-or-equals, slant] n), which is alternative to those given by Burr and Cane (1961) and Gibbons (1971). Finally, the {{probability distribution}} of Xn, Ln(k) is established, where Xn, Ln(k) denotes the number of times in the n <b>Bernoulli</b> <b>trials</b> that the length of the longest success run is equal to k. <b>Bernoulli</b> <b>trials</b> successes number of success runs of length k binomial distribution of order k length of the longest success run open problems...|$|R
25|$|In other words, a Bernoulli {{process is}} a {{sequence}} of independent identically distributed <b>Bernoulli</b> <b>trials.</b>|$|R
40|$|In [Zaigraev, A., Kaniovski, S., 2010. Exact bounds on the {{probability}} of at least k successes in n exchangeable <b>Bernoulli</b> <b>trials</b> {{as a function of}} correlation coefficients. Statist. Probab. Lett. 80, 1079 - 1084] the authors present sharp bounds for {{the probability}} Rk,n of having k successes out of n exchangeable <b>Bernoulli</b> <b>trials,</b> {{as a function of the}} marginal probability of success. The result is obtained by linear programming arguments. In this paper we develop further the result utilizing a geometrical approach to the problem, and find sharp bounds for Rk,n given the marginal probability of success and the correlation among the exchangeable variables. Exchangeable <b>Bernoulli</b> <b>trials</b> Convex polytopes Condorcet's Jury Theorem...|$|R
5000|$|Since a <b>Bernoulli</b> <b>trial</b> {{has only}} two {{possible}} outcomes, it can be framed as some [...] "yes or no" [...] question. For example: ...|$|E
5000|$|... {{where the}} {{independent}} random variables Xn are each equal to 0 or 1 with equal probabilities - this is a <b>Bernoulli</b> <b>trial</b> of each digit of the binary expansion.|$|E
50|$|One {{would like}} to be able to {{interpret}} the return period in probabilistic models. The most logical interpretation for this is to take the return period as the counting rate in a Poisson distribution since it is the expectation value of the rate of occurrences. An alternative interpretation is to take it as the probability for a yearly <b>Bernoulli</b> <b>Trial</b> in the Binomial Distribution. This is disfavoured because each year does not represent an independent <b>Bernoulli</b> <b>trial</b> but is an arbitrary measure of time. This question is mainly academic as the results obtained will be similar under both the Poisson and Binomial interpretations.|$|E
50|$|The {{geometric}} distribution models {{the number of}} independent and identical <b>Bernoulli</b> <b>trials</b> needed to get one success.|$|R
40|$|This work {{is focused}} on {{selected}} probability characteristics of runs in a sequence of <b>Bernoulli</b> <b>trials</b> and on some randomness tests based on these runs. Based on Markov chains, an explicit formula is derived for {{the probability that the}} first success run of a lenght $k$ in a sequence of independent <b>Bernoulli</b> <b>trials</b> occurs in the $n$-th trial and other formulas for this probability are mentioned. Furthermore, approximations of the exact value of this probability (particularly the Feller approximation), bounds of these approximations, and their numeric relations are examined. Lastly, a test of randomness based on the lenght of the longest run in a sequence of $n$ <b>Bernoulli</b> <b>trials</b> and a test based on the total amount of runs are derived...|$|R
2500|$|... for {{any given}} random {{variable}} [...] out of the infinite sequence of <b>Bernoulli</b> <b>trials</b> that compose the Bernoulli process.|$|R
5000|$|Gy's {{sampling}} theory uses a {{model in}} which the sample taking is represented by independent Bernoulli trials for every particle in the parent population from which the sample is drawn. The two possible outcomes of each <b>Bernoulli</b> <b>trial</b> are: (1) the particle is selected and (2) the particle is not selected. The probability of selecting a particle may be different during each <b>Bernoulli</b> <b>trial.</b> The model used by Gy is mathematically equivalent to Poisson sampling. Using this model, the following equation for the variance of the sampling error in the mass concentration in a sample was derived by Gy: ...|$|E
50|$|The {{mathematical}} formalisation of the <b>Bernoulli</b> <b>trial</b> {{is known}} as the Bernoulli process. This article offers an elementary introduction to the concept, whereas the article on the Bernoulli process offers a more advanced treatment.|$|E
50|$|In {{the theory}} of finite {{population}} sampling, Poisson sampling is a sampling process where each element {{of the population is}} subjected to an independent <b>Bernoulli</b> <b>trial</b> which determines whether the element becomes part of the sample.|$|E
5000|$|Random {{variables}} describing <b>Bernoulli</b> <b>trials</b> {{are often}} encoded using the convention that 1 = [...] "success", 0 = [...] "failure".|$|R
5000|$|X is {{the number}} of successes in twelve {{independent}} <b>Bernoulli</b> <b>trials</b> with probability Œ∏ of success on each trial, and ...|$|R
40|$|An {{approach}} to randomness testing for <b>Bernoulli</b> <b>trials</b> {{on the base}} of universal predictors is considered. We propose two strategies for using universal predictors and derive the power of statistical test constructed {{on the base of}} maximum-likelihood predictor for <b>Bernoulli</b> <b>trials.</b> The results are extended to CTW, SPM and Lempel-Ziv universal predictors. Comparison of test constructed on the base of Lempel-Ziv predictor with Lempel-Ziv compression test, proposed in NIST SP 800 - 22, is performed...|$|R
