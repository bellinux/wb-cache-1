5|15|Public
5000|$|Each node in [...] {{contains}} an array with at most [...] triples. Each triple {{is of the}} form , where [...] is a <b>block</b> <b>identifier</b> and [...] is {{the contents of the}} block. Here, [...] is a security parameter and is [...]|$|E
5000|$|The {{programmer}} {{is responsible}} for computing the full direct access <b>block</b> <b>identifier,</b> MBBCCHHR. System algorithms are available for calculating the MBBCCHHR from a TTRN. System data, {{in the form of}} [...] "track capacity tables", are available for calculating the TTRN from a block number, for any direct access device type. Later versions of the OS facilitate accessing very large capacity devices by using the TRKADDR macro.|$|E
40|$|This paper {{describes}} a network storage system, called Venti, intended for archival data. In this system, a unique hash of a block's contents {{acts as the}} <b>block</b> <b>identifier</b> for read and write operations. This approach enforces a write-once policy, preventing accidental or malicious destruction of data. In addition, duplicate copies of a block can be coalesced, reducing the consumption of storage and simplifying the implementation of clients. Venti is a building block for constructing a variety of storage applications such as logical backup, physical backup, and snapshot file systems...|$|E
5000|$|Within this <b>block,</b> <b>identifiers</b> {{can be used}} {{exactly as}} they are declared. Outside of this block, the {{namespace}} specifier must be prefixed. For example, outside of , [...] must be written [...] to be accessed. C++ includes another construct that makes this verbosity unnecessary. By adding the line [...] using namespace abc; ...|$|R
50|$|A vehicle {{designation}} {{is sometimes}} referred to as a Mission Design Series (MDS), referring to the three main parts of the designation, that combine to form a unique profile for each vehicle. The first series of letters (up to four) determine the type of craft and designed mission. A series number identifies major types which are of the same type and mission, and finally a series of variant and <b>block</b> <b>identifiers</b> clarify the exact configuration of the vehicle.|$|R
40|$|This paper {{focuses on}} {{generating}} efficient software pipelined schedules for in-order machines, {{which we call}} Converged Trace Schedules. For a candidate loop, we form a string of trace <b>block</b> <b>identifiers</b> by hashing together addresses of aggressively scheduled instructions from multiple iterations of a loop. In this process, the loop is unrolled and scheduled until we identify a repeating pattern in the string. Instructions corresponding to this repeating pattern form the kernel for our software pipelined schedule. We evaluate this approach to create aggressive schedules by using it in dynamic hardware and software optimization systems for an in-order architecture...|$|R
40|$|This paper {{describes}} a network storage system, called Venti, intended for archival data. In this system, a unique hash of a block’s contents {{acts as the}} <b>block</b> <b>identifier</b> for read and write operations. This approach enforces a write-once policy, preventing accidental or malicious destruction of data. In addition, duplicate copies of a block can be coalesced, reducing the consumption of storage and simplifying the implementation of clients. Venti is a building block for constructing a variety of storage applications such as logical backup, physical backup, and snapshot file systems. We have built a prototype {{of the system and}} present some preliminary performance results. The system uses magnetic disks as the storage technology, resulting in an access time for archival data that is comparable to non-archival data. The feasibility of the write-once model for storage is demonstrated using data from over a decade’s use of two Plan 9 file systems. 1...|$|E
40|$|Rights to {{individual}} papers {{remain with the}} author or the author's employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must {{be included in the}} reproduced paper. USENIX acknowledges all trademarks herein. Venti: a new approach to archival storage This paper describes a network storage system, called Venti, intended for archival data. In this system, a unique hash of a block’s contents acts as the <b>block</b> <b>identifier</b> for read and write operations. This approach enforces a write-once policy, preventing accidental or malicious destruction of data. In addition, duplicate copies of a block can be coalesced, reducing the consumption of storage and simplifying the implementation of clients. Venti is a building block for constructing a variety of storage applications such as logical backup, physical backup, and snapshot file systems. We have built a prototype of the system and present some preliminary performance results. The system uses magnetic disks as the storage technology, resulting in an access time for archival data that is comparable to non-archival data. The feasibility of the write-once model for storage is demonstrated using data from over a decade’s use of two Plan 9 file systems. 1...|$|E
5000|$|We can {{see from}} Fig. how the m LRU queues are placed in the cache. Also see from Fig. how the Qout [...] stores the <b>block</b> <b>identifiers</b> and their {{corresponding}} access frequencies. a was placed in Q0 as it was accessed only once recently and we can check in Qout how b and c were placed in Q1 and Q2 respectively as their access frequencies are 2 and 4. The queue in which a block is placed is dependent on access frequency(f) as log2(f). When the cache is full, the first block to be evicted will {{be the head of}} Q0 in this case a. If a is accessed one more time it will move to Q1 below b.|$|R
50|$|The Individual Address Block (IAB) is a <b>block</b> of <b>identifiers</b> that {{is formed}} by concatenating a 24-bit Organizationally Unique Identifier (OUI) that {{is owned by}} the IEEE Registration Authority with an {{additional}} 12-bit extension identifier that is assigned by the IEEE Registration Authority and then reserving an additional 12 bits for use by the assignee. The resulting 48-bit identifier uniquely identifies the assignee of the IAB and provides 4096 unique EUI-48 numbers for use by the organization that purchased the IAB. The assignee may create unique identifiers by concatenating a 12-bit extension identifier that is assigned by the organization that purchases the IAB in the bit positions occupied by the 12 additional bits mentioned previously. The purpose of the IAB is to allow organizations to purchase smaller <b>blocks</b> of <b>identifiers.</b>|$|R
50|$|ORCID (Open Researcher and Contributor ID) {{identifiers}} {{consist of}} a reserved <b>block</b> of ISNI <b>identifiers</b> for scholarly researchers and administered by a separate organisation. Individual researchers can create and claim their own ORCID identifier. The two organisations coordinate their efforts.|$|R
50|$|ORCID is {{a subset}} of the International Standard Name Identifier (ISNI), {{under the auspices of the}} International Organization for Standardization (as ISO 27729) and the two {{organizations}} are cooperating. ISNI will uniquely identify contributors to books, television programmes, and newspapers, and has reserved a <b>block</b> of <b>identifiers</b> for use by ORCID, in the range 0000-0001-5000-0007 to 0000-0003-5000-0001. It is therefore possible for a person to legitimately have both an ISNI and an ORCID - effectively, two ISNIs.|$|R
5000|$|Every Census Block Group has {{a unique}} 12-digit FIPS code. The <b>Block</b> Group's unique <b>identifier</b> is the 12th digit of the FIPS Code. This number determines the first digit of all the census blocks {{contained}} within a block group. For instance, census Block Group 2 includes any block numbered 2000 to 2999.|$|R
5000|$|The Multi Queue Algorithm or MQ was {{developed}} to improve the performance of second level buffer cache for e.g. a server buffer cache. It is introduced in a paper by Zhou, Philbin, and Li.The MQ cache contains an m number of LRU queues: Q0, Q1, ..., Qm-1. Here, the value of m represents a hierarchy based on the lifetime of all blocks in that particular queue. For example, if j>i, blocks in Qj will have a longer lifetime than those in Qi. In addition to these there is another history buffer Qout, a queue which maintains {{a list of all}} the <b>Block</b> <b>Identifiers</b> along with their access frequencies. When Qout is full the oldest <b>identifier</b> is evicted. <b>Blocks</b> stay in the LRU queues for a given lifetime, which is defined dynamically by the MQ algorithm to be the maximum temporal distance between two accesses to the same file or the number of cache blocks, whichever is larger. If a block has not been referenced within its lifetime, it is demoted from Qi to Qi−1 or evicted from the cache if it is in Q0. Each queue also has a maximum access count; if a block in queue Qi is accessed more than 2i times, this block is promoted to Qi+1 until it is accessed more than 2i+1 times or its lifetime expires. Within a given queue, blocks are ranked by the recency of access, according to LRU.|$|R
40|$|Improved fix to # 3989 (parsing of HTML tags {{containing}} > in {{an attribute}} or comment). The previous fix (in 2. 0. 1) only worked in certain cases. FB 2 writer (Alexander Krotov) : Add unrecognised genre to (Alexander Krotov). XML schema requires {{at least one}} genre. Remove from. CommonMark writer: fix strikethrough for gfm (# 4038). Use texmath 0. 10, which adds support for {{a wider range of}} symbols and fixes default column alignments in MathML and OMML. Highlighting fixes, using skylighting 0. 4. 3. 2 : Fix invalid CSS. Support lineAnchors (or line-anchors) in HTML code blocks. Ensure that code lines don't get duplicate identifiers (# 4031). The line identifiers are built using the code <b>block's</b> <b>identifier</b> as a prefix. If the code <b>block</b> has null <b>identifier,</b> we use cb 1, cb 2, etc. Added a few abbreviations to data/abbreviations, and sorted the list (# 3984, Wandmalfarbe). Improved support for columns in HTML writer (# 4028). Remove width attribute from the div. Remove space between elements, since this prevents columns whose widths sum to 100 % (the space takes up space). Move as much as possible of the CSS to the template. Ensure that all the HTML-based templates (including epub) contain the CSS for columns. Columns default to 50 % width unless they are given a width attribute. So if you want two equal-width columns, you can use a div with class column and no width attribute. SelfContained: use base 64 for css links with media attribute (# 4026). This fixes [...] self-contained with S 5. Improve pandoc-template-mode. el (Vaclav Haisman). Issue INFO, not WARNING, when a. sty file cannot be read in LaTeX reader. It is normally not an issue requiring a fix from the user if. sty files are not found. INSTALL. md: MacOS instructions needed xar -f (adam 234). MANUAL. txt: Clarify that –setext-headers doesn't affect gfm output (# 4035). Clarify what is needed to open and close a div in fenced_divs (# 4039, Tristano Ajmone). Removed reference to default. beamer in docs (# 4024). Also added mention of other templates affecting PDF output with different settings...|$|R
40|$|The {{intent of}} this policy is to {{prescribe}} standards under which client information can be used and disclosed if information that can identify a person has been removed or restricted to a limited data set. Policy 1. General a. De-identified information is client information from which DHS or another entity has deleted, redacted, or <b>blocked</b> <b>identifiers,</b> so that the remaining information cannot reasonably {{be used to identify}} a person. b. Unless otherwise restricted or prohibited by other federal or state law, DHS can use and disclose information as appropriate for the work of DHS, without further restriction, if DHS or another entity has taken steps to de-identify the information consistent with the requirements and restrictions {{of this policy}} in Section (2.). c. DHS may use or disclose a limited data set that meets the requirements of Section (4.) of this Policy, if DHS enters into a data use agreement with the limited data set recipient (or with the data source, if DHS will be the recipient of the limited data set) in accordance with the requirements of Section (5.) of this Policy. d. DHS may disclose a limited data set only for the purposes of research, program operations, or public health purposes. However, unless DHS has obtained a limited data set that is subject to a data use agreement, DHS is not restricted to using a limited data set for its own activities or operations. e. If DHS knows of a pattern or activity or practice of the limited data set recipient that constitutes a material breach or violation of a data set agreement, DHS will take reasonable steps to cure the breach or end the violation. If such steps are unsuccessful...|$|R
40|$|During {{the last}} years, some {{operators}} have expressed {{concerns about the}} continued growth of the BGP routing tables in the default-free zone. Several proposed solutions for this issue are centered around the idea of separating the network node’s identifier from its topological location. Among the existing proposals, the Locator/ID Separation Protocol (LISP) has seen important development and implementation effort. LISP relies on a mapping system to provide bindings between locators and identifiers. The mapping system is a critical protocol component, and its design is still an open issue. In this paper we present a new mapping system: LISP-TREE. It is based on DNS and has a similar hierarchical topology: <b>blocks</b> of <b>identifiers</b> are assigned to the levels of the hierarchy by following the current IP address allocation policies. We also present measurementdriven simulations of mapping systems’ performance, assuming a deployment of LISP in the current Internet...|$|R
40|$|SUMMARY In this paper, we {{describe}} a single system image (SSI) architecture for distributed systems. The SSI architecture is constructed through three components: single process space (SPS), process migration, and dynamic load balancing. These components attempt to share all available {{resources in the}} cluster among all executing processes, so that the distributed system operates like a single node with much more computing power. To this end, we first resolve broken pipe problems and bind errors on server socket in process migration. Second, we realize SPS based on <b>block</b> process <b>identifier</b> (PID) allocation. Finally, we design and implement a dynamic load balancing scheme. The dynamic load balancing scheme exploits our novel metric, effective tasks, toeffectively distribute jobs to a large distributed system. The experimental results show that these three components present scalability, new functionality, and performance improvement in distributed systems. key words: single system image, process migration, load balancing, grid computing, distributed system 1...|$|R
40|$|Abstract—We {{present a}} generalizable formal model of {{software}} readability {{based on a}} human study of 5000 participants. Readability is fundamental to maintenance, but remains poorly understood. Previous models focused on symbol counts of small code snippets. By contrast, we approach code as read on screens by humans and propose to analyze visual, spatial and linguistic features, including structural patterns, sizes of code <b>blocks,</b> and verbal <b>identifier</b> content. We construct a readability metric based on these notions and show that it agrees with human judgments {{as well as they}} agree with each other and better than previous work. We identify universal features of readability and languageor experience-specific ones. Our metric also correlates with an external notion of defect density. We address multiple programming languages and different length samples, and evaluate using an order of magnitude more participants than previous work, all suggesting our model is more likely to generalize. I...|$|R
40|$|This thesis {{presents}} {{methods for}} multiple imputation {{that can be}} applied to missing data and data with confidential variables. Imputation is useful for missing data because it results in a data set that can be analyzed with complete data statistical methods. The missing data are filled in by values generated from a model fit to the observed data. The model specification will depend on the observed data pattern and the missing data mechanism. For example, when the reason why the data is missing is related to the outcome of interest, that is nonignorable missingness, we need to alter the model fit to the observed data to generate the imputed values from a different distribution. Imputation is also used for generating synthetic values for data sets with disclosure restrictions. Since the synthetic values are not actual observations, they can be released for statistical analysis. The interest is in fitting a model that approximates well the relationships in the original data, keeping the utility of the synthetic data, while preserving the confidentiality of the original data. We consider applications of these methods to data from social sciences and epidemiology. The first method is for imputation of multivariate continuous data with nonignorable missingness. Regular imputation methods have been used to deal with nonresponse in several types of survey data. However, in some of these studies, the assumption of missing at random is not valid since the probability of missing depends on the response variable. We propose an imputation method for multivariate data sets when there is nonignorable missingness. We fit a truncated Dirichlet process mixture of multivariate normals to the observed data under a Bayesian framework to provide flexibility. With the posterior samples from the mixture model, an analyst can alter the estimated distribution to obtain imputed data under different scenarios. To facilitate that, I developed an R application that allows the user to alter the values of the mixture parameters and visualize the imputation results automatically. I demonstrate this process of sensitivity analysis with an application to the Colombian Annual Manufacturing Survey. I also include a simulation study to show that the correct complete data distribution can be recovered if the true missing data mechanism is known, thus validating that the method can be meaningfully interpreted to do sensitivity analysis. The second method uses the imputation techniques for nonignorable missingness to implement a procedure for adaptive design in surveys. Specifically, I develop a procedure that agencies can use to evaluate {{whether or not it is}} effective to stop data collection. This decision is based on utility measures to compare the data collected so far with potential follow-up samples. The options are assessed by imputation of the nonrespondents under different missingness scenarios considered by the analyst. The variation in the utility measures is compared to the cost induced by the follow-up sample sizes. We apply the proposed method to the 2007 U. S. Census of Manufactures. The third method is for imputation of confidential data sets with spatial locations using disease mapping models. We consider data that include fine geographic information, such as census tract or street <b>block</b> <b>identifiers.</b> This type of data can be difficult to release as public use files, since fine geography provides information that ill-intentioned data users can use to identify individuals. We propose to release data with simulated geographies, so as to enable spatial analyses while reducing disclosure risks. We fit disease mapping models that predict areal-level counts from attributes in the file, and sample new locations based on the estimated models. I illustrate this approach using data on causes of death in North Carolina, including evaluations of the disclosure risks and analytic validity that can result from releasing synthetic geographies. Dissertatio...|$|R

