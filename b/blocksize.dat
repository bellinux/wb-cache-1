87|13|Public
25|$|RAID-Z is a data/parity {{distribution}} scheme like RAID-5, but uses dynamic stripe width: every {{block is}} its own RAID stripe, regardless of <b>blocksize,</b> resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID5 {{because it does not}} need to perform the usual read-modify-write sequence.|$|E
25|$|In TFTP, a {{transfer}} is {{initiated by the}} client issuing a request to read or write a particular file on the server. The request can optionally include a set of negotiated transfer parameters proposed by the client under the terms specified by RFC 2347. If the server grants the request, the file is sent in fixed length blocks of 512 bytes by default or the number specified in the <b>blocksize</b> negotiated option defined by RFC 2348. Each block of transferred data which is usually carried within a single IP packet {{in order to avoid}} IP fragmentation, must be acknowledged by an acknowledgment packet before the next block can be sent. A data packet of less than 512 bytes or the agreed <b>blocksize</b> option signals termination of {{a transfer}}. If a packet gets lost in the network, the intended recipient will timeout and may retransmit their last packet (which may be data or an acknowledgment), thus causing the sender of the lost packet to retransmit that lost packet. The sender has to keep just one packet on hand for retransmission, since the lock step acknowledgment guarantees that all older packets have been correctly received. Notice that both devices involved in a transfer are considered senders and receivers. One sends data and receives acknowledgments, the other sends acknowledgments and receives data.|$|E
2500|$|The {{original}} protocol has {{a transfer}} file size limit of 512 bytes/block x 65535 blocks = 32MB. In 1998 this limit {{was extended to}} 65535 bytes/block x 65535 blocks = 4GB by TFTP <b>Blocksize</b> Option RFC 2348. If the defined <b>blocksize</b> produces an IP packet size that exceeds the minimum MTU at any point of the network path, IP fragmentation and reassembly will occur not only adding more overhead but also leading to total transfer failure when the minimalist IP stack implementation in a host's BOOTP or PXE ROM does not (or fails to properly) implement IP fragmentation and reassembly [...] If TFTP packets should be kept within the standard Ethernet MTU (1500), the <b>blocksize</b> value is calculated as 1500 minus headers of TFTP (4 bytes), UDP (8 bytes) and IP (20 bytes) = 1468 bytes/block, this gives a limit of 1468 bytes/block x 65535 blocks = 92MB. Today most servers and clients support block number roll-over (block counter going back to 0 after 65535) which gives an essentially unlimited transfer file size.|$|E
50|$|OS4000 {{uses its}} own {{proprietary}} filesystem. The filesystem is extent based, and variable block size — different files {{can be created}} with different <b>blocksizes,</b> ranging from 256 bytes to 16384 bytes in 256 byte multiples.|$|R
40|$|The aim of {{this project}} is to {{implement}} the basic factorization routines for solving linear systems of equations and least squares problems from LAPACK—namely, the blocked versions of LU with partial pivoting, QR, and Cholesky on a distributed-memory machine. We discuss our implementation {{of each of the}} algorithms and the results we obtained using varying orders of matrices and <b>blocksizes.</b> ...|$|R
40|$|This thesis {{investigates the}} {{scalability}} of explicitly blocked algorithms for solving discrete-time matrix equations for shared memory environments when implemented using high-performance GEMM-updates, a fast kernel solver from the HPC library RECSY and different <b>blocksizes.</b> The algorithms have been parallelized using OpenMP. We consider two different algorithms, which uses two different strategies for traversal of the righthand side matrix. We also show by numerical experiments {{that the second}} algorithm with the more advanced anti-diagonal solving metho...|$|R
2500|$|Since TFTP {{utilizes}} UDP, it has {{to supply}} its own transport and session support. Each file transferred via TFTP constitutes an independent exchange. Classically, this transfer is performed in lock-step, with only one packet (either a block of data, or an 'acknowledgement') alternatively in flight on the network at any time. Due to this single data block strategy instead of sending a fluid amount of data blocks before to pause the transfer waiting for an acknowledge (windowing), TFTP provides low throughput especially over high latency links. Microsoft introduced windowed TFTP in Windows 2008 {{as part of their}} Windows Deployment Services (WDS), in January 2015 TFTP Windowsize Option RFC 7440 was published. This substantially improves performance for things like PXE booting without the IP fragmentation side effect sometimes observed on <b>Blocksize</b> Option RFC 2348 ...|$|E
2500|$|The Preboot Execution Environment (PXE) was {{introduced}} {{as part of}} the Wired for Management framework by Intel and is described in the specification published by Intel and SystemSoft. PXE version 2.0 was released in December 1998, and the update 2.1 was made public in September 1999. The PXE environment makes use of several standard client‑server protocols like DHCP and TFTP (now defined by the 1992 published RFC 1350). Within the PXE schema the client [...] side of the provisioning equation is now {{an integral part of the}} PXE standard and it is implemented either as a Network Interface Card (NIC) BIOS extension or today in modern devices as UEFI code. This distinctive firmware layer makes available at the client the functions of a basic Universal Network Driver Interface (UNDI), a minimalistic UDP/IP stack, a Preboot (DHCP) client module and a TFTP client module, together forming the PXE application programming interfaces (APIs) used by the NBP when needing to interact with the services offered by the server counterpart of the PXE environment. TFTP's low throughput, especially when used over high-latency links, has been initially mitigated by the TFTP <b>Blocksize</b> Option RFC 2348 published in May 1998, and later by the TFTP Windowsize Option RFC 7440 published in January 2015.|$|E
5000|$|... {{function}} hmac (key, message) { if (length(key) > <b>blocksize)</b> { key = hash(key) // keys {{longer than}} <b>blocksize</b> are shortened } if (length(key) < <b>blocksize)</b> { [...] // keys shorter than <b>blocksize</b> are zero-padded (where ∥ is concatenation) key = key ∥ * (<b>blocksize</b> - length(key)) // Where * is repetition. } [...] o_key_pad = * <b>blocksize</b> ⊕ key // Where <b>blocksize</b> {{is that of}} the underlying hash function i_key_pad = * <b>blocksize</b> ⊕ key // Where ⊕ is exclusive or (XOR) [...] return hash(o_key_pad ∥ hash(i_key_pad ∥ message)) // Where ∥ is concatenation } ...|$|E
40|$|International audienceIn OFDMA, {{the total}} {{available}} subcarriers {{are assigned to}} different users for simultaneous transmission. In case of an unsynchronized uplink OFDMA, each user has a different carrier frequency offset (CFO), which results in loss of orthogonality among subcarriers and causes severe intercarrier interference. In this paper, a self-successive interference canceller (self-SIC) algorithm is proposed for uplink OFDMA systems to mitigate both the self-intercarrier interference and multiuser interference resulting from carrier frequency offsets. We show that for a carrier allocation with larger <b>blocksizes,</b> the proposed compensation algorithm significantly reduces the implementation complexity without performance degradation...|$|R
40|$|Dynamic {{volumetric}} (four dimensional- 4 D) medical {{images are}} typically huge in file size and require {{a vast amount}} of resources for storage and transmission purposes. In this paper, we propose an efficient lossless compression method for 4 D medical images that is based on a multi-frame motion compensation process employing a 4 D search, variable <b>blocksizes</b> and bi-directional prediction. Data redundancies are reduced by recursively applying multi-frame motion compensation in the spatial and temporal dimensions. The proposed method also uses a novel differential coding algorithm to reduce redundancies in motion vectors and a new context-based adaptive binary arithmetic coder (CABAC) for compression of the residual data. Performance evaluations on real medical images of varying modality resulted in lossless compression ratios of up to 16 : 1. Index Terms — 4 D medical images, motion compensation, lossless compression, fMRI, CABAC. 1...|$|R
40|$|Virtual memory, even on {{the largest}} and fastest {{contemporary}} computers, is neither large enough nor fast enough for all applications. Some data structures must {{be held in the}} file system, and some "performance hints" must be given to the memory-management runtime routines. For these reasons, most large-memory application codes are littered with system-specific names, constants, file-migration policies, pragmas and hints. Such codes are very difficult to develop, maintain, and port. I propose a new paradigm for the design of large-memory codes, providing many of the performance advantages and few of the drawbacks of systemspecific coding techniques. In my paradigm, programmers must organize their data structures into a series of (nesting) data blocks, with <b>blocksizes</b> B h increasing in a fractal (power-law) progression B h = RB ffi hΓ 1; B 1 = 1. Furthermore, the larger blocks must be referenced much less frequently than the smaller blocks. I argue that efficient algorithms for se [...] ...|$|R
5000|$|... dump -0123456789acLnSu records <b>blocksize</b> cachesize dumpdates density file | -P pipecommand level feet date {{filesystem}} ...|$|E
50|$|The {{following}} pseudocode {{demonstrates how}} HMAC may be implemented. <b>Blocksize</b> is 64 (bytes) when {{using one of}} the following hash functions: SHA-1, MD5, RIPEMD-128/160.|$|E
5000|$|Higher coding {{accuracy}} for transient signals (AAC uses a <b>blocksize</b> of 128 or 120 samples, allowing {{more accurate}} coding than MP3's 192 sample blocks) ...|$|E
40|$|Hiermit versichere ich, die vorliegende Bachelor-Thesis ohne Hilfe Dritter nur mit den angegebenen Quellen und Hilfsmitteln angefertigt zu haben. Alle Stellen, die aus Quellen entnommen wurden, sind als solche kenntlich gemacht. Diese Arbeit hat in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegen. Darmstadt, den 08. 07. 2013 (Tobias Hamann) The best {{reduction}} algorithm for lattices {{with high}} dimensions known {{today is the}} BKZ reduction algorithm by Schnorr and Euchner. The running time of the BKZ reduction algorithm, however, increases significantly with higher <b>blocksizes.</b> The BKZ simulation algorithm by Chen and Nguyen predicts the Gram-Schmidt norms of a lattice basis after a given time of rounds of BKZ reduction. Given {{the cost of the}} enumeration subroutine used in BKZ reduction, the simulation algorithm {{can also be used to}} estimate the running time of BKZ reduction. This Bachelor Thesis provides a detailed explanation of the BKZ simulation algorithm, the underlying assumptions and an implementation in Java. Furthermore, own experiments that reinforce the results o...|$|R
40|$|In {{communication}} systems, specifically wirelessmobile communication applications, Size and Speed aredominant factors while {{meeting the}} performance requirements. Turbo codes {{play an important}} role in such practical applicationsdue to their better error-correcting capability. In Turbodecoders, Maximum A Posterior probability (MAP) algorithmhas been widely used for its optimum error correctingperformance. But it is very difficult to design high-speed MAPDecoder because of its recursive computations. This paperproposes a Max-Log maximum a posteriori (ML-MAP) algorithm with dual mode SB/DB decoding. A Parallel VLSIarchitecture comprising multiple SISO elements, to form LogLikelihood ratio (LLR) unit in order to reduce the critical pathdelay. A dual mode single-binary (SB) and double-binary (DB) decoding algorithm has been used to reduce the arbitrary <b>blocksizes</b> for high throughput decoding. The computational modulesand storages of the dual-mode (SB/DB) MAP decoding aredesigned to achieve high area utilization. This architecture with aSB/DB decoding can achieve comparable processing speed about 11 % and area efficiency of 5. 71 bits/mm 2...|$|R
40|$|Adaptive low-rate source coders are {{described}} in this dissertation. These coders adapt by adjusting {{the complexity of the}} coder to match the local coding difficulty of the image. This is accomplished by using a threshold driven maximum distortion criterion to select the specific coder used. The different coders are built using variable <b>blocksized</b> transform techniques, and the threshold criterion selects small transform blocks to code the more difficult regions and larger blocks to code the less complex regions. A theoretical framework is constructed from which the study of these coders can be explored. An algorithm for selecting the optimal bit allocation for the quantization of transform coefficients is developed. The bit allocation algorithm is more fully developed, and can be used to achieve more accurate bit assignments than the algorithms currently used in the literature. Some upper and lower bounds for the bit-allocation distortion-rate function are developed. An obtainable distortion-rate function is developed for a particular scalar quantizer mixing method {{that can be used to}} code transform coefficients at any rate...|$|R
5000|$|Higher coding {{efficiency}} for stationary signals (AAC uses a <b>blocksize</b> of 1024 or 960 samples, allowing {{more efficient}} coding than MP3's 576 sample blocks) ...|$|E
5000|$|... 1440 - {{a highly}} totient number and a 481-gonal number. Also, {{the number of}} minutes in one day, the <b>blocksize</b> of a {{standard}} ″ floppy disk, and the horizontal resolution of WXGA(II) computer displays ...|$|E
5000|$|The {{original}} protocol has {{a transfer}} file size limit of 512 bytes/block x 65535 blocks = 32 MB. In 1998 this limit {{was extended to}} 65535 bytes/block x 65535 blocks = 4 GB by TFTP <b>Blocksize</b> Option RFC 2348. If the defined <b>blocksize</b> produces an IP packet size that exceeds the minimum MTU at any point of the network path, IP fragmentation and reassembly will occur not only adding more overhead but also leading to total transfer failure when the minimalist IP stack implementation in a host's BOOTP or PXE ROM does not (or fails to properly) implement IP fragmentation and reassembly [...] If TFTP packets should be kept within the standard Ethernet MTU (1500), the <b>blocksize</b> value is calculated as 1500 minus headers of TFTP (4 bytes), UDP (8 bytes) and IP (20 bytes) = 1468 bytes/block, this gives a limit of 1468 bytes/block x 65535 blocks = 92 MB. Today most servers and clients support block number roll-over (block counter going back to 0 after 65535) which gives an essentially unlimited transfer file size.|$|E
40|$|The Uniform Memory Hierarchy (UMH) model {{introduced}} {{in this paper}} captures performance-relevant aspects of the hierarchical nature of computer memory. It is used to quantify architectural requirements of several algorithms and to ratify the faster speeds achieved by tuned implementations that use improved data-movement strategies. A sequential computer's memory is modelled as a sequence hM 0; M 1; :::i of increasingly large memory modules. Computation takes place in M 0. Thus, M 0 might model a computer's central processor, while M 1 might be cache memory, M 2 main memory, and so on. For each module M U, a bus B U connects it with the next larger module M U+ 1. All buses may be active simultaneously. Data is transferred along a bus in fixed-sized blocks. The size of these blocks, {{the time required to}} transfer a block, and the number of blocks that fit in a module are larger for modules farther from the processor. The UMH model is parameterized by {{the rate at which the}} <b>blocksizes</b> i [...] ...|$|R
40|$|Century-long {{global climate}} {{simulations}} at high resolutions generate {{large amounts of}} data in a parallel architecture. Currently, Community Atmosphere Model (CAM), the atmospheric component of the NCAR Community Climate System Model (CCSM), uses sequential I/O which causes a serious bottleneck for these simulations. We describe the parallel I/O development of CAM in this paper. The parallel I/O combines a novel remapping of 3 D arrays with the parallel netCDF library as the I/O interface. Because CAM history variables are stored in disk file in a different index order {{than the one in}} CPU resident memory due to parallel decomposition, an index reshuffle is done on the fly. Our strategy is first to remap 3 D arrays from its native decomposition to Z-decomposition on a distributed architecture, and from there, write data out to disk. Because Z-decomposition is consistent with the last array dimension, the data transfer can occur at maximum <b>blocksizes</b> and, therefore, achieve maximum I/O bandwidth. We also incorporate the recently developed Parallel NetCDF library at Argonne/Northwestern as the collective I/O interface, which resolves a long standing issue because netCDF data format is extensively used in climate system models. Benchmark tests are performed on several platform...|$|R
40|$|Global {{digital data}} bases on the {{distribution}} and environmental characteristics of natural wetlands, compiled by Matthews and Fung (1987), were archived for public use. These data bases were developed to evaluate the role of wetlands in the annual emission of methane from terrestrial sources. Five global 1 deg latitude by 1 deg longitude arrays are included on the archived tape. The arrays are: (1) wetland data source, (2) wetland type, (3) fractional inundation, (4) vegetation type, and (5) soil type. The first three data bases on wetland locations were published by Matthews and Fung (1987). The last two arrays contain ancillary information about these wetland locations: vegetation type is from the data of Matthews (1983) and soil type from the data of Zobler (1986). Users should consult original publications for complete discussion of the data bases. This short paper is designed only to document the tape, and briefly explain the data sets and their initial application to estimating the annual emission of methane from natural wetlands. Included is information about array characteristics such as dimensions, read formats, record lengths, <b>blocksizes</b> and value ranges, and descriptions and translation tables for the individual data bases...|$|R
50|$|A memo (.DBT) file {{consists}} of blocks numbered sequentially (0,1,2, and so on). SET <b>BLOCKSIZE</b> determines {{the size of}} each block. The first block in the memo file, block 0, is the memo file header.|$|E
5000|$|In the Benaloh cryptosystem, if {{the public}} key is the modulus m and the base g with a <b>blocksize</b> of c, then the {{encryption}} of a message x is , for some random [...] The homomorphic property is then ...|$|E
5000|$|... (2) , where [...] is a monic {{irreducible}} polynomial {{of degree}} , guarantee {{the existence of}} a p-ary, linear cyclic code : of <b>blocksize</b> , such that the augmented code [...] is the Hadamard exponent, for Hadamard matrix , with , where the core of [...] is cyclic matrix.|$|E
40|$|The data {{of natural}} images is not stationary, and the coding {{complexity}} of images varies from region to region. How well any particular source coding system works {{is dependent upon}} its design assumptions and how well these assumptions match the data. In this dissertation adaptive low-rate source coders are developed. These coders adapt by adjusting {{the complexity of the}} coder to match the local coding difficulty of the image. This is accomplished by using a threshold driven maximum distortion criterion to select the specific coder used. The different coders are built using variable <b>blocksized</b> transform techniques, and the threshold criterion selects small transform blocks to code the more difficult regions and larger blocks to code the less complex regions. ^ The different coders are interconnected using a quad tree structure. The algorithm that generates the tree and tests the thresholds is independent of the design of block coders. This allows the system designer to select different coders for the different types of images being coded without having to change the entire coding system. A progressive transmission scheme based upon these coders is developed. ^ A set of example systems are constructed to test the feasibility of these systems of source coders. These systems use scalar quantized and vector quantized transform block coders. Some of the systems are extended to include a new modified block truncation coding scheme. They are used to code both monochromatic and color images with good results to rates as low as 0. 3 bits/pel. ^ A theoretical framework is constructed from which the study of these coders can be explored, and an algorithm for selecting the optimal bit allocation for the quantization of transform coefficients is developed. The bit allocation algorithm is more fully developed, and can be used to achieve more accurate bit assignments than the algorithms currently used in the literature. Some upper and lower bounds for the bit-allocation distortion-rate function are developed. An obtainable distortion-rate function is developed for a particular scalar quantizer mixing method {{that can be used to}} code transform coefficients at any rate is also presented. ...|$|R
50|$|If x bits {{are lost}} from the ciphertext, the cipher will output {{incorrect}} plaintext until the shift register once again equals a state it held while encrypting, {{at which point}} the cipher has resynchronized. This will result in at most one <b>blocksize</b> of output being garbled.|$|E
50|$|LTFS v2.0.0 was {{released}} in March 2011, improving the text to clarify and remove ambiguity. It also added support for sparse files; persistent file identifiers; virtual extended attributes for filesystem metadata and control - and defined minimum and recommended <b>blocksize</b> values for LTFS volumes, for compatibility across various HBA hardware implementations.|$|E
50|$|Flashcache {{is built}} {{on top of the}} Linux kernel's device mapper. The data {{structure}} of the cache is a set-associative hash table, in which the cache is divided up into a number of fixed-size sets (buckets), using linear probing within a set to find blocks. The device mapper layer breaks up all I/O requests into <b>blocksize</b> chunks before passing the requests to the cache layer.|$|E
50|$|RAID-Z is a data/parity {{distribution}} scheme like RAID-5, but uses dynamic stripe width: every {{block is}} its own RAID stripe, regardless of <b>blocksize,</b> resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 {{because it does not}} need to perform the usual read-modify-write sequence.|$|E
5000|$|... int main(void){ [...] int i = 0; [...] int entries = 50; /* {{total number}} to process */ int repeat; /* {{number of times}} for while.. */ int left = 0; /* {{remainder}} (process later) */ [...] /* If the number of elements is not be divisible by <b>BLOCKSIZE,</b> */ [...] /* get repeat times required to do most processing in the while loop */ ...|$|E
50|$|AES has a {{fixed block}} size of 128 bits {{and a key}} size of 128, 192, or 256 bits, whereas Rijndael can be {{specified}} with block and key sizes in any multiple of 32 bits, {{with a minimum of}} 128 bits. The <b>blocksize</b> has a maximum of 256 bits, but the keysize has no theoretical maximum. AES operates on a 4×4 column-major order matrix of bytes, termed the state (versions of Rijndael with a larger block size have additional columns in the state).|$|E
50|$|Using {{flash memory}} (NAND memory devices) for caching allows Linux kernel to service random disk IO with better {{performance}} than without the cache. This caching {{applies to all}} disk content, not just the page file or system binaries. Flash memory based devices are usually a magnitude faster than spinning HDDs for random IO, but with less advantage or even slower in sequential read/writes. By default, flashcache caches all full <b>blocksize</b> IOs, but can be configured to only cache random IO whilst ignoring sequential IO.|$|E
50|$|The ZFS {{filesystem}} provides RAID-Z, a data/parity distribution scheme {{similar to}} RAID 5, but using dynamic stripe width: every block {{is its own}} RAID stripe, regardless of <b>blocksize,</b> resulting in every RAID-Z write being a full-stripe write. This, when combined with the copy-on-write transactional semantics of ZFS, eliminates the write hole error. RAID-Z is also faster than traditional RAID 5 {{because it does not}} need to perform the usual read-modify-write sequence. RAID-Z does not require any special hardware, such as NVRAM for reliability, or write buffering for performance.|$|E
