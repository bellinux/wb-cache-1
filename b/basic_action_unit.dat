0|2820|Public
5000|$|... thumbA {{distinctive}} feature of A Game of Thrones is that players place order tokens to every region with {{a unit that}} the player controls, outlining the <b>basic</b> <b>actions</b> <b>units</b> in that region can perform. All orders are revealed once all have been placed, requiring players to strategize and outthink their opponents. In addition, players only have a limited supply of each type of order token, {{limiting the number of}} various actions that can be planned.|$|R
40|$|The {{growing demand}} for service robots {{requires}} a better and more natural human-machine interaction. Given that {{an important part of}} human communication is non-verbal, it is necessary to endow robots with gestural communication capabilities similar to humans. This paper describes the design and construction of a realistic, mechatronic head with high gesture capacity. The proposed design is based on the human anatomy and the facial expressions are defined through the Facial Action Coding System (FACS). The paper shows the implementation details of the mechatronic head and the way a set of servomotors can generate the <b>basic</b> <b>action</b> <b>units</b> of FACS as well as the basic and more complex emotional gestures. This work was supported by the Ministry of Science and Innovation, fundamental research project ref. DPI- 2011 - 25489 and the Junta de Castilla y León, research project ref. VA 013 A 12 - 2...|$|R
5000|$|One of {{the most}} {{important}} attempts to describe facial movements was Facial Action Coding System (FACS). Originally developed by Carl-Herman Hjortsjö [...] in the 1960s and updated by Ekman and Friesen in 1978, FACS defines 46 <b>basic</b> facial <b>Action</b> <b>Units</b> (AUs). A major group of these <b>Action</b> <b>Units</b> represent primitive movements of facial muscles in actions such as raising brows, winking, and talking. Eight AU's are for rigid three-dimensional head movements, (i.e. turning and tilting left and right and going up, down, forward and backward). FACS has been successfully used for describing desired movements of synthetic faces and also in tracking facial activities.|$|R
40|$|AbstractIn {{the study}} of Reliability Technology, {{failures}} reflect in the inability to perform of basic parts. In order to seek the fundamental causes, FMA (function-motion-action) function decomposition model was utilized. By the decomposition stepwise, all <b>basic</b> <b>actions</b> formed the complex mechanism functions were found out, which were called Action. The failure model of <b>action</b> <b>units</b> was built by the fuzzy control theory. The key actions were worked out and the improved design measures were put forward, which proves the method valid...|$|R
40|$|ACCV 2014 Workshops : Singapore, Singapore, November 1 - 2, 2014 Most {{previous}} work of facial action recognition {{focused only on}} verifying whether a certain facial <b>action</b> <b>unit</b> appeared or not on a face image. In this paper, we report our investigation on the semantic relationships of facial <b>action</b> <b>units</b> and introduce a novel method for facial <b>action</b> <b>unit</b> recognition based on <b>action</b> <b>unit</b> classifiers and a Bayes network called Facial <b>Action</b> <b>Unit</b> Association Network (FAUAN). Compared with other methods, the proposed method attempts to identify a set of facial <b>action</b> <b>units</b> of a face image simultaneously. We achieve this goal by three steps. At first, the histogram of oriented gradients (HOG) is extracted as features and after that, a Multi-Layer Perceptron (MLP) is trained for the preliminary detection of each individual facial <b>action</b> <b>unit.</b> At last, FAUAN fuses the responses of all the facial <b>action</b> <b>unit</b> classifiers to determine a best set of facial <b>action</b> <b>units.</b> The proposed method achieves a promising performance on the extended Cohn-Kanade Dataset. Experimental results also show that when the individual unit classifiers are not so good, the performance could improve by nearly 10 % in some cases when FAUAN is used. Department of Electronic and Information Engineerin...|$|R
40|$|This Closure Report {{documents}} the activities undertaken to close Corrective <b>Action</b> <b>Unit</b> 335 : Area 6 Injection Well and Drain Pit, {{according to the}} Federal Facility Agreement and Consent Order. Corrective <b>Action</b> <b>Unit</b> 335 was closed {{in accordance with the}} Nevada Division of Environmental Protection-approved Corrective Action Plan for Corrective <b>Action</b> <b>Unit</b> 335...|$|R
5000|$|By {{identifying}} different facial cues, {{scientists are}} able to map them to their corresponding <b>action</b> <b>unit</b> code. Consequently, they have proposed the following classification of the six basic emotions, according to their <b>action</b> <b>units</b> ("+" [...] here mean [...] "and"): ...|$|R
40|$|We {{propose a}} novel {{convolutional}} neural network architecture to address the fine-grained recognition problem of multi-view dynamic facial <b>action</b> <b>unit</b> detection. We leverage recent gains in large-scale object recognition by formulating the task of predicting {{the presence or absence}} of a specific <b>action</b> <b>unit</b> in a still image of a human face as holistic classification. We then explore the design space of our approach by considering both shared and independent representations for separate <b>action</b> <b>units,</b> and also different CNN architectures for combining color and motion information. We then move to the novel setup of the FERA 2017 Challenge, in which we propose a multi-view extension of our approach that operates by first predicting the viewpoint from which the video was taken, and then evaluating an ensemble of <b>action</b> <b>unit</b> detectors that were trained for that specific viewpoint. Our approach is holistic, efficient, and modular, since new <b>action</b> <b>units</b> can be easily included in the overall system. Our approach significantly outperforms the baseline of the FERA 2017 Challenge, which was the previous state-of-the-art in multi-view dynamic <b>action</b> <b>unit</b> detection, with an absolute improvement of 14 %...|$|R
5|$|The key {{components}} of Feynman's presentation of QED are three <b>basic</b> <b>actions.</b>|$|R
40|$|This Corrective Action Investigation Plan (CAIP) {{contains}} the environmental sample collection objectives and {{the criteria for}} conducting site investigation activities at the Area 3 Compound, specifically Corrective <b>Action</b> <b>Unit</b> (CAU) Number 427, which {{is located at the}} Tonopah Test Range (TTR). The TTR, included in the Nellis Air Force Range, is approximately 255 kilometers (140 miles) northwest of Las Vegas, Nevada. The Corrective <b>Action</b> <b>Unit</b> Work Plan, Tonopah Test Range, Nevada divides investigative activities at TTR into Source Groups. The Septic Tanks and Lagoons Group consists of seven CAUs. Corrective <b>Action</b> <b>Unit</b> Number 427 is one of three septic waste system CAUs in TTR Area 3. Corrective <b>Action</b> <b>Unit</b> Numbers 405 and 428 will be investigated at a future data. Corrective <b>Action</b> <b>Unit</b> Number 427 is comprised of Septic Waste Systems Number 2 and 6 with respective CAS Numbers 03 - 05 - 002 -SW 02 and 03 - 05 - 002 -SW 06...|$|R
40|$|This closure report {{documents}} that the closure activities performed at Corrective <b>Action</b> <b>Unit</b> 358 : Areas 18, 19, 20 Cellars/Mud Pits, were {{in accordance with}} the Nevada Division of Environmental Protection approved Streamlined Approach for Environmental Restoration Plan for Corrective <b>Action</b> <b>Unit</b> 358...|$|R
40|$|Automated {{analysis}} of facial expressions can benefit many domains, from marketing to clinical diagnosis of neurodevelopmental disorders. Facial expressions are typically encoded {{as a combination}} of facial muscle activations, i. e., <b>action</b> <b>units.</b> Depending on context, these <b>action</b> <b>units</b> co-occur in specific patterns, and rarely in isolation. Yet, most existing methods for automatic <b>action</b> <b>unit</b> detection fail to exploit dependencies among them, and the corresponding facial features. To address this, we propose a novel multi-conditional latent variable model for simultaneous fusion of facial features and joint <b>action</b> <b>unit</b> detection. In particular, the proposed model performs feature fusion in a generative fashion via a low-dimensional shared subspace, while simultaneously performing <b>action</b> <b>unit</b> detection using a discriminative classification approach. We show that by combining the merits of both approaches, the proposed methodology outperforms existing purely discriminative/generative methods for the target task. To {{reduce the number of}} parameters, and avoid overfitting, a novel Bayesian learning approach based on Monte Carlo sampling is proposed, to integrate out the shared subspace. We validate the proposed method on posed and spontaneous data from three publicly available data sets (CK+, DISFA, and Shoulder-pain), and show that both feature fusion and joint learning of <b>action</b> <b>units</b> leads to improved performance compared with the state-of-the-art methods for the task...|$|R
5000|$|<b>Basic</b> <b>Actions</b> {{concerning}} Cultural Objects {{being offered}} for sale over the Internet (INTERPOL-UNESCO-ICOM) - 2007 ...|$|R
50|$|Little {{is known}} about the Immediate <b>Action</b> <b>Unit</b> except that it is an elite People's Armed Police unit rather than being drawn from the People's Liberation Army. In {{comparison}} to the Special Police Units which are the SWAT units at the provincial level, the Immediate <b>Action</b> <b>Unit</b> is the SWAT team at the national level. Given the challenges faced by the Government of China in the 21st century, it is suspected that the Immediate <b>Action</b> <b>Unit</b> is primarily tasked with responding to internal emergencies of great concern to the national government.|$|R
40|$|This Corrective Action Decision Document {{has been}} {{prepared}} for the Nevada Test Site's Area 23 Mercury Fire Training Pit (Corrective <b>Action</b> <b>Unit</b> 342) {{in accordance with the}} Federal Facility Agreement and Consent Order (FFACO, 1996). Corrective <b>Action</b> <b>Unit</b> 342 is comprised of Corrective Action Site 23 - 56 - 01. The purpose of this Corrective Action Decision Document is to identify and provide a rationale for the selection of a recommended corrective action alternative for Corrective <b>Action</b> <b>Unit</b> 342. The scope of this document consists of the following: Develop corrective action objectives; Identify corrective action alternative screening criteria; Develop corrective action alternatives; Perform detailed and comparative evaluations of corrective action alternatives in relation to corrective action objectives and screening criteria; and Recommend and justify a preferred corrective action alternative for the Corrective <b>Action</b> <b>Unit...</b>|$|R
40|$|Capturing real facial motions from videos enables {{automatic}} {{creation of}} dynamic models for facial animation. In this paper, we propose an explanation-based facial motion tracking algorithm {{based on a}} piecewise Bézier volume deformation model (PBVD). The PBVD is a suitable model both for synthesis and analysis of facial images. With this model, basic facial movements, or <b>action</b> <b>units,</b> are first interactively defined. Then, by linearly combining these <b>action</b> <b>units,</b> various facial movements are synthesized. The magnitudes of these <b>action</b> <b>units</b> can be estimated from real videos using a model-based tracking algorithm. The predefined PBVD <b>action</b> <b>units</b> may also be adaptively modified to customize the dynamic model for a particular face. In this paper, we first briefly introduce the PBVD model and its application in computer facial animation. Then a coarse-to-fine PBVD-based motion tracking algorithm is presented. We also describe an explanation-based tracking algorithm that takes a collection of predefined <b>action</b> <b>units</b> as the initial dynamic model and adaptively improves this model during the tracking process. Experimental results on PBVD-based animation, model-based tracking, and explanation-based tracking are demonstrated. ...|$|R
40|$|AbstractIn {{reasoning}} about actions, it {{is commonly}} assumed that the dynamics of domains satisfies the Markov Property: the executability conditions {{and the effects of}} all actions are fully determined by the present state of the system. This is true in particular in Reiter's <b>Basic</b> <b>Action</b> Theories in the Situation Calculus. In this paper, we generalize <b>Basic</b> <b>Action</b> Theories by removing the Markov property restriction, making it possible to directly axiomatize actions whose effects and executability conditions may depend on past and even alternative, hypothetical situations. We then generalize Reiter's regression operator, which is the main computational mechanism used for reasoning with <b>Basic</b> <b>Action</b> Theories, {{so that it can be}} used with non-Markovian theories...|$|R
5000|$|Vietnam Civil <b>Actions</b> <b>Unit</b> Citation (also an {{individual}} award) ...|$|R
5000|$|Next Action Star (2004) TV series (producer: Cinema <b>Action</b> <b>Unit)</b> ...|$|R
40|$|Corrective <b>Action</b> <b>Unit</b> 214, Bunkers and Storage Areas, is {{identified}} in the Federal Facility Agreement and Consent Order of 1996. Corrective <b>Action</b> <b>Unit</b> 214 consists of nine Corrective Action Sites located in Areas 5, 11, and 25 of the Nevada Test Site. The Nevada Test Site is located approximately 105 kilometers (65 miles) northwest of Las Vegas, Nevada, in Nye County. Corrective <b>Action</b> <b>Unit</b> 214 was previously characterized in 2004, and results were presented in the Corrective Action Decision Document for 214. Site characterization indicated that soil and/or debris exceeded clean-up criteria for Total Petroleum Hydrocarbons, pesticides, metals, and radiological contamination...|$|R
50|$|The Facial Action Coding System or FACS {{is used to}} {{identify}} facial expression. This identifies the muscles that produce the facial expressions. To measure the muscle movements the <b>action</b> <b>unit</b> (AU) was developed. This system measures the relaxation or contraction of each individual muscle and assigns a unit. More than one muscle can be grouped into an <b>Action</b> <b>Unit</b> or the muscle may be divided into separate <b>action</b> <b>units.</b> The score consists of duration, intensity and asymmetry. This {{can be useful in}} identifying depression or measurement of pain in patients that are unable to express themselves.|$|R
5000|$|Republic of Vietnam Civil <b>Actions</b> <b>Unit</b> Citation with {{palm and}} frame ...|$|R
40|$|Corrective <b>Action</b> <b>Unit</b> 210, Storage Areas and Contaminated Material, is {{identified}} in the Federal Facilities Agreement and Consent Order. This Corrective <b>Action</b> <b>Unit</b> consists of four Corrective Action Sites located in Areas 10, 12, and 15 of the Nevada Test Site. This report documents that the closure activities conducted meet the approved closure standards...|$|R
40|$|Cascade {{regression}} framework {{has been}} shown to be effective for facial landmark detection. It starts from an initial face shape and gradually predicts the face shape update from the local appearance features to generate the facial landmark locations in the next iteration until convergence. In this paper, we improve upon the cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF) for simultaneous facial <b>action</b> <b>unit</b> recognition and facial landmark detection, which are two related face analysis tasks, but are seldomly exploited together. In particular, we first learn the relationships among facial <b>action</b> <b>units</b> and face shapes as a constraint. Then, in the proposed constrained joint cascade regression framework, with the help from the constraint, we iteratively update the facial landmark locations and the <b>action</b> <b>unit</b> activation probabilities until convergence. Experimental results demonstrate that the intertwined relationships of facial <b>action</b> <b>units</b> and face shapes boost the performances of both facial <b>action</b> <b>unit</b> recognition and facial landmark detection. The experimental results also demonstrate the effectiveness of the proposed method comparing to the state-of-the-art works. Comment: International Conference on Computer Vision and Pattern Recognition, 201...|$|R
40|$|Limited {{annotated}} {{training data}} is a challenging prob-lem in <b>Action</b> <b>Unit</b> recognition. In this paper, we investigate how {{the use of}} large databases labelled according to the 6 universal facial expressions can increase the generaliza-tion ability of <b>Action</b> <b>Unit</b> classifiers. For this purpose, we propose a novel learning framework: Hidden-Task Learn-ing. HTL aims to learn a set of Hidden-Tasks (<b>Action</b> <b>Units)</b> for which samples are not available but, in contrast, train-ing data is easier to obtain from a set of related Visible-Tasks (Facial Expressions). To that end, HTL is able to ex-ploit prior knowledge about the relation between Hidden and Visible-Tasks. In our case, we base this prior knowl-edge on empirical psychological studies providing statisti-cal correlations between <b>Action</b> <b>Units</b> and universal facial expressions. Additionally, we extend HTL to Semi-Hidden Task Learning (SHTL) assuming that <b>Action</b> <b>Unit</b> training samples are also provided. Performing exhaustive exper-iments over four different datasets, we show that HTL and SHTL improve the generalization ability of AU classifiers by training them with additional facial expression data. Addi-tionally, we show that SHTL achieves competitive perfor-mance compared with state-of-the-art Transductive Learn-ing approaches which face the problem of limited training data by using unlabelled test samples during training. 1...|$|R
50|$|During a Phase, a {{character}} can perform {{one of several}} <b>Basic</b> <b>Actions,</b> such as Attack, Block (Parry), or Move.|$|R
5000|$|... : Limited use by Special <b>Actions</b> <b>Unit</b> of the Royal Malaysia Police ...|$|R
30|$|The {{second step}} of the {{experiment}} proposed that the facial expressions of the <b>action</b> <b>units</b> (Eyes, Mouth, Eyebrows and Forehead) the lecturers to identify the involvement and comprehension {{of the students in}} the classroom during the lecture. The analysis made to check the effectiveness of the interpretation of student’s comprehension through facial expressions signaled by <b>action</b> <b>units.</b>|$|R
40|$|We {{propose a}} simple {{relaxation}} of Reiter&# 039;s <b>basic</b> <b>action</b> theories, based on fluents without successor state axioms, that accommodates incompleteness beyond the initial database. We prove that fundamental results about <b>basic</b> <b>action</b> theories can be fully recovered {{and that the}} generalized framework allows for natural specifications of various forms of incomplete causal laws. We illustrate this by showing how the evolution of incomplete databases, guarded action theories, and non-deterministic actions can be conveniently specified...|$|R
40|$|Print ISBN: 978 - 1 - 4244 - 9140 - 7 International audienceThis study {{presents}} {{a combination of}} geometric and appearance features used to automatically detect <b>Action</b> <b>Units</b> in face images. We use one multi-kernel SVM for each <b>Action</b> <b>Unit</b> we want to detect. The first kernel matrix is computed using Local Gabor Binary Pattern (LGBP) histograms and a histogram intersection kernel. The second kernel matrix is computed from AAM coefficients and a RBF kernel. During the training step, we combine these two type s of features using the recent SimpleMKL algorithm. SVM outputs are then filtered to exploit dynamic relationships between <b>Action</b> <b>Units...</b>|$|R
40|$|This Streamlined Approach for Environmental Restoration plan {{addresses}} the action {{necessary for the}} clean closure of Corrective <b>Action</b> <b>Unit</b> 461 (Test Area Joint Test Assembly Sites) and Corrective <b>Action</b> <b>Unit</b> 495 (Unconfirmed Joint Test Assembly Sites). The Corrective <b>Action</b> <b>Units</b> are located at the Tonopah Test Range in south central Nevada. Closure for these sites will be completed by excavating and evaluating the condition of each artillery round (if found); detonating the rounds (if necessary); excavating the impacted soil and debris; collecting verification samples; backfilling the excavations; disposing of the impacted soil and debris at an approved low-level waste repository at the Nevada Test Sit...|$|R
50|$|<b>Action</b> <b>Units</b> (AUs) are the {{fundamental}} actions of individual muscles {{or groups of}} muscles.|$|R
5000|$|Korean Presidential Unit Citation, Vietnam Gallantry Cross Unit Citation, Vietnam Civil <b>Actions</b> <b>Unit</b> Citation ...|$|R
5000|$|Republic of Vietnam Civil <b>Actions</b> <b>Unit</b> Citation, First Class, Streamer {{embroidered}} for VIETNAM 1970-1971 ...|$|R
5000|$|... #Subtitle level 3: List of <b>Action</b> <b>Units</b> and <b>Action</b> Descriptors (with {{underlying}} facial muscles) ...|$|R
50|$|<b>Action</b> <b>units</b> (AUs) can be {{examined}} frame by frame, since these micro-expressions are often rapid. Paul Ekman’s research in facial deception has found several constants in certain expressions, with the <b>action</b> <b>units</b> relating to lip-corner pulling (AU12) and cheek-raising (AU6) qualifiers for happiness in most people. Brow-lowering (AU4) and lip-stretching (AU20) are disqualifiers for happiness. Emotional leakage appears in these fleeting expressions.|$|R
