13|4|Public
50|$|In December 1966 SDS shipped the {{entirely new}} Sigma series, {{starting}} with the 16-bit Sigma 2 and the 32-bit Sigma 7, both using common hardware internally. The success of the IBM System/360 {{and the rise of}} the 7-bit ASCII character standard was pushing all vendors to the 8-bit standard from their earlier 6-bit ones. SDS was one of the first companies to offer a machine intended as an alternative to the IBM System/360; although not compatible with the 360, it used similar data formats, the EBCDIC character code, and in other ways, such as its use of multiple registers rather than an accumulator, it was designed to have specifications that were comparable to those of the 360. Various versions of the Sigma 7 followed, including the cut-down Sigma 5 and re-designed Sigma 6. The Xerox Sigma 9 was a major re-design with instruction lookahead and other advanced features, while the Sigma 8 and Sigma 9 mod 3 were low-end machines offered as a migration path for the Sigma 5. Meanwhile, the French national champion CII, as licensee of SDS, sold about 60 Sigma 7 machines in Europe, and developed an upgrade with virtual memory and <b>biprocessor</b> capability, the Iris 80. CII also manufactured and sold some 160 Sigma 2 systems.The Sigma range was very successful in the niche real-time processing field, due to the sophisticated hardware interrupt structure and independent I/O processor. The first node of ARPANET was established by Leonard Kleinrock at UCLA with an SDS Sigma 7 system.|$|E
40|$|Multiprocessors {{and high}} {{performance}} networks offer {{the opportunity to}} construct CLUster of MultiProcessors' (CLUMPs') and use them as parallel computing platforms. The distinctive feature of the CLUMPs' over traditional parallel computers' is their hybrid memory model (message passing between the nodes and shared memory inside the nodes). In this paper, we investigate the performance &aracteristics of a cluster of <b>biprocessor</b> PCs for the NAS 2. 3 parallel Ben&mark using a programming model based on MPI for message passing between <b>biprocessor</b> nodes and OpenMP for shared memory inside <b>biprocessor</b> nodes. The paper provides several contributions. These include: a) Speed-up measurements' of a cluster of <b>biprocessor</b> PCs over a cluster of uniprocessor PCs using the hybrid memory model b) A {{detailed analysis of the}} speed-up results from a breakdown of the ben&marks execution time and c) A performance comparison of a commodity CLUMP with some high performance parallel computers'...|$|E
3000|$|The first {{implementation}} described used {{a shared}} memory <b>biprocessor</b> with hyperthreading (i.e., four virtual processors). Taking {{into consideration the}} parallel algorithm proposed in Section 6.1 for four processors, the following grouping of tasks has been made: [...]...|$|E
40|$|We present {{experimental}} {{results for the}} evaluation of PC clusters that differ on several aspect of their architecture, such as being mono or <b>biprocessors</b> and having a high bandwidth interconnection network or not. These experiments confirm that a good equilibrium between the speed (throughput) of the different components is crucial for a satisfactory performance of such a machine. On the other hand they also show that the latency of the underlying infrastructure is of less importance and can be hidden by applying techniques from coarse grained parallelism...|$|R
40|$|Colloque avec actes et comité de lecture. internationale. International audienceWe present {{experimental}} {{results for the}} evaluation of PC clusters that differ on several aspect of their architecture, such as being mono or <b>biprocessors</b> and having a high bandwidth interconnection network or not. These experiments confirm that a good equilibrium between the speed (throughput) of the different components is crucial for a satisfactory performance of such a machine. On the other hand they also show that the latency of the underlying infrastructure is of less importance and can be hidden by applying techniques from coarse grained parallelism...|$|R
40|$|The Chinese-Italian {{experiment}} ARGO-YBJ in Tibet {{is going}} to start next spring the first period of data-taking with 36 RPC clusters installed. The paper describes the computing model and the hardware resources required for data reconstruction and analysis. The present configuration of the processing farm, composed by 24 <b>biprocessors</b> computing elements and 4 diskserves (7. 5 TB total) is describeds. The software designed and developed for data organization, reconstruction job submission, data analysis and error management is briefly reported. The management software of the farm, including the GRIDWARE queuing and submission package, the DB design and the monitoring software are also described...|$|R
40|$|The {{availability}} of multiprocessors and high performance networks offer {{the opportunity to}} construct CLUMPs (Cluster of Multiprocessors) and use them as paxallel computing platforms. The main distinctive feature of the CLUMP axchitecture over the usual paxallel computers is its hybrid memory model (message passing between the nodes and shaxed memory inside the nodes). Some of the primaxy issues to address for the CLUMP axe: 1) {{to be able to}} execute the existing programs with few modifications 2) to provide some programming models coherent with the performance hieraxchy of the data movements inside the CLUMP 3) to limit the effort of the programmer while ensuring the portability of the codes on a wide vaxiety of CLUMP configurations. We investigate an approach based on the MPI and OpenMP standaxds. The approach consists in the intra-node paxallelization of the MPI programs with an OpenMP directire based paxallel compiler. The paper presents a detailed study of the approach {{in the context of the}} <b>biprocessor</b> PC CLUMPs. It provides three contributions. First, it evaluates the ability of <b>biprocessor</b> PCs to effectively provide a speed up over single processor PCs in the context of shaxed memory paxallel programs. Second, it investigates the method to transform MPI paxallel programs in order to execute them on a CLUMP. Third, it presents the performance evaluation of this method applied on the NAS paxallel benchmaxks executed on a cluster of <b>biprocessor</b> PCs...|$|E
40|$|The {{availability}} of multiprocessors and high performance netvorks offer {{an opportunity to}} construct CLUster of MultiProcessors (CLUMPs) and use them as parallel computing platforms. The distinctive feature of the CLUMPs over traditional parallel computers is their hybrid memory model (message passing betveen the nodes and shared memory inside the nodes). In this paper, ve investigate the performance characteristics of a CLUMP using a programming model close to the hardvare memory model. The programming model is based on MPI for message passing part and OpenMP for shared memory part. The paper provides three contributions. These include: a) Performance potential of <b>biprocessor</b> PC as a single node {{in the context of}} shared memory parallel programs and also as being the processing node of a parallel platform in the context of MPI programs, b) Performance measurements of a cluster of <b>biprocessor</b> PCs for NAS 2. 3 parallel benchmarks using the hybrid memory model and c) Some explanations for the performance results by examining a breakdovn of the benchmarks execution time and also by showing the existence of a theoretical limit for the intra-mukiprocessor speedup...|$|E
40|$|We have {{designed}} an algorithm {{which allows the}} OpenGL geometry transformations to be processed on a multiprocessor system. We have integrated it in Mesa, a 3 D graphics library with an API which {{is very similar to}} that of OpenGL. We get speedup up to 1. 8 for <b>biprocessor</b> without any modification of the application using the library. In this paper we study the issues of our algorithm and how we solve them...|$|E
40|$|In this work, the {{performance}} of a Large Eddy Simulation (LES) code for compressible flows is investigated. A nondissipative sixth-order finite difference scheme is used in order to solve the Favre-filtered governing equations, with a low-pass sixth-order spatial filtering scheme to avoid the growth of high-frequency modes and a fourth order Runge-Kutta scheme is employed to advance the solution in time. The open Navier–Stokes characteristic boundary conditions are implemented to avoid reflection of pressure waves and the Smagorinsky model is used to compute the subgrid scale turbulent viscosity. The LES code is written in Fortran 90 using the Message Passing Interface (MPI) parallelization library. In order to assess the parallel algorithm efficiency, {{the performance}} of the LES code is analyzed in terms of speed-up by using two multiprocessors parallel machines and two parallel implementations of the Thomas algorithm. The code, that is fully portable, shows a good scalability on the Linux Cluster AMD Opteron with 72 <b>biprocessors</b> nodes TYAN GX 28 of the Inter-University Consortium for the Application of Super-Computing for Universities and Research (CASPUR -Rome). Better results are achieved when the parallel partition LU algorithm is used for the solution of the tridiagonal systems of equations. This is a fundamental issue dealing with parallel applications, specifically when the code employs hundreds of processors. This is the case for fully three-dimensional simulations. In order to validate the code, the simulation of a n-heptane non-reacting plane jet entering into air at high pressure and temperature is shown. The fuel is impulsively injected into air. The statistically-averaged velocity profiles are computed at several locations along the axial direction together with the spreading rate of the jet, thus showing a very good agreement with the experimental data. The future development of this work will be the analysis of reacting and non-reacting three-dimensional jets under Diesel conditions...|$|R
40|$|International audienceWe {{study the}} problem of {{minimizing}} the makespan for the precedence multiprocessor constrained scheduling problem with hierarchical communications (Parallel Process. Lett. 10 (1) (2000) 133). We propose an approximation algorithm for the Unit Communication Time hierarchical problem with arbitrary but integer processing times and an unbounded number of <b>biprocessor</b> machines. We extend this result in the case where each cluster has m processors (where m is a fixed constant) by presenting a (2 − 2 /(2 m+ 1)) -approximation algorithm...|$|E
40|$|We {{consider}} the scheduling of <b>biprocessor</b> jobs under sum objective (BPSMS). Given {{a collection of}} unit-length jobs where each job {{requires the use of}} two processors, find a schedule such that no two jobs involving the same processor run concurrently. The objective is to minimize the sum of the completion times of the jobs. Equivalently, we would like to find a sum edge coloring of a given multigraph, i. e. a partition of its edge set into matchings M 1, [...] ., Mt minimizing ∑ t i= 1 i|Mi|. This problem is APX-hard, even in the case of bipartite graphs [M 04]. This special case is closely related to the classic open shop scheduling problem. We give a 1. 8298 -approximation algorithm for BPSMS improving the previously best ratio known of 2 [BBH + 98]. The algorithm combines a configuration LP with greedy methods, using non-standard randomized rounding on the LP fractions. We also give an efficient combinatorial 1. 8886 -approximation algorithm for the case of simple graphs. Key-words: Edge Scheduling, Configuration LP, Approximation Algorithms...|$|E
40|$|In Direct Numerical Simulation, {{the great}} demand for both memory and CPU time is even {{increased}} dealing with chemical reactive phenomena. Hence, parallel computing techniques are needed. Only recently, DNS of reacting phe nomena has been approached in 3 -D conﬁguration [1, 2]. In this work, massively parallel computations of a three-dimensional reacting transient jet are presented. The simulations {{have been performed}} on a Linux Beowulf Cluster HP-IA 32, with 36 <b>biprocessor</b> nodes ProLiant DL 360. An MPI DNS code has been developed and its performance has been assessed by computing the characteristic parameters of the parallel implementation. The values of speed up show good scalability of the code. An initial computation {{has been carried out}} by simulating the n-heptane ignition in a transient jet with a detailed kinetic mechanism [3]. As a consequence of the choice of initial conditions, the ignition takes place {{in the early stages of}} the development of the mixing and the phenomenon is mainly dominated by the large two-dimensional vortex structures [4]. Hence, the results, in terms of ignition delay time and localization of ignition spots, are comparable with those obtained in two-dimensional conﬁgurations [5]. Next, the inﬂuence of 3 -D vortex stretching on the ignition phenomenon will be anal ysed...|$|E
40|$|We {{consider}} scheduling {{problems in}} which jobs {{need to be}} processed through a (shared) network of machines. The network is given {{in the form of}} a graph the edges of which represent the machines. We are also given a set of jobs, each specified by its processing time and a path in the graph. Every job needs to be processed in the order of edges specified by its path. We assume that jobs can wait between machines and preemption is not allowed; that is, once a job is started being processed on a machine, it must be completed without interruption. Every machine can only process one job at a time. The makespan of a schedule is the earliest time by which all the jobs have finished processing. The flow time (a. k. a. the completion time) of a job in a schedule is the difference in time between when it finishes processing on its last machine and when the it begins processing on its first machine. The total flow time (or the sum of completion times) is the sum of flow times (or completion times) of all jobs. Our focus is on finding schedules with the minimum sum of completion times or minimum makespan. In this paper, we develop several algorithms (both approximate and exact) for the problem both on general graphs and when the underlying graph of machines is a tree. Even in the very special case when the underlying network is a simple star, the problem is very interesting as it models a <b>biprocessor</b> scheduling with applications to data migration...|$|E
40|$|International audienceWe assess 3 D frequency-domain {{acoustic}} full-waveform inversion data {{as a tool}} {{to develop}} high-resolution velocity models from low-frequency global-offset data. The inverse problem is based on a classic gradient method. Inversion is applied to few discrete frequencies allowing management of a limited subset of the 3 D data volume. The forward problem is solved with a finite-difference frequency-domain method based on a massively parallel direct solver allowing efficient multiple-shot simulations involving several thousands of sources. The inversion code is fully parallelized for distributed-memory platforms taking advantage of a domain decomposition of the modeled wavefields performed by the direct solver. After validation on simple synthetic tests, full-waveform inversion was applied to two targets (channel and thrust system) of the 3 D SEG/EAGE overthrust model corresponding to 3 D domains of 7 × 8. 8 × 3. 3 km 3 and 13. 5 × 13. 5 × 4. 65 km 3 respectively. The maximum inverted frequencies were 15 Hz and 5 Hz for these 2 applications respectively. A maximum of 20 dual core <b>biprocessor</b> nodes with 8 gigabytes of share memory per node was used for the second case study. The main structures were successfully imaged at a resolution scale consistent with the inverted frequencies. This study confirms the feasibility of 3 D frequency domain full-waveform inversion of global-offset data on large distributed-memory platforms to develop high-resolution velocity models. Further work is required to [i] perform more representative applications on larger computational platforms, [ii] assess the sensitivity of the 3 D full-waveform inversion to the acquisition geometry and to the starting model and [iii] assess whether velocity models developed by full-waveform inversion can be used as improved background model for prestack depth migration...|$|E
40|$|The Finite-Difference Time-Domain (FDTD) {{method is}} applied to the {{analysis}} of vibroacoustic problems and to study the propagation of longitudinal and transversal waves in a stratified media. The potential of the scheme and the relevance of each acceleration strategy for massively computations in FDTD are demonstrated in this work. In this paper, we propose two new specific implementations of the bidimensional scheme of the FDTD method using multi-CPU and multi-GPU, respectively. In the first implementation, an open source message passing interface (OMPI) has been included in order to massively exploit the resources of a <b>biprocessor</b> station with two Intel Xeon processors. Moreover, regarding CPU code version, the streaming SIMD extensions (SSE) and also the advanced vectorial extensions (AVX) have been included with shared memory approaches that take advantage of the multi-core platforms. On the other hand, the second implementation called the multi-GPU code version is based on Peer-to-Peer communications available in CUDA on two GPUs (NVIDIA GTX 670). Subsequently, this paper presents an accurate analysis of the influence of the different code versions including shared memory approaches, vector instructions and multi-processors (both CPU and GPU) and compares them in order to delimit the degree of improvement of using distributed solutions based on multi-CPU and multi-GPU. The performance of both approaches was analysed and it has been demonstrated that the addition of shared memory schemes to CPU computing improves substantially the performance of vector instructions enlarging the simulation sizes that use efficiently the cache memory of CPUs. In this case GPU computing is slightly twice times faster than the fine tuned CPU version in both cases one and two nodes. However, for massively computations explicit vector instructions do not worth it since the memory bandwidth is the limiting factor and the performance tends to be the same than the sequential version with auto-vectorisation and also shared memory approach. In this scenario GPU computing is the best option since it provides a homogeneous behaviour. More specifically, the speedup of GPU computing achieves an upper limit of 12 for both one and two GPUs, whereas the performance reaches peak values of 80 GFlops and 146 GFlops for the performance for one GPU and two GPUs respectively. Finally, the method {{is applied to}} an earth crust profile in order to demonstrate the potential of our approach and the necessity of applying acceleration strategies in these type of applications. Peer ReviewedPostprint (author's final draft...|$|E

