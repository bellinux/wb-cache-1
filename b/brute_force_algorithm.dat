102|3700|Public
25|$|Some hobbyists have {{developed}} computer {{programs that will}} solve Sudoku puzzles using a backtracking algorithm, which {{is a type of}} brute force search. Backtracking is a depth-first search (in contrast to a breadth-first search), because it will completely explore one branch to a possible solution before moving to another branch. Although it has been established that approximately 6.67 x 1021 final grids exist, a <b>brute</b> <b>force</b> <b>algorithm</b> can be a practical method to solve Sudoku puzzles.|$|E
500|$|One {{can test}} whether a graph [...] {{contains}} a -vertex clique, and find any such clique that it contains, using a <b>brute</b> <b>force</b> <b>algorithm.</b> This algorithm examines each subgraph with [...] vertices and checks {{to see whether}} it forms a clique. It takes time , as expressed using big O notation.|$|E
2500|$|The two-element Boolean algebra is also {{important}} in {{the general theory of}} Boolean algebras, because an equation involving several variables is generally true in all Boolean algebras if and only if it is true in the two-element Boolean algebra (which can be checked by a trivial <b>brute</b> <b>force</b> <b>algorithm</b> for small numbers of variables). This can for example be used to show that the following laws (Consensus theorems) are generally valid in all Boolean algebras: ...|$|E
40|$|The preset {{distinguishing}} sequence generation {{problem is}} {{converted into a}} SAT problem to investigate the performance of SAT solvers for generating preset distinguishing sequences. An initial set of experiments are carried out and it is shown that the heuristics of SAT solvers can perform better than <b>brute</b> <b>force</b> <b>algorithms</b> {{that are used to}} generate preset distinguishing sequences...|$|R
40|$|Abstract. In {{the last}} couple of years many {{research}} papers have been devoted to Abelian complexity of words. Recently, Constantinescu and Ilie (Bulletin EATCS 89, 167 – 170, 2006) introduced the notion of Abelian period. In this article we present two quadratic <b>brute</b> <b>force</b> <b>algorithms</b> for computing Abelian periods for special cases and a quasi-linear algorithm for computing all the Abelian periods of a word...|$|R
500|$|For finding -vertex cliques, the <b>brute</b> <b>force</b> search <b>algorithm</b> has {{running time}} [...] Because the {{exponent}} of [...] depends on , this algorithm is not fixed-parameter tractable.|$|R
2500|$|A <b>brute</b> <b>force</b> <b>algorithm</b> {{visits the}} empty cells in some order, filling in digits sequentially, or {{backtracking}} {{when the number}} {{is found to be}} not valid. Briefly, a program would solve a puzzle by placing the digit [...] "1" [...] in the first cell and checking if it is allowed to be there. If there are no violations (checking row, column, and box constraints) then the algorithm advances to the next cell, and places a [...] "1" [...] in that cell. When checking for violations, if it is discovered that the [...] "1" [...] is not allowed, the value is advanced to [...] "2". If a cell is discovered where none of the 9 digits is allowed, then the algorithm leaves that cell blank and moves back to the previous cell. The value in that cell is then incremented by one. This is repeated until the allowed value in the last (81st) cell is discovered.|$|E
5000|$|... #Caption: A Sudoku {{designed}} to work against the <b>brute</b> <b>force</b> <b>algorithm.</b>|$|E
50|$|That {{in effect}} {{means that we}} can compute the matrix in {{logarithmic}} space without using the <b>brute</b> <b>force</b> <b>algorithm</b> to verify that a code has a given satisfied distance.|$|E
40|$|One of {{the known}} methods for solving the {{problems}} with exponential time complexity such as NP-complete problems is using the <b>brute</b> <b>force</b> <b>algorithms.</b> Recently, a new parallel computational framework called Membrane Computing is introduced which can be applied in <b>brute</b> <b>force</b> <b>algorithms.</b> The usual way {{to find a solution}} for the problems with exponential time complexity with Membrane Computing techniques is by P System with active membrane using division rule. It makes an exponential workspace and solves the problems with exponential complexity in a polynomial (even linear) time. On the other hand, searching is currently one of the most used methods for finding solution for problems in real life, that the blind search algorithms are accurate, but their time complexity is exponential such as breadth-first search (BFS) algorithm. In this paper, we proposed a new approach for implementation of BFS by using P system with division rule technique for first time. The theorem shows time complexity of BSF in this framework on randomly binary trees reduced from O(2 d) to O(d) ...|$|R
40|$|Summary. The {{usual way}} to find a {{solution}} for a NP complete problem with Membrane Computing techniques is by <b>brute</b> <b>force</b> <b>algorithms</b> where all the feasible solutions are generated and they are checked simultaneously by using massive parallelism. These solutions work from a theoretical point of view but they are implementable only for small instances of the problem. In this paper we provide a family of P systems which brings techniques from Artificial Intelligence into Membrane Computing and apply them to solve the N-queens problem. ...|$|R
40|$|This paper {{describes}} an {{improvement of the}} <b>brute</b> <b>force</b> determinization <b>algorithm</b> {{in the case of}} homogeneous NFAs, as well as its application to pattern matching. <b>Brute</b> <b>force</b> determinization with limited memory may provide a partially determinized automaton, but its bounded complexity makes it be a fail-safe procedure contrary to the classical subset construction...|$|R
5000|$|... #Caption: The <b>brute</b> <b>force</b> <b>algorithm</b> finds a 4-clique in this 7-vertex graph (the {{complement}} of the 7-vertex path graph) by systematically checking all C(7,4) = 35 4-vertex subgraphs for completeness.|$|E
5000|$|Given n items, {{there will}} be at most [...] subsets and for each legal {{candidate}} set, the running time of computing the values earned is [...] Thus, the efficiency class of <b>brute</b> <b>force</b> <b>algorithm</b> is , being exponential.|$|E
5000|$|While the <b>brute</b> <b>force</b> <b>algorithm</b> is order , several {{efficient}} algorithms exist {{which can}} compute the autocorrelation in order [...] For example, the Wiener-Khinchin theorem allows computing the autocorrelation from {{the raw data}} [...] with two Fast Fourier transforms (FFT): ...|$|E
25|$|In the 2000s, {{by using}} <b>brute</b> <b>force</b> search {{computer}} <b>algorithms,</b> Hex boards {{up to size}} 9×9 (as of 2016) have been completely solved.|$|R
50|$|Suffix {{stripping}} approaches {{enjoy the}} benefit of being much simpler to maintain than <b>brute</b> <b>force</b> <b>algorithms,</b> assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those lexical categories which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.|$|R
40|$|Abstract. A new exact {{algorithm}} for computing answer sets {{of logic}} programs is presented and analyzed. The algorithm takes a logic program in Kernel normal form as an input and computes its answer sets {{by reducing the}} problem to a suitable version of graph colorability. Even though worst-case complexity is exponential, thanks to a straightforward formulation we can prove that the algorithm works in time O ∗ (1. 6181 n), which is asymptotically better than the trivial bound O ∗ (2 n) of the <b>brute</b> <b>force</b> <b>algorithms.</b> We argue that this new algorithm represents a significant progress in terms of worst-case time complexity over traditional branch-and-bound algorithms. ...|$|R
50|$|The maximum {{independent}} set {{problem is}} NP-hard. However, {{it can be}} solved more efficiently than the O(n2 2n) time that would be given by a naive <b>brute</b> <b>force</b> <b>algorithm</b> that examines every vertex subset and checks whether it is an independent set.|$|E
50|$|Some hobbyists have {{developed}} computer {{programs that will}} solve Sudoku puzzles using a backtracking algorithm, which {{is a type of}} brute force search. Backtracking is a depth-first search (in contrast to a breadth-first search), because it will completely explore one branch to a possible solution before moving to another branch. Although it has been established that approximately 6.67 x 1021 final grids exist, a <b>brute</b> <b>force</b> <b>algorithm</b> can be a practical method to solve Sudoku puzzles.|$|E
50|$|This {{algorithm}} finds {{extensive use}} in Computational Fluid Dynamics (CFD) {{in terms of}} holecutting and interpolation when two meshes lie one inside the other. The other variations of the problem would be something like this: Given a place, at which latitude and longitude does it lie? The <b>brute</b> <b>force</b> <b>algorithm</b> would find {{the distance of the}} point from every mesh point and see which is smallest. Another approach would be to use a binary search algorithm which would yield a result comparable in speed to the stencil jumping algorithm. A combination of both the binary search and the stencil jumping algorithm will yield an optimum result in the minimum possible time.|$|E
40|$|The {{usual way}} to find a {{solution}} for a NP complete problem with Membrane Computing techniques is by <b>brute</b> <b>force</b> <b>algorithms</b> where all the feasible solutions are generated and they are checked simultaneously by using massive parallelism. These solutions work from a theoretical point of view but they are implementable only for small instances of the problem. In this paper we provide a family of P systems which brings techniques from Artificial Intelligence into Membrane Computing and apply them to solve the N-queens problem. Ministerio de Ciencia e Innovación TIN 2008 - 04487 -EMinisterio de Ciencia e Innovación TIN- 2009 - 13192 Junta de Andalucía P 08 -TIC- 0420...|$|R
40|$|Part 9 : Tools and Methods IIInternational audienceBack in 1950, Shannon {{introduced}} {{planning in}} board games like Chess as a selective approach, where {{the main idea}} is to select specific branches of the game tree that satisfy certain conditions. He contrasted this approach with <b>brute</b> <b>force</b> Minimax-like methods, based on an exhaustive search of the game tree, that aims to select the best path inside a given search horizon. Historically, the <b>brute</b> <b>force</b> approach won hands down against planning in complex games such as Chess, as the strongest Chess programs nowadays all exploit <b>brute</b> <b>force</b> <b>algorithms.</b> However, planning is still interesting and even necessary in some game-playing domains, for instance based on incomplete information, {{where there is no}} way to evaluate precisely or even build the game tree. In this paper we describe a technique that produced positive results in Kriegspiel, a variant of Chess played as an incomplete information game. Our main result is the definition of an algorithm for combining MonteCarlo search with planning; we tested the algorithm on a strong Kriegspiel program based on MonteCarlo search, and obtained a clear improvement...|$|R
40|$|We {{present an}} {{algorithm}} for computing depth-optimal decompositions of logical operations, leveraging a meet-in-the-middle technique {{to provide a}} significant speed-up over simple <b>brute</b> <b>force</b> <b>algorithms.</b> As an illustration of our method we implemented this algorithm and found factorizations of the commonly used quantum logical operations into elementary gates in the Clifford+T set. In particular, we report a decomposition of the Toffoli gate over the set of Clifford and T gates. Our decomposition achieves a total T-depth of 3, thereby providing a 40 % reduction over the previously best known decomposition for the Toffoli gate. Due {{to the size of}} the search space the algorithm is only practical for small parameters, such as the number of qubits, and the number of gates in an optimal implementation. Comment: 23 pages, 15 figures, 1 table; To appear in IEEE Transactions on Computer-Aided Design of Integrated Circuits and System...|$|R
5000|$|The <b>brute</b> <b>force</b> <b>algorithm</b> {{to solve}} this problem is to {{identify}} all possible subsets of the items without exceeding the capacity and select the one with the optimal value. The pseudo-code is provided as follows:// Input:// Profits (stored in array p)// Quadratic profits (stored in matrix P)// Weights (stored in array w)// Number of items (n)// Knapsack capacity (W)int max =0for all subset S do int value, weight = 0 for i from 0 to S.size-1 do: value = value +pi weight = weight +wi for j from i+1 to S.size-1 do: [...] value = value + Pij if weight <= W then: if value > max then: [...] max = value ...|$|E
5000|$|One {{can test}} whether a graph [...] {{contains}} a -vertex clique, and find any such clique that it contains, using a <b>brute</b> <b>force</b> <b>algorithm.</b> This algorithm examines each subgraph with [...] vertices and checks {{to see whether}} it forms a clique. It takes time , as expressed using big O notation.This is because there are [...] subgraphs to check, {{each of which has}} [...] edges whose presence in [...] needs to be checked. Thus, the problem may be solved in polynomial time whenever [...] is a fixed constant. However, when [...] does not have a fixed value, but instead may vary as part of the input to the problem, the time is exponential.|$|E
5000|$|The Bernstein-Sato {{functional}} equation {{is used in}} computations {{of some of the}} more complex kinds of singular integrals occurring in quantum field theory [...] Such computations are needed for precision measurements in elementary particle physics as practiced e.g. at CERN (see the papers citing [...] ). However, the most interesting cases require a simple generalization of the Bernstein-Sato {{functional equation}} to the product of two polynomials , with x having 2-6 scalar components, and the pair of polynomials having orders 2 and 3. Unfortunately, a brute force determination of the corresponding differential operators [...] and [...] for such cases has so far proved prohibitively cumbersome. Devising ways to bypass the combinatorial explosion of the <b>brute</b> <b>force</b> <b>algorithm</b> would be of great value in such applications.|$|E
40|$|The {{solution}} of many problems {{in science and}} engineering is based on computational kernels for the numerical treatment of partial differential equations (PDEs) or N-body problems. Traditional solution methods however reduce these to linear algebra or <b>brute</b> <b>force</b> <b>algorithms</b> on structured data sets. Larger and larger simulations require smarter algorithms to be tractable. Hierarchical tree algorithms represent such a class, both for PDEs and for N-body problems. However, their efficient parallelization is not straightforward. Some difficulties can be removed, if one can provide a fast dynamic load-balancing scheme to cope with the tree variations of the unstructured data sets. In this paper we propose a very cheap yet very efficient load-balancing scheme for tree algorithms based on space-filling curves. Furthermore we present the Parnass 2 cluster, on which such parallel codes perform extremely well. The cluster consists of SMP PCs and a Myrinet network at Gigabit/s speed configured with [...] ...|$|R
40|$|This {{bachelor}} thesis {{shows the}} basics of landscape rendering in computer games. It explains fundamental principles of computer graphics creation {{and implementation of the}} three parts of the landscape - terrain, water surface and the sky. In more detail it describes <b>Brute</b> <b>force</b> and ROAM <b>algorithms</b> for terrain rendering and compares the achieved results of the both of them...|$|R
40|$|Abstract Background In {{population-based}} studies, it {{is generally}} recognized that single nucleotide polymorphism (SNP) markers are not independent. Rather, they are carried by haplotypes, groups of SNPs that tend to be coinherited. It is thus possible to choose a much smaller number of SNPs to use as indices for identifying haplotypes or haplotype blocks in genetic association studies. We refer to these characteristic SNPs as index SNPs. In order to reduce costs and work, a minimum number of index SNPs that can distinguish all SNP and haplotype patterns should be chosen. Unfortunately, this is an NP-complete problem, requiring <b>brute</b> <b>force</b> <b>algorithms</b> that are not feasible for large data sets. Results We have developed a double classification tree search algorithm to generate index SNPs that can distinguish all SNP and haplotype patterns. This algorithm runs very rapidly and generates very good, though not necessarily minimum, sets of index SNPs, as {{is to be expected}} for such NP-complete problems. Conclusions A new algorithm for index SNP selection has been developed. A webserver for index SNP selection is available at [URL] </p...|$|R
5000|$|A <b>brute</b> <b>force</b> <b>algorithm</b> {{visits the}} empty cells in some order, filling in digits sequentially, or {{backtracking}} {{when the number}} {{is found to be}} not valid. Briefly, a program would solve a puzzle by placing the digit [...] "1" [...] in the first cell and checking if it is allowed to be there. If there are no violations (checking row, column, and box constraints) then the algorithm advances to the next cell, and places a [...] "1" [...] in that cell. When checking for violations, if it is discovered that the [...] "1" [...] is not allowed, the value is advanced to [...] "2". If a cell is discovered where none of the 9 digits is allowed, then the algorithm leaves that cell blank and moves back to the previous cell. The value in that cell is then incremented by one. This is repeated until the allowed value in the last (81st) cell is discovered.|$|E
30|$|A closer {{study of}} Fig. 16 {{shows that the}} maximum network {{throughput}} of the considered network can never exceed the throughput of the <b>Brute</b> <b>force</b> <b>algorithm.</b> However, in the scenarios when the best channel clients queue subsequently, the FIFO algorithm can attain the throughput performance of the <b>Brute</b> <b>force</b> <b>algorithm.</b> This case applies to the FIFO combined with Best of the Two Choices algorithm as well, given that the first client in the queue happens to be among {{one of the best}} channels available.|$|E
40|$|Wireless sensor {{networks}} {{are widely used}} {{in a variety of}} fields including industrial environments. In case of a clustered network the location of cluster head affects the reliability of the network operation. Finding of the optimum location of the cluster head, therefore, is critical for the design of a network. This paper discusses the optimisation approach, based on the <b>brute</b> <b>force</b> <b>algorithm,</b> in the context of topology optimisation of a cluster structure centralised wireless sensor network. Two examples are given to verify the approach that demonstrate the implementation of the <b>brute</b> <b>force</b> <b>algorithm</b> to find an optimum location of the cluster head...|$|E
40|$|AbstractIn {{this paper}} we borrow some ideas from quantum computing, and we propose three “quantum” <b>brute</b> <b>force</b> <b>algorithms</b> {{to solve the}} 3 -SAT NP-complete {{decision}} problem. The first algorithm builds, for any instance ϕ of 3 -SAT, a quantum Fredkin circuit that computes a superposition of all classical evaluations of ϕ in a given output line. Similarly, {{the second and third}} algorithms compute the same superposition on a given register of a quantum register machine, and as the energy of a given membrane in a quantum P system, respectively. Assuming that a specific non-unitary operator, built using a truncated version of the well known creation and annihilation operators, can be realized as a quantum gate, as an instruction of the quantum register machine, and as a rule of the quantum P system, respectively, we show how to decide whether ϕ is a positive instance of 3 -SAT. The construction relies also upon the assumption that an external observer is able to discriminate, {{as the result of a}} measurement, a null vector from a non-null vector...|$|R
40|$|Abstract. The K-Nearest Neighbor (KNN) join is an {{expensive}} but important operation in many data mining algorithms. Several recent ap-plications need to perform KNN join for high dimensional sparse data. Unfortunately, all existing KNN join algorithms are designed for low di-mensional data. To fulfill this void, we investigate the KNN join problem for high dimensional sparse data. In this paper, we propose three KNN join algorithms: a <b>brute</b> <b>force</b> (BF) <b>algorithm,</b> an inverted index-based(IIB) algorithm and an improved inverted index-based(IIIB) algorithm. Extensive experiments on both synthetic and real-world datasets were conducted to demonstrate the effectiveness of our algorithms for high dimensional sparse data. ...|$|R
40|$|Standard Sudoku is a logic-based puzzle {{in which}} the user must fill a 9 x 9 board with the {{appropriate}} digits so that each row, column, and individual sub-grids (must be perfect squares) contains the digits 1 - 9. Sudoku is essentially a form of a constraint satisfaction problem, therefore we will use methods discussed in literature and lecture to help us find a solution that is both fast and efficient. Some techniques that we may build from are constraint propagation (forward checking, backtracking search, etc.) and local consistency verification(arc, path, etc.). The standard Sudoku by itself is a very well known problem. It has been solved using just about every possible method, multiple times. <b>Brute</b> <b>force</b> <b>algorithms</b> can solve Sudoku, but aren't efficient in any sense of the term. Constraint satisfaction is exceptionally common and Sudoku almost seems like a mascot for attracting new people to look into constraint programming, since it provides an interesting foundation for constraint solving while being difficult enough to remain interesting. Less conventional means of solving that involve various searching techniques and even genetic algorithms have also been used. In the paper "Stochastic Optimizatio...|$|R
