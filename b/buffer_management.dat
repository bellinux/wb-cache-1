1018|153|Public
2500|$|The {{problem of}} buffer {{overflows}} {{is common in}} the C and C++ languages because they expose low level representational details of buffers as containers for data types. [...] Buffer overflows must thus be avoided by maintaining {{a high degree of}} correctness in code which performs <b>buffer</b> <b>management.</b> It has also long been recommended to avoid standard library functions which are not bounds checked, such as gets, scanf and strcpy. The Morris worm exploited a gets call in fingerd.|$|E
2500|$|Well-written {{and tested}} {{abstract}} data type libraries which centralize and automatically perform <b>buffer</b> <b>management,</b> including bounds checking, can reduce the occurrence and impact of buffer overflows. [...] The two main building-block data types in these languages in which buffer overflows commonly occur are strings and arrays; thus, libraries preventing buffer overflows in these data types can provide {{the vast majority of}} the necessary coverage. [...] Still, failure to use these safe libraries correctly can result in buffer overflows and other vulnerabilities; and naturally, any bug in the library itself is a potential vulnerability. [...] "Safe" [...] library implementations include [...] "The Better String Library", Vstr [...] and Erwin. The OpenBSD operating system's C library provides the strlcpy and strlcat functions, but these are more limited than full safe library implementations.|$|E
5000|$|<b>Buffer</b> <b>management</b> by {{both the}} {{processors}} and the communication network.|$|E
40|$|This paper aims to {{describe}} variation of rice buffer stock in Indonesia, to analyze rice <b>buffer</b> stock <b>management</b> and making of rice <b>buffer</b> stock <b>management</b> strategy in Indonesia. This research used literature research method which analyzing in systematic descriptive and simple tabulation. The result of {{study showed that}} rice <b>buffer</b> stock <b>management</b> in Indonesia was done following the principal of management science which began from planning, organizing, directing and controlling which arranged in minister regulation No. 30 in 2008 about rice buffer stock in Indonesia. At last, Rice <b>buffer</b> stock <b>management</b> strategy in Indonesia were strengthening rice buffer stock in domestic level, making people had power to improving rice buffer stock and developing price scheme in regional level...|$|R
40|$|Grid {{technologies}} are {{emerging as the}} next generation of distributed computing, allowing the aggregation of resources that are geographically distributed across different locations. The network remains an important requirement for any Grid application, as entities involved in a Grid system (such as users, services, and data) need {{to communicate with each other}} over a network. The performance of the network must therefore be considered when carrying out tasks such as scheduling, migration or monitoring of jobs. Network <b>buffers</b> <b>management</b> policies affect the network performance, as they can lead to poor latencies (if buffers become too large), but also leading to a lot of packet droppings and low utilization of links, when trying to keep a low buffer size. Therefore, network <b>buffers</b> <b>management</b> policies should be considered when simulating a real Grid system. In this paper, we introduce network <b>buffers</b> <b>management</b> policies into the GridSim simulation toolkit. Our framework allows new policies to be implemented easily, thus enabling researchers to create more realistic network models. Fields which will harness our work are scheduling, or QoS provision. We present a comprehensive description of the overall design and a use case scenario demonstrating the conditions of links varied over time. ...|$|R
40|$|Storage {{management}} {{is an important}} part of DB 2. The buffer pool in DB 2 is used to catch the disk pages of the database, and its management algorithm can significantly affect performance. Because of the complexity of DB 2 and the workloads running on it, the <b>buffer</b> pool <b>management</b> algorithm is hard to study, config, and tune. In order to investigate the <b>buffer</b> pool <b>management</b> algorithm under controlled circumstances, a trace of buffer pool requests was collected and a trace-driven simulator was developed. The impact of various parameters of the <b>buffer</b> pool <b>management</b> algorithm was studied in the simulator. Relationships among different activities competing for storage spac...|$|R
50|$|Mesa GBM is an {{abstraction}} of the graphics driver specific <b>buffer</b> <b>management</b> APIs (for instance the various libdrm_* libraries), implemented internally by calling into the Mesa GPU drivers.|$|E
50|$|In FreeBSD, all device {{files are}} in fact raw devices. Support for non-raw devices was removed in FreeBSD 4.0 {{in order to simplify}} <b>buffer</b> <b>management</b> and {{increase}} scalability and performance.|$|E
50|$|The {{research}} work driven on Tunisia in the 1980s on network flow control based on <b>buffer</b> <b>management</b> is {{considered as a}} base to the now selective reject algorithms used in the Internet.|$|E
40|$|The Buffer Zone (BZ) {{concept has}} been {{introduced}} in Nepal as {{a key component of}} the national biodiversity conservation strategy to mitigate the impacts of protected areas on local communities, and thereby reduce adverse impacts of local people on protected areas. Unlike traditional Buffer Zone programmes which are mostly limited to creating a protective layer and/or distributing economic benefits to local people, the <b>Buffer</b> Zone <b>management</b> approach in Nepal integrates livelihoods and conservation issues and their linkages in a more holistic and balanced manner. The programme has been successful in establishing a network of community institutions and in mobilising large numbers of local communities in conservation and community development. The research findings clearly indicate that the current <b>Buffer</b> Zone <b>management</b> approach based on park revenue sharing for community development has been successful in developing positive attitudes among local people towards protected areas. There is also evidence of improvement in the condition of forests and biodiversity in the Buffer Zone and a decrease in pressure inside the protected areas for basic forestry resources. The BZ communities also feel empowered by the <b>Buffer</b> Zone <b>management</b> programme. These outputs suggest that if properly designed, the <b>Buffer</b> Zone <b>management</b> programme can achieve both conservation and development objectives ensuring the long-term integrity of the protected areas. At the same time, however, the research has also revealed that the existing incentives and institutional arrangements adopted in the <b>Buffer</b> Zone <b>management</b> programme were necessary but not sufficient to address present and potential challenges in Chitwan National Park. There is a need to use additional instruments to demonstrate <b>Buffer</b> Zone <b>management</b> as a viable conservation governance strategy to expand conservation into the areas beyond park boundaries ensuring greater stability of the Park. Any park management strategy seeking to make tangible impacts on conservation, livelihood and governance should have five elements, namely; incentive, empowerment, education, enforcement and integration (IEEEI); and appropriate policy and institutional frameworks to implement them in an integrated way. If issues such as inclusion, equity, empowerment and integration are properly incorporated into the policies and programmes of the <b>Buffer</b> Zone <b>management,</b> the <b>Buffer</b> Zone <b>management</b> strategy adopted in Chitwan could be promoted as a viable model for the sustainable management of protected areas situated in a human dominated landscape...|$|R
5000|$|MVPG {{is used by}} VSAM Local Shared Resources (LSR) <b>buffer</b> pool <b>management</b> {{to access}} <b>buffers</b> in a hiperspace in Expanded Storage.|$|R
40|$|This paper {{presents}} {{a method for}} buffering critical resources in make-to-order shop floor control in manufacturing complex products. The research was done in Marine Diesel Engines Factory, HCP S. A. Poznan conditions. HCP S. A. Poznan is the biggest producer of high- power marine engines in Europe. These methods of buffering critical resources include procedures of <b>buffers</b> <b>management,</b> (<b>buffers</b> configuration and optimization), a system of disruptions compensation and feedback protecting from destructive influence of “wandering bottlenecks. ” The flow simulations show that this method {{is more effective than}} classical drum-buffer-rope production solution, according to TOC, using FIFO rule. Finally, the lead-time and the level of work-in-process were about 20 % lower than the solution recommended to TOC. Keywords: TOC – (Theory of Constraints), Make-to-order company, A-plant (V-A-T analysis...|$|R
50|$|Today DSPnano claims full POSIX {{capabilities}} for threads, communication, synchronization and I/O. A {{full complement}} of I/O is included as is a {{full complement of}} DSP optimized features including: DSP libraries, fix size <b>buffer</b> <b>management,</b> software pipelines and more. It has also been moved to FPGA platforms to accelerate DSP applications.|$|E
50|$|Generic <b>Buffer</b> <b>Management</b> (GBM) is an API which {{provides}} a mechanism for allocating buffers for graphics rendering tied to Mesa. GBM {{is intended to be}} used as a native platform for EGL on drm or openwfd. The handle it creates can be used to initialize EGL and to create render target buffers.|$|E
50|$|Once {{inventory}} {{is managed}} as described above, continuous {{efforts should be}} undertaken to reduce RT, late deliveries, supplier minimum order quantities (both per SKU and per order) and customer order batching. Any improvements in these areas will automatically improve both availability and inventory turns, thanks to the adaptive nature of <b>Buffer</b> <b>Management.</b>|$|E
40|$|Because of {{the slow}} access time of disk storage, storage {{management}} {{is crucial to}} the performance of many large scale computer systems. This thesis studies performance issues in <b>buffer</b> cache <b>management</b> and disk layout management, two important components of storage <b>management.</b> The <b>buffer</b> cache stores popular disk pages in memory to speed up the access to them. <b>Buffer</b> cache <b>management</b> algorithms used in real systems often have many parameters that require careful hand-tuning to get good performance. A self-tuning algorithm is proposed to automatically tune the page cleaning activity in the <b>buffer</b> cache <b>management</b> algorithm by monitoring the I/O activities of the buffer cache. This algorithm achieves performance comparable to the best manually tuned system. The global data structure used by the <b>buffer</b> cache <b>management</b> algorithm is protected by a lock. Access to this lock can cause contention which can significantly reduce system throughput in multi-processor systems. Current solutions to eliminate lock contention decrease the hit ratio of the buffer cache, which causes poor performance when the system is I/O-bound. A new approach, called the multi-region cache, is proposed. This approach eliminates lock contention, maintains the hit ratio of the buffer cache, and incurs little overhead. Moreover, this approach can be applied to most <b>buffer</b> cache <b>management</b> algorithms. Disk layout management arranges the layout of pages on disks to improve the disk I/O efficiency. The typical disk layout approach, called Overwrite, is optimized for sequential I/Os from a single file. Interleaved writes from multiple users can significantly decrease system throughput in large scale systems using Overwrite. Although the Log-structured File System (LFS) is optimized for such workloads, its garbage collection overhead can be expensive. In modern and future disks, because of the much faster improvement of disk transfer bandwidth over disk positioning time, LFS performs much better than Overwrite in most workloads, unless the disk is close to full. A new disk layout approach, called HyLog, is proposed. HyLog achieves performance comparable to the best of existing disk layout approaches in most cases...|$|R
40|$|International audienceThe article {{presents}} the results of research on creating a dedicated planning system and shop floor control in the conditions of make-to-order in manufacturing complex products. The research was done in Marine Diesel Engines Factory, HCP S. A. Poznan conditions. HCP S. A. Poznan is the biggest producer of high-power marine engines in Europe. The results of the research is working out a method for buffering critical resources shop floor control. These methods of buffering critical resources include procedures of <b>buffers</b> <b>management,</b> (<b>buffers</b> configuration and optimization), a system of disruptions compensation and feedback protecting from destructive influence of “wandering bottlenecks. ” The flow simulations show that this method is more effective than classical drum-buffer-rope production solution, according to TOC, using FIFO rule. Finally, the lead-time and the level of work-in-process were about 20 % lower than the solution recommended to TOC. In {{the last part of the}} article a place and a method of implementing the solution in a planning system of an enterprise are discussed...|$|R
50|$|In 1999, the {{conservation}} area was {{converted into a}} buffer zone. Under the <b>Buffer</b> Zone <b>Management</b> Guidelines {{the conservation}} of forests, wildlife and cultural resources received top priority, followed by conservation of other natural resources and development of alternative energy.|$|R
5000|$|Yet another Android-specific {{solution}} is [...] "Gralloc". Gralloc handles device memory i.e. it does allocation, arbitration, it handles synchronization via Android/Linux fence file descriptors (FDs). Gralloc competes with other solutions like e.g. Mesa's Generic <b>Buffer</b> <b>Management</b> (GBM) or Nvidia's EGLStreams. The gralloc hardware abstraction layer (HAL) {{is used to}} allocate the buffers that underly [...] "surfaces".|$|E
5000|$|The {{service was}} used maliciously to crash MS DNS servers running Microsoft Windows NT 4.0 by piping the {{arbitrary}} characters {{straight into the}} DNS server listening port (telnet ntbox 19 | telnet ntbox 53). [...] However, the attack was presumably a symptom of improper <b>buffer</b> <b>management</b> {{on the part of}} Microsoft's DNS service and not directly related to the CHARGEN service.|$|E
50|$|Critical chain project {{management}} uses <b>buffer</b> <b>management</b> instead of {{earned value management}} to assess {{the performance of a}} project. Some project managers feel that the earned value management technique is misleading, because it does not distinguish progress on the project constraint (i.e., on the critical chain) from progress on non-constraints (i.e., on other paths). Event chain methodology can determine a size of project, feeding, and resource buffers.|$|E
30|$|The {{authors of}} [10] argue that, for {{performance}} reasons, kernels {{are unable to}} enforce a strict IOMMU security policy regarding DMA <b>buffers</b> <b>management.</b> <b>Buffers</b> shared between a driver and a peripheral for DMA are allocated and deallocated at a given frequency. Each time a DMA buffer is allocated or freed, the IOMMU configuration has to be updated. To commit configuration changes, the CPU has to flush the IOMMU translation caches called Input/Output Translation Lookaside Buffer (IOTLB) to update the security policy. However, the update cannot be committed at each change of the DMA memory map because that will severely degrade the system performances. Indeed, this operation consumes an average of 2000 CPU cycles in Intel Sandy Bridge architecture. This is why the IOTLB flush is deferred by the kernel and performed at a lower frequency than DMA mapping changes in the system. This performance scaling technique opens potential time intervals during which a buffer is still available to a peripheral {{despite the fact that}} it had been reallocated. To our knowledge, this vulnerability has not yet been exploited to date.|$|R
50|$|Applicants {{are able}} to choose {{from a wide range}} of options (e.g. hedgerow management, low input grassland, <b>buffer</b> strips, <b>management</b> plans and options to protect soils), {{covering}} all farming types. Each option will earn ‘points’ (e.g. 100 points per hectare) towards their points total.|$|R
40|$|In the {{previous}} lecture {{we talked about}} the Combined Input Output Queued (CIOQ) switch, we divided the switch policy into two main parts: 1. Scheduling – How we pass packets from the switch inputs to its outputs. 2. <b>Buffers</b> <b>Management</b> (When the size of the buffers is finite) – accepting/throwing packets from the buffers. In this part of the lecture we will focus on the management of a single buffer, where the buffer has a fixed size. Our goal is to maximize the throughput of the switch, that is, the number of packets transmitted. In the more general case, each packet has a weight/value, which can stand for its priority (Diff Serv), and our goal is to maximize the sum of weights/values of packets transmitted...|$|R
5000|$|To {{handle all}} these new buffers, the Direct Rendering Manager had to {{incorporate}} new functionality, specifically a graphics memory manager. DRI2 was initially developed using the experimental TTM memory manager, but it was later rewritten to use GEM after it {{was chosen as the}} definitive DRM memory manager. The new DRI2 internal <b>buffer</b> <b>management</b> model also solved two major performance bottlenecks present in the original DRI implementation: ...|$|E
5000|$|The {{problem of}} buffer {{overflows}} {{is common in}} the C and C++ languages because they expose low level representational details of buffers as containers for data types. Buffer overflows must thus be avoided by maintaining {{a high degree of}} correctness in code which performs <b>buffer</b> <b>management.</b> It has also long been recommended to avoid standard library functions which are not bounds checked, such as , [...] and [...] The Morris worm exploited a [...] call in fingerd.|$|E
50|$|<b>Buffer</b> <b>management,</b> therefore, {{represents}} a crucial attribute {{of the theory}} of constraints. There are many ways to apply buffers, but the most often used is a visual system of designating the buffer in three colors: green (okay), yellow (caution) and red (action required). Creating this kind of visibility enables the system as a whole to align and thus subordinate to the need of the constraint in a holistic manner. This can also be done daily in a central operations room that is accessible to everybody.|$|E
40|$|Abstract: Storage {{management}} {{is important for}} overall performance of the system. Slow access time of disk storage is crucial for performance of large scale computer systems. Two important components of storage <b>management</b> are <b>buffer</b> cache and disk layout <b>management.</b> The <b>buffer</b> cache is used to store disk pages in memory {{to speed up the}} access to them. <b>Buffer</b> cache <b>management</b> algorithms require careful hand-tuning for good performance. Aself-tuning algorithm automatically manages to clean the page activity in the <b>buffer</b> cache <b>management</b> algorithm by monitoring the I/O activities. <b>Buffer</b> cache <b>management</b> algorithm is used for global data structure and it is protected by lock. Lock can because contention is helpful in reducing the throughput of the multi-processing system. Currently used solution for eliminating lock contention will directly degrade hit ratio of the buffer cache will result in poor performance in the I/O bound. The new approach, multi-region cache eliminates the lock contention without affecting the hit ratio of buffer cache. Disk layout approach improves the disk I/O efficiency, called Overwrite, and is optimized for sequential me /so from a single file. A new disk layout approach, called HyLog. HyLog achieves performance comparable to the best of existing disk layout approaches in most cases...|$|R
40|$|This paper {{presents}} a simulation {{of a new}} dynamic <b>buffer</b> allocation <b>management</b> scheme in ATM networks. To achieve this objective, an algorithm that detects congestion and updates the dynamic buffer allocation scheme was developed for the OPNET simulation package via {{the creation of a}} new ATM module...|$|R
40|$|Due to the {{explosive}} increases {{of data from}} both the cyber and physical worlds, the demand for database support in embedded systems is increasing. Databases for embedded systems, or embedded databases, are expected to provide timely in-situ data services under various resource constraints, such as limited energy. However, traditional <b>buffer</b> cache <b>management</b> schemes, in which the primary goal is to minimize the number of I/O operations, is problematic since they do not consider the constraints of modern embedded devices such as limited energy and distinctive underlying storage. In particular, due to asymmetric read/write characteristics of flash memory-based storage of modern embedded devices, minimum buffer cache misses neither coincide with minimum power consumption nor minimum I/O deadline misses. In this {{paper we propose a}} novel power- and time-aware <b>buffer</b> cache <b>management</b> scheme for embedded databases. A novel multi-dimensional feedback control architecture is proposed and the characteristics of underlying storage of modern embedded devices is exploited for the simultaneous support of the desired I/O power consumption and the I/O deadline miss ratio. We have shown through an extensive simulation that our approach satisfies both power and timing requirements in I/O operations under a variety of workloads while consuming significantly smaller buffer space than baseline approaches. Keywords: Real-time database, embedded database, <b>buffer</b> cache <b>management,</b> power-awareness, time awareness, feedback contro...|$|R
50|$|The CHARGEN service {{may be used}} as {{a source}} of a byte-stream for {{debugging}} TCP network code for proper bounds checking and <b>buffer</b> <b>management.</b> It may also be a source of generic payload for bandwidth measurement and/or QoS fine-tuning. Although consideration must be given if hardware compression is active, as the output from the CHARGEN service is easily and efficiently compressed. This compression can cause bandwidth tests to report the size of the data after decompression, instead of the actual amount of data which passed the wire.|$|E
50|$|It is {{necessary}} to discuss the formation of deadlines. Deadlines are the constraints for soon-to-be replaced data accessed by the transaction. Deadlines can be either observant or predictive. In an observant deadline system, all unfinished transactions are examined and the processor determines whether any had met its deadline. Problems arise in this method because of variations caused by seek time variations, <b>buffer</b> <b>management</b> and page faults (An Overview of Real-Time Database Systems). A more stable way of organizing deadlines is the predictive method. It builds a candidate schedule and determines if a transaction would miss its deadline under the schedule.|$|E
50|$|MINIX 3 uses fixed-length {{messages}} for internal communication, which eliminates certain buffer overflows and <b>buffer</b> <b>management</b> problems. Also, many exploits work by overrunning a buffer to trick the program into {{returning from a}} function call using an overwritten stack return address pointing into attacker controlled memory, usually the overrun buffer. In MINIX 3, this attack is mitigated because instruction and data space are split and only code in (read-only) instruction space can be executed, termed executable space protection. However attacks which rely on running legitimately executable memory in a malicious way (return-to-libc, return-oriented programming) are not prevented by this mitigation.|$|E
40|$|Sequentiality of {{requested}} blocks on disks, {{or their}} spatial locality, {{is critical to}} the performance of disks, where the throughput of accesses to sequentially placed disk blocks can be an order of magnitude higher than that of accesses to randomly placed blocks. Unfortunately, spatial locality of cached blocks is largely ignored and only temporal locality is considered in system <b>buffer</b> cache <b>management.</b> Thus, disk performance for workloads without dominant sequential accesses can be seriously degraded. To address this problem, we propose a scheme called DULO (DUal LOcality), which exploits both temporal and spatial locality in <b>buffer</b> cache <b>management.</b> Leveraging the filtering effect of the buffer cache, DULO can influence the I/O request stream by making the requests passed to disk more sequential, significantly increasing the effectiveness of I/O scheduling and prefetching for disk performance improvements. DULO ha...|$|R
5|$|Each {{background}} processor {{consisted of}} a computation section, a control section and local memory. The computation section performed 64-bit scalar, floating point and vector arithmetic. The control section provided instruction <b>buffers,</b> memory <b>management</b> functions, and a real-time clock. 16 kwords (128 kbytes) of high-speed local memory was incorporated into each background processor for use as temporary scratch memory.|$|R
40|$|Abstract:- Heterogeneous {{devices have}} been adopted widely in mobile storage systems because a {{combination}} of such devices can supply a synergistically useful storage solution {{by taking advantage of}} each device. In heterogeneous storage systems there have been several researches for enhancing I/O performance by devising proper <b>buffer</b> cache <b>management</b> algorithms. This paper presents a novel device-aware <b>buffer</b> cache <b>management</b> algorithm by employing adaptive cache replacement called ARC, considering both I/O cost per device and workload patterns in mobile computing systems with a heterogeneous storage pair of a hard disk and a NAND flash memory. The proposed algorithm seeks to enhance an existing device-aware cache partitioning technique. Extensive simulations show that the proposed algorithm reduces the total I/O cost over the existing buffer cache algorithms on typical mobile traces. Key-Words:- heterogeneous mobile storage, performance optimization, device-aware cache management, dynamic cache partitioning, workload-aware management, adaptive cache replacemen...|$|R
