10000|85|Public
5|$|Bovens, L. and Hartmann, S. (2003), <b>Bayesian</b> Epistemology, Oxford University Press, Oxford.|$|E
25|$|Efficient {{algorithms}} {{exist that}} perform inference {{and learning in}} <b>Bayesian</b> networks. <b>Bayesian</b> networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic <b>Bayesian</b> networks. Generalizations of <b>Bayesian</b> networks that can represent and solve decision problems under uncertainty are called influence diagrams.|$|E
25|$|One of {{the many}} {{applications}} of Bayes' theorem is <b>Bayesian</b> inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the <b>Bayesian</b> probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. <b>Bayesian</b> inference is fundamental to <b>Bayesian</b> statistics.|$|E
40|$|While {{a lot of}} {{attention}} has been given to two <b>Bayesians</b> with different prior distributions, using the same data, there has to date been no consideration given to two <b>Bayesians</b> using the same prior with independent data sets coming from the same source. In this paper we consider two such <b>Bayesians</b> and show under what conditions on the prior they will be guaranteed to eventually agree with each other as the samples increase...|$|R
40|$|Say that {{an agent}} is "epistemically humble" {{if she is}} less than certain that her {{opinions}} will converge to the truth, given an appropriate stream of evidence. Is such humility rationally permissible? According to the orgulity argument (Belot 2013) : the answer is "yes" but long-run convergence-to-the-truth theorems force <b>Bayesians</b> to answer "no. " That argument has no force against <b>Bayesians</b> who reject countable additivity as a requirement of rationality. Such <b>Bayesians</b> are free to count even extreme humility as rationally permissible...|$|R
50|$|However, it {{is common}} among <b>Bayesians</b> to {{consider}} an alternative parametrization of the normal distribution {{in terms of the}} precision, defined as the reciprocal of the variance, which allows the gamma distribution to be used directly as a conjugate prior. Other <b>Bayesians</b> prefer to parametrize the inverse gamma distribution differently, as a scaled inverse chi-squared distribution.|$|R
25|$|For {{more on the}} {{application}} of Bayes’ theorem under the <b>Bayesian</b> interpretation of probability, see <b>Bayesian</b> inference.|$|E
25|$|In 2011 it {{was shown}} by Polson and Scott that the SVM admits a <b>Bayesian</b> {{interpretation}} through the technique of data augmentation. In this approach the SVM {{is viewed as a}} graphical model (where the parameters are connected via probability distributions). This extended view allows for the application of <b>Bayesian</b> techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the <b>Bayesian</b> SVM was developed by Wenzel et al. enabling the application of <b>Bayesian</b> SVMs to big data.|$|E
25|$|The {{theory of}} <b>Bayesian</b> {{integration}} {{is based on}} the fact that the brain must deal with a number of inputs, which vary in reliability. In dealing with these inputs, it must construct a coherent representation of the world that corresponds to reality. The <b>Bayesian</b> integration view is that the brain uses a form of <b>Bayesian</b> inference. This view has been backed up by computational modeling of such a <b>Bayesian</b> inference from signals to coherent representation, which shows similar characteristics to integration in the brain.|$|E
50|$|An {{alternative}} approach to formalising probability, favoured by some <b>Bayesians,</b> {{is given by}} Cox's theorem.|$|R
40|$|In a {{paper that}} has been widely-cited within the {{philosophy}} of science community, Glymour 1 claims to show that <b>Bayesians</b> cannot learn from old data. His argument contains elementary errors, ones which E. T. Jaynes and others have often warned against. I explain exactly where Glymour went wrong, and {{how to handle the}} problem correctly. When the problem is fixed, it is seen that <b>Bayesians,</b> just like logicians, can indeed learn from old data...|$|R
50|$|Overprecision {{could have}} {{important}} implications for investing behavior and stock market trading. Because <b>Bayesians</b> cannot agree to disagree, classical finance theory has trouble explaining why, if stock market traders are fully rational <b>Bayesians,</b> there is so much trading in the stock market. Overprecision might be one answer. If market actors are too sure their estimates of an asset's value is correct, they will be too willing to trade with others who have different information than they do.|$|R
25|$|Bayes’ theorem is {{fundamental}} to <b>Bayesian</b> inference. It is a subset of statistics, providing a mathematical framework for forming inferences through the concept of probability, in which evidence about the true {{state of the world}} is expressed in terms of degrees of belief through subjectively assessed numerical probabilities. Such a probability is known as a <b>Bayesian</b> probability. The fundamental ideas and concepts behind Bayes’ theorem, and its use within <b>Bayesian</b> inference, have been developed and added to over the past centuries by Thomas Bayes, Richard Price and Pierre Simon Laplace as well as numerous other mathematicians, statisticians and scientists. <b>Bayesian</b> inference has experienced spikes in popularity as it has been seen as vague and controversial by rival frequentist statisticians. In the past few decades <b>Bayesian</b> inference has become widespread in many scientific and social science fields such as marketing. <b>Bayesian</b> inference allows for decision making and market research evaluation under uncertainty and limited data.|$|E
25|$|This book {{discusses}} the <b>Bayesian</b> approach towards item response modeling. The {{book will be}} useful for persons (who are familiar with IRT) {{with an interest in}} analyzing item response data from a <b>Bayesian</b> perspective.|$|E
25|$|Together with Ho's student Robert Lee at MIT, {{the paper}} A <b>Bayesian</b> {{approach}} to problems in stochastic estimation and control formulated a general class of stochastic estimation and control problems from a <b>Bayesian</b> Decision-Theoretic viewpoint.|$|E
5000|$|According to the Oxford English Dictionary, {{the term}} 'frequentist' {{was first used}} by M. G. Kendall in 1949, to {{contrast}} with <b>Bayesians,</b> whom he called [...] "non-frequentists". He observed ...|$|R
40|$|To {{what extent}} is the logic of {{rational}} belief an extension of classical logic? <b>Bayesians</b> explicate the notion of rational belief via the theory of subjective probability, and the axioms of probability imply that logical truths (respectively falsehoods) are given probability 1 (respectively 0), so for <b>Bayesians</b> {{there is a very}} strong tie between classical logic and rational belief. Indeed, there is a strong tradition amongst philosophers (including [Keynes 1921], [Ramsey 1926] and more recently [Howson 1997]), which interprets the theory of probability as a straightforward generalisation of classical logic. I shall give some reasons why I don't think the link between classical logic and rational belief is quite as close as many <b>Bayesians</b> would like to make out, and suggest a way that the link can be relaxed. Consider a hypothetical rational agent, Xenelda say, or X for short, who forms beliefs over sentences s of a logical language. The function bel X over those sentences [...] ...|$|R
40|$|The {{marginalization}} paradox {{involves a}} disagreement between two <b>Bayesians</b> who use two different procedures for calculating a posterior {{in the presence}} of an improper prior. We show that the argument used to justify the procedure of one of the <b>Bayesians</b> is inapplicable. There is therefore no reason to expect agreement, no paradox, and no evidence that improper priors are inherently inconsistent. We show further that the procedure in question can be interpreted as the cancellation of infinities in the formal posterior. We suggest that the implicit use of this formal procedure {{is the source of the}} observed disagreement. Comment: 5 pages. Minor change to discussion section. Comments welcom...|$|R
25|$|In {{the late}} 1980s Judea Pearl's text Probabilistic Reasoning in Intelligent Systems and Richard E. Neapolitan's text Probabilistic Reasoning in Expert Systems {{summarized}} {{the properties of}} <b>Bayesian</b> networks and established <b>Bayesian</b> networks as a field of study.|$|E
25|$|A <b>Bayesian</b> network, Bayes network, belief network, <b>Bayes(ian)</b> {{model or}} {{probabilistic}} directed acyclic graphical {{model is a}} probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a <b>Bayesian</b> network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases.|$|E
25|$|<b>Bayesian</b> {{networks}} {{are a very}} general tool {{that can be used}} for a large number of problems: reasoning (using the <b>Bayesian</b> inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic <b>Bayesian</b> networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).|$|E
2500|$|<b>Bayesians</b> {{point to}} the work of Ramsey (p 182) and de Finetti (p 103) as proving that {{subjective}} beliefs must follow the laws of probability {{if they are to be}} coherent. [...] Evidence casts doubt that humans will have coherent beliefs.|$|R
50|$|Together {{with the}} {{sufficiency}} principle, Birnbaum's {{version of the}} principle implies the famous likelihood principle. Although {{the relevance of the}} proof to data analysis remains controversial among statisticians, many <b>Bayesians</b> and likelihoodists consider the likelihood principle foundational for statistical inference.|$|R
5000|$|<b>Bayesians</b> {{point to}} the work of Ramsey (p 182) and de Finetti (p 103) as proving that {{subjective}} beliefs must follow the laws of probability {{if they are to be}} coherent. [...] Evidence casts doubt that humans will have coherent beliefs.|$|R
25|$|<b>Bayesian</b> {{inference}} is one proposed {{alternative to}} significance testing. (Nickerson cited 10 sources suggesting it, including Rozeboom (1960)). For example, <b>Bayesian</b> parameter estimation can provide rich {{information about the}} data from which researchers can draw inferences, while using uncertain priors that exert only minimal influence on the results when enough data is available. Psychologist John K. Kruschke has suggested <b>Bayesian</b> estimation as an alternative for the t-test. Alternatively two competing models/hypothesis can be compared using Bayes factors. <b>Bayesian</b> methods could be criticized for requiring information that is seldom available in the cases where significance testing is most heavily used. Neither the prior probabilities nor the probability distribution of the test statistic under the alternative hypothesis are often available in the social sciences.|$|E
25|$|At {{about the}} same time, Dan Roth proved that exact {{inference}} in <b>Bayesian</b> networks is in fact #P-complete (and thus as hard as {{counting the number of}} satisfying assignments of a CNF formula) and that approximate inference, even for <b>Bayesian</b> networks with restricted architecture, is NP-hard.|$|E
25|$|Likened to a <b>Bayesian</b> network, an HTM {{comprises}} {{a collection}} of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A <b>Bayesian</b> belief revision algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to <b>Bayesian</b> networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.|$|E
40|$|Trivials are {{effect sizes}} {{associated}} with statistically non-significant results. Trivials are like Tribbles in the Star Trek television show. They are cute and loveable. They proliferate without limit. They probably growl at <b>Bayesians.</b> But they are troublesome. This brief report discusses {{the trouble with}} trivials...|$|R
40|$|The {{literature}} on the Sleeping Beauty problem {{has been dominated by}} <b>Bayesians.</b> 1 Even those authors who are not <b>Bayesians</b> 2 have addressed the problem without using much of the rich machinery available to objective probability theorists. We show that the objective probability theorist has a very simple argument for thirdism. Objective Probability <b>Bayesians</b> take “definite ” or “single-case ” probabilities to be basic. Definite probabilities attach to closed formulas or propositions. We write them here using small caps: PROB(P) and PROB(P/Q). Most objective probability theories begin instead with “indefinite ” or “general ” probabilities (sometimes called “statistical probabilities”). Indefinite probabilities attach to open formulas or propositions. We write indefinite probabilities using lower case “prob ” and free variables: prob(Bx/Ax). The indefinite probability of an A being a B is not about any particular A, but rather about the property of being an A. In this respect, its logical form is {{the same as that of}} relative frequencies. For instance, we might talk about the probability of a human baby being female. That probability is about human babies in general — not about individuals. If we examine a baby and determine conclusively that she is female, then the definite probability of her being female is 1, bu...|$|R
5000|$|Probability is {{measured}} by an interval (some mistake this as an affinity to Dempster-Shafer theory, but Kyburg firmly rejects their rule of combination; his work remained closer to confidence intervals, and was often interpreted by <b>Bayesians</b> as a commitment {{to a set of}} distributions, which Kyburg did not repudiate) ...|$|R
25|$|There {{are three}} main {{inference}} tasks for <b>Bayesian</b> networks.|$|E
25|$|There {{is also a}} <b>Bayesian</b> {{characterization}} of the Kullback–Leibler divergence.|$|E
25|$|An empiricist, <b>Bayesian</b> {{approach}} to the foundations of probability theory.|$|E
25|$|Subjectivists, {{also known}} as <b>Bayesians</b> or {{followers}} of epistemic probability, give the notion of probability a subjective status by regarding it {{as a measure of}} the 'degree of belief' of the individual assessing the uncertainty of a particular situation. Epistemic or subjective probability is sometimes called credence, as opposed to the term chance for a propensity probability.|$|R
50|$|Further, as {{discussed}} at decision theory: alternatives to probability theory, probabilists, particularly <b>Bayesians</b> probabilists, argue that optimal decision rules (formally, admissible decision rules) {{can always be}} derived by probabilistic methods (this is the statement of the complete class theorems), and thus that non-probabilistic methods such as info-gap are unnecessary and do not yield new or better decision rules.|$|R
40|$|Abstract P-values are a {{practical}} success but a critical fail-ure. Scientists {{the world over}} use them, but scarcely a statistician can be found to defend them. <b>Bayesians</b> in particular find them ridiculous, but even the modern fre-quentist has little time for them. In this essay, I consider what, if anything, might be said in their favour...|$|R
