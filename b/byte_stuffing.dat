15|4|Public
25|$|Within the entropy-coded data, after any 0xFF byte, a 0x00 byte is {{inserted}} by the encoder {{before the next}} byte, so that there {{does not appear to}} be a marker where none is intended, preventing framing errors. Decoders must skip this 0x00 byte. This technique, called <b>byte</b> <b>stuffing</b> (see JPEG specification section F.1.2.3), is only applied to the entropy-coded data, not to marker payload data. Note however that entropy-coded data has a few markers of its own; specifically the Reset markers (0xD0 through 0xD7), which are used to isolate independent chunks of entropy-coded data to allow parallel decoding, and encoders are free to insert these Reset markers at regular intervals (although not all encoders do this).|$|E
5000|$|... 8-bit data length indicator: 0-16 (not {{counting}} additional bytes inserted by the <b>byte</b> <b>stuffing</b> rule) ...|$|E
50|$|The Media Access Control {{sublayer}} also determines {{where one}} frame of data {{ends and the}} next one starts - frame synchronization. There are four means of frame synchronization: time based, character counting, <b>byte</b> <b>stuffing</b> and bit stuffing.|$|E
5000|$|In SDLC the {{transmitted}} bit sequence [...] "01111110" [...] containing six adjacent 1 bits is the Flag <b>byte.</b> Bit <b>stuffing</b> {{ensures that}} this pattern can never occur in normal data, {{so it can}} be used as a marker for the beginning and end of frame without any possibility of being confused with normal data.|$|R
50|$|For STS-1, the payload is {{referred}} to as the synchronous payload envelope (SPE), which in turn has 18 <b>stuffing</b> <b>bytes,</b> leading to the STS-1 payload capacity of 756 bytes.|$|R
50|$|Value of each coil is binary (0 for off, 1 for on). First {{requested}} coil {{is stored}} as {{least significant bit}} of first byte in request.If number of coils is not a multiple of 8, most significant bit(s) of last <b>byte</b> should be <b>stuffed</b> with zeros. See example for function codes 1 and 2.|$|R
5000|$|<b>Byte</b> <b>stuffing</b> {{precedes the}} frame {{with a special}} byte {{sequence}} such as DLE STX and succeeds it with DLE ETX. Appearances of DLE (byte value 0x10) have to be escaped with another DLE. The start and stop marks are detected at the receiver and removed {{as well as the}} inserted DLE characters.|$|E
5000|$|The syncwords {{can be seen}} as a kind of delimiter. Various {{techniques}} are used to avoid delimiter collision, orin other wordsto [...] "disguise" [...] bytes of data at the data link layer that might otherwise be incorrectly recognized as the syncword. For example, HDLC uses bit stuffing or [...] "octet stuffing", while other systems use ASCII armor or Consistent Overhead <b>Byte</b> <b>Stuffing</b> (COBS).|$|E
50|$|As {{the last}} step of a data exchange, the master {{finishes}} a command by sending a synchronization byte SYN (0xaa). This signals to other masters that the bus is available for use again. A <b>byte</b> <b>stuffing</b> rule (0xa9 -> 0xa9 0x00, 0xaa -> 0xa9 0x01) is applied to all other bytes sent, to ensure that 0xaa does not appear on the wire as part of any transmitted data other than in the final synchronization byte.|$|E
5000|$|Value of each coil/discrete input is binary (0 for off, 1 for on). First {{requested}} coil/discrete input {{is stored}} as {{least significant bit}} of first byte in reply.If number of coils/discrete inputs is not a multiple of 8, most significant bit(s) of last <b>byte</b> will be <b>stuffed</b> with zeros.For example, if eleven coils are requested, two bytes of values are needed. Suppose states of those successive coils are on, off, on, off, off, on, on, on, off, on, on, then the response will be [...] in hexadecimal.|$|R
50|$|<b>Byte</b> <b>stuffing</b> is {{a process}} that {{transforms}} a sequence of data bytes that may contain 'illegal' or 'reserved' values (such as packet delimiter) into a potentially longer sequence that contains no occurrences of those values. The extra length of the transformed sequence is typically referred to as the overhead of the algorithm. The COBS algorithm tightly bounds the worst-case overhead, limiting it to no more than one byte in 254. The algorithm is computationally inexpensive and its average overhead is low compared to other unambiguous framing algorithms.|$|E
50|$|Consistent Overhead <b>Byte</b> <b>Stuffing</b> (COBS) is an {{algorithm}} for encoding data bytes {{that results}} in efficient, reliable, unambiguous packet framing regardless of packet content, thus making it easy for receiving applications to recover from malformed packets. It employs a particular byte value, typically zero, {{to serve as a}} packet delimiter (a special value that indicates the boundary between packets). When zero is used as a delimiter, the algorithm replaces each zero data byte with a non-zero value so that no zero data bytes will appear in the packet and thus be misinterpreted as packet boundaries. The value substituted for each zero data byte is equal to one plus the number of non-zero data bytes that follow.|$|E
50|$|Within the entropy-coded data, after any 0xFF byte, a 0x00 byte is {{inserted}} by the encoder {{before the next}} byte, so that there {{does not appear to}} be a marker where none is intended, preventing framing errors. Decoders must skip this 0x00 byte. This technique, called <b>byte</b> <b>stuffing</b> (see JPEG specification section F.1.2.3), is only applied to the entropy-coded data, not to marker payload data. Note however that entropy-coded data has a few markers of its own; specifically the Reset markers (0xD0 through 0xD7), which are used to isolate independent chunks of entropy-coded data to allow parallel decoding, and encoders are free to insert these Reset markers at regular intervals (although not all encoders do this).|$|E
40|$|<b>Byte</b> <b>stuffing</b> is {{a process}} that {{transforms}} a sequence of data bytes that may contain ‘illegal ’ or ‘reserved ’ values into a potentially longer sequence that contains no occurrences of those values. The extra length is referred to in this paper as the overhead of the algorithm. To date, <b>byte</b> <b>stuffing</b> algorithms, such as those used by SLIP [RFC 1055], PPP [RFC 1662] and AX. 25 [ARRL 84], have been designed to incur low average overhead but have made little effort to minimize worst case overhead. Some increasingly popular network devices, however, care more about the worst case. For example, the transmission time for ISM-band packet radio transmitters is strictly limited by FCC regulation. To adhere to this regulation, the practice is to set the maximum packet size artificially low so that no packet, even after worst case overhead, can exceed the transmission time limit. This paper presents a new <b>byte</b> <b>stuffing</b> algorithm, called Consistent Overhead <b>Byte</b> <b>Stuffing</b> (COBS), that tightly bounds the worst case overhead. It guarantees in the worst case to add no more than one byte in 254 to any packet. Furthermore, the algorithm is computationally cheap, and its average overhead is very competitive with that of existing algorithms. 1...|$|E
40|$|This {{full text}} paper was peer {{reviewed}} {{at the direction}} of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. Bandwidth-efficient <b>byte</b> <b>stuffing</b> Abstract — <b>Byte</b> <b>stuffing</b> is a technique to allow the transparent transmission of arbitrary sequences with constrained sequences. To date, most of the existing algorithms, such as PPP, attain a low average overhead by sacrificing the worstcase scenario. An exception is COBS which was designed for a low worst-case overhead; however, it imposes always a nonzero overhead, even on small packets. In this work is proposed a <b>byte</b> <b>stuffing</b> algorithm that simultaneously controls the average and worst-case overhead, performing close to the theoretical bound. It is shown analytically that the proposed algorithm achieves improved average and worst-case rates over state of the art methods. Furthermore, this technique is generalized to hybrid methods, with lower computing complexity. It is further analysed and compared experimentally the behaviour of the proposed algorithm against established algorithms in terms of byte overhead and computational time. I...|$|E
40|$|Abstract—Byte {{stuffing}} is {{a process}} that encodes a sequence of data bytes that may contain ‘illegal ’ or ‘reserved ’ values, using a potentially longer sequence that contains no occurrences of these values. The extra length is referred to here as the overhead of the encoding. To date, <b>byte</b> <b>stuffing</b> algorithms, such as those used by SLIP, PPP and AX. 25, have been designed to incur low average overhead, but little effort has been made to minimize their worstcase overhead. However, there are some increasingly popular network devices whose performance is determined more by the worst case than by the average case. For example, the transmission time for ISM-band packet radio transmitters is strictly limited by FCC regulation. To adhere to this regulation, the current practice is to set the maximum packet size artificially low so that no packet, even after worst-case overhead, can exceed the transmission time limit. This paper presents a new <b>byte</b> <b>stuffing</b> algorithm, called Consistent Overhead <b>Byte</b> <b>Stuffing</b> (COBS), which tightly bounds the worst-case overhead. It guarantees in the worst case to add no more than one byte in 254 to any packet. For large packets this means that their encoded size is no more than 100. 4 % of their pre-encoding size. This is much better than the 200 % worst-case bound that is common for many <b>byte</b> <b>stuffing</b> algorithms, and is close to the information-theoretic limit of about 100. 07 %. Furthermore, the COBS algorithm is computationally cheap, and its average overhead is very competitive with that of existing algorithms...|$|E
40|$|Abstract—Byte {{stuffing}} is {{a technique}} to allow the trans-parent transmission of arbitrary sequences with constrained sequences. To date, most of the existing algorithms, such as PPP, attain a low average overhead by sacrificing the worst-case scenario. An exception is COBS which was designed for a low worst-case overhead; however, it imposes always a nonzero overhead, even on small packets. In this work is proposed a <b>byte</b> <b>stuffing</b> algorithm that simultaneously controls the average and worst-case overhead, performing close to the theoretical bound. It is shown analytically that the proposed algorithm achieves improved average and worst-case rates over {{state of the art}} methods. Furthermore, this technique is generalized to hybrid methods, with lower computing complexity. It is further analysed and compared experimentally the behaviour of the proposed algorithm against established algorithms in terms of byte overhead and computational time. I...|$|E
40|$|<b>Byte</b> <b>stuffing</b> is {{a process}} that encodes a {{sequence}} of data bytes that may contain "illegal" or "reserved" values, using a potentially longer sequence that contains no occurrences of these values. The extra length is referred to here as the overhead of the encoding. To date, byte-stuffing algorithms, such as those used by SLIP, PPP, and AX. 25, have been designed to incur low average overhead, but little effort has been made to minimize their worst-case overhead. However, there are some increasingly popular network devices whose performance is determined more by the worst case than by the average case. For example, the transmission time for ISM-band packet radio transmitters is strictly limited by FCC regulation. To adhere to this regulation, the current practice is to set the maximum packet size artificially low so that no packet, even after worst-case overhead, can exceed the transmission time limit. This paper presents a new byte-stuffing algorithm, called consistent overhead byte st [...] ...|$|E
40|$|There are two key {{components}} for high throughput distributed anonymizing applications. The first key component is overhead due to message {{complexity of the}} utilized algorithms. The second key component is an nonscalable architecture {{to deal with this}} high throughput. These issues are compounded by the need for anonymization. Using a state of the art serialization technology has been shown to increase performance, in terms of CPU utilization, by 65 %. This is due to the compression (<b>byte</b> <b>stuffing)</b> used by this technology. It also decreased lines of code in the Tribler project by roughly 2000 lines. Single-core architectures are shown to be optimizable by performing a minimum s,t-cut on the data flows within the original architecture: between the entry point and the most costly CPU-utilizing component as derived from profiling the application. This method is used on the Tribler technology to create a multi-core architecture. The resulting architecture is shown to be significantly more efficient in terms of consumed CPU for the delivered file download speed. Lastly it is shown that, by utilizing a cutting-edge cryptographic protocol, anonymizing file streaming applications can be sped up to a degree such that a web application implementations become feasible. Electrical Engineering, Mathematics and Computer ScienceSoftware TechnologyParallel and Distributed Systems grou...|$|E

