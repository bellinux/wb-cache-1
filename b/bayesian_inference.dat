7069|1111|Public
5|$|The {{problem of}} {{induction}} discussed above {{is seen in}} another form in debates over the foundations of statistics. The standard approach to statistical hypothesis testing avoids claims about whether evidence supports a hypothesis or makes it more probable. Instead, the typical test yields a p-value, which is the probability of the evidence being such as it is, {{under the assumption that}} the hypothesis being tested is true. If the p-value is too low, the hypothesis is rejected, in a way analogous to falsification. In contrast, <b>Bayesian</b> <b>inference</b> seeks to assign probabilities to hypotheses. Related topics in philosophy of statistics include probability interpretations, overfitting, and the difference between correlation and causation.|$|E
25|$|Bayes’ theorem is {{fundamental}} to <b>Bayesian</b> <b>inference.</b> It is a subset of statistics, providing a mathematical framework for forming inferences through the concept of probability, in which evidence about the true {{state of the world}} is expressed in terms of degrees of belief through subjectively assessed numerical probabilities. Such a probability is known as a Bayesian probability. The fundamental ideas and concepts behind Bayes’ theorem, and its use within <b>Bayesian</b> <b>inference,</b> have been developed and added to over the past centuries by Thomas Bayes, Richard Price and Pierre Simon Laplace as well as numerous other mathematicians, statisticians and scientists. <b>Bayesian</b> <b>inference</b> has experienced spikes in popularity as it has been seen as vague and controversial by rival frequentist statisticians. In the past few decades <b>Bayesian</b> <b>inference</b> has become widespread in many scientific and social science fields such as marketing. <b>Bayesian</b> <b>inference</b> allows for decision making and market research evaluation under uncertainty and limited data.|$|E
25|$|One of {{the many}} {{applications}} of Bayes' theorem is <b>Bayesian</b> <b>inference,</b> a particular approach to statistical inference. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. <b>Bayesian</b> <b>inference</b> is fundamental to Bayesian statistics.|$|E
40|$|We correct some {{conclusions}} {{presented by}} Consonni and Marin (2007) {{on the performance}} of mean-field variational approximations to <b>Bayesian</b> <b>inferences</b> {{in the case of a}} simple probit model. We show that some of their presentations are misleading and thus their results do not fairly present the performance of such approximations in terms of point estimation under the specified model. Variational <b>inference</b> <b>Bayesian</b> probit model Gibbs sampling...|$|R
40|$|In this paper, {{we propose}} new {{estimation}} techniques {{in connection with}} the system of S-distributions. Besides “exact” maximum likelihood (ML), we propose simulated ML and a characteristic function-based procedure. The “exact” and simulated likelihoods can be used to provide numerical, MCMC-based <b>Bayesian</b> <b>inferences...</b>|$|R
40|$|We {{consider}} various {{properties of}} <b>Bayesian</b> <b>inferences</b> related to repeated sampling interpretations, {{when we have}} a proper prior. While these can be seen as particularly relevant when the prior is diffuse, we argue that it is generally reasonable to consider such properties as part of our assessment of <b>Bayesian</b> <b>inferences.</b> We discuss the logical implications for how repeated sampling properties should be assessed {{when we have a}} proper prior. We develop optimal <b>Bayesian</b> repeated sampling <b>inferences</b> using a generalized idea of what it means for a credible region to contain a false value and discuss the practical use of this idea for error assessment and experimental design. We present results that connect Bayes factors with optimal inferences and develop a generalized concept of unbiasedness for credible regions. Further, we consider the effect of reparameterizations on hpd-like credible regions and argue that one reparameterization is most relevant, when repeated sampling properties and the prior are taken into account...|$|R
25|$|In marketing, <b>Bayesian</b> <b>inference</b> {{allows for}} {{decision}} making and market research evaluation under uncertainty and with limited data.|$|E
25|$|For {{more on the}} {{application}} of Bayes’ theorem under the Bayesian interpretation of probability, see <b>Bayesian</b> <b>inference.</b>|$|E
25|$|This {{question}} {{can be answered}} by {{counting the number of}} geographic transitions on the phylogeny via parsimony, maximum likelihood or through <b>Bayesian</b> <b>inference.</b>|$|E
40|$|This paper {{examines}} {{signal detection}} {{in the presence}} of noise, with particular emphasis on nuclear activation analysis. The specific problem is to decide whether the signal-plus-background or the no-signal hypothesis fits the data better. Our solution is based on the use of <b>Bayesian</b> <b>inferences</b> to test the two hypotheses...|$|R
40|$|Explanation {{facilities}} are a particularly {{important feature of}} expert system frameworks. It is {{an area in which}} traditional rule-based expert system frameworks have had mixed results. While explanations about control are well handled, {{facilities are}} needed for generating better explanations concerning knowledge base content. This paper approaches the explanation problem by examining the effect an event has on a variable of interest within a symmetric <b>Bayesian</b> <b>inferencing</b> system. We argue that any effect measure operating in this context must satisfy certain properties. Such a measure is proposed. It forms the basis for an explanation facility which allows the user of the Generalized <b>Bayesian</b> <b>Inferencing</b> System to question the meaning of the knowledge base. That facility is described in detail. Comment: Appears in Proceedings of the Second Conference on Uncertainty in Artificial Intelligence (UAI 1986...|$|R
40|$|FIGURE 2. Maximum {{likelihood}} dendrogram of Rhacophorus {{derived from}} the analysis of 843 bp of 16 S rRNA mtDNA gene. Voucher samples and GenBank accession numbers are given in Table 1. Numbers near nodes represent bootstrap support values (1000 replicates) for ML / and posterior probabilities (PP) for <b>Bayesian</b> <b>inferences</b> respectively...|$|R
25|$|The {{theory of}} Bayesian {{integration}} {{is based on}} the fact that the brain must deal with a number of inputs, which vary in reliability. In dealing with these inputs, it must construct a coherent representation of the world that corresponds to reality. The Bayesian integration view is that the brain uses a form of <b>Bayesian</b> <b>inference.</b> This view has been backed up by computational modeling of such a <b>Bayesian</b> <b>inference</b> from signals to coherent representation, which shows similar characteristics to integration in the brain.|$|E
25|$|Monte Carlo {{methods are}} used in various fields of {{computational}} biology, for example for <b>Bayesian</b> <b>inference</b> in phylogeny, or for studying biological systems such as genomes, proteins, or membranes.|$|E
25|$|There {{are three}} {{different}} classes of method for ancestral reconstruction. In chronological order of discovery, these are maximum parsimony, maximum likelihood, and <b>Bayesian</b> <b>Inference.</b> Maximum parsimony considers all evolutionary events equally likely; maximum likelihood {{accounts for the}} differing likelihood of certain classes of event; and Bayeisan inference relates the conditional probability of an event to {{the likelihood of the}} tree, as well as the amount of uncertainty that is associated with that tree. Maximum parsimony and maximum likelihood yield a single most probable outcome, whereas <b>Bayesian</b> <b>inference</b> accounts for uncertainties in the data and yields a sample of possible trees.|$|E
40|$|In the {{analysis}} of spatial data, the inverse of the covariance matrix needs to be calculated. For example, the inverse is needed for best linear unbiased prediction or kriging, and is repeatedly calculated in the maximum likelihood estimation or the <b>Bayesian</b> <b>inferences.</b> Since the spatial sample size can be quite large...|$|R
5000|$|<b>Bayesian</b> coalescent <b>inference</b> of past {{population}} dynamics from molecular sequences ...|$|R
40|$|Modern higher-order {{asymptotic}} {{theory for}} frequency inference, as largely described in Barn-dorff-Nielsen & Cox (1994), is fundamentally likelihood-oriented. This {{has led to}} various indica-tions of more unity in frequency and <b>Bayesian</b> <b>inferences</b> than previously recognised, e. g. Sweeting (1987, 1992). A main distinction, the stronger dependence of frequency inference on the sampl...|$|R
25|$|<b>Bayesian</b> <b>inference</b> is {{the method}} that many have argued {{is the most}} accurate. In general, Bayesian {{statistical}} methods allow investigators to combine pre-existing information with new hypothesis. In the case of evolution, it combines {{the likelihood of the}} data observed with the likelihood that the events happened in the order they did, while recognizing the potential for error and uncertainty. Overall, it is the most accurate method for reconstructing ancestral genetic sequences, as well as protein stability. Unlike the other two methods, <b>Bayesian</b> <b>inference</b> yields a distribution of possible trees, allowing for more accurate and easily interpretable estimates of the variance of possible outcomes.|$|E
25|$|It is also {{possible}} {{to take a more}} principled approach to the statistics of n-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in <b>Bayesian</b> <b>inference.</b>|$|E
25|$|In signal {{processing}} and <b>Bayesian</b> <b>inference,</b> particle filters and sequential Monte Carlo techniques are {{a class of}} mean field particle methods for sampling and computing the posterior distribution of a signal process given some noisy and partial observations using interacting empirical measures.|$|E
40|$|Many {{scientific}} and engineering problems require to perform <b>Bayesian</b> <b>inferences</b> in function spaces, {{in which the}} unknowns are of infinite dimension. In such problems, many standard Markov Chain Monte Carlo (MCMC) algorithms become arbitrary slow under the mesh refinement, which {{is referred to as}} being dimension dependent. In this work we develop an independence sampler based MCMC method for the infinite dimensional <b>Bayesian</b> <b>inferences.</b> We represent the proposal distribution as a mixture of a finite number of specially parametrized Gaussian measures. We show that under the chosen parametrization, the resulting MCMC algorithm is dimension independent. We also design an efficient adaptive algorithm to adjust the parameter values of the mixtures from the previous samples. Finally we provide numerical examples to demonstrate the efficiency and robustness of the proposed method, even for problems with multimodal posterior distributions...|$|R
40|$|Abstract: An {{example was}} given in the {{textbook}} All of Statistics (Wasserman, 2004, pages 186 - 188) for arguing that, in the problems with a great many parameters <b>Bayesian</b> <b>inferences</b> are weak, because they rely heavily on the likelihood function that captures information of {{only a tiny fraction}} of the total parameters. Alternatively he suggested non-Bayesian Horwitz-Thompson estimator, which cannot be obtained from a likelihood-based approaches, including Bayesian approaches. He argued that Horwitz-Thompson estimator is good since it is unbiased and consistent. In this paper, I compared the mean square errors of Horwitz-Thompson estimator with a Bayes estimator at a wide range of parameter configurations. I also simulated these two estimators to visualize them directly. From these comparisons, I conclude that the simple Bayes estimator works better than Horwitz-Thompson estimator for most parameter configurations. Hence <b>Bayesian</b> <b>inferences</b> are not weak for this example...|$|R
40|$|Abstract. Bayesian {{networks}} {{have been applied}} for several uncertainty management problems in the artificial intelligence and Web intelligence communities. However, one may {{require the use of}} Bayesian networks, yet lack the background knowledge to build them. Moreover, it is widely acknowledged in the Bayesian network community that understanding <b>Bayesian</b> network <b>inference</b> is an arduous task. In this paper, we solve this dilemma by proposing a Web-based interface for hiding <b>Bayesian</b> network <b>inference.</b> This approach allows a much wider audience to utilize <b>Bayesian</b> network <b>inference</b> without having to understand how the inference process is actually carried out. ...|$|R
25|$|Current {{research}} in sensory processing is divided among a biophysical modelling of different subsystems {{and a more}} theoretical modelling of perception. Current models of perception {{have suggested that the}} brain performs some form of <b>Bayesian</b> <b>inference</b> and integration of different sensory information in generating our perception of the physical world.|$|E
25|$|Stan (software) — Stan is an {{open-source}} {{package for}} obtaining <b>Bayesian</b> <b>inference</b> using the No-U-Turn sampler, {{a variant of}} Hamiltonian Monte Carlo. It's somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. RStan is the R interface to Stan. It is maintained by Andrew Gelman and colleagues.|$|E
25|$|Markov chain {{methods have}} also become very {{important}} for generating sequences of random numbers to accurately reflect very complicated desired probability distributions, via a process called Markov chain Monte Carlo (MCMC). In recent years this has revolutionized the practicability of <b>Bayesian</b> <b>inference</b> methods, allowing {{a wide range of}} posterior distributions to be simulated and their parameters found numerically.|$|E
40|$|When {{case studies}} are {{constructed}} as narratives, then causal explanation {{can be achieved}} without either comparison or generalization. Narratives provide paths of causal links on a chronology of actions or events. The links, in turn, can be studied as <b>Bayesian</b> <b>inferences</b> generating <b>Bayesian</b> narratives. The causal paths in a narrative have a Boolean structure...|$|R
40|$|FIGURE 8. Maximum Likelihood {{tree for}} species of Pareuchiloglanis and its closely allied forms within the glyptosternoid fishes {{inferred}} from mitochondrial cytochrome b gene sequences {{based on a}} TIM 1 +G model (-Ln likelihood = 5749. 5099). Clade credibility values are given for nodes with bootstrap support for ML (above branch) and posterior probability for <b>Bayesian</b> <b>inferences</b> (below branch) ...|$|R
40|$|This paper {{describes}} the implementation {{and performance of}} PBPI, a parallel implementation of <b>Bayesian</b> phylogenetic <b>inference</b> method for DNA sequence data. By combining the Markov Chain Monte Carlo (MCMC) method with likelihood-based assessment of phylogenies, <b>Bayesian</b> phylogenetic <b>inferences</b> can incorporate complex statistic models into the process of phylogenetic tree estimation. However, Bayesian analyses are extremely computationally expensive. PBPI uses algorithmic improvements and parallel processing to achieve significant performance improvement over comparable <b>Bayesian</b> phylogenetic <b>inference</b> programs. We evaluated the performance and accuracy of PBPI using a simulated dataset on System X, a terascale supercomputer at Virginia Tech. Our results show that PBPI identifies equivalent tree estimates 1424 times faster on 256 processors than a widely-used, best-available (albeit sequential), <b>Bayesian</b> phylogenetic <b>inference</b> program. PBPI also achieves linear speedup {{with the number of}} processors for large problem sizes. Most importantly, the PBPI framework enables Bayesian phylogenetic analysis of large datasets previously impracticable. 1...|$|R
25|$|The {{dispute over}} {{formulations}} is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study Neyman–Pearson theory in graduate school. Mathematicians {{are proud of}} uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible or complementary. The dispute has become more complex since <b>Bayesian</b> <b>inference</b> has achieved respectability.|$|E
25|$|In {{the related}} concept of overfitting, {{excessively}} complex models {{are affected by}} statistical noise (a problem {{also known as the}} bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, <b>Bayesian</b> <b>inference,</b> etc.).|$|E
25|$|Some recent {{models of}} {{language}} acquisition have centered around methods of <b>Bayesian</b> <b>Inference</b> {{to account for}} infants' abilities to appropriately parse streams of speech and acquire word meanings. Models of this type rely heavily {{on the notion of}} conditional probability (the probability of A given B), in line with findings concerning infants' use of transitional probabilities of words and syllables to learn words.|$|E
40|$|We {{consider}} Markov chain Monte Carlo algorithms which combine Gibbs updates with Metropolis-Hastings updates, {{resulting in}} a conditional Metropolis-Hastings sampler (CMH sampler). We develop conditions under which the CMH sampler will be geometrically or uniformly ergodic. We illustrate our results by analysing a CMH sampler used for drawing <b>Bayesian</b> <b>inferences</b> about the entire sample path of a diffusion process, based only upon discrete observations...|$|R
5000|$|Bois F., 2009, GNU MCSim: <b>Bayesian</b> {{statistical}} <b>inference</b> for SBML-coded systems biology models, Bioinformatics, 25:1453-1454, doi: 10.1093/bioinformatics/btp162.|$|R
40|$|This paper {{examines}} {{signal detection}} {{in the presence}} of noise, with a particular emphasis to the nuclear activation analysis. The problem is to decide what between the signal-plus-background and no-signal hypotheses fits better the data and to quantify the relevant signal amplitude or detection limit. Our solution is based on the use of <b>Bayesian</b> <b>inferences</b> to test the different hypotheses. Comment: 10 pages, 4 figures, 3 tables, submitted to Metrologi...|$|R
