76|943|Public
25|$|Croteam used {{an array}} of {{automated}} and in-place tools to help rapidly design, debug, and test the game for playability. In one aspect, they recognized {{in the development of}} a puzzle game was that while puzzles could be designed with specific solutions, the process of creating the video game around the puzzle could create unsolvable situations or unforeseen shortcuts. To address this, they used a bot, developed by Croteam member Nathan Brown who had previously developed bots for other games including the ones incorporated into ports of Serious Sam 3 for consoles. The bot, named Bot, would watch the playthrough of a puzzle by a human player in terms of broad actions such as placing boxes on a switch for the completion of a puzzle. Then, as the puzzle's environment was tuned and decorated, they would have Bot attempt to solve the puzzle, testing to make sure it did not run into any dead-ends. If it did encounter any, Bot reported these through an in-house <b>bug</b> <b>reporting</b> system and then used game cheats to move on and finish out testing, which took between 30 and 60 minutes for the full game. As such, they were able to quickly iterate and resolve such problems when new features were introduced to the game. Overall, Croteam estimates they logged about 15,000 hours with Bot before the release of the public test version, and expect to use similar techniques in future games. They also used human playtesters to validate other more aesthetic factors of the games prior to the title's release.|$|E
500|$|PHP/FI {{could help}} to build simple, dynamic web applications. To {{accelerate}} <b>bug</b> <b>reporting</b> {{and to improve}} the code, Lerdorf initially announced the release of PHP/FI as [...] "Personal Home Page Tools (PHP Tools) version 1.0" [...] on the [...] Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995. This release already had the basic functionality that PHP has [...] This included Perl-like variables, form handling, {{and the ability to}} embed HTML. The syntax resembled that of Perl but was simpler, more limited and less consistent.|$|E
5000|$|Suggestions - allow {{suggestions}} {{to be made}} allowing outside participation and <b>bug</b> <b>reporting</b> ...|$|E
40|$|An {{open source}} project {{typically}} maintains an open bug repository so that <b>bug</b> <b>reports</b> {{from all over}} the world can be gathered. When a new <b>bug</b> <b>report</b> is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing <b>bug</b> <b>report.</b> If it is, the triager marks it as DUPLICATE and the <b>bug</b> <b>report</b> is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate <b>bug</b> <b>reports.</b> In this paper we present a new approach that further involves execution information. In our approach, when a new <b>bug</b> <b>report</b> arrives, its natural language information and execution information are compared with those of the existing <b>bug</b> <b>reports.</b> Then, a small number of existing <b>bug</b> <b>reports</b> are suggested to the triager as the most similar <b>bug</b> <b>reports</b> to the new <b>bug</b> <b>report.</b> Finally, the triager examines the suggested <b>bug</b> <b>reports</b> to determine whether the new <b>bug</b> <b>report</b> duplicates an existing <b>bug</b> <b>report.</b> We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67 %- 93 % of duplicate <b>bug</b> <b>reports</b> in the Firefox bug repository, compared to 43 %- 72 % using natural language information alone...|$|R
40|$|Bugs are {{prevalent}} in software systems. To improve {{the reliability of}} software systems, developers often allow end users to provide feedback on bugs that they encounter. Users could perform this by sending a <b>bug</b> <b>report</b> in a <b>bug</b> <b>report</b> management system like Bugzilla. This process however is uncoordinated and distributed, which means that many users could submit <b>bug</b> <b>reports</b> reporting the same problem. These {{are referred to as}} duplicate <b>bug</b> <b>reports.</b> The existence of many duplicate <b>bug</b> <b>reports</b> may cause much unnecessary manual efforts as often a triager would need to manually tag <b>bug</b> <b>reports</b> as being duplicates. Recently, {{there have been a number}} of studies that investigate duplicate <b>bug</b> <b>report</b> problem which in effect answer the following question: given a new <b>bug</b> <b>report,</b> retrieve k other similar <b>bug</b> <b>reports.</b> This, however, still requires substantive manual effort which could be reduced further. Jalbert and Weimer are the first to introduce the direct detection of duplicate <b>bug</b> <b>reports,</b> it answers the question: given a new <b>bug</b> <b>report,</b> classify if it as a duplicate <b>bug</b> <b>report</b> or not. In this paper, we extend Jalbert and Weimer 2 ̆ 7 s work by improving the accuracy of automated duplicate <b>bug</b> <b>report</b> identification. We experiments with <b>bug</b> <b>reports</b> from Mozilla <b>bug</b> tracking system which were reported between February 2005 to October 2005, and find that we could improve the accuracy of the previous approach by about 160...|$|R
40|$|A <b>bug</b> <b>report</b> {{contains}} many fields, such as product, component, severity, priority, fixer, operating system (OS), platform, etc., which provide important {{information for the}} bug triaging and fixing process. It is important {{to make sure that}} bug information is correct since previous studies showed that the wrong assignment of <b>bug</b> <b>report</b> fields could increase the bug fixing time, and even delay the delivery of the software. In this paper, we perform an empirical study on <b>bug</b> <b>report</b> field reassignments in open-source software projects. To better understand why <b>bug</b> <b>report</b> fields are reassigned, we manually collect 99 recent <b>bug</b> <b>reports</b> that had their fields reassigned and emailed their reporters and developers asking why these fields got reassigned. Then, we perform a large-scale empirical study on 8 types of <b>bug</b> <b>report</b> field reassignments in 4 open-source software projects containing a total of 190, 558 <b>bug</b> <b>reports.</b> In particular, we investigate 1) the number of <b>bug</b> <b>reports</b> whose fields get reassigned, 2) the difference in bug fixing time between <b>bug</b> <b>reports</b> whose fields get reassigned and those whose fields are not reassigned, 3) the duration a field in a <b>bug</b> <b>report</b> gets reassigned, 4) the number of fields in a <b>bug</b> <b>report</b> that get reassigned, 5) the number of times a field in a <b>bug</b> <b>report</b> gets reassigned, and 6) whether the experience of bug reporters affect the reassignment of <b>bug</b> <b>report</b> fields. We find that a large number (approximately 80 %) of <b>bug</b> <b>reports</b> have their fields reassigned, and the <b>bug</b> <b>reports</b> whose fields get reassigned require more time to be fixed than those without field reassignments...|$|R
5000|$|... {{compiler}} {{developers and}} <b>bug</b> <b>reporting</b> {{part of the}} community only need to know the language being compiled.|$|E
50|$|On January 29, 2014 the Exponent CMS site {{switched}} to new forum software. In addition, a software <b>bug</b> <b>reporting</b> system is available.|$|E
50|$|On 25 February 2013, Samsung {{announced}} that it will stop developing Bada, moving development to Tizen instead. <b>Bug</b> <b>reporting</b> was finally terminated in April 2014.|$|E
40|$|Abstract—Bugs are {{prevalent}} in software systems. To improve {{the reliability of}} software systems, developers often allow end users to provide feedback on bugs that they encounter. Users could perform this by sending a <b>bug</b> <b>report</b> in a <b>bug</b> <b>report</b> management system like Bugzilla. This process however is uncoordinated and distributed, which means that many users could submit <b>bug</b> <b>reports</b> reporting the same problem. These {{are referred to as}} duplicate <b>bug</b> <b>reports.</b> The existence of many duplicate <b>bug</b> <b>reports</b> may cause much unnecessary manual efforts as often a triager would need to manually tag <b>bug</b> <b>reports</b> as being duplicates. Recently, {{there have been a number}} of studies that investigate duplicate <b>bug</b> <b>report</b> problem which in effect answer the following question: given a new <b>bug</b> <b>report,</b> retrieve k other similar <b>bug</b> <b>reports.</b> This, however, still requires substantive manual effort which could be reduced further. Jalbert and Weimer are the first to introduce the direct detection of duplicate bug reports; it answers the question: given a new <b>bug</b> <b>report,</b> classify if it as a duplicate <b>bug</b> <b>report</b> or not. In this paper, we extend Jalbert and Weimer’s work by improving the accuracy of automated duplicate <b>bug</b> <b>report</b> identification. We experiments with <b>bug</b> <b>reports</b> from Mozilla <b>bug</b> tracking system which were reported between February 2005 to October 2005, and find that we could improve the accuracy of the previous approach by about 160 %. Keywords-Duplicate bug reports; Relative similarity; Bugzilla I...|$|R
40|$|Both {{developers}} and users submit <b>bug</b> <b>reports</b> to a <b>bug</b> repository. These <b>reports</b> can help reveal defects and improve software quality. As {{the number of}} <b>bug</b> <b>reports</b> in a <b>bug</b> repository increases, {{the number of the}} potential duplicate <b>bug</b> <b>reports</b> increases. Detecting duplicate <b>bug</b> <b>reports</b> helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing <b>bug</b> <b>reports.</b> This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of <b>bug</b> <b>reports</b> on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given <b>bug</b> <b>report</b> using natural language and execution information...|$|R
40|$|Context: <b>Bug</b> <b>report</b> assignment, namely, {{to assign}} new <b>bug</b> <b>reports</b> to {{developers}} for timely and effective bug resolution, {{is crucial for}} software quality assurance. However, with the increasing size of software system, {{it is difficult to}} assign bugs to appropriate developers for bug managers. Objective: This paper propose an approach, called KSAP (K-nearest-neighbor search and heterogeneous proximity), to improve automatic <b>bug</b> <b>report</b> assignment by using historical <b>bug</b> <b>reports</b> and heterogeneous network of bug repository. Method: When a new <b>bug</b> <b>report</b> was submitted to the bug repository, KSAP assigns developers for the <b>bug</b> <b>report</b> by using a two-phase procedure. The first phase is to search historically-resolved similar <b>bug</b> <b>reports</b> to the new <b>bug</b> <b>report</b> by K-nearest-neighbor (KNN) method. The second phase is to rank the developers who contributed to those similar <b>bug</b> <b>reports</b> by heterogeneous proximity. Results: We collected bug repositories of Mozilla, Eclipse, Apache Ant and Apache Tomcat 6 projects to investigate the performance of the proposed KSAP approach. Experimental results demonstrate that KSAP can improve the recall of <b>bug</b> <b>report</b> assignment between 7. 5 - 32. 25 % in comparison with the state of art techniques. When there is {{only a small number of}} developer collaborations on common <b>bug</b> <b>reports,</b> KSAP has shown its excellence over other sate of art techniques. When we tune the parameters of the number of historically-resolved similar <b>bug</b> <b>reports</b> (K) and the number of developers (Q) for recommendation, KSAP keeps its superiority steadily. Conclusion: This is the first paper to demonstrate how to automatically build heterogeneous network of a bug repository and extract meta-paths of developer collaborations from the heterogeneous network for <b>bug</b> <b>report</b> assignment. (C) 2015 Elsevier B. V. All rights reserved. Context: <b>Bug</b> <b>report</b> assignment, namely, to assign new <b>bug</b> <b>reports</b> to developers for timely and effective bug resolution, is crucial for software quality assurance. However, with the increasing size of software system, it is difficult to assign bugs to appropriate developers for bug managers. Objective: This paper propose an approach, called KSAP (K-nearest-neighbor search and heterogeneous proximity), to improve automatic <b>bug</b> <b>report</b> assignment by using historical <b>bug</b> <b>reports</b> and heterogeneous network of bug repository. Method: When a new <b>bug</b> <b>report</b> was submitted to the bug repository, KSAP assigns developers for the <b>bug</b> <b>report</b> by using a two-phase procedure. The first phase is to search historically-resolved similar <b>bug</b> <b>reports</b> to the new <b>bug</b> <b>report</b> by K-nearest-neighbor (KNN) method. The second phase is to rank the developers who contributed to those similar <b>bug</b> <b>reports</b> by heterogeneous proximity. Results: We collected bug repositories of Mozilla, Eclipse, Apache Ant and Apache Tomcat 6 projects to investigate the performance of the proposed KSAP approach. Experimental results demonstrate that KSAP can improve the recall of <b>bug</b> <b>report</b> assignment between 7. 5 - 32. 25 % in comparison with the state of art techniques. When there is only a small number of developer collaborations on common <b>bug</b> <b>reports,</b> KSAP has shown its excellence over other sate of art techniques. When we tune the parameters of the number of historically-resolved similar <b>bug</b> <b>reports</b> (K) and the number of developers (Q) for recommendation, KSAP keeps its superiority steadily. Conclusion: This is the first paper to demonstrate how to automatically build heterogeneous network of a bug repository and extract meta-paths of developer collaborations from the heterogeneous network for <b>bug</b> <b>report</b> assignment. (C) 2015 Elsevier B. V. All rights reserved...|$|R
50|$|The last {{official}} {{release is}} Dig 3.2.0b6 announced on 16 June 2004. Only sporadic maintenance is in evidence since that date at the project's <b>bug</b> <b>reporting</b> page. It {{was used by}} the GNU project's website for a long time but was replaced in 2008 with Hyper Estraier.|$|E
50|$|GCCSDK {{comprises}} GCC for RISC OS and the GCC Software Development Kit itself. The GCC for RISC OS Initiative {{was founded}} in 1999 and the combined GCCSDK was hosted via riscos.info (as version 3.4.5) in 2006. Version 4.1.1 (Release 2) was released in 2009, with <b>bug</b> <b>reporting</b> added in 2010.|$|E
50|$|Official {{modifications}} to the POV-Ray source tree are done and/or approved by the POV-Team. Most patch submission and/or <b>bug</b> <b>reporting</b> {{is done in the}} POV-Ray newsgroups on the news.povray.org news server (with a Web interface also available). Since POV-Ray's source is available there are unofficial forks and patched versions of POV-Ray available from third parties; however, these are not officially supported by the POV-Team.|$|E
30|$|Some <b>bug</b> <b>reports</b> already {{inform the}} {{location}} of the defect in the source code, by mentioning the file where the bug was observed. Kochhar et al. (2014) demonstrated that including these <b>bug</b> <b>reports</b> on the evaluation of a bug localization technique significantly influences the results by artificially increasing the reported effectiveness. The authors classified <b>bug</b> <b>reports</b> in three categories: fully localized, partially localized, and not localized, which mean that a <b>bug</b> <b>report</b> mentions all, some or none of the files modified to fix a bug; respectively. We removed fully and partially localized <b>bug</b> <b>reports</b> from our evaluation, meaning that we included only those <b>bug</b> <b>reports</b> that contained no mention of the faulty files. Although this step contributes to more realistic results, it reduced the number of available <b>bug</b> <b>reports</b> in 51 %, from the 878 reported in Table 1 to 450 (3 % of the initial issue count).|$|R
30|$|In this stage, we {{selected}} the concurrency bugs from the <b>bug</b> <b>report</b> database including <b>bug</b> <b>reports</b> {{from the period}} 2006 - 2015, i.e., the last decade. In total, the projects <b>bug</b> <b>report</b> databases contain 11860 issues in this period that are tagged as “Bug”.|$|R
40|$|Abstract <b>Bug</b> <b>reports</b> are {{essential}} software artifacts that describe software bugs, especially in open-source software. Lately, {{due to the}} availability {{of a large number of}} <b>bug</b> <b>reports,</b> a considerable amount of research has been carried out on bug-report analysis, such as automatically checking duplication of <b>bug</b> <b>reports</b> and localizing <b>bugs</b> based on <b>bug</b> <b>reports.</b> To review the work on bug-report analysis, this paper presents an exhaustive survey on the existing work on bug-report analysis. In particular, this paper first presents some background for <b>bug</b> <b>reports</b> and gives a small empirical study on the <b>bug</b> <b>reports</b> on Bugzilla to motivate the necessity for work on bug-report analysis. Then this paper summaries the existing work on bug-report analysis and points out some possible problems in working with bug-report analysis...|$|R
50|$|The project {{started out}} of a {{portable}} version of Mozilla Firefox in March 2004. John T. Haller then expanded the project to include Mozilla Thunderbird and OpenOffice.org. Soon the open source group of portable programs outgrew Haller's personal website and he moved it to a community site, PortableApps.com. The site currently hosts various projects created by forum members. The site is also used for <b>bug</b> <b>reporting</b> and suggestions.Some PortableApps distributions are hosted on SourceForge.|$|E
50|$|Ubuntu 5.10 (Breezy Badger), {{released}} on 12 October 2005, was Canonical's third release of Ubuntu. Ubuntu 5.10's support ended on 13 April 2007. Ubuntu 5.10 added several new features including a graphical bootloader (Usplash), an Add/Remove Applications tool, a menu editor (Alacarte), an easy language selector, logical volume management support, full Hewlett-Packard printer support, OEM installer support, a new Ubuntu logo in the top-left, and Launchpad integration for <b>bug</b> <b>reporting</b> and software development.|$|E
5000|$|DataMelt {{can be used}} {{everywhere}} {{where an}} analysis of large numerical data volumes,data mining, statistical data analysisand mathematics are essential. The program {{can be used in}} natural sciences,engineering, modeling and analysis of financial markets.While the program falls into the category of open source software, it is not completely free for commercial usage (see below),no source code is available on the home page, and all documentation and even <b>bug</b> <b>reporting</b> requires [...] "membership".|$|E
40|$|Knowledge Systems Institute Graduate SchoolIn this paper, {{we propose}} a semi-supervised text {{classification}} approach for bug triage {{to avoid the}} deficiency of labeled <b>bug</b> <b>reports</b> in existing supervised approaches. This new approach combines naive Bayes classifier and expectationmaximization {{to take advantage of}} both labeled and unlabeled <b>bug</b> <b>reports.</b> This approach trains a classifier with a fraction of labeled <b>bug</b> <b>reports.</b> Then the approach iteratively labels numerous unlabeled <b>bug</b> <b>reports</b> and trains a new classifier with labels of all the <b>bug</b> <b>reports.</b> We also employ a weighted recommendation list to boost the performance by imposing the weights of multiple developers in training the classifier. Experimental results on <b>bug</b> <b>reports</b> of Eclipse show that our new approach outperforms existing supervised approaches in terms of classification accuracy...|$|R
40|$|Abstract—In this paper, {{we propose}} a semi-supervised text {{classification}} approach for bug triage {{to avoid the}} deficiency of labeled <b>bug</b> <b>reports</b> in existing supervised approaches. This new approach combines naive Bayes classifier and expectation-maximization {{to take advantage of}} both labeled and unlabeled <b>bug</b> <b>reports.</b> This approach trains a classifier with a fraction of labeled <b>bug</b> <b>reports.</b> Then the approach iteratively labels numerous unlabeled <b>bug</b> <b>reports</b> and trains a new classifier with labels of all the <b>bug</b> <b>reports.</b> We also employ a weighted recommendation list to boost the performance by imposing the weights of multiple developers in training the classifier. Experimental results on <b>bug</b> <b>reports</b> of Eclipse show that our new approach outperforms existing supervised approaches in terms of classification accuracy. Keywords- automatic bug triage; expectation-maximization; semi-supervised text classification; weighted recommendation list I...|$|R
40|$|Duplicate <b>bug</b> <b>reports</b> {{are often}} {{unfavorable}} {{because they tend}} to take many man hours for being identified as duplicates, marked so and eventually discarded. In this time, no progress occurs on the program in question, and is justifiably an overhead which should be minimized. Considerable research has been carried out to alleviate this problem. Many methods have been proposed for <b>bug</b> <b>report</b> categorization and duplicate <b>bug</b> <b>report</b> detection. However, it is often the case that a duplicate <b>bug</b> <b>report</b> can provide some additional information about a problem which could help in faster resolution of the bug. We propose that duplicate <b>bug</b> <b>reports</b> be merged when possible instead of being discarded, so that maximum information is captured. We propose a clustering-based algorithm to group together similar sentences and create a union of <b>bug</b> <b>reports</b> considered duplicates of each other. ...|$|R
50|$|It {{can play}} various {{roles in the}} {{translation}} process. The simplest displays statistics for the body of translations hosted by the server. Its suggestion mode allows users to make translation suggestions and corrections for later review, thus it {{can act as a}} translation-specific <b>bug</b> <b>reporting</b> system. It allows online translation with various translators and lastly it can operate as a management system where translators translate using an offline tool and use Pootle to manage the workflow of the translation.|$|E
50|$|Applications in Ojuba 4: Archive Manager, Dictionary, gedit, Gnote, StarDict, Take Screenshot, arcade games, logic & puzzles games, GNU Paint, Shotwell Photo Manager, Simple Scan, Empathy, Firefox, Pino - Twitter and Identi.ca client, Remote Desktop Viewer, Transmission, Uget, Project Management, Audio CD Extractor, Brasero, Cheese, Istanbul Desktop Session Recorder, Movie Player, Ojuba MiMiC, Pitivi Video Editor, Rhythmbox, Automatic <b>Bug</b> <b>Reporting</b> Tool, CD/DVD Creator, Déjà Dup, Disk Usage Analyzer, Disk Utility, Multilingual Terminal, Ojuba Personal Lock, Ojuba Virtual CD/DVD, System Monitor and Terminal.|$|E
5000|$|PHP/FI {{could help}} to build simple, dynamic web applications. To {{accelerate}} <b>bug</b> <b>reporting</b> {{and to improve}} the code, Lerdorf initially announced the release of PHP/FI as [...] "Personal Home Page Tools (PHP Tools) version 1.0" [...] on the Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995. This release already had the basic functionality that PHP has [...] This included Perl-like variables, form handling, {{and the ability to}} embed HTML. The syntax resembled that of Perl but was simpler, more limited and less consistent.|$|E
40|$|In this paper, {{we propose}} a semi-supervised text {{classification}} approach for bug triage {{to avoid the}} deficiency of labeled <b>bug</b> <b>reports</b> in existing supervised approaches. This new approach combines naive Bayes classifier and expectation-maximization {{to take advantage of}} both labeled and unlabeled <b>bug</b> <b>reports.</b> This approach trains a classifier with a fraction of labeled <b>bug</b> <b>reports.</b> Then the approach iteratively labels numerous unlabeled <b>bug</b> <b>reports</b> and trains a new classifier with labels of all the <b>bug</b> <b>reports.</b> We also employ a weighted recommendation list to boost the performance by imposing the weights of multiple developers in training the classifier. Experimental results on <b>bug</b> <b>reports</b> of Eclipse show that our new approach outperforms existing supervised approaches in terms of classification accuracy. Comment: 6 pages, 1 figure, Proceedings of 22 nd International Conference on Software Engineering and Knowledge Engineering (SEKE 2010), 201...|$|R
30|$|Saha et al. (2013) {{developed}} BLUiR (Bug Localization Using information Retrieval), a bug localization technique {{based on}} the concept of structured information retrieval. In structured IR, fields from a <b>bug</b> <b>report</b> and code constructs, such as class or method names, are separately modeled as distinct documents. Consequently, <b>bug</b> <b>reports</b> and source files are not counted as single documents. Instead, BLUiR breaks <b>bug</b> <b>reports</b> into summary and description, while source files are split into class names, method names, variable names, and comments. Each part of a <b>bug</b> <b>report</b> is compared to each part from the source file. The similarity of a <b>bug</b> <b>report</b> and a source file is given by the sum of the calculated similarities.|$|R
40|$|Context. Bug {{tracking}} systems {{play an important}} role in software maintenance. They allow users to submit <b>bug</b> <b>reports.</b> However, it has been observed that often a <b>bug</b> <b>report</b> submitted is a duplicate (when several users submit <b>bug</b> <b>reports</b> for the same problem, these reports are called duplicated issue reports) which results in considerable duplicate <b>bug</b> <b>reports</b> in a <b>bug</b> tracking system. Solutions for automating the process of duplicate <b>bug</b> <b>reports</b> detection can increase the productivity of software maintenance activities, as new incoming <b>bug</b> <b>reports</b> are directly compared with the existing <b>bug</b> <b>reports</b> to identify their similar <b>bug</b> <b>reports,</b> which is no need for the human to spend time reading, understanding, and searching. Although recently there has been considerable research on such solutions, there is still much room for improvement regarding accuracy and recall rate during the duplicate detection process. Besides, very few tools were evaluated in an industrial setting. Objectives. In this study, firstly, we aim to characterize automated duplicate <b>bug</b> <b>report</b> detection methods by exploring categories of all those methods, identifying proposed evaluation methods, specifying performance difference between the categories of methods. Then we propose a method leveraging recent advances on using semantic model – Doc 2 vec and present an overall framework - preprocessing, training a semantic model, calculating and ranking similarity, and retrieving duplicate <b>bug</b> <b>reports</b> of the proposed method. Finally, we apply an experiment to evaluate the performance of the proposed method and compare it with the selected best methods for the task of duplicate <b>bug</b> <b>report</b> detection Methods. To classify and analyze all existing research on automated duplicate <b>bug</b> <b>report</b> detection, we conducted a systematic mapping study. To evaluate our proposed method, we conducted an experiment with an identified number of <b>bug</b> <b>reports</b> on the internal <b>bug</b> <b>report</b> database of Axis Communication AB. Results. We classified automated duplicate <b>bug</b> <b>report</b> detection techniques into three categories - TOP N recommendation and ranking approach, binary classification approach, and decision-making approach. We found that recall-rate@k is the most common evaluation metric, and found that TOP N recommendation and ranking approach has the best performance among the identified approaches. The experimental results showed that the recall rate of our proposed approach is significantly higher than the combination of TF-IDF with Word 2 vec and the combination of TF-IDF with LSI. Our combination of Doc 2 vec and TF-IDF approach, has a recall rate@ 1 - 10 of 18. 66 %- 42. 88 % in the TROUBLE data, which is an improvement of 1. 63 %- 9. 42 % to the state-of-art. Conclusions. In this thesis, we identified and classified 44 automated duplicate <b>bug</b> <b>report</b> detection research papers by conducting a systematic mapping study. We provide an overview of the state-of-art, identifying evaluation metrics, investigating the scientific evidence in the reported results, and identifying needs for future research. We implemented a bug tracking system with a duplicate <b>bug</b> <b>report</b> detection module where a list of Top-N related <b>bug</b> <b>reports</b> (along with a numerical value representing a similar score) is created. After conducting the experiment, we found that our proposed approach - the combination of Doc 2 vec and TF-IDF approach produces the best recall rate. Keywords: Simila...|$|R
50|$|During Summer 2009, Kunos Simulazioni {{announced}} that the next version of netKar Pro, dubbed 1.1, was to be released through a three-stage Public Beta {{in an effort to}} iron out all the bugs before reaching gold status, through the help of the community. A new <b>bug</b> <b>reporting</b> feature was added specifically for this purpose. This was meant to address concerns in the simracing community that v1.1 might have seen the same fate of v1.0 which remained unplayable for a long time for many simracers.|$|E
50|$|The creator, Danilo Leggieri, put {{the site}} winPenPack.com online on 23 November 2005. The {{project and the}} {{associated}} community then grew quickly. Since that date, 15 new versions and hundreds of open-source portable applications were released. The project {{is well known in}} Italy and abroad. It is hosted on SourceForge. The collections are regularly distributed bundled with popular PC magazines in Italy and worldwide. A thriving community of users is actively contributing to the growth of the project. The site currently hosts various projects created and suggested by forum members, and is also used for <b>bug</b> <b>reporting</b> and suggestions.|$|E
50|$|Croteam used {{an array}} of {{automated}} and in-place tools to help rapidly design, debug, and test the game for playability. In one aspect, they recognized {{in the development of}} a puzzle game was that while puzzles could be designed with specific solutions, the process of creating the video game around the puzzle could create unsolvable situations or unforeseen shortcuts. To address this, they used a bot, developed by Croteam member Nathan Brown who had previously developed bots for other games including the ones incorporated into ports of Serious Sam 3 for consoles. The bot, named Bot, would watch the playthrough of a puzzle by a human player in terms of broad actions such as placing boxes on a switch for the completion of a puzzle. Then, as the puzzle's environment was tuned and decorated, they would have Bot attempt to solve the puzzle, testing to make sure it did not run into any dead-ends. If it did encounter any, Bot reported these through an in-house <b>bug</b> <b>reporting</b> system and then used game cheats to move on and finish out testing, which took between 30 and 60 minutes for the full game. As such, they were able to quickly iterate and resolve such problems when new features were introduced to the game. Overall, Croteam estimates they logged about 15,000 hours with Bot before the release of the public test version, and expect to use similar techniques in future games. They also used human playtesters to validate other more aesthetic factors of the games prior to the title's release.|$|E
40|$|A {{software}} <b>Bug</b> <b>report</b> contains {{information about}} the bug {{in the form of}} problem description and comments using natural language texts. Managing <b>reported</b> <b>bugs</b> is a significant challenge for a project manager when the number of bugs for a software project is large. Prior to the assignment of a newly <b>reported</b> <b>bug</b> to an appropriate developer, the triager (e. g., manager) attempts to categorize it into existing categories and looks for duplicate bugs. The goal is to reuse existing knowledge to fix or resolve the new bug, and she often {{spends a lot of time}} in reading a number of <b>bug</b> <b>reports.</b> When fixing or resolving a bug, a developer also consults with a series of relevant <b>bug</b> <b>reports</b> from the repository in order to maximize the knowledge required for the fixation. It is also preferable that developers new to a project first familiarize themselves with the project along with the <b>reported</b> <b>bugs</b> before actually working on the project. Because of the sheer numbers and size of the <b>bug</b> <b>reports,</b> manually analyzing a collection of <b>bug</b> <b>reports</b> is time-consuming and ineffective. One of the ways to mitigate the problem is to analyze summaries of the <b>bug</b> <b>reports</b> instead of analyzing full <b>bug</b> <b>reports,</b> and there have been a number of summarization techniques proposed in the literature. Most of these techniques generate extractive summaries of <b>bug</b> <b>reports.</b> However, it is not clear how useful those generated extractive summaries are, in particular when the developers do not have prior knowledge of the <b>bug</b> <b>reports.</b> In order to better understand the usefulness of the <b>bug</b> <b>report</b> summaries, in this thesis, we first reimplement a state of the art unsupervised summarization technique and evaluate it with a user study with nine participants. Although in our study, 70 % of the time participants marked our developed summaries as a reliable means of comprehending the software bugs, the study also reports a practical problem with extractive summaries. An extractive summary is often created by choosing a certain number of statements from the <b>bug</b> <b>report.</b> The statements are extracted out of their contexts, and thus often lose their consistency, which makes it hard for a manager or a developer to comprehend the <b>reported</b> <b>bug</b> from the extractive summary. Based on the findings from the user study and in order to further assist the managers as well as the developers, we thus propose an interactive visualization for the <b>bug</b> <b>reports</b> that visualizes not only the extractive summaries but also the topic evolution of the <b>bug</b> <b>reports.</b> Topic evolution refers to the evolution of technical topics discussed in the <b>bug</b> <b>reports</b> of a software system over a certain time period. Our visualization technique interactively visualizes such information which can help in different project management activities. Our proposed visualization also highlights the summary statements within their contexts in the original report for easier comprehension of the <b>reported</b> <b>bug.</b> In order to validate the applicability of our proposed visualization technique, we implement the technique as a standalone tool, and conduct both a case study with 3914 <b>bug</b> <b>reports</b> and a user study with six participants. The experiments in the case study show that our topic analysis can reveal useful keywords or other insightful {{information about the}} <b>bug</b> <b>reports</b> for aiding the managers or triagers in different management activities. The findings from the user study also show that our proposed visualization technique is highly promising for easier comprehension of the <b>bug</b> <b>reports...</b>|$|R
40|$|Abstract—Locating buggy code is a {{time-consuming}} task {{in software}} development. Given a new <b>bug</b> <b>report,</b> developers must search {{through a large}} number of files in a project to locate buggy code. We propose BugScout, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a <b>bug</b> <b>report.</b> BugScout assumes that the textual contents of a <b>bug</b> <b>report</b> and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new <b>bug</b> <b>report.</b> We develop a specialized topic model that represents those technical aspects as topics in the textual contents of <b>bug</b> <b>reports</b> and source files, and correlates <b>bug</b> <b>reports</b> and corresponding buggy files via their shared topics. Our evaluation shows that BugScout can recommend buggy files correctly up to 45 % of the cases with a recommended ranked list of 10 files. Index Terms—Defect Localization, Topic Modeling. I...|$|R
40|$|Texto completo: acesso restrito. p. 39 - 66 Duplicate <b>bug</b> <b>report</b> {{entries in}} <b>bug</b> {{trackers}} {{have a negative}} impact on software maintenance and evolution. This is due, among other factors, to the increased time spent on report analysis and validation, which in some cases takes over 20 min. Therefore, a considerable amount of time is lost in duplicate <b>bug</b> <b>report</b> analysis. In order to understand the possible factors that cause <b>bug</b> <b>report</b> duplication and its impact on software development, this paper presents an exploratory study in which bug tracking data from private and open source projects were analyzed. The results show, for example, that all projects we investigated had duplicate <b>bug</b> <b>reports</b> and a considerable amount of time was wasted by this duplication. Furthermore, features such as project lifetime, staff size, and the number of <b>bug</b> <b>reports</b> do not seem to be significant factors for duplication, while others, such as the submitters’ profile and the number of submitters, do seem to influence the <b>bug</b> <b>report</b> duplication...|$|R
