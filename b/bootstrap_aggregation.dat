56|3|Public
50|$|Breiman's work {{helped to}} bridge the gap between {{statistics}} and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. <b>Bootstrap</b> <b>aggregation</b> was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest.|$|E
30|$|Bagging (Breiman 1996) {{which is}} also known as <b>bootstrap</b> <b>aggregation</b> {{involves}} training multiple models with training sets of data randomly drawn with replacement from the base training data sets. The training data sets for the base models are called bootstraps. Hence, bagging involves training different models with different samples and usually predictions are obtained by averaging the results of the different base models for a regression problem.|$|E
30|$|Bagging (also {{known as}} <b>bootstrap</b> <b>aggregation)</b> is an {{ensemble}} learner {{that attempts to}} improve {{the stability of the}} classifiers and reduce variance. Bagging works well with unstable classifiers such as decision trees [31]. The algorithm works by sampling with replacement, thus creating a number of so-called bags. Each bag contains a sample representative of the original dataset and the sampling is done with replacement.|$|E
5000|$|This {{butterfly}} {{has been}} {{subject of a}} pioneering DNA barcoding study of a 648 base pair sequence from COI, purportedly showing [...] "that A. fulgerator is a complex of at least 10 species in Area de Conservación Guanacaste World Heritage Site, NW Costa Rica."A later reanalysis of the DNA sequence data using neighbor joining <b>bootstrap,</b> population <b>aggregation</b> analysis and cladistic haplotype analysis found that: [...] "at least three, {{but not more than}} seven mtDNA clades that may correspond to cryptic species are supported by the evidence." [...] Also, [...] "... sequences that did not fit the general host plant pattern were simply dismissed with an ad hoc and manifestly incorrect explanation" [...] by Hébert and his coworkers. The improper use of taxonomic vocabulary was also criticized; Hébert et al. apply the terms [...] "species" [...] and [...] "taxa" [...] as if they were synonymous, but nowhere validly describe their proposed [...] "species".|$|R
40|$|Abstract Background Genome-wide {{association}} {{studies have}} successfully identified several loci underlying complex diseases in humans. The development of high density SNP maps in domestic animal species should allow {{the detection of}} QTLs for economically important traits through association studies with much higher accuracy than traditional linkage analysis. Here we report the association analysis of the dataset simulated for the XII QTL-MAS meeting (Uppsala). We used two strategies, single marker association and haplotype-based association (Blossoc) that were applied to i) the raw data, and ii) the data corrected for infinitesimal, sex and generation effects. Results Both methods performed similarly in detecting the most strongly associated SNPs, about ten loci in total. The most significant ones were located in chromosomes 1, 4 and 5. Overall, the largest {{differences were found between}} corrected and raw data, rather than between single and multiple marker analysis. The use of raw data increased greatly the number of significant loci, but possibly also the rate of false positives. <b>Bootstrap</b> model <b>aggregation</b> removed most of discrepancies between adjusted and raw data when SMA was employed. Conclusion Model choice should be carefully considered in genome-wide association studies. </p...|$|R
40|$|Abstract—It is {{the goal}} of this paper to examine how the {{aggregation}} and feature randomization principles underlying the algorithm RANDOM FOREST [1], originally proposed in the classification/regression setup, can be adapted to bipartite ranking, in order to increase the performance of scoring rules produced by the TREERANK algorithm [2], a recently developed tree induction method, specifically tailored for this global learning problem. Since TREERANK may be viewed as a recursive implementation of a cost-sensitive version of the popular classification algorithm CART [3], with a cost locally depending on the data lying within the node to split, various strategies can be considered for ”randomizing ” the features involved in the tree growing stage. In parallel, several ways of combining/averaging ranking trees may be used, including techniques inspired from rank aggregation methods recently popularized in Web applications. Ranking procedures based on such approaches are called RANKING FORESTS. Beyond preliminary theoretical background, results of experiments based on simulated data are provided in order to give evidence of their statistical performance. Index Terms—Bipartite Ranking, data with binary labels, ROC optimization, AUC criterion, tree-based ranking rules, <b>bootstrap,</b> bagging, rank <b>aggregation,</b> median procedure, feature randomization. hal- 00452577, version 1 - 2 Feb 201...|$|R
40|$|We {{present some}} new density {{estimation}} algorithms obtained by <b>bootstrap</b> <b>aggregation</b> like Bagging. Our algorithms are analyzed and empirically {{compared to other}} methods found in the statistical literature, like stacking and boosting for density estimation. We show by extensive simulations that ensemble learning are effective for density estimation like for classification. Although our algorithms do not always outperform other methods, {{some of them are}} as simple as bagging, more intuitive and has computational lower cost...|$|E
40|$|In this paper, we {{consider}} a nonlinear model based on neural networks {{as well as}} linear models to forecast the daily volatility of the S&P 500 and FTSE 100 futures. As a proxy for daily volatility, {{we consider}} a consistent and unbiased estimator of the integrated volatility that is computed from high-frequency intraday returns. We also consider a simple algorithm based on bagging (<b>bootstrap</b> <b>aggregation)</b> in order to specify the models analysed in this paper...|$|E
40|$|Arcing – {{adaptive}} re-weighting and combining– {{refers to}} reusing or selecting data to improve classification • Includes both bagging and boosting • Simplest method is bagging • Name derived from <b>bootstrap</b> <b>aggregation</b> • Uses multiple versions of training set Bagging Method • Drawing n ’ < n samples from D with replacement • Each bootstrap set {{is used to}} train a different component classifier • Final classification decision based on vote of each component classifier • Component classifiers are all of the same form – all HMMs, all neural networks, all decision trees – Different parameter values due to different training set...|$|E
40|$|We {{set out a}} {{strategy}} for quantizing attribute <b>bootstrap</b> <b>aggregation</b> to enable variance-resilient quantum machine learning. To do so, we utilise the linear decomposability of decision boundary parameters in the Rebentrost et al. Support Vector Machine to guarantee that stochastic measurement of the output quantum state will give rise to an ensemble decision without destroying the superposition over projective feature subsets induced within the chosen SVM implementation. We achieve a linear performance advantage, O(d), {{in addition to the}} existing O(log(n)) advantages of quantization as applied to Support Vector Machines. The approach extends to any form of quantum learning giving rise to linear decision boundaries...|$|E
40|$|Mixture-of-experts models, or mixture models, are a divide-and-conquer {{learning}} method {{derived from}} the mixture estimation paradigm [DH 73] that is heavily studied in artificial neural network research [JJ 94]. They reduce complexity by decomposing learning tasks and variance by combining multiple classifiers. Recent research has shown how inductive learning algorithms can be augmented by aggregation mixtures such as <b>bootstrap</b> <b>aggregation</b> (or bagging) [Br 96] and stacked generalization [Wo 92], and by partitioning mixtures such as boosting [FS 96] and hierarchical mixtures of experts (or HME) [J J 94]. Figure 1 depicts a new hierarchical mixture model called a specialist-moderator (S-M) network, which combines classifiers in a bottom-up fashion. Its primar...|$|E
40|$|In {{this paper}} {{we present a}} {{technique}} for using the bootstrap to estimate the operating characteristics and their variability for certain types of ensemble methods. Bootstrapping a model can require {{a huge amount of}} work if the training data set is large. Fortunately in many cases the technique lets us determine the effect of infinite resampling without actually refitting a single model. We apply the technique to the study of meta-parameter selection for random forests. We demonstrate that alternatives to <b>bootstrap</b> <b>aggregation</b> and to considering √(d) features to split each node, where d is the number of features, can produce improvements in predictive accuracy. Comment: 17 pages, 8 figure...|$|E
40|$|This study {{introduces}} <b>bootstrap</b> <b>aggregation</b> (bagging) in {{modeling and}} forecasting tourism demand. The {{aim is to}} improve the forecast accuracy of predictive regressions while considering fully automated variable selection processes which are particularly useful in industry applications. The procedures considered for variable selection is the general-to-specific (GETS) approach based on statistical inference and stepwise search procedures based on a measure of predictive accuracy (MPA). The evidence based on tourist arrivals from six source markets to Australia overwhelmingly suggests that bagging is effective for improving the forecasting accuracy of the models considered. School of Hotel and Tourism Management 2016 - 2017 > Academic research: refereed > Publication in refereed journalbcm...|$|E
40|$|We propose density-ratio bagging (dragging), a semi-supervised {{extension}} of <b>bootstrap</b> <b>aggregation</b> (bagging) method. Additional unlabeled training data {{are used to}} calculate the weight on each labeled training point by a density-ratio estimator. The weight is then used to construct a weighted labeled empirical distribution, from which bags of bootstrap samples are drawn. Asymptotically, dragging is proved to be no worse than bagging and requires no semi-supervised learning assumptions other than $iid$-ness. We show that compared to bagging, the dragging predictor achieves less asymptotic variance, which leads to a smaller MSE. We conduct real experiments on several regression and classification tasks. The performance of dragging, bagging, semi-supervised learning with density-ratio estimator, and basic supervised learning is compared and discussed...|$|E
40|$|Abstract—A {{resampling}} {{scheme for}} clustering with similarity to <b>bootstrap</b> <b>aggregation</b> (bagging) is presented. Bagging {{is used to}} improve the quality of pathbased clustering, a data clustering method that can extract elongated structures from data in a noise robust way. The results of an agglomerative optimization method are influenced by small fluctuations of the input data. To increase the reliability of clustering solutions, a stochastic resampling method is developed to infer consensus clusters. A related reliability measure allows us to estimate the number of clusters, based on the stability of an optimized cluster solution under resampling. The quality of path-based clustering with resampling is evaluated on a large image dataset of human segmentations. Index Terms—Clustering, resampling, color segmentation...|$|E
40|$|Abstract. We {{propose a}} cross-modal {{approach}} based on separate au-dio and image data-sets {{to identify the}} artist of a given music video. The identification process {{is based on an}} ensemble of two separate clas-sifiers. Audio content classification is based on audio features derived from the Million Song Dataset (MSD). Face recognition is based on Lo-cal Binary Patterns (LBP) using a training-set of artist portrait images. The two modalities are combined using <b>bootstrap</b> <b>aggregation</b> (Bagging). Different versions of classifiers for each modality are generated from sub-samples of their according training-data-sets. Predictions upon the final artist labels are based on weighted majority voting. We show that the visual information provided by music videos improves the precision of music artist identification tasks. ...|$|E
40|$|Bagging forms a {{committee}} of classifiers by <b>bootstrap</b> <b>aggregation</b> of training sets {{from a pool of}} training data. A simple alternative to bagging is to partition the data into disjoint subsets. Experiments on various datasets show that, given the same size partitions and bags, the use of disjoint partitions results in better performance than the use of bags. Many applications (e. g., protein structure prediction) involve the use of datasets that are too large to handle in the memory of the typical computer. Our results indicate that, in such applications, the simple approach of creating {{a committee}} of classifiers from disjoint partitions is to be preferred over the more complex approach of bagging...|$|E
40|$|Abstract: Statistical inferences {{based on}} small {{dimension}} samples represent {{a big problem}} and a made to measure challenge. In the biomedical domain there are many situations when costs or ethical reasons enforce {{that only a few}} data are collected. Nevertheless, inferences must be made. In this paper, an alternative to the classical statistical approach is discussed. The article is focused on a simulation strategy through bootstrap re-sampling. Linked to this, an important phenomenon appears during bootstrapping, namely <b>bootstrap</b> <b>aggregation.</b> This is also revealed through simulation. The model is tested on real small samples of medical data concerning the oxidative stress at newborns, in term and premature, comparatively. All calculations implied by the model are implemented through Matlab scripts...|$|E
40|$|This paper {{explores the}} {{usefulness}} of factor and <b>bootstrap</b> <b>aggregation</b> forecasting in predicting regional GDP in Italy. We use methods designed to target the set of potential predictors. We compute the mean square forecasting error (MSE) by using direct multi-step forecasts for the period 2004 - 2005. Our findings can be summarized as follows. First, factor and bagging forecasts generally show lower mean square forecasting error than the mean square error of the autoregressive AR(3) model used as a benchmark. Secondly, bagging methods seem to produce similar MSE as factor augmented models, especially for predicting aggregate GDPs. In synthesis, our analysis shows that using factors and bagging methods reduces the prediction mean squared error relative to standard forecasting methods...|$|E
40|$|A {{successful}} {{approach to}} building QSAR models was proposed by other researchers. It uses binary {{particle swarm optimization}} (BPSO) for feature selection in the first stage, and a back propagation neural network in the second stage to generate a QSAR model based on the features selected in the first stage. This paper starts by re-establishing {{the results of this}} approach on an extended number of data sets. A new method is then proposed that addresses the limitation of back propagation. The new approach uses particle swarm optimization (PSO) in the second stage for training and <b>bootstrap</b> <b>aggregation</b> (Bagging) in order to overcome the instability of PSO. The proposed approach yields robust QSAR models, while reducing the variability due to the choice of the back propagation parameters. 1...|$|E
40|$|Exponential {{smoothing}} {{is one of}} {{the most}} popular forecasting methods. We present a tech-nique for <b>bootstrap</b> <b>aggregation</b> (bagging) of exponential smoothing methods, which results in significantly improved forecasts. The bagging uses a Box-Cox transformation followed by an STL decomposition to separate the time series into trend, seasonal part, and remainder. The remainder is then bootstrapped using a moving block bootstrap, and a new series is assembled using this bootstrapped remainder. On the bootstrapped series, an ensemble of exponential smoothing models is estimated, and the resulting point forecasts are combined. We evaluate this new method on the M 3 data set, showing that it consistently outperforms the original exponential smoothing models. On the monthly data, we achieve better results than any of the original M 3 participants...|$|E
40|$|Objectives: Demonstration of the {{applicability}} of a framework called indirect classification to the example of glaucoma classification. Indirect classification combines medical a priori knowledge and statistical classification methods. The method is compared to direct classification approaches {{with respect to the}} estimated misclassification error. Methods: Indirect classification is applied using classification trees and the diagnosis of glaucoma. Misclassification errors are reduced by <b>bootstrap</b> <b>aggregation.</b> As direct classification methods linear discriminant analysis, classification trees and bootstrap aggregated classification trees are utilized in the problem of glaucoma diagnosis. Misclassification rates are estimated via 10 -fold cross-validation. Results: Indirect classification techniques reduce the misclassification error in the context of glaucoma classification compared to direct classification methods. Conclusions: Embedding a priori knowledge into statistical classification techniques can improve misclassification results. Indirect classification offers a framework to realize this combination...|$|E
40|$|One of {{the attractions}} of crossvalidation, {{as a tool}} for smoothing-parameter choice, is its {{applicability}} {{to a wide variety of}} estimator types and contexts. However, its detractors comment adversely on the relatively high variance of crossvalidatory smoothing parameters, noting that this compromises the performance of the estimators in which those parameters are used. We show that the variability can be reduced simply, significantly and reliably by employing <b>bootstrap</b> <b>aggregation</b> or bagging. We establish that in theory, when bagging is implemented using an adaptively chosen resample size, the variability of crossvalidation can be reduced by an order of magnitude. However, it is arguably more attractive to use a simpler approach, based for example on half-sample bagging, which can reduce variability by approximately 50 %. Copyright 2009, Oxford University Press. ...|$|E
40|$|ABSTRACT. The {{literature}} on excess return prediction has considered {{a wide array}} of estimation schemes, among them unrestricted and restricted regression coefficients. We propose <b>bootstrap</b> <b>aggregation</b> (bagging) as a means of imposing parameter restrictions. In this context, bagging results in a soft threshold as opposed to the hard threshold that is implied by a simple restricted estimation. We show analytically that the resulting forecast has lower variance than the forecast that results from a simple restricted estimator. In an empirical application using the same data set as in Campbell and Thompson (2008), “Predicting the Equity Premium Out of Sample: Can Anything Beat the Historical Average? ” forthcoming in the Review of Financial Studies, we show that the resulting forecasts have more predictive power than those resulting from simple parameter restrictions...|$|E
40|$|In this paper, we {{introduce}} {{a new approach to}} the classification of streaming data based on <b>bootstrap</b> <b>aggregation</b> (bagging). The proposed approach creates an ensemble model by using ID 3 classifier, naïve Bayesian classifier, and k-Nearest-Neighbor classifier for a learning scheme where each classifier gives the weighted prediction. ID 3, naïve Bayesian, and k-Nearest-Neighbor classifiers are very well known data mining methods, which have been already used in many real life classification problems. The proposed approach addresses the practical problems of the classification of streaming data and successfully tested on a number of benchmark problems including large intrusion detection dataset from the UCI machine learning repository to produce a comparison with the established approaches. The experimental results demonstrate that the proposed ensemble classifier achieved high classification rates and generated very low misclassification error...|$|E
40|$|We {{forecast}} daily realized volatilities with linear and nonlinear {{models and}} evaluate the benefits of <b>bootstrap</b> <b>aggregation</b> (bagging) in producing more precise forecasts. We consider the linear autoregressive (AR) model, the Heterogeneous Autoregressive model (HAR), and a non-linear HAR model based on a neural network specification that allows for logistic transition effects (NNHAR). The models and the bagging schemes are applied to the realized volatility time series of the S&P 500 index from 3 -Jan- 2000 through 30 -Dec- 2005. Our main findings are: (1) For the HAR model, bagging successfully averages over the randomness of variable selection; however, when the NN model is considered, {{there is no clear}} benefit from using bagging; (2) including past returns in the models improves the forecast precision; and (3) the NNHAR model outperforms the linear alternatives. ...|$|E
40|$|The Major work in data {{pre-processing}} {{is handling}} Missing value imputation in Hepatitis Disease Diagnosis {{which is one}} of the primary stage in data mining. Many health datasets are typically imperfect. Just removing the cases from the original datasets can fetch added problems than elucidations. A appropriate technique for missing value imputation can assist to generate high-quality datasets for enhanced scrutinizing in clinical trials. This paper investigates the exploit of a machine learning technique as a missing value imputation process for incomplete Hepatitis data. Mean/mode imputation, ID 3 algorithm imputation, decision tree imputation and proposed <b>bootstrap</b> <b>aggregation</b> based imputation are used as missing value imputation and the resultant datasets are classified using KNN. The experiment reveals that classifier performance is enhanced when the Bagging based imputation algorithm is used to foresee missing attribute values...|$|E
30|$|The vanilla task of link {{prediction}} as we {{have defined}} it has a wealth of supporting literature, {{and it would be}} impossible to cover it all here. Liben-Nowell and Kleinberg offered a seminal guide to the topic in (Liben-Nowell and Kleinberg 2007). The work most directly related uses structural forms to inform transition probabilities Juszczyszyn et al. (2011 a, 2011 b). VCPs take advantage of the supervised classification framework in (Lichtenwalter et al. 2010), which involves undersampling, <b>bootstrap</b> <b>aggregation,</b> and random forest or random subspace classification algorithms by substituting the simple feature vector derived from topological analysis with the VCP. There are several other supervised classification frameworks (Al Hasan et al. 2006; Wang et al. 2007) for link prediction that use basic topological characteristics, unsupervised link predictors, node attributes, and other information to construct their feature vectors.|$|E
40|$|Abstract—The {{analysis}} of high-throughput data sets, such as microarray data, often requires that individual variables (genes, for example) be grouped into small clusters such {{that all the}} variables in any given cluster have highly correlated values across all samples. Gene shaving is an established method for generating such clusters, but is overly sensitive to the input data: changing one sample out of hundreds can {{determine whether or not}} an entire cluster is found, even if the search is continued for many hundreds of clusters. This paper describes a clustering method based on the <b>bootstrap</b> <b>aggregation</b> (bagging) of gene shaving clusters, which overcomes this and other problems. The paper also describes the application of the new method to a large meta-data set of gene expression microarray data from brain tumor samples. Index Terms—bootstrap aggregation, clustering, gene shaving, glioblastoma. I...|$|E
40|$|We {{present a}} novel {{approach}} for density estimation using Bayesian networks when faced with scarce and partially observed data. Our approach relies on Efron’s bootstrap framework, and replaces the standard model selection score by a <b>bootstrap</b> <b>aggregation</b> objective aimed at sifting out bad decisions during the learning procedure. Unlike previous bootstrap or MCMC based approaches that are only aimed at recovering specific structural features, we learn a concrete density model {{that can be used}} for probabilistic generalization. To make use of our objective when some of the data is missing, we propose a bagged structural EM procedure that does not incur the heavy computational cost typically associated with a bootstrap-based approach. We compare our bagged objective to the Bayesian score and the Bayesian information criterion (BIC), as well as other bootstrap-based model selection objectives, and demonstrate its effectiveness in improving generalization performance for varied real-life datasets. ...|$|E
40|$|Abstract: This paper uses a {{predictive}} regression {{framework to}} examine the out-of-sample predictability of South Africa’s equity premium, using a host of financial and macroeconomic variables. We employ various methods of forecast combination, <b>bootstrap</b> <b>aggregation</b> (bagging), diffusion index (principal component) and Bayesian regressions {{to allow for a}} simultaneous role of the variables under consideration, besides individual predictive regressions. We assess both the statistical and economic significance of the individual predictive regressions, combination methods, bagging, principal components and Bayesian regressions. Our results show that forecast combination methods and principal component regressions improve the predictability of the equity premium relative to the benchmark autoregressive model of order one (AR(1)). However, the Bayesian predictive regressions are found to be the standout performers with the models outperforming the individual regressions, forecast combination methods, bagging and principal component regressions, both in terms of statistical (forecasting) and economic (utility) gains...|$|E
40|$|Classification in Peer-to-Peer (P 2 P) {{networks}} {{is important}} to many real applications, such as distributed intrusion detection, distributed recommendation systems, and distributed antispam detection. However, it is very challenging to perform classification in P 2 P networks due to many practical issues, such as scalability, peer dynamism, and asynchronism. This article investigates the practical techniques of constructing Support Vector Machine (SVM) classifiers in the P 2 P networks. In particular, we demonstrate how to efficiently cascade SVM in a P 2 P network {{with the use of}} reduced SVM. In addition, we propose to fuse the concept of cascade SVM with <b>bootstrap</b> <b>aggregation</b> to effectively balance the trade-off between classification accuracy, model construction, and prediction cost. We provide theoretical insights for the proposed solutions and conduct an extensive set of empirical studies on a number of large-scale datasets. Encouraging results validate the efficacy of the proposed approach...|$|E
40|$|This paper uses a {{predictive}} regression {{framework to}} examine the out-of-sample predictability of South Africa’s equity premium, using a host of financial and macroeconomic variables. We employ various methods of forecast combination, <b>bootstrap</b> <b>aggregation</b> (bagging), diffusion index (principal component) and Bayesian regressions {{to allow for a}} simultaneous role of the variables under consideration, besides individual predictive regressions. We assess both the statistical and economic significance of the individual predictive regressions, combination methods, bagging, principal components and Bayesian regressions. Our results show that forecast combination methods and principal component regressions improve the predictability of the equity premium relative to the benchmark autoregressive model of order one (AR(1)). However, the Bayesian predictive regressions are found to be the standout performers with the models outperforming the individual regressions, forecast combination methods, bagging and principal component regressions, both in terms of statistical (forecasting) and economic (utility) gains. [URL]...|$|E
40|$|Topological {{spatial data}} can be useful for the {{classification}} and analysis of biomedical data. Neural networks have been used previously to make diagnostic classifications of corneal disease using summary statistics as network inputs. This approach neglects global shape features (used by clinicians when they make their diagnosis) and produces results that are di#cult to interpret clinically. In this study we propose the use of Zernike polynomials to model the global shape of the cornea and use the polynomial coe#cients as features for decision tree classifier. We use this model to classify a sample of normal patients and patients with corneal distortion caused by keratoconus. Extensive experimental results, including a detailed study on enhancing model performance via adaptive boosting and <b>bootstrap</b> <b>aggregation</b> leads us {{to conclude that the}} proposed method can be highly accurate and a useful tool for clinicians. Moreover, the resulting model is easy to interpret using visual cues...|$|E
40|$|Thesis (M. Sc. (Statistics)) [...] North-West University, Potchefstroom Campus, 2011. The {{purpose of}} this study is to {{thoroughly}} discuss kernel hazard function estimation, both in the complete sample case as well as in the presence of random right censoring. Most of the focus is on the very important task of automatic bandwidth selection. Two existing selectors, least–squares cross validation as described by Patil (1993 a) and Patil (1993 b), as well as the bootstrap bandwidth selector of Gonzalez–Manteiga, Cao and Marron (1996) will be discussed. The bandwidth selector of Hall and Robinson (2009), which uses <b>bootstrap</b> <b>aggregation</b> (or 'bagging'), will be extended to and evaluated in the setting of kernel hazard rate estimation. We will also make a simple proposal for a bootstrap bandwidth selector. The performance of these bandwidth selectors will be compared empirically in a simulation study. The findings and conclusions of this study are reported. Master...|$|E
40|$|We {{investigate}} {{the performance of}} bagging methods {{in the presence of}} outliers. The results are best illustrated and intuitively explained for classical classification problems to which we shall restrict our focus in this paper. It is shown that bagging methods can improve the resistance of classification rules to outlier contamination, especially if an m-out-of-n bagging scheme is used. However, the outlier-reduction property does not improve performance {{to such an extent that}} outliers are no longer noticeable. It is also shown that in the absence of contamination by outliers the effects of bagging are negligible. Therefore, when bagging is not really needed, deleterious effects that result from employing it are quite small. 1 Introduction The method of <b>bootstrap</b> <b>aggregation,</b> or bagging, was introduced by Breiman (1996 a). Recent accounts of its properties, in the settings of both prediction and classification, include those of Breiman (1996 b) and Tibshirani (1996). When used for class [...] ...|$|E
