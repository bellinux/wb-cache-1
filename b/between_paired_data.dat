9|10000|Public
40|$|Abstract. In {{neuroscience}} {{it became}} popular to represent neuroimaging {{data from the}} human brain as networks. The edges of these (weighted) graphs rep-resent a spatio-temporal similarity <b>between</b> <b>paired</b> <b>data</b> channels. The tem-poral series of graphs is commonly averaged to a weighted graph of which edge weights are eventually thresholded. Graph measures are then applied to this network to correlate them, e. g. with clinical variables. This approach has some major drawbacks we will discuss in this paper. We identify three limi-tations of static graphs: selecting a similarity measure, averaging over time, choosing an (arbitrary) threshold value. The latter two procedures should not be performed due {{to the loss of}} brain activity dynamics. We propose to work on series of weighted graphs to obtain time series of graph measures. We use vector autoregressive (VAR) models to facilitate a statistical analy-sis of the resulting time series. Machine learning techniques are used to find dependencies between VAR parameters and clinical variables. We conclude with a discussion and possible ideas for future work...|$|E
40|$|Correlated {{information}} {{between different}} views incorporate useful for learning in multi view data. Canonical correlation analysis (CCA) plays {{important role to}} extract these information. However, CCA only extracts the correlated information <b>between</b> <b>paired</b> <b>data</b> and cannot preserve correlated information between within-class samples. In this paper, we propose a two-view semi-supervised learning method called semi-supervised random correlation ensemble base on spectral clustering (SS_RCE). SS_RCE uses a multi-view method based on spectral clustering which takes advantage of discriminative information in multiple views to estimate labeling information of unlabeled samples. In order to enhance discriminative power of CCA features, we incorporate the labeling information of both unlabeled and labeled samples into CCA. Then, we use random correlation between within-class samples from cross view to extract diverse correlated features for training component classifiers. Furthermore, we extend a general model namely SSMV_RCE to construct ensemble method to tackle semi-supervised learning {{in the presence of}} multiple views. Finally, we compare the proposed methods with existing multi-view feature extraction methods using multi-view semi-supervised ensembles. Experimental results on various multi-view data sets are presented to demonstrate the effectiveness of the proposed methods...|$|E
40|$|Background: loss of {{skeletal}} muscle mass (sarcopenia) {{is one of}} the most profound changes affecting the human body with ageing (Narici &Maffulli 2010) as it reduces strength, mobility and ability to dispose of circulating blood-sugar, eventually reducing quality of life of individuals. However there is paucity of data on the prevalence of sarcopenia in non-clinical populations and in particular how this affects mobility and quality of life. Aim: to investigate the association between sarcopenia and variables related to quality of life in a population of medically stable, community-dwelling elderly men and women. Method: 88 participants (40 male and 48 female), age range 64 - 83 years (mean 72. 3 ± 4. 6 years), body mass index (BMI) men: 26. 5 ± 3. 7 and women: 24. 7 ± 3. 5, were recruited for this study and underwent the following measurements: a) Body Composition analysis (by bioelectrical impedance, BIA), b) Gait speed, measured over 4 m, c) Quality of Life (QoL) measured using the Medical Outcomes Survey Short-form General Health Survey (SF- 36) translated and validated in Italian (Apolone and Mosconi, 1998) and, d) Physical Activity level, measured with the IPAQ-short version, (Mannocci et al. 2010). The data were analyzed with descriptive statistics and strength of monotonic relationship <b>between</b> <b>paired</b> <b>data</b> was tested with the Spearman correlation coefficient. Results: BIA Skeletal Muscle Index (SMI) was 34. 3 ± 3. 2 for men and 30. 8 ± 4. 9 for women; 45 participants (51...|$|E
40|$|Now a days {{most of the}} {{traditional}} clustering mechanisms based on linear space. Relation exists <b>between</b> the <b>pair</b> <b>data</b> objects either implicitly or explicitly. In {{the traditional}} mechanism uses a single view point, In this paper we proposes a novel mechanism for multiview point (i. e. n –dimensional space) with different similarity measure. Using the multiple viewpoints, more informative assessment of similarity can be achieved. Different mechanisms used for efficient clustering mechanisms...|$|R
5000|$|If {{two or more}} sets of {{data are}} being added {{together}} datapoint by datapoint, the standard deviation of {{the result can be}} calculated if the standard deviation of each data set and the covariance <b>between</b> each <b>pair</b> of <b>data</b> sets is known: ...|$|R
5000|$|Linear {{interpolation}} {{on a set}} of {{data points}} (x0, y0), (x1, y1), ..., (xn, yn) is defined as the concatenation of linear interpolants <b>between</b> each <b>pair</b> of <b>data</b> points. This results in a continuous curve, with a discontinuous derivative (in general), thus of differentiability class [...]|$|R
40|$|Introduction: For decades {{cellular}} differentials {{have been}} generated exclusively on analog tabletop cell counters. With {{the advent of}} tablet computers, digital cell counters - {{in the form of}} mobile applications ("apps") - now represent an alternative to analog devices. However, app-based counters have not been widely adopted by clinical laboratories, perhaps owing to a presumed decrease in count accuracy related to the lack of tactile feedback inherent in a touchscreen interface. We herein provide the first systematic evidence that digital cell counters function similarly to standard tabletop units. Methods: We developed an app-based cell counter optimized for use in the clinical laboratory setting. Paired counts of 188 peripheral blood smears and 62 bone marrow aspirate smears were performed using our app-based counter and a standard analog device. Differences <b>between</b> <b>paired</b> <b>data</b> sets were analyzed using the correlation coefficient, Student′s t-test for paired samples and Bland-Altman plots. Results: All counts showed excellent agreement across all users and touch screen devices. With the exception of peripheral blood basophils (r = 0. 684), differentials generated for the measured cell categories within the paired data sets were highly correlated (all r ≥ 0. 899). Results of paired t-tests did not reach statistical significance for any cell type (all P > 0. 05), and Bland-Altman plots showed a narrow spread of the difference about the mean without evidence of significant outliers. Conclusions: Our analysis suggests that no systematic differences exist between cellular differentials obtained via app-based or tabletop counters and that agreement between these two methods is excellent...|$|E
40|$|Recently {{there have}} been several {{advances}} in instrumental methods for the flame emission analysis of electrolytecations of clinical signifi-cance. Greater precision and increased ease of operation are among today’s clinical requirements. This study compares a new automated flame photometer, “Klina Flame ” (Beckman Instru-ments) with a “Techtron-AA 5 ” atomic absorption emission unit (Varian Instruments) and an “IL 143 ” flame system (Instrumentation Laboratory). Day. to. day variation <b>between</b> <b>paired</b> <b>data</b> over a three-month period showed comparable and ex-plainable similarities and variations between the three instruments. Results of lithium determina-tions gave favorable comparison statistics. Long-term stability on all the systems were generally comparable. A comparison of manual vs. auto-matic modes of dilution with serum samples of various viscosities generally gave predictable vari-ation for instruments of similar quality. The piston dilutor feature of the Klina system seems to prevent the variability seen at higher viscosities when the IL system is used, which has a peristaltic dilutor. Additional Keyphrases sodium, potassium, lithium #{ 149 }“Techtron-AA#{ 212 } ” #{ 149 }IL-i 43 #{ 149 } electrolyte cations #{ 149 }precision, stability, recovery Electrolytes usually account for about 6 to 12 % of the total clinical laboratory workload, which is increasing by 10 to 15 % per year. ’ In view of this particular workload, current flame emission in-struments were investigated in an attempt to select that which would better fit our increasing requirements. Recently {{there have been}} several advances in the field of instrumental methods for flame emis-sion analysis of electrolyte cations of clinical significance (1). Among the new developments are instrument...|$|E
40|$|A central {{challenge}} {{faced by}} biological and medical {{research is to}} understand the impact of chemical entities on living cells. Identifying {{the relationships between the}} chemical structures and their cellular responses is valuable for improving drug design and targeted therapies. The chemical structures and their detailed molecular responses need to be combined through a systematic analysis to learn the complex dependencies, which can then assist in improving understanding of the molecular mechanisms of drugs as well as predictions on the effects of unknown molecules. Moreover, with emerging drug-response data sets being profiled over several disease types and phenotypic details, it is pertinent to develop advanced computational methods {{that can be used to}} study multiple sets of data together. In this thesis, a novel multi-disciplinary challenge is undertaken for computationally analyzing interactions between multiple biological responses and chemical properties of drugs, while simultaneously advancing the computational methods to better learn these interactions. Specifically, multi-view dependency modeling of paired data sets is formulated as a means of systematically studying the drug-response relationships. First, the systematic analysis of drug structures and their genome-wide responses is presented as a multi-set dependency modeling problem and established methods are adopted to test the novel hypothesis. Several novel extensions of the drug-response analysis are then presented that explore responses measured over multiple disease types and multiple levels of phenotypic detail, uncovering novel biological insights of potential impact. These analyses are made possible by novel advancements in multi-view methods. Specifically, the first Bayesian tensor canonical correlation analysis and its extensions are introduced to capture the underlying multi-way structure and applied in analyzing novel toxicogenomic interactions. The results illustrate that modeling the precise multi-view and multi-way formulation of the data is valuable for discovering interpretable latent components as well as for the prediction of unseen responses of drugs. Therefore, the original contribution to knowledge in this dissertation is two-fold: first, the data-driven identification of relationships between structural properties of drugs and their genome-wide responses in cells and, second, novel advancements of multi-view methods that find dependencies <b>between</b> <b>paired</b> <b>data</b> sets. Open source implementations of the new methods have been released to facilitate further research...|$|E
40|$|We {{develop a}} {{methodology}} for solving high dimensional dependency estimation problems <b>between</b> <b>pairs</b> of <b>data</b> types, which is viable {{in the case}} where the output of interest has very high dimension, e. g. thousands of dimensions. This is achieved by mapping the objects into continuous or discrete spaces, using joint kernels. Known correlations between input and output can be defined by such kernels, some of which can maintain linearity in the outputs to provide simple (closed form) pre-images. We provide examples of such kernels and empirical results on mass spectrometry prediction and mapping between images...|$|R
40|$|International audienceThis paper {{introduces}} Pairwise Constrained Component Analysis (PCCA), a new algorithm {{for learning}} distance metrics from sparse pairwise similarity/dissimilarity constraints in high dimensional input space, problem for which most existing distance metric learning approaches are not adapted. PCCA learns a projection into a low-dimensional {{space where the}} distance <b>between</b> <b>pairs</b> of <b>data</b> points respects the desired constraints, exhibiting good generalization properties in presence of high dimensional data. The paper also shows how to efficiently kernelize the approach. PCCA is experimentally validated on two challenging vision tasks, face verification and person re-identification, for which we obtain state-of-the-art results...|$|R
40|$|Measuring {{relationships}} <b>between</b> <b>pairs</b> of <b>data</b> {{objects in}} Wikipedia is challenging task {{in real world}} data. For the Wikipedia graph, consisting of the articles together with the hyperlinks between them, the preferential attachment rule explains portion of the constitution, but instinct says that the themes of each article also performs a crucial position. This proposed system concentrate on small datasets extracted from the Wikipedia database. The matter of researching individual search space intents has attracted intensive consideration from both enterprise and academia. However, state-of-the-art intent researching techniques go through from different drawbacks when only utilizing a unmarried variety of statistics supply. For instance, query textual content has issue in distinguishin...|$|R
40|$|Abstract Background Minimally-invasive {{measurement}} of continuous inter-vertebral motion in clinical settings {{is difficult to}} achieve. This paper describes the reliability, validity and radiation exposure levels in a new Objective Spinal Motion Imaging Assessment system (OSMIA) based on low-dose fluoroscopy and image processing. Methods Fluoroscopic sequences in coronal and sagittal planes were obtained from 2 calibration models using dry lumbar vertebrae, plus the lumbar spines of 30 asymptomatic volunteers. Calibration model 1 (mobile) was screened upright, in 7 inter-vertebral positions. The volunteers and calibration model 2 (fixed) were screened on a motorised table comprising 2 horizontal sections, one of which moved through 80 degrees. Model 2 was screened during motion 5 times and the L 2 -S 1 levels of the volunteers twice. Images were digitised at 5 fps. Inter-vertebral motion from model 1 was compared to its pre-settings to investigate accuracy. For volunteers and model 2, the first digitised image in each sequence was marked with templates. Vertebrae were tracked throughout the motion using automated frame-to-frame registration. For each frame, vertebral angles were subtracted giving inter-vertebral motion graphs. Volunteer data were acquired twice {{on the same day}} and analysed by two blinded observers. The root-mean-square (RMS) differences <b>between</b> <b>paired</b> <b>data</b> were used as the measure of reliability. Results RMS difference between reference and computed inter-vertebral angles in model 1 was 0. 32 degrees for side-bending and 0. 52 degrees for flexion-extension. For model 2, X-ray positioning contributed more to the variance of range measurement than did automated registration. For volunteer image sequences, RMS inter-observer variation in intervertebral motion range in the coronal plane was 1. 86 degreesand intra-subject biological variation was between 2. 75 degrees and 2. 91 degrees. RMS inter-observer variation in the sagittal plane was 1. 94 degrees. Radiation dosages in each view were below the levels recommended for a plain film. Conclusion OSMIA can measure inter-vertebral angular motion patterns in routine clinical settings if modern image intensifier systems are used. It requires skilful radiography to achieve optimal positioning and dose limitation. Reliability in individual subjects can be judged from the variance of their averaged inter-vertebral angles and by observing automated image registration. </p...|$|E
40|$|In {{this thesis}} three studies are reported, which aim to {{investigate}} the performance of an interviewer-administered, open-ended diet history for the use in clinical studies where dietary variables are manipulated. The studies presented here are focused on validating self-reported macronutrient and energy intakes in Australian adults who have volunteered to participate in dietary intervention research. The evaluations are performed using food records, 24 -hour recalls and biochemical markers of intake for comparison. In addition, a pilot study is presented, which examines the ability of hair and sebaceous lipid composition to reflect dietary fatty acid intake in healthy volunteers. Clinical trials are the highest form of evidence for examining both the treatment and prevention of disease by dietary nutrients. Given this, {{it is imperative that}} these nutrients are measured as accurately as possible to avoid erroneous conclusions regarding their capabilities. Despite the importance of data from clinical trials, the literature relating to the performance of dietary assessment methods in this context is sparse. Despite evidence to show that food records and 24 -hour recalls are not always able to measure intake accurately, many dietary intervention trials are using these methods to measure dietary intake during the intervention period. Since the diet history method is used in clinical practice, albeit in a less rigorous manner, and the clinical trial is, in essence, a highly controlled form of clinical practice, examination of the diet history method in this research context is warranted. The first study in this thesis examined the validity of an interviewer-administered, open-ended, non-structured diet history in adults with Type II diabetes mellitus. The trial examined the viability of a high monounsaturated fat diet for the treatment of associated blood and insulin abnormalities. The dietary data were obtained retrospectively and the diet history was examined relative to a 3 -day food record using a number of statistical techniques. The results showed that the diet history method was able to measure both energy and macronutrient intakes reasonably well at both the group level and the individual level. There was, however, a failure to measure the fatty acid composition of the diet, which was due to a few outliers who reported discrepant values. Nethertheless, the relationship <b>between</b> <b>paired</b> <b>data</b> for these variables was significant once outliers were removed. There was no relationship between bias and intake indicating that systematic error was not operating in this group. An important finding from this study was the large proportion of individuals underreporting energy intake. This was not method-specific, but rather subject-specific and tended to occur in the larger individuals, however the use of a nutrient density model diluted in the effects of underreporting for comparisons of the diet history with the food record. The non-structured diet history method was found to estimate energy and nutrient intakes relatively well in a clinical population with Type II diabetes. The second study also used retrospective data from both the former study and another clinical trial, which examined the effects of a high monounsaturated fat diet on the prevention of diabetes in Australian adults with mild insulin resistance. The aim of this study was the compare the performance of the interviewer-administered, non-structured diet history between healthy individuals and individual with Type II diabetes mellitus undergoing similar intervention protocols with differing intensity. The results showed that the values measured by the DH in the healthy individuals were closer to those of the food record than in the individuals with Type II diabetes. Moreover, the degree of underreporting in the healthy individuals was significantly lower than in individuals with diabetes. Biases in the measurements of target nutrients also showed movement in different directions between the two sample groups. The third study was designed to fit within the research protocol of a dietary supplementation trials, which examined the effects of fish oil and soy isoflavone on the risk profile for cardiovascular disease. The study presented in this thesis aimed to assess the validity and reproducibility of a diet history method using both food records and multiple 24 hour recalls for reference, in addition to urine nitrogen and erythrocyte membrane phospholipid fatty acid composition as biomarkers of dietary intake. The biochemical markers not only provided information as to the performance of diet history method, but also the reference methods to determine if discrepancies <b>between</b> <b>paired</b> <b>data</b> were indeed due to diet history failings. The diet history method used in this study was slightly different to the aforementioned non-structured approach with the record of foods taking place on a structured meal-based form with food prompts for the dietitian. The allowed the dietitian to prompt for the context of eating as well as the foods consumed. Since the concern in this study was the accuracy of the data for a valid trial outcome, prompting was not considered to introduce bias, but rather to improve memory of foods consumed. The results showed that the diet history method was measuring higher absolute intakes of energy and macronutrients than both the food records and the 24 -hour recalls, however, both reference methods showed a higher degree of underreporting and an underestimation of nitrogen intake of energy and macronutirents than both the food record and the 24 hour recalls, however, both reference methods showed a higher degree of underreporting and an underestimation of nitrogen intake {{during the course of the}} intervention indicating that the reference methods were prone to underestimation. The diet history values also moved in line with those of the food record and the 24 -hour recall. In addition, the diet history was shown to be better at estimating long chain fatty acid intake than the food record for variables, which were of importance in a fish oil supplementation trial. Nevertheless, this study showed that the structured diet history method was both valid and reproducible for estimating macronutrient and energy intake in a sample of Australian adults participating in a dietary supplementation trial. The final study was designed in response to the lack of biomarkers available for fat intake. This study was pilot study, which aimed to investigate if the fatty acid composition of both hair and sebaceous secretions was related to the fatty acid composition of the diet. If a relationship was shown then a potential non-invasive method for assessing what may be a single or a few fatty acids in the usual diet was possible. The results showed that there were no significant relationships between hair or sebaceous fatty acid composition and corresponding dietary fatty acids due, in part, to large numbers of undetectable fatty acids in the different tissues, which prevented correlation. Future research will indicate if hair or sebaceous fatty acid composition is valuable for assessing changes in dietary fat. As a result of the studies presented in this thesis, the interviewer-administered, structured diet history method has been used in numerous studies to date conducted through the author�s institution and requests for its use continue as clinical dietitians conduct research within their own institutions. If anything, this thesis has shown that relative validation of dietary methods is valuable and should be done in context. The issues that affect persons participating in trials with large subject burdens are not the same for those who undertake just a validation study. The methods used in intervention trials needs to be validated within the context of the trial and its participants. However, objective markers should always be used as an adjunct to any validation study especially those that inform on absolute nutrient intake such as urine nitrogen. This thesis reports on a number of interesting limitations associated with measuring dietary intake in free-living individuals especially in larger individuals in the sample group. A number of interesting macronutrient-specific biases are also reported on in this work. Further research in dietary methodology should focus on the need for contextual validation of dietary methods since the administration environment affects the accuracy of the test method. In addition, clinical trails, which are dependent on nutrient targets for outcome, should make validation studies a priority and should question the accuracy of the reference methods used for validation purposes. This thesis has shown that the diet history interview is a valuable method for measuring dietary intake in clinical trials in free-living individuals. Of importance, is the contextual aspect of administration where the diet history would need to be validated in specific research environments, however, the studies presented here provide insight into the performance of the diet history method in clinical research...|$|E
40|$|Clustering {{is a type}} of {{unsupervised}} learning that one seeks to partition data into reasonable groups. – Distribution based clustering • Fit data with a mixture model to partition data. • The objective is to maximize goodness of fit of a model. • The measure of relationship <b>between</b> <b>pairs</b> of <b>data</b> points changes with different choice of mixture parameters. – Distance (similarity) based clustering • Distance measure between data points is fixed. • View data points as vertices of a graph and distances as edge weights. • The objective is to remove a fixed number of edges or to optimize an objective function defined on graphs. 4...|$|R
3000|$|... [...]. We {{can call}} it “the similar value {{calculation}} of subscript i”. In this way, the similar value <b>between</b> each <b>pair</b> of <b>data</b> points can be calculated only once. The apparent “similar value calculation of subscript i” and “similar value calculation of other subscripts” are independent from each other. Therefore, if we distribute different subscripts to different machines, then “similar value calculation of subscript i” can be operated in distributed environment.|$|R
40|$|Given {{time series}} market {{observations}} {{for a price}} process, the parameters in an assumed underlying model can be determined through maximum likelihood estimation. Transition probability densities need to be estimated <b>between</b> each <b>pair</b> of <b>data</b> points. We show that Gaussian radial basis function approximation of the Fokker-Planck equations for the densities leads to a convenient mathematical representation. We present numerical results for one and two factor interest rate models...|$|R
40|$|Spectral {{clustering}} is {{a powerful}} method for finding structure in data through the eigenvectors of a similarity matrix. It often out-performs traditional clustering algorithms such as k-means when {{the structure of the}} individual clusters is highly non-convex. Its accuracy depends on how the similarity <b>between</b> <b>pairs</b> of <b>data</b> points is defined. When a Gaussian similarity function is used, the choice of a scale parameter o is crucial. It is often suggested to select o by running the spectral algorithm repeatedly for different values of o and selecting the one that provides the best clustering according to some criterium. In this paper we propose a low cost technique for selecting a suitable o based on the minimal spanning tree (MST) associated to the graph of the distances <b>between</b> <b>pairs</b> of points. A numerical experimentation on both artificial and real-world datasets validates the effectiveness of the proposed technique...|$|R
30|$|Interferometric SAR coherence-change {{analysis}} {{obtained with}} single polarization {{has also been}} used to detect damaged urban areas. Yonezawa and Takeuchi (2001) used coherence obtained <b>between</b> a <b>data</b> <b>pair</b> including the 1995 Hyogoken-nanbu earthquake and a pair before the earthquake {{to show that the}} decrease in interferometric coherence was a good representative of the damaged urban area.|$|R
40|$|A {{series of}} printed samples on {{substrate}} of semi-gloss paper {{and with the}} magnitude of threshold color difference were prepared for scaling the visual color difference and to evaluate the performance of different method. The probabilities of perceptibly was used to normalized to Z-score and different color differences were scaled to the Z-score. The visual color difference was got, and checked with the STRESS factor. The results indicated that only the scales have been changed but the relative scales <b>between</b> <b>pairs</b> in the <b>data</b> are preserved...|$|R
40|$|Unsupervised two-view learning, or {{detection}} of depen-dencies <b>between</b> two <b>paired</b> <b>data</b> sets, is typically done by some variant of canonical correlation analysis (CCA). CCA searches for a linear projection for each view, {{such that the}} correlations between the projections are maximized. The solution is invariant to any linear transformation of either or both of the views; for tasks with small sample size such flexibility implies overfitting, which is even worse for more flexible nonparametric or kernel-based dependency discov-ery methods. We develop variants which reduce the degrees of freedom by assuming constraints on similarity of the pro-jections in the two views. A particular example is provided by a cancer gene discovery application where chromosomal distance affects the dependencies between gene copy num-ber and activity levels. Similarity constraints are shown to improve detection performance of known cancer genes. 1...|$|R
40|$|In {{this paper}} we {{introduce}} an efficient, effective and scalable clustering method de-noted as Replicator Graph Clustering. Our method takes measures of similarity <b>between</b> <b>pairs</b> of <b>data</b> points (i. e. an affinity matrix) as input and identifies {{a set of}} clusters and unique cluster assignments in a fully unsupervised manner, where the cluster granularity is adaptable by a single parameter. We provide clustering results in three subsequent steps: (a) diffusing affinities by finding personalized evolutionary stable strategies of non-cooperative games (b) building a mutual k-nearest neighbor graph representing the underlying manifold and (c) applying a graph based clustering strategy which identifies the final clusters. Individual steps have low computational complexity which leads to an efficient clustering method, scaling well with {{an increasing number of}} data points. Ex-perimental evaluation demonstrates competitive performance to state-of-the-art in several application fields. ...|$|R
40|$|Clypeaster isolatus sp. nov. is {{described}} from 61 specimens taken off San Felix Island. It is distinguished by its concave oral side, moderately high test (23 to 36 {{percent of the}} test length), broad paired petals, primary spines without a hyaline point, short, stout aboral primary spines and three to six primary tubercles on the ridge <b>between</b> the pore <b>pairs.</b> <b>Data</b> on test morphology are given for use in analysis of intraspecific variation. The new species is most closely related to C. australasiae from southeastern Australia. This affinity to the Australian fauna supports the theory of west-wind-drift dispersal...|$|R
40|$|Maintaining materialized {{views that}} have join {{conditions}} <b>between</b> arbitrary <b>pairs</b> of <b>data</b> sources possibly with cycles {{is critical for}} many applications. In this work, we model view maintenance as the process of answering a set of inter-related distributed multi-join queries. We illustrate two strategies for maintaining as well as optimizing such general join views. We propose a cost-driven view maintenance framework which generates optimized maintenance plans tuned to a given environmental settings. This framework can significantly improve view maintenance performance especially in a distributed environment. ...|$|R
40|$|In this paper, {{we propose}} an {{unsupervised}} genetic clustering algorithm, which produces a new chromosome without any conventional GAs operators, and instead {{according to the}} gene reproducing probabilities determined by Markov chain modeling. Selection of cluster centers from the dataset enables construction of a look-up table that saves the distances <b>between</b> all <b>pairs</b> of <b>data</b> points. The experimental {{results show that the}} proposed algorithm not only solves the premature problem to provide a more stable clustering performance in terms of number of clusters and clustering results, but also improves the time efficiency. 1...|$|R
40|$|This article {{addresses}} 2 -dimensional {{layout of}} high-dimensional biomedical datasets, which {{is useful for}} browsing them efficiently. We employ the Isomap technique, {{which is based on}} classical MDS (multi-dimensional scaling) but seeks to preserve the intrinsic geometry of the data, as captured in the geodesic manifold distances <b>between</b> all <b>pairs</b> of <b>data</b> points while classical approaches can see just the Euclidean structure. According to first two of Isomap's coordinates, the high-dimensional data points are arranged in a plane. Experimental results with images of marine creatures' shapes and 3 D bone renderings are presented...|$|R
40|$|This paper {{describes}} two novel learning algorithms for abrupt change detection in multivariate {{sensor data}} streams {{that can be}} applied when no explicit models of data distributions before and after the change are available. One of the algorithms, MB-GT, uses average Euclidean distances <b>between</b> <b>pairs</b> of <b>data</b> sets as the decision variable, and the other, MB-CUSUM, is a direct extension of the CUSUM algorithm to the case when the unknown probability density functions are estimated by means of kernel density estimates. The algorithms operate on a sliding memory buffer of the most recent N data readings, and consider all possible splits of that buffer into two contiguous windows before and after the change. Despite the apparent computational complexity of O(N^ 4) of this computation, our proposed algorithmic solutions exploit the structure present in their respective decision functions and exhibit computational complexity of only O(N^ 2) and memor...|$|R
40|$|Various {{methods have}} been used in an attempt to reveal salient {{parameters}} for musical timbre perception and to estimate perceptual distances between timbres based on their spectra. The multidimensional scaling method compares exemplars of musical sounds using listener estimation of timbral dissimilarity <b>between</b> sound <b>pairs.</b> <b>Data</b> are coalesced into a space whose dimensions are easily interpretable in terms of acoustic parameters such as spectral centroid, spectral flux, spectral irregularity, and attack time. Sounds can be normalized with respect to some parameters in order to reveal others. The direct comparison method attempts to discover how discrimination, recognition, and degree of perceptual dissimilarity depend on the degree of modification of the time‐varying spectrum or of particular features of the spectrum. Morphing <b>between</b> spectrum <b>pairs</b> is one method of producing spectral modifications. Dissimilarity can be predicted by computing a “distance” between two time‐varying spectra. For musical sounds,spectral distance has been computed by taking the average Cartesian distance between spectralmeasures treated as vectors. For constant‐F 0 sounds, the most obvious vector is the set of harmonic amplitudes, either in linear or decibel units. Other vectors are based on critical bands and cepstral coefficients. [Work supported by Research Grants Council Grant No. 613508. ...|$|R
40|$|Affinity {{propagation}} (AP) is a clustering {{method that}} can find data centers or clusters by sending messages <b>between</b> <b>pairs</b> of <b>data</b> points. Seed Affinity Propagation {{is a novel}} semisupervised text clustering algorithm {{which is based on}} AP. AP algorithm couldn’t cope up with part known data direct. Therefore, focusing on this issue a semi-supervised scheme called incremental affinity propagation clustering is present in the paper where pre-known information is represented by adjusting similarity matrix The standard affinity propagation clustering algorithm also suffers from a limitation {{that it is hard to}} know the value of the parameter “preference ” which can yield an optimal clustering solution. This limitation can be overcome by a method named, adaptive affinity propagation. The method first finds out the range of “preference”, then searches the space of “preference ” to find a good value which can optimize the clustering result. Keywords- Affinity propagation, Clustering, Incremental, Partition adaptive affinity propagatio...|$|R
40|$|The Affinity Propagation {{algorithm}} {{is a novel}} clustering method proposed by Frey and Dueck in 2007 [1], which combines advantages of both affinity-based clustering (like Hierarchical algorithms) and model-based clustering (like Expectation-Maximisation). The method takes as input measures of similarity <b>between</b> <b>pairs</b> of <b>data</b> points and real-valued messages are exchanged between data points until a set of centres (called exemplars) and corresponding clusters gradually emerge. The Affinity Propagation algorithm also provides a procedure {{to determine the number}} of clusters to be considered. In this talk, the approach proposed by Frey and Dueck will be presented and results coming from the application of the technique to breast cancer data sets will be reported and compared with the original ones. Results from Affinity Propagation are consistent with the results already obtained having the advantage of providing an indication about the number of clusters to consider in the analysis. Moreover, results also provide novel insights, with respect to conventional approaches...|$|R
40|$|Targeted {{surveillance}} of high risk invasion sites using insect traps {{is becoming an}} important tool in border biosecurity, aiding in early detection and subsequent monitoring of eradication attempts. The mark-release-recapture technique is widely used to study the dispersal of insects, and to generate unbiased estimates of population density. It may also {{be used in the}} biosecurity context to quantify the efficacy of surveillance and eradication monitoring systems. Marked painted apple moths were released at three different locations in Auckland, New Zealand over six weeks during a recent eradication campaign. The results of the mark-release-recapture experiment were used to parameterise a process-based mechanistic dispersal model {{in order to understand the}} moth dispersal pattern in relation to wind patterns, and to provide biosecurity agencies with an ability to predict moth dispersal patterns. A genetic algorithm was used to fit some model parameters. Different objective functions were tested: 1) Cohen’s Kappa test, 2) the sum of squared difference on trap catches, 3) the sum of squared difference weighted by distance from the release site, 4) the sum of squared difference weighted on distance <b>between</b> best-fit <b>paired</b> <b>data.</b> The genetic algorithm proved to be a powerful fitting method, but the model results were highly dependant on the objective function used. Objective functions for fitting spatial data need to characterise spatial patterns as well as density (ie. recapture rate). For fitting stochastic models to datasets derived from stochastic spatial processes, objective functions need to accommodate the fact that a perfect fit is practically impossible, even if the models are the same. Applied on mark-release-recapture data, the Cohen’s Kappa test and the sum of squared difference on trap catches captured respectively the distance component of the spatial pattern and the density component adequately but failed to capture both requirements whereas the sum of squared difference weighted by distance from the release site did. However, in order to integrate the stochastic error generated by the model underlying stochastic process, only the sum of squared difference weighted on distance <b>between</b> best-fit <b>paired</b> <b>data</b> was adequate. The relevance of each of the fitting methods is detailed, and their respective strengths and weaknesses are discussed in relation to their ability to capture the spatial patterns of insect recaptures...|$|R
3000|$|... [...]. Because {{data points}} of {{different}} classes might overlap, we added a {{weight to the}} interval such as (2 %[*]∼[*] 10 %), depending on the distance <b>between</b> the <b>data</b> <b>pair</b> of different types. Therefore, our approach involves adding nonzero weight to <b>data</b> <b>pairs</b> {{close to each other}} but excludes the nearest and farthest pairs. Thus, the distribution of closing point pairs is considered to calculate the projection vector, and the time cost is also effectively reduced. In the experiment, this approach produced a considerably strong performance. Hence, a more reasonable classification plane and projection direction were achieved.|$|R
30|$|All {{data were}} blinded for the {{examiner}} calculating the respiratory rates. In both experiments, state 3 respiration was approximated {{as the highest}} rate of oxygen consumption. In the first experiment, state 4 respiration was derived from the oxygen consumption after depletion of ADP. The data are presented as oxygen consumption per unit of CS to compensate for the varying amount of mitochondria in each experiment. In the second experiment, state 2 respiration was derived from oxygen consumption before addition of ADP. The data are presented per milligrams of wet weight. Student’s t test and ANOVA were used for comparison <b>between</b> the <b>paired</b> groups. <b>Data</b> are presented as means[*]±[*]SD unless otherwise indicated.|$|R
40|$|Recently, {{spectral}} clustering (a. k. a. normalized graph cut) {{techniques have}} become popular for their potential ability at finding irregularlyshaped clusters in data. The input to these methods is a similarity measure <b>between</b> every <b>pair</b> of <b>data</b> points. If the clusters are well-separated, the eigenvectors of the similarity matrix {{can be used}} to identify the clusters, essentially by identifying groups of points that are related by transitive similarity relationships. However, these techniques fail when the clusters are noisy and not well-separated, or when the scale parameter that is used to map distances between points to similarities is not set correctly. Our approach to solvin...|$|R
40|$|Web {{communities}} involve {{networks of}} loosely coupled data sources. Members in those communities {{should be able}} to pose queries and gather results from all data sources in the network, where available. At the same time, data sources should have limited restrictions on how to organize their data. If a global schema is not available for such a network, query processing is strongly based on the existence of (hard to maintain) mapping rules <b>between</b> <b>pairs</b> of <b>data</b> sources. If a global schema is available, local schemas of data sources have to follow strict modelling restrictions posed by that schema. In this paper, we suggest an architecture to provide better support for distributed data management in loosely coupled data sources. In our approach, data sources can maintain diverse schemas. No explicit mapping rules between data sources are needed to facilitate query processing. Data sources can join and leave the network any time, at no cost for the community. We demonstrate our approach, describing SDQNET, a prototype platform to support semantic query processing in loosely coupled data sources. © Springer-Verlag Berlin Heidelberg 2006...|$|R
40|$|Clustering data by {{identifying}} {{a subset of}} representative examples is important for processing sensory signals and detecting patterns in data. Such “exemplars ” can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called “affinity propagation,” which takes as input measures of similarity <b>between</b> <b>pairs</b> of <b>data</b> points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time. Clustering data based on a measure of similarity is a critical step in scientific data analysis and in engineering systems. A common approach is to use data to learn a set of centers such that the sum o...|$|R
