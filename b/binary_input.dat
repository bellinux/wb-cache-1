344|365|Public
25|$|An {{example of}} an {{accepting}} state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the <b>binary</b> <b>input</b> string contains an even number of 0s.|$|E
2500|$|A binary {{symmetric}} channel (BSC) with {{crossover probability}} p is a <b>binary</b> <b>input,</b> binary output channel that flips the input bit with probability p. The BSC has {{a capacity of}} [...] bits per channel use, where [...] is the binary entropy function to the base-2 logarithm: ...|$|E
2500|$|A binary erasure channel (BEC) with erasure {{probability}} p is a <b>binary</b> <b>input,</b> ternary output channel. The possible channel outputs are 0, 1, {{and a third}} symbol 'e' {{called an}} erasure. The erasure represents complete loss of information about an input bit. The capacity of the BEC is [...] bits per channel use.|$|E
5000|$|... 1961: MINOS 1 First {{perceptron}} machine, {{responds to}} {{a pattern of}} <b>binary</b> <b>inputs</b> using weights.|$|R
5000|$|In general, the encoder outputs + {{level for}} a <b>binary</b> 1 <b>input</b> and a − level for a <b>binary</b> 0 <b>input.</b>|$|R
50|$|For {{a system}} of 3 {{processors}} {{with one of them}} Byzantine, there is no solution for the consensus problem in a synchronous message passing model with <b>binary</b> <b>inputs.</b>|$|R
2500|$|Another way of {{handling}} {{this is a}} reduction for the above algorithm. Using Gray code, starting from zero, determine the change to the next value. If the change is a 1 turn left, {{and if it is}} 0 turn right. Given a <b>binary</b> <b>input,</b> B, the corresponding gray code, G, is given by [...] "G = B XOR (B>>1)". Using G'i and G'i−1, turn equals" [...] (not G'i) AND G'i−1".|$|E
2500|$|The {{mechanism}} that positions the typing element ("ball") takes a <b>binary</b> <b>input,</b> and converts this to character offsets using two mechanical digital-to-analog converters, which are [...] "whiffletree" [...] linkages {{of the type}} used for adding and subtracting in linkage-type mechanical analog computers. (The nomenclature used by IBM Office Product Customer Engineers and in IBM maintenance publications for the machine's [...] "whiffletrees" [...] is [...] "Rotate and Tilt Differentials.") Every character position on the element has a two-part binary code, one for tilt and one for rotate.|$|E
5000|$|The [...] are polynomials with binary coefficients, and {{corresponding}} to the <b>binary</b> <b>input</b> of length [...]|$|E
5000|$|Parity {{problems}} {{are widely used}} as benchmark problems in genetic programming but inherited from the artificial neural network community. Parity is calculated by summing all the <b>binary</b> <b>inputs</b> and reporting if the sum is odd or even. This is considered difficult because: ...|$|R
40|$|XCS {{is widely}} {{accepted}} {{as one of}} the most reliable Michigan-style Learning Classifier System for data mining. Many studies found that XCS is able to provide good generalization using a ternary representation for <b>binary</b> <b>inputs</b> as well as interval representation for continuous-valued inputs. Since distributed data mining is becoming more popular due to massive data sets spread across a network at many organizations, we have proposed an XCS system for distributed data mining called DXCS. DXCS has been tested on <b>binary</b> <b>inputs.</b> The results showed that DXCS does not only achieve as good performance as the centralized XCS system, but also reduces data transmission in the network. In this paper, we further examine DXCS with real-valued inputs in dynamic environments. I...|$|R
40|$|We {{implemented}} {{two versions}} of Naïve Bayesian classifiers, one for <b>binary</b> <b>inputs</b> and one for continuous inputs. Both make independence assumptions between the <b>input</b> variables/features. The <b>binary</b> version uses frequency counts to estimate probabilities. The continuous version assumes a Gaussian distribution of the samples in each class...|$|R
50|$|An {{example of}} an {{accepting}} state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the <b>binary</b> <b>input</b> string contains an even number of 0s.|$|E
5000|$|Expect {{the output}} of every program to become the input to another, as yet unknown, program. Don't clutter output with {{extraneous}} information. Avoid stringently columnar or <b>binary</b> <b>input</b> formats. Don't insist on interactive input.|$|E
5000|$|A binary {{symmetric}} channel (BSC) with {{crossover probability}} p is a <b>binary</b> <b>input,</b> binary output channel that flips the input bit with probability p. The BSC has {{a capacity of}} [...] bits per channel use, where [...] is the binary entropy function to the base-2 logarithm: ...|$|E
40|$|Tri-state Self Organizing Map (bSOM), {{which takes}} <b>binary</b> <b>inputs</b> and {{maintains}} tri-state weights, {{has been used}} for classification rather than clustering in this paper. The major contribution here is the demonstration of the potential use of the modified bSOM in security surveillance, as a recognition system on FPGA...|$|R
40|$|A {{method of}} error {{detection}} is proposed for noisy logical computer elements. The proposal extends {{the range of}} the propositional variables so that residue class check symbols may be used in error detection. The principal consequence is that individual logical elements may be designed to process <b>binary</b> <b>inputs</b> with arbitrary reliability and nonzero channel capacity...|$|R
40|$|A new algorithm, called Hamming Clustering (HC), for the {{solution}} of classification problems with <b>binary</b> <b>inputs</b> is proposed. It builds a logical network containing only and, or and not ports, which, besides satisfying all the input-output pairs included in a given finite consistent training set, is able to reconstruct the underlying Boolean function. The basi...|$|R
5000|$|A binary {{symmetric}} channel with {{crossover probability}} p denoted by , is a channel with <b>binary</b> <b>input</b> and binary output and {{probability of error}} p; that is, if X is the transmitted random variable and Y the received variable, then the channel {{is characterized by the}} conditional probabilities ...|$|E
5000|$|It {{consists}} of one input layer, one hidden layer and one output layer. The number of neurons in the output layer {{depends on the}} number of hidden units K. Each hidden neuron has N <b>binary</b> <b>input</b> neurons: The weights between input and hidden neurons are also binary: ...|$|E
5000|$|A binary erasure channel (BEC) with erasure {{probability}} p is a <b>binary</b> <b>input,</b> ternary output channel. The possible channel outputs are 0, 1, {{and a third}} symbol 'e' {{called an}} erasure. The erasure represents complete loss of information about an input bit. The capacity of the BEC is 1 &minus; p bits per channel use.|$|E
40|$|We {{consider}} finite state channels {{where the}} state of the channel is its previous output. We refer to these as POST (Previous Output is the STate) channels. We first focus on POST(α) channels. These channels have <b>binary</b> <b>inputs</b> and outputs, {{where the state}} determines if the channel behaves as a Z or an S channel, both with parameter α. ...|$|R
40|$|Abstract. In {{this paper}} we analyse a simple {{model of a}} digital {{communications}} channel. This model proves to be closely related to an iterated function system (IFS) related to the well-known Bernoulli convolution. We derive it from a randomly forced first-order ordinary differential equation. This allows the parameter of the Bernoulli convolution—the contraction rate, λ—to {{be related to the}} rate at which symbols are input to the channel. It is shown that for a channel with equiprobable <b>binary</b> <b>inputs</b> the mutual information between input and output distributions is the stationary measure of the complement of the overlap region of the IFS. We show that the mutual information is Hölder continuous with respect to λ and decreases hyperexponentially as λ → 1. We also study the case of non-equiprobable <b>binary</b> <b>inputs</b> and show that the maximum of the mutual information—the channel capacity— does not always correspond to equiprobable inputs...|$|R
40|$|Abstract—In this paper, {{we explain}} {{how to build}} a turbo-like {{structure}} with <b>binary</b> <b>inputs</b> and chaotic outputs for efficient coding and decoding in additive white Gaussian noise (AWGN). We analyze the convergence of the decoding algorithm, the performance in the error floor region and explain minimum distance properties of the resulting codes. Index Terms—Channel coding, chaos, concatenated coding, modulation coding, error analysis. I...|$|R
5000|$|A binary erasure channel with erasure {{probability}} Pe is {{a channel}} with <b>binary</b> <b>input,</b> ternary output, and probability of erasure Pe. That is, let X be the transmitted random variable with alphabet {0, 1}. Let Y be the received variable with alphabet {0, 1, e}, where e is the erasure symbol. Then, the channel {{is characterized by}} the conditional probabilities ...|$|E
5000|$|In the CC4 network, {{which is}} a three-stage network, the number of input nodes is one more than {{the size of the}} {{training}} vector, with the extra node serving as the biasing node whose input is always 1. For <b>binary</b> <b>input</b> vectors, the weights from the input nodes to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula: ...|$|E
5000|$|We choose {{concrete}} {{values for}} the parameters n, m, and p as follows: n = 64, m= 16, p= 257. For these parameters, any fixed compression function in the family takes a <b>binary</b> <b>input</b> of length mn = 1024 bits (128 bytes), to an output in the range , which has size [...] An output in [...] can easily be represented using 528 bits (66 bytes).|$|E
40|$|An {{adaptive}} network {{which can}} produce any real-valued function of its <b>binary</b> <b>inputs</b> is presented. The network can synthesize any Boolean {{function as a}} special case. The appropriate values of the adjustable parameters may be calculated by the network itself, {{or they may be}} calculated by a simple linear operation. The parameters form a uniformly distributed memory for the input-output function...|$|R
40|$|In {{this paper}} we analyse a simple {{model of a}} digital {{communications}} channel. This model proves to be closely related to an iterated function system (IFS) related to the well-known Bernoulli convolution. We derive it from a randomly forced first-order ordinary di#erential equation. This allows the parameter of the Bernoulli convolution [...] -the contraction rate, # [...] -to {{be related to the}} rate at which symbols are input to the channel. It is shown that for a channel with equiprobable <b>binary</b> <b>inputs</b> the mutual information between input and output distributions is the stationary measure of the complement of the overlap region of the IFS. We show that the mutual information is Holder continuous with respect to # and decreases hyperexponentially 1. We also study the case of non-equiprobable <b>binary</b> <b>inputs</b> and show that the maximum of the mutual information [...] -the channel capacity [...] - does not always correspond to equiprobable inputs...|$|R
3000|$|Consider a {{convolutional}} code C(n,k,ν), where ν, k and n are the overall constraint length, {{the number of}} <b>binary</b> <b>inputs</b> and <b>binary</b> outputs, respectively, while the code rate is R = k/n. Every {{convolutional code}} can be represented by a semi-infinite trellis which (apart from a short transient in its beginning) is periodic, the shortest period being a trellis module. The conventional trellis module Φ [...]...|$|R
50|$|The H-ternary code {{has three}} levels for signal representation; these are {{positive}} (+), zero (0), and negative (−). These three levels {{are represented by}} three states. The state of the line code could be {{in any one of}} these three states. A transition takes place to the next state {{as a result of a}} <b>binary</b> <b>input</b> 1 or 0 and the encoder's present output state. The encoding procedure is as follows.|$|E
5000|$|Another way of {{handling}} {{this is a}} reduction for the above algorithm. Using Gray code, starting from zero, determine the change to the next value. If the change is a 1 turn left, {{and if it is}} 0 turn right. Given a <b>binary</b> <b>input,</b> B, the corresponding gray code, G, is given by [...] "G = B XOR (B>>1)". Using Gi and Gi−1, turn equals" [...] (not Gi) AND Gi−1".|$|E
5000|$|A Z-channel (or {{a binary}} {{asymmetric}} channel) is a channel with <b>binary</b> <b>input</b> and binary output where the crossover 1 → 0 occurs with nonnegative probability p, whereas the crossover 0 → 1 never occurs. In other words, if X and Y are the random variables describing the probability distributions of the input and {{the output of}} the channel, respectively, then the crossovers of the channel are characterized by the conditional probabilities ...|$|E
40|$|An {{iterative}} decoding threshold {{analysis for}} terminated regular LDPC convolutional (LDPCC) codes is presented. Using density evolution techniques, the convergence behavior of an iterative belief propagation decoder is analyzed for the binary erasure channel and the AWGN channel with <b>binary</b> <b>inputs.</b> It is shown {{that for a}} terminated LDPCC code ensemble, the thresholds are better than for corresponding regular and irregular LDPC block codes...|$|R
40|$|A new {{synchronisation}} {{technique for}} a T out of N-user multiple access adder channel (MAC) with <b>binary</b> <b>inputs</b> is described. The application of artificial neural networks at the decoder is {{considered as a}} fast and robust technique for filtering, sorting, shifting and decision making upon the incoming MAC data. An efficient decoding technique is introduced for the extraction of block synchronisation information, thus helping to reconstruct the original signals accurately...|$|R
40|$|This paper {{suggests}} the systolic array implementation of block based Hopfield neural network architecture using completely digital circuits. The design {{is based on}} rewriting the energy equation of Hopfield neural network to a systolic (or modular) form. The performance of the proposed architecture is evaluated by applying various <b>binary</b> <b>inputs</b> and {{it is observed that}} the network provides massive parallelism and can be extended by cascading identical chips. 1...|$|R
