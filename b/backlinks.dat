128|42|Public
5|$|To {{convert the}} {{backlink}} data gathered by BackRub's web crawler into {{a measure of}} importance for a given web page, Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the <b>backlinks</b> that connected one Web page to another, and allowed the number of links and their rank, to determine the rank of the page. Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network. After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.|$|E
25|$|Although Pinging adds {{no value}} to a website's rank-ability in Search Engines, it does help search engine Web {{crawlers}} discover content quicker, which may give it (the content) the visibility needed to attract traffic (often resulting in conversions such as sharing the website, which means additional <b>backlinks,</b> social shares, etc., which does help a website rank.|$|E
25|$|To {{convert the}} {{backlink}} data gathered by BackRub's web crawler into {{a measure of}} importance for a given web page, Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the <b>backlinks</b> that connected one Web page to another.|$|E
5000|$|... memberof: {{support for}} memberOf and similar <b>backlink</b> {{attributes}} ...|$|R
5000|$|Rank {{higher in}} Google´s {{semantic}} search through relevant <b>backlinking</b> ...|$|R
50|$|Some {{other words}} for <b>backlink</b> are {{incoming}} link, inbound link, inlink, inward link, and citation.|$|R
500|$|BackRub already {{exhibited}} the rudimentary functions {{and characteristics of}} a search engine: a query input was entered and it provided a list of <b>backlinks</b> ranked by importance. Page recalled: [...] "We realized {{that we had a}} querying tool. It gave you a good overall ranking of pages and ordering of follow-up pages." [...] Page said that in mid-1998 they finally realized the further potential of their project: [...] "Pretty soon, we had 10,000 searches a day. And we figured, maybe this is really real." ...|$|E
2500|$|Blogosphere: The {{collective}} {{community of}} all blogs and blog authors, particularly notable and widely read blogs, {{is known as}} the blogosphere. Since all blogs are on the internet by definition, they may be seen as interconnected and socially networked, through blogrolls, comments, linkbacks (refbacks, trackbacks or pingbacks), and <b>backlinks.</b> Discussions [...] "in the blogosphere" [...] are occasionally used by the media as a gauge of public opinion on various issues. Because new, untapped communities of bloggers and their readers can emerge {{in the space of a}} few years, Internet marketers pay close attention to [...] "trends in the blogosphere".|$|E
2500|$|Page {{focused on}} the problem of finding out which web pages link to a given page, {{considering}} the number and nature of such <b>backlinks</b> as valuable information for that page—the role of citations in academic publishing would also become pertinent for the research. Sergey Brin, a fellow Stanford PhD student, would soon join Page's research project, nicknamed [...] "BackRub." [...] Together, the pair authored a research paper titled [...] "The Anatomy of a Large-Scale Hypertextual Web Search Engine", which {{became one of the most}} downloaded scientific documents in the history of the Internet at the time.|$|E
50|$|<b>Backlink</b> data allows webmasters {{to access}} data about their {{referring}} links. Part of this feature was acquired from Yahoo! Site Explorer.|$|R
5000|$|Link bidding {{is a form}} of Internet {{marketing}} where {{a website}} owner bids on other websites to post a <b>backlink</b> to his/her website.|$|R
40|$|From a user’s perspective, {{hypertext}} links on the {{web form}} a directed graph between distinct information sources. We investigate the effects of discovering “backlinks ” from web resources, namely links pointing to the resource. We describe tools for <b>backlink</b> navigation on both the client and server side, using an applet for the client and a module for the Apache web server. We also discuss possible extensions to the HTTP protocol to facilitate the collection and navigation of <b>backlink</b> information in the world wide web. ...|$|R
50|$|Websites often employ SEO {{techniques}} {{to increase the}} number of <b>backlinks</b> pointing to their website. Some methods are free for use by everyone whereas some methods, like linkbaiting, require quite a bit of planning and marketing to work. There are also paid {{techniques to}} increase the number of <b>backlinks</b> to a target site. For example, private blog networks can be used to purchase <b>backlinks.</b>|$|E
5000|$|Unlinked content: pages {{which are}} not linked to by other pages, which may prevent web {{crawling}} programs from accessing the content. This content {{is referred to as}} pages without <b>backlinks</b> (also known as inlinks). Also, search engines do not always detect all <b>backlinks</b> from searched web pages.|$|E
50|$|Domain Authority (DA), {{a website}} metric {{developed}} by Moz, is a predictive metric {{to determine a}} website's traffic and organic search engine rankings. Domain Authority is based on different link metrics, such as number of linking root domains, number of total <b>backlinks,</b> and the distance of <b>backlinks</b> from the home page of websites.|$|E
50|$|If {{the file}} header is an {{extension}} header, <b>BACKLINK</b> contains the file ID of the primary header; otherwise, it contains the file ID of the directory file containing the primary entry for the file.|$|R
50|$|On {{playing out}} the material, the {{audio and video}} signals are usually transported from the playout area to the network via a studio/transmitter link (STL), which may be fibre <b>backlink,</b> {{microwave}} or satellite uplink.|$|R
50|$|A <b>backlink</b> {{for a given}} web {{resource}} is a link from some other website (the referrer) to that web resource (the referent). A web resource may be (for example) a website, web page, or web directory.|$|R
50|$|A {{backlink}} is {{a reference}} comparable to a citation. The quantity {{and sources of}} <b>backlinks</b> for a web page are among the factors that Google's PageRank algorithm evaluates in order to estimate how important the page is. The PageRank score is, in turn, one of the variables that Google Search uses to determine how high a web page should go in search results. This weighting of <b>backlinks</b> is analogous to citation analysis of books, scholarly papers, and academic journals.|$|E
5000|$|In 2011, Boumaaza {{formed an}} online YouTube {{initiative}} called [...] "Together to the Top" [...] (TTTT) {{with the objective}} of promoting new YouTube content creators by featuring smaller producers' videos and offering <b>backlinks</b> on his YouTube channel. In addition to providing <b>backlinks,</b> Boumaaza pledged to film a personal video (individual video responses) for each of his fans who subscribed and provided a backlink to Boumaaza's channel. The project was ultimately a failure and Boumaaza never delivered on his promises.|$|E
50|$|In 2014 Ruddie {{said that}} Google was making things more {{difficult}} for reputation management companies, stressing the value of good content and high-quality <b>backlinks.</b>|$|E
5000|$|Recent tools use often stable {{links to}} trace dependencies. This {{can be done}} on all levels, amongst them specification, blueprint, bugs, commits. Despite this, the use of <b>backlink</b> {{checkers}} known from search engine optimization is not common. Research {{in this area is}} done as well, just to name use case maps ...|$|R
50|$|Widgets come in many {{shapes and}} sizes, {{but two of}} the major types today are Web widgets and Desktop widgets. Web widgets are {{intended}} for use on (embedding in) webpages and have major implications in areas such as site performance, SEO and even <b>backlink</b> campaigns. Desktop widgets are embedded on local computers, and do not impact SEO or webpage performance.|$|R
40|$|Paper {{repository}} {{at higher}} education {{is a collection of}} scientific articles created by the academic society. This study took as many as 80 universities in the Webometrics ranking of repositories in the Southeast Asia region. The tools used in this research is Google for number of web page and Google Scholar for number of document paper repository and Ahrefs for referring page, <b>backlink</b> and reffering domain. The result of this study, Eprints is the most widely used tools in higher education, as many as 37 higher educations (46, 25 %). Institut Teknologi Sepuluh November got the highest score in number of web page in Google (2. 010. 000), Bogor Agricultural University Scientific Repository got the highest score for number of document paper (44. 300). University of Sumatera Utara Repository got the highest score for reffering page (82588) and <b>backlink</b> (86421). Universiti Teknologi Malaysia Institutional Repository got the highest score for reffering domain (532) ...|$|R
5000|$|<b>Backlinks</b> {{showed the}} most {{important}} correlation (and also Google’s Toolbar PageRank, suggesting that older links are an advantage because the Toolbar has not been updated in a long time).|$|E
50|$|Since social {{bookmarking}} generates <b>backlinks,</b> social bookmark link generating services {{may be used}} by some webmasters {{in an attempt to}} artificially improve their websites' rankings in search engine results pages.|$|E
50|$|When {{the search}} results pop {{up after the}} user has input their keyword, DeepPeep ranks the links based on 3 features: term content, number of <b>backlinks.</b> and pagerank. Firstly, the term content is simply {{determined}} by {{the content of the}} web link and its relevance. <b>Backlinks</b> are hyperlinks or links that direct the user to a different website. Pageranks is the ranking of websites in search engine results and works by counting the amount and quality of links to website to determine its importance. Pagerank and back link information are obtained from outside sources such as Google, Yahoo, and Bing.|$|E
50|$|Search engines require ways {{to confirm}} page relevancy. A known {{method is to}} examine for one-way links coming {{directly}} from relevant websites. The process of building links {{should not be confused}} with being listed on link farms, as the latter requires reciprocal return links, which often renders the overall <b>backlink</b> advantage useless. This is due to oscillation, causing confusion over which is the vendor site and which is the promoting site.|$|R
2500|$|John Battelle, {{cofounder of}} Wired magazine, wrote that Page had {{reasoned}} that the: ... entire Web was {{loosely based on}} the premise of citation—after all, what is a link but a citation? If he could devise a method to count and qualify each <b>backlink</b> on the Web, as Page puts it [...] "the Web would become a more valuable place." [...] Battelle further described how Page and Brin began working together on the project: ...|$|R
50|$|Serpstat is an {{all-in-one}} SEO platform, {{which consists}} of five different modules and is used for Keyword Research, Competitor Analysis, Site Audit, <b>Backlink</b> Analysis and Rank Tracking. Initially, it was used within the Ukrainian digital marketing agency Netpeak that became its main investor. Serpstat's database includes information on 12 geographical regions of Google (USA, Australia, United Kingdom, Bulgaria, Latvia, Lithuania, Ukraine, Russia, Kazakhstan, South Africa, Canada, Belarus) and 3 regional databases of Yandex - Yandex Ukraine, Yandex Moscow, and Yandex Saint-Petersburg.|$|R
5000|$|Google {{has been}} {{updating}} its algorithm {{for as long}} as it has been fighting the manipulation of organic search results. However, up until May 10, 2012, when Google launched the Google Penguin update, many people wrongly believed that low-quality <b>backlinks</b> would not negatively affect ranks. While this viewpoint was common, it was not correct, as Google had been applying such link-based penalties for many years, but not made public how the company approached and dealt with what they called [...] "link spam". Since this time there has been a much wider acknowledgement about the dangers of bad SEO and a forensic analysis of <b>backlinks</b> to ensure there are no harmful links.|$|E
5000|$|Webometrics {{can be used}} {{to assess}} an academic's {{visibility}} based on [...] "web mentions" [...] or references to academic work by web pages. Metrics include number of URLs, Google Pagerank, and number of sources linking to associated pages (<b>backlinks).</b>|$|E
50|$|Some programmers {{who create}} scraper sites may {{purchase}} a recently expired domain name to reuse its SEO power in Google. Whole businesses focus on understanding all expired domains and utilising {{them for their}} historical ranking ability exist. Doing so will allow SEOs to utilize the already-established <b>backlinks</b> to the domain name. Some spammers may try to match {{the topic of the}} expired site or copy the existing content from the Internet Archive to maintain the authenticity of the site so that the <b>backlinks</b> don't drop. For example, an expired website about a photographer may be re-registered to create a site about photography tips or use the domain name in their private blog network to power their own photography site.|$|E
40|$|The {{capability}} {{to understand the}} needs and responses of stakeholders is critical in today's business environments. A <b>backlink</b> search tool called Redips was developed to collect information on a company's business communities. This tool has been tested by student subjects and received satisfactory feedback on its usefulness and functionality. However, {{in order to gain}} acceptance from management, the perceptions and expectations for the tool should not be overlooked. This study presents a wide range of perceptions from a group of information systems practitioners on the potential use of Redips...|$|R
25|$|Cho et al. {{made the}} first study on {{policies}} for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. The ordering metrics tested were breadth-first, <b>backlink</b> count and partial Pagerank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his Ph.D. dissertation at Stanford on web crawling.|$|R
40|$|This paper {{proposes a}} method for finding related Web pages based on {{connectivity}} information of hyperlinks. As claimed by Kumar, a complete bipartite graph of Web pages {{can be regarded as}} a Web community sharing a common interest. However, preparing Web snapshot data for the search of such communities is not an easy task since the Web is huge and is growing. In our method, connectivity information is acquired from a search engine by <b>backlink</b> search. A system based on the method succeeds in finding several genres of Web communities only from a few input URLs without analyzing the contents of Web pages...|$|R
