0|10000|Public
50|$|The {{following}} {{is a list of}} common inductive <b>biases</b> <b>in</b> machine learning <b>algorithms.</b>|$|R
40|$|Evidence for {{variable}} selection <b>bias</b> <b>in</b> classification tree <b>algorithms</b> {{based on the}} Gini Index is reviewed from the literature and embedded into a broader explanatory scheme: Variable selection <b>bias</b> <b>in</b> classification tree <b>algorithms</b> based on the Gini Index can be caused {{not only by the}} statistical effect of multiple comparisons, but also by an increasing estimation bias and variance of the splitting criterion when plug-in estimates of entropy measures like the Gini Index are employed. The relevance of these sources of {{variable selection}} <b>bias</b> <b>in</b> the different simulation study designs is examined. Variable selection bias due to the explored sources applies to all classification tree algorithms based on empirical entropy measures like the Gini Index, Deviance and Information Gain, and to both binary and multiway splitting algorithms. ...|$|R
30|$|If an {{autonomous}} robot swarm {{is used to}} detect and report the locations of survivors, what issues might cause <b>bias</b> <b>in</b> reporting? Computer <b>algorithms</b> are developed by humans and cannot {{be said to be}} entirely free of bias and politics (Sandvik et al. 2014). Different algorithms, test cases, or detection equipment could create <b>bias</b> <b>in</b> the detection and reporting process.|$|R
5000|$|Benjamin Edelman, an {{assistant}} professor at Harvard Business School, claims that Google has [...] "hard-coded" [...] <b>bias</b> <b>in</b> the <b>algorithm</b> it employs to generate its OneBox results which are shown usually at the top when a query can be answered quickly or a direct link can be given. Edelman, states that in such cases, Google services such as Google Finance receive preferential listing over more popular finance sites such as Yahoo! Finance. However, Barry Schwartz, CEO of RustyBrick, points out that OneBox results are not organic and therefore should not been viewed as algorithmic.|$|R
40|$|In {{previous}} work a method was proposed {{to determine the}} <b>bias</b> <b>in</b> localization <b>algorithms</b> using range or bearing data. In this paper the method is extended to be more generic; in particular, different types of measurement data are permitted, {{and there may be}} more measurements than there are variables to estimate. The method combines the Taylor series and Jacobian matrices to determine the bias, and leads to an easily calculated analytical bias expression, despite the general unavailability of analytic expressions for the solution of most localization problems. The method is used to estimate the <b>bias</b> <b>in</b> scan-based localization. Monte Carlo simulation results verify the performance of the proposed method in this context. Yiming Ji, Changbin Yu, Brian D. O. Anderson and Samuel P. Drak...|$|R
40|$|We discuss {{testing methods}} for exposing origin-seeking <b>bias</b> <b>in</b> PSO motion <b>algorithms.</b> The {{strategy}} of resizing the initialization space, proposed by Gehlhaar and Fogel and made {{popular in the}} PSO context by Angeline, is shown to be insufficiently general for revealing an algorithm's tendency to focus its efforts on regions {{at or near the}} origin. An alternative testing method is proposed that reveals problems with PSO motion algorithms that are not visible when merely resizing the initialization space...|$|R
40|$|We {{introduce}} a quantum algorithm for efficient biased {{sampling of the}} rare events generated by classical memoryful stochastic processes. We show that this quantum algorithm gives an extreme advantage over known classical <b>biased</b> sampling <b>algorithms</b> <b>in</b> terms of the memory resources required. The quantum memory advantage ranges from polynomial to exponential and when sampling the rare equilibrium configurations of spin systems the quantum advantage diverges. Comment: 11 pages, 9 figures; [URL]...|$|R
40|$|A noniterative method using {{optical flow}} {{to recover the}} {{translation}} direction of a moving camera has been previously proposed in [4]. We present a detailed explanation of the <b>bias</b> <b>in</b> this <b>algorithm</b> and compare methods for eliminating this bias, as well as presenting a comprehensive error analysis. This analysis includes a necessary modification to the Cram'erRao lower bound (CRLB). We propose a simple iterative modification to the algorithm which produces unbiased translation direction estimates that approach the CRLB. Numerical results are {{used to compare the}} various techniques on synthetic and real image sequences. Index Terms [...] translation direction estimation, linear constraints, optical flow, error analysis, performance comparison 3 This research has been partially supported by grants from the Information Technology Research Centre of Ontario and the Natural Sciences and Engineering Research Council of Canada. This work was presented in part at the 1995 IEEE International Confe [...] ...|$|R
40|$|This paper {{presents}} an algorithm for pricing American options using Monte Carlo simulation. The method {{is based on}} using a parametric representation of the exercise boundary. Error bounds are constructed using two di erent estimates, one which is biased low, and one which is biased high. I show that both are consistent and asymptotically unbiased estimators of the true option value. Results for high-dimensional American options con rm the viability ofthe numerical procedure. The convergence results of the paper shed light into the <b>biases</b> present <b>in</b> other <b>algorithms</b> proposed <b>in</b> the literature...|$|R
40|$|This paper {{presents}} a signal-to-noise {{perspective of the}} search bias introduced by genetic algorithms. A decision theoretic signal-tonoise framework is used {{to show that there}} are two fundamental modes of introducing difficulty through search bias: 1) sending a wrong signal and 2) increasing or decreasing noise depending on the direction of signal. The main {{purpose of this paper is}} to identify crosstalk as another possible source of problems, caused by search <b>bias</b> <b>in</b> genetic <b>algorithms.</b> I show that a small modification of the one-max problem can convert it to difficult to solve because of crosstalk. This paper also studies the royal road functions (Forrest and Mitchell, 1994) and demonstrates that crosstalk plays a major role in making R 2 harder than R 1 to solve. This clearly shows that increasing signal in the right direction alone does not necessarily make a problem easy for GA, unless the noise is also reduced. 1 INTRODUCTION A blackbox optimization problem can be difficult to solv [...] ...|$|R
40|$|Type inferencing {{according}} to the standard algorithms often yields uninformative error messages. Many times, this {{is a consequence of}} a <b>bias</b> inherent <b>in</b> the <b>algorithms.</b> The method developed here is to first collect constraints from the program, and to solve these afterwards, possibly under the influence of a heuristic. We show the soundness and completeness of our algorithm. The algorithms turn out to be deterministic instances of our method, giving the correctness for with respect to the Hindley-Milner typing rules for free. We also show that our algorithm is more flexible, because it naturally allows the generation of multiple messages...|$|R
40|$|This paper explores {{a method}} for {{analyzing}} the expressive range of a procedural level generator, and applies this method to Launchpad, a level generator for 2 D platformers. Instead of focusing {{on the number of}} levels that can be created or the amount of time it takes to create them, we instead examine the variety of generated levels and the impact of changing input parameters. With the rise in the popularity of PCG, {{it is important to be}} able to fairly evaluate and compare different generation techniques within similar domains. We have found that such analysis can also expose unexpected <b>biases</b> <b>in</b> the generation <b>algorithm</b> and holes <b>in</b> the expressive range that drive future work...|$|R
40|$|Estimating {{the optimal}} number of {{clusters}} {{is a major}} challenge in applying cluster analysis to any type of dataset, especially to biomedical datasets, which are high-dimensional and complex. Here, we introduce an improved method, Progeny Clustering, which is stability-based and exceptionally efficient in computing, to find the ideal number of clusters. The algorithm employs a novel Progeny Sampling method to reconstruct cluster identity, a co-occurrence probability matrix to assess the clustering stability, {{and a set of}} reference datasets to overcome inherent <b>biases</b> <b>in</b> the <b>algorithm</b> and data space. Our method was shown successful and robust when applied to two synthetic datasets (datasets of two-dimensions and ten-dimensions containing eight dimensions of pure noise), two standard biological datasets (the Iris dataset and Rat CNS dataset) and two biological datasets (a cell phenotype dataset and an acute myeloid leukemia (AML) reverse phase protein array (RPPA) dataset). Progeny Clustering outperformed some popular clustering evaluation methods in the ten-dimensional synthetic dataset {{as well as in the}} cell phenotype dataset, and it was the only method that successfully discovered clinically meaningful patient groupings in the AML RPPA dataset...|$|R
40|$|Research on <b>bias</b> <b>in</b> machine {{learning}} <b>algorithms</b> {{has generally been}} concerned {{with the impact of}} bias on predictive accuracy. We believe that there are other factors that should also {{play a role in the}} evaluation of bias. One such factor is the stability of the algorithm; in other words, the repeatability of the results. If we obtain two sets of data from the same phenomenon, with the same underlying probability distribution, then we would like our learning algorithm to induce approximately the same concepts from both sets of data. This paper introduces a method for quantifying stability, based on a measure of the agreement between concepts. We also discuss the relationships among stability, predictive accuracy, and bias. Comment: 14 page...|$|R
40|$|This paper {{presents}} an algorithm for pricing American options using Monte Carlo simu-lation. The method {{is based on}} using a parametric representation of the exercise boundary. It is shown that, {{as long as this}} parametric representation subsumes all relevant stopping-times, error bounds can be constructed using two different estimates, one which is biased low and one which is biased high. Both are consistent and asymptotically unbiased estima-tors of the true option value. Results for high-dimensional American options confirm the viability of the numerical procedure. The convergence results of the paper shed light into the <b>biases</b> present <b>in</b> other <b>algorithms</b> proposed <b>in</b> the literature. JEL classification: C 15, C 63, G 13...|$|R
40|$|Abstract: Numerical {{function}} approximation over a Boolean domain is {{a classical}} problem with wide application to data modeling tasks and {{various forms of}} learning. A great many function approximation algorithms have been devised over the years. Because {{the goal is to}} produce an approximating function that has low expected error, algorithms are typically guided by error reduction. This guiding force, to reduce error, can <b>bias</b> the <b>algorithm</b> <b>in</b> a detrimental manner. We illustrate this bias, and then propose an alternative approach based on a notion of value unification. ...|$|R
40|$|Publisher's PDF. Estimating {{the optimal}} number of {{clusters}} {{is a major}} challenge in applying cluster analysis to any type of dataset, especially to biomedical datasets, which are high-dimensional and complex. Here, we introduce an improved method, Progeny Clustering, which is stability-based and exceptionally efficient in computing, to find the ideal number of clusters. The algorithm employs a novel Progeny Sampling method to reconstruct cluster identity, a co-occurrence probability matrix to assess the clustering stability, {{and a set of}} reference datasets to overcome inherent <b>biases</b> <b>in</b> the <b>algorithm</b> and data space. Our method was shown successful and robust when applied to two synthetic datasets (datasets of two-dimensions and ten-dimensions containing eight dimensions of pure noise), two standard biological datasets (the Iris dataset and Rat CNS dataset) and two biological datasets (a cell phenotype dataset and an acute myeloid leukemia (AML) reverse phase protein array (RPPA) dataset). Progeny Clustering outperformed some popular clustering evaluation methods in the tendimensional synthetic dataset {{as well as in the}} cell phenotype dataset, and it was the only method that successfully discovered clinically meaningful patient groupings in the AML RPPA dataset. University of Delaware. Department of Biomedical Engineering...|$|R
40|$|Ultrasonic {{pulse-echo}} rf waveform {{analysis and}} selected pattern rec-ognition methods {{were applied to}} classification of breast tissue. Emphasis {{was placed on the}} classification of solid tissue areas since fluid areas are easily identified by present B-scan techniques. Pattern recognition techniques such as the Fisher Linear Discriminant (FLD), Probability Density Function (PDF) curves, jackknife estimate and committee vote were used to construct and evaluate a two class algorithm, malignant versus benign tissue areas. domain features from 100 pathologically confirmed tissue areas from 87 patients were used to train the algorithm. Algorithm performance was acquired via the generalized jackknife procedure to significantly reduce the <b>bias</b> frequently encountered <b>in</b> <b>algorithm</b> evaluation. Estimated values of algorithm performance are sensitivity and specificity values of 96 percent and 68 percent, respectively...|$|R
40|$|Copyright 2002 IEEEIn this paper, {{we present}} a novel nonparametric {{algorithm}} for short term electricity demand forecasting. The algorithm is based on local linear regression using sliding window with variable length. The method for selecting optimal window length for each local fit offers close insight into trade-off between bias and standard deviation of local regressions. Optimal window length is selected for each value in the load time-series: large window for linear change of load to reduce variability and small window when load departs from linear function to control <b>bias.</b> <b>In</b> the presented <b>algorithm</b> local linear regression is used to estimate trend component of the load time series and to forecast trend component by extrapolating with the fitted local linear function. Some features of the <b>algorithm</b> are demonstrated <b>in</b> the paper using examples from the historic load data recorded in the Namibian Power Utility...|$|R
40|$|Abstract. The {{classical}} {{on-line learning}} algorithms {{such as the}} LMS (Least Mean Square) algorithm have attracted renewed interest recently because of many variants based on non-standard parametrisations that <b>bias</b> the <b>algorithms</b> <b>in</b> favour of certain prior knowledge of the problem. Tools such as link functions and Bregman divergences {{have been used in}} the design and analysis of such <b>algorithms.</b> <b>In</b> this paper we reconsider the development of such variants of classical stochastic gradient descent (SGD) in a purely geometric setting. The Bregman divergence is replaced by the (squared) Riemannian distance. The property of convexity of a loss function is replaced by a more general notion of compatibility with a metric. The ideas are explicated in the development and analysis of an algorithm for online learning on spheres. ...|$|R
40|$|Algorithms are {{presented}} for managing sensor information {{to reduce the}} eects of bias when tracking interacting targets. When targets are close enough together that their measurement validation gates overlap, the measurement from one target can be confused with another. Data association algorithms such as the Joint Probabilistic Data Association (JPDA) algorithm can eectively continue to track targets under these conditions, but the target estimates may become biased. A modi cation of the Covariance Control approach for sensor management can reduce this eect. Sensors are chosen based {{on their ability to}} reduce the extent of measurement gate overlap as judged by a set of heuristic parameters derived in this paper. Monte Carlo simulation results show that these are eective methods of reducing target estimate <b>bias</b> <b>in</b> the JPDA <b>algorithm</b> when targets are close together. An analysis of the computational demands of these algorithms shows that while they are computationally demanding, they are not prohibitively so...|$|R
40|$|In {{probability}} theory and statistics notions of correlation among random variables, decay of correlation, and bias-variance trade-off are fundamental. In this work we introduce analogous notions in optimization, and we show their usefulness {{in a concrete}} setting. We propose a general notion of correlation among variables in optimization procedures {{that is based on}} the sensitivity of optimal points upon (possibly finite) perturbations. We present a canonical instance in network optimization (the min-cost network flow problem) that exhibits locality, i. e., a setting where the correlation decays {{as a function of the}} graph-theoretical distance in the network. In the case of warm-start reoptimization, we develop a general approach to localize a given optimization routine in order to exploit locality. We show that the localization mechanism is responsible for introducing a <b>bias</b> <b>in</b> the original <b>algorithm,</b> and that the bias-variance trade-off that emerges can be exploited to minimize the computational complexity required to reach a prescribed level of error accuracy...|$|R
40|$|We {{report the}} {{development}} and testing of software called QuantiFly: an automated tool to quantify Drosophila egg laying. Many laboratories count Drosophila eggs as a marker of fit-ness. The existing method requires laboratory researchers to count eggs manually while looking down a microscope. This technique is both time-consuming and tedious, especially when experiments require daily counts of hundreds of vials. The basis of the QuantiFly soft-ware is an algorithm which applies and improves upon an existing advanced pattern recog-nition and machine-learning routine. The accuracy of the baseline algorithm is additionally increased in this study through correction of <b>bias</b> observed <b>in</b> the <b>algorithm</b> output. The QuantiFly software, which includes the refined algorithm, {{has been designed to}} be immedi-ately accessible to scientists through an intuitive and responsive user-friendly graphical in-terface. The software is also open-source, self-contained, has no dependencies and is easily installe...|$|R
40|$|The {{research}} {{presented in}} this paper demonstrates a model for aiding human-robot companionship based on the principle of 'human' cognitive biases applied to a robot. The aim of this work is to study how cognitive biases can affect human-robot companionship in long-time. In the current paper, we show comparative results of the experiments using five <b>biased</b> <b>algorithms</b> <b>in</b> three different robots such as ERWIN, MyKeepon and MARC. The results were analysed to determine what difference if any of biased vs unbiased interaction has on the interaction with the robot and if the participants were able to form any kind of ‘preference’ toward the different algorithms. The experimental presented show that the participants have more of a preference towards the biased algorithm interactions than the robot without the bias...|$|R
40|$|We stacked the X-ray {{data from}} the ROSAT All Sky Survey for over 4, 000 {{clusters}} selected from the 2 MASS catalog and divided into five richness classes. We detected excess X-ray emission over background {{at the center of}} the stacked images in all five richness bins. The interrelationships between the mass, X-ray temperature and X-ray luminosity of the stacked clusters agree well with those derived from catalogs of X-ray clusters. Poisson variance in the number of galaxies occupying halos of a given mass leads to significant differences between the average richness at fixed mass and the average mass at fixed richness that we can model relatively easily using a simple model of the halo occupation distribution. These statistical effects probably explain recent results in which optically-selected clusters lie on the same X-ray luminosity-temperature relations as local clusters but have lower optical richnesses than observed for local clusters with the same X-ray properties. When we further binned the clusters by redshift, we did not find significant redshift-dependent <b>biases</b> <b>in</b> the sense that the X-ray luminosities for massive clusters of fixed optical richness show little dependence on redshift beyond that expected from the effects of Poisson fluctuations. Our results demonstrate that stacking of RASS data from optically selected clusters can be a powerful test for <b>biases</b> <b>in</b> cluster selection <b>algorithms.</b> Comment: 35 pages, 14 figures, Accepted by ApJ, with updated Lx normalizatio...|$|R
40|$|This paper investigates {{a general}} {{framework}} tor learning concepts that allows to generate accurate and comprehensible concept representations. It {{is known that}} <b>biases</b> used <b>in</b> learning <b>algorithms</b> directly affect their performance {{as well as their}} comprehensibility. A critical problem is that, most of the time, the most “comprehensible” representations are not the best performer in terms of classification! In this paper, we argue that concept learning systems should employ Multiple-Knowledge Representation: a deep knowledge level optimised from recognition (classification task) and a shallow one optimised for comprehensibility (description task). Such a model of concept learning assumes that the system can use an interpretation function of the deep knowledge level to build an approximately correct comprehensible description of it. This approach is illustrated through our GEM system which learns concepts in a numerical attribute space using a Neural Network representation as the deep knowledge level and symbolic rules as the shallow level. SCOPUS: cp. kinfo:eu-repo/semantics/publishe...|$|R
40|$|Preterm {{birth is}} a major {{contributor}} to infant mortality worldwide. Cervical length and previous history of preterm birth are the only two indicators which can help in identifying preterm birth but have a low positive identifying rate. Quantitative ultrasound parameters like attenuation can provide additional details about the tissue microstructure besides the diagnostic image. Attenuation can be used to detect preterm cases as the attenuation decreases with the increasing gestation age and this decrease can be seen earlier in cases of preterm birth. The algorithm {{and the size of the}} region of interest (ROI) play a vital role in calculating valid estimates of attenuation. In this paper, we compared the ability of the Spectral log difference algorithm and the Spectral difference algorithm to detect changes in the cervix leading to delivery for both full term and preterm births under varying ROI sizes. Spectral log difference yields a more consistent decrease in the attenuation as we approach delivery for both the preterm and full term patients. ROI size doesn 2 ̆ 7 t significantly alter the observed trends for this study. For preterm birth a maximum decreases of 0. 35 dB/cm-MHz was observed. The <b>bias</b> <b>in</b> attenuation <b>algorithms</b> can be removed by selecting homogenous regions inside the cervix, but the cervix is a heterogeneous tissue. Gamma mixture model is used to segment the cervix into different tissue types and attenuation algorithm are then applied to individual tissue type to get an estimate of attenuation. The area under the receiver operating characteristic curve increases from 56...|$|R
40|$|Many {{real-world}} {{networks are}} prohibitively large for data retrieval, storage {{and analysis of}} all of its nodes and links. Understanding the structure and dynamics of these networks entails creating a smaller representative sample of the full graph while preserving its relevant topological properties. In this report, we show that graph sampling <b>algorithms</b> currently proposed <b>in</b> the literature {{are not able to}} preserve network properties even with sample sizes containing as many as 20 % of the nodes from the original graph. We present a new sampling algorithm, called Tiny Sample Extractor, with a new goal of a sample size smaller than 5 % of the original graph while preserving two key properties of a network, the degree distribution and its clustering co-efficient. Our approach is based on a new empirical method of estimating measurement <b>biases</b> <b>in</b> crawling <b>algorithms</b> and compensating for them accordingly. We present a detailed comparison of best known graph sampling <b>algorithms,</b> focusing <b>in</b> particular on how the properties of the sample subgraphs converge to those of the original graph as they grow. These results show that our sampling algorithm extracts a smaller subgraph than other algorithms while also achieving a closer convergence to the degree distribution, measured by the degree exponent, of the original graph. The subgraph generated by the Tiny Sample Extractor, however, is not necessarily representative of the full graph with regard to other properties such as assortativity. This indicates that the problem of extracting a truly representative small subgraph from a large graph remains unsolved...|$|R
40|$|Sequence {{alignment}} underpins all {{of comparative}} genomics, yet it remains an incompletely solved problem. In particular, the statistical uncertainty within inferred alignments is often disregarded, while parametric or phylogenetic inferences are considered meaningless without confidence estimates. Here, {{we report on}} a theoretical and simulation study of pairwise alignments of genomic DNA at human-mouse divergence. We find that > 15 % of aligned bases are incorrect in existing whole-genome alignments, and we identify three types of alignment error, each leading to systematic <b>biases</b> <b>in</b> all <b>algorithms</b> considered. Careful modeling of the evolutionary process improves alignment quality; however, these improvements are modest compared with the remaining alignment errors, even with exact knowledge of the evolutionary model, emphasizing the need for statistical approaches to account for uncertainty. We develop a new algorithm, Marginalized Posterior Decoding (MPD), which explicitly accounts for uncertainties, is less biased and more accurate than other algorithms we consider, and reduces the proportion of misaligned bases by a third compared with the best existing algorithm. To our knowledge, {{this is the first}} nonheuristic algorithm for DNA sequence alignment to show robust improvements over the classic Needleman-Wunsch algorithm. Despite this, considerable uncertainty remains even in the improved alignments. We conclude that a probabilistic treatment is essential, both to improve alignment quality and to quantify the remaining uncertainty. This is becoming increasingly relevant with the growing appreciation of the importance of noncoding DNA, whose study relies heavily on alignments. Alignment errors are inevitable, and should be considered when drawing conclusions from alignments. Software and alignments to assist researchers in doing this are provided at [URL]...|$|R
40|$|We {{propose a}} simple {{algorithm}} to detect dominating synonymous codon usage <b>bias</b> <b>in</b> genomes. The <b>algorithm</b> {{is based on}} a precise mathematical formulation of the problem that lead us to use the Codon Adaptation Index (CAI) as a ‘universal’ measure of codon bias. This measure has been previously employed in the specific context of translational bias. With the set of coding sequences as a sole source of biological information, the algorithm provides a reference set of genes which is highly representative of the bias. This set can be used to compute the CAI of genes of prokaryotic and eukaryotic organisms, including those whose functional annotation is not yet available. An important application concerns the detection of a reference set characterizing translational bias which is known to correlate to expression levels; in this case, the algorithm becomes a key tool to predict gene expression levels, to guide regulatory circuit reconstruction, and to compare species. The algorithm detects also leading–lagging strands bias, GC-content bias, GC 3 bias, and horizontal gene transfer. The approach is validated on 12 slow-growing and fast-growing bacteria, Saccharomyces cerevisiae, Caenorhabditis elegans and Drosophila melanogaster...|$|R
40|$|Dendritic {{morphology}} constrains brain activity, as it determines first which neuronal circuits {{are possible}} and second which dendritic computations {{can be performed}} over a neuron's inputs. It is known that a range of chemical cues can influence the final shape of dendrites during development. Here, we investigate {{the extent to which}} self-referential influences, cues generated by the neuron itself, might influence morphology. To this end, we developed a phenomenological model and algorithm to generate virtual morphologies, which are then compared to experimentally reconstructed morphologies. In the model, branching probability follows a Galton-Watson process, while the geometry is determined by "homotypic forces" exerting influence on the direction of random growth in a constrained space. We model three such homotypic forces, namely an inertial force based on membrane stiffness, a soma-oriented tropism, and a force of self avoidance, as directional <b>biases</b> <b>in</b> the growth <b>algorithm.</b> With computer simulations we explored how each bias shapes neuronal morphologies. We show that based on these principles, we can generate realistic morphologies of several distinct neuronal types. We discuss the extent to which homotypic forces might influence real dendritic morphologies, and speculate about the influence of other environmental cues on neuronal shape and circuitry...|$|R
40|$|The game Starcraft {{is one of}} {{the most}} {{interesting}} arenas to test new machine learning and computational intelligence techniques; however, StarCraft matches take a long time and creating a good dataset for training can be hard. Besides, analyzing match logs to extract the main characteristics can also be done in many different ways to the point that extracting and processing data itself can take an inordinate amount of time and of course, depending on what you choose, can <b>bias</b> learning <b>algorithms.</b> <b>In</b> this paper we present a simplified dataset extracted from the set of matches published by Robinson and Watson, which we have called RedDwarfData, containing several thousand matches processed to frames, so that temporal studies can also be undertaken. This dataset is available from GitHub under a free license. An initial analysis and appraisal of these matches is also made...|$|R
40|$|AbstractTime-resolved single {{molecule}} fluorescence measurements {{may be used}} {{to probe}} the conformational dynamics of biological macromolecules. The best time resolution in such techniques will only be achieved by measuring the arrival times of individual photons at the detector. A general approach to the estimation of molecular parameters based on individual photon arrival times is presented. The amount of information present in a data set is quantified by the Fisher information, thereby providing a guide to deriving the basic equations relating measurement uncertainties and time resolution. Based on these information-theoretical considerations, a data analysis algorithm is presented that details the optimal analysis of single-molecule data. This method natively accounts and corrects for background photons and cross talk, and can scale to an arbitrary number of channels. By construction, and with corroboration from computer simulations, we show that this algorithm reaches the theoretical limit, extracting the maximal information out of the data. The <b>bias</b> inherent <b>in</b> the <b>algorithm</b> is considered and its implications for experimental design are discussed. The ideas underlying this approach are general and are expected to be applicable to any information-limited measurement...|$|R
40|$|Graphs {{are natural}} {{representations}} {{of problems and}} data in many fields. For example, in computational biology, interaction networks model the functional relationships between genes in living organisms; in the social sciences, graphs are used to represent friendships and business relations among people; in chemoinformatics, graphs represent atoms and molecular bonds. Fields like these are often rich in data, {{to the extent that}} manual analysis is not feasible and machine learning algorithms are necessary to exploit the wealth of available information. Unfortunately, in machine learning research, there is a huge <b>bias</b> <b>in</b> favor of <b>algorithms</b> operating only on continuous vector valued data, algorithms that are not suitable for the combinatorial structure of graphs. In this thesis, we show how to leverage both the expressive power of graphs and the strength of established machine learning tools by introducing methods that combine geometric embeddings of graphs with standard learning algorithms. We demonstrate the generality of this idea by developing embedding algorithms for both simple and weighted graphs and applying them in both supervised and unsupervised learning problems such as classification and clustering. Our results provide both theoretical support for the usefulness of graph embeddings in machine learning and empirical evidence showing that this framework is often more flexible and better performing than competing machine learning algorithms for graphs...|$|R
40|$|Regional {{differences}} in the Sea-viewing Wide Field-of-view Sensor chlorophyll algorithm uncertainty were observed in a large global data set containing coincident in situ measurements of chlorophyll a concentration (Chla) and spectral radiometry. The uncertainty {{was found to be}} systematic when the data were sorted by ocean: Atlantic, Pacific, Southern, and Indian Oceans. Artifacts associated with different instrumentation and analytical methods had been previously ruled out. Given these oceanic <b>biases</b> <b>in</b> the chlorophyll <b>algorithm,</b> we hypothesized that the oceans may be optically different, and their optical differences may be intrinsically related to regional {{differences in}} phytoplankton community structure or biogeochemical processes. The oceanic biases, originally observed using radiometric measurements, were independently verified using total absorption measurements in a subset of the data. Moreover, they were explained through oceanic {{differences in the}} absorption of colored detrital matter (CDM) and phytoplankton. Both effects were considered together in explaining the ocean biases through a stepwise linear regression analysis. Significant oceanic differences in the amount of CDM and in phytoplankton cell sizes and pigmentation would give rise to optical differences, but we raise a concern for the spatial coverage of the data. We do not suggest the application of ocean-based algorithms but rather emphasize the importance of consolidating regional data sets before reaching this conclusion...|$|R
40|$|ABSTRACT Time-resolved single {{molecule}} fluorescence measurements {{may be used}} {{to probe}} the conformational dynamics of biological macromolecules. The best time resolution in such techniques will only be achieved by measuring the arrival times of individual photons at the detector. A general approach to the estimation of molecular parameters based on individual photon arrival times is presented. The amount of information present in a data set is quantified by the Fisher information, thereby providing a guide to deriving the basic equations relating measurement uncertainties and time resolution. Based on these information-theoretical considerations, a data analysis algorithm is presented that details the optimal analysis of single-molecule data. This method natively accounts and corrects for background photons and cross talk, and can scale to an arbitrary number of channels. By construction, and with corroboration from computer simulations, we show that this algorithm reaches the theoretical limit, extracting the maximal information out of the data. The <b>bias</b> inherent <b>in</b> the <b>algorithm</b> is considered and its implications for experimental design are discussed. The ideas underlying this approach are general and are expected to be applicable to any information-limited measurement...|$|R
