34|5|Public
50|$|A {{stratified}} {{analysis is}} one way to either accommodate a lack of <b>bivariate</b> <b>normality,</b> or to isolate the correlation resulting from one factor while controlling for another. If W represents cluster membership or another factor that it is desirable to control, we can stratify the data based on the value of W, then calculate a correlation coefficient within each stratum. The stratum-level estimates can then be combined to estimate the overall correlation while controlling for W.|$|E
50|$|Statistical {{inference}} for Pearson's {{correlation coefficient}} {{is sensitive to}} the data distribution. Exact tests, and asymptotic tests based on the Fisher transformation can be applied if the data are approximately normally distributed, but may be misleading otherwise. In some situations, the bootstrap {{can be applied to}} construct confidence intervals, and permutation tests can be applied to carry out hypothesis tests. These non-parametric approaches may give more meaningful results in some situations where <b>bivariate</b> <b>normality</b> does not hold. However the standard versions of these approaches rely on exchangeability of the data, meaning that there is no ordering or grouping of the data pairs being analyzed that might affect the behavior of the correlation estimate.|$|E
40|$|Based {{upon the}} idea of {{construction}} of data driven smooth tests for composite hypotheses presented in Inglotet al. (1997) and Kallenberg and Ledwina (1997), two versions of data driven smooth test for <b>bivariate</b> <b>normality</b> are proposed. Asymptotic null distributions are derived, and consistency of the newly introduced tests against every bivariate alternative with marginals having finite variances is proved. Included results of power simulations show {{that one of the}} proposed tests performs very well in comparison with other commonly used tests for <b>bivariate</b> <b>normality.</b> Schwarz's BIC criterion tests of <b>bivariate</b> <b>normality</b> goodness-of-fit score test smooth test Neyman's test Monte Carlo simulations...|$|E
40|$|A {{relatively}} simple and convenient score test of <b>normality</b> in the <b>bivariate</b> probit model is derived. Monte Carlo simulations {{show that the}} small sample performance of the bootstrapped test is quite good. The test may be readily extended to testing normality in related models. Score test, <b>bivariate</b> probit, <b>normality,</b> Gram-Charlier series...|$|R
40|$|This paper proposes and conducts direct {{tests of}} the mixture of {{distributions}} model for stock prices. By exploiting the model 2 ̆ 7 s <b>bivariate</b> conditional <b>normality</b> of price changes and trading volume, these restrictions can be tested under very weak assumptions regarding the daily flow of information to the market. As a technical by product, important parameters governing the distribution of this unobservable information flow are estimated...|$|R
40|$|A {{variety of}} estimators for the {{parameters}} of the Generalized Pareto distribution, the approximating distribution for excesses over a high threshold, have been proposed, always assuming the underlying data to be independent. We recently proved that the likelihood moment estimators are consistent estimators for {{the parameters of}} the Generalized Pareto distribution for the case where the underlying data arises from a (stationary) linear process with heavy-tailed innovations. In this paper we derive the <b>bivariate</b> asymptotic <b>normality</b> under some additional assumptions and give an explicit example on how to check these conditions by using asymptotic expansions. Comment: 21 page...|$|R
40|$|AbstractBased {{upon the}} idea of {{construction}} of data driven smooth tests for composite hypotheses presented in Inglotet al. (1997) and Kallenberg and Ledwina (1997), two versions of data driven smooth test for <b>bivariate</b> <b>normality</b> are proposed. Asymptotic null distributions are derived, and consistency of the newly introduced tests against every bivariate alternative with marginals having finite variances is proved. Included results of power simulations show {{that one of the}} proposed tests performs very well in comparison with other commonly used tests for <b>bivariate</b> <b>normality...</b>|$|E
3000|$|To avoid relying {{too heavily}} on the {{assumption}} of <b>bivariate</b> <b>normality</b> for identification by functional form, I incorporate into the analysis a key identifying variable z [...]...|$|E
40|$|We {{construct}} and evaluate LM and Neyman’s C(α) tests based on bivariate Edgeworth expansions for {{the consistency of}} the Heckman’s two-step estimator in selection models, that is, for the marginal normality and linearity of the conditional expectation of the error terms. The proposed tests are robust to local misspecification in nuisance distributional parameters. Monte Carlo results show that instead of testing <b>bivariate</b> <b>normality,</b> testing marginal normality and linearity of the conditional expectations separately have a better size performance. Moreover, the robust variants of the tests have better size and similar power to non-robust tests, which determines that these tests can be successfully applied to detect specific departures from the null model of <b>bivariate</b> <b>normality.</b> We apply the tests procedures to women’s labor supply data...|$|E
40|$|It is {{well known}} that a bivariate {{distribution}} belongs to the domain of attraction of an extreme value distribution G if and only if the marginals belong to the domain of attraction of the univariate marginal extreme value distributions and the dependence function converges to the stable tail dependence function of G. Hall and Welsh (1984,Ann. Statist. 12, 1079 - 1084) and Drees (1997 b,Ann. Statist., to appear) addressed the problem of finding optimal rates of convergence for estimators of the extreme value index of an univariate distribution. The present paper deals with the corresponding problem for the stable tail dependence function. First an upper bound on the rate of convergence for estimators of the stable tail dependence function is established. Then it is shown that this bound is sharp by proving that it is attained by the tail empirical dependence function. Finally, we determine the limit distribution of this estimator if the dependence function satisfies a certain second-order condition. asymptotic <b>normality,</b> <b>bivariate</b> extreme value distribution, domain of attraction, rate of convergence, stable tail dependence function, tail empirical dependence function...|$|R
40|$|Wiraditya Sandi, 2010. The Correlation between Waist Circumference and Waist-to-Hip Ratio with Serum Fasting Glucose Level on Men, Medical Faculty of Sebelas Maret University, Surakarta. Objectives: To {{discern the}} {{correlation}} between waist circumference and waist-to-hip ratio with serum fasting glucose level on men {{as well as their}} possible predictive value. Method: This was an analytic-observational study with cross-sectional approach. Subjects were male population of Desa Ngoresan and Ngemplak Sutan, Jebres, Surakarta with age of 30 - 50 years old. As many as 43 subjects were selected by incidental sampling method based on inclusion and exclusion criteria. The measurement of waist circumference and hip circumference (with 1 mm proximity) as well as blood samples collection were performed on every subject. The waist-to-hip ratios calculations and serum fasting glucose level measurements were performed afterward. The collected data were analyzed by using <b>Normality,</b> <b>Bivariate</b> and Multivariate Tests. Result: Bivariate test showed correlation between waist circumference and waist-to-hip ratio with serum fasting glucose level (r= 0. 522; p< 0. 05 and r= 0. 333, p< 0. 05; respectively). Multivariate test showed a stronger correlation of waist circumference than that of waist-to-hip ratio. Accordingly, it also showed that waist circumference was a better predictor of serum fasting glucose level. As the serum fasting glucose level is one of the metabolic syndrome parameters, it was assumed that waist circumference was a better predictor of metabolic syndrome. Conclusion: Waist circumference and waist-to-hip ratio correlated with serum fasting glucose level. Waist circumference was a better predictor of serum fasting glucose level as well as metabolic syndrome. Keywords: waist circumference, waist-to-hip ratio, fasting glucose level,...|$|R
40|$|This article {{constructs}} and evaluates Lagrange multiplier (LM) and Neyman's C(α) tests {{based on}} bivariate Edgeworth series expansions for {{the consistency of}} the Heckman's two-step estimator in sample selection models, that is, for marginal normality and linearity of the conditional expectation of the error terms. The proposed tests are robust to local misspecification in nuisance distributional parameters. Monte Carlo results show that testing marginal normality and linearity of the conditional expectations separately have a better size performance than testing <b>bivariate</b> <b>normality.</b> Moreover, the robust variants of the tests have better empirical size than nonrobust tests, which determines that these tests can be successfully applied to detect specific departures from the null model of <b>bivariate</b> <b>normality.</b> Finally, the tests are applied to women's labor supply data. Heckman's two-step, LM tests, Neyman's C(α) tests,...|$|E
40|$|Background: Heckman-type {{selection}} {{models have}} been used to control HIV prevalence estimates for selection bias when participation in HIV testing and HIV status are associated after controlling for observed variables. These models typically rely on the strong assumption that the error terms in the participation and the outcome equations that comprise the model are distributed as bivariate normal. - Methods: We introduce a novel approach for relaxing the <b>bivariate</b> <b>normality</b> assumption in selection models using copula functions. We apply this method to estimating HIV prevalence and new confidence intervals (CI) in the 2007 Zambia Demographic and Health Survey (DHS) by using interviewer identity as the selection variable that predicts participation (consent to test) but not the outcome (HIV status). - Results: We show in a simulation study that selection models can generate biased results when the <b>bivariate</b> <b>normality</b> assumption is violated. In the 2007 Zambia DHS, HIV prevalence estimates are similar irrespective {{of the structure of the}} association assumed between participation and outcome. For men, we estimate a population HIV prevalence of 21...|$|E
40|$|The {{classification}} of twin pairs based on zygosity into monozygotic (MZ) or dizygotic (DZ) twins {{is the basis}} of most twin analyses. When zygosity information is unavailable, a normal finite mixture distribution (mixture distribution) model can be used to estimate components of variation for continuous traits. The main assumption of this model is that the observed phenotypes on a twin pair are bivariately normally distributed. Any deviation from normality, in particular kurtosis, could produce biased estimates. Using computer simulations and analyses {{of a wide range of}} phenotypes from the U. K. Twins 2 ̆ 7 Early Developments Study (TEDS), where zygosity is known, properties of the mixture distribution model were assessed. Simulation results showed that, if normality assumptions were satisfied and the sample size was large (e. g., 2, 000 pairs), then the variance component estimates from the mixture distribution model were unbiased and the standard deviation of the difference between heritability estimates from known and unknown zygosity in the range of 0. 02 - 0. 20. Unexpectedly, the estimates of heritability of 10 variables from TEDS using the mixture distribution model were consistently larger than those from the conventional (known zygosity) model. This discrepancy was due to violation of the <b>bivariate</b> <b>normality</b> assumption. A leptokurtic distribution of pair difference was observed for all traits (except non-verbal ability scores of MZ twins), even when the univariate distribution of the trait was close to normality. From an independent sample of Australian twins, the heritability estimates for IQ variables were also larger for the mixture distribution model in six out of eight traits, consistent with the observed kurtosis of pair difference. While the known zygosity model is quite robust to the violation of the <b>bivariate</b> <b>normality</b> assumption, this novel finding of widespread kurtosis of the pair difference may suggest that this assumption for analysis of quantitative trait in twin studies may be incorrect and needs revisiting. A possible explanation of widespread kurtosis within zygosity groups is heterogeneity of variance, which could be caused by genetic or environmental factors. For the mixture distribution model, violation of the <b>bivariate</b> <b>normality</b> assumption will produce biased estimates...|$|E
40|$|In {{this thesis}} we introduce, {{implement}} and compare several multivariate goodness-of-fit tests. First of all, we {{will focus on}} universal mul- tivariate tests that do not place any assumptions on parametric families of null distributions. Thereafter, we will be concerned with testing of multi- variate normality and, by using Monte Carlo simulations, we will compare power of five different tests of <b>bivariate</b> <b>normality</b> against several alternati- ves. Then we describe multivariate skew-normal distribution and propose a new test of multivariate skew-normality based on empirical moment genera- ting functions. In the final analysis, we compare its power with other tests of multivariate skew-normality. ...|$|E
40|$|Abstract: In {{this study}} a Shewhart type control chart namely chart is {{proposed}} for improved monitoring of process variability (targeting moderate and large shifts which is major concern of Shewhart type control charts) of a quality characteristic of interest Y. The proposed control chart is {{an improvement over}} conventional chart to monitor process variance using a single auxiliary variable X. Assuming <b>bivariate</b> <b>normality</b> of (Y, X), design structure of chart is developed and its comparison is made with the well-known Shewhart control chart namely chart. Using power curves as a performance measure {{it is observed that}} chart outperforms the chart under certain conditions on z...|$|E
40|$|Using {{data from}} the National Longitudinal Survey of Youth (NLSY) we {{introduce}} and estimate various Bayesian hierarchical models that investigate the nature of unobserved heterogeneity in returns to schooling. We consider a variety of possible forms for the heterogeneity, some motivated by previous theoretical and empirical work and some new ones, and let the data decide among the competing specifications. Empirical results indicate that heterogeneity is present in returns to education. Furthermore, we find strong evidence that the heterogeneity follows a continuous rather than a discrete distribution, and that <b>bivariate</b> <b>normality</b> provides a very reasonable description of individual-level heterogeneity in intercepts and returns to schooling...|$|E
40|$|Assuming <b>bivariate</b> <b>normality</b> with {{correlation}} r, dichotomizing one variable at {{the mean}} {{results in the}} reduction in variance accounted for to. 647 r²; and dichotomizing both at the mean, to. 405 r². These losses, in turn, result in reduction in statistical power equivalent to discarding 38 % and 60 % of the cases under representative conditions. As dichotomization departs from the mean, the costs in variance accounted for and in power are even larger. Consequences of this practice in measurement applications are considered. These losses may not be quite so large in real data, but since methods are available for making use of all the original scaling information, {{there is no reason}} to sustain them...|$|E
40|$|Abstract. Structural {{equation}} modeling (SEM) with ordinal indicators rely on {{an assumption}} of categorized normality. This assumption may {{be tested for}} pairs of variables using the likelihood ratio G 2 or Pearson’s X 2 statistics. For increased computational efficiency, SEM programs usually estimate polychoric correlations in two stages. However, two-stage polychoric estimates are not asymptotically efficient and G 2 and X 2 need not be asymptotically chi-square when the estimator is not efficient. Recently, Maydeu-Olivares and Joe (2005) have introduced a new statistic, Mn, that is asymptotically chi-square even for estimators that are not efficient. We investigate the behavior of G 2, X 2, and Mn when testing underlying <b>bivariate</b> <b>normality</b> with polychoric correlations estimated in two stages...|$|E
40|$|In this study, a Shewhart-type {{control chart}} is {{proposed}} for the improved monitoring of process mean level (targeting both moderate and large shifts which is the major concern of Shewhart-type control charts) of a quality characteristic of interest Y. The proposed control chart, namely the M-r chart, {{is based on the}} regression estimator of mean using a single auxiliary variable X. Assuming <b>bivariate</b> <b>normality</b> of (Y, X), the design structure of M-r chart is developed for phase I quality control. The comparison of the proposed chart is made with some existing control charts used for the same purpose. Using power curves as a performance measure, better performance of the proposed M-r chart is observed for detecting the shifts in mean level of the characteristic of interest...|$|E
40|$|The {{power of}} {{statistical}} tests based on four popular product-moment correlation coefficients was examined when relatively small samples (10 &le; N &le; 100) {{are drawn from}} bivariate populations of several different distributional shapes. Analytical procedures for deter-mining theoretical power under conditions of <b>bivariate</b> <b>normality</b> are presented for the Pearson (rP), Spearman (rs), point-biserial (rpb), and phi (rfp) coefficients. A monte carlo study supported previous conclusions that t {{as a test of}} H 0 : p = 0, with rP estimating p, is robust over a wide range of non-normality; however, frequent use of rs leads to greater power under identical distri-butional assumption violations. The proportion of power due to Type III errors was also specified both analytically and empirically, and revealed the relativ...|$|E
30|$|As a first step, {{we provide}} maximum {{likelihood}} estimates with no exclusion restrictions. As explained in Cameron and Trivedi (2009, p. 543), among others, in this case, we exploit the exogenous variation {{due to the}} high non-linearity of the selection equation as a determinant of labor force participation. This is not without cost. The maximum likelihood estimates {{are based on a}} <b>bivariate</b> <b>normality</b> assumption. This is the reason why a two-step procedure, based on a univariate normality assumption is considered to be more reliable. Nonetheless, the inverse Mills ratio might be highly collinear with the independent variables of the main equation if the set of variables in the selection equation are exactly the same. This suggests that it is appropriate to use instruments to identify the selection equation.|$|E
40|$|Edgeworth {{expansions}} {{are known}} to be useful for approximating probability distributions and moments. In our case, we exploit the expansion in the context of models of double selection embedded in a trivariate normal structure. We assume <b>bivariate</b> <b>normality</b> among the random disturbance terms in the two selection equations but allow the distribution of the disturbance term in the outcome equation to be free. This sets the stage for a control function approach to correction of selectivity bias that affords tests for the more common trivariate normality specifi-cation. Other recently proposed methods for handling multiple outcomes are Multinomial Logit based selection correction models. An empirical example is presented to document the differ-ences among the results obtained from our selectivity correction approach, trivariate normality specification and Multinomial Logit based selection correction models...|$|E
40|$|In {{this article}} we {{introduce}} a test for the normality assumption in the sample selection model. The test {{is based on a}} flexible parametric specification of the density function of the error terms in the model. This specification follows a Hermite series with <b>bivariate</b> <b>normality</b> as a special case. All parameters of the model are estimated both under normality and under the more general flexible parametric specification, which enables testing for normality using a standard likelihood ratio test. If normality is rejected, then the flexible parametric specification provides consistent parameter estimates. The test has reasonable power, as is shown by a simulation study. The test also detects some types of ignored heteroscedasticity. Finally, we apply the flexible specification of the density to a travel demand model and test for normality in this model...|$|E
40|$|We {{consider}} the efficient estimation of a bivariate distribution function (DF) under {{the class of}} radially symmetric distributions and propose an estimator based on {{the mean of the}} empirical distribution and survival functions. We obtain the mean and variance of the estimator and show that it has an asymptotic normal distribution. We also show that the nonparametric maximum likelihood estimator of the bivariate DF coincides with the new estimator under radial symmetry. We study the asymptotic relative efficiency of this estimator and show that it results in a minimum of 50 % reduction in sample size over the empirical DF at any point (x,y) in. A bootstrap procedure to test whether the data support a radially symmetric model is examined. A simulation study compares the size and power of this test under <b>bivariate</b> <b>normality,</b> against alternatives in the Plackett's family of bivariate distributions, to two other procedures based on Kolmogorov-Smirnov distance. Distribution function Nonparametric MLE Radial symmetry Bootstrap Bivariate Kolmogorov-Smirnov test Copula...|$|E
40|$|Specific {{magnitudes}} of crosswinds {{may exist}} {{that could be}} constraints {{to the success of}} an aircraft mission such as the landing of the proposed space shuttle. A method is required to determine the orientation or azimuth of the proposed runway which will minimize the probability of certain critical crosswinds. Two procedures for obtaining the optimum runway orientation relative to minimizing a specified crosswind speed are described and illustrated with examples. The empirical procedure requires only hand calculations on an ordinary wind rose. The theoretical method utilizes wind statistics computed after the bivariate normal elliptical distribution is applied to a data sample of component winds. This method requires only the assumption that the wind components are bivariate normally distributed. This assumption seems to be reasonable. Studies are currently in progress for testing wind components for <b>bivariate</b> <b>normality</b> for various stations. The close agreement between the theoretical and empirical results for the example chosen substantiates the bivariate normal assumption...|$|E
40|$|This study {{examined}} {{bias in the}} sample correlation coefficient, r, and its correction by unbiased estimators. Computer simulations revealed that the expected value of correlation coefficients in samples from a normal population is slightly less than the population correlation, ρ, and that the bias is almost eliminated by an estimator suggested by R. A. Fisher and is more completely eliminated by a related estimator recommended by Olkin and Pratt. Transformation of initial scores to ranks and calculation of the Spearman rank correlation, rS, produces somewhat greater bias. Type I error probabilities of significance tests of zero correlation based on the Student t statistic and exact tests based on critical values of rS obtained from permutations remain fairly close to the significance level for normal and several non-normal distributions. However, significance tests of non-zero values of correlation based on the r to Z transformation are grossly distorted for distributions that violate <b>bivariate</b> <b>normality.</b> Also, significance tests of non-zero values of rS based on the r to Z transformation are distorted eve...|$|E
40|$|Pension {{plans and}} life {{insurances}} offering minimum performance guarantees are very com-mon worldwide. In some markets, {{for example the}} brazilian case, besides the minimum guar-anteed rate, the costumers of some defined contribution plans {{have the right to}} receive, over their savings, the positive difference between the return of a specified investment fund, usually a fixed income fund, and the minimum guaranteed rate, commonly defined as the composition of a fixed interest rate and a floating inflation rate. This instrument can be characterized as an option to exchange one asset, the minimum guaranteed rate, for another, the return of the specified investment fund. In this paper, we provide a closed formula to evaluate this liabil-ity that depends on two stochastic rates assuming <b>bivariate</b> <b>normality.</b> We also provide some examples assuming others copulas functions, using Monte Carlo simulation, and compare the effects of the copula and marginals specification in the price of the option. The model makes use of a one-factor Vasicek framework for the term structures of interest rate and inflation rate...|$|E
40|$|Bayesian {{modelling}} for cost-effectiveness {{data has}} received much attention {{in both the}} health economics and the statistical literature in recent years. Cost-effectiveness data are characterised by a relatively complex structure of relationships linking the suitable measure of clinical benefit (QALYs) and the associated costs. Simplifying assumptions, such as (<b>bivariate)</b> <b>normality</b> of the underlying distributions are usually not granted, particularly for the cost variable, which is characterised by markedly skewed distributions. In addition, individual-level datasets are often characterised {{by the presence of}} structural zeros in the cost variable. Hurdle models can be used to account for the presence of excess zeros in a distribution and have been applied in the context of cost data. We extend their application to cost-effectiveness data, defining a full Bayesian model which consists of a selection model for the subjects with null costs, a marginal model for the costs and a conditional model for the measure of effectiveness (conditionally on the observed costs). The model is presented using a working example to describe its main features. Comment: 15 pages, 2 figure...|$|E
40|$|Heterogeneous {{phenotypic}} correlations may be {{suggestive of}} underlying changes in genetic covariance among life-history, morphology, and behavioural traits, and their detection is therefore relevant to many biological studies. Two new statistical tests are proposed and their performances compared with existing methods. Of all tests considered, the existing approximate test of homogeneity of product-moment correlations provides the greatest power to detect heterogeneous correlations, when based on Hotelling’s z-transformation. The {{use of this}} transformation and test is recommended under conditions of <b>bivariate</b> <b>normality.</b> A new distribution-free randomisation test of homogeneity of Spearman’s rank correlations is described and recommended for use when the bivariate samples are taken from populations with non-normal or unknown distributions. An alternative randomisation test of homogeneity of product-moment correlations is {{shown to be a}} useful compromise between the approximate tests and the randomisation tests on Spearman’s rank correlations: it is not as sensitive to departures from normality as the approximate tests, but has greater power than the rank correlation test. An example is provided that shows how choice of test will have a considerable influence on the conclusions of a particular study...|$|E
40|$|In many {{statistical}} {{studies the}} relationship between two random variables X and Y is investigated {{and in particular the}} question whether X and Y are independent and normally distributed is of interest. Smooth tests may be used for testing this. They consist of several components, the first measuring linear (in) dependence, the next ones correlations of higher powers of X and Y. Since alternatives are not restricted to <b>bivariate</b> <b>normality,</b> not only linear (in) dependence is of interest and therefore all components come in. Moreover, normality itself is also tested by the smooth test. It is well-known that choosing the number of components in a smooth test is an important issue. Recently, data driven methods are developed for doing this. The resulting new test statistics for testing independence and normality are introduce in this paper. For a very large class of alternatives, including also independent X and Y with nonnormal marginals, consistency is proved. Monte Carlo results show that the data driven smooth test behaves very well for finite sample sizes...|$|E
40|$|The polychoric {{correlation}} coefficient {{is a measure}} of association between two ordinal variables. It {{is based on the assumption}} that two latent bivariate normally distributed random variables generate couples of ordinal scores. Categories of the two ordinal variables correspond to intervals of the corresponding continuous variables. Thus, measuring the association between ordinal variables means estimating the product moment correlation between the underlying normal variables (Olsonn, 1979). When the hypothesis of la- tent <b>bivariate</b> <b>normality</b> is empirically or theoretically implausible, other dis- tributional assumptions can be made. In this paper a new and more °exible {{polychoric correlation}} coe±cient is proposed assuming that the underlying variables are skew-normally distributed (Roscino, 2005). The skew normal (Azzalini and Dalla Valle, 1996) is a family of distributions which includes the normal distribution as a special case, but with an extra parameter to reg- ulate the skewness. As for the original polychoric correlation coe±cient, the new coe±cient was estimated by the maximization of the log-likelihood func- tion with respect to the thresholds of the continuous variables, the skewness and the correlation parameters. The new coe±cient was then tested on sam- ples from simulated populations di®ering in the number of ordinal categories and the distribution of the underlying variables. The results were compared with those of the original polychoric correlation coe±cient...|$|E
40|$|Methods are {{developed}} that use aggregate data, possibly {{based on a}} large num-ber of individuals, and individual level data, from {{a small fraction of}} individuals from the same or similar population, to eliminate ecological bias inherent in the analysis of aggregate data. The primary focus is on estimating the individual level correlation coefficient but the proposed methodology can be extended to estimate regression coefficients. Two approaches, the method of moments and the maxi-mum likelihood, {{are developed}} for a bivariate distribution, but can be extended to a multivariate distribution. The method of moments develops a corrected esti-mate of the within-group covariance matrix, which is then used to estimate the individual level correlation and regression coefficients. The second method assumes <b>bivariate</b> <b>normality</b> and maximizes the combined likelihood function based on the two data sets. The maximum likelihood estimates are obtained using the EM-algorithm. A simulation study investigates the repeated sampling prop-erties of these procedures in terms of bias and the mean square error of the point estimates and the actual coverage of the confidence intervals. The maximum like-lihood estimates are almost unbiased and the confidence intervals are well cali-brated for simulation conditions considered. The method of moments estimates have the same desirable properties for some simulation conditions. Under all con-ditions, the correlation coefficient between aggregate variables is severely biased as an estimate of the individual level correlation coefficient...|$|E
40|$|The {{most popular}} method {{for trying to}} detect an {{association}} between two random variables is to test H 0 : ρ= 0, the hypothesis that Pearson's correlation is equal to zero. It is well known, however, that Pearson's correlation is not robust, roughly meaning that small changes in any distribution, including any bivariate normal distribution as a special case, can alter its value. Moreover, the usual estimate of ρ, r, is sensitive to only a few outliers which can mask a true association. A simple alternative to testing H 0 : ρ = 0 is {{to switch to a}} measure of association that guards against outliers among the marginal distributions such as Kendall's tau, Spearman's rho, a Winsorized correlation, or a so-called percentage bend correlation. But it is known that these methods fail {{to take into account the}} overall structure of the data. Many measures of association that do take into account the overall structure of the data have been proposed, but it seems that nothing is known about how they might be used to detect dependence. One such measure of association is selected, which is designed so that under <b>bivariate</b> <b>normality,</b> its estimator gives a reasonably accurate estimate of ρ. Then methods for testing the hypothesis of a zero correlation are studied. Skipped correlation coefficient, inferences, random variables, Pearson's correlation,...|$|E
40|$|We {{address the}} problem of {{determining}} the therapeutic window of a drug by finding its minimum effective and maximum safe doses (MINED and MAXSD). The MINED is the lowest dose that exceeds the mean efficacy of the zero dose by a specified threshold, and the MAXSD is the highest dose that does not exceed the mean toxicity of the zero dose by a specified threshold. Step-down multiple test procedures are proposed to identify the MINED and MAXSD assuming a bivariate normal model. These procedures control the type I familywise error probability of declaring any ineffective dose as effective or any unsafe dose as safe at a prespecified level. A new multivariate t-distribution is introduced whose critical points are required to implement the exact normal theory procedures. Because these critical points depend on the unknown correlation coefficient between the efficacy and safety variables, the Bonferroni method is proposed as an alternative, which amounts to separately testing for efficacy and safety, each at type I familywise error rate of / 2. The bootstrap versions of the exact normal theory procedures provide an approximate way to jointly test for efficacy and safety without the knowledge of the correlation coefficient, as well as to relax the <b>bivariate</b> <b>normality</b> assumption. The Bonferroni and bootstrap procedures are compared in a simulation study. It is shown that significant power gains are achieved by jointly testing for both efficacy and safety using bootstrap procedures. Coded data from an arthritis drug trial are analyzed to illustrate the procedures...|$|E
40|$|The ECMWF {{temperature}} and precipitation ensemble reforecasts are evaluated for biases in the mean, spread and forecast probabilities, {{and how these}} biases propagate to streamflow ensemble forecasts. The forcing ensembles are subsequently post-processed to reduce bias and increase skill, and to investigate whether this leads to improved streamflow ensemble forecasts. Multiple post-processing techniques are used: quantile-to-quantile transform, linear regression with an assumption of <b>bivariate</b> <b>normality</b> and logistic regression. Both the raw and post-processed ensembles are run through a hydrologic model of the river Rhine to create streamflow ensembles. The results are compared using multiple verification metrics and skill scores: relative mean error, Brier skill score and its decompositions, mean continuous ranked probability skill score and its decomposition, and the ROC score. Verification of the streamflow ensembles is performed at multiple spatial scales: relatively small headwater basins, large tributaries and the Rhine outlet at Lobith. The streamflow ensembles are verified against simulated streamflow, in order to isolate the effects of biases in the forcing ensembles and any improvements therein. The {{results indicate that the}} forcing ensembles contain significant biases, and that these cascade to the streamflow ensembles. Some of the bias in the forcing ensembles is unconditional in nature; this was resolved by a simple quantile-to-quantile transform. Improvements in conditional bias and skill of the forcing ensembles vary with forecast lead time, amount, and spatial scale, but are generally moderate. The translation to streamflow forecast skill is further muted, and several explanations are considered, including limitations in the modelling of the space–time covariability of the forcing ensembles and the presence of storages...|$|E
