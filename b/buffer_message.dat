2|176|Public
40|$|A {{method for}} {{communicating}} data between peripheral devices and an embedded processor that includes receiving, at a data buffer {{unit of the}} embedded processor, the data from a peripheral device. The method also includes copying data from the data buffer unit into the bridge buffer of the embedded processor as a bridge <b>buffer</b> <b>message.</b> Additionally, the method includes creating, after storing the data as a bridge <b>buffer</b> <b>message,</b> a peripheral device message comprising the bridge <b>buffer</b> <b>message,</b> and sending the peripheral device message to a thread message queue of a subscriber...|$|E
40|$|BM {{compression}} is {{a straightforward}} and operable {{way to reduce}} <b>buffer</b> <b>message</b> length {{as well as to}} improve system performance. In this paper, we thoroughly discuss the principles and protocol progress of different compression schemes, {{and for the first time}} present an original compression scheme which can nearly remove all redundant information from <b>buffer</b> <b>message.</b> Theoretical limit of compression rates are deduced in the theory of information. Through the analysis of information content and simulation with our measured BM trace of UUSee, the validity and superiority of our compression scheme are validated in term of compression ratio. Comment: 13 pages, 12 figure...|$|E
3000|$|... from (14) {{and thus}} {{validate}} the revocation message. If the revocation message is correctly verified, the receiver discards the <b>buffered</b> <b>messages</b> from [...]...|$|R
30|$|The “:=” {{operator}} both declares and initializes a variable. The arrow is {{the operator}} used both for sending (as an infix operator) and receiving (as a prefix operator). The close operation closes a channel so a receiver can {{know that there}} is no further data going through it (once all <b>buffered</b> <b>messages</b> have been consumed).|$|R
30|$|In [11], {{the authors}} apply the {{epidemic}} algorithm concept of [12] to partially connected ad-hoc networks. Nodes <b>buffer</b> <b>messages</b> they receive {{even if they}} do not know a route to the destination. When two nodes come into contact, they exchange messages. In this way, a message is delivered to every contacting node and finally delivered to its destination.|$|R
50|$|The CANpie API {{supports}} {{the concept of}} hardware <b>message</b> <b>buffers</b> (mailboxes) with a total limit of 255 <b>buffers.</b> A <b>message</b> <b>buffer</b> has a unique direction (receive or transmit). As an option {{it is possible to}} connect a FIFO with arbitrary size to a <b>message</b> <b>buffer</b> for both transfer directions. The total number of CAN channels is limited to 255, the API provides a method to gather information about the features of each CAN hardware channel. This is especially important for an application designer who wants to write the code only once. The CAN frame time-stamping (specified by CiA 603, CAN Frame time-stamping - Requirements for network time management) is supported with a resolution of 1 nano-second.|$|R
50|$|This example {{shows us}} a thread safe <b>buffer</b> and <b>message</b> queue {{with the basic}} {{operations}} put and get.|$|R
40|$|International audienceThis paper {{focuses on}} the {{transfer}} of large data in SMP systems. Achieving good performance for intranode communication is critical for developing an efficient communication system, especially {{in the context of}} SMP clusters. We evaluate the performance of five transfer mechanisms: sharedmemory <b>buffers,</b> <b>message</b> queues, the Ptrace system call, kernel module-based copy, and a high-speed network. We evaluate each mechanism based on latency, bandwidth, its impact on application cache usage, and its suitability to support MPI twosided and one-sided messages...|$|R
40|$|SPIN is {{a general}} {{verification}} tool for proving correctness properties of distributed or concurrent systems. The systems can interact through shared memory, through ren-dezvous operations, or through <b>buffered</b> <b>message</b> exchanges. The coordination problems that these interactions may create can effectively be debugged with the SPIN system. Once a correct design of the system has been obtained, a rigorous proof of its correctness can be provided. SPIN {{is based on the}} paradigm of on-the-fly model checking. The first version of SPI...|$|R
30|$|In the Clive’s Go compiler, {{both the}} sender and the {{receiver}} may close a channel, to stop I/O once the <b>buffered</b> <b>messages</b> (if any) have been processed. Also, an error indication {{may be given}} to close, unlike in standard Go. Such error can be retrieved by calling a new cerror primitive. Furthermore, the send operation has been modified to let the sender check whether the send could proceed or not (e.g., when the channel was closed). Unlike in standard Go, closing a closed channel is a no-operation.|$|R
40|$|It is {{possible}} to design and run real time software, driven by real time buffer-free asynchronous messaging without using message queues, with no need to use schedulers. “Real time buffer-free asynchronous messaging, ” sounds like a contradiction of terms. I shall point out here why it {{is possible}} using the messaging technology of TICC TM *. There is a common assumption in DE and process network (PN) systems [1, 2] that asynchronous messaging should necessarily use <b>buffered</b> <b>message</b> queues. This is consistent with th...|$|R
30|$|Furthermore, we have {{performed}} advanced real-world {{evaluation of the}} developed solution, and compared the results obtained with those of a simple theoretical framework that {{has been shown to}} be able to predict the system's performance under some simplifying assumptions. Specifically, the model is able to predict the shape of the delivery delay pdf, although it has some bias towards the lower delays, explained by some optimistic assumptions, namely the one that bandwidth is unlimited and all <b>buffered</b> <b>messages</b> can be delivered in a single contact. Seeing as the model fits reality, it may be used to predict the solution's performance under different operating conditions, so long as the mobility patterns share some similarity.|$|R
40|$|Abstract — This paper {{focuses on}} the {{transfer}} of large data in SMP systems. Achieving good performance for intranode communication is critical for developing an efficient communication system, especially {{in the context of}} SMP clusters. We evaluate the performance of five transfer mechanisms: sharedmemory <b>buffers,</b> <b>message</b> queues, the Ptrace system call, kernel module-based copy, and a high-speed network. We evaluate each mechanism based on latency, bandwidth, its impact on application cache usage, and its suitability to support MPI twosided and one-sided messages. I. MOTIVATION AND SCOPE Designing a communication system tailored for a particular architecture requires understanding the achievable performance levels of the underlying hardware and software. Such understanding is key to a more efficient design an...|$|R
40|$|Reliable {{multicast}} protocols provide all-or-none {{delivery to}} participants. Traditionally, such protocols suffer from large buffering requirements, as receivers have to <b>buffer</b> <b>messages,</b> and <b>buffer</b> sizes grow {{with the number}} of participants. In this paper, we describe an optimization that allows such protocols {{to reduce the amount of}} buffering drastically at the cost of a very small probability that all-or-none delivery is violated. We analyze this probability, and simulate an optimized version of an epidemic multicast protocol to validate the effectiveness of the optimization. We find that the buffering requirements are sub-constant, that is, the requirements shrink with group size, while the probability of all-or-none violation can be set to very small values...|$|R
40|$|Abstract — In Delay Tolerant Networks (DTNs), {{the optimal}} use of buffer {{management}} polices {{can improve the}} network throughput. In this paper, we propose a buffer management strategy called as Message Drop Control (MDC). This technique controls the message drop by using an upper bound which is the count of <b>buffered</b> <b>message</b> at router {{and size of the}} messages. If count exceed the upper bound drop will not occurs else large size message will be dropped from queue. We examine the performance of proposed drop policy by comparing it with existing MOFO, DOA, and LIFO. The simulation results proves that MDC policy out perform well as existing ones in terms of delivery probability, message drop, overhead and buffer time average...|$|R
40|$|In various situations, mobile {{agents at}} {{different}} hosts must cooperate {{with one another}} by sharing information and making decisions collectively. To ensure effective interagent communication, communication protocols must track target agent locations and deliver messages reliably. Researchers have proposed {{a wide range of}} schemes for agent tracking and reliable message delivery. However, each scheme has its own assumptions, design goals, and methodology. As a result, no uniform or structured methods exist for characterizing current protocols, making it difficult to evaluate their relative effectiveness and performance. The authors propose a mailbox-based scheme for designing mobile agent communication protocols. This scheme assigns each agent a mailbox to <b>buffer</b> <b>messages,</b> but decouples the agent and mailbox to let them reside at different hosts and migrate separately. Department of Computin...|$|R
40|$|This article {{presents}} {{an overview of}} PUMA (Performance-oriented, User-managed Messaging Architecture), a message-passing kernel for massively parallel systems. Message passing in PUMA is based on portals – {{an opening in the}} address space of an application process. Once an application process has established a portal, other processes can write values into the portal using a simple send operation. Because messages are written directly into the address space of the receiving process, {{there is no need to}} <b>buffer</b> <b>messages</b> in the PUMA kernel and later copy them into the applications address space. PUMA consists of two components: the quintessential kernel (Q-Kernel) and the process control thread (PCT). Although the PCT provides management decisions, the Q-Kernel controls access and implements the policies specified by the PCT...|$|R
50|$|SMS-COMMAND {{may be used}} {{to query}} for a <b>message</b> <b>buffered</b> in the SMSC, to modify its {{parameters}} or to delete it.|$|R
50|$|While a {{standard}} library package featuring {{most of the}} classical concurrency control structures (mutex locks, etc.) is available, idiomatic concurrent programs instead prefer channels, which provide send messages between goroutines. Optional <b>buffers</b> store <b>messages</b> in FIFO order and allow sending goroutines to proceed before their messages are received.|$|R
30|$|The MTD Service is {{responsible}} for listening to disconnected-MN messages produced by Gateways and, thereafter, to collect all messages {{that could not be}} delivered to the mobile node during its HO or offline period. However, as soon as the node is connected to a new GW, which will also be announced by the corresponding GW, the MTD Service will resend all the <b>buffered</b> <b>messages</b> through the DDS domain to deliver them to the node through the new GW. Because not all applications require such reliable delivery, the MTD service is optional in SDDL, and of course, the buffering capacity of MTS is limited by the amount of memory allocated to it at deployment time. Thus far, we have not implemented any specific garbage-collection algorithm for minimising message loss due to buffer overflow.|$|R
40|$|Abstract. Analysis of {{worst-case}} message transmission {{times in}} CAN networks is usually performed assuming {{the availability of}} an infinite length priority queue of <b>message</b> <b>buffers</b> at the network adapter with zero access time. In reality, adapters provide {{a finite number of}} <b>buffers</b> for <b>message</b> transmission. This paper shows how to account for the availability of a limited number of buffers at the adapter and how to model the impossibility of performing preemption once a message has been copied into the adapter buffer and it is awaiting transmission. A new worst-case bound for message transmission is provided and evaluated with respect to the SAE benchmark. 1...|$|R
5000|$|A <b>buffer</b> {{from which}} <b>messages</b> can be 'put' and 'got'. See Actor model and process calculi for {{discussion}} {{on the use of}} channels.|$|R
40|$|Many {{distributed}} applications {{can be understood}} in terms of components interacting in an open environment. This interaction is not always uniform as the network may consist of subnets with different quality: Some components are tightly connected with order preservation of communicated messages, whereas others are more loosely connected such that overtaking of messages and even message loss may occur. Furthermore, certain components may communicate over wireless networks, where sending and receiving must be synchronized, since the wireless medium cannot <b>buffer</b> <b>messages.</b> This paper proposes a formal framework for such systems, which allows high-level modeling and formal analysis of distributed systems where interaction is managed by a variety of nets, including wireless ones. We introduce a simple modeling language for objectoriented components, extending the Creol language. An operational semantics for the language is defined in rewriting logic, which directly provides an executable implementation in Maude...|$|R
40|$|As {{high-performance}} computing increases in popularity and performance, {{the demand for}} similarly capable input and output systems rises. Parallel I/O takes advantage of many data server machines to provide linearly scaling performance to parallel applications that access storage over the system area network. The demands placed on the network by a parallel storage system are considerably different than those imposed by message-passing algorithms or datacenter operations; and, there are many popular and varied networks in use in modern parallel machines. These considerations lead us to develop a network abstraction layer for parallel I/O which is efficient and thread-safe, provides operations specifically required for I/O processing, and supports multiple networks. The <b>Buffered</b> <b>Message</b> Interface (BMI) has low processor overhead, minimal impact on latency, and can improve throughput for parallel file system workloads {{by as much as}} 40 % compared to other more generic network abstractions...|$|R
40|$|Delay Tolerant Networking (DTN) {{considers}} how {{to provide}} communication in contexts where {{it is unreasonable to}} assume end-to-end connectivity. Network devices exchange <b>buffered</b> <b>messages</b> when they come into communication range; <b>messages</b> may be <b>buffered</b> and carried physically several times before ultimately being received. Service characteristics of a DTN depend intimately on the underlying movement of devices through physical space; correspondingly, an assessment of DTN technology (e. g. routing protocols, message exchange policies, etc.) depends on that same movement. Existing mobility models provided in simulators lack characteristics one expects in post-disaster communication. We propose a mobility model that includes the impact of the disaster on the transportation network, and that models population and relief vehicle movement. We augment the “Opportunistic Network Environment ” (ONE) simulator of DTNs with required extensions and show that characteristics of the DTN are very different using the new model than it is under models that ONE currently provides. ...|$|R
40|$|We {{develop an}} {{availability}} solution, called SafetyNet, {{that uses a}} unified, lightweight checkpoint/recovery mechanism to support multiple long-latency fault detection schemes. At an abstract level, SafetyNet logically maintains multiple, globally consistent checkpoints {{of the state of}} a shared memory multiprocessor (i. e., processors, memory, and coherence permissions), and it recovers to a pre-fault checkpoint of the system and re-executes if a fault is detected. SafetyNet efficiently coordinates checkpoints across the system in logical time and uses “logically atomic ” coherence transactions to free checkpoints of transient coherence state. SafetyNet minimizes performance overhead by pipelining checkpoint validation with subsequent parallel execution. We illustrate SafetyNet avoiding system crashes due to either dropped coherence messages or the loss of an interconnection network switch (and its <b>buffered</b> <b>messages).</b> Using full-system simulation of a 16 -way multiprocessor running commercial workloads, we find that SafetyNet (a) adds statistically insignificant runtime overhead in the common-case of fault-free execution, and (b) avoids a crash when tolerated faults occur. ...|$|R
40|$|There {{exists a}} unique minimal {{generalisation}} of a UML sequence diagram (SD) that is race free, {{known as the}} inherent causal scenario. However, practitioners sometimes regard this solution as invalid since it is a purely mathematical construct that apparently does not describe a concrete software engineering solution for resolving race conditions. Practitioners often implement SDs with random access input <b>buffers.</b> <b>Messages</b> are then consumed correctly {{regardless of the order}} or time at which they arrive, which appears to avoid race conditions altogether. However, this approach changes the observable system behaviour from that specified. We refer to this approach as the lazy buffer realization of a SD. We introduce an operational semantics for the lazy buffer realization. We prove the inherent causal scenario global behaviour is bisimulation equivalent to the global behaviour of lazy buffer semantics. Hence, in this sense, the practitioners solution is theoretically the best possible. Also this proves that the inherent causal scenario does represent a ‘real-world’ software solution...|$|R
40|$|The ideal {{tool for}} making {{underwater}} applications viable is an Underwater Acoustic Network (UAN). Unique characteristics, such as long propagation delay of the acoustic signal and extreme volatility in the channel quality, make UANs different from terrestrial radio networks. As a result contemporary networking solutions developed for terrestrial radio networks {{are not applicable}} to UANs. In this dissertation we focus {{on one of the}} fundamental aspects of any communication network, efficient delivery of messages from their source to their destination. The first step in this regard is the development of an efficient routing protocol. Existing routing protocols designed for UANs impose some sort of restriction on the network settings making the application domain limited. Terrestrial ad hoc routing protocols are more generic but suffer from high communication overhead under water. To deal with these issues, we propose LOARP, a low overhead routing protocol suitable for UANs having arbitrary network settings. Although simulation results show that LOARP performs better than existing routing protocols, the overall performance is still poor. Further analysis shows us that the absence of a reliable MAC protocol is the cause behind this poor performance. Secondly, we study the effects of propagation delay on the reliability of MAC. Analysis shows that underwater MAC protocols have various limitations and terrestrial MAC protocols are either unreliable or inadequate. This prompted us to explore Delay Tolerant Networking which is based on the store-carry-forward paradigm. The investigation led us to remove the carry part resulting in the design of LOARP 2 which employs the store-and-forward concept to aid the MAC protocol. Results show that LOARP 2 significantly improves the efficiency and reliability of message delivery but at the cost of high latency. Analysis demonstrates that LOARP 2 <b>buffers</b> <b>messages</b> without considering network surroundings which increases their waiting delay causing the latency to increase. To perform message buffering more intelligently, in the final step, we propose Adaptive LOARP 2, which <b>buffers</b> <b>messages</b> based on channel busyness and only for as long as necessary. Simulation results show that Adaptive LOARP 2 achieves the same level of efficiency and reliability as LOARP 2 but with much lower and tolerable end-to-end latency...|$|R
40|$|This paper {{describes}} {{the recognition of}} implicit serialization due to coarse-grain, synchronous communication and demonstrates the conversion to asynchronous communication for the exchange of boundary condition information in the Thin-Layer Navier Stokes 3 -Dimensional Multi Block (TLNS 3 DMB) code. The implementation details of using asynchronous communication is provided including <b>buffer</b> allocation, <b>message</b> identification, and barrier control. The IBM SP 2 {{was used for the}} tests presented...|$|R
40|$|Most of {{the efforts}} to {{characterize}} DTN routing {{are focused on the}} trade-off between delivery ratio and delay. Buffer occupancy is usually not considered a problem and most of the related work assumes infinite buffers. In the present work, we focus on the drop ratio for message forwarding considering finite <b>buffers.</b> We model <b>message</b> drops with a continuous time Markov chain (CTMC). To the best of our knowledge, there is no previous work with such approach. We focus on the worst case with 1 -packet <b>buffers</b> for <b>message</b> forwarding in homogeneous inter-contact times (ICT) and 2 -class heterogeneous ICT. Our main contribution is to link the encounter rate(s) with the drop ratio. We show that the modeled drop ratio fits simulation results obtained with synthetic traces for both cases...|$|R
50|$|The {{program can}} also be used to detect probes or attacks, including, but not limited to, {{operating}} system fingerprinting attempts, semantic URL attacks, <b>buffer</b> overflows, server <b>message</b> block probes, and stealth port scans.|$|R
40|$|In this paper, {{we propose}} a {{modified}} dynamic decode-and-forward (MoDDF) relaying protocol {{to meet the}} critical requirements for user equipment (UE) relays in next-generation cellular systems (e. g., LTE-Advanced and beyond). The proposed MoDDF realizes the fast jump-in relaying and the sequential decoding with an application of random codeset to encoding and re-encoding process at the source and the multiple UE relays, respectively. A subframe-by-subframe decoding based on the accumulated (or <b>buffered)</b> <b>messages</b> is employed to achieve energy, information, or mixed combining. Finally, possible early termination of decoding at the end user {{can lead to the}} higher spectral efficiency and more energy saving by reducing the frequency of redundant subframe transmission and decoding. These attractive features eliminate the need of directly exchanging control messages between multiple UE relays and the end user, which is an important prerequisite for the practical UE relay deployment. Copyright: © 2016 Nam et al. This is an open access article distributed {{under the terms of the}} Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited...|$|R
30|$|When a node {{receives}} {{messages from}} another node, those messages {{are added to}} the <b>buffer</b> and all <b>messages</b> in the <b>buffer</b> are sorted in the order of their priority (i.e., high: red, yellow, green, then low).|$|R
5000|$|... dmesg (display message or driver message) is {{a command}} on most Unix-like {{operating}} systems that prints the <b>message</b> <b>buffer</b> of the kernel. The output of this command typically contains the messages {{produced by the}} device drivers.|$|R
5000|$|For short {{messages}} (fewer than 256 bytes) the kernel copies the <b>message</b> <b>buffers</b> between processes, {{from the}} address {{space of the}} sending process to the system address space, {{and from there to}} the receiving process' address space.|$|R
40|$|In {{the realm}} of {{cooperative}} Intelligent Transportation Systems, vehicles are {{able to communicate with}} each other and with the available telecommunications infrastructure to support various safety-related services, traffic-efficiency solutions, and a wide variety of infotainment applications. As the number of mobile devices and information they need to exchange steadily increases, this will further strain mobile networks. It is for that reason that communication over wireless local area networks (WLANs) is becoming the primary means of transporting data to/from vehicles. However, due to their limit range WLANs only offer intermittent connectivity, and due to node mobility the established connections are only available for a short period of time. The present paper investigates how inter-vehicle communication can help minimize the upload/download delivery times in the context of delay tolerant traffic and intermittent communication. To do so, each vehicular station distributes part of its message to neighboring nodes prior to accessing the infrastructure. In turn, vehicles transmit their <b>buffered</b> <b>messages</b> when passing by the infrastructure terminals to complete the file transfer. This paper presents a thorough study of the aforementioned strategy providing a mathematical framework and optimized forwarding techniques. Performance results validate the applicability of the proposed solution...|$|R
