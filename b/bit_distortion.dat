0|64|Public
40|$|Paper is {{dedicated}} to solving the problem of increasing the efficiency of data transmission error detecting, localization and correcting in spectrum modulation channel by properties such errors appearance accounted. For correction of the <b>bit</b> <b>distortions</b> caused by one or two errors of channel error transmission the weighed check sum with special weigh coefficients has been proposed. Algorithm for error detection and correction has been worked out. It {{has been shown that}} proposed error correction techniques is more simple and demanded control bits less than error correction codes...|$|R
5000|$|The Allmusic {{review by}} Scott Yanow says [...] "Unfortunately the live set {{does have a}} <b>bit</b> of <b>distortion</b> in spots (particularly {{on some of the}} trumpet notes); {{otherwise}} this strong postbop set would have received a higher rating".|$|R
50|$|In a {{communication}} system, the receiver side BER {{may be affected}} by transmission channel noise, interference, <b>distortion,</b> <b>bit</b> synchronization problems, attenuation, wireless multipath fading, etc.|$|R
40|$|Abstract — In {{the first}} part of this paper, we derive a source model {{describing}} the relationship between <b>bits,</b> <b>distortion,</b> and quantization step size for transform coders. Based on this source model, a variable frame rate coding algorithm is developed. The basic idea is to select a proper picture frame rate to ensure a minimum picture quality for every frame. Because our source model can predict approximately the number of coded bits when a certain quantization step size is used, we could predict the quality and bits of coded images without going through the entire realcoding process. Therefore, we could skip the right number of picture frames to accomplish the goal of constant image quality. Our proposed variable frame rate coding schemes are simple but quite effective as demonstrated by simulation results. The results of using another variable frame rate scheme, Test Model for H. 263 (TMN- 5), and the results of using a fixed frame rate coding scheme, Reference Model 8 for H. 261 (RM 8), are also provided for comparison. Index Terms — Image coding, rate distortion theory, source coding. I...|$|R
30|$|Before {{encoding}} a GOP a new pattern codebook {{is generated}} using all frames of the GOP. Then encode that GOP {{using the new}} codebook and the previous codebook (if there is one, for the first GOP there is no previous codebook). We have selected the bits stream based on the minimum cost function (using Equation 3 with new Lagrangian Multiplier (see Section 5.1) using the average <b>bits</b> and <b>distortion</b> (sum of square difference) per MB.|$|R
40|$|Abstract—A {{source model}} {{describing}} {{the relationship between}} <b>bits,</b> <b>distortion,</b> and quantization step sizes of a large class of block-transform video coders is proposed. This model is initially derived from the rate-distortion theory and then modified to match the practical coders and real image data. The realistic constraints such as quantizer dead-zone and threshold coefficient selection are included in our formulation. The most attractive feature of this model is its simplicity in its final form. It enables us to predict the bits needed to encode a picture at a given distortion or to predict the quantization step size at a given bit rate. There are two aspects of our contribution: one, we extend the existing results of rate-distortion theory to the practical video coders, and two, the nonideal factors in real signals and systems are identified, and their mathematical expressions are derived from empirical data. One application of this model, {{as shown in the}} second part of this paper, is the buffer/quantizer control on a CCITT P 2 64 k coder with the advantage that the picture quality is nearly constant over the entire picture sequence. Index Terms — Image coding, rate distortion theory, source coding. I...|$|R
40|$|The {{need for}} video {{summarization}} originates primarily from a viewing time or a bit budget constraint. A shorter {{version of the}} original video sequence is desirable {{in a number of}} applications. Clearly, a shorter version is also necessary in applications where storage, communication bandwidth and/or power are limited, which translates into a bit budget constraint. Our work is based on a <b>bit</b> ratesummary <b>distortion</b> optimization formulation. New metrics for video summary distortion are introduced. An optimal algorithm based on Lagrangian relaxation and dynamic programming is presented. 1...|$|R
30|$|From Figs.  12, 14, and 16 (corresponding to DB 1, DB 2, and DB 3, respectively), it may {{be noted}} that PE-VQ method yields higher PSNR values {{compared}} to IP-VQ method. As {{the size of the}} error code word in PE-VQ method (6 bits) is less compared to the image pixel code word size in IP-VQ method (8 <b>bits),</b> the <b>distortion</b> error Eu given in Eq. (5) is reduced resulting in higher PSNR values. In other words, a more optimum error codebook is obtained in the PE-VQ method when compared to IP-VQ method.|$|R
40|$|Image Processing is a {{powerful}} era of the Modern Digital Technology. Compression {{is a process of}} minimizing the size in bytes of a graphics file without degrading the quality of the image to an unacceptable level. In this paper, we have discusses about Digital Image Compression for the good performance complexity of still imagery and the comparative study of several algorithms. In future we are going to propose a new plan to provide a reduction in computation over the sparse matrix and using the various test images for the entropy coding and quality scalability is enabled by simply truncating the generated <b>bit</b> rate <b>distortion</b> performance...|$|R
40|$|This paper {{addresses}} {{performance of}} RMSE with power allocation methods for multimedia signals over wireless channels. The {{objective is to}} minimize total power allocated for image compression and transmission, while the power for each bit is kept at a predetermined value. Different image formats exist in multimedia. RMSE is analysed for five types of image formats. In this work, an approach for minimizing the total power allocated of a multimedia like image due to source compression and transmission subject to a fixed <b>bit</b> source <b>distortion</b> for different image format is considered. Simulations are performed using unilevel haar wavelet over AWGN channel with maxima and mimima power control...|$|R
30|$|To {{accommodate}} extra pattern modes in the H. 264 video {{coding standard}} for testing, {{we need to}} modify its bitstream structure and Lagrangian multiplier. For inclusion of pattern mode we change the header information for MB type, pattern identification code, and shape of patterns. Inclusion of pattern mode also demands modification of the Lagrangian multiplier as the pattern mode is biased to <b>bits</b> rather than <b>distortion.</b>|$|R
30|$|For {{multiple}} pattern modes, the ASPVC-Global {{needs only}} <b>bit</b> and <b>distortion</b> calculation without ME. The ME, irrespective of a scene's complexity, typically comprises more than 60 % of the computational overhead required to encode an inter picture with a software codec using the DCT [27, 28], when full search is used. Thus, maximum of 10 % operations {{are needed for}} one pattern mode as each pattern mode will process one-fourth of the MB. As a result, the ASPVC-Global algorithm using five random starts and up to four pattern modes may requires extra 0.58 of a mode ME&MC operations compared to the H. 264 which {{would not be a}} problem in real time processing.|$|R
40|$|This paper {{addresses}} power allocation {{methods for}} multimedia signals over wireless channels. The {{aim of this}} paper is to minimize total power allocated for image compression and transmission, while the power for each bit is kept at a predetermined value. Maximum and minimum power control algorithms are proposed. In this work, an approach for minimizing the total power allocated to a multimedia like image due to source compression and transmission subject to a fixed <b>bit</b> source <b>distortion</b> using different parameters are analysed. Simulations are performed using haar wavelet over AWGN channel. Structural Content (SC), Average Difference, Normalized Absolute Error (NAE), etc are analysed. Maximum Power Adaptation Algorithm shows better performance than Conventional Power Adaptation Algorithm. ...|$|R
40|$|In {{this paper}} an image {{transmission}} {{system has been}} proposed where Joint Photographic Experts Group (JPEG) algorithm is used as an image coder and Rate Compatible Punctured Convolution (RCPC) channel coder is used for transmission of coded image over wireless channels (AWGN and Rayleigh fading). JPEG bit stream is partitioned into DC and AC bit streams. AC bit stream further classified using edge density property of block. Priorities based Unequal error Protection (UEP) applied to <b>bit</b> stream. <b>Distortion</b> analysis (MSE) is given for proposed image transmission scheme. The simulation results shows reduction in distortion compared to conventional Equal Error Protection (EEP). This proposed algorithm can be applied for low frequency as well as high frequency images...|$|R
40|$|This paper {{addresses}} power allocation {{methods for}} multimedia signals over wireless channels. The {{objective is to}} minimize total power allocated for image compression and transmission, while the power for each bit is kept at a predetermined value. Minimum and maximum power adaptation methods are proposed. In this work, an approach for minimizing the total power allocated of a multimedia like image due to source compression and transmission subject to a fixed <b>bit</b> source <b>distortion.</b> Simulations are performed using Haar wavelet over AWGN channel using BPSK modulation. The numerical analysis of Bit Error Rate (BER) shows that optimized power methods can reduce the total power allocated by a significant factor and the Bit error rate is reduced considerably compared to the Conventional power method using Haar wavelet. ...|$|R
50|$|Sources of glitch sound {{material}} {{are usually}} malfunctioning or abused audio recording devices or digital electronics, such as CD skipping, electric hum, digital or analog <b>distortion,</b> <b>bit</b> rate reduction, hardware noise, software bugs, crashes, vinyl record hiss or scratches, and system errors.In a Computer Music Journal {{article published in}} 2000, composer and writer Kim Cascone classifies glitch as a subgenre of electronica and used the term post-digital to describe the glitch aesthetic.|$|R
40|$|Abstract – Image-based {{features}} are now {{commonly used to}} perform matching between objects in a query image and objects in database images. Previous {{studies have been done}} to apply these features in a real-time cell-phone-based mobile augmented reality (MAR) to recognize buildings. The database feature sets need to be transmitted from the database to the cell phone. In order to minimize cost associated to this transmission, a compression scheme should be developed. This document investigates the effectiveness of several possible compression algorithms. Initially, principal component analysis (PCA) is applied to maximize the coding gain. The transformed coefficients are quantized using a uniform quantizer and entropy-coded using several known entropy coding schemes. By varying the quantization level, the relationship between achievable <b>bit</b> rate, <b>distortion,</b> and query-to-database image matching rate could be determined...|$|R
30|$|For {{content-based}} pattern generation, we need {{to transmit}} the pattern codebook after a certain interval. To determine whether {{we need to}} transmit the newly generated codebook or continue with the current one, we consider the <b>bits</b> and <b>distortions</b> generated with both the current and the previous pattern codebooks. The GOP [26] {{may be the best}} choice as after a GOP we need to send a fresh Intra picture in the bitstream. Note that this GOP may be different from the group of frames used for codebook generation. To trade off the bitstream size and quality we can use Lagrangian optimization function as it is used to control the rate-distortion performance. Here we consider average <b>distortion</b> and <b>bits</b> per MB in both cases. We select the current codebook if it provides less Lagrangian cost as compared to the previous one.|$|R
40|$|The recent {{scalable}} video coding (SVC) {{extension to}} the H. 264 /AVC video coding standard has unprecedented compression efficiency while supporting {{a wide range of}} scalability modes, including temporal, spatial, and quality (SNR) scalability, as well as combined spatiotemporal SNR scalability. The traffic characteristics, especially the bit rate variabilities, of the individual layer streams critically affect their network transport. We study the SVC traffic statistics, including the <b>bit</b> rate <b>distortion</b> and <b>bit</b> rate variability <b>distortion,</b> with long CIF resolution video sequences and compare them with the corresponding MPEG- 4 Part 2 traffic statistics. We consider (i) temporal scalability with three temporal layers, (ii) spatial scalability with a QCIF base layer and a CIF enhancement layer, as well as (iii) quality scalability modes FGS and MGS. We find that the significant improvement in RD efficiency of SVC is accompanied by substantially higher traffic variabilities as compared to the equivalent MPEG- 4 Part 2 streams. We find that separately analyzing the traffic of temporal-scalability only encodings gives reasonable estimates of the traffic statistics of the temporal layers embedded in combined spatiotemporal encodings and in the base layer of combined FGS-temporal encodings. Overall, we find that SVC achieves significantly higher compression ratios than MPEG- 4 Part 2, but produces unprecedented levels of traffic variability, thus presenting new challenges for the network transport of scalable video. Copyright © 2008 Geert Van der Auwera et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properl...|$|R
50|$|I {{recorded}} the band tracks in my Santa Monica studio utilizing a Korg 01/W workstation, Emax 8-bit sampler, and a Roland R-8 rhythm composer - recorded on a Otari MX one-inch 16-track. For the session, Michael {{showed up with}} a Dean, twin-necked guitar, which I plugged into a Rockman then direct into the board. Preferring a <b>bit</b> more <b>distortion,</b> I patched in a stomp box that Michael brought as well. I don’t believe {{that he had heard}} the band tracks prior to the session - so, as a result, Michael took time to work out the main solo and harmony on a single-neck but would refer to the double-neck to verify that the two parts could be performed in a dual-necked fashion. In our quest to get precision solos in one short studio evening, Michael performed his parts on a single-neck for instant accuracy. Overdubs were added for a fatter sound.|$|R
40|$|This paper {{describes}} {{an effort to}} extend the LempelZiv algorithm to a practical universal lossy compression algorithm. It {{is based on the}} idea of approximate string matching with a rate-distortion (R Γ D) criterion, and is addressed within the framework of vector quantization (VQ) [4]. A practical one pass algorithm for VQ codebook construction and adaptation for individual signals is developed which assumes no prior knowledge of the source statistics and involves no iteration. We call this technique rate-distortion Lempel-Ziv (RDLZ). As {{in the case of the}} Lempel-Ziv algorithm, the encoded bit stream consists of codebook (dictionary) updates as well as indices (pointers) to the codebook. The idea of "trading" <b>bits</b> for <b>distortion</b> in modifying the codebook will be introduced. Experimental results show that, for Gaussian sources as well as real images, RDLZ performs comparably, sometimes favorably, to static codebook VQ trained on the corresponding sources or images. 1. INTRODUCTION [...] ...|$|R
30|$|In this work, {{the audio}} signal used for {{experimentation}} is speech signal. Generally, the speech signal has been classified into voiced and unvoiced parts. The voiced {{part of speech}} consists of high-energy and low-frequency component whereas the unvoiced part of speech contains low-energy and high-frequency component [34]. The voiced frames are selected for embedding watermark <b>bits</b> because the <b>distortion</b> created in high-energy frames are less audible compared to low-energy frames. Further, the modification in DCT coefficients of high-energy frames introduces minimal distortion compared to low-energy frames and hence provides better scope of embedding the watermark.|$|R
30|$|In this paper, we {{introduce}} {{an efficient}} arbitrary-shaped pattern-based video coding (ASPVC) scheme using a content-based pattern generation strategy from decoded frames and the McFIS {{to avoid any}} frame delay and pattern shape coding (as both encoder and decoder use the same procedure and frames to generate pattern templates). The experimental results confirm that the proposed method outperforms the two recent and relevant algorithms by improving image quality significantly. The preliminary idea is published in [23]. This paper has been extended by including the following things: (1) generation of arbitrary-shaped content-based pattern templates from the decoded frames, (2) embedding the pattern mode into the H. 264 framework by adjusting the corresponding <b>bits</b> and <b>distortion,</b> (3) new McFIS generation strategy based on the theoretical relationship between the distortion and quantization step size, (4) computational complexity comparison with other relevant existing techniques, and (5) more insight reasoning supported by data and analysis to show {{the superiority of the}} proposed algorithms compared to the algorithms.|$|R
30|$|The {{proposed}} methods outperform {{the relevant}} state-of-the-art methods for the fixed and/or moderate camera motion video sequences (e.g., Tennis and Trevor). However, the proposed techniques {{in their current}} state could not provide better rate-distortion performance compared to the H. 264 for the videos with high activities (i.e., camera/object motions) as the McFIS is least relevant for the referencing of the high camera motion videos {{and the number of}} RMBs of the high object motion videos (e.g., Football and Flower) is insignificant (around 3 %) to improve the R-D performance of the proposed methods. Note that Figure  13 also confirms that unlike the PVC scheme [12], the R-D performance of the proposed schemes {{is similar to that of}} the existing relevant schemes for high-activity videos. This establishes our hypothesis on the adjustment of the <b>bits</b> and <b>distortion</b> in the Lagrangian cost function determination for the pattern mode while embedding the pattern mode into the existing H. 264 framework.|$|R
40|$|The {{recently}} developed H. 264 /AVC video codec compresses video significantly {{more efficiently than}} previous codecs and is therefore expected {{to be used for}} compressing the majority of the video transported over communication networks. The traffic characteristics of encoded video {{have a significant impact on}} the network transport of compressed video, making it very important to study the characteristics of H. 264 /AVC video traffic. In this paper we examine the <b>bit</b> rate <b>distortion</b> performance, <b>bit</b> rate variability, and long range dependence of the H. 264 /AVC codec for long videos up to High Definition resolution. We also explore the impact of smoothing on the H. 264 /AVC video traffic. We find that compared to the MPEG- 2 and MPEG- 4 Part 2 codecs, the H. 264 /AVC codec achieves the lower average bit rate for a given video quality at the expense of significantly increased traffic variability that remains at a high level even with smoothing...|$|R
40|$|This paper {{presents}} a novel real-time algorithm for reducing and dynamically controlling the computational complexity of an H. 264 video encoder implemented in software. A fast Mode Decision algorithm, {{based on a}} Pareto optimal MacroBlock classification scheme, is combined with a Dynamic Complexity Control algorithm that adjusts the MB Class decisions such that a constant frame rate is achieved. The average coding efficiency of the proposed algorithm {{was found to be}} similar to that of conventional encoding operating at half the frame rate. The proposed algorithm was found to provide lower average <b>bit</b> rate and <b>distortion</b> than Static Complexity Scaling...|$|R
40|$|Distortion {{allocation}} {{varying with}} wavelength in lossy compression of hyperspectral imagery is investigated, {{with the aim}} of minimizing the spectral distortion between original and decompressed data. The absolute angular error, or spectral angle mapper (SAM), is used to quantify spectral distortion, while radiometric distortions are measured by maximum absolute deviation (MAD) for near-lossless methods, for example, differential pulse code modulation (DPCM), or mean-squared error (MSE) for lossy methods, for example, spectral decorrelation followed by JPEG 2000. Two strategies of interband distortion allocation are compared: given a target average <b>bit</b> rate, <b>distortion</b> may be set to be constant with wavelength. Otherwise, it may be allocated proportionally to the noise level of each band, according to the virtually lossless protocol. Comparisons with the uncompressed originals show that the average SAM of radiance spectra is minimized by constant distortion allocation to radiance data. However, variable distortion allocation according to the virtually lossless protocol yields significantly lower SAM in case of reflectance spectra obtained from compressed radiance data, if compared with the constant distortion allocation at the same compression ratio...|$|R
2500|$|This implies {{another level}} of masking, because the {{economic}} character masks are then straightforwardly ("vulgarly") equated with authentic behaviour. The effect in this case is, that the theory of [...] "how the economy works" [...] masks how it actually works, by conflating its surface appearance with its real nature. Its generalities seem to explain it, but in reality they do not. The theory is therefore (ultimately) arbitrary. Either things are studied in isolation from the total {{context in which they}} occur, or generalizations are formed which leave essential <b>bits</b> out. Such <b>distortion</b> can certainly be ideologically useful to justify an economic system, position or policy as a good thing, but it can become a hindrance to understanding.|$|R
30|$|As {{mentioned}} earlier, the IQA algorithm can be {{not only}} used for quality assessment tasks but also pervasively used in many other applications. A direct application of IQA measures is {{to use them to}} benchmark the image processing algorithms and systems [43]. For example, the rate distortion (RD) curves are often used to characterize the performance of image coding systems, where the RD function is defined as the <b>bit</b> rate <b>distortion</b> between the original and decoded images. A lower RD curve indicates a better image coder. To compute this distortion and obtain the RD curve, a lot of methods based on MSE are proposed. However, these methods suffer from low accuracy. As we mentioned earlier, the RD curve can be used to precisely evaluate the image coder only if the IQA methods have higher accuracy. To improve the accuracy, VIF, FSIM, and MSSSIM are proposed. However, these methods suffer from low computation efficiency, which renders them cannot be used in many applications. Different from previous work, our proposed IDSSIM not only has the high accuracy but also achieves the high efficiency, which is very attractive and competitive for real-time applications.|$|R
30|$|A novel {{reversible}} watermarking algorithm with two-stage {{data hiding}} strategy {{is presented in}} this paper. The core idea is two-stage data hiding (i.e., hiding data twice in a pixel of a cell), where the distortion after {{the first stage of}} embedding can be rarely removed, mostly reduced, or hardly increased after the second stage. Note that even the increased distortion is smaller compared to that of other methods under the same conditions. For this purpose, we compute lower and upper bounds from ordered neighboring pixels. In the first stage, the difference value between a pixel and its corresponding lower bound is used to hide one <b>bit.</b> The <b>distortion</b> can be removed, reduced, or increased by hiding another bit of data by using a difference value between the upper bound and the modified pixel. For the purpose of controlling capacity and reducing distortion, we determine appropriate threshold values. Finally, we present an algorithm to handle overflow/underflow problems designed specifically for two-stage embedding. Experimental study is carried out using several images, and the results are compared with well-known methods in the literature. The results clearly highlight that the proposed algorithm can hide more data with less distortion.|$|R
30|$|H. 264, {{the latest}} video coding {{standard}} [1, 2], outperforms its competitors such as H. 263, MPEG- 2, MPEG- 4, etc. {{due to a}} number of innovative features in the intra- and inter-frame coding techniques. Variable block size (VBS) motion estimation and motion compensation (ME&MC) are the most prolific features. In the VBS scheme, a 16 [*]×[*] 16 pixel macroblock (MB) is partitioned into several small rectangular- or square-shaped blocks. ME&MC are carried out for all possible combinations, and the ultimate block size is selected based on the Lagrangian optimization[3 – 5] using the <b>bits</b> and <b>distortions</b> of the corresponding blocks. Real-world objects, by nature, may be in any arbitrary shapes, and ME&MC using only rectangular- and square-shaped blocks just approximate the real shape; thus, the coding gain would not be satisfactory. A number of research works are conducted by non-rectangular block partitioning [6 – 11] using geometric shape partitioning, motion-based implicit block partitioning, and L-shaped partitioning. The requirement of excessively high computational complexity in the segmentation process and the marginal improvement over the H. 264 make them less effective for real-time applications [12]. Moreover, the requirement of valuable bits for encoding the area covering almost static background makes the abovementioned algorithms inefficient in terms of rate-distortion performance.|$|R
40|$|Abstract—In H. 264 /advanced video coding, the encoder em-ploys the {{rate-distortion}} optimization (RDO) {{to select}} the optimal coding mode of each block. Although it is effective to employ the RDO technique for mode decision, the computation load increases drastically. To reduce the computation complexity of the RDO technique, in this paper, we propose efficient algorithms for the estimation of block-level rate and distortion. For rate estimation, we model the transform coefficients with accurate generalized Gaussian distributions, and the weighted sum of absolute quantized transform coefficients is proposed as an efficient rate estimator, where the weights provide an implicit mechanism for evaluating different contributions of different fre-quency components to the coding <b>bits.</b> For <b>distortion</b> estimation, we first analyze the origins of distortion thoroughly. Then a direct relationship between the discarded bits in quantization and the distortion is explored. According to this investigation, a simple and efficient algorithm is proposed for the distortion estimation. With above proposed algorithms, the RDO technique can be efficiently implemented in a low-complexity way. Exten-sive experimental results demonstrate that, compared with the original RDO implementation, the proposed algorithms achieve about 32 % reduced total encoding time with ignorable coding performance degradation. Index Terms—Distortion estimation, GGD, H. 264 /AVC, rate estimation, rate-distortion optimization, video coding...|$|R
40|$|The {{existing}} {{rate control}} schemes {{in the literature}} calculate quantization parameters of macro-blocks (MB) based on the quadratic/log rate models. Actually, this model may not be good because this model does not estimate well at some particular bit rates. This work investigates the relationships of <b>bit</b> rate and <b>distortion</b> with MB quantization step size and standard deviation of MB prediction errors. We find that linear rate and linear distortion models are established. Then we present the proposed rate control schemes based on these models. The proposed algorithm introduces a low quantization overhead, low computation complexity to calculate the optimal quantization step size, independence of the distortion factors and low memory usage compared with TMN 8 rate control. The experimental results suggest that our scheme can achieve PSNR gain over TMN 8...|$|R
40|$|Texture-plus-depth ” {{has become}} a popular coding format for mul-tiview image compression, where a decoder can {{synthesize}} images at intermediate viewpoints using encoded texture and depth maps of closest captured view locations via depth-image-based rendering (DIBR). As in other resource-constrained scenarios, limited avail-able bits must be optimally distributed among captured texture and depth maps to minimize the expected signal distortion at the decoder. A specific challenge of multiview image compression for DIBR is that the encoder must allocate bits without the knowledge of how many and which specific virtual views will be synthesized at the decoder for viewing. In this paper, we derive a cubic synthesized view distortion model to describe the visual quality of an interpo-lated view {{as a function of}} the view’s location. Given the model, one can easily find the virtual view location between two coded views where the maximum synthesized distortion occurs. Using a multi-view image codec based on shape-adaptive wavelet transform, we show how optimal bit allocation can be performed to minimize the maximum view synthesis distortion at any intermediate viewpoint. Our experimental results show that the optimal bit allocation can outperform a common uniform bit allocation scheme by up to 1. 0 dB in coding efficiency performance, while simultaneously being com-petitive to a state-of-the-art H. 264 codec. Index Terms — multiview imaging, depth-image-based render-ing, <b>bit</b> allocation, <b>distortion</b> modeling 1...|$|R
40|$|Most {{existing}} {{rate control}} schemes {{in the literature}} calculate quantization parameters of the macro-blocks (MB) based on the standard deviation of the residue before quantization. The probability of non-zero coefficients after quantization is a new factor that has an interesting property. The rate tends to be linear with this probability. In this work, we design the rate control based on this special factor with linear property. This work investigates the relationships of <b>bit</b> rate and <b>distortion</b> with MB quantization step size and this interesting probability. We establish linear rate and linear distortion models and propose a rate control scheme based on these models. The proposed algorithm has a low quantization overhead, low computation complexity and is independent of the distortion factors compared with TMN 8 rate control. The experimental results suggest that our scheme can achieve PSNR gain over TMN 8...|$|R
40|$|We {{propose a}} new wavelet {{compression}} algorithm {{based on the}} rate-distortion optimization for densely sampled triangular meshes. Exploiting the normal remesher of Guskov et al., the proposed algorithm includes a wavelet transform and an original bit allocation optimizing the quantization of the wavelet coefficients. The allocation process minimizes the reconstruction error for a given <b>bit</b> budget. As <b>distortion</b> measure, we use the mean square error of the normal mesh quantization, expressed according to the quantization error of each subband. We show that this metric is a suitable criterion to evaluate the reconstruction error, i. e., the geometric distance between the input mesh and the quantized normal one. Moreover, to design a fast bit allocation, we propose a model-based approach, depending on distribution of the wavelet coefficients. Compared to the state-of-the-art methods for normal meshes, our algorithm provides improvements in coding performance, up to dB compared to the original zerotree coder...|$|R
