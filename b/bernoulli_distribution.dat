351|218|Public
5|$|The {{first part}} concludes {{with what is}} now known as the <b>Bernoulli</b> <b>distribution.</b>|$|E
25|$|The {{lower bound}} is {{realized}} by the <b>Bernoulli</b> <b>distribution.</b>|$|E
25|$|Several authors, {{including}} N. L. Johnson and S. Kotz, use {{the symbols}} p and q (instead of α and β) for the shape {{parameters of the}} beta distribution, reminiscent of the symbols traditionally used for {{the parameters of the}} <b>Bernoulli</b> <b>distribution,</b> because the beta distribution approaches the <b>Bernoulli</b> <b>distribution</b> in the limit when both shape parameters α and β approach the value of zero.|$|E
40|$|The {{mixture of}} <b>Bernoulli</b> <b>distributions</b> [6] is a {{technique}} that is frequently used for the modeling of binary random vectors. They differ from (restricted) Boltzmann Machines in {{that they do not}} model the marginal distribution over the binary data space X as a product of (conditional) <b>Bernoulli</b> <b>distributions,</b> but as a weighted sum of <b>Bernoulli</b> <b>distributions.</b> Despite the non-identifiability of the mixtur...|$|R
5000|$|This {{probability}} distribution is decomposable (as {{the sum of}} two <b>Bernoulli</b> <b>distributions)</b> if ...|$|R
40|$|The {{objective}} {{and purpose of}} this note is to generate bivariate distri-butions via extreme <b>Bernoulli</b> <b>distributions</b> and obtain results on posi-tively and negatively dependent families of bivariate binomial distribu-tions generated by extreme <b>Bernoulli</b> <b>distributions.</b> Some distributional properties and results are presented. The factorial moment generat-ing functions, correlation functions, conditional distributions and the regression functions are given. Mathematics Subject Classifications: 60 E 05; 60 F 9...|$|R
25|$|The <b>Bernoulli</b> <b>distribution</b> is {{a special}} case of the {{binomial}} distribution, where n=1. Symbolically, X~B(1,p) has the same meaning as X~B(p). Conversely, any binomial distribution, B(n,p), is {{the distribution of the}} sum of n Bernoulli trials, B(p), each with the same probability p.|$|E
25|$|Well-known {{discrete}} probability distributions used in statistical modeling {{include the}} Poisson distribution, the <b>Bernoulli</b> <b>distribution,</b> the binomial distribution, the geometric distribution, {{and the negative}} binomial distribution. Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.|$|E
25|$|A single success/failure {{experiment}} is also called a Bernoulli trial or Bernoulli experiment and {{a sequence of}} outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a <b>Bernoulli</b> <b>distribution.</b> The binomial distribution {{is the basis for}} the popular binomial test of statistical significance.|$|E
40|$|AbstractWe {{consider}} {{measures of}} languages induced by <b>Bernoulli</b> <b>distributions</b> on {{the letters of}} a given alphabet. Of particular interest are languages having a measure equal to 1 with respect to all positive <b>Bernoulli</b> <b>distributions</b> (<b>Bernoulli</b> sets). The main object of the paper is to study conditions ensuring that a given language has a finite Bernoulli completion, i. e., it is included in a finite Bernoulli set. Some characterizations of languages having finite Bernoulli completions are given. In {{the case of a}} two-letter alphabet it is shown that one can decide whether a finite language has a finite Bernoulli completion or not. Moreover, any finite code over a two-letter alphabet has a finite Bernoulli completion. Finally, we prove that two finite languages have the same measure with respect to all <b>Bernoulli</b> <b>distributions</b> if and only if each of the two languages can be obtained from the other by using a finite number of times three suitable measure-invariant transformations...|$|R
40|$|In {{this paper}} we {{consider}} a (possibly continuous) space of Bernoulli experiments. We {{assume that the}} <b>Bernoulli</b> <b>distributions</b> are correlated. All evidence data {{comes in the form}} of successful or failed experiments at different points. Current state-ofthe-art methods for expressing a distribution over a continuum of <b>Bernoulli</b> <b>distributions</b> use logistic Gaussian processes or Gaussian copula processes. However, both of these require computationally expensive matrix operations (cubic in the general case). We introduce a more intuitive approach, directly correlating beta distributions by sharing evidence between them according to a kernel function, an approach which has linear time complexity. The approach can easily be extended to multiple outcomes, giving a continuous correlated Dirichlet process, and can be used for both classification and learning the actual probabilities of the <b>Bernoulli</b> <b>distributions.</b> We show results for a number of data sets, as well as a case-study where a mixture of continuous beta processes is used as part of an automated stroke rehabilitation system. ...|$|R
5000|$|The <b>Bernoulli</b> <b>distributions,</b> (number of {{successes in}} one trial with {{probability}} [...] of success). The {{cumulant generating function}} is [...] The first cumulants are [...] and [...] The cumulants satisfy a recursion formula ...|$|R
2500|$|<b>Bernoulli</b> <b>distribution,</b> for {{the outcome}} of a single Bernoulli trial (e.g. success/failure, yes/no) ...|$|E
2500|$|Categorical distribution, for {{a single}} {{categorical}} outcome (e.g. yes/no/maybe in a survey); a generalization of the <b>Bernoulli</b> <b>distribution</b> ...|$|E
2500|$|Beta distribution, for {{a single}} {{probability}} (real number between 0 and 1); conjugate to the <b>Bernoulli</b> <b>distribution</b> and binomial distribution ...|$|E
3000|$|... ∗≤ 1. The Poisson, {{negative}} binomial, and binomial distributions are popular, classical {{tools for}} modeling count data {{of a particular}} (in)finite form. What is most interesting about these distributions is that they each represent sums of other classical distributions, namely the Poisson, geometric, and <b>Bernoulli</b> <b>distributions,</b> respectively. The Poisson, geometric and <b>Bernoulli</b> <b>distributions</b> are themselves special cases of the Conway-Maxwell-Poisson (CMP) distribution – a two-parameter flexible count distribution that generalizes the Poisson distribution to accommodate data over- or under-dispersion. This work introduces and thus considers the sum of CMP random variables to establish the flexible class of distributions that encompass the Poisson, geometric, Bernoulli, negative binomial, binomial, and CMP distributions as special cases.|$|R
40|$|The {{class of}} finite {{mixtures}} of multivariate <b>Bernoulli</b> <b>distributions</b> {{is known to}} be nonidentifiable, i. e., different values of the mixture parameters can correspond to exactly the same probability distribution. In principle, this would mean that sample estimates using this model would give rise to different interpretations...|$|R
50|$|The {{assumptions}} on {{distributions of}} features {{are called the}} event model of the Naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and <b>Bernoulli</b> <b>distributions</b> are popular. These assumptions lead to two distinct models, which are often confused.|$|R
2500|$|... α = β → 0 is a 2-point <b>Bernoulli</b> <b>distribution</b> {{with equal}} {{probability}} 1/2 at each Dirac delta function end x = 0 and x = 1 and zero probability everywhere else. A coin toss: one {{face of the}} coin being x = 0 and the other face being x = 1.|$|E
2500|$|A {{distribution}} {{with negative}} excess kurtosis is called platykurtic, or platykurtotic. [...] "Platy-" [...] means [...] "broad". In terms of shape, a platykurtic distribution has thinner tails. Examples of platykurtic distributions include the continuous or discrete uniform distributions, and the raised cosine distribution. The most platykurtic distribution {{of all is}} the <b>Bernoulli</b> <b>distribution</b> with p = ½ (for example the number of times one obtains [...] "heads" [...] when flipping a coin once, a coin toss), for which the excess kurtosis is −2. Such distributions are sometimes termed sub-Gaussian.|$|E
2500|$|In {{probability}} and statistics, a Bernoulli {{process is}} a finite or infinite sequence of binary random variables, {{so it is a}} discrete-time stochastic process that takes only two values, canonically 0 and1. The component Bernoulli variables X'i are identically distributed and independent. [...] Prosaically, a Bernoulli {{process is a}} repeated coin flipping, possibly with an unfair coin (but with consistent unfairness). [...] Every variable X'i in the sequence is associated with a Bernoulli trial or experiment. They all have the same <b>Bernoulli</b> <b>distribution.</b> Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided die); this generalization is known as the Bernoulli scheme.|$|E
40|$|An {{implementation}} of the iterative proportional fitting (IPFP), maximum likelihood, minimum chi-square and weighted least squares procedures for updating a N-dimensional array with respect to given target marginal distributions (which, in turn can be multi-dimensional). The package also provides an application of the IPFP to simulate multivariate <b>Bernoulli</b> <b>distributions...</b>|$|R
5000|$|The Dirichlet {{distribution}} is a multivariate generalization of the beta distribution. Univariate marginals of the Dirichlet distribution have a beta distribution. The beta {{distribution is}} conjugate to the binomial and <b>Bernoulli</b> <b>distributions</b> {{in exactly the}} same way as the Dirichlet distribution is conjugate to the multinomial distribution and categorical distribution.|$|R
40|$|We express each Fréchet {{class of}} multivariate <b>Bernoulli</b> <b>distributions</b> with given margins as the convex hull {{of a set}} of densities, which belong to the same Fréchet class. This {{characterisation}} allows us to establish whether a given correlation matrix is compatible with the assigned margins and, if it is, to easily construct one of the corresponding joint densities. ...|$|R
2500|$|The plot {{of excess}} {{kurtosis}} {{as a function}} of the variance and the mean shows that the minimum value of the excess kurtosis (−2, which is the minimum possible value for excess kurtosis for any distribution) is intimately coupled with the maximum value of variance (1/4) and the symmetry condition: the mean occurring at the midpoint (μ = 1/2). This occurs for the symmetric case of α = β = 0, with zero skewness. [...] At the limit, this is the 2 point <b>Bernoulli</b> <b>distribution</b> with equal probability 1/2 at each Dirac delta function end x = 0 and x = 1 and zero probability everywhere else. (A coin toss: one face of the coin being x = 0 and the other face being x = 1.) [...] Variance is maximum because the distribution is bimodal with nothing in between the two modes (spikes) at each end. [...] Excess kurtosis is minimum: the probability density [...] "mass" [...] is zero at the mean and it is concentrated at the two peaks at each end. [...] Excess kurtosis reaches the minimum possible value (for any distribution) when the probability density function has two spikes at each end: it is bi-"peaky" [...] with nothing in between them.|$|E
2500|$|Values for the {{skewness}} {{and excess}} kurtosis below the lower boundary (excess kurtosis + 2 − skewness2 = 0) cannot occur for any distribution, and hence Karl Pearson appropriately called the region below this boundary the [...] "impossible region." [...] The boundary for this [...] "impossible region" [...] {{is determined by}} (symmetric or skewed) bimodal [...] "U"-shaped distributions for which parameters α and β approach zero and hence all the probability density is concentrated at the ends: x = 0, 1 with practically nothing in between them. [...] Since for α ≈ β ≈ 0 the probability density is concentrated at the two ends x = 0 and x = 1, this [...] "impossible boundary" [...] is determined by a 2-point distribution: the probability can only take 2 values (<b>Bernoulli</b> <b>distribution),</b> one value with probability p {{and the other with}} probability q = 1−p. For cases approaching this limit boundary with symmetry α = β, skewness ≈ 0, excess kurtosis ≈ −2 (this is the lowest excess kurtosis possible for any distribution), and the probabilities are p ≈ q ≈ 1/2. [...] For cases approaching this limit boundary with skewness, excess kurtosis ≈ −2 + skewness2, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities [...] at the left end x = 0 and [...] at the right end x = 1.|$|E
2500|$|Therefore, for {{symmetric}} beta distributions, {{the excess}} kurtosis is negative, increasing from a minimum value of −2 at the limit as {α = β} → 0, and approaching a maximum value of zero as {α = β} → ∞. [...] The value of −2 is {{the minimum value}} of excess kurtosis that any distribution (not just beta distributions, but any distribution of any possible kind) can ever achieve. [...] This minimum value is reached when all the probability density is entirely concentrated at each end x = 0 and x = 1, with nothing in between: a 2-point <b>Bernoulli</b> <b>distribution</b> with equal probability 1/2 at each end (a coin toss: see section below [...] "Kurtosis bounded by {{the square of the}} skewness" [...] for further discussion). [...] The description of kurtosis as a measure of the [...] "potential outliers" [...] (or [...] "potential rare, extreme values") of the probability distribution, is correct for all distributions including the beta distribution. When rare, extreme values can occur in the beta distribution, the higher its kurtosis; otherwise, the kurtosis is lower. For α ≠ β, skewed beta distributions, the excess kurtosis can reach unlimited positive values (particularly for α → 0 for finite β, or for β → 0 for finite α) because the side away from the mode will produce occasional extreme values. [...] Minimum kurtosis takes place when the mass density is concentrated equally at each end (and therefore the mean is at the center), and there is no probability mass density in between the ends.|$|E
30|$|The {{approximate}} <b>Bernoulli</b> <b>distributions</b> {{are supported}} by the EN number of defaults and a total breakdown of the system. This shows that the system essentially randomizes between extreme states. While the probability of the negative outcome can be controlled by cross-holdings for the chosen level of bankruptcy costs and fire sales, the number of defaults in this adverse scenario is not mitigated.|$|R
40|$|In this note, {{we study}} a {{continuous}} matrix-valued Anderson-type model. Both leading Lyapounov exponents {{of this model}} are proved to be positive and distincts for all energies in (2,+∞) except those in a discrete set, which leads to absence of absolutely continuous spectrum in (2,+∞). The methods, using group theory results by Breuillard and Gelander, allow for singular <b>Bernoulli</b> <b>distributions...</b>|$|R
40|$|The {{problem of}} word {{categorisation}} is formulated {{as one of}} unsupervised mixture modelling where <b>Bernoulli</b> <b>distributions</b> capture contextual information. We detail how the free parameters of the mixture models can be estimated through an EM procedure. A deterministic word-to-class mapping is derived from this model using a hierarchical clustering algorithm. Keywords : Word categorisation, Bernoulli mixture model, clustering, language modelling. ...|$|R
50|$|While {{the related}} beta {{distribution}} is the conjugate prior {{distribution of the}} parameter of a <b>Bernoulli</b> <b>distribution</b> expressed as a probability, the beta prime distribution is the conjugate prior distribution of the parameter of a <b>Bernoulli</b> <b>distribution</b> expressed in odds. The distribution is a Pearson type VI distribution.|$|E
50|$|The Beta {{distribution}} is the conjugate prior of the <b>Bernoulli</b> <b>distribution.</b>|$|E
5000|$|Rule {{of three}} (statistics), a similar result for the <b>Bernoulli</b> <b>distribution</b> ...|$|E
40|$|We derive {{analytic}} approximations for {{the expectation}} of exit times of Exponentially Weighted Moving Average (EWMA) procedure by using the martingale technique. Based on this technique, martingale approach is able to adapt to monitoring of changes of light-tailed distributions such as Gaussian, Poisson and <b>Bernoulli</b> <b>distributions.</b> Simple procedures are addressed for obtaining the optimal design of EWMA. A comparison with Monte Carlo simulation is also presente...|$|R
5000|$|Dirichlet {{distributions}} {{are most}} commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called [...] "multinomial variables". Such a usage is liable to cause confusion, just as if <b>Bernoulli</b> <b>distributions</b> and binomial distributions were commonly conflated.) ...|$|R
40|$|Mixture {{modelling}} of class-conditional densities is {{a standard}} pattern recognition technique. Although most research on mixture models has concentrated on mixtures for continuous data, emerging pattern recognition applications demand extending research eorts to other data types. This paper focuses on the application of mixtures of multivariate <b>Bernoulli</b> <b>distributions</b> to binary data. More concretely, a text classi cation task aimed at improving language modelling for machine translation is considered...|$|R
