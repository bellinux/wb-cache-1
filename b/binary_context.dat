28|63|Public
2500|$|... 1.001b×2d11b or 1.001b×10b11b using binary numbers (or shorter 1.001×1011 if <b>binary</b> <b>context</b> is obvious). In E-notation, this {{is written}} as 1.001bE11b (or shorter: 1.001E11) {{with the letter}} E now {{standing}} for [...] "times two (10b) to the power" [...] here. In order to better distinguish this base-2 exponent from a base-10 exponent, a base-2 exponent is sometimes also indicated by using the letter B instead of E, a shorthand notation originally proposed by Bruce Alan Martin of Brookhaven National Laboratory in 1968, as in 1.001bB11b (or shorter: 1.001B11). For comparison, the same number in decimal representation: 1.125×23 (using decimal representation), or 1.125B3 (still using decimal representation). Some calculators use a mixed representation for binary floating point numbers, where the exponent is displayed as decimal number even in binary mode, so the above becomes 1.001b×10b3d or shorter 1.001B3.|$|E
5000|$|For example, in base-2 {{scientific}} notation, {{the number}} 1001b in binary (=9d) is written as 1.001b × 2d11b or 1.001b × 10b11b using binary numbers (or shorter 1.001 × 1011 if <b>binary</b> <b>context</b> is obvious). In E-notation, this is written as 1.001bE11b (or shorter: 1.001E11) {{with the letter}} E now standing for [...] "times two (10b) to the power" [...] here. In order to better distinguish this base-2 exponent from a base-10 exponent, a base-2 exponent is sometimes also indicated by using the letter B instead of E, a shorthand notation originally proposed by Bruce Alan Martin of Brookhaven National Laboratory in 1968, as in 1.001bB11b (or shorter: 1.001B11). For comparison, the same number in decimal representation: 1.125 × 23 (using decimal representation), or 1.125B3 (still using decimal representation). Some calculators use a mixed representation for binary floating point numbers, where the exponent is displayed as decimal number even in binary mode, so the above becomes 1.001b × 10b3d or shorter 1.001B3.|$|E
50|$|The initial {{objective}} of Lattice Miner was {{to focus on}} lattice drawing and visualizationeither as a flat or nested structure by {{taking into account the}} cognitive process of human beings and known principles for lattice drawing (e.g., reducing the number of edge intersections, ensuring diagram symmetry). Some well-known visualization techniques were implemented such as focus & context and fisheye view. The basic idea behind focus & context visualization paradigm is to allow a viewer to see key (important) objects in full detail in the foreground (focus) {{while at the same time}} an overview of all the surrounding information (context) remains available in the background. Lattice Miner translates the focus & context paradigm into clear and blurred elements while the size of nodes and the intensity of their color were used to indicate their importance. Various forms of highlighting, labelling and animation are also provided.In order to better handle the display of large lattices, nested line diagrams are offered in the tool. Figure 3 shows the third level of the nested line diagram corresponding to the <b>binary</b> <b>context</b> of Figure 1 where three levels of nesting are defined. Each one of the inner nodes of this diagram represents a combination of attributes from the previous two (outer) levels. Real inner concepts (see the node on the left hand-side of the diagram) are identified by colored nodes whilevoid elements are in grey color. Each node of levels 1 and 2 can be expanded to exhibit its internal line diagram. Both flat and nested diagrams can be saved as an image. Simple (flat) lattices can also be saved as an XML format file.|$|E
40|$|CORON is a {{framework}} for levelwise algorithms {{that are designed to}} find frequent and/or frequent closed itemsets in <b>binary</b> <b>contexts.</b> Datasets can be very different in size, number of objects, number of attributes, density, etc. As there is no one best algorithm for arbitrary datasets, we want to give a possibility for users to try different algorithms and choose the one that best suits their needs...|$|R
40|$|A {{significant}} cost in PPM {{data compression}} (and often the major cost) is the provision and efficient coding of escapes while building contexts. This paper presents some recent work on eliminating escapes in PPM compression, using bit-wise compression with <b>binary</b> <b>contexts.</b> It shows that PPM without escapes can achieve averages of 2. 5 bits per character on the Calgary Corpus and 2. 2 bpc on the Canterbury Corpus, both values comparing well with accepted good compressors. ...|$|R
40|$|Abstract. Standard Galois Lattices are {{effective}} tools for data analysis and knowledge discovery. Several algorithms were proposed to generate concepts of lattices, among which the ScalingNextClosure algorithm. In order {{to share the}} production workload between several processors {{when the number of}} closed itemsets to determine is very large, this algorithm leans on the sequential character of the closed itemsets determination of a Galois Lattice by the Ganter algorithm. In this paper, we prove that the parallelised version of the ScalingNextClosure can be extended to more general contexts (even some complex data) than usual <b>binary</b> <b>contexts</b> and that the partition of the workload between processors can be made with all the wished precision. ...|$|R
30|$|A fuzzy <b>binary</b> <b>context</b> (or fuzzy binary relation) is a {{fuzzy set}} defined {{on the product}} of two sets O (set of objects) and P (set of properties). Hence, X=O×P.|$|E
3000|$|..., {{relative}} to a fuzzy <b>binary</b> <b>context</b> F_BR, {{if and only if}} {x}∪ S_x is a domain of a concept of F_BR, and the closure (S_x) = {x}∪ S_x, where x∉S [...]...|$|E
30|$|Operator f {{defines the}} {{properties}} {{shared by all}} elements of A, and operator g defines objects sharing the same properties included in set B. The operators f and g define a Galois connection between the sets X and Y {{with respect to the}} <b>binary</b> <b>context</b> (X,Y,R)[12, 18].|$|E
5000|$|Strong {{conjunction}} [...] (<b>binary).</b> In the <b>context</b> of substructural logics, {{the sign}} [...] {{and the names}} group, intensional, multiplicative, or parallel conjunction are often used for strong conjunction.|$|R
5000|$|Implication [...] (<b>binary).</b> In the <b>context</b> {{of other}} than t-norm-based fuzzy logics, the t-norm-based {{implication}} {{is sometimes called}} residual implication or R-implication, as its standard semantics is the residuum of the t-norm that realizes strong conjunction.|$|R
50|$|The bits {{selected}} by these coding passes then get encoded by a context-driven binary arithmetic coder, namely the <b>binary</b> MQ-coder. The <b>context</b> of a coefficient is {{formed by the}} state of its nine neighbors in the code block.|$|R
40|$|International audienceThis paper {{describes}} {{a method to}} identify so-called ecological traits of species based on the analysis of their biological characteristics. This biological dataset has a complex structure that can be formalized as a fuzzy many-valued context and transformed into a <b>binary</b> <b>context</b> through histogram scaling. The core of the method relied on the construction and interpretation of formal concepts and was used on a 50 species x 124 histogram attributes table. The concepts were analyzed {{with the help of}} an hydrobiologist, leading to a set of ecological traits which were inserted in the original context for validation...|$|E
40|$|A new {{algorithm}} {{for context}} modeling of binary sources with application to video compression is presented. Our proposed method {{is based on}} a tree rearrangement and tree selection process for an optimized modeling of <b>binary</b> <b>context</b> trees. We demonstrate its use for adaptive context-based coding of selected syntax elements in a video coder. For that purpose we apply our proposed technique to the H. 264 /AVC standard and evaluate its performance for different sources and different quantization parameters. Experimental results show that by using our proposed algorithm coding gains similar or superior to those obtained with the H. 264 /AVC CABAC algorithm can be achieved...|$|E
40|$|We present {{worst case}} bounds for the {{learning}} rate of a known prediction method {{that is based}} on hierarchical applications of <b>binary</b> <b>Context</b> Tree Weighting (CTW) predictors. A heuristic application of this approach that relies on Huffman’s alphabet decomposition is known to achieve state-of-the-art performance in prediction and lossless compression benchmarks. We show that our new bound for this heuristic is tighter than the best known performance guarantees for prediction and lossless compression algorithms in various settings. This result substantiates the efficiency of this hierarchical method and provides a compelling explanation for its practical success. In addition, we present the results of a few experiments that examine other possibilities for improving the multi-alphabet prediction performance of CTW-based algorithms. 1...|$|E
40|$|This paper {{analyzes}} {{a spatial}} Probit model for cross sectional dependent {{data in a}} <b>binary</b> choice <b>context.</b> Observations are divided by pairwise groups and bivariate normal distributions are specified within each group. Partial maximum likelihood estimators are introduced and they are shown to be consistent and asymptotically normal under some regularity conditions. Consistent covariance matrix estimators are also provided. Finally, a simulation study shows the advantages of our new estimation procedure in this setting. Our proposed partial maximum likelihood estimators are shown to be more efficient than the generalized method of moments counterparts...|$|R
5000|$|Strong {{disjunction}} [...] (<b>binary).</b> In the <b>context</b> of substructural logics it is {{also called}} group, intensional, multiplicative, or parallel disjunction. Even though standard in contraction-free substructural logics, in t-norm fuzzy logics it is usually used only {{in the presence of}} involutive negation, which makes it definable (and so axiomatizable) by de Morgan's law from strong conjunction: ...|$|R
40|$|We {{present the}} {{summary of the}} recent {{investigations}} of double black hole <b>binaries</b> in <b>context</b> of their formation and merger rates. In particular we discuss the spectrum of black hole masses, the formation scenarios in the local Universe and the estimates of detection rates for gravitational radiation detectors like LIGO and VIRGO. Our study is based on observed properties of known Galactic and extra-galactic stellar mass black holes and evolutionary predictions. We argue that the binary black holes are the most promising source of gravitational radiation. Comment: 4 pages, 1 figure, Proceedings of the The Rencontres de Moriond and GPhyS colloquium, La Thuile, Italy, 20 - 27 March 201...|$|R
40|$|INTRODUCTION We {{describe}} {{an extension of}} Bunched Typing, or the ##- calculus, as described by O'Hearn [2], intended to increase {{the flexibility of the}} system and to suggest further possible routes for investigation of bunched type systems. The system, called #sep extends the two <b>binary</b> <b>context</b> formers of the ##-calculus to n-place context formers with arbitrary separation relations between their members. These relations express pairwise separation constraints on the resources used by the members of the context. The system then allows a slight distinction between the combination of resources and the expression of constraints between resources. This system can express certain constraints that the ##-calculus cannot, and can express other constraints in a clearer way. This system is described more fully, with categorical and functor category semantics, in [1]. This research was supported by the MRG project (IST 2001 - 33149) which is funded by the EC under the FET proactive initiative o...|$|E
40|$|Based on 26 {{interviews}} with second-generation minority professionals working in Flemish organizations, this paper explores the hybrid identity struggles of individuals trying to transcend boundaries between categories and reconcile identity positions {{that are considered}} mutually exclusive. To do so, we draw on both the literature on hybridity, describing the mixture of cultural elements held to be different, and the broader identity literature, stressing the link between identity processes and insecurity. We first show how hybrid identities are constructed {{in the midst of}} identity insecurities. Second, we illustrate how these hybrid identifications might be a new source of insecurities and identity struggles, as individuals risk falling in-between the categories they try to combine. Third, we show how such individuals try to deal with this situation by constructing narratives of richness, by trying to escape the current <b>binary</b> <b>context</b> or by accepting it as a given. In this way, we provide insight in the identity struggles of both minority individuals and boundary-crossers in general. status: publishe...|$|E
40|$|International audienceAnnotating {{data with}} {{concepts}} of an ontology {{is a common}} practice in the biomedical domain. Resulting annotations (data-concept relationships) are useful for data integration whereas the background ontology can guide the analysis of integrated data. Formal Concept Analysis (FCA) allows to build from a <b>binary</b> <b>context</b> a concept lattice {{that can be used}} for data analysis purposes. However annotated biomedical data are not binary and a binarization procedure is required as a preprocessing, coming with classical problems, e. g. a trade-o between expressivity and the large number of induced binary attributes. Interestingly, pattern structures o er a general method for building a concept lattice from any set of objects associated with partially ordered descriptions. In this paper, we show how to instantiate this general framework when the space of descriptions is based on an ontology. We illustrate our approach with the analysis of biomedical annotations and we show its capabilities for knowledge discovery...|$|E
40|$|Background: The NASA Metrics Data Program (MDP) {{data sets}} have been heavily used in {{software}} defect prediction research. Aim: To highlight the data quality issues present in these data sets, {{and the problems}} that can arise when they are used in a <b>binary</b> classification <b>context.</b> Method: A thorough exploration of all 13 original NASA data sets, followed by various experiments demonstrating {{the potential impact of}} duplicate data points when data mining. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: The bulk of defect prediction experiments based on the NASA MDP data sets may have led to erroneous findings. This is mainly due to repeated/duplicate data points potentially causing substantial amounts of training and testing data to be identical...|$|R
50|$|The context module offers all {{the basic}} {{operations}} and structures to manipulate <b>binary</b> and valued <b>contexts</b> {{as well as}} context decomposition to produce nested line diagrams. Basic context operations include apposition, subposition, generalization, clarification, reduction {{as well as the}} complementary context computation. The module provides also the arrow relations (for context reduction and decomposition) 2. The tool has an input LMB format and recognizes the binary format SLF found in Galicia and the format CEX produced by ConExp.|$|R
40|$|Recent {{advances}} {{in the development of}} interferometric instruments coupled to long baseline facilities such as the VLTI allow us to observe astronomical targets at the milli-arcsecond scale. In the context of the study of massive stars, this new tool is likely to provide crucial information on the multiplicity of massive stars, and on interactions in colliding-wind <b>binaries.</b> In this <b>context,</b> our group is leading observation campaigns with the VLTI aiming at investigating massive multiple systems. I will briefly present these campaigns and discuss prospects for observations with next generation instruments such as the VLTI-Spectro-Imager (VSI) ...|$|R
40|$|The {{concepts}} news flow, global journalism/news {{and media}} regime are under theoretical construction. News media content {{is becoming increasingly}} deterritorialized, involving complex relations and flows across national borders and continents. Consequently, it becomes more difficult to categorize news in the traditional <b>binary</b> <b>context</b> as either national or international news {{as was the case}} with news flow studies since the mid- 1990 s. These changes are perhaps most evident in centres outside the global North, where rapid development of media infrastructures, coupled with political and social shifts as a result of widespread democratization since the mid- 1990 s, have brought about complex configurations of the local/global relationship in news. Global journalism/ news is suggested as an alternative concept and the notion of media regimes is introduced as a way to interrogate assumptions about global news flows as it relates to Africa. A content analysis of TV news channels in three world regions was conducted to facilitate the analysis. © 2010 Taylor & Francis. Articl...|$|E
40|$|International audienceExtracting {{knowledge}} from huge {{data in a}} reasonable time is still a challenging problem. Most real data (structured or not) can be mapped to an equivalent <b>binary</b> <b>context,</b> with or without using a scaling method, as for extracting associations between words in a text, or in machine learning systems. In this paper, our objective {{is to find a}} minimal coverage of a relation R with formal concepts. The problem is known to be NP-complete. 1 In this paper, we exploit a particular difunctional relation embedded in any binary relation R, the fringe of R, to find an approximate conceptual coverage of R. We use formal properties of fringes to find better algorithms calculating the minimal rectangular coverage of binary relation. Here, a formal context is considered as a binary relation. By exploiting some background on relational algebra in the present work, we merge some results of Belohlavek and Vichodyl, 2 using formal concept analysis with previous results obtained by Kcherif et al. 3 using relational algebra. We finally propose decomposition algorithms based on the relational formalization and fringe relation...|$|E
40|$|National audienceAnnotating {{data with}} {{concepts}} of an ontology {{is a common}} practice in the biomedical domain. Resulting annotations, i. e., data-concept relationships, are useful for data integration whereas the reference ontology can guide the analysis of integrated data. Then the analysis of annotations can provide relevant knowledge units to consider for extracting and understanding possible cor- relations between data. Formal Concept Analysis (FCA) which builds from a <b>binary</b> <b>context</b> a concept lattice {{can be used for}} such a knowledge discovery task. However annotated biomedical data are usually not binary and a scaling procedure for using FCA is required as a prepro- cessing, leading to problems of expressivity, ranging from loss of information to the generation of a large num- ber of additional binary attributes. By contrast, pattern structures o er a general FCA-based framework for buil- ding a concept lattice from complex data, e. g., a set of objects with partially ordered descriptions. In this pa- per, we show how to instantiate this general framework when descriptions are ordered by an ontology. We illus- trate our approach with the analysis of annotations of drug related documents, and we show the capabilities of the approach for knowledge discovery...|$|E
40|$|A few weeks, maybe months… is an {{exhibition}} of photographs that contemplates the societal expectations of the gender <b>binary</b> in the <b>context</b> of travel. The work considers the intersection of manhood with my transgender identity and examines definitions of masculinity and gender. The American road trip is historically a masculine rite of passage which in my case will never fully come to pass. Discrepancies in imagery throughout the work interrupt the conventional narrative structure of the road trip. Acknowledging that the acquisition of masculinity is a futile act, the journey then becomes a complex act of reconciliation...|$|R
40|$|Event-related {{potentials}} (ERPs) {{were recorded}} while subjects performed a memory retrieval task requiring old/new judgements to visually presented old (previously studied) and new words. For words judged old, subjects made two <b>binary</b> forced-choice <b>context</b> (hereafter source) judgements, denoting the voice (male/female) and task (action/liking) {{with which the}} test word had been associated at study. By separating the ERPs according {{to the accuracy of}} the voice and task judgements, it was possible to test the prediction that the differences between ERPs to correctly identified old and new words at parietal scalp sites (parietal old/new effects) are sensitive to the amount or quality of information that is retrieved from episodic memory (Rugg, M. D., Cox, C. J. C., Doyle, M. C., Wells, T., 1995. Event-related potentials and the recollection of low and high frequency words. Neuropsychologia 33, 471 – 484). In keeping with this proposal, the magnitude of the parietal old/new effects co-varied with the number of accurate source judgements. This finding is consistent with proposals that the parietal old/new effect indexes recollection in a graded fashion...|$|R
40|$|A new simple scoring {{technique}} is {{developed in a}} <b>binary</b> supervised classification <b>context</b> when only a few observations areavailable. It consists in two steps: in the first one partial scores are obtained, one for each predictor, either categorical or continuous. Each partial score is a discrete variable with 7 values ranging from - 3 to 3, based upon an empirical comparison of the distributions for each class. In a second step the partial scores are added and standardised into a global score, which allows a decision rule. This simple {{technique is}} successfully compared with classical supervised techniques for a classical benchmark and has been proved to be especially well fitted in an industrial problem...|$|R
40|$|Phylogenetic {{networks}} are a generalisation of phylogenetic trees {{that allow for}} more complex evolutionary histories that include hybridisation-like processes. It is of considerable interest whether a network can be considered `tree-like' or not, which lead {{to the introduction of}} tree-based networks in the rooted, <b>binary</b> <b>context.</b> Tree-based {{networks are}} those networks which can be constructed by adding additional edges into a given phylogenetic tree, called the base tree. Previous extensions have considered extending to the binary, unrooted case and the nonbinary, rooted case. We extend tree-based networks to the context of unrooted, nonbinary networks in three ways, depending on the types of additional edges that are permitted. A phylogenetic network in which every embedded tree is a base tree is termed a fully tree-based network. We also extend this concept to unrooted, nonbinary phylogenetic networks and classify the resulting networks. We also derive some results on the colourability of tree-based networks, which can be useful to determine whether a network is tree-based. Comment: Primarily minor textual changes to improve clarity. Revision of Theorem 4. 3 to include star tree case, small corrections to Lemma 5. 2 and Theorem 5. 3. Added acknowledgement...|$|E
40|$|Association rules {{extraction}} from {{a binary}} relation {{as well as}} reasoning and information retrieval are generally based on the initial representation of the binary relation as an adjacency matrix. This presents some inconvenience in terms of space memory and knowledge organization. A coverage of a binary relation by a minimal number of non enlargeable rectangles generally reduces memory space consumption without any loss of information. It also {{has the advantage of}} organizing objects and attributes contained in the binary relation into a conceptual representation. In this paper, we propose new algorithms to extract association rules (i. e. data mining), conclusions from initial attributes (i. e. reasoning), as well as retrieving the total objects satisfying some initial attributes, by using only the minimal coverage. Finally we propose an incremental approximate algorithm to update a binary relation organized as a set of non enlargeable rectangles. Two main operations are mostly used during the organization process: First, separation of existing rectangles when we delete some pairs. Second, join of rectangles when common properties are discovered, after addition or removal of elements from a <b>binary</b> <b>context.</b> The objective is the minimization of the number of rectangles and the maximization of their structure. The article also raises the problems of equational modeling of the minimization criteria, as well as incrementally providing equations to maintain them...|$|E
40|$|Context {{has been}} playing an {{increasingly}} important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1 st-order) context feature is computed {{as a set of}} randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1 st-order <b>binary</b> <b>context</b> features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver. 5) [13] by 3. 3 % in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48 % to 46 % and the miss rate at 1 FPPI from 25 % to 23 %, compared with the best prior art [6]. 1...|$|E
40|$|International audienceAbstract [...] Galois lattices' (GLs) {{definition}} is defined for a <b>binary</b> table (called <b>context).</b> Therefore, {{in the presence}} of continuous data, a discretization step is needed. Discretization is classically performed before the lattice construction in a global way. However, local discretization is reported to give better classification rates than global discretization when used jointly with other symbolic classification methods such as decision trees (DTs). We present a new algorithm performing local discretization for GLs using the lattice properties. Our local discretization algorithm is applied iteratively to particular nodes (called concepts) of the GL. Experiments are performed to assess the efficiency and the effectiveness of the proposed algorithm compared to global discretization...|$|R
40|$|Paper {{presented}} at the 9 th International Conference on Heat Transfer, Fluid Mechanics and Thermodynamics, Malta, 16 - 18 July, 2012. Theoretical and experimental aspects on association phenomena generated by hydrogen bonding, dispersive and electrostatic interactions in ternary systems consisting of a proton-donor solvent (N,N-dimethylformamide or methanol), a proton-acceptor solvent (water), and a proton-acceptor polymer (polysulfone with different alkyl side chains), are investigated. In this <b>context,</b> <b>binary</b> and ternary thermodynamic interaction parameters are corrected {{on the basis of}} different association constants. Numerical values for these constants were evaluated {{as a function of the}} system composition, by mathematical simulations. As a result, mathematical simulations allow a good theoretical description of the preferential adsorption in agreement with experimental data. dc 201...|$|R
40|$|We {{discuss the}} {{gravitational}} lensing of gravitational waves from merging neutron star <b>binaries,</b> in the <b>context</b> of advanced LIGO type gravitational wave detectors. We consider {{properties of the}} expected observational data with cut on the signal-to-noise ratio ρ, i. e., ρ> ρ 0. An advanced LIGO should see unlensed inspiral events with a redshift distribution with cut-off at a redshift zmax zmax should be lensed. We compute the expected total number of events which are present due to gravitational lensing and their redshift distribution for an advanced LIGO in a flat Universe. If the matter fraction in compact lenses is close to 10 %, an advanced LIGO should see a few strongly lensed events per year with ρ> 5...|$|R
