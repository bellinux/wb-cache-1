203|0|Public
5000|$|Albeck, Y. and Konishi, M. (1995) Responses {{of neurons}} in the owl's time {{processing}} pathway to partially <b>binaurally</b> correlated signals. J. Neurophysiol. 74: 1689-1700.|$|E
50|$|The sensors in the PAL detect correct {{non-nutritive sucking}} {{characteristics}} and activate a CD player which reproduces lullabies through small speakers placed <b>binaurally</b> in the incubator above the infant's head.|$|E
50|$|ABR {{typically}} uses click or tone-burst stimuli {{in one ear}} at a time, but ASSR can be used <b>binaurally</b> while evaluating broad bands or four frequencies (500, 1k, 2k, & 4k) simultaneously.|$|E
50|$|The {{cochlear}} nerve spans from the cochlea {{of the inner}} ear to the ventral cochlear nuclei located in the pons of the brainstem, relaying auditory signals to the superior olivary complex where {{it is to be}} <b>binaurally</b> integrated.|$|E
50|$|ZBS did a 1984-85 radio series, The Cabinet of Dr. Fritz, later {{releasing}} some {{shows in}} the series on cassettes and CDs. These productions were recorded <b>binaurally</b> using a Neumann Ku81 Kunstkopf microphone. ZBS also produced a widely acclaimed dramatization of Stephen King's The Mist recording in binaural sound.|$|E
5000|$|Binaural fusion or {{binaural}} {{integration is}} a cognitive process {{that involves the}} [...] "fusion" [...] of different auditory information presented <b>binaurally,</b> or to each ear. In humans, this process is essential in understanding speech as one ear may pick up {{more information about the}} speech stimuli than the other.|$|E
5000|$|HearFones is a {{patented}} {{instrument that}} enables a person speaking or singing {{to hear the}} actual sound of the voice they are producing. [...] HearFones fit over the head and redirect sound <b>binaurally</b> from the mouth to the ears using ellipsoidal reflectors, one focus of {{which is at the}} user's mouth and the other focus at the ear canals.|$|E
5000|$|Jarvis Cocker from Pulp {{recorded}} {{a show in}} the village with Master Musicians of Joujouka for the fifth season of his Wireless Nights series for BBC Radio 4. It was first broadcast on 13th March 2017 and is available on podcast. It was recorded <b>binaurally</b> and was championed by The Guardian as [...] "the best radio of the week".|$|E
50|$|As in the Dalmatian, white Boxer, and {{the white}} Bull Terrier, the dogo may {{experience}} pigment-related deafness. There is possibility of an approximate 10% deafness rate overall with some dogos afflicted uniaurally (one deaf ear) and some <b>binaurally</b> (deaf in both ears). Studies {{have shown that the}} incidence of deafness is drastically reduced when the only breeding stock used is that with bilaterally normal hearing.|$|E
50|$|Brüel & Kjær has {{developed}} an NVH tool for surreptitious recording and analysis called SoNoScout. It can be carried in the tester’s pocket on a train or bus without distracting the passengers or in a competitor’s vehicle {{where it can be}} combined with a GPS unit to generate a map of the test route, measure speed and even generate an RPM profile. The software runs on a PDA with microphones <b>binaurally</b> mounted on a headset for quick recording.|$|E
5000|$|Pitch-shifting frequency-altered {{auditory}} feedback (FAF) changes the pitch {{at which the}} user hears his or her voice. Varying pitch from quarter, half or full octave shift typically results in 55-74% decreases stuttering in short reading tasks. Individuals differ as to direction and extent of the pitch shift required to maximally reduce stuttering. [...] In studies that gave longer exposure to FAF and used more meaningful daily life tasks such as generating a monologue, only some participants experienced a reduction in stuttering. Initial claims that AAF was more powerful than FAF in reducing stuttering have not been supported by subsequent research. [...] FAF is, like DAF, more effective when presented <b>binaurally.</b> In the last years a number of smart phone apps have been developed that implement DAF/FAF as software and are much cheaper than the special hardware devices, see External links.|$|E
50|$|ST models {{include a}} {{non-contact}} optical sensing system, featuring continuous grayscale shutters for each key and optical window style shutters on each hammer. Optical sensors {{are also used}} for the damper, soft and sostenuto pedals. This sensor system allows the user to natively capture their own performance in standard MIDI format, {{without the need for}} external or special software. In addition, a “Silent System” that does not require special installation or instrument modification is added to allow for headphone connectivity and access to the instrument’s digital sounds, which include a special <b>binaurally</b> captured CFX Concert Grand sample. Because piano components and solenoids can be affected by environmental changes, a patented DSP servo drive system that monitors and controls key and pedal movement to ensure accurate performance reproduction is active during playback. This DSP system provides feedback to the instrument’s processor effectively making the system a “closed-loop”. If the system detects any physical movement that does not correlate with the provided performance data, it will automatically adjust itself to correct any deviation in real-time.|$|E
5000|$|The {{effect of}} delayed {{auditory}} feedback (DAF) in reducing stuttering has been noted since the 1950s. [...] A DAF user hears his or her voice in headphones, delayed {{a fraction of a}} second. Typical delays are in the 50 millisecond to 200 millisecond range. In stutterers, DAF may produce slow, prolonged but fluent speech. In the 1960s to 1980s, DAF was mainly used to train prolongation and fluency. As the stutterer masters fluent speech skills at a slow speaking rate, the delay is reduced in stages, gradually increasing speaking rate, until the person can speak fluently at a normal speaking rate. [...] It was not until the 1990s that research began to focus on DAF in isolation. Recent studies have moved from longer delays to shorter delays in the 50 millisecond to 75 millisecond range, and have found that speakers can maintain fast rates and achieve increased fluency at these delays. Delayed auditory feedback presented <b>binaurally</b> (i.e. in both ears) is more effective than that presented in monaurally, or in one ear only.|$|E
50|$|As sound travels {{into the}} inner eardrum of {{vertebrate}} mammals, it encounters the hair cells that line the basilar membrane of the cochlea in the inner ear. The cochlea receives auditory information to be <b>binaurally</b> integrated. At the cochlea, this information is converted into electrical impulses that travel {{by means of the}} cochlear nerve, which spans from the cochlea to the ventral cochlear nucleus, which is located in the pons of the brainstem. The lateral lemniscus projects from the cochlear nucleus to the superior olivary complex (SOC), a set of brainstem nuclei that consists primarily of two nuclei, the medial superior olive (MSO) and the lateral superior olive (LSO), and is the major site of binaural fusion. The subdivision of the ventral cochlear nucleus that concerns binaural fusion is the anterior ventral cochlear nucleus (AVCN). The AVCN consists of spherical bushy cells and globular bushy cells and can also transmit signals to the medial nucleus of the trapezoid body (MNTB), whose neuron projects to the MSO. Transmissions from the SOC travel to the inferior colliculus (IC) via the lateral lemniscus. At the level of the IC, binaural fusion is complete. The signal ascends to the thalamocortical system, and sensory inputs to the thalamus are then relayed to the primary auditory cortex.|$|E
40|$|THE ACCESSORY, or medial {{nucleus of}} the {{superior}} olivary complex is the first nucleus in the classical ascending auditory pathway that receives bilateral input. Both the anatomical structure and electrophysiological responses of the accessory nucleus {{suggest that it is}} a locus of binaural signal processing. The cells of the accessory nucleus {{have been shown to be}} responsive to interaural time and intensity disparities of <b>binaurally</b> presented clicks (5, 6) and to the interaural phase relations of <b>binaurally</b> presented tones (11). The field potentials of the accessory nucleus evoked by click and pure-tone stimuli have been correlated with the anatomical distribution of synaptic input from the two ears (3, 5, 17). Evidence of binaural interaction in the slowwave responses of the superior olivary complex to <b>binaurally</b> presented clicks was found by Rosenzweig and Amon (13). In the present study the field potentials of the superior olivary complex evoked by <b>binaurally</b> presented pure tones were analyzed to define further the mechanisms of binaural interaction...|$|E
40|$|The {{subjective}} {{representation of}} the sounds delivered to the two ears of a human listener is {{closely associated with the}} interaural delay and correlation of these two-ear sounds. When the two-ear sounds, e. g., arbitrary noises, arrive simultaneously, the single auditory image of the <b>binaurally</b> identical noises becomes increasingly diffuse, and eventually separates into two auditory images as the interaural correlation decreases. When the interaural delay increases from zero to several milliseconds, the auditory image of the <b>binaurally</b> identical noises also changes from a single image to two distinct images. However, measuring the effect of these two factors on an identical group of participants has not been investigated. This study examined the impacts of interaural correlation and delay on detecting a binau-rally uncorrelated fragment (interaural correlation = 0) embedded in the <b>binaurally</b> correlated noises (i. e., binaural gap or break in interaural correlation). We found that the minimum duration of the binaural gap for its detection (i. e., duration threshold) increased exponen-tially as the interaural delay between the <b>binaurally</b> identical noises increased linearly from 0 to 8 ms. When no interaural delay was introduced, the duration threshold also increase...|$|E
3000|$|... {{external}} point sources. This {{is performed}} here <b>binaurally,</b> that is, by combining signals from both ears (see Figure 1). The binaural filtering operations {{can be described}} {{by a set of}} four [...]...|$|E
40|$|Abstract—The {{purposes}} of this investigation were two-fold: 1) to prospectively investigate the effect of pro-longed lack of binaural amplification in the unaided ears of adults with bilaterally symmetrical sensorineural hear-ing impairment (BSSHI) fitted monaurally; and, 2) to prospectively investigate the effects of amplification on speech-recognition performance in the aided ears of monaurally and <b>binaurally</b> fitted subjects. Subjects con-sisted of 19 monaurally aided adults, 28 <b>binaurally</b> aided adults, and 19 control adults. Both ears of the experimen-tal subjects (<b>binaurally</b> and monaurally aided adults) had BSSHI. The speech measures included the W- 22 CID suprathreshold speech-recognition test, nonsense syllable test, and speech-perception-in-noise test. Initial testing was done between 6 and 12 weeks following hearing-aid fitting. Retests were performed approximately 1 year following the initial test. The results revealed that the mean aided minus unaided ear score for the nonsense syllable and W- 22 tests increased significantly from the initial test to retest, reflecting a slight improvement in speech performance in the aided ear and a slightly greater decrement in the unaided ear. The findings were inter-preted {{with respect to the}} theories of auditory deprivation and acclimatization. Key words: acclimatization, auditory deprivation, binau-ral amplification, monaural amplification, nonsense sylla-Address all correspondence and requests for reprints to: Shlomo Silman...|$|E
40|$|The {{investigations}} {{focused on}} the binaural perception of amplitude modulated (AM) and frequency modulated (FM) signals. They are comprised of two experiments. In the first experiment <b>binaurally</b> perceived (matched) modulation depth for AM signals was determined under diotic conditions (i. e. for the same values of modulation depth coeffi-cient, m, presented to the left (ml) and right (mr) ears) and under dichotic conditions (i. e. for different values of these coefficients ml 6 = mr). The measurements were made for the interaural differences in modulation depth coefficient ∆m, changing from 0 to 100 % and a few selected modulating frequencies (4, 64 and 128 Hz) and carrier frequencies (250 and 1000 Hz). In the second experiment <b>binaurally</b> perceived (matched) frequency devi-ation of FM signals was determined under diotic conditions (i. e. for the same values of frequency deviation, ∆f, presented to the left (∆fl) and right (∆fr) ear (∆fl = ∆fr) and under dichotic conditions (i. e. for different values of this deviation (∆fl 6 = ∆fr)). The measurements were made for the interaural differences of frequency deviation changing from 0 to 20 Hz; a few selected modulating frequencies (32, 64 and 128 Hz) and carrier frequencies (500 and 1000 Hz). It was found in Experiment I that for small interaural differences in modulation depth, ∆m, the <b>binaurally</b> perceived modulation depth, m, {{is equal to the}} arithmetic mean of the depths presented to the left and right ears, whereas for large values of ∆m, the value of m is smaller than the mean. The results of Experi-ment II revealed that the <b>binaurally</b> perceived frequency deviation is a linear function of interaural differences of this deviation and is equal to the arithmetic mean of deviations presented to the left and right ears. 1...|$|E
40|$|A {{framework}} for comparative studies of <b>binaurally</b> resynthesized acoustical environments is presented. It {{consists of a}} software-controlled, automated head and torso simulator with multiple degrees of freedom, an integrated measurement device for the acquisition of binaural impulse responses in high spatial resolution, a head-tracked realtime convolution software capable to render multiple acoustic scenes at a time, and a user interface to conduct listening tests according to different test designs. Methods to optimize the measurement process are discussed, as well as different approaches to datareduction. Results of a perceptive evaluation of the system are shown, where acoustical reality and binaural resynthesis of an acoustic scene were confronted in direct A/B comparison. The framework permits, for the first time, to study the perception of a listener instantaneously relocated to different <b>binaurally</b> rendered acoustical scenes...|$|E
40|$|Ambisonic {{encodings}} can {{be rendered}} <b>binaurally</b> {{as well as}} for speaker arrays. This process is developed for general high-order Ambisonic encodings of soundfields containing near as well as far sources. For sufficently near sources an error is identified resulting from the limited field of validity of the freefield harmonic expansion. A modified expansion is derived that can render such sources correctly...|$|E
40|$|Binaural {{interaction}} was examined by recording human auditory brainstem responses to clicks from scalp electrodes. Deviations of <b>binaurally</b> evoked {{responses from the}} sum of monaurally evoked potentials were observed during waves IV through VI. Amplitude and latency of the interactions depended on click polarity: condensation clicks produced interactions of larger magnitude and longer latency than did rarefaction clicks. Latency differences cannot {{be accounted for by}} small latency shifts of the components of monaurally or <b>binaurally</b> evoked potentials resulting from changes in click polarity. Binaural interaction amplitude decreased as click intensity decreased and interaural delay increased. Attenuation of binaural interaction with interaural time differences was maximal at an interaural delay of 900 μs. Latency of {{interaction was}} prolonged in one subject with low- and high-frequency hearing loss; latency of binaural interaction in subjects with only high-frequency hearing loss was normal. These results suggest that binaural interaction in these potentials reflects binaural processing of low-frequency acoustic stimulation. © 1981 American Medical Association All rights reserved...|$|E
30|$|In [16], some {{experiments}} were {{performed with the}} aim to characterize the influence of sound quality, sound information, and sound localization on users' self-ratings of presence. The sounds used in their study were mainly <b>binaurally</b> recorded ecological sounds, that is, footsteps, vehicles, doors, and so forth. It was found that especially two factors had high positive correlation with sensed presence: sound information and sound localization.|$|E
40|$|A soundscape {{assessment}} method that {{is suitable for}} the automatic categorization of <b>binaurally</b> recorded sound in urban public places is presented. Soundscape categories are established {{as a result of}} an automatic clustering lgorithm based on multi-parameter analysis by 13 acoustical parameters used as similarity measures, on a large set of sound recordings. One of the main advantages of the followed approach allows to take into account an optimized set of parameters that are judged relevant and necessary for an appropriate description of the sampled acoustical scenarios. The Euclidian distance based clustering of the 370 recordings of typical situations based on these parameters, allows to categorize each <b>binaurally</b> recorded sound sample into one of 20 proposed clusters (soundscape categories). The common features among members within each cluster allow to identify ‘‘how the acoustical scenario of the members sounds like’’. The hybrid use of an optimized set of standard acoustical quantities, such as sound pressure level, together with well known psychoacoustical parameters that directly relate to human perception of sound, makes the propose method very robust. status: publishe...|$|E
40|$|Interaural time {{difference}} (ITD) arises whenever a sound {{outside of the}} median plane arrives at the two ears. There is evidence that ITD in the rapidly varying fine structure of a sound is most important for sound localization and for understanding speech in noise. Cochlear implants (CIs), neural prosthetic devices that restore hearing in the profoundly deaf, are increasingly implanted to both ears to provide implantees with the advantages of binaural hearing. CI listeners {{have been shown to}} be sensitive to fine structure ITD at low pulse rates, but their sensitivity declines at higher pulse rates that are required for speech coding. We hypothesize that this limitation in electric stimulation is at least partially due to binaural adaptation associated with periodic stimulation. Here, we show that introducing <b>binaurally</b> synchronized jitter in the stimulation timing causes large improvements in ITD sensitivity at higher pulse rates. Our experimental results demonstrate that a purely temporal trigger can cause recovery from binaural adaptation. Thus, <b>binaurally</b> jittered stimulation may improve several aspects of binaural hearing in bilateral recipients of neural auditory prostheses...|$|E
40|$|Copyright © 2008 by the American Physiological Society. 2 Survival {{in natural}} environments for small {{animals such as}} rats often depends upon precise neural coding of {{life-threatening}} acoustic signals, and binaural unmasking of species-specific pain calls is especially critical. This study investigated how speciesspecific tail-pain chatter is represented in the rat amygdala, which receives afferents from both auditory thalamus and auditory association cortex, and whether the amygdaloid representation of the chatter can be <b>binaurally</b> unmasked. The results show that chatter with a fundamental frequency (F 0) of 2. 1 kHz was able to elicit salient phase-locked frequencyfollowing responses (FFRs) in the lateral amygdala nucleus in anesthetized rats. FFRs to the F 0 of <b>binaurally</b> presented chatter were sensitive to the interaural time difference (ITD), with the preference of ipsilateral-ear leading, as well as showing features of binaural inhibition. When interaurally correlated masking noises were added and ipsilateral chatter led contralateral chatter, introducing an ITD disparity between the chatter and masker significantly enhanced (unmasked) the FFRs. This binaural unmasking was furthe...|$|E
40|$|Noise {{annoyance}} {{in the cab}} {{of mobile}} work machines was investigated by subjective listening tests. A number of psychoacoustic indices were fitted into the test results by a linear regression model. A binaural system for evaluating sound quality was developed. It consists of tools and methods on how to record, edit using digital signal processing, and present sound samples <b>binaurally</b> to many subjects simultaneously. Also, a technique was developed for making binaural recordings using the real human head. 1...|$|E
40|$|This {{submission}} {{includes a}} MATLAB script and function, input and example output sound files {{as well as}} an accompanying text document. The aim of this project was to source exiting MATLAB code which were able to take a single channel sound and reproduce it <b>binaurally</b> (over headphones) in a way that simulates the motion of a sound source in a straight line as it passes in front of an observer. Architecture & Allied ArtsDESC 9115 : Digital Audio System...|$|E
40|$|Presented at the 22 nd International Conference on Auditory Display (ICAD- 2016) Attention {{redirection}} {{trials were}} carried out using a wearable interface incorporating auditory and visual cues. Visual cues were delivered via the screen on the Recon Jet - a wearable computer resembling a pair of glasses - while auditory cues were delivered over a bone conduction headset. Cueing conditions included the delivery of individual cues, both auditory and visual, and in combination with each other. Results indicate {{that the use of}} an auditory cue drastically decreases target acquisition times. This is true especially for targets that fall outside the visual field of view. While auditory cues showed no difference when paired with any of the visual cueing conditions for targets within the field of view of the user, for those outside the field of view a significant improvement in performance was observed. The static visual cue paired with the <b>binaurally</b> spatialised, dynamic auditory cue appeared to provide the best performance in comparison to any other cueing conditions. In the absence of a visual cue, the <b>binaurally</b> spatialised, dynamic auditory cue performed the best...|$|E
40|$|Simulations of {{monaural}} cochlear implants in normal-hearing listeners {{have shown}} that the deleterious effects of upward spectral shifting on speech perception can be overcome with training. This thesis examines whether the same is true when simulating bilateral stimulation. Can listeners adapt to upward-shifted speech information presented together with contralateral unshifted information? In two series of experiments, perceptual adaptation was investigated for both speech in quiet with a large interaural spectral mismatch, and speech in noise with a moderate interaural spectral mismatch. For speech in quiet, a six-channel dichotic sine-carrier vocoder simulated the <b>binaurally</b> mismatched frequency-to-place map. Odd channels were presented to one ear with an upward shift equivalent to a 6 mm basilar membrane distance, while even channels were presented to the contralateral ear unshifted. For speech in noise, the number of vocoded channels was increased to ten, and the upward spectral shift applied to the odd channels was decreased to 3. 8 mm. Prior to vocoding, speech was combined with speech-shaped noise at a signal-to-noise ratio of 10 dB (or 0 dB for vowels). Listeners were trained with Connected Discourse Tracking for 5. 3 hours or 10 hours, with the <b>binaurally</b> mismatched processor and/or just the shifted monaural bands. Speech perception was tested with sentence and vowel tests before, during and after training. Listeners showed adaptation to the upwardly shifted speech, but for nearly every speech test, intelligibility with the <b>binaurally</b> mismatched processor matched intelligibility with just the unshifted bands. Consistent with earlier findings with monaural spectral shifts, then, this research suggests that listeners are capable of adapting to a spectral shift, even in the presence of background noise. However, they appear to be resistant to integrating mismatches in frequency-place maps between the ears. A theory of “better ear” listening is proposed to account for this resistance. The findings are consistent with psychophysical studies of binaural hearing, which show maximal ITD and ILD sensitivity for similar interaural cochlear places. In optimizing bilateral cochlear implants for speech perception, it may thus be important to keep frequency-to-place maps similar in the two ears...|$|E
40|$|This paper {{presents}} a system developed at NASA Langley Research Center to render aircraft flyovers {{in a virtual}} reality environment. The present system uses monaural recordings of actual aircraft flyover noise and presents these <b>binaurally</b> using head tracking information. The three-dimensional audio is simultaneously rendered with a visual presentation using a headmounted display (HMD). The final system will use flyover noise synthesized using data from various analytical and empirical modeling systems. This will permit presentation of flyover noise from candidate low-noise flight operations to subjects for psychoacoustical evaluation. 1...|$|E
40|$|The current {{research}} explored pseudoneglect for the mental representation of real-world scenes generated from aural-verbal description in the {{complete absence of}} direct visual processing. Healthy participants listened <b>binaurally</b> or monaurally to aural-verbal descriptions of novel real-world scenes with familiar landmarks (e. g., 'shop', 'cafe', 'school') to be imagined on the left- or right-hand side. Participants were asked to mentally represent the street scene using a visuospatial template though {{it was up to}} participants how they mentally represented each individual landmark within the street (i. e., in terms of colour and size). There were two main tasks: a relative judgement task (which side of the street contains the most landmarks?) and a recall task (recall the landmarks on the left vs. right side of the street). When stimuli were presented monaurally to the left ear (favouring the activation of the right hemisphere) participants demonstrated representational pseudoneglect and showed a bias towards responding that there were more landmarks on the left compared to the right. However, this did not lead to enhanced recall for left side landmarks. When stimuli were presented <b>binaurally</b> or monaurally to the right ear, {{there was no evidence of}} representational pseudoneglect for the relative judgement or recall task. The current study discusses how the use of monaural presentation may boost right hemisphere activation in aural-verbal experimental paradigms designed to explore representational pseudoneglect. Joanna L. Brooks, Maria A. Brandimont...|$|E
40|$|Applying whole-head {{functional}} {{magnetic resonance}} imaging (fMRI) in 11 neurologically intact subjects, hemodynamic responses to mon- or <b>binaurally</b> presented auditory stimuli were measured. To expand on previous studies in this research area, we used tones and consonant–vowel (CV) syllables. In one group of subjects (n= 6) the perceived loudness of the monaurally presented stimuli were adjusted so that they matched the loudness of the <b>binaurally</b> presented stimuli. In a second group (n= 5) no loudness adjustment was performed, thus the monaural stimuli were perceived less loud (∼ 10 dB) than the binaural stimuli. These extensions allowed us to test whether CV syllables and tones produce different contralaterality effects (stronger hemodynamic responses in the auditory cortex contralateral to the stimulated ear) and whether binaural stimulation results in stronger activations in the auditory areas than during both monaural stimulation conditions (binaural summation) independent of loudness influences. In summary, we obtained the following findings: (1) strong contralaterality effects during monaural acoustic stimulation in the posterior superior temporal gyrus (STG) comprising the planum temporale and the dorsal bank of the superior temporal sulcus to CV syllables and tones; (2) the hemodynamic responses to contralaterally presented stimuli (during the monaural conditions) were mostly stronger than those to <b>binaurally</b> presented CV syllables; (3) there was no interaction between stimulus type {{and the size of the}} contralaterality effect; (4) there was no indication of binaural summation, rather we found stronger hemodynamic responses to the sum of both monaural stimulations (right and left ear) than to binaural stimulation in all auditory areas; (5) there were generally stronger hemodynamic responses to CV syllables than to tones in the posterior STG, while the hemodynamic responses to tones were stronger in the anterior part of the STG (temporal pole); and finally (6) there was no general difference in terms of hemodynamic response in the auditory cortex between the two groups when receiving either loudness-matched or non-loudness-matched monaural stimulation. These findings are discussed in the context of the underlying neurophysiological mechanisms, the peculiarities of functional fMRI, and the direct access and callosal relay models of hemispheric lateralization...|$|E
30|$|On {{the other}} hand, the {{synthesis}} stage does not assume a virtual loudspeaker setup nor makes a different treatment between diffuse and nondiffuse components. This makes the synthesis processing even more simple than in DirAC. In fact, in our method, diffuseness information {{is assumed to}} be inherently encoded by the DOA estimates since the variance found on the directional information over the time-frequency domain is already a representation of the diffuseness characteristics of the recorded sound. In this context, {{there is no need for}} assuming a specific loudspeaker reproduction setup since each time-frequency element is <b>binaurally</b> reproduced according to its estimated direction.|$|E
40|$|Wet Day (2004) is a fixed medium acousmatic work {{commissioned}} by Reeling &# 38; Writhing Theatre {{to accompany the}} play Standing Wave - Delia Derbyshire in the ' 60 s, performed at the Tron Theatre, Glasgow in 2004. The play {{told the story of}} BBC Radiophonic Workshop composer Delia Derbyshire, with music by Pippa Murphy. Along with the play ran several standalone works intended as homage to the playfulness and adventure of Delia’s sonic spirit, of which Wet Day was one. The work is based on recordings of a rainy day in Glasgow, recorded <b>binaurally</b> from the inside of a hood...|$|E
40|$|Four studies {{constitute}} {{this series}} of experiments in which profoundly deaf children were trained to discriminate linearly amplified speech and speech coded by three different processes. The purpose of the experiments {{was to determine whether}} subjects could acquire better speech discrimination skills with frequency transposed (Coded) speech than with conventional linear amplification. In none of the four studies were discrimination scores for coded speech significantly different from scores for linearly amplified speech. In the final experiment, coded speech to the right ear and linearly amplified speech to the left did not lead to better results than either form of amplification presented <b>binaurally...</b>|$|E
