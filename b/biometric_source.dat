13|57|Public
40|$|This work {{presents}} {{an approach to}} integrate <b>biometric</b> <b>source</b> weighting in the calculation of neighbors distance ratios to be used within a classification-based multi-biometric fusion process. The neighbors distance ratio represents the elevation of the top ranked identification match to the following ranks. Using <b>biometric</b> <b>source</b> weighing can help achieve more accurate initial identity ranking necessary for neighbors distance ratios. It also influences the effect of each <b>biometric</b> <b>source</b> on the ratios values. The proposed approach is developed and evaluated using the Biometric Scores Set BSSR 1 database. The results are presented in the verification scenario as receiver operating curves (ROC). The achieved performance is compared {{to a number of}} baseline solutions and a satisfying and stable performance was achieved with a clear benefit of integrating the <b>biometric</b> <b>source</b> weights...|$|E
40|$|We {{describe}} {{a method of}} cryptographically-secure key extraction from a noisy <b>biometric</b> <b>source.</b> The computational security of our method can be clearly argued through hardness of Learning Parity With Noise (LPN). We use a fuzzy commitment scheme so the extracted key is cho-sen by definition to have uniformly random bits. The <b>biometric</b> <b>source</b> is used as the noise term in the LPN problem. A key idea in our construction is to use additional ‘confidence ’ information produced by the source for polynomial-time key recovery even under high-noise settings, i. e., Θ(m) errors, where m {{is the number of}} biometric bits. The confidence information is never exposed and is used as a noise-avoiding trapdoor to exponentially reduce key recovery complexity. Previous computational fuzzy extractors were unable to correct Θ(m) errors or would run in exponential time in m. A second key result is that we relax the requirement on the noise in the LPN problem, which relaxes the requirement on the <b>biometric</b> <b>source.</b> Through a reduction argument, we show that in the LPN problem, correlation in the bits generated by the <b>biometric</b> <b>source</b> {{can be viewed as a}} bias on the bits, which affects the security parameter, but not the security of the overall construction. Using a silicon Physical Unclonable Function (PUF) as a concrete example, we show how keys can be extracted securely and efficiently even under extreme environmental variation...|$|E
40|$|The {{topic of}} {{multi-modal}} biometrics has attracted {{strong interest in}} recent years. This paper categorizes approaches to multi-modal biometrics based on the <b>biometric</b> <b>source,</b> the type of sensing used, {{and the depth of}} collaborative interaction in the processing. This paper also attempts to identify some of the challenges and issues that confront research in multimodal biometrics. 1...|$|E
40|$|In 1999 Juels and Wattenberg {{introduced}} the fuzzy commitment scheme. Fuzzy commitment {{is a particular}} realization of a binary biometric secrecy system with a chosen secret key. Three cases of <b>biometric</b> <b>sources</b> are considered, i. e. memory- less and totally-symmetric <b>biometric</b> <b>sources,</b> memoryless and input-symmetric <b>biometric</b> <b>sources,</b> and memoryless <b>biometric</b> <b>sources.</b> It is shown that fuzzy commitment is only optimal for memoryless totally-symmetric <b>biometric</b> <b>sources</b> and only at the maximum secret-key rate. Moreover, it is demonstrated that for memoryless <b>biometric</b> <b>sources,</b> which are not input-symmetric, the fuzzy com- mitment scheme leaks information on both the secret key and the biometric data...|$|R
40|$|Abstract. In this study, two {{techniques}} that {{can improve the}} authentication process are examined: (i) multiple samples and (ii) multiple <b>biometric</b> <b>sources.</b> We propose the fusion of multiple samples obtained from multiple <b>biometric</b> <b>sources</b> at the score level. By using the average operator, both the theoretical and empirical results show that integrating as many samples and as many <b>biometric</b> <b>sources</b> as possible can improve the overall reliability of the system. This strategy is called multi-sample multi-source approach. This strategy was tested on a real-life database using neural networks trained in one-versus-all configuration...|$|R
40|$|Multi-biometrics aims at {{building}} more accurate unified biometric {{decisions based on}} the information provided by multiple <b>biometric</b> <b>sources.</b> Information fusion is used to optimize the process of creating this unified decision. In previous works dealing with score-level multi-biometric fusion, the scores of different <b>biometric</b> <b>sources</b> belonging to the comparison of interest are used to create the fused score. This is usually achieved by assigning static weights for the different <b>biometric</b> <b>sources</b> with more advanced solutions considering supplementary dynamic information like sample quality and neighbours distance ratio. This work proposes embedding score coherence information in the fusion process. This is based on our assumption that a minority of <b>biometric</b> <b>sources,</b> which points out towards a different decision than the majority, might have faulty conclusions and should be given relatively smaller role in the final decision. The evaluation was performed on the BioSecure multimodal biometric database with different levels of simulated noise. The proposed solution incorporates, and was compared to, three baseline static weighting approaches. The enhanced performance induced by including the coherence information within a dynamic weighting scheme in comparison to the baseline solution was shown by the reduction of the equal error rate by 45 % to 85 % over the different test scenarios and proved to maintain high performance when dealing with noisy data...|$|R
40|$|Template {{protection}} {{techniques are}} used within biometric systems {{in order to}} protect the stored biometric template against privacy and security threats. A great portion of template protection techniques are based on extracting a key from, or binding a key to the binary vector derived from the biometric sample. The size of the key plays an important role, as the achieved privacy and security mainly depend on the entropy of the key. In the literature, it can be observed that there is a large variation on the reported key lengths at similar classification performance of the same template protection system, even when based on the same biometric modality and database. In this work, we determine the analytical relationship between the classification performance of the fuzzy commitment scheme and the theoretical maximum key size given as input a Gaussian <b>biometric</b> <b>source.</b> We show the effect of the system parameters such as the <b>biometric</b> <b>source</b> capacity, the number of feature components, the number of enrolment and verification samples, and the target performance on the maximum key size. Furthermore, we provide an analysis of the effect of feature interdependencies on the estimated maximum key size and classification performance. Both the theoretical analysis, as well as an experimental evaluation using the MCYT fingerprint database showed that feature interdependencies have a large impact on performance and key size estimates. This property can explain the large deviation in reported key sizes in literature...|$|E
40|$|Multi-{{biometric}}s {{tries to}} build a unified biometric decision based on multiple biometric sources {{in an effort to}} gain more accuracy and robustness. Multi-biometric fusion aims at optimally combining the information produced by the multiple biometric sources, this usually requires assigning relative weights for the biometric sources to optimize their effect on the final decision. This work presents a new approach for biometric sources weighting within a score-level multi-biometric system. The presented solution tries to investigate the properties of the cumulative match characteristic (CMC) curve, which represents the biometric performance under the identification scenario, and extract <b>biometric</b> <b>source</b> weights based on those properties. The proposed solution is evaluated along with a set of state of the art and best practice weighting techniques. The evaluation was performed on the Biometric Scores Set BSSR 1 database and a satisfying and stable performance was achieved...|$|E
40|$|This work {{presents}} a new weighting algorithm for biometric sources within a score-level multi-biometric system. Those weights {{are used in}} the effective and widely used weighted sum fusion rule to produce multi-biometric decisions. The presented solution is mainly based on the characteristic of the overlap region between the genuine and imposter scores distributions. It also integrates the performance of the <b>biometric</b> <b>source</b> represented by its equal error rate. This solution aims at avoiding the shortcomings of previously proposed solutions such as low generalization abilities and sensitiveness to outliers. The proposed solution is evaluated along with {{the state of the art}} and best practice techniques. The evaluation was performed on two databases, the Biometric Scores Set BSSR 1 and the Extended Multi Modal Verification for Teleservices and Security applications database and a satisfying and stable performance was achieved...|$|E
40|$|Multi-biometrics aims at {{building}} more accurate unified biometric {{decisions based on}} the information provided by multiple <b>biometric</b> <b>sources.</b> Information fusion is used to optimize the process of creating this unified decision. In previous works dealing with score-level multibiometric fusion, the scores of different <b>biometric</b> <b>sources</b> belonging to the comparison of interest are used to create the fused score. This is usually achieved by assigning static weights for the different <b>biometric</b> <b>sources.</b> In contrast, we focus on integrating the information imbedded in the relative relation between the comparison scores (within a 1 :N comparison) in the biometric fusion process using a dynamic weighting scheme. This is performed by considering the neighbors distance ratio in the ranked comparisons to influence the dynamic weights of the fused scores. The evaluation was performed on the Biometric Scores Set BSSR 1 database. The enhanced performance induced by including the neighbors distance ratio information within a dynamic weighting scheme in comparison to the baseline solution was shown by an average reduction of the equal error rate by more than 40 % over the different test scenarios...|$|R
40|$|Abstract: Multimodal {{biometrics}} provides high recognition {{accuracy and}} population coverage by combining different <b>biometric</b> <b>sources.</b> However, some multimodal biometrics may obtain smaller-than-expected improvement of recognition accuracy if the combined <b>biometric</b> <b>sources</b> are dependent {{in terms of}} a false acceptance by mistakenly perceiving biometric features from two different persons as being from the same person. In this paper, we propose our multimodal biometric prototype that captures a palm vein and three fingerprints simultaneously and we evaluate whether or not their combination is statistically independent. By evaluating false acceptance using the palm vein images and the fingerprint images collected with our prototype, we confirmed that the combination of the palm vein and the fingerprints is almost independent. ...|$|R
40|$|Multi-biometrics aims at {{building}} more accurate unified biometric {{decisions based on}} the information provided by multiple <b>biometric</b> <b>sources.</b> Information fusion is used to optimize the process of creating this unified decision. In previous works dealing with score-level multibiometric fusion, the scores of different <b>biometric</b> <b>sources</b> belonging to the comparison of interest are used to create the fused score. The novelty of this work focuses on integrating the relation of the fused scores to other comparisons within a 1 :N comparison. This is performed by considering the neighbors distance ratio in the ranked comparisons set within a classification-based fusion approach. The evaluation was performed on the Biometric Scores Set BSSR 1 database and the enhanced performance induced by the integration of neighbors distance ratio was clearly presented...|$|R
40|$|Human gait is an {{effective}} <b>biometric</b> <b>source</b> for human identification and visual surveillance; therefore human gait recognition becomes to be a hot topic in recent research. However, the elapsed time problem, which is in its infancy, still receives poor performance. In this paper, we introduce a novel discriminant analysis method to improve the performance. The new model inherits the merits from the tensor rank one analysis, which handles the small samples size problem naturally, and the linear discriminant analysis, which is optimal for classification. Although 2 DLDA and DATR also benefit from these two methods, they cannot converge during the training procedure. This means they can be hardly utilized for practical applications. Based {{on a lot of}} experiments on elapsed time problem in human gait recognition, the new method is demonstrated to significantly outperform the existing appearance-based methods, such as the principle component analysis, the linear discriminant analysis, and the tensor rank one analysis. 1...|$|E
40|$|It is {{generally}} recognised {{that no one}} biometric data source or processing platform is universally appropriate for optimising performance across all problem domains. Multibiometric processors, which combine identity information obtained {{from more than one}} <b>biometric</b> <b>source</b> are commonly promoted as optimal structures for maximising performance, and much research has been carried out to investigate appropriate strategies for combining the available information. However, the techniques of multiclassifier pattern recognition also offer opportunities to improve the performance of systems operating within a unimodal environment, yet such solutions have been less extensively investigated n the specific case of biometric applications. This study presents an empirical study of the relations between these two different approaches to enhancing the performance indicators delivered by biometric systems. In particular, we are interested to increase our understanding of the relative mertis of, on the one hand, multiclassifier/single modality systems and, on the other, full multibiometric configurations. We focus our study on three modalities, the fingerprint and hand geometry (two physiological biometrics) and the handwritten signature (a behavioural biometric) ...|$|E
40|$|We {{consider}} {{the problem of}} identification and authentication based on secret key generation from some user-generated source data (e. g., a <b>biometric</b> <b>source).</b> The goal is to reliably identify users pre-enrolled in a database as well as authenticate them based on the estimated secret key while preserving {{the privacy of the}} enrolled data and of the generated keys. We characterize the optimal tradeoff between the identification rate, the compression rate of the users' source data, information leakage rate, and secret key rate. In particular, we provide a coding strategy based on layered random binning which is shown to be optimal. In addition, we study a related secure identification/authentication problem where an adversary tries to deceive the system using its own data. Here the optimal tradeoff between the identification rate, compression rate, leakage rate, and exponent of the maximum false acceptance probability is provided. The results reveal a close connection between the optimal secret key rate and the false acceptance exponent of the identification/authentication system. Comment: 33 pages, 3 figures, submitted to IEEE Transactions on Information Theor...|$|E
40|$|Multibiometrics {{provides}} high recognition {{accuracy and}} population coverage by combining different <b>biometric</b> <b>sources.</b> However, some multibiometrics may obtain smaller-than-expected improvement of recognition accuracy if the combined <b>biometric</b> <b>sources</b> are dependent {{in terms of}} a false acceptance by mistakenly perceiving biometric features from two different persons as being from the same person. In this paper, we evaluate whether or not features of multiple fingerprints are statistically independent. By evaluating false acceptance error using matchign scores obtained by Verifinger SDK, we confirmed that these features are dependent in some degree and have no small effect on the FAR obtained by their fusion. Mathematics and Computer Science : Proceedings of Annual Workshop on Mathematics and Computer Science, held at Josai University on March 25 in 2014 / edited by Masatoshi IIDA, Manabu INUMA, Kiyoko NISHIZAW...|$|R
40|$|Abstract. Two <b>sources</b> of weak <b>biometric</b> {{data are}} {{investigated}} {{for the development}} of user identification models. A probabilistic neural network model is created from keystroke digraph features extracted from raw typing data. A second model is developed from scan-path features extracted from raw eye-tracking data. A third model is created from a combination of features from the two <b>biometric</b> <b>sources.</b> Experimental results show that models based on keystroke biometric data perform very well, whereas the models involving the eye-tracking data are not as successful but encourage further study...|$|R
40|$|Fusion is {{a popular}} {{practice}} to combine multiple <b>sources</b> of <b>biometric</b> information to achieve systems with greater performance and flexibility. In this paper various approaches to fusion within a multibiometrics context are considered and an application to the fusion of 2 D and 3 D face information is discussed. An optimal method for fusing the accept/reject decisions of individual <b>biometric</b> <b>sources</b> by means of simple logical rules is presented. Experimental results on the FRGC 2 D and 3 D face data show that the proposed technique performs effectively {{without the need for}} score normalization...|$|R
40|$|Template {{protection}} {{techniques are}} used within biometric systems {{in order to}} protect the stored biometric template against privacy and security threats. A great portion of template protection techniques are based on extracting a key from or binding a key to a biometric sample. The achieved protection depends on the size of the key and its closeness to being random. In the literature it can be observed that there is a large variation on the reported key lengths at similar classification performance of the same template protection system, even when based on the same biometric modality and database. In this work we determine the analytical relationship between the system performance and the theoretical maximum key size given a <b>biometric</b> <b>source</b> modeled by parallel Gaussian channels. We consider the case where the source capacity is evenly distributed across all channels and the channels are independent. We also determine the effect of the parameters such as the source capacity, the number of enrolment and verification samples, and the operating point selection on the maximum key size. We show that a trade-off exists between the privacy protection of the biometric system and its convenience for its users...|$|E
40|$|Current two {{dimensional}} face recognition methods rely on visible photometric or geometric attributes that {{are present in}} the intensity image. In many of these approaches a technique called Principal Component Analysis (PCA) is extensively used. PCA extracts the maximum intensity variations from the set of input images in the form of 2 ̆ 2 eigen 2 ̆ 2 faces which are used as a feature vector. In these approaches the intensity images used were mostly that of the subject 2 ̆ 7 s frontal face, which yielded promising results after doing PCA. These approaches however fail in the presence of facial expression, unstable lighting conditions and artifacts such as make-up, glasses etc. Thus, it is desirable to establish a new <b>biometric</b> <b>source</b> that will be least affected bythe afore mentioned factors. This study describes a face recognition method that is designed based on the consideration of anatomical and biomechanical characteristics of facial tissues. During facial expressions such as smile, frown, anger etc, various muscles get activated in tandem. A strain pattern inferred from a face expression can reveal an individual 2 ̆ 7 s signature associated with the underlying anatomical structure, and thus has the potential for face recognition. In this study, the strain is computed by measuring the displacement of a point on the face that results from a facial expression such as opening the mouth. The information provided by the change in the depth value for the face across the open and close mouth frames does not provide any information required for computing the strain maps, because the strain map depends on the relative displacements of two points on the face, which remains same with rigid motions of the face such as rotation and translation. Hence the information in the 2 D spaceis sufficient to compute strain since the depth is assumed constant. The approach used to calculate strain computes the strain distribution directly using the mathematical definition of strain as the derivative of displacement in 2 D space (XY plane). The strain values obtained are converted to gray scale intensity images, which are used as inputs for the intensity based PCA analysis. Experiments were conducted using 62 subjects. The data set comprised of two pairs of images for a subject: closed mouth and open mouth under bright and low light...|$|E
40|$|Access {{control systems}} {{are used to}} grant or deny the access to a person of a {{particular}} resource. There has been an enormous change in the trend of access control systems in recent times. Starting {{with the use of}} physical access control systems such as tokens, passwords etc., for the identification of a person, the trend has swayed towards designing and deploying of access control systems which use biometric identification of individual persons, for the grant or denial of access to resources. Biometric identification methods use various sources like retina, fingerprint, DNA etc., Biometric sources can be classified into two, namely physiological and behavioral. The former includes face, fingerprint, hand, iris, DNA and the latter includes keystroke, signature and voice. The access control systems using these biometric sources fundamentally identify and recognize a particular personal trait of person and compare it with the information available to grant or deny access to such person who seeks to interact with such system. As amongst such biometric sources to develop reliable access control systems researches have shown overt interest in using the face of a person (face recognition). Such inclination of the researchers is due to the various strategic advantages face recognition systems have like, its global application, wide and compatible collectability of data, cost effectiveness in implementation (for example existing surveillance cameras can be used to deploy such systems) when compared to other biometric methods and many more. The current project fundamentally aims at successfully designing and implementing a face recognition technology to develop an access control system. Out of the various available methods for developing a face recognition technology such as Fishersface, Hidden Markov model, dynamic link matching, three-dimensional face recognition, Eigenfaces etc., this project adopts Eigenfaces method of face recognition to achieve such aim. The project fundamentally detects and identifies human faces that work as a <b>biometric</b> <b>source.</b> This project aims to provide solution for the development of face recognition system by assuming that, problems involved in developing such systems are intrinsically a two dimensional rather than a three dimensional. It basically identifies the face of a person in a face image and then identifies the specific characteristics of such face image, then compares such characteristics with an existing database containing specific characteristics of several faces of different individuals, to decide if the former matches with any of the existing faces in the database. Eigenfaces method is utilised to achieve the above result where, face image is projected onto a feature space that spans the significant variations among known faces. The best variation among different images is calculated where during such calculation not exactly the facial features like eyes, nose, ears, etc., are classified. Instead it learns each face in an under constant observation as a whole...|$|E
40|$|International audienceDifferent {{features}} {{carry more}} or less rich and varied pieces of information to characterize a pattern. The fusion of these different sources of information can {{provide an opportunity to}} develop more efficient biometric system compared when using a feature vector. Thus a new automatic fusion methodology using different sources of information (different feature sets) is presented here. Dempster-Shafer evidence theory is employed for this purpose. For performance evaluation significantly large data sets of the <b>biometric</b> <b>sources</b> signature and hand shape are used. The results on combining different feature vectors compared to a single vector with our approach prove the importance of a fusion proces...|$|R
40|$|International audienceIn this paper, {{the issue}} of {{generating}} and sharing biometrics based secure cryptographic keys is addressed. In particular, we propose a protocol which integrates multi-biometrics, in which information from multiple <b>biometric</b> <b>sources</b> is combined. This protocol allows generation and sharing of multi-biometrics based crypto-biometric session keys. The protocol achieves mutual authentication between a client and a server without the need of third party certificates. The stored templates are revocable/cancelable and thus protect user privacy. The most distinctive feature of this protocol {{is that it can}} integrate multi-biometrics and depending on the required security level, the choice of the biometric modalities to use can be made at runtim...|$|R
40|$|In the {{practical}} use of multi-biometric solutions, <b>biometric</b> <b>sources</b> involved in producing the verification or identification decision do occasionally {{fail to produce}} results. This work discusses solutions for missing data in multi-biometric score-level fusion. A missing data estimation solution based on support vector regression was presented in this work and compared to four baseline solutions. The evaluation was carried under both the verification and the identification scenarios {{in an effort to}} show the effect of missing data estimation on the relatively understudied multi-biometric identification scenario. Evaluation was performed on the Biosecure DS 2 score database and satisfying performance was achieved under both biometric scenarios...|$|R
40|$|Authentication {{is a key}} {{building}} block in security systems and many applications to prevent access to information, services, assets or locations for non-authorized persons or processes. Common methods based on knowledge or possession are however not scalable and practical in human-to-machine communication. Passwords are difficult to remember if chosen appropriately and distinct for {{the increasing number of}} different applications, they can be forgotten, spied-out and passed on to other persons. Tokens, like keys or cards, can be forwarded, stolen, lost or destroyed in a similar way. Biometric systems, as the third factor, use body properties to allow for convenient authentication. The main difference lies in a strong link between electronic identifier and physical identity which leads to desirable properties like non-repudiation, difficulty of replication, theft and loss. On the other hand this may challenge privacy and may lead to identity theft, disclosure of sensitive information and profiling if digital biometric identifiers are exposed. Vascular biometric systems use information about the blood vessel structures inside the hand area (finger, palm or wrist) and overcome problems of latent prints (as with fingerprints, DNA) or unnoticed acquisition on distance (as with face) and liveness issues. Still, the before mentioned problems of biometric systems exist and privacy enhancing technologies (PETs) were introduced to overcome them. Some PETs enable revocation of biometric references, unlimited references from the same <b>biometric</b> <b>source</b> and unlinkability between the generated templates. In addition the sensitive data is sealed. In order to utilize PETs like the helper data scheme (HDS) some requirements, like a fixed-length structure of the feature representation, have to be met. The goal of this thesis is to meet those requirements and {{to make use of the}} HDS. In addition we strive for the application in real-life scenarios. One of the main applications that we identified for such a system is online banking. Those systems, as of today, are secured with authentication systems based on knowledge or possession and constantly a vulnerable target of criminal activity. Since the recent systems are mostly broken, new alternatives are needed. So we designed a protocol based on the HDS that merges information about online transactions with secure biometric references to enable secure online banking with the desirable properties of biometric systems: hence the name BTAP – biometric transaction authentication protocol. The work on representing patterns compatible to the HDS has been achieved for fingerprint- based systems using spectral minutiae. Therefore we designed algorithms to extract minutiae to represent the topology of the blood vessel network with its branch and end points. Transforming the location and orientation information of the feature points into spectral vein minutiae leads to a translation invariant, fixed-length representation that allows for alignment-free scale and rotation corrections. Those properties are especially important for hygienic, contact-free sensors without guidance for the hand or finger. A performance evaluation revealed that the transformed spectral vein minutiae lead to low recognition errors for sub-modalities including palm, palm dorsal (back of hand) and wrist vein patterns. In a multi-reference scenario the performance for quantized spectral minutiae based on palm vein patterns and a simple Hamming distance could even be improved to a perfect separation between genuine and imposter attempts. The quantized, binary feature vectors are utilized in the first stages of the HDS, they are very compact and could also be used for extremely fast comparison systems or for biometric indexing. In conclusion our work shows that vascular patterns can be transformed into highi performance representations meeting the requirements of the privacy-enhancing HDS that is the core for the proposed online banking protocol BTAP. After solving issues with the reproducibility of feature vectors, we will be able to combine vein patterns with BTAP to overcome drawbacks of biometric systems to perform secure, convenient, affordable and user accepted biometric online banking transaction authentication. Bringing the work into a larger perspective, we can state that BTAP is an innovative instance where a biometric system is shifted from a binary authentication decision-making scheme to an integral part of an abstract security protocol. The combination of data from the application with keys released from biometric templates opens new possibilities and represents a recent paradigm shift in biometric systems. General digital biometric signature schemes and biometric message authentication primitives with a strong relation to a natural person are the next step...|$|E
40|$|The {{topic of}} {{multi-modal}} biometrics has attracted {{great interest in}} recent years. This talk will categorize different approaches to multi-modal biometrics based on the <b>biometric</b> <b>sources,</b> the type(s) of sensing used, {{and the depth of}} collaborative interaction in the processing. By “biometric source ” we mean the property of the person that is used for identification, such as fingerprint, voice, face appearance or iris texture. By type of sensing we mean different sensor modalities, such as 2 D, 3 D, or infra-red. By collaboration we mean {{the degree to which the}} processing of one biometric is influenced by the results of processing other biometrics. One commo...|$|R
40|$|Abstract—The {{fundamental}} secret-key rate vs. privacy-leakage rate trade-offs for secret-key {{generation and}} transmission for i. i. d. Gaussian <b>biometric</b> <b>sources</b> are determined. These results are the Gaussian equivalents {{of the results}} that were obtained for the discrete case by the authors and independently by Lai et al. in 2008. Also the effect that binary quantization of the biometric sequences has on {{the ratio of the}} secret-key rate and privacy-leakage rate is considered. It is shown that the squared correlation coefficient must be increased by a factor of π 2 / 4 to compensate for such a quantization action, for values of the privacy-leakage rate that approach zero, when the correlation coefficient is close to zero. I...|$|R
40|$|A {{new video}} based {{recognition}} method {{is presented to}} recognize non-cooperating individuals at a distance in video, who expose side views to the camera. Information from two <b>biometric</b> <b>sources,</b> side face and gait, is utilized and integrated at feature level. For face, a high-resolution side face image is constructed from multiple video frames. For gait, Gait Energy Image (GEI), a spatio-temporal com-pact representation of gait in video, is used to characterize human walking properties. Face features and gait features are obtained separately using Principal Component Analy-sis (PCA) and Multiple Discriminant Analysis (MDA) com-bined method from the high-resolution side face image and Gait Energy Image (GEI), respectively. The system is tested on a database of video sequences corresponding to 46 peo-ple. The {{results showed that the}} integrated face and gait features carry the most discriminating power compared to any individual biometric. 1...|$|R
40|$|Abstract — Multibiometric systems fuse evidences from {{multiple}} <b>biometric</b> <b>sources</b> typically resulting in better recognition accuracy. These systems can consolidate information at various levels. For systems {{operating in the}} identification mode, rank level fusion presents a viable option. In this paper, several simple but powerful modifications are suggested to enhance the performance of rank-level fusion schemes {{in the presence of}} weak classifiers or low quality input images. These modifications do not require a training phase, therefore making them suitable {{in a wide range of}} applications. Experiments conducted on a multimodal database consisting of a few hundred users indicate that the suggested modifications to the highest rank and Borda count methods significantly enhance the rank- 1 accuracy. Experiments also reveal that including image quality in the fusion scheme enhances the Borda count rank- 1 accuracy by ∼ 40 %. I...|$|R
40|$|Abstract — Multibiometric systems fuse {{information}} from different sources {{to compensate for}} the limitations in performance of individual matchers. We propose a framework for optimal combination of match scores that is based on the likelihood ratio test. The distributions of genuine and impostor match scores are modeled as finite Gaussian mixture model. The proposed fusion approach is general in its ability to handle (i) discrete values in biometric match score distributions, (ii) arbitrary scales and distributions of match scores, (iii) correlation between the scores of multiple matchers and (iv) sample quality of multiple <b>biometric</b> <b>sources.</b> Experiments on three multibiometric databases indicate that the proposed fusion framework achieves consistently high performance compared to commonly used score fusion techniques based on score transformation and classification. Index Terms — Multibiometric systems, score level fusion, Neyman-Pearson theorem, likelihood ratio test, Gaussian mixture model, image quality I...|$|R
40|$|Abstract — With {{the advent}} of most modern {{electronic}} sophisticated applications automatic person identification has become a very important topic. Traditional based person identification techniques had several drawbacks since they are easy for spoofing and performing fraudulent operations. In this aspect we are developing a fused biometric person recognition using support vector machine in a multimodal approach for an efficient identification in several security aspects. A biometric trait is his/her unique behavioural and physiological trait which is used for identification aspects. Almost all systems use the application of unimodal biometric identification but they are vulnerable for spoofing. So a multimodal identification process has evolved which integrates information from multiple <b>biometric</b> <b>sources.</b> For the fusion we are using score level fusion approach and clustering using k means clustering. The classification is done by a multi svm machine which is an efficient classification...|$|R
40|$|Multimodal {{biometric}} fusion {{is gaining}} more attraction among researchers. As multimodal biometric consolidates {{the information from}} multiple <b>biometric</b> <b>sources,</b> the effective fusion of information obtained at score level is a challenging task. In this paper, we propose a novel frame work for optimal combination of match scores using Gaussian mixture model (GMM) and Monte Carlo method. The proposed fusion approach {{has the ability to}} handle 1) small size of match scores as is more commonly encountered in biometric fusion and 2) arbitrary distribution of match scores. The proposed fusion scheme is compared with more robust fusion schemes such as SUM rule, weighted SUM rule, Fishers linear discriminate analysis (FLD) and likelihood ratio (LR) method. Extensive experiments are carried out on three different build multimodal biometric databases. Experimental results indicate that proposed fusion scheme achieves higher performance as compared with other fusion techniques...|$|R
40|$|As the {{information}} age matures, biometric identification technology {{will be at the}} heart of computer interaction with humans and the biosphere in which they reside. Automated biometric systems for human identification measure a “signature ” of the human body, compare the resulting characteristic to a database, and render an application dependent decision. These biometric systems for personal authentication and identification are based upon physiological or behavioral features which are typically distinctive, although time varying, such as fingerprints, hand geometry, face, voice, lip movement, gait, and iris patterns. Multi-biometric systems, which consolidate information from multiple <b>biometric</b> <b>sources,</b> are gaining popularity because they are able to overcome limitations such as non-universality, noisy sensor data, large intra-user variations and susceptibility to spoof attacks that are commonly encountered in uni-biometric systems. In this paper, it addresses the con-cept issues and the applications strategies of multi-biometric systems...|$|R
40|$|Abstract—Multibiometric systems fuse {{information}} from different sources {{to compensate for}} the limitations in performance of individual matchers. We propose a framework for the optimal combination of match scores that is based on the likelihood ratio test. The distributions of genuine and impostor match scores are modeled as finite Gaussian mixture model. The proposed fusion approach is general in its ability to handle 1) discrete values in biometric match score distributions, 2) arbitrary scales and distributions of match scores, 3) correlation between the scores of multiple matchers, and 4) sample quality of multiple <b>biometric</b> <b>sources.</b> Experiments on three multibiometric databases indicate that the proposed fusion framework achieves consistently high performance compared to commonly used score fusion techniques based on score transformation and classification. Index Terms—Multibiometric systems, score level fusion, Neyman-Pearson theorem, likelihood ratio test, Gaussian mixture model, image quality. ...|$|R
40|$|Biometrics based {{personal}} {{identification is}} regarded as an effective method for automatically recognizing, with a high confidence a person’s identity. A multimodal biometric systems consolidate the evidence presented by multiple <b>biometric</b> <b>sources</b> and typically better recognition performance compare to system based on a single biometric modality. This paper proposes an authentication method for a multimodal biometric system identification using two traits i. e. face and palmprint. The proposed system is designed for application where the training data contains a face and palmprint. Integrating the palmprint and face features increases robustness of the person authentication. The final decision is made by fusion at matching score level architecture in which features vectors are created independently for query measures and are then compared to the enrolment template, which are stored during database preparation. Multimodal biometric system is developed through fusion of face and palmprint recognition...|$|R
40|$|Abstract. A {{multimodal}} {{biometric system}} integrates information from multiple <b>biometric</b> <b>sources</b> {{to compensate for}} the limitations in performance of each individual biometric system. We propose an optimal framework for combining the matching scores from multiple modalities using the likelihood ratio statistic computed using the generalized densities estimated from the genuine and impostor matching scores. The motivation for using generalized densities is that some parts of the score distributions can be discrete in nature; thus, estimating the distribution using continuous densities may be inappropriate. We present two approaches for combining evidence based on generalized densities: (i) the product rule, which assumes independence between the individual modalities, and (ii) copula models, which consider the dependence between the matching scores of multiple modalities. Experiments on the MSU and NIST multimodal databases show that both fusion rules achieve consistently high performance without adjusting for optimal weights for fusion and score normalization on a case-by-case basis...|$|R
