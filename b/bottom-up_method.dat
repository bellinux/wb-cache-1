198|422|Public
5000|$|XCOM and XA were {{subsequently}} changed to instead use {{a variant of}} Donald Knuth's LR parser <b>bottom-up</b> <b>method.</b> [...] XCOM's variant is called Simple LR or SLR. It handles more grammars than MSP but not quite as many grammars as LALR or full LR(1). The differences from LR(1) are mostly in the table generator's algorithms, not in the compile-time parser method. XCOM and XA predate the widespread availability of Unix and its yacc parser generator tool. XA and yacc have similar purposes.|$|E
50|$|In the 1960s and 1970s, Walker {{authored}} several {{volumes of}} knitting references {{which have become}} landmarks for their comprehensiveness and clarity. Her Knitting Treasury series documents over a thousand different knitting stitches. Other books considered mosaic knitting, for producing multicolored designs while knitting only one color per row, and constructing knitted garments {{from the top down}} rather than the usual <b>bottom-up</b> <b>method</b> used in Western knitting tradition. Most of Walker's best-known knitting books have been reprinted, and starting in the mid-1990s, she has published new knitting books.|$|E
5000|$|The design {{ideas and}} {{relevant}} issues that arise {{during the process}} {{should be included in}} the affinity diagram. Any holes in the data and areas that need more information should also be labeled. After completing the wall, participants [...] "walk" [...] the affinity diagram to stimulate new ideas and identify any remaining issues or holes in data. The affinity diagram is a <b>bottom-up</b> <b>method.</b> Consolidated data may also be used to create a cause-and-effect diagram or a set of personas describing typical users of the proposed system.|$|E
25|$|Fabrication of nano-channels is {{categorized}} into top-down and <b>bottom-up</b> <b>methods.</b> Top-down {{methods are}} the conventional processes utilized in the IC industry and Microelectromechanical systems research. It begins with photolithography on a bulk silicon wafer. <b>Bottom-up</b> <b>methods,</b> in contrast, starts with atoms or molecules with intrinsic nano-scaled dimension. By organize and combine these building blocks together, it {{is able to}} form a nanostructures as small as only a few nanometers.|$|R
40|$|AbstractThe <b>bottom-up</b> <b>methods</b> {{for energy}} {{benchmarking}} aim to derive a yardstick for energy performance {{based on a}} theoretical analysis of a building. While the top-down methods drive performance improvement by ranking a building against its peers, the <b>bottom-up</b> <b>methods</b> {{are focused on the}} building’s specific context. Consequently, the <b>bottom-up</b> <b>methods</b> can help identify how performance improvement could be materialised. These two complementary approaches can improve design practice and facilities’ management. Two <b>bottom-up</b> <b>methods</b> {{that could be used for}} energy benchmarking have been reviewed using UK schools as case studies: Building physics and aggregated end-use. The aim is to demonstrate how these methods could be used for benchmarking and identify their benefits and limitations. When all energy components are included in a model under expected operating conditions, the building physics method can be used to establish a baseline for energy performance. It is demonstrated that where this method is used under standardised operating conditions and is subject to minimum energy performance requirements, as prescribed by the Energy Performance of Buildings Directive (EPBD), it can be used to establish a benchmark for energy performance. It is also shown how aggregated end-use methods such as CIBSE TM 22 can be used to define system level benchmarks, and identify the root causes for discrepancy between measured performance and design intent in a systematic way...|$|R
30|$|As to the {{preparation}} methods, {{they could be}} divided into two categories: top-down <b>methods</b> and <b>bottom-up</b> <b>methods.</b> The <b>bottom-up</b> <b>methods</b> usually suffer from complex processes, or expensive starting materials and severe synthetic conditions, which are unlikely to be extended significantly in the near future [23]. Alternatively, bottom-up synthetic approaches based on chemistry have been desired to achieve C-dots with fluorescence. Presently, Li et al. reported a facile hydrothermal method to prepare luminescent carbon dots (L-CDs) with high quantum yield value (44.7 %) and controllable emission wavelengths and used prepared carbon dots to detect toxic Be 2 + ions [6].|$|R
50|$|Another way {{to produce}} nanosensors {{is through the}} <b>bottom-up</b> <b>method,</b> which {{involves}} assembling the sensors out of even more minuscule components, most likely individual atoms or molecules. This would involve moving atoms of a particular substance one by one into particular positions which, though it has been achieved in laboratory tests using tools such as atomic force microscopes, is still a significant difficulty, especially to do en masse, both for logistic reasons {{as well as economic}} ones. Most likely, this process would be used mainly for building starter molecules for self-assembling sensors.|$|E
5000|$|Micromasonry [...] is a {{fabrication}} {{technique to}} self-assemble micrometric and sub-micrometric three-dimensional units into larger structures. The technique {{is based on}} the self-assembly construction and hierarchical design of biological materials.It was developed by Spanish materials scientist Javier G. Fernandez of the Massachusetts Institute of Technology to produce artificial organs by self-assembling groups of biological cells in biocompatible polymers. This method allows the self-assembly of encapsulated cells in 3D geometries, enabling the fabrication of biological tissue following an approach similar to building a brick structure. Because this <b>bottom-up</b> <b>method</b> of fabrication, the resulting constructions are some times referred as [...] "bio-Legos".|$|E
50|$|The Johannesburg {{negotiations}} {{concluded that}} Type II partnerships must meet seven key criteria: i) {{they should be}} voluntary and based on shared responsibility, ii) they must complement, rather than substitute, intergovernmental sustainable development strategies, and must meet the agreed outcomes of the Johannesburg summit, iii) they must consist {{of a range of}} multi-level stakeholders, preferably within a given area of work, iv) they must ensure transparency and accountability, v) they must produce tangible results, vi) the partnership must be new, and adequate funding must be available, and vii) a follow-up process must be developed. If these requirements were successfully fulfilled, it was hoped that Type II partnerships could create a fundamental shift in sustainable development discourse, leading to an increasingly participatory, <b>bottom-up</b> <b>method</b> of governing the issue.|$|E
40|$|Hierarchies provide {{scalability}} {{in large}} networks and are integral to many widely-used protocols and applications. Previous approaches to constructing hierarchies have typically either assumed static hierarchy configuration, or have used <b>bottom-up</b> construction <b>methods.</b> We describe how to construct hierarchies in a top-down fashion, {{and show that}} our method is much more efficient than <b>bottom-up</b> <b>methods.</b> We also show that top-down hierarchy construction is a better choice when administrative policy constraints are imposed on hierarchy formation...|$|R
40|$|This paper {{gives an}} {{overview}} of the most commonly used techniques for nanostructuring and nanochannel fabrication employed in nanofluidics. They are divided into two large categories: top-down and <b>bottom-up</b> <b>methods.</b> Top-down methods are based on patterning on large scale while reducing the lateral dimensions to the nanoscale. <b>Bottom-up</b> <b>methods</b> arrange atoms and molecules in nanostructures. Here, we review {{the advantages and disadvantages of}} those methods and give some future perspectives. It is concluded that technology in the region of 1 – 10 nm is lacking and potentially can be covered by using the pulsed-laser deposition method as a controlled way for thin film deposition (thickness of a few nanometers) and further structuring by the top-down method...|$|R
40|$|It is {{difficult}} to measure or even estimate the Dutch greenhouse gas emissions of the landscape, because we cannot put a flux chamber over the entire Netherlands. Greenhouse gas emissions are usually estimated by <b>bottom-up</b> <b>methods</b> or top-down methods. The Netherlands report most emissions to the Kyoto protocol at Tier 2 level, meaning a bottom-up inventory of emissions by using relatively simple linear methods and country-specific data. For <b>bottom-up</b> <b>methods</b> {{it is important to}} measure, using measurement chambers, on many different landscape elements and to map these landscape elements accurately. In addition, measurements should be conducted on the largest and most uncertain sources in combination with the occurrence of these sources. These methods will help to reduce the uncertainty. Comparison between simple and complex <b>bottom-up</b> <b>methods</b> can help to gain insight into greenhouse gas emission processes. Top-down methods are capable of estimating emissions from large areas; however they struggle with the difficulty of finding the emission footprint, determining country borders and translating greenhouse gas fluxes to emissions. Model-free verification and Radon measurements can help to solve these problems. Using the proposed methods can contribute to the verification of the national inventory and to the reduction of uncertainties. ...|$|R
50|$|The {{genetics}} {{of social}} behavior {{is an area}} of research that attempts to address the question of the role that genes play in modulating the neural circuits in the brain which influence social behavior. Model genetic species, such as D.melanogaster (common fruit fly) and Apis mellifera (honey bee), have been rigorously studied and proven to be instrumental in developing the science of genetics. Many examples of genetic factors of social behavior have been derived from a <b>bottom-up</b> <b>method</b> of altering a gene and observing the change it produces in an organism. Sociogenomics is an integrated field that accounts for the complete cellular genetic complement of an organism from a top-down approach, accounting for all biotic influences that effect behavior on a cellular level.|$|E
5000|$|Availability of modular metal {{centers and}} organic {{building}} blocks generate wide diversity in synthetic versatility. Their applications range from industrial use to chemiresistive sensors. The ordered {{structure of the}} frame is largely determined by the coordination geometry of the metal and directionality of functional groups upon the organic linker. Consequently, MOFs contain highly defined pore dimensions when compared with conventional amorphous nanoporous materials and polymers. Reticular Synthesis of MOFs is a term that has been recently coined to describe the <b>bottom-up</b> <b>method</b> of assembling cautiously designed rigid molecular building blocks into prearranged structures held together by strong chemical bonds. The synthesis of two-dimensional MOFs begins {{with the knowledge of}} a target [...] "blueprint" [...] or a network, followed by identification of the required building blocks for its assembly.|$|E
50|$|It was {{concluded}} that the tasks performed well by H.J.A included the tasks involving ranges of imagery-based tasks, accurately make judgments about global representations such as the angles of a clock, maintained visual patters over inter-stimulus intervals, mentally rotating letters, manipulating two elements of an image. In contrast, impairment was gauged to be the lack of the ability in recalling spatial layouts, judging spatial directions, judging relative positions of objects, The Brooks Matrix Test, Compass Direction Task, reproducing abstract patterns, and reproducing both possible and impossible figures. Due to the impairment, it was identified that the patient did not have intact imagery and visual short-term memory, made apparent by the spatial relations test. Due to Integrative Agnosia, the patients take information from a top-down manner, using stored knowledge to retrieve an objects perceptual properties. It {{is much more difficult}} for patients to use a <b>bottom-up</b> <b>method,</b> or perceiving through a visual stimulus, because of the inability to accurately code the patterns in the visual short-term memory.|$|E
50|$|There {{are three}} primary {{categories}} of tree construction methods: top-down, <b>bottom-up,</b> and insertion <b>methods.</b> Top-down methods proceed by partitioning the input set into two (or more) subsets, bounding {{them in the}} chosen bounding volume, then keep partitioning (and bounding) recursively until each subset consists of only a single primitive (leaf nodes are reached). Top-down methods are easy to implement, fast to construct and {{by far the most}} popular, but do not result in the best possible trees in general. <b>Bottom-up</b> <b>methods</b> start with the input set as the leaves of the tree and then group two (or more) of them to form a new (internal) node, proceed in the same manner until everything has been grouped under a single node (the root of the tree). <b>Bottom-up</b> <b>methods</b> are more difficult to implement, but likely to produce better trees in general. Both top-down and <b>bottom-up</b> <b>methods</b> are considered off-line methods as they both require all primitives to be available before construction starts. Insertion methods build the tree by inserting one object at a time, starting from an empty tree. The insertion location should be chosen that causes the tree to grow as little as possible according to a cost metric. Insertion methods are considered on-line methods since they do not require all primitives to be available before construction starts and thus allow updates to be performed at runtime.|$|R
40|$|The Prolog {{evaluation}} algorithm {{has become}} the standard for logic program evaluation, and <b>bottom-up</b> <b>methods</b> have long been considered impractical because they compute irrelevant facts. Recently, however, bottom-up evaluation algorithms that retain the focusing property of top-down evaluation have been proposed, and in view of these algorithms the choice between top-down and <b>bottom-up</b> <b>methods</b> needs to be re-examined. 	In order to motivate {{a closer look at}} <b>bottom-up</b> <b>methods,</b> we identify certain classes of logic programs for which bottom-up evaluation provides polynomial time evaluation algorithms where Prolog takes exponential time. We also demonstrate that techniques such as predicate factoring can provide further O(n) improvement, when they are applicable. We argue that no one evaluation method is uniformly preferable, and suggest that a choice of the appropriate method must be made by the compiler based on the given program. We present several results that shed light on this choice. 	The bottom-up approach can be refined in a number of ways, and we show how various results in the literature can be combined to provide a coherent evaluation framework. Further, we indicate how ideas in the tabulation literature for functional programs can be adapted to improve the memory utilization of <b>bottom-up</b> <b>methods.</b> We also consider the program transformation techniques pioneered by Burstall and Darlington, and study their relationship to deductive database program transformations such as Magic Templates. The comparison indicates that the bottom-up approach, with the refinements discussed in the paper, often achieves the same gains as these sophisticated transformation systems, and thus illustrates the power of the approach. 	To keep this paper self-contained, we have included brief surveys of all the techniques that we discuss. We have not attempted to be comprehensive in our survey; it is our hope that the reader will be motivated to pursue these ideas in greater detail by following up on the references...|$|R
30|$|Typically, the {{strategies}} for synthesizing CDs {{can be divided}} into two major categories: top-down and <b>bottom-up</b> <b>methods.</b> Top-down methods consist of laser ablation [24], arc discharge [25], and chemical oxidation [26], where the CDs are formed by cutting a large carbon structure into small pieces. In <b>bottom-up</b> <b>methods,</b> the CDs are synthesized by carbonization of organic molecular precursors through solvothermal methods [27, 28], microwave treatment [29], ultrasonic-assisted synthetic methods [30], and so on. Among them, the top-down methods often require sophisticated and expensive energy-consuming equipment [31]. The ultrasonic-assisted synthetic methods rely on strong acids or bases [30]. Typically, CDs can be synthesized within minutes by microwave irradiation, while they suffer from uncontrollable reaction conditions. The hydrothermal route is mostly preferred because of its simplicity, controlled reaction conditions, and cost-effectiveness [31].|$|R
40|$|The Iceberg-Cube {{problem is}} to {{identify}} the combina-tions of values {{for a set of}} attributes for which a specified aggregation function yields values over a specified aggre-gate threshold. We implemented bottom-up and top-down methods for this problem. The <b>bottom-up</b> <b>method</b> included pruning. Results show that the top-down method, with or without pruning, was slower than the <b>bottom-up</b> <b>method</b> be-cause of less effective pruning. 1...|$|E
30|$|We {{analyze and}} {{optimize}} the online trained classifiers by a <b>bottom-up</b> <b>method</b> {{which makes the}} hand segmentation robust to varying environmental conditions.|$|E
40|$|The {{problem of}} how to {{partition}} a software system and thus capture its overall architecture and its constituent components has become a research focus in the community of software engineering. In the literature, many methods have been proposed for solving this problem. For example, both top-down and bottom-up methods based on analyzing the graph representation of software systems have been proposed. In this paper, we report an experimental study of a top-down method and a <b>bottom-up</b> <b>method.</b> In our study, we focus on the capability of component capture, the capability of architecture recovery and the time complexity for the two methods. According to our results on two real world systems, the studied <b>bottom-up</b> <b>method</b> is superior to the studied top-down method in both aspects, although the time complexity of the <b>bottom-up</b> <b>method</b> remains a big concern for large systems. 1...|$|E
3000|$|... [10], the <b>bottom-up</b> merge <b>method</b> is {{introduced}} for the fast intermode selection. In the <b>bottom-up</b> merge <b>method,</b> 16 × 16 block is split into 4 × 4 blocks, and then 4 × 4 blocks {{with the same}} class are merged based on MVs and the edge information. Ren et al.|$|R
40|$|In concept {{learning}} from instances, a partially ordered concept space is searched. Whereas top-down methods search the whole space, <b>bottom-up</b> <b>methods</b> search a reduced space whose elements are most specific generalizations of positive instances. In this work {{we give the}} foundations for this reduction, and propose a method that performs a bidirectional search of this reduced space using a Tabu search strategy...|$|R
30|$|The {{production}} of nanoparticles majorly involves {{physical and chemical}} processes. Silver nanomaterials {{can be obtained by}} both the so-called ‘top-down’ and ‘bottom-up’ methods. The top-down method involves the mechanical grinding of bulk metals and subsequent stabilization of the resulting nanosized metal particles by the addition of colloidal protecting agents[11, 12]. The <b>bottom-up</b> <b>methods,</b> on the other hand, include reduction of metals, electrochemical methods, and sonodecomposition.|$|R
30|$|The {{process of}} calculating the {{proposed}} ATOL indicator {{is characterized by}} being a <b>bottom-up</b> <b>method</b> (i.e., the user-level information is aggregated at cell level) and can be summarized in three phases.|$|E
40|$|Biomimetic {{extracellular}} matrix (ECM) topographies {{driven by the}} magnetic-field-directed self-assembly of ECM protein-coated magnetic beads are fabricated. This novel <b>bottom-up</b> <b>method</b> allows us to program isotropic, anisotropic, and diverse hybrid ECM patterns without changing other physicochemical properties of the scaffold material. It is demonstrated that this 3 D anisotropic matrix is able to guide the dendritic protrusion of cells. ope...|$|E
40|$|This paper {{describes}} two automatic approaches used {{to study}} connected speech processes (CSPs) in Dutch. The first approach was from a linguistic point of view - the top-down method. This method {{can be used for}} verification of hypotheses about CSPs. The second approach - the <b>bottom-up</b> <b>method</b> - uses a constrained phone recognizer to generate phone transcriptions. An alignment was carried out between the two transcriptions and a reference transcription. A comparison between the two methods showed that 68 % agreement was achieved on the CSPs. Although phone accuracy is only 63 %, the bottom-up approach is useful for studying CSPs. From the data generated using the <b>bottom-up</b> <b>method,</b> indications of which CSPs are present in the material can be found. These indications can be used to generate hypotheses which can then be tested using the top-down method. 1. INTRODUCTION Connected speech processes (CSPs) are responsible for a large amount of variation in spontaneous speech. A precise understan [...] ...|$|E
50|$|<b>Bottom-up</b> {{self-assembly}} <b>methods</b> {{are considered}} promising alternatives that offer cheap, parallel synthesis of nanostructures under relatively mild conditions.|$|R
40|$|Defect {{tolerance}} {{will become}} more important as feature sizes shrink closer to single digit nanometer dimensions. This is true whether the chips are manufactured using topdown methods (e. g., photolithography) or <b>bottom-up</b> <b>methods</b> (e. g., chemically assembled electronic nanotechnology, or CAEN). In this paper, we propose a defect tolerance methodology centered around reconfigurable devices, a scalable testing method, and dynamic place-and-route. Our methodology is particularly well suited for CAEN. ...|$|R
30|$|Several {{fabrication}} {{techniques have}} been reported for Si NWs, and these can be divided in two categories: <b>bottom-up</b> and top-down <b>methods.</b> In the <b>bottom-up</b> <b>methods,</b> atoms and molecules {{can be used as}} building blocks for the nanostructures utilizing vapor-liquid-solid (VLS) technique [4], molecular beam epitaxy (MBE) [5], or laser ablation [6]. The top-down methods including deep reactive-ion etching (DRIE) [7, 8] and metal-assisted chemical etching (MACE) [9, 10] have been introduced for nanostructures by downscaling bulk materials. Recently, a facile and high-throughput method for large-area Si NW arrays of the same dimensions has been proposed by combing MACE with nanosphere lithography (NSL) [11, 12].|$|R
40|$|Current Description Logic {{reasoning}} systems {{provide only}} limited support for debugging logically erroneous knowledge bases. In this {{paper we propose}} new non-standard reasoning services which we designed and implemented to pinpoint logical contradictions when developing the medical terminology DICE. We provide complete algorithms for unfoldable ACC-TBoxes based on minimisation of axioms using Boolean methods for minimal unsatisfiability-presening sub-TBoxes, and an incomplete <b>bottom-up</b> <b>method</b> for generalised incoherence-preserving terminologies. ...|$|E
40|$|The parsing {{problem is}} one of the key {{problems}} of graph grammars. The typical parsing algorithm uses the <b>bottom-up</b> <b>method.</b> The time-complexity of this method is high, and it is difficult to apply. In order to reduce the time-complexity, this paper uses the top-down method for parsing. This method avoids the subgraph isomorphism judgment and selects the productions specifically, so that the time-complexity is greatly reduced...|$|E
40|$|The Iceberg-Cube {{problem is}} to apply an {{aggregate}} function over a set of attributes to determine which combinations of attribute values are above a specified aggregate threshold. We implemented bottom-up and top-down methods for this problem. The bottom-down method we used already used pruning. Results show that even when the top-down method employed pruning, it was slower than the <b>bottom-up</b> <b>method</b> {{because it did not}} prune as efficiently. 1...|$|E
25|$|The {{most common}} <b>method</b> {{utilized}} for <b>bottom-up</b> fabrication is self-assembled monolayers (SAM). This method usually use biological materials {{to form a}} molecular monolayer on the substrate. Nano-channels can also be fabricated from the growth of carbon nanotubes (CNT) and quantum wires. The <b>bottom-up</b> <b>methods</b> usually give well-defined shapes with characteristic length about few nanometers. For these structures to be utilized as nanofluidic devices, the interconnection between nano-channels and microfluidic systems becomes an important issue.|$|R
30|$|Top-down and <b>bottom-up</b> <b>methods</b> are {{two types}} of {{approaches}} used in nanotechnology and nanofabrication [1]. The bottom-up approach is more advantageous than the top-down approach because the former has a better chance of producing nanostructures with less defects, more homogenous chemical composition, and better short- and long-range ordering [2]. Semiconductor nanorods (NRs) and nanowires possess convenient and useful physical, electrical, and optoelectronic properties, and thus, they are highly suitable for diverse applications [3, 4].|$|R
30|$|In this section, {{we apply}} the {{proposed}} method on three public eye tracking datasets (two color image datasets and one gray image dataset) {{to evaluate the}} performance of human fixation prediction. These datasets comprise natural images, containing different objects and scenes, and the corresponding human fixations. The proposed method is compared with the state-of-the-art <b>bottom-up</b> <b>methods</b> based on a well-known validation approach. The qualitative and quantitative assessments of detection results are reported.|$|R
