6|2|Public
40|$|In image analysis, {{the images}} are often {{represented}} by multiple visual features (also known as multiview features), that aim to better interpret them for achieving remarkable performance of the learning. Since the processes of feature extraction on each view are separated, the multiple visual features of images may include overlap, noise, and redundancy. Thus, learning with all the derived views of the data could decrease the effectiveness. To address this, this paper simultaneously conducts a hierarchical feature selection and a multiview multilabel (MVML) learning for multiview image classification, via embedding a proposed a new <b>block-row</b> regularizer into the MVML framework. The <b>block-row</b> regularizer concatenating a Frobenius norm (F-norm) regularizer and an l(2, 1) -norm regularizer is designed to conduct a hierarchical feature selection, in which the F-norm regularizer is used to conduct a high-level feature selection for selecting the informative views (i. e., discarding the uninformative views) and the l(2, 1) -norm regularizer is then used to conduct a low-level feature selection on the informative views. The rationale {{of the use of}} a <b>block-row</b> regularizer is to avoid the issue of the over-fitting (via the <b>block-row</b> regularizer), to remove redundant views and to preserve the natural group structures of data (via the F-norm regularizer), and to remove noisy features (the l(2, 1) -norm regularizer), respectively. We further devise a computationally efficient algorithm to optimize the derived objective function and also theoretically prove the convergence of the proposed optimization method. Finally, the results on real image datasets show that the proposed method outperforms two baseline algorithms and three state-of-the-art algorithms in terms of classification performance...|$|E
40|$|The {{irregular}} {{nature of}} the data structures required to efficiently store arbitrary sparse matrices and the architectural constraints of a SIMD computer {{make it difficult to}} design an algorithm that can efficiently multiply an arbitrary sparse matrix by a vector. A new "block-row" algorithm is proposed. It allows the "regularity" of a data structure with a row-major mapping to be varied by changing a parameter (the "blocksize"); a heuristic to find a very good approximation of the optimal blocksize is also described. The <b>block-row</b> algorithm has been implemented on a 16, 384 processor MasPar MP- 1, and, for the matrices studied, the algorithm was found to be faster {{than any of the other}} algorithms considered. 1. INTRODUCTION This paper presents a new <b>block-row</b> algorithm for matrix-vector multiplication which was primarily developed for the large unstructured sparse matrices arising from a scattering-matrix approach to device simulation [1],[3]. The algorithm has been implemented on a 16 [...] ...|$|E
40|$|We have {{undertaken}} experiments {{to determine the}} comparative quality of sparse matrix partitioners. A large selection of test matrices are partitioned and then permuted so that the resulting form exhibits a block structure. This form is useful for implementing sparse matrix-vector multiplication in a parallel computing environment where each <b>block-row</b> strip will be assigned to a single computing node. Key words. sparse matrix, matrix-vector multiplication, matrix partitioning. ...|$|E
40|$|We {{propose a}} {{multi-layer}} parallel decoding algorithm and VLSI architecture for decoding of structured quasi-cyclic low-density parity-check codes. In the conventional layered decoding algorithm, the <b>block-rows</b> of the parity check matrix are processed sequentially, or layer after layer. The {{maximum number of}} rows that can be simultaneously processed by the conventional layered decoder {{is limited to the}} sub-matrix size. To remove this limitation and support layer-level parallelism, we extend the conventional layered decoding algorithm and architecture to enable simultaneously processing of multiple (K) layers of a parity check matrix, which will lead to a roughly K-fold throughput increase. As a case study, we have designed a double-layer parallel LDPC decoder for the IEEE 802. 11 n standard. The decoder was synthesized for a TSMC 45 -nm CMOS technology. With a synthesis area of 0. 81 mm 2 and a maximum clock frequency of 815 MHz, the decoder achieves a maximum throughput of 3. 0 Gbps at 15 iterations...|$|R
40|$|A sparse QR-factorization {{algorithm}} SPARQR for coarse-grained parallel computations is described. The coefficient matrix, {{which is}} assumed to be general sparse, is reordered in an attempt to bring as many zero elements in the lower left corner as possible. The reordered matrix is then partitioned into block rows, and Givens plane rotations are applied in each <b>block-row.</b> These are independent tasks and can be done in parallel. Row and column permutations are carried out within the diagonal blocks in an attempt to preserve better the sparsity of the matrix. The algorithm can be used for solving least squares problems either directly or combined with an iterative method (preconditioned conjugate gradients are used). Small non-zero elements can optionally be dropped in the latter case. This leads to a better preservation of the sparsity and, therefore, to a faster factorization. The price which has to be paid is some loss of accuracy. The iterative method is used to regain the [...] ...|$|E
40|$|We {{propose a}} novel <b>block-row</b> {{partitioning}} method {{in order to}} improve the convergence rate of the block Cimmino algorithm for solving general sparse linear systems of equations. The convergence rate of the block Cimmino algorithm depends on the orthogonality among the block rows obtained by the partitioning method. The proposed method takes numerical orthogonality among block rows into account by proposing a row inner-product graph model of the coefficient matrix. In the graph partitioning formulation defined on this graph model, the partitioning objective of minimizing the cutsize directly corresponds to minimizing the sum of inter-block inner products between block rows thus leading to an improvement in the eigenvalue spectrum of the iteration matrix. This in turn leads to a significant {{reduction in the number of}} iterations required for convergence. Extensive experiments conducted on a large set of matrices confirm the validity of the proposed method against a state-of-the-art method. Comment: A draft of the manuscript that will be submitted to a SIAM Journa...|$|E
40|$|Numerical {{handling}} of partial differential equations (PDEs) plays {{a crucial role}} in modeling physical processes. It involves discretization of these PDEs using for example finite difference or finite element methods and often requires the solution of large sparse linear systems. The linear systems at hand may be solved using direct or iterative methods. Direct methods are more reliable and are used when the lower and upper triangular factors do not run out of memory due to fill-in. Iterative algorithms are more amenable to parallelism and may be applied to solve larger linear systems. We focus on the parallel iterative algorithms for large sparse nonsymmetric linear systems. The most popular iterative schemes for these problems are part of the Krylov subspace family of methods and include BiCGStab, GMRES and CGNR methods. We look at their reliability using ILU/IQR-preconditioning techniques and suggest two alternative schemes. The first is a hybrid scheme based on algebraic domain decomposition techniques that uses direct and iterative algorithms to solve in parallel a single linear system. The second is a novel iterative scheme that uses a deflation-based preconditioned <b>block-row</b> projection method with an outer iterative solver to solve in parallel a single linear system. ...|$|E

