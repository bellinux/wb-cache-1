275|10000|Public
2500|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with {{probability}} density function [...]|$|E
2500|$|Theorem (Salem–Zygmund): Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> {{distributed}} uniformly on , and , where ...|$|E
2500|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> whose {{distribution}} can {{be described}} by the cumulative distribution function [...]|$|E
5000|$|Since V <b>is</b> <b>a</b> <b>random</b> <b>variable</b> and m_0 <b>is</b> <b>a</b> {{constant}} (...) , {{the false}} positive ratio <b>is</b> also <b>a</b> <b>random</b> <b>variable,</b> ranging between 0-1.|$|R
5000|$|... where X <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with {{probability}} distribution F(x).|$|R
30|$|It {{is assumed}} that the wind speed <b>is</b> <b>a</b> <b>random</b> <b>variable.</b>|$|R
2500|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with a {{countable}} set of finite outcomes , , ..., occurring with probabilities , , ..., respectively, ...|$|E
2500|$|Let X (integrable) <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with finite {{expected}} value μ and finite non-zero variance σ2. Then for any real number , ...|$|E
2500|$|Let N <b>be</b> <b>a</b> <b>random</b> <b>variable,</b> {{independent}} of the sequence, and suppose that N has a Poisson distribution with mean [...] Then the random sum ...|$|E
5000|$|... <b>is</b> <b>a</b> <b>random</b> <b>variable</b> on that {{probability}} {{space with}} finite expectation.|$|R
5000|$|If X|N <b>is</b> <b>a</b> {{binomial}} (N,p) <b>random</b> <b>variable,</b> where parameter N <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with Poisson (μ) distribution, then X <b>is</b> distributed as <b>a</b> Poisson (μp).|$|R
5000|$|... where Y <b>is</b> <b>a</b> {{logistic}} <b>random</b> <b>variable,</b> X <b>is</b> <b>a</b> half-logistic <b>random</b> <b>variable.</b>|$|R
2500|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with {{a finite}} number of finite {{outcomes}} , , ..., [...] occurring with probabilities , , ..., , respectively. The expectation of [...] is defined as ...|$|E
2500|$|Proposition (extremal {{property}} of [...] Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable,</b> and [...] Then [...] and [...] are finite, and [...] {{is the best}} least squares approximation for [...] among constants. Specifically, ...|$|E
2500|$|Assume, for {{the sake}} of the model, that {{surnames}} are passed on to all male children by their father. [...] Suppose the number of a man's sons to <b>be</b> <b>a</b> <b>random</b> <b>variable</b> distributed on the set{0,1,2,3,...}. [...] Further suppose the numbers of different men's sons to be independent random variables, all having the same distribution.|$|E
5000|$|Theorem: If X ≥ 0 <b>is</b> <b>a</b> <b>random</b> <b>variable</b> withfinite variance, then ...|$|R
5000|$|If X <b>is</b> <b>a</b> Binomial (n,p) <b>random</b> <b>variable,</b> and {{parameter}} p <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with beta (α, β) distribution, then X <b>is</b> distributed as <b>a</b> Beta-Binomial(α, β,n).|$|R
5000|$|If X <b>is</b> <b>a</b> negative-binomial (m,p) <b>random</b> <b>variable,</b> and {{parameter}} p <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with beta (α, β) distribution, then X <b>is</b> distributed as <b>a</b> Beta-Pascal(α, β,m).|$|R
2500|$|There is a {{straightforward}} {{extension of the}} vector version of Chebyshev's inequality to infinite dimensional settings. Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> which takes values in a Fréchet space [...] (equipped with seminorms [...] ). This includes most common settings of vector-valued random variables, e.g., when [...] is a Banach space (equipped with a single norm), a Hilbert space, or the finite-dimensional setting as described above.|$|E
2500|$|However, it does not. For {{the white}} noise, its {{integral}} from 0 to 1 should <b>be</b> <b>a</b> <b>random</b> <b>variable</b> distributed N(0,1). In contrast, the integral (from 0 to 1) of [...] is undefined. Even worse, ' fails {{to be almost}} surely measurable. Still worse, the probability of ' being measurable is undefined. And the worst thing: if X is a random variable distributed (say) uniformly on (0,1) and independent of ', then '(X) is not a random variable at all! (It lacks measurability.) ...|$|E
5000|$|The {{population}} {{value of}} distance covariance {{can be defined}} along the same lines. Let X <b>be</b> <b>a</b> <b>random</b> <b>variable</b> that takes values in a p-dimensional Euclidean space with probability distribution [...] and let Y <b>be</b> <b>a</b> <b>random</b> <b>variable</b> that takes values in a q-dimensional Euclidean space with probability distribution , and suppose that X and Y have finite expectations. Write ...|$|E
5000|$|... <b>be</b> <b>an</b> integer <b>random</b> <b>variable</b> where [...] is {{the maximum}} score for item i. That is, the <b>variable</b> [...] <b>is</b> <b>a</b> <b>random</b> <b>variable</b> {{that can take}} on integer values between 0 and a maximum of [...]|$|R
5000|$|If X|N <b>is</b> <b>a</b> {{binomial}} (N,p) <b>random</b> <b>variable,</b> where parameter N <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with negative-binomial (m, r) distribution, then X <b>is</b> distributed as <b>a</b> negative-binomial (m, r/(p+qr)).|$|R
5000|$|Suppose [...] <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with <b>a</b> {{standard}} normal distribution, whose {{density is}} ...|$|R
50|$|Let X <b>be</b> <b>a</b> <b>random</b> <b>variable</b> whose {{distribution}} can {{be described}} by the cumulative distribution function F.|$|E
5000|$|Let X <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with finite {{variance}} Var(x). Let Z be the standardised form {{defined as}} ...|$|E
5000|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> {{distributed}} {{with the}} skewed generalized t distribution.The [...] moment (i.e. [...] ), for , is: ...|$|E
5000|$|Theorem: If Z ≥ 0 <b>is</b> <b>a</b> <b>random</b> <b>variable</b> withfinite variance, and if , then ...|$|R
5000|$|The {{conditional}} variance {{tells us}} how much variance is left if we use [...] to [...] "predict" [...] Y.Here, as usual, [...] stands for the conditional expectation of Y given X,which we may recall, <b>is</b> <b>a</b> <b>random</b> <b>variable</b> itself (<b>a</b> function of X, determined up to probability one).As a result, [...] itself <b>is</b> <b>a</b> <b>random</b> <b>variable</b> (and <b>is</b> <b>a</b> function of X).|$|R
2500|$|Throughout the following, {{we assume}} that (X'n) <b>is</b> <b>a</b> {{sequence}} of <b>random</b> <b>variables,</b> and X <b>is</b> <b>a</b> <b>random</b> <b>variable,</b> {{and all of them}} are defined on the same probability space [...]|$|R
5000|$|Let [...] <b>be</b> <b>a</b> <b>random</b> <b>variable</b> over [...] and let [...] Let [...] be a 2-universal hash function. If ...|$|E
5000|$|For an illustrative example, let X <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with a {{standard}} normal distribution N(0,1). Then its CDF is ...|$|E
5000|$|Let X <b>be</b> <b>a</b> <b>random</b> <b>variable</b> with a {{discrete}} probability distribution p depending on a parameter θ. Then the function ...|$|E
30|$|The cost {{to service}} {{warranty}} claim (for repair/replacement of failed components) <b>is</b> <b>a</b> <b>random</b> <b>variable.</b>|$|R
30|$|If X <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with density {{function}} (24), we write X∼ XTGG (p,α,λ,β).|$|R
30|$|If X <b>is</b> <b>a</b> <b>random</b> <b>variable</b> with density {{function}} (25), we write X∼ MWG (p,α,λ,γ).|$|R
