15|14|Public
40|$|We {{consider}} {{extending the}} LARD system to handle dynamic documents {{as well as}} static documents. The front-end node directs incoming requests to the <b>back-end</b> <b>nodes</b> {{in a manner that}} increases the overall system throughput. This will be achieved by keeping information about the expected required resources to execute and deliver dynamic documents and about current resources available at the <b>back-end</b> <b>nodes.</b> Problem Statement In the LARD system, the front-end node distributes incoming web requests (for static web pages) to the <b>back-end</b> <b>nodes</b> in a manner that increases the overall aggregate system throughput (number of requests served per second). This is achieved by locality enhancement (locality-aware request distribution) and dynamic load balancing (by monitoring the number of open connections between the clients and the <b>back-end</b> <b>nodes).</b> Web requests for dynamic documents require more computing and I/O access to be carried out by the <b>back-end</b> <b>nodes.</b> Handling dynamic documents seems more c [...] ...|$|E
40|$|We {{consider}} cluster-based {{network servers}} {{in which a}} front-end directs incoming requests to one of a num-ber of back-ends. Specifically, we consider content-based request distribution: the front-end uses the content re-quested, in addition to information about the load on the <b>back-end</b> <b>nodes,</b> to choose which back-end will han-dle this request. Content-based request distribution can improve locality in the back-ends ’ main memory caches, increase secondary storage scalability by partitioning the server’s database, and provide the ability to employ <b>back-end</b> <b>nodes</b> that are specialized for certain types of requests. As a specific policy for content-based request dis-tribution, we introduce a simple, practical strategy for locality-aware request distribution (LARD). Wit...|$|E
40|$|Abstract—In {{this paper}} we show {{a small but}} fast popularity-based {{front-end}} cache can provide provable DDOS prevention for randomly partitioned cluster services with replication. To achieve this, we first give the best strategy for an adversary to overload the system, and then prove that the cache size is lower bounded by O(n log log n / log d), where n {{is the number of}} <b>back-end</b> <b>nodes</b> and d is the replication factor. Since log log n / log d < 2 holds for almost all the current clusters (i. e., the number of <b>back-end</b> <b>nodes</b> n < 105 and the replication factor d ≥ 3), this result implies an O(n) lower bound on the required cache size. Our analysis and results are well validated through extensive simulations. I...|$|E
3000|$|Identifier of metadata, quality attributes, and {{filtering}} {{policy are}} transmitted to NetworkService (in a HTTP POST), which is executed in the <b>Back-end</b> <b>node</b> (Fig.  8) [...]...|$|R
3000|$|When all tweets {{have been}} streamed, tweet {{extractor}} informs upper layer (QualityEvaluationService in the <b>Back-end</b> <b>node)</b> about completion (by providing a transaction identifier). The indication will be utilized for stopping {{of the data}} evaluation process (step 24 in Fig.  10) [...]...|$|R
3000|$|Calculating a {{corresponding}} polling weight value for each <b>back-end</b> service <b>node</b> {{according to the}} prediction results by using HHGA-RBF model; [...]...|$|R
40|$|We {{present a}} {{scalable}} architecture for content-aware request distribution in Web server clusters. In this architecture, a level- 4 switch {{acts as the}} point of contact for the server on the Internet and distributes the incoming requests {{to a number of}} <b>back-end</b> <b>nodes.</b> The switch does not perform any content-based distribution. This function is performed by each of the <b>back-end</b> <b>nodes,</b> which may forward the incoming request to another back-end based on the requested content. In terms of scalability, this architecture compares favorably to existing approaches where a front-end node performs content-based distribution. In our architecture, the expensive operations of TCP connection establishment and hando are distributed among the back-ends, rather than being centralized in the front-end node. Only a minimal additional latency penalty is paid for much improved scalability. We have implemented this new architecture, and we demonstrate its superior scalability by comparing it to a system tha [...] ...|$|E
40|$|We {{investigated}} {{extending the}} LARD system to handle web requests with dynamic content {{as well as}} static content. The front-end node directs incoming requests to the <b>back-end</b> <b>nodes</b> {{in a manner that}} increases the overall system throughput. This is achieved by maintaining information about the expected required resources to execute and deliver the web requests, and about current resources available at the <b>back-end</b> <b>nodes.</b> Introduction With the evolution of faster networks and higher computing power, cluster-based network servers combine cutting-edge performance and low cost. Static requests are more used than dynamic requests. However, it is expected that dynamic requests will be more widespread to enable distributed web services, especially online and real-time services. Although there are efficient mechanisms to serve web requests for static requests, there are few mechanisms to efficiently handle web requests with dynamic content. In the LARD system, the front-end node distributes in [...] ...|$|E
40|$|We {{describe}} a prototype scalable and highly available web server, built on an IBM SP- 2 system, and analyze its scalability. The system architecture {{consists of a}} set of logical front-end or network nodes and a set of back-end or data nodes connected by a switch, and a load balancing component. A combination of TCP routing and Domain Name Server (DNS) techniques are used to balance the load across the front-end nodes that run the Web ([URL] daemons. The scalability achieved is quantified and compared with that of the known DNS technique. The load on the <b>back-end</b> <b>nodes</b> is balanced by striping the data objects across the <b>back-end</b> <b>nodes</b> and disks. High availability is provided by detecting node or daemon failures and reconfiguring the system appropriately. The scalable and highly available web server is combined with parallel databases, and other back-end servers, to provide integrated scalable and highly available solutions. 1 Introduction With the explosion of traffic on the World Wi [...] ...|$|E
30|$|The {{external}} APIs {{were implemented}} based on REST-based communication paradigm. Interface messages were defined in {{extensible markup language}} (XML) format, and implemented with Jersey [45]. REST API of the QualityEvaluator in the <b>Back-end</b> <b>node</b> (Fig.  8) was implemented as an embedded Jetty server [46] (Jersey could also have been used). Word 2 Vec service’s API (Fig.  8) in the relevancy server was implemented as a TCP server, which accepted words of tweets as input, and returned calculated relevancy value as a response.|$|R
30|$|The {{stores have}} been {{implemented}} to Cassandra, which is deployed in the Front-end node. The Front-end node also provides a representational state transfer (REST) API for interacting with end users (provided by MetadataQualityManagement). The <b>Back-end</b> <b>node</b> performs data quality evaluation of tweets. QualityEvaluator provides a REST API for execution of quality evaluation operations based on requests received from the Front-end node. TwitterAnalysis component is executed in Spark Streaming [42] environment. Especially, QualityEvaluator deploys TwitterAnalysis process to a Spark cluster. The Relevancy node performs relevancy evaluation of tweets. Word 2 Vec-service provides a TCP socket interface for relevancy evaluation, which is utilized by the TwitterAnalysis-component.|$|R
30|$|Metadata {{management}} {{refers to}} creation of metadata, and {{providing access to}} it. MetadataCollectionEngine and MetadataSearchEngine components of the Front-end node (Fig.  5) encapsulated functionality of metadata management. Quality management refers to managing quality aspects of data sets with user-defined quality rules. In the prototype system MetadataQualityManagement, MetadataQualityEvaluator, and MetadataQualityPolicyManager of the Front-end node implemented quality management functionality (Fig.  5). Quality evaluation refers to analysing quality of social media data sets based on quality metrics, which have been selected to a context based on quality rules. In the prototype system quality evaluation was comprised of QualityEvaluator and TwitterAnalysis components of the <b>Back-end</b> <b>node</b> (Fig.  8), and Word 2 Vec-service of the relevancy-node (Fig.  4).|$|R
40|$|Wepresentanend-to-endsystemcapableofreal-timecapturingand {{displaying}} {{with full}} horizontal parallax high-quality 3 D video contents on a cluster-driven multiprojector light-field display. The capture component is {{an array of}} low-cost USB cameras connected to a singlePC. RawM-JPEGdatacomingfromthesoftware-synchronized cameras are multicast over Gigabit Ethernet to the <b>back-end</b> <b>nodes</b> oftherenderingcluster,wheretheyaredecompressedandrendered. Forall-in-focusrendering,view-dependentdepthisestimatedonthe GPUusingacustomizedmultiview space-sweepingapproachbased on fast Census-based area matching implemented in CUDA. Realtime performance is demonstrated on a system with 18 VGA cameras and 72 SVGA rendering projectors. Index Terms — Multi-view capture and display, GPU, light field renderin...|$|E
40|$|Load {{balancing}} requests {{across a}} cluster of back-end servers is critical for avoiding performance bottlenecks and meeting servicelevel objectives (SLOs) in large-scale cloud computing services. This paper shows how a small, fast popularity-based front-end cache can ensure load balancing for an important class of such services; furthermore, we prove an O(n log n) lower-bound on the necessary cache size and show that this size depends only on {{the total number of}} <b>back-end</b> <b>nodes</b> n, not the number of items stored in the system. We validate our analysis through simulation and empirical results running a key-value storage system on an 85 -node cluster...|$|E
40|$|We {{consider}} cluster-based {{network servers}} {{in which a}} front-end directs incoming requests to {{one of a number}} of back-ends. Speci cally, we consider content-based request distribution: the front-end uses the content requested, in addition to information about the load on the <b>back-end</b> <b>nodes,</b> to choose which back-end will handle this request. Content-based request distribution can improve locality in the back-ends ' main memory caches, increase secondary storage scalability by partitioning the server's database, and provide the ability to employ <b>back-end</b> <b>nodes</b> that are specialized for certain types of requests. As a speci c policy for content-based request distribution, we introduce a simple, practical strategy for locality-aware request distribution (LARD). With LARD, the front-end distributes incoming requests in a manner that achieves high locality in the back-ends' main memory caches as well as load balancing. Locality is increased by dynamically subdividing the server's working set over the back-ends. Trace-based simulation results and measurements on a prototype implementation demonstrate substantial performance improvements over state-of-the-art approaches that use only load information to distribute requests. On workloads with working sets that do not t in a single server node's main memory cache, the achieved throughput exceeds that of the state-of-the-art approach by a factor of two to four. With content-based distribution, incoming requests must be handed o to a back-end in a manner transparent to the client, after the front-end has inspected the content of the request. To this end, we introduce an e cient TCP hando protocol that can hand o an established TCP connection in a client-transparent manner...|$|E
30|$|Performance of {{data quality}} {{evaluation}} was studied. In the experiments 176, 478 tweets (~ 798  MB) were transmitted for quality evaluation. The tweets were {{extracted from the}} public Twitter API [49]. The average size of a tweet was ~ 4.6  KB. The experiments were executed within the Eucalyptus cloud computing environment (Fig.  4), where Front-end node had two vCPUs, and 4  GB RAM. <b>Back-end</b> <b>node</b> and relevancy nodes had six vCPUs and 40  GB RAM. Two vCPUs and 4  GB RAM was allocated for Twitter analysis-component (Fig.  8) within the Spark streaming cluster. An additional node (had six vCPUs and 40  GB RAM) was used for simulating a Twitter data source, which utilized Netcat for streaming of tweets using a TCP connection. TCP was used instead of HTTP to minimise protocol overhead in the experiments. The tests were performed five times, and average processing rate of tweets is reported.|$|R
40|$|This Paper {{presents}} the design, implementation {{and evaluation of}} a load balancer for cluster-based SIP servers. Our load balancer performs session-aware request assignment to ensure that SIP transactions are routed to the proper <b>back-end</b> <b>node</b> that contains the appropriate session state. We presented three novel algorithms: CJSQ, TJSQ, and TLWL. The TLWL algorithms result in the best performance, {{both in terms of}} response time and throughput, followed by TJSQ. TJSQ has the advantage that no knowledge is needed of relative overheads of different transaction types. SIP applications that require good quality of service, these dramatically lower response times are significant. We showed that these algorithms provide significantly better response time by distributing requests across the cluster more evenly, thus minimizing occupancy and the corresponding amount of time a particular request waits behind others for service. TLWL- 1. 75 provides 25 % better throughput than a standard hashbased algorithm and 14 % better throughput than a dynamic round-robin algorithm. ...|$|R
40|$|This paper {{describes}} our distributed architectural simulator {{of shared}} memory multiprocessors named Shaman. The simulator runs on a PC cluster {{that consists of}} multiple front-end nodes to simulate the instruction level behavior of a target multiprocessor in parallel and a <b>back-end</b> <b>node</b> to simulate the target memory system. The front-end also simulates the logical behavior of the shared memory using software DSM technique and generates memory references to drive the back-end. A remarkable feature of our simulator is the reference filtering {{to reduce the amount}} of the references transferred from the front-end to the backend utilizing the DSM mechanism and coherent cache simulation on the front-end. This technique and our sophisticated DSM implementation discussed in this paper give an extraordinary performance to the Shaman simulator. We achieved 335 million and 392 million simulation clock per second for LU decomposition and FFT in SPLASH- 2 kernel benchmarks respectively, when we used 16 front-end nodes to simulate a 16 -way target SMP. ...|$|R
40|$|This paper studies {{mechanisms}} and policies for supporting HTTP/ 1. 1 persistent connections in cluster-based Web servers that employ content-based request distribution. We present two mechanisms for the efficient, content-based distribution of HTTP/ 1. 1 requests among the <b>back-end</b> <b>nodes</b> of a cluster server. A trace-driven simulation shows that these mechanisms, {{combined with an}} extension of the locality-aware request distribution (LARD) policy are effective in yielding scalable performance for HTTP/ 1. 1 requests. We implemented the simpler of these two mechanisms, back-end forwarding. Measurements of this mechanism in connection with extended LARD on a prototype cluster, driven with traces from actual Web servers, confirm the simulation results. The throughput of the prototype is up to four times better than that achieved by conventional weighted round-robin request distribution. In addition, throughput with persistent connections is up to 26 % better than without...|$|E
40|$|We {{present an}} {{analytic}} technique for modeling load balancing policies on {{a cluster of}} servers conditioned {{on the fact that}} the service times of arriving tasks are drawn from heavy tail distributions. We propose a new modeling methodology for the exact solution of an M/Hk/ 1 server and illustrate its use for modeling two distinct load balancing policies in a distributed multi-server system. Our analytic results provide exact information regarding the distribution of task sizes that compose the waiting queue on each server and suggest an easy and inexpensive way to provide load balancing based on the sizes of the incoming tasks. 1. INTRODUCTION We consider the resource allocation problem in a distributed multi-server system. We assume that tasks arrive to a front-end system, which is responsible for dispatching them to the <b>back-end</b> <b>nodes.</b> This happens according to a task scheduling policy that aims to route the request to the " back-end server, since a task can potentially be serve [...] ...|$|E
40|$|Abstract: Differentiated service, {{as a key}} {{solution}} to meet the heterogenicity of Web clients ’ QoS requirements, has been widely used to optimize the server utilization without over-providing resources. Based on the relative differentiated service, this paper treats the application of proportional delay as a optimal control problem, and focuses on the cluster-side architecture improvement as well as QoS controller design. A load balancing Web cluster architecture supported differentiated service is proposed and implemented. By system identification and resource optimal control, the front-end dispatcher could adjust the resource quotas assigned to different classes in every single back-end server, and Multi-class based Maximum Idle First load balancing strategy is designed to ensure a fair resource consumption among <b>back-end</b> <b>nodes.</b> As a result, the end-to-end delay is controlled and proportional delay is guaranteed. The experiments demonstrate that no matter using Round-Robin, Least Connection Scheduling or Maximum Idle First load balancing strategy, the proposed resource optimal controller could hold the relationship among different classes. Compared t...|$|E
40|$|Widely adopted, distributor-based systems forward user {{requests}} {{to a balanced}} set of waiting servers in complete transparency to the users. The policy employed in forwarding requests from the front-end distributor to the backend servers {{plays an important role}} in the overall system performance. The locality-aware request distribution (LARD) scheme improves the system response time by having the requests serviced by the web servers that contain the data in their cache. In this paper, we propose a proactive request distribution (PRORD) to apply an intelligent proactive-distribution at the front-end and complementary pre-fetching at the <b>back-end</b> server <b>nodes</b> to acquire the data into their caches. The pre-fetching scheme fetches the web pages in advance into the memory based o...|$|R
40|$|The new {{generation}} of Web systems provides more complex services than those related to Web publishing sites. Users are increasingly reliant on the Web for up-to-date personal and business information and services. Web architectures able to guarantee the quality of service (QoS) that rules the relationship between users and Web service providers require a large investment in new algorithms and systems for dispatching, load balancing, and information consistency. In this paper, we consider Web cluster architectures composed of multiple <b>back-end</b> server <b>nodes</b> and one front-end dispatcher and we analyze how to provide differentiated service levels to various classes of users. We demonstrate through simulation experiments under realistic workload models that the proposed mechanisms are able to satisfy QoS requirements {{of the most valuable}} users classes, without impacting too negatively on the other users...|$|R
40|$|As web-based {{transactions}} {{become an}} essential element of everyday corporate and commerce activities, it becomes increasingly important that the performance of web-based services be predictable and guaranteed even in the presence of wildly fluctuating input loads. In this paper, we propose a general implementation framework to provide quality of service (QoS) guarantee for cluster-based Internet services, such as E-commerce or directory service. We describe the design, implementation, and evaluation of a web request distribution system called Gage, which can provide every subscriber with distinct guarantee on the number of generic web requests that are serviced per second regardless of the total input loads at run time. Gage {{is one of the first}} systems that can support QoS guarantee involving multiple system resources, i. e., CPU, disk, and network. The frontend request distribution server of Gage distributes incoming requests among a cluster of <b>back-end</b> web server <b>nodes</b> so as to maintain per-subscriber QoS guarantee and load balance among the back-end servers. Each <b>back-end</b> web server <b>node</b> includes a Gage module, which performs distributed TCP splicing and detailed resource usage accounting. Performance evaluation of the fully operational Gage prototype demonstrates that the proposed architecture can indeed provide the guaranteed request throughput for different classes of web accesses, even in the presence of excessive input loads. The additional performance overhead associated with QoS support in Gage is merely 3. 06 %. ...|$|R
40|$|Abstract—This paper {{describes}} a mechanism allowing nodes to hand-off active connections by utilizing connection splicing at an edge-switch {{serving as a}} gateway to a server cluster. The mechanism is primarily intended {{to be used as}} part of a content aware request distribution strategy. Our approach uses an extended form of network address translation which maps inbound connection information (ie. address, port, and sequence number) to a separate outbound connection. A key difference in our approach is that while the switch performs network address translation and TCP splicing, the actual hand-off is triggered by the <b>back-end</b> <b>nodes.</b> This relieves the switch of performing any application layer responsibilities. Nodes may hand-off connections by first initiating a new connection to the destination and then sending a message to the gateway which splices the two connections together. The gateway modifies subsequent packet headers {{in order to create a}} transparent hand-off. This mechanism requires no modification to the operating system on the servers or the clients and supports HTTP/ 1. 1 persistent connections and pipelined requests. To test our design, we implemented a soft-switch using Linux Netfilter which includes the extended network address translation. We provide some preliminary performance analysis and make recommendations for future work. I...|$|E
40|$|Abstract We {{present an}} {{analytic}} technique for modeling load balancing policies on {{a cluster of}} servers conditioned {{on the fact that}} the arriving tasks are drawn from heavy tail distributions. We propose a new modeling methodology for the exact solution of an M/Hk/ 1 server and illustrate its use for modeling two distinct load balancing policies in a distributed multi-server system. Our analytic results provide exact information regarding the distribution of task sizes that compose the waiting queue on each server and suggest an easy and inexpensive way to provide load balancing based on the sizes of the incoming tasks. 1 Introduction We consider the resource allocation problem in a distributed multi-server system. We assume that tasks arrive to a front-end system, which is responsible for dispatching them to the <b>back-end</b> <b>nodes.</b> This happens according to a task scheduling policy that aims to route the request to the &quot;best &quot; back-end server, since a task can potentially be served by any server. Such a system can be considered as an abstraction of a distributed web server [7, 10, 19]. Balancing the load across the back-end servers is critical for performance [7]. In the past two decades, there has been a significant research effort in task scheduling and load balancing (see [11] and references therein). The basic assumption in much of this work is that the service demands of the various tasks are governed by an exponential distribution. In contrast to the above assumption, there is very strong evidence that the size of web documents, and accordingly their service demands, are governed by heavy-tailed distributions [3, 4, 1, 2]. As a consequence, load balancing in distributed servers must be re-examined...|$|E
40|$|If {{you know}} how to use JavaScript in the browser, you already have the skills you need to put JavaScript to work on <b>back-end</b> servers with <b>Node.</b> This {{hands-on}} book shows you how to use this popular JavaScript platform to create simple server applications, communicate with the client, build dynamic pages, work with data, and tackle other tasks. Although Node has a complete library of developer-contributed modules to automate server-side development, this book will show you how to program with Node on your own, so you truly understand the platform. Discover firsthand how well Node works as a w...|$|R
40|$|Widely adopted distributor-based systems forward user {{requests}} {{to a balanced}} set of waiting servers in complete transparency to the users. The policy employed in forwarding requests from the front-end distributor to the backend servers dominates the overall system performance. The locality-aware request distribution (LARD) scheme improves the system response time by having the requests serviced by the web servers that contain the data in their caches. In this paper, we propose a proactive request distribution (PRORD) that applies an intelligent proactivedistribution at the front-end and complementary prefetching at the <b>back-end</b> server <b>nodes</b> to obtain the data of high relation to the previous requests in their caches. The pr-fetching scheme fetches the web pages in advance into the memory based on a confidence value of the web page, which is predicted by the proactive distribution scheme. Designed {{to work with the}} prevailing web technologies, such as HTTP 1. 1, our scheme aims to provide reduced response time to the users. Simulations carried out with traces derived from the log files of real web servers witness performance boost of 15 - 45 % compared to the existing distribution policies. 1...|$|R

