1|25|Public
40|$|Block of Na/Ca {{exchange}} (NCX) {{has potential}} therapeutic applications, in particular, if a mode-selective block could be achieved, but also carries serious risks for disturbing the normal Ca 2 + balance maintained by NCX. We {{have examined the}} effects of partial inhibition of NCX by SEA- 0400 (1 or 0. 3 mu mol/L) in left ventricular myocytes from healthy pigs or mice and from mice with heart failure (MLP-/-). During voltage clamp ramps with [Ca 2 +](i) <b>buffering,</b> <b>block</b> of reverse mode block was slightly larger than of forward mode (by 25 +/- 5 %, P < 0. 05). In the absence of [Ca 2 +](i) buffering and with sarcoplasmic reticulum (SR) fluxes blocked, rate constants for Ca 2 + influx and Ca 2 + efflux were reduced to the same extent (to 36 +/- 6 % and 32 +/- 4 %, respectively). With normal SR function the reduction of inward NCX current (I-NCX) was 57 +/- 10 % (n= 10); during large caffeine-induced Ca 2 + transients, it was larger (82 +/- 3 %). [Ca 2 +](i) transients evoked during depolarizing steps increased (from 424 +/- 27 to 994 +/- 127 nmol/L at + 10 mV, P < 0. 05), despite a reduction of I-CaL by 27 %. Resting [Ca 2 +](i) increased; there was a small decrease in the rate of decline of [Ca 2 +](i). SR Ca 2 + content increased more than 2 -fold. Contraction amplitude of field-stimulated myocytes increased in healthy myocytes but not in myocytes from MLP-/- mice, in which SR Ca 2 + content remained unchanged. These data provide proof-of-principle that even partial inhibition of NCX results in a net gain of Ca 2 +. Further development of NCX blockers, in particular, for heart failure, must balance potential benefits of I-NCX reduction against effects on Ca 2 + handling by refining mode dependence and/or including additional targets. status: publishe...|$|E
40|$|On-chip caches {{consume a}} {{significant}} fraction {{of energy in}} current microprocessors. Hence, hardware techniques such as <b>block</b> <b>buffering</b> have been developed and shown {{to be effective in}} reducing on-chip cache energy consumption. We are not aware of any software solutions to exploit <b>block</b> <b>buffering.</b> This paper presents a compiler-based approach that modifies both code and variable layout to effectively exploit <b>block</b> <b>buffering,</b> and is aimed at the class of embedded codes that make heavy use of scalar variables. Unlike previous work that uses only storage pattern optimization, our solution integrates both code restructuring and storage pattern optimization. Experimental results on a set of complete programs demonstrate that our solution leads to significant energy savings. ...|$|R
40|$|After {{completing}} this chapter, {{you should}} be able to: • enumerate necessary input data for physical design. • explain the effects of <b>buffering</b> disk <b>blocks</b> in main memory. • explain the basic structure of a B +-tree. • explain which queries can be evaluated faster using an index. • explain why indexes have not only advantages. • formulate the basic CREATE INDEX command in SQL...|$|R
40|$|Cuches usually {{consume a}} {{significant}} amount of energy in modern microprocessors (e. g. superpipelined or supersca-lar processors). In this paper; we examine contemporary cuche design techniques and provide an analytical model for estimating cache energy consumption. We also present several novel techniques for designing an energy efji-ciency cache, which include <b>block</b> <b>buffering,</b> cache sub-banking, and Gray code addressing. The experimental results suggest that both <b>block</b> <b>buffering</b> and Gray code addressing techniques are ideal for instruction cache designs which tend to be accessed in consecutive sequence. Cache sub-bunking is ideal,for both instruction and data caches. Overall, these techniques can achieve an order qf magnitude energy reduction on cuches. ...|$|R
40|$|On-chip caches {{consume a}} {{significant}} {{fraction of the}} energy in current microprocessors. As a result, architectural/circuit-level techniques such as <b>block</b> <b>buffering</b> and sub-banking have been proposed and shown to be very effective in reducing the energy consumption of on-chip caches. While {{there has been some}} work on evaluating the energy and performance impact of different <b>block</b> <b>buffering</b> schemes, we are not aware of software solutions to take advantage of on-chip cache block buffers. This article presents a compiler-based approach that modifies code and variable layout to take better advantage of <b>block</b> <b>buffering.</b> The proposed technique is aimed at a class of embedded codes that make heavy use of scalar variables. Unlike previous work that uses only storage pattern optimization or only access pattern optimization, we propose an integrated approach that uses both code restructuring (which affects the access sequence) and storage pattern optimization (which determines the storage layout of variables). We use a graph-based formulation of the problem and present a solution for determining suitable variable placements and accompanying access pattern transformations. The proposed technique has been implemented using an experimental compiler and evaluated using a set of complete programs. The experimental results demonstrate that our A preliminary version of this article appeared in Proceedings of the ACM/IEEE Internationa...|$|R
40|$|Organizational {{techniques}} for reducing energy dissipation in one:hip processor caches {{as well as}} off chip caches have been observed to provide substantial energy savings in a technology independent manner. We propose and evaluate the use of <b>block</b> <b>buffering</b> using multiple <b>block</b> buffers, subbanking and bit line isolation to reduce the power dissipation within on chip caches for superscalar CPUs. We use a detailed registerlevel superscalar simulator to glean transition counts that occur within various cache components during the execution of SPEC 95 benchmarks. These transition counts are fed into an energy dissipation model for a 0. 8 micron cache to allow power dissipation within various cache components to be estimated accurately. We show {{that the use of}} 4 block buffers, with subbanking and bit line isolation can reduce the energy dissipation of conventional caches very significantly, often by as much as 600 %...|$|R
40|$|We {{investigate}} {{the use of}} organizational alternatives that lead to more energy [...] efficient caches for contemporary microprocessors. Dissipative transitions {{are likely to be}} highly correlated and skewed in caches, precluding the use of simplistic hit/miss ratio based power dissipation models for accurate power estimations. We use a detailed register [...] level simulator for a typical pipelined CPU and its multi [...] level caches, and simulate the execution of the SPECint 92 benchmarks to glean accurate transition counts. A detailed dissipation model for CMOS caches is introduced for estimating the energy dissipation based on electrical parameters of a typical circuit implementation (reported in [WiJo 94]) and the transition counts collected by simulation. A <b>block</b> <b>buffering</b> scheme is presented to allow cache energy requirements to be reduced without increasing access latencies. We report results for a system with an off [...] chip L 2 cache. We conclude that <b>block</b> <b>buffering,</b> with sub [...] banking to be very effective in reducing energy dissipation in the caches, and in the off [...] chip I/O pad drivers...|$|R
40|$|Parallel I/O {{systems are}} an {{integral}} component of modern high performance systems, providing large secondary storage capacity, and having the potential to alleviate the I/O bottleneck of data intensive applications. In these systems the I/O buffer {{can be used for}} two purposes (a) improve I/O parallelism by <b>buffering</b> prefetched <b>blocks</b> and making the load on disks more uniform, and (b) improve I/O latency by caching blocks to avoid repeated disk accesses for the same block. To make best use of available parallelism and locality in I/O accesses, it is necessary to design and implement prefetching and buffer management algorithms that schedule reads intelligently so that the most useful blocks are prefetched into the buffer and the most valuable blocks are retained in the buffer when the need for evictions arises. This dissertation focuses on prefetching and buffer management algorithms for parallel I/O systems. Our aim is to exploit the high parallelism provided by multiple disks by usin [...] ...|$|R
40|$|In {{this report}} we {{consider}} the problem of merging two sorted lists of m and n keys each in-place. We survey known techniques for this problem, focussing on correctness and the attributes of Stability and Practicality. We demonstrate a class of unstable in-place merge algorithms that uses block rearrangement and internal buffering that actually does not merge {{in the presence of}} sufficient duplicate keys of a given value. We show four relatively simple block sorting techniques {{that can be used to}} correct these algorithms. In addition, we show relatively simple and robust techniques that does stable local block merge followed by stable block sort to create a merge. Our internal merge is base on Kronrod’s method of internal <b>buffering</b> and <b>block</b> partitioning. Using block size of O (√ m + n) we achieve complexity of no more than 1. 5 (m+n) +O (√ m + n lg(m+n)) comparisons and 4 (m+n) +O (√ m + n lg(m+n)) data moves. Using block size of O((m + n) / lg(m + n)) gives complexity of no more tha...|$|R
40|$|An open {{queueing}} {{network model}} (QNM) is proposed for wormhole-routed hypercubes with finite buffers and deterministic routing {{subject to a}} compound Poisson arrival process (CPP) with geometrically distributed batches or, equivalently, a generalised exponential (GE) interarrival time distribution. The GE/G/ 1 /K queue and appropriate GE-type flow formulae are adopted, as cost-effective building blocks, in a queue-by-queue decomposition of the entire network. Consequently, analytic expressions for the channel holding time, <b>buffering</b> delay, contention <b>blocking</b> and mean message latency are determined. The validity of the analytic approximations is demonstrated against results obtained through simulation experiments. Moreover, it is shown that the wormholerouted hypercubes suffer progressive performance degradation with increasing traffic variability (burstiness) ...|$|R
40|$|Classic caching {{algorithms}} leverage recency, access count, and/or other {{properties of}} cached blocks at per-block gran-ularity. However, for media such as flash which have perfor-mance and wear penalties for small overwrites, implement-ing cache policies at a larger granularity is beneficial. Re-cent {{research has focused}} on <b>buffering</b> small <b>blocks</b> and writ-ing in large granularities, called containers, but it has not explored the ramifications and best strategies for caching compound blocks consisting of logically distinct, but physi-cally co-located, blocks. Containers may have highly diverse blocks, with mixtures of frequently accessed, infrequently accessed, and invalidated blocks. We propose and evaluate Pannier, a flash cache middle-ware that provides high performance while extending flash lifespan. Pannier uses three main techniques: (1) leverag-ing block access counts to manage cache containers, (2) in-corporating block liveness as a property to improve flash cache space efficiency, and (3) designing a multi-step feed-back controller to ensure a flash cache does not wear out in its lifespan while maintaining performance. Our evaluation shows that Pannier improves flash cache performance and extends lifespan beyond previous per-block and container-aware caching policies. More fundamentally, our investiga-tion highlights the importance of creating new policies for caching compound blocks in flash...|$|R
30|$|One {{of the key}} {{challenges}} to implement 3 D convolution is how to keep a fast {{access to all the}} data elements needed for a 19 -point operations. As the data items are generally stored in one direction, when you want to access the data items in a 3 D pattern, you need to either buffer a large amount of data items or access them in a very slow nonlinear pattern. In our FPGA design, we solve this problem by <b>buffering</b> the current <b>block</b> we process into the BRAM FIFOs. ASC provides a convenient interface to automatically buffer the input values into BRAMs and the users can access them by specifying the cycle number that the value gets read in. Thus, we can easily index into the stream to obtain values already sent to the FPGA and perform the 3 D operator.|$|R
40|$|Abstract. In this paper, {{we examine}} {{different}} methods using techniques of <b>blocking,</b> <b>buffering,</b> and padding for efficient implementations of bit-reversals. We evaluate the merits {{and limits of}} each technique and its application and architecture-dependent conditions for developing cache-optimal methods. Besides testing the methods on different uniprocessors, we conducted both simulation and measurements on two commercialsymmetric multiprocessors (SMP) to provide architecturalinsights into the methods and their implementations. We present two contributions in this paper: (1) Our integrated blocking methods, which match cache associativity and translation-lookaside buffer (TLB) cache size and which fully use the available registers, are cache-optimal and fast. (2) We show that our padding methods outperform other software-oriented methods, and we believe they are the fastest in terms of minimizing both CPU and memory access cycles. Since the padding methods are almost independent of hardware, they could be widely used on many uniprocessor workstations and multiprocessors...|$|R
40|$|VLSI devices {{with high}} power demands have several {{important}} drawbacks; power {{to run the}} chip must be supplied externally, and power is dissipated as heat, which must {{be removed from the}} circuit. Processor architects tend to view these issues as circuit technology or packaging problems. However, these solutions are limited, and do not necessarily provide insight into more direct approaches to energyefficient architecture. In this paper, we present a model for estimating dynamic instruction fetch energy dissipation for CMOS microprocessors as a function of architectural parameters, cache events, and instruction traffic. Starting with a parameterized baseline design for a simple pipelined RISC, we use the model and software simulation to explore the energy-budget implications of architectural and cache design decisions. We evaluate two instruction set encodings that yield significantly different density and traffic, and show how increased code density, a cache <b>block</b> <b>buffering</b> sc [...] ...|$|R
40|$|JPEG 2000 is a {{recently}} standardized image compression system that provides substantial improvements over the existing JPEG compression scheme. This improvement in performance {{comes with an}} associated cost in increased implementation complexity, such that a purely software implementation is inefficient. This work identifies the arithmetic coder as a bottleneck in efficient hardware implementations, and explores various design options to improve arithmetic coder speed and size. The designs produced improve the critical path of the existing arithmetic coder designs, and then extend the coder throughput to 2 or more symbols per clock cycle. Subsequent work examines more system level implementation issues. This work examines the communication between hardware blocks and utilizes certain modes of operation to add flexibility to buffering solutions. It becomes possible to significantly {{reduce the amount of}} intermediate <b>buffering</b> between <b>blocks,</b> whilst maintaining a loose synchronization. Full hardware implementations of the standard are necessarily limited in the amount of features that they can offer, in order to constrain complexity and cost. To circumvent this, a hardware / software codesign is produced using the Altera NIOS II softcore processor. By keeping the majority of the standard implemented in software and using hardware to accelerate those time consuming functions, generality of implementation can be retained, whilst implementation speed is improved. In addition to this, there is the opportunity to explore parallelism, by providing multiple identical hardware blocks to code multiple data units simultaneously...|$|R
40|$|This paper {{presents}} {{a structure of}} TLB (translation lookaside buffer) for low power consumption but high performance. The propsed TLB is constructed as a combination of one block buffer and two-way banked TLBs. The processor can access the block buffer or one of two banked TLBs selectively. This feature {{is quite different from}} that used in the traditional <b>block</b> <b>buffering</b> technique. Simulation results show its effectiveness in terms of power consumption and Energy*Delay product. The proposed TLB can reduce power consumptions by about 40 %, 10 %, 23 %, and 23 %, compared with a FA (fully associative) -TLB, a micro-TLB, a victim-TLB, and a banked-TLB respectively. Also the proposed TLB can reduce Energy*Delay products by about 38 %, 28 %, 21 %, and 21 %, compared with a FA-TLB, a micro-TLB, a victim-TLB, and a banked-TLB respectively. Therefore the proposed TLB can achieve low power consumption and high performance with a simple architecture...|$|R
40|$|Endocannabinoids (eCBs) {{modulate}} synaptic transmission in the brain, {{but little}} is known of their reg- ulatory role in nigral dopaminergic neurons, and whether transmission to these neurons is tonically inhibited by eCBs as seen in some other brain regions. Using whole-cell recording in midbrain slices, we observed potentiation of evoked IPSCs (eIPSCs) in these neurons after blocking CB 1 receptors with rimonabant or LY- 320, 135, indicating {{the presence of an}} eCB tone reducing inhibitory synaptic trans- mission. Increased postsynaptic calcium <b>buffering</b> and <b>block</b> of mGluR 1 or postsynaptic G-protein coupled receptors prevented this potentiation. Increasing spillover of endogenous glutamate by inhib- iting uptake attenuated eIPSC amplitude, while enhancing the potentiation by rimonabant. Group I mGluR activation transiently inhibited eIPSCs, which could be prevented by GDP-b-S, increased calcium buffering or rimonabant. We explored the possibility that the dopamine-derived eCB N-arachidonoyl dopamine (NADA) is involved. The eCB tone was abolished by preventing dopamine synthesis, and enhanced by L-DOPA. It was not detected in adjacent non-dopaminergic neurons. Preventing 2 -AG synthesis did not affect the tone, while inhibition of NADA production abolished it. Quantification of ventral midbrain NADA suggested a basal level that increased following prolonged depolarization or mGluR activation. Since block of the tone was not always accompanied by attenuation of depolarization- induced suppression of inhibition (DSI) and vice versa, our results indicate DSI and the eCB tone are mediated by distinct eCBs. This study provides evidence that dopamine modulates the activity of SNc neurons not only by conventional dopamine receptors, but also by CB 1 receptors, potentially via NADA...|$|R
40|$|In {{parallel}} I/O systems the I/O buffer {{can be used}} {{to improve}} I/O parallelism by improving I/O latency by caching blocks to avoid repeated disk accesses for the same block, and also by <b>buffering</b> prefetched <b>blocks</b> and making the load on disks more uniform. To make best use of available parallelism and locality in I/O accesses, it is necessary to design prefetching and caching algorithms that schedule reads intelligently so that the most useful blocks are prefetched into the buffer and the most valuable blocks are retained in the buffer when the need for evictions arises. This dissertation focuses on algorithms for buffer management in parallel I/O systems. Our aim is to exploit the high parallelism provided by multiple disks to reduce the average read latency seen by an application. The thesis is that traditional greedy strategies fail to exploit I/O parallelism thereby necessitating new algorithms {{to make use of the}} available I/O resources. We show that buffer management in parallel I/O systems is fundamentally different from that in systems with a single disk, and develop new algorithms that carefully decide which blocks to prefetch and when, together with which blocks to retain in the buffer. Our emphasis is on designing computationally simple algorithms for optimizing the number of I/Os performed. We consider two classes of I/O access patterns, read-once and read-often, based on the frequency of accesses to the same data. With respect to buffer management for both classes of accesses, we identify fundamental bounds on performance of online algorithms, study the performance of intuitive strategies, and present randomized and deterministic algorithms that guarantee higher performance...|$|R
40|$|On [...] chip caches in {{high end}} superscalar {{microprocessors}} dissipate a large {{fraction of the}} total power due to high density of transistors and high operating frequency. Organizational techniques such as <b>block</b> <b>buffering</b> and subbanking have been observed to provide substantial energy savings in a technology independent manner. We propose and evaluate the advantages of using multiple block buffers and bit line isolation without compromising the cache cycle time in on [...] chip caches for superscalar CPUs. We also study the behavior of on [...] chip cache power with the degree of instruction dispatch in the superscalar pipeline. A superscalar cache power estimation tool (SCAPE) is used to simulate the execution of SPEC 95 benchmarks and measure the transition activity within the various cache components. The transition counts are fed into an energy dissipation model for a 0. 8 micron cache to estimate the power dissipation within various cache components. We show that the use of 4 [...] 8 block buffers provide an additional saving of 31 [...] 38 % over single block buffer and when used in conjunction with subbanking and bit line isolation, a substantial reduction of 70 [...] 75 % is achievable compared to conventional cache...|$|R
40|$|Bit-reversals are {{representative}} and important data reordering operations in many scientific computations. Performance degradation is {{mainly caused by}} cache conflict misses. Bit-reversals are often repeatedly used as fundamental subroutines for many scientific programs. Thus, {{in order to gain}} the best performance, cache-optimal methods and their implementations should be carefully and precisely done at the programming level. This type of performance programming for some special programs, such as the data reorderings, may significantly outperform an optimization from an automatic tool, such as a compiler. In this paper, we examine different methods using techniques of <b>blocking,</b> <b>buffering,</b> and padding for efficient implementations. We evaluate the merits and limits of each technique and their application and architecture-dependent conditions for developing cache-optimal methods. We present two contributions in this paper: (1) Our integrated blocking methods, which match cache associativity and TLB cache size and which fully use the available registers are cache-optimal and fast. (2) We show that our padding methods outperform other software oriented methods, and believe they are the fastest in terms of minimizing both CPU and memory access cycles. Since the padding methods are almost independent of hardware, they could be widely used on many uniprocessor workstations and SMP multiprocessors. ...|$|R
40|$|The {{translation}} lookaside buffer (TLB) is {{an essential}} component used {{to speed up the}} virtual-to-physical address translation. Due to frequent lookup, however, the power consumption of the TLB is usually considerable. This paper presents an energy-efficient TLB design for the embedded processors. In our design, we first propose a real-time filter scheme to facilitate the <b>block</b> <b>buffering</b> to eliminate the redundant TLB accesses without comparator delay. By modifying the address registers {{to be sensitive to the}} contents variation, the proposed real-time filter can distinguish the redundant TLB access as soon as the virtual address is generated. The second technique is a banking-like design, which aims to reduce the energy consumption per TLB access in case of block buffer miss. To alleviate the performance penalty introduced by the conventional banking technique, we develop two adaptive variants of the banked TLB. Both variants can achieve the high energy efficiency as the banked TLB while maintaining the low miss ratio as the nonbanked TLB. The experimental results show that by filtering out all the redundant TLB accesses and then minimizing the energy consumption per access, without any performance penalty our design can effectively improve the Energy*Delay product of the TLB, especially for the data TLB with poor locality...|$|R
40|$|AbstractÐEfficient and {{effective}} <b>buffering</b> of disk <b>blocks</b> in main memory {{is critical for}} better file system performance due to a wide speed gap between main memory and hard disks. In such a buffering system, {{one of the most}} important design decisions is the block replacement policy that determines which disk block to replace when the buffer is full. In this paper, we show that there exists a spectrum of block replacement policies that subsumes the two seemingly unrelated and independent Least Recently Used (LRU) and Least Frequently Used (LFU) policies. The spectrum is called the LRFU (Least Recently/Frequently Used) policy and is formed by how much more weight we give to the recent history than to the older history. We also show that there is a spectrum of implementations of the LRFU that again subsumes the LRU and LFU implementations. This spectrum is again dictated by how much weight is given to recent and older histories and the time complexity of the implementations lies between O(1) (the time complexity of LRU) and O…log 2 n† (the time complexity of LFU), where n is the number of blocks in the buffer. Experimental results from trace-driven simulations show that the performance of the LRFU is at least competitive with that of previously known policies for the workloads we considered. Index TermsÐBuffer cache, LFU, LRU, replacement policy, trace-driven simulation. ...|$|R
40|$|Efficient and {{effective}} <b>buffering</b> of disk <b>blocks</b> in main memory {{is critical for}} better file system performance due to a wide speed gap between main memory and hard disks. In such a buffering system, {{one of the most}} important design decisions is the block replacement policy that determines which disk block to replace when the buffer is full, In this paper, we show that there exists a spectrum of block replacement policies that subsumes the two seemingly unrelated and independent Least Recently Used (LRU) and Least Frequently Used (LFU) policies. The spectrum is called the LRFU (Least Recently/Frequently Used) policy and is formed by how much more weight we give to the recent history than to the older history. We also show that there is a spectrum of implementations of the LRFU that again subsumes the LRU and LFU implementations. This spectrum is again dictated by how much weight is given to recent and older histories and the time complexity of the implementations lies between O(1) (the time complexity of LRU) and O(log(2) n) (the time complexity of LFU), where n is the number of blocks in the buffer, Experimental results from trace-driven simulations show that the performance of the LRFU is at least competitive with that of previously known policies for the workloads we considered. clos...|$|R
40|$|Improving {{the quality}} of {{software}} artifacts and products is an essential activity for everyone working {{on the development of}} software. Testing is one approach to reveal defects and faults in software. In recent years, message-passing systems have grown to a significant degree due to the rise of distributed systems, embedded systems, and so forth. In message-passing systems, components communicate with each other through sending and receiving messages. This message-passing mechanism introduces new opportunities for testing programs {{due to the fact that}} the time a message is delivered is not guaranteed, so the order in which messages are delivered is also not guaranteed. This non-determinism introduces interleaving and parallelization and subsequently a new source of software defects like race conditions. In this thesis, we have explained a new approach to testing a given component for identifying software faults related to the order in which messages are received by that component. We reorder messages coming to a certain component and deliver them in a different distinct ordering each time. We have three different methods for achieving message reordering: <b>Blocking,</b> <b>Buffering,</b> and Adaptive Buffering. We evaluate the effectiveness of our new testing methods using four metrics: Ordering Coverage, Coverage Rate, Slowdown Overhead, and Memory Overhead. We have implemented our Reordering Framework on QNX Neutrino 6. 5. 0 and compared our reordering methods with each other and with the naive random case using our experiments. We have also showed that our testing approach applies to real programs and can reveal real bugs in software...|$|R

