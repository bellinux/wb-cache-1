1386|281|Public
5|$|The ARC {{became the}} driving force behind the design and {{development}} of the oN-Line System (NLS). He and his team developed computer interface elements such as bitmapped screens, the mouse, hypertext, collaborative tools, and precursors to the graphical user interface. He conceived and developed many of his user interface ideas in the mid-1960s, long before the personal computer revolution, at a time when most computers were inaccessible to individuals who could only use computers through intermediaries (see <b>batch</b> <b>processing),</b> and when software tended to be written for vertical applications in proprietary systems.|$|E
5|$|While a {{graduate}} student and member of TMRC, Dennis introduced his students to the TX-0 on loan to MIT indefinitely from Lincoln Laboratory. In the spring of 1959, McCarthy taught the first course in programming that MIT offered to freshmen. Outside classes, Kotok, David Gross, Peter Samson, Robert A. Saunders and Robert A. Wagner, all friends from TMRC, reserved time on the TX-0. They {{were able to use}} the TX-0 as a personal, single-user tool rather than a <b>batch</b> <b>processing</b> system, thanks to Dennis, faculty advisors and John McKenzie, the operations manager.|$|E
5|$|At first clients sent hole-punched {{accounting}} records to a Reynolds processing center, which would print a complete accounting that is {{sent back to}} the client by mail. The development of modems and internet technology in the 1970s led to several advancements. Reynolds provided 3,600 specialized modems to dealerships between 1974 and 1978. The modems communicated with Reynolds' VIM-brand minicomputers at 80 Reynolds locations, which provided computing power and printed forms. This eliminated the need for clients to ship data to Reynolds in tapes and allowed daily access to online services. By the end of the 1970s, <b>batch</b> <b>processing</b> and computer processing centers were being phased out in response to personal computers kept at the dealership. In the years 1978 and 1982, Reynolds introduced VIM-brand computer systems that were kept at dealerships.|$|E
5000|$|Interactive and <b>batch</b> RAW <b>processing</b> (non-destructive RAW editing) ...|$|R
5000|$|<b>Batching</b> and <b>Processing</b> Proof Work (On-Us/Not-On-Us Checks, Payment Coupons, Counter Slips, etc.) ...|$|R
50|$|The Apache Flink {{platform}} for distributed stream and <b>batch</b> data <b>processing</b> is built upon Akka.|$|R
25|$|The <b>batch</b> <b>processing</b> feature permits {{users to}} submit {{more than one}} {{sequence}} to Phyre2 by uploading a file of sequences in FASTA format. By default, users have a limit of 100 sequences in a batch. This limit can be raised by contacting the administrator.|$|E
25|$|The initial {{version of}} TTM was {{implemented}} {{to run in}} a conversational manner under the Caltech Basic Time Sharing System for the IBM System/360 Model 50. Other versions have been written {{to run in the}} <b>batch</b> <b>processing</b> environment of OS/360 and to operate in front of or in conjunction with various language processors.|$|E
25|$|Support for 5250 display {{operations}} is provided via display files, an interface between workstations, keyboards and displays, and interactive applications, {{as opposed to}} <b>batch</b> <b>processing</b> {{with little or no}} user interaction. ASCII terminals and PC workstations are equally and well supported, also via internet or LAN network access supplemented by either IBM or non-IBM communication software, for example TELNET or TELNET 5250.|$|E
5000|$|BatchPipes is a <b>batch</b> job <b>processing</b> utility {{designed}} for the MVS/ESA operating system, and all later incarnations - OS/390 and z/OS.|$|R
40|$|This paper {{describes}} a workflow manager developed and deployed at Yahoo called Nova, which pushes continuallyarriving data through graphs of Pig programs executing on Hadoop clusters. (Pig is a structured dataflow language and runtime for the Hadoop map-reduce system.) Nova is like data stream managers in {{its support for}} stateful incremental processing, but unlike them in that it deals with data in large <b>batches</b> using disk-based <b>processing.</b> <b>Batched</b> incremental <b>processing</b> is {{a good fit for}} a large fraction of Yahooâ€™s data processing use-cases, which deal with continually-arriving data and benefit from incremental algorithms, but do not require ultra-low-latency processing...|$|R
50|$|On IBM mainframes, BatchPipes is a <b>batch</b> job <b>processing</b> utility {{which runs}} under the MVS/ESA {{operating}} system and later versions - OS/390 and z/OS.|$|R
25|$|Innovative Routines International (IRI), Inc. is an American {{software}} company first known for bringing mainframe sort merge functionality into open systems. IRI {{was the first}} vendor to develop a commercial replacement for the Unix sort command, and combine data transformation and reporting in Unix <b>batch</b> <b>processing</b> environments. In 2007, IRI's coroutine sort ("CoSort") became the first product to collate and convert multi-gigabyte XML and LDIF files, join and lookup across multiple files, and apply role-based data privacy functions (including AES-256 encryption) for fields within sensitive files.|$|E
25|$|Control Data Corporation {{developed}} the SCOPE operating {{system in the}} 1960s, for <b>batch</b> <b>processing.</b> In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was {{an extension of the}} Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois {{developed the}} PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.|$|E
25|$|With the 1950s came {{increasing}} {{awareness of}} the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from <b>batch</b> <b>processing</b> to online modes, from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with library programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, {{as well as other}} professional programs, such as law and medicine in their curriculum. By the 1980s, large databases, such as Grateful Med at the National Library of Medicine, and user-oriented services such as Dialog and Compuserve, were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous special interest groups to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web.|$|E
5000|$|A <b>batch</b> {{transaction}} <b>processing</b> system, which accepts {{large volumes}} of homogeneous transactions, processes it (possibly updating a database), and produces output such as reports or computations.|$|R
50|$|Notes: In {{the process}} below, it is arguable {{that the change}} {{committee}} should be responsible not only for accept/reject decisions, but also prioritization, which influences how change requests are <b>batched</b> for <b>processing.</b>|$|R
40|$|The problem {{considered}} is make {{to order}} situations where products due dates {{and raw materials}} delivery times are constraints that the user must try to satisfy or negotiate. Those constraints, coupled with recipe and resource constraints, can be conveniently handled through constraints propagation techniques both in planning and scheduling. An interactive planning system is discussed where the user gets, after each decision, a feedback in terms of <b>batches</b> <b>processing</b> time windows, equipment units load and estimated resources utilization. APS-like scheduling heuristics exploit these measures to implement internal procedures to select batches in the sequential scheduling procedure...|$|R
500|$|Around {{the same}} time, the EZPpay system was {{expanded}} for overseas {{use during the}} aftermath of the 1992-1995 War in Bosnia and Herzegovina, where U.S. personnel were deployed on peace-keeping missions. [...] Named [...] "EagleCash", the overseas system functions similarly to the EZpay system, but with the added ability of soldiers to attach personal bank accounts to the card, allowing them to load, and reload, without having to access their financial institutions back home. [...] As 386th Air Expeditionary Wing financial manager, Catherine Miles explained in a 2007 article, [...] "It's like a gift card. [...] You can put as little or as much money as you want on it and it comes from your checking account." [...] Unlike regular debit cards, the Eagle Cash is managed on-base, using <b>batch</b> <b>processing</b> which ensures that the cards remained useful even when connections to banks and credit unions State-side are severed. [...] The system was given widespread acceptance in 1999, just before the War in Iraq; it has since been expanded to many military bases such as Camp Anaconda on the front lines.|$|E
2500|$|The {{following}} {{description is}} taken from the original TTM reference manual [...] and the subsequent <b>batch</b> <b>processing</b> extension.|$|E
2500|$|The hot {{end of a}} glassworks {{is where}} the molten glass is formed into glass products, {{beginning}} when the batch is fed into the furnace at a slow, controlled rate by the <b>batch</b> <b>processing</b> system (batch house). The furnaces are natural gas- or fuel oil-fired, and operate at temperatures up to [...] The temperature is limited only {{by the quality of}} the furnaceâ€™s superstructure material and by the glass composition. [...] Types of furnaces used in container glass making include 'end-port' (end-fired), 'side-port', and 'oxy-fuel'. [...] Typically, furnace [...] "size" [...] is classified by metric tons per day (MTPD) production capability.|$|E
50|$|The Optimized Local Adapters are {{implemented}} as an external subsystem to IMS. Usage is supported for Message <b>Processing</b> Programs (MPP), <b>Batch</b> Message <b>Processing</b> programs (BMP), IMS Fast Path (IFP) and Batch DL/I applications.|$|R
5000|$|Batch-O-Matic: a Windows utility for <b>processing</b> <b>batch</b> files with {{external}} variable lists.|$|R
5000|$|Ability {{to break}} volume into chunks then rejoin for <b>batch</b> of {{parallel}} <b>processing.</b>|$|R
2500|$|In April 1961 a Philco 2000 {{computer}} {{had been}} installed in Building P4 Annex at Ent AFB for dedicated 1st Aero use. [...] The Phlco 2000 {{was considered the}} fastest computer {{in the world at}} the time of installation. [...] 1st Aero still used IBM punch cards for data, as did Project Space Track. [...] The Philco 2000 was infamous for devouring the cards. [...] The computer was programmed using FORTRAN for <b>batch</b> <b>processing</b> and the TAC assembly language for other work. However, Orbital Analysts still had Friden Square Root Calculators on their desks, a necessary tool.|$|E
2500|$|Through the 1950s, {{many major}} {{features}} were pioneered {{in the field}} of operating systems on mainframe computers, including <b>batch</b> <b>processing,</b> input/output interrupt, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. [...] In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094.|$|E
2500|$|<b>Batch</b> <b>processing</b> {{is one of}} {{the initial}} steps of the glass-making process. [...] The batch house simply houses the raw {{materials}} in large silos (fed by truck or railcar) and holds anywhere from 1â€“5 days of material. [...] Some batch systems include material processing such as raw material screening/sieve, drying, or pre-heating (i.e. cullet). [...] Whether automated or manual, the batch house measures, assembles, mixes, and delivers the glass raw material recipe (batch) via an array of chutes, conveyors, and scales to the furnace. [...] The batch enters the furnace at the 'dog house' or 'batch charger'. [...] Different glass types, colors, desired quality, raw material purity / availability, and furnace design will affect the batch recipe.|$|E
5000|$|PowerBuilder.NET 12.5 {{introduces}} {{support for}} multi-threading (included in Classic for many releases), {{the ability to}} consume RESTful (Representational State Transfer) Web Services and <b>Batch</b> Command <b>Processing.</b> For the [...] "classic" [...] PowerScript, various smaller enhancements were added,too.|$|R
5000|$|<b>Batch</b> File Based <b>Processing</b> (emulates some {{of actual}} stream processing, but much lower {{performance}} in general) ...|$|R
50|$|The ARSA system {{continuously}} collects xenon {{from the}} air in <b>batch</b> mode, <b>processing</b> approximately 48 m3 in an 8-hour period. The average amount of xenon collected in this period is approximately 2-cc. The xenon gas is then transferred into nuclear detection system consisting of a beta-gamma coincidence spectrometer.|$|R
2500|$|Anaerobic {{digestion}} can {{be performed}} as a batch process or a continuous process. In a batch system, biomass {{is added to the}} reactor {{at the start of the}} process. The reactor is then sealed for the duration of the process. In its simplest form <b>batch</b> <b>processing</b> needs inoculation with already processed material to start the anaerobic digestion. In a typical scenario, biogas production will be formed with a normal distribution pattern over time. Operators can use this fact to determine when they believe the process of digestion of the organic matter has completed. There can be severe odour issues if a batch reactor is opened and emptied before the process is well completed. A more advanced type of batch approach has limited the odour issues by integrating anaerobic digestion with in-vessel composting. In this approach inoculation takes place through the use of recirculated degasified percolate. After anaerobic digestion has completed, the biomass is kept in the reactor which is then used for in-vessel composting before it is opened [...] As the batch digestion is simple and requires less equipment and lower levels of design work, it is typically a cheaper form of digestion. Using more than one batch reactor at a plant can ensure constant production of biogas.|$|E
2500|$|The {{majority}} of these software packages are designed for analyzing genetic sequence data. [...] For example, PAML {{is a collection of}} programs for the phylogenetic analysis of DNA and protein sequence alignments by maximum likelihood. Ancestral reconstruction can be performed using the codeml program. [...] In addition, [...] is a collection of Python scripts that wrap the ancestral reconstruction functions of PAML for <b>batch</b> <b>processing</b> and greater ease-of-use. , , and [...] are also software packages for the phylogenetic analysis of sequence data, but are designed to be more modular and customizable. [...] HyPhy implements a joint maximum likelihood method of ancestral sequence reconstruction that can be readily adapted to reconstructing a more generalized range of discrete ancestral character states such as geographic locations by specifying a customized model in its batch language. [...] Mesquite provides ancestral state reconstruction methods for both discrete and continuous characters using both maximum parsimony and maximum likelihood methods. [...] It also provides several visualization tools for interpreting the results of ancestral reconstruction. [...] MEGA is a modular system, too, but places greater emphasis on ease-of-use than customization of analyses. [...] As of version 5, MEGA allows the user to reconstruct ancestral states using maximum parsimony, maximum likelihood, and empirical Bayes methods.|$|E
2500|$|An {{operating}} system (OS) is software (programs and data) {{that runs on}} computers and manages the computer hardware and provides common services for efficient execution of various application software. For hardware functions such as input and output and memory allocation, the {{operating system}} acts as an intermediary between application programs and the computer hardware, although the application code is usually executed directly by the hardware, but will frequently call the OS or be interrupted by it. Operating systems are found on almost any device that contains a computerâ€”from cellular phones and video game consoles to supercomputers and web servers. The GM-NAA I/O, created by Owen Mock and Bob Patrick of General Motors Research Laboratories in early 1956 (or late 1955) for their IBM 701 mainframe computer is {{generally considered to be}} the first [...] "batch processing" [...] operating system and possibly the first [...] "real" [...] operating system. Rudimentary forms of operating systems existed before <b>batch</b> <b>processing,</b> the Input/Output Control System (IOCS) being one example. However, what specifically differentiated and made the GM-NAA I/O as {{the first of its kind}} was that instead of having a human operator manually load each program as what previous systems were only capable of doing, computerized software as used on GM-NAA I/O, thereafter handled the scheduling, management, and multi-tasking of all computer applications.|$|E
40|$|Basic algorithmic, {{implementation}} and numerical {{issues involved in}} the new, subspacebased linear multivariable system identification toolbox [...] -SLIDENT [...] -incorporated in the freely available Subroutine Library in Control Theory (SLICOT) are described. Reliability, efficiency, and ability to solve large, industrial identification problems received a special consideration in the toolbox development process. Flexibility of usage is enhanced by providing various options: two algorithmic subspace-based approaches (MOESP and N 4 SID) and their combination, standard or fast techniques for data compression (including abilities to exploit the block-Hankel structure), multiple (possibly connected) data <b>batches</b> <b>processing,</b> availability of both, fully documented drivers and computational routines, the use of structure exploiting algorithms and dedicated linear algebra tools, etc. In addition, optionally, {{the quality of the}} intermediate results can be assessed by inspecting the associated co [...] ...|$|R
40|$|Design, {{synthesis}} {{and scheduling}} issues are considered simultaneously for multipurpose <b>batch</b> plants. <b>Processing</b> recipes {{are represented by}} the State-Task Network. The proposed model {{takes into account the}} trade-offs between capital costs, revenues and operational flexi-bility. Both linear and nonlinear examples are studied, resulting in MILP and MINLP problems, respectively. Comparisons with another approach are presented...|$|R
40|$|The {{development}} and {{capabilities of the}} Montana geodata system are discussed. The system is entirely dependent on the state's central data processing facility which serves all agencies and is therefore restricted to <b>batch</b> mode <b>processing.</b> The computer graphics equipment is briefly described along with its application to state lands and township mapping {{and the production of}} water quality interval maps...|$|R
