88|85|Public
5000|$|In telecommunication, {{the term}} <b>bandwidth</b> <b>compression</b> has the {{following}} meanings: ...|$|E
50|$|<b>Bandwidth</b> <b>compression</b> {{implies a}} {{reduction}} in normal bandwidth of an information-carrying signal without reducing the information content of the signal. This can be accomplished with lossless data compression techniques. For more information read the Increasing speeds section in the Modem article. <b>Bandwidth</b> <b>Compression</b> is a core feature of WAN Optimization appliances to improve bandwidth efficiency.|$|E
5000|$|... 1981-86: Build {{first ever}} digital TV {{platform}} with ITT at General Electric {{as well as}} member of team for first ever <b>Bandwidth</b> <b>Compression</b> Set-Top-Box. Member of GE Financial Management Program ...|$|E
5000|$|Compression, {{which reduces}} storage costs and I/O <b>bandwidth.</b> High <b>compression</b> is {{possible}} because columns of homogeneous datatype are stored together and because updates {{to the main}} store are batched.|$|R
30|$|Part of {{the problem}} is the fact that {{transmitting}} decent streaming video quality requires a large amount of <b>bandwidth.</b> Video <b>compression</b> techniques do exist to help alleviate the problem, but certain techniques are susceptible to packet losses in transmitted compressed video streams, which could damage video quality [4, 5].|$|R
40|$|A raw video {{takes up}} a lot of space for storage and {{bandwidth}} for transmission. Uncompressed video from a video recording device may take up about 20 MB per second of video. Thus, there is a clear necessity for an efficient mechanism which enables the video to be stored and transmitted over limited <b>bandwidths.</b> <b>Compression</b> can be broadly classified into two types, lossy compression and lossless compression. In lossy compression, the video is compressed to very low bit rates which considerably reduce the quality of the video whereas in lossless compression no loss of data is present and it is more visually appealing than lossy compression. This paper discusses about H. 264 video compression standard which is a motion-block oriented codec standard developed by ITU-T. The aim of this algorithm is to provide visually better video quality with fewer amount of information transfer. Various motion estimation algorithms are also studied...|$|R
50|$|Voice over IP (VoIP) Essentials - {{teaches the}} {{principles}} of transmitting voice calls and fax over the Internet, and explores VoIP networks, <b>bandwidth</b> <b>compression,</b> the gateway, packet prioritization, RSVP, H.320 and H.323, and WAN engineering issues.|$|E
50|$|This {{has led to}} the use of Voice Coders (vocoders) {{to achieve}} tight <b>bandwidth</b> <b>compression</b> of the speech signals. NSA's STU-III, KY-57 and SCIP are {{examples}} of systems that operate over existing voice circuits. The STE system, by contrast, requires wide bandwidth ISDN lines for its normal mode of operation. For encrypting GSM and VoIP, which are digital anyway, the standard protocol ZRTP could be used as an end-to-end encryption technology.|$|E
5000|$|Neural signals {{responding}} to {{motions of the}} basilar membrane show responses in one direction as in rectification. At all but low frequencies, the neural measure averages over multiple cycles to give the equivalent of rectification followed by averaging (low-pass filtering). Over the entire cochlea, response shows as a pattern that varies more slowly that the applied frequency but that does follow the envelope of the applied signal. Each group of cells can {{give rise to a}} semi-periodic wave that can be analyzed by neurons in the brain. The total pattern that arises from a sound can thus {{be thought of as a}} two-dimensional pattern in time with one axis being the distance along the basilar membrane and the other being distance along some sequence of neurons. These patterns, varying at rates less than lower audio frequencies, have shapes that can be identified much like patterns in vision. The concept of the [...] "neural analyzer" [...] as an extension of cochlear patterns is discussed in U. S. patent 3,387,093, [...] "Speech <b>Bandwidth</b> <b>Compression</b> System", June 4, 1968 (filed in 1964).|$|E
40|$|The {{file size}} and picture quality are factors to be {{considered}} for streaming, storage and transmitting videos over networks. This work compares Cinepak, Intel, Microsoft Video and Indeo Codec for video compression. The peak signal to noise ratio is used to compare the quality of such video compressed using AVI codecs. The most widely used objective measurement by developers of video processing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise Ration is measured on a logarithmic scale and depends on the mean squared error (MSE) between an original and an impaired image or video, relative to (2 n- 1) 2. Previous research done regarding assessing of video quality has been mainly by the use of subjective methods, and there is still no standard method for objective assessments. Although it has been considered that compression might not be significant in future as storage and transmission capabilities improve, but at low <b>bandwidths</b> <b>compression</b> makes communication possible. Comment: 13 pages, 1 figure, 7 tables, journal pape...|$|R
50|$|ATSC 3.0 {{will provide}} even more {{services}} to the viewer and increased <b>bandwidth</b> efficiency and <b>compression</b> performance, which requires breaking backwards compatibility with the current version. ATSC 3.0 is expected to emerge within the next decade.|$|R
40|$|The Cassini Radio and Plasma Wave Science {{experiment}} {{will employ}} data compression to make {{effective use of}} the available data telemetry <b>bandwidth.</b> Some <b>compression</b> will be achieved by use of a lossless data compression chip and some by software in a dedicated 80 C 85 processor. A description of the instrument and data compression system are included in this report. Also, the selection of data compression systems and acceptability of data degradation is addressed...|$|R
50|$|Melpar {{performed}} diverse {{research in}} physical and life sciences over 40 years. One project {{begun in the}} late 1950s (foreshadowing artificial intelligence) linked biology and the design of electrical devices-computer programs that emulated artificial nerve cells and simulated functions such as learning ability and initiative. The intent was to construct a thinking machine, similar to a human nervous system that learned pattern recognition and avoided mistakes (e.g. programming a mobile satellite on the moon to avoid a deep precipice). Studies {{on the nature of}} speech led to more efficient use of the radio spectrum through <b>bandwidth</b> <b>compression,</b> and improved long distance and coded communications-other related studies focused on elimination of speech deficiencies and development of a phonetic typewriter. In medical research Melpar developed synthetic materials (tissue growth) to be compatible with the heart (later known as the Jarvik heart), and produced an electronic heart monitor (Cardiac Sentry) which detected out-of-norm variations, maintained hospital records and administered prescribed treatments. The company was a pioneer in plasma physics and developed materials for communicating with manned space vehicles reentering the atmosphere. Using solar energy, desalinization experiments were conducted to make saline and brackish water drinkable. Detection systems to measure highway curves, motions and vibrations were developed to improvemass transit systems.|$|E
40|$|A novel electro-holographic <b>bandwidth</b> <b>compression</b> technique, fringelet <b>bandwidth</b> <b>compression,</b> is {{described}} and implemented. This technique uses spatial subsampling {{to reduce the}} bandwidth and complexity of holographic fringe computation for real-time 3 -D holographic displays. Fringelet <b>bandwidth</b> <b>compression</b> {{is a type of}} diffraction-specific fringe computation, an approach that considers only the reconstruction process in holographic imaging. The fringe pattern is treated as a spectrum that is sampled in space (as holographic elements or "hogels") and in spatial frequency (as "hogel vectors"). Fringelet <b>bandwidth</b> <b>compression</b> achieves a compression ratio of 16 : 1 without conspicuously degrading image quality. Further increase in compression ratio and speed is possible with additional image degradation. Fringelet decoding is extremely simple, involving the replication of fringelet sample values. This simplicity enables an overall increase in fringe computation speed of over 3000 times co [...] ...|$|E
40|$|Graduation date: 1968 This paper {{presents}} a theoretical study of real-time <b>bandwidth</b> <b>compression</b> systems using the Hilbert transform and associated analytic signal concept. An analytic signal {{is defined as}} u(t) = s(t) + [superscript ¯]js(t), where [superscript ¯]s(t) is the Hilbert transform of the signal s(t). A complete discussion of the properties of analytic signals is included. Speech <b>bandwidth</b> <b>compression</b> is emphasized, but the proposed compression schemes {{can be applied to}} any narrow-band (band-limited signal. The extent of compression depends largely upon the characteristics of the input signal. Two presently used real-time speech <b>bandwidth</b> <b>compression</b> systems, the Vobanc and channel Vocoder, are described in order to provide a comparison between the proposed systems and existing systems designed without recourse to analytic signal ideas. A recently introduced two-to-one <b>bandwidth</b> <b>compression</b> method based upon the analytic signal is described in detail. Methods for achieving bandwidth compressions by factors greater than two are considered. A new real-time <b>bandwidth</b> <b>compression</b> system referred to as the "repeated half-angle" system is suggested to obtain an eight-to-one speech bandwidth reduction. This system is relatively easy to implement as compared to present systems. The performance of this system is highly dependent upon the accuracy of the many multipliers, squarers, and root extractors required...|$|E
40|$|Uncompressed {{multimedia}} such as data, image, {{audio and}} video requires transmission bandwidth and storage capacity. Due to demand for data storage capacity and data transmission <b>bandwidth,</b> Image <b>compression</b> is used which provides data security, reduces the storage requirements, provides rich quality signals for visual data representation and reduces the image fidelity. The focus {{of this study is}} the Discrete Cosine Transform (DCT) and Inverse Discrete Cosine Transform (IDCT) based on the sub-band decomposition for image compression. The simulated results are shown that sub-band decomposition based DCT provides good compression rate and better peak signal to noise ratio than conventional DCT...|$|R
30|$|Micro-data centre [28] is a {{small and}} fully {{functional}} data centre containing multiple servers and is capable of provisioning many virtual machines. Many technologies, including Fog computing, can benefit from Micro data centres as it reduces latency, enhances reliability, relatively portable, has built-in security protocols, saves <b>bandwidth</b> consumption by <b>compression</b> and can accommodate many new services.|$|R
40|$|Motion {{estimation}} is {{a fundamental}} step for high quality, low <b>bandwidth</b> video <b>compression.</b> Recently the MPEG- 4 group has proposed some low complexity algorithms. They have almost the same performance in term of PSNR of the Full Search algorithm, {{but at the same}} time the complexity is dramatically reduced. However it is difficult to realize these algorithms with conventional hardware structures. For this reason we presented a Hardware Oriented Region Based algorithm (HORB) with similar performances, but that can be implemented with a simple hardware structure. Here we present an architecture tailored to meet the design constraint of the HORB algorithm (and at the same time capable of realizin...|$|R
40|$|This paper investigates <b>bandwidth</b> <b>compression</b> of delta {{modulated}} video signals using {{a variety}} of novel techniques. The video signal is encoded at 16 MHz with an adaptive delta modulator (ADM) using the Song video mode algorithm. One method of achieving <b>bandwidth</b> <b>compression</b> is through the detection and reduction of steady state patterns in the DM bit stream. Further reduction is obtained by deleting the horizontal and vertical drive information. Finally, a modified ADM algorithm is introduced to increase the frequency of steady state patterns. When the modified ADM is combined with entropy coding a 30 - 50 % data compression can be realized...|$|E
3000|$|... where α is the <b>bandwidth</b> <b>compression</b> factor, α=Δf×T; Δf is the {{subcarrier}} spacing; T is the SEFDM symbol interval; N is {{the number}} of subcarriers; sl,n is the nth subcarrier in the lth SEFDM symbol.|$|E
40|$|An {{approach}} to hybrid digital–analog (HDA) source–channel coding for the communication of analog sources over memoryless Gaussian channels is introduced. The HDA system, which exploits {{the advantages of}} both digital and analog systems, generalizes a scheme previously presented by the authors, and can operate for any bandwidth ratio (<b>bandwidth</b> <b>compression</b> and expansion). It is based on vector quantization and features turbo coding in its digital component and linear/nonlinear processing in its analog part. Simulations illustrate that, under both <b>bandwidth</b> <b>compression</b> and expansion modes of operation, the HDA system provides a robust and graceful performance with good reproduction fidelity {{for a wide range}} of channel conditions...|$|E
40|$|Digital video {{transmission}} over wireless channels is a challenging problem. Wireless channels have high bit error rates and low bandwidths that conflicts high <b>bandwidth</b> and high <b>compression</b> requirements of digital video. In this {{paper we address}} the various aspects of this problem and present a review of current state-of-art solutions in the field. The techniques reviewed include system layer issues as well as source and channel coding methods...|$|R
40|$|Cryptography {{is one of}} the {{technological}} means to provide security to data beingtransmitted on information and communication systems. When it is necessary to securelytransmit data in limited <b>bandwidth,</b> both <b>compression</b> and encryption must be performed. Researchers have combined compression and encryption together to reduce the overallprocessing time. In this paper, new partial encryption schemes are proposed to encrypt only part of thecompressed image. Soft and hard threshold compression methods are used in thecompressionstep and the Advanced Encryption Standard (AES) cipher is used forthe encryption step. The effect of different threshold values on the performance of the proposed schemes are studied. The proposed partial encryption schemes are fast, secure, and do not reduce the compression performance of the underlying selected compression methods...|$|R
40|$|Image {{compression}} {{is useful}} {{because it helps}} to reduce the consumption of expensive resources, such as hard disk space or transmission <b>bandwidth.</b> Many <b>compression</b> techniques are in place {{but there is a}} scope for high compression with good reconstruction of original image. The problem of reconstruction of data from sub signals from each filter channel is removed by lifting scheme because this structure itself assures reversibility. In this paper we are introducing lifting scheme with SPIHT coder for image compression and the results are obtained for compression ratio, PSNR and RMSE. A study in lossless image compression using the lifting scheme is presented. We first suggest why lossless image compression is an important issue and the general idea behind lifting...|$|R
40|$|We {{show that}} light pulses can be stopped and stored all-optically, with {{a process that}} {{involves}} an adiabatic and reversible pulse <b>bandwidth</b> <b>compression</b> occurring entirely in the optical domain. Such a process overcomes the fundamental bandwidth-delay constraint in optics, and can generate arbitrarily small group velocities for light pulses with a given bandwidth, {{without the use of}} any coherent or resonant light-matter interactions. We exhibit this process in optical resonator systems, where the pulse <b>bandwidth</b> <b>compression</b> is accomplished only by small refractive index modulations performed at moderate speeds. (Accepted for publication in Phys. Rev. Lett. Submitted on Sept. 10 th 2003) Comment: 18 pages including 3 figures. Accepted for publication in Phys. Rev. Let...|$|E
40|$|Modified {{electronic}} voice encoder /Vocoder/ {{includes an}} independent analog {{mode of operation}} {{in addition to the}} conventional digital mode. The Vocoder is a <b>bandwidth</b> <b>compression</b> equipment that permits voice transmission over channels, having {{only a fraction of the}} bandwidth required for conventional telephone-quality speech transmission...|$|E
40|$|Hybrid quantum {{networks}} rely on efficient interfacing of dissimilar quantum nodes, since elements {{based on}} parametric down-conversion sources, quantum dots, color centres or atoms are fundamentally different in their frequencies and bandwidths. While pulse manipulation {{has been demonstrated}} in very different systems, to date no interface exists that provides both an efficient <b>bandwidth</b> <b>compression</b> and a substantial frequency translation at the same time. Here, we demonstrate an engineered sum-frequency-conversion process in Lithium Niobate that achieves both goals. We convert pure photons at telecom wavelengths to the visible range while compressing the bandwidth {{by a factor of}} 7. 47 under preservation of non-classical photon-number statistics. We achieve internal conversion efficiencies of 75. 5 %, significantly outperforming spectral filtering for <b>bandwidth</b> <b>compression.</b> Our system thus makes the connection between previously incompatible quantum systems as a step towards usable quantum networks...|$|E
30|$|Changes in both {{compression}} {{technologies and}} uses of bandwidth suggest caution in interpreting raw <b>bandwidth.</b> As <b>compression</b> technology continues to improve, {{a fixed amount}} of data may require less bandwidth. At the same time, changing uses of bandwidth can result in demand shifts. For example, as video compression technology improves, demand for transmitting videos may increase thus requiring increased capacity. Furthermore, there are country-specific policies which may result in different demand patterns. For instance, {{at the time of}} this writing, China is blocking access to Google and related sites. Finally, it is important to keep in mind that our analysis focuses solely on international bandwidth as measured by direct country to country connections. This makes sense given our focus on global interconnectedness, but it also means that we are ignoring domestic bandwidth.|$|R
40|$|Photonic time stretch {{significantly}} {{extends the}} effective bandwidth of existing analog-to-digital convertors by {{slowing down the}} input high-speed RF signals. Non-uniform photonic time stretch further enables time bandwidth product reduction in RF signal detection by selectively stretching high-frequency features more. However, it requires the prior knowledge of spectral-temporal distribution of the input RF signal and has to reconfigure the time stretch filter for different RF input signals. Here we propose {{for the first time}} an adaptive non-uniform photonic time stretch method based on microwave photonics pre-stretching that achieves blind detection of high-speed RF signals with reduced time bandwidth product. Non-uniform photonic time stretch using both quadratic and cubic group delay response has been demonstrated and time <b>bandwidth</b> product <b>compression</b> ratios of 72 % and 56 % have been achieved respectively...|$|R
50|$|The target application, normal mapping, is an {{extension}} of bump mapping that simulates lighting on geometric surfaces by reading surface normals from a rectilinear grid analogous to a texture map - giving simple models the impression of increased complexity. This additional channel however increases the load on the graphics system's memory <b>bandwidth.</b> Pre-existing lossy <b>compression</b> algorithms implemented on consumer 3D hardware lacked the precision necessary for reproducing normal maps without excessive visible artefacts, justifying the development of 3Dc.|$|R
40|$|In this paper, {{we propose}} a multirate {{teletraffic}} loss {{model of a}} single link with certain bandwidth capacity that accommodates Poisson arriving calls, which can tolerate <b>bandwidth</b> <b>compression</b> (elastic traffic), under the threshold policy. When compression occurs, the service time of new and in-service calls increases. The threshold policy provides different QoS among service-classes by {{limiting the number of}} calls of a service-class up to a pre-defined threshold, which can be different for each service-class. Due to the <b>bandwidth</b> <b>compression</b> mechanism, the steady state probabilities in the proposed model do not have a product form solution. However, we approximate the model by a reversible Markov chain, and prove recursive formulas for the calculation of call blocking probabilities and link utilization. The accuracy of the proposed formulas is verified through simulation and found to be very satisfactory...|$|E
40|$|The current {{literature}} on image <b>bandwidth</b> <b>compression</b> was surveyed and those methods relevant to compression of multispectral imagery were selected. Typical satellite multispectral data was then analyzed statistically {{and the results}} used to select a smaller set of candidate <b>bandwidth</b> <b>compression</b> techniques particularly relevant to earth resources data. These were compared using both theoretical analysis and simulation, under various criteria of optimality such as mean square error (MSE), signal-to-noise ratio, classification accuracy, and computational complexity. By concatenating {{some of the most}} promising techniques, three multispectral data compression systems were synthesized which appear well suited to current and future NASA earth resources applications. The performance of these three recommended systems was then examined in detail by all of the above criteria. Finally, merits and deficiencies were summarized and a number of recommendations for future NASA activities in data compression proposed...|$|E
40|$|Abstract — We {{consider}} {{the transmission of}} a bivariate Gaussian source S = (S 1, S 2) across a power-limited two-user Gaussian broadcast channel. User i (i = 1, 2) observes the transmitted signal corrupted by Gaussian noise with power σ 2 i and wants to estimate Si. We study hybrid digital-analog (HDA) joint source-channel coding schemes and analyze these schemes to obtain achievable (squared-error) distortion regions. Two cases are considered: 1) source and channel bandwidths are equal, 2) broadcasting with <b>bandwidth</b> <b>compression.</b> We adapt HDA schemes of Wilson et al. [1] and Prabhakaran et al. [2] to provide various achievable distortion regions for both cases. Using numerical examples, we demonstrate that for <b>bandwidth</b> <b>compression,</b> a three-layered coding scheme consisting of analog, superposition, and Costa coding performs well {{compared to the other}} provided HDA schemes. In the case of matched bandwidth, a three-layered coding scheme with an analog layer and two layers, each consisting of a Wyner-Ziv coder followed by a Costa coder, performs best. I...|$|E
40|$|Nowadays, {{still images}} are used {{everywhere}} in the digital world. The shortages of storage capacity and transmission <b>bandwidth</b> make efficient <b>compression</b> solutions essential. A revolutionary mathematics tool, wavelet transform, has already shown its power in image processing. The major topic of this paper, is improve the compresses of still images by Multiwavelet based on estimation the high Multiwavelet coefficients in high frequencies sub band by interpolation instead of sending all Multiwavelet coefficients. When comparing the proposed approach with other compression methods Good result obtained...|$|R
40|$|The use of {{bandwidth}} constrained {{links in}} wireless networks necessitates {{the use of}} <b>bandwidth</b> saving header <b>compression</b> schemes. In these schemes, a compressor and a decompressor collaborate to encode bulky IP headers into streamlined compressed headers. Intuitively, the gain from compression is the average compressed header length divided by the uncompressed header size. In this paper, we show {{that this is not}} true in many situations: this does not account for the cost introduced by the variability of the header sizes during compression context initialization...|$|R
40|$|The {{past decade}} has {{witnessed}} a proliferation of personal computers at homes, businesses, classrooms, libraries, etc. Most often, these systems are used to disseminate information. Recently, multimedia repositories have added to the excitement of this information age by allowing a user to retrieve and manipulate continuous media data types (audio and video objects). The design and implementation of these systems is challenging due to both the large size of objects that constitute this media type and their continuous <b>bandwidth</b> requirement. <b>Compression</b> {{in combination with the}} availability of fast CPUs (for real-time decompression) provide effective support for a continuous display of those objects with a high bandwidth requirement. Hierarchical storage structures (consisting of RAM, disk and tertiary storage devices) provide a cost-effective solution for the large size of their repositories. The focus of this study is on personal computers (single user, single display) that employ fast C [...] ...|$|R
