75|63|Public
5000|$|... 6 (<b>bucket</b> <b>size</b> for digits of 0: 002, 024, 045, 066, 075, 090) ...|$|E
5000|$|<b>Bucket</b> <b>size</b> {{planning}} long-term planning {{approach in}} Scrumban, {{which is based}} on moving the plans through a few steps.|$|E
5000|$|The {{expected}} <b>bucket</b> <b>size</b> {{while using}} 2-choice hashing is: [...] This improvement {{is due to}} the randomized concept known as The Power of Two Choices.|$|E
40|$|We {{investigate}} a combinatorial two-player game, {{in which one}} player wants to keep the behavior of an underlying water-bucket system stable whereas the other player wants to cause overflows. This game is motivated by data management applications in wireless sensor networks. We construct optimal strategies and characterize optimal <b>bucket</b> <b>sizes</b> for many instances of this game...|$|R
50|$|As is true {{with all}} hash tables, the {{performance}} {{is based on the}} largest bucket. Although there are instances where <b>bucket</b> <b>sizes</b> happen to be large based on the values and the hash functions used, this is rare. Having two hash functions and, therefore, two possible locations for any one value, makes the possibility of large buckets even more unlikely to happen.|$|R
50|$|The {{quadratic}} {{size of the}} k2 space {{ensures that}} randomly creating a table with collisions is infrequent and independent {{of the size of}} k, providing linear amortized construction time. Although each second-level table requires quadratic space, if the keys inserted into the first-level hash table are uniformly distributed, the structure as a whole occupies expected O(n) space, since <b>bucket</b> <b>sizes</b> are small with high probability.|$|R
50|$|Their {{tramming}} capacities {{varies from}} 1 to 17-25 metric tons. Their <b>bucket</b> <b>size</b> varies from 0.8 to 10 m3. Bucket height range from 1.8 to 2.5 m.|$|E
5000|$|Let's {{assume that}} for this {{particular}} example, the <b>bucket</b> <b>size</b> is 1. The first two keys to be inserted, k1 and k2, can be distinguished by the most significant bit, and would be inserted into the table as follows: ...|$|E
5000|$|NO. 48: Hydraulic Excavator UH03 is {{the first}} evolved type, made in Japan in 1965, having double {{hydraulic}} pumps and double valves with <b>bucket</b> <b>size</b> 0.35 m³ and engine output 58 hp. The excavators made in Japan before UH03 are single hydraulic pump and single valve type under technical tied up with Europe. - Ibaraki Prefecture ...|$|E
40|$|Abstract: A file of fixed-length {{records in}} {{auxiliary}} storage using a key-to-address transformation to assign records to addresses is con-sidered. The file {{is assumed to}} be in steady state, that is that the rates of additions to and of deletions from the file are equal. The loading factors that minimize file maintenance costs in terms of storage space and additional accesses are computed for different <b>bucket</b> <b>sizes</b> and different operational conditions...|$|R
50|$|Top-down radix sort {{can be seen}} as {{a special}} case of bucket sort where both the range of values and the number of buckets is {{constrained}} to be a power of two. Consequently, each <b>bucket's</b> <b>size</b> is also a power of two, and the procedure can be applied recursively. This approach can accelerate the scatter phase, since we only need to examine a prefix of the bit representation of each element to determine its bucket.|$|R
40|$|We {{demonstrate}} that parallel deterministic sample sort for many-core GPUs (GPU Bucket Sort) {{is not only}} considerably faster than the best comparison-based sorting algorithm for GPUs (Thrust Merge [Satish et. al., Proc. IPDPS 2009]) but also as fast as randomized sample sort for GPUs (GPU Sample Sort [Leischner et. al., Proc. IPDPS 2010]). However, deterministic sample sort has the advantage that <b>bucket</b> <b>sizes</b> are guaranteed and therefore its running time {{does not have the}} input data dependent fluctuations that can occur for randomized sample sort...|$|R
50|$|Bucket sort {{can be seen}} as a {{generalization}} of counting sort; in fact, if each bucket has size 1 then bucket sort degenerates to counting sort. The variable <b>bucket</b> <b>size</b> of bucket sort allows it to use O(n) memory instead of O(M) memory, where M is the number of distinct values; in exchange, it gives up counting sort's O(n + M) worst-case behavior.|$|E
5000|$|Counting filters {{provide a}} way to {{implement}} a delete operation on a Bloom filter without recreating the filter afresh. In a counting filter the array positions (buckets) are extended from being a single bit to being an n-bit counter. In fact, regular Bloom filters {{can be considered as}} counting filters with a <b>bucket</b> <b>size</b> of one bit. Counting filters were introduced by [...]|$|E
5000|$|Now, if k3 {{were to be}} hashed to the table, it wouldn't {{be enough}} to {{distinguish}} all three keys by one bit (because both k3 and k1 have 1 as their leftmost bit). Also, because the <b>bucket</b> <b>size</b> is one, the table would overflow. Because comparing the first two most significant bits would give each key a unique location, the directory size is doubled as follows: ...|$|E
40|$|We {{present and}} {{evaluate}} GPU Bucket Sort, a parallel deterministic sample sort algorithm for many-core GPUs. Our method is considerably faster than Thrust Merge (Satish et. al., Proc. IPDPS 2009), the best comparison-based sorting algorithm for GPUs, {{and it is}} {{as fast as the}} new randomized sample sort for GPUs by Leischner et. al. (to appear in Proc. IPDPS 2010). Our deterministic sample sort has the advantage that <b>bucket</b> <b>sizes</b> are guaranteed and therefore its running time does not have the input data dependent fluctuations that can occur for randomized sample sort...|$|R
40|$|This paper {{details the}} {{implementation}} and trialling of a prototype in-bucket bulk density monitor on a production dragline. Bulk density information can provide feedback to mine planning and scheduling to improve blasting and consequently facilitating optimal <b>bucket</b> <b>sizing.</b> The bulk density measurement builds upon outcomes {{presented in the}} AMTC 2009 paper titled ‘Automatic In-Bucket Volume Estimation for Dragline Operations’ and utilises payload information from a commercial dragline monitor. While the previous paper explains the algorithms and theoretical basis for the system design and scaled model testing this paper {{will focus on the}} full scale implementation and the challenges involved...|$|R
40|$|In {{this paper}} we {{describe}} graph-based parallel algorithms for entity resolution that improve over the map-reduce approach. We compare two approaches to parallelize a Locality Sensitive Hashing (LSH) accelerated, Iterative Match-Merge (IMM) entity resolution technique: BCP, where records hashed together are compared {{at a single}} node/reducer, vs an alternative mechanism (RCP) where comparison load is better distributed across processors especially {{in the presence of}} severely skewed <b>bucket</b> <b>sizes.</b> We analyze the BCP and RCP approaches analytically as well as empirically using a large synthetically generated dataset. We generalize the lessons learned from our experience and submit that the RCP approach is also applicable in many similar applications that rely on LSH or related grouping strategies to minimize pair-wise comparisons...|$|R
5000|$|... 2-choice hashing employs two {{different}} hash functions, h1(x) and h2(x), for the hash table. Both hash functions {{are used to}} compute two table locations. When an object is inserted in the table, then it {{is placed in the}} table location that contains fewer objects (with the default being the h1(x) table location if there is equality in <b>bucket</b> <b>size).</b> 2-choice hashing employs the principle of the power of two choices.|$|E
5000|$|... #Caption: The Bucyrus Erie 3850-B Power Shovel named [...] "Big Brother" [...] {{went to work}} {{next door}} to Paradise Fossil Plant for Peabody Coal Company's Sinclair Surface Mine in 1962. When it started work it was {{received}} with grand fanfare and was the Largest Shovel in The World with a <b>bucket</b> <b>size</b> of 115 cubic yards. After it finished work in the mid-1980s, it was buried in a pit on the mine's property. It remains there still today.|$|E
50|$|Samplesort {{is often}} used in {{parallel}} systems, including distributed systems such as bulk synchronous parallel machines. This is done by splitting the sorting for each processor or node, where the number buckets {{is equal to the}} number of processors. Sample sort is efficient in parallel systems because each processor receives approximately the same <b>bucket</b> <b>size.</b> Since the buckets are sorted concurrently, the processors will complete the sorting at approximately the same time, thus not having a processor wait for others.|$|E
500|$|In {{addition}} to completing quests and missions, players {{can participate in}} several side-minigames and other activities. One such minigame is fishing, where players can measure their strength against the fish they attempt to catch. Another is clamming, where players collect as many fish or sea creatures as possible without going over their <b>bucket's</b> <b>size</b> limit. Gardening allows players to raise plants in their residence, or [...] "Mog House" [...] {{as it is known}} in the game. The raising and breeding of Chocobos was a long-requested activity enabled in the summer 2006 update. Chocobo racing began in March 2007, which allowed for the racing of player-raised Chocobos against non-player characters (NPCs). Winning racers can earn [...] "Chocobucks", which can be used to buy, for example, items that assist Chocobo breeding.|$|R
40|$|There {{has been}} a wealth of {{research}} in the area of parallel join algorithms. Among them, hash-based algorithms are particularly suitable for shared-nothing database systems. The eectiveness of these techniques depends on the uniformity in the distribution of the join attribute values. When this condition is not met, a severe uctuation may occur among the <b>bucket</b> <b>sizes</b> causing uneven workload for the processing nodes. Many parallel join algorithms with load balancing capability have been proposed to address this problem. Among them, the sampling and incremental approaches have been shown to provide improvement over the more conventional methods. The comparison between these two approaches, however, have not been investigated. In this paper, we improve these techniques, and implement them on nCUBE/ 2 parallel computer to compare their performance. Our study indicates that the sampling technique is the better approach...|$|R
40|$|Linda [Gel 85, CG 89] is a {{coordination}} language providing generative communication via tuple spaces. Each tuple {{space is}} a global associative memory consisting of a bag (or multi-set) of tuples, where the tuples are variable-sized objects. Repeated insertions and removals of variable-sized tuples are liable to result in the memory containing several small areas of free memory to which larger, variable-sized tuples can not be allocated. This memory redundancy then has to be removed by compacting the memory to produce a large, contiguous area of free memory to which large, variable-sixed tuples can be allocated. Furthermore, when attempting to implement tuple spaces in content-addressable memory (CAM), variable-sized tuples require variable <b>bucket</b> <b>sizes</b> for hash-based schemes. This, along {{with the need to}} compare variable-sized blocks of memory when matching, result in implementation problems. In this report, tuplets are proposed as a solution to the problems of variable-sized tuples. Tup [...] ...|$|R
5000|$|The {{ideal is}} to pick {{splitters}} that separate the data into j buckets of size n/j, where n {{is the number}} of elements to be sorted. This is to achieve an even distribution among the buckets, this way no one bucket takes longer than others to be sorted. This can be accomplished by selecting splitters in the sample by stepping through the sorted sample using a/j. Where sample size is a, and <b>bucket</b> <b>size</b> is j such that the values are a/j, 2a/j, ... (j - 1)a/j.|$|E
5000|$|Dynamic {{pull system}} [...] is an {{advanced}} version of pull system which encompasses the best feature of traditional pull system & MRP. The major disadvantage of traditional kanban {{system is the}} fixed kanban size and requirement of at least 2 bins for full operation. In the event of sudden demand decrease, kanban system can result in extra inventory {{and the value of}} unused inventory can go up to 2 bin size.Similarly, In case of unexpected demand increasing, it can result in line down and the issue will be severe if the lead times are not short.Dynamic pull system overcomes this issue by recalculating the <b>bucket</b> <b>size</b> (kanban size/lot size) before creating any supply (requisitions/purchase order/work order). Each time a new supply is created, system automatically decides the best supply size as per the existing actual demand.|$|E
50|$|<b>Bucket</b> <b>size</b> {{planning}} {{brings the}} possibility of long-term planning to Scrumban. It {{is based on the}} system of three buckets that the work items need to go through before making it on the Scrumban board. The three buckets represent three different stages of the plan and are usually called 1-year, 6-month and 3-month buckets. The 1-year bucket is dedicated for long-term goals that the company has, like penetrating a new market, releasing new product, etc. When the company decides to move forward with a plan, it is moved to the 6-month bucket, where the main requirements of this plan are crystallized. When a company is ready to start implementing the plan, the requirements are moved into the 3-month bucket and divided into clear tasks to be completed by the project team. It is from this bucket that the team draws tasks during their on demand planning meeting and starts working on the tasks.|$|E
40|$|In this paper, we use {{prediction}} algorithms {{to estimate}} the behavior of VBR video traffic based on statistical short-term earthquake prediction methods. The probabilistic estimation of future inter-arrival times of peak values of MPEG-VBR video stream is considered in terms of Bayes theorem, and the Bayesian analysis of peak rate sample is evaluated {{on the basis of}} the proportion of successes rather than in the number of successes. The estimation results of aftershock peak values are satisfactory. The distribution of VBR video data magnitudes based on maximum entropy distribution method will also be investigated. As an application, the method will be implemented to adapt the leaky bucket rate in order to police VBR video streams in ATM networks without using very high <b>bucket</b> <b>sizes.</b> 1. Introduction Broadband Integrated Services Digital Networks (BISDN) is a network providing integrated services including voice, video and data. B-ISDN should satisfy the offered quality of service (QoS) [...] ...|$|R
50|$|If {{the set of}} {{key-value}} pairs {{is fixed}} and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, <b>bucket</b> table <b>size,</b> and internal data structures. In particular, one {{may be able to}} devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table.|$|R
40|$|The main {{contribution}} {{of this report}} is {{the introduction of a}} new mathematical tool that we call the Diagonal Poisson Transform, and its application to the analysis of some linear probing hashing schemes. We also present {{what appears to be the}} rst exact analysis of a linear probing hashing scheme with <b>buckets</b> of <b>size</b> b. First, we present the Diagonal Poisson Transform. We show its main properties and apply it to solve recurrences, nd inverse relations and obtain several generalizations of Abel's summation formula. We follow with the analyisis of LCFS hashing with linear probing. It is known that the Robin Hood linear probing algorithm minimizes the variance of the cost of successful searches for all linear probing algorithms. We prove that the variance of the LCFS scheme is within lower order terms of this optimum. Finally we present the rst exact analysis of linear probing hashing with <b>buckets</b> of <b>size</b> b. From the generating function for the Robin Hood heuristic, we obtain exact expressions for the cost of successful searches when the table is full. Then, with the hel...|$|R
50|$|The M2 {{parameter}} is not {{the whole}} story in specifying beam quality. A low M2 only implies that the second moment of the beam profile expands slowly. Nevertheless, two beams with the same M2 may not have the same fraction of delivered power in a given area. Power-in-the-bucket and Strehl ratio are two attempts to define beam quality as a function of how much power is delivered to a given area. Unfortunately, there is no standard <b>bucket</b> <b>size</b> (D86 width, Gaussian beam width, Airy disk nulls, etc.) or bucket shape (circular, rectangular, etc.) and there is no standard beam to compare for the Strehl ratio. Therefore, these definitions must always be specified before a number is given and it presents much difficulty when trying to compare lasers. There is also no simple conversion between M2, power-in-the-bucket, and Strehl ratio. The Strehl ratio, for example, has been defined as the ratio of the peak focal intensities in the aberrated and ideal point spread functions. In other cases, it has been defined as the ratio between the peak intensity of an image divided by the peak intensity of a diffraction-limited image with the same total flux. Since there are many ways power-in-the-bucket and Strehl ratio have been defined in the literature, the recommendation is to stick with the ISO-standard M2 definition for the beam quality parameter and be aware that a Strehl ratio of 0.8, for example, does not mean anything unless the Strehl ratio is accompanied by a definition.|$|E
40|$|The {{function}} of master scheduling is {{to plan the}} flow of order from its arrival to its completion. In this study, the problem of <b>bucket</b> <b>size</b> for master scheduling is taken up. The <b>bucket</b> <b>size</b> for master scheduling has much influence on the lead time of the order. However, to date {{there is no clear}} method for how to set the optimum <b>bucket</b> <b>size.</b> The {{purpose of this study is}} to propose a method to set the optimum <b>bucket</b> <b>size.</b> In this paper, an equation to estimate the optimum <b>bucket</b> <b>size</b> is proposed for the case where a production system and products have hierarchical structure...|$|E
40|$|An {{empirical}} {{study has}} been conducted to determine the optimal <b>bucket</b> <b>size</b> for the bucket PR octree while calculating the vertex distortion of tetrahedral meshes. The motivation is to {{study the effects of}} the bucket PR octree data structure. This study implemented the data structure using the Java programming language and its libraries and found that the optimal <b>bucket</b> <b>size</b> was 32. This paper also discusses the extension of this project, the PR-star octree, and the work that was done on a recent paper accepted to the ACM SIGSPATIAL GIS 2011 conference. ...|$|E
40|$|The Internet is {{now widely}} {{expected}} to become an important communication infrastructure of the society, and therefore {{it is no longer}} sufficient to simply be able to provide connections. A higher quality of service (QoS) in communications is increasingly being required. As a new framework for providing QoS services, DiffServ is undergoing a speedy standardization process at the IETF. DiffServ not only can offer tiered level of services, but can also provide guaranteed QoS to a certain extent. In this paper, we examine a single DiffServ node model which utilizes token bucket as the policing mechanism, and propose a way of configuring various control parameters in order to accommodate various UDP/TCP traffic. Then, through simulation, we evaluate the throughput characteristics for various cases when TCP and UDP are mixed or separated in different queues, and study the appropriateness of the configurations. The results indicate that the throughputs can be maintained as specified in the Service Level Agreements (SLA) over a considerably wide range of <b>bucket</b> <b>sizes</b> and discard thresholds in the QoS control mechanism...|$|R
5000|$|As {{mentioned}} in step 2 above, {{the goal of}} the distribution step is to distribute the sorted subarrays into q buckets [...] The distribution step algorithm maintains two invariants. The first is that each <b>bucket</b> has <b>size</b> at most [...] at any time, and any element in bucket [...] is no larger than any element in bucket [...] The second is that every bucket has an associated pivot, a value which is greater than all elements in the bucket.|$|R
40|$|One of {{the most}} {{commonly}} used sampling techniques to capture leaf litter amphibians, lizards and small mammals is a set of pitfall traps with drift fences. However, there are still many speculations concerning the effectiveness of different designs of pitfall traps and the most adequate size of each trap. To address this problem, we conducted the first standardized comparison of patterns of species richness, rank-abundance, and community structure of leaf litter amphibians, lizards and small mammals for two trap designs (I and Y format) and three <b>bucket</b> <b>sizes</b> (35, 62, and 100 L) in a Neotropical forest. Results are very similar for the herpetofauna, regardless of the pitfall trap design or size used, while for small mammals values of species richness were higher for 100 L pitfall traps, as compared to the smaller traps. Therefore, the use of 100 L pitfall traps is recommended to sample the terrestrial vertebrate fauna, in multidisciplinary studies. For surveys aiming only the herpetofauna the use of smaller (35 L) traps is acceptable, taking into consideration the cost-benefits obtained by the smaller traps, in comparison to the larger ones...|$|R
