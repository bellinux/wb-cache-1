5|13|Public
5000|$|Many {{military}} {{and other organizations}} that use Morse code have adopted additional codes, including the Z code used by most European and NATO countries. The Z code adds commands and questions adapted for military radio transmissions, for example, [...] "ZBW 2", which means [...] "change to <b>backup</b> <b>frequency</b> number 2", and [...] "ZNB abc", which means [...] "my checksum is abc, what is yours?" ...|$|E
5000|$|FS-FHSS is {{developed}} {{to be applied}} in various wireless communication fields. It is innovated by GONSIN, a Chinese company. It has developed the unique FS-FHSS based on the spread spectrum communication technology. The new technology can monitor and select the undisturbed frequency band. FS-FHSS makes sure {{the stability of the}} communication. It is applied to GONSIN wireless conference system, which implements the discussion、simultaneous interpretation、voting and others comprehensive conference application functions. The feature of FS-FHSS: it uses the 2.4 GHz global frequency band, it does not require certificate. The wireless frequency points are abundant, 80 frequency points could be used; 2.4 GHz has high frequency carrier, the signal diffractivity is relatively weak so that other 2.4 GHz signal could not interfere the conference signal. Adopt FS-FHSS technology, it detect the wireless environment and select the useable frequency point before the conference, during the conference, it hops between the <b>backup</b> <b>frequency</b> points, which can avoid the interference source. It utilizes the efficient digital audio encode and decode technology, one frequency point can transmit 4 channels, it just need two frequency points to transmit 8 channels simultaneous interpretation signals. Because the system occupies relatively less frequency points resource and utilizes the FS-FHSS technology to operate, it can co-work with WIFI and other 2.4 GHz system. It applies the digital audio encryption and digital modulation to avoid any interception and malicious interference; the signal transmitting power could be modified in according with the application occasion, the adjusting range is from 50 meters to 500 meters( [...] the customized system can cover 2,000 meters).|$|E
3000|$|... backup {{functions}} {{should always}} be enabled, and the way, the chosen storage medium (e.g. stand-alone devices such as USB pen drives, CD rom, external hard disk, or network backup disks) and <b>backup</b> <b>frequency</b> should be defined through appropriate SOPs; restore functions of the backed up data should be verified, as well; [...]...|$|E
40|$|Introduction: High-intensity noninvasive {{ventilation}} (NIV) {{has been}} shown to improve outcomes in stable chronic obstructive pulmonary disease patients. However, there is insufficient knowledge about whether with this more controlled ventilatory mode optimal respiratory muscle unloading is provided without an increase in patient-ventilator asynchrony (PVA). Patients and methods: Ten chronic obstructive pulmonary disease patients on home mechanical ventilation were included. Four different ventilatory settings were investigated in each patient in random order, each for 15 min, varying the inspiratory positive airway pressure and <b>backup</b> breathing <b>frequency.</b> With surface electromyography (EMG), activities of the intercostal muscles, diaphragm, and scalene muscles were determined. Furthermore, pressure tracings were derived simultaneously in order to assess PVA. Results: Compared to spontaneous breathing, the most pronounced decrease in EMG activity was achieved with the high-pressure settings. Adding a high breathing frequency did reduce EMG activity per breath, while the decrease in EMG activity over 1 min was comparable with the high-pressure, low-frequency setting. With high <b>backup</b> breathing <b>frequencies</b> less breaths were pressure supported (25 % vs 97 %). PVAs occurred more frequently with the low-frequency settings (P= 0. 017). Conclusion: High-intensity NIV might provide optimal unloading of respiratory muscles, without undue increases in PVA...|$|R
40|$|Part 2 : Resilience and ReliabilityInternational audienceData {{deduplication}} yields {{an important}} role in modern backup systems for its demonstrated ability to improve storage efficiency. However, in deduplication-based backup systems, the consequent exhausting fragmentation problem has drawn ever-increasing attention over in terms of <b>backup</b> <b>frequencies,</b> which leads to the degradation of restoration speed. Various Methods are proposed to address this problem. However, most of them purchase restore speed at the expense of deduplication ratio reduction, which is not efficient. In this paper, we present a Dynamic Adaptive Forward Assembly Area Method, called DASM, to accelerate restore speed for deduplication-based backup systems. DASM exploits the fragmentation information within the restored backup streams and dynamically trades off between chunk-level cache and container-level cache. DASM is a pure data restoration module which pursues optimal read performance without sacrificing deduplication ratio. Meanwhile, DASM is a resource independent and cache efficient scheme, which works well under different memory footprint restrictions. To demonstrate the effectiveness of DASM, we conduct several experiments under various backup workloads. The results show that, DASM is sensitive to fragmentation granularity and can accurately adapt to the changes of fragmentation size. Besides, experiments also show that DASM improves the restore speed of traditional LRU and ASM methods by up to 58. 9  % and 57. 1  %, respectively...|$|R
40|$|Bachelor {{thesis is}} focused on {{explaining}} the basic concepts of backup issues. Discussed {{the different types of}} <b>backups,</b> rotation schemes, <b>frequency</b> of <b>backup,</b> data recovery and present recording media. The analysis is monitored by {{the current state of the}} relationship of respondents to the backup. The practical part is concerned with the backup using the ROBOCOPY program, and Backup and Restore in Windows...|$|R
40|$|AbstractWe {{consider}} {{strategies for}} backups {{from the viewpoint}} of competitive analysis of online problems. We concentrate upon the realistic case that faults are rare, i. e. the cost of work between two faults is typically large compared to the cost of one backup. Instead of the (worst-case) competitive ratio we use a refined and more expressive quality measure, in terms of the average fault frequency. The interesting matter is, roughly speaking, to adapt the <b>backup</b> <b>frequency</b> to the fault frequency, while future faults are unpredictable. We give an asymptotically optimal deterministic strategy and propose a randomized strategy whose expected cost beats the deterministic bound...|$|E
40|$|Marieke L Duiverman, 1 Anouk S Huberts, 2 Leo A van Eykern, 3 Gerrie Bladder, 1 Peter J Wijkstra 1 1 Department of Pulmonary Diseases and Home Mechanical Ventilation, University Medical Centre Groningen, 2 Faculty of Medical Sciences, University of Groningen, 3 Inbiolab B. V., Groningen, the Netherlands Introduction: High-intensity noninvasive {{ventilation}} (NIV) {{has been}} shown to improve outcomes in stable chronic obstructive pulmonary disease patients. However, there is insufficient knowledge about whether with this more controlled ventilatory mode optimal respiratory muscle unloading is provided without an increase in patient–ventilator asynchrony (PVA). Patients and methods: Ten chronic obstructive pulmonary disease patients on home mechanical ventilation were included. Four different ventilatory settings were investigated in each patient in random order, each for 15 min, varying the inspiratory positive airway pressure and <b>backup</b> breathing <b>frequency.</b> With surface electromyography (EMG), activities of the intercostal muscles, diaphragm, and scalene muscles were determined. Furthermore, pressure tracings were derived simultaneously in order to assess PVA. Results: Compared to spontaneous breathing, the most pronounced decrease in EMG activity was achieved with the high-pressure settings. Adding a high breathing frequency did reduce EMG activity per breath, while the decrease in EMG activity over 1 min was comparable with the high-pressure, low-frequency setting. With high <b>backup</b> breathing <b>frequencies</b> less breaths were pressure supported (25 % vs 97 %). PVAs occurred more frequently with the low-frequency settings (P= 0. 017). Conclusion: High-intensity NIV might provide optimal unloading of respiratory muscles, without undue increases in PVA. Keywords: electromyography, high-intensity NIV, chronic obstructive pulmonary disease, ineffective effort...|$|R
50|$|Each {{station has}} an {{identical}} setup of equipment. A dual set of transmitters, a primary and a backup, provide for constant {{transmission of the}} time code. However, {{it is not possible}} given the current design configuration for one site to act as a lower power alternate <b>frequency</b> <b>backup</b> for the other. The backups are set to automatically take over in the event that the primary transmission system has a failure. The Time Signal Control Room generates the standard LF signal and time code that is broadcast.|$|R
50|$|The {{original}} frequency was 162.550 MHz, with 163.275 MHz recommended as a <b>backup.</b> However, this <b>frequency</b> {{was dropped}} due to interference issues with other federal agencies, and 162.400 MHz was added in 1970; the 162.475 MHz frequency was introduced for NWR transmission in 1975 (the use of 162.475 {{for several years}} was limited only to special cases where required to avoid channel interference, and transmitter power output was restricted to 300 Watts). Honolulu NWR station KBA99 transmitted on 169.075 MHz for twelve years until it was moved to 162.550 in 1975.|$|R
40|$|Since {{the launch}} of TerraSAR-X on June 15, 2007, the {{required}} precise orbit products have been provided by the German Space Operations Center (DLR/GSOC) to support operational space-borne Synthetic Aperture Radar (SAR) and interferometric SAR (InSAR) image processing. The TerraSARX precise trajectory is reconstructed solely based on the Global Positioning Systems (GPS) measurements from a geodetic grade dual-frequency Integrated Geodetic and Occultation Receiver (IGOR) onboard the spacecraft. The GPS-based precise orbit determination (POD) strategy used in the estimation of the precise TerraSAR-X orbit and its performance will be fully described in this paper. A 5 -month statistics from {{the internal and external}} orbit assessment indicate a root-mean-squared (RMS) 3 D orbit accuracy of better than 10 cm and 20 cm for the precise science orbit (PSO) and precise rapid orbit (PRO) products respectively. The POD performance of the <b>backup</b> single <b>frequency</b> MosaicGNSS receiver to support operational PRO product generation in case of IGOR tracking failure or interruptions is described as well...|$|R
40|$|This paper {{presents}} and analyzes a point-tomultipoint (P 2 MP) network {{that uses a}} number of freespace optical (FSO) links for data transmission from the central node to the different remote nodes of the network. A common <b>backup</b> radio <b>frequency</b> (RF) link {{can be used by}} the central node for data transmission to any remote node in case any one of the FSO links fails. Each remote node is assigned a transmit buffer at the central node. Considering the transmission link from the central node to a tagged remote node, we study various performance metrics. Specifically,we study the throughput from the central node to the tagged node, the average transmit buffer size, the symbol queuing delay in the transmit buffer, the efficiency of the queuing system, the symbol loss probability, and the RF link utilization. Numerical examples are presented to compare the performance of the proposed P 2 MP hybrid FSO/RF network with that of a P 2 MP FSO-only network and show that the P 2 MP hybrid FSO/RF network achieves considerable performance improvement over the P 2 MP FSO-only network...|$|R
50|$|Following the Edison project, the Nikola {{project was}} started {{which focused on}} demonstrating the V2G {{technology}} in a lab setting, located at the Risø Campus (DTU). DTU is a partner along with Nuvve and Nissan. The Nikola project is finishing in 2016, and lays the groundwork for Parker, which will use a fleet of EVs to demonstrate the technology in a real-life setting. this project is partnered by DTU, Insero, Nuvve, Nissan and Frederiksberg Forsyning (Danish DSO in Copenhagen). Besides demonstrating the technology the project also aims to clear the path for V2G-integration with other OEMs as well as calculating the business case for several types of V2G, such as Adaptive charging, overload protection, peak shaving, emergency <b>backup</b> and <b>frequency</b> balancing. the project starts in August 2016 and runs for 2 years. Other notable projects in Denmark are the SEEV4-City Interreg project which will demonstrate V2G in a car sharing fleet in the north harbour of Copenhagen and the ECOGrid 2.0, which will not include EVs but build the aggregator software to fully integrate it into the Danish electricity markets.|$|R
30|$|At 30 and 60  min {{after the}} {{beginning}} of NIV, the patient performed upon request a voluntary slow and maximal expiration. In brief, the patients were asked to slowly empty their lungs as much and {{for as long as}} possible. The expired CO 2 value displayed {{at the end of this}} active expiration maneuver was recorded. A passive expiratory maneuver was then performed with the help of an experienced respiratory therapist (bilateral chest compression during slow expiration), and the corresponding expired CO 2 value was recorded. The naso-buccal mask was not removed during the maximal expiratory maneuvers meaning that the patient expired through the nasobuccal sensor and the ventilator circuit and thus against the set PEEP. The <b>backup</b> safety respiratory <b>frequency</b> of the ventilator was set at 6 by minute to allow expiratory maneuvers of 10  s.|$|R
40|$|Allowing {{applications}} to survive hardware failure is a expensive undertaking, which generally involves re-engineering software to include complicated recovery logic {{as well as}} deploying special-purpose hardware; this represents a severe barrier to improving the dependability of large or legacy applications. We describe {{the construction of a}} general and transparent high-availability service that allows existing, unmodified software to be protected from the failure of the physical machine on which it runs. Remus provides an extremely high degree of fault tolerance, to the point that a running system can transparently continue execution on an alternate physical host in the face of failure with only seconds of downtime, completely preserving host state such as active network connections. We describe our approach, which encapsulates protected software in a virtual machine, asynchronously propagates VM state to a <b>backup</b> host at <b>frequencies</b> as high as forty times a second, and uses speculative execution to concurrently run the active VM slightly ahead of the replicated system state...|$|R
40|$|Helium {{tests of}} the main {{propulsion}} system in the Space Shuttle and on hydrogen leaks are examined. The hazardous gas detection system (HGDS) in the mobile launch pad uses mass spectrometers (MS) to monitor the shuttle environment for leaks. The mass spectrometers are fed by long tubes to sample gas from the payload bay, mid-body, aft engine compartment, and external tank. The purpose {{is to improve the}} HGDS, especially in its potential for locating cryogen leaks. Pre-existing leak data was analyzed for transient information to determine if the leak location could be pinpointed from test data. A rapid response leak detection experiment was designed, built, and tested. Large eddies and vortices were visually seen with Schlieren imaging, and they were detected in the time plots of the various instruments. The response time of the MS was found in the range of 0. 05 to 0. 1 sec. Pulsed concentration waves were clearly detected at 25 cycles per sec by spectral analysis of MS data. One conclusion is that the <b>backup</b> HGDS sampling <b>frequency</b> should be increased above the present rate of 1 sample per second...|$|R
40|$|Abstract Background Progress in {{automated}} image analysis, virtual microscopy, hospital information systems, and interdisciplinary {{data exchange}} require image standards {{to be applied}} in tissue-based diagnosis. Aims To describe the theoretical background, practical experiences and comparable solutions in other medical fields to promote image standards applicable for diagnostic pathology. Theory and experiences Images used in tissue-based diagnosis present with pathology – specific characteristics. It seems appropriate to discuss their characteristics and potential standardization {{in relation to the}} levels of hierarchy in which they appear. All levels can be divided into legal, medical, and technological properties. Standards applied to the first level include regulations or aims to be fulfilled. In legal properties, they have to regulate features of privacy, image documentation, transmission, and presentation; in medical properties, features of disease – image combination, human – diagnostics, automated information extraction, archive retrieval and access; and in technological properties features of image acquisition, display, formats, transfer speed, safety, and system dynamics. The next lower second level has to implement the prescriptions of the upper one, i. e. describe how they are implemented. Legal aspects should demand secure encryption for privacy of all patient related data, image archives that include all images used for diagnostics for a period of 10 years at minimum, accurate annotations of dates and viewing, and precise hardware and software information. Medical aspects should demand standardized patients' files such as DICOM 3 or HL 7 including history and previous examinations, information of image display hardware and software, of image resolution and fields of view, of relation between sizes of biological objects and image sizes, and of access to archives and retrieval. Technological aspects should deal with image acquisition systems (resolution, colour temperature, focus, brightness, and quality evaluation procedures), display resolution data, implemented image formats, storage, cycle <b>frequency,</b> <b>backup</b> procedures, operation system, and external system accessibility. The lowest third level describes the permitted limits and threshold in detail. At present, an applicable standard including all mentioned features does not exist to our knowledge; some aspects can be taken from radiological standards (PACS, DICOM 3); others require specific solutions or are not covered yet. Conclusion The progress in virtual microscopy and application of artificial intelligence (AI) in tissue-based diagnosis demands fast preparation and implementation of an internationally acceptable standard. The described hierarchic order as well as analytic investigation in all potentially necessary aspects and details offers an appropriate tool to specifically determine standardized requirements. </p...|$|R

