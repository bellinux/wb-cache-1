21|38|Public
40|$|An {{efficient}} motion-estimation algorithm {{based on}} partial <b>block</b> <b>distortion</b> using sorted significant features including bit-plane and mean is proposed. The proposed algorithm can obtain relatively accurate motion vectors with a reduced computational load. Simulation {{results show that}} the proposed method achieves its MSE performance very close to the full search method, while requiring only 6 - 8 % of the computation needed by the full search. Furthermore, the performance of our method is better than other algorithms based on partial <b>block</b> <b>distortion</b> search. 1...|$|E
30|$|To {{reduce the}} {{required}} operations, many fast algorithms have been developed, including the 2 D logarithmic search (LOGS) [16], the three-step search (TSS) [17], the new three-step search (NTSS) [18], the novel four-step search (NFSS) [19], the block-based gradient descent search (BBGDS) [20], the diamond search (DS) [21], the hexagonal search (HEX) [22], the unrestricted center-biased diamond search (UCBDS) [23], and so forth. The basic idea behind these multistep fast search algorithms is to check {{a few of}} block points at current step, and restrict the search in next step to the neighboring of points that minimizes the <b>block</b> <b>distortion</b> measure.|$|E
40|$|A {{modified}} full-search (MFS) {{algorithm is}} presented for block-based motion estimation applications, which introduces the novel concept of variable distance dependent thresholds. The {{performance of the}} MFS algorithm is analyzed and quantitatively compared with both the traditional and exhaustive full-search (FS) technique, and the computationally faster, non-exhaustive three-step-search (TSS) algorithm. Experimental results show that by applying an appropriate threshold function, the MFS algorithm not only matches {{the speed of the}} TSS algorithm, but both retains a <b>block</b> <b>distortion</b> error comparable to the global minimum produced by the FS algorithm, and avoids the problem of identifying large numbers of spurious motion vectors in the search process...|$|E
30|$|Row (b) shows {{unprocessed}} depth after strong compression (H. 264 with QP = 51) {{frame and}} its rendering capability. Objects edges are particularly affected by <b>block</b> <b>distortions.</b>|$|R
40|$|Abstract — This study aims {{to remove}} <b>blocking</b> <b>distortions</b> in {{compressed}} moving pictures. We propose to apply subband transform (ST) and an edge adaptive deblocking filter {{to remove the}} <b>blocking</b> <b>distortions</b> caused by transforms and the block motion compensated prediction in moving picture coding, respectively. We evaluate coding efficiency and distortion level experimentally by PSNR value and subjectively through visual appearance. It give good PSNR values compared to the discrete cosine transform (DCT) and the discrete wavelet transform (DWT). The edge adaptive deblocking filter produced an improved quality of the decoded pictures. The ST with an edge adaptive deblocking filter is {{an efficient way to}} decode pictures and proves from the PSNR values and the quality of pictures...|$|R
40|$|Two {{representative}} sample images of Band 4 of the Landsat Thematic Mapper are compressed with the JPEG algorithm at 8 : 1, 16 : 1 and 24 : 1 Compression Ratios for experimental browsing purposes. We then apply the Optimal PSNR Estimated Spectra Adaptive Postfiltering (ESAP) algorithm {{to reduce the}} DCT <b>blocking</b> <b>distortion.</b> ESAP reduces the <b>blocking</b> <b>distortion</b> while preserving most of the image's edge information by adaptively postfiltering the decoded image using the block's spectral information already obtainable from each block's DCT coefficients. The algorithm iteratively applied a one dimensional log-sigmoid weighting function to the separable interpolated local block estimated spectra of the decoded image until it converges to the optimal PSNR {{with respect to the}} original using a 2 -D steepest ascent search. Convergence is obtained in a few iterations for integer parameters. The optimal logsig parameters are transmitted to the decoder as a negligible byte of overhead data. A unique maxima is guaranteed due to the 2 -D asymptotic exponential overshoot shape of the surface generated by the algorithm. ESAP is based on a DFT analysis of the DCT basis functions. It is implemented with pixel-by-pixel spatially adaptive separable FIR postfilters. PSNR objective improvements between 0. 4 to 0. 8 dB are shown together with their corresponding optimal PSNR adaptive postfiltered images...|$|R
40|$|Absrraci- Motion {{estimation}} {{plays an}} important role in the motion compensated video coding framework. Due to the high computational complexity of the exhaustive search, many sub-optimal fast search algorithms, aiming to achieve the best trade off between distortion and search speed, are proposed. We observe that distortion gradient of a search point on the <b>block</b> <b>distortion</b> surface (BDS) monotonously decreases with increasing distance from that point to the global minimum point. Based on this property, we propose a novel adaptive cross search (ACS) algorithm that can distribute the computation powers over the search Space efficiently. The simulation shows that ACS achieves competitive reconstrnction visual quality as well reduced computational complexity...|$|E
40|$|In {{this paper}} we propose a new method of image quality {{assessment}} for the evaluation of the <b>block</b> <b>distortion</b> through artificial neural networks (ANNs). The approach is new and intends {{to address the problem of}} the assessment of the visual quality of compres sed images from an original point of view. ANN's in particular are applied in order to detect the presence of blocking errors inside pre-processed pictures. To this purpose, a new local blockirtg distortion parameter is introduced. Experiments and simulations, even if very preliminary, have confirmed the interest of the proposed approach. A complete formalization of the problem will also be presented...|$|E
40|$|A {{fast motion}} {{estimation}} algorithm, called distance dependent thresholding search (DTS), is presented for block-based true motion estimation applications, and introduces the novel concept of variable distance dependent thresholds. The {{performance of the}} DTS algorithm is analysed and quantitatively compared with both the traditional and exhaustive full-search (FS) technique, and the computationally faster, non-exhaustive three-step-search (TSS) algorithm. Experimental results show that by applying an appropriate threshold function, the DTS algorithm not only matches {{the speed of the}} TSS algorithm, but both retains a <b>block</b> <b>distortion</b> error comparable to the global minimum produced by the FS algorithm, and avoids the problem of identifying a large number of spurious motion vectors in the search process...|$|E
30|$|Measuring the {{transient}} {{behavior of}} distorting audio devices is difficult in general. Typically, the transient response depends {{heavily on the}} amplitude and frequency content of the test signal, so it is usually not possible to generalize the results for other types of input signals. Nevertheless, it is still useful to compare the input/output waveforms of a distorting device when a transient signal is used as input, for example because certain types of dynamic distortion (such as the <b>blocking</b> <b>distortion,</b> as in [7]) can be identified from the waveforms.|$|R
40|$|The {{invention}} presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model {{a number}} of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) <b>blocking</b> <b>distortion</b> caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images...|$|R
40|$|Two {{impairment}} metrics {{presented in}} this paper are for quanti cation of blocking artifacts and ringing artifacts, respectively, in digitally coded monochrome video sequences. They are based on a multichannel vision model which has been parameterised using the subjective quality assessment data recently provided by the VQEG (Video Quality Experts Group). Segmentation algorithms are used to identify regions dominated by blocking and ringing, respectively, and perceptual distortions in these regions are summed up to form perceptual distortion metrics. As an example, a perceptual <b>blocking</b> <b>distortion</b> metric (PBDM) is presented which is based on a simpli ed distortion detection model. Subjective and objective tests have been conducted, and the results show a strong correlation between the objective blocking ratings and the mean opinion scores on blocking artifacts. 1...|$|R
40|$|Abstract—This paper proposes an {{efficient}} block-matching motion estimation algorithm, called cross directional rectangle search (CDRS), {{which can be}} applied {{to a wide range of}} video applications. The algorithm relies on the hypothesis of monotonic <b>block</b> <b>distortion</b> surface and the cross center-biased characteristic of motion vector probability distribution which has been used in literature. A cross pattern in the initial step and one of four possible directional rectangle search patterns in the next step are repeatedly used to find the motion vectors. The algorithm can achieve a faster computation speed with similar distortion performance compared to some other well-known fast block-matching motion estimation algorithms. Keywords-motion estimation; block-matching; cross center-biased; video coding; CDRS. I...|$|E
40|$|In this letter, a new {{fast motion}} {{estimation}} (FME) algorithm {{capable of producing}} sub-sample motion vectors at low computational-complexity is proposed. Unlike existing FME algorithms, the proposed algorithm considers the low complexity sub-sample accuracy in designing the search pattern for FME. The proposed FME algorithm is designed {{in such a way}} that the <b>block</b> <b>distortion</b> measure is modeled as a parametric surface in the vicinity of the integer-sample motion vector; this modeling enables low computational-complexity sub-sample motion estimation (ME) without pixel interpolation. Experimental results on video test sequences show that the proposed FME algorithm reduces computational complexity of integer and sub-sample ME considerably compared with traditional methods at the cost of negligible performance degradation...|$|E
40|$|One-bit {{transform}} (1 BT) -based block {{motion estimation}} (ME) {{has been proposed}} to reduce the computational complexity by using Boolean exclusive-OR matching of one-bit representations of image frames. In this paper, a novel early termination algorithm for 1 BT-based ME is proposed in order to decrease the calculations of the <b>block</b> <b>distortion</b> measure. Unlike the classical early termination schemes, the proposed algorithm utilizes {{a new approach to}} reduce computations. It employs the binomial distribution based on the characteristic of binary plane which is composed of only two elements: 0 and 1. Experimental results show that the proposed algorithm keeps its peak signal-to-noise ratio (PSNR) performance very close to the full search algorithm (FSA) while the computational complexity of ME is reduced considerably...|$|E
5000|$|The DS-XG [...]vxd drivers under Windows 95, 98, 98SE or ME {{can switch}} to Sondius-XG which allows {{emulation}} of the VL70-m tone generator (except for the <b>distortion</b> <b>block).</b>|$|R
40|$|A {{number of}} novel {{adaptive}} image compression {{methods have been}} developed using {{a new approach to}} data representation, a mixture of principal components (MPC). MPC, together with principal component analysis (PCA) and vector quantization (VQ), form a spectrum of representations. The MPC approach still suffers from <b>block</b> effect <b>distortion.</b> While existing lapped transforms eliminate this distortion, they {{not take into account the}} need for adaptation on a block-to-block basis. Further, the basis vectors are fixed so they cannot be adapted in any optimal fashion from one image to another. In this paper, a lapped orthogonal projection is used to generate subblocks for both the classic Karhunen-Lo`eve transform (KLT) and the adaptive MPC. The resulting images are free of <b>block</b> effect <b>distortion.</b> Further, the squared error can be reduced. Therefore, both the nonadaptive and adaptive methods under the projection tend to outperform the respective block methods both in terms of subjective criteri [...] ...|$|R
5000|$|High-definition Digital Signal Transmission Technology {{can be used}} {{to improve}} the sound quality during the data {{transmission}} process by <b>blocking</b> the <b>distortion</b> signal that can reduce sound quality during the transmission process. Letting only the data signal pass from the transmitter to the receiver, it preserves the digital data close to its original state. This feature is developed based on K2 technology’s principle of “closer to the music truth” ...|$|R
40|$|In H. 264 /AVC, new coding {{techniques}} introduce many op-tional macroblock encoding modes. Exhaustive search {{over all}} possible modes can achieve optimal coding efficiency but is highly computationally expensive. In this paper, a fast macroblock mode selection (FMMS) algorithm {{based on the}} motion content classification is proposed. Each mac-roblock is categorized into complex motion or simple mo-tion contents by a fuzzy classifier, then different mode search orders with distinct early termination schemes are employed according to the classification. The proposed method can be readily incorporated with fast motion estimation algo-rithms, and simulation results show that it can further save 40 %- 70 % of the <b>block</b> <b>distortion</b> calculations {{on the basis of}} conventional fast motion estimation algorithms, while maintaining similar rate distortion performance. 1...|$|E
40|$|Many fast block-matching {{algorithms}} reduce computations {{by limiting}} the number of checking points. They can achieve high computation reduction, but often result in relatively higher matching error compared with the full-search algorithm. In this letter, a novel fast block-matching algorithm named normalized partial distortion search is proposed. The proposed algorithm reduces computations by using a halfway-stop technique in the calculation of <b>block</b> <b>distortion</b> measure. In order to increase the probability of early rejection of non-possible candidate motion vectors, the proposed algorithm normalized the accumulated partial distortion and the current minimum distortion before comparison. Experimental results show that the proposed algorithm can maintain its mean square error performance very close to the full search algorithm while achieving an average computation reduction of 12 [...] 13 times, with respect to the full-search algorithm. I. INTRODUCTION M OTION compensation is a vital comp [...] ...|$|E
40|$|This paper {{proposes a}} hybrid {{adaptive}} search algorithm (HASA) for block-based motion estimation. The proposed algorithm exploits {{the correlation between}} the <b>block</b> <b>distortion</b> measure (BDM) of the starch origin (0, 0) and its displacement from the motion vector to predict the range of motion. Based on the predicted motion type and the center-biased statistical distribution of motion vectors in low bit rate applications, one of the two block matching algorithms: Four-step search (4 SS) or center-biased orthogonal search (CBOSA) is employed to find the motion vector. Experimental results show that HASA outperforms the well-known three-step search (3 SS) and new three-step search (N 3 SS) in terms of number of search points while maintains acceptable mean square errors (MSEs). Moreover, it could effectively predict the stationary motion so as to stop the search at the first search point. The proposed algorithm is very suitable for low bit rate applications using software based video encoder...|$|E
40|$|In this paper, {{investigations}} are conducted to simplify and rene a vision model based video quality metric without compromising its prediction accuracy. Unlike other vision model based quality metrics, the proposed metric is parameterized using subjective quality assessment data recently {{provided by the}} Video Quality Experts Group (VQEG). The quality metric is able to generate a perceptual distortion map for each and every video frame. A Perceptual <b>Blocking</b> <b>Distortion</b> Metric (PBDM) is introduced which utilizes this simplied quality metric. The PBDM is formulated based on the observation that blocking artifacts are noticeable only in certain regions of a picture. A method to segment blocking dominant regions is devised, and perceptual distortions in these regions are summed up to form an objective measure of blocking artifacts. Subjective and objective tests are conducted, {{and the performance of}} the PBDM is assessed by a number of measures such as the Spearman rank order correlation, the Pearson correlation and the average absolute error. The results show a strong correlation between the objective blocking ratings and the mean opinion scores on blocking artifacts...|$|R
40|$|The term {{matching}} pursuits {{refers to}} a greedy algorithm which matches signal structures to a large, diverse dictionary of functions. The technique was proposed by Mallat and Zhang with an application to signal analysis. In this paper, we show how matching pursuits {{can be used to}} effectively code the motion residual in a hybrid video coding system at bit rates below 20 kbit/s. One advantage of this technique at low bit rates is that bits are assigned progressively to high energy areas in the motion residual. The proper choice of a dictionary set can lead to other advantages. For instance, a large dictionary {{with a wide variety of}} structures can represent a residual signal using fewer coefficients than the DCT basis. Also, a dictionary which is not block-based can reduce <b>block</b> <b>distortions</b> common to low bit rate DCT systems. Experimental results are presented in which the DCT residual coder from a standard coding system is replaced by a matching pursuit coder. These results show a substantial improvement in both PSNR and perceived visual quality. Further improvements result when the matching pursuit coder is paired with a smooth motion model using overlapping motion blocks...|$|R
40|$|Summary. Introduction of {{information}} technology to improve the efficiency of security activity leads {{to the need to}} consider a number of negative factors associated with in consequence of the use of these technologies as a key element of modern security systems. One of the most notable factor is the exposure to information processes in protection systems security threats. This largely relates to integrated security systems (ISS) is the system of protection with the highest level of informatization security functions. Significant damage to protected objects that they could potentially incur as a result of abnormal operation ISS, puts a very actual problem of assessing factors that reduce the efficiency of the ISS to justify the ways and methods to improve it. Because of the nature of threats and <b>blocking</b> <b>distortion</b> {{of information}} in the ISS of interest are: the volume undistorted ISF working environment, as a characteristic of data integrity; time access to information as a feature of its availability. This in turn leads to the need to use these parameters as the performance characteristics of information processes in the ISS - the completeness and timeliness of information processing. The article proposes performance indicators of information processes in integrated security systems in terms of optimal control procedures to protect information from unauthorized access. Set the considered parameters allows to conduct comprehensive security analysis of integrated security systems, and to provide recommendations to improve the management of information security procedures in them...|$|R
30|$|A {{novel and}} {{effective}} {{approach has been}} proposed in [9] with the NTIA-video quality metric (VQM) which combines in a single score the perceptual impact of different video artifacts (<b>block</b> <b>distortion,</b> noise, jerkiness, blurring, and color modifications). The NTIA-VQM is a general purpose video quality model designed and tested {{in a wide range}} of quality and bit rates. It is based on a preprocessing consisting in spatial and temporal alignment of reference and impaired sequences, region extractions, gain, and offset correction. Following, feature extraction, spatio-temporal parameter estimation, and local indexes polling are performed for computing the overall quality score. The features considered in VQM are extracted from the spatial gradients of the luminance and chrominance components and from measuring contrast and temporal information extracted from the luminance component only. It has been validated by exploiting extensive subjective and objective tests. The high correlation with MOS shown in the performed subjective tests is the reason for the wide use of this metric as a standard tool (ANSI) for FR video assessment[10].|$|E
40|$|Mapping from optical {{satellite}} images without ground control points (GCPs) {{is one of}} the goals of photogrammetry. Aiming at the mapping without GCPs, a large-scale block adjustment method based on the virtual control points is proposed in this paper, in which the single image is regarded as an adjustment unit to be organized. To overcome the <b>block</b> <b>distortion</b> caused by unstable adjustment without GCPs and the excessive accumulation of errors, the virtual control points created by the initial RPC model of the images are regarded as the weighted observations and introduced into the adjustment model to refine the adjustment. To verify the availability and the accuracy of the presented method, 26 406 three-linear-array images of ZY- 3 covering China are used as the basic data and 8000 uniformly distributed high precision check points are applied to evaluate the geometric accuracy of the DOM (digital ortho model) and DSM (digital surface model) production. The experimental results indicate that the standard deviations of plane and elevation are both better than 4 m and the mosaic accuracy of neighboring DOM is within better than 1 pixel, which could meet the demand of seamless mosaic...|$|E
40|$|Abstract. This paper {{proposes a}} novel and simple {{multipath}} search with atted-hexagon or diamond pattern for block motion estimation to achieve adjustable speed/accuracy in block-matching algorithm (BMA). To improve {{the accuracy of}} the fast BMA near to that of full search (FS), the inherent problem of being trapped at the local minimum <b>block</b> <b>distortion</b> measure (BDM) should be overcome substantially. In the proposed method, a threshold of BDM is introduced to determine the possible-optimal search directions in order to escape from being trapped into a local minimum BDM, followed by a atted-hexagon or diamond search performed in these directions with a BDM below a threshold. Then, the estimated motion vector will be re ned at each search step until the searching process is stopped. The BDM threshold will be adjustable for the purpose of adjusting the search speed and search accuracy speci ed in the certain applications. Experimental results show that the proposed multipath search algorithm can achieve an average match- ing probability up to 98 % near to that of FS and about 10 times of checking points faster than FS in most of real-world sequences...|$|E
40|$|This paper {{proposes a}} novel no-reference Perception-based Image Quality Evaluator (PIQUE) for {{real-world}} imagery. A {{majority of the}} existing methods for blind image quality assessment rely on opinion-based supervised learning for quality score prediction. Unlike these methods, we propose an opinion unaware methodology that attempts to quantify distortion {{without the need for}} any training data. Our method relies on extracting local features for predicting quality. Additionally, to mimic human behavior, we estimate quality only from perceptually significant spatial regions. Further, the choice of our features enables us to generate a fine-grained <b>block</b> level <b>distortion</b> map. Our algorithm is competitive with the state-of-the-art based on evaluation over several popular datasets including LIVE IQA, TID & CSIQ. Finally, our algorithm has low computational complexity despite working at the block-level...|$|R
30|$|The decoded WZ frame {{from the}} first stage is then {{predicted}} by block-based motion search and compensation as in conventional video coding using four references: the previous, forward, left camera, and right camera frames. More specifically, for each block in the decoded frame, the best matching <b>block</b> with minimum <b>distortion</b> is selected using the square absolute difference (SAD) as the distortion metric as shown in Figure 21. This generates a final SI.|$|R
40|$|Short-time {{instrumental}} {{neutron activation}} analysis, with high throughput, sensitivity and accuracy without matrix interferences, {{can be achieved}} {{in spite of the}} initial high count rate from both short and long-lived nuclide activation, if the experimental conditions are optimized by the combination of techniques. Thus the initial usually high counting rate can be faced by a loss-free counting system to avoid <b>blocking</b> or <b>distortion</b> of the system. The rapid radioactive decay and the consequent low counting statistics of short lived nuclides can be compensated by source-detector distance variation during the counting period and by cyclic and cumulative activation. Matrix spectral interferences can be reduced by ion exchange before activation for selective element preseparation and by neutron spectrum optimization such as epithermal neutron activation for selective element peak enhancement. © Springer-Verlag 1996...|$|R
40|$|Mapping from optical {{satellite}} images without ground control {{is one of}} the goals of photogrammetry. Using 8802 three linear array stereo images (a total of 26406 images) of ZY 3 over China, we propose a large-scale and non-control block adjustment method of optical {{satellite images}} based on the RPC model, in which a single image is regarded as an adjustment unit to be organized. To overcome the <b>block</b> <b>distortion</b> caused by unstable adjustment without ground control and the excessive accumulation of errors, we use virtual control points created by the initial RPC model of the images as the weighted observations and add them into the adjustment model to refine the adjustment. We use 8000 uniformly distributed high precision check points to evaluate the geometric accuracy of the DOM (Digital Ortho Model) and DSM (Digital Surface Model) production, for which the standard deviations of plane and elevation are 3. 6  m and 4. 2  m respectively. The geometric accuracy is consistent across the whole block and the mosaic accuracy of neighboring DOM is within a pixel, thus, the seamless mosaic could take place. This method achieves the goal of an accuracy of mapping without ground control better than 5  m for the whole China from ZY 3 satellite images...|$|E
30|$|Full-reference metrics (FR): the {{evaluation}} system {{has access to}} the original media. FR metrics can also be subclassified according to the computational complexity required to attain the metric: simple or complex objective metrics. Simple objective metrics are attractive because they are computed in a fast way, while often resulting in low fidelity matching of perceived quality of images and video. Probably, the most relevant example of a simple objective metric is the PSNR, which is widely used to perform a fast (and simple) quality evaluation. The MSE (mean square error) is also in this class. Some examples include the works by Daly [16], Lubin [17], Watson et al. [18], Wolf et al. [19], KPN research (DMOS-KPN) [20], and Winkler [21]. The Structural SIMilarity (SSIM) index is a widely used method for measuring the similarity between two images [22]. A more complete survey of the available FR video quality metrics is presented in [23]. The commonly adopted metric for assessing video quality is the NTIA-VQM model that has been developed by the National Telecommunications and Information Administration (NTIA). It uses objective parameters to measure perceptual effects such as blurring, <b>block</b> <b>distortion,</b> error blocks, noise, and unnatural motion. Given the good correlation with the subjective quality scores, the NTIA General VQM was adopted both as a national standard and as an international recommendation. The model returns values in the range from zero to one, which, respectively, denote no perceptual impairment and maximum perceived impairment.|$|E
30|$|This article {{proposes a}} simple but {{effective}} bitrate control algorithm that applies spatial resolution controls to the conventional QP-based bitrate control. In the proposed algorithm, a new model that represents {{the relationship between the}} spatial resolution and the peak signal-to-noise ratio (PSNR) for low bitrate coding is proposed. By using the proposed model, the spatial resolution that gives the highest PSNR at a given bitrate is estimated. In the proposed model, the computational complexity is very low as it only requires a small number of parameters and the value of parameters is obtained in a heuristic manner. The two scalability tools, the QP and the spatial resolution, are processed sequentially to achieve the target PSNR. The proposed spatial resolution adjustment which is applied in the group of pictures (GOP)-level is used as a coarse-grain bitrate control. Inside a GOP, QP is changed by a conventional bitrate control to meet the allocated bits in a fine-grain manner. Thus, the target bitrate is satisfied with a combination of two control methods. To estimate the perceptual quality of encoded video sequences with reduced spatial resolution, Video Quality Metric (VQM) software [20] is used to measure the subjective quality in addition to the PSNR which measures the objective quality. The VQM computes the perceptual effects of video impairments including blurring, jerky/unnatural motion, global noise, <b>block</b> <b>distortion</b> and color distortion, and combines them into a single metric. Experimental results show that proposed bitrate control scheme outperforms the conventional QP-based bitrate control algorithm at a variety of bitrates. At a low bitrate, the PSNR and VQM values with the proposed spatial resolution control scheme are improved up to 1.85 and 5.15 dB, respectively, when compared to that with the conventional QP-only control. There is only a small difference between the real optimal spatial resolution and the spatial resolution obtained using the proposed scheme.|$|E
40|$|This paper {{introduces}} a correlation-based method for the three-dimensional reconstruction of scenes from a multicamera imaging system. Our technique is {{to cast the}} matching and reconstruction problems into a single 3 D process that uses perspective distortions to directly retrieve, in a dense fashion, the 3 D planes locally tangent to the scene. Avoiding image assumptions like fronto-parallelism, <b>block</b> shapes, perspective <b>distortion</b> models or camera models, our method only needs a local planarity hypothesis of the scene. 1...|$|R
40|$|International audienceThis paper proposes an {{efficient}} approach to retarget images based on stretchability-aware block scaling. The image stretchability is first evaluated based on gradient, saliency and color features, {{and is used}} to generate the stretchable space. Then the optimal size of the stretched image is determined under the constraint of stretchable space and the same aspect ratio as the target image. Based on the analysis of image stretchability measures, the original image is partitioned into non-stretchable blocks and stretchable blocks, and their scaling factors are calculated based on their stretchability measures and the stretched image size, in order to possibly preserve non-stretchable <b>blocks</b> without <b>distortion</b> and reasonably resize stretchable blocks. Finally, the stretched image is uniformly scaled to generate the target image. Experimental results {{on a variety of}} images and the user study demonstrate that our approach achieves an overall better retargeting performance compared to the state-of-the-art image retargeting approaches...|$|R
40|$|In this paper, we {{analyze the}} {{formation}} of blocking artifacts {{as a result of}} quantization of the discrete cosine transform (DCT) coefficients. These artifacts are known to be the dominant distortion of lossy block-based hybrid image and video coding schemes when operating at very low data rates. Our analysis is carried out based on the theory of edge detection and by means of a mathematical framework to model the properties of receptive fields, which are known to be the main mechanism for sensing intensity contrasts and edges. Based on our analysis we propose a metric to quantify the <b>blocking</b> artifact <b>distortion.</b> The proposed distortion metric considers the susceptibility of a region in a reconstructed (decoded) image to {{the formation of}} blocking artifacts via a probabilistic model, which quantifies the possibility of a detected edge to be a distortion. * Index Terms — Blocking artifacts, receptive field, perceptua...|$|R
