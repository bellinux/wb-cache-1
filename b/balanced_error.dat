22|490|Public
40|$|Grammatical Evolution Neural Networks (GENN) is a {{computational}} method designed to detect gene-gene interactions in genetic epidemiology, but {{has so far}} only been evaluated in situations with balanced numbers of cases and controls. Real data, however, rarely has such perfectly balanced classes. In the current study, we test the power of GENN to detect interactions in data {{with a range of}} class imbalance using two fitness functions (classification error and <b>balanced</b> <b>error),</b> as well as data resampling. We show that when using classification error, class imbalance greatly decreases the power of GENN. Re-sampling methods demonstrated improved power, but using balanced accuracy resulted in the highest power. Based on the results of this study, <b>balanced</b> <b>error</b> has replaced classification error in the GENN algorithm...|$|E
40|$|Determining pitch accents in a {{sentence}} is a key task for a text-to-speech (TTS) system. We describe some methods for pitch accent assignment which make use of features that contain information about a complete phrase or sentence, in contrast to most previous work which has focused on using features local to a syllable or word. Pitch accent prediction is performed using three different techniques: N-gram models of syllable sequences, dynamic programming to match sequences of features, and decision trees. Using a C 4. 5 decision tree trained {{on a wide range}} of features, most notably each word's orthographic form and information extracted from the syntactic parse of the sentence, our feature set achieved a <b>balanced</b> <b>error</b> rate of 46. 6 %. This compares with the feature set used in [11] which had a <b>balanced</b> <b>error</b> rate of 55. 55 %...|$|E
30|$|The {{special case}} of the <b>balanced</b> <b>error</b> loss {{function}} is weighted quadratic loss when ω= 0. The balance loss function was introduced by Zellner [21] to reflect two criteria: goodness of fit and precision of estimation. Then the associated risk function with respect to (1), will be R(θ, δ)= E_θ [L(θ, δ)]. For more details {{about the use of}} this loss, we refer to Zinodiny et al. [22], Peng et al. [17], Cao and He [2] and Zinodiny et al. [23], to mention a few.|$|E
5000|$|Validates aqueous {{solutions}} (charge <b>balance</b> <b>error,</b> parameter adjustment) ...|$|R
30|$|Analytical {{results were}} checked by {{computing}} the ionic <b>error</b> <b>balance.</b> Results with ionic <b>balance</b> <b>error</b> > 5 % were rejected {{in accordance with}} international standards.|$|R
40|$|We {{sought to}} {{investigate}} the one-week and within-session reliability of the instrumented <b>balance</b> <b>error</b> scoring system test and the concurrent validity/one-week reliability of two neurocognitive assessments available through C 3 Logix. (n = 37) Participants completed two <b>balance</b> <b>error</b> scoring system tests separated by the Trails A, Trails B, and Symbol Digit Modality test available through C 3 Logix, and with paper and pencil. We found that the instrumented <b>balance</b> <b>error</b> scoring system test demonstrated strong one-week reliability and that neuropsychological tests available through C 3 Logix show acceptable concurrent validity with standard (comparable) paper and pencil measures...|$|R
40|$|Feature Selection is an {{important}} issues in data mining, its main purpose is to reduce dimensionality and to filter the noise. In this project, we will perform classification on data sets with preprocessing by feature selection. We focus on two feature selection methods, PCA and univariate significance test and combined them with SVM to do predictions. The output prediction <b>balanced</b> <b>error</b> rate and area under curve(AUC) has improved by using Feature selection methods. We will also have short discuss on different classifiers ’ performance, such as KNN...|$|E
30|$|To {{build the}} matrices, each outlook has a {{utilization}} matrix constructed {{for it and}} then singular value decomposition (SVD) is performed, as to align the marginal distributions, using the utilization matrices. This is done during the matching by rotation process which is performed to derive the transformation matrices for the class groupings. When presented with outlooks of different dimensions, the smaller utilization matrices are padded with zeros to equalize the dimensions overall. After deriving the transformations, one can apply a standard classification algorithm using the target outlook and the transformed source outlooks. Experiments were performed using homogeneous and heterogeneous environments for activity recognition using data from wearable sensors. These experiments are multiclass in nature, thus a multi-class SVM {{was used as the}} classifier. <b>Balanced</b> <b>error</b> rate was used as the performance metric due to the uneven class distribution of the dataset. For the heterogeneous experiments, the proposed MOMAP algorithm was compared with an SVM trained only on the limited target data and an SVM trained on a fully-labeled target dataset. The results showed that the proposed MOMAP algorithm was outperformed by the SVM trained on a fully-labeled target as one would expect. On the other hand, the proposed method outperformed the SVM trained only on the limited target data in most cases by having a lower <b>balanced</b> <b>error</b> rate. This, in effect, demonstrated the effectiveness of the proposed method.|$|E
40|$|We {{present and}} study the contribution-selection {{algorithm}} (CSA), a novel algorithm for feature selection. The algorithm {{is based on the}} multiperturbation shapley analysis (MSA), a framework that relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. It can optimize various performance measures over unseen data such as accuracy, <b>balanced</b> <b>error</b> rate, and area under receiver-operator-characteristic curve. Empirical comparison with several other existing feature selection methods shows that the backward elimination variant of CSA leads to the most accurate classification results on an array of data sets. ...|$|E
40|$|KEY POINTS •	Sources of mass <b>balance</b> <b>error</b> in a process-based {{hydrological}} {{model of}} surface-subsurface flow interaction are investigated {{to improve the}} model’s coupling scheme •	These sources of mass <b>balance</b> <b>error</b> are identified by using a set of dimensionless indices and the analysis of temporal and spatial patterns of error •	A time step control based on a degree of coupling index is proposed and the interpolation algorithm used to pass exchange variables of surface-subsurface flow interaction is improve...|$|R
40|$|A {{reliable}} {{prediction of}} the total runoff hydrograph is necessary for water resources management. This study investigates two approaches to generate total runoff hydrograph by adding baseflow to direct runoff hydrographs. The first approach uses a method, derived from a digital filter algorithm for hydrograph separation, to generate baseflow hydrographs from direct runoff hydrographs. The method appears to perform well in producing the overall shape {{of the total}} runoff hydrographs and the acceptable mass <b>balance</b> <b>errors</b> for a year of water cycle. For application, the recession baseflow constant needs to be estimated reliably and the initial baseflow could be approximated to the long-term mean dry weather flow. The second approach assumes a constant baseflow rate. Although this approach is still capable of producing the overall hydrograph shape, it yields high mass <b>balance</b> <b>errors</b> in the total runoff hydrographs for both monthly and long-term periods. Further analysis shows that two-third of the mass <b>balance</b> <b>errors</b> are contributed from periods with direct runoff, implying that the constant baseflow assumption could introduce significant errors into the computations of total runoff hydrograp...|$|R
30|$|The {{reaction}} (cationic and anionic <b>balance)</b> <b>error</b> (E) of all {{the groundwater}} samples was less than the accepted limit of ± 10 %, an added proof for the precision of the data (Matthess 1982; Domenico and Schwartz 1990).|$|R
40|$|We {{investigate}} {{the performance of}} MLPs with four risk functionals: the classical mean square error (MSE), the cross-entropy (CE), a generalized exponential risk (EXP), and the Shannon entropy of the classifier’s output error (HS). The performance is compared with an SVM with RBF kernel in terms of average balanced and unbalanced error rates, and their generalization, on practical classification tasks. For this purpose we carried out experiments on 35 public real-world datasets. A battery of statistical tests applied to the experimental results showed no significant difference among the classifiers in terms of unbalanced error rates. However, in terms of <b>balanced</b> <b>error</b> rates SVM-RBF performed significantly worse than MLP-CE and MLP-EXP. Regarding generalization, SVM-RBF and MLP-EXP scored as the classification methods with significantly better generalization, {{both in terms of}} balanced and unbalanced error rates. ...|$|E
40|$|In {{this paper}} a feature {{selection}} algorithm CSSFFS (Constrained search sequential floating forward search) based on SVM is proposed for detecting breast cancer. It is a greedy algorithm with search strategy of constrained search. The {{aim of this}} algorithm is to achieve a feature subset with minimal BER (<b>Balanced</b> <b>error</b> rate). This is a hybrid algorithm with the combination of filters and wrappers. Feature ranking with SVM acts as filters for removing irrelevant features. Then SFFS acts as wrapper which further removes the redundant features yielding the optimal subset of features. WDBC dataset from UCI machine learning depository {{is used for the}} experiment. The experiments are conducted in WEKA. After feature selection the accuracy and BER for WDBC dataset is 98. 2425 and 0. 0226 respectively with 15 features...|$|E
30|$|The problem domain {{defined by}} Harel [46] is of limited labeled target data and {{multiple}} labeled data sources where an asymmetric transformation is desired for each source {{to resolve the}} mismatch in feature space. The {{first step in the}} process is to normalize the features in the source and target domains, then group the instances by class in the source and target domains. For each class grouping, the features are mean adjusted to zero. Next, each individual source class group is paired with the corresponding target class group, and a singular value decomposition process is performed to find the specific transformation matrix for that class grouping. Once the transformation is performed, the features are mean shifted back reversing the previous step, and the final target classifier is trained using the transformed data. Finding the transformation matrix using the singular value decomposition process allows for the marginal distributions within the class groupings to be aligned while maintaining the structure of the data. This approach {{is referred to as the}} Multiple Outlook MAPping algorithm (MOMAP). The experiments use data taken from wearable sensors for the application of activity classification. There are five different activities defined for the experiment which include walking, running, going upstairs, going downstairs, and lingering. The source domain contains similar (but different) sensor readings as compared to the target. The method proposed by Harel [46] is compared against a baseline method that trains a classifier with the limited labeled target data and an upper bound method that uses a significantly larger set of labeled target data to train a classifier. An SVM learner is used as the base classifier and a <b>balanced</b> <b>error</b> rate (due to an imbalance in the test data) is measured as the performance metric. The Harel [46] approach outperforms the baseline method in every test and falls short of the upper bound method in every test with respect to the <b>balanced</b> <b>error</b> rate.|$|E
3000|$|... {{indicates}} the identified crotonic acid in the supernatant by HPLC. As a consequence, a closer value to 0 % indicates a better mass balance. In this study, {{most of the}} experiments had mass <b>balance</b> <b>errors</b> smaller than 5 % (see Table  1).|$|R
40|$|Sports-related {{concussion}} is {{a common}} sports injury that might induce potential long-term consequences without early diagnosis and intervention in the field. However, there are few options of such sensor systems available. The aim {{of the study is}} to propose and validate an automated concussion administration and scoring approach, which is objective, affordable and capable of detecting all <b>balance</b> <b>errors</b> required by the <b>balance</b> <b>error</b> scoring system (BESS) protocol in the field condition. Our approach is first to capture human body skeleton positions using two Microsoft Kinect sensors in the proposed configuration and merge the data by a custom-made algorithm to remove the self-occlusion of limbs. The standing <b>balance</b> <b>errors</b> according to BESS protocol were further measured and accessed automatically by the proposed algorithm. Simultaneously, the BESS test was filmed for scoring by an experienced rater. Two results were compared using Pearson coefficient r, obtaining an excellent consistency (r = 0. 93, p < 0. 05). In addition, BESS test–retest was performed after seven days and compared using intraclass correlation coefficients (ICC), showing a good test–retest reliability (ICC = 0. 81, p < 0. 01). The proposed approach could be an alternative of objective tools to assess postural stability for sideline sports concussion diagnosis...|$|R
40|$|Click on {{the link}} to access this {{abstract}} at the publisher's website. The <b>Balance</b> <b>Error</b> Scoring System (BESS) is a subjective clinical balance assessment frequently used by various healthcare providers. A test administrator records the number of pre-defined errors committed by the test subject as they perform a number of balance stances. The <b>Balance</b> <b>Error</b> Scoring System (BESS) is a subjective clinical balance assessment frequently used by various healthcare providers. The test consists {{of a total of}} three stances including bipedal standing (feet together), non-dominant single-leg stance, and tandem standing (heel-to-toe with non-dominant foot behind the dominant foot). Stances are first performed on a firm surface, and then on a compliant foam surface. For each 20 second trial, a test administrator records the number of pre-defined errors committed by the test subject. A total score is calculated by summing the total number of errors committed. A modified version of the BESS has also been developed. This version omits the stances performed on a foam surface. However, it is possible that a 10 second limit for each stance is sufficient time for balance assessment. PURPOSE: The {{purpose of this study was}} to determine if 10 seconds for each balance stance provides adequate time for balance assessment using the modified BESS assessment. METHODS: 39 college age Division I athletes (20 male, 19 female; aged 19. 5 + 1. 5 years) performed a modified BESS assessment. Subjects performed a familiarization trial immediately followed by an experimental trial. All BESS scoring was completed by a certified Athletic Trainer. A timer was placed in view of the scorer and the total number of <b>balance</b> <b>errors</b> committed during the first 10 seconds and final 10 seconds of the test were recorded separately. RESULTS: Mean total BESS score was 5. 8 (+ 3. 9). Mean BESS scores for the first 10 seconds and final 10 seconds were 2. 6 (+ 2. 2) and 3. 2 (+ 2. 3) respectively. Paired sample t-test revealed no significant difference between the mean number of <b>balance</b> <b>errors</b> committed during the first and last 10 seconds of the assessment (p> 0. 05). CONCLUSION: No significant difference in mean <b>balance</b> <b>errors</b> were observed between the first 10 seconds and final 10 seconds of the BESS assessment. This indicates that limiting each balance stance to 10 seconds may provide sufficient evidence of the subjects overall balance status...|$|R
40|$|We {{discuss the}} {{reconstruction}} of piecewise smooth data from its (pseudo-) spectral information. Spectral projections enjoy superior resolution provided the function is globally smooth, while the presence of jump discontinuities is responsible for spurious O(1) Gibbs’ oscillations {{in the neighborhood of}} edges and an overall deterioration of the convergence rate to the unacceptable first order. Classical filters and mollifiers are constructed to have compact support in the Fourier (frequency) and physical (time) spaces respectively, and are dilated by the projection order or the width of the smooth region to maintain this compact support in the appropriate region. Here we construct a noncompactly supported filter and mollifier with optimal joint time-frequency localization for a given number of vanishing moments, resulting in a new fundamental dilation relationship that adaptively links the time and frequency domains. Not giving preference to either space allows for a more <b>balanced</b> <b>error</b> decomposition, which when minimized yields an optimal filter and mollifier that retain the robustness of classical filters, yet obtain true exponential accuracy...|$|E
40|$|This {{paper is}} {{dedicated}} to Eitan Tadmor for his direction. Abstract. We discuss the reconstruction of piecewise smooth data from its (pseudo-) spectral information. Spectral projections enjoy superior resolution provided the function is globally smooth, while the presence of jump discontinuities is responsible for spurious O(1) Gibbs ' oscillations {{in the neighborhood of}} edges and an overall deterioration of the convergence rate to the unacceptable rst order. Classical lters and molliers are constructed to have compact support in the Fourier (frequency) and physical (time) spaces respectively, and are dilated by the projection order or the width of the smooth region to maintain this compact support in the appropriate region. Here we construct a non-compactly supported lter and mollier with optimal joint time-frequency localization for a given number of vanishing moments, resulting in a new fundamental dilation relationship that adaptively links the time and frequency domains. Not giving preference to either space allows for a more <b>balanced</b> <b>error</b> decomposition, which when minimized yields an optimal lter and molli er that retain the robustness of classical lters, yet obtain true exponential accuracy...|$|E
40|$|Single image shadow {{detection}} is a {{very challenging}} problem because of the limited amount of information available in one image, {{as well as the}} scarcity of annotated training data. In this work, we propose a novel adversarial training based framework that yields a high performance shadow detection network (D-Net). D-Net is trained together with an Attenuator network (A-Net) that generates adversarial training examples. A-Net performs shadow attenuation in original training images constrained by a simplified physical shadow model and focused on fooling D-Net's shadow predictions. Hence, it is effectively augmenting the training data for D-Net with hard to predict cases. Experimental results on the most challenging shadow detection benchmark show that our method outperforms the state-of-the-art with a 38 % error reduction, measured in terms of <b>balanced</b> <b>error</b> rate (BER). Our proposed shadow detector also obtains state-of-the-art results on a cross-dataset task testing on UCF with a 14 % error reduction. Furthermore, the proposed method can perform accurate close to real-time shadow detection at a rate of 13 frames per second...|$|E
40|$|VIC 4. 2. c (12 December 2015) This release {{contains}} {{a number of}} bug fixes. Bug Fixes 	Fixed water <b>balance</b> <b>errors</b> in lake model (# 308, # 316) 	Fixed OUTPUT_FORCE behavior related to veg_hist features (# 305, # 307) 	Updated bare soil evaporation behavior related to veg_hist update from 4. 2 (# 306, # 315...|$|R
40|$|In this research, a {{systematic}} set of numerical simulations is conducted {{to investigate the}} accuracy of the Fire Dynamics Simulator (FDS 6) in predicting under-ventilated enclosure fires and external flaming. The accuracy of the predictions is examined first in terms of mass balance at the steady-state stage for the enclosure volume. The required fineness of the grid was determined by analyze the mass <b>balance</b> <b>error</b> using two non-dimensional length scales. The first one considers the ventilation factor, and the second one considers the hydraulic diameter of the opening. When these two length scales are larger than 10, the corresponding mass <b>balance</b> <b>error</b> is lower than 4 %. The predicted results for the (1) heat release rate inside the enclosure, (2) average gas temperature inside the enclosure, (3) external flame height, and (4) heat flux to the façade, show good agreement with experimental results or empirically-derived correlations...|$|R
40|$|The virtual {{lysimeter}} {{concept was}} tested {{in comparison with}} a real lysimeter and found to be suitable for quantifying effective deep seepage dynamics in sandy soils. Discharge measurements and calculation results agreed well. Preconditions are accurate water content and tension measurements with high temporal resolution below the zero flux plane and an <b>error</b> free water <b>balance</b> of the calibration period. The calibration procedure has resulted in an effective unsaturated hydraulic conductivity function which allows to perform deep seepage calculations {{based on the measured}} water content dynamics only. The assumption of the unit gradient produced adequate results in sandy soils. The calculation results are exponentially sensitive to errors of water content measurements and linearly sensitive to water <b>balance</b> <b>errors.</b> However, a single incorrect water content produces only a single incorrect deep seepage value, whereas the water <b>balance</b> <b>error</b> sums up. Therefore, the quality of the water balance estimation is of crucial importance...|$|R
40|$|Fano’s {{inequality}} lower bounds {{the probability}} of transmission error through a communication channel. Applied to classification problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in {{more than just the}} error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are <b>balanced</b> <b>error</b> rate (BER) and F-score. In this work, we focus on the two-class problem and use a general definition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classifier. As is widely known, a threshold of 0. 5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER...|$|E
40|$|The {{need for}} multivariate {{analysis}} of magnetic resonance spectroscopy (MRS) data was recognized about 20 years ago, when it became evident that spectral patterns were characteristic of some diseases. Despite this, there is no generally accepted methodology for performing pattern recognition (PR) analysis of MRS data sets. Here, the data acquisition and processing requirements for performing successful PR as applied to human MRS studies are introduced, and the main techniques for feature selection, extraction, and classification are described. These include methods of dimensionality reduction such as principal component analysis (PCA), independent component analysis (ICA), non-negative matrix factorization (NMF), and feature selection. Supervised methods such as linear discriminant analysis (LDA), logistic regression (LogR), and nonlinear classification are discussed separately from unsupervised and semisupervised classification techniques, including k –means clustering. Methods for testing and metrics for gauging the performance of PR models (sensitivity and specificity, the ‘Confusion Matrix’, ‘k –fold cross-validation’, ‘Leave One Out’, ‘Bootstrapping’, the ‘Receiver Operating Characteristic curve’, and <b>balanced</b> <b>error</b> and accuracy rates) are briefly described. This article ends with {{a summary of the}} main lessons learned from PR applied to MRS to date...|$|E
40|$|Section 303 (d) of the Clean Water Act {{requires}} {{states to}} establish a list of water bodies that do not meet water quality standards. State Water Resources Control Board staff recently proposed the binomial hypothesis test when deciding to list or delist a water body. The traditional binomial test effectively controls the alpha error rate (i. e., the chance of incorrectly rejecting a true null hypothesis) at or below the proposed nominal significance level of 10 %. Several authors, however, {{have suggested that the}} beta error rate (i. e., the chance of incorrectly failing to reject a false null hypothesis) should be considered when listing or delisting and that alpha and beta rates should be equally balanced, if possible. The methodology and probability equations used to derive sampling plans based on observed exceedances is reviewed, both for the proposed traditional binomial test and a binomial test based on a <b>balanced</b> <b>error</b> approach. Approximate error balancing provides an equitable way to decide whether a water body should be listed or delisted, as long as a sufficient number of samples are collected to keep the error rates below a moderate level...|$|E
30|$|Power {{produced}} {{should be}} instantly {{consumed in the}} power system to maintain the nominal system frequency. Any contingency such as loss of generation or sudden increase in the load would disturb the steady state and frequency will deviate from the nominal value. Spinning reserve has been traditionally deployed in the power system to maintain the <b>balance</b> <b>error</b> between generation and load demand.|$|R
40|$|The MMOC {{procedure}} for approximating the solutions of transport-dominated diffusion problems {{does not automatically}} preserve integral conservation laws, leading to (mass) <b>balance</b> <b>errors</b> in many kinds of flow problems. The variant, called the MMOCAA, discussed herein preserves the conservation law at a minor additional computational cost. The application of the MMOCAA to a problem in two-phase, immiscible flow in porous media is discussed...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 150 - 152). Issued also on microfiche from Lange Micrographics. This thesis addresses two important issues related to streamline-based flow simulation. The first issue deals with velocity models, specifically, how to determine the velocity models in various geometries in 2 D and 3 D space, and how to draw streamlines and calculate time of flight in various gridblock systems. The present research builds on the previous work of Cordes and Kinzelbach, 1992. Transformations from the real space of {x,y,z} to the unit cube space of {?, ?, ?} have been extensively utilized to draw the streamlines in 2 D and 3 D corner point cell geometry and also to calculate the time of flight. The use of cell-averaged jacobian of the transformation, as is typically done in streamline simulators to determine the time of flight is investigated for its accuracy. The impact of cell skewness in time of flight calculations is also investigated. Our results indicate that the cell-averaged jacobian leads to incorrect time of flight. The second issue that is addressed is related to material <b>balance</b> <b>error</b> during streamline simulation. The material <b>balance</b> <b>error</b> is generated during the discretization of gridblock into streamtubes ([]-discretizations) and also during the saturation assignment to a streamline from the gridblock and vice versa ([]-discretizations). The behavior of the material <b>balance</b> <b>error</b> associated with []-discretizations is studied with respect to parameters like the selection of the gridblock and the number of streamlines in the gridblock. Furthermore, we also examine the behavior of the error associated with []-discretizations with respect {{to the size of the}} discretizations along a streamline. Our results show that the material <b>balance</b> <b>error</b> associated with []-discretizations converges slowly (with a slope of - 1. 0) in a gridblock, if it has a stagnation point. However, if the gridblock does not have any stagnation point in it, then the error converges faster (with a slope of - 2. 0). For the []-discretizations along a streamline, the L? and L[] Norm errors are shown to be inversely proportional to the number of []-discretizations. The L? Norm error is shown to be inversely proportional to the square root of the number of []-discretizations...|$|R
40|$|This work is {{motivated}} by the needs of state agencies which are responsible for managing various risks in social life issue advisories to the public to prevent and mitigate various hazards. We investigated how information about a common food born health hazard, known as Campylobacter, spreads once it was delivered to {{a random sample of}} individuals in France. The aim of the work is to study how properties of the individual and broader social network eect the diusion of information within a simulated process. To this eect we modelled a social network using a set of characteristics of individuals over Erdos-Renyi [1] and Small World [3] random graph models. The social network was based on the data collected following a survey on Campylobacter conducted in France. Using this information we were also able to learn when information transmissions occurred between pairs of individuals, and could then simulate the diusion process over the simulated network and measure certain properties of interest. The results uncovered a strong predictability of information transmission, providing a <b>balanced</b> <b>error</b> rate of 0 : 092. The diusion model was studied {{in the context of the}} eect of the network structure in the overall diusion, and also how prediction was made for a particular social tie. Furthermore, graph visualisation methods were used in order to understand the largest connecte...|$|E
40|$|Purpose: We {{propose a}} fully {{automated}} {{method for detection}} and segmentation of the abnormal tissue associated with brain tumour (tumour core and oedema) from Fluid- Attenuated Inversion Recovery (FLAIR) Magnetic Resonance Imaging (MRI). Methods: The method is based on superpixel technique and classification of each superpixel. A number of novel image features including intensity-based, Gabor textons, fractal analysis and curvatures are calculated from each superpixel within the entire brain area in FLAIR MRI to ensure a robust classification. Extremely randomized trees (ERT) classifier is compared with support vector machine (SVM) to classify each superpixel into tumour and non-tumour. Results: The proposed method is evaluated on two datasets: (1) Our own clinical dataset: 19 MRI FLAIR images of patients with gliomas of grade II to IV, and (2) BRATS 2012 dataset: 30 FLAIR images with 10 low-grade and 20 high-grade gliomas. The experimental results demonstrate the high detection and segmentation performance of the proposed method using ERT classifier. For our own cohort, the average detection sensitivity, <b>balanced</b> <b>error</b> rate and the Dice overlap measure for the segmented tumour against the ground truth are 89. 48 %, 6 % and 0. 91, respectively, while, for the BRATS dataset, the corresponding evaluation results are 88. 09 %, 6 % and 0. 88, respectively. Conclusions: This provides a close match to expert delineation across all grades of glioma, leading to a faster and more reproducible method of brain tumour detection and delineation to aid patient management...|$|E
40|$|Early {{recognition}} of ventricular fibrillation (VF) and electrical therapy are key {{for the survival}} of out-of-hospital cardiac arrest (OHCA) patients treated with automated external defibrillators (AED). AED algorithms for VF-detection are customarily assessed using Holter recordings from public electrocardiogram (ECG) databases, which may be different from the ECG seen during OHCA events. This study evaluates VF-detection using data from both OHCA patients and public Holter recordings. ECG-segments of 4 -s and 8 -s duration were analyzed. For each segment 30 features were computed and fed to state of the art machine learning (ML) algorithms. ML-algorithms with built-in feature selection capabilities were used to determine the optimal feature subsets for both databases. Patient-wise bootstrap techniques were used to evaluate algorithm performance in terms of sensitivity (Se), specificity (Sp) and <b>balanced</b> <b>error</b> rate (BER). Performance was significantly better for public data with a mean Se of 96. 6 %, Sp of 98. 8 % and BER 2. 2 % compared to a mean Se of 94. 7 %, Sp of 96. 5 % and BER 4. 4 % for OHCA data. OHCA data required two times more features than the data from public databases for an accurate detection (6 vs 3). No significant differences in performance were found for different segment lengths, the BER differences were below 0. 5 -points in all cases. Our results show that VF-detection is more challenging for OHCA data than for data from public databases, and that accurate VF-detection is possible with segments as short as 4 -s...|$|E
40|$|Context: Current {{concussion}} assessments use static {{measures that}} may overlook subtle impairments. Visual-motor tracking tasks provide {{an option for}} dynamic evaluation of behavior post-concussion, which might reveal subtle and meaningful changes in motor behavior. Objective: This study compared measurements of performance regularity over time (approximate entropy; ApEn), obtained from a visual-motor tracking task, captured both pre and post concussion. Visual-motor tracking performance was compared to the Standard Assessment of Concussion (SAC) and the modified <b>Balance</b> <b>Error</b> Scoring System (mBESS) ...|$|R
40|$|In this work, a first-order upwind and a {{high-order}} flux-limiter {{schemes for}} solving the advection–diffusion equation on unstructured grids were evaluated. The numerical schemes were implemented as a module of an unstructured two-dimensional depth-averaged circulation model for shallow lakes (IPH-UnTRIM 2 D), {{and they were}} applied to the Guaíba River in Brazil. Their performances were evaluated by comparing mass conservation <b>balance</b> <b>errors</b> for two scenarios of a passive tracer released into the Guaíba River. The circulation model showed good agreement with observed data collected at four water level stations along the Guaíba River, where correlation coefficients achieved values up to 0. 93. In addition, volume conservation errors were lower than 1 % of the total volume of the Guaíba River. For all scenarios, the higher order flux-limiter scheme {{has been shown to}} be less diffusive than a first-order upwind scheme. Accumulated conservation mass <b>balance</b> <b>errors</b> calculated for the flux limiter reached 8 %, whereas for a first-order upwind scheme, they were close to 18 % over a 15 -day period. Although both schemes have presented mass conservation errors, these errors are assumed negligible compared with kinetic processes such as erosion, sedimentation or decay rates...|$|R
30|$|Regarding the MSE {{shown in}} Fig.  8, the equal-training set has {{a wider range}} of {{estimated}} ages, in addition to <b>balanced</b> <b>errors</b> for most estimated age groups. Moreover, the MSE of the equal-training set presents a clear trend with respect to the age groups, which provides the possibility of performance improvement. By contrast, the age groups less than 5 years old in the unequal-training set have larger MSEs than the other age groups, which also indicates that there is room to improve performance.|$|R
