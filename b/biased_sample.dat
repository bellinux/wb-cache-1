226|3690|Public
25|$|Choice-based {{sampling}} {{is one of}} the stratified sampling strategies. In choice-based sampling, {{the data}} are stratified on the target and a sample is taken from each stratum so that the rare target class will be more represented in the sample. The model is then built on this <b>biased</b> <b>sample.</b> The effects of the input variables on the target are often estimated with more precision with the choice-based sample even when a smaller overall sample size is taken, compared to a random sample. The results usually must be adjusted to correct for the oversampling.|$|E
2500|$|If the <b>biased</b> <b>sample</b> {{variance}} (the second {{central moment}} of the sample, which is a downward-biased estimate of the population variance) is used to compute {{an estimate of the}} population's standard deviation, the result is ...|$|E
2500|$|Hence [...] {{gives an}} {{estimate}} of the population variance that is biased by a factor of [...] For this reason, [...] {{is referred to as the}} <b>biased</b> <b>sample</b> variance. Correcting for this bias yields the unbiased sample variance: ...|$|E
40|$|Data mining {{in large}} data sets often {{requires}} a sampling or summarization step {{to form an}} in-core representation of the data that can be processed more e#ciently. Uniform random sampling is frequently used in practice and also frequently criticized because it will miss small clusters. Many natural phenomena are known to follow Zipf's distribution and the inability of uniform sampling to #nd small clusters is of practical concern. Density <b>Biased</b> <b>Sampling</b> is proposed to probabilistically under-sample dense regions and over-sample light regions. A weighted sample is used to preserve the densities of the original data. Density <b>biased</b> <b>sampling</b> naturally includes uniform sampling as a special case. A memory e#cient algorithm is proposed that approximates density <b>biased</b> <b>sampling</b> using only a single scan of the data. We empirically evaluate density <b>biased</b> <b>sampling</b> using synthetic data sets that exhibit varying cluster size distributions. Our proposed method scales linearly and out performs uni [...] ...|$|R
50|$|Press {{developed}} {{square root}} <b>biased</b> <b>sampling</b> {{as a way}} to sample long sequences of DNA. It had also been developed independently by Ruben Abagyan, a professor at TSRI in La Jolla, California, for use in a different biological context. An even earlier discovery was by Martin L. Shooman, who used square root <b>biased</b> <b>sampling</b> in a test apportionment model for software reliability.|$|R
40|$|This paper {{describes}} {{the use of}} <b>Biased</b> <b>Sampling</b> to solve an inventory planning problem at a firm which produces plastic coated steel products. In this application, the <b>Biased</b> <b>Sampling</b> solutions provided {{an estimate of the}} inventory investment savings to be obtained from a major capital investment proposal. The problem structure, the development of biasing procedures to guide the solution process, and the computational results are presented. ...|$|R
2500|$|Secondly, {{the sample}} {{variance}} does not generally minimize {{mean squared error}} between sample variance and population variance. Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see [...] ), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than n−1), and is a simple example of a shrinkage estimator: one [...] "shrinks" [...] the unbiased estimator towards zero. For the normal distribution, dividing by n+1 (instead of n−1 or n) minimizes mean squared error. The resulting estimator is biased, however, and {{is known as the}} <b>biased</b> <b>sample</b> variation.|$|E
50|$|The {{problem of}} a size <b>biased</b> <b>sample</b> was {{discussed}} by Cox in 1969 on a problem of sampling fibres. The expectation of size <b>biased</b> <b>sample</b> is equal to its contraharmonic mean.|$|E
5000|$|... #Subtitle level 2: Statistical {{corrections}} for a <b>biased</b> <b>sample</b> ...|$|E
40|$|We {{introduce}} a quantum algorithm for efficient <b>biased</b> <b>sampling</b> {{of the rare}} events generated by classical memoryful stochastic processes. We show that this quantum algorithm gives an extreme advantage over known classical <b>biased</b> <b>sampling</b> algorithms {{in terms of the}} memory resources required. The quantum memory advantage ranges from polynomial to exponential and when sampling the rare equilibrium configurations of spin systems the quantum advantage diverges. Comment: 11 pages, 9 figures; [URL]...|$|R
50|$|However, {{selection}} <b>bias</b> and <b>sampling</b> <b>bias</b> {{are often}} used synonymously.|$|R
40|$|Abstract: In studies {{involving}} lifetimes, observed survival {{times are}} frequently censored and possibly subject to <b>biased</b> <b>sampling.</b> In this paper, we model survival times under <b>biased</b> <b>sampling</b> (a. k. a., biased survival data) by a semiparametric model, {{in which the}} selection function w(t) (that leads to the <b>biased</b> <b>sampling)</b> is specified up to an unknown finite dimensional parameter θ, while the density function f(t) of the survival times is assumed only to be smooth. Under this model, two estimators are derived to estimate the density function f, and a pseudo maximum likelihood estimation procedure is developed to estimate θ. The identifiability of the estimation problem is discussed {{and the performance of}} the new estimators is illustrated via both simulation studies and a real data application. 1...|$|R
5000|$|... sn2 is the <b>biased</b> <b>sample</b> {{variance}} (i.e. without Bessel's correction) ...|$|E
5000|$|... where m2 in the {{denominator}} is the (<b>biased)</b> <b>sample</b> second central moment.|$|E
5000|$|<b>Biased</b> <b>sample</b> - When {{the above}} happen because of (personal) bias of the {{sampling}} entity.|$|E
50|$|Another methodological {{aspect is}} the {{avoidance}} of bias, which can involve cognitive bias, cultural <b>bias,</b> or <b>sampling</b> <b>bias.</b> Methods for avoiding or overcoming such <b>biases</b> include random <b>sampling</b> and double-blind trials.|$|R
5000|$|The higher D/H ratios in Martian meteorites {{could be}} a {{consequence}} of <b>biased</b> <b>sampling</b> since Mars may have never had an effective crustal recycling process ...|$|R
40|$|In studies {{involving}} lifetimes, observed survival {{times are}} frequently censored and possibly subject to <b>biased</b> <b>sampling.</b> In this paper, we model survival times under <b>biased</b> <b>sampling</b> (a. k. a., biased survival data) by a semi-parametric model, {{in which the}} selection function w(t) (that leads to the <b>biased</b> <b>sampling)</b> is specified up to an unknown finite dimensional parameter θ, while the density function f(t) of the survival times is assumed only to be smooth. Under this model, two estimators are derived to estimate the density function f, and a pseudo maximum likelihood estimation procedure is developed to estimate θ. The identifiability of the estimation problem is discussed {{and the performance of}} the new estimators is illustrated via both simulation studies and a real data application. Comment: Published at [URL] in the IMS Lecture Notes [...] Monograph Series ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
5000|$|The biased {{weighted}} sample variance [...] is defined similarly {{to the normal}} <b>biased</b> <b>sample</b> variance : ...|$|E
50|$|This {{provides}} an estimated correction of bias {{due to the}} estimation method. The jackknife does not correct for a <b>biased</b> <b>sample.</b>|$|E
50|$|Multiplying the <b>biased</b> <b>sample</b> {{variance}} by the factorgives an {{unbiased estimator}} {{of the population}} variance. In some literature, the above factor is called Bessel's correction.|$|E
40|$|In this paper, {{we present}} a general {{statistical}} framework within which we can draw a new interpretation of the Laakso-Taagepera effective number of parties fragmentation index. With the particular method of sampling with probability proportional to the party sizes, we show that the Laakso-Taagepera effective number of parties is the inverse of the size biased version of the traditional expected party size in shares. Further, we provide an axiomatic definition of the Laakso-Taagepera effective number of parties. Fragmentation, effective number of parties, concentration index, size <b>biased</b> <b>sampling,</b> length <b>biased</b> <b>sampling.</b> ...|$|R
40|$|Longitudinal {{observations}} are sometimes costly or not available. Cross sectional sampling {{can be an}} alternative. Observations are drawn then at a specific point in time from a population of durations whose distributions satisfy a core model. Subsequently, one has a choice. One may process the data immediately, obtaining so called current duration data. Or one waits until the sampled durations are known completely obtaining the full durations via length <b>biased</b> <b>sampling.</b> We compare the Fisher information for the Euclidean parameter corresponding to an Accelerated Failure Time core model when the {{observations are}} obtained by either current duration or length <b>biased</b> <b>sampling...</b>|$|R
50|$|The {{asymptotic}} efficiency of length <b>biased</b> <b>sampling</b> depends compared to random sampling on the underlying distribution. if f(x) is log normal the efficiency is 1 while if {{the population is}} gamma distributed with index b, the efficiency is b/(b−1).|$|R
5000|$|If the <b>biased</b> <b>sample</b> {{variance}} (the second {{central moment}} of the sample, which is a downward-biased estimate of the population variance) is used to compute {{an estimate of the}} population's standard deviation, the result is ...|$|E
5000|$|Hence [...] {{gives an}} {{estimate}} of the population variance that is biased by a factor of [...] For this reason, [...] {{is referred to as the}} <b>biased</b> <b>sample</b> variance. Correcting for this bias yields the unbiased sample variance: ...|$|E
5000|$|In 2000, Card and Krueger redid {{their study}} using a data set from the Bureau of Labor Statistics and {{reproduced}} their earlier conclusions. [...] They {{also showed that}} Neumark and Wascher's results were due to a non-random <b>biased</b> <b>sample</b> of restaurants.|$|E
25|$|These {{conclusions}} can {{of course}} {{be applied to}} <b>biased</b> <b>sampling</b> of other items than fish. In general, {{we can say that}} the odds parameter has a stronger effect in Wallenius' distribution than in Fisher's distribution, especially when n/N is high.|$|R
40|$|Cryptic {{relatedness}} {{was suggested}} {{to be an}} important source of confounding in population-based association studies (PBAS). The magnitude and manner of cryptic relatedness affecting the performance of PBAS of continuous traits remain to be investigated. We simulated a set of related <b>samples</b> through <b>biased</b> <b>sampling</b> and inbreeding, and evaluated the power and type I error rates of simple association tests (SAT) without correcting for cryptic relatedness. We also used extended likelihood ratio tests (ELRT) to conduct PBAS accounting for cryptic relatedness, and compared it with genomic control (GC). Cryptic relatedness decreased the power as well as increased the type I error rates of SAT in both <b>biased</b> <b>sampling</b> and inbreeding models. The impact of cryptic relatedness on the performance of SAT appeared to be limited in the <b>biased</b> <b>sampling</b> model. However, cryptic relatedness in inbred populations may result in excessive false positive results of SAT. Compared with SAT and GC, ELRT obtained improved power and type I error rates under various scenarios. Ignoring cryptic relatedness may increase spurious association results in PBAS. Our ELRT provides a novel approach to control cryptic relatedness in PBAS of human continuous traits...|$|R
40|$|The {{application}} of reinforcement learning to problems with continuous domains requires representing the value function {{by means of}} function approximation. We identify two aspects of reinforcement learning that make the function approximation process hard: non-stationarity of the target function and <b>biased</b> <b>sampling.</b> Non-stationarity {{is the result of}} the bootstrapping nature of dynamic programming where the value function is estimated using its current approximation. <b>Biased</b> <b>sampling</b> occurs when some regions of the state space are visited too often, causing a reiterated updating with similar values which fade out the occasional updates of infrequently sampled regions. We propose a competitive approach for function approximation where many different local approximators are available at a given input and the one with expectedly best approximation is selected by means of a relevance function. The local nature of the approximators allows their fast adaptation to non-stationary changes and mitigates the <b>biased</b> <b>sampling</b> problem. The coexistence of multiple approximators updated and tried in parallel permits obtaining a good estimation much faster than would be possible with a single approximator. Experiments in different benchmark problems show that the competitive strategy provides a faster and more stable learning than non-competitive approaches. Peer reviewe...|$|R
50|$|How {{much the}} {{premises}} support the conclusion depends upon (a) {{the number in}} the sample group, (b) the number in the population, and (c) {{the degree to which}} the sample represents the population (which may be achieved by taking a random sample). The hasty generalization and the <b>biased</b> <b>sample</b> are generalization fallacies.|$|E
5000|$|This {{correction}} is {{so common}} {{that the term}} [...] "sample variance" [...] and [...] "sample standard deviation" [...] are frequently used to mean the corrected estimators (unbiased sample variation, less <b>biased</b> <b>sample</b> standard deviation), using n − 1. However caution is needed: some calculators and software packages may provide for both or only the more unusual formulation. This article uses the following symbols and definitions: ...|$|E
5000|$|The median {{cost of a}} wedding, {{including}} both the ceremony and reception, in the United States, as of 2012, was about US $18,000, according to a large survey at an online wedding website. [...] Regional differences are significant, with residents of Manhattan paying more than six {{times as much as}} residents of Alaska. [...] Additionally, the survey probably overestimates the typical cost because of a <b>biased</b> <b>sample</b> population.|$|E
5000|$|RRT*-Smart, {{a method}} for {{accelerating}} the convergence rate of RRT* by using path optimization (in a similar fashion to Theta*) and intelligent <b>sampling</b> (by <b>biasing</b> <b>sampling</b> towards path vertices, which - after path optimization - {{are likely to be}} close to obstacles) ...|$|R
3000|$|Our new AGB {{equations}} outperformed all existing local equations. Most of {{the local}} models tended to have a systematic errors, potentially due to field measurement errors or <b>biased</b> <b>samples.</b> The existing pantropical equations performed only slightly worse than our new equations. The DGH [...]...|$|R
40|$|Art price indexes {{typically}} rely on heavily <b>biased</b> <b>samples</b> {{for several}} reasons. A refined hedonic index is developed which explicitly addresses such concerns. An empirical illustration is then provided for Symbolist paintings appearing at auctions between 1990 and 2001. Paintings Hedonic price Price index...|$|R
