878|105|Public
25|$|Unconditional {{branches}} {{update the}} program counter {{with a new}} address computed {{in the same way}} as conditional branches. They also save the address of the instruction following the unconditional branch to a register. There are two such instructions, and they differ only in the hints provided for the <b>branch</b> <b>prediction</b> hardware.|$|E
25|$|There {{are four}} jump {{instruction}}s. These all perform the same operation, saving {{the address of}} the instruction following the jump, and providing the program counter with a new address from a register. They differ in the hints provided to the <b>branch</b> <b>prediction</b> hardware. The unused displacement field is used for this purpose.|$|E
25|$|Upon {{agreement}} {{of a system}} design, RTL designers then implement the functional models in a hardware description language like Verilog, SystemVerilog, or VHDL. Using digital design components like adders, shifters, and state machines as well as computer architecture concepts like pipelining, superscalar execution, and <b>branch</b> <b>prediction,</b> RTL designers will break a functional description into hardware models of components on the chip working together. Each of the simple statements described in the system design can easily turn into thousands of lines of RTL code, {{which is why it}} is extremely difficult to verify that the RTL will do the right thing in all the possible cases that the user may throw at it.|$|E
50|$|In Computer Architecture, While <b>Branch</b> <b>{{predictions}}</b> <b>Branch</b> queue takes place. When Branch Predictor predicts if {{the branch}} is taken or not, Branch queue stores the predictions {{that to be}} used later.|$|R
50|$|Some superscalar {{processors}} (MIPS R8000, Alpha 21264 and Alpha 21464 (EV8)) fetch {{each line}} of instructions with a pointer {{to the next}} line. This next line predictor handles <b>branch</b> target <b>prediction</b> as well as <b>branch</b> direction <b>prediction.</b>|$|R
50|$|Trace {{lines are}} stored in the trace cache based on the program counter of the first {{instruction}} in the trace {{and a set of}} <b>branch</b> <b>predictions.</b> This allows for storing different trace paths that start on the same address, each representing different branch outcomes. This method of tagging helps to provide path associativity to the trace cache. Other method can include having only starting PC as tag in trace cache. In the instruction fetch stage of a pipeline, the current program counter along with a set of <b>branch</b> <b>predictions</b> is checked in the trace cache for a hit. If there is a hit, a trace line is supplied to fetch unit which does not {{have to go to a}} regular cache or to memory for these instructions. The trace cache continues to feed the fetch unit until the trace line ends or until there is a misprediction in the pipeline. If there is a miss, a new trace starts to be built.|$|R
2500|$|A 2008 re-evaluation of this {{algorithm}} {{showed it}} to be no faster than ordinary heapsort for integer keys, though, presumably because modern <b>branch</b> <b>prediction</b> nullifies {{the cost of the}} predictable comparisons which bottom-up heapsort manages to avoid. [...] (It still has an advantage if comparisons are expensive.) ...|$|E
2500|$|The ARM7 {{and earlier}} {{implementations}} have a three-stage pipeline; the stages being fetch, decode and execute. Higher-performance designs, {{such as the}} ARM9, have deeper pipelines: Cortex-A8 has thirteen stages. Additional implementation changes for higher performance include a faster adder and more extensive <b>branch</b> <b>prediction</b> logic. The difference between the ARM7DI and ARM7DMI cores, for example, was an improved multiplier; hence the added [...] "M".|$|E
2500|$|PPE {{consists}} of three main units: Instruction Unit (IU), Execution Unit (XU) and vector/scalar execution unit (VSU). IU contains L1 instruction cache, <b>branch</b> <b>prediction</b> hardware, instruction buffers and dependency checking login. XU contains integer execution units (FXU) and load-store unit (LSU). VSU contains all of the execution resources for FPU and VMX. [...] Each PPE can complete two double precision operations per clock cycle using a scalar fused-multiply-add instruction, which translates to 6.4GFLOPS at 3.2GHz; or eight single precision operations per clock cycle with a vector fused-multiply-add instruction, which translates to 25.6GFLOPS at 3.2GHz.|$|E
40|$|This paper {{compares the}} {{performance}} {{characteristics of the}} Alpha 21164 to the previous-generation 21064 microprocessor. Measurements on the 21164 -based AlphaServer 8200 system are compared to the 21064 based DEC 7000 server using several commercial and technical workloads. The data analyzed includes cycles per instruction, multiple-issued instructions, <b>branch</b> <b>predictions,</b> stall components, cache misses, and instruction frequencies. The AlphaServer 8200 provides 2 to 3 times {{the performance of the}} DEC 7000 server based on the faster clock, larger on-chip cache, expanded multiple-issuing, and lower cache/memory latencies and higher bandwidth...|$|R
5000|$|The ISA manual {{recommends}} that software be optimized to avoid branch stalls {{by using the}} default <b>branch</b> <b>predictions.</b> This reuses the most significant bit of the signed relative address as a [...] "hint bit" [...] to tell whether the conditional branch will be taken or not. So, no other hint bits are needed in the operation codes of RISC-V branches. This makes more bits available in the branch operation codes. Simple, inexpensive CPUs can merely follow the default predictions and still perform well with optimizing compilers. Compilers can still perform statistical path optimization, if desired.|$|R
40|$|Many high {{performance}} processors predict condi-tional branches and consume processor resources {{based on the}} prediction. In some situations, resource allocation can be better optimized if a confidence level is assigned to a branch prediction; i. e. if the quantity of resources allo-cated {{is a function of}} the confidence level. To support such optimizations, we consider hardware mechanisms that partition conditional <b>branch</b> <b>predictions</b> into two sets: those which are accurate a relatively high percen-tage of the time, and those which are accurate a relatively low percentage of the time. The objective is to concen-trate as many of the mispredictions as practical into a relatively small set of low confidence dynamic branches. We first study an ideal method that pro$iles <b>branch</b> <b>predictions</b> and sorts static branches into high and low confidence sets, depending on the accuracy with which they are dynamically predicted. We find that about 63 percent of the mispredictions can be localized to a set of static branches that account for 20 percent of the dynamic branches. We then study idealized dynamic confidence methods using both one and two levels of branch correct-ness history. We find that the single level method per-forms at least as well as the more complex two level method and is able to isolate 89 percent of the mispredic-tions into a set containing 20 percent of the dynamic branches. Finally, we study practical, less expensive implementations and find that they achieve most of the performance of the idealized methods. 1...|$|R
2500|$|The P5 {{microarchitecture}} {{was designed}} by the same Santa Clara team which designed the 386 and 486. [...] Design work started in 1989; [...] the team decided to use a superscalar architecture, with on-chip cache, floating-point, and <b>branch</b> <b>prediction.</b> [...] The preliminary design was first successfully simulated in 1990, followed by the laying-out of the design. [...] By this time, the team had several dozen engineers. [...] The design was taped out, or transferred to silicon, in April 1992, at which point beta-testing began. [...] By mid-1992, the P5 team had 200 engineers. [...] Intel at first planned to demonstrate the P5 in June 1992 at the trade show PC Expo, and to formally announce the processor in September 1992, but design problems forced the demo to be cancelled, and the official introduction of the chip was delayed until the spring of 1993.|$|E
2500|$|Each SPE is a dual {{issue in}} order {{processor}} {{composed of a}} [...] "Synergistic Processing Unit", SPU, and a [...] "Memory Flow Controller", MFC (DMA, MMU, and bus interface). SPEs don't have any <b>branch</b> <b>prediction</b> hardware (hence there is a heavy burden on the compiler). Each SPE has 6 execution units divided among odd and even pipelines on each SPE : The SPU runs a specially developed instruction set (ISA) with 128-bit SIMD organization for single and double precision instructions. With {{the current generation of}} the Cell, each SPE contains a 256KiB embedded SRAM for instruction and data, called [...] "Local Storage" [...] (not to be mistaken for [...] "Local Memory" [...] in Sony's documents that refer to the VRAM) which is visible to the PPE and can be addressed directly by software. Each SPE can support up to 4 GiB of local store memory. The local store does not operate like a conventional CPU cache since it is neither transparent to software nor does it contain hardware structures that predict which data to load. The SPEs contain a 128-bit, 128-entry register file and measures 14.5mm2 on a 90nm process. An SPE can operate on sixteen 8-bit integers, eight 16-bit integers, four 32-bit integers, or four single-precision floating-point numbers in a single clock cycle, as well as a memory operation. Note that the SPU cannot directly access system memory; the 64-bit virtual memory addresses formed by the SPU must be passed from the SPU to the SPE memory flow controller (MFC) to set up a DMA operation within the system address space.|$|E
50|$|<b>Branch</b> <b>prediction</b> {{which is}} used to avoid {{stalling}} for control dependencies to be resolved. <b>Branch</b> <b>prediction</b> is used with speculative execution.|$|E
40|$|A {{processor}} executes {{the full}} dynamic instruction stream {{in order to}} compute the final output of a program, yet we observe equivalent, smaller instruction streams that produce the same cor-rect output. Based on this observation, we attempt to identify large, dynamically-contiguous regions of instructions that are ineffectual as a whole: they either contain no writes, writes that are never referenced, or writes that do not modify {{the value of a}} location. The architectural impli-cation is that instruction fetch/execution can quickly bypass predicted-ineffectual regions, while another thread of control verifies that the implied <b>branch</b> <b>predictions</b> in the region are correct and that the region is truly ineffectual. 1...|$|R
5000|$|RISC-V's ISA {{requires}} default <b>branch</b> <b>predictions</b> for CPUs: Backward conditional branches {{should be}} predicted [...] "taken." [...] Forward conditional branches predict [...] "not taken." [...] The predictions {{are easy to}} decode in a pipelined CPU: Branch addresses are signed numbers added to the PC. Backward branches have negative two's complement addresses, and therefore have a one in the most significant bit of the address. Forward branches have a zero. The most significant bit is in a fixed location in the operation code in order {{to speed up the}} pipeline. Complex CPUs can add branch predictors to work well even with unusual data or situations.|$|R
40|$|Continuing recent {{architectural}} {{trends that}} rely on <b>branch</b> <b>predictions,</b> value predictions, speculative execution, and reuse of results from instruction execution, we investigate the reuse of results from previous function invocations. In this paper we show that for integer benchmarks, {{it is possible to}} eliminate some function invocations since the same function is executed repeatedly with the same arguments. We feel that along with compiler techniques such as function cloning and partial evaluations, dynamic, hardware based techniques to check if a function should be invoked or the results from a prior execution can be used will lead to dramatic performance gains. We use HP Alpha based instrumentation tool called ATOM in our experiments on SPEC 2000 integer benchmarks...|$|R
50|$|<b>Branch</b> <b>prediction</b> is not {{the same}} as branch target prediction. <b>Branch</b> <b>prediction</b> {{attempts}} to guess whether a conditional jump will be taken or not. Branch target prediction attempts to guess the target of a taken conditional or unconditional jump before it is computed by decoding and executing the instruction itself. <b>Branch</b> <b>prediction</b> and branch target prediction are often combined into the same circuitry.|$|E
50|$|The {{problem with}} {{software}} <b>branch</b> <b>prediction</b> {{is that it}} requires a complex software development process. To run any software, hardware Branch predictors moved the statistics into the electronics. Branch predictors are parts of a processor that guess {{the outcome of a}} conditional branch. Then the processor's logic gambles on the guess by beginning to execute the expected instruction flow. An example of a simple hardware <b>branch</b> <b>prediction</b> scheme is to assume that all backward branches (i.e. to a smaller program counter) are taken (because {{they are part of a}} loop), and all forward branches (to a larger program counter) are not taken (because they leave a loop). Better branch predictors are developed and validated statistically by running them in simulation on a variety of test programs. Good predictors usually count the outcomes of previous executions of a branch. Faster, more expensive computers can then run faster by investing in better <b>branch</b> <b>prediction</b> electronics. In a CPU with hardware <b>branch</b> <b>prediction,</b> branch hints let the compiler's presumably superior <b>branch</b> <b>prediction</b> override the hardware's more simplistic <b>branch</b> <b>prediction.</b>|$|E
50|$|The Burroughs B4900, a microprogrammed COBOL machine {{released}} around 1982, was pipelined {{and used}} <b>branch</b> <b>prediction.</b> The B4900 <b>branch</b> <b>prediction</b> history state is stored {{back into the}} in-memory instructions during program execution. The B4900 implements 4-state <b>branch</b> <b>prediction</b> by using 4 semantically equivalent branch opcodes to represent each branch operator type. The opcode used indicated the history of that particular branch instruction. If the hardware determines that the <b>branch</b> <b>prediction</b> state of a particular branch needs to be updated, it rewrites the opcode with the semantically equivalent opcode that hinted the proper history. This scheme obtains a 93% hit rate. US patent 4,435,756 and others were granted on this scheme.|$|E
40|$|In superscalar computers, average branch {{instruction}} latency {{is crucial to}} the overall performance of the computer. Among all the branch predictors that have been proposed, neural branch predictors tend to have a better performance than traditional branch predictors, especially on benchmarks with long branch history correlation. However, huge latency of training process and power overhead makes it impossible to integrate neural branch predictors with modern computer chips. On April 30, 2008, HP Labs announced the development of switching memristor. The electrical property of memristor makes it a promising candidate of building fast neural branch predictors. We propose architectural algorithm enhancement as well as circuit level design with memristors to overcome the shortcomings of existing neural <b>branch</b> <b>predictions...</b>|$|R
40|$|Redundant {{threading}} architectures duplicate all {{instructions to}} detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor single-thread performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident <b>branch</b> <b>predictions</b> as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication {{with the performance of}} single-thread execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confiden...|$|R
40|$|Instruction Level Parallelism (ILP) {{in modern}} Superscalar and VLIW {{processors}} is achieved using out-of-order execution, <b>branch</b> <b>predictions,</b> value predictions, and speculative executions of instructions. These techniques are not scalable. This {{has led to}} multithreading and multi-core systems. However, such processors require compilers to automatically extract thread level or task level parallelism. Loop carried dependencies and aliases caused by complex array subscripts and pointer data types limit compilers’ ability to parallelize code. Hardware support for threadlevel speculation (TLS) allows compilers to more aggressively parallelize programs using speculative thread execution, since hardware will enforce correct order of execution. In this paper, we show how thread-level speculation can be implemented {{within the context of}} our Scheduled Dataflow architecture and provide preliminary performance analysis. ...|$|R
50|$|Some {{processors}} allow <b>branch</b> <b>prediction</b> hints to {{be inserted}} into the code to tell whether the static prediction should be taken or not taken. The Intel Pentium 4 accepts <b>branch</b> <b>prediction</b> hints while this feature is abandoned in later processors.|$|E
50|$|Microprogrammed processors, popular {{from the}} 1960s to the 1980s and beyond, took {{multiple}} cycles per instruction, and generally {{did not require}} <b>branch</b> <b>prediction.</b> However, {{in addition to the}} IBM 3090, there are several other examples of microprogrammed designs that incorporated <b>branch</b> <b>prediction.</b>|$|E
50|$|Scott McFarling {{proposed}} combined <b>branch</b> <b>prediction</b> in his 1993paper.|$|E
40|$|The {{achievement}} of fast, precise interrupts and {{the implementation of}} multiple levels of <b>branch</b> <b>predictions</b> {{are two of the}} problems associated with the dynamic scheduling of instructions for superscalar processors. Their solution is especially difficult if short cycle time operation is desired. We present solutions to these problems through the development of the Fast Dispatch Stack (FDS) system. We show that the FDS is capable of scheduling storage, branch, and register-to-register instructions for concurrent and out-of-order executions; the FDS implements fast and precise interrupts in a natural, efficient way; and it facilitates speculative execution [...] Instructions preceding and following one or more predicted conditional branch instructions may issue. When necessary, their effects are undone in one machine cycle. We evaluated the FDS system with extensive simulations. 1...|$|R
40|$|To improve {{communication}} efficiency companies nowadays {{start making}} their business services available electronically over the Internet. Examples {{can be found}} in the supply-chain management, finance relations and other sectors mainly for Business-toBusiness (B 2 B) interaction. The communication of electronic services follows specific interaction patterns that can be described through processes. A process description may be created manually or can be derived through process mining techniques from execution logs. The focus of this work is the automatic generation of branching conditions from statistical evaluations during the process mining. Our approach is to apply fuzzy logic [Zim 91, Cox 92, SK 92] instead of crisp conditions to express uncertainty. The anticipated branching conditions are based on selected state attributes. The results are fuzzy rules suitable for a flexible description of branching conditions that allow for dynamic <b>branching</b> <b>predictions...</b>|$|R
40|$|Static compilers use {{profiling}} {{to predict}} run-time program behavior. Generally, this requires multiple input sets to capture wide variations in run-time behavior. This is expensive {{in terms of}} resources and compilation time. We introduce a new mechanism, 2 D-profiling, which profiles with only one input set and predicts whether {{the result of the}} profile would change significantly across multiple input sets. We use 2 D-profiling to predict whether a <b>branch’s</b> <b>prediction</b> accuracy varies across input sets. The key insight is that if the prediction accuracy of an individual branch varies significantly over a profiling run with one input set, then {{it is more likely that}} the prediction accuracy of that branch varies across input sets. We evaluate 2 D-profiling with the SPEC CPU 2000 integer benchmarks and show that it can identify input-dependent branches accurately. 1...|$|R
5000|$|<b>Branch</b> <b>prediction</b> {{analysis}} attacks - on RSA {{public-key cryptography}} ...|$|E
50|$|The {{trade-off}} between fast <b>branch</b> <b>prediction</b> {{and good}} <b>branch</b> <b>prediction</b> is sometimes {{dealt with by}} having two branch predictors. The first branch predictor is fast and simple. The second branch predictor, which is slower, more complicated, and with bigger tables, will override a possibly wrong prediction made by the first predictor.|$|E
50|$|The 740 and 750 added dynamic <b>branch</b> <b>prediction</b> and a 64-entry branch target {{instruction}} cache (BTIC). Dynamic <b>branch</b> <b>prediction</b> uses the recorded {{outcome of a}} branch stored in a 512-entry by 2-bit branch history table (BHT) to predict its outcome. The BTIC caches the first two instructions at a branch target.|$|E
3000|$|... for non-zero p, q and r, are {{predicted}} to emanate from the double-cluster state <b>branches.</b> Translating this <b>prediction</b> {{to the network}} of ten HH neurons, the [...]...|$|R
40|$|A value's {{degree of}} use [...] -the number of dynamic uses of that value [...] -provides the most {{essential}} information needed to optimize its communication. We present simulation results demonstrating the properties of degree of use of values, including their predictability: most static instructions generate values with few degrees of use and these exhibit temporal locality. We use these results to guide {{the design of a}} degree of use predictor. The development and detailed characterization of this predictor is the focus of this paper. Our predictor leverages future control flow information (e. g., <b>branch</b> <b>predictions)</b> to select among different possible degrees of use. We study the effects of several optimizations and variations in the predictor's algorithms to tune the predictor for maximum performance. The resulting predictor generates correct degree of use predictions for over 92 % of all dynamic values and has a misprediction rate below 2. 5 %. Such a predictor has a wide range of potential applications in optimizing value communication...|$|R
40|$|In this paper, {{we present}} a Branch Target Buffer (BTB) design for energy savings in set-associative {{instruction}} caches. We extend the functionality of a BTB by caching way predictions in addition to branch target addresses. Way <b>prediction</b> and <b>branch</b> target <b>prediction</b> are done in parallel. Instruction cache energy savings are achieved by accessing one cache way if the way prediction for a fetch is available...|$|R
