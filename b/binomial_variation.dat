9|5|Public
3000|$|... 2 test {{is applied}} to check whether these rates vary {{according}} to <b>binomial</b> <b>variation</b> under the hypothesis of the same true spontaneous rate for all experiments. A significant result signals the presence of biological variation between experiments. If present, its standard deviation is reported. It is recommended to determine the spontaneous lethality very early under separate test conditions.|$|E
40|$|Arbitrary order forced difference-delay {{systems are}} considered, from which {{generated}} infinite sums of <b>Binomial</b> <b>variation</b> may {{be represented in}} closed form. It will be proved that the infinite sums may be expressed in a closed form which depends on dominant zeros of the polynomial characteristic functions. A connection between the infinite sum and generalised hypergeometric series will also be demonstrated...|$|E
40|$|The {{method of}} {{generalized}} estimating equations {{has become almost}} standard for analysing longitudinal and other correlated response data. However, {{we have found that}} if binary responses have less than <b>binomial</b> <b>variation</b> over clusters, and are modelled using exchangeable correlations, prevailing software implementa-tions may give unreliable results. Bounding the negative correlation away from its theoretical minimum may not always be a satisfactory solution. In such instances, using the independence working correlation structure and robust SEs is a more trustworthy alternative. Copyright (2000 John Wiley & Sons, Ltd. 1...|$|E
40|$|We prove two {{new forms}} of Jacobi-type J-fraction {{expansions}} generating the binomial coefficients, x+nn and xn, over all n ≥ 0. Within the article we establish {{new forms of}} integer congruences for these <b>binomial</b> coefficient <b>variations</b> modulo any (prime or composite) h ≥ 2 and compare our results with existing known congruences for the binomial coefficients modulo primes p and prime powers p^k. We also prove new exact formulas for these binomial coefficient cases from the expansions of the h^th convergent functions to the infinite J-fraction series generating these coefficients for all n...|$|R
40|$|Upper {{and lower}} bounds are {{given for the}} total {{variation}} distance between the distribution of a sum S of n independent, non-identically distributed 0 - 1 random variables and the binomial distribution (n, p) having the same expectation as S. The proof uses the Stein [...] Chen technique. Equivalence of the total variation and the Kolmogorov distance is established, and an application to sampling with and without replacement is presented. Poisson binomial distribution <b>binomial</b> approximation total <b>variation</b> hypergeometric distribution...|$|R
40|$|We discuss {{problems}} {{posed by}} the quantitative study of time inhomogeneous Markov chains. The two main notions for our purpose are merging and stability. Merging (also called weak ergodicity) occurs when the chain asymptotically forgets where it started. It is a loss of memory property. Stability relates {{to the question of}} whether or not, despite temporary variations, there is a rough shape describing the long time behavior of the chain. For instance, we will discuss an example where the long time behavior is roughly described by a <b>binomial,</b> with temporal <b>variations...</b>|$|R
40|$|Many count {{statistics}} {{are used to}} evaluate pitchers such {{as the number of}} wins and losses, the number of strikeouts, the number of walks, and the number of runs allowed. For a given measure such as strikeouts, this paper focuses on the estimation of pitchers probabilities of striking out a batter. The variation in the season strikeout rates among a group of pitchers is due to differences in the pitchers probabilities and also due to chance <b>binomial</b> <b>variation.</b> Among all the various rates, we find that a strikeout rate {{is one of the most}} accurate estimates of the corresponding probability of a pitcher performing the associated task. We examine the distribution of strikeout, walk and runs-prevented true rates of pitchers across the years. By use of our model, we are able to judge the magnitude of a great strikeout season. A z-score statistic is used to rank the greatest strikeout seasons of baseball history and this ranking is contrasted with other traditional ways of ranking pitchers. ...|$|E
40|$|The {{powerful}} general Pacala^Hassell host^parasitoid {{model for}} a patchy environment, which allows host density-dependent heterogeneity (HDD) to be distinguished from between-patch, host density-independent heterogeneity (HDI), is reformulated within the class of the generalized linear model (GLM) family. This improves accessibility through the provision of general software within well-known statistical systems, and allows a rich variety of models to be formulated. Covariates such as age class, host density and abiotic factors may be included easily. For the case {{where there is no}} HDI, the formu-lation is a simple GLM. When there is HDI in addition to HDD, the formulation is a hierarchical generalized linear model. Two forms of HDI model are considered, both with between-patch variability: one has <b>binomial</b> <b>variation</b> within patches and one has extra-binomial, overdispersed variation within patches. Examples are given demonstrating parameter estimation with standard errors, and hypothesis testing. For one example given, the extra-binomial component of the HDI heterogeneity in parasitism is itself shown to be strongly density dependent...|$|E
40|$|Many {{clinical}} trials are analysed using an intention-to-treat (ITT) approach. A full {{application of the}} ITT approach is only possible when complete outcome data are available for all randomized subjects. In {{a recent survey of}} clinical trial reports including an ITT analysis, complete case analysis (excluding all patients with a missing response) was common. This does not comply with the basic principles of ITT since not all randomized subjects are included in the analysis. Analyses of data with missing values are based on untestable assumptions, and so sensitivity analysis presenting a range of estimates under alternative assumptions about the missing-data mechanism is recommended. For binary outcome, extreme case analysis has been suggested as a simple form of sensitivity analysis, but this is rarely conclusive. A graphical sensitivity analysis is proposed which displays the results of all possible allocations of cases with missing binary outcome. Extension to allow <b>binomial</b> <b>variation</b> in outcome is also considered. The display is based on easily interpretable parameters and allows informal examination of the effects of varying prior beliefs...|$|E
40|$|The charged {{particle}} multiplicity distribution is studied for non-single-diffractive ß + p collisions at p s = 22 GeV, in central rapidity intervals {{separated from the}} outer regions by empty gaps. A priori unexpectedly, also these distributions yield good negative <b>binomial</b> fits. The <b>variation</b> of the parameter 1 =k {{as a function of}} the gap size can be understood in terms of cascading clans of limited rapidity range if events with a large number of clans preferably contribute to the central region. The Fritiof- 3 model agrees quantitatively with the data. 1 Onderzoeker IIKW, Brussels, Belgium 2 Bevoegdverklaard Navorser NFWO, Belgium 3 Now with Ericsson Telecommunicatie B. V., Rijen, The Netherlands 4 On leave from Institute of Nuclear Physics, Krakow 5 Now at CERN, Geneva, Switzerland 6 Partially supported by grants from CPBP 01. 06 and 01. 09 7 Partially funded by the German Federal Minister for Research and Technology (BMFT) under the contract number 053 AC 41 P In a multip [...] ...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2017 - 03 The "Influence of Hospital Resource Factors on Adverse Health Outcomes" {{covers the}} exploratory, {{observational}} study {{that aims to}} determine the association between resource accessibility variables and adverse health outcomes in a hospital setting. Past {{studies have examined the}} association between resource accessibility and outcomes, and have focused on the patient populations from a single hospital admission source, such as the emergency or surgical departments. These studies have employed various statistical methods-–such as logistic regression, weighted least squares regression, and Cox proportional hazard modeling-–to determine associations between resource accessibility variables and health outcome variables. To date, a more holistic approach that accounts for multiple resource accessibility (number of beds available, number of staff available, and daily patient volume) as well as outcomes of readmission and mortality have not been studied extensively across different patient populations. As differences can be observed given patient age, admission sources, and existing health conditions, these patient characteristics are important to consider and model. Few studies have also accounted for the hierarchical data structures of the unit-patient relationship, and explored how patient to staffing ratio affects the hierarchical data structures. This research study aims to identify the influence of controllable hospital resource factors on two (2) adverse health outcomes: 1) 30 -day readmission and 2) in-hospital mortality. The following research aims will be addressed: Aim 1 : Determine the patient risk factors that influence the risk of each adverse health outcome. Aim 2 : Determine the hospital resource factors that influence the risk of each adverse health outcome. Aim 3 : Determine if the risk-adjusted association between hospital resources and adverse health outcomes (as defined in Aim 2) differ by nursing unit. If so, determine if demand:supply ratio measures, including bed occupancy and staffing ratios, contribute to the unit-to-unit <b>variation.</b> <b>Binomial</b> & LASSO logistic regression methods are used to relate explanatory variables to patient health outcomes. Resource accessibility (explanatory) variables include nurse staffing volume, bed availability, admission volume, discharge volume, day and time of admission, and day and time of discharge. The patient health outcome (response) variables are 30 -day readmission and in-hospital mortality rates. The model accounts for patient characteristics that include age, severity of illness, risk of mortality, and admission source. The hospital setting used for this study was the University of Washington Medical Center. Such research can help hospital decision/policy makers determine resource capacity and healthcare delivery needs in order to mitigate the occurrence of adverse patient outcomes...|$|R
40|$|In {{the context}} of item {{response}} theory, {{it is not uncommon}} that person-by-item data are correlated beyond the correlation that is captured by the model—in other words, there is extra <b>binomial</b> <b>variation.</b> Heterogeneity of the parameters can explain this variation. There is a need for proper statistical methods to indicate possible extra heterogeneity and its location because investigating all different combinations of random parameters is not practical and sometimes even unfeasible. The ignored random person effects are the focus of this study. Considering the random weights linear logistic test model, random effects can occur as a general latent trait and as weights of covariates. A simulation study was conducted with different sources and degrees of heterogeneity to investigate and compare various methods: individual analyses (one per person), marginal modeling, principal component analysis of the raw data, DIMTEST, and DETECT. The methods are illustrated with an application on deductive reasoning. Index terms: heterogeneity, binary data, covariates, PCA, marginal modeling, DIMTEST, DETECT Test data are often of a binary type and may be considered as repeated measures because differ-ent items are presented to the same persons. The focus of this article is on binary repeated mea-sures with a design. A test has a design when the items are carefully controlled, selected, and counterbalanced based on a number of design factors (Embretson, 1985). Test design became a popular approach in cognitive psychology, for example, for the study of intelligence (Sternberg...|$|E
40|$|Inadequacy of the Poisson assumption, {{due to the}} {{presence}} of overdispersion, in analysing count data has been reported by several authors (see McCaughran and Arnold (1976), Bliss and Owen (1958) etc.). Negative binomial distribution has been widely used to incorporate overdispersion in analysing the count data. Several test statistics for detecting negative <b>binomial</b> <b>variation</b> have been presented-C(α) tests, range-justified tests (appealing to the nonnegativity of the dispersion parameter) are compared with the static presented by Collings and Margolin (1985). One-way layout of data in the form of counts is often reported as a result of laboratory experiment or field work. Assuming the underlying distribution for the groups to be negative binomial with common dispersion parameter, two C(α) tests are developed for comparing the means of the groups. Their performance is compared in terms of level and power with the likelihood ratio test and test based on variance establishing transformation (Anscombe (1948)). A test for checking the validity of assumption of common dispersion is also developed. In several situations the assumption of a common dispersion parameter might not be tenable. A C(α) test is derived for comparing the means of negative binomial distributions with unequal dispersion parameters. For two groups, this test is compared with the Welch 2 ̆ 7 s approximate degree of freedom formula and Banerji 2 ̆ 7 s procedure (1960) for empirical level and power. Methods for testing {{the presence}} of an outlier in data coming from a population following Poisson distribution have also been derived. In deriving the C(α) test statistics for the above problems, the method presented by Neyman (1959) has been presented under a more general setting, which covers many situations concerning inferences on several parameters in presence of nuisance parameters. Dept. of Mathematics and Statistics. Paper copy at Leddy Library: Theses 2 ̆ 6 Major Papers - Basement, West Bldg. / Call Number: Thesis 1990. B 276. Source: Dissertation Abstracts International, Volume: 52 - 11, Section: B, page: 5913. Supervisor: S. R. Paul. Thesis (Ph. D.) [...] University of Windsor (Canada), 1989...|$|E
40|$|Includes bibliographical references. A dissimilarity {{coefficient}} for estimating the dissimilarity between two bird atlas distributions is developed. This coefficient {{is based on}} the Euclidean distance concept. The atlas distributions are compared over all quarter degree grid cells. Existing coefficients are not suitable for the comparison of distributions with different total areas and species with different mean reporting rates. In each grid cell the reliability of the reporting rates depends on the number of checklists collected for the grid cell. Weights are used to solve this problem. To solve the problem of different levels of abundance and conspicuousness of species, the reporting rates are sorted into percentiles, using five or 10 categories for the strictly positive reporting rates. Each grid cell is weighted by a function of the number of checklists collected for the grid cell. The coefficient is scaled by the maximum possible sum of the differences which would occur if there is no overlap between the two distributions, so that the dissimilarity coefficient lies between zero (a perfect match) and one (no overlap). A variety of these coefficients are investigated and compared. The continuity of observed reporting rates in a spatial cellular map is an indication of spatial autocorrelation present, especially between observations which are in close vicinity. We are particularly interested in measuring and comparing the continuity of the reporting rates in the bird distributions from The Atlas of Southern African Birds. The variogram, developed in geostatistics, estimates this spatial autocorrelation. The classical variogram estimator, however, is dependent on the scale of measurement and assumes that the data are intrinsically stationary. The bird atlas distribution maps contain trend and the variance of each observation (reporting rate) {{is a function of the}} number of checklists collected for the grid cell and the underlying probability of encountering the species in the grid cell. The approach of removing this binomial measurement error from the variogram developed by McNeill (1991) is investigated but not found satisfactory. A weighted variogram, where each squared difference is weighted by a function of the smaller number of checklists, is developed. To make the variogram values comparable between species a function of the mean reporting rates is used to scale the variogram. We were particularly interested in the first variogram value of each species distribution, 2 y(1). The bird distribution maps in The Atlas of Southern African Birds show the raw observed reporting rates. Each of these reporting rates is a random variable dependent on sampling error due to <b>binomial</b> <b>variation</b> based on the number of checklists collected for the grid cell and on the underlying probability of encountering the species. The distribution maps show this measurement error. It is believed that a smoothed version of the bird distribution maps will to some extent improve the statement these observed distributions are aiming to make. Single-step regression methods are investigated for a fast approach to this problem. These cause problems because of frequent 'zero' observed reporting rates and because they smooth the maps too heavily. Generalized Linear Models are investigated and this iterative procedure is applied to model the reporting rates with a binomial distribution on square blocks of nine grid cells where a value for the central cell is 'predicted' in each regression. This approach is especially suited to accommodate the binomial distribution characteristics and is found to smooth the bird atlas distributions well. Because only a local window is taken for each regression, the spatial autocorrelation is adequately included in the spatial explanatory variables...|$|E

