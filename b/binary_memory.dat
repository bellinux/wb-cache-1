29|116|Public
5000|$|Since each <b>binary</b> <b>memory</b> element, {{such as a}} flip-flop, {{has only}} two {{possible}} states, [...] "one" [...] or [...] "zero", {{and there is a}} finite number of memory elements, a digital circuit has only a certain finite number of possible states. If N is the number of <b>binary</b> <b>memory</b> elements in the circuit, the maximum number of states a circuit can have is 2N.|$|E
50|$|The {{output of}} a {{sequential}} circuit or computer program {{at any time}} is completely determined by its current inputs and current state. Since each <b>binary</b> <b>memory</b> element has only two possible states, 0 or 1, {{the total number of}} different states a circuit can assume is finite, and fixed by the number of memory elements. If there are N <b>binary</b> <b>memory</b> elements, a digital circuit can have at most 2N distinct states. The concept of state is formalized in an abstract mathematical model of computation called a finite state machine, used to design both sequential digital circuits and computer programs.|$|E
5000|$|The JEDEC DDR3 SDRAM {{standard}} JESD-79-3d uses Mb and Gb {{to specify}} <b>binary</b> <b>memory</b> capacity: [...] "The {{purpose of this}} Standard is to define the minimum set of requirements for JEDEC compliant 512 Mb through 8 Gb for x4, x8, and x16 DDR3 SDRAM devices." ...|$|E
5000|$|It stores {{two or more}} bits of {{information}} per cell rather than just one, in an architecture called multi-level cell (MLC). This is accomplished by storing intermediate voltage levels instead of using only the two levels (discharged = [...] "0" [...] and charged = [...] "1") of traditional <b>binary</b> <b>memories.</b> The StrataFlash technology evolved out of Intel's ETOX flash memory products. Two bits per cell are achieved with four levels of voltage, while three bits per cell can be achieved with eight levels.|$|R
5000|$|... #Subtitle level 2: Mapping multi-byte <b>binary</b> {{values to}} <b>memory</b> ...|$|R
40|$|We {{present an}} {{algorithm}} to store <b>binary</b> <b>memories</b> in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models. In {{the case of}} memories without noise, our algorithm provably achieves optimal pattern storage (which we show {{is at least one}} pattern per neuron) and outperforms classical methods both in speed and memory recovery. Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals. We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples. Comment: 6 pages, 4 figures, 2012 Neural Information Processing Systems (NIPS) workshop on Discrete Optimization in Machine Learning (DISCML...|$|R
50|$|The {{memory cell}} is the {{fundamental}} building block of memory. It can be implemented using different technologies, such as bipolar, MOS, and other semiconductor devices. It can also be built from magnetic material such as ferrite cores or magnetic bubbles. Regardless of the implementation technology used, {{the purpose of the}} <b>binary</b> <b>memory</b> cell is always the same. It stores one bit of binary information and it must be set to store a 1 and reset to store a 0.|$|E
50|$|The {{underlying}} {{idea behind}} a SDM is the mapping {{of a huge}} <b>binary</b> <b>memory</b> onto a smaller set of physical locations, so-called hard locations. As a general guideline, those hard locations should be uniformly distributed in the virtual space, to mimic {{the existence of the}} larger virtual space as accurately as possible. Every datum is stored distributed by a set of hard locations, and retrieved by averaging those locations. Therefore, recall may not be perfect, accuracy depending on the saturation of the memory.|$|E
40|$|The {{performance}} of a coding system consisting of a convolutional encoder and a Viterbi decoder is analytically found by the well-known transfer function bounding technique. For the partial-unit-memory byte-oriented convolutional encoder with m sub 0 <b>binary</b> <b>memory</b> cells and (k sub 0 m sub 0) inputs, a state diagram of 2 (K) (sub 0) was for the transfer function bound. A reduced state diagram of (2 (m sub 0) + 1) is used for easy evaluation of transfer function bounds for partial-unit-memory codes...|$|E
40|$|This paper {{introduces}} an Enhanced Boolean {{version of}} the Correlation Matrix Memory (CMM), which is useful to work with <b>binary</b> <b>memories.</b> A novel Boolean Orthonormalization Process (BOP) is presented to convert a non-orthonormal Boolean basis, i. e., a set of non-orthonormal binary vectors (in a Boolean sense) to an orthonormal Boolean basis, i. e., a set of orthonormal binary vectors (in a Boolean sense). This work shows {{that it is possible}} to improve the performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm has a lot of additional fields of applications, e. g. : Steganography, Hopfield Networks, Bi-level image processing, etc. Finally, it is important to mention that the BOP is an extremely stable and fast algorithm. Comment: 4 pages, 4 figure...|$|R
40|$|The Little-Hopfield {{network is}} an auto-associative {{computational}} model of neural memory storage and retrieval. This model {{is known to}} robustly store collections of randomly generated binary patterns as stable-states of the network dynamics. However, the number of <b>binary</b> <b>memories</b> so storable scales linearly {{in the number of}} neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model. In this note, we design simple families of Little-Hopfield networks that provably solve this problem affirmatively. As a byproduct, we produce a set of novel (nonlinear) binary codes with an efficient, highly parallelizable denoising mechanism. Comment: This paper has been withdrawn by the authors. preliminary early draft unsuitable for viewing and attribution, instead, see: arXiv: 1411. 462...|$|R
40|$|Abstract—The {{generalized}} write-once memory {{introduced by}} Fiat and Shamir is a q-ary information storage medium. Each storage cell {{is expected to}} store one of q symbols, and the legal state transitions are described by an arbitrary directed acyclic graph. This memory model {{can be understood as}} a generalization of the <b>binary</b> write-once <b>memory</b> which was introduced by Rivest and Shamir. During the process of updating information, the contents of a cell can be changed from a 0 -state to a 1 -state but not vice versa. We study the problem of reusing a generalized write-once memory for T successive cycles (generations). We determine the zero-error capacity region and the maximum total number of information bits stored in the memory for T consecutive cycles for the situation where the encoder knows and the decoder does not know the previous state of the memory. These results extend the results of Wolf, Wyner, Ziv, and Körner for the <b>binary</b> write-once <b>memory.</b> Index Terms—Capacity, directed acyclic graph, information, WOM-codes, write-once memory...|$|R
40|$|Abstract. Arrays {{exist in}} many nontrivial programs. Array {{operations}} can cause subtle information leaks. This paper allows array as first-class value and regards discretional array as array of array by alias array. Arrays are given types {{of the form}} 1 alias τ τ, where 1 τ is the security class of the array and 2 τ is the security class of the array's alias. To distinguish array from its alias, we propose a novel <b>binary</b> <b>memory</b> model [] 1 2;μ μ. The soundness of our type system is proved by noninterference property...|$|E
40|$|We {{present a}} pulsing {{protocol}} that significantly increases the endurance of a titanium-manganite interface {{used as a}} <b>binary</b> <b>memory</b> cell. The core of this protocol is an algorithm that searches for the proper values for the set and reset pulses, canceling the drift in the resistance values. A set of experiments show the drift-free operation for more than $ 10 ^{ 5 }$ switching cycles, {{as well as the}} detrimental effect by changing the amplitude of pulses indicated by the protocol. We reproduced the results with a numerical model, which provides information on the dynamics of the oxygen vacancies during the switching cycles. Comment: 10 pages, 4 figures. To be published in Applied Physics Letter...|$|E
40|$|Abstract—This work {{is aimed}} at a novel program method that is {{assisted}} by light for capacitorless 1 T-DRAM based on parasitic bipolar junction transistor operation. Experimental results clearly show that {{a flash of light}} triggers a distinctive <b>binary</b> <b>memory</b> state in the capacitorless 1 T-DRAM. During the operation of the 1 T-DRAM, the gate voltage is sustained at a negative, constant value. The sensing margin is 54 μA and the hold state corre-sponding to the data retention time is retained over a few seconds. The proposed program method can therefore be considered as a promising candidate for future DRAM applications based on an optical interconnection system. Index Terms—Capacitorless 1 T-DRAM, DRAM, eDRAM, FD SOI, MOSFET, optical interconnection, optical memory...|$|E
40|$|The content {{analysis}} of noisy facsimile images is approached by {{the application of the}} Generalised Hough Transform (GHT). The GHT is a frequently-used algorithm for the location of arbitrary objects in images, but it is known to suffer from sensitivity to image clutter. This leads to large numbers of false positive identifications of objects. This thesis describes the implementation of the GHT using a <b>binary</b> associative <b>memory</b> neural network, and proposes the addition of feed-back as a mechanism for enforcing consistency constraints between the features identified in an image. It investigates the effect which the addition of feed-back to the architecture has on the performance of the algorithm. A model for the recognition process is developed which accounts for errors in sampling the image and in evidence accumulation. The performance of the GHT, implemented using the <b>binary</b> associative <b>memory,</b> is shown to agree with the model predictions. The recogniser is shown to be robust to low le [...] ...|$|R
40|$|In this {{tutorial}}, {{we try to}} give a tutorial {{overview of}} The Context Tree Weighting Method. We confine our discussion to <b>binary</b> bounded <b>memory</b> tree sources and describe a sequential universal data compression procedure, which achieves a desirable coding distribution for tree sources with unknown model and unknown parameters. Computational and storage complexity of the proposed procedure are both linear in the source sequence length. 1...|$|R
40|$|Magnetoresistive <b>binary</b> digital <b>memories</b> of {{proposed}} new type expected to feature high speed, nonvolatility, ability to withstand ionizing radiation, high density, and low power. In memory cell, magnetoresistive effect exploited more efficiently {{by use of}} ferromagnetic material to store datum and adjacent magnetoresistive material to sense datum for readout. Because relative change in sensed resistance between "zero" and "one" states greater, shorter sampling and readout access times achievable...|$|R
40|$|Neural {{networks}} used as content-addressable memories show unequaled retrieval {{and speed}} capabilities in problems srreh as vision and pattern recognition. We propose a new {{implementation of a}} VLSI fully interconnected neural network with only two <b>binary</b> <b>memory</b> points per synapse. The small area of single synaptic cells allows implementation of neural networks with hundreds of neurons. Classical learning algorithms like the Hebb’s rule show a poor storage capacity, especially in VLSI neural networks where {{the range of the}} synapse weights is limited by the number of memory points contained in each connectiorq we propose a new algorithm for programming a Hopfield neuraf network as a high-storage content-addressable memory. The storage capacity obtained with this algorithm is very promising for pattern recognition applications...|$|E
40|$|Abstrud-Hopfield’s neural {{networks}} show retrieval and speed capabili-ties {{that make them}} good candidates for content-addressable memories (CAM’s) in problems such as pattern recognition and optimization. This paper presents a new implementation of a VLSI fully interconnected neural network with only two <b>binary</b> <b>memory</b> points per synapse (the connection weights are restricted to three different values: + 1,O and- 1). The small area of single synaptic cells (about lo 4 pm’) allows the implementation of {{neural networks}} with more thut 500 neurons. Because of the poor storage capability of Hebb’s learning rule, especially in VLSI neural networks where {{the range of the}} synapse weights is limited by the number of memory points contained in each connection, a new algorithm is proposed for programming a Hopfield neural network as a high-storage capacity CAM. The results of the VLSI circuit programmed with this new algorithm are very promising for pattern recognition applications. I...|$|E
40|$|The sum-product {{algorithm}} (probability propagation) can be mapped directly into analog transistor circuits. These circuits enable {{the construction of}} analog-VLSI decoders for turbo codes, low-density parity-check codes, and similar codes. Patent application pending. This {{research was supported by}} the Swiss National Science Foundation under Grant 21 - 49619. 96. a Endora Tech AG, Gartenstrasse 120, CH- 4052 Basel, Switzerland. E-mail: haloeliger@access. ch b ISI / Electrical Eng., ETH Zentrum, CH- 8092 Zurich, Switzerland. E-mail: lustenbe@isi. ee. ethz. ch c ISI / Electrical Eng., ETH Zentrum, CH- 8092 Zurich, Switzerland. E-mail: helfenst@isi. ee. ethz. ch d Endora Tech AG, Gartenstrasse 120, CH- 4052 Basel, Switzerland. E-mail: ftarkoey@access. ch 1 Introduction Algebraic coding theory and digital VLSI have long been known to be a happy match: the common primitive digital circuits (<b>binary</b> <b>memory</b> cells and logic gates) are ideally suited for finite-field arithmetic. Such a match does no [...] ...|$|E
40|$|We {{present an}} {{algorithm}} to store <b>binary</b> <b>memories</b> in a Little-Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models. In {{the case of}} memories without noise, our algorithm provably achieves optimal pattern storage (which we show {{is at least one}} pattern per neuron) and outperforms classical methods both in speed and memory recovery. Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals. We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples. 1 Introduction. In 1982, motivated by neural modeling work of [1] and the Ising spin glass model from statistical physics [2], Hopfield introduced a method for the storage and retrieval of binary patterns in an auto-associative neural-network [3]. Even today, this model and its various extensions [4, 5] provide a plausible mechanism for memory formation in the brain. However, existing techniques for training Little-Hopfield networks suffer either from limited pattern capacity or excessiv...|$|R
50|$|Audio, video, subtitle, and {{ancillary}} streams are multiplexed into an MPEG {{transport stream}} and stored on media as <b>binary</b> files. Usually, <b>memory</b> cards and HDDs use the FAT file system, while optical discs employ UDF or ISO9660.|$|R
5000|$|Like {{other gas}} {{discharge}} lamps, the neon bulb has negative resistance; its voltage falls with increasing current after the bulb reaches its breakdown voltage. [...] Therefore, the bulb has hysteresis; its turn-off (extinction) voltage {{is lower than}} its turn-on (breakdown) voltage. [...] This allows it {{to be used as}} an active switching element. Neon bulbs were used to make relaxation oscillator circuits, [...] for low frequency applications such as flashing warning lights, stroboscopes tone generators in electronic organs, and as time bases and deflection oscillators in early cathode ray oscilloscopes. [...] Neon bulbs can also be bistable, and were even used to build digital logic circuits such as logic gates, flip-flop, <b>binary</b> <b>memories,</b> and digital counters. [...] At least some of these lamps had a glow concentrated into a small spot on the cathode, which made them unsuited to use as indicators. These were sometimes called [...] "circuit-component" [...] lamps, the other variety being indicators. A variant of the NE-2 type lamp, the NE-77, had three parallel wires (in a plane) instead of the usual two. It was also intended primarily to be a circuit component.|$|R
40|$|The paper {{compares the}} {{boundary}} surfaces {{with help of}} cross-sections in three projection planes, for the four changes of Chua’s circuit parameters. It is known that due to changing the parameters, the Chua’s circuit can be characterized {{in addition to a}} stable limit cycle also by one double scroll chaotic attractor, two single scroll chaotic attractors or other two stable limit cycles. Chua’s circuit can even start working as a <b>binary</b> <b>memory.</b> It is not known yet, how changes in parameters and conseqently in attractors in the circuit will affect the morphology of the boundary surface. The boundary surface separates the double scroll chaotic attractor from the stable limit cycle. In a variation of the parameters presented in this paper the boundary surface will separate even single scroll chaotic attractors from each other. Dividing the state space into regions of attractivity for different attractors, however, remains fundamentally the same...|$|E
40|$|We {{investigate}} the pattern completion performance of neural auto-associative memories composed of binary threshold neurons for sparsely coded <b>binary</b> <b>memory</b> patterns. Focussing on iterative retrieval, effective threshold control strategies are introduced. These are investigated {{by means of}} computer simulation experiments and analytical treatment. To evaluate the systems performance we consider the completion capacity C and the mean retrieval errors. The asymptotic completion capacity values for the recall of sparsely coded binary patterns in one-step retrieval {{is known to be}} ln 2 = 4 ß 17 : 32 % for binary Hebbian learning, and 1 =(8 ln 2) ß 18 % for additive Hebbian learning [Palm, 1988]. These values are accomplished with vanishing error probability and yet are higher than those obtained in other known neural memory models. Recent investigations on binary Hebbian learning have proved that iterative retrieval as a more refined retrieval method does not improve the asymptotic completion capacit [...] ...|$|E
40|$|In 1961 Landauer {{pointed out}} that {{resetting}} a <b>binary</b> <b>memory</b> requires a minimum energy of k_BT (2) where k_B is the Boltzmann constant and T the absolute temperature of the memory device. Any memory however, is doomed to loose its content as time proceeds if no action is taken. In order to avoid memory loss, a refresh procedure is periodically performed with time interval t_R. In this paper we show that it does exist a fundamental bound to the minimum energy required to preserve one bit of information for a time t̅, with probability of error less than P_E, and that this energy is a monotonically decreasing function of t_R. Two main conclusions are drawn: {{the good news is}} that, in principle, the cost of remembering can be arbitrarily reduced if the refresh procedure is performed often enough. The bad news is that no memory can be preserved forever, no matter how much energy is invested. Comment: 18 pages, 5 figures, 1 supplementary informatio...|$|E
50|$|Using {{flash memory}} (NAND memory devices) for caching allows Linux kernel to service random disk IO with better {{performance}} than without the cache. This caching {{applies to all}} disk content, not just the page file or system <b>binaries.</b> Flash <b>memory</b> based devices are usually a magnitude faster than spinning HDDs for random IO, but with less advantage or even slower in sequential read/writes. By default, flashcache caches all full blocksize IOs, but can be configured to only cache random IO whilst ignoring sequential IO.|$|R
40|$|Optoelectronic {{apparatus}} acts as artificial {{neural network}} performing associative recall of binary images. Recall process is iterative one involving optical computation of inner products between binary input vector and one or more reference <b>binary</b> vectors in <b>memory.</b> Inner-product method requires far less memory space than matrix-vector method...|$|R
50|$|The binary {{interpretation}} of metric prefixes is still prominently {{used by the}} Microsoft Windows operating system, which is used on 90% of the world's personal computers. They are also used for random-access memory capacities, such as main memory and CPU cache sizes, due to the <b>binary</b> addressing of <b>memory.</b>|$|R
40|$|The helicity-orbital {{coupling}} is {{an intriguing}} feature of magnetic skyrmions in frustrated magnets. Here, we explore the skyrmion dynamics in a frustrated magnet {{based on the}} $J_{ 1 }$-$J_{ 2 }$-$J_{ 3 }$ classical Heisenberg model explicitly by including the dipole-dipole interaction. The skyrmion energy acquires a helicity dependence due to the dipole-dipole interaction, resulting in the current-induced translational motion with a fixed helicity. The lowest energy states are the degenerate Bloch-type states, {{which can be used}} for building the <b>binary</b> <b>memory.</b> By increasing the driving current, the helicity locking-unlocking transition occurs, where the translational motion changes to the rotational motion. Furthermore, we demonstrate that two skyrmions can spontaneously form a bound state. The separation of the bound state forced by a driving current is also studied. In addition, we show the annihilation of a pair of skyrmion and antiskyrmion. Our results reveal the distinctive frustrated skyrmions may enable viable applications in topological magnetism. Comment: 10 pages, 10 figure...|$|E
40|$|The {{number of}} {{elementary}} <b>binary</b> <b>memory</b> devices {{necessary for the}} realization of an arbitrary asynchronous sequential switching circuit is considered. The least upper bound is discovered to be approximately equal to twice the greatest lower bound. The minimum conceivable interstate transition time for a sequential circuit is the reaction time of a single memory element. A solution which achieves this minimum time is derived {{and its relationship to}} the Hamming single-error correcting code is shown. The fundamental limitations of error correction schemes which compensate for malfunctioning of memory elements are discussed. These schemes are feasible in synchronous circuits but have slightly impaired practicability in asynchronous circuits. I. MEANS FOR THE TERMINAL DESCRIPTION OF SWITCHING CIRCUITS A switching circuit has the property that binary signals appear on each of its input leads and on each of its output leads (Fig. 1). The two possible values of each variable are customarily assigned the notations 0 and 1. The meanings of these two symbols depend upon the physical nature of the binary variables represented. Thus, in on...|$|E
40|$|The Little-Hopfield {{network is}} an auto-associative {{computational}} model of neural memory storage and retrieval. This model {{is known to}} robustly store collections of randomly generated binary patterns as stable-states of the network dynamics. However, the number of binary memories so storable scales linearly {{in the number of}} neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model. In this note, we design simple families of Little-Hopfield networks that provably solve this problem affirmatively. As a byproduct, we produce a set of novel (nonlinear) binary codes with an efficient, highly parallelizable denoising mechanism. 1 1. Introduction. Inspired by early work of McCulloch-Pitts [1] and Hebb [2], the Little-Hopfield model [3, 4] is a distributed neural network architecture for <b>binary</b> <b>memory</b> storage and denoising. In [4], Hopfield showed experimentally, using the outer-product learning rule (OPR), that. 15 n binary patterns (generated uniformly at random) can be robustly stored in such an n-node network if some fixed percentage of errors in a recovered pattern were tolerated. Later, it was verified that thi...|$|E
5000|$|He {{competed in}} Memoriad 2008 World Mental Olympics {{and won the}} three {{categories}} of memory in İstanbul. He was crowned with titles of [...] "Memoriad 2008 Speed Cards World Memory Champion", [...] "Memoriad 2008 Numbers Marathon World Memory Champion" [...] and [...] "Memoriad 2008 <b>Binary</b> Digits World <b>Memory</b> Champion".|$|R
50|$|Dynamic loading is a {{mechanism}} by which a computer program can, at run time, load a library (or other <b>binary)</b> into <b>memory,</b> retrieve the addresses of functions and variables contained in the library, execute those functions or access those variables, and unload the library from memory. It {{is one of the}} 3 mechanisms by which a computer program can use some other software; the other two are static linking and dynamic linking. Unlike static linking and dynamic linking, dynamic loading allows a computer program to start up in the absence of these libraries, to discover available libraries, and to potentially gain additional functionality.|$|R
5000|$|Program {{invocation}} under TRSDOS, DOS and UNIX {{is done by}} filename; no explicit LOAD {{command is}} required for normal binary executables nor for text command files (batch files in DOS and shell scripts in UNIX/Linux). The LOAD command under TRSDOS would load a <b>binary</b> program into <b>memory,</b> but would not execute it; neither DOS nor UNIX has an equivalent.|$|R
