18|176|Public
5000|$|... iSCSI: The [...] "target-utils" [...] iscsi {{package on}} many GNU/Linux distributions. The tgtd can {{configure}} the <b>backing</b> <b>storage</b> of a LUN {{to be any}} block device (disk, partition, etc.). This has widest adoption amongst IP-based block device presentation protocols.|$|E
50|$|HECToR's initial configuration, {{known as}} Phase 1, {{featured}} 60 Cray XT4 cabinets containing 1416 compute blades, giving {{a total of}} 11,328 2.8 GHz AMD Opteron processor cores, connected to 576 terabytes of RAID <b>backing</b> <b>storage,</b> later increased to 934 TB. The peak performance of the system was 59 teraflops.|$|E
5000|$|Shadows in the WPS are not {{filesystem}} objects, as aliases are. They {{are derived}} from the WPAbstract class, and thus their <b>backing</b> <b>storage</b> is the user INI file, not a file in the file system. [...] Thus shadows are invisible to applications that do not use the WPS API.|$|E
5000|$|CE-1600M {{program module}} {{providing}} 32K of battery <b>backed</b> <b>storage</b> ...|$|R
50|$|After {{a prison}} stretch for jewel robbery, three {{beautiful}} women {{search for a}} pearl necklace the police never found. Unfortunately for them, the warehouse where they hid it was sold for <b>back</b> <b>storage</b> fees to Shangri-La Upholstering Company operated by the Stooges.|$|R
50|$|Release 5.1 {{achieved}} new {{records with}} a 4 node and 6 node cluster benchmark with DS8700 as <b>backed</b> <b>storage</b> device. SVC broke its own record of 274,997.58 SPC-1 IOPS in March 2010, with 315,043.59 for the 4 node cluster and 380,489.30 with the 6 node cluster, records that stood until October 2011.|$|R
50|$|The ICT 1300 was {{identical}} to the 1301 in every way except that its card reader was limited to 300 cards per minute and its line printer was limited to 300 lines per minute. It tended to be sold with less core storage and drum storage and without magnetic tape. A drum with {{only a quarter of}} the read/write heads fitted was commonly used, giving 3,000 words of 48 bits as <b>backing</b> <b>storage.</b>|$|E
50|$|With {{the release}} of Mac OS X Leopard (10.5) Apple chose {{to move away from}} using the NetInfo {{directory}} service (originally found in NeXTSTEP and OpenStep) which had been used by default for all local accounts and groups in every release of Mac OS X from 10.0 to 10.4. Mac OS X 10.5 now uses Directory Services and its plugins for all directory information. Local accounts are now registered in the Local Plugin, which uses XML property list (plist) files stored in /var/db/dslocal/nodes/Default/ as its <b>backing</b> <b>storage.</b>|$|E
40|$|An {{algorithm}} for {{the numerical}} solution of a dense large general linear system is presented. This algorithm {{is based on}} the Gauss elimination method. No special properties of the left side matrix are assumed and its dimensions depend only on the <b>backing</b> <b>storage</b> available. An error analysis is performed and a test example is given. Detailed documentation of the FORTRAN list appended, is also given. © 1981...|$|E
50|$|Approximately {{three months}} every year (Dec-Mar), the <b>back</b> <b>storage</b> at Hale's Ballard {{location}} is open as a performance venue called The Palladium for live {{events such as}} comedy shows, dances, parties, dinner theatre, or soirees. This is because during {{this time of the}} year the storage room is empty and not used by the brewery.|$|R
50|$|In April 2017, {{just days}} before the 2017 NFL Draft, Rudolph’s father, Darryl, was shot & killed while working at a strip club in West Palm Beach, FL. Police say Rudolph was working in a <b>back</b> <b>storage</b> room when a coworker, in an {{adjacent}} room, accidentally discharged a rifle while moving it off a shelf. The bullet traveled through the wall and struck Rudolph in the neck.|$|R
50|$|Another of the re-clad Bellman hangars (Hangar 230) {{has been}} home to the free-entry Jurby Transport Museum since 2010. The museum is home to many buses and trams that have formed part of the islands public {{transport}} network for many years. In keeping with their aviation surroundings, there are the airship parts mentioned above, a Spitfire replica and a glider kept in pieces in the <b>back</b> <b>storage</b> yard.|$|R
40|$|The Mach {{external}} pager interface allows {{applications to}} supply their own routines for moving pages {{to and from}} second-level store. Mach doesn’t allow applications {{to choose their own}} page replacement policy, however. Some applications have access patterns that may make least recently used page replacement inappropriate. In this paper, we describe an extension to the external pager interface that allows the programmer to specify the page replacement policy as well as the <b>backing</b> <b>storage</b> for a region of virtual memory. ...|$|E
40|$|In this paper, we {{investigate}} a virtual memory management technique for multicomputers called memory servers. The memory server model extends the memory hierarchy of multicomputers by introducing a remote memory server layer. Memory servers are multicomputer nodes whose memory {{is used for}} fast <b>backing</b> <b>storage</b> and logically lie between the local physical memory and disks. The paper presents the model, describes how the model supports sequential programs, message-passing programs and shared virtual memory systems, discusses several design issues, and shows preliminary results of a prototype implementation on an Intel iPSC/ 860. Keywords: distributed memory, memory server, multicomputer, memory management unit, virtual memory. Introduction Multicomputers can provide very high processor performance and tremendous total storage capacity at relatively low costs. Scalable massively parallel multicomputers are attractive architectures because they {{take advantage of the}} performance curves of [...] ...|$|E
40|$|The Mach {{external}} pager interface allows {{applications to}} supply their own routines for moving pages {{to and from}} second-level store. Mach doesn't allow applications {{to choose their own}} page replacement policy, however. Some applications have access patterns that may make least recently used page replacement inappropriate. In this paper, we describe an extension to the external pager interface that allows the programmer to specify the page replacement policy as well as the <b>backing</b> <b>storage</b> for a region of virtual memory. 1 Introduction An operating system attempts to {{be all things to all}} users. Because of this, sometimes compromises have to be made; performance may be sacrificed for generality, or modularity for performance. Virtual memory page replacement schemes are an example of such a tradeoff. While the LRU page replacement policy rarely misbehaves grossly, it rarely provides optimal performance for any application. For some groups of applications we may suspect that a different pag [...] ...|$|E
50|$|The {{customer}} {{should never}} have to manage the <b>back</b> end <b>storage</b> repositories in order to back up and recover data.|$|R
50|$|The tomb was {{discovered}} in 1983 and the museum opened in 1988. It is 20 meters under Xianggang Shan (Elephant Hill) in Guangzhou on a construction site for a hotel, and was excavated. The tomb is nearly 11 meters long and over 12 meters wide. It is divided in seven parts, with a front chamber, east and west wing rooms, the main coffin chamber, east and west side rooms, and a <b>back</b> <b>storage</b> chamber.|$|R
25|$|An issue arose as {{to whether}} the Admiralty should retain control of {{warehouses}} at Gibraltar, which were then underused. Tryon reported that in the event of war, stores immediately available at Gibraltar might be vitally important to the fleet, and that at such a time it would be virtually impossible to get <b>back</b> <b>storage</b> space relinquished in peace time. He {{was one of the few}} at that time to recognise the port's strategic significance for the fleet.|$|R
40|$|Recently {{there has}} {{been a great deal of}} {{interest}} in the operating systems research community in prefetching and caching data from parallel disks, as a technique for enabling serial applications to improve I/O performance. [16, 30, 32, 41, 51, 42]. We consider algorithms for integrated prefetching and caching in a model with a fixed-size cache and any number of <b>backing</b> <b>storage</b> devices (which we will call disks). The integration of caching and prefetching with a single disk was previously considered by Cao et al. [8]. We show that the natural extension of their aggressive algorithm to the parallel disk case is suboptimal by a factor of (nearly) the number of disks in the worst case. Our main result is a new algorithm, reverse aggressive, with near-optimal performance for integrated prefetching and caching in the presence of multiple disks. 1 Introduction 1. 1 Motivation Recent advances in technology have made magnetic disks both cheaper and smaller. As a result, parallel disk arrays hav [...] ...|$|E
40|$|Network-attached storage (NAS) {{provides}} cyberphysical systems (CPS) {{with the}} scalable, efficient, and reliable <b>backing</b> <b>storage,</b> {{such as the}} mobile virtual desktop based on cloud infrastructure. Within this storage architecture, virtual machine (VM) instances running in the NAS client usually receive data from the complex physical world and then persist them in the neat cyberspace in the NAS server. In this paper, we propose Triple-L to improve VM disk I/O performance in the NAS architecture. According to the specific storage semantic, Triple-L decouples the VM image file into several subfiles at the host layer and then selectively moves them into the NAS clients. In such a way, a VM disk I/O request may be proceeded locally in the NAS client, instead of walking the external networking path repetitively between NAS server and client. We have implemented Triple-L in a Xen-based NAS system. An accessory solution for dealing with storage failure and VM live migration on Triple-L is also discussed and evaluated. The experimental result shows that our work can effectively improve the disk I/O performance of VMs. Meanwhile, it brings moderate overhead for VM live migration...|$|E
40|$|Abstract—We present Nswap 2 L, a fast <b>backing</b> <b>storage</b> {{system for}} general purpose clusters. Nswap 2 L {{implements}} a single device interface {{on top of}} multiple heterogeneous physical storage devices, particularly targeting fast random access devices such as Network RAM and flash SSDs. A key design feature of Nswap 2 L is {{the separation of the}} interface from the underlying physical storage; data that are read and written to our “device ” are managed by our underlying system and may be stored in local RAM, remote RAM, flash, local disk or any other cluster-wide storage. Nswap 2 L chooses which physical device will store data based on cluster resource usage and the characteristics of various storage media. In addition, it migrates data from one physical device to another in response to changes in capacity and {{to take advantage of the}} strengths of different types of physical media, such as fast writes over the network and fast reads from flash. Performance results of our prototype implementation of Nswap 2 L added as a swap device on a 12 node Linux cluster show speed-ups of over 30 times versus swapping to disk and over 1. 7 times versus swapping to flash. In addition, we show that for parallel benchmarks, Nswap 2 L using Network RAM and a flash device that is slower than Network RAM can perform better than Network RAM alone. I...|$|E
50|$|The ATF Dingo has a {{modular design}} with five elements: chassis, {{protection}} cell, storage space, engine compartment, and bottom mine blast deflector. Its design is lighter and includes an armored chassis with a blast pan {{instead of the}} more common monocoque hull found in modern blast resistant vehicles. IBD's layered MEXAS is used and the windows are angled to deflect blasts and bullets. A tarpaulin is used over the <b>back</b> <b>storage</b> area instead of metal to save weight.|$|R
2500|$|While {{pointing}} the shotgun at Owens’ back, Williams directed {{him to a}} <b>back</b> <b>storage</b> room and ordered him to lie down. Coward said that he next {{heard the sound of}} a round being chambered into the shotgun. He then heard a shot and glass breaking, followed by two more shots. [...] Records show that he shot at a security monitor and then killed Owens, shooting him twice in the back at point-blank range as he lay prone on the storage room floor.|$|R
50|$|Gollum is a wiki using git as the <b>back</b> end <b>storage</b> mechanism, {{and written}} mostly in Ruby. It is the wiki system {{used by the}} GitHub web hosting system.|$|R
40|$|Current {{generation}} solid-state {{storage devices}} are exposing a new bottlenecks in the SCSI and block {{layers of the}} Linux kernel, where IO throughput is limited by lock contention, inefficient interrupt handling, and poor memory locality. To address these limitations, the Linux kernel block layer underwent a major rewrite with the blk-mq project to move from a single request queue to a multi-queue model. The Linux SCSI subsystem rework {{to make use of}} this new model, known as scsi-mq, has been merged into the Linux kernel and work is underway for dm-multipath support in the upcoming Linux 4. 0 kernel. These pieces were necessary {{to make use of the}} multi-queue block layer in a Lustre parallel filesystem with high availability requirements. We undertook adding support of the 3. 18 kernel to Lustre with scsi-mq and dm-multipath patches to evaluate the potential of these efficiency improvements. In this paper we evaluate the block-level performance of scsi-mq with <b>backing</b> <b>storage</b> hardware representative of a HPC-targerted Lustre filesystem. Our findings show that SCSI write request latency is reduced by as much as 13. 6 %. Additionally, when profiling the CPU usage of our prototype Lustre filesystem, we found that CPU idle time increased by a factor of 7 with Linux 3. 18 and blk-mq as compared to a standard 2. 6. 32 Linux kernel. Our findings demonstrate increased efficiency of the multi-queue block layer even with disk-based caching storage arrays used in existing parallel filesystems. Comment: International Workshop on the Lustre Ecosystem: Challenges and Opportunities, March 2015, Annapolis M...|$|E
40|$|We present Nswap 2 L-FS, a fast, adaptable, and {{heterogeneous}} {{storage system}} for backing file data in clusters. Nswap 2 L-FS particularly targets backing temporary files, {{such as those}} created by data-intensive applications for storing intermediate results. Our work addresses {{the problem of how}} to efficiently and effectively make use of heterogeneous storage devices that are increasingly common in clusters. Nswap 2 L-FS implements a two-layer device design. The top layer transpar- ently manages a set of bottom layer physical storage devices, which may include SSD, HDD, and its own implementation of network RAM. Nswap 2 L-FS appears to node operating systems as a single, fast <b>backing</b> <b>storage</b> device for file systems, hiding the complexity of heterogeneous storage management from OS subsystems. Internally, it implements adaptable and tunable policies that specify where data should be placed and whether data should be migrated from one underlying physical device to another based on resource usage and the characteristics of different devices. We present solutions to challenges that are specific to supporting backing filesystems, including how to efficiently support a wide range of I/O request sizes and balancing fast storage goals with expectations of persistence of stored file data. Nswap 2 L-FS defines relaxed persistence guarantees on individual file writes to achieve faster I/O accesses; less stringent persistence semantics allow it to make use of network RAM to store file data, resulting in faster file I/O to applications. Relaxed persistence guarantees are acceptable in many situations, particularly those involving short-lived data such as temporary files. Nswap 2 L-FS provides a persistence snapshot mechanism that can be used by applications or checkpointing systems to ensure that file data are persistent at certain points in their execution. Nswap 2 L-FS is implemented as a Linux block device driver that can be added as a file partition on individual cluster nodes. Experimental results show that file-intensive applications run faster when using Nswap 2 L-FS as backing store. Additionally, its adaptive data placement and migration policies, which make effective use of different underlying physical storage devices, result in performance exceeding that of any single device...|$|E
40|$|With the {{dramatic}} advances in electronic device industry, {{the availability of}} high speed non-volatile memory (NVRAM) has introduced a new tier into the storage hierarchy, and holds great promise for reduction in latency, power consumption, and improved performance. Among them, flash-memory has become a popular storage medium to replace hard disk as a permanent storage device and DRAM as a temporary storage device. Yet, despite their fast random I/O performance, {{the design of a}} flash-based storage system achieves suboptimal performance or suffers from reduced endurance {{due to the nature of}} flash memory, for example, out of place update, asymmetric read-write throughput, and limited write cycles. Lack of application aware design makes flash memory less efficient, and hence cannot meet various performance requirements. To this end, we investigate the roles flash memory plays in different storage applications and their performance and reliability requirements. By examining the behavior of these systems and their consequent data access characteristics, as well as the performance impact, we propose solutions that tradeoff performance, cost, endurance and reliability to achieve high efficiency for flash memory in different storage applications with reduced overhead. We first explore the use of flash memory as a write-through cache in a tiered storage. We demonstrate the individual and cumulative contributions of cache admission policy, cache eviction policy, flash garbage collection policy, and flash device configuration on a flash caching device. We show that workloads on Solid State Caches (SSCs) have significantly greater write pressures than their storage counterparts. we propose HEC, a High Endurance Cache that aims to improve overall device endurance via reduced media writes and erases while increasing or maintaining cache hit rate performance. To further characterize the behavior of flash memory used in different storage applications, we focus on flash as a primary <b>backing</b> <b>storage</b> device in the second part of this dissertation. We explore the drawbacks of a typical log-structured file system on top of a log-structured FTL flash device. We characterize the interactions between multiple levels of independent logs, and describe several practical scenarios which arises in real log-on-log systems. We then propose a log-aware coordination to tune the layout of logs, so that when multiple layers of logs exist in the system we can still achieve high performance with minimum interference among each log. In the third part of this dissertation, we explore the approaches to collapsing logs. While there are several popular ways that utilize the nature of flash memory translation layer to eliminate multiple layers of logs in the entire system, we focus on the benefit we can obtain from a log-less object-based flash aware system. We show from simulation experiments that advanced features could be embedded to improve overall performance for object-based flash system with low overhead through a rich interface...|$|E
50|$|The MPFS {{protocol}} {{was developed by}} EMC for use in NAS storage environments utilizing EMC Celerra and <b>back</b> end <b>storage</b> environments such as the EMC CLARiiON, EMC VNX and Symmetrix.|$|R
50|$|They are {{tasked with}} finding a {{replacement}} accelerator ring (protected by a cloaking field) {{so that their}} base and regain enough power to facilitate repairs. They wear crystal-driven power suits which enhance their strength and speed and {{protect them from the}} elements as they venture out into a mutant wilderness. They also carry energy guns with a stun setting. Their suits have buttons which summon a hovercycle or put on a protective helmet (another button removes it, collapsing into a <b>back</b> <b>storage</b> compartment) or shoot a grappling hook with retractable cord to pull them up instead of climbing.|$|R
5000|$|December 22, 1999 - Bounds {{walked into}} a tiny flower shop on a busy street in broad {{daylight}} and killed the 30-year-old clerk, Karen Moore Hayden, leaving her face down in a <b>back</b> <b>storage</b> room {{in a pool of}} blood. The young wife and mother’s body was found by a delivery man sent from the main Greenville-Pelham Florist Shop to check on her. He had to unlock the door to get inside after finding all of the store’s lights turned off and the “Sorry, we’re closed” sign hanging in the front window. Hayden’s throat had been slit.|$|R
40|$|Includes bibliographical references. Since the {{emergence}} of the digital computer in the 1940 s, computer architecture has been largely dictated by the requirements of mathematicians and scientists. Trends have thus been towards processing data as quickly and as accurately as possible. Even now, in the age of large scale integration culminating in the microprocessor, internal structures remain committed to these ideals. This is not surprising since the main users of computers are involved with data processing and scientific computing. The process control engineer, who turned to the digital computer to provide the support he required in his ever increasing strive towards automation, has had therefore to use these generalized computing structures. His basic requirements however, are somewhat different to those of the data processing manager or the scientific user. He has to contend with an inherent problem of synchronizing the computer to the real-world timing of his plants. He is far more interested in the response time of the computer to an external occurrence than he is to sheer 'number-crunching' power. Despite the trends in process control towards distributed computing, even the most advanced systems require a relatively large central processor. This processor is called upon to carry out a wide variety of different tasks most of which are 'requested' by external events. Multiprogramming facilities are therefore essential and are normally effected by means of a real-time operating system. One of the prime objectives of such a real time operating system is to permit the various programs to be run at the required time on some priority basis. In many cases these routines can be large - thus requiring access to <b>backing</b> <b>storage.</b> Traditionally the backing store, implemented by a moving-head disc for example is {{under the control of the}} real-time operating system. This can have serious consequences. If real-time requirements are to be met, transfer to and from the disc must be made as rapidly as possible. Also, in initiating and controlling such transfer, the computer is using time which otherwise could be avai 1 ab 1 e for useful, process-orientated work. With the rapid advancement of digital technology, the time is c 1 ear 1 y right to examine our present computer architecture. This dissertation explores the problem area previously discussed - the control over the bulk storage device in a real-time process-control computer system. It is proposed that a possible solution lies in the development of an intelligent backing-store controller. This essentially combines the conventional low-level backing store interface with a special purpose processor which handles all file routines. This dissertation demonstrates how such a structure can be implemented using current technology, and will evaluate its inherent advantages...|$|E
40|$|We have set-up an OpenStack-based cloud {{infrastructure}} {{in the framework}} of a publicly funded project, PRISMA, aimed at the implementation of a fully integrated PaaS+IaaS platform to provide services in the field of smart-government (e-health, e-government, etc.). The IaaS testbed currently consists of 18 compute nodes providing in total almost 600 cores, 3550 GB of RAM, 400 TB of storage (disks). Connectivity is ensured through 2 NICs, 1 Gbit/s and 10 Gbit/s. Both the backend (MySQL database and RabbitMq message broker) and the core services (nova, keystone, glance, neutron, etc.) have been configured in high-availability using HA clustering techniques. The full capacity available by 2015 will provide 2000 cores and 8 TB of RAM. In this work we present the storage solutions that we are currently using as backend for our production cloud services. Storage {{is one of the key}} components of the cloud stack and can be used both to host the running VMs (“ephemeral” storage), and to host persistent data such as the block devices used by the VMs or users’ archived unstructured data, backups, virtual images, etc [...] The storage-as-service is implemented in Openstack by the Block Storage project, Cinder, and the Object Storage project, Swift. Selecting the right software to manage the underlying backend storage for these services is very important and decisions can depend on many factors, not only merely technical, but also economic: in most cases they result from a trade-off between performance and costs. Many operators use separate compute and storage hosts. We decided not to follow this mainstream trend aiming at the best cost-performance scenario: for us it makes sense to run compute and storage on the same machines since we want to be able to dedicate as many of our hosts as possible to running instances. Therefore, each compute node is configured with a significant amount of disk space and a distributed file system (GlusterFS and/or Ceph) ties the disks from each compute node into a single file-system. In this case, the reliability and stability of the shared file-system is critical and defines the effort to maintain the compute hosts: tests have been performed to asses the stability of the shared file-systems changing the replica factor. For example, we observed that GlusterFS in replica 2 cannot be used in production because highly unstable even at moderate storage sizes. Our experience can be useful for all those organizations that have specific constraints in the procurement of a compute cluster or need to deploy on pre-existing servers for which they have little or no control over their specifications. Moreover, the solution we propose is flexible enough, since it is always possible to add external storage when additional storage is required. We currently use GlusterFS distributed file system for: - storage of the running VMs enabling the live migration, - storage of the virtual images (as primary Glance image store), - implementation of one of the Cinder backends for block devices. In particular, we have been using Cinder with LVM-iSCSI driver since Grizzly release when the GlusterFS driver for Cinder did not support advanced features like snapshots and clones, fundamental for our use-cases. In order to exploit GlusterFS advantages even using LVM driver, we created the Cinder volume groups on GlusterFS loopback devices. Upgrading our infrastructure to Havana, we decided to enable Ceph as additional backend of Cinder in order to compare features, reliability and performances of the two solutions. Our interest for Ceph derives also from the possibility to consolidate the infrastructure overall backend storage into a unified solution. To this aim, currently we are testing Ceph to run the Virtual Machines, both using RBD and Ceph-FS protocols, and to implement the object storage. In order to test the scalability and performance of the deployed system using test cases which are derived from the typical pattern of storage utilization. The tools used for testing are standard software widely used for this purpose such as: iozone and/or dd for block storage and specific benchmarking tools like Cosbench, swift-bench and ssbench for the object storage. Using different tools for testing the file-system and comparing their results with the observation of the real test case, is also a good possibility for testing the reliability of the benchmarking tools. Throughput tests have been planned and conducted on the two system configurations in order to understand the performance of both storage solutions and its impacts to applications aiming at achieving the better SLA and end-users experience. Implementing our cloud platform, we focused also on providing transparent access to data using standardized protocols (both de-iure and de-facto standards). In particular, Amazon-compliant S 3 and the CDMI (Cloud Data Management Interface) interfaces have been installed on top of the Swift Object Storage in order to promote interoperability also at PaaS/SaaS levels. Data is important for businesses of all sizes. Therefore, one of the most common user requirement is the possibility to backup data in order to minimize their loss, stay compliant and preserve data integrity. Implementing this feature is particularly challenging when the users come from the public administrations and the scientific communities that produce huge quantities of heterogeneous data and/or can have strict constraints. An interesting feature of the Swift Object Storage is the geographic replica that can be used in order to add a disaster-recovery feature to the set of data and services exposed by our infrastructure. Also Ceph provides a similar feature: the geo-replication through RADOS gateway. Therefore, we have installed and configured both a Swift global cluster and a Ceph federated cluster, distributed on three different geographic sites. Results of the performance tests conducted on both clusters are presented along with a description of the parameters tuning that has been performed for optimization. The different replication methods implemented in the two middlewares, Swift and Ceph, are compared in terms of network traffic bandwidth, cpu and memory consumption. Another important aspect we are taking care of is the QoS (Quality of Service) support, i. e. the capability of providing different levels of storage service optimized wrt the user application profile. This can be achieved defining different tiers of storage and setting parameters like how many I/Os the storage can handle, what limit it should have on latency, what availability levels it should offer and so on. Our final goal is also to set-up a (semi-) automated system that is able of self-optimising. Therefore we are exploring the cache tiering feature of Ceph, that handles the migration of data between the cache tier and the <b>backing</b> <b>storage</b> tier automatically. Results of these testing activities will be shown too in this presentation...|$|E
50|$|A final {{attempt was}} made in February 2002 to return the set to service, but after three days it again failed and was placed <b>back</b> in <b>storage,</b> never to operate in revenue service again.|$|R
50|$|The Pup was a single-seat {{parasol wing}} {{monoplane}} with an Ava flat-four pusher engine {{mounted on the}} wing trailing edge. The wings could be folded <b>back</b> for <b>storage.</b> The Pup registered G-AELR first flew in July 1936.|$|R
40|$|Traditional {{wholesalers}} {{of fresh}} produce in China display and sell all {{the products that}} they have brought to the wholesale market every day. Products left {{by the end of}} the day are deteriorated due to high temperatures at which products are displayed, and are thus sold at a low price or disposed. Modern wholesalers can preserve product quality and carry over unsold products to the next day, by using a <b>back</b> <b>storage</b> with cooling facilities at or close to the market site. Modern wholesalers thus face a multi-period decisions problem, with decision related to amongst other the number of products to keep behind in the cooled <b>back</b> <b>storage,</b> and the number of products to display in the open air from each quality class. During a market day these decision can be revised. Hence the decision problem is not only a multi-stage problem because of carrying over products from one day to the next, but also because of intra-day periods at which the display decision is updated. The price decreases over the day. So does the demand, and the demand depends on the quality of the displayed products. Products that are of (too) low quality are sold/disposed to a secondary market. We modeled the inventory control problem as a Markov decision process model that maximizes the profit of the wholesaler. Based on the model and numerical result for a realistic setting, we analyze the structure of the optimal policies and derive heuristics for practical use. The heuristic methods perform close to the optimal policy, resulting in managerial insights to practitioners. </p...|$|R
25|$|Bellingham's {{proximity}} to the Strait of Juan de Fuca and to the Inside Passage to Alaska helped keep some cannery operations here. P.A.F., for example, shipped empty cans to Alaska, where they were packed with fish and shipped <b>back</b> for <b>storage.</b>|$|R
