65|17|Public
25|$|The Haskell User's Gofer System (Hugs) is a <b>bytecode</b> <b>interpreter.</b> It {{was once}} one of the {{implementations}} used most widely, alongside the GHC compiler, but has now been mostly replaced by GHCi. It also comes with a graphics library.|$|E
2500|$|On June 2, 2008, the WebKit project {{announced}} they rewrote JavaScriptCore as [...] "SquirrelFish", a <b>bytecode</b> <b>interpreter.</b> The project evolved into SquirrelFish Extreme (abbreviated SFX, marketed as Nitro), announced on September 18, 2008, which compiles JavaScript into native machine code, {{eliminating the need}} for a <b>bytecode</b> <b>interpreter</b> and thus speeding JavaScript execution.|$|E
2500|$|On June 2, 2008, the WebKit project {{announced}} they rewrote JavaScriptCore as [...] "SquirrelFish", a <b>bytecode</b> <b>interpreter.</b> The project evolved into SquirrelFish Extreme (abbreviated SFX), announced on September 18, 2008, which compiles JavaScript into native machine code, {{eliminating the need}} for a <b>bytecode</b> <b>interpreter</b> and thus speeding up JavaScript execution. Initially, the only supported processor architecture for SFX was the x86, {{but at the end of}} January 2009, SFX was enabled for OS X on x86-64 as it passes all tests on that platform.|$|E
5000|$|<b>Bytecode</b> <b>interpreters</b> or virtual {{machines}} for internally hosted third party applications ...|$|R
50|$|Control tables - {{that do not}} {{necessarily}} ever need to pass through a compiling phase - dictate appropriate algorithmic control flow via customized interpreters in similar fashion to <b>bytecode</b> <b>interpreters.</b>|$|R
40|$|Usage of the platform-neutral <b>bytecode</b> <b>interpreters</b> {{is often}} limited by their {{restricted}} performance. Just-in-time compilers effectively solve this problem. However they {{are hard to}} develop and retarget. This paper demonstrates that dynamic code generation from the templates created by a C compiler {{can be used to}} build a simple and highly-portable JIT compiler...|$|R
50|$|For each {{hardware}} architecture {{a different}} Java <b>bytecode</b> <b>interpreter</b> is needed. When a computer has a Java <b>bytecode</b> <b>interpreter,</b> it can run any Java bytecode program, {{and the same}} program can be run on any computer that has such an interpreter.|$|E
5000|$|Scheme 48 {{implementation}} of Scheme using <b>bytecode</b> <b>interpreter</b> ...|$|E
5000|$|... #Subtitle level 3: <b>Bytecode</b> <b>interpreter</b> and {{just-in-time}} compiler ...|$|E
50|$|The Little Smalltalk {{system was}} the first Smalltalk {{interpreter}} produced outside of Xerox PARC. Although it lacked many {{of the features of}} the original Smalltalk-80 system, it helped popularize the ideas of object-oriented programming, virtual machines, and <b>bytecode</b> <b>interpreters.</b> Timothy Budd later rewrote Little Smalltalk in Java, and distributes it as the SmallWorld system.|$|R
40|$|Achieving good {{performance}} in <b>bytecoded</b> language <b>interpreters</b> is difficult without sacrificing both simplicity and portability. This {{is due to}} the complexity of dynamic translation ("just-in-time compilation") of bytecodes into native code, which is the mechanism employed universally by highperformance interpreters. We demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators that can attain as much as 70 % the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower "pure" <b>bytecode</b> <b>interpreters.</b> Keywords: <b>bytecode</b> interpretation, threaded code, inlining, dynamic translation, just-in-time compilation. 1 Introduction Bytecoded languages such as Smalltalk [Gol 83], Caml [Ler 97] and Java [Arn 96, Lin 97] offer significant engineering advantages over more conventional languages: higher levels of abst [...] ...|$|R
40|$|MLJ compiles SML' 97 into verifier-compliant Java bytecodes. Its {{features}} include type-checked interlanguage working extensions which allow ML and Java code to call each other, automatic recompilation management, compact compiled code and runtime performance which, using a `just in time' compiling Java virtual machine, usually exceeds that of existing specialised <b>bytecode</b> <b>interpreters</b> for ML. Notable {{features of the}} compiler itself include whole-program optimisation based on rewriting, compilation of polymorphism by specialisation, a novel monadic intermediate lang [...] ...|$|R
5000|$|On June 2, 2008, the WebKit project {{announced}} they rewrote JavaScriptCore as [...] "SquirrelFish", a <b>bytecode</b> <b>interpreter.</b> The project evolved into SquirrelFish Extreme (abbreviated SFX, marketed as Nitro), announced on September 18, 2008, which compiles JavaScript into native machine code, {{eliminating the need}} for a <b>bytecode</b> <b>interpreter</b> and thus speeding JavaScript execution.|$|E
5000|$|... mawk is a {{very fast}} AWK {{implementation}} by Mike Brennan based on a <b>bytecode</b> <b>interpreter.</b>|$|E
50|$|TrueType: TrueType <b>bytecode</b> <b>interpreter</b> {{was turned}} off in 2.0 Beta 8, and then it {{was turned off}} again in 2.0.2. The TrueType <b>bytecode</b> <b>interpreter</b> has been fixed to produce exactly the same output as FreeType 1.x in 2.1.0. The {{unpatented}} hinter became default in 2.3.0, which detects the need of the hinter by font name. The ability to detect patented bytecode instructions and toggling of unpatented hinter were added in 2.3.5. TrueType <b>bytecode</b> <b>interpreter</b> is enabled by default in 2.4.0. TrueType subpixel hinting support is implemented in 2.4.11. The subpixel hinting mode in version 40 of the bytecode engine was added in 2.6.4, and was disabled in 2.6.5, then enabled by default in 2.7. The 'GETVARIATION' bytecode operator was implemented in 2.7.|$|E
5000|$|Indirect {{threading}} uses pointers to {{locations that}} in turn point to machine code. The indirect pointer may be followed by operands which are stored in the indirect [...] "block" [...] rather than storing them repeatedly in the thread. Thus, indirect code is often more compact than direct-threaded code, but the indirection also typically makes it slower, though still usually faster than <b>bytecode</b> <b>interpreters.</b> Where the handler operands include both values and types, the space savings over direct-threaded code may be significant. Older FORTH systems typically produce indirect-threaded code.|$|R
40|$|This paper {{introduces}} superoperators, an {{optimization technique}} for <b>bytecoded</b> <b>interpreters.</b> Superoperators are virtual machine operations automatically synthesized from smaller operations to avoid costly per-operation overheads. Superoperators decrease executable size and can {{double or triple}} the speed of interpreted programs. The paper describes a simple and e ective heuristic for inferring powerful superoperators from the usage patterns of simple operators. The paper describes the design and implementation of a hybrid translator/interpreter that employs superoperators. From a speci cation of the superoperators (either automatically inferred or manually chosen), the system builds an e cient implementation of the virtual machine in assembly language. The system is easily retargetable and currently runs on the MIPS R 3000 and the SPARC...|$|R
40|$|<b>Bytecode</b> <b>interpreters</b> are {{a common}} {{implementation}} strategy for scripting languages. Source code is translated to bytecode to improve time and memory performance. The Android platform includes the Dalvik virtual machine, which typically executes bytecode compiled from Java source code. This thesis describes how this virtual machine can be reused to execute bytecode compiled from a scripting language. A compiler is written for a test bed scripting language and the time and memory performance is evaluated. The Dalvik virtual machine, designed for a statically typed object-oriented language, was flexible enough to successfully host a dynamically typed scripting language that allows for objects to be transported cheaply between scripts and Java code. The compiled code executes one to two orders of magnitude faster than with a naive interpreting implemetation. Numeric performance is lacking in general, though simpler cases are optimized...|$|R
5000|$|On June 2, 2008, the WebKit project {{announced}} they rewrote JavaScriptCore as [...] "SquirrelFish", a <b>bytecode</b> <b>interpreter.</b> The project evolved into SquirrelFish Extreme (abbreviated SFX), announced on September 18, 2008, which compiles JavaScript into native machine code, {{eliminating the need}} for a <b>bytecode</b> <b>interpreter</b> and thus speeding up JavaScript execution. Initially, the only supported processor architecture for SFX was the x86, {{but at the end of}} January 2009, SFX was enabled for OS X on x86-64 as it passes all tests on that platform.|$|E
50|$|YARV (Yet another Ruby VM) is a <b>bytecode</b> <b>interpreter</b> {{that was}} {{developed}} for the Ruby programming language by Koichi Sasada. The goal {{of the project was}} to greatly reduce the execution time of Ruby programs.|$|E
50|$|However, for interpreters, an AST causes more {{overhead}} than a <b>bytecode</b> <b>interpreter,</b> {{because of}} nodes related to syntax performing no useful work, of a less sequential representation (requiring traversal of more pointers) and of overhead visiting the tree.|$|E
40|$|C-Mix {{is a tool}} {{based on}} {{state-of-the-art}} technology that solves the dilemma of whether to write easy-to-understand but slow programs or ecient but incomprehensible programs. C-Mix allows you {{to get the best}} of both worlds: you write the easy-to-understand programs, and C-Mix turns them into equivalent, ecient ones. As C-Mix is fully automatic, this allows for faster and more reliable maintenance of software systems: system programmers need not spend hours on guring out and altering the complicated, ecient code. C-Mix is a program specializer: Given a program written in C for solving a general problem, C-Mix generates faster programs that solve more specic instances of the problem. Application areas include model simulators, hardware verication tools, scientic numerical calculations, ray tracers, in-terpreters for programming languages (Java <b>bytecode</b> <b>interpreters,</b> task-specic interpreters), pattern matchers and operating system routines. C-Mix currently runs on Unix systems supporting the GNU C compiler, and treats programs strictly conforming to the ISO C standard. Future releases of C-Mix are intended to run on a variety of platforms...|$|R
40|$|We present Pycket, a {{high-performance}} tracing JIT compiler for Racket. Pycket supports {{a wide variety}} of the sophisticated fea-tures in Racket such as contracts, continuations, classes, structures, dynamic binding, and more. On average, over a standard suite of benchmarks, Pycket outperforms existing compilers, both Racket’s JIT and other highly-optimizing Scheme compilers. Further, Pycket provides much better performance for proxies than existing systems, dramatically reducing the overhead of contracts and gradual typ-ing. We validate this claim with performance evaluation on multiple existing benchmark suites. The Pycket implementation is of independent interest as an ap-plication of the RPython meta-tracing framework (originally cre-ated for PyPy), which automatically generates tracing JIT compilers from interpreters. Prior work on meta-tracing focuses on <b>bytecode</b> <b>interpreters,</b> whereas Pycket is a high-level interpreter based on the CEK abstract machine and operates directly on abstract syntax trees. Pycket supports proper tail calls and first-class continuations. In the setting of a functional language, where recursion and higher-order functions are more prevalent than explicit loops, the most significant performance challenge for a tracing JIT is identifying which control flows constitute a loop—we discuss two strategies for identifying loops and measure their impact. 1...|$|R
40|$|Interpretation and {{run-time}} compilation {{techniques are}} increasingly {{important because they}} can support heterogeneous architectures, evolving programming languages, and dynamically-loaded code. Interpretation is simple to implement, but yields poor performance. Run-time compilation yields better performance, but is costly to implement. One way to preserve simplicity but obtain good performance is to apply program specialization to an interpreter in order to generate an efficient implementation of the program automatically. Such specialization {{can be carried out}} at both compile time and run time. Recent advances in program-specialization technology have significantly improved the performance of specialized interpreters. This paper presents and assesses experiments applying program specialization to both <b>bytecode</b> and structured-language <b>interpreters.</b> The results show that for some general-purpose bytecode languages, specialization of an interpreter can yield speedups of up to a factor [...] ...|$|R
5000|$|The Haskell User's Gofer System (Hugs) is a <b>bytecode</b> <b>interpreter.</b> It {{used to be}} one of the {{implementations}} {{used most}} widely, alongside the GHC compiler, but has now been mostly replaced by GHCi. It also comes with a graphics library.|$|E
50|$|RapidQ {{features}} a bytecode compiler that produces standalone executables by binding the generated bytecode with the interpreter. No external run time libraries are needed; the <b>bytecode</b> <b>interpreter</b> is self-contained. The file sizes of executable files created by RapidQ are about 150 kilobytes or larger for console applications.|$|E
50|$|Hugs (Haskell User's Gofer System), also Hugs 98, is a <b>bytecode</b> <b>interpreter</b> for the {{functional}} programming language Haskell. Hugs is the successor to Gofer, and was originally derived from Gofer version 2.30b. Hugs and Gofer were originally developed by Mark P. Jones, now a professor at Portland State University.|$|E
40|$|A {{smart card}} is an {{embedded}} {{system that is}} generally used to supply security to an information system. Traditionally the application and the OS were developed in a secure environment by the card issuer. For a few years, open platforms (e. g., Java Card, MultOS and Smart Card for Windows) have provided new facilities for application developers. They allow dynamic storage and execution of downloaded executable code. Such architecture introduces new risks: it offers the possibility to attack the card from an applet by exploiting some implementation faults. This document {{provides an overview of}} a set of techniques required to obtain Common Criteria (CC) high Evaluation Assurance Levels (EALs) of a Java Card. It is not dedicated to smart card specialists as it presents the security stakes of such a technology. We present the motivation for a Java Card evaluation: reach the same security level for the new open smart card than for traditional embedded platforms. We introduce the UML and the B method to illustrate the semi-formal and formal models required for a high level evaluation. The B method has been already used in GEMPLUS to formally model security mechanisms of the Java Card: <b>bytecode</b> verifier, <b>interpreter</b> and firewall. These case studies reveal the interest of using the B method to formalize the Java Card Virtual Machine (JCVM). In a CC evaluation the use of semi-formal and formal techniques is required to obtain the assurance of a high security level...|$|R
40|$|Abstract. Interpretation and {{run-time}} compilation {{techniques are}} increasingly {{important because they}} can support heterogeneous architectures, evolving programming languages, and dynamically-loaded code. Interpretation is simple to implement, but yields poor performance. Run-time compilation yields better performance, but is costly to implement. One way to preserve simplicity but obtain good performance is to apply program specialization to an interpreter in order to generate an e cient implementation of the program automatically. Such specialization {{can be carried out}} at both compile time and run time. Recent advances in program-specialization technology have signi cantly improved the performance of specialized interpreters. This paper presents and assesses experiments applying program specialization to both <b>bytecode</b> and structured-language <b>interpreters.</b> The results show that for some general-purpose bytecode languages, specialization of an interpreter can yield speedups of up to a factor of four, while specializing certain structured-language interpreters can yield performance comparable to that of an implementation in a general-purpose language, compiled using an optimizing compiler. Keywords: partial evaluation, compilation, compiler design, Just-In-Time compilation, run-time code generation, domain-speci c languages, bytecode languages...|$|R
40|$|The {{runtime system}} for the Android {{platform}} has changed to ART. ART differs from previously used Dalvik {{in that it is}} to be a runtime environment for the application’s machine code. As a result, ART does not execute Dalvik <b>bytecode</b> through an <b>interpreter</b> but executes the machine code itself, leading to high performance and many other benefits. This change in runtime system also has many implications for mobile security. While we can anticipate with certainty the resurgence of modified malicious activity or malicious applications previously used with Dalvik or the emergence of completely new structures of malicious techniques, we can no longer ascertain the feasibility of the analysis techniques and analysis tools used against these malicious applications that operated in Dalvik. To combat future potential malicious techniques for ART, we must first have a clear understanding of ART and, with this foundation, to effectively and accurately utilize the correct analysis technique. Thus, this paper serves to introduce an analysis on the operating method and architecture of ART and, based on this information, address the executable feasibility of the analysis techniques in ART. Furthermore, we present the test results of running these analysis tools and techniques in ART...|$|R
50|$|Spin code {{is written}} on the Propeller Tool, a GUI-oriented {{software}} development platform written for Windows XP. This compiler converts the Spin code into bytecodes that can be loaded (with the same tool) into the main 32 KB RAM, and optionally into the I²C boot electrically erasable programmable read-only memory (EEPROM), of the Propeller chip. After booting, the propeller a <b>bytecode</b> <b>interpreter</b> is copied from the built in ROM into the 2 KB RAM of the primary COG. This COG will then start interpreting the bytecodes in the main 32 KB RAM. More than one copy of the <b>bytecode</b> <b>interpreter</b> can run in other COGs, so several Spin code threads can run simultaneously. Within a Spin code program, assembly code program(s) can be inline inserted. These assembler program(s) will then run on their own COGs.|$|E
5000|$|There is a {{spectrum}} of possibilities between interpreting and compiling, depending {{on the amount of}} analysis performed before the program is executed. For example, Emacs Lisp is compiled to bytecode, which is a highly compressed and optimized representation of the Lisp source, but is not machine code (and therefore not tied to any particular hardware). This [...] "compiled" [...] code is then interpreted by a <b>bytecode</b> <b>interpreter</b> (itself written in C). The compiled code in this case is machine code for a virtual machine, which is implemented not in hardware, but in the <b>bytecode</b> <b>interpreter.</b> Such compiling interpreters are sometimes also called compreters. In a byte code interpreter each instruction starts with a byte, and therefore byte code interpreters have up to 256 instructions, although not all may be used. Some byte codes may take multiple bytes, and may be arbitrarily complicated.|$|E
50|$|In 1985-7, a {{team from}} Fuji Xerox {{developed}} a C implementation of the microcoded <b>bytecode</b> <b>interpreter,</b> and, together with Xerox AI Systems (XAIS) in Sunnyvale, California, completed the port {{of the environment and}} emulator to the Sun Microsystems SPARC 4 architecture. In 1987, XAIS was spun off into Envos Corporation, which almost immediately failed.|$|E
40|$|Code {{virtualization}} {{built upon}} virtual machine (VM) technologies {{is emerging as}} a viable method for implementing code obfuscation to protect programs against unauthorized analysis. State-of-the-art VM-based protection approaches use a fixed scheduling structure where the program follows a single, static execution path for the same input. Such approaches, however, are vulnerable to certain scenarios where the attacker can reuse knowledge extracted from previously seen software to crack applications using similar protection schemes. This paper presents DSVMP, a novel VM-based code obfuscation approach for software protection. DSVMP brings together two techniques to provide stronger code protection than prior VM-based schemes. Firstly, it uses a dynamic instruction scheduler to randomly direct the program to execute different paths without violating the correctness across different runs. By randomly choosing the program execution paths, the application exposes diverse behavior, making {{it much more difficult}} for an attacker to reuse the knowledge collected from previous runs or similar applications to perform attacks. Secondly, it employs multiple VMs to further obfuscate the relationship between VM <b>bytecode</b> and their <b>interpreters,</b> making code analysis even harder. We have implemented DSVMP in a prototype system and evaluated it using a set of widely used applications. Experimental results show that DSVMP provides stronger protection with comparable runtime overhead and code size when compared to two commercial VMbased code obfuscation tools...|$|R
40|$|This paper {{describes}} a new method for code space optimization for interpreted languages called LZW-CC. The method {{is based on}} a well-known and widely used compression algorithm, LZW, which has been adapted to compress executable program code represented as bytecode. Frequently occurring sequences of bytecode instructions are replaced by shorter encodings for newly generated <b>bytecode</b> instructions. The <b>interpreter</b> for the compressed code is modified to recognize and execute those new instructions. When applied to systems where a copy of the interpreter is supplied with each user program, space is saved not only by compressing the program code but also by automatically removing the unused implementation code from the interpreter. The method's implementation within two compiler systems for the programming languages Haskell and Java is described and implementation issues of interest are presented, notably the recalculations of target jumps and the automated tailoring of the interpreter to program code. Applying LZW-CC to nhc 98 Haskell results in bytecode size reduction by up to 15. 23 % and executable size reduction by up to 11. 9 %. Java bytecode is reduced by up to 52 %. The impact of compression on execution speed is also discussed; the typical speed penalty for Java programs is between 1. 8 and 6. 6 %, while most compressed Haskell executables run faster than the original. 25 page(s...|$|R
50|$|NumPy targets the CPython {{reference}} implementation of Python, {{which is a}} non-optimizing <b>bytecode</b> <b>interpreter.</b> Mathematical algorithms written for this version of Python often run much slower than compiled equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy.|$|E
