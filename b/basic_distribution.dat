107|306|Public
5000|$|The <b>basic</b> <b>distribution</b> {{function}} {{uses the}} Boltzmann constant [...] and temperature [...] {{with the number}} density to modify the normal distribution: ...|$|E
5000|$|... a {{mechanism}} for indicating or requesting {{one of the two}} <b>basic</b> <b>distribution</b> patterns, competing- and non-competing- consumers, through the distribution modes move and copy respectively ...|$|E
5000|$|The {{creation}} is usually {{based on an}} existing Live CD. Some of the GIS and Remote sensing applications are already included in the basic Linux distribution. Some other packages has got [...] "ready to install" [...] versions while some others need to be translated from the source code. After having got all applications and related contents (e.g. documentations, sample data) installed {{the image of the}} Live DVD is create by the <b>basic</b> <b>distribution</b> provided tools.|$|E
40|$|For {{quantitative}} {{randomized response}} models, optimal unbiased estimation of regular functionals of distribution functions is considered. In this context, distribution theory of Hoeffding's U-statistics and von Mises' differentiable statistical functions is extended to randomized response models. Estimation of the <b>basic</b> <b>distributions</b> is also considered...|$|R
5000|$|An {{early use}} of the word [...] "copyleft" [...] was in Li-Chen Wang's Palo Alto Tiny <b>BASIC's</b> <b>distribution</b> notice [...] "@COPYLEFT ALL WRONGS RESERVED" [...] in June 1976, but Tiny BASIC was not {{distributed}} under any form of copyleft distribution terms, so the wordplay is the only similarity.|$|R
40|$|The <b>basic</b> Weibull <b>distribution</b> is {{considered}} the most fundamental and <b>basic</b> lifetime <b>distribution.</b> Various extensions of the Weibull distribution have been proposed since the 1970 s and are useful in the modeling of complex lifetime data that are beyond the capability of the basic Weibull. This article reviews {{the properties of the}} <b>basic</b> Weibull <b>distribution</b> and lists the various extensions. It describes the use of Weibull probability plots as a tool for model selection and discusses the parameter estimation and model validation. It concludes with some topics for future research...|$|R
5000|$|Installation The {{ability to}} install {{comes from the}} basic Linux {{distribution}} usually. To keep this feature of the basic Linux distribution, the thematic content needs to be stored according {{to the standards of}} the <b>basic</b> <b>distribution.</b> As part of the installation, the system makes an update that can spoil the set-up of the added applications. In order the fully reserve this feature, repositories must be created and regularly maintained for all the installed applications.|$|E
50|$|Players {{begin by}} {{creating}} a human character. After choosing a name and gender, a number of specific questions lead a <b>basic</b> <b>distribution</b> of attribute points. These questions have five possible answers. One of the five answers provides no benefit {{at the end of}} character creation, while each of the other four will increase different attributes. The final part of the character creation allows the player to roll attribute scores. This function is not limited, so a player may choose to use it again until they seem to fit; however, the maximum value of the character's attributes is limited by the player's responses to the preceding questions.|$|E
50|$|Many {{languages}} {{lack the}} equivalents of the English definite and indefinite articles, e.g. the Slavic languages. Thus in these languages, determiners appear much {{less often than}} in English, where the definite article the and the indefinite article a are frequent. What this means for the DP-analysis is that null determiners are a common occurrence in these languages. In other words, the DP-analysis must posit the frequent occurrence of null determiners {{in order to remain}} consistent about its analysis of DPs. DPs that lack an overt determiner actually involve a covert determiner in some sense. The problem is evident in English as well, where mass nouns can appear with or without a determiner, e.g. milk vs. the milk, beer vs. the beer. Plural nouns as well, they can appear with or without a determiner, e.g. books vs. the books, ideas vs. the ideas, etc. Since nouns that lack an overt determiner have the same <b>basic</b> <b>distribution</b> as nouns with a determiner, the DP-analysis should, if it wants to be consistent, posit the existence of a null determiner every time an overt determiner is absent. The traditional NP analysis is not confronted with this necessity, since for it, the noun {{is the head of the}} noun phrase regardless of whether a determiner is or is not present. Thus the traditional NP analysis requires less of the theoretical apparatus, since it does not need all those null determiners, the existence of which is non-falsifiable. Other things being equal, less is better according to Occam's Razor.|$|E
40|$|This paper {{considers}} {{an important}} concept which suggested {{to take stock}} of the over estimation in reliability characteristics or under estimation of hazard rate. Using this concept, the study considers the analysis of the reliability characteristics of an exponential lifetime model when prior variations in its parameters are suspected. Key Words: Robustness, adjustment factor, updated and predictive <b>basic</b> <b>distributions</b> 1...|$|R
40|$|Health and Disability Advocates and Rob Paral and Associates have {{developed}} estimates of uninsured persons in Illinois by state legislative districts. These {{estimates of the}} uninsured household population are categorized by poverty level and age. This report presents the <b>basic</b> <b>distributions</b> by age and poverty status. This report produced by Rob Paral and Associates in collaboration with United Power for Action and Justice and Heartland Alliance...|$|R
40|$|We develop exact random variate {{generators}} {{for several}} distributions {{related to the}} Jacobi theta function. These include the distributions of the maximum of a Brownian bridge, a Brownian meander and a Brownian excursion, and distributions of certain first passage times of Bessel processes. The algorithms {{are based on the}} alternating series method. Furthermore, we survey various distributional identities and point out ways of dealing with generalizations of these <b>basic</b> <b>distributions.</b> ...|$|R
40|$|Based {{upon the}} method of Analytic Hierarchy Process (AHP) a model for multicriterial {{analysis}} and assessment of the transition from 10 kV to 20 kV voltage {{on the level of}} <b>basic</b> <b>distribution</b> network areas and operational units is given. The goal is a global classification of the entire HEP DSO distribution network (about 80 <b>basic</b> <b>distribution</b> areas) in sense of feasibilities, needs and benefits for the transition from 10 kV to 20 kV voltage level. The paper is an overview of the project conducted to develop the method and procedure based on a selected representative sample of 24 <b>basic</b> <b>distribution</b> areas...|$|E
3000|$|The <b>basic</b> <b>distribution</b> {{data for}} the service object {{includes}} demand, location, time window, and so on; [...]...|$|E
30|$|Using the {{methodology}} and metrics described previously in Sects. 3 – 5, we present our empirical findings and analysis in this section. We begin our discussion with <b>basic</b> <b>distribution</b> analysis, followed by findings in {{each type of}} analysis within the talent flow analytics phase.|$|E
40|$|We {{consider}} {{top quark}} pair production {{in association with}} a hard jet through next-to-leading order in perturbative QCD. Top quark decays are treated in the narrow width approximation and spin correlations are retained throughout the computation. We include hard jet radiation by top quark decay products and explore their importance for <b>basic</b> kinematic <b>distributions</b> at the Tevatron and the LHC. Our results suggest that QCD corrections and jet radiation in decays can lead to significant changes in shapes of <b>basic</b> <b>distributions</b> and, therefore, need to be included for the description of ttbar+jet production. We compare {{the shape of the}} transverse momentum distribution of a top quark pair recently measured by the D 0 collaboration with the result of our computation and find reasonable agreement. Comment: 22 pages, 8 figure...|$|R
40|$|Background. One of the epidemiologist's {{most basic}} tasks Is {{estimation}} of disease occurrence. To perform this task, the epidemiologist frequently models variability in disease occurrence {{using one of}} three distributions—the binomial, the Poisson or the exponential distribution. Although epidemiologists often use them and their properties appear in standard texts, we know of no text or review that compares and contrasts epidemiological application of these distributions. Methods. In this commentary, we discuss these three <b>basic</b> <b>distributions.</b> We note key assumptions as well as limita-tions, and compare results from analyses based on each distribution. Results and Conclusions. We Illustrate that the three distributions, although superficially different, often lead to similar results. We argue that epidemiologists should often obtain similar results regardless of which distribution they use. We {{also point out that}} application of all three distributions can be inappropriate if assumptions of independence or homo-geneity of risks fail to hold. Finally, we briefly review how these <b>basic</b> <b>distributions</b> can be used to justify use of other dis-tributions, such as the Gaussian distribution, for studying disease-exposure associations. Assessment of disease occurrence is one of the epidemio-logist's most basic tasks. To perform this task, the epi-demiologist typically uses either of two basic measures...|$|R
40|$|During this summer, I {{focused on}} the quality {{assurance}} of the pp Minimum Bias and High Multiplicity data collected in 2015. The main strategy to achieve such goal was to repeated the pseudorapidity measurement on Minimum Bias data collected in different periods, x-checking also the agreement of the <b>basic</b> <b>distributions</b> between the data and the simulation, and investigating various effects like the trigger conditions, {{the change in the}} LHC beam structure and the response of the inner Tracking System to these changes...|$|R
40|$|This thesis {{deals with}} the design of {{technological}} components for drum brake motorcycle. The introduction {{deals with the}} history of development of braking systems and their <b>basic</b> <b>distribution.</b> The documentation includes technological production process, the building of NC program in 3 D design software and technical-economic evaluation...|$|E
40|$|We generalize {{the notion}} of submersive second-order {{differential}} equations by relaxing the condition that the decoupling stems from the tangent lift of a <b>basic</b> <b>distribution.</b> It is shown that this leads to adapted coordinates in which a number of first-order equations decouple from the remaining second-order ones...|$|E
40|$|Wadley's problem {{results from}} quantal {{dose-response}} experiments when the <b>basic</b> <b>distribution</b> is Poisson, rather than binomial. We review {{approaches to the}} problem, with particular reference to using GLIM and incorporating overdispersion. Two examples are provided, {{and the importance of}} estimating overdispersion from the fit of a saturated model is emphasized...|$|E
40|$|Parts I and II of {{this paper}} have {{described}} a new theory {{for the analysis of}} games with incomplete information. Two cases have been distinguished: consistent games in which there exists some <b>basic</b> probability <b>distribution</b> from which the players' subjective probability distributions can be derived as conditional probability distributions; and inconsistent games in which no such <b>basic</b> probability <b>distribution</b> exists. Part III will now show that in consistent games, where a <b>basic</b> probability <b>distribution</b> exists, it is essentially unique. It will also be argued that, in the absence of special reasons to the contrary, one should try to analyze any given game situation with incomplete information in terms of a consistent-game model. However, it will be shown that our theory can be extended also to inconsistent games, in case the situation does require the use of an inconsistent-game model. ...|$|R
40|$|A new {{numerical}} formulation with computed results, is presented. This formulation {{combines the}} adaptability to complex shapes offered by paneling schemes with the smoothness {{and accuracy of}} the loading function methods. The formulation employs a continuous distribution of singularity strength over a set of panels on a paneled wing. The <b>basic</b> <b>distributions</b> are independent, and each satisfies all of the continuity conditions required of the final solution. These distributions are overlapped both spanwise and chordwise (termed 'spline'). Boundary conditions are satisfied in a least square error sense over the surface using a finite summing technique to approximate the integral...|$|R
40|$|Abstract. Many {{stochastic}} {{models have}} been used in solving reliability problems, motivated by a high degree of variability or randomness of the studied phenomena. Therefore different types of stochastic laws derived from the <b>basic</b> <b>distributions</b> are proposed for modelling a hazard-rate func-tion too. The main contribution of the present paper is to proposed new adaptive hazard rate functions, derived from classical models. A numerical example is provided for the introduced models and comparison is also dis-cussed. It can be seen that these new adaptive functions are competitive models for describing the bathtub-shaped failure rate of the lifetime data...|$|R
40|$|Communicated by Manuel de León) Abstract. We generalize {{the notion}} of submersive second-order {{differential}} equations by relaxing the condition that the decoupling stems from the tangent lift of a <b>basic</b> <b>distribution.</b> It is shown that this leads to adapted coordinates in which a number of first-order equations decouple from the remaining second-order ones...|$|E
40|$|Bachelor thesis {{deals with}} topic of cutting ceramics. In {{the first part}} is given a <b>basic</b> <b>distribution</b> of cutting {{materials}} and their general characteristics. The second part is focused on itself cutting ceramics, its history, production, distribution and mechanical and physical properties. In the third part is discribed assortment Ceramtec, Iscar, Kennametal, Sandvik-Coromant, Saint-Gobain Advanced Ceramics...|$|E
40|$|This bachelor‘s thesis {{focuses on}} anodic {{aluminum}} oxidation, especially on its {{use as a}} surface treatment of sports equipment. The study presents {{the problem of the}} surface protection of aluminum alloys, its <b>basic</b> <b>distribution,</b> and introduces the process technology and its individual procedures. There is also a complete overview of survey anodising standards and some examples of anodised products...|$|E
40|$|In {{this paper}} changes in wind speed and wind {{direction}} from a measured wind field are being analyzed at high frequencies. This {{is used to}} estimate changes in the angle of attack (AOA) on a blade segment over short time periods for different estimated turbine concepts. Here a statistical approach is chosen to grasp {{the characteristics of the}} probability distributions to give an over all view of the magnitude and rate of the changes. The main interest is the generation of <b>basic</b> <b>distributions</b> for the calculation of dynamic stall effects and stall flutter due to wind fluctuations...|$|R
3000|$|... {{between each}} pair of nodes we are not {{interested}} in linear interpolation (<b>basic</b> probability <b>distribution</b> and zero nodes combination), but there ought to be some generalization (even for two nodes only) with other probability distributions and nodes combinations.|$|R
40|$|Marshall-Olkin {{extended}} distributions offer a {{wider range}} of behaviour than the <b>basic</b> <b>distributions</b> from which they are derived and therefore may find applications in modeling lifetime data, especially within proportional odds models, and elsewhere. The present paper carries out a simulation study of likelihood ratio, Wald and score tests for the parameter that distinguishes the extended <b>distribution</b> from the <b>basic</b> one, for the Weibull and exponential cases, allowing for right censored data. The likelihood ratio test is found to perform better than the others. The test is shown to have sufficient power to detect alternatives that correspond to interesting departures from the basic model and can be useful in modeling. © 2008 Springer-Verlag...|$|R
40|$|This work {{deals with}} the use of {{composite}} materials in the automotive industry. The first part is a familiar historical overview of the evolution of materials explaining the definition of terms, the <b>basic</b> <b>distribution</b> of composite materials and processing methods and technology composite production. The second part focuses on the application of composite materials in the automotive industry at the moment...|$|E
40|$|This bachelor´s thesis {{deals with}} the {{technology}} of precision forging. The thesis describes <b>basic</b> <b>distribution</b> of the forging according {{to the nature of}} the material flow and his procedure. After the introductory part this thesis {{deals with the}} analysis of the methods of the drop forging with fin groove. The next section contains a description, parameters and methods of precision forging. Technical a economical evaluation of the component technology is as a conclusion of the thesis...|$|E
40|$|The {{goal of this}} {{bachelor}} {{thesis is}} desctription of function and structure of piezoeletric motor „piezowawes“, creation his laboratory formation and folowing measuring his static characteristic (feed spead / drawing force). Except experimental conclusions is this thesis aim on theoretical analysis and physical explanation of piezoeletric phenomenon. This analys contains materials and important definition which are delaing with this phenomenon. This thesis also contains <b>basic</b> <b>distribution</b> of each kind of piezomotor and discuss aboat their practical use...|$|E
40|$|High-performance {{parallel}} file {{systems are}} {{a critical component}} of the largest computer systems, are primarily proprietary, and are specialized to high end computing systems that have many access patterns known to be unusual in enterprise and productivity workplaces. Yet little knowledge of even the <b>basic</b> <b>distributions</b> of file systems and file ages are publicly available, even though significant effort and importance is increasingly associated with small files, for example. In this paper we report on the statistics of supercomputing file systems at rest from a variety of national resource computing sites, contrast these to studies of the 80 s and 90 s of academic and software development campuses and observe the most interesting characteristics in this novel data...|$|R
40|$|The paper {{characterizes the}} <b>basic</b> <b>distributions</b> of failure of {{elements}} {{that constitute the}} technical (mechatronical) systems: exponential, Weibull, normal, log‐normal distribution. The description of two‐element parallel technical systems with reliability characteristics has been made. Specific cases are studied where up time of the technical system elements have exponential, Weibull, normal, log‐normal distributions and where the system consists of two parts with parallel reliability structure and {{of different types of}} distributions of elements up times. The order of elements in the analysis does not matter. The relevant characteristics of reliability for a system with two parallel elements are presented: up time distribution function of the system, system reliability, up time probability density of the system, the system failure intensity...|$|R
40|$|Abstract. The paper reviews two <b>basic</b> time–frequency <b>distributions,</b> {{spectrogram}} and cone–shaped kernel distribution. We study, {{analyze and}} compare properties {{and performance of}} these quadratic representations on speech signals. Cone–shaped kernel distribution was successfully applied to speech features extraction due to several useful properties in time–frequency analysis of speech signals. ...|$|R
