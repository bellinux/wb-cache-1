283|613|Public
2500|$|When {{applying}} for a position at the Russian Academy of Sciences, Lexell submitted a paper called [...] "Method of analysing some differential equations, illustrated with examples", which was highly praised by Leonhard Euler in 1768. Lexell's method is as follows: for a given nonlinear differential equation (e.g. second order) we pick an intermediate integral—a first-order differential equation with undefined coefficients and exponents. After differentiating this intermediate integral we compare it with the original equation and get the equations for the coefficients and exponents of the intermediate integral. After we express the undetermined coefficients via the known coefficients we substitute them in the intermediate integral and get two particular solutions of the original equation. Subtracting one particular solution from another {{we get rid of}} the differentials and get a general solution, which we analyse at various values of constants. The method of reducing the order of the differential equation was known at that time, but in another form. Lexell's method was significant because it was applicable to a broad range of linear differential equations with constant coefficients that were important for physics applications. In the same year, Lexell published another article [...] "On integrating the differential equation a'n'd'n'y + ba'n-1'd'm-1'ydx + ca'n-2'd'm-2'ydx'2 + ... + rydx'n = Xdx'n"d'n'y + ba'n-1'd'm-1'ydx + ca'n-2'd'm-2'ydx'2 + ... + rydx'n = Xdx'n|journal=Novi Commentarii Academia Scientarum Imperialis Petropolitanae|volume=14|issue=1|pages=215–237}} presenting a general highly <b>algorithmic</b> <b>method</b> of solving higher order linear differential equations with constant coefficients.|$|E
50|$|Algorithmic cooling is an <b>algorithmic</b> <b>method</b> for {{transferring}} heat (or entropy) {{from some}} qubits to others {{or outside the}} system and into the environment, thus resulting in a cooling effect. This cooling effect may have usages in initializing cold (highly pure) qubits for quantum computation and in increasing polarization of certain spins in nuclear magnetic resonance.|$|E
5000|$|The case {{in which}} the <b>{{algorithmic}}</b> <b>method</b> is reversible, such that the total entropy of the system is not changed, was first named [...] "molecular scale heat engine", and is also named [...] "reversible algorithmic cooling". This process cools some qubits while heating the others. It is limited by a variant of Shannon's bound on data compression and it can asymptotically reach quite close to the bound.|$|E
30|$|Effort {{estimation}} methods {{can be divided}} into two major groups <b>algorithmic</b> <b>methods</b> and non-algorithmic <b>methods.</b> <b>Algorithmic</b> <b>methods</b> carries mathematical formula, which is regression model of historical data. The most famous methods are COCOMO (Boehm 1984), FP (Atkinson and Shepperd 1994) and UCP (Karner 1993). But {{there is a lot of}} <b>algorithmic</b> <b>methods.</b> To the second category belong methods like expert judgement and analogy based methods. The most famous methods is Delphi (Rowe and Wright 1999).|$|R
5|$|Many <b>algorithmic</b> <b>methods</b> {{for finding}} graph homomorphisms, like backtracking, {{constraint}} propagation and local search, {{apply to all}} CSPs.|$|R
40|$|Abstract: A {{class of}} {{generalized}} approximate inverse finite element matrix <b>algorithmic</b> <b>methods</b> for solving nonlinear parabolic and elliptic PDE’s, is presented. Fully parameterized singularly perturbed non-linear parabolic and elliptic PDE’s are considered and explicit preconditioned generalized conjugate gradient- type schemes are presented for the efficient {{solution of the}} resulting nonlinear systems of algebraic equations. Applications of the proposed <b>algorithmic</b> <b>methods</b> on characteristic twodimensional non-linear boundary value and initial value problems are discussed and numerical results are given...|$|R
50|$|Algorithmic cooling is an <b>algorithmic</b> <b>method</b> for {{transferring}} heat (or entropy) {{from some}} qubits to others {{or outside the}} system and into the environment, which results in a cooling effect. This method uses regular quantum operations on ensembles of qubits, {{and it can be}} shown that it can succeed beyond Shannon's bound on data compression. The phenomenon {{is a result of the}} connection between thermodynamics and information theory.|$|E
50|$|ICC III was {{released}} on 6 July 2007 including a new <b>algorithmic</b> <b>method</b> of calculating player performances and a 3D match engine with motion captured shots. These changes are arguably {{a result of the}} competition that International Cricket Captain now faces from the popular Cricket Coach series, developed by Rockingham Software. The new engine replaces the original graphic highlights which were retained for each of the previous versions of the game, save minor cosmetic tweaks.|$|E
50|$|Sister Celine is most {{remembered for}} the method that bears her name, first {{described}} in her Ph.D. thesis concerning recurrence relations in hypergeometric series. The thesis demonstrated a purely <b>algorithmic</b> <b>method</b> to find recurrence relations satisfied by sums of {{terms of a}} hypergeometric polynomial and requires only the series expansions of the polynomial. The beauty of her method is that it lends itself readily to computer automation. The work of Wilf and Zeilberger generalized the algorithm and established its correctness.|$|E
50|$|Lexical {{ambiguity}} can {{be addressed}} by <b>algorithmic</b> <b>methods</b> that automatically associate the appropriate meaning with a word in context, a task referred to as word sense disambiguation.|$|R
50|$|Loewy's {{results have}} been {{extended}} to linear partial differential equations (PDEs) in two independent variables. In this way, <b>algorithmic</b> <b>methods</b> for solving large classes of linear pde's have become available.|$|R
5000|$|ATMOS, the Workshop on Algorithmic Approaches for Transportation Modeling, Optimization and Systems, {{formerly}} the Workshop on <b>Algorithmic</b> <b>Methods</b> and Models for Optimization of Railways, {{has been part}} of ALGO in 2003-2006 and 2008-2009.|$|R
5000|$|Simultaneous {{perturbation}} stochastic approximation (SPSA) is an <b>algorithmic</b> <b>method</b> for optimizing {{systems with}} multiple unknown parameters. It {{is a type}} of stochastic approximation algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation optimization, and atmospheric modeling. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992).|$|E
50|$|The {{problem of}} {{determining}} whether a formula is a tautology is fundamental in propositional logic. If there are n variables occurring in a formula then there are 2n distinct valuations for the formula. Therefore, the task of determining {{whether or not the}} formula is a tautology is a finite, mechanical one: one need only evaluate the truth value of the formula under each of its possible valuations. One <b>algorithmic</b> <b>method</b> for verifying that every valuation causes this sentence to be true is to make a truth table that includes every possible valuation.|$|E
5000|$|Wilks {{was educated}} at Torquay Boys' Grammar School, {{followed}} by Pembroke College, Cambridge, where {{he joined the}} Epiphany Philosophers and obtained his Doctor of Philosophy degree (1968) under Professor R. B. Braithwaite for the thesis 'Argument and Proof'; he was an early pioneer in meaning-based approaches {{to the understanding of}} natural language content by computers. His main early contribution in the 1970s was called [...] "Preference Semantics" [...] (Wilks, 1973; Wilks and Fass, 1992), an <b>algorithmic</b> <b>method</b> for assigning the [...] "most coherent" [...] interpretation to a sentence in terms of having the maximum number of internal preferences of its parts (normally verbs or adjectives) satisfied. That early work was hand-coded with semantic entries (of the order of some hundreds) as was normal at the time, but since then has led to the empirical determinations of preferences (chiefly of English verbs) in the 1980s and 1990s.|$|E
50|$|The ideal {{theory in}} {{question}} had {{been based on}} elimination theory, but in line with David Hilbert's taste moved away from <b>algorithmic</b> <b>methods.</b> Gröbner basis theory has now reversed the trend, for computer algebra.|$|R
3000|$|<b>Algorithmic</b> <b>methods</b> can {{facilitate}} natural-scene picture assessment, {{and the same}} image files {{can be used for}} both subjective and objective measurements. The <b>algorithmic</b> <b>methods</b> can be classified into no-reference (NR), reduced-reference (RR), and full-reference (FR) methods. This classification is related to reference images' availability and use. An NR metric does not need a reference image, an RR metric needs some information about a reference, and an FR metric needs a pixel-wise reference image. [...] "Pixel-wise" [...] means that the corresponding pixels in two images are found at the same pixel coordinate locations. This is not the case when characterizing cameras, e.g., for benchmarking purposes.|$|R
50|$|The {{cavity method}} has proved {{useful in the}} {{solution}} of optimization problems such as k-satisfiability and graph coloring. It has yielded not only ground states energy predictions in the average case, but also has inspired <b>algorithmic</b> <b>methods.</b>|$|R
5000|$|When {{applying}} for a position at the Russian Academy of Sciences, Lexell submitted a paper called [...] "Method of analysing some differential equations, illustrated with examples", which was highly praised by Leonhard Euler in 1768. Lexell's method is as follows: for a given nonlinear differential equation (e.g. second order) we pick an intermediate integral—a first-order differential equation with undefined coefficients and exponents. After differentiating this intermediate integral we compare it with the original equation and get the equations for the coefficients and exponents of the intermediate integral. After we express the undetermined coefficients via the known coefficients we substitute them in the intermediate integral and get two particular solutions of the original equation. Subtracting one particular solution from another {{we get rid of}} the differentials and get a general solution, which we analyse at various values of constants. The method of reducing the order of the differential equation was known at that time, but in another form. Lexell's method was significant because it was applicable to a broad range of linear differential equations with constant coefficients that were important for physics applications. In the same year, Lexell published another article [...] "On integrating the differential equation andny + ban-1dm-1ydx + can-2dm-2ydx2 + ... + rydxn = Xdxn" [...] presenting a general highly <b>algorithmic</b> <b>method</b> of solving higher order linear differential equations with constant coefficients.|$|E
40|$|AbstractWe give a {{characterization}} of primary ideals of finitely generated commutative monoids {{and in the}} case of finitely generated cancellative monoids we give an <b>algorithmic</b> <b>method</b> for deciding if an ideal is primary or not. Finally we give some properties of primary elements of a cancellative monoid and an <b>algorithmic</b> <b>method</b> for determining the primary elements of a finitely generated cancellative monoid...|$|E
40|$|The {{table and}} <b>algorithmic</b> <b>method</b> of {{calculation}} of polynomials based on preliminary coefficient processing is offered. Possibility of acceleration of calculation of polynomials {{in comparison with}} realization of the well-known table and algorithmic methods is shown. ????????? ????????-??????????????? ????? ?????????? ???????????, ?????????? ?? ??????????????? ????????? ?????????????. ???????? ??????????? ????????? ?????????? ??????????? ?? ????????? ? ??????????? ????????? ????????-??????????????? ???????...|$|E
40|$|We {{characterize}} Cohen-Macaulay and Gorenstein rings {{obtained from}} {{certain types of}} convex body semigroups. <b>Algorithmic</b> <b>methods</b> to check if a polygonal or circle semigroup is Cohen-Macaulay/Gorenstein are given. We also provide some families of Cohen-Macaulay and Gorenstein rings. Comment: 11 pages, 5 figure...|$|R
40|$|The {{aim of this}} {{contribution}} is to show the possibilities for solving ordinary differential equations with <b>algorithmic</b> <b>methods</b> using Sophus Lie’s ideas and computer means. Our material is related especially to Lie’s work on transformations and differential equations—essential ideas are already containe...|$|R
40|$|This paper {{explains}} {{the need and}} provides the main stages {{of the development of}} new curricula "Mathematical analysis" for students training areas - computer science, which includes the study of the possible applications of mathematical analysis in information processing <b>algorithmic</b> <b>methods</b> using computer technology...|$|R
40|$|Purpose: The {{result of}} this paper is {{verification}} of relation between constructional features and technological features. The basic tool of the series of types of technologies creating is <b>algorithmic</b> <b>method.</b> The worked out datum of these methods is already elaborated ordered families of construction in form of series of types or modular series of elements construction. Design/methodology/approach: This paper shows algorithmic understanding of getting input data in the process of technology creating based on data of construction. Basic tool of realization of data selection for the process of manufacture is <b>algorithmic</b> <b>method</b> and use of advanced graphic programme. Findings: The basic result of the analyzed problem is the realization of relations between construction and technology for specified series of types of elements of machine engines. Moreover the <b>algorithmic</b> <b>method</b> with its essential constituents which determine input date of algorithmization of processes of selection of technological features on bases of constructional features was introduced. Research limitations/implications: Analyzed methods develop algorithmization of designing environment and support integration with the prepare production process (relational databases, theory of automatic classification). Practical implications: Described methods were developed on practical examples of creating the technological module systems of hydraulic cylinders used in mining, slag cars used in metallurgy and gears series of types The represented methods are applied for the series of type of units of servo-motors hydraulic practical in mining. Originality/value: <b>Algorithmic</b> <b>method</b> and CAM method are basis for selection of technological features in the process of already ordered technology families creating. This method is characterized with possibility of shortening time connected with preparation of manufacture...|$|E
40|$|Summary. The char-set {{method of}} {{polynomial}} equations-solving is naturally {{extended to the}} differential case which gives rise to an <b>algorithmic</b> <b>method</b> of solving arbitrary systems of algebrico-differential equations. As an illustration of the method, the Devil’s Problem of Pommaret is solved in details...|$|E
40|$|Kummer type {{formulas}} are {{identities of}} hypergeometric series. A symmetry by the permutations of n-letters yields these formulas. We will present an <b>algorithmic</b> <b>method</b> to derive known and new Kummer type formulas. The algorithm utilizes several algorithms in algebra and geometry for generating Kummer type formulas. ...|$|E
40|$|Annals of Mathematics and Artificial Intelligence {{presents}} {{a range of}} topics of concern to scholars applying quantitative, combinatorial, logical, algebraic and <b>algorithmic</b> <b>methods</b> to diverse areas of Artificial Intelligence, from decision support, automated deduction, and reasoning, to knowledge-based systems, machine learning, computer vision, robotics and planning...|$|R
40|$|A novel {{combination}} of emergent <b>algorithmic</b> <b>methods,</b> powerful computational platforms and supporting infrastructure is described. These complementary tools and technologies {{are used to}} launch systematic attacks on combinatorial problems of significance. As a case study, optimal solutions to very large instances of the NP-hard vertex cover problem are computed...|$|R
40|$|<b>Algorithmic</b> <b>methods</b> in D modules {{have been}} used in {{mathematical}} study of hypergeometric functions and in computational algebraic geometry. In this paper, we show that these algorithms give correct algorithms to perform several operations for holonomic functions and also generates substantial information for numerical evaluation of holonomic functions. 1...|$|R
40|$|Starting {{with the}} basic {{principles}} of circuits, this book derives their analytic properties in both the time and frequency domains. It develops an <b>algorithmic</b> <b>method</b> to design common and uncommon types of circuits, such as prototype filters, lumped delay lines, constant phase difference circuits, and delay equalizers...|$|E
40|$|In {{the main}} part of this thesis we develop a {{completely}} new <b>algorithmic</b> <b>method</b> for τ-lepton identification {{within the framework of the}} fast simulation of the ATLAS detector. We have found out that this method is reproducing quite accurately the full detector simulation Tau ID performance (efficiency and rejection).|$|E
40|$|The primary {{objective}} of the research is to study the non-linear behavior of irregular tensegrity structures and formulate a computational generative, evaluative and <b>algorithmic</b> <b>method</b> to design a structurally dynamic tensegrity system, with inherent potential {{to adapt to the}} varying contexts and its respective demands, requirements and spatial needs...|$|E
50|$|Novel <b>Algorithmic</b> <b>Methods.</b> Tayur is {{recognized}} for his Operations Research work in developing novel algorithms for models in stochastic inventory theory (using Infinitesimal Perturbation Analysis), integer programming (using Hilbert's Nullstellensatz, Algebraic Geometry, and Gröbner Basis Theory) and chance constrained programs (using random walks and rapidly mixing Markov-chain theory).|$|R
40|$|We {{present a}} {{methodology}} for the verification of temporal properties of hybrid systems. The methodology {{is based on}} the deductive transformation of hybrid diagrams, which represent the system and its properties, and which can be algorithmically checked against the specification. This check either gives a positive answer to the verification problem, or provides guidance for the further transformation of the diagrams. The resulting methodology is complete for quantifier-free lineartime temporal logic. 1 Introduction Specification and verification methodologies for hybrid systems range from <b>algorithmic</b> <b>methods</b> for the verification of linear-time temporal logic properties [2, 1], to deductive approaches for proving linear-time temporal logic properties [11, 7] and interval-based and duration properties [5, 3]. In this paper we present an approach that combines deductive and <b>algorithmic</b> <b>methods</b> into a methodology that is complete (relative to first-order reasoning) for proving linear-tim [...] ...|$|R
40|$|AbstractWe provide <b>algorithmic</b> <b>methods</b> for the {{solution}} of the classification problem of bifurcations by vanishing and non-vanishing derivatives. These methods come from a generalization of standard bases for modules in local rings. We introduce the necessary theory of bifurcations, provide the algorithmic tools, and show the effectiveness of these tools in several examples, including a new one...|$|R
