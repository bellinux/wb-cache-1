21|56|Public
50|$|With {{the rise}} in {{popularity}} of cloud computing, concern has grown over access to cloud-hosted data {{in the event of}} a natural disaster. Amazon Web Services (AWS) has been in the news for major network outages in April 2011 and June 2012. AWS, like other major cloud hosting companies, prepares for typical outages and large-scale natural disasters with backup power as well as backup data centers in other locations. AWS divides the globe into five regions and then splits each region into availability zones. A data center in one <b>availability</b> <b>zone</b> should be backed up by a data center in a different <b>availability</b> <b>zone.</b> Theoretically, a natural disaster would not affect more than one <b>availability</b> <b>zone.</b> This theory plays out as long as human error is not added to the mix. The June 2012 major storm only disabled the primary data center, but human error disabled the secondary and tertiary backups, affecting companies such as Netflix, Pinterest, Reddit, and Instagram.|$|E
50|$|To make EC2 more fault-tolerant, Amazon {{engineered}} Availability Zones {{that are}} designed to be insulated from failures in other availability zones. Availability zones do not share the same infrastructure. Applications running in more than one <b>availability</b> <b>zone</b> can achieve higher availability.|$|E
5000|$|June 2016 - Alibaba Cloud {{expanded}} its data center operations in Singapore {{with the establishment}} of a second <b>availability</b> <b>zone.</b> Alibaba Cloud also achieved two new certifications overseas: Singapore Multi-Tier Cloud Security (MTCS) standard Level 3, and the Payment Card Industry Data Security Standard (PCI-DSS).|$|E
30|$|Amazon {{data centers}} {{are divided into}} several <b>availability</b> <b>zones</b> to {{distribute}} impact of (hardware) failures. For resilience reasons users distribute their data over different <b>availability</b> <b>zones.</b> As {{a result of the}} outage EC 2 customers permanently lost data, although services were hosted on different EC 2 <b>availability</b> <b>zones.</b> A company offering webservice usage monitoring lost 11 hours of historical data [7].|$|R
5000|$|Dealing with {{large-scale}} outages, such as {{failure of}} entire <b>availability</b> <b>zones</b> and regions ...|$|R
5000|$|... #Caption: Map showing Amazon Web Services' <b>availability</b> <b>zones</b> within {{geographic}} regions {{around the}} world.|$|R
50|$|Zettagrid Pty Ltd was {{launched}} in May 2010 by the Applications Development division of ZettaServe Pty Ltd. Zettagrid launched offering VMware vSphere virtual private server hosting from Perth, Western Australia. Nicholas Power was appointed General Manager in June 2010. In June 2011, Nicholas Power stepped down as General Manager and was succeeded by Nicki Pereira. Nicholas Power went on to manage the merger of Highway 1 and Global Dial Pty Ltd into the Zetta Group of companies. In July 2011 Zettagrid launched a virtual data centre service based upon VMware vCloud. In September 2011, Zettagrid expanded establishing a second <b>availability</b> <b>Zone</b> in Sydney, New South Wales. In November 2012, Zettagrid established its third <b>availability</b> <b>zone</b> infrastructure in Melbourne, Victoria. In September 2013, Nicki Pereira was appointed CTO of the Zetta Group and the Nicholas Power was reappointed General Manager and VP of Zettagrid.|$|E
50|$|Multi-Availability Zone {{deployments}} {{are targeted}} for production environments. Multi-AZ deployments aim to provide enhanced availability and data durability for MySQL instances. When a database instance is created or modified {{to run as}} a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different <b>Availability</b> <b>Zone</b> (independent infrastructure in a physically separate location). In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date standby, allowing database operations to resume without administrative intervention.|$|E
3000|$|The {{deployment}} of the streaming cluster {{is the first step}} performed by the setup procedure. 3 Distribute initializes and configures a video streaming cluster in the European <b>availability</b> <b>zone,</b> initially composed of a single origin server and one edge server. In order to put a load on the video streaming cluster a consumer virtual machine is instantiated in the <b>availability</b> <b>zone</b> [...] "us-west- 1 ".|$|E
25|$|As of December 2014, Amazon Web Services {{operated}} 1.4 Million servers across 11 {{regions and}} 28 <b>availability</b> <b>zones.</b>|$|R
50|$|AWS has {{announced}} that 3 new regions (having 7 <b>Availability</b> <b>Zones)</b> will come online throughout 2017 in China, India, and the United Kingdom.|$|R
5000|$|Allows {{customers}} to replicate instances across different Amazon Web Services <b>Availability</b> <b>Zones</b> (Multi-AZ) with instant failover {{in case of}} a disruption to an entire Amazon data center.|$|R
3000|$|... [*]Another {{cluster of}} virtual {{machines}} hosted in the AWS EC 2 <b>availability</b> <b>zone</b> [...] "us-west- 1 ", simulating users consuming the content {{provided by the}} streaming service.|$|E
3000|$|... [*]Virtual {{machines}} hosted in the AWS EC 2 <b>availability</b> <b>zone</b> [...] "eu-west- 1 ", {{implementing a}} streaming service {{provided by a}} streaming cluster (Origin server and edge servers).|$|E
30|$|Location. The {{location}} of the virtual machine down to the smallest unit. The exact nomenclature is cloud dependant but in general terms, this will correspond to a data center, <b>availability</b> <b>zone,</b> region or other abstraction.|$|E
5000|$|Each {{region is}} wholly {{contained}} {{within a single}} country {{and all of its}} data and services stay within the designated region. Each region has multiple [...] "Availability Zones", which consist of one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities. <b>Availability</b> <b>Zones</b> do not automatically provide additional scalability or redundancy within a region, since they are intentionally isolated from each other to prevent outages from spreading between Zones. Several services can operate across <b>Availability</b> <b>Zones</b> (e.g., S3, DynamoDB) while others can be configured to replicate across Zones to spread demand and avoid downtime from failures.|$|R
50|$|Amazon added {{three new}} {{features}} on March 27, 2008, static IP addresses, <b>availability</b> <b>zones,</b> and user selectable kernels.On August 20, 2008, Amazon added Elastic Block Store (EBS)This provides persistent storage, {{a feature that}} had been lacking since the service was introduced.|$|R
50|$|As of December 2014, Amazon Web Services {{operated}} {{an estimated}} 1.4 Million servers across 28 <b>availability</b> <b>zones.</b> The global network of AWS Edge locations consists of 54 {{points of presence}} worldwide, including locations in the United States, Europe, Asia, Australia, and South America.|$|R
30|$|If we need {{to verify}} more than 670 virtual {{machines}} within a day, {{we need to}} build a new AZ (<b>Availability</b> <b>Zone)</b> to verify virtual machines in parallel with existing AZs. Note that hypervisors or storage are not shared in different AZs, parallel verifications can be executed in different AZs.|$|E
30|$|After the {{streaming}} cluster {{has been}} deployed and configured, the stream publisher is manually enabled {{to launch the}} transmission of stereoscopic 3 D live video content to the origin server of the streaming cluster. The consumption of streams is additionally started on the virtual machine in the other <b>availability</b> <b>zone</b> using Flazr. The number of streams to be consumed is initially calculated regarding the bandwidth of the stream and the network output capacity of the edge server.|$|E
30|$|Higher {{fault and}} lineament density {{can be used}} to {{speculate}} secondary porosity as most of the faults or fracture system serves as conduits for movement and storage of groundwater. Krishnamurthy et al. (2000) pointed out that a buffer zone of 300  m around fracture system of faults and lineaments are treated as appropriate groundwater recharge and <b>availability</b> <b>zone.</b> The lineament and fault density map in the study area was categorized into four, i.e., very km 2 area, respectively (Fig.  4 i).|$|E
50|$|Fujitsu has {{launched}} their Cloud Service K5 offering with <b>Availability</b> <b>Zones</b> currently deployed in Japan and the UK, further deployments are progressing across Europe. Cloud Service K5 {{is based on}} open-standard technologies and can be consumed as a Public Cloud, Virtual Private Cloud or a Private Cloud.|$|R
30|$|We tested our {{implementation}} of MCCO in experiment {{where it was}} hosted on a 32 Bit Java Virtual Machine version 1.6 and Glassfish application server on a Windows 7 with an Intel Core 2 Duo 2.5 Ghz CPU and 2 [*]GB of RAM. We configured the MCCO with security credentials required for orchestrating Amazon EC 2 and Amazon S 3 resources hosted in US Oregon, US North California, and US Virginia <b>availability</b> <b>zones.</b> We hosted both media appliances using an Amazon EC 2 large instance across these <b>availability</b> <b>zones.</b> By default, a large instance has the following hardware configuration: 7.5 [*]GB of main memory, 4 EC 2 Compute Unit (i.e. 2 virtual cores with 2 EC 2 Compute Unit), 850 [*]GB of local instance storage, and a 64 -bit platform.|$|R
3000|$|... 1982). Moisture <b>availability</b> <b>zones</b> {{are based}} on the ratio of the {{measured}} average annual rainfall to the calculated average annual evaporation. The area is generally hot, with average temperatures ranging between 23 and 25 °C, having about 10 °C difference between the minimum temperatures in June/July and the maximum temperatures in October/March.|$|R
30|$|However, meshed video {{streaming}} clusters have another limitation: If a {{video streaming}} cluster {{is located in}} just a single <b>availability</b> <b>zone</b> of a cloud, {{it is possible to}} saturate the maximum available networking bandwidth of the provider. Whereas it is possible to support 25 Million streams with a tree height of one, a streaming cluster could theoretically already transmit 125 ∗ 109 streams with a height of two, resulting in a networking output bandwidth of 250 PBit per second. Such a load could only be handled, if the cluster would be operated in a multi cloud environment or across various availability zones.|$|E
30|$|We {{anticipate}} that the cost–estimation tool {{will need a}} formal description language for expressing both the deployment description of the consumer’s application and the provider’s pricing policies. Deployment description will need to include information such as the constituent resources and their connectivities, geographical location of the resources, amount of input and output data, number of users to support and so forth. Pricing policy description will need {{to take into account}} the particularities of the provider, such as for Amazon, there are no charges for VMI to VMI traffic within a single <b>availability</b> <b>zone.</b> Development of such as language is suggested as a topic for further research.|$|E
3000|$|Increased Confidence: There is {{increased}} confidence as a provider {{has a number}} of availability zones responsible for hosting multiple locations globally. As PAR# 3 stated, “Ah, {{but one of the things}} that they’re very good about is that you can’t rely on one <b>availability</b> <b>zone</b> being always available.” In addition, there {{is increased}} confidence due to the very good track record of the service provider. As PAR# 7 stated, “The fact that they’ve got great history doing the same thing over and over again…very well.” This confidence view is also shared by PAR# 18 claiming that, “Increasing the customer’s confidence is attributed to the provider understanding their customer’s business model.” [...]...|$|E
3000|$|... {{exploit the}} {{weaknesses}} in VM placement algorithms {{and the lack}} of location privacy in cloud environments to gain and verify co-location. Using network-based methods, internal IP addresses assigned to instances can be mapped to <b>availability</b> <b>zones</b> and instance types, as described by the case-study for Amazon’s Elastic Compute Cloud (EC 2) [24].|$|R
30|$|The {{deployment}} {{shown in}} Figure 11 involves two Amazon regions (US East and US West) and two <b>availability</b> <b>zones</b> (av–zoneA and av–zoneB) {{located within the}} US West region. The arrowed lines represent bi–directional communication channels. Omitted from the figure are the communication channels used by the client to issue administrative commands to the VMIs (launch, stop, reboot, etc.) and the EBS (create volume, attach volume, etc.).|$|R
50|$|For {{public cloud}} deployments, GlusterFS offers an Amazon Web Services (AWS) Amazon Machine Image (AMI), which is {{deployed}} on Elastic Compute Cloud (EC2) instances rather than physical servers {{and the underlying}} storage is Amazon’s Elastic Block Storage (EBS). In this environment, capacity is scaled by deploying more EBS storage units, performance is scaled by deploying more EC2 instances, and availability is scaled by n-way replication between AWS <b>availability</b> <b>zones.</b>|$|R
30|$|Concerning pricing, in general, Amazon {{charges for}} traffic {{in and out}} (Data Transfer–In and Data transfer–Out respectively) of the Amazon cloud and for traffic {{in and out of}} the EC 2 cloud. However, Amazon does not charge for traffic between a VMI and another {{resource}} (say S 3) located within the same region. Neither do they charge for traffic between two VMIs located within the same <b>availability</b> <b>zone.</b> However, Amazon charges for inter–region traffic between a VMI and another resource (for example, S 3) located within a different region. In these situations, the sender of the data will be charged for Data Transfer–Out whereas the receiver will be charged for Data Transfer–In.|$|E
30|$|In {{the similar}} vain, the charges for V M I 2 will {{take into account}} traffic {{consumption}} and resource consumption. The traffic consumed {{will be determined by}} the amount of Data Transfer–Out and Data Transfer–In sent and received, respectively, along two channels: the channel that leads to the client’s application and the one that leads to V M I 3. There are no charges for traffic consumed on the channel that leads to V M I 1 because the two instances are within the same <b>availability</b> <b>zone.</b> Again, resource consumption will be counted as the number of instance hours of V M I 2. The charges for V M I 3 can be calculated similarly to V M I 2.|$|E
30|$|With these pricing {{policies}} in mind, let us study the charges for V M I 1. Of concern to us here is traffic consumption and resource consumption. V M I 1 {{will be charged}} for inter–region traffic (Data Transfer–In and Data Transfer–Out) consumed on the channel that links it to S 3. In addition, V M I 1 will be charged for traffic (Data Transfer–In and Data Transfer–Out) consumed on the channel that links V M I 1 to the client application, as the latter is outside the Amazon cloud. There are no charges for the traffic consumed by the interaction against E B S 1 as the traffic consumed by the interaction between VMIs and EBSs is free. Neither are there charges for traffic consumed by the interaction against V M I 2 since V M I 1 and V M I 2 share <b>availability</b> <b>zone</b> A. Resource consumption of V M I 1 will be counted {{as the number of}} hours that this instance is run.|$|E
30|$|We {{describe}} the communication architecture {{employed by the}} coordination service in terms of three groupings: cloud, region and sub-region. Cloud and region map neatly to the well established terminology, a cloud is a top level abstraction which includes multiple regions and a region is a geographic area which hosts cloud resources. A sub-region is a further sub division of a region which includes a subset of provisioned cloud resources {{this is similar to}} the notion of (<b>availability)</b> <b>zones</b> but may transcend or overlap actual zones.|$|R
50|$|A BOSH {{deployment}} {{is basically}} a YAML deployment manifest, where the user describes the BOSH releases and BOSH stemcells to use, and how to setup and compose jobs into groups of identical instances (historically misnamed “jobs” and later renamed “instance groups”). Within these “instance groups”, BOSH can span identical instances (VMs or containers) across different <b>availability</b> <b>zones,</b> in order to minimise the risk for all instances to go down at the same time. This is particularly useful when deploying highly available databases or applications.|$|R
40|$|IaaS {{platforms}} such as Amazon EC 2 allow clients {{access to}} massive computational {{power in the}} form of instances. Amazon hosts three different instance purchasing options, each with its own SLA covering pricing and availability. Amazon also offers access to a number of geographical regions, zones, and instance types to select from. In this thesis, the problem of utilizing Spot and On-Demand instances is analyzed and two approaches are presented in order to exploit the cost and performance diversity among different instance types and <b>availability</b> <b>zones,</b> and among the Spot markets they represent. We first develop RAMP, a framework designed to calculate the expected profit of using a specific Spot or On-Demand instance through an evaluation of instance reliability. RAMP is extended to develop RAMC-DC, a framework designed to allocate the most cost effective instance through strategies that facilitate interchangeability of instances among short jobs, reliability of instances among long jobs, and a comparison of the estimated costs of possible allocations. RAMC-DC achieves fault tolerance through comparisons of the price dynamics across instance types and <b>availability</b> <b>zones,</b> and through an examination of three basic checkpointing methods. Evaluations demonstrate that both frameworks take a large step toward low-volatility, high cost-efficiency resource provisioning. While achieving early-termination rates as low as 2. 2...|$|R
