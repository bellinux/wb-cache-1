260|272|Public
5000|$|Hadoop Distributed File System (HDFS) - a {{distributed}} file-system that stores data on commodity machines, providing {{very high}} <b>aggregate</b> <b>bandwidth</b> across the cluster; ...|$|E
5000|$|Striping — Large {{files are}} divided into chunks (up to 64 megabytes) that might be stored on {{different}} chunk servers {{in order to achieve}} higher <b>aggregate</b> <b>bandwidth.</b>|$|E
50|$|In analog modems, {{multiple}} dial-up links over POTS may be bonded. Throughput {{over such}} bonded connections can {{come closer to}} the <b>aggregate</b> <b>bandwidth</b> of the bonded links than can throughput under routing schemes which simply load-balance outgoing network connections over the links.|$|E
40|$|DE 102008009087 A 1 UPAB: 20090928 NOVELTY - The method {{involves}} receiving {{bandwidth requirements}} from terminals (150, 160, 170) by a broadcasting station (140). An <b>aggregated</b> <b>bandwidth</b> requirement {{is determined by}} the broadcasting station. The <b>aggregated</b> <b>bandwidth</b> requirement is sent from the broadcasting station to base stations (110, 120). An <b>aggregated</b> <b>bandwidth</b> authorization is received by the broadcasting station from the base stations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following: (1) a computer program product with a program code for executing a controlling method of a broadcasting station of a portable radio network; and (2) a system consisting of two broadcasting stations. USE - Method for controlling a broadcasting station of a portable radio network (Claimed). ADVANTAGE - The <b>aggregated</b> <b>bandwidth</b> requirement {{is determined by the}} broadcasting station and the <b>aggregated</b> <b>bandwidth</b> requirement is sent from the broadcasting station to base stations, and hence enables to receive and send the data traffic between the base station and one or more terminals wirelessly...|$|R
5000|$|Texture cache. (for <b>aggregating</b> <b>bandwidth</b> from texture memory).|$|R
30|$|Similar {{researches}} on <b>aggregating</b> <b>bandwidth</b> {{of multiple}} interfaces or mobiles’ cooperation to improve efficiency {{have been investigated}} in prior work [9 – 21].|$|R
5000|$|Similar {{capabilities}} as above, but offering 16 X 16Gb FC towards server mezzanine and 8 external. Standard license offers 12 connections {{which can}} be increased by 12 to support all 24 ports. auto-sensing speed 2,4,8 and 16Gb. Total <b>aggregate</b> <b>bandwidth</b> 384 GB ...|$|E
50|$|A major {{drawback}} of the Computing Surface architecture {{was poor}} I/O bandwidth for general data shuffling. Although <b>aggregate</b> <b>bandwidth</b> for special case data shuffling {{could be very}} high, the general case has very poor performance relative to the compute bandwidth. This made the Meiko Computing Surface uneconomic for many applications.|$|E
50|$|In 2014, a {{group of}} {{researchers}} described an implementation of a communication link over 8 millimetre-wave channels multiplexed {{using a combination of}} OAM and polarization-mode multiplexing to achieve an <b>aggregate</b> <b>bandwidth</b> of 32 Gbit/s over a distance of 2.5 metres. These results agree well with predictions about severely limited distances made by Edfors et al.|$|E
40|$|ABSTRACT: We present {{test results}} and {{characterization}} of a data transmission {{system based on}} a last generation FPGA and a commercial QSFP+ (Quad Small Form Pluggable +) module. QSFP+ standard defines a hot-pluggable transceiver available in copper or optical cable assemblies for an <b>aggregated</b> <b>bandwidth</b> of up to 40 Gbps. We implemented a complete testbench based on a commercial development card mounting an Altera Stratix IV FPGA with 24 serial transceivers at 8. 5 Gbps, together with a custom mezzanine hosting three QSFP+ modules. We present test results and signal integrity measurements up to an <b>aggregated</b> <b>bandwidth</b> of 12 Gbps...|$|R
50|$|SPI-4 is an {{interface}} for packet {{and cell}} transfer between a physical layer (PHY) device and a link layer device, for <b>aggregate</b> <b>bandwidths</b> of OC-192 Asynchronous Transfer Mode(ATM) and Packet over SONET/SDH (POS), {{as well as}} 10 Gigabit Ethernet applications.|$|R
40|$|We present {{test results}} and {{characterization}} of a data transmission {{system based on}} a last generation FPGA and a commercial QSFP+ (Quad Small Form Pluggable +) module. QSFP+ standard defines a hot-pluggable transceiver available in copper or optical cable assemblies for an <b>aggregated</b> <b>bandwidth</b> of up to 40 Gbps. We implemented a complete testbench based on a commercial development card mounting an Altera Stratix IV FPGA with 24 serial transceivers at 8. 5 Gbps, together with a custom mezzanine hosting three QSFP+ modules. We present test results and signal integrity measurements up to an <b>aggregated</b> <b>bandwidth</b> of 12 Gbps. Comment: 5 pages, 3 figures, Published on JINST Journal of Instrumentation proceedings of Topical Workshop on Electronics for Particle Physics 2010, 20 - 24 September 2010, Aachen, Germany(R Ammendola et al 2010 JINST 5 C 12019...|$|R
50|$|In 2009, the {{university}} upgraded residential Internet bandwidth {{by more than}} two-fold, to 100 Mbit/s. During that year, a new connection to the Washington State K-20 Educational Network was also installed, bringing {{the university}}'s <b>aggregate</b> <b>bandwidth</b> to 150 Mbit/s. Further bandwidth upgrades have brought student bandwidth to 250 Mbit/s for the 2012-13 academic year.|$|E
5000|$|SFI-5 or SerDes Framer Interface Level 5, a {{standardized}} Electrical Interface by the OIF for connecting a SONET Framer component to an optical SerDes for OC-768, about 40 Gbit/s. [...] Electrically, {{it consists of}} 16 pairs of SerDes channels each running at 3.125 Gbit/s which gives an <b>aggregate</b> <b>bandwidth</b> of 50 Gbit/s accommodating up to 25% of Forward Error Correction ...|$|E
50|$|The Type 1 Communication Scanner was {{an entry}} level device which {{presented}} an interrupt on every received bit. Transmission also required an interrupt for every bit. In theory {{this would have}} allowed for rather imaginative uses such as Morse Code and connection to devices with unusual framing methods. A maximum of 64 half-duplex lines could be attached. The <b>aggregate</b> <b>bandwidth</b> was restricted due to the heavy processing requirements.|$|E
5000|$|<b>Aggregate</b> network <b>bandwidth</b> is bottlenecked by {{the weakest}} link between two nodes.|$|R
30|$|The uplink {{protocol}} data units (PDUs) {{have two}} statuses: transmission-preparing (tp) and transmission-ready (tr). The incoming uplink data are {{packed into the}} PDUs in tp status first. Once SS requests bandwidth for a BRU, the BR message takes the <b>aggregated</b> <b>bandwidth</b> requirement for all its PDUs to BS, and the PDUs of the BRU in tp status are transited to tr status accordingly.|$|R
50|$|Within {{the range}} of system development, LTE-Advanced and WiMAX 2 can use up to 8x8 MIMO and 128 QAM in {{downlink}} direction. Example performance: 100 MHz <b>aggregated</b> <b>bandwidth,</b> LTE-Advanced provides almost 3.3 Gbit peak download rates per sector of the base station under ideal conditions. Advanced network architectures combined with distributed and collaborative smart antenna technologies provide several years road map of commercial enhancements.|$|R
50|$|One way {{to solve}} the {{scalability}} problem is by using a multi-level approach, where per-microflow resource reservation (i.e. resource reservation for individual users) {{is done in the}} edge network, while in the core network resources are reserved for aggregate flows only. The routers that lie between these different levels must adjust the amount of <b>aggregate</b> <b>bandwidth</b> reserved from the core network so that the reservation requests for individual flows from the edge network can be better satisfied.|$|E
50|$|The {{processor}} has two unidirectional 32-bit double {{data rate}} (DDR) buses (one for reads, {{the other for}} writes) to the system controller chip (northbridge) running at {{one quarter of the}} processor core speed. The buses also carry addresses and control signals in addition to data so only a percentage of the peak bandwidth can be realized (6.4 GB/s at 450 MHz). As the buses are unidirectional, each direction can realize only half the <b>aggregate</b> <b>bandwidth,</b> or 3.2 GB/s.|$|E
5000|$|In {{digital audio}} and video broadcasting, for example, a {{statistical}} multiplexer is a content aggregating device that allows broadcasters to provide {{the greatest number of}} audio or video services for a given bandwidth by sharing a pool of fixed bandwidth among multiple services or streams of varying bitrates. The multiplexer allocates to each service the bandwidth required for its real-time needs so that services with complex scenes receive more bandwidth than services with less complex ones. This bandwidth sharing technique produces the best video quality at the lowest possible <b>aggregate</b> <b>bandwidth.</b>|$|E
40|$|The 3 G and LTE {{technologies}} made video on-demand {{a popular}} entertainment for users on the go. However, bandwidth insufficiency is an obstacle in providing high quality and smooth video playout in cellular networks. The {{objective of the}} proposed PhD research {{is to provide a}} user with high quality video streaming with minimal stalling time by <b>aggregating</b> <b>bandwidth</b> from ubiquitous nearby devices that may be using different radio networks...|$|R
40|$|International audienceIn an {{environment}} where IEEE 802. 16 access networks are used as a backbone for a heterogeneous wireless access network, the question of Quality of Service brings along a questioning on several other aspects: scalability, guarantees, scheduling among others. We give in this paper a general solution answering those questions. This solution {{is based on an}} <b>aggregated</b> <b>bandwidth</b> management in the backbone. This aggregation along with specific bandwidth request and scheduling policies are herein specified and discussed...|$|R
40|$|An OTDM router {{architecture}} using {{a highly}} scalable time slot tuner is discussed. Results for a 100 -Gb/s, 16 -channel router using a computer-controlled interface are presented. The scalability and latency of the router {{based upon the}} time slot tuner shows that <b>aggregate</b> <b>bandwidths</b> beyond 1 Tb/s are possible. We show that a maximum hardware time slot access latency of less than 3. 2 ns can be achieved with this architecture enabling ultrafast optical packet routing...|$|R
50|$|It was a {{premise of}} the Cosmic Cube {{experiment}} that the internode communication should scale well to very large numbers of nodes. A direct network like the hypercube satisfies this requirement, with respect to both the <b>aggregate</b> <b>bandwidth</b> achieved across the many concurrent communication channels and the feasibility of the implementation. The hypercube is actually a distributed variant of an indirect logarithmic switching network like the Omega or banyan networks: the kind that might be used in shared-storage organizations. With the hypercube, however, communication paths traverse different numbers of channels and so exhibit different latencies. It is possible, therefore, {{to take advantage of}} communication locality in placing processes in nodes.|$|E
50|$|When {{a twisted}} pair or fiber link segment is used and neither end is {{connected}} to a repeater, full-duplex Ethernet becomes possible over that segment. In full-duplex mode, both devices can transmit and receive to and from {{each other at the}} same time, and there is no collision domain. This doubles the <b>aggregate</b> <b>bandwidth</b> of the link and is sometimes advertised as double the link speed (for example, 200 Mbit/s). The elimination of the collision domain for these connections also means that all the link's bandwidth can be used by the two devices on that segment and that segment length is not limited by the need for correct collision detection.|$|E
50|$|HP c7000 {{enclosure}} {{was announced}} in June 2006. In 2007 there was a minor update including larger Onboard Administrator display (3 inches, up from 2 inches). The next update was in 2009 and brought RoHS compatibility, increased backplane speed (5Tbit/s, up from 4Tbit/s) and 1Gbit/s Onboard Administrator connectivity. Fourth version - c7000 Platinum {{was announced in}} February 2013. It features location discovery services, thermal discovery services and redesigned backplane. The new backplane increased <b>aggregate</b> <b>bandwidth</b> 40% from 5 to 7 Tbit/s to allow use newest high-speed interconnect modules (such as 16Gbit/s FC and 56Gbit/s FDR InfiniBand). Also the new Platinum Plus rating power supplies were announced with higher efficiency than previous Gold Plus rating power supplies.|$|E
40|$|Dramatic {{increases}} in processing power have rapidly scaled on-chip <b>aggregate</b> <b>bandwidths</b> into the Tb/s range. This necessitates a corresponding {{increase in the}} amount of data communicated between chips, so as not to limit overall system performance. To meet the increasing demand for interchip communication bandwidth, researchers are investigating the use of high-speed optical interconnect architectures. Unlike their electrical counterparts, optical interconnects offer high bandwidth and negligible frequency-dependent loss, making possible per-channel data rates of more than 10 Gb/s. High-Spee...|$|R
40|$|A concurrent, tunable, tri-band, {{single chain}} radio {{receiver}} for 5 G radio access networks is evaluated. The three concurrent bands are independently tunable over a frequency range from 600 MHz to 2. 7 GHz. A hardware-in-the-loop test-bed provides a system level {{evaluation of the}} proposed receiver using direct RF digitization. The test-bed emulates a 5 G heterogeneous network supporting three wideband, simultaneous connections. By measuring the receiver EVM, we demonstrate sufficient isolation between concurrent bands achieving 60 MHz of <b>aggregated</b> <b>bandwidth</b> as well as strong resilience to adjacent blockers...|$|R
40|$|System {{performance}} scaling imposes {{an increase of}} package-to-package <b>aggregate</b> <b>bandwidths</b> to interface chips in high performance computing. This scaling is expected to encounter several I/O bottlenecks (pin count, speed, power consumption) when implemented in the electrical domain. Several optical interface technologies are being proposed among which silicon photonics, considered as a promising candidate. In this paper we will review the recent progress made in this technology that may enable multi-channel WDM links for package-to-package interconnects: 1. 0 V drivers with microring modulators and compact manufacturable microring filters with efficient thermal tuning...|$|R
50|$|Most QoS {{approaches}} tend {{to focus}} on only one QoS parameter (e.g., packet loss, end-to-end delay, and bandwidth). For example, while many of the QoS-related schemes are successful in reducing packet loss by adding redundancy in the packet, they do this at the expense of end-to-end delay. Because packet loss and end-to-end delay are inversely related, it may not be possible to find a path that simultaneously satisfies the delay, packet loss, and bandwidth constraints. Some proposed QoS routing algorithms do consider multiple metrics, but without considering cross-layer cooperation. Multipath routing is another type of QoS routing that has received much attention, since it can provide load balancing, fault tolerance, and higher <b>aggregate</b> <b>bandwidth.</b> Although this approach decreases packet loss and end-to-end delay, it is only efficient and reliable if a relationship can be found between the number of paths and QoS constraints.|$|E
5000|$|To help {{alleviate}} this problem, Sincoskie invented VLANs {{by adding a}} tag to each Ethernet frame. These tags could {{be thought of as}} colors, say red, green, or blue. Then each switch could be assigned to handle frames of a single color, and ignore the rest. The networks could be interconnected with three spanning trees, one for each color. By sending a mix of different frame colors, the <b>aggregate</b> <b>bandwidth</b> could be improved. Sincoskie referred to this as a multitree bridge. He and Chase Cotton created and refined the algorithms necessary to make the system feasible. This [...] "color" [...] is what is now known in the Ethernet frame as the IEEE 802.1Q header, or the VLAN tag. While VLANs are commonly used in modern Ethernet networks, using them for the original purpose would be rather unusual.|$|E
50|$|Using the 5 GHz band gives 802.11a a {{significant}} advantage, since the 2.4 GHz band is heavily {{used to the}} point of being crowded. Degradation caused by such conflicts can cause frequent dropped connections and degradation of service. However, this high carrier frequency also brings a slight disadvantage: The effective overall range of 802.11a is slightly less than that of 802.11b/g; 802.11a signals cannot penetrate as far as those for 802.11b because they are absorbed more readily by walls and other solid objects in their path and because the path loss in signal strength is proportional to the square of the signal frequency. On the other hand, OFDM has fundamental propagation advantages when in a high multipath environment, such as an indoor office, and the higher frequencies enable the building of smaller antennas with higher RF system gain which counteract the disadvantage of a higher band of operation. The increased number of usable channels (4 to 8 times as many in FCC countries) and the near absence of other interfering systems (microwave ovens, cordless phones, baby monitors) give 802.11{{a significant}} <b>aggregate</b> <b>bandwidth</b> and reliability advantages over 802.11b/g.|$|E
40|$|We study {{rate control}} of {{aggregated}} TCP connections, i. e., multiple TCP connections {{treated as a}} single aggregate for purposes of rate control, via traffic conditioning mechanisms such as traffic policing and shaping. This is a likely scenario given the current trends in policy-based service differentiation on the Internet (e. g., Differentiated Services) and content aggregation on the Web (e. g., virtual hosting). Traffic aggregation is increasingly necessary for cost-effective, scalable provisioning and management of network and server resources. We propose and evaluate a set of mechanisms for fair sharing of an <b>aggregate’s</b> allocated <b>bandwidth</b> between connections comprising the aggregate, for traffic conditioning via policing with marking and shaping. We propose logical token buckets (common token bucket with logical partitions) that account for the round-trip times of individual connections to provide fair bandwidth sharing while achieving high <b>aggregate</b> throughput and <b>bandwidth</b> utilization. We propose modifications to TCP’s congestion window increase during congestion avoidance to achieve fairness between short and long-lived connections, and introduce the notion of aggregate fairness. We demonstrate that the proposed mechanisms provide {{a high degree of}} fairness and bandwidth utilization while limiting an <b>aggregate’s</b> <b>bandwidth</b> usage to the desired rate. We also describe the key protocol stack extensions for our AIX-based prototype implementation to enable efficient rate control of TCP connection aggregates. ...|$|R
40|$|Abstract. In this paper, {{we propose}} a simple {{scheduler}} called SBQ (Service-Based Queuing) {{to share the}} bandwidth fairly between unicast and multicast flows {{according to a new}} definition of fairness referred as the inter-service fairness. We utilize our recently proposed active queue management mechanism MFQ (Multicast Fair Queuing) to fairly share the bandwidth among all competing flows in the multicast queue. The simulation results obtained for very heterogeneous sources and links characteristics suggest that, on the one hand, our scheduler achieves the expected <b>aggregated</b> <b>bandwidth</b> sharing among unicast and multicast service, {{and on the other hand}} the multicast flows remain TCP-friendly. ...|$|R
5|$|Thunderbolt {{combines}} PCI Express and Mini DisplayPort {{into a new}} {{serial data}} interface. Original Thunderbolt implementations have two channels, each with a transfer speed of 10Gbit/s, resulting in an <b>aggregate</b> unidirectional <b>bandwidth</b> of 20Gbit/s.|$|R
