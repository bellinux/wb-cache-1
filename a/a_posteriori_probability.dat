781|10000|Public
50|$|Alternatively, (1) can {{be derived}} from an {{analysis}} of <b>a</b> <b>posteriori</b> <b>probability</b> distributions(beta distributions).|$|E
5000|$|... and {{delivers}} it to [...] [...] {{is called the}} logarithm of the likelihood ratio (LLR). [...] is the <b>a</b> <b>posteriori</b> <b>probability</b> (APP) of the [...] data bit which shows the probability of interpreting a received [...] bit as [...] Taking the LLR into account, [...] yields a hard decision; i.e., a decoded bit.|$|E
5000|$|The {{ultimate}} {{objective of}} this model is to classify images into classes [...] "object present" [...] (class [...] ) and [...] "object absent" [...] (class [...] ) given the observation [...] To accomplish this, Weber & Welling run part detectors from the learning step exhaustively over the image, examining different combinations of detections. If occlusion is considered, then combinations with missing detections are also permitted. The goal is then to select the class with maximum <b>a</b> <b>posteriori</b> <b>probability,</b> by considering the ratio ...|$|E
40|$|AbstractThis paper {{analyses}} the {{asymptotic behaviour}} {{of the so-called}} <b>a</b> <b>posteriori</b> <b>probabilities</b> in the Multiple Model method when applied for change detection of autoregressive processes. The convergence properties of <b>a</b> <b>posteriori</b> <b>probabilities</b> are deduced by application of martingale convergence theorems...|$|R
40|$|Bayesian {{estimation}} {{is often}} applied in pattern recognition problems. We formulate estimation errors of <b>a</b> <b>posteriori</b> Bayesian <b>probabilities</b> to be propagated from observation. Next, we apply {{the scheme of}} the formulation to a practical image recognition problem : based on <b>a</b> <b>posteriori</b> <b>probabilities,</b> sectionalized regions in outdoor-scene images are classified into five categories of landform elements, i, e., asphalt, concrete, sand/soil, gravel, and grass. The errors originate from RGB pixel values, and propagate to the <b>a</b> <b>posteriori</b> <b>probabilities</b> via intermediary HIS color measures within a region. We concretely clarify a mechanism of the propagation for all steps, and show an effectiveness of the scheme by adducing changeovers between <b>a</b> <b>posteriori</b> <b>probabilities</b> with two kinds of landform elements...|$|R
30|$|These <b>a</b> <b>posteriori</b> symbol <b>probabilities</b> are {{provided}} to the MAP algorithms as a priori information, as seen in (5). At the last iteration, the final decoded outputs are the hard decisions based on the <b>a</b> <b>posteriori</b> <b>probabilities.</b>|$|R
50|$|In Bayesian statistics, {{a maximum}} <b>a</b> <b>posteriori</b> <b>probability</b> (MAP) {{estimate}} is {{an estimate of}} an unknown quantity, that equals the mode of the posterior distribution. The MAP {{can be used to}} obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation.|$|E
50|$|Claude Shannon proved, using {{information}} theory considerations, that the one-time pad has a property he termed perfect secrecy; that is, the ciphertext C gives absolutely no {{additional information about}} the plaintext. This is because, given a truly random key which is used only once, a ciphertext can be translated into any plaintext of the same length, and all are equally likely. Thus, the a priori probability of a plaintext message M {{is the same as}} the <b>a</b> <b>posteriori</b> <b>probability</b> of a plaintext message M given the corresponding ciphertext. Mathematically, this is expressed as H(M)=H(M|C), where H(M) is the entropy of the plaintext and H(M|C) is the conditional entropy of the plaintext given the ciphertext C. Perfect secrecy is a strong notion of cryptanalytic difficulty.|$|E
5000|$|In coding theory, a zigzag code {{is a type}} {{of linear}} error-correcting code {{introduced}} by [...] They are defined by partitioning the input data into segments of fixed size, and adding sequence of check bits to the data, where each check bit is the exclusive or of the bits in a single segment and of the previous check bit in the sequence. The code rate is high: [...] where [...] is the number of bits per segment. Its worst-case ability to correct transmission errors is very limited: in the worst case it can only detect a single bit error and cannot correct any errors. However, it works better in the soft-decision model of decoding: its regular structure allows the task of finding a maximum-likelihood decoding or <b>a</b> <b>posteriori</b> <b>probability</b> decoding to be performed in constant time per input bit.|$|E
3000|$|... gOptimal {{decoding}} algorithm uses <b>a</b> <b>posteriori</b> <b>probabilities</b> for {{the entire}} sequence of data transmitted.|$|R
3000|$|... {{where the}} <b>a</b> <b>posteriori</b> <b>probabilities</b> are {{conditioned}} on the extrinsic {{information from the}} SISO detector in the last iteration, [...]...|$|R
40|$|In {{this work}} {{we try to}} {{estimate}} <b>a</b> <b>posteriori</b> <b>probabilities</b> needed for speech recognition by the K-Nearest-Neighbors rule (KNN), using a Multi-Layered Perceptrom (MLP) to obtain the distances between neighbors. Thus, we can distinguish two different works: on the one hand, we es-timate <b>a</b> <b>posteriori</b> <b>probabilities</b> using KNN {{with the aim of}} using them in a speech recognition system [1][2]; and, on the other hand, we propose a new distance measure, MLP-distance. 1...|$|R
5000|$|... where Y is {{the side}} {{information}}, X is {{the outcome of}} the betable event, and I is the state of the bookmaker's knowledge. This is the average Kullback-Leibler divergence, or information gain, of the <b>a</b> <b>posteriori</b> <b>probability</b> distribution of X given the value of Y relative to the a priori distribution, or stated odds, on X. Notice that the expectation is taken over Y rather than X: we need to evaluate how accurate, in the long term, our side information Y is before we start betting real money on X. This is a straightforward application of Bayesian inference. Note that the side information Y might affect not just our knowledge of the event X but also the event itself. For example, Y might be a horse that had too many oats or not enough water. The same mathematics applies in this case, because from the bookmaker's point of view, the occasional race fixing is already taken into account when he makes his odds.|$|E
30|$|At the end, BayesOC {{chooses the}} class {{with the largest}} <b>a</b> <b>posteriori</b> <b>probability.</b>|$|E
40|$|In this paper, a {{new method}} of {{composing}} a multiclass classifier using pairwise classifiers is proposed. A “Resemblance Model ” is exploited to calculate <b>a</b> <b>posteriori</b> <b>probability</b> for combining pairwise classifiers. We proved {{the validity of}} this model by using approximation of <b>a</b> <b>posteriori</b> <b>probability</b> formula. Using this theory, we can obtain the optimal decision. An experimental result of handwritten numeral recognition is presented, supporting the effectiveness of our method. 1...|$|E
3000|$|...], n∈ [1,Q], {{represent}} approximations of the <b>a</b> <b>posteriori</b> <b>probabilities</b> (APPs) of the symbols. These {{values are}} further {{passed to the}} MF part.|$|R
40|$|The {{calculation}} method of membership functions of <b>a</b> <b>posteriori</b> <b>probabilities</b> {{of the states}} of the controlled object is offered for fuzzy Bayes expert system. ?????????? ???????? ??????? ??????? ?????????????? ????????????? ???????????? ????????? ??????????????? ??????? ??? ???????? ????????? ?????????? ???????...|$|R
40|$|This paper 1 {{proposes a}} method to extract {{nonlinear}} discriminant features from given input measurements by using outputs of multilayer Perceptron (MLP). Linear Discriminant Analysis (LDA) {{is one of the}} best known methods to construct linear features which are suitable for class discrimination. Otsu showed that LDA can be extended to nonlinear if we can estimate Bayesian <b>a</b> <b>posteriori</b> <b>probabilities.</b> Recently, MLP have been successfully applied to many kinds of pattern recognition problems. It is also regarded that outputs of MLP trained for pattern classification approximate Bayesian <b>a</b> <b>posteriori</b> <b>probabilities.</b> Thus we can construct nonlinear discriminant features that maximizes the discriminant criterion by using outputs of MLP as the estimates of Bayesian <b>a</b> <b>posteriori</b> <b>probabilities.</b> 1 Introduction Feature extraction has been recognized as one of the most important problems in pattern recognition. We often want to extract useful information for class discrimination from given K clas [...] ...|$|R
30|$|The {{a priori}} {{probability}} is calculated from the last <b>a</b> <b>posteriori</b> <b>probability</b> using a probabilistic process (state transition) model f(⋅).|$|E
3000|$|Maximum {{likelihood}} detection - As with MLSE, {{signal detection}} method based on maximum <b>a</b> <b>posteriori</b> <b>probability</b> {{used in the}} MIMO scheme [...]...|$|E
3000|$|The {{goal is to}} {{determine}} the <b>a</b> <b>posteriori</b> <b>probability</b> density function (PDF) of every possible channel characterization given all channel observations: [...]...|$|E
3000|$|..., {{computed}} at {{a previous}} iteration, two steps are performed. In the first one, that is, Estimation step, we will compute the different <b>a</b> <b>posteriori</b> <b>probabilities</b> (APP): [...]...|$|R
40|$|Abstract — We study a list-sequential (LISS) {{multiuser}} detector for coded {{applications in}} a turbo scheme. Optimal {{with respect to}} the bit error rate within that iterative scheme would be an APP detector that calculates the <b>a</b> <b>posteriori</b> <b>probabilities.</b> Unfortunately, this detector suffers from a prohibitively high computational complexity for a large number of users. Therefore, we apply a sequential algorithm operating on a tree that approximates the <b>a</b> <b>posteriori</b> <b>probabilities</b> with reduced complexity. The tradeoff between performance and computational burden can be controlled {{by the size of the}} available memory. I...|$|R
30|$|SPA is a soft {{decision}} algorithm that calculates the a priori probabilities of the received code bits and uses <b>a</b> <b>posteriori</b> <b>probabilities</b> for decoding operation. These probabilities {{are known as}} log-likelihood ratios.|$|R
3000|$|The <b>a</b> <b>posteriori</b> <b>probability</b> is {{calculated}} from the a priori probability using a probabilistic measurement model h(⋅) and the current measurement z [...]...|$|E
3000|$|Maximum {{likelihood}} sequence estimation - Signal {{detection method}} based on maximum <b>a</b> <b>posteriori</b> <b>probability.</b> The calculation complexity becomes huge {{in exchange for}} high detection accuracy [...]...|$|E
40|$|This paper {{introduces}} a novel way {{to compute the}} membership function of a fuzzy set approximating the distribution of some observed data starting with their histogram. This membership function is in turn used to obtain <b>a</b> <b>posteriori</b> <b>probability</b> through a suitable version of the Bayesian formula. The ordering imposed by an  overtaking relation between fuzzy numbers translates immediately into a dominance of the <b>a</b> <b>posteriori</b> <b>probability</b> of a class over another for a given observed value. In this way a crisp classification is eventually obtained...|$|E
40|$|In pattern {{recognition}} systems, Chow's rule {{is commonly used}} to reach a trade-off between error and reject probabilities. In this paper, we investigate the effects of estimate errors affecting the <b>a</b> <b>posteriori</b> <b>probabilities</b> on the optimality of Chow's rule. We show that the optimal error-reject tradeoff is not provided by Chow's rule if the <b>a</b> <b>posteriori</b> <b>probabilities</b> are affected by errors. The use of multiple reject thresholds related to the data classes is then proposed. The authors have proved in another work that the reject rule based on such thresholds provides a better error-reject trade-off than in Chow's rule...|$|R
40|$|The {{estimation}} {{and tracking}} of the fractional carrier frequency offset (CFO) {{is a crucial}} issue {{in the implementation of}} orthogonal frequency division multiplexing (OFDM) systems. In this contribution, we present a novel code-aided fractional CFO estimation algorithm based on the Expectation-Maximization (EM) algorithm. The proposed algorithm exchanges soft information from the channel decoder, in the form of <b>a</b> <b>posteriori</b> <b>probabilities</b> of the coded symbols, between the demapper, the decoder, and the CFO estimator in an iterative way. The proposed estimation scheme can work with any detector as long as the detector is able to compute the <b>a</b> <b>posteriori</b> <b>probabilities</b> (APPs) of the data symbols...|$|R
40|$|This paper sheds a {{new light}} on the gap between a priori and <b>a</b> <b>posteriori</b> <b>probabilities</b> by concentrating on the {{evolution}} of the mathematical concept. It identifies the illegitimate use of Bernoulli’s law of large numbers as the probabilists’ original sin. The resulting confusion on the mathematical foundation for statistical inference was detrimental to Laplace’s definition of probability in terms of equi-possible outcomes as well as to von Mises’ frequentist approach. On the opposite, Kolmogorov’s analytical axiomatization of probability theory enables a priori and <b>a</b> <b>posteriori</b> <b>probabilities</b> to relate to each other without contradiction, allowing a consistent mathematical specification of the dual nature of probability. Therefore, only in Kolmorogorov’s formalism is statistical inference rigorously framed. info:eu-repo/semantics/publishe...|$|R
40|$|This paper {{proposes a}} Markov random field (MRF) model-based method for {{unsupervised}} segmentation of multispectral images consisting of multiple textures. To model such textured images, a hierarchical MRF is used with two layers, the first layer representing an unobservable region {{image and the}} second layer representing multiple textures which cover each region. This method uses the Expectation and Maximization (EM) method for model parameter estimation, where in order to overcome the well-noticed computational problem in the expectation step, we approximate the Baum function using mean-field-based decomposition of <b>a</b> <b>posteriori</b> <b>probability.</b> Given provisionally estimated parameters at each iteration in the EM method, a provisional segmentation is carried out using local <b>a</b> <b>posteriori</b> <b>probability</b> (LAP) of each pixel's region label, which is derived by mean-field-based decomposition of <b>a</b> <b>posteriori</b> <b>probability</b> of the whole region image. Experiments show {{that the use of}} LAPs is essential to perform a good image segmentation...|$|E
3000|$|... [...]). The {{values of}} the output layer {{can be viewed as}} the <b>a</b> <b>posteriori</b> <b>probability</b> of {{selecting}} the different user intention given the current situation of the dialogue.|$|E
3000|$|... {{lies on the}} {{position}} xt- 1 at the previous time interval. Thus, {{the resolution of the}} location of blind nodes can be converted into <b>a</b> <b>posteriori</b> <b>probability</b> density function p(x [...]...|$|E
40|$|The {{process of}} turbo-code {{decoding}} {{starts with the}} formation of <b>a</b> <b>posteriori</b> <b>probabilities</b> (APPs) for each data bit, which is followed by choosing the data-bit value that corresponds to the maximum <b>a</b> <b>posteriori</b> (MAP) <b>probability</b> for that data bit. Upon reception of a corrupted code-bit sequence, the process of decision making with APPs allows the MAP algorithm to determine the most likely information bit to have been transmitted at each bit time. Comment: 8 pages, exposed on 4 th International Conferences "Actualities and Perspectives on Hardware and Software" - APHS 2007, Timisoara, Romani...|$|R
30|$|EM for {{channel and}} noise {{variance}} estimation: In TnDFnD channels, EM-based joint channel and noise variance estimation algorithms are proposed in [14 – 16]. However, data detection is obtained by an extra ML estimator and <b>a</b> maximizing <b>a</b> <b>posteriori</b> <b>probabilities</b> (APP) detector in [14, 15], respectively.|$|R
3000|$|For encoding, the Tanner graph is {{read from}} left to right. For decoding, the Tanner graph is read {{backward}} from right to left. The well-known principle is to exchange <b>a</b> <b>posteriori</b> <b>probabilities</b> between information and check nodes until the probabilities converge after several iterations and we could decode: m [...]...|$|R
