205|100|Public
25|$|By {{measuring}} an infant's {{attention to}} certain <b>audiovisual</b> <b>stimuli,</b> a response {{that is consistent}} with the McGurk effect can be recorded. From just minutes to a couple of days old, infants can imitate adult facial movements, and within weeks of birth, infants can recognize lip movements and speech sounds. At this point, the integration of audio and visual information can happen, but not at a proficient level. The first evidence of the McGurk effect can be seen at four months of age; however, more evidence is found for 5-month-olds. Through the process of habituating an infant to a certain stimulus and then changing the stimulus (or part of it, such as ba-voiced/va-visual to da-voiced/va-visual), a response that simulates the McGurk effect becomes apparent. The strength of the McGurk effect displays a developmental pattern that increases throughout childhood and extends into adulthood.|$|E
25|$|This {{effect may}} be {{experienced}} when {{a video of}} one phoneme's production is dubbed with a sound-recording of a different phoneme being spoken. Often, the perceived phoneme is a third, intermediate phoneme. As an example, the syllables /ba-ba/ are spoken over the lip movements of /ga-ga/, and the perception is of /da-da/. McGurk and MacDonald originally believed that this resulted from the common phonetic and visual properties of /b/ and /g/. Two types of illusion in response to incongruent <b>audiovisual</b> <b>stimuli</b> have been observed: fusions ('ba' auditory and 'ga' visual produce 'da') and combinations ('ga' auditory and 'ba' visual produce 'bga'). This is the brain's effort to provide the consciousness with its best guess about the incoming information. The information coming from the eyes and ears is contradictory, and in this instance, the eyes (visual information) have had a greater effect on the brain and thus the fusion and combination responses have been created.|$|E
50|$|MovAlyzeR can be {{customized}} {{for many}} different pen-movement tests, including goal-directed movements, drawing and handwriting up {{to a full}} page of text. It can also process scanned handwriting images for use in, e.g., forensic document examination. Immediately after each trial, consistency with the required pen-movement task is verified so that the user can decide to correct or redo a trial. MovAlyzeR can generate animated <b>audiovisual</b> <b>stimuli</b> which can be edited using its Stimulus Editor.|$|E
50|$|Another {{theory that}} has been used in the {{explanation}} of the Colavita effect is the ‘Failure of Binding’. This theory suggests that participants bind together the visual and auditory components of an <b>audiovisual</b> <b>stimulus,</b> which can hinder the processing of the auditory component of the <b>audiovisual</b> <b>stimulus.</b> This occurs because the visual component alone provides enough information about the <b>audiovisual</b> <b>stimulus.</b> This theory only applies when the visual and auditory stimuli presented are congruent. When they are incongruent, the visual component is not an accurate representation of the auditory component. In this case, incongruency can act as a cue to inform participants that a bimodal target occurred.|$|R
40|$|This study aims {{to further}} examine the {{cross-cultural}} differences in multisensory emotion perception between Western and East Asian people. In this study, we recorded the <b>audiovisual</b> <b>stimulus</b> video of Japanese and Dutch actors saying neutral phrase {{with one of}} the basic emotions. Then we conducted a validation experiment of the stimuli. In the first part (facial expression), participants watched a silent video of actors and judged what kind of emotion the actor is expressing by choosing among 6 options (ie, happiness, anger, disgust, sadness, surprise, and fear). In the second part (vocal expression), they listened to the audio part of the same videos without video images while the task was the same. We analyzed their categorization responses based on accuracy and confusion matrix and created a controlled <b>audiovisual</b> <b>stimulus</b> set...|$|R
40|$|Emotional {{communication}} uses {{verbal and}} nonverbal means. In case of conflicting signals, nonverbal information is assumed to have a stronger impact. It is unclear, however, whether perceptual nonverbal dominance varies between individuals and whether it is linked to emotional intelligence. Using <b>audiovisual</b> <b>stimulus</b> material comprising {{verbal and nonverbal}} emotional cues that were varied independently, perceptual nonverbal dominance profiles and their relations to emotional intelligence were examined. Nonverbal dominance was found in every participant, ranging from 55 to 100...|$|R
50|$|By {{measuring}} an infant's {{attention to}} certain <b>audiovisual</b> <b>stimuli,</b> a response {{that is consistent}} with the McGurk effect can be recorded. From just minutes to a couple of days old, infants can imitate adult facial movements, and within weeks of birth, infants can recognize lip movements and speech sounds. At this point, the integration of audio and visual information can happen, but not at a proficient level. The first evidence of the McGurk effect can be seen at four months of age; however, more evidence is found for 5-month-olds. Through the process of habituating an infant to a certain stimulus and then changing the stimulus (or part of it, such as ba-voiced/va-visual to da-voiced/va-visual), a response that simulates the McGurk effect becomes apparent. The strength of the McGurk effect displays a developmental pattern that increases throughout childhood and extends into adulthood.|$|E
50|$|In addition, a study {{conducted}} by Laurienti and colleagues showed that, under certain conditions, responses to <b>audiovisual</b> <b>stimuli</b> can be affected by semantic congruence or incongruence. More specifically, their findings showed that participants responded faster to congruent auditory and visual stimuli than to incongruent stimuli. In addition, Koppen, Alsius and Spence conducted a study which investigated whether the Colavita effect would be modulated by the semantic congruency between the visual and auditory stimulus, using stimuli of similar semantic meaning and complexity. The findings from this study showed that semantic congruency had no effect on the magnitude of the Colavita effect in the experiments, yet it had a significant effect on participants’ performance in the speeded discrimination task. Participants showed a pattern that reflected difficulties with separating the auditory stimulus from the visual stimulus when these stimuli had congruent semantic meaning and were presented simultaneously. For incongruent stimuli, participants had faster response times, which could also be explained by the previously mentioned theory of ‘Failure of Binding’.|$|E
50|$|This {{effect may}} be {{experienced}} when {{a video of}} one phoneme's production is dubbed with a sound-recording of a different phoneme being spoken. Often, the perceived phoneme is a third, intermediate phoneme. As an example, the syllables /ba-ba/ are spoken over the lip movements of /ga-ga/, and the perception is of /da-da/. McGurk and MacDonald originally believed that this resulted from the common phonetic and visual properties of /b/ and /g/. Two types of illusion in response to incongruent <b>audiovisual</b> <b>stimuli</b> have been observed: fusions ('ba' auditory and 'ga' visual produce 'da') and combinations ('ga' auditory and 'ba' visual produce 'bga'). This is the brain's effort to provide the consciousness with its best guess about the incoming information. The information coming from the eyes and ears is contradictory, and in this instance, the eyes (visual information) have had a greater effect on the brain and thus the fusion and combination responses have been created.|$|E
5000|$|The Colavita visual {{dominance}} effect {{refers to the}} phenomenon where participants respond more often to the visual component of an <b>audiovisual</b> <b>stimulus,</b> when presented with bimodal stimuli. Research has shown that vision is the most dominant sense for human beings [...] who do not suffer from sensory difficulties (e.g. blindness, cataracts). Theorists have proposed that the Colavita visual {{dominance effect}} demonstrates a bias toward visual sensory information, because the presence of auditory stimuli is commonly neglected during audiovisual events.|$|R
40|$|This study {{examines}} how audiovisual signals are combined {{in time for}} a temporal analogue of the ventriloquist effect in a purely temporal context, that is, no spatial grounding of signals or other spatial facilitation. Observers were presented with two successive intervals, each defined by a 1250 -ms tone, and indicated in which interval a brief <b>audiovisual</b> <b>stimulus</b> (visual flash + noise burst) occurred later. In “test“ intervals, the <b>audiovisual</b> <b>stimulus</b> was presented with a small asynchrony, while in “probe” intervals it was synchronous and presented at various times guided by an adaptive staircase to find the perceived temporal location of the asynchronous stimulus. As in spatial ventriloquism, and consistent with maximum likelihood estimation (MLE), the asynchronous audiovisual signal was shifted toward the more reliably localized component (audition, for all observers). Moreover, these temporal shifts could be forward or backward in time, depending on the asynchrony order, suggesting perceived timing is not entirely determined by physical timing. However, the critical signature of MLE combination—better bimodal than unimodal precision—was not found. Regardless of the underlying model, these results demonstrate temporal ventriloquism in a paradigm that is defined in a purely temporal context...|$|R
40|$|Prolonged {{exposure}} to asynchronous <b>audiovisual</b> <b>stimulus</b> pairs changes {{the perception of}} audiovisual simultaneity. It has been proposed that this change occurs by adjusting the perceptual latency of stimuli {{in order to minimize}} perceived audiovisual asynchrony. How is this adjustment achieved? For signals with a gradual onset, perceptual latency can be minimized by decreasing detection threshold (or vice-versa). Here we assess whether this occurs following recalibration of simultaneity. Participants were exposed for 5 minutes to asynchronous (150 ms) <b>audiovisual</b> <b>stimulus</b> pairs with either light or sound leading. Auditory stimuli were presented via headphones, visual via an LED. Detection thresholds for visual and auditory stimuli were then measured with a 2 IFC task interleaved with short re-exposures to the asynchrony. Results indicate that while the detection threshold for visual stimuli does not significantly vary, the detection threshold for auditory stimuli critically depends on which modality leads during asynchronous audiovisual exposure. All nine participants tested were more sensitive in detecting auditory stimuli after light-leading exposure than after sound-leading. We suggest that by becoming more or less sensitive to sound the brain is able to change the perceptual latency of auditory <b>stimuli</b> to minimize <b>audiovisual</b> asynchrony, while keeping the perceptual latency of visual stimuli relatively constant...|$|R
5000|$|For example, Sinnet and {{his colleagues}} {{conducted}} an experiment in which they presented participants with three response keys, one {{for each type of}} response (unimodal visual, unimodal auditory and bimodal audiovisual); instead of just two, and they instructed participants to press the bimodal key when responding to <b>audiovisual</b> <b>stimuli.</b> This new manipulation resulted in a significant reduction of the Colavita effect because errors in bimodal trials were only committed in a small number of trials. In another experiment, Sinnett {{and his colleagues}} conducted a pre-specified target detection task where auditory targets were more frequent than visual or bimodal targets. This led to the elimination of the Colavita effect. The authors suggested that this was due to the introduction of a bias toward auditory stimuli. Ngo and her colleagues conducted a similar study where the results were replicated, because their findings showed that under the appropriate conditions and task demand, the Colavita effect can be reversed. [...] Also, Sinnett and his colleagues mention that animals and humans increase their reliance on auditory stimuli in high-arousal situations and when facing potential threats, which could imply that the Colavita effect is situation and context dependent.|$|E
5000|$|In 1974, Francis B. Colavita {{conducted}} an experiment, which provided evidence for visual dominance in humans when performing an audiovisual discrimination task. In his seminal experiment, Colavita (1974) presented participants with an auditory (tone) or visual (light) stimulus, {{to which they}} were instructed to respond by pressing the ‘tone key’ or ‘light key’ respectively. Throughout the experiment, unimodal auditory trials, unimodal visual trials and a small number of audiovisual bimodal trials were randomly presented. Colavita deceived the participants by informing them that the bimodal trials in the experiment occurred “accidentally”. During practice trials, Colavita would “accidentally” present <b>audiovisual</b> <b>stimuli,</b> and would then draw the participants’ attention to what had just happened and would apologize for such ‘accident’. In addition, the participants were not instructed on how to respond on such trials or whether this type of trials would occur again [...] The results showed that participants had almost equivalent response times for auditory and visual stimuli in unimodal trials. Additionally, Colavita found that participants pressed the ‘light key’ in the majority of the bimodal trials. This was seen as evidence of visual dominance because participants failed to acknowledge the presence of the auditory stimulus in most bimodal trials. However, due to Colavita’s deception of the “accidental” occurrence of bimodal trials, researchers have proposed that experimenter expectancy effects, task demands or methodological problems {{may have contributed to the}} visual dominance effect reported in Colavita’s original study. Nevertheless, subsequent experiments have discontinued the use of deception, and continue to show a robust Colavita visual dominance effect.|$|E
40|$|Background. Studies of perceptual {{learning}} {{have largely}} focused on unisensory stimuli. However, multisensory interactions are ubiquitous in perception, even at early processing stages, {{and thus can}} potentially {{play a role in}} learning. Here, we examine the effect of auditory-visual congruency on visual learning. Methodology/Principle Findings. Subjects were trained over five days on a visual motion coherence detection task with either congruent audiovisual, or incongruent <b>audiovisual</b> <b>stimuli.</b> Comparing performance on visual-only trials, we find that training with congruent <b>audiovisual</b> <b>stimuli</b> produces significantly better learning than training with incongruent <b>audiovisual</b> <b>stimuli</b> or with only visual stimuli. Conclusions/ Significance. This advantage from stimulus congruency during training suggests that the benefits of multisensory training may result from audiovisual interactions at a perceptual rather than cognitive level...|$|E
40|$|Synchrony {{judgments}} involve {{deciding whether}} cues {{to an event}} are in synch or out of synch, while temporal order judgments involve deciding which of the cues came first. When the cues come from different sensory modalities these judgments {{can be used to}} investigate multisensory integration in the temporal domain. However, evidence indicates that that these two tasks should not be used interchangeably as it is unlikely that they measure the same perceptual mechanism. The current experiment further explores this issue across a variety of different <b>audiovisual</b> <b>stimulus</b> types...|$|R
40|$|Background: Synchrony {{judgments}} involve {{deciding whether}} cues {{to an event}} are in synch or out of synch, while temporal order judgments involve deciding which of the cues came first. When the cues come from different sensory modalities these judgments {{can be used to}} investigate multisensory integration in the temporal domain. However, evidence indicates that that these two tasks should not be used interchangeably as it is unlikely that they measure the same perceptual mechanism. The current experiment further explores this issue across a variety of different <b>audiovisual</b> <b>stimulus</b> types. Methodology/Principal Findings: Participants were presented with 5 <b>audiovisual</b> <b>stimulus</b> types, each at 11 parametrically manipulated levels of cue asynchrony. During separate blocks, participants had to make synchrony judgments or temporal order judgments. For some stimulus types many participants were unable to successfully make temporal order judgments, but they were able to make synchrony judgments. The mean points of subjective simultaneity for synchrony judgments were all video-leading, while those for temporal order judgments were all audio-leading. In the within participants analyses no correlation was found across the two tasks for either the point of subjective simultaneity or the temporal integration window. Conclusions: Stimulus type influenced how the two tasks differed; nevertheless, consistent {{differences were found between the}} two tasks regardless of stimulus type. Therefore, in line with previous work, we conclude that synchrony and tempora...|$|R
30|$|In literature, EEG theta band {{dynamics}} including {{important information}} about emotions {{can be seen}} [24, 26, 27]. Therefore, theta band dynamics were used {{in order to obtain}} feature vectors. Selection of suitable mother wavelet and the number of decomposition levels is very important to analyze non-stationary signals [10]. In this study, the number of decomposition levels was chosen as 4 because theta band dynamics of EEG signals were considered to recognize different emotions based on <b>audiovisual</b> <b>stimulus.</b> Dabuechies wavelets have provided useful results in analyzing EEG signals [10], and hence Daubechies wavelet of order 2 (db 2) was chosen in this study.|$|R
40|$|Saccades to {{combined}} <b>audiovisual</b> <b>stimuli</b> {{often have}} reduced saccadic reaction times (SRTs) {{compared with those}} to unimodal stimuli. Neurons in the intermediate/deep layers of the superior colliculus (dSC) are capable of integrating converging sensory inputs to influence the time to saccade initiation. To identify how neural processing in the dSC contributes to reducing SRTs to <b>audiovisual</b> <b>stimuli,</b> we recorded activity from dSC neurons while monkeys generated saccades to visual or <b>audiovisual</b> <b>stimuli.</b> To evoke crossmodal interactions of varying strength, we used auditory and visual stimuli of different intensities, presented either in spatial alignment or to opposite hemifields. Spatially aligned <b>audiovisual</b> <b>stimuli</b> evoked the shortest SRTs. In the case of low-intensity stimuli, {{the response to the}} auditory component of the aligned audiovisual target increased the activity preceding the response to the visual component, accelerating the onset of the visual response and facilitating the generation of shorter-latency saccades. In the case of high-intensity stimuli, the auditory and visual responses occurred much closer together in time and so there was little opportunity for the auditory stimulus to influence previsual activity. Instead, the reduction in SRT for high-intensity, aligned <b>audiovisual</b> <b>stimuli</b> was correlated with increased premotor activity (activity after visual burst but preceding saccade-aligned burst). These data provide a link between changes in neural activity related to stimulus modality with changes in behavior. They further demonstrate how crossmodal interactions are not limited to the initial sensory activity but can also influence premotor activity in the SC...|$|E
30|$|The aim of {{this study}} was to {{classify}} EEG signals related to different emotions based on <b>audiovisual</b> <b>stimuli</b> with the preprocessing of channel selection.|$|E
40|$|The McGurk {{effect was}} {{investigated}} {{in a group of}} ten-yearold dyslexic children and in two control groups of normal readers. Audio and <b>audiovisual</b> <b>stimuli</b> were presented in silence or with a masking noise. The results indicated {{no significant differences between the}} three groups for the auditory stimuli. For the <b>audiovisual</b> <b>stimuli,</b> the dyslexic group showed fewer illusory percepts than the group of sameage normal readers but performed similarly to the group with the same reading level. Index Terms: McGurk effect, dyslexics, reading age, speech perception, speech readin...|$|E
40|$|AbstractA {{change in}} sound {{intensity}} can facilitate luminance change detection. We {{found that this}} effect did not depend on whether sound intensity and luminance increased or decreased. In contrast, luminance identification was strongly influenced by the congruence of luminance and sound intensity change leaving only unsigned stimulus transients {{as the basis for}} audiovisual integration. Facilitation of luminance detection occurred even with varying <b>audiovisual</b> <b>stimulus</b> onset asynchrony and even when the sound lagged behind the luminance change by 75 ms supporting the interpretation that perceptual integration rather than a reduction of temporal uncertainty or effects of attention caused the effect...|$|R
40|$|Three {{children}} with autism were taught to identify pictures of objects. Their speed of acquisition of receptive speech skills was compared across two conditions. In the cue-value condition, a compound <b>audiovisual</b> <b>stimulus</b> was presented after correct responses and again when a primary reinforcer was delivered after a 5 -s delay; in the response-marking condition, a second stimulus was presented after both correct and incorrect responses, but not prior to the primary reinforcer. In both conditions primary reinforcement was delayed for 5 s. Although the children learned receptive speech skills in both conditions, acquisition was faster in the cue-value condition...|$|R
40|$|In many natural {{audiovisual}} events (e. g., a clap {{of the two}} hands), {{the visual}} signal precedes the sound and thus allows observers to predict when, where, and which sound will occur. Previous studies have already reported that there are distinct neural correlates of temporal (when) versus phonetic/semantic (which) content on audiovisual integration. Here we examined the effect of visual prediction of auditory location (where) in <b>audiovisual</b> biological motion <b>stimuli</b> by varying the spatial congruency between the auditory and visual part of the <b>audiovisual</b> <b>stimulus.</b> Visual stimuli were presented centrally, whereas auditory stimuli were presented either centrally or at 90 &# 176; azimuth. Typical subadditive amplitude reductions (AV – V < A) were found for the auditory N 1 and P 2 for spatially congruent and incongruent conditions. The new finding is that the N 1 suppression was larger for spatially congruent stimuli. A very early audiovisual interaction was also found at 30 - 50 ms in the spatially congruent condition, while no effect of congruency {{was found on the}} suppression of the P 2. This indicates that visual prediction of auditory location can be coded very early in auditory processing...|$|R
40|$|Following {{prolonged}} exposure to asynchronous multisensory signals, the brain adapts {{to reduce the}} perceived asynchrony. Here, in three separate experiments, participants performed a synchrony judgment task on audiovisual, audiotactile or visuotactile stimuli and we used inter-trial analyses to examine whether temporal recalibration occurs rapidly {{on the basis of}} a single asynchronous trial. Even though all combinations used the same subjects, task and design, temporal recalibration occurred for <b>audiovisual</b> <b>stimuli</b> (i. e., the point of subjective simultaneity depended on the preceding trial’s modality order), but none occurred when the same auditory or visual event was combined with a tactile event. Contrary to findings from prolonged adaptation studies showing recalibration for all three combinations, we show that rapid, inter-trial recalibration is unique to <b>audiovisual</b> <b>stimuli.</b> We conclude that recalibration occurs at two different timescales for <b>audiovisual</b> <b>stimuli</b> (fast and slow), but only on a slow timescale for audiotactile and visuotactile stimuli...|$|E
40|$|Integration of {{an action}} and its sensory {{feedback}} is important in interacting with an uncertain environment and construct a consistent model of the world. In this process, multisensory data need to be processed, in which audition and vision play important roles. Subjective simultaneity of <b>audiovisual</b> <b>stimuli</b> is affected by various factors. To investigate the relation between our subjective simultaneity of <b>audiovisual</b> <b>stimuli</b> and our action, we conducted an experiment in which subjects ’ action affected the temporal patterns of resulting stimuli. The modes of contingency between action and stimuli were made variable. We found significant correlations between the accuracies of actions and the "window " of subjective simultaneity among subjects, although their task performances were widely varied. In addition, the correlation patterns were found {{to depend on the}} contingency between the key pressing and stimuli. These results suggest that the subjective simultaneity of <b>audiovisual</b> <b>stimuli</b> correlates with the accuracy of execution of action, indicating a common mechanism engaging the perception of subjective simultaneity in sensorimotor integration and action execution...|$|E
40|$|A {{method for}} {{creating}} and presenting video-recorded synchronized <b>audiovisual</b> <b>stimuli</b> {{at a high}} frame rate-which would be highly useful for psychophysical studies on, for example, just-noticeable differences and gating-is presented. Methods for accomplishing this include recording audio and video separately using an exact synchronization signal, editing the recordings and finding exact synchronization points, and presenting the synchronized <b>audiovisual</b> <b>stimuli</b> with a desired frame rate on a cathode ray tube display using MATLAB and Psychophysics Toolbox 3. The methods from an empirical gating study (Moradi, Lidestam, and Ronnberg, Frontiers in Psychology 4 : 359, 2013) are presented {{as an example of}} the implementation of playback at 120 fps...|$|E
40|$|Background Synchrony {{judgments}} involve {{deciding whether}} cues {{to an event}} are in synch or out of synch, while temporal order judgments involve deciding which of the cues came first. When the cues come from different sensory modalities these judgments {{can be used to}} investigate multisensory integration in the temporal domain. However, evidence indicates that that these two tasks should not be used interchangeably as it is unlikely that they measure the same perceptual mechanism. The current experiment further explores this issue across a variety of different <b>audiovisual</b> <b>stimulus</b> types. Methodology/Principal Findings Participants were presented with 5 <b>audiovisual</b> <b>stimulus</b> types, each at 11 parametrically manipulated levels of cue asynchrony. During separate blocks, participants had to make synchrony judgments or temporal order judgments. For some stimulus types many participants were unable to successfully make temporal order judgments, but they were able to make synchrony judgments. The mean points of subjective simultaneity for synchrony judgments were all video-leading, while those for temporal order judgments were all audio-leading. In the within participants analyses no correlation was found across the two tasks for either the point of subjective simultaneity or the temporal integration window. Conclusions Stimulus type influenced how the two tasks differed; nevertheless, consistent {{differences were found between the}} two tasks regardless of stimulus type. Therefore, in line with previous work, we conclude that synchrony and temporal order judgments are supported by different perceptual mechanisms and should not be interpreted as being representative of the same perceptual process...|$|R
40|$|Abstract — The {{application}} of hi-end technologies in multimedia platforms provides the dynamic {{setting of the}} parameters for the reproduction of <b>audiovisual</b> <b>stimulus</b> from natural environments (virtualization through real-time interaction). Additionally, the integration of cartographic products that describe quantitative and qualitative spatial properties expands multimedia’s capabilities for representations with geographical reference. The proposed interface combines data that are used for the mapping of a sonic environment, as well as sonic elements derived from field recordings and photographic material, in order to reconstruct in-vitro the soundscape of a protected area around Lake Antinioti, at northern Corfu, Greece. foreground) [3] and sound recordings as well. Furthermore, as sound varies according to space and time, information for soundscape has to be treated in both spatial and temporal scales [7]. I...|$|R
40|$|Abstract—We {{present a}} novel {{clustering}} method to probe inter-subject variability in functional {{magnetic resonance imaging}} (fMRI) data acquired in complex <b>audiovisual</b> <b>stimulus</b> environments, such as during watching movies. We calculate voxel-wise inter-subject correlation matrices across individual subject fMRI time-series and cluster them over the cerebral cortex. We address correlation matrix clustering problem and modify a standard K-means algorithm to cope better with spurious observations. We investigate suitability of the modified K-means with hierarchical clustering based postprocessing to correlation matrix clustering with several artificially generated data sets. We also present clustering of fMRI movie data. Preliminary results suggest that our methodology can be a valuable tool to investigate inter-subject variability in brain activity in different brain regions, such as prefrontal cortex. I...|$|R
40|$|One of {{the central}} {{questions}} in cognitive neuroscience is the precise neural representation, or brain pattern, associated with a semantic category. In this study, we explored the influence of <b>audiovisual</b> <b>stimuli</b> on the brain patterns of concepts or semantic categories through a {{functional magnetic resonance imaging}} (fMRI) experiment. We used a pattern search method to extract brain patterns corresponding to two semantic categories: "old people" and "young people. " These brain patterns were elicited by semantically congruent audiovisual, semantically incongruent audiovisual, unimodal visual, and unimodal auditory stimuli belonging to the two semantic categories. We calculated the reproducibility index, which measures the similarity of the patterns within the same category. We also decoded the semantic categories from these brain patterns. The decoding accuracy reflects the discriminability of the brain patterns between two categories. The results showed that both the reproducibility index of brain patterns and the decoding accuracy were significantly higher for semantically congruent <b>audiovisual</b> <b>stimuli</b> than for unimodal visual and unimodal auditory stimuli, while the semantically incongruent stimuli did not elicit brain patterns with significantly higher reproducibility index or decoding accuracy. Thus, the semantically congruent <b>audiovisual</b> <b>stimuli</b> enhanced the within-class reproducibility of brain patterns and the between-class discriminability of brain patterns, and facilitate neural representations of semantic categories or concepts. Furthermore, we analyzed the brain activity in superior temporal sulcus and middle temporal gyrus (STS/MTG). The strength of the fMRI signal and the reproducibility index were enhanced by the semantically congruent <b>audiovisual</b> <b>stimuli.</b> Our results support the use of the reproducibility index as a potential tool to supplement the fMRI signal amplitude for evaluating multimodal integration...|$|E
40|$|Face {{identification}} and voice identification were examined using a standard old/new recognition task {{in order to}} see whether seeing and hearing the target interfered with subsequent recognition. Participants studied either visual or <b>audiovisual</b> <b>stimuli</b> prior to a face recognition test, and studied either audio or <b>audiovisual</b> <b>stimuli</b> prior to a voice recognition test. Analysis of recognition performance revealed a greater ability to recognise faces than voices. More importantly, faces accompanying voices at study interfered with subsequent voice identification but voices accompanying faces at study did not interfere with subsequent face identification. These results are similar to those obtained in previous research using a lineup methodology, and are discussed with respect to the interference that can result when earwitnesses are also eyewitnesses...|$|E
40|$|A general {{principle}} of EEG measurements {{is that each}} recorded channel represents the difference in activity between two electrodes on the head. Use of different reference schemes between studies may contribute, among other factors, to inconsistencies of findings (Hagemann, 1998). This study investigates the effect of choice of reference on the CAEPs to auditory and <b>audiovisual</b> <b>stimuli</b> in normal hearing adults. The CAEPs were recorded to natural speech tokens presented as auditory, visual and <b>audiovisual</b> <b>stimuli.</b> The CAEPs were recorded with FCz as a reference and re-referenced offline to right mastoid, left mastoid, mean mastoids and an average reference respectively. The amplitude of the average reference waveforms were significantly smaller {{compared to the other}} references. Three of the four references (right, left and mean mastoids) showed a temporal facilitation with shorter latencies for N 1 in the audiovisual condition. No latency differences were found for the average reference. All references showed a modality effect with <b>audiovisual</b> <b>stimuli</b> resulting in significantly enhanced N 1 /P 2 amplitudes compared to auditory. The results suggest that the choice of reference has a significant effect on the temporal analysis of CAEPs and emphasise the importance of indication and justification of any choice of reference. 1 page(s...|$|E
40|$|International audienceAudiovisual fusion {{in speech}} {{perception}} is generally {{conceived as a}} process independent from scene analysis, {{which is supposed to}} occur separately in the auditory and visual domain. On the contrary, we have been proposing in the last years that scene analysis such as what takes place in the cocktail party effect was an audiovisual process. We review here a series of experiments illustrating how audiovisual speech scene analysis occurs in the context of competing sources. Indeed, we show that a short contextual <b>audiovisual</b> <b>stimulus</b> made of competing auditory and visual sources modifies the perception of a following McGurk target. We interpret this in terms of binding, unbinding and rebinding processes, and we show how these processes depend on audiovisual correlations in time, attentional processes and differences between junior and senior participants...|$|R
30|$|Recently, Haxby et al. (2012) {{proposed}} a method termed ‘hyperalignment’ which eliminates the penalty in classification accuracy across subjects. First, a supervised feature mapping approach (a specialized feature selection) is used across subjects, {{based on data}} gathered during the viewing of a rich <b>audiovisual</b> <b>stimulus</b> (a film). Without using any spatial constraints, sets of voxels are identified across subjects, which collectively exhibit similar functional sensitivity across {{the time course of}} the fMRI data. The training data is labelled, {{in the sense that the}} fMRI recording is temporally aligned to the film, and there is a direct equivalence between the time points across subjects. After this feature selection/mapping stage, different data from the same pairs (or set) of subjects are used for cross-subject learning (e.g. training on labelled data from participant A and testing on similarly labelled data from participant B).|$|R
40|$|SummaryNormal brain {{function}} requires the dynamic interaction of functionally specialized but widely distributed cortical regions. Long-range synchronization of oscillatory signals {{has been suggested}} to mediate these interactions within large-scale cortical networks, but direct evidence is sparse. Here we show that oscillatory synchronization is organized in such large-scale networks. We implemented an analysis approach that allows for imaging synchronized cortical networks and applied this technique to EEG recordings in humans. We identified two networks: beta-band synchronization (∼ 20  Hz) in a fronto-parieto-occipital network and gamma-band synchronization (∼ 80  Hz) in a centro-temporal network. Strong perceptual correlates support their functional relevance: the strength of synchronization within these networks predicted the subjects' percept of an ambiguous <b>audiovisual</b> <b>stimulus</b> {{as well as the}} integration of auditory and visual information. Our results provide evidence that oscillatory neuronal synchronization mediates neuronal communication within frequency-specific, large-scale cortical networks...|$|R
