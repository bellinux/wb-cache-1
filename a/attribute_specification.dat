13|126|Public
30|$|Finally, the <b>{{attribute}}</b> <b>specification</b> {{defines the}} attribute values and metrics for the optimization process. This specification is made manually {{in the same}} configuration file. Each metric is defined by an aggregate function over an attribute, and a specific weight, as explained in Section 2.3. 2. An attribute value is defined in the configuration file providing its name, e.g., startup time, constant value, e.g., 0.5 seconds, and the associated software element, e.g., shadow removal component.|$|E
30|$|The mapping and <b>attribute</b> <b>specification</b> {{are done}} by {{software}} engineers that {{are concerned with}} the system implementation. The mapping activity implies linking software elements (components and parameters) to features, and system events to feature selection/deselections (event rules). Currently, the approach is limited to one-to-one mappings between features and software elements, such as in feature-oriented software development (Apel and Kästner 2009) where features are mapped to implementation assets. The mapping must be specified manually in the configuration file, providing the name of the feature and its corresponding software element.|$|E
40|$|THALES is a soilware {{package for}} plane {{geometry}} constructions, supplied {{with a natural}} language interface. Using THALES requires no knowledge of a programming language. The interface is capable of processing practically all kinds of instructions within the subset of plane geometry English. The "static semantic" module has been generated {{on the basis of}} a high-level <b>attribute</b> <b>specification.</b> Transportability, modifiability and generality [...] the key issues of natural language interface design [...] are investigated in the project note. The notion of specifiability is introduced to replace the three features mentioned above...|$|E
5000|$|A {{descriptive}} {{item identification}} {{according to the}} Item Identification Guide (IIG) {{on the basis of}} material technical <b>attributes</b> <b>specification.</b>|$|R
50|$|This contextual {{information}} of business data include meaning and content, policies that govern, technical <b>attributes,</b> <b>specifications</b> that transform, {{and programs that}} manipulate.|$|R
40|$|Such an {{extension}} will support RDF Reification, annotation of SBML <b>attributes,</b> <b>specification</b> {{of relationships between}} annotations, negation of annotations and cross-references and cross-element annotations...|$|R
40|$|We {{present a}} scheme for online, {{unsupervised}} state discovery and detection from streaming, multi-featured, asynchronous data in high-frequency financial markets. Online feature correlations are computed using an unbiased, lossless Fourier estimator. A high-speed maximum likelihood clustering algorithm is {{then used to}} find the feature cluster configuration which best explains the structure in the correlation matrix. We conjecture that this feature configuration is a candidate descriptor for the temporal state of the system. Using a simple cluster configuration similarity metric, {{we are able to}} enumerate the state space based on prevailing feature configurations. The proposed state representation removes the need for human-driven data pre-processing for state <b>attribute</b> <b>specification,</b> allowing a learning agent to find structure in streaming data, discern changes in the system, enumerate its perceived state space and learn suitable action-selection policies. Comment: 19 pages, 6 figures, 3 tables, under review at Pattern Recognition Letter...|$|E
40|$|Despite {{the vital}} role of utility {{functional}} form in welfare measurement, {{the implications of}} working with incorrect utility specifications have not been examined in the choice experiments (CE) literature. This paper addresses {{the importance of the}} specification of both non-monetary attributes and the marginal utility of income. Monte Carlo experiments have been conducted wherein different attribute specifications and assumptions for the Cost parameter -that is, different functional forms of utility- have been assumed to generate simulated choices on which Multi-Nomial Logit and Mixed Logit models have been estimated under correct and incorrect assumptions about the true, underlying utility function. The inferred values have been compared with the true ones directly calculated from the true utility specifications. Results show that working with simple experimental designs and continuous-linear specifications makes <b>attribute</b> <b>specification</b> irrelevant for measuring attribute marginal values regardless of the true effects the attribute has on utility. utility specification; attributes; welfare measurement; accuracy; efficiency; choice experiments; Monte Carlo analysis...|$|E
40|$|Modern {{enterprises}} are irreversibly {{dependent on}} large-scale, adaptive, component-based information systems whose complexity frequently exceeds current engineering capabilities for intellectual control, resulting in persistent difficulties in system development, management, and evolution. We propose an innovative framework of engineering representation and reasoning methods for developing these complex systems: the Flow-Service-Quality (FSQ) Framework. In dynamic network information systems with constantly varying function and usage, work flows and their corresponding traces of system services act as stable foundations for functional and non-functional (quality <b>attribute)</b> <b>specification,</b> design, and operational control. Our {{objective is to}} provide theoretical foundations, language representations, and rigorous yet practical unified engineering methods to represent and reason about system flows as essential artifacts of system specification, design, and operation. 1. Intellectual Control in Large-Scale System Development Modern enterprises are irreversibly dependent on large-scale information systems whose complexity frequently exceeds current engineering capabilities for intellectual control, resulting in persistent difficulties in system development, management, and evolution. Critical enterprise missions depend on system services composed of complex combinations of distributed computation, communication, and human components whose interactions are often not fully understood. Thes...|$|E
40|$|Abstract. Database {{evolution}} can {{be considered}} a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an <b>attribute’s</b> <b>specification,</b> semantics and/or range of allowable values changes. We present a model in which mesodata – an additional domain definition layer containing domain structure and intelligence – is used to alleviate and in some cases obviate the need for data conversion or coercion. We present the nature and use of mesodata as it affects do-main evolution, such as when a domain changes, when the semantics of a domain alter and when the <b>attribute’s</b> <b>specification</b> is modified. ...|$|R
40|$|An attribute-managed solver makes {{assignments}} of workload {{units to}} devices based on <b>attributes</b> <b>specifications</b> that describe the workload units and the devices. This paper presents the sets of attributes that model workloads and devices. It also discusses the necessary formalization of an attribute-managed solver {{in terms of}} objective functions and constraint expressions. These formalization...|$|R
40|$|In {{response}} to low commodity prices and increasing production cost, agricultural producers {{have attempted to}} capture a portion of downstream market value of their products by organizing and investing in what are now called "New Generation Co-ops". These cooperative efforts often involve the construction of processing facilities (soybean crushing, corn processing, wheat milling) requiring an extensive capital commitment and a substantial financial risk for their members. An alternative strategy to the brick and mortar cooperatives are producer affiliations that involve the negotiation of contractual agreements with product users, providing protocols for maintaining quality standards, and collecting and sharing information. AgGuild of Illinois {{is an example of}} such a producer cooperative arrangement. The AgGuild, an alliance of some fifty central Illinois grain producers, attempts to capture a premium over the general commodity grain price by producing crops in viable quantities that meet the quality and <b>attribute</b> <b>specifications</b> desired by contracting users. To this point, the AgGuild has focused its efforts on producing non-genetically modified soybeans that have higher yields of isoflavones (a naturally occurring chemical compound in plants that are considered to provide a number of health benefits for consumers). The AgGuild attempts to capture a premium over the general commodity grain price by producing crops in viable quantities that meet the quality and <b>attribute</b> <b>specifications</b> desired by contracting users. To this point, the Guild has focused its efforts on producing non-genetically modified soybeans that have higher yields of isoflavones. This paper describes the operation of the AgGuild, assesses its current status and identifies some potential future challenges. Agribusiness,...|$|R
40|$|Despite {{the vital}} {{role of the}} utility {{function}} in welfare measurement, the implications of working with incorrect utility specifications have been largely neglected in the choice experiments literature. This paper addresses the importance of specification with a special emphasis {{on the effects of}} mistaken assumptions about the marginal utility of income. Monte Carlo experiments were conducted using different functional forms of utility to generate simulated choices. Multi-Nomial Logit and Mixed Logit models were then estimated on these choices under correct and incorrect assumptions about the true, underlying utility function. Estimated willingness to pay measures from these choice modeling results are then compared with the equivalent measures directly calculated from the true utility specifications. Results show that for the parameter values and functional forms considered, a continuous-quadratic or a discrete-linear <b>attribute</b> <b>specification</b> is a good option regardless of the true effects the attribute has on utility. We also find that mistaken assumptions about preferences over costs magnify attribute mis-specification effects. Utility specification Attributes Welfare measurement Accuracy Efficiency Choice experiments Monte Carlo analysis...|$|E
40|$|This book chapter was {{published}} in: Innovation and the Knowledge Economy: Issues, Applications, Case Studies [© IOS Press] [URL] It was previously presented at eChallenges e 2005 19 - 21 October 2005, Ljubljana, Slovenia [URL] level service support mechanisms {{are an integral}} part of the future vision for Web / Grid Services. This paper argues that the areas of discovery, differentiation, negotiation, monitoring and non-repudiation of agreements cannot be considered in isolation to each other. The areas outlined above are examined, primarily from a trust perspective, focusing on the use of contracts to guarantee QoS attributes. The paper explores the need for greater standardisation in the area, to specify semantics more clearly; in doing so we outline the progress of our own QoSOnt QoS <b>attribute</b> <b>specification</b> ontology. The paper goes on to briefly discuss two tools; firstly, SQRM, designed to allow service discovery, querying and requirement specification utilising the QoSOnt ontology; and secondly, TRANSACT, an existing contract negotiation tool designed to provide end-to-end contracting, encompassing an automated negotiation engine...|$|E
40|$|Abstract. The Open Science Grid offers {{access to}} {{hundreds}} of computing and storage resources via standard Grid interfaces. Before the deployment of an automated resource selection system, users had to submit jobs directly to these resources. They would manually select a resource and specify all relevant attributes in the job description prior to submitting the job. The necessity of a human intervention in resource selection and <b>attribute</b> <b>specification</b> hinders automated job management components from accessing OSG resources and it is inconvenient for the users. The Resource Selection Service (ReSS) project addresses these shortcomings. The system integrates condor technology, for the core match making service, with the gLite CEMon component, for gathering and publishing resource information in the Glue Schema format. Each one of these components communicates over secure protocols via web services interfaces. The system is currently used in production on OSG by the DZero Experiment, the Engagement Virtual Organization, and the Dark Energy. It is also the resource selection service for the Fermilab Campus Grid, FermiGrid. ReSS is considered a lightweight solution to push-based workload management. This paper describes the architecture, performance, and typical usage of the system...|$|E
5000|$|Diagrammatic {{and style}} {{information}}: information about rendered items (like image size <b>attributes)</b> and visual <b>specifications,</b> as Cascading Style Sheets (CSS).|$|R
40|$|The Second International Conference on Accelerating Biopharmaceutical Development {{was held}} in Coronado, California. The meeting was {{organized}} by the Society for Biological Engineering (SBE) and the American Institute of Chemical Engineers (AIChE); SBE is a technological community of the AIChE. Bob Adamson (Wyeth) and Chuck Goochee (Centocor) were co-chairs of the event, which had the theme “Delivering cost-effective, robust processes and methods quickly and efficiently. ” The first day focused on emerging disruptive technologies and cutting-edge analytical techniques. Day two featured presentations on accelerated cell culture process development, critical quality <b>attributes,</b> <b>specifications</b> and comparability, and high throughput protein formulation development. The final day was dedicated to discussion of technology options and new analysis methods provided by emerging disruptive technologies; functional interaction, integration and synergy in platform development; and rapid and economic purification process development...|$|R
50|$|The Trusted Computing Group (TCG) is {{developing}} a standard TNC SWID Messages and <b>Attributes</b> for IF-M <b>Specification</b> that utilizes tag data for security purposes.|$|R
40|$|An {{overview}} {{is given}} of a design optimization project {{that is in}} progress at the GE Research and Development Center {{for the past few}} years. The objective of this project is to develop a methodology and a software system for design automation and optimization of structural/mechanical components and systems. The effort focuses on research and development issues and also on optimization applications that can be related to real-life industrial design problems. The overall technical approach is based on integration of numerical optimization techniques, finite element methods, CAE and software engineering, and artificial intelligence/expert systems (AI/ES) concepts. The role of each of these engineering technologies {{in the development of a}} unified design methodology is illustrated. A software system DESIGN-OPT has been developed for both size and shape optimization of structural components subjected to static as well as dynamic loadings. By integrating this software with an automatic mesh generator, a geometric modeler and an <b>attribute</b> <b>specification</b> computer code, a software module SHAPE-OPT has been developed for shape optimization. Details of these software packages together with their applications to some 2 - and 3 -dimensional design problems are described...|$|E
40|$|The Open Science Grid offers {{access to}} {{hundreds}} of computing and storage resources via standard Grid interfaces. Before the deployment of an automated resource selection system, users had to submit jobs directly to these resources. They would manually select a resource and specify all relevant attributes in the job description prior to submitting the job. The necessity of a human intervention in resource selection and <b>attribute</b> <b>specification</b> hinders automated job management components from accessing OSG resources and it is inconvenient for the users. The Resource Selection Service (ReSS) project addresses these shortcomings. The system integrates condor technology, for the core match making service, with the gLite CEMon component, for gathering and publishing resource information in the Glue Schema format. Each one of these components communicates over secure protocols via web services interfaces. The system is currently used in production on OSG by the DZero Experiment, the Engagement Virtual Organization, and the Dark Energy. It is also the resource selection service for the Fermilab Campus Grid, FermiGrid. ReSS is considered a lightweight solution to push-based workload management. This paper describes the architecture, performance, and typical usage of the system...|$|E
30|$|The {{statistically}} inferred {{importance is}} derived by statistical means, for example, beta {{values of a}} regression analysis, statistical correlations or structural equation modeling (Taplin 2012). This is an indirect method that is hinged {{on the relationship between}} performance of attributes and customer satisfaction (Pezeshki et al. 2009). Grigoroudis and Spyridaki (2003) states that derived importance is estimated by a regression-type quantitative technique using customer judgments for the performance of the criteria. With respect to the statistically inferred importance, customers are asked to rate both their satisfaction with the current performance of the different requirements or features or attributes and their overall satisfaction with the requirement, feature or attribute. Researchers have used several approaches in capturing this kind of importance, namely: multiple regressions, partial least squares with reflective or formative <b>attribute</b> <b>specification,</b> pair-wise estimation, and principal component regression, etc., Gustafson and Johnson as cited in (Tontini and Silveira 2007). Requirements or attributes with higher regression coefficients are deemed to be more important than others. Although this approach removes the tendency of getting all attributes important and also discriminates better the relative importance between them, Garver as cited in (Tontini and Silveira 2007), it is still deficient. There is usually the occurrence of multi co-linearity among independent variables, which tends to be problematic. Also, the relationship between the satisfaction with an attribute or feature and overall satisfaction do not always follow linearity (Tontini and Silveira 2007). In addition, customers tend to say they are “satisfied” when in reality they are not, but actually in a neutral state. This leads to bias in the data (Tontini and Silveira 2007). In this study, the self-stated importance approach is employed. The bias observed in this review was controlled by using a fair sample size and utilizing a trained and skilled interviewer.|$|E
40|$|Attribute Grammars are the {{specification}} language of many tools that automatically generate programming language implementations. We consider {{the problem of}} verifying properties of <b>attribute</b> grammar <b>specifications,</b> particularly properties that are not well supported by existing tools. Rather than propose methods for extending existing tool implementation techniques, we propose the use of off-the-shelf formal methods tools {{as the basis for}} attribute grammar verification. Off-the-shelf tools can provide significant expressive power at a much lower cost than extending an existing evaluator generator. As a specific example, we describe how to use the Alloy model finding and checking tool to verify properties of remote attribution constructs in the LIDO <b>attribute</b> grammar <b>specification</b> language. A naive application of this approach has significant performance overheads; we discuss techniques for limiting the scope of the problems that are solved to make the approach tractable. 22 page(s...|$|R
40|$|AbstractAttribute grammar {{specification}} languages, {{like many}} domain specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars {{are often not}} adopted {{to the degree that}} their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible <b>attribute</b> grammar <b>specification</b> language, and show how it can be extended with general purpose features such as pattern matching and domain specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an <b>attribute</b> grammar <b>specification</b> language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner...|$|R
40|$|AbstractIn this paper, three {{solutions}} are proposed {{to simplify the}} surface texture specification without significant information loss. The first solution is operating additional default values for simplification. The second solution is utilizing simple symbols in CAD systems but affiliated with complete <b>specifications</b> <b>attribute</b> data which can be transferred to other CAx systems or end users for reading and analyzing. The third solution is using both default values and CAD <b>specifications</b> <b>attribute</b> data. After the combination, the shortest specifications can be generated and also can be employed in both paper technical drawings and CAD systems...|$|R
40|$|The {{construction}} {{industry is a}} knowledge intensive industry. Thousands of documents are generated by construction projects. Documents, as information carriers, must be managed effectively to ensure successful project management. The fact that a single project can produce thousands of documents and {{that a lot of}} the documents are generated in a textual/unstructured format greatly complicates the task of information management. Conventionally, project documents are organized based on classifying documents according to fixed/predefined classes and document metadata, e. g. according to document type, originator, project <b>attribute,</b> <b>specification</b> division, date, etc. While such classification method is easy to implement, it is only advantageous for document search and retrieval if the document seeker has prior knowledge of the content of the document corpus. In many cases and for various project management activities this is not the case, resulting in frustration of the search task with delayed or incomplete search results. ^ An alternative framework for organizing project documents based on document content is proposed. The framework takes into account important characteristics of construction project documents and leverages such characteristics to facilitate document search and retrieval. The premise for the framework is the fact that documents are not produced haphazardly, but are generated as a result of certain events or circumstances occurring in the project. As such documents can be linked to each other on the semantic level; a point that is overlooked by document management systems which generally manage documents in vacuo by disregarding or failing to utilize such semantic connections between the documents. Organizing project documents based on the semantic relations that exist between them (revealed from the document content and not just the document attributes) facilitates information retrieval and retains the knowledge of the actual project participants, thereby supporting knowledge reuse. ^ Another aspect of the thesis investigates the use of document content analysis to enable automated document management. If textual similarities between documents correlate with what human users recognize through their semantic abilities, then content analysis of documents can be used to automatically organize documents according to the proposed framework. Text classifiers based on machine learning techniques were evaluated to determine their performance in identifying which group of semantically-similar documents a test document belongs. Also, an unsupervised learning method was adapted and evaluated for the task of clustering documents based on textual similarity into sets of documents that are semantically related. The purpose of such evaluations is to equip electronic document management systems with content analysis capabilities that facilitate document search and retrieval. ...|$|E
5000|$|Data {{manipulation}} enhancements: allocatable components (incorporating TR 15581), deferred type parameters, [...] <b>attribute,</b> explicit type <b>specification</b> {{in array}} constructors and allocate statements, pointer enhancements, extended initialization expressions, and enhanced intrinsic procedures ...|$|R
40|$|In this paper, three {{solutions}} are proposed to short the surface texture specification without significant information loss. The first solution is operating more default values {{to simplify the}} specification without information loss. The second solution is utilizing simple surface texture symbols in CAD systems but affiliated with complete <b>specifications</b> <b>attribute</b> data which can be transferred to other CAx systems or end users for reading and analyzing. The third solution is using both default value and CAD <b>specifications</b> <b>attribute</b> data. After the combination, the shortest specifications can be generated and also can be employed in both paper technical drawings and CAD systems...|$|R
40|$|Abstract — Providers, {{operators}} and customers {{understand that the}} concept of an IT service offers a beneficial abstraction from actual IT operations, effectively encapsulating the provisioning of the service. Yet, IT services are in fact provided by installations composed of very large numbers of managed elements. Hence, management of a service implies the management of the multitude of elements and sub-services upon which the service relies. The mapping of a service’s attributes onto resources to “make the service visible ” is as much of a challenge as the execution of service management actions (i. e. actions directed at a service) on the underlying infrastructure. Even today, practical solutions to these issues are scarce. In this paper, we propose a methodology for the synthesis of service attributes to create the foundation for a service management information base. We present a declarative language suitable for the representation of service attributes in dependence of management attributes of the provisioning infrastructure. In addition, we discuss a service monitoring architecture driven by service <b>attribute</b> <b>specifications.</b> I...|$|R
50|$|Establishing design {{requirement}} analysis, sometimes termed problem definition, {{is one of}} the most important elements in the design process, and this task is often performed at the same time as a feasibility analysis. The {{design requirement}}s control the design of the project throughout the engineering design process. These include basic things like the functions, <b>attributes,</b> and <b>specifications</b> - determined after assessing user needs. Some design requirements include hardware and software parameters, maintainability, availability, and testability.|$|R
40|$|We {{show how}} new {{syntactic}} forms and static analysis {{can be added}} to a programming language to support abstractions provided by libraries. Libraries have the important characteristic that programmers can use multiple libraries in a single program. Thus, any attempt to extend a language’s syntax and analysis should be done in a composable manner so that similar extensions that support other libraries can be used by the programmer in the same program. To accomplish this we have developed an extensible <b>attribute</b> grammar <b>specification</b> of Java 1. 4 written in the <b>attribute</b> grammar <b>specification</b> language Silver. Library writers can specify, as an attribute grammar, new syntax and analysis that extends the language and supports their library. The Silver tools automatically compose the grammars defining the language and the programmer-selected language extensions (for their chosen libraries) into a specification for a new custom language that has language-level support for the libraries. We demonstrate how syntax and analysis are added to a language by extending Java with syntax from the query language SQL and static analysis of these constructs so that syntax and type errors in SQL queries can be detected at compile-time. 1...|$|R
5000|$|Conventional Web {{documents}} contain {{large amounts}} of structured data that can be rendered in web browsers. This approach works fine for publishing purposes, however, {{a large amount of}} data stored in Web documents cannot be processed this way. XHTML+RDFa can provide machine-readable metadata within the markup code which makes additional user functionalities available. Most important of all, actions can be performed automatically that enables up-to-date publishing, structured search and sharing.RDFa can serve as a bridge between the [...] "human and data webs".The potential in web documents enriched with RDFa is increasing since major search engines begin to process them while indexing. Yahoo indexes RDFa and microformats since 2008 and Google since 2009.The RDFa <b>attribute</b> <b>specifications</b> make it possible to describe structured data in any markup language. The RDFa markup in XHTML+RDFa reuses the markup code, thus eliminating the need for unnecessary duplications.XHTML+RDFa is not widely distributed yet, probably {{due to the lack of}} support in authoring tools and content management systems. However, there is good tendency. Drupal 7, for example, supports RDFa.Since the “a” in RDFa stands for attributes, it is straightforward to use CSS selectors to style the code.|$|R
50|$|Currently, only Remington makes {{production}} rifles in this chambering(Savage previously did so.) Remington, DoubleTap and Nosler are {{the only}} sources of factory ammunition. Loading dies and reloading data are readily available to the handloader. Double Tap loads to the <b>specifications</b> <b>attributed</b> to handloader limits.|$|R
40|$|AbstractBuilding {{verified}} compilers is difficult, {{especially when}} complex analyses such as type checking or data-flow analysis must be performed. Both the type checking and program optimization communities have developed methods for proving the correctness {{of these processes}} and developed tools for using, respectively, verified type systems and verified optimizations. However, {{it is difficult to}} use both of these analyses in a single declarative framework since these processes work on different program representations: type checking on abstract syntax trees and data-flow analysis-based optimization on control flow or program dependency graphs. We present an <b>attribute</b> grammar <b>specification</b> language that has been extended with constructs for specifying attribute-labelled control flow graphs and both CTL and LTL-FV formulas that specify data-flow analyses. These formulas are model-checked on these graphs to perform the specified analyses. Thus, verified type rules and verified data-flow analyses (verified either by hand or with automated proof tools) can both be transcribed into a single declarative framework based on attribute grammars to build a high-confidence language implementations. Also, the <b>attribute</b> grammar <b>specification</b> language is extensible so that it is relatively straight-forward to add new constructs for different temporal logics so that alternative logics and model checkers can be used to specify data-flow analyses in this framework...|$|R
40|$|Building {{verified}} compilers is difficult, {{especially when}} complex analyses such as type checking or data-flow analysis must be performed. Both the type checking and program optimization communities have developed methods for proving the correctness {{of these processes}} and developed tools for using, respectively, verified type systems and verified optimizations. However, {{it is difficult to}} use both of these analyses in a single declarative framework since these processes work on different program representations: type checking on abstract syntax trees and data-flow analysis-based optimization on control flow or program dependency graphs. We present an <b>attribute</b> grammar <b>specification</b> language that has been extended with constructs for specifying attribute-labelled control flow graphs and both CTL and LTL-FV formulas that specify data-flow analyses. These formulas are modelchecked on these graphs to perform the specified analyses. Thus, verified type rules and verified data-flow analyses (verified either by hand or with automated proof tools) can both be transcribed into a single declarative framework based on attribute grammars to build a high-confidence language implementations. Also, the <b>attribute</b> grammar <b>specification</b> language is extensible so that it is relatively straight-forward to add new constructs for different temporal logics so that alternative logics and model checkers can be used to specify data-flow analyses in this framework. Key words: compiler optimization, optimization verification, data flow analysis, attribute grammars...|$|R
40|$|Abstract. We {{describe}} how the standard genotype-phenotype mapping process of Grammatical Evolution (GE) {{can be enhanced}} with an attribute grammar to allow GE to operate as a decoder-based Evolutionary Algorithm (EA). Use of an attribute grammar allows GE to maintain context-sensitive and semantic information pertinent to the capacity constraints of the 01 Multiconstrained Knapsack Problem (MKP). An <b>attribute</b> grammar <b>specification</b> is used to perform decoding similar to a first-fit heuristic. The results presented are encouraging, demonstrating that GE in conjunction with attribute grammars can provide an improvement over the standard context-free mapping process for problems in this domain. ...|$|R
40|$|This paper {{presents}} XLOP (XML Language-Oriented Processing), {{an environment}} that permits the description of XML processing applications with attribute grammars (formalism {{used to describe the}} semantics of computer languages). The environment also includes a generator able to translate <b>attribute</b> grammar-based <b>specifications</b> into the specification language of CUP, a well-known YACC-like environment for writing language processors. The final goal of XLOP is to facilitate the development and maintenance of XML processing applications, whilst preserving the flexibility of general-purpose XML processing models. Also, since it is based on attribute grammars, XLOP provides a higher abstraction level than other similar translation scheme-based approaches...|$|R
