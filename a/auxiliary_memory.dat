67|17|Public
5000|$|The {{most common}} forms of <b>auxiliary</b> <b>memory</b> are flash memory, optical discs, {{magnetic}} disks and magnetic tape [...] The latest {{addition to the}} <b>auxiliary</b> <b>memory</b> family is flash memory. This form is much faster as compared to its predecessors, as this form of <b>auxiliary</b> <b>memory</b> does not involve any moving parts. In some laptops, this type of <b>auxiliary</b> <b>memory</b> {{is referred to as}} a solid state drive.|$|E
50|$|Media <b>Auxiliary</b> <b>Memory</b> or Medium <b>Auxiliary</b> <b>Memory</b> (MAM) {{refers to}} a chip {{embedded}} into a digital media device (usually a tape cartridge) that stores {{a small amount of}} data or metadata that a computer can read without having to read the actual tape.|$|E
5000|$|... drum <b>auxiliary</b> <b>memory</b> (50 [...] "fields" [...] of 2048 words each) and IBM 728 {{magnetic}} tape drives (32-bit words) ...|$|E
40|$|Caching is a {{fundamental}} technique commonly employed to hide the latency gap between memory and the CPU by exploiting locality in memory accesses. On today’s architectures a cache miss may cost several hundred CPU cycles [1]. In a two-level memory hierarchy, a cache performs faster than auxiliary storage, but is more expensive. Cost concerns thus usually limit cache size to {{a fraction of the}} <b>auxiliary</b> <b>memory’s</b> size. This paper represents a comparative predictability about some of the traditional and new replacement techniques in contrast with OPTIMAL replacement technique...|$|R
40|$|The ATLAS muon Cathode Strip Chamber (CSC) backend readout {{system has}} been upgrade during the LHC 2013 - 2015 {{shutdown}} {{to be able to}} handle the higher Level- 1 trigger rate of 100 kHz and the higher occupancy at Run 2 luminosity. The readout design is based on the Reconfigurable Cluster Element (RCE) concept for high bandwidth generic DAQ implemented on the Advanced Telecommunication Computing Architecture (ATCA) platform. The RCE design is based on the new System on Chip XILINX ZYNQ series with a processor-centric architecture with ARM processor embedded in FPGA fabric and high speed I/O resources together with <b>auxiliary</b> <b>memories</b> to form a versatile DAQ building block that can host applications tapping into both software and firmware resources. The Cluster on Board (COB) ATCA carrier hosts RCE mezzanines and an embedded Fulcrum network switch to form an online DAQ processing cluster. More compact firmware solutions on the ZYNQ for G-link, S-link and TTC allowed the full system of 320 G-links from the 32 chambers to be processed by 6 COBs in one ATCA shelf through software waveform feature extraction to output 32 S-links. The full system was installed in Sep/ 2014. We will present the RCE/COB design concept, the firmware and software processing architecture, and the experience from the intense commissioning towards LHC Run 2...|$|R
40|$|In {{this paper}} we analyze the {{performance}} of a traditional parametric search system and compare it to a system using an in <b>memory</b> <b>auxiliary</b> index. An analysis shows traditional database-based parametric systems incur a huge time hit due to disk accesses. We show that using an in-memory index in such scenarios results in huge time savings...|$|R
50|$|SSDs had {{origins in}} the 1950s with two similar technologies: {{magnetic}} core memory and charged capacitor read-only storage (CCROS). These <b>auxiliary</b> <b>memory</b> units (as contemporaries called them) emerged during the era of vacuum-tube computers, though their use ceased {{with the introduction of}} cheaper drum storage units.|$|E
50|$|Out-of-core or {{external}} memory algorithms are algorithms {{that are designed}} to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (<b>auxiliary</b> <b>memory)</b> such as hard drives or tape drives.|$|E
50|$|In {{computer}} science, {{selection sort}} is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists, and generally performs {{worse than the}} similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where <b>auxiliary</b> <b>memory</b> is limited.|$|E
40|$|We {{consider}} a scenario {{where we want}} to query a large dataset that is stored in external memory and does not fit into main memory. The most constrained resources in such a situation are the size of the main memory and the number of random accesses to external memory. We note that sequentially streaming data from external memory through main memory is much less prohibitive. We propose an abstract model of this scenario in which we restrict the size of the main memory and the number of random accesses to external memory, but do not restrict sequential reads. A distinguishing feature of our model is that it admits the usage of unlimited external memory for storing intermediate results, such as several hard disks that can be accessed in parallel. In practice, such <b>auxiliary</b> external <b>memory</b> can be crucial. For example, in a first sequential pass the data can be annotated, and in a second pass this annotation can be used to answer the query. Koch’s [9] ARB system for answering XPath queries is based on such a strategy. In this model, we prove lower bounds for sorting the input data. As opposed to related results for models without <b>auxiliary</b> external <b>memory</b> for intermediate results, we cannot rely on communication complexity to establish these lower bounds. Instead, we simulate our model by a non-uniform computation model for which we can establish the lower bounds by combinatorial means. 1...|$|R
50|$|In {{computer}} main <b>memory,</b> <b>auxiliary</b> {{storage and}} computer buses, data redundancy is {{the existence of}} data that is additional to the actual data and permits correction of errors in stored or transmitted data. The additional data can simply be a complete copy of the actual data, or only select pieces of data that allow detection of errors and reconstruction of lost or damaged data {{up to a certain}} level.|$|R
40|$|Abstract—This paper {{proposes a}} new fault {{tolerant}} cache organ-ization capable of dynamically mapping the in-use defective loca-tions in a processor cache to an <b>auxiliary</b> parallel <b>memory,</b> creating a defect-free {{view of the}} cache for the processor. While voltage scaling has a super-linear effect on reducing power, it exponentially increases the defect rate in memory. The ability of the proposed cache organization to tolerate {{a large number of}} defects makes it a perfect candidate for voltage-scalable architectures, especially in smaller geometries where manufacturing induced process varia-tion (MIPV) is expected to rapidly increase. The introduced fault tolerant architecture consumes little energy and area overhead, but enables the system to operate correctly and boosts the system per-formance close to a defect-free system. Power savings of over 40 % is reported on standard benchmarks while the performance degra-dation is maintained below 1 %. Index Terms—Cache, fault tolerance, low power, process varia-tion, SRAM, voltage scaling. I...|$|R
50|$|<b>Auxiliary</b> <b>memory,</b> {{also known}} as {{auxiliary}} storage, secondary storage, secondary memory or external memory, is a non-volatile memory (does not lose stored data when the device is powered down) that is not directly accessible by the CPU, {{because it is not}} accessed via the input/output channels (it is an external device). In RAM devices (as flash memory) data can be directly deleted or changed.|$|E
50|$|As block sort is non-recursive {{and does}} not require the use of dynamic allocations, this leads to {{constant}} stack and heap space. It uses O(1) <b>auxiliary</b> <b>memory</b> in a transdichotomous model, which accepts that the O(log n) bits needed {{to keep track of the}} ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated.|$|E
50|$|This {{proprietary}} hardware standard supported {{a wide range}} of accessories. The interface inspired many creative products because it made it possible to use the PDA as a flexible interface to otherwise technically difficult to manage sophisticated hardware. Among the many available modules were <b>auxiliary</b> <b>memory,</b> game plug-ins, GPS navigation receivers, cameras, GSM and CDMA mobile phones, versatile infrared remote controls, wired and wireless network interface cards, Bluetooth modules, MP3 players, cordless phones with answering machines, and voice recorders.|$|E
40|$|International audienceTree automata {{with one}} memory have been {{introduced}} in 2001. They generalize both pushdown (word) automata and the tree automata with constraints of equality between brothers of Bogaert and Tison. Though it has a decidable emptiness problem, the main weakness of this model is its lack of good closure properties. We propose a generalization of the visibly pushdown automata of Alur and Madhusudan to a family of tree recognizers which carry along their (bottom-up) computation an <b>auxiliary</b> unbounded <b>memory</b> with a tree structure (instead of a symbol stack). In other words, these recognizers, called visibly Tree Automata with Memory (VTAM) define a subclass of tree automata with one memory enjoying Boolean closure properties. We show in particular {{that they can be}} determinized and the problems like emptiness, inclusion and universality are decidable for VTAM. Moreover, we propose an extension of VTAM whose transitions may be constrained by structural equality and disequality tests between memories, and show that this extension preserves the good closure and decidability properties...|$|R
40|$|An {{algorithm}} {{is presented to}} answer window queries in a quadtree-based spatial database environment by retrieving all of the quadtree blocks in the underlying spatial database that cover the quadtree blocks that comprise the window. It works by decomposing the window operation into sub-operations over smaller window partitions. These partitions are the quadtree blocks corresponding to the window. Although a block b in the underlying spatial database may cover several of the smaller window partitions, b is only retrieved once rather than multiple times. This is achieved by using an <b>auxiliary</b> main <b>memory</b> data structure called the active border which requires O(n) additional storage for a window query of size n n. As a result, the algorithm generates an optimal number of disk I/O requests to answer a window query (i. e., one request per covering quadtree block). A proof of correctness and {{an analysis of the}} algorithm's execution time and space requirements are given, as are some experimental results...|$|R
40|$|A ternary/binary data coding {{algorithm}} and {{conditions under which}} Hopfield networks implement optimal convolutional or Hamming de{{coding algorithm}}s has been described. Using the coding/decoding approach (an optimal Binary Signal Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model (NNAMM) is built. The model provides optimal (the best) basic memory performance and demands {{the use of a}} new memory unit architecture with two-layer Hopfield network, N-channel time gate, <b>auxiliary</b> reference <b>memory,</b> and two nested feedback loops. NNAMM explicitly describes the dependence on time of a memory trace retrieval, gives a possibility of metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. A model of smallest inseparable part or an "atom" of consciousness is also defined. The NNAMM's neurobiological backgrounds and its applications to solving some interdisciplinary problems are shortly discussed. BSDT could implement the "best neural code" used in nervous tissues of animals and humans. Comment: The revised contribution submitted to the 1 st International Workshop on the Future of Neural Networks (FUNN 2003) held on July 5, 2003, Eindhoven, the Netherlands (Workshop affiliated to ICALP 2003, June 30 - July 4, 2003), 12 pages, 3 Figures, 23 reference...|$|R
50|$|A Stream X-Machine (SXM) is an {{extended}} {{finite state machine}} with <b>auxiliary</b> <b>memory,</b> inputs and outputs. It is {{a variant of the}} general X-machine, in which the fundamental data type X = Out* × Mem × In*, that is, a tuple consisting of an output stream, the memory and an input stream. A SXM separates the control flow of a system from the processing carried out by the system. The control is modelled by a finite state machine (known as the associated automaton) whose transitions are labelled with processing functions chosen from a set Φ (known as the type of the machine), which act upon the fundamental data type.|$|E
5000|$|Because of {{the above}} equivalence, {{attention}} may focus on the way a Stream X-Machine processes inputs into outputs, using an <b>auxiliary</b> <b>memory.</b> Given an initial memory state mem0 and an input stream ins, the machine executes in a step-wise fashion, consuming one input at a time, and generating one output at a time. Provided that (at least) one recognised path path = φ1 ... φn exists leading to a state in which the input has been consumed, the machine yields a final memory state memn and an output stream outs. In general, {{we can think of}} this as the relation computed by all recognised paths: | path | : In* → Out*. This is often called the behaviour of the Stream X-Machine.|$|E
50|$|Saturn System's Accelerator II was the {{original}} accelerator for the Apple II series of computers. The card accelerated the Apple II and the Apple II Plus using a faster MOS 6502 microprocessor and on-board high speed RAM. When the accelerator card was activated, software would execute within the CPU and memory on the card, not utilizing those components on the motherboard. The card used a series of 8 DIP switches to configure slot access speeds {{as well as the}} speed of the card. Since the Accelerator was released before Apple Computer's introduction of the Apple IIe. The card would run in an Apple IIe, however software which required a 65C02 microprocessor or used <b>auxiliary</b> <b>memory</b> would not function properly; a problem which was solved with the Accelerator //e, a complete redesign. Saturn Systems changed their name during the early 1980s to Titan Technologies due to trademark complications.|$|E
40|$|An {{extended}} abstract containing some of {{the results}} {{presented in this paper}} has appeared in the proceeding of FOSSACS' 07. International audienceTree automata with one memory have been introduced in 2001. They generalize both pushdown (word) automata and the tree automata with constraints of equality between brothers of Bogaert and Tison. Though it has a decidable emptiness problem, the main weakness of this model is its lack of good closure properties. We propose a generalization of the visibly pushdown automata of Alur and Madhusudan to a family of tree recognizers which carry along their (bottom-up) computation an <b>auxiliary</b> unbounded <b>memory</b> with a tree structure (instead of a symbol stack). In other words, these recognizers, called Visibly Tree Automata with Memory (VTAM) define a subclass of tree automata with one memory enjoying Boolean closure properties. We show in particular that they can be determinized and the problems like emptiness, membership, inclusion and universality are decidable for VTAM. Moreover, we propose several extensions of VTAM whose transitions may be constrained by different kinds of tests between memories and also constraints a la Bogaert and Tison. We show that {{some of the}}se classes of constrained VTAM keep the good closure and decidability properties, and we demonstrate their expressiveness with relevant examples of tree languages...|$|R
40|$|Tree automata {{with one}} memory have been {{introduced}} in 2001. They generalize both pushdown (word) automata and the tree automata with constraints of equality between brothers of Bogaert and Tison. Though it has a decidable emptiness problem, the main weakness of this model is its lack of good closure properties. We propose a generalization of the visibly pushdown automata of Alur and Madhusudan to a family of tree recognizers which carry along their (bottom-up) computation an <b>auxiliary</b> unbounded <b>memory</b> with a tree structure (instead of a symbol stack). In other words, these recognizers, called Visibly Tree Automata with Memory (VTAM) define a subclass of tree automata with one memory enjoying Boolean closure properties. We show in particular {{that they can be}} determinized and the problems like emptiness, membership, inclusion and universality are decidable for VTAM. Moreover, we propose several extensions of VTAM whose transitions may be constrained by different kinds of tests between memories and also constraints a la Bogaert and Tison comparing brother subtrees in the tree in input. We show that some of these classes of constrained VTAM keep the good closure and decidability properties, and we demonstrate their expressiveness with relevant examples of tree languages. Comment: 36 pages including an appendi...|$|R
40|$|Under Revision for submissionInternational audienceHigh-order Discontinuous Galerkin Methods (DGM) are now {{routinely}} {{used for}} simulation of wave propagation, especially for geophysic applications. However, to fully {{take advantage of}} the high-order space discretization, it is relevant to use a high-order time discretization. Hence, DGM are classicaly coupled with ADER schemes, which leads to high-order explicit time schemes, but requires the introduction of <b>auxiliary</b> unknowns. The <b>memory</b> can thus be considerably cluttered up. In this work, we propose a new high order time scheme, the so-called Nabla-p scheme. This scheme does not increase the storage costs since it is a single step method which does not require the storage of auxiliary unknowns. Numerical results show that it requires less storage and less computational costs than the DG-ADER scheme for a given accuracy...|$|R
5000|$|... def lcs(S): S += S # Concatenate {{string to}} it self to avoid modular {{arithmetic}} f = -1 * len(S) # Failure function k = 0 # Least rotation of string found so far for j in xrange(1,len(S)): sj = Sj i = fj-k-1 while i != -1 and sj != Sk+i+1: if sj < Sk+i+1: k = j-i-1 i = fi if sj != Sk+i+1: # if sj != Sk+i+1, then i == -1 [...] if sj < Sk: # k+i+1 = k k = j fj-k = -1 else: fj-k = i+1 return kOf interest is that removing all {{lines of code}} which modify the value of [...] results in the original Knuth-Morris-Pratt preprocessing function, as [...] (representing the rotation) will remain zero. Booth's algorithm runs in [...] time, where [...] is {{the length of the}} string. The algorithm performs at most [...] comparisons in the worst case, and requires <b>auxiliary</b> <b>memory</b> of length [...] to hold the failure function table.|$|E
50|$|Word {{line core}} memory was {{often used to}} provide {{register}} memory. Other names for this type are linear select and 2-D. This form of core memory typically wove three wires through each core on the plane, word read, word write, and bit sense/write. To read or clear words, the full current is applied {{to one or more}} word read lines; this clears the selected cores and any that flip induce voltage pulses in their bit sense/write lines. For read, normally only one word read line would be selected; but for clear, multiple word read lines could be selected while the bit sense/write lines ignored. To write words, the half current is applied to one or more word write lines, and half current is applied to each bit sense/write line for a bit to be set. In some designs, the word read and word write lines were combined into a single wire, resulting in a memory array with just two wires per bit. For write, multiple word write lines could be selected. This offered a performance advantage over X/Y line coincident-current in that multiple words could be cleared or written with the same value in a single cycle. A typical machine's register set usually used only one small plane of this form of core memory. Some very large memories were built with this technology, for example the Extended Core Storage (ECS) <b>auxiliary</b> <b>memory</b> in the CDC 6600, which was up to 2 million 60-bit words.|$|E
50|$|The Oak Ridge National Laboratory Review states:Ridge National Laboratory {{engineers}} assisted Argonne Laboratory {{during the}} early 1950s in design and fabrication of the Oak Ridge Automatic Computer and Logical Engine. Its name was selected with reference to a lyrical acronym from Greek mythology—ORACLE, defined as 'a shrine in which a deity reveals hidden knowledge.'Assembled before the development of transistors and microchips, the ORACLE was a large scientific digital computer that used vacuum tubes. It had an original storage capacity of 1024 words of 40 bits each (later doubled to 2048 words). The computer also contained a magnetic-tape <b>auxiliary</b> <b>memory</b> and an on-line cathode-tube plotter, a recorder, and a typewriter. Operational in 1954, for a time the ORACLE had the fastest speed and largest data storage capacity of any computer in the world. Problems that would have required two mathematicians with electric calculators three years to solve could {{be done on the}} ORACLE in 20 minutes.Alston Householder and the Mathematics Panel used the ORACLE to analyze radiation and shielding problems. In 1957, Hezz Stringfield and Ward Foster, both of the Budget Office, also adopted the ORACLE for more mundane but equally important tasks—annual budgeting and monthly financial accounting. As one of the last 'homemade computers,' the ORACLE became obsolete by the 1960s. The Laboratory then purchased or leased its mainframe computers from commercial suppliers. From the initial applications of the ORACLE to nuclear aircraft problems, computer enthusiasm spread like lightning throughout the Laboratory, and in time, use of the machines became common in all the Laboratory's divisions.|$|E
40|$|It {{has been}} shown that a neural network model {{recently}} proposed to describe basic memory performance is based on a ternary/binary coding/decoding algorithm which leads to a new neural network assembly memory model (NNAMM) providing maximum-likelihood recall/recognition properties and implying a new memory unit architecture with Hopfield two-layer network, N-channel time gate, <b>auxiliary</b> reference <b>memory,</b> and two nested feedback loops. For the data coding used, conditions are found under which a version of Hopfied network implements maximum-likelihood convolutional decoding algorithm and, simultaneously, linear statistical classifier of arbitrary binary vectors with respect to Hamming distance between vector analyzed and reference vector given. In addition to basic memory performance and etc, the model explicitly describes the dependence on time of memory trace retrieval, gives a possibility of one-trial learning, metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. It {{has been shown}} that an assembly memory unit may be viewed as a model of a smallest inseparable part or an 'atom' of consciousness. Some nontraditional neurobiological backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and error detector neurons, early precise spike firing, etc) and the model's application to solve some interdisciplinary problems from different scientific fields are discussed. Comment: A report presented at the Vth International Congress on Mathematical Modeling held in Dubna, Russia on September 30 - October 6, 2002 (Book of Abstracts, vol. 2. p. 91), 39 pages, 5 Figures, 1 Table, 70 reference...|$|R
40|$|Abstract—Flash {{memory is}} now {{widely used in}} the design of {{solid-state}} disks (SSDs) as they are able to sustain significantly higher I/O rates than even high-performance hard disks, while using significantly less power. These characteristics make SSDs especially attractive for use in enterprise storage systems, and it is predicted that the use of SSDs will save 58, 000 MWh/year by 2013. However, Flash-based SSDs are unable to reach peak performance on common enterprise data patterns such as log-file and metadata updates due to slow write speeds (an orderof-magnitude slower than reads) and the inability to do in-place updates. In this paper, we utilize an <b>auxiliary,</b> byte-addressable, non-volatile <b>memory</b> to design a general purpose merge cache that significantly improves write performance. We also utilize simple read policies that further improve the performance of the SSD without adding significant overhead. Together, these policies reduce the average response time by more than 75 %, making it possible to meet performance requirements with fewer drives. Keywords-Servers; Storage; Solid-State Disks I...|$|R
40|$|The {{ungrammatical}} sentence "The key to {{the cabinets}} are on the table" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence "The {{key to the}} cabinet are on the table". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account {{is in terms of}} cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the <b>auxiliary</b> initiates a <b>memory</b> access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts. Comment: 6 pages, 2 figures, 1 table, submitted to MathPsych/ICCM 2017, Warwick, U...|$|R
40|$|A multiple-valued input address {{generator}} produces a unique address given a multiple-valued input data vector. This paper presents methods to realize multiple-valued input {{address generator}}s by multi-level networks of p-input q-output memories. It shows {{a method to}} simplify the address generators using an <b>auxiliary</b> <b>memory.</b> ...|$|E
40|$|Catalytic {{computation}}, {{defined by}} Buhrman, Cleve, Koucký, Loff and Speelman (STOC 2014), is a space-bounded computation where {{in addition to}} our working memory we have an exponentially larger <b>auxiliary</b> <b>memory</b> which is full; the <b>auxiliary</b> <b>memory</b> may be used throughout the computation, {{but it must be}} restored to its initial content {{by the end of the}} computation. Motivated by the surprising power of this model, we set out to study the non-deterministic version of catalytic computation. We establish that non-deterministic catalytic log-space is contained in ZPP, which is the same bound known for its deterministic counterpart, and we prove that non-deterministic catalytic space is closed under complement (under a standard derandomization assumption). Furthermore, we establish hierarchy theorems for non-deterministic and deterministic catalytic computation...|$|E
40|$|AbstractGiven a {{rectangular}} m×n matrix stored as a two-dimensional array, {{we want to}} transpose it in place and measure the cost {{by the number of}} memory writes and the number of auxiliary cells used. We propose a transposition algorithm with optimal complexity O(mn) using only min(m,n) <b>auxiliary</b> <b>memory</b> cells...|$|E
40|$|New features: Spinor-GTO {{evaluator}} Dirac-Kohn-Sham (LDA functional) EDIIS and ADIIS Periodic CCSD with k-point sampling Periodic EOM-IP-CCSD and EOM-EA-CCSD {{for single}} k-point calculation spin-square value (per unit) of KUHF calculation Update interface to fciqmc for standalone executing Routines in fciqmc {{to read in}} the spinned one and two RDMs Heat-Bath CI Functions in dmrgci interface to access 3 -pdm and 4 -pdm Function get_fermi UCCSD lambda equation and 1, 2 -particle density matrix SCF wfn stability analysis Many-Body van der Waals Interactions (MBD) Second order SCF solver for periodic HF and DFT TDDFT for periodic k-point HF and DFT U-TDHF and U-TDDFT for molecular and crystal systems Many-body dispersion MP 2 -F 12 and F 12 basis and F 12 RI basis Cartesian GTO (6 d 10 f) basis in molecular calculations CP 2 K's HF pseudopotential data Frozen core MP 2 Molecular electrostatic potential (MEP) CPHF and UCPHF solver Non-relativistic RHF, UHF 4 -component UHF spin-spin coupling Generalized Hartree-Fock (GHF) Second order SCF solver for GHF non-relativistic UHF, UKS g-tensor non-relativistic UHF, UKS hyperfine coupling SHCI interface to Dice program spin-orbital CISD UCISD and UCISD 1 - and 2 -RDM Restricted CC 2 method Density-fitting CCSD Heat-bath selected CI (HCI) spin-orbital 1 - and 2 -RDM "scanner" function for HF, DFT and CCSD to simplify energy or gradients scanning for systems of different geometry. Interface to pyberny geometry optimizer (geometry optimization for RHF, RKS and RCCSD are supported). Improvements: Performance of PBC-Gaussian function evaluator Performance of analytical Fourier transformation for AO product Performance of PBC 3 -center integrals Performance of PBC PP local-part integrals Numerical stability associated to OpenMP reduce function Performance of FCI 2 -electron contraction function Basis parser for Pople style basis sets Arbitrary problem size in FCI solver Symmetry labels in orbital coefficients Disk usage of integral transformation in MP 2 Performance of J/K contractions in molecular density fitting code Input geometry parser for ghost atoms U-CCSD(T) performance ECP basis localization in Mulliken pop analysis Changing the CASCI/CASSCF default FCI solver (the default solver will not use spin symmetry for singlet state) Supporting remove_linear_dep function to handle basis linear dependence in k-point SCF cell. rcut estimation for better integral accuracy Convergence rates of PM localization MP 2 and RCISD integral transformation performance Disk usage of CCSD-DIIS Input basis parser to support union basis set (eg mol. basis=(bas 1,bas 2)) SCF initial guess for systems with pseudopotential (or ECP) SCF initial guess for low-dimension PBC systems Kinetic energy cutoff estimation Density fitting default <b>auxiliary</b> basis <b>Memory</b> usage for FFTDF module libxc interface See also the release notes _ This release contains contributions from: Qiming Sun, Lucas K. Wagner, James D. McClain, Timothy Berkelbach, Alexander Sokolov, jim, Bastien Mussard, Jan Hermann, Mark J. Williamson, Mark Williamson, Kevin Koh, Susi Lehtola, Sandeep Sharma, januseriksen, James Smith, George Booth...|$|R
40|$|Algorithms are {{developed}} which construct from a given LL(l) grammar a recursive descent parser with as much, recursion resolved by iteration as is possible without introducing <b>auxiliary</b> <b>memory.</b> Unlike other proposed methods {{in the literature}} designed to arrive at parsers of this kind, the algorithms do not require extensions of the notational formalism nor alter the grammar in any way. The algorithms constructing the parsers operate in 0 (k«s) steps, where s {{is the size of}} the grammar, i. e. the sum of the lengths of all productions, and k is a grammar- dependent constant. A speedup of the algorithm is possible which improves the bound to 0 (s) for all LL(l) grammars, and constructs smaller parsers with some <b>auxiliary</b> <b>memory</b> in form of parameters to some of the routines...|$|E
40|$|Abstract. Using an <b>auxiliary</b> <b>memory</b> {{smaller than}} {{the size of this}} abstract, the LogLog {{algorithm}} makes it possible to estimate in a single pass and within a few percents the number of different words in the whole of Shakespeare’s works. In general the LogLog algorithm makes use of m “small bytes ” of <b>auxiliary</b> <b>memory</b> in order to estimate in a single pass the number of distinct elements (the “cardinality”) in a file, and it does so with an accuracy that is of the order of 1 / √ m. The “small bytes ” to be used in order to count cardinalities till Nmax comprise about log log Nmax bits, so that cardinalities well in the range of billions can be determined using one or two kilobytes of memory only. The basic version of the LogLog algorithm is validated by a complete analysis. An optimized version, super–LogLog, is also engineered and tested on real-life data. The algorithm parallelizes optimally. ...|$|E
