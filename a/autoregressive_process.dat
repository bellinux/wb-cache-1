926|1093|Public
25|$|In {{statistic}}s, the Durbin–Watson statistic is a {{test statistic}} used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order <b>autoregressive</b> <b>process.</b> Later, John Denis Sargan and Alok Bhargava developed several von Neumann–Durbin–Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order autoregression (Sargan and Bhargava, 1983). Note that the distribution of this test statistic {{does not depend on}} the estimated regression coefficients and the variance of the errors.|$|E
500|$|The autoregressive {{and moving}} average {{processes}} are types of stochastic {{processes that are}} used to model discrete-time empirical time series data, especially in economics. The <b>autoregressive</b> <b>process</b> or model treats a stochastic variable as depending on its own prior values and on a current independently and identically distributed stochastic term. The moving average model treats a stochastic variable as depending on the current and past values of an iid stochastic variable.|$|E
5000|$|... for {{parameter}} [...] The [...] case of {{the tent}} map is the present case of [...] A sequence {} {{will have the same}} autocorrelation function [...] as will data from the first-order <b>autoregressive</b> <b>process</b> [...] with {} independently and identically distributed. Thus data from an asymmetric tent map cannot be distinguished, using the autocorrelation function, from data generated by a first-order <b>autoregressive</b> <b>process.</b>|$|E
40|$|The {{coexistence}} of cycles with different periods complicates {{the assessment of}} the current macroeconomic conditions. In order to overcome this problem, a modeling of multiple <b>autoregressive</b> <b>processes</b> in a univariate time series is presented. In the proposed model, individual <b>autoregressive</b> <b>processes</b> are assumed to be mutually uncorrelated, and the number of individual <b>autoregressive</b> <b>processes</b> is determined using information criteria. Simulation results show that the proposed procedure is sufficiently applicable for measuring major and minor cycles. Empirical applications suggest the usefulness and limitations of the proposed method. Major cycle Minor cycle Multiple <b>autoregressive</b> <b>processes...</b>|$|R
40|$|The local {{asymptotic}} normality {{for a class}} of generalized random coefficient <b>autoregressive</b> <b>processes</b> is established. This property implies the asymptotic optimality of the maximum likelihood estimator and the related test statistics. The model includes standard random coefficient <b>autoregressive</b> <b>processes,</b> Markovian bilinear models, and random coefficient exponential <b>autoregressive</b> <b>processes</b> as special cases. Nonlinear time series Asymptotic inference, Random coefficient models Local {{asymptotic normality}}...|$|R
40|$|Almost sure {{convergence}} {{properties of}} least-squares estimates in stochastic regression models and an asymptotic theory of related Euclidean projections are developed herein. Applications to <b>autoregressive</b> <b>processes</b> and to dynamic input-output systems are also discussed. Stochastic regressors least squares estimates projections strong consistency dynamic models <b>autoregressive</b> <b>processes</b> minimum eigenvalue martingales...|$|R
5000|$|Consider a discrete-time {{stochastic}} process , and suppose {{that it can}} be written as an <b>autoregressive</b> <b>process</b> of order p: ...|$|E
5000|$|... where k is {{the slope}} of the trend and [...] is noise (white noise in the {{simplest}} case; more generally, noise following its own stationary <b>autoregressive</b> <b>process).</b> Here any transient noise will not alter the long-run tendency for [...] to be on the trend line, as also shown in the graph. This process is said to be trend stationary because deviations from the trend line are stationary.|$|E
5000|$|He made {{contributions}} {{in the areas of}} optimal filter banks, nonlinear phase extensions of discrete Walsh-Hadamard transform [...] and discrete Fourier transform, principal component analysis of first-order <b>autoregressive</b> <b>process,</b> sparse approximation, [...] financial signal processing and quantitative finance. He co-authored and co-edited the books in financial signal processing & engineering entitled A Primer for Financial Engineering: Financial Signal Processing and Electronic Trading and Financial Signal Processing and Machine Learning, respectively.|$|E
40|$|This paper {{establishes}} several almost sure asymptotic {{properties of}} general <b>autoregressive</b> <b>processes.</b> By {{making use of}} these properties, we obtain a proof of the strong consistency of the least-squares estimates of {{the parameters of the}} process without any assumption on the roots of the characteristic polynomial. <b>Autoregressive</b> <b>processes</b> characteristic polynomial purely explosive and non-explosive models least-squares estimates strong consistency...|$|R
40|$|It {{is shown}} here that Bahadur's [Ann. Math. Statist. (1966) 37, 577 - 580] almost sure (a. s.) {{asymptotic}} {{representation of a}} sample quantile for independent and identically distributed random variables holds under certain regularity conditions for a general class of stationary multivariate <b>autoregressive</b> <b>processes.</b> This yields the asymptotic (multi-) normality of the standardized forms of quantiles in <b>autoregressive</b> <b>processes.</b> Other useful applications will be considered in a subsequent paper. Almost sure representation asymptotic normality <b>autoregressive</b> (multivariate) <b>process</b> sample quantiles...|$|R
40|$|We {{consider}} stationary <b>autoregressive</b> <b>processes</b> with coefficients {{restricted to}} an ellipsoid, which includes <b>autoregressive</b> <b>processes</b> with absolutely summable coefficients. We provide consistency results under different norms for {{the estimation of}} such processes using constrained and penalized estimators. As an application we show some weak form of universal consistency. Simulations show that directly including the constraint in the estimation can lead to more robust results...|$|R
5000|$|... where [...] is a constant, [...] the {{coefficient}} on a time trend and [...] the lag {{order of the}} <b>autoregressive</b> <b>process.</b> Imposing the constraints [...] and [...] corresponds to modelling a random walk and using the constraint [...] corresponds to modeling a random walk with a drift. Consequently, there are three main versions of the test, analogous to the ones discussed on Dickey-Fuller test (see that page for a discussion on dealing with uncertainty about including the intercept and deterministic time trend terms in the test equation.) ...|$|E
5000|$|Granger {{causality}} index {{showing the}} driving of channel x by channel y {{is defined as}} the logarithm of the ratio of residual variance for one channel to the residual variance of the two-channel model:GCIy→x = ln (e/e1) This definition can be extended to the multichannel system by considering how the inclusion of the given channel changes the residual variance ratios. To quantify directed influence from a channel xj to xi for n channel <b>autoregressive</b> <b>process</b> in time domain we consider n and n&minus;1 dimensional MVAR models. First, the model is fitted to whole n-channel system, leading to the residual variance Vi,n(t) = var(Ei,n(t)) for signal xi. Next, a n&minus;1 dimensional MVAR model is fitted for n&minus;1 channels, excluding channel j, which leads to the residual variance Vi,n&minus;1(t) = var (Ei,n&minus;1(t)). Then Granger causality is defined as: ...|$|E
50|$|In {{statistic}}s, the Durbin-Watson statistic is a {{test statistic}} used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order <b>autoregressive</b> <b>process.</b> Later, John Denis Sargan and Alok Bhargava developed several von Neumann-Durbin-Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order autoregression (Sargan and Bhargava, 1983). Note that the distribution of this test statistic {{does not depend on}} the estimated regression coefficients and the variance of the errors.|$|E
40|$|AbstractIt {{is shown}} here that Bahadur's [Ann. Math. Statist. (1966) 37, 577 – 580] almost sure (a. s.) {{asymptotic}} {{representation of a}} sample quantile for independent and identically distributed random variables holds under certain regularity conditions for a general class of stationary multivariate <b>autoregressive</b> <b>processes.</b> This yields the asymptotic (multi-) normality of the standardized forms of quantiles in <b>autoregressive</b> <b>processes.</b> Other useful applications will be considered in a subsequent paper...|$|R
40|$|We {{consider}} {{large and}} moderate {{deviations for the}} empirical mean and covariance of hilbertian <b>autoregressive</b> <b>processes.</b> As an application we obtain moderate deviations principles for the eigenvalues and associated projectors of the empirical covariance. Deviations principles <b>Autoregressive</b> hilbertian <b>processes</b> Covariance operators Functional principal component analysis...|$|R
50|$|Dmitrienko, A. and Vexler, A. (1996). Renewal theory {{results for}} <b>autoregressive</b> <b>processes.</b> Math. Methods Statist., 5 no. 4, 477-490.|$|R
40|$|Abstract. The <b>autoregressive</b> <b>process</b> {{takes an}} {{important}} part in predicting problems leading to decision making. In practise, we use the least squares method to estimate the parameter of the <b>autoregressive</b> <b>process.</b> In the case of the order one <b>autoregressive</b> <b>process</b> we know that the least squares estimator converge in probability to the unknown parameter µ. In this work we show that the least squares estimator converge almost surely to µ and so we construct the inequalities of type Bernstein-Fréchet for the coe¢cient of the order 1 <b>autoregressive</b> <b>process.</b> Using these inequalities a con…dence interval is then obtained. 1. Introduction. In general, linears process, and in particular, <b>autoregressive</b> <b>process,</b> take {{an important part}} in predicting problems leading to decision making. In their beginning works, Chan et Wei [2] evaluated the limit in law of the least squares estimate, in this case the errors make up a sequence of di¤erences betwee...|$|E
40|$|We {{present a}} new {{stochastic}} process called the punctured autoregressive (AR) process, {{and use it}} to model both the variable bit rate (VBR) video traffic and the wireless channel dynamics. To model the VBR video traffic, we propose to use punctured autoregressive processes modulated by a doubly Markov process. The doubly Markov process models the state of each video frame while the autoregressive processes describe the number of bits of each frame at each state. The punctured <b>autoregressive</b> <b>process</b> considers the timing information between frames of the same state and thus gives better modeling performance. The model captures the long-range dependency (LRD) characteristics as well as the shortrange dependency (SRD) characteristics of the video traffic. Queuing behavior of the punctured <b>autoregressive</b> <b>process</b> is also closer to the real video traffic than the conventional <b>autoregressive</b> <b>process.</b> In addition to video traffic modeling, we also apply the same model to wireless channel dynamics. The channel error rate is modeled as a single Markov modulated punctured <b>autoregressive</b> <b>process.</b> The synthetic channel error rate generated by the punctured <b>autoregressive</b> <b>process</b> performs closer to the real channel error rate than the one generated by the conventional <b>autoregressive</b> <b>process.</b> I...|$|E
40|$|The most {{suitable}} probability distribution for estimating flood risks using short-term data {{is presented in}} this paper. A simulation method using <b>autoregressive</b> <b>process</b> to represent daily flows are used. Several distributions were fitted to the annual peaks of <b>autoregressive</b> <b>process.</b> The levels determined using the distributions were compared with the levels from empirical distribution determined using ordered peaks of 1000 <b>autoregressive</b> <b>process.</b> It {{was found that the}} Lognormal III, Pearson Type III and the Weibull gave good risks estimates of flood of high return period...|$|E
50|$|Unit root processes, trend {{stationary}} <b>processes,</b> <b>autoregressive</b> <b>processes,</b> {{and moving}} average processes are specific forms of processes with autocorrelation.|$|R
3000|$|... see Rieder [28], Iacus [29], Bishwal [30], Shimizu [31] and Zhang and Zhang [32]), {{constant}} coefficient <b>autoregressive</b> <b>processes</b> (when [...]...|$|R
40|$|We {{consider}} stationary <b>autoregressive</b> <b>processes</b> {{of order}} p which have positive innovations. We propose consistent parameter estimators based on linear programming. Under conditions, including regular variation {{of either the}} left or right tail of the innovations distribution, we prove that the estimators have a limit distribution. The rate of convergence of our estimator is favorable compared with the Yule [...] Walker estimator under comparable circumstances. Poisson <b>processes</b> Linear programming <b>Autoregressive</b> <b>processes</b> Parameter estimation Weak convergence Consistency Time series analysis...|$|R
40|$|We {{introduce}} a stationary uniform <b>autoregressive</b> <b>process</b> of second order. Spectral density, autocovariance and autocorrelation functions are derived. The unknown parameters {{of this model}} are estimated by the conditional least squares. Uniform <b>autoregressive</b> <b>process</b> of the second order Conditional least squares estimation Strong consistency Asymptotic normality...|$|E
40|$|An {{empirical}} Bayes predictor {{is derived}} for a vector of future observations in a mixed linear model with errors following a first-order <b>autoregressive</b> <b>process.</b> The consistency and the asymptotic normality of the empirical Bayes predictor are established. Mixed linear model <b>Autoregressive</b> <b>process</b> Bayes and empirical Bayes prediction Asymptotics...|$|E
40|$|In time series, {{structural}} {{break point}} {{can be considered}} as Level Shift, one type of aberrant observation. Types of aberrant observations, especially Level Shift, and Bayesian <b>autoregressive</b> <b>process</b> are mentioned in the study. In this extent, the ability of finding Level Shift with Bayesian <b>Autoregressive</b> <b>process</b> is also demonstrated on real data...|$|E
40|$|We {{consider}} {{the law of}} the iterated logarithm for the empirical covariance of Hilbertian <b>autoregressive</b> <b>processes.</b> As an application, we obtain laws of the iterated logarithm for the eigenvalues and associated projectors of the empirical covariance. Laws of the iterated logarithm <b>Autoregressive</b> Hilbertian <b>processes</b> Covariance operators Functional principal component analysis...|$|R
40|$|Heavy-tailed <b>autoregressive</b> <b>processes</b> defined {{with minimum}} or maximum {{operator}} are good alternatives to classic linear ARMA with heavy tail noises, in what concerns extreme values modeling. In this paper {{we present a}} full characterization of the tail dependence of the <b>autoregressive</b> minima <b>process,</b> Yeh-Arnold-Robertson Pareto(III). Fundação para a Ciência e a Tecnologia (FCT...|$|R
40|$|Nonlinear <b>autoregressive</b> <b>processes</b> {{constitute}} a potentially important class of nonlinear signal models {{for a wide}} range of signal processing applications involving both natural and man-made phenomena. A state space characterization is used to develop algorithms for modeling and estimating signals as nonlinear <b>autoregressive</b> <b>processes</b> from noise-corrupted measurements. Special attention is given to chaotic processes, which form an important subclass of nonlinear <b>autoregressive</b> <b>processes.</b> The modeling algorithms are based on the method of total least-squares, and exploit the local structure of the signals in state space. The recursive estimation algorithms for addressing problems of filtering, prediction, and smoothing, are based on extended Kalman estimators, and jointly exploit aspects of both the temporal and state-space structure in these processes. The resulting algorithms are practical both in terms of computation and storage requirements, and their effectiveness is verified throug [...] ...|$|R
40|$|AbstractA general Markov {{process with}} {{innovation}} is introduced and its properties are studied. Based {{on the structure}} of this process, one can develop any <b>autoregressive</b> <b>process</b> of first order minification structure as a special case of this. A necessary and sufficient condition for the general <b>autoregressive</b> <b>process</b> to be stationary is presented. A characterization of semi-Pareto process is obtained...|$|E
40|$|The maximum {{likelihood}} {{estimation of the}} parameters of a complex-valued zero-mean normal stationary first-order <b>autoregressive</b> <b>process</b> is investigated. It is shown that the likelihood function corresponding to independent replicated series is uniquely maximized {{at a point in}} the interior of the parameter space. A closed-form expression is given for the estimator. <b>autoregressive</b> <b>process</b> complex-valued {{maximum likelihood}} estimation...|$|E
40|$|Abstract. The {{purpose of}} this paper is to {{investigate}} the asymptotic behavior of the Durbin-Watson statistic for the general stable p−order <b>autoregressive</b> <b>process</b> when the driven noise is given by a first-order <b>autoregressive</b> <b>process.</b> We estab-lish the almost sure convergence and the asymptotic normality for both the least squares estimator of the unknown vector parameter of the <b>autoregressive</b> <b>process</b> as well as for the serial correlation estimator associated with the driven noise. In addition, the almost sure rates of convergence of our estimates are also provided. Then, we prove the almost sure convergence and the asymptotic normality for the Durbin-Watson statistic. Finally, we propose a new bilateral statistical proce-dure for testing the presence of a significative first-order residual autocorrelation and we also explain how our procedure performs better than the commonly used Box-Pierce and Ljung-Box statistical tests for white noise applied to the stable <b>autoregressive</b> <b>process,</b> even on small-sized samples. 1...|$|E
40|$|This thesis {{considers}} continuous time <b>autoregressive</b> <b>processes</b> {{defined by}} stochastic differential equations and develops some methods for modelling time series data by such processes. The {{first part of}} the thesis looks at continuous time linear <b>autoregressive</b> (CAR) <b>processes</b> defined by linear stochastic differential equations. These processes are well-understood and there is a large body of literature devoted to their study. I summarise some of the relevant material and develop some further results. In particular, I propose a new and very fast method of estimation using an approach analogous to the Yule–Walker estimates for discrete time <b>autoregressive</b> <b>processes.</b> The models so estimated may be used for preliminary analysis of the appropriate model structure and {{as a starting point for}} maximum likelihood estimation. A natural extension of CAR processes is the class of continuous time threshold <b>autoregressive</b> (CTAR) <b>processes</b> defined by piecewise linear stochastic differentia...|$|R
40|$|The main {{objective}} {{of the current study}} is to handle the identification problem of <b>autoregressive</b> <b>processes</b> from the Bayesian point of view. Two Bayesian identification approaches are considered. They are referred to as the direct and the indirect approaches. The two approaches are employed to solve the Bayesian identification problem of <b>autoregressive</b> <b>processes</b> using three well known priors. These priors are the G prior, the Natural-Conjugate prior and Jeffrey's prior. The theoretical derivations related to the two Bayesian identification approaches are conducted using the above mentioned priors. Moreover, the performance of the two techniques, using each of the three priors, is investigated via comprehensive simulation studies. Simulation results show that the two techniques are adequate to solve the identification problem of <b>autoregressive</b> <b>processes.</b> The increase in the time series length leads to better performance for each technique. The use of different priors doesn't affect the previous results. </p...|$|R
40|$|We {{propose a}} new class of Markov-switching models useful for {{business}} cycle analysis, with transition probabilities following independent beta <b>autoregressive</b> <b>processes.</b> We study the effects of the autoregressive dynamics on the regime duration. We propose a full Bayesian inference approach and particular attention is paid to the parameters of the latent beta <b>autoregressive</b> <b>processes.</b> We discuss the choice of the prior distributions and propose a Markov-chain Monte Carlo algorithm for estimating both the parameters and the latent variables. Finally, we provide an application to the Euro area business cycle. ...|$|R
