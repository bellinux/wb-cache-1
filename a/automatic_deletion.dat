22|3|Public
2500|$|A Grand Jury in 2015 {{found that}} the city {{government}} did not comply with state law on public records, by deleting most emails after 30 days instead of the required two years. All emails were labeled automatically as [...] "unsaved drafts" [...] unless designated manually for retention. The city did not keep any record of councilmember emails, which used fremont.gov addresses and were relayed on to councilmembers' private email accounts. Though city officials held that <b>automatic</b> <b>deletion</b> would reduce data storage costs, the Grand Jury determined {{that the cost of}} complying with the state law would not be significant.|$|E
2500|$|Stallman {{mentions}} the dangers some e-books bring compared to paper books, with {{the example of}} the Amazon Kindle e-reader that prevents the copying of e-books and allows Amazon to order <b>automatic</b> <b>deletion</b> of a book. He says that such e-books present a big step backward with respect to paper books by being less easy to use, copy, lend to others or sell, also mentioning that Amazon e-books cannot be bought anonymously. His short story [...] "The Right to Read" [...] provides a picture of a dystopian future if the right to share books is impeded. He objects to many of the terms within typical end-user license agreements that accompany e-books.|$|E
50|$|New in 2.4.0: Unified LMDB based journal, new {{statistics}} module, <b>automatic</b> <b>deletion</b> {{of retired}} DNSSEC keys.|$|E
40|$|Abstract. This paper {{presents}} a new evolutionary system using genetic algorithm for evolving artificial neural networks (ANNs). The proposed algorithm is “Permutation free Encoding Technique for Evolving Neural Networks”(PETENN) {{that uses a}} novel encoding scheme for representing ANNs. Existing genetic algorithms (GAs) for evolving ANNs suffer from the permutation problem, resulting from the recombination operator. Evolutionary Programming (EP) does not use recombination operator entirely. But the proposed encoding scheme avoids permutation problem by applying a sorting technique. PETENN uses two types of recombi-nation operators that ensure <b>automatic</b> addition or <b>deletion</b> of nodes or links during the crossover process. The evolutionary system has been implemented and applied {{to a number of}} benchmark problems in ma-chine learning and neural networks. The experimental results show that the system can dynamically evolve ANN architectures, showing compet-itiveness and, in some cases, superiority in performance...|$|R
40|$|In {{order to}} combat the spread of digital forgeries, {{researchers}} have developed a variety of forensic techniques to verify the authenticity of digital multimedia files. Though many of these techniques can reliably detect traditional forgeries, recent {{research has shown that}} they can easily be fooled by anti-forensic operations designed to hide evidence of forgery. In response, new forensic techniques have been developed to detect the use of anti-forensics. In light of this, there is now a need to develop a theoretical understanding of the interactions between a forger using anti-forensics and a forensic investigator. In this paper, we propose techniques to evaluate the performance of anti-forensic algorithms along with a game theoretic framework for analyzing the interplay between forensics and antiforensics. Furthermore, we propose a new <b>automatic</b> video frame <b>deletion</b> detection technique along with a technique to detect the use of video anti-forensics. We evaluate these techniques using our proposed analytical framework...|$|R
40|$|This article reports two lexical {{decision}} {{experiments that}} {{provide evidence for}} the <b>automatic</b> activation of <b>deletion</b> neighbors: words that overlap with the presented word save for the deletion of one letter. Experiment 1 showed slower and less accurate “No ” decisions for nonwords with deletion neighbors (e. g., come in scome), relative to control nonwords. Experiment 2 showed slower and less accurate “Yes ” decisions for words with higher-frequency deletion neighbors, relative to control words. An important methodological implication of these results is that stimuli should be equated using a different definition of orthographic neighborhood from that which is currently the norm. The results also have significant theoretical implications for input coding schemes and the mechanisms underlying recognition of familiar words. 3 The effects of orthographic similarity on visual word identification have attracted considerable theoretical and empirical attention (for a review, see Andrews, 1997). The research suggests that processing of a written word results in the automatic activation o...|$|R
5000|$|The handset should delete any message {{received}} with a TP-DCS value {{falling to}} the [...] "Message Marked for <b>Automatic</b> <b>Deletion</b> Coding Group" [...] after user has read it.|$|E
50|$|In late 1995 WDJW's old {{transmitter}} failed {{for good}} and the station went off the air. WDJW was faced with deletion since a recent FCC rule change called for <b>automatic</b> <b>deletion</b> of any license that was off the air for one year. WWUH at the University of Hartford offered the BOE a loan of a transmitter in return {{for use of the}} WDJW signal when local programming was unavailable. WDJW went back on the air in late 1996. WDJW moved its studios and transmitter to the new high school building in the late 1990s.|$|E
5000|$|A Grand Jury in 2015 {{found that}} the city {{government}} did not comply with state law on public records, by deleting most emails after 30 days instead of the required two years. All emails were labeled automatically as [...] "unsaved drafts" [...] unless designated manually for retention. The city did not keep any record of councilmember emails, which used fremont.gov addresses and were relayed on to councilmembers' private email accounts. Though city officials held that <b>automatic</b> <b>deletion</b> would reduce data storage costs, the Grand Jury determined {{that the cost of}} complying with the state law would not be significant.|$|E
5000|$|Stallman {{mentions}} the dangers some e-books bring compared to paper books, with {{the example of}} the Amazon Kindle e-reader that prevents the copying of e-books and allows Amazon to order <b>automatic</b> <b>deletion</b> of a book. He says that such e-books present a big step backward with respect to paper books by being less easy to use, copy, lend to others or sell, also mentioning that Amazon e-books cannot be bought anonymously. His short story [...] "The Right to Read" [...] provides a picture of a dystopian future if the right to share books is impeded. He objects to many of the terms within typical end-user license agreements that accompany e-books.|$|E
5000|$|Clear Channel {{opted not}} to return the license while it sought a new {{transmitter}} site. The callsign WLVE (long associated with what is now WMIA-FM Miami) was [...] "parked" [...] on the license beginning in April 2009, {{in order not to}} lose control of the sign to another station operator. Low-powered transmissions resumed for a few days each November - by way of a wire strung up at WUSQ-FM's site - as the Telecommunications Act mandates <b>automatic</b> <b>deletion</b> of a station that is continuously silent for one year. WLVE was later one of several moribund AM stations that made up a planned 2009 donation to the Washington, D.C. nonprofit Minority Media and Telecommunications Council; however, for unclear reasons, it is absent from later reports on the plan and never changed hands.|$|E
40|$|Participant {{was asked}} to {{translate}} first part of utterances of "Elicitation Taa (Survey) " from Kgalagadi (translated by 268 -FT from Afrikaans) into his variety of Taa. Recorded by CN. (Repetition of first part of survey that was lost the day before because of <b>automatic</b> <b>deletion.)</b> Cf. fieldnotes CN 08 B: 27 - 50...|$|E
40|$|This paper {{describes}} {{tools that}} we built {{to support the}} construction of an object-oriented operating system in C++. The tools provide the <b>automatic</b> <b>deletion</b> of unwanted objects, first-class classes, dynamically loadable classes, and class-oriented debugging. As a consequence of our experience building Choices, we advocate these features as useful, simplifying and unifying many aspects of system programming...|$|E
40|$|MessyBoard is a {{projected}} networked 2 D bulletin board. Our experience with MessyBoard {{suggests that it}} is useful for some kinds of communication, but users have requested new behaviors such as <b>automatic</b> <b>deletion</b> of old content, automatic posting of new content from the web, and simple collaborative games. In order to rapidly build and experiment with automatic behaviors, we have integrated the Python scripting language into the MessyBoard client and created a simple development environment. A group of scripters have built several interesting behaviors, including puzzles, magnetic poetry, automatic posting of images, and a news ticker...|$|E
40|$|In {{this paper}} we {{introduce}} a coarsening algorithm for quadrilateral meshes that generates quality, quad-only connectivity during level-of-coarsening creation. A novel {{aspect of this}} work is {{development and implementation of}} a localized adaptation of the polychord collapse operator to better control and preserve important surface components. We describe a novel weighting scheme for <b>automatic</b> <b>deletion</b> selection that considers surface attributes, as well as localized queue updates that allow for improved data structures and computational performance opportunities over previous techniques. Additionally, this work supports optional and intuitive user controls for tailored simplification results. Categories and Subject Descriptors (according to ACM CCS) : I. 3. 5 [Computer Graphics]: Computational Geometry and Object Modeling—Curve, surface, solid and object representation...|$|E
40|$|Abstract—A {{method is}} {{described}} for finding decision bound-aries, approximated by piecewise linear segments, for classify-ing patterns in <N;N 2, using an elitist model of genetic algorithms. It involves generation and placement {{of a set}} of hyperplanes (represented by strings) in the feature space that yields minimum misclassification. A scheme for the <b>automatic</b> <b>deletion</b> of redundant hyperplanes is also developed in case the algorithm starts with an initial conservative estimate of the number of hyperplanes required for modeling the decision boundary. The effectiveness of the classification methodology, along with the generalization ability of the decision boundary, is demonstrated for different parameter values on both artificial data and real life data sets having nonlinear/overlapping class boundaries. Results are compared extensively with those of the Bayes classifier, k-NN rule and multilayer perceptron. Index Terms — Evolutionary computation, hyperplane fitting, pattern recognition, variable mutation probability. I...|$|E
40|$|Abstract. This paper {{suggests}} {{an approach to}} neural network training through the simultaneous optimization of architectures and weights with a Particle Swarm Optimization (PSO) -based multiobjective algorithm. Most evolutionary computation-based training methods formulate the problem in a single objective manner by taking a weighted sum of the objectives from which a single neural network model is generated. Our goal is to determine whether Multiobjective Particle Swarm Optimization can train neural networks involving two objectives: accuracy and complexity. We propose rules for <b>automatic</b> <b>deletion</b> of unnecessary nodes from the network based on the following idea: a connection is pruned if its weight {{is less than the}} value of the smallest bias of the entire network. Experiments performed on benchmark datasets obtained from the UCI machine learning repository show that this approach provides an effective means for training neural networks that is competitive with other evolutionary computation-based methods. ...|$|E
40|$|Abstract — Digital {{photos are}} massively {{produced}} while digital cameras are becoming popular; however, not every photo has good quality. Blur {{is one of}} the conventional image quality degradation which is caused by various factors like limited contrast; inappropriate exposure time and improper device handling. Blurry images make up a significant percentage of anyone's picture collections. So, an efficient tool is requiring for detecting blurry images and labelling or separating them for <b>automatic</b> <b>deletion</b> in order to preserve storage capacity. There are various methods to detect the blur from the blurry images some of which requires transforms like DCT or Wavelet and some don’t require transform. A new technique is presented which automatically detect blurry images and separate them for later processing. The method will find out key points for both original and filtered image by using SIFT algorithm. After that calculate variance value for both the key points. Draw and analyse the plotted graph to determine whether the image is blurry or not...|$|E
40|$|Approaches to data {{protection}} have traditionally focused on physical security and/or cryptographic security, but the <b>automatic</b> <b>deletion</b> of data {{as a form}} of protection has not been widely employed. However, a vast amount of data is stored that has only a limited useful life and presents a risk to privacy and security after that time period. Approaches to data deletion can be broadly classified as physical destruction, data overwrite and cryptographic deletion. Cryptographic data deletion does not require any physical destruction and can be done quickly without any need to alter the data that is stored. Previous approaches to cryptographic data deletion have focused on bulk sanitization of storage devices. In this paper we introduce the notion of “forgetful ” storage that employs cryptographic data deletion using a time-based approach to key management. By periodically erasing old keys and creating new keys, old data becomes automatically and instantly inaccessible, or virtually deleted. This data deletion can be accomplished without depending on any actions {{on the part of the}} user, application software or operating system. A refresh policy can be utilized to cause information that is accessed to be re-encrypted using a newer key, thereby extending the time before it will expire...|$|E
40|$|To {{develop a}} novel method for rapid {{myocardial}} T 1 mapping at high spatial resolution. METHODS: The proposed strategy represents a single-shot inversion-recovery (IR) experiment triggered to early diastole during a brief breathhold. The measurement combines an adiabatic inversion pulse with a real-time readout by highly undersampled radial FLASH, iterative image reconstruction and T 1 fitting with <b>automatic</b> <b>deletion</b> of systolic frames. The method was implemented on a 3 T MRI system using a GPU-equipped bypass computer for online application. Validations employed a T 1 reference phantom including analyses at simulated heart rates from 40 to 100 bpm. In vivo applications involved myocardial T 1 mapping in short-axis views of healthy young volunteers. RESULTS: At 1 mm in-plane resolution and 6 mm section thickness, the IR measurement could be shortened to 3 s without compromising T 1 quantitation. Phantom studies demonstrated T 1 accuracy and high precision for values ranging from 300 to 1500 ms {{and up to}} a heart rate of 100 bpm. Similar results were obtained in vivo yielding septal T 1 values of 1246 ± 24 ms (base), 1256 ± 33 ms (mid-ventricular) and 1288 ± 30 ms (apex), respectively (mean ± SD, n= 6). CONCLUSION: Diastolic myocardial T 1 mapping with use of single-shot inversion-recovery FLASH offers high spatial resolution, T 1 accuracy and precision, practical robustness and speed. Advances in knowledge: The proposed method will be beneficial for clinical applications relying on native and post-contrast T 1 quantitation...|$|E
40|$|As for {{the network}} {{technology}} that supports today 2 ̆ 7 s information society,an entire network behavior might become unstable due to the unpredictable,anomalous behavior and disturbances. From such a background, this thesisfocuses on the knowledge acquisition of network behavior {{as a means of}} thestabilization. This thesis summarizes research achievements of modeling of networksystems knowledge acquisition and applying to the robust design andoperation. The model is characterized in the following three points. (1) Knowledge acquisition intended for the behavior of finite statemachine(2) Conversion the analysis of the practical data and the practicalexperiment to knowledge concerning network behavior(3) The model is applied to execution and control of network behavior fromtwo views of predictability in a time transition. In the design phase, the design support of the service specification of thecall control software and the verification support of the call processingprogram are introduced. The model is applied to conversion into the form ofknowledge representation of frame theory used in the artificial intelligencefield. As a result, uncanny behavior is controlled by automatic generation ofthe indispensable task, <b>automatic</b> <b>deletion</b> of the unnecessary task,automatic correction of the logic error of source codes and so on. Consequently, it is shown that the model is useful for stabilization in thedetailed design. In the operation phase, an autonomous robust control is introduced in theIP traffic engineering over MPLS (Multi-Protocol Label Switching). Themodel is applied to the block diagram of dynamic traffic control facilities. Asa result, traffic balance is controlled by suppression of oscillation behavior,improvement of performance, and robustness against the influence ofdisturbances. Consequently, it is shown that the model is useful forstabilization in the IP flow balancing. 電気通信大学 200...|$|E
40|$|Continuum-based {{discrete}} element {{method is}} an explicit numerical method, {{which is a}} combination of block discrete element method (DEM) and FEM. When simulating large deformation problems, such as cutting, blasting, water-like material flowing, the distortion of elements will lead to no convergence of the numerical system. To solve the convergence problem, a particle contact-based meshfree method (PCMM) is introduced in. The paper aims to discuss this issue. PCMM is based on traditional particle DEM, and use particle contacts to generate triangular elements. If three particles are contact with each other, the element will be created. Once elements are created, the macroscopic constitutive law could be introduced in. When large deformation of element occurs, the contact relationship between particles will be changed. Those elements that do not meet the contact condition will be deleted, and new elements that coincide with the relationship will be generated. By the deletion and creation of elements, the convergence problem induced by element distortion will be eliminated. To solve FEM and PCMM coupled problems, a point-edge contact model is introduced in, and normal and tangential springs are adopted to transfer the contact force between particles and blocks. According to the deletion and recreation of elements based on particle contacts, PCMM could simulate large deformation problems. Some numerical cases (i. e. elastic field testing, uniaxial compression analysis and wave propagation simulation) show the accuracy of PCMM, and others (i. e. soil cutting, contact burst and water-like material flowing) show the rationality of PCMM. In traditional particle DEM, contact relationships are used to calculate contact forces. But in PCMM, contact relationships are adopted to generate elements. Compared to other meshfree methods, in PCMM, the element <b>automatic</b> <b>deletion</b> and recreation technique is used to solve large deformation problems. Peer ReviewedPostprint (published version...|$|E
40|$|Comparisons {{between groups}} {{play a central}} role in {{clinical}} research. As these comparisons often entail many potentially correlated response variables, the classical multivariate general linear model has been accepted as a standard tool. However, parametric methods require distributional assumptions such as multivariate normality while non-normal data often exist in clinical research. For example, a clinical trial investigating a treatment for depression is designed as a longitudinal study and the main outcome is survey scores of subjects on several time points, while the scores are ordinal. Although non-parametric multivariate methods are available in the statistical literature, they are not seen to be commonly used in clinical research. Moreover, <b>automatic</b> <b>deletion</b> of cases with missing values in response variables is a shortcoming of standard software when performing multivariate tests. This dissertation addresses the issues of violation of multivariate normality assumption and missing data, focusing on the non-parametric multivariate Kruskal-Wallis (MKW) test, likelihood-based and permutation-based methods. First, an R-based program is written to compute the p-value of MKW test for group comparison. Simulation studies show that the permutation-based MKW test provides better coverage and higher power level than likelihood-based MKW test and classical MANOVA. Second, an extension of MKW test is proposed for multivariate data with missingness. The proposed method retrieves information in partially observed cases and is permutation-based. A sensitivity analysis compares the performance of the proposed extension and the standard test utilizing only complete cases. Results show that the proposed extended method provides higher power level, encompassing a broad spectrum of multivariate effect sizes. An illustrative example using data from a psychiatric clinical trial is provided. The R program is ready to use for applied statistician. The public health relevance of this work lies in the development of a new powerful methodology with user-friendly computer software for group comparisons in non-normal multivariate data with or without missingness...|$|E
40|$|This {{deliverable}} {{presents an}} assessment of the privacy and data protection compliance framework of the eVACUATE project, as evaluated in the previous tasks and during the four validation demonstrations. It contains analysis of the different elements of the eVACUATE solution from an ethical, privacy and data protection perspective and a summary of the insights on the impact of the technology on individuals’ rights as observer during the demonstrations. The deliverable lays specific emphasis on the regulatory impact of the entry into force of the General Data Protection Regulation (GDPR) on the different elements of eVACUATE and pays attention to the data protection by design and by default measures considered in the context of eVACUATE. It formulates future-proof recommendations for end users when considering the implementation of eVACUATE in a production environment and also provides guidance on the new requirement of the GDPR of conducting a data protection impact assessment (DPIA). The outcomes of the privacy and data protection analysis could be summarised as follows: RFID technology provides for detailed information for the data subjects, its use is entirely voluntary and sufficient safeguards respecting the data subjects’ rights have been suggested; data minimisation techniques have been implemented such as non-individualisation of the chipless RFID tags and not using unique identification numbers; clear retention periods or criteria for determining such periods must be established in a production environment. MobiMESH technology has clearly defined purposes and processes proportionate amount of data to the defined purposes of processing; data security measures have been implemented; in a production environment, data controllers should elaborate a clear privacy policy and terms of use, taking into account the conditions for valid consent of Article 7 GDPR (if consent is relied upon); clear retention periods or criteria for determining such periods must be established in a production environment. eVAMAPP applications are used on a purely voluntary basis; compliance with the GDPR’s intensified information and transparency obligations is essential; data protection by design and by default measures have been implemented: device’s location is not tracked, GPS satellites and iBeacons are not used to identify or store the IDs of the terminal devices, applications do not store unnecessary identity attributes; the key moment for activation of identification functions is proportionately related to the moment of declaring an evacuation. SoNeMa technology provides for targeted content analysis but it should be performed in a limited timespan, eg, when an emergency is clearly present; the legal ground for processing must be carefully assessed by the controller in a production environment. Crowd behaviour detection technology has narrowly defined purposes and does not perform identification of natural persons; it has improved accuracy; facial images from optical cameras should not be processed to perform identification; data from behaviour detection algorithms should not be linked with data about identifiable persons; transparency obligations are key, especially when combining this technology with relatively known legacy technologies, such as CCTV; the technology should not be active at all times. Counting technology has clearly defined purposes and improved data accuracy; data minimisation includes video of low quality which does not allow for performing facial recognition operations and <b>automatic</b> <b>deletion</b> of frames shortly after their technical processing by the system. nrpages: 190 status: publishe...|$|E
40|$|MODIStsp is a "R" package {{devoted to}} automatizing the {{creation}} of time series of rasters derived from MODIS Land Products data. MODIStsp allows to perform several preprocessing steps (e. g., download, mosaicking, reprojection and resize) on MODIS data available within a given time period. Users {{have the ability to}} select which specific layers of the original MODIS HDF files they want to process. They also can select which additional Quality Indicators should be extracted from the aggregated MODIS Quality Assurance layers and, in the case of Surface Reflectance products, which Spectral Indexes should be computed from the original reflectance bands. For each output layer, outputs are saved as single-band raster files corresponding to each available acquisition date. Virtual files allowing access to the entire time series as a single file can be also created. All processing parameters can be easily selected with a user-friendly GUI, although non-interactive execution exploiting a previously created Options File is possible. Stand-alone execution outside an "R" environment is also possible, allowing to use scheduled execution of MODIStsp to automatically update time series related to a MODIS product and extent whenever a new image is available. MODIStsp v 1. 2. 1 Release Notes v 1. 2. 1 was released on 20 / 04 / 2016 Major Changes 	 	Modified format of "R" output time series from rts objects to RasterStack objects with temporal information added in the "z" attribute via setZ() 	 	 	Major changes/improvements in MODIStsp_extract function: 	 		Use of plain rasterstack with "z" attribute instead than rasterstackts 		Use of gdal_rasterize (gdalUtils) instead of rasterize (rgdal) to improve speed. Temporary shapes and rasters necessay are saved in "R" temporary folder and removed automatically 		Fixed bugs on functionality for point/lines shapefiles, according to what specified by the "small" and "small_method" parameters 		Added functionality for retrieving data for small polygons 		Added out_format selection - xts or plain data. frame 		Added possibility to use a shp filename as input directly 		Added conformity checks on inputs 		Added functionality to run without specifying start and end dates 		Added id_field parameter for choosing which column of the input SP object should be used for "naming" the columns of the output 	 	 	 	Removed possibility to use "complex" resampling methods when reprojecting (e. g., bilinear, cubic, etc.) to avoid incorrect resampling on categorical variables and "contamination" of good pixels' data. 	 Minor Changes 	Changed the input method for starting and ending dates selection in the GUI. Now a text field is used 	Added functionaluty for writing data ignore value on ENVI files 	Removed <b>automatic</b> <b>deletion</b> of XML files created by writeRaster to keep metadata information 	Changed names of products in the GUI for products with both TERRA and AQUA dataset to M*D 09 A 1, M*D 13 Q 1, etc [...] . 	Modified code syntax to satisfy R code styling guidelines 	Modified roxygen parameters so that only required functions are imported from imported packages 	Updated and corrected the list of dependencies 	Updated required "R" version to 3. 2, and minimum versions for dependent packages to current versions. 	Added Welcome message 	Updated links to LPDAAC product description pages 	Changed all "print" and "cat" calls to show messages/warnings to "message" or "warning" to allow easy disabling MODIStsp verbose messages 	Using "R" tempfile/tempdir to save vrt files Bug Fixes 	Corrected a bug that threw an error in case incorrect bounding box specifie...|$|E

