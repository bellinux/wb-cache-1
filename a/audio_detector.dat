5|20|Public
50|$|The {{frequency}} mixer {{was of the}} pentagrid converter design to save {{the cost of a}} separate oscillator tube. The detector and first audio stage were provided by a dual diode/triode combination tube. When the detector/first audio tube contained a second diode, it could be used to provide automatic gain control (AGC), or AGC bias could be derived from the <b>audio</b> <b>detector</b> diode.|$|E
40|$|Music {{is one of}} {{the basic}} human needs for {{recreation}} and entertainment. As song files are digitalized now a days and options for random play of songs are common in use as shuffle. Shuffle randomly picks a song and has a tendency to stick around mostly played songs. Thus {{there is a need to}} retrieve and recommend songs on the basis of one’s mood just by his/her first made choice. In this paper we will present a well-defined architecture to play songs on basis of song chosen using audio content analysis and <b>audio</b> <b>detector.</b> In audio content analysis we will use features such as intensity, timbre and rhythm to map related feature music. Finally, <b>audio</b> <b>detector</b> will detect and play similar featured songs. KEYWORDS Shuffle’s alternative, automatic playing, playing on one’s mood. 1...|$|E
40|$|For many speech {{telecommunication}} technologies {{a robust}} speech activity detector is important. An audio-only speech detector will givefalse positives when the interfering signal is speech or has speech characteristics. The modality video is suitable {{to solve this}} problem. In this report the approach to and implementation of a decision-based audiovisual speech detector is given. Acoustic and visual features of speech are first separately investigated. Firstly, a common method for speech detection based on audio has been built. Secondly, from the video data the mouth features have been extracted {{with the implementation of}} an own idea. The visual features were used to create a conservative visual non-speech detector. The low false detection rate makes the visual non-speech detector suitable to rule out some false speech detections of an audio only solution. Finally, the combinationof the <b>audio</b> <b>detector</b> and the video detector leads to an audiovisualspeech detector which uses basic mouth features and a common acousti-cal speech detection method to outperform an audio-only solution...|$|E
50|$|Challenged {{to explain}} {{sightings}} of unidentified lights and luminous phenomena {{in the sky}} around Piedmont, Missouri, Rutledge decided to subject these reports to scientific analysis. He put together a team of observers with college training in the physical sciences, including a large array of equipment: RF spectrum analyzers, Questar telescopes, low-high frequency <b>audio</b> <b>detectors,</b> electromagnetic frequency analyzer, cameras, and a galvanometer to measure variations in the Earth's gravitational field.|$|R
5000|$|There {{are three}} types of [...] "real time" [...] <b>audio</b> bat <b>detector</b> in common use: the heterodyne, {{frequency}} division, and time expansion. Some bat detectors combine two or all three types.|$|R
40|$|A {{small set}} of {{extensions}} to the PureData architecture {{and to its}} external API is proposed, which will facilitate the creation of <b>audio</b> feature <b>detectors</b> (pitch followers, beat trackers, onset detectors, etc.) as Pd patches. An experimental implementation is discussed, and two application examples are demonstrated. 1...|$|R
40|$|Model based hardware/software {{synthesis}} {{can lead}} to fast and efficient embedded system implementations, by enabling quick design space exploration. High level hardware modeling and implementation can be accelerated by using functionally verified parameterized models that are optimized for high level hardware synthesis. Such models can be designed {{so that they can}} be easily integrated with a high level modeling environment, such as Simulink, and at the same time provide ample flexibility to perform design space exploration when mapped to hardware. During signal processing hardware design, the focus is mostly on the architectural representation (data parallelism, pipelining, memory access, etc.) to meet throughput requirement and on data path modeling to analyze the effects of quantization. In this paper we present our experience of modeling an FFT block that can be integrated with the Simulink model based design environment for simulation and verification, and later can be used to perform architectural design space exploration and hardware implementation with optimal data path selection. A key advantage of our model is that the very same bit-accurate C code is used for simulation and for high-level synthesis, because it has been written with both aspects in mind (while for software implementation either our code or the code provided by the Mathworks can be used equally well). To prove the feasibility of our proposed approach we synthesized our FFT for two DSP applications with very different performance and cost requirements, namely a frequency domain <b>audio</b> <b>detector</b> and a GPS acquisition algorithm, and compared it with existing manual implementation...|$|E
40|$|Hardware blocks {{are used}} to execute very {{different}} Digital Signal Processing tasks, ranging from simple audio applications to {{state of the art}} LTE mobile communication and high definition video processing. Users expect continuous and very rapid innovation, and each new product to be more energy efficient and more powerful in terms of functionality than the previous generation. These user driven market requirements force companies to continuously struggle to design new devices with novel features and improved performance. Also the large number of semiconductor companies (especially fab-less) results in a strong competition, especially in terms of time to market. In summary, two key factors that determine the success of a new product are its differentiating features and its design time. These requirements ultimately translate into an increased pressure on the hardware and software designers to come up with new and innovative designs in short time spans, which can only be achieved with novel design tools and flows. For hardware design the standard RTL-based flow employed in the industry consists of using pre-built and pre-verified hardware IPs plus some customized blocks that are specially designed for new devices. These customized hardware blocks are one of the main ingredients that enable product differentiation in the market. These blocks are normally designed using a procedure that consists of two major designs and modeling steps. The first step models the algorithm in some C-like language that allows algorithmic analysis and verification. The second step manually implements a hardware architecture coded in a hardware description language. The manual translation involved in the second step requires a long time, and in theory it should be iterated for many possible hardware architectures, in order to find the best one. This hardware Design Space Exploration is especially lengthy because verification must be repeated for each new RTL implementation. The best way to cope with these issues is to use hardware synthesis tools that allow modeling in a C-like language at the algorithmic level, and can directly and quickly perform design space exploration with as little manual effort as possible. This observation is the basic motivation for the methods and results described in this thesis. The model-based design paradigm has been studied extensively by the research community to raise the level of abstraction for design, verification and synthesis of hardware and software. The scope of the term is very broad, and it cannot be captured easily in a few lines. However, in the scope of hardware and software design for embedded systems model-based design can be defined as a design paradigm that allows one to rapidly design, verify, validate and implement hardware and software systems starting from pre-verified abstract component models. Model-based design usually starts from abstract graphical drag-and-drop models which are usually modeled in a proprietary way (as in the case of Simulink) or in a loosely standardized way (as {{in the case of the}} UML). The components used for modeling range from simple atomic blocks like arithmetic operations to complex blocks like a Fast Fourier Transform, a Finite Impulse Response filter, a Viterbi decoder, a Discrete Cosine Transform etc. The entire algorithm is modeled in this way, and then automatically synthesized to an implementation-dependent model, in order to enhance re-use under different application scenarios. Model-based design is normally carried out using frameworks and design environments, such as Simulink or Labview, that are equipped with powerful model-to-model translation tools. These tools can be used to implement different low level models for software and hardware simulation and implementation from a single "golden" verified algorithmic model. Most of the model-based design tools that are available in industry and academia today allow automatic software code generation from graphical models. The software code generators can generate optimized code that can come close to hand optimized code in terms of size and performance, because they exploit information about the target processor architecture and the associated memory hierarchy. Model-based design tools also offer the capability to generate hardware from graphical models. However, they either rely on parameterized hand-optimized implementations of the larger macro blocks, or require the designer to use smaller blocks at an abstraction level that is very close to RTL. In other words these tools either rely either on pre-modeled intellectual proprietary blocks (IPs) with fixed hardware architecture or tend to decrease the abstraction level in a proprietary manner. The reliance on a single architectural template can result in a sub-optimal implementation if this architecture is not suitable for particular application, while working at lower abstraction level design space exploration becomes again very expensive, essentially defeating one of the main purposes of model-based design. Simulink (from The Mathworks) is a very powerful modeling, simulation and analysis tool that is used in this thesis as the starting point for model-based design. It has a huge set of libraries and allows modeling systems and algorithms from very different application domains, ranging from embedded control to video and communication systems. It includes many parameterized built in components that can be used for modeling, as well as components that allow easy mathematical and graphical analysis of data in the time and frequency domains, thus making the debugging and verification very easy and intuitive. Simulink also has tools that perform fixed point analysis analysis and ease the transition from floating to fixed point models. Simulink comes bundled with different code-generators that can generate C/C++ code from models for different embedded processors. There are many code generators available with Simulink that can generate C/C++ software code for a variety of processor architectures. These code generators are called Target Language Compilers (TLC) in Simulink terminology. Simulink also supports for hardware generation from graphical models, but as explained above for complex algorithms it relies on parameterizable fixed architectural templates which are not flexible enough to target different application domains with different requirements or it requires modeling at a low abstraction level, resulting in longer design times. The research presented in this thesis uses Simulink as graphical modeling environment front-end for hardware synthesis and hardware/software trade-off analysis. It proposes a novel design flow, which incorporates a more flexible and powerful approach for hardware synthesis and design space exploration than the industrial and research state of the art. Several high level synthesis (HLS) tools are available to synthesize RTL hardware from C/C++/SystemC abstract specifications. Normally these high level synthesis tools also require a set of constraints which drive them to synthesize a specific macro and micro-architecture from the C-like specification. Hence Design Space Exploration can be performed only by changing these constraints, while preserving the same pre-verified C code. Simulink used as a modeling front-end to high level hardware synthesis results in a very powerful flow that allows flexible hardware design and synthesis. Since Simulink supports code generation for software implementation, hardware/software trade-off analysis can also be performed easily starting from Simulink models. During the course of this thesis Simulink was evaluated as graphical modeling front-end for high level hardware synthesis and hardware software trade-off analysis. The tool that was used for high level hardware synthesis is Cadence C-to-Silicon, or CtoS. In the input, C++ constructs {{are used to}} specify the algorithm, while SystemC constructs are used for more hardware oriented artifacts, such as bit-true IOs and timing. Hence our goal is to define a methodology to convert the C output from Simulink and efficiently synthesize it with CtoS. In particular, by exploring the variety of TLCs available from Simulink, we found that Embedded Real Time coder (ERT), which was specifically designed to produce readable and optimized code for embedded processors, can generate C code that is well suited for high level hardware synthesis. ERT has many optimization that can be selectively enabled or disabled to generate code that is suitable for a particular application. We first analyzed them to understand which ones can be used to generate code that is best suited for high level hardware synthesis. Also different graphical modeling options were analyzed to enable hierarchical and modular code generation, that is especially useful when model partitioning is required in order to obtain a more concurrent hardware implementation. Simulink allows one to easily control the level of granularity, by grouping sub-designs into aggregate blocks that can become single function calls in the generated C code. In order to illustrate how one can perform hardware/software trade-off analysis, we used a case study from the domain of wireless video surveillance sensor networks, where cameras are triggered to capture video whenever an interesting audio event is detected by an <b>audio</b> <b>detector.</b> The platform that was used as implementation target consisted of ARM Cortex-M 3 processor. The audio algorithm was modeled in Simulink and then model was grouped into aggregate Simulink blocks to generate modular code through ERT. This modular code was then used in the form of different C-code partitions to perform extensive hardware/software trade-off analysis, where final implementation decisions were made based on very accurate low level power/area/throughput estimates. An accurate picture of power, area and throughput was easily generated because most of the design space exploration was performed using CtoS with minor or no changes to the C-model. Very simple and abstract architectural constraints to automatically generate the low level synthesis scripts for both CtoS and the downstream tools (e. g. RTL Compiler). Once a C-model is generated from Simulink through ERT it can be synthesized using CtoS by providing architectural constraints. But in order to efficiently perform design space exploration one must gain a high level understanding of the computationally most expensive loops and code segments. This may again become a complex and tedious task for automatically generated code. Hence a very important part of our research was to devise and implement a mechanism that can shield the designer from understanding the automatically generated code during high level synthesis. We implemented an automated TCL script to perform automatic design space exploration. This tool takes as input a set of simple high-level directives that limit the design space to be explored, and then produces many points in the design space that can be used for further low level throughput, power and area analysis. The directives specify, for example, how many resources such as multipliers or adders must be considered, what type of memories must be used, how many operations should be scheduled in a single cycle etc. In the second part of this thesis, we also explored in detail how to implement algorithms, like an FFT, that are used in a multitude of different applications and may require inherently different macro/micro-architectures to satisfy very different performance requirements. The fully sequential C-code that is generated by ERT for such complex blocks limits the design space that can be explored. Especially more concurrent implementations are very difficult to derive from such code because it is inherently sequential and not well suited to achieve the level of concurrency which can be essential for some hardware implementations.. We thus proposed and experimented with a technique that allowed us to represent complex block as proprietary blocks in Simulink. These blocks can be considered as IPs that are defined at a higher abstraction level to enable efficient hardware synthesis and extended hardware design space exploration, still starting from a single C model. The modeling strategy that we used for defining these high level synthesis IPs (HLS-IP) relies on the S-function modeling mechanism. This is a mechanism provided by Simulink to extend its component library. It allows modeling of algorithms in a well-defined manner to enable integration into the simulation environment and C-code generation. We used plain C for modeling S-functions because C-like specifications are also well suited for high level hardware synthesis, which is the target of our proposed HLS-IP scheme. Complex algorithms, like an FFT, have many different signal flow graph representations, where each representation is suited for different purpose. Some representations may be best suited for software implementation and some are more interesting for hardware implementation. The choice of signal flow graph and template architecture in our case was made based on flexibility and opportunity to map to different macro/micro-architectures, to enable more extended hardware design space exploration. This mostly involves a change of the level of concurrency, because state of the art high level synthesis tools are hardly able to increase the level of concurrency starting from sequential C-like specifications, thus essentially limiting a very important dimension of the hardware design space that can explored. We experimented extensively with an FFT test case for designing high level flexible IPs that can be used for modeling, simulation and verification in the Simulink environment and then for efficient hardware synthesis and design space exploration. In the end we were able to formulate and write an FFT IP-generator that generates flexible HLS-IPs following all the guidelines described above, including all the wrappers that are required for integration into Simulink and the high level synthesis scripts. The HLS-IPs are generated in a way that allows separation of pure functionality and behavioral constructs from the SystemC interfaces and threads. The functional part of HLS-IP that is generated remains the same in order to perform verification once, and only the level of parallelism and datapath bit-width is changed to reflect the application requirement...|$|E
50|$|The 1S5 tube {{functioned as}} a {{regenerative}} <b>detector.</b> <b>Audio</b> detected by the 1S5 was resistance-coupled to the 3V4, where it was amplified and supplied to the earphone.|$|R
50|$|On 3 July 2009, Maxim Anisiutkin {{published}} an open source DVD <b>Audio</b> watermark <b>detector</b> and neutralizer computer {{program to the}} SourceForge web site. The software package contains {{a detailed description of}} the method and embedding parameters used in creating the DVD Audio or SDMI (Secure Digital Music Initiative) watermark, which was created by Verance Inc and was the earlier version of the Cinavia watermarking technology.|$|R
5000|$|The tall conical {{plexiglass}} dome {{that covered}} the head housed the detailed mechanisms representing Robby's electronic [...] "brain". These included a 'pilot light' at the very top, an intricate apparatus terminating in three chromed wire-frame spheres that rotate in planetary fashion (representing his gyroscopic stabilisers), a pair of reciprocating arms {{in the shape of}} an inverted [...] "V", multiple flashing lights, and an elaborate horizontal array of moving levers resembling saxophone keys. Conical protuberances attached to each side of the head carry two small forward-facing blinking lights (representing his electronic 'eyes') and two rotating chromed rings, one mounted vertically and the other horizontally, which represent Robby's <b>audio</b> <b>detectors</b> (his 'ears'). The bottom front section of the head is a curved grille consisting of parallel rows of thin blue neon tubes, which light up in synchronization with Robby's 'synthetic' speech. This neon grille also enabled the operator to see out (and breathe). The joint between the head and chest section was fitted with a custom-made bearing that allowed the head to rotate 45 degrees in either direction.|$|R
50|$|Microwave Impedator (aka Mister Fuck Up): Roughly a briefcase-sized device, it can render useless {{infrared}} {{and other}} photo-electric detectors. Also can jam transmitters of <b>audio</b> and motion <b>detectors,</b> which operate upon the Doppler principle. Has a built-in self-destruct mechanism.|$|R
40|$|Automatic {{generation}} of sports highlights from recorded audiovisual content has been object {{of great interest}} in recent years. The problem is indeed important {{in the production of}} second and third division leagues highlights videos where the quantity of raw material is significant and does not contain manual annotations. Many approaches are mostly based on the analysis of the video and disregard the important information provided by the audio track. In this paper, a new approach that combines audio and video descriptors for automatic soccer highlights generation is proposed. The approach is based on the segmentation of the video contents into shots that are further analyzed in order to determine its relevance and interest. These video-shots are scored taking into account the fusion between different audio and video features. The paper is mainly focused to emphasize the importance of <b>audio</b> <b>detectors</b> that {{play a key role in}} the analysis and scoring of the video-shots. Specifically, a new algorithm for referee's whistle detection is proposed. The algorithm has been proven to be very robust and efficiently discriminates professional whistles against other types of noises such as public cheering-up, music instruments, etc. Several results have been produced using real soccer video sequences that prove the validity of the proposed audio and video fusion scheme. Peer ReviewedPostprint (published version...|$|R
30|$|In Section 4, the {{performance}} of the proposed method is evaluated and compared to two previously proposed schemes, namely Yang et al.[2] and Liu et al.[3, 4] in the fake quality detection scenario. In order to do the same comparison in the audio tampering scenario, in the same section, we present also the application of the proposed scheme and of the two state-of-the-art algorithms to the forgery scenario we have considered: in particular, we tested the accuracy of such doubly compressed <b>audio</b> file <b>detectors</b> when the <b>audio</b> track length is reduced and we exploited their use for tampering localization.|$|R
40|$|An {{evaluation}} method for <b>audio</b> beat <b>detectors</b> Beat and tempo tracking {{is something that}} is easily done by humans, but is rather difficult for computers. Though {{there are a number of}} different approaches to tackle this problem, and there are a large number of applications for beat and tempo trackers, none of the implementations score one hundred percent. To make matters worse, not all implementations are evaluated in the same way. This makes comparing different algorithms very difficult. A single method to evaluate and compare beat and tempo trackers is desirable, and the initial steps are given in this paper...|$|R
5000|$|Today, {{descendants}} of the original valve (or vacuum tube) still {{play an important role}} in a range of applications. They can be found in the power stages of radio and television transmitters, in musical instrument amplifiers (particularly electric guitar and bass amplifiers), in some high-end <b>audio</b> amplifiers, as <b>detectors</b> of optical and short wavelength radiation, and in sensitive equipment that must be [...] "radiation-hard".|$|R
30|$|The {{filter bank}} is {{composed}} of elementary and advanced filters. Elementary filters are defined as filters that detect or classify events in each shot by using features directly provided by the low-level audio-visual descriptors. Filters that fall into this category are: the long shot detector, the zoom detector, the whistle detector, the replay detector, the persons detector, the high motion <b>detector,</b> the <b>audio</b> power <b>detector,</b> the <b>audio</b> intra power shot <b>detector,</b> the <b>audio</b> inter power shot detector, the long duration classifier, the medium duration classifier, the short duration classifier and the very short duration classifier. Duration classifiers categorize {{the length of a}} shot. The rest of the detectors have already been detailed in “Audio and video description extraction”.|$|R
40|$|The present paper {{discusses}} {{the implementation of}} direction of arrival estimation using Incoherent Wideband Music algorithm. The direction of arrival of an audio source is estimated using two microphones plugged in “Line-in” input of a DSP development board. A {{solution to the problem}} of fluctuating estimation of the angle of arrival has been proposed. The solution consists on adding an <b>audio</b> activity <b>detector</b> before going on processing. Only voiced sound frames are considered as they fulfill the theoretical constraint of the used estimation technique. Furthermore, the latter operation is followed by integration over few frames. Only two sensors are used. For such a reduced number of sensors, the obtained results are promising...|$|R
40|$|Audio Event Detection (AED) aims to {{recognize}} sounds within {{audio and video}} recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence {{it is difficult to}} model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models. The <b>audio</b> event <b>detectors</b> are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set. Results showed an improvement of the AED, and uncovered challenges of using web audio from videos. Comment: 5 page...|$|R
40|$|An {{audio device}} {{includes}} a first audio path with a loudspeaker for reproducing an audio signal, {{and a second}} audio path. The second audio path includes in series a band-pass filter for filtering an <b>audio</b> signal, a <b>detector</b> for detecting the amplitude of the band-pass filtered audio signal, a multiplier for multiplying a periodic signal by the amplitude of the band-pass filtered audio signal, and a vibration device for reproducing the multiplied periodic signal. The frequency of the periodic signal is substantially equal to the resonance frequency of the vibration devic...|$|R
40|$|With the {{popularity}} of online multimedia videos, {{there has been much}} interest in recent years in acoustic event detection and classification for the improvement of online video search. The audio component of a video has the potential to contribute significantly to multimedia event classification. Recent research in audio document classification has drawn parallels to text and image document retrieval by employing what {{is referred to as the}} bag-of-audio words (BoAW) method. Compared to supervised approaches where <b>audio</b> concept <b>detectors</b> are trained using annotated data and extracted labels are used as lowlevel features for multimedia event classification. The BoAW approach extracts audio concepts in an unsupervised fashion. Hence this method has the advantage that it can be employed easily for a new set of audio concepts in multimedia videos without going through a laborious annotation effort. In this paper, we explore variations of the BoAW method and present results on NIST 2011 multimedia event detection (MED) dataset. Index Terms: Bag-of-audio-words, multimedia event detection 1...|$|R
40|$|Audio tagging aims {{to assign}} one or several tags to an audio clip. Most of the {{datasets}} are weakly labelled, which means only the tags of the clip are known, {{without knowing the}} occurrence time of the tags. The labeling of an audio clip is often based on the audio events in the clip and no event level label is provided to the user. Previous works have used the bag of frames model assume the tags occur all the time, {{which is not the}} case in practice. We propose a joint detection-classification (JDC) model to detect and classify the audio clip simultaneously. The JDC model has the ability to attend to informative and ignore uninformative sounds. Then only informative regions are used for classification. Experimental results on the "CHiME Home" dataset show that the JDC model reduces the equal error rate (EER) from 19. 0 % to 16. 9 %. More interestingly, the <b>audio</b> event <b>detector</b> is trained successfully without needing the event level label. Comment: Submitted to ICASSP 201...|$|R
40|$|Detecting when {{voice is}} or is not present is an {{outstanding}} prob-lem for speech transmission, enhancement and recognition. Here we present a novel multichannel source activity detector that ex-ploits the spatial localization of the target <b>audio</b> source. The <b>detector</b> uses an array signal processing technique to maximize the signal-to-interference ratio for the target source thus decreas-ing the activity detection error rate. We compare our two-channel voice activity detector (VAD) with the AMR voice detection algo-rithms on real data recorded in a noisy car environment. The new algorithm shows improvements in error rates of 55 - 70 % compared to the state-of-the-art adaptive multi-rate algorithm AMR 2 used in present voice transmission technology. ...|$|R
40|$|In {{the last}} decades, several systems based on video {{analysis}} {{have been proposed}} for automatically detecting accidents on roads to ensure a quick intervention of emergency teams. However, in some situations, the visual information is not sufficient or sufficiently reliable, whereas the use of microphones and <b>audio</b> event <b>detectors</b> can significantly improve the overall reliability of surveillance systems. In this paper, we propose a novel method for detecting road accidents by analyzing audio streams to identify hazardous situations such as tire skidding and car crashes. Our method {{is based on a}} two-layer representation of an audio stream: at a low level, the system extracts a set of features that is able to capture the discriminant properties of the events of interest, and at a high level, a representation based on a bag-of-words approach is then exploited in order to detect both short and sustained events. The deployment architecture for using the system in real environments is discussed, together with an experimental analysis carried out on a data set made publicly available for benchmarking purposes. The obtained results confirm the effectiveness of the proposed approach...|$|R
40|$|Abstract—In {{the past}} few years, thanks to the {{increasing}} avail-ability of multimedia sharing platforms, the online availability of user generated content has incredibly grown. However, since media sharing is often not well regulated, copyright infringement cases may occur. One classic example is the pirate distribution of audio bootlegs, i. e., concerts illegally recorded using portable devices. In order to guarantee copyrights and avoid the sharing of such illicit material, {{in this paper we}} propose an automatic <b>audio</b> bootleg <b>detector.</b> This can be used to analyze audio data in bulk, in order to filter out from a database the audio tracks recorded, e. g., by fans during a live performance. To this purpose, we propose to use a set of acoustic features to characterize audio bootlegs, justified by theoretical foundations. Then, we train a bi-nary classifier that operates on this set of features to discriminate between: i) audio tracks recorded at either concerts, clubs, or theaters; ii) legally distributed live performances professionally mixed and edited. In order to validate our system, we tested it on a dataset of more than 250 audio excerpts considering different musical genres and different kinds of music performances. The results achieved are promising, showing a high bootleg detection accuracy. I...|$|R

