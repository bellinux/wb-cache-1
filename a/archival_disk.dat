3|7|Public
40|$|A {{significant}} {{weakness of}} many existing parallel supercomputers is {{their lack of}} high-performance parallel I/O. This weakness has prevented, in many cases, the full exploitation of the true potential of MPP systems. As part of a joint project with IBM, we have designed a parallel I/O system for an IBM SP system that can provide sustained I/O rates of greater than 160 MB/s from collections of compute nodes to <b>archival</b> <b>disk</b> and peak transfer rates that should exceed 400 MB/s from compute nodes to I/O servers. This testbed system {{will be used for}} a number of projects. First, it will provide a high-performance experimental I/O system for traditional computational science applications; second, it will be used as an I/O software and development environment for new parallel I/O algorithms and operating systems support; and third, it will be used as the foundation for a number of new projects designed to develop enabling technology for the National Information Infrastructure. This report descr [...] ...|$|E
40|$|A signicant {{weakness}} of many existing parallel su-percomputers is {{their lack of}} high-performance parallel I/O. This weakness has prevented, in many cases, the full exploitation of the true potential of MPP systems. As part of a joint project with IBM, we have designed a parallel I/O system for an IBM SP system that can provide sustained I/O rates of greater than 160 MB/s from collections of compute nodes to <b>archival</b> <b>disk</b> and peak transfer rates that should exceed 400 MB/s from compute nodes to I/O servers. This testbed system {{will be used for}} a number of projects. First, it will provide a high-performance experimental I/O system for tra-ditional computational science applications; second, it will be used as an I/O software and development en-vironment for new parallel I/O algorithms and oper-ating systems support; and third, it will be used as the foundation for a number of new projects designed to develop enabling technology for the National Informa-tion Infrastructure. This report describes the system under development at Argonne National Laboratory, provides some preliminary performance results, and outlines future experiments and directions. ...|$|E
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1989 / Town & Country Hotel & Convention Center, San Diego, CaliforniaGe Aerospace is {{developing}} three classes of optical disk devices for future telemetry applications. SpaceSTORE is a 10 GByte, dual port, rewritable magnetooptic disk drive. Each port suppports continuous write or read at 150 Mbits per second, with an aggregate data rate of 300 Mb/s per drive. One drive and two controllers will be packaged {{in a total}} volume of one cubic foot. Drive and controller modules are configurable in groups which are slaved by Group Controllers to provide single port data rates up to 1800 Mbits per second and capacities up to 1012 bits. Typical applications are Space Station, Polar Orbiting Platforms, Mars Rover, and ground support operations. DuraSTORE is a 5 GByte, rewritable magneto-optic disk drive. It is a single port device and supports continuous write or read at 25 Mbits per second and burst I/O at 50 Mb/s. The drive and SCSI controller will be packaged in a MIL-E- 5400 5. 6 cubic foot rack mount enclosure. The rewritable double sided (10 GByte total) disks are in cartridges, and are removable. A companion 10 disk mini-jukebox provides 100 GBytes capacity and 10 second access. Typical applications are real time signal capture in RC- 135 aircraft and C and 3 image mass storage data bases in van and shelter mobile computer systems. UltraSTORE is a 2. 5 terabyte <b>archival</b> <b>disk</b> jukebox. It utilizes double sides disks with 20 - 25 GByte capacity each. I can be configured with 1 - 3 drives, each operating at data rates (options) from 25 Mbits per second to 1 Gbits/second. Typical applications are ground telemetry data bases, mass storage libraries, and file servers...|$|E
50|$|Founded in 1986, Delkin Devices, Inc. is an {{independently}} owned {{business and}} the largest US memory card manufacturer in San Diego, serving both industrial and consumer markets worldwide. Corporate headquarters are maintained in Poway, California with an international branch located in Birmingham, England. Delkin's digital camera accessory products include: CompactFlash and Secure Digital memory cards and Readers/Writers, the eFilm PicturePad, ImageRouter, <b>Archival</b> Gold <b>Disks,</b> Fat Gecko Camera Mounts, and the patented SensorScope Cleaning System.|$|R
50|$|Electric Crayon {{operated}} for {{slightly over}} {{a year as a}} comic book coloring company. Eventually, the mounting costs of <b>archival</b> hard <b>disk</b> storage and the slow computers of the time made it impractical for the company to scale without either selling to a larger company or raising rates beyond what the market would pay. Buccellato and Siry sold their interest to Rosen and proceeded to form a creative studio, Mad Science Media, focused on content publishing.|$|R
5000|$|Digital {{scanners}} [...] {{can capture}} images {{as large as}} 65mm in full resolution. That is the typical image size on a traditional (as opposed to the IMAX process) 70mm film which used {{a portion of the}} film surface for its multitrack magnetic sound stripe. A 70mm print of a two and a half hour film [...] ran upwards of $170,000. A hard disk capable of storing such a movie is a few hundred dollars. An <b>archival</b> optical <b>disk</b> will be less. The problem of having to transfer the data as new generations of equipment come along will continue, however, until true archival standards are put in place.|$|R
40|$|It {{takes more}} than a good {{algorithm}} to achieve high performance: inner-loop performance and data locality are also important. Tiling is a well-known method for parallelization and for improving data locality. However, tiling {{has the potential of}} being even more beneficial. At the finest granularity, {{it can be used to}} guide register allocation and instruction scheduling; at the coarsest level, it can help manage magnetic storage media. It also can be useful in overlapping data movement with computation, for instance by prefetching data from <b>archival</b> storage, <b>disks</b> and main memory into cache and registers, or by choreographing data movement between processors. Hierarchical tiling is a framework for applying both known tiling methods and new techniques to an expanded set of uses. It eases the burden on several compiler phases that are traditionally treated separately, such as scalar replacement, register allocation, generation of message passing calls, and storage mapping. By explicitly [...] ...|$|R
40|$|Outbursts {{on young}} stars are usually {{interpreted}} as accretion bursts caused by instabilities in the disk or the star–disk connection. However, some protostellar outbursts may {{not fit into}} this framework. In this paper, we analyze optical and near-infrared spectra and photometry to characterize the 2015 outburst of the probable young star ASASSN- 15 qi. The ~ 3. 5 mag brightening in the V band was sudden, with an unresolved rise time of less than one day. The outburst decayed exponentially by 1 mag for 6 days and then gradually back to the pre-outburst level after 200 days. The outburst is dominated by emission from ~ 10, 000 K gas. An explosive release of energy accelerated matter from the star in all directions, seen in a spectacular cool, spherical wind with a maximum velocity of 1000 km s^(− 1). The wind and hot gas both disappeared as the outburst faded and the source returned to its quiescent F-star spectrum. Nebulosity near the star brightened with a delay of 10 – 20 days. Fluorescent excitation of H 2 is detected in emission from vibrational levels as high as v = 11, also with a possible time delay in flux increase. The mid-infrared spectral energy distribution does not indicate the presence of warm dust emission, though the optical photospheric absorption and CO overtone emission {{could be related to}} a gaseous <b>disk.</b> <b>Archival</b> photometry reveals a prior outburst in 1976. Although we speculate about possible causes for this outburst, none of the explanations are compelling...|$|R
40|$|We {{present a}} Spitzer InfraRed Spectrometer search for 10 - 36 micron {{molecular}} emission {{from a large}} sample of protoplanetary disks, including lines from H 2 O, OH, C 2 H 2, HCN and CO 2. This paper describes the sample and data processing and derives the detection rate of mid-infrared molecular emission {{as a function of}} stellar mass. The sample covers a range of spectral type from early M to A, and is supplemented by <b>archival</b> spectra of <b>disks</b> around A and B stars. It is drawn from a variety of nearby star forming regions, including Ophiuchus, Lupus and Chamaeleon. In total, we identify 22 T Tauri stars with strong mid-infrared H 2 O emission. Integrated water line luminosities, where water vapor is detected, range from 5 x 10 ^- 4 to 9 x 10 ^- 3 Lsun, likely making water the dominant line coolant of inner disk surfaces in classical T Tauri stars. None of the 5 transitional disks in the sample show detectable gaseous molecular emission with Spitzer upper limits at the 1 % level in terms of line-to-continuum ratios (apart from H 2). We find a strong dependence on detection rate with spectral type; no disks around our sample of 25 A and B stars were found to exhibit water emission, down to 1 - 2 % line-to-continuum ratios, in the mid-infrared, while almost 2 / 3 of the disks around K stars show sufficiently intense water emission to be detected by Spitzer. Some Herbig Ae/Be stars show tentative H 2 O/OH emission features beyond 20 micron at the 1 - 2 level, however, and one of them shows CO 2 in emission. We argue that the observed differences between T Tauri disks and Herbig Ae/Be disks is due to a difference in excitation and/or chemistry depending on spectral type and suggest that photochemistry may be playing {{an important role in the}} observable characteristics of mid-infrared molecular line emission from protoplanetary disks. Comment: 19 pages, accepted for publication in Ap...|$|R
40|$|The Department of Energy s Leadership Computing Facility, {{located at}} Oak Ridge National Laboratory s National Center for Computational Sciences, {{recently}} polled scientific teams that had large allocations {{at the center}} in 2007, asking them to identify computational science requirements for future exascale systems (capable of an exaflop, or 1018 floating point operations per second). These requirements are necessarily speculative, since an exascale system will not be realized until the 2015 2020 timeframe, and are expressed where possible relative to a recent petascale requirements analysis of similar science applications [1]. Our initial findings, which beg further data collection, validation, and analysis, did in fact align with many of our expectations and existing petascale requirements, yet they also contained some surprises, complete with new challenges and opportunities. First and foremost, the {{breadth and depth of}} science prospects and benefits on an exascale computing system are striking. Without a doubt, they justify a large investment, even with its inherent risks. The possibilities for return on investment (by any measure) are too large to let us ignore this opportunity. The software opportunities and challenges are enormous. In fact, as one notable computational scientist put it, the scale of questions being asked at the exascale is tremendous and the hardware has gotten way ahead of the software. We are in grave danger of failing because of a software crisis unless concerted investments and coordinating activities are undertaken to reduce and close this hardwaresoftware gap over the next decade. Key to success will be a rigorous requirement for natural mapping of algorithms to hardware in a way that complements (rather than competes with) compilers and runtime systems. The level of abstraction must be raised, and more attention must be paid to functionalities and capabilities that incorporate intent into data structures, are aware of memory hierarchy, possess fault tolerance, exploit asynchronism, and are power-consumption aware. On the other hand, we must also provide application scientists with the ability to develop software without having to become experts in the computer science components. Numerical algorithms are scattered broadly across science domains, with no one particular algorithm being ubiquitous and no one algorithm going unused. Structured grids and dense linear algebra continue to dominate, but other algorithm categories will become more common. A significant increase is projected for Monte Carlo algorithms, unstructured grids, sparse linear algebra, and particle methods, and a relative decrease foreseen in fast Fourier transforms. These projections reflect the expectation of much higher architecture concurrency and the resulting need for very high scalability. The new algorithm categories that application scientists expect to be increasingly important in the next decade include adaptive mesh refinement, implicit nonlinear systems, data assimilation, agent-based methods, parameter continuation, and optimization. The attributes of leadership computing systems expected to increase most in priority over the next decade are (in order of importance) interconnect bandwidth, memory bandwidth, mean time to interrupt, memory latency, and interconnect latency. The attributes expected to decrease most in relative priority are <b>disk</b> latency, <b>archival</b> storage capacity, <b>disk</b> bandwidth, wide area network bandwidth, and local storage capacity. These choices by application developers reflect the expected needs of applications or the expected reality of available hardware. One interpretation is that the increasing priorities reflect the desire to increase computational efficiency to take advantage of increasing peak flops [floating point operations per second], while the decreasing priorities reflect the expectation that computational efficiency will not increase. Per-core requirements appear to be relatively static, while aggregate requirements will grow with the system. This projection is consistent with a relatively small increase in performance per core with a dramatic {{increase in the number of}} cores. Leadership system software must face and overcome issues that will undoubtedly be exacerbated at the exascale. The operating system (OS) must be as unobtrusive as possible and possess more stability, reliability, and fault tolerance during application execution. As applications will be more likely at the exascale to experience loss of resources during an execution, the OS must mitigate such a loss with a range of responses. New fault tolerance paradigms must be developed and integrated into applications. Just as application input and output must not be an afterthought in hardware design, job management, too, must not be an afterthought in system software design. Efficient scheduling of those resources will be a major obstacle faced by leadership computing centers at the exas [...] ...|$|R

