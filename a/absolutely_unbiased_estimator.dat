0|2509|Public
40|$|In a {{companion}} paper (McRobie(2013) arxiv: 1304. 3918), a simple set of `elemental' estimators was presented for the Generalized Pareto tail parameter. Each elemental estimator: involves only three log-spacings; is <b>absolutely</b> <b>unbiased</b> for all {{values of the}} tail parameter; is location- and scale-invariant; and is valid for all sample sizes $N$, even as small as $N= 3 $. It was suggested that linear combinations of such elementals could then be used to construct efficient <b>unbiased</b> <b>estimators.</b> In this paper, the analogous mathematical approach is taken to the Generalised Extreme Value (GEV) distribution. The resulting elemental <b>estimators,</b> although not <b>absolutely</b> <b>unbiased,</b> are found to have very small bias, and may thus provide a useful basis {{for the construction of}} efficient estimators. Comment: 18 pages, 11 figure...|$|R
5000|$|In {{statistics}} a minimum-variance <b>unbiased</b> <b>estimator</b> (MVUE) or uniformly minimum-variance <b>unbiased</b> <b>estimator</b> (UMVUE) is an [...] <b>unbiased</b> <b>estimator</b> {{that has}} lower variance {{than any other}} <b>unbiased</b> <b>estimator</b> for all possible values of the parameter.|$|R
5000|$|Further, {{while the}} {{corrected}} sample variance {{is the best}} <b>unbiased</b> <b>estimator</b> (minimum mean square error among <b>unbiased</b> <b>estimators)</b> of variance for Gaussian distributions, if the distribution is not Gaussian then even among <b>unbiased</b> <b>estimators,</b> the best <b>unbiased</b> <b>estimator</b> of the variance may not be ...|$|R
3000|$|... is {{the minimum}} {{variance}} <b>unbiased</b> <b>estimator</b> (MVUE), i.e., {{it is an}} <b>unbiased</b> <b>estimator</b> that has lower variance than any other <b>unbiased</b> <b>estimator</b> for all possible values of the parameter [26].|$|R
50|$|Values of MSE may be {{used for}} {{comparative}} purposes. Two or more statistical models may be compared using their MSEs as a measure of how well they explain a given set of observations: An <b>unbiased</b> <b>estimator</b> (estimated from a statistical model) with the smallest variance among all <b>unbiased</b> <b>estimators</b> is the best <b>unbiased</b> <b>estimator</b> or MVUE (Minimum Variance <b>Unbiased</b> <b>Estimator).</b>|$|R
50|$|Among <b>unbiased</b> <b>estimators,</b> there often exists {{one with}} the lowest {{variance}}, called the minimum variance <b>unbiased</b> <b>estimator</b> (MVUE). In some cases an <b>unbiased</b> efficient <b>estimator</b> exists, which, {{in addition to having}} the lowest variance among <b>unbiased</b> <b>estimators,</b> satisfies the Cramér-Rao bound, which is an absolute lower bound on variance for statistics of a variable.|$|R
40|$|The {{problem of}} {{unbiased}} {{estimation of the}} mean life and the reliability for an exponential life distribution using time censored sample data are considered. We prove that there does not exist an <b>unbiased</b> <b>estimator</b> of the mean life based on a time censored sample. On the other hand, for reliability estimation at an arbitrarily specified time point, we give necessary and sufficient conditions {{for the existence of}} (i) an <b>unbiased</b> <b>estimator</b> (ii) the uniformly minimum variance <b>unbiased</b> <b>estimator.</b> We also provide a characterization of the class of <b>unbiased</b> <b>estimators</b> of the reliability, based on a sufficient statistic in situations where <b>unbiased</b> <b>estimators</b> exist. ...|$|R
40|$|An {{identity}} of integrals for the l 1 -norm symmetric matrix variate distributions with unknown common location parameter and unknown and possibly unequal scale {{parameters of the}} columns is established. An <b>unbiased</b> <b>estimator</b> for the location parameter is obtained and is shown to dominate the maximum likelihood estimator under the squared error loss. Under certain conditions this <b>unbiased</b> <b>estimator</b> is the uniformly minimum variance <b>unbiased</b> <b>estimator.</b> <b>Unbiased</b> <b>estimator</b> MLE UMVUE Exponential distribution...|$|R
40|$|The Cramér-Rao Inequality {{provides}} a lower {{bound for the}} variance of an <b>unbiased</b> <b>estimator</b> of a parameter. It allows us to conclude that an <b>unbiased</b> <b>estimator</b> is a minimum variance <b>unbiased</b> <b>estimator</b> for a parameter. In these notes we prove the Cramér-Rao inequality and examine some applications. We conclude with a discussion of...|$|R
5000|$|... where [...] is {{the unique}} {{symmetric}} <b>unbiased</b> <b>estimator</b> {{of the third}} cumulant and [...] is the symmetric <b>unbiased</b> <b>estimator</b> of the second cumulant (i.e. the variance).|$|R
50|$|For a Gaussian {{distribution}} this is {{the best}} <b>unbiased</b> <b>estimator</b> (that is, it has the lowest MSE among all <b>unbiased</b> <b>estimators),</b> but not, say, for a uniform distribution.|$|R
40|$|Two {{classes of}} <b>unbiased</b> <b>estimators</b> of the density {{function}} of ergodic {{distribution for the}} diffusion process of observations are proposed. The estimators are square-root consistent and asymptotically normal. This curious situation is entirely different from the case of discrete-time models (Davis 1977) where the <b>unbiased</b> <b>estimator</b> rarely exists and usually the estimators are not square-root consistent. Diffusion process Nonparametric estimation Density function estimation <b>Unbiased</b> <b>estimator</b> Asymptotic normality...|$|R
40|$|This paper {{considers}} the percentage {{impact of a}} dummy variable regressor {{on the level of}} the dependent variable in a semilogarithmic regression equation with normal disturbances. We derive an exact <b>unbiased</b> <b>estimator,</b> its variance, and an exact <b>unbiased</b> <b>estimator</b> of the variance. The main practical contribution lies in a convenient approximation for the <b>unbiased</b> <b>estimator</b> of the variance, which can be reported together with Kennedy's approximate <b>unbiased</b> <b>estimator</b> of the percentage change. The two approximations are very simple, yet highly reliable. The results are applied to teacher earnings and further illustrated by examples from the literature...|$|R
5000|$|By the Rao-Blackwell theorem, if [...] is an <b>unbiased</b> <b>estimator</b> of θ then [...] defines an <b>unbiased</b> <b>estimator</b> of θ {{with the}} {{property}} that its variance is not {{greater than that}} of [...]|$|R
40|$|The {{problem is}} to {{estimate}} {{the mean of the}} selected population. The selection rule is to choose the population with the largest sample mean when such sample means are calculated from the first stage sample. An estimator of the selected mean is unbiased if its expected value equals the expected value of the selected mean. We seek conditionally <b>unbiased</b> <b>estimators</b> of the selected mean given the ordering of the set of sample means based on the first stage sample. Conditionally <b>unbiased</b> <b>estimators</b> are of course unconditionally unbiased. For several distributions such as the normal, with unknown mean, and binomial, no conditionally <b>unbiased</b> <b>estimators</b> exist based on a one stage sample. We propose a two stage sample where observations at stage two are taken from the selected population only. Such a procedure has the advantage of yielding conditionally <b>unbiased</b> <b>estimators</b> and enables, possibly a better allocation of available sample points. We find the uniformly minimum variance conditionally <b>unbiased</b> <b>estimators</b> (UMVCUE) for the normal case when the variance is known or when a common unknown variance is present. We also find the UMVCUE for the gamma case and indicate that the method is suitable for many other cases as well. <b>unbiased</b> <b>estimators</b> two stage sample selected mean uniformly minimum variance conditionally <b>unbiased</b> <b>estimator...</b>|$|R
40|$|Abstract. The minimum {{variance}} <b>unbiased</b> <b>estimator</b> of {{the intensity of}} intersections is found for stationary Poisson process of segments with parameterized distribution of primary grain with known and unknown parameters. The {{minimum variance}} <b>unbiased</b> <b>estimators</b> are compared with commonly used estimators...|$|R
40|$|The {{canonical}} {{form for the}} comparison of certain linear estimators using Pitman's Measure of Closeness is generalized to the class of all linear estimators. Under the assumption of normality, the equivalence of Pitman-closest linear <b>unbiased</b> <b>estimators</b> and best linear <b>unbiased</b> <b>estimators</b> is shown. A sufficient condition is given for which the BLUE will be Pitman-closer than the best linear equivalent estimator (BLEE). Pitman's measure of closeness order statistics best linear <b>unbiased</b> <b>estimators</b> best linear equivariant estimators...|$|R
50|$|See also minimum-variance <b>unbiased</b> <b>estimator.</b>|$|R
5000|$|As it {{has been}} stated before, the {{condition}} of [...] {{is equivalent to the}} property that the best linear <b>unbiased</b> <b>estimator</b> of [...] is [...] (best {{in the sense that it}} has minimum variance). To see this, let [...] another linear <b>unbiased</b> <b>estimator</b> of [...]|$|R
30|$|Therefore, from (26), ĥ is an <b>unbiased</b> <b>estimator.</b>|$|R
50|$|In particular,Pn(A) is an <b>unbiased</b> <b>estimator</b> of P(A).|$|R
5000|$|... k-statistic, a minimum-variance <b>unbiased</b> <b>estimator</b> of a {{cumulant}} ...|$|R
50|$|Minimizing MSE {{is a key}} {{criterion}} {{in selecting}} estimators: see minimum mean-square error. Among <b>unbiased</b> <b>estimators,</b> minimizing the MSE is equivalent to minimizing the variance, and the estimator that does this is the minimum variance <b>unbiased</b> <b>estimator.</b> However, a biased estimator may have lower MSE; see estimator bias.|$|R
5000|$|It is {{important}} to keep in mind this correction only produces an <b>unbiased</b> <b>estimator</b> for normally and independently distributed X. When this condition is satisfied, another result about s involving c4(n) is that the standard error of s is , while the standard error of the <b>unbiased</b> <b>estimator</b> is ...|$|R
50|$|All else being equal, an <b>unbiased</b> <b>estimator</b> is {{preferable}} to a biased estimator, but in practice all else is not equal, and biased estimators are frequently used, generally with small bias. When a biased estimator is used, bounds of the bias are calculated. A biased estimator {{may be used for}} various reasons: because an <b>unbiased</b> <b>estimator</b> does not exist without further assumptions about a population or is difficult to compute (as in unbiased estimation of standard deviation); because an estimator is median-unbiased but not mean-unbiased (or the reverse); because a biased estimator reduces some loss function (particularly mean squared error) compared with <b>unbiased</b> <b>estimators</b> (notably in shrinkage estimators); or because in some cases being unbiased is too strong a condition, and the only <b>unbiased</b> <b>estimators</b> are not useful. Further, mean-unbiasedness is not preserved under non-linear transformations, though median-unbiasedness is (see effect of transformations); for example, the sample variance is an <b>unbiased</b> <b>estimator</b> for the population variance, but its square root, the sample standard deviation, is a biased estimator for the population standard deviation. These are all illustrated below.|$|R
40|$|Unbiased {{estimation}} for {{parameters of}} maximal distribution {{is a very}} fundamental problem in the statistical theory of sublinear expectation. In this paper, we proved that the maximum estimator is the largest <b>unbiased</b> <b>estimator</b> for the upper mean and the minimum estimator is the smallest <b>unbiased</b> <b>estimator</b> the the lower mean...|$|R
50|$|The Theil-Sen <b>estimator</b> is an <b>unbiased</b> <b>estimator</b> of {{the true}} slope in simple linear regression. For many {{distributions}} of the response error, this estimator has high asymptotic efficiency relative to least-squares estimation. Estimators with low efficiency require more independent observations to attain the same sample variance of efficient <b>unbiased</b> <b>estimators.</b>|$|R
5000|$|If an <b>unbiased</b> <b>estimator</b> of [...] exists, {{then one}} can prove {{there is an}} {{essentially}} unique MVUE. Using the Rao-Blackwell theorem one can also prove that determining the MVUE is {{simply a matter of}} finding a complete sufficient statistic for the family [...] and conditioning any <b>unbiased</b> <b>estimator</b> on it.|$|R
2500|$|In fact, the minimum-variance <b>unbiased</b> <b>estimator</b> (MVUE) for θ is ...|$|R
5000|$|It {{does not}} yield an <b>unbiased</b> <b>estimator</b> of {{standard}} deviation.|$|R
5000|$|The unique minimum {{variance}} <b>unbiased</b> <b>estimator</b> radj {{is given by}} ...|$|R
40|$|We {{investigate}} {{the estimation of}} specific intrinsic volumes of stationary Boolean models by local digital algorithms; that is, by weighted sums of n × [...] . × n configuration counts. We show that asymptotically <b>unbiased</b> <b>estimators</b> for the specific surface area or integrated mean curvature do not exist if the dimension is {{at least two or}} three, respectively. For 3 -dimensional stationary, isotropic Boolean models, we derive asymptotically <b>unbiased</b> <b>estimators</b> for the specific surface area and integrated mean curvature. For a Boolean model with balls as grains we even obtain an asymptotically <b>unbiased</b> <b>estimator</b> for the specific Euler characteristic. Comment: 28 page...|$|R
40|$|Model {{selection}} criteria often arise by constructing unbiased or approximately <b>unbiased</b> <b>estimators</b> of measures known as expected overall discrepancies (Linhart & Zucchini, 1986, p. 19). Such measures quantify {{the disparity between}} the true model (i. e., the model which generated the observed data) and a fitted candidate model. For linear regression with nor-mally distributed error terms, the “corrected ” Akaike information criterion and the “mod-ified ” conceptual predictive statistic have been proposed as exactly <b>unbiased</b> <b>estimators</b> of their respective target discrepancies. We expand on previous work to additionally show that these criteria achieve minimum variance within the class of <b>unbiased</b> <b>estimators...</b>|$|R
40|$|In {{this note}} {{we deal with}} some {{admissibility}} conditions proved by G. B. Tranquilli to be sufficient {{in the class of}} <b>unbiased</b> <b>estimators</b> of finite population parameters and with respect to (w. r. t.) a quadratic loss function. We show that the same conditions:/) are sufficient for the admissibility of an <b>unbiased</b> <b>estimator</b> with any loss function; ii) imply hyperadmissibility with reference to a particular (critical) population of the. From this fact we deduce that, for a fixed critical population, there is at most one estimator, in the class of all <b>unbiased</b> <b>estimator</b> of a finite population parameter, which satisfies Tranquilli condition...|$|R
2500|$|... minimum-variance <b>unbiased</b> <b>{{estimator}}</b> (UMVU) estimator for {{the maximum}} {{is given by}} ...|$|R
5000|$|If {{the weights}} are {{frequency}} weights, then the <b>unbiased</b> <b>estimator</b> is: ...|$|R
