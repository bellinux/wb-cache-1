19|25|Public
5000|$|... {{production}} of adequate reference {{works for the}} translator, including the adaptation of glossaries that now exist primarily for <b>automatic</b> <b>dictionary</b> look-up in machine translation ...|$|E
5000|$|Parquet has an <b>automatic</b> <b>dictionary</b> {{encoding}} enabled dynamically {{for data}} {{with a small}} number of unique values ( [...] < 10^5 [...] ) that aids in significant compression and boosts processing speed.|$|E
40|$|Natural Language Processing is {{constituted}} by {{a whole set}} of activities, ranging from <b>automatic</b> <b>dictionary</b> look-up to syntactic and semantic analysis. When elaborating algorithms or developing tools for one particular activity, it seems (theoretically) wise to keep abstract enough not to inhibit future uses of the same means for the treatment of other sets of phenomena...|$|E
50|$|The report recommended, however, that tools be {{developed}} to aid translators — <b>automatic</b> <b>dictionaries,</b> for example — and that some research in computational linguistics {{should continue to be}} supported.|$|R
5000|$|The first patents for [...] "translating machines" [...] {{were applied}} {{for in the}} mid-1930s. One proposal, by Georges Artsrouni was simply an <b>automatic</b> bilingual <b>dictionary</b> using paper tape. The other proposal, by Peter Troyanskii, a Russian, was more detailed. It {{included}} both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto.|$|R
50|$|Automatic {{installation}} of the extension is possible online using Firefox's or Thunderbird's Add-ons tool or Moji's own automatic tool. Using Moji requires the {{installation of}} the Moji extension itself and of at least two dictionaries (a word dictionary and a kanji <b>dictionary).</b> <b>Automatic</b> installation requires JavaScript, but all necessary files can be downloaded and installed manually.|$|R
40|$|Dictionary {{methods for}} cross-language {{information}} retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases has km {{shown to be}} one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasaI translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with <b>automatic</b> <b>dictionary</b> translation. ...|$|E
40|$|Given K utterances of a {{word and}} a set of sub-word units one may need a {{generalization}} of the conventional one-dimensional Viterbi algorithm to jointly decode them in order to derive their underlying word model (pronunciation). This extension is called k-dimensional Viterbi. However, as the number of utterances increases, the complexity of the k-dimensional Viterbi algorithm exponentially increases causing prohibitive computational burden. Here, we propose an approximation algorithm for the k-dimensional Viterbi which efficiently uses the available utterances to estimate the pronunciation. In addition to <b>automatic</b> <b>dictionary</b> generation, it can be used in computationally expensive applications such as lexicon-free training and joint pattern alignment. Index Terms: pronunciation, joint decoding, k-dimensional, viterb...|$|E
40|$|The {{position}} {{of political parties}} on policy issues is crucial for many questions of political science, including studies of political representation. This research note examines different methods for obtaining party positions on immigration in retrospective. Party positions are obtained using pooled expert surveys, manual coding of party manifestos with a conventional codebook, manual coding of manifestos using check-lists, and automatic coding of manifestos using Wordscores and a dictionary of keywords respectively. In addition, positions from a media analysis and a retrospective evaluation of researchers {{in the field of}} immigration are used. The results suggest that most methods differentiate the same order of party positions. While there are high correlations between many methods, the different methods tend not to agree on the exact positions. The <b>automatic</b> <b>dictionary</b> approach does not seem to measure party positions reliably...|$|E
40|$|In {{this paper}} {{we present a}} system for {{protecting}} the privacy of cryptograms to avoid detection by censors. The system transforms ciphertext into innocuous text which can be transformed back into the original ciphertext. The expandable set of tools allows experimentation with custom <b>dictionaries,</b> <b>automatic</b> simulation of writing style, {{and the use of}} Context-Free-Grammars to control text generation. ...|$|R
40|$|Most {{existing}} {{methods of}} <b>automatic</b> bilingual <b>dictionary</b> induction rely on prior alignments between {{the source and}} target languages, such as parallel corpora or seed dictionaries. For many language pairs, such supervised alignments are not readily available. We propose an unsupervised approach for learning a bilingual dictionary {{for a pair of}} languages given their independently-learned monolingual word embeddings. The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other. We show experimentally that the performance of the bilingual alignments learned using the unsupervised method is comparable to supervised bilingual alignments using a seed dictionary. Comment: 10 pages, 7 figure...|$|R
40|$|The {{aim of this}} {{bachelor}} thesis was to make {{a system}} for <b>automatic</b> creation of <b>dictionaries</b> from translations. It describes the implementation of a system that generates Czech-English dictionary from the aligned parallel corpus and summarizes the results. It also analyzed CzEng parallel corpus, which was used as the data source for dictionaries and explainS the theoretical concepts related to this topic...|$|R
40|$|Dictionary {{methods for}} cross-language {{information}} retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases {{has been shown}} to be one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasal translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with <b>automatic</b> <b>dictionary</b> translation. 1 Introduction The development of IR systems for languages other than English has focused on building mono-lingual systems. Increased availability of on-line text in languages other than English and increased multi-national collaboration have motivated research in cross-language information retrieval (CLIR) - the development of systems to perform retrieval across languages. There have been three main approaches to CLIR: translation via machine t [...] ...|$|E
40|$|Domain-speci#c text {{analysis}} requires a dictionary of linguistic patterns that identify references to relevant {{information in a}} text. This paper describes CRYSTAL, a fully automated tool that induces such a dictionary of text extraction rules. We discuss some key issues in developing an <b>automatic</b> <b>dictionary</b> induction system, using CRYSTAL as a concrete example. CRYSTAL derives text extraction rules from training instances and generalizes each rule as far as possible, testing the accuracy of each proposed rule on the training corpus. An error tolerance parameter allows CRYSTAL to manipulate a trade-o# between recall and precision. We discuss issues involved with creating training data, de#ning a domain ontology, and allowing a #exible and expressive representation while designing a search control mechanism that avoids intractability. 1 Domain-speci#c Text Analysis Considerable domain knowledge is needed by a system that analyzes unrestricted text and identi#es inform [...] ...|$|E
40|$|In {{this paper}} we {{approach}} the multiword expressions (MWE) {{from the viewpoint of}} rule-based machine translation (MT). Rather than trying to find a theoretical definition to the concept, we {{look at it as a}} problem of translation result. Because rule-based MT relies on a detailed description of the source language, each translation mistake can be traced and eventually corrected. A large part of translation mistakes can be attributed to inadequate handling of MWEs. The discussion is carried out in the context of SALAMA (Swahili Language manager), which, inter alia, translates text from Swahili to English. Various types of MWEs are discussed and solutions for handling them are introduced. The performance of SALAMA in identifying MWEs was tested. Also the distribution of various types of MWEs in text is presented. Key Words: multiword expressions, rule-based machine translation, Constraint Grammar, Swahili language, <b>automatic</b> <b>dictionary</b> compilatio...|$|E
40|$|In {{this thesis}} {{we present a}} system for {{protecting}} the privacy of cryptograms to avoid detection by censors. The system transforms ciphertext into innocuous text which is transformed back into the original ciphertext. The expandable set of tools allows experimentation with custom <b>dictionaries,</b> <b>automatic</b> simulation of writing style, {{and the use of}} Context-Free-Grammars to control text generation. Keywords: Ciphertext, Privacy, Information-Hiding G. I. Davida Date iii iv Contents...|$|R
40|$|Automatic {{transliteration}} and back-transliteration across languages with drastically different alphabets and phonemes inventories such as English/Korean, English/Japanese, English/Arabic, English/Chinese, etc, have practical {{importance in}} machine translation, cross-lingual information retrieval, and <b>automatic</b> bilingual <b>dictionary</b> compilation, etc. In this paper, a bi-directional {{and to some}} extent language independent methodology for English/Korean transliteration and back-transliteration is described. Our method is composed of character alignment and decision tree learning. We induce transliteration rules for each English alphabet and back-transliteration rules for each Korean alphabet. For the training of decision trees we need a large labeled examples of transliteration and back-transliteration. However this kind of resources are generally not available. Our character alignment algorithm is capable of highly accurately aligning English word and Korean transliteration in a desired way. 1...|$|R
40|$|In this paper, {{we discuss}} {{automatically}} generating a phonetic pronunciation from an orthographic spelling of words. The letter-sequence to phoneme-sequence mapping {{is useful in}} a variety of contexts, including text-to-speech applications, automatic spelling correction, and generating a pronunciation lexicon for a new training dataset which contains out-of-vocabulary words. A system based on hidden Markov models is described, and is then used to generate pronunciations for outof-vocabulary the words and word fragments in the Fisher conversational telephone speech corpus. The Fisher phonetic pronunciations are analyzed to show that for conversational speech and a typical phonetic dictionary, a large amount of lexical ambiguity remains even when the word boundaries and phonetic transcriptions are known. Index Terms: <b>automatic</b> pronunciation <b>dictionary</b> generation, letter-to-phone string mappin...|$|R
40|$|Bilingual {{dictionaries}} are the {{key component}} of the cross-lingual similarity estimation methods. Usually such dictionary generation is accomplished by manual or automatic means. Automatic generation approaches include to exploit parallel or comparable data to derive dictionary entries. Such approaches require large amount of bilingual data {{in order to produce}} good quality dictionary. Many time the language pair does not have large bilingual comparable corpora and in such cases the best <b>automatic</b> <b>dictionary</b> is upper bounded by the quality and coverage of such corpora. In this work we propose a method which exploits continuous quasi-comparable corpora to derive term level associations for enrichment of such limited dictionary. Though we propose our experiments for English and Hindi, our approach can be easily extendable to other languages. We evaluated dictionary by manually computing the precision. In experiments we show our approach is able to derive interesting term level associations across languages...|$|E
40|$|A {{system of}} {{procedures}} and computer programs is {{proposed for the}} semi-automatic synthesis of Russian-English translation algorithms. For the purposes of automatic formula finding, a large corpus of Russian scientific and technical text may be processed by an automatic Russian-English dictionary, the resulting word-by-word translation post-edited according to a systematic procedure, and the final translation trans-cribed back onto magnetic tape for input to a computer. The operation of the proposed system {{is based on the}} automatic comparison of magnetic tapes containing the original <b>automatic</b> <b>dictionary</b> outputs with ones containing the parallel post-edited texts. It is expected that, when given proper clues, the formula finder will be capable of synthesizing algorithms {{that can be used to}} convert one text into the other. The clues corresponding to a desired algorithm consist mainly of a list of logical variables that might in some combination govern the appli-cation of a specified post-editing transformation. Whenever a product o...|$|E
40|$|Abstract. This paper {{describes}} an <b>automatic</b> <b>dictionary</b> construction method for Named Entity Recognition (NER) on specific domains such as restaurant guides. NER {{is the first}} step toward Information Extraction (IE), and we believe that such a dictionary construction method for NER is crucial for developing IE systems for a wide range of domains in the World Wide Web (WWW). One serious problem in NER on specific domains is that the performance of NER heavily depends on the amount of the training corpus, which requires much human labor to develop. We attempt to improve the performance of NER by using dictionaries automatically constructed from HTML documents instead of by preparing a large annotated corpus. Our dictionary construction method exploits the cooccurrence strength of two expressions in HTML itemizations calculated from average mutual information. Experimental results show that the constructed dictionaries improved the performance of the NER on a restaurant guide domain. Our method increased the F 1 -measure by 2. 3 without any additional manual labor. ...|$|E
5000|$|In the mid-1930s {{the first}} patents for [...] "translating machines" [...] were applied for by Georges Artsrouni, for an <b>automatic</b> bilingual <b>dictionary</b> using paper tape. Russian Peter Troyanskii {{submitted}} {{a more detailed}} proposal [...] that included both the bilingual dictionary and a method for dealing with grammatical roles between languages, based on the grammatical system of Esperanto. This system was separated into three stages: stage one consisted of a native-speaking editor in the source language to organize the words into their logical forms and to exercise the syntactic functions; stage two required the machine to [...] "translate" [...] these forms into the target language; and stage three required a native-speaking editor in the target language to normalize this output. Troyanskii's proposal remained unknown until the late 1950s, by which time computers were well-known and utilized.|$|R
40|$|A Part-of-Speech (POS) tagger is a {{tool that}} {{automatically}} resolves the ambiguities that would occur if a text was tagged {{with the help of}} a <b>dictionary.</b> <b>Automatic</b> tagging of texts is used in many applications (grammar checkers, etc.), and quite high accuracy can be achieved. This document describes a stochastic POS tagger that uses a unigram version of the Viterbi algorithm. The overall idea behind the stochastic POS tagger and the Viterbi algorithm is also described. The unigra...|$|R
40|$|BEYTrans (Better Environment for Your TRANSlation) is {{a generic}} Wiki tool {{designed}} to support communities of volunteer translators {{not only by}} offering them an online translation editor and helps to manage the translation progress, but a complete online computer-assisted translation (CAT) environment including a translation editor (BT-editor), translation memories, free <b>dictionaries,</b> <b>automatic</b> calls to MT systems, and support to collaborative volunteer translation. We present the basic concepts of BEYTrans and its experimentation on the translation fro...|$|R
40|$|The paper {{introduces}} a query translation model that re ects {{the structure of}} the cross-language information retrieval task. The model is based on a structured bilingual dictionary in which the translations of each term are clustered into groups with distinct meanings. Query translation is modeled as a two-stage process, with the system rst determining the intended meaning of a query term and then selecting translations appropriate to that meaning that might appear in the document collection. An implementation of structured translation based on <b>automatic</b> <b>dictionary</b> clustering is described and evaluated by using Chinese queries to retrieve English documents. Structured translation achieved an average precision that was statistically indistinguishable from Pirkola's technique for very short queries, but Pirkola's technique outperformed structured translation on long queries. The paper concludes with some observations on future work to improve retrieval e ectiveness and on other potential uses of structured translation in interactive cross-language retrieval applications. 1...|$|E
40|$|Citation {{practices}} {{have been and}} continue to be a concentrated area of research activity among writing researchers, spanning many disciplines. This research presents a re-analysis of a common data set contributed by Karatsolis (this issue), which focused on the citation practices of 8 PhD advisors and 8 PhD advisees across four disciplines. Our purpose in this paper is to show what automated dictionary methods can uncover on the same data based on a text analysis and visualization environment we have been developing over many years. The results of our analysis suggest that, although <b>automatic</b> <b>dictionary</b> methods cannot reproduce the fine granularity of interpretative coding schemes designed for human coders, it can find significant non-adjacent patterns distributed across a text or corpus that will likely elude the analyst relying solely on serial reading. We report on the discovery of several of these patterns that we believe complement Karatsolis’ original analysis and extend the citation literature at large. We conclude the paper by reviewing some of the advantages and limits of dictionary approaches to textual analysis, as well as debunking some common misconceptions against them...|$|E
40|$|The article {{deals with}} the {{linguistic}} database for the system of automatic generation of English advertising texts on cosmetics and perfumery. The database for such a system includes two main blocks: <b>automatic</b> <b>dictionary</b> (that contains semantic and morphological information for each word), and semantic-syntactical formulas of the texts in a special formal language SEMSINT. The database is built on {{the result of the}} analysis of 30 English advertising texts on cosmetics and perfumery. First, each word was given a unique code. For example, N stands for nouns, A – for adjectives, V – for verbs, etc. Then all the lexicon of the analyzed texts was distributed into different semantic categories. According to this semantic classification each word was given a special semantic code. For example, the record N 01 that is attributed to the word «lip» in the dictionary means that this word refers to nouns of the semantic category «part of a human’s body». The second block of the database includes the semantic-syntactical formulas of the analyzed advertising texts written in a special formal language SEMSINT. The author gives a brief description of this language, presenting its essence and structure. Also, an example of one formalized advertising text in SEMSINT is provided. </p...|$|E
40|$|We {{introduce}} a multi-language named-entity recognition {{system based on}} HMM. Japanese, Chinese, Korean and English versions have already been implemented. In principle, it can analyze any other language if we have training data of the target language. This system has a common analytical engine and it can handle any language simply by changing the lexical analysis rules and statistical language model. In this paper, we describe the architecture and accuracy of the named-entity system, and report preliminary experiments on <b>automatic</b> bilingual named-entity <b>dictionary</b> construction using the Japanese and English named-entity recognizer. 1...|$|R
40|$|This paper {{presents}} an <b>automatic</b> Generator of <b>dictionary</b> definitions for concrete entities, {{based on information}} extracted from a Computational Lexicon (CL) containing semantic information. The aim of the adopted approach, combining NLG techniques with the exploitation of the formalised and systematic lexical information stored in CL, is to produce well formed dictionary definitions free from the shortcomings of traditional dictionaries. The architecture {{of the system is}} presented, focusing on the adaptation of the NLG techniques to the specific application requirements, and on the interface between the CL and the Generator. Emphasis is given on the appropriateness of the CL for the application purposes. 1...|$|R
40|$|Multilingual {{information}} access applications, {{which are}} driven by modeling lexical correspondences between different human languages, are obviously reliant on lexical resources to a high degree — {{the quality of the}} lexicon is the main bottleneck for quality of performance and coverage of service. While automatic text and speech translation have been the main multilingual tasks for most of the history of computational linguistics, today the recent awareness within the information access field of the multilingual reality of information sources has made the availability of lexica an all the more critical system component. Machine readable lexica in general, and machine readable multilingual lexica in particular, are difficult to come across. Manual approaches to lexicon construction vouch for high quality results, but are time- and labour-consuming to build, costly and complex to maintain, and inherently static as to their nature: tuning an existing lexicon to a new domain is a complex task that risks compromising existing information and corrupting usefulness for previous application areas. As a specific case, human-readable dictionaries, even if digitized and made available to automatic processing, are not vectored towards <b>automatic</b> processing. <b>Dictionaries</b> originally designed for human perusal leave much information unsaid, and belabor fine points that may not be of immediate us...|$|R
40|$|What is {{believed}} to be the first thesis ever written in the field of mechanical translation was presented in May to the Harvard University Division of Applied Science by Anthony G. Oettinger. The thesis, which earned Dr. Oettinger the degree of Ph. D. in Applied Mathematics, was entitled “A Study for the Design of an <b>Automatic</b> <b>Dictionary.</b> ” An abstract of the thesis is to be found on page 35 of this issue. BOOK W. N. Locke of M. I. T. and A. D. Booth of Birkbeck College, London, are co-editing a book of up to date essays on mechanical translation. There will be some dozen chapters written by nearly all of the active workers in the field. It is hoped that the book, to be published jointly by the Technology Press of M. I. T. and John Wiley & Sons, will be ready in the fall. to the trouble of completing it and publishing it in perhaps a year or two. The completed bibliography would probably run to some 50 pages. We wonder if our readers would be sufficiently interested to see this in a future issue of MT. ERWIN REIFLER, University of Washington, Seattle, Washington, read a paper on “MT, its psychological aspects and its significance for the human society ” before the SECOND CON...|$|E
40|$|Normal 0 14 MicrosoftInternetExplorer 4 /* Style Definitions */ table. MsoNormalTable 	{mso-style-name:"Tabella normale"; 	mso-tstyle-rowband-size: 0; 	mso-tstyle-colband-size: 0; 	mso-style-noshow:yes; 	mso-style-parent:""; 	mso-padding-alt: 0 cm 5. 4 pt 0 cm 5. 4 pt; 	mso-para-margin: 0 cm; 	mso-para-margin-bottom:. 0001 pt; 	mso-pagination:widow-orphan; 	font-size: 10. 0 pt; 	font-family:"Times New Roman";} Per gli apprendenti sinofoni la lingua italiana, con la sua complessità e fragilità morfologica, rappresenta  un ostacolo notevole. Il corso  pone attenzione alla lontananza tipologica delle due lingue con schede grammaticali in cinese e, soprattutto, i livelli A 1 e A 2 hanno una progressione molto controllata per fissare le basi della lingua italiana. Sono disponibili un Glossario, tre diversi tipi di dizionario (il dizionario automatico, il dizionario, visuale e il dizionario fraseologico) e approfondimenti culturali. Si tratta di un corso completo che consente di sviluppare le competenze previste  dal Quadro Comune Europeo; il modello formativo è flessible e prevede varie opzioni di erogazione.   Normal 0 14 MicrosoftInternetExplorer 4 /* Style Definitions */ table. MsoNormalTable 	{mso-style-name:"Tabella normale"; 	mso-tstyle-rowband-size: 0; 	mso-tstyle-colband-size: 0; 	mso-style-noshow:yes; 	mso-style-parent:""; 	mso-padding-alt: 0 cm 5. 4 pt 0 cm 5. 4 pt; 	mso-para-margin: 0 cm; 	mso-para-margin-bottom:. 0001 pt; 	mso-pagination:widow-orphan; 	font-size: 10. 0 pt; 	font-family:"Times New Roman";} For Chinese {{speakers}} Italian, {{in all its}} {{complexity and}} morphological fragility, represents a great obstacle.   This course takes into careful account {{the distance between the}} two language types, with grammar fact cards in Chinese and, above all, progression at levels A 1 and A 2 which is very carefully controlled in order to establish the bases of the Italian language.   A Glossary, three different types of dictionary (an <b>automatic</b> <b>dictionary,</b> a visual dictionary and a dictionary of phrases) and cultural details are available. This is a complete course which makes it possible to develop the competences foreseen by the Common European Framework; the teaching pattern is flexible and can be provided in different forms. <br /...|$|E
40|$|Machine Translation (MT) {{technology}} has been widely used in the localisation industry to boost the productivity of professional translators. However, due to the high quality of translation expected, the translation performance of an MT system in isolation is less than satisfactory due to various generated errors. This study focuses on translation of prepositions from English into Chinese within technical documents in an industrial localisation context. The aim {{of the study is}} to reveal the salient errors in the translation of prepositions and to explore possible methods to remedy these errors. This study proposes three new approaches to improve the translation of prepositions. All approaches attempt {{to make use of the}} strengths of the two most popular MT architectures at the moment: Rule-Based MT (RBMT) and Statistical MT (SMT). The approaches include: firstly building an <b>automatic</b> preposition <b>dictionary</b> for the RBMT system; secondly exploring and modifing the process of Statistical Post-Editing (SPE) and thirdly pre-processing the source texts to better suit the RBMT system. Overall evaluation results (both human evaluation and automatic evaluation) show the potential of our new approaches in improving the translation of prepositions. In addition, the current study also reveals a new function of automatic metrics in assisting researchers to obtain more valid or purpose-specific human valuation results. ...|$|R
40|$|Symbolic {{techniques}} {{are useful in}} very restricted subject domains that exhibit standard patterns: � e. g., mining of a weather report In most settings language is characterized by: � a variety of patterns that express the same or similar meanings � ambiguous patterns that receive their meaning based on the context Patterns change in time (e. g., blog and chat languages) © 2008 M. -F. Moens K. U. Leuven 2 Problem ⇒ manual effort is huge to build all the needed (contextual) patterns {{for all kinds of}} information extraction tasks � IE in terrorism domain: experiment of Riloff (1996) : <b>automatic</b> construction of <b>dictionary</b> of extraction patterns from an annotated training corpus achieved 98 % of the performance of handcrafted patterns => machine learning of extraction patterns [Riloff AI 1996...|$|R
40|$|Paper {{presented}} at International Conference on Recent Advances in Natural Language Processing 2015 (RANLP 2015) Definition Extraction (DE) {{is the task}} to extract textual definitions from naturally occurring text. It is gaining popularity as a prior step for constructing taxonomies, ontologies, <b>automatic</b> glossaries or <b>dictionary</b> entries. These fields of application motivate greater interest in well-formed encyclopedic text from which to extract definitions, and therefore DE for academic or lay discourse has received less attention. In this {{paper we propose a}} weakly supervised bootstrapping approach for identifying textual definitions with higher linguistic variability than the classic encyclopedic genus-et-differentia definition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reflect opposed ways of expressing definitional knowledge. This work is partially funded by the SKATER project, TIN 2012 - 38584 -C 06 - 03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP 7 -ICT- 2013. 8. 1 611383) ...|$|R
40|$|The <b>automatic</b> {{generation}} of <b>dictionaries</b> from raw text {{has previously been}} based on parallel or comparable corpora. Here we describe an approach requiring only a single monolingual corpus to generate bilingual dictionaries for several language pairs. A constraint is that all language pairs have their target language in common, which needs to be {{the language of the}} underlying corpus. Our approach is based on the observation that monolingual corpora usually contain a considerable number of foreign words. As these are often explained via translations typically occurring close by, we can identify these translations by looking at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. ...|$|R
