469|581|Public
5000|$|<b>Adaptive</b> <b>resonance</b> <b>theory,</b> a {{neural network}} {{architecture}} developed by Stephen Grossberg.|$|E
50|$|Fusion <b>adaptive</b> <b>resonance</b> <b>theory</b> (fusion ART) is a {{generalization}} of self-organizing neural networks known as <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> for learning recognition categories (or cognitive codes) across multiple pattern channels. It unifies {{a number of}} neural network models, supports several learning paradigms, notably unsupervised learning, supervised learning, and reinforcement learning, and can be applied for domain knowledge integration, memory representation, and modelling of high level cognition.|$|E
50|$|Fusion <b>adaptive</b> <b>resonance</b> <b>theory</b> {{models is}} a natural {{extension}} of the original <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART) models developed by Stephen Grossberg and Gail A. Carpenter from a single pattern field to multiple pattern channels. Whereas the original ART models perform unsupervised learning of recognition nodes in response to incoming input patterns, fusion ART learns multi-channel mappings simultaneously across multi-modal pattern channels in an online and incremental manner.|$|E
40|$|Principles {{derived from}} an {{analysis}} of experimental literatures in vision, speech, cortical development, and reinforcement learning, including attentional blocking and cognitive-emotional interactions, led {{to the introduction of}} <b>adaptive</b> <b>resonance</b> as a <b>theory</b> of human cognitive information processing (Grossberg, 1976). The theory has evolved as a series of real-time neura...|$|R
40|$|Neurological {{findings}} suggest that the human striate cortex (V 1) is an indispensable component of a neural substratum subserving static achromatic form perception in its own right and not simply as a central distributor of retinally derived information to extrastriate visual areas. This view is further supported by physiological evidence in primates that the finest-grained conjoined representation of spatial detail and retinotopic localization that underlies phenomenal visual experience for local brightness discriminations is selectively represented at cortical levels by the activity of certain neurons in V 1. However, at first glance, support for these ideas would appear to be undermined by incontrovertible neurological evidence (visual hemineglect and the simultanagnosias) and recent psychophysical results on 2 ̆ 7 crowding 2 ̆ 7 that confirm that activation of neurons in V 1 may, at times, be insufficient to generate a percept. Moreover, a recent proposal suggests that neural correlates of visual awareness must project directly to those in executive space, thus automatically excluding V 1 from a related perceptual space because V 1 lacks such direct projections. Both sets of concerns are, however, resolved within the context of <b>adaptive</b> <b>resonance</b> <b>theories.</b> Recursive loops, linking the dorsal lateral geniculate nucleus (LGN) through successive cortical visual areas to the temporal lobe by means of a series of ascending and descending pathways, provide a neuronal substratum at each level within a modular framework for mutually consistent descriptions of sensory data. At steady state, such networks obviate the necessity that neural correlates of visual experience project directly to those in executive space because a neural phenomenal perceptual space subserving form vision is continuously updated by information from an object recognition space equivalent to that destined to reach executive space. Within this framework, activity in V 1 may engender percepts that accompany figure-ground segregations only when dynamic incongruities are resolved both within and between ascending and descending streams. Synchronous neuronal activity on a short timescale within and across cortical areas, proposed and sometimes observed as perceptual correlates, may also serve as a marker that a steady state has been achieved, which, in turn, may be a requirement for the longer time constants that accompany the emergence and stability of perceptual states compared to the faster dynamics of adapting networks and the still faster dynamics of individual action potentials. Finally, the same consensus of neuronal activity across ascending and descending pathways linking multiple cortical areas that in anatomic sequence subserve phenomenal visual experiences and object recognition may underlie the normal unity of conscious experience...|$|R
40|$|<b>Adaptive</b> <b>resonance</b> is a <b>theory</b> of {{cognitive}} information processing {{which has been}} realized as a family of neural network models. In recent years, these models have evolved to incorporate new capabilities in the cognitive, neural, computational, and technological domains. Minimal models provide a conceptual framework, for formulating questions {{about the nature of}} cognition; an architectural framework, for mapping cognitive functions to cortical regions; a semantic framework, for precisely defining terms; and a computational framework, for testing hypotheses. These systems are here exemplified by the distributed ART (dART) model, which generalizes localist ART systems to allow arbitrarily distributed code representations, while retaining basic capabilities such as stable fast learning and scalability. Since each component is placed {{in the context of a}} unified real-time system, analysis can move from the level of neural processes, including learning laws and rules of synaptic transmission, to cognitive processes, including attention and consciousness. Local design is driven by global functional constraints, with each network synthesizing a dynamic balance of opposing tendencies. The self-contained working ART and dART models can also be transferred to technology, in areas that include remote sensing, sensor fusion, and content-addressable information retrieval from large databases. Office of Naval Research and the defense Advanced Research Projects Agency (N 00014 - 95 - 1 - 0409, N 00014 - 1 - 95 - 0657); National Institutes of Health (20 - 316 - 4304 - 5...|$|R
5000|$|... perceptual and {{cognitive}} development, social cognition, working memory, cognitive information processing, planning, numerical estimation, and attention: <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (ART), ARTMAP, STORE, CORT-X, SpaN, LIST PARSE, lisTELOS, SMART, CRIB; ...|$|E
50|$|<b>Adaptive</b> <b>resonance</b> <b>theory</b> (ART) is {{a theory}} {{developed}} by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.|$|E
50|$|LAPARTThe Laterally Primed <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (LAPART) neural {{networks}} couple two Fuzzy ART algorithms {{to create a}} mechanism for making predictions based on learned associations. The coupling of the two Fuzzy ARTs has a unique stability that allows the system to converge rapidly towards a clear solution. Additionally, it can perform logical inference and supervised learning similar to fuzzy ARTMAP.|$|E
40|$|Abstract—In this paper, an {{empirical}} {{study of the}} development and application of a committee of neural networks on online pattern classification tasks is presented. A multiple classifier framework is designed by adopting an <b>Adaptive</b> <b>Resonance</b> Theory-based (ART) autonomously learning neural network as the building block. A number of algorithms for combining outputs from multiple neural classifiers are considered, and two benchmark data sets {{have been used to}} evaluate the applicability of the proposed system. Different learning strategies coupling offline and online learning approaches, as well as different input pattern representation schemes, including the “ensemble ” and “modular ” methods, have been examined experimentally. Benefits and shortcomings of each approach are systematically analyzed and discussed. The results are comparable, and in some cases superior, with those from other classification algorithms. The experiments demonstrate the potentials of the proposed multiple neural network systems in offering an alternative to handle online pattern classification tasks in possibly nonstationary environments. Index Terms—Adaptive <b>Resonance</b> <b>Theory,</b> benchmark studies, decision combination algorithms, multiple neural network systems, online learning. I...|$|R
40|$|We can {{recognize}} objects through receiving continuously huge temporal information including redundancy and noise, and can memorize them. This paper proposes a {{neural network model}} which extracts pre-recognized patterns from temporally sequential patterns which include redundancy, and memorizes the patterns temporarily. This model consists of an <b>adaptive</b> <b>resonance</b> system and a recurrent time-delay network. The extraction is executed by the matching mechanism of the <b>adaptive</b> <b>resonance</b> system, and the temporal information is processed and stored by the recurrent network. Simple simulations are examined to exemplify the property of extraction. Matsushita Electric Industrial Co., Ltd., Tokyo Information Systems Research Laboratory, Tokyo, Japa...|$|R
40|$|This paper reports our {{evaluation}} of k Nearest Neighbor (kNN), Support Vector Machines (SVM), and <b>Adaptive</b> <b>Resonance</b> Associative Map (ARAM) on Chinese web page classification. Benchmark experiments {{based on a}} Chinese web corpus showed that their predictive performance were roughly comparable although ARAM and kNN slightly outperformed SVM in small categories. In additio...|$|R
5000|$|Fusion ART {{employs a}} {{multi-channel}} architecture (as shown below), comprising a category field [...] {{connected to a}} fixed number of (K) pattern channels or input fields [...] through bidirectional conditionable pathways. The model unifies a number of network designs, most notably <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (ART), Adaptive Resonance Associative Map (ARAM) and Fusion Architecture for Learning and COgNition (FALCON), developed over the past decades {{for a wide range}} of functions and applications.|$|E
50|$|Together with Stephen Grossberg {{and their}} {{students}} and colleagues, Gail has, since the 1980s, developed the <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART) family of neural networks for fast stable online learning, pattern recognition, and prediction, including both unsupervised (ART 1, ART 2, ART 2-A, ART 3, fuzzy ART, distributed ART) and supervised (ARTMAP, fuzzy ARTMAP, ART-EMAP, ARTMAP-IC, ARTMAP-FTR, distributed ARTMAP, default ARTMAP) systems. These ART models {{have been used for}} a wide range of applications, including remote sensing, medical diagnosis, automatic target recognition, mobile robots, and database management.|$|E
50|$|Massimiliano Versace's {{research}} interests {{are focused on}} neural networks - also called Deep Learning, in particular applied to cortical models of learning and memory, and how to build intelligent machines equipped with low-power, high density neural chips that implement large-scale brain circuits of increasing complexity. His Synchronous Matching <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (SMART) model shows spiking laminar cortical circuits self-organize and stably learn relevant information, and how these circuits be embedded in low-power, memristor based hybrid CMOS chip and used to solve challenging pattern recognition problems. His work has been featured on Fortune, Inc, Tech Crunch, IEEE Spectrum, Venture Beat, among others.|$|E
40|$|The {{correspondence}} between the forced magnetic reconnection induced by perturbing the boundary of the simple Taylor model and the surface-wave-induced magnetic reconnection given by Alfven <b>resonance</b> <b>theory</b> is pointed out explicitly by showing that the theory of forced magnetic reconnection is actually embedded in the Alfven <b>resonance</b> <b>theory.</b> The advantages of viewing the forced reconnection as surface-wave-induced reconnection are briefly discussed {{in the context of}} the formation of small-scale structures at the magnetospheric boundary and solar coronal heating...|$|R
40|$|Functional ond {{mechanistic}} {{comparisons are}} mode between several network models of cognitive processing: competitive learning, interactive activation, <b>adaptive</b> <b>resonance,</b> and back propagation. The {{starting point of}} this comparison is the article of Rumelhart ond Zipser (1985) on feature discovery through competitive learning. All the models which Rumelhart and Zipser (1985) have described were shown in Grossberg (1976 b) to exhibit a type of learning which is temporally unstable. Competitive learning mechanisms con be stabilized {{in response to an}} arbitrary input environment by being supplemented with mechanisms for learn-ing top-down expectancies, or templates; for matching bottom-up input patterns with the top-down expectancies; and for releasing orienting reactions in o mismatch situation, thereby updating short-term memory ond searching for another internal representation. Network architectures which embody all of these mechonisms were called <b>adaptive</b> <b>resonance</b> models by Grossberg (1976 ~). Self-stabilizing learning models are candidates for use in real-world applications where unpredictable changes can occur in complex input environments. Competitive learning postulates ore inconsistent with the postulates of the interactive activotion model of McClelland and Rumelhart (1981). and suggest different levels of processing and interaction rules for the analysis of word recognition. <b>Adaptive</b> <b>resonance</b> models use these alternative levels and interaction rules. The selforganizing learning of on odaptive resonance model is compared ond contrasted with the teacher-directed learning of a back propagation model. A number of criteria for evaluating reol-time network models of cognitive processing ore described and applied. 1...|$|R
40|$|Contrary to {{the claim}} by von Uexkuell et al. (1983), their {{observations}} of upward phase propagation of umbral oscillations in the chromosphere {{are in agreement}} with the photospheric <b>resonance</b> <b>theory</b> of Thomas and Scheuer (1982) and in contradiction to the chromospheric <b>resonance</b> <b>theory</b> of Zhugzhda et al. (1983). Other observational evidence also indicates that the fundamental 3 -min umbral oscillation is due to a photospheric resonance, although the closely-spaced multiple peaks sometimes seen in the power spectrum of chromospheric oscillations may well be due to chromospheric resonances...|$|R
5000|$|Among {{neural network}} models, the {{self-organizing}} map (SOM) and <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART) {{are commonly used}} unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between {{members of the same}} clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was [...] "ART1", developed by Carpenter and Grossberg (1988).|$|E
50|$|With Gail Carpenter, Grossberg {{developed}} the <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART). ART is a cognitive and neural theory {{of how the}} brain can quickly learn, and stably remember and recognize, objects and events in a changing world. ART proposed a solution of the stability-plasticity dilemma; namely, how a brain or machine can learn quickly about new objects and events without just as quickly being forced to forget previously learned, but still useful, memories. ART predicts how learned top-down expectations focus attention on expected combinations of features, leading to a synchronous resonance that can drive fast learning. ART also predicts how large enough mismatches between bottom-up feature patterns and top-down expectations can drive a memory search, or hypothesis testing, for recognition categories with which to better learn to classify the world. ART thus defines a type of self-organizing production system. ART was practically demonstrated through the ART family of classifiers (e.g., ART 1, ART 2, ART 2A, ART 3, ARTMAP, fuzzy ARTMAP, ART eMAP, distributed ARTMAP), developed with Gail Carpenter, which {{has been used in}} large-scale applications in engineering and technology where fast, yet stable, incrementally learned classification and prediction are needed.|$|E
50|$|As {{noted in}} the section on Education and Early Research, Grossberg has studied how brains give rise to minds since he took the {{introductory}} psychology course as a freshman at Dartmouth College in 1957. At that time, Grossberg introduced the paradigm of using nonlinear systems of differential equations to show how brain mechanisms can give rise to behavioral functions. This paradigm is helping to solve the classical mind/body problem, and is the basic mathematical formalism that is used in biological neural network research today. In particular, in 1957-1958, Grossberg discovered widely used equations for (1) short-term memory (STM), or neuronal activation (often called the Additive and Shunting models, or the Hopfield model after John Hopfield's 1984 application of the Additive model equation); (2) medium-term memory (MTM), or activity-dependent habituation (often called habituative transmitter gates, or depressing synapses after Larry Abbott's 1997 introduction of this term); and (3) long-term memory (LTM), or neuronal learning (often called gated steepest descent learning). One variant of these learning equations, called Instar Learning, was introduced by Grossberg in 1976 into <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> and Self-Organizing Maps for the learning of adaptive filters in these models. This learning equation was also used by Kohonen in his applications of Self-Organizing Maps starting in 1984. Another variant of these learning equations, called Outstar Learning, was used by Grossberg starting in 1967 for spatial pattern learning. Outstar and Instar learning were combined by Grossberg in 1976 in a three-layer network for the learning of multi-dimensional maps from any m-dimensional input space to any n-dimensional output space. This application was called Counter-propagation by Hecht-Nielsen in 1987.|$|E
40|$|In {{the present}} paper, the {{mathematical}} formulation of <b>resonance</b> <b>theory</b> is discussed. In fact, <b>resonance</b> <b>theory</b> is a representative {{form of the}} unitary group approach. Using the standard projection operator of the symmetric group, a new basis for an irreducible representation of the unitary group, called bonded tableau, can be constructed to describe a resonance structure correspondingly. The relationships between bonded tableau and Weyl tableaux and between valence bond and molecular orbital approaches are revealed. Finally, test calculations on ozone and benzene are performed...|$|R
40|$|The {{construction}} of a <b>resonance</b> <b>theory</b> involving hadrons requires implementing the information from higher scales into the couplings of the effective Lagrangian. We consider the large-Nc chiral <b>resonance</b> <b>theory</b> incorporating scalars and pseudoscalars, and we find that, by imposing LO short-distance constraints on form factors of QCD currents constructed within this theory, the chiral low-energy constants satisfy resonance saturation at NLO in the 1 /Nc expansion. Comment: 5 pages, 2 figures. Version published in Physical Review D. Some equations to facilitate the discussion have been adde...|$|R
40|$|Categorical {{perception}} {{is a well}} studied phenomenon in, for example, colour perception, phonetics and music. In this ar-ticle we implement a dynamical systems model of categorical rhythm perception based on the <b>resonance</b> <b>theory</b> of rhythm perception developed by Large (2010). This model is used to simulate the categorical choices of participants in two experi-ments of Desain and Honing (2003). The model is able to ac-curately replicate the experimental data. Our results supports that <b>resonance</b> <b>theory</b> is a viable model of rhythm perception and they show that by viewing rhythm perception as a dynami-cal system {{it is possible to}} model properties of categorical per-ception...|$|R
40|$|Supervised <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> is an {{extension}} of <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (ART) to perform incremental supervised learning of recognition categories (pattern classes) and multidimensional maps of both binary and analog patterns. Two classical examples of supervised ART systems are ARTMAP [3, 4] and its bidirectional compressed variant, known as the Adaptive Resonance Associative Map (ARAM) [15]...|$|E
40|$|In this brief, a new {{neural network}} model called {{generalized}} <b>adaptive</b> <b>resonance</b> <b>theory</b> (GART) is introduced. GART is a hybrid model that comprises a modified Gaussian <b>adaptive</b> <b>resonance</b> <b>theory</b> (MGA) and the generalized regression neural network (GRNN). It is an enhanced version of the GRNN, which preserves the online learning properties of <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART). A series of empirical studies to assess the effectiveness of GART in classification, regression, and time series prediction tasks is conducted. The results demonstrate that GART is able to produce good performances as {{compared with those of}} other methods, including the online sequential extreme learning machine (OSELM) and sequential learning radial basis function (RBF) {{neural network model}}s. <br /...|$|E
40|$|Clustering is an {{important}} function in data mining. Its typical application includes the analysis of consumer’s materials. <b>Adaptive</b> <b>resonance</b> <b>theory</b> network (ART) is very popular in the unsupervised neural network. Type I <b>adaptive</b> <b>resonance</b> <b>theory</b> network (ART 1) deals with the binary numerical data, whereas type II <b>adaptive</b> <b>resonance</b> <b>theory</b> network (ART 2) deals with the general numerical data. Several information systems collect the mixing type attitudes, which included numeric attributes and categorical attributes. However, ART 1 and ART 2 do not deal with mixed data. If the categorical data attributes are transferred to the binary data format, the binary data {{do not reflect the}} similar degree. It influences the clustering quality. Therefore, this paper proposes a modified <b>adaptive</b> <b>resonance</b> <b>theory</b> network (M-ART) and the conceptual hierarchy tree to solve similar degrees of mixed data. This paper utilizes artificial simulation materials and collects a piece of actual data about the family income to do experiments. The results show that the M-ART algorithm can process the mixed data and has a great effect on clustering...|$|E
30|$|Previous {{researches}} {{have been}} carried out on the theories of low frequency oscillation, including the negative damping theory [4], the forced power oscillation theory [5, 6] and the parametric <b>resonance</b> <b>theory</b> [7], etc.|$|R
40|$|Categorization of rhythmic {{patterns}} is prevalent in musical practice, {{an example of}} this being the transcription of (possibly not strictly metrical) music into musical notation. In this article we implement a dynamical systems' model of rhythm categorization based on the <b>resonance</b> <b>theory</b> of rhythm perception developed by Large (2010). This model is used to simulate the categorical choices of participants in two experiments of Desain and Honing (2003). The model accurately replicates the experimental data. Our results support <b>resonance</b> <b>theory</b> as a viable model of rhythm perception and show that by viewing rhythm perception as a dynamical system it is possible to model central properties of rhythm categorization...|$|R
3000|$|... b, Kilpatrick 2015; Kohonen 2001 b; Sandstede 2007; Troy 2008 a,b; Werning 2012 a). Thus, {{the neural}} {{information}} storage and retrieval {{in the long-term}} memory, for example, can be understand by means of computational <b>adaptive</b> <b>resonance</b> mechanisms in the dominant waveforms, or “modes” (Grossberg and Somers 1991), and by warming up and annealing of oscillation modes by streams of informational processes in the context of computational “energy functions,” like in “Harmony Theory” (Smolensky and Legendre 2006 a).|$|R
40|$|<b>Adaptive</b> <b>Resonance</b> <b>Theory</b> {{provides}} {{a model of}} pattern classification that addresses the plasticity stability dilemma and allows a neural network to detect when to construct a new category without {{the assistance of a}} supervisor. We show that <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> {{can be applied to the}} study of natural concept development. Specifically, a model is presented which is able to categorise different usages of a common noun and group the polysemous senses appropriately...|$|E
40|$|Abstract—In this brief, a new {{neural network}} model called {{generalized}} <b>adaptive</b> <b>resonance</b> <b>theory</b> (GART) is introduced. GART is a hybrid model that comprises a modified Gaussian <b>adaptive</b> <b>resonance</b> <b>theory</b> (MGA) and the generalized regression neural network (GRNN). It is an enhanced version of the GRNN, which preserves the online learning properties of <b>adaptive</b> <b>resonance</b> <b>theory</b> (ART). A series of empirical studies to assess the effectiveness of GART in classification, regression, and time series prediction tasks is conducted. The results demonstrate that GART is able to produce good performances as {{compared with those of}} other methods, including the online sequential extreme learning machine (OSELM) and sequential learning radial basis function (RBF) {{neural network model}}s. Index Terms—Adaptive resonance theory (ART), Bayesian theorem, generalized regression neural network (GRNN), online sequential extreme learning machine. I...|$|E
40|$|We discuss {{implementations}} of the <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (ART) on {{a serial}} machine. The standard formulation of ART, which {{was inspired by}} recurrent brain structures, corresponds to a recursive algorithm. This induces an algorithmic complexity of order O(N 2) +O(MN) in worst and average case, N being the number of categories, and M the input dimension. It is possible, however, to formulate ART in a non-recursive algorithm such that the complexity is of order O(MN) only. Keywords: fuzzy systems, neural networks, unsupervised learning, <b>adaptive</b> <b>resonance</b> <b>theory,</b> algorithmic complexity 1 Introduction The <b>Adaptive</b> <b>Resonance</b> <b>Theory</b> (ART) [1] is an outstanding example of how designing artificially intelligent systems and understanding the brain may benefit from each other. ART is inspired by the recurrent structure of information processing in the cortex and deeper lying structures (see [2] for the visual cortex as an example). The biological significance of recurrent loops in the b [...] ...|$|E
40|$|A novel {{methodology}} for the fault diagnosis of rolling bearing in strong background noise, based on sensitive intrinsic mode functions (IMFs) selection of ensemble empirical mode decomposition (EEMD) and <b>adaptive</b> stochastic <b>resonance,</b> is proposed. The original vibration signal is decomposed {{into a group}} of IMFs and a residual trend item by EEMD. Constructing weighted kurtosis index difference spectrum (WKIDS) to adaptively select sensitive IMFs, this method can overcome the shortcomings of the existing methods such as subjective choice or need to determine a threshold using the correlation coefficient. To further reduce noise and enhance weak characteristics, the <b>adaptive</b> stochastic <b>resonance</b> is employed to amplify each sensitive IMF. Then, the ensemble average is used to eliminate the stochastic noise. The simulation and rolling element bearing experiment with an inner fault are performed to validate the proposed method. The {{results show that the}} proposed method not only overcomes the difficulty of choosing sensitive IMFs, but also, combined with <b>adaptive</b> stochastic <b>resonance,</b> can better enhance the weak fault characteristics. Moreover, the proposed method is better than EEMD and <b>adaptive</b> stochastic <b>resonance</b> of each sensitive IMF, demonstrating the feasibility of the proposed method in highly noisy environments...|$|R
40|$|The large {{cross section}} of the {{reaction}} H 2 + H 3 → He 4 + n + 17. 6 Mev makes it interesting both from the standpoint of its use as a neutron source and because the magnitude of the cross section is essentially the maximum possible predicted by nuclear <b>resonance</b> <b>theory.</b> The published data indicates a maximum cross section at around 200 kev triton energy. Allen and Poole give the maximum cross section as 5. 0 barns. The purpose of the present work was to make accurate measurements of the cross section over a wide range of particle energies and to compare results with predictions of a single level <b>resonance</b> <b>theory...</b>|$|R
40|$|PTHThe {{relation}} between the quark–gluon description of QCD and the hadronic picture is studied up to order αs. The analysis of the spin- 1 correlators is developed within the large NC framework. Both representations are shown to be equivalent in the Euclidean domain, where the operator product expansion is valid. By considering different models for the hadronic spectrum at high energies, one is able to recover the αs running in the correlators, to fix the ρ(770) and a 1 (1260) couplings, and to produce a prediction for {{the values of the}} condensates. The operator product expansion is improved by the large NC <b>resonance</b> <b>theory,</b> extending its range of validity. Dispersion relations are employed in order to study the Minkowskian region and some convenient sum rules, specially sensitive to the resonance structure of QCD, are worked out. A first experimental estimate of these sum rules allows a cross-check of former determinations of the QCD parameters and helps to discern and to discard some of the considered hadronical models. Finally, the truncated <b>resonance</b> <b>theory</b> and the minimal hadronical approximation arise as a natural approach to the full <b>resonance</b> <b>theory,</b> not as a model...|$|R
