15|10000|Public
50|$|The biggest {{challenge}} in measuring customer profitability is the <b>assignment</b> <b>of</b> <b>costs</b> to customers. While {{it is usually}} clear what revenue each customer generated, it is often not clear at all what costs the firm incurred serving each customer. Activity Based Costing can sometimes {{be used to help}} determine the costs associated with each customer or customer group. For components of cost not directly related to serving customers, the calculation of customer profit must use some method to fully allocate these costs to customers if the total of customer profit is to match the operating profit of the firm. If the firm decides not to allocate these non-customer costs to customers, then the sum of customer profit will be greater than the operating profit of the firm.|$|E
40|$|There are {{frequently}} problems in arriving at an equitable <b>assignment</b> <b>of</b> <b>costs</b> to consumers and producers of utilities. A more equitable <b>assignment</b> <b>of</b> <b>costs</b> {{will lead to}} better operating decisions which improve overall profitability. This paper discusses some of the concepts and considerations which should be addressed and proposes possible alternatives {{to move in the}} direction of more suitable steam and power values...|$|E
40|$|Abstract: We {{present a}} semi-symbolic {{algorithm}} for synthesizing efficient controllers in a stochastic environment, implemented as an add-on to the probabilistic model checker PRISM. The user specifies {{the environment and}} the controllable actions using a Markov Decision Process (MDP), modeled in the PRISM language. Controller efficiency is defined with respect to a user-specified <b>assignment</b> <b>of</b> <b>costs</b> and rewards to the controllable actions. An optimally efficient strategy minimizes the ratio between the encountered costs and rewards. At the core of the implementation is the first semi-symbolic algorithm based on a recently developed strategy improvement algorithm for MDPs with ratio objectives. We show the effectiveness of our implementation using a set of benchmarks. ...|$|E
50|$|The {{structure}} <b>of</b> <b>Cost</b> Accounting Standard {{consists of}} Introduction, Objectives of issuing standards, Scope of standard, Definitions and {{explanations of the}} terms used in the standard, Principles <b>of</b> Measurement, <b>Assignment</b> <b>of</b> <b>Cost,</b> Presentation and Disclosure.|$|R
5000|$|... #Subtitle level 2: Identification and <b>assignment</b> <b>of</b> F&A <b>costs</b> ...|$|R
5000|$|... 1962. [...] "Incentives, Decentralized Control, the <b>Assignment</b> <b>of</b> Joint <b>Costs</b> and Internal Pricing," [...] Management Science, 8(3), pp. 325-343 ...|$|R
40|$|In Bayesian {{hypothesis}} testing, {{a decision}} is made based on a prior probability dis-tribution over the hypotheses, an observation with a known conditional distribution given the true hypothesis, and an <b>assignment</b> <b>of</b> <b>costs</b> to different types of errors. In a setting with multiple agents and the principle of “one person, one vote”, the decisions of agents are typically combined by the majority rule. This thesis considers collections of group hypothesis testing problems over which the prior itself varies. Motivated by constraints on memory or computational resources of the agents, quantization of the prior probabilities is introduced, leading to novel analysis and design problems. Two hypotheses and three agents are sufficient to reveal various intricacies o...|$|E
40|$|From {{what was}} once a pure public good, the {{internet}} now comprises three key dimensions in the global economy: public sector database communications systems, public and private electronic mail services, and electronic commerce. While public sector database communications systems still affect some key sectors of the internet, electronic mail and electronic commerce drive much of the product innovation now taking place. As a quasi-public good, the internet raises fundamental policy questions, notably the <b>assignment</b> <b>of</b> <b>costs</b> in product development, the pricing of internet services by sector users, and the selection of optimal financing modes for its operation and expansion. In this paper, we examine key economic dimensions {{in the evolution of the}} internet within a framework for the optimal pricing of internet services. Optimal Pricing of Internet Service...|$|E
40|$|We {{define a}} general method for ranking the {{solutions}} of a search process by associating costs with equivalence classes of state transitions of the process. We {{show how the}} method accommodates models based on probabilistic, discriminative, and distance cost functions, including <b>assignment</b> <b>of</b> <b>costs</b> to unseen events. By applying the method to our machine translation prototype, {{we are able to}} experiment with different cost functions and training procedures, including an unsupervised procedure for training the numerical parameters of our English-Chinese translation model. Results from these experiments show that the choice of cost function leads to significant differences in translation quality. 1 INTRODUCTION Standard applications of preference scores in machine translation [3, 4, 15], and more generally for disambiguation in language processing [8, 14], employ special-purpose training and scoring mechanisms. To gain more flexibility in assigning costs to translations produced by a speec [...] ...|$|E
50|$|The Institute <b>of</b> <b>Cost</b> Accountants <b>of</b> India (ICAI), {{recognizing}} {{the need for}} structured approach to the measurement <b>of</b> <b>cost</b> in manufacture or service sector and to provide guidance to the user organizations, government bodies, regulators, research agencies and academic institutions to achieve uniformity and consistency in classification, measurement and <b>assignment</b> <b>of</b> <b>cost</b> to product and services, has constituted Cost Accounting Standards Board (CASB) in the year 2001 with the objective <b>of</b> formulating the <b>Cost</b> Accounting Standards (CASs).|$|R
5000|$|Cost-type {{accounting}} separates costs like labor, materials, and depreciation, {{followed by}} each cost account then being {{broken down into}} fixed and proportional costs along with the <b>assignment</b> <b>of</b> these <b>cost</b> accounts to cost centers.|$|R
5000|$|All local search {{algorithms}} use {{a function}} that evaluates the quality <b>of</b> <b>assignment,</b> {{for example the}} number of constraints violated by the assignment. This amount is called the <b>cost</b> <b>of</b> the <b>assignment.</b> The aim <b>of</b> local search is that <b>of</b> finding an <b>assignment</b> <b>of</b> minimal <b>cost,</b> which is a solution if any exists.|$|R
40|$|Increased foreign competition, automation, and {{deregulation}} {{have caused}} American companies {{to question the}} efficiency of their cost accounting systems. The Cost accounting topic receiving the most attention is allocation of overhead to products. This paper and case analysis examine several methods of improving the accuracy of overhead allocation. First, the effect of changing the number of cost centers from 1 to 2 and then to 31 was examined. Second, the effect of using "activity" overhead bases rather than direct labor hours was investigated. Third, the <b>assignment</b> <b>of</b> <b>costs</b> to various cost centers was studied. In all three cases, the use of sophisticated cost allocation procedures resulted in substantially different product costs. If companies continue to use the outdated, traditional techniques, they risk discontinuing production of profitable products and selling new products that are unprofitable. B. S. (Bachelor of Science...|$|E
40|$|This article {{presents}} a literature {{review of the}} method Time Driven Activity Based Costing, like an instrument to better <b>assignment</b> <b>of</b> <b>costs</b> to activities and their comparison with antecedent method Activity Based Costing. Paper shows the implementation of this method in the condition of manufacturing corporations, distribution centres, agriculture, {{but also in the}} field of services, especially in the hospitality. The article is trying to point out the benefits of this method for whole range of companies without difference to branch classification, determine base presumptions for implementation, but also disclose some drawbacks in the application of this new method in the practice with help of case studies, which have been published until this time. The aim of paper is to find out the base principles of method Time Driven Activity Based Costing in its right application. Activity Based Costing, Time Driven Activity Based Costing, time equations, customer profitability analysis, costs of processes...|$|E
40|$|This thesis first {{critically}} analyzes John Rawls?s second {{principle of}} justice {{as a democratic}} conception of equality and the challenge posed to that conception by Ronald Dworkin?s 'Equality of Resources. ' Democratic equality is defended over luck egalitarianism as an articulation of liberal egalitarianism. However, where Rawls deems social primary goods to be unconditionally regulated by institutions, Rawls is largely silent about the fair <b>assignment</b> <b>of</b> <b>costs</b> and burdens that correspond to the fair provision of opportunities and primary goods. Dworkin?s notion of 'opportunity costs' is argued to improve {{on the role of}} responsibility in democratic egalitarianism by making clear that the provision of primary goods creates costs and burdens within a system of social cooperation. The second section illustrates this argument by considering claims to self-government by Canadian Aboriginals. By formulating a distributive criterion that treats Aboriginal self-government as a primary good, I show that claims of culture and identity can be resolved responsibly within the framework of distributive justice...|$|E
30|$|The {{traditional}} APP {{models have}} often been studied for analyzing the production planning of one production plant. This paper presented a new mathematical programming model for APP problems of multiple cooperating plants. We quantified the cost-saving opportunity of the cooperation of plants caused by decreases in inventory and workforce levels. It {{was found that the}} job security and satisfaction of workers can be dramatically raised because of plants’ cooperation. Several methods of CGT including Shapley value, τ-value, the least core and equal cost saving methods were utilized for <b>assignment</b> <b>of</b> <b>cost</b> saving to cooperating plants. We found that fair allocation <b>of</b> cooperation <b>cost</b> saving can ensure the production plants satisfaction.|$|R
40|$|The Weighted Constraint Satisfaction Problem (WCSP) is a {{well known}} soft {{constraint}} framework for modeling over-constrained problems with practical applications in domains such as resource allocation, combinatorial auctions and bioinformatics. WCSP is an optimization version of the CSP framework in which constraints are extended by associating costs to tuples. Solving a WCSP instance, which is NP-hard, consists of finding a complete <b>assignment</b> <b>of</b> minimal <b>cost...</b>|$|R
40|$|Two of {{the basic}} methods <b>of</b> traffic <b>{{assignment}}</b> being static <b>assignment</b> <b>of</b> link <b>costs</b> and a dynamic assignment based on microscopic traffic simulation results are combined to derive a good starting solution for the more precise microscopic approach from the coarse macroscopic solution. In addition a new tool from the SUMO suite and some first results of applying the schema {{to the city of}} Berlin are presented...|$|R
40|$|I hereby {{declare that}} I am the sole {{author of this}} thesis. This is a true copy of the thesis, {{including}} any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii. This thesis first critically analyzes John Rawls’s second principle of justice as a democratic conception of equality and the challenge posed to that conception by Ronald Dworkin’s ‘Equality of Resources. ’ Democratic equality is defended over luck egalitarianism as an articulation of liberal egalitarianism. However, where Rawls deems social primary goods to be unconditionally regulated by institutions, Rawls is largely silent about the fair <b>assignment</b> <b>of</b> <b>costs</b> and burdens that correspond to the fair provision of opportunities and primary goods. Dworkin’s notion of ’opportunity costs ’ is argued to improve {{on the role of}} responsibility in democratic egalitarianism by making clear that the provision of primary goods creates costs and burdens within a system of social cooperation. The second section illustrates this argument by considering claims to selfgovernmen...|$|E
40|$|Activity-Based Costing (ABC) {{started out}} as a vehicle for (i) {{improved}} product costing for use in pricing, switched later to (ii) profit priorities using hierarchies of cost assignment, and now focuses on (iii) accounting for capacity constraints situations. Firstly, the paper demonstrates that the data requirements for these three different uses of ABC can be met by recording the following characteristics of resource utilization in a relational database: quantitative -non-monetary -utilization in the form of 'type of production factor', 'organizational unit' and the immediate 'objective for the use of resources'; discharge horizon, absolute and relative divisibility of production factors employed; and <b>assignment</b> <b>of</b> <b>costs</b> to classification objects, observing principles of non-arbitrariness. These features are utilized in 'Variability Accounting'. Secondly, the paper argues that, in complying with these principles, the evaluation of improvements in cost-accounting systems is on much more solid ground than if system changes are evaluated in terms of the resulting conse-quences for full-cost product costs. Based on these findings, the paper concludes that variability accounting can serve as a source of inspiration for constructors of ABC systems when the various versions are merged into an integrated cost system based on intercon-nected databases. It is also concluded that those variability accounting users with complex sales/distribution and production structures may find inspiration in ABC to identify areas in need of improved behaviour, although more research is required in this area. ...|$|E
40|$|Abstract We {{discuss a}} graph-algorithmic {{approach}} to comparing shapes. We focus {{in this paper}} on comparing simple closedcurves in the plane. Our approach is to (1) represent such a shape by its skeleton, which is a tree embedded in the plane,and (2) compare two shapes by comparing their skeletons via tree edit-distance. In this paper, we define our version of tree edit-distance (it differs from that previously described in the literature), and give a polynomial-time algorithm to compute the distance between two trees. 1 Introduction This paper arose out of a collaboration between a computer-vision researcher and an algorithms researcher. Kimia et al. [4] had previously compared shapes by comparing their graphs using a heuristic for general graph-comparison. The heuristic, due to Gold and Rangarajan [3], is based on finding a local minimum to a quadratic program. This approach had several disadvantages, however, and Kimia was searching for another approach. Klein suggested {{that the notion of}} edit-distance might be appropriate. The notion of edit-distance originated in a paper by Wagner and Fischer [10] on comparing two character strings. There are three kinds of edit operations: deleting a character, inserting a character, and changing one character into another. For a given <b>assignment</b> <b>of</b> <b>costs</b> to all such edit operations, one can use dynamic programming to compute the minimum-cost sequence of operations required to convert one given string to another...|$|E
40|$|A {{major problem}} in trying to assess cost {{accuracy}} lies in the understanding and “proper” <b>assignment</b> <b>of</b> overhead <b>costs.</b> Activity-Based Costing (ABC) has been offered {{as a means to}} address the inaccuracies generated by traditional costing systems, particularly the problem <b>of</b> “cross-subsidization” <b>of</b> <b>costs.</b> Much <b>of</b> the research on ABC has focused on the for-profit sector, primarily in manufacturing. More recently, services have received attention with respect to ABC analysis. We provide an ABC analysis of a nonprofit/governmental service organization, specifically the Hong Kong Housing Authority (HKHA) ...|$|R
30|$|First, {{the usual}} <b>assignment</b> <b>of</b> setup <b>costs</b> and times to {{products}} does not realistically reflect the changeover processes prevalent in advanced manufacturing technology. In {{a great number}} of industrial settings, we observed that setup conditions are related to the processing mode of the production equipment rather than to individual product types. Hence, the common <b>assignment</b> <b>of</b> setup <b>costs</b> and times to individual products appears to be questionable since setup costs are often caused by changing the basic processing mode and not for switching between different product types. As an example, consider the bottling of beverages (see the case-based example in Sect. 5.1) where stretch blow-moulding machines are set up for a specific type of plastic bottles by mounting the required moulds into the processing head of the machine. Once the machine is set up for a specific type of bottle, a variety of beverages can be bottled with only a minor changeover between the different product types. Therefore, the definition of lot sizes should primarily refer to the retention of a basic setup condition of the production equipment instead to the production quantity of an individual item.|$|R
40|$|We {{have found}} that some {{messages}} of BnB-ADOPT are redundant. Removing most of those redundant mes-sages we obtain BnB-ADOPT+, which achieves the optimal solution and terminates. In practice, BnB-ADOPT+ causes substantial reductions on communi-cation costs {{with respect to the}} original algorithm. BnB-ADOPT (Yeoh, Felner, and Koenig 2008) is a reference algorithm for distributed constraint optimization (DCOP), defined as follows. There is a finite number of agents, each holding one variable that can take values from a finite and discrete domain, related by binary cost functions. The <b>cost</b> <b>of</b> a variable assigning a value is the sum <b>of</b> <b>cost</b> functions evaluated on that assignment. The goal is to find a complete <b>assignment</b> <b>of</b> minimum <b>cost</b> by message passin...|$|R
40|$|The term dray {{dates back}} to the 14 th century when it was used {{commonly}} to describe a type of very sturdy sideless cart. In the 1700 s the word drayage came into use meaning “to transport by a sideless cart”. Today, drayage commonly refers to the transport of containerized cargo to and from port or rail terminals and inland locations. With the phenomenal growth of containerized freight since the container’s introduction in 1956, the drayage industry has also experienced significant growth. In fact, according to the Bureau for Transportation Statistics, the world saw total maritime container traffic grow to approximately 417 million twenty foot equivalent units (TEUs) in 2006. Unfortunately, the drayage portion of a door-to-door container move tends to be the most costly part of the move. There are a variety of reasons for this disproportionate <b>assignment</b> <b>of</b> <b>costs,</b> including a great deal of uncertainty at the interface of modes. For example, trucks moving containers to and from a port terminal are often uncertain as to {{how long it will take}} them to pick up a designated container coming from a ship, from the terminal stack, or from customs. This uncertainty leads to much difficulty and inefficiency in planning a profitable routing for multiple containers in one day. We study this problem from three perspectives using both empirical and theoretical techniques...|$|E
40|$|In {{this paper}} we {{consider}} a layered-security {{model in which}} the containers and their nestings are given {{in the form of}} a rooted tree T. A cyber-security model is an ordered three-tuple M = (T, C, P) where C and P are multisets of penetration costs for the containers and target-acquisition values for the prizes that are located within the containers, respectively, both of the same cardinality as the set of the non-root vertices of T. The problem that we study is to assign the penetration costs to the edges and the target-acquisition values to the vertices of the tree T in such a way that minimizes the total prize that an attacker can acquire given a limited budget. For a given <b>assignment</b> <b>of</b> <b>costs</b> and target values we obtain a security system, and we discuss three types of them: improved, good, and optimal. We show that in general it is not possible to develop an optimal security system for a given cyber-security model M. We define P- and C-models where the penetration costs and prizes, respectively, all have unit value. We show that if T is a rooted tree such that any P- or C-model M = (T,C,P) has an optimal security system, then T is one of the following types: (i) a rooted path, (ii) a rooted star, (iii) a rooted 3 -caterpillar, or (iv) a rooted 4 -spider. Conversely, if T is one of these four types of trees, then we show that any P- or C-model M = (T,C,P) does have an optimal security system. Finally, we study a duality between P- and C-models that allows us to translate results for P-models into corresponding results for C-models and vice versa. The results obtained give us some mathematical insights into how layered-security defenses should be organized. Comment: 27 pages, 3 figure...|$|E
40|$|In Bayesian {{hypothesis}} testing, {{a decision}} is made based on a prior probability distribution over the hypotheses, an observation with a known conditional distribution given the true hypothesis, and an <b>assignment</b> <b>of</b> <b>costs</b> to different types of errors. In a setting with multiple agents and the principle of "one person, one vote", the decisions of agents are typically combined by the majority rule. This thesis considers collections of group hypothesis testing problems over which the prior itself varies. Motivated by constraints on memory or computational resources of the agents, quantization of the prior probabilities is introduced, leading to novel analysis and design problems. Two hypotheses and three agents are sufficient to reveal various intricacies of the setting. This could arise {{with a team of}} three referees deciding by majority rule on whether a foul was committed. The referees face a collection of problems with different prior probabilities, varying by player. This scenario illustrates that even as all referees share the goal of making correct foul calls, opinions on the relative importance of missed detections and false alarms can vary. Whether cost functions are identical and whether referees use identical quantizers create variants of the problem. When referees are identical in both their cost functions and their quantizers for the prior probabilities, it is optimal for the referees to use the same decision rules. The homogeneity of the referees simplifies the problem to an equivalent single-referee problem with a lower-variance effective noise. Then the quantizer optimization problem is reduced to a problem previously solved by Varshney and Varshney (2008). Centroid and nearest-neighbor conditions that are necessary for quantizer optimality are provided. On the contrary, the problem becomes complicated when variations in cost functions or quantizers are allowed. In this case, decision-making and quantization problems create strategic form games; the decision-making game does always have a Nash equilibrium. The analysis shows that conflict between referees, in the form of variation in cost functions, makes overall team performance worse. Two ways to optimize quantizers are introduced and compared to each other. In the setting that referees purely collaborate, in the form of having equal cost functions, the effect of variations between their quantizers is analyzed. It is shown that the referees have incentive to use different quantizers rather than identical quantizers even though their cost functions are identical. In conclusion, a diverse team with a common goal performs best. by Joong Bum Rhim. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 85 - 87) ...|$|E
40|$|The present tesina is orientated {{to design}} a System <b>of</b> <b>Costing</b> based on {{activities}} for the Lubricating one and Riveter " Tarquino JR ", dedicated to the sale of supplies and to offer diverse services of maintenance {{for all kinds of}} vehicle. Our principal lens is to provide the owners of the company a System <b>of</b> <b>Costs</b> based on activities, which will allow them to know their structure <b>of</b> <b>costs,</b> mainly on with the <b>assignment</b> <b>of</b> indirect <b>costs</b> to each <b>of</b> the activities that intervene in the different services that the company offers. What is expected with the development of this is system with the good utilization of the ABC tool, to increase the profitability, which {{can be used as a}} management tool in the decisions making process principally in the determination of a fair and competitive price for one of their services...|$|R
40|$|A {{development}} of the in plane open loop rotational equations of motion for the proposed Spacecraft Control Laboratory Experiment (SCOLE) in orbit configuration is presented based on an Eulerian formulation. The mast {{is considered to be}} a flexible beam connected to the (rigid) shuttle and the reflector. Frequencies and mode shapes are obtained for the mast vibrational appendage modes (assumed to be decoupled) for different boundary conditions based on continuum approaches and also preliminary results are obtained using a finite element representation of the mast reflector system. The linearized rotational in plane equation is characterized by periodic coefficients and open loop system stability can be examined with an application of the Floquet theorem. Numerical results are presented to illustrate the potential instability associated with actuator time delays even for delays which represent only a small fraction of the natural period of oscillation of the modes contained in the open loop model of the system. When plant and measurement noise effects are added to the previously designed deterministic model of the hoop column system, it is seen that both the system transient and steady state performance are degraded. Mission requirements can be satisfied by appropriate <b>assignment</b> <b>of</b> <b>cost</b> function weighting elements and changes in the ratio of plant noise to measurement noise...|$|R
40|$|AbstractIn K(n,n) with edges colored either red or blue, we {{show that}} the problem of finding a {{solution}} matching, a perfect matching consisting of exactly r red edges, and (n−r) blue edges for specified 0 ⩽r⩽n, is a nontrivial integer program. We present an alternative, logically simpler proof of a theorem in (Kibernetika 1 (1987) 7 – 11) which establishes necessary and sufficient conditions for the existance of a solution matching, and a new O(n 2. 5) algorithm. This shows that the problem <b>of</b> finding an <b>assignment</b> <b>of</b> specified <b>cost</b> r in an assignment problem on the complete bipartite graph with a 0 − 1 cost matrix is efficiently solvable...|$|R
40|$|Simple {{topological data}} {{structures}} consisting of links and nodes {{have often been}} used for modeling flows through a network because of their processing efficiency and ease of implementation. These representations of networks are typically assumed to be flat and metrical; links have the same <b>cost</b> <b>of</b> travel in either direction and no costs are associated with traversing nodes. This paper describes three additions to the common model that retain its advantages while offering a capability for much more realistic analysis of network flows. The first enhancement of the simple network model is the <b>assignment</b> <b>of</b> separate <b>costs</b> for traversing a link in either direction. This simple modification allows consideration of physical constraints such as slope and temporal contraints such as traffic. Second, costs are associated with each link-to-link turn. This allows consideration of the delays experienced at network nodes from congestion and controls, and permits impossible turns (e. g. from an overpass) {{to be removed from}} paths of flow. Finally, it is shown through examples that integration of an enhanced network model with a relational database management system simplifies the <b>assignment</b> <b>of</b> appropriate travel <b>costs</b> enough that a single network can support realistic analyses that consider the unique characteristics of various types of links and nodes and flows across them in various congestion and control scenarios...|$|R
40|$|This paper {{describes}} an algorithm for computing the optical flow field between two consecutive frames. The algorithm {{takes advantage of}} image segmentation to overcome inherent problems of conventional optical flow algorithms, which are the handling of untextured regions and the estimation of correct flow vectors near motion discontinuities. Each segment’s motion is described by the affine motion model. Initial motion segments are clustered to derive a set of robust layers. The <b>assignment</b> <b>of</b> segments to layers is then improved by optimization <b>of</b> a global <b>cost</b> function that measures {{the quality of a}} solution via image warping. Occlusions in both views are detected and handled in the warping procedure. Furthermore, the cost function aims at generating smooth optical flow fields. Since finding the <b>assignment</b> <b>of</b> minimum <b>costs</b> is N P-complete, an efficient greedy algorithm searches a local optimum. Good quality results are achieved at moderate computational expenses. ...|$|R
5000|$|A {{drawback}} of {{hill climbing}} with moves {{that do not}} decrease cost, {{is that it may}} cycle over <b>assignments</b> <b>of</b> the same <b>cost.</b> Tabu search overcomes this problem by maintaining a list of [...] "forbidden" [...] assignments, called the tabu list. In particular, the tabu list typically contains the list of the last changes. More precisely, it contains the last variable/value such that the variable has been recently assigned to the value.|$|R
40|$|The Quadrennial Needs Study was {{developed}} {{to assist in the}} identification of highway needs and the distribution of road funds in Iowa among the various highway entities. During the period 1978 to 1990, the process has seen large shifts in needs and associated funding distribution in individual counties with no apparent reasons. This study investigated the reasons for such shifts. The study identified program inputs that can result in major shifts in needs either up or down from minor changes in the input values. The areas of concern were identified as the condition ratings for roads and structures, traffic volume and mix counts, and the <b>assignment</b> <b>of</b> construction <b>cost</b> areas. Eight counties exhibiting the large shifts (greater than 30...|$|R
40|$|An {{assignment}} {{is a perfect}} matching on a bipartite graph. An algorithm is given that outputs the K smallest <b>cost</b> <b>assignments</b> in order <b>of</b> increasing <b>cost.</b> The time is 0 (K min(V^ 3, VE log V)) and the space is 0 (E+K), where V and E are the number of vertices and edges. This compares favorably to a previous algorithm with runtime 0 (KV’). The speed-up is achieved by using a shortest path calculation to generate one optimal assignment from another. Two special cases of the problem are also discussed: finding all assignments in order, and finding all minimum cost assignments. The latter is done in 0 (min(V^ 3,VE log V) +ME) time and 0 € space, where M is the number <b>of</b> minimum <b>cost</b> <b>assignments.</b> A previous algorithm for finding all perfect matchings is used. The algorithms extend to ranking maximum matchings. The closely related problems of finding the kth smallest assignment and finding an <b>assignment</b> <b>of</b> given <b>cost</b> C are shown NP-hard...|$|R
40|$|This paper {{describes}} an R package, rpartOrdinal, that implements alternative splitting functions for fitting a classification tree when interest lies in predicting an ordinal response. This includes the generalized Gini impurity function, which was introduced {{as a method}} for predicting an ordinal response by including <b>costs</b> <b>of</b> misclassification into the impurity function, {{as well as an}} alternative ordinal impurity function due to Piccarreta (2008) that does not require the <b>assignment</b> <b>of</b> misclassification <b>costs.</b> The ordered twoing splitting method, which is not defined as a decrease in node impurity, is also included in the package. Since, in the ordinal response setting, misclassifying observations to adjacent categories is a less egregious error than misclassifying observations to distant categories, this package also includes a function for estimating an ordinal measure of association, the gamma statistic. ...|$|R
