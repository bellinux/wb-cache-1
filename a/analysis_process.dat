4418|10000|Public
25|$|Cine film {{or video}} {{recordings}} using footage from single or multiple cameras {{can be used}} to measure joint angles and velocities. This method has been aided by the development of analysis software that greatly simplifies the <b>analysis</b> <b>process</b> and allows for analysis in three dimensions rather than two dimensions only.|$|E
25|$|To {{demonstrate}} the rational <b>analysis</b> <b>process</b> as described above, let’s examine the policy paper “Stimulating {{the use of}} biofuels in the European Union: Implications for climate change policy” by Lisa Ryan where the substitution of fossil fuels with biofuels has been proposed in the European Union (EU) between 2005–2010 {{as part of a}} strategy to mitigate greenhouse gas emissions from road transport, increase security of energy supply and support development of rural communities.|$|E
25|$|On 29 October 2015, the USAF {{announced}} that Seymour Johnson Air Force Base, North Carolina, {{was chosen as}} the preferred alternative for the first Reserve-led KC-46A Pegasus main operating base. The KC-46As will begin arriving at Seymour Johnson in fiscal year 2019. Tinker Air Force Base, Oklahoma; Westover Air Reserve Base, Massachusetts; and Grissom Air Reserve Base, Indiana, were named as the reasonable alternatives. The Air Force plans to begin the Environmental Impact <b>Analysis</b> <b>Process</b> (EIAP). Once the requirements of the EIAP are complete, the Air Force will make its final basing decision.|$|E
40|$|Policy {{conflict}} <b>analysis</b> <b>processes</b> {{based solely}} on the examination of policy language constructs can not readily discern the semantics associated with the managed system for which the policies are being defined. However, by developing <b>analysis</b> <b>processes</b> that can link the constructs of a policy language to the entities of an information model, we can harness knowledge relating to relationships and associations, constraint information, behavioural specifications codified by finite state machines, and extensive semantic information expressed via ontologies to provide powerful policy <b>analysis</b> <b>processes...</b>|$|R
5000|$|Analysis and {{visualization}} modules {{are integrated}} to support standardized <b>analysis</b> <b>processes</b> ...|$|R
40|$|OF THE DISSERTATION Composition of Image <b>Analysis</b> <b>Processes</b> through Object-Centered Hierarchical Planning by Leiguang Gong, Ph. D. Dissertation Director: Professor Casimir A. Kulikowski The present thesis {{describes}} {{a new approach}} to biomedical image interpretation [...] - the knowledge-based composition of image <b>analysis</b> <b>processes</b> through object-centered hierarchical planning. Its computer implementation in the VISIPLAN system has been tested on different image analysis tasks using Magnetic Resonance (MR) scans. The approach {{has been shown to be}} flexible and effective in handling a reasonably broad class of image analysis problems. The new approach defines the problem of composing image <b>analysis</b> <b>processes</b> primarily as that of detecting a sequence of reference objects to establish the context for segmentation and recognition of a goal object at the highest level of problem solving. At a lower level it defines the composition of specific <b>analysis</b> <b>processes,</b> which are scheduled for each of the [...] ...|$|R
25|$|Industrial {{application}} of CSP to software design has usually focused on dependable and safety-critical systems. For example, the Bremen Institute for Safe Systems and Daimler-Benz Aerospace modeled a {{fault management system}} and avionics interface (consisting of some 23,000 lines of code) intended for use on the International Space Station in CSP, and analyzed the model to confirm that their design was free of deadlock and livelock. The modeling and <b>analysis</b> <b>process</b> was able to uncover a number of errors {{that would have been}} difficult to detect using testing alone. Similarly, Praxis High Integrity Systems applied CSP modeling and analysis during the development of software (approximately 100,000 lines of code) for a secure smart-card Certification Authority to verify that their design was secure and free of deadlock. Praxis claims that the system has a much lower defect rate than comparable systems.|$|E
25|$|The Draft Presidential Memorandum (DPM)—intended for the White House {{and usually}} {{prepared}} by the systems analysis office—was a method to study and analyze major defense issues. Sixteen DPMs appeared between 1961 and 1968 on such topics as strategic offensive and defensive forces, NATO strategy and force structure, military assistance, and tactical air forces. OSD sent the DPMs to the services and the Joint Chief of Staff (JCS) for comment; in making decisions, McNamara included in the DPM a statement of alternative approaches, force levels, and other factors. The DPM in its final form became a decision document. The DPM was hated by the JCS and uniformed military in that it cut their ability to communicate directly to the White House. The DPMs were also disliked because the systems <b>analysis</b> <b>process</b> was so heavyweight {{that it was impossible}} for any service to effectively challenge its conclusions.|$|E
2500|$|This {{approach}} of integrating situation awareness, workload, signal processing theory, decision theory, etc. tends to subtly change {{the questions that}} are asked during the <b>analysis</b> <b>process</b> from quantifying and qualifying the SA to measures of the probabilistic aspects of a decision, such {{as the number of}} interrelationships, the certainty and time-lag of the information arriving, risk to desired outcome or effect, etc., together with the processing aspects, to do with the number of signals, accuracy and completeness of the information and importance to the operational context. In other words, instead of asking does a modification to the system provide more SA, we are asking does this modification to the system provide more SA in a form that can be used at the time when it is needed? ...|$|E
40|$|International audienceAnalyzing data {{coming from}} {{e-learning}} environments can produce knowledge and potentially improve pedagogical efficiency. Nevertheless, TEL community faces heterogeneity concerning e-learning traces, <b>analysis</b> <b>processes</b> and tools leading these analyses. Therefore, <b>analysis</b> <b>processes</b> {{have to be}} redefined when their implementation context changes: they cannot be reused, shared nor easily improved. There is no capitalization and we consider this drawback as an obstacle for the whole community. In this paper, we propose an independent formalism to describe <b>analysis</b> <b>processes</b> of e-learning interaction traces, in order to capitalize them and avoid these technical dependencies. We discuss both this capitalization and its place and effects in the iterative learning analysis procedure...|$|R
5000|$|Qualitative and {{quantitative}} <b>process</b> <b>analysis</b> (e.g. <b>process</b> simulation) ...|$|R
40|$|Abstract. Policy {{conflict}} <b>analysis</b> <b>processes</b> {{based solely}} on the examination of policy language constructs can not readily discern the semantics associated with the managed system for which the policies are being defined. However, by developing <b>analysis</b> <b>processes</b> that can link the constructs of a policy language to the entities of an information model, we can harness knowledge relating to relationships and associations, constraint information, behavioural specifications codified by finite state machines, and extensive semantic information expressed via ontologies to provide powerful policy <b>analysis</b> <b>processes.</b> 1 Research Problem Existing approaches to policy conflict detection are primarily concerned with analysing the information contained within individual policies defined for a specific managed system. However, this approach, in general, {{does not take into account}} application specific semantics. This semantic information can be represented using information models and ontologies relating to a specific managed system. By tightl...|$|R
5000|$|There {{are several}} {{individual}} {{steps in the}} <b>analysis</b> <b>process</b> as a whole in alaboratory such as: ...|$|E
50|$|The vector signal {{analyzer}} spectrum <b>analysis</b> <b>process</b> typically has a down-convert & digitizing {{stage and}} a DSP & display stage.|$|E
50|$|SigmaXL also {{includes}} tools for data manipulation, measurement systems <b>analysis,</b> <b>process</b> capability (including nonnormal process capability), reliability/Weibull analysis {{and design of}} experiments.|$|E
40|$|International audienceThis paper {{presents}} the DOP 8 _Cycle, a Data Mining Iterative Cycle that improves the classical data life cycle. While the latter only combines the data production and data analysis phases, the DOP 8 _Cycle also integrates the analysis operators life cycle (a {{combination of both}} the analysis operator design and data analysis phases). Considering these two life cycles in the same environment leads to fostering the link between data and operators and to creating <b>analysis</b> <b>processes</b> {{for a wide range}} of Technology Enhanced Learning (TEL) research. This concept is reified in a new computing platform, called UnderTracks. The latter provides a high flexibility on storing and sharing data, operators and <b>analysis</b> <b>processes.</b> Several real TEL analysis scenarios are now present into the platform, and we analyze them (1) to test the DOP 8 _Cycle efficiency, (2) to test Undertracks flexibility on storing data and operators and (3) to test Undertracks flexibility on designing <b>analysis</b> <b>processes.</b> This evaluation shows that the DOP 8 _Cycle allows data, operators and <b>process</b> <b>analysis</b> to be shared between different TEL domains...|$|R
30|$|All {{the test}} results of each {{condition}} in the two tests are sufficiently effective. All the results are listed as the average value of each condition to simplify the comparison and <b>analysis</b> <b>processes.</b>|$|R
50|$|FAIR {{seeks to}} provide a {{foundation}} and framework for performing risk analyses. Much of the FAIR framework {{can be used to}} strengthen, rather than replace, existing risk <b>analysis</b> <b>processes</b> like those mentioned above.|$|R
50|$|One of {{the more}} popular methods to perform a risk {{analysis}} in the computer field is called facilitated risk <b>analysis</b> <b>process</b> (FRAP).|$|E
5000|$|There are {{numerous}} computational challenges that arise {{at various stages}} of the network construction and <b>analysis</b> <b>process</b> in field of climate networks: ...|$|E
50|$|In practice, {{these three}} sub-processes are {{followed}} by a process of risk monitoring that can give input to the situation and risk <b>analysis</b> <b>process.</b>|$|E
40|$|This article {{reflects}} on key methodological issues emerging from {{children and young}} people's involvement in data <b>analysis</b> <b>processes.</b> We outline a pragmatic framework illustrating different approaches to engaging children, using two case studies of children's experiences of participating in data analysis. The article highlights methods of engagement and important {{issues such as the}} balance of power between adults and children, training, support, ethical considerations, time and resources. We argue that involving children in data <b>analysis</b> <b>processes</b> can have several benefits, including enabling a greater understanding of children's perspectives and helping to prioritise children's agendas in policy and practice. (C) 2007 The Author(s). Journal compilation (C) 2007 National Children's Bureau...|$|R
40|$|Many {{enterprises}} {{have recently}} been pursuing process innovation or improvement to attain their performance goals. To align a business process with enterprise performances, this study proposes a two-stage <b>process</b> <b>analysis</b> for <b>process</b> (re) design that combines the process-based performance measurement framework (PPMF) and business process simulation (BPS). The two-stage analysis consists of macro and micro analyses of business processes. At the early stage of business <b>process</b> <b>analysis</b> (BPA), macro <b>process</b> <b>analysis</b> is conducted to identify {{the influence of a}} business process on a target key performance indicator (KPI) or the contribution of a target KPI to other KPIs. If target business processes that need improvement are identified through the macro <b>process</b> <b>analysis</b> and to-be <b>processes</b> are newly designed, micro <b>process</b> <b>analysis</b> using simulation is conducted to predict the performance. The proposed method is validated by application to a real business process within the setting of a large Korean company. By using the proposed, two-stage <b>process</b> <b>analysis,</b> company staff involved in process innovation projects can determine the processes with the greatest influence on enterprise strategy, and can systematically evaluate the performance prediction of the newly designed process. close 61...|$|R
5000|$|SNP Business <b>Process</b> <b>Analysis,</b> the SAP-focused <b>Process</b> Mining {{solution}} by SNP Schneider-Neureither & Partner AG ...|$|R
50|$|Examining Malicious {{software}} involves several stages, {{however the}} below stages mentioned {{can be considered}} as discrete and sequential steps over-simplifies the steps malware <b>analysis</b> <b>process.</b>|$|E
50|$|Accurate {{detection}} and quantification {{are the most}} vital components of the TOC <b>analysis</b> <b>process.</b> Conductivity and non-dispersive infrared (NDIR) are the two common detection methods used in modern TOC analyzers.|$|E
50|$|A set of Eclipse-based {{customization}} tooling, LanguageWare Resource Workbench, {{is available}} on IBM's alphaWorks site, and allows domain knowledge to be compiled into these resources and thereby incorporated into the <b>analysis</b> <b>process.</b>|$|E
50|$|Many organisations highly {{depend on}} data and visual <b>analysis</b> <b>processed</b> by {{information}} system. For this reason, organisation must obtain accurate analysis based on exact data {{and must be}} cautious for mistaken output that impacts whole process of their business.|$|R
40|$|This master´s thesis {{deals with}} the {{selection}} of the information system for the chosen company. based on the <b>analysis</b> <b>processes</b> in the company. The theoretical part contains theoretical knowledge for the issue. In the analytical part is performed analysis of current situation of the company and <b>analysis</b> of <b>processes</b> of the company. In the suggestion part are mentioned requirements for the selection of a new information system and it performed selection of the information system based on requirements...|$|R
40|$|While {{exploring}} data {{using information}} visualization, analysts {{try to make}} sense of the data, build cases, and present them to others. However, if the exploration is long or done in multiple sessions, it can be hard for analysts to remember all interesting visualizations and the relationships among them they have seen. Often, they will see the same or similar visualizations, and are unable to recall when, why and how they have seen something similar. Recalling and retrieving interesting visualizations are important tasks for the <b>analysis</b> <b>processes</b> such as problem solving, reasoning, and conceptualization. In this paper, we argue that offering support for thinking based on past <b>analysis</b> <b>processes</b> is important, and present a solution for this...|$|R
50|$|The risk {{evaluation}} process receives as input {{the output of}} risk <b>analysis</b> <b>process.</b> It compares each risk level against the risk acceptance criteria and prioritise the risk list with risk treatment indications.|$|E
5000|$|A second trend may {{concern the}} more recent {{publications}} that include a step-by-step music <b>analysis</b> <b>process</b> at the neutral level, such as the generative theory of tonal music and Narmour's Implication-Realization model.|$|E
5000|$|A common {{principle}} in behaviour management {{is looking for}} the message an individual is communicating through their challenging behaviour: [...] "All behaviour has meaning". This is a core in the functional <b>analysis</b> <b>process.</b>|$|E
40|$|International audienceIn {{the context}} of our {{research}} team (multidisciplinary with numerous and various TEL systems), {{we have been working}} during the last three years on the design and implementation of an open platform to collect, save and share experimental data drawn from the interaction with TEL systems, which could build, save and share <b>analysis</b> <b>processes</b> executed on these data. From our point of view both data and <b>analysis</b> <b>processes</b> are worth to be stored and shared, and moreover have to be joined in a unique repository to get the whole picture. This communication presents the <b>analysis</b> <b>processes</b> part of the project. Sharing <b>analysis</b> <b>processes,</b> i. e. the whole complex process, is rather unusual, whereas contemporary platforms or software already propose generic algorithms to work on data (for instance with a statistical point of view or a data mining point of view). Hence, we attempt to model the main concepts of global treatments for experimental data analysis in order to collect, execute, save and then share them in a platform, dedicated to TEL Systems. The execution part is the most difficult and constraining part of our work. This needs to be implemented with a complex architecture. An important part of the communication is so devoted to the description of the architecture, and to the link between the global {{point of view of the}} whole process and the local point of view of elementary or specific algorithms used during the process. A short, but realistic, example of application of our platform is given, with the definition of a global process and the definition of an elementary algorithm used in the global process. The process is executed on real data leading to a graphical display of results, which are then briefly analyzed...|$|R
5000|$|Three Models of Discourse <b>Analysis.</b> Discourse <b>Processes</b> 3:101-131.|$|R
30|$|Insight 6. Complexity {{features}} {{can be used}} {{to build}} regression models that allow the estimation of signature effectiveness. These prediction models can constitute a signature quality metric that can be exploited by security specialists to evaluate the available rulesets, prioritize the generated alerts, and facilitate the forensics <b>analysis</b> <b>processes.</b>|$|R
