11|46|Public
50|$|Determines {{the size}} of the character’s Mentality Pool. The amount of Mentality used per shot is {{proportional}} to the amount of the <b>Accuracy</b> <b>statistic</b> a character has. Having more Accuracy will increase the amount of Mentality used. If your Mentality is reduced to 10% or less of its total pool size, your swing bar will move much faster while putting.|$|E
30|$|Hydrologic {{modelling}} {{was performed}} {{with the use of}} the continuous rainfall-runoff model SAC-SMA (Sacramento) model (Tsanis & Apostolaki 2008). Precipitation and potential evapotranspiration are used as model inputs to generate an estimated flow which can be used to calibrate the model with a genetic algorithm optimization process (Wang 1997). The objective function of the optimization is the Nash-Sutcliffe model <b>accuracy</b> <b>statistic.</b> An optimization method based on genetic algorithms was used in 17 gauged basins using a monthly time step. In general, the calibration yielded satisfactory results with the Nash–Sutcliffe criterion between 0.51 and 0.90. The averaging for all 110 basin parameters was implemented using a “naïve” parameter to make estimations for the total area of the island of Crete (Tsanis et al. 2011).|$|E
40|$|Optimizing over {{a variant}} of the Mean Optimal Subpattern Assignment (MOSPA) metric is {{equivalent}} to optimizing over the track <b>accuracy</b> <b>statistic</b> often used in target tracking bench-marks. Past work has shown how obtaining a Minimum MO-SPA (MMOSPA) estimate for target locations from a Prob-ability Density Function (PDF) outperforms more traditional methods (e. g. maximum likelihood (ML) or Minimum Mean Squared Error (MMSE) estimates) with regard to track accu-racy metrics. In this paper, we derive an approximation to the MMOSPA estimator in the two-target case, which is generally very complicated, based on minimizing a Bhattacharyya-like bound. It has a particularly nice form for Gaussian mixtures. We thence compare the new estimator to that obtained from using the MMSE and the optimal MMOSPA estimators. Index Terms — MOSPA, tracking, state estimation 1...|$|E
40|$|This {{paper is}} devoted {{to the study of the}} {{estimation}} of the population mean using of product estimator under ranked set. Strategies for different alternative estimators are developed and compared with theirs simple random sampling with replacement counterparts. The sample errors of them are compared analytically. KEY WORDS: product estimators, gain in <b>accuracy,</b> order <b>statistic...</b>|$|R
40|$|In this work, {{there is}} {{described}} an abrasive process including technological terms with an identification of motions and {{a speed of}} grinding. Then there are described abrasive tools, machines, their distribution and description. Further technological factors on accuracy of abrasive process and some reached parameters of the <b>accuracy</b> and their <b>statistic</b> interpretation...|$|R
40|$|Abstract — Various {{types of}} online {{learning}} algorithms {{have been developed}} so far to handle concept drift in data streams. We perform more detailed evaluation of these algorithms through new performance metrics- prequential <b>accuracy,</b> kappa <b>statistic,</b> CPU evaluation time, model cost, and memory usage. Experimental evaluation using various artificial and real-world datasets prove that the various concept drifting algorithms provide highly accurate results in classifying new data instances even in a resource constrained environment, irrespective of size of dataset, type of drift or presence of noise in the dataset. We also present empirically the impact of various features- size of ensemble, period value, threshold value, multiplicative factor {{and the presence of}} noise on all the key performance metrics...|$|R
40|$|In this study, a {{segmentation}} algorithm {{based on}} steepest changes of a probabilistic cost function was tested on non-processed and pre-processed dense breast images {{in an attempt}} to determine the efficacy of pre-processing for dense breast masses. Also, the inter-observer variability between expert radiologists is studied. The preprocessing method used was background trend correction. The algorithm, based on searching the steepest changes on a probabilistic cost function, was tested on 107 cancerous masses and 98 benign masses. Their density ratings were 3 and 4 according to the ACR density rating scale. The computer-segmented results were validated using the overlap, accuracy, sensitivity, specificity, Dice similarity index, and kappa statistics. The mean values for the <b>accuracy</b> <b>statistic</b> ranged from 0. 71 - 0. 84 for cancer cases and 0. 81 - 0. 86 for benign cases. For nearly all statistics there were statistically significant differences between the expert radiologists...|$|E
40|$|The {{accuracy}} of an {{artificial neural network}} (ANN) algorithm is a crucial issue in the estimation of an oil field reservoir`s properties from remotely sensed seismic data. This paper demonstrates {{the use of the}} k-fold cross validation technique to obtain confidence bounds on an ANN`s <b>accuracy</b> <b>statistic</b> from a finite sample set. In addition, we also show that an ANN`s classification accuracy is dramatically improved by transforming the ANN`s input feature space to a dimensionally smaller, new input space. The new input space represents a feature space that maximizes the linear separation between classes. Thus, the ANN`s convergence time and accuracy are improved because the ANN must merely find nonlinear perturbations to the starting linear decision boundaries. These techniques for estimating ANN accuracy bounds and feature space transformations are demonstrated on the problem of estimating the sand thickness in an oil field reservoir based only on remotely sensed seismic data...|$|E
40|$|The aim of {{this study}} was to {{validate}} the use of the spectrophotometer Minolta CM- 600 d for measuring the colour and the proportions of different myoglobin redox forms (oxymyoglobin, deoxymyoglobin, and metmyoglobin) on the surface of meat. One vacuum-packaged (VP) striploin was supplied by a Belgian food wholesaler. It was cut in 3 cm thick steaks, repacked under vacuum and stored at − 0. 5 °C until analyses. The measurement of colour in the C. I. E. L*a*b* space and the determination of oxymyoglobin, deoxymyoglobin, and metmyoglobin were performed on VP and modified atmosphere-packed (70 % O 2 / 30 % CO 2 for 24 h) samples (n = 10). Results obtained were compared to two reference methods (colour measurement using a chromameter Minolta CR- 400 and spectrophotometric determination of different myoglobin redox forms in aqueous meat extracts) by F-test for precision and t-test for <b>accuracy.</b> <b>Statistic</b> significance level was established at 5 %. The two colour measurement methods presented the same precision, when considering VP samples only, and different accuracies, probably because of the different detectors and observation angles used by both devices. The two methods for determining the different myoglobin forms presented also the same precision but different accuracies, probably due to the fact that oxygenation is favoured during some steps of the reference method (e. g. extraction, filtration). In conclusion, the results for colour measurement obtained by both devices cannot be compared. It is necessary to compare both methods for determining oxymyoglobin, deoxymyoglobin, and metmyoglobin in complete anaerobic conditions in order to eliminate the oxygenation bias. Peer reviewe...|$|E
40|$|Multiple {{sequence}} alignments have wide applicability in {{many areas}} of computational biology including comparative genomics, functional annotation of proteins, gene finding, and modeling evolutionary processes. Because of the computational difficulty of multiple sequence alignment and the availability of numerous tools, it is critical to be able to assess the reliability of multiple alignments. We present a tool called StatSigMA to assess whether multiple alignments of nucleotide or amino acid sequences are contaminated with one or more unrelated sequences. There are numerous applications for which StatSigMA can be used. Two such applications are to distinguish homologous sequences from nonhomologous ones and to compare alignments produced by various multiple alignment tools. We present examples of both types of applications. Index terms multiple sequence alignment, discordance, alignment <b>accuracy,</b> Karlin-Altschul <b>statistic...</b>|$|R
40|$|Abstract — In this paper, a {{statistical}} discrete-time model is proposed for simulating wideband MIMO channels which experience spatially and temporally correlated, widesense stationary uncorrelated scattering (WSSUS) multipath Rayleigh fading. A new method is also presented to efficiently generate the correlated MIMO channel coefficients, {{which can be}} used for accurate simulation of physical continuous-time MIMO channels. The <b>statistic</b> <b>accuracy</b> of the discrete-time MIMO channel model is rigorously verified through theoretical analysis and extensive simulations in different criteria. I...|$|R
40|$|As {{the number}} of {{processing}} elements (PE) on a single chip increases with each generation of CMOS technology, network on-chip (NoC) has become a de-facto communication fabric for these PEs. Due to high design and test costs for real many-core chips, simulators which allow exploring the best design options for a system before actually building it have been becoming highly necessary in system design and optimization flows. This paper presents NoCTweak, a highly parameterizable NoC simulator used for early exploration of performance and energy efficiency of on-chip networks. The simulator has been developed in SystemC, a C++ plugin, which allows fast modeling of concurrent hardware modules at the cycle-level <b>accuracy.</b> The <b>statistic</b> output results provided by the simulator are the average network latency, throughput, router power and energy per transferred data packet corresponding to a given network configuration, a certain traffic pattern and load. Area, timing and power of router components are post-layout data based on standard-cell libraries. ...|$|R
40|$|The fire {{severity}} of the 2013 – 2014 fire season within Sudanian ecosystems in Burkina Faso was evaluated from Landsat 8 images using derivatives of the Normalized Burn Ratio algorithm (NBR). The relationship between the image-derived severity and the field observed severity i. e. Composite Burn Index (CBI) was best described by a nonlinear model of the form y = a + b*EXP(CBI *c) (R 2 = 0. 66). Classification of the image-derived burned area into burn severity classes achieved a classification Kappa <b>accuracy</b> <b>statistic</b> of 0. 56. Highly severely burned areas were mapped with the highest accuracy (user's accuracy 77 %, producer's accuracy 86 %). The {{severity of}} the burn varied across phyto-geographical zones, protected status, land cover regimes, and forest management practices. The south Sudanian zone burned with a higher severity (low = 7 %, moderate = 16 % and high = 13 %) than the north Sudanian zone (low = 5 %, moderate = 10 % and high = 5 %). The mean of the highly severely burned areas differed significantly among the forest management practices (P = 0. 005). A pair-wise comparison of the severity mean area indicated that the highly burned areas within forests managed for wildlife purposes differed significantly with that of both forests under the joint management (P = 0. 006) and those under no management (P = 0. 024). Among the management practices, forests jointly managed by the local communities and the government had the highest unburned area and the least highly severely burned areas reflecting the impacts of bottom-up forestry management where the local communities are {{actively involved in the}} management. German Federal Ministry for Economic Cooperation and DevelopmentPeer Revie...|$|E
40|$|The {{purpose of}} this project is to test High Resolution Stereoscopic (HRS) data of SPOT 5. This project {{is done in the}} {{framework}} of the “HRS study Team ” international test, supported by ISPRS-CNES. The Digital Elevation Model (DEM) was derived from the HRS data and was evaluated by using a 3 D reference data set. The study site for HRS data is located in Manosque, France. It is a countryside landscape, with fields and villages. The height relief is medium and in some areas considerably intensive. The HRS data was acquired on 14 th August 2002. A number of different DEM generation experiments were carried out, in order to examine the influence of different processing parameters on the final accuracy of the DEM. Thus: 1) The same date acquisition of the stereo-pair reduces the radiometric variations of images; nevertheless, the fact that the images are captured by two different sensors has led us to apply a histogram matching technique for image pre-processing, in some experiments. 2) The georeferencing and the stereo model set-up of the images were established in four different ways: by using only the metadata; by using the metadata and tie points that have been generated automatically; by using the metadata, tie points that have been generated automatically and some tie points that have been derived manually; The same as the second solution but in the triangulation process 6 GCP’s (XYZ coordinates) have been used. For the extraction of the GCP’s an additional procedure has been performed, which consisted basically of the production of an orthoimage based on the reference DEM. The study of the <b>accuracy</b> <b>statistic</b> results of 8 different derived DEM,s showed that the elevation accuracy was better, about 0 - 5 m, in flat and even areas than in mountainous and steep areas. 1...|$|E
40|$|The {{purpose of}} this {{research}} is to develop design and accuracy criteria for large-scale topographic surveys. The large-scale topographic survey is a ‘sparse surface model’ consisting of sampling prescription, surface modeling algorithm, and partitioned terrain complexity. Predictable survey outcomes can be achieved by partitioning the site into unitary land forms such that the amplitude of resident terrain microform is less than one-half the accuracy specification. A paraboloid of revolution can be used to model unitary land form. The limiting radius of surface curvature and the accuracy specification can be used to compute a sampling interval. Where the resident microform, without regard to its distribution, is less than one-half the accuracy specification, sampling prescriptions based on curvature provide predictable sampling outcomes. The sampling in such cases is accomplished on the trend surface without regard to the distribution of the microform amplitude. Where the amplitude of resident microform is greater than one-half of the desired accuracy specification, composite sampling is the result. Predictable sampling outcomes cannot be achieved in such cases, however the better results are achieved within the one-quarter mean wavelength where the microform can be shown to be periodic. Such sampling takes place {{on the surface of the}} microform without regard to curvature on the underlying trend surface. ^ The Draft National Standard for Spatial Data Accuracy (NSSDA) vertical <b>accuracy</b> <b>statistic</b> has a high probability of failure on unitary land forms because of the lack of a measure of central tendency. The error distribution on a unitary land form needs a measure of central tendency to locate the center of mass of the error residuals. ^ Unsymmetrical error distributions that occur with sparse data sets produce inflated confidence intervals because variance is a second-degree function. This fact can be mitigated by the use of the median and average deviation to locate and describe the mass of the error residuals. These smaller intervals will encompass from 87 to 91 % of error residuals. The residuals in the tails can be reported with a count and magnitude range. This provides much more information about the large-scale topographic survey to the engineer than a single statistic. ...|$|E
40|$|Abstract Background Physicians {{are often}} unable {{to eat and}} drink {{properly}} during their work day. Nutrition has been linked to cognition. We aimed to examine the effect of a nutrition based intervention, that of scheduled nutrition breaks during the work day, upon physician cognition, glucose, and hypoglycemic symptoms. Methods A volunteer sample of twenty staff physicians from a large urban teaching hospital were recruited from the doctors' lounge. During both the baseline and the intervention day, we measured subjects' cognitive function, capillary blood glucose, "hypoglycemic" nutrition-related symptoms, fluid and nutrient intake, level of physical activity, weight, and urinary output. Results Cognition scores as measured by a composite score of speed and <b>accuracy</b> (Tput <b>statistic)</b> were superior on the intervention day on simple (220 vs. 209, p = 0. 01) and complex (92 vs. 85, p Conclusions Our study provides evidence in support of adequate workplace nutrition as a contributor to improved physician cognition, adding to the body of research suggesting that physician wellness may ultimately benefit not only the physicians themselves but also their patients and the health care systems in which they work. </p...|$|R
40|$|Chapter 2 : This chapter {{introduces}} the five study sites (Ras Al‐Qasabah; Al Wajh; Yanbu; Farasan Banks; and Western Farasan Islands) {{along with the}} fieldwork and detailed benthic mapping and bathymetry mapping conducted in the Saudi Arabian Red Sea. In the Western Farasan Islands two candidate mapping technologies were compared. Firstly, the QuickBird multispectral satellite sensor and secondly the CASI‐ 550 airborne hyperspectral sensor. In processing the CASI imagery, {{it was necessary to}} customize processing to correct for an unusual across‐track artifact caused by lens condensation. On the basis of cost, logistical constraints, spectral reliability, and project needs, multispectral imagery was found to be the most appropriate technology for regional‐scale mapping. Over 20, 000 sq. km of high quality QuickBird imagery were amassed across the five study sites. This represents approximately half the shallow water (3 ̆c 20 m) environment of the Saudi Arabian Red Sea. The work presented in this chapter provides a blueprint for processing such large image data sets. Maps with a minimum mapping unit (MMU) of 7. 5 sq. m, and a thematic resolution of fifteen habitat classes were produced at an overall <b>accuracy</b> (Tau <b>statistic)</b> of 70...|$|R
40|$|The {{estimated}} accuracy of a classifier is a random quantity with variability. A {{common practice in}} supervised machine learning, is thus to test if the {{estimated accuracy}} is significantly better than chance level. This method of signal detection is particularly popular in neuroimaging and genetics. We provide evidence that using a classifier's accuracy as a test statistic can be an underpowered strategy for finding differences between populations, compared to a bona-fide statistical test. It is also computationally more demanding than a statistical test. Via simulation, we compare test statistics {{that are based on}} classification accuracy, to others based on multivariate test statistics. We find that probability of detecting differences between two distributions is lower for accuracy based statistics. We examine several candidate causes for the low power of accuracy tests. These causes include: the discrete nature of the <b>accuracy</b> test <b>statistic,</b> the type of signal accuracy tests are designed to detect, their inefficient use of the data, and their regularization. When the purposes of the analysis is not signal detection, but rather, the evaluation of a particular classifier, we suggest several improvements to increase power. In particular, to replace V-fold cross validation with the Leave-One-Out Bootstrap...|$|R
40|$|The Water vapour Strong Lines at 183 GHz (183 -WSL) {{algorithm}} {{is a method}} for the retrieval of rain rates and precipitation type classification (convectivestratiform), that makes use of the water vapor absorption lines centered at 183. 31 GHz of the Advanced Microwave Sounding Unit module B (AMSU-B) and of the Microwave Humidity Sounder (MHS) flying on NOAA- 15 - 18 and NOAA- 19 Metop-A satellite series, respectively. The characteristics of this algorithm were described in Part I of this paper together with comparisons against analogous precipitation products. The focus of Part II is {{the analysis of the}} performance of the 183 -WSL technique based on surface radar measurements. The ground truth dataset consists of 2. 5 years of rainfall intensity fields from the NIMROD European radar network which covers North-Western Europe. The investigation of the 183 -WSL retrieval performance is based on a twofold approach: 1) the dichotomous statistic is used to evaluate the capabilities of the method to identify rain and no-rain clouds; 2) the <b>accuracy</b> <b>statistic</b> is applied to quantify the errors in the estimation of rain rates. The results reveal that the 183 -WSL technique shows good skills in the detection of rainno-rain areas and in the quantification of rain rate intensities. The categorical analysis shows annual values of the POD, FAR and HK indices varying in the range 0. 80 - 0. 82, 0. 330. 36 and 0. 39 - 0. 46, respectively. The RMSE value is 2. 8 millimeters per hour for the whole period despite an overestimation in the retrieved rain rates. Of note is the distribution of the 183 -WSL monthly mean rain rate with respect to radar: the seasonal fluctuations of the average rainfalls measured by radar are reproduced by the 183 -WSL. However, the retrieval method appears to suffer for the winter seasonal conditions especially when the soil is partially frozen and the surface emissivity drastically changes. This fact is verified observing the discrepancy distribution diagrams where 2 the 183 -WSL performs better during the warm months, while during the winter time the discrepancies with radar measurements tends to maximum values. A stable behavior of the 183 -WSL {{algorithm is}} demonstrated over the whole study period with an overall overestimation for rain rates intensities lower than 1 millimeter per hour. This threshold is crucial especially in wintertime where the low precipitation regime is difficult to be classified...|$|E
40|$|The Functional Movement Screen (FMS) {{is a tool}} {{designed}} to identify limitations and compensatory movement patterns of individuals {{in order to help}} improve performance and decrease injuries. While research has demonstrated that the FMS can be assessed reliably across raters, evidence for the validity of the FMS to predict injury and performance is scarce. PURPOSE: The purposes of this study were to: (a) examine the ability of the Functional Movement Screen and a bilateral weight distribution measure to predict injuries in collegiate track and field athletes {{over the course of a}} season, and (b) determine if FMS scores are related to performance using the standing vertical jump performance test. METHODS: Collegiate track and field athletes (N = 36) completed the FMS protocol, a bilateral weight distribution test as a comparison measure, and the vertical jump as a performance test. The FMS consists of seven functional movements scored on a 0 - 3 scale rating the quality of movement patterns. A bilateral weight distribution test was used as a comparison measure to examine injury prediction. The bilateral weight distribution test was conducted by having the athlete stand on two scales with one foot on each to record asymmetrical differences in body mass. Measurements were taken with the athlete’s feet at shoulder width distance apart and again with feet placed apart at 1 / 3 of his or her height. Results were categorized into positive and negative tests for the FMS based on a previously established cutoff score. A positive test was defined two ways: (a) a difference in body mass between scales of ≥ 3 % and (b) a difference in body mass between scales of ≥ 5 %. Interrater reliability was estimated by using a one-way analysis of variance to assess the consistency between two raters who concurrently scored 15 athletes. Injuries were monitored by the team’s head athletic trainer and reported weekly throughout the season. Sensitivity and specificity values were calculated to examine the accuracy of the different screening methods to identify participants who were injuries or not injured. An <b>accuracy</b> <b>statistic</b> was calculated to show the probability of correct diagnosis (sustaining an injury and having a positive test or not sustaining an injury and having a negative test). Pearson product-moment correlations were calculated to examine the association between FMS and standing vertical jump performance. RESULTS: Interrater reliability for the total FMS score between the lead and secondary rater was excellent (ICC =. 98, 95 % CI =. 94 -. 99). The mean ± SD for total FMS scores among all participants was 14. 9 ± 2. 7 (male: 15. 4 ± 2. 9; female: 14. 8 ± 2. 7). Of the 35 athletes, 25 athletes (71 %) experienced at least one injury during the 9 -week outdoor season. Due to a small sample size for males (n = 11), analysis focused on the female sample (n = 25). For the FMS among females, sensitivity was. 65 and specificity was. 75. Accuracy results showed that the FMS (accuracy = 68 %) correctly diagnosed injury status better than the bilateral weight distribution measure with feet at shoulder width (accuracy = 32 % at ≥ 5 % difference in body mass and accuracy = 36 % at ≥ 3 % difference in body mass) and at 1 / 3 of the participant’s height (accuracy = 48 % at ≥ 5 % difference in body mass and accuracy = 36 % at ≥ 3 % difference in body mass. Both tests were poor at predicting injury in the sample. No significant relationship was found between total FMS score and vertical jump performance for females (r =. 11, p =. 61). CONCLUSION: The FMS can be used reliably with track and field athletes; however, the ability of the FMS to accurately predict injury status was not supported in the current study. Although the FMS was slightly more accurate than the bilateral weight distribution measures at predicting injury status, the accuracy of the FMS to predict injury status in female in track and field athletes was modest. Total FMS scores were not related to vertical jump performance in female track and field athletes. M. S...|$|E
40|$|Maximum Likelihood (ML) and Artificial Neural Network (ANN) {{supervised}} classification methods {{were used to}} demarcate land cover types within IKONOS and Landsat ETM+ imagery. Three additional data sources were integrated into the classification process: Canopy Height Model (CHM), Digital Terrain Model (DTM) and Thermal data. Both the CHM and DTM were derived from multiple return small footprint LIDAR. Forty maps were created and assessed for overall map accuracy, user 2 ̆ 7 s accuracy, producer 2 ̆ 7 s <b>accuracy,</b> kappa <b>statistic</b> and Z statistic using classification schemes from U. S. G. S. 1976 levels 1 and 2 and T. G. l. C. 1999 levels 2 and 4. Results for overall accuracy of land cover maps derived from multiple sources ranged from 13. 67 to 57. 56 percent for U. S. G. S. level 2 and T. G. l. C. level 4 across ML and ANN classifications. Results for overall map accuracy ranged from 26. 00 to 72. 33 percent for U. S. G. S. level 1 and T. G. I. C. level 2 across ML and ANN classifications. Land cover maps, derived using ML classification methodology, were consistently more accurate than land cover maps derived using an ANN classification algorithm...|$|R
40|$|Insufficient {{precision}} {{remains in}} accurately identifying left ventricular noncompaction (LVNC) from the healthy normal morphologic spectrum. We aim {{to provide a}} better distinction between normal left ventricular trabeculations and LVNC. We used a previously well-defined cohort of 120 healthy volunteers for normal reference values of the trabecular/compacted ratio derived from a consistent selection of short-axis cardiovascular magnetic resonance images. We performed forward selection of logistic regression models, selecting the best model that was subsequently assessed for discrimination and calibration, validated, and converted into a clinical diagnostic chart to benchmark the boundaries of detection from a cohort of 30 patients considered to have LVNC. We showed that 3 combinations of a maximal end-diastolic trabecular/compacted ratio (≥ 1 [apex], > 1. 8 [midcavity]), (> 2 [apex], ≥ 0. 6 [midcavity]), or (> 0. 5 [base], > 1. 8 [midcavity]) separate the cohorts with the highest <b>accuracy</b> (C <b>statistic</b> [95 % confidence interval] of 0. 9749 (0. 9748 to 0. 9751) for the diagnostic chart). Quantitative cardiovascular magnetic resonance also shows that patients considered to have LVNC have a significantly reduced ejection fraction compared with normal volunteers. At midcavity and apical level, {{it is difficult to}} identify papillary muscles that are replaced by a dense trabecular meshwork. In conclusion, we developed a new, refined, diagnostic tool for identifying LVNC, based on an a priori assessment of the trabecular architecture in healthy volunteers...|$|R
40|$|Reply to Reviewers First, {{we would}} like to thank the {{anonymous}} referees for their comments, that will im-prove the quality of our paper. The revised version will include several of their sugges-tions. Anonymous referee # 1 General comments: a) Applications of the results to droplet collisions in clouds: Of course, we agree with the referee that simulations with monodisperse initial distribu-tions with the mentioned above kernels, are not realistic in the cloud physics context, but they are the only way to check the <b>accuracy</b> of the <b>statistic</b> proposed (the maximum S 7648 of the ratio of the standard deviation for the largest droplet mass over all the realizations to the averaged value calculated from the Monte Carlo). Traditionally, to test the accuracy of the approximations methods developed for the solution of the KCE (Kovetz and Olund (1969), Bleck (1970), Berry and Reinhardt (1974) ...|$|R
40|$|AbstractEpidermal {{growth factor}} {{receptor}} (EGFR/ErbB 1) is a transmembrane protein that can drive cell {{growth and survival}} via the ligand-induced dimerization of receptors. Because dimerization is a common mechanism for signal transduction, {{it is important to}} improve our understanding of how the dimerization process and membrane structure regulate signal transduction. In this study, we examined the effect of lipid nanodomains on the dimerization process of EGFR molecules. We discovered that after ligand binding, EGFR molecules may move into lipid nanodomains. The lipid nanodomains surrounding two liganded EGFRs can merge during their correlated motion. The transition rates between different diffusion states of liganded EGFR molecules are regulated by the lipid domains. Our method successfully captures both the sensitivity of single-molecule processes and <b>statistic</b> <b>accuracy</b> of data analysis, providing insight into the connection between the mobile clustering process of receptors and the hierarchical structure of plasma membrane...|$|R
40|$|Background-Post hoc {{analyses}} {{from several}} randomized, controlled trials have established the prognostic importance of different measures of ST-segment recovery in highly selected patients undergoing primary percutaneous coronary intervention (PCI) for ST-segment-elevation myocardial infarction (STEMI). In this single-center registry, we investigated whether various measures of ST-segment recovery {{can be applied}} to unselected STEMI patients undergoing primary PCI. Methods and Results-We analyzed 12 -lead ECGs from 2124 consecutive STEMI patients who underwent primary PCI at our institution between November 1, 2000, and January 1, 2007. ECGs were recorded at the catheterization laboratory immediately before arterial puncture {{and at the end of}} PCI. We examined measures assessing ST-segment recovery on the postprocedural ECG and measures comparing both ECGs and related these to 1 -year, all-cause mortality. Cumulative ST-segment recovery (Sigma ST-D resolution) at a 50 % cutoff had the highest unadjusted <b>accuracy</b> (C <b>statistic,</b> 0. 646; 95 % confidence interval, 0. 602 to 0. 689; P < 0. 001) as compared with the other 8 measures evaluated. Furthermore, Sigma ST-D resolution was the strongest contributor to both the net reclassification and integrated discrimination improvement. Conclusions-Although each measure of ST-segment recovery provided univariable prognostic information, the Sigma ST-D resolution measure comparing summed ST-segment deviations on the preprocedural and postprocedural ECG was the best independent predictor of 1 -year mortality in all-comer STEMI patients after primary PCI. (Circ Cardiovasc Qual Outcomes. 2010; 3 : 522 - 529. ...|$|R
30|$|We built {{a random}} forest model of 10, 000 trees to {{estimate}} the predictor variable importance for violent and nonviolent environmental protests. The random number of predictor variables included in each split (mtry) was three which was the square root {{of the total number}} of predictor variables as recommended by Breiman (Strobl et al. 2008). We also performed a sensitivity analysis for different numbers of trees and mtry values, but few changes in model output occurred for mtry ranging from 2 to 10, so the final model used the mtry default value of three and 10, 000 trees since computation time was not a hindrance to model construction. Informative predictor variables were determined following Strobl et al. (2008) who indicated that can be considered informative and important if their variable importance value is above the absolute value of the lowest negative-scoring variable. We also estimated various model performance parameters in addition to the standard variable importance measures produced including overall model <b>accuracy,</b> the Kappa <b>Statistic,</b> and the area under the curve (AUC) of the receiver operator characteristic.|$|R
5000|$|There is {{some debate}} over the {{validity}} of these figures. The LDS Church itself notes that [...] "In reporting their findings, the two researchers noted {{that if there were}} some measure of religious commitment comparable to temple marriage among other religions, statistics for those groups might also be more favorable." [...] The <b>accuracy</b> of this <b>statistic</b> is also disputed {{on the grounds that the}} process required to obtain a temple recommend artificially limits the test group to those who are already less likely to divorce. For example, the temple recommend requires Church members to abstain from pre-marital sex, a behavior associated with a higher divorce rate. This statistic also fails to take into account couples who enter into a temple marriage and subsequently obtain a civil divorce, yet fail to apply for a cancellation of temple sealings. Nevertheless, numerous studies show a strong link in the Latter-day Saint culture between marriage in the temple and a lower divorce rate, and that among members [...] "the temple marriage is the most resistant to divorce." ...|$|R
40|$|Predictive {{modeling}} on data streams {{plays an}} important role in modern data analysis, where data arrives continuously and needs to be mined in real time. In the stream setting the data distribution is often evolving over time, and models that update themselves during operation are becoming the state-of-the-art. This paper formalizes a learning and evaluation scheme of such predictive models. We theoretically analyze evaluation of classifiers on streaming data with temporal dependence. Our findings suggest that the commonly accepted data stream classification measures, such as classification <b>accuracy</b> and Kappa <b>statistic,</b> fail to diagnose cases of poor performance when temporal dependence is present, therefore they should not be used as sole performance indicators. Moreover, classification accuracy can be misleading if used as a proxy for evaluating change detectors with datasets that have temporal dependence. We formulate the decision theory for streaming data classification with temporal dependence and develop a new evaluation methodology for data stream classification that takes temporal dependence into account. We propose a combined measure for classification performance, that takes into account temporal dependence, and we recommend using it as the main performance measure in classification of streaming data...|$|R
40|$|Abstract: This paper {{investigates the}} {{effectiveness}} of the genetic algorithm evolved neural network classifier and its application to the land cover classification of remotely sensed multispectral imagery. First, the key issues of the algorithm and the general procedures are described in detail. Our methodology adopts a real coded GA strategy and hybrid with a back propagation (BP) algorithm. The genetic operators are carefully designed to optimize the neural network, avoiding premature convergence and permutation problems. Second, a SPOT- 4 XS imagery is employed to evaluate its accuracy. Traditional classification algorithms, such as maximum likelihood classifier, back propagation neural network classifier, are also involved for a comparison purpose. Based on an evaluation of the user’s <b>accuracy</b> and kappa <b>statistic</b> of different classifiers, the superiority of applying the discussed genetic algorithm-based classifier for simple land cover classification using multi-spectral imagery data is established. Thirdly, a more complicate experiment on CBERS (China-Brazil Earth Resources Satellite) data and discussion also demonstrates that carefully designed genetic algorithm-based neural network outperforms than gradient descent-based neural network. This has been supported by the analysis of the changes of connection weights and biases of the neural network. Finally, some concluding remarks and suggestions are also presented...|$|R
40|$|ABSTRACT: Delineating {{concrete}} {{roads and}} building footprint plays {{a crucial role}} in Object-Based Image Analysis (OBIA) using eCognition software. Unavailability of high resolution images such as Orthorectified Photograph (Orthophoto) image leave lidar data unaided for land cover classification. Misclassifications are extensive mostly among bare ground and road as well as on buildings and other taller vegetation. Thus, this paper aims to simplify the vegetation analysis through delineating roads and buildings using lidar data with the application of Lastools extensions. Fusion of lidar derivatives and intensity metrics, set with specific scale parameters, composition of homogeneity criterion, and assigning weights in each of every layer in eCognition software, segmented and extracted roads and buildings were generated. Kappa statistics of eCogntion software was used to compute the accuracy assessment of roads and buildings delineated using merely lidar data to delineation using orthophoto image. A total of 96. 22 % overall <b>accuracy</b> and kappa <b>statistic</b> of 0. 94 was calculated indicating a strong agreement or accuracy between the lidar delineated roads and buildings to reference delineated orthophoto. This study shows the capability of delineating road networks and building footprint using low dense lidar aided with lidar derivatives and metrics. 1...|$|R
40|$|In {{the present}} paper, a term {{weighting}} classification method using the chi-square statistic is proposed and evaluated in the classification subtask at NTCIR- 6 patent retrieval task. In this task, {{large numbers of}} patent applications are classified into F-term categories. Therefore, a patent classification system requires high classification speed, as well as high classification <b>accuracy.</b> The chi-square <b>statistic</b> can calculate the frequency of word appearance in the F-term {{and the frequency of}} word non-appearance in the F-term. The proposed method treats words as a scalar value and a ranking algorithm simply adds the word values of each word included in the test patent document in each F-term. Therefore, the proposed method provides classification that is significantly faster than other methods. The proposed method is evaluated in A-precision, R-precision, and F-measure. Although the proposed method did not obtain the best score, this method achieves a classification accuracy that is as high as those of other methods using machine learning or the vector classification method. In this task, the processing speed is not evaluated. Therefore, processing speed is also evaluated. The evaluation results show that the proposed method is much faster than that using the vector classificatio...|$|R
40|$|Remotely sensed {{data have}} huge {{importance}} to determine land use/cover changes for sustainable region planning and management. Variety of techniques {{in order to}} detect land cover dynamics using remote sensing imagery have been developed, tested and assessed with the results varying according to the change scenario, the information required and the imagery applied. In this study, the modified Change Vector Analysis (mCVA) technique was implemented on SPOT 4 and SPOT 5 multispectral (MS) data to monitor the dynamics of land use/land cover (LULC) change in Terkos Water Basin, İstanbul. mCVA was applied to multi-temporal data to compare {{the differences in the}} time-trajectory of the Tasseled Cap (TC) brightness, greenness and wetness for two successive time periods – 2003 and 2007. Gram Schmidt Orthogonalization Technique was used to derive the related TC coefficients for SPOT data. The efficiency of the technique was assessed based on error matrix. The overall <b>accuracy</b> and Kappa <b>statistic</b> was 84. 32 % and 0. 81, respectively. The results indicated {{that it is possible to}} produce accurate change detection maps with the help of mCVA and SPOT 4 &SPOT 5 satellite data...|$|R
40|$|We use the {{bipartite graph}} {{representation}} of longitudinally linked employer-employee data, {{and the associated}} projections onto the employer and employee nodes, respectively, to characterize the set of potential statistical summaries that the trusted custodian might produce. We consider noise infusion as the primary confidentiality protection method. We show that a relatively straightforward extension of the dynamic noise-infusion method used in the U. S. Census Bureau’s Quarterly Workforce Indicators {{can be adapted to}} provide the same confidentiality guarantees for the graph-based statistics: all inputs have been modified by a minimum percentage deviation (i. e., no actual respondent data are used) and, as the number of entities contributing to a particular <b>statistic</b> increases, the <b>accuracy</b> of that <b>statistic</b> approaches the unprotected value. Our method also ensures that the protected statistics will be identical in all releases based on the same inputs. We acknowledge financial support from the U. S. Census Bureau and the National Science Foundation Grants SES- 9978093 and SES- 0427889 to Cornell University (Cornell Institute for Social and Economic Research), the National Institute on Aging Grant R 01 AG 018854 - 01, and the Alfred P. Sloan Foundation for LEHD infrastructure support. Abowd acknowledges additional funding through NSF Grants SES- 0922005, SES- 1042181, TC- 1012593 and SES- 1131848...|$|R
40|$|Ischemic Heart Disease (IHD) is {{difficult}} to diagnose {{since most of the}} symptoms and clinical presentations are similar to other diseases. It is a very common, harmful disease, which is identified mostly during the mortality of an individual. The objective is to build a clinical decision support system, which will diagnose the presence of IHD with an integrated automated classifier using Artificial Intelligence (AI) techniques. A retrospective data set that included 800 clinical cases was taken for the work. A total of 88 sets were discarded during pre-processing. Tests were run on 712 cases using the Weka classifiers available in Weka 3. 7. 0. Out of 113 classifiers, 16 were identified to be the best based on the following parameters: sensitivity, specificity, <b>accuracy,</b> F-measure, kappa <b>statistic,</b> correctly classified cases, time taken to run the model, and the Receiver Operating Characteristic (ROC) curve. The diagnoses made by the Clinical Decision Support System (CDSS) were compared with those made by physicians during patient consultations. The KSTAR algorithm showed the best diagnoses with the highest accuracy 97. 32 %, sensitivity 98 %, specificity 97 % kappa 0. 95, and ROC 0. 995. The authors thus conclude that a CDSS can be developed to assist expert physicians in separating the positive and the negative cases of heart disease...|$|R
40|$|Land cover {{analysis}} {{plays an}} important role in many environmental applications nowadays. Satellite images can provide a high spatial resolution land cover map. This study investigated the potential of using digital camera for land cover mapping over Kedah, Malaysia using ALOS AVNIR data. The standard supervised classification techniques, such as the maximum likelihood, minimum distance-to-mean, parallelepiped and neural network, were applied to the multispectral satellite images. The land cover information was extracted from the digital spectral bands using PCI Geomatica 9. 1. 8 software package. The neural network classifier was performed to the satellite images and the results were compared with four standard supervised classification techniques, such as the maximum likelihood, minimum distance-to-mean and parallelepiped. Training sites were selected within each scene and land cover classes were assigned to each classifier. High overall accuracy (> 90 %) and Kappa coefficient (> 0. 90) was achieved by the neural network classifier in this study. The best supervised classifier was chosen based on the highest overall <b>accuracy</b> and Kappa <b>statistic.</b> The results produced by this study indicated that the neural network classifier could be used to classify the land features into a land cover map. This study suggested that the land cover types of Kedah, Malaysia can be accurately mapped using ALOS data...|$|R
40|$|AbstractThe {{objective}} {{of this research was}} to evaluate the accuracy of random forest classification rule using object based image analysis (OBIA) application (eCognition Developer) and the results were compared with common pixel-based classification algorithm (maximum likelihood/ML) for mangrove land cover mapping in Kembung River, Bengkalis Island, Indonesia. Seven data input model derived from Landsat 5 TM bands, ALOS PALSAR FBD, and spectral transformations (NDVI, NDWI, NDBI) were examined by both classifiers. Feature objects statistical parameters were selected and implemented on random forest classifier. Overall accuracy (OA) as well as user and producer <b>accuracies</b> and Kappa <b>statistic</b> were used to compare classification results. Our results showed that the more data model used produced higher overall accuracy and kappa statistics for RF classifier. For each data input model, random forest classifier has higher overall accuracy than maximum likelihood. The best mangrove discrimination in RF classifier was achieved when the combination of Landsat 5 TM, SAR, and spectral transformation were used, while in ML classifier, the best mangrove discrimination was achieved when the combination of Landsat 5 TM and ALOS PALSAR was used. The overall accuracy achieved by RF classifier was 81. 1 % and 0. 76 for Kappa statistic. Meanwhile, for ML classifier, the overall accuracy achieved was 77. 7 % and 0. 71 for Kappa statistic...|$|R
