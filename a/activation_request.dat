7|30|Public
50|$|As the {{transfer}} of an <b>activation</b> <b>request</b> usually happens encrypted or at least obfuscated, the user cannot see or check if additional data from his/her machine gets transferred, creating privacy concerns.|$|E
50|$|The {{males who}} {{do not belong to}} the reserve may only be {{activated}} in case of a full mobilization, and those rank-and-file personnel who have fulfilled 50 years of age only with a specific parliamentary decision. In addition to the mandatory refresher exercises, the reservist may be activated for voluntary exercises, but participation in these is, as name suggests, voluntary. However, when the reservist answers affirmatively to a voluntary <b>activation</b> <b>request,</b> he becomes {{an active member of the}} military for the duration of the exercise, with same rights and obligations as a mandatorily activated reservist, though with only an allowance corresponding only to the conscript's lowest allowance. The reservists who have been activated mandatorily for any reason receive a taxable pay of 58.85 - 64.50 euros per diem, based on their rank and a taxfree allowance corresponding to the lowest conscript's allowance. Similarly to the induction, an activated reservist becomes a military person at the moment when he was ordered to report to service. Disobeying an activation order is prosecuted as an absence without leave or as desertion, depending on the length of absence.|$|E
40|$|Abstract—In {{this work}} a control system for {{restoration}} reserve providers is proposed in which optimal biddings of restoration reserve capacity are made {{based on the}} predicted flexibility of the reserve resources within the portfolio of the reserve provider. It is assumed that the gate closure time for submitting reserve capacity bids is 1 hour before activation time. The reserve capacity bids need to be formed so that activation of the capacity is always feasible, irrespective of the consumption of the portfolio before an <b>activation</b> <b>request.</b> The determination of the optimal reserve capacity bids is only based on aggregated flexibility constraint information received by the individual flexible resources within the portfolio of the reserve provider. No further resource-specific information is {{used to determine the}} optimal reserve capacity bid. The activation and dispatch of the required power consumption at real time is done through a market-based multi-agent control system. A simulation example, in which the reserve capacity of a portfolio of batteries is simulated, proves the feasibility of the proposed approach and shows that a high precision of the portfolio response can be obtained. I...|$|E
5000|$|The Associated Press also {{reported}} in 2007 that some users {{were unable to}} activate their phones because, according to AT&T, [...] "a high volume of <b>activation</b> <b>requests</b> was taxing the company's computer servers." [...] On Oct 29, 2007, the Usenet newsgroup misc.phone.mobile.iphone was created.|$|R
50|$|The DHNetwork is {{composed}} of several members who form Solution Teams when the network is activated. DHNetwork Coordinators review <b>activation</b> <b>requests</b> and rapidly liaise with the different volunteer & technical teams {{who are members of}} Digital Humanitarians to build a Solution Team best able to act on a request. The Coordinators aim to provide a response to every request within 24 hours.|$|R
5000|$|Hatsune Miku {{remained}} {{the most popular}} vocal for the software. Upon release she overloaded the yamaha server as users made <b>activation</b> <b>requests.</b> [...] As well as this, a backlog of 10,000 copies of her software was reported at one stage as supplies {{failed to meet the}} demand. [...] The Hatsune Miku v3 product was credited for an increase of Vocaloid related sales by 19.7% in 2014.|$|R
40|$|Purpose: The RENEB {{accident}} {{exercise was}} carried out in order to train the RENEB participants in coordinating and managing potentially large data sets that would be generated in case of a major radiological event. Materials and methods: Each participant was offered the possibility to activate the network by sending an alerting email about a simulated radiation emergency. The same participant had to collect, compile and report capacity, triage categorization and exposure scenario results obtained from all other participants. The exercise was performed over 27 weeks and involved the network consisting of 28 institutes: 21 RENEB members, four candidates and three non-RENEB partners. Results: The duration of a single exercise never exceeded 10 days, while the response from the assisting laboratories never came later than within half a day. During each week of the exercise, around 4500 samples were reported by all service laboratories (SL) to be examined and 54 scenarios were coherently estimated by all laboratories (the standard deviation from the mean of all SL answers for a given scenario category and a set of data was not larger than 3 patient codes). Conclusions: Each participant received training in both the role of a reference laboratory (activating the network) and of a service laboratory (responding to an <b>activation</b> <b>request).</b> The procedures in the case of radiological event were successfully established and tested...|$|E
40|$|The {{increasing}} {{availability of}} satellite imagery acquired by existing and new sensors allows {{a wide variety}} of new applications that depend on the use of diverse spectral and spatial resolution data sets. One of the pre-conditions for the use of hybrid image data sets is a consistent geo-correction capacity. We demonstrate how a novel fast template matching approach implemented on a graphics processing unit (GPU) allows us to accurately and rapidly geo-correct imagery in an automated way. The key difference with existing geo-correction approaches, which do not use a GPU, is the possibility to match large source image segments (8, 192 by 8, 192 pixels) with relatively large templates (512 by 512 pixels) significantly faster. Our approach is sufficiently robust to allow for the use of various reference data sources. The need for accelerated processing is relevant in our application context, which relates to mapping activities in the European Copernicus emergency management service. Our new method is demonstrated over an area northwest of Valencia (Spain) for a large forest fire event in July 2012. We use the Disaster Monitoring Constellation’s (DMC) DEIMOS- 1 and RapidEye imagery for the delineation of burnt scar extent. Automated geo-correction of each full resolution image set takes approximately one minute. The reference templates are taken from the TerraColor data set and the Spanish national ortho-imagery database, through the use of dedicated web map services. Geo-correction results are compared to the vector sets derived in the Copernicus emergency service <b>activation</b> <b>request...</b>|$|E
40|$|Natural {{disasters}} - {{including such}} events as tropical storms, earthquakes, floods, volcanic eruptions, and wildfires -effect {{hundreds of millions}} of people worldwide, and also cause billions of dollars (USD) in damage to the global economy. Remotely sensed data acquired by orbital sensor systems has emerged as a vital tool to identify the extent of damage resulting from a natural disaster, as well as providing near-real time mapping support to response efforts on the ground and humanitarian aid efforts. The International Space Station (ISS) is a unique terrestrial remote sensing platform for acquiring disaster response imagery. Unlike automated remote-sensing platforms it has a human crew; is equipped with both internal and externally-mounted remote sensing instruments; and has an inclined, low-Earth orbit that provides variable views and lighting (day and night) over 95 percent of the inhabited surface of the Earth. As such, it provides a useful complement to free-flyer based, sun-synchronous sensor systems in higher altitude polar orbits. While several nations have well-developed terrestrial remote sensing programs and assets for data collection, many developing nations do not have ready access to such resources. The International Charter, Space and Major Disasters (also known as the "International Disaster Charter", or IDC; [URL] addresses this disparity. It is an agreement between agencies of several countries to provide - on a best-effort basis - remotely sensed data of natural disasters to requesting countries in support of disaster response. The lead US agency for interaction with the IDC is the United States Geological Survey (USGS); when an IDC request or "activation" is received, the USGS notifies the science teams for NASA instruments with targeting information for data collection. In the case of the ISS, the Earth Sciences and Remote Sensing (ESRS) Unit, part of the Astromaterials Research and Exploration Science Directorate and supporting the ISS Program Science Office at NASA's Johnson Space Center, receives notification from the USGS and coordinates targeting and data collection with the NASA ISS sensor teams. If data is collected, it is passed back to the USGS for posting on their Hazards Data Distribution System and made available for download. The ISS International Partners (CSA, ESA, JAXA, Roscosmos/Energia) have their own procedures for independently supporting IDC activations using their assets on ISS, and there is currently no joint coordination with NASA ISS sensor teams. Following completion of ISS assembly, NASA remote sensing assets began collecting IDC response data in May 2012. The initial NASA ISS sensor systems available to respond to IDC activations included the ISS Agricultural Camera (ISSAC), an internal multispectral visible-near infrared wavelength system mounted in the Window Observational Research Facility, or WORF; the Crew Earth Observations (CEO) Facility, where the crew collects imagery through Station windows using off-the-shelf handheld digital visible-wavelength cameras; and the Hyperspectral Imager for the Coastal Oceans (HICO), a visible to near-infrared system mounted externally on the Japan Experiment Module Exposed Facility. The ISSAC completed its primary mission and was removed from the WORF in January 2013. It was replaced by the very high resolution ISS SERVIR Environmental Research and Visualization System (ISERV) Pathfinder, a visible-wavelength digital camera, telescope, and pointing system. Since the start of IDC response by NASA sensors on the ISS in May 2012 and as of this report, there have been eighty IDC activations; NASA sensor systems have collected data for twenty-three of these events. Of the twenty-three successful data collections, five involved 2 or more ISS sensor systems responding to the same event. Data has also been collected by International Partners in response to natural disasters, most notably JAXA and Roscosmos/Energia through the Urugan program. Data collected in response to IDC activations is delivered by the ISS sensor teams to the ESRS for quality review and transfer to the USGS, where it is ingested into the Hazards Data Distribution System, or HDDS ([URL] figure 1). This system allows the local agencies that issued the IDC <b>activation</b> <b>request</b> to review and download data. The data is then used to develop secondary products useful for humanitarian response such as flood maps. As of this report, approximately 1000 images collected by NASA ISS sensor systems have been downloaded from the HDDS, indicating that the ISS has assumed a valuable role in disaster response efforts. The ISS is also a unique platform in that it will have multiple users over its lifetime, and that no single remote sensing system has a permanent internal or external berth. This scheduled turnover provides for development of new remote sensing capabilities relevant to disaster response -as well as both research and applied science-and represents a significant contribution to continuance and enhancement of the NASA mission to investigate changes on our home planet...|$|E
40|$|Objectives: To adapt a {{tailored}} {{short message}} service (SMS) text message smoking cessation intervention (MiQuit) for use without active health professional endorsement in routine antenatal care settings, to estimate ‘real-world’ uptake and test the feasibility of its use. Design: Single-site service evaluation. Setting: A Nottinghamshire (UK) antenatal clinic. Participants: Pregnant women accessing the antenatal clinic (N= 1750) over 6 months. Intervention: A single-sheet A 5 leaflet provided in the women's maternity notes folder describing the MiQuit text service. Similar materials were left on clinic desks and noticeboards. Outcome measures: MiQuit <b>activation</b> <b>requests</b> and system interactions were logged for two time frames: 6 months (strict) and 8 months (extended). Local hospital data were used to estimate the denominator of pregnant smokers exposed to the materials. Results: During the strict and extended time frames, 13 and 25 <b>activation</b> <b>requests</b> were received, representing 3 % (95 % CI 2 % to 5 %) and 4 % (95 % CI 3 % to 6 %) of estimated smokers, respectively. Only 11 (44 %) of the 25 <b>requesting</b> <b>activation</b> sent a correctly formatted initiation text. Of those activating MiQuit, and invited to complete tailoring questions (used to tailor support), 6 (67 %) completed all 12 questions by text or website and 5 (56 %) texted a quit date to the system. Of the 11 activating MiQuit, 5 (45 %, 95 % CI 21 % to 72 %) stopped the programme prematurely. Conclusions: A low-intensity, cheap cessation intervention promoted at very low cost, resulted in a small but potentially impactful uptake rate by pregnant smokers...|$|R
40|$|International audienceIn this paper, we {{investigate}} a realistic and low-cost deployment of large scale direct control of inelastic home appliances whose energy demand cannot be shaped, but simply deferred. The {{idea is to}} exploit 1) some simple actuators {{to be placed on}} the electric plugs for connecting or disconnecting appliances with heterogeneous control interfaces, including non-smart appliances, and 2) the Internet connections of customers for transporting the <b>activation</b> <b>requests</b> from the actuators to a centralized controller. Our solution requires no interaction with home users: in particular, it does not require them to express their energy demand in advance. A queuing theory model is derived to quantify how many users should adopt this solution in order to control a significant aggregated power load without significantly impairing their quality of service...|$|R
30|$|Similar reports {{shortly after}} the Haiti crisis from the Harvard Humanitarian Initiative (2011) {{asserted}} {{that the lack of}} a formal contact point with the V&TCs only served to overwhelm overworked responders managing data within a fast-changing situation. Entities, such as the DHNetwork, prove that a successful operation requires a set of partnerships. The DHNetwork is a leaderless umbrella group coordinated by an annually rotating body of four volunteers from one of their partner organisations, who are obliged to act objectively {{as a representative of the}} entire consortium (DHNetwork 2016). Rather than fulfilling a leadership role, the coordinators are tasked with responding to <b>activation</b> <b>requests</b> made by external organisations requiring specialist assistance in an emergency and matching these requests to the appropriate DHNetwork partner group (DHNetwork 2016). Many open source humanitarian communities are adopting similar distributed models.|$|R
40|$|Today, {{ubiquitous}} mobile {{devices have}} not only arrived but entered the safety critical domain. There, systems {{are about to be}} controlled where human health or even human life is put at risk. For example, in automation systems first ideas surface to control parts of the system via a COTS smartphone. Another example is the idea to control the autonomous parking function of a car via a COTS smartphone too. As beneficial and convenient these ideas are on the first thought, on the second thought, dangers of these approaches become obvious. Especially in case of failures the system’s safety has to be maintained. The open question is how to achieve this mandatory requirement with COTS components, e. g. smartphones that are not developed following the development process necessary for safetycritical systems. This paper presents a concept to reliably detect human interaction while activating safety critical functions via COTS mobile devices. Thus a means is provided to detect erroneous <b>activation</b> <b>requests</b> for the safetycritical function...|$|R
40|$|International audienceThe idea of {{harnessing}} {{the inherent}} flexibility in demand of {{many types of}} electric loads has been largely discussed {{in the last years}} for coping with the need to maintain the energy demand-supply balance. In particular, the fine tuning of the operation conditions of different thermostatic loads (such as air-conditioning, refrigerators, etc.) has appeared as the most natural solution for load control with minimal user discomfort. In this paper we focus on an alternative approach: deploying simple open-loop control strategies for deferrable loads with minimal communication overhead. The idea is to send a multicast control message to a group of users, {{on the basis of the}} expected and desired load profiles, for probabilistically enabling or deferring the <b>activation</b> <b>requests</b> of a specific load type. The control law and the most important performance metrics can be easily derived analytically. Despite the simplicity of the approach, which requires minimal or null investments, our results show that significant load shifts can be achieved...|$|R
40|$|Nowadays, system {{architecture}} of the fifth generation (5 G) cellular system is becoming of increasing interest. To reach the ambitious 5 G targets, a dense base station (BS) deployment paradigm is being considered. In this case, the conventional always-on service approach may not be suitable due to the linear energy/density relationship when the BSs are always kept on. This suggests a dynamic on/off BS operation to reduce the energy consumption. However, this approach may create coverage holes and the BS activation delay in terms of hardware transition latency and software reloading could result in service disruption. To tackle these issues, we propose a predictive BS activation scheme under the control/data separation architecture (CDSA). The proposed scheme exploits user context information, network parameters, BS sleep depth and measurement databases to send timely predictive <b>activation</b> <b>requests</b> in advance before the connection is switched to the sleeping BS. An analytical model is developed and closed-form expressions are provided for the predictive activation criteria. Analytical and simulation {{results show that the}} proposed scheme achieves a high BS activation accuracy with low errors w. r. t. the optimum activation time...|$|R
40|$|Balancing {{energy demand}} and {{production}} {{is becoming a}} more and more challenging task for energy utilities {{also because of the}} larger penetration of renewable energies which are more difficult to predict and control. While the traditional solution is to dynamically adapt energy production to follow time-varying demand, a new trend is to drive demand itself. Most of the ongoing actions in this direction involve greedy energy consumers, like industrial plants, supermarkets or large buildings. Pervasive communication technologies may allow in the near future to push further the granularity of such approach, by having the energy utility interacting with residential appliances. In this paper we study large scale direct control of inelastic home appliances whose energy demand cannot be shaped, but simply deferred. Our solution does not suppose any particular intelligence at the appliances. The actuators are rather smart plugs [...] -simple devices with local communication capabilities that can be inserted between appliances' plugs and power sockets and are able to interrupt/reactivate power flow through the plug. A simple control message can be broadcast to a large set of smart plugs for probabilistically enabling or deferring the <b>activation</b> <b>requests</b> of a specific load type in order to satisfy a probabilistic bound on the aggregated power consumption. The control law can be easily derived analytically...|$|R
40|$|Partial and {{dynamically}} reconfigurable SRAM-based FPGAs (Field Programmable Gate Arrays) {{enable the}} implementation of reconfigurable systems hosting several applications simultaneously, which share the available resources according to the functional requirements that are present at any given moment. Time and space sharing strategies enabled the concept of virtual hardware, supporting the concurrent implementation of applications which would otherwise require far more complex resources. However, the performance of these applications (e. g. execution speed and reliability, activation delay) is directly influenced by {{the efficiency of the}} management strategies that allocate the logic space to the various functions that are waiting to be activated (each function requiring a specific amount of logic resources). Because the <b>activation</b> <b>requests</b> are in most cases not predictable, all resource allocation decisions have to be made online. The consequences of such working contexts are twofold: � All FPGA resources must be tested regularly, to exclude malfunctioning due to the allocation of faulty elements. Since the process of launching / halting active functions takes place asynchronously at any given moment, an online concurrent test scheme is the only way of ensuring reliable system operation and predictable fault detection latency; � As the resources are allocated to functions and later released, many small “islands ” of free resources are created. If these areas become too small, they will be left unused due to routing restrictions. The defragmentation of the FPG...|$|R
50|$|Applied {{examples}} {{that many people}} could encounter in their day-to-day lives include elevator call buttons and crosswalk buttons. The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial <b>activation</b> and the <b>request</b> being satisfied have no effect.|$|R
50|$|In 2017, an ''''Earn Money'''' button {{was added}} to the top right corner of Tune Player. The basic purpose of this button was to make the process of revenue {{generation}} on user content easier. With the button, users will be able to <b>request</b> <b>activation</b> of monetization program with only a click. This will provide an easy gateway for publishers to connect with Tune.pk.|$|R
5000|$|Tagès {{may be used}} to bind an {{activated}} game copy to {{the hardware}} configuration in the time the game was installed. This has led to issues in some games where even slightly modifying the hardware (e.g. replacing the graphic card) would require reactivating the game (either by using up one of the limited extra activations provided by the game publisher, or trying to <b>request</b> <b>activation</b> renewal from technical support).|$|R
50|$|A 1 ms frame {{carrying}} 144 bits of 2B+D data {{is mapped}} to 108 ternary symbols. These symbols are scrambled, with different scrambling codes {{for the two}} transmission directions, in order reduce correlation between transmitted and received signal. To this frame, an 11-symbol preamble and a symbol from the CL channel are added, yielding a frame size of 120 ternary symbols and a symbol rate of 120 kilobaud. The CL channel is used to <b>request</b> <b>activation</b> or deactivation of a loopback in either the NT1 or a line regenerator.|$|R
40|$|Anecdoctal {{evidence}} {{accumulated over}} almost 20 years {{has shown that}} many different cell types are killed by sustained exposure to high concentrations of extracellular ATP. The plasma membrane receptors involved have been pharmacologically characterized and cloned during the last 3 years, and named purinergic P 2 X. P 2 X receptors share an intriguing structural relatedness with Caenorhabditis elegans degenerins and mammalian amiloride-sensitive Na channels (ENaCs). Depending on the ATP dose, length of stimulation and receptor subtype, P 2 X receptor stimulation may cause necrosis or apoptosis. The intracellular pathways activated are poorly known, but the perturbation in intracellular ion homeostasis clearly plays a major role. ICE proteases (caspases) are also triggered, nonetheless their <b>activation</b> is not <b>requested</b> for ATP-dependent cell death. The physiological meaning of P 2 X receptor-dependent cytotoxicity is not understood, but an involvement in immune-mediated reactions is postulated...|$|R
50|$|Weather Forecast Offices (WFO) of the National Weather Service outline {{warnings}} for tornadoes {{and severe}} thunderstorms in polygonal shapes for their map-based weather hazard products, {{based on the}} projected path of a storm {{at the time of}} the warning's issuance as estimated by Doppler radar. Warnings were issued on a per-county basis before October 2007, and they are now usually delineated on maps in polygon shapes and in text by a sections of counties, although entire counties are sometimes included, especially if the total area of the division is small. Storm Prediction Center and other NWS products, as well as severe weather alert displays used by some U.S. television stations, highlight severe thunderstorm warnings with a yellow or orange polygon or filled county/parish outlines. Depending on the severity of the storm, some NWS offices may also <b>request</b> <b>activation</b> of the Emergency Alert System at their discretion.|$|R
5000|$|HTML e-mail {{messages}} can easily implement {{a form of}} [...] "phoning home". Images and other files required by the e-mail body may generate extra requests to a remote web server {{before they can be}} viewed. The IP address of the user's own computer is sent to the web server (an unavoidable process if a reply is required), and further details embedded in request URLs can further identify the user by e-mail address, marketing campaign etc. Such extra page resources have been referred to as [...] "web bugs" [...] and they {{can also be used to}} track off-line viewing and other uses of ordinary web pages. So as to prevent the <b>activation</b> of these <b>requests,</b> many e-mail clients do not load images or other web resources when HTML e-mails are first viewed, giving users the option to load the images only if the e-mail is from a trusted source.|$|R
30|$|This paper {{reports the}} {{transient}} photoluminescence (PL) properties of an InGaN/GaN multiple quantum well (MQW) light-emitting diode (LED) with green emission. Recombination of localized excitons was {{proved to be}} the main microscopic mechanism of green emission in the sample. The PL dynamics were ascribed to two pathways of the exciton recombination, corresponding to the fast decay and the slow decay, respectively. The origins of slow decay and fast decay were assigned to local compositional fluctuations of indium and thickness variations of InGaN layers, respectively. Furthermore, the contributions of two decay pathways to the green PL were found to vary at different emission photon energy. The fraction of fast decay pathway decreased with decreasing photon energy. The slow radiative PL from deep localized exciton recombination suffered less suppression from non-radiative delocalization process, for the higher <b>requested</b> <b>activation</b> energy. All these results supported a clear microscopy mechanism of excitation-emission process of the green MQW LED structure.|$|R
40|$|International audienceReactive {{mechanical}} grinding (MG under H 2) {{of magnesium}} powder improves the hydrogen sorption properties. The hydrogenation of Mg starts in situ during the milling process that allows suppressing the <b>activation</b> procedure generally <b>requested</b> for Mg. The addition of Co, which {{acts as a}} catalyst for the dissociation of H 2, also leads to an improvement of the hydrogen sorption properties (but a strong dependence upon the milling time is reported). The hydriding is determined to be a two-step process: nucleation and diffusion. A direct relationship exists between the nucleation duration and the specific surface. A critical milling time exists below which the diffusion process is improved and above which no more improvement is observed (the maximum internal stress in the powder is also reached at this critical time). The diffusion is controlled by the number of crystallites per particle which can be decreased by increasing the milling time up to 10 h. However, the sorption properties of Mg-Co mixtures are a little under those reported for MgH 2 -metal mixtures...|$|R
40|$|Reactive {{mechanical}} grinding (MG under H 2) {{of magnesium}} powder improves the hydrogen sorption properties. The hydrogenation of Mg starts in situ during the milling process that allows suppressing the <b>activation</b> procedure generally <b>requested</b> for Mg. The addition of Co, which {{acts as a}} catalyst for the dissociation of H 2, also leads to an improvement of the hydrogen sorption properties (but a strong dependence upon the milling time is reported). The hydriding is determined to be a two-step process: nucleation and diffusion. A direct relationship exists between the nucleation duration and the specific surface. A critical milling time exists below which the diffusion process is improved and above which no more improvement is observed (the maximum internal stress in the powder is also reached at this critical time). The diffusion is controlled by the number of crystallites per particle which can be decreased by increasing the milling time up to 10 h. However, the sorption properties of Mg–Co mixtures are a little under those reported for MgH 2 –metal mixtures. hal- 00070916, version 1 - 30 May 2012 1...|$|R
40|$|International audienceThe use of {{mechanical}} grinding (MG) under H 2 of magnesium powder improves the hydrogen sorption properties. The hydrogenation of Mg starts in situ during the milling process that allows suppressing the <b>activation</b> procedure generally <b>requested</b> for Mg. The {{effects of the}} addition of various elements or compounds have been studied. The hydriding is a two-step process: nucleation and diffusion. A direct relationship exists between the nucleation duration and the specific surface. A critical milling time exists below which the diffusion process is improved and above which no further improvement is observed (the maximum internal stress in the powder is also reached at this critical time). The diffusion {{is controlled by the}} number of crystallites per particle that can be reduced by increasing the milling time up to 10 h. The addition of Co (catalyst), YNi (hydrogen pump) or oxides (abrasive element and nucleation centre) leads to an improvement of the hydrogen sorption properties (but a strong dependence upon the milling time is reported). Finally, the sorption properties of our mixtures are comparable with thus reported for MgH 2 -metal mixtures...|$|R
50|$|Tornado {{warnings}} {{are generated}} via the Advance Weather Interactive Processing System (AWIPS) and then disseminated through various communication routes accessed {{by the media}} and various agencies, on the internet, to NOAA satellites, and on NOAA Weather Radio. Tornado sirens are also usually activated for the affected areas if present (the actual areas where sirens are activated may vary depending on the relay structure of a given jurisdiction's siren network, with some municipalities activating all sirens within their network even in areas not referenced as being included in the warning). Local police or fire departments may dispatch crews not assigned to an existing emergency call to travel within a designated area to warn residents to take tornado safety precautions if sirens are disabled due to technical problems or are not present, while automated phone calls may be made to residents for the same purpose in some areas should such disruptions occur. Additionally, if it is deemed necessary, the National Weather Service has the option of <b>requesting</b> <b>activation</b> of the Emergency Alert System to interrupt television and radio broadcasts to get the bulletin out quickly.|$|R
2500|$|After the Kent State shootings, the Northwestern {{student body}} and faculty senate voted to suspend classes {{starting}} May 6 and join a nationwide strike {{in protest of}} the Vietnam war and violence at Kent State. The student protesters demanded that campus security be disarmed, the University endowment divest itself from [...] "war stocks", and Northwestern's NROTC program be stripped of academic credit and its facilities turned into a childcare center. Much {{to the consternation of}} Evanston residents, student protesters also erected a barricade at the major intersection of Sheridan Road and Chicago Avenue, both major thoroughfares in Evanston. A May 8 rally at Dyche Stadium that attracted approximately 5,000 students and community members prompted Evanston city officials to <b>request</b> <b>activation</b> of the National Guard, but this potential conflict was avoided after the Guard were diverted away from the stadium. The Rebecca Crown center, Vogelback computing center, and Lunt Hall (housing the NROTC program) were closed following disturbances by student protestors. On May 13, a group of at least 33 students vandalized the NROTC's headquarters, but this was the only violent episode of the strike. The strike ended and campus re-opened for class on May 13, the longest span of time Northwestern has ever been closed.|$|R
5000|$|In {{addition}} to anti-war demonstrations, students also protested against the university's alleged {{complicity in the}} military industrial complex with regards to corporate and military recruitment on campus, military applications of research, and financial support by corporations and the military. Companies like Dow Chemical were targeted when they came on-campus to conduct interviews because of their involvement in the manufacture or research of war material and Navy ROTC activities were also frequent target of anti-war demonstrations.After the Kent State shootings, the Northwestern student body and faculty senate voted to suspend classes starting May 6 and join a nationwide strike in protest of the Vietnam war and violence at Kent State. The student protesters demanded that campus security be disarmed, the University endowment divest itself from [...] "war stocks", and Northwestern's NROTC program be stripped of academic credit and its facilities turned into a childcare center. Much {{to the consternation of}} Evanston residents, student protesters also erected a barricade at the major intersection of Sheridan Road and Chicago Avenue, both major thoroughfares in Evanston. A May 8 rally at Dyche Stadium that attracted approximately 5,000 students and community members prompted Evanston city officials to <b>request</b> <b>activation</b> of the National Guard, but this potential conflict was avoided after the Guard were diverted away from the stadium. The Rebecca Crown center, Vogelback computing center, and Lunt Hall (housing the NROTC program) were closed following disturbances by student protestors. On May 13, a group of at least 33 students vandalized the NROTC's headquarters, but this was the only violent episode of the strike. The strike ended and campus re-opened for class on May 13, the longest span of time Northwestern has ever been closed.|$|R
40|$|Ensuring that {{software}} can display different behavior in different use contexts requires adapting software at runtime in dynamically created scopes (e. g. in a thread, in a client session, in a collaboration). Context-Oriented Programming (COP) offers dedicated language constructs for performing such dynamically scoped adaptations. COP supports power-ful customizations such as concurrent customization where independent clients can concurrently customize the functionality {{of a shared}} compo-nent, and system-wide customizations that cross technical boundaries. However, like any dynamic software adaptation technique, COP hits a conceptual barrier when new variations of existing program entities are in-tegrated into a running system: if applied uncarefully, dynamically scoped adaptations may lead to faulty behavior especially when layer activation occurs at stages where the affected system components themselves are in a transitional, inconsistent state. Inherently, the ways of managing state consistency is highly specific to the application at hand and therefore re-quires additional application-specific logic from the system itself. What is needed is a consistency management framework that guides the program-mer in specifying this application-specific logic in a principled way. 1 Flavors of context-oriented programming Many COP languages exists to date. Althoug there may be different flavors of COP, a common goal of most COP languages is to enable activation of layers of behavior in a specific dynamic scope of the program, in parallel with other dynamic scopes of the program. For example, our research platform Lasagne [7] supports per-client <b>request</b> <b>activation</b> of aspectual layers. This allows different clients to customize the same component instance in parallel. Looking further however at the different flavors of COP, {{it can be seen}} that at least two types of dynamically scoped adaptations are supported [2]: loyal adaptations where layers, once activated, are continously activated in their cur-rent dynamic scope, versus prompt adaptations where layers can be flexibly activated and deactivated in the same dynamic scope. For instance, Lasagne is a loyal COP language. The notion of per-request activation implies that th...|$|R
40|$|Cloud {{computing}} {{systems have}} emerged as a new paradigm of computing systems by providing on demand based services which utilize large size computing resources. Service providers offer Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) to users depending on their demand and users pay only for the user resources. The Cloud system has become a successful business model and is expanding its scope through collaboration with various applications such as big data processing, Internet of Things (IoT), robotics, and 5 G networks. Cloud computing systems are composed {{of large numbers of}} computing, network, and storage devices across the geographically distributed area and multiple tenants employ the cloud systems simultaneously with heterogeneous resource requirements. Thus, efficient operation of cloud computing systems is extremely difficult for service providers. In order to maximize service providers 2 ̆ 7 profit, the cloud systems should be able to serve large numbers of tenants while minimizing the OPerational EXpenditure (OPEX). For serving as many tenants as possible tenants using limited resources, the service providers should implement efficient resource allocation for users 2 ̆ 7 requirements. At the same time, cloud infrastructure consumes a significant amount of energy. According to recent disclosures, Google data centers consumed nearly 300 million watts and Facebook 2 ̆ 7 s data centers consumed 60 million watts. Explosive traffic demand for data centers will keep increasing because of expansion of mobile and cloud traffic requirements. If service providers do not develop efficient ways for energy management in their infrastructures, this will cause significant power consumption in running their cloud infrastructures. In this thesis, we consider optimal datasets allocation in distributed cloud computing systems. Our objective is to minimize processing time and cost. Processing time includes virtual machine processing time, communication time, and data transfer time. In distributed Cloud systems, communication time and data transfer time are important component of processing time because data centers are distributed geographically. If we place data sets far from each other, this increases the communication and data transfer time. The cost objective includes virtual machine cost, communication cost, and data transfer cost. Cloud service providers charge for virtual machine usage according to usage time of virtual machine. Communication cost and transfer cost are charged based on transmission speed of data and data set size. The problem of allocating data sets to VMs in distributed heterogeneous clouds is formulated as a linear programming model with two objectives: the cost and processing time. After finding optimal solutions of each objective function, we use a heuristic approach to find the Pareto front of multi-objective linear programming problem. In the simulation experiment, we consider a heterogeneous cloud infrastructure with five different types of cloud service provider resource information, and we optimize data set placement by guaranteeing Pareto optimality of the solutions. Also, this thesis proposes an adaptive data center activation model that consolidates adaptive activation of switches and hosts simultaneously integrated with a statistical request prediction algorithm. The learning algorithm predicts user requests in predetermined interval by using a cyclic window learning algorithm. Then the data center activates an optimal number of switches and hosts in order to minimize power consumption that is based on prediction. We designed an adaptive data center activation model by using a cognitive cycle composed of three steps: data collection, prediction, and <b>activation.</b> In the <b>request</b> prediction step, the prediction algorithm forecasts a Poisson distribution parameter lambda in every determined interval by using Maximum Likelihood Estimation (MLE) and Local Linear Regression (LLR) methods. Then, adaptive activation of the data center is implemented with the predicted parameter in every interval. The adaptive activation model is formulated as a Mixed Integer Linear Programming (MILP) model. Switches and hosts are modeled as M/M/ 1 and M/M/c queues. In order to minimize power consumption of data centers, the model minimizes the number of activated switches, hosts, and memory modules while guaranteeing Quality of Service (QoS). Since the problem is NP-hard, we use the Simulated Annealing algorithm to solve the model. We employ Google cluster trace data to simulate our prediction model. Then, the predicted data is employed to test adaptive activation model and observed energy saving rate in every interval. In the experiment, we could observe that the adaptive activation model saves 30 to 50...|$|R

