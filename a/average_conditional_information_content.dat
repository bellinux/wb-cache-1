1|10000|Public
5000|$|... {{conditional}} entropy, mean conditional information content, <b>average</b> <b>conditional</b> <b>information</b> <b>content</b> H(X|Y) ...|$|E
40|$|We {{describe}} a compression-based distance for genomic sequences. Instead {{of using the}} usual conjoint <b>information</b> <b>content,</b> as in the classical Normalized Compression Distance (NCD), it uses the <b>conditional</b> <b>information</b> <b>content.</b> To compute this Normalized Conditional Compression Distance (NCCD), we need a normal conditional compressor, that we built using a mixture of static and dynamic finite-context models. Using this approach, we measured chromosomal distances between Hominidae primates and also between Muroidea (rat and mouse), observing several insights of evolution that so far have not {{been reported in the}} literature. Comment: Full version of DCC 2014 paper "A conditional compression distance that unveils insights of the genomic evolution...|$|R
40|$|Given {{a list of}} N {{states with}} probabilities 0 <p_ 1 ≤ [...] . ≤ p_N, the <b>average</b> <b>conditional</b> {{algorithmic}} <b>information</b> I̅ to specify one of these states obeys the inequality H≤I̅<H+O(1), where H=-∑ p_j_ 2 p_j and O(1) is a computer-dependent constant. We show how any universal computer can be slightly modified {{in such a way}} that the inequality becomes H≤I̅<H+ 1, thereby eliminating the computer-dependent constant from statistical physics. Comment: 15 pages in REVTEX 3. 0, 3 postscript figures in uuencoded format, submitted to Physical Review...|$|R
40|$|Given {{a list of}} N {{states with}} probabilities 0 < p 1 ≤ · · · ≤ pN, the <b>average</b> <b>conditional</b> {{algorithmic}} <b>information</b> Ī to specify one of these states obeys the inequality H ≤ Ī < H + O(1), where H = − ∑ pj log 2 pj and O(1) is a computer-dependent constant. We show how any universal computer can be slightly modified {{in such a way}} that the inequality becomes H ≤ Ī < H + 1, thereby eliminating the computer-dependent constant from statistical physics. I...|$|R
40|$|If the <b>conditional</b> <b>information</b> of a {{classical}} probability distribution of three random variables is zero, then it obeys a Markov chain condition. If the <b>conditional</b> <b>information</b> {{is close to}} zero, then {{it is known that}} the distance (minimum relative entropy) of the distribution to the nearest Markov chain distribution is precisely the <b>conditional</b> <b>information.</b> We prove here that this simple situation does not obtain for quantum <b>conditional</b> <b>information.</b> We show that for tri-partite quantum states the quantum <b>conditional</b> <b>information</b> is always a lower bound for the minimum relative entropy distance to a quantum Markov chain state, but the distance can be much greater; indeed the two quantities can be of different asymptotic order and may even differ by a dimensional factor. Comment: 14 pages, no figures; not for the feeble-minde...|$|R
40|$|This study {{examines}} {{the effects of}} systemic risk on global hedge fund returns. We consider systemic risk as a <b>conditional</b> <b>information</b> variable to predict the underlying exposures to various asset market returns and risk factors. This {{study examines}} a proxy for global systemic risk employed by investment professionals known as the Treasury/Eurodollar (TED) spread. The findings reveal that increases in systemic risk causes some hedge fund investment styles to dynamically reduce their equity and stock momentum exposures while others increase their exposures to investment grade bonds and commodities. The <b>information</b> <b>content</b> of systemic risk via the TED spread assists us in better understanding the behaviour of global hedge fund returns. Griffith Business School, Department of Accounting, Finance and EconomicsFull Tex...|$|R
3000|$|..., respectively, the <b>average</b> <b>conditional</b> POC is {{obtained}} as p 1 (collision| 1 visible) = { 23.6 %, 1.53 %}.|$|R
40|$|We study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy {{that hold}} for {{distributions}} whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot {{be extended to}} any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: Submitted to the IEEE Transactions on Information Theor...|$|R
40|$|The aim of {{this paper}} is to present, by axiomatic way, an idea about the general <b>conditional</b> <b>information</b> of a single, fixed fuzzy set when the {{conditioning}} fuzzy event is variable. The properties of this <b>conditional</b> <b>information</b> are translated in a system of functional equations. Some classes of solutions of this functional system are founded...|$|R
3000|$|... is 14.5 and 12.2 %, respectively. So, the <b>average</b> <b>conditional</b> POC is p 1 (collision| 1 visible) = { 23.6 %, 1.53 %}.|$|R
5000|$|... is the <b>conditional</b> <b>information</b> entropy of the {{sequence}} of random variables. Equivalently, one has ...|$|R
40|$|We {{discuss the}} notions of mutual <b>information</b> and <b>conditional</b> <b>information</b> for noncomposite systems, {{classical}} and quantum; both the mutual <b>information</b> and the <b>conditional</b> <b>information</b> {{are associated with the}} presence of hidden correlations in the state of a single qudit. We consider analogs of the entanglement phenomena in the systems without subsystems related to strong hidden quantum correlations. Comment: 12 page...|$|R
2500|$|The {{conditional}} entropy or conditional uncertainty of [...] given random variable [...] (also called the equivocation of [...] about [...] ) is the <b>average</b> <b>conditional</b> entropy over : ...|$|R
40|$|We {{present an}} {{extension}} of the well-known information bottleneck framework, called <b>conditional</b> <b>information</b> bottleneck, which takes negative relevance information into account by maximizing a <b>conditional</b> mutual <b>information</b> score. This general approach can be utilized in a data mining context to extract relevant information that {{is at the same time}} novel relative to known properties or structures of the data. We present possible applications of the <b>conditional</b> <b>information</b> bottleneck in information retrieval and text mining for recovering non-redundant clustering solutions, including experimental results on the WebKB data set which validate the approach...|$|R
40|$|To {{appear in}} IEEE Transactions on Information Theory. An Early Access article is {{available}} in IEEE Xplore {{in advance of the}} final print version. International audienceWe study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy that hold for distributions whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot be extended to any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
40|$|International audienceIn 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we prove that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
40|$|In 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we show that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: v 4 : substantial corrections; 13 page...|$|R
40|$|The {{ability to}} {{anticipate}} forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations {{are critical to}} the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the <b>conditional</b> probability (and <b>information</b> <b>content)</b> of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation {{was found between the}} probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400 - 450 ms), (ii) beta band (14 - 30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity...|$|R
40|$|This paper {{presents}} a dual method of closed-form analysis and lightweight simulation that enables {{an evaluation of}} the performance of mobile ad hoc networks that is more realistic, efficient, and accurate than those found in existing publications. Some features accommodated by the new analysis are shadowing, exclusion and guard zones, and distance-dependent fading. Three routing protocols are examined: least-delay, nearest-neighbor, and maximum-progress routing. The tradeoffs among the path reliabilities, <b>average</b> <b>conditional</b> delays, <b>average</b> <b>conditional</b> number of hops, and area spectral efficiencies are examined. Comment: 6 pages, 6 figures, to appear in IEEE Military Commun. Conf. (MILCOM), 201...|$|R
5000|$|Like mutual <b>information,</b> <b>conditional</b> mutual <b>information</b> can be {{expressed}} as a Kullback-Leibler divergence: ...|$|R
40|$|The <b>Average</b> <b>Conditional</b> Exceedance Rate and Peak Over Threshold Markov Chain Monte Carlo are two {{extreme value}} {{statistical}} methods, compared in this work. They are tested for both extrapolations and prediction intervals. The methods are compared for difference scenarios {{concluding that the}} Peak Over Threshold Markov Chain Monte generally prefered better for prediction intervals. It {{also seems to be}} preferable for extrapolation of independent and identically distributed data, and data approximately so. There are some indications that the <b>Average</b> <b>Conditional</b> Exceedance Rate method maybe favorable for capturing the data dependencies and extrapolation for correlated observations, but more work is needed for a conclusive result on that aspect...|$|R
40|$|We {{show that}} the {{separability}} of states in quantum mechanics has a close counterpart in classical physics, and that <b>conditional</b> mutual <b>information</b> (a. k. a. <b>conditional</b> <b>information</b> transmission) is a very useful quantity {{in the study of}} both quantum and classical separabilities. We also show how to define entanglement of formation in terms of <b>conditional</b> mutual <b>information.</b> This paper lays the theoretical foundations for a sequel paper which will present a computer program that can calculate a decomposition of any separable quantum or classical state. 1...|$|R
40|$|The {{objective}} {{of this study is}} to investigate whether net income, net sale, and operating cash flow has incremental <b>information</b> <b>content.</b> This study also investigate whether net sales and operating cash flow have relative <b>information</b> <b>content.</b> Sample of this study is collected from Indonesian Capital Market (IDX). Collected sample is done during 2000 to 2004 in manufacturing industry. The result of this study is net income has not incremental <b>information</b> <b>content.</b> Net sale and operating cash flow have incremental <b>information</b> <b>content.</b> Net sale has not relative <b>information</b> <b>content</b> but operating cash flow has relative <b>information</b> <b>content...</b>|$|R
40|$|As digital terrain {{models are}} {{indispensable}} for visualizing and modeling geographic processes, terrain <b>information</b> <b>content</b> {{is useful for}} terrain generalization and representation. For terrain generalization, if the terrain information is considered, the generalized terrain may be of higher fidelity. In other words, the richer the terrain information at the terrain surface, the smaller the degree of terrain simplification. Terrain <b>information</b> <b>content</b> is also important for {{evaluating the quality of}} the rendered terrain, e. g., the rendered web terrain tile service in Google Maps (Google Inc., Mountain View, CA, USA). However, a unified definition and measures for terrain <b>information</b> <b>content</b> have not been established. Therefore, in this paper, a definition and measures for terrain <b>information</b> <b>content</b> from Digital Elevation Model (DEM, i. e., a digital model or 3 D representation of a terrain’s surface) data are proposed and are based on the theory of map <b>information</b> <b>content,</b> remote sensing image <b>information</b> <b>content</b> and other geospatial <b>information</b> <b>content.</b> The <b>information</b> entropy was taken as the information measuring method for the terrain <b>information</b> <b>content.</b> Two experiments were carried out to verify the measurement methods of the terrain <b>information</b> <b>content.</b> One is the analysis of terrain <b>information</b> <b>content</b> in different geomorphic types, and the results showed that the more complex the geomorphic type, the richer the terrain <b>information</b> <b>content.</b> The other is the analysis of terrain <b>information</b> <b>content</b> with different resolutions, and the results showed that the finer the resolution, the richer the terrain information. Both experiments verified the reliability of the measurements of the terrain <b>information</b> <b>content</b> proposed in this paper...|$|R
40|$|The {{interaction}} between a turbulent free-stream and a {{turbulent boundary layer}} is investigated through particle image velocimetry measurements. An `interaction layer' located between 0. 12 < y/δ< 0. 19, {{at the end of}} the log layer, is identified whereby the kinetic energy in this layer describes the flow above it. <b>Conditional</b> <b>averages</b> about the interaction layer indicate that it is home to peaks in the Reynolds stresses and that it is the location of a change in the vortical structure. Furthermore, the <b>conditional</b> <b>information</b> identifies that low kinetic energy deficit states in the interaction layer result in a more full boundary layer profile due to increased movement of the bulk flow towards the wall...|$|R
40|$|This study {{distinguishes between}} {{incremental}} and relative <b>information</b> <b>content.</b> Incremental comparisons ask whether one accounting measure provides <b>information</b> <b>content</b> beyond that provided by another, and apply when one measure {{is viewed as}} given and an assessment is desired regarding the incremental contribution of another (e. g., a supplemental disclosure). Relative comparisons ask which measure has greater <b>information</b> <b>content,</b> and apply when making mutually exclusive choices among alternatives, or when rankings by <b>information</b> <b>content</b> are desired (e. g., when comparing alternative disclosures). Questions of both incremental and relative <b>information</b> <b>content</b> arise frequently in accounting. However, few previous studies have examined questions of relative <b>information</b> <b>content.</b> Possible explanations include unfamiliarity with the relative versus incremental distinction, and the additional statistical complexity involved in testing for relative <b>information</b> <b>content.</b> First, we examine analytically the relation between incremental and relative <b>information</b> <b>content,</b> demonstrating that they address different research questions and require different tests for statistical significance. Second, we identify accounting research contexts in which questions of relative and incremental <b>information</b> <b>content</b> arise. Third, we propose a new regression-based test for relative <b>information</b> <b>content.</b> This test applies to both returns and valuation studies, generalizes to any number of predictor variables, {{and can be used}} in conjunction with White's (1980) adjustment for heteroskedasticity. Fourth, we illustrate tests for relative and incremental <b>information</b> <b>content</b> in a familiar research setting that compares the <b>information</b> <b>contents</b> of net income, cash flows, and net sales in 40 industries. link_to_subscribed_fulltex...|$|R
40|$|We study {{extremes}} of moving averages of totally skewed [alpha]-stable motion for [alpha] [epsilon] (1, 2]. Proofs use a new formula for conditional second moments of stable random variables. Extrema Local extrema [alpha]-stable process Moving average Non-anticipating moving <b>average</b> <b>Conditional</b> moment Totally skewed stable distribution...|$|R
40|$|In {{this paper}} we propose new nonparametric estimators {{for a family of}} <b>conditional</b> mutual <b>information</b> and divergences. Our estimators are easy to compute; they only use simple k nearest {{neighbor}} based statistics. We prove that the proposed <b>conditional</b> <b>information</b> and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well. ...|$|R
40|$|This paper {{presents}} our extractive summarization {{systems at}} the update summarization track of TAC 2009. This system {{is based on}} our newly developed document summarization framework under the theory of <b>conditional</b> <b>information</b> distance among many objects. The best summary is defined in this paper {{to be the one}} which has the minimum information distance to the entire document set. The best update summary has the minimum <b>conditional</b> <b>information</b> distance to a document cluster given that a prior document cluster has already been read. Experiments on the TAC dataset have proved that our method has got a good performance in many categories. ...|$|R
40|$|In {{this paper}} two notions of <b>information</b> <b>content</b> for the {{characteristic}} sequences of sets are compared. One is the minimal-program {{complexity of the}} sequences and represents a quantitative <b>information</b> <b>content,</b> {{and the other is}} the degree of unsolvability of the underlying set and represents a qualitative <b>information</b> <b>content.</b> The major conclusion from this work is that with few exceptions these measures of <b>information</b> <b>content</b> are unrelated. Various tradeoffs between these measures are also demonstrated...|$|R
40|$|A simpler {{approach}} to the characterization of vanishing <b>conditional</b> mutual <b>information</b> is presented. Some remarks are given as well. More specifically, relating the <b>conditional</b> mutual <b>information</b> to a commutator is a very promising {{approach to}}wards the approximate version of SSA. That is, it is conjectured that small <b>conditional</b> mutual <b>information</b> implies small perturbation of quantum Markov chain. Comment: LaTex, 9 pages. Minor modifications are made. Any comments are welcome...|$|R
5000|$|The {{stronger}} {{properties of}} the [...] quantities, which allow the definition of <b>conditional</b> <b>information</b> and mutual information from communication theory, may be very important in other applications, or entirely unimportant, depending on those applications' requirements.|$|R
40|$|Black hole {{is called}} optimal if <b>information</b> <b>content</b> is minimal at the University region, {{consisting}} of usual substance and one(n) black hole(s). Optimal black hole mass {{does not depend}} on the mass of the Universe region. Optimal black holes can exist when at least the two types of substance are available in the Universe: with non-linear and linear correspondence between <b>information</b> <b>content</b> and mass. <b>Information</b> <b>content</b> of optimal black hole is proportional to squared coefficient correlating <b>information</b> <b>content</b> with mass in usual substance and in inverse proportion to coefficient correlating <b>information</b> <b>content</b> with black hole mass. Concentration of mass in optimal black hole minimizes <b>information</b> <b>content</b> in the system "usual substance - black holes". Minimal <b>information</b> <b>content</b> of the Universe consisting of optimal black holes only is twice as less as <b>information</b> <b>content</b> available of the Universe of the same mass filled with usual substance only. Under the radiation temperature T ≈ 1 E + 12 K the mass of optimal black holes that emerged in the systems "radiation - black hole" is equal to the mass of optimal black holes that emerged in the systems "hydrogen (protons) - black hole". Comment: 15 page...|$|R
3000|$|..., {{which is}} {{statistically}} significant at any significance level. Therefore, {{there is evidence}} that the data are highly over-dispersed even after conditioning on the regressors. As far as the reporting process is concerned, this model predicts that the <b>average</b> <b>conditional</b> probability of reporting a committed crime, calculated as [...]...|$|R
40|$|Abstract Based on the {{indiscernible}} {{relation of}} rough set, {{the inevitability of}} superposition and inconsistency of data makes the reduction of attributes very important in information system. Rough set has difficulty in the difference of attribute reduction between consistent and inconsistent information system. In this paper, we propose the new uncertainty measure and attribute reduction algorithm by Bayesian posterior probability for correlation analysis between condition and decision attributes. We compare the proposed method and the <b>conditional</b> <b>information</b> entropy to address the uncertainty of inconsistent information system. As the result, our method has more accuracy than <b>conditional</b> <b>information</b> entropy in dealing with uncertainty via mutual information of conditio...|$|R
40|$|A {{measure of}} the <b>information</b> <b>content</b> of an {{evidence}} inducing a belief function or a possibility function is axiomatically defined. Its major property is to be additive for distinct evidences. Measures are developed of the <b>information</b> <b>content</b> of an evidence appropriate when belief or possibility functions are used. The additivity means that the <b>information</b> <b>content</b> derived from two distinct and non-conflictual evidences {{is the sum of}} the <b>information</b> <b>contents</b> of each evidence. Properties of these measures are studied. Refs. SCOPUS: NotDefined. jinfo:eu-repo/semantics/publishe...|$|R
