73|50|Public
2500|$|The {{compression}} of FASTA files requires a specific compressor to handle both channels of information: identifiers and sequence. For improved compression results, these are mainly divided in two streams where the compression is made assuming independence. For example, the algorithm MFCompress [...] performs lossless {{compression of}} these files using context modelling and <b>arithmetic</b> <b>encoding.</b>|$|E
50|$|The {{definition}} of the decoding process is designed to facilitate low-complexity implementations of <b>arithmetic</b> <b>encoding</b> and decoding. Overall, CABAC provides improved coding efficiency compared with CAVLC-based coding, {{at the expense of}} greater computational complexity.|$|E
5000|$|The {{compression}} of FASTA files requires a specific compressor to handle both channels of information: identifiers and sequence. For improved compression results, these are mainly divided in two streams where the compression is made assuming independence. For example, the algorithm MFCompress [...] performs lossless {{compression of}} these files using context modelling and <b>arithmetic</b> <b>encoding.</b>|$|E
5000|$|FLIF (Free Lossless Image Format) - a {{work-in-progress}} lossless {{image format}} which claims to outperform PNG, lossless WebP, lossless BPG and lossless JPEG2000 {{in terms of}} compression ratio. It uses the MANIAC (Meta-Adaptive Near-zero Integer <b>Arithmetic</b> Coding) entropy <b>encoding</b> algorithm, {{a variant of the}} CABAC (context-adaptive binary <b>arithmetic</b> coding) entropy <b>encoding</b> algogithm.|$|R
5000|$|Arithmetic encoding: An <b>arithmetic</b> coder <b>encodes</b> each bin {{according}} to the selected probability model. Note that there are just two sub-ranges for each bin (corresponding to [...] "0" [...] and [...] "1").|$|R
3000|$|..., are generated, {{we apply}} <b>arithmetic</b> code to <b>encode</b> each symbol to the {{specific}} codeword using the corresponding codebook, Q [...]...|$|R
5000|$|... for [...] Although it is {{the optimal}} symbol-by-symbol coding for such {{probability}} distributions, Golomb coding achieves better compression capability for the geometric distribution {{because it does not}} consider input symbols independently, but rather implicitly groups the inputs. For the same reason, <b>arithmetic</b> <b>encoding</b> performs better for general probability distributions, as in the last case above.|$|E
5000|$|Moreover, the claimed symbol probabilities were , but {{the actual}} {{frequencies}} in this example are [...] If the intervals are readjusted for these frequencies, the entropy of the message would be 4.755 bits and the same NEUTRAL NEGATIVE ENDOFDATA message could be encoded as intervals [...] and a binary interval of [...] This is also {{an example of how}} statistical coding methods like <b>arithmetic</b> <b>encoding</b> can produce an output message that is larger than the input message, especially if the probability model is off.|$|E
50|$|Range coding is {{very similar}} to <b>arithmetic</b> <b>{{encoding}},</b> except that encoding is done with digits in any base, instead of with bits, and so it is faster when using larger bases (e.g. a byte) at small cost in compression efficiency. After the expiration of the first (1978) arithmetic coding patent, range encoding appeared to clearly be free of patent encumbrances. This particularly drove interest in the technique in the open source community. Since that time, patents on various well-known arithmetic coding techniques have also expired.|$|E
40|$|This paper proposes low {{complexity}} codec for {{lossy compression}} on a sample hyperspectral image. These images have {{two kinds of}} redundancies: 1) spatial; and 2) spectral. A discrete cosine transform (DCT) - based Distributed Source Coding(DSC) paradigm with Arithmetic code for low complexity is introduced. Here, Set-partitioning based approach is applied to reorganize DCT coefficients into wavelet like tree structure as Setpartitioning works on wavelet transform, and extract the sign, refinement, and significance bitplanes. The extracted refinement bits are <b>Arithmetic</b> <b>encoded,</b> then by applying low density parity check based (LDPC-based) Slepian-Wolf coder is implement to our DSC strategy. Experimental results for SAMSON (Spectroscopic Aerial Mapping System with Onboard Navigation) data show that proposed scheme achieve peak {{signal to noise ratio}} and compression to a very good extent for water cube compared to building, land or forest cube...|$|R
3000|$|... are generated, {{we apply}} <b>arithmetic</b> code to <b>encode</b> each symbol to the {{specific}} codeword using the corresponding codebook at each stage. Cardinalities of different codebooks Q [...]...|$|R
40|$|We {{propose a}} {{progressive}} encoding method for geometric information of 3 D object model, which is represented with binary voxels. For progressive transmission, we create multi-resolution {{model of the}} 3 D models by pyramidal decomposition. However, a simple pyramidal decomposition, which decimates 3 D model by 3 D sampling, can not preserve the detailed structure of the 3 D model, which is very sensitive to human vision system, in low resolution model. The proposed algorithm creates adaptive pyramidal decomposition models for the detailed region in the 3 D model. First, comparing the surface normal, the detailed region is classified. The detailed region is described by smaller size voxels {{than that of the}} other region. Thus the detailed region can preserve higher resolution. For encoding, each resolution model is predicted from its lower resolution model, and the prediction errors are <b>arithmetic</b> <b>encoded.</b> It is demonstrated by simulation results that the proposed algorithm provides better coding gain than the conventional mesh-based algorithm, especially for natural objects. 1...|$|R
50|$|Every {{programmatic}} {{implementation of}} <b>arithmetic</b> <b>encoding</b> {{has a different}} compression ratio and performance. While compression ratios vary only a little (usually under 1%), the code execution time can vary {{by a factor of}} 10. Choosing the right encoder from a list of publicly available encoders is not a simple task because performance and compression ratio depend also on the type of data, particularly {{on the size of the}} alphabet (number of different symbols). One of two particular encoders may have better performance for small alphabets while the other may show better performance for large alphabets. Most encoders have limitations on the size of the alphabet and many of them are specialized for alphabets of exactly two symbols (0 and 1).|$|E
5000|$|Each {{universal}} code, {{like each}} other self-delimiting (prefix) binary code, has its own [...] "implied probability distribution" [...] given by p(i)=2−l(i) where l(i) is {{the length of the}} ith codeword and p(i) is the corresponding symbol's probability. If the actual message probabilities are q(i) and Kullback-Leibler divergence DKL(q||p) is minimized by the code with l(i), then the optimal Huffman code for that set of messages will be equivalent to that code. Likewise, how close a code is to optimal can be measured by this divergence. Since universal codes are simpler and faster to encode and decode than Huffman codes (which is, in turn, simpler and faster than <b>arithmetic</b> <b>encoding),</b> the universal code would be preferable in cases where DKL(q||p) is sufficiently small.https://web.archive.org/web/20080807041150/http://www.cs.tut.fi/~albert/Dev/pucrunch/ ...|$|E
5000|$|Arithmetic coding {{is a form}} of entropy {{encoding}} used in lossless data compression. Normally, a string of characters such as the words [...] "hello there" [...] is represented using a fixed number of bits per character, as in the ASCII code. When a string is converted to <b>arithmetic</b> <b>encoding,</b> frequently used characters will be stored with fewer bits and not-so-frequently occurring characters will be stored with more bits, resulting in fewer bits used in total. Arithmetic coding differs from other forms of {{entropy encoding}}, such as Huffman coding, in that rather than separating the input into component symbols and replacing each with a code, arithmetic coding encodes the entire message into a single number, an arbitrary-precision fraction q where 0.0 ≤ q < 1.0. It represents the current information as a range, defined by two numbers. Recent Asymmetric Numeral Systems family of entropy coders allows for faster implementations thanks to directly operating on a single natural number representing the current information.|$|E
40|$|Abstract The paper {{undertakes}} three interdisciplinary tasks. The {{first one}} consists in constructing a formal {{model of the}} basic arithmetic competence, that is, the competence sufficient for solving simple arithmetic story-tasks which do not require any mathematical mastery knowledge about laws, definitions and theorems. The second task is to present a generalized arithmetic theory, called the arithmetic of indexed numbers (INA). All models {{of the development of}} counting abilities presuppose the common assumption that our simple, folk <b>arithmetic</b> <b>encoded</b> linguistically in the mind is based on the linear number representation. This classical conception is rejected and a competitive hypothesis is formulated according to which the basic mature representational system of cognitive arithmetic is a structure composed of many numerical axes which possess a common constituent, namely, the numeral zero. Arithmetic of indexed numbers is just a formal tool for modelling the basic mature arithmetic competence. The third task is to develop a standpoint called temporal pluralism, which is motivated by neo-Kantian philosophy of arithmetic. ...|$|R
40|$|This {{paper is}} devoted to the {{complexity}} analysis of certain uniformity properties owned by all known symbolic methods of parametric polynomial equation solving (geometric elimination). It is shown that any parametric elimination procedure which is parsimonious with respect to branchings and divisions must necessarily have a non-polynomial sequential time complexity, even if highly ecient data structures (as e. g. the <b>arithmetic</b> circuit <b>encoding</b> of polynomials) are used...|$|R
40|$|We {{consider}} {{the problem of}} solving floating-point constraints obtained from software verification. We present UppSAT [...] - a new implementation of a systematic approximation refinement framework [ZWR 17] as an abstract SMT solver. Provided with an approximation and a decision procedure (implemented in an off-the-shelf SMT solver), UppSAT yields an approximating SMT solver. Additionally, UppSAT includes a library of predefined approximation components which can be combined and extended to define new encodings, orderings and solving strategies. We propose that UppSAT {{can be used as}} a sandbox for easy and flexible exploration of new approximations. To substantiate this, we explore several approximations of floating-point arithmetic. Approximations can be viewed as a composition of an encoding into a target theory, a precision ordering, and a number of strategies for model reconstruction and precision (or approximation) refinement. We present encodings of floating-point arithmetic into reduced precision floating-point arithmetic, real-arithmetic, and fixed-point <b>arithmetic</b> (<b>encoded</b> in the theory of bit-vectors). In an experimental evaluation, we compare the advantages and disadvantages of approximating solvers obtained by combining various encodings and decision procedures (based on existing state-of-the-art SMT solvers for floating-point, real, and bit-vector arithmetic) ...|$|R
5000|$|Number representation: Let us {{consider}} {{an example of}} a system which gets one of its data from a sensor. Most of the times, the sensor may be measuring some noise and for this example, let {{us consider}} that the values being measured are (0) and (-1) alternatively. For a 32-bit data bus, value 0 translates to 0x00000000 (0000 0000 0000 0000 0000 0000 0000 0000) while (-1) translates to 0xFFFFFFFF (1111 1111 1111 1111 1111 1111 1111 1111) in a 2’s complement representation. We see that the hamming distance in this case is 32 (since all 32-bits are changing their state). Instead, if we encode the bus to use signed integer representation (MSB is sign bit), we can represent 0 as 0x00000000 (0000 0000 0000 0000 0000 0000 0000 0000) and -1 as 0x80000001 (1000 0000 0000 0000 0000 0000 0000 0001) [...] In this case, we see that the hamming distance between the numbers is just 2. Hence by using a 2’s complement to signed <b>arithmetic</b> <b>encoding,</b> we are able to reduce the activity from a factor of 32 to 2.|$|E
40|$|The {{security}} of bio-metric information - finger print, retina mapping, DNA mapping {{and some other}} chemical and biological modified genes related information - transfer through low bandwidth and unreliable or covert channel is challenging task. Therefore, Security of biometric information is essential requirement in this fast developing communication world. Thus, in this paper, we propose efficient and effective mechanism for confidentiality and authentication for biometric information transmitted by using <b>arithmetic</b> <b>encoding</b> representation over low bandwidth and unreliable channel. It enhances the speed of encryption, decryption and authentication process. It uses <b>arithmetic</b> <b>encoding</b> scheme and public key cryptography e. g. modified version of RSA algorithm called RSA- 2 algorithm. Comment: 9 Pages, 2 Figure...|$|E
40|$|JPEG 2000 {{is a new}} {{standard}} that was developed {{to take over the}} widely used JPEG standard. In JPEG 2000, the use of wavelet transform and EBCOT greatly improves the quality of the image and the compression ratio. They do, however, take up a lot of processing time. We therefore implemented the processing intensive tasks, namely the reversible discrete wavelet transform (DWT), <b>arithmetic</b> <b>encoding,</b> and a portion of coefficient bit modeling with the reconfigurable processor DRP- 1 by NEC Electronics. By using the DRP- 1, performance of DWT, <b>arithmetic</b> <b>encoding,</b> and a portion of coefficient bit modeling improved by 6. 0, 1. 6, and 2. 0 times over the Texas Instruments DSP TMS 320 C 6713, respectively. These tasks were implemented with very little hardware by time-multiplexing the hardware resources available on the DRP- 1. We report on the methods used for the implementation and its results. ...|$|E
40|$|Orders of {{vanishing}} of zeros of zeta functions {{have much}} <b>arithmetic</b> information <b>encoded</b> in them. For the absolute zeta function, Dinesh Thakur gave sufficient {{conditions for the}} order of vanishing of its zeros when the finite field has two elements. Such conditions consider only principal ideals. This result was generalized by Thakur and Diaz-Vargas. Now the conditions involve not only the principal ideals but all the classes of ideals, still {{in the field of}} two elements. In this work, we generalize these results to arbitrary finite fields, using similar proofs of Thakur and Diaz-Vargas. Comment: Rocky Mountain Journal of Mathematics, to appea...|$|R
40|$|The second {{author has}} {{recently}} {{introduced a new}} class of L-series in the arithmetic theory of function fields over finite fields. We show that the value at one of these L-series <b>encode</b> <b>arithmetic</b> informations of certain Drinfeld modules defined over Tate algebras. This enables us to generalize Anderson's log-algebraicity Theorem and Taelman's Herbrand-Ribet Theorem. Comment: final versio...|$|R
40|$|Integral imaging is a {{technique}} capable of displaying 3 D images with continuous parallax in full natural color. It has been reported by many research groups and is becoming a viable alternative for 3 D television. With the development of 3 D integral imaging, image compression becomes mandatory for the storage and transmission of 3 D integral images. In this paper, {{the use of the}} lifting scheme in the application of a 3 D Wavelet Transform for the compression of 3 D Integral Images is proposed. The method requires the extraction of different viewpoint images from an integral image. The 3 D wavelet decomposition is computed by applying three separate 1 D transforms along the coordinate axes of the given sequence of Viewpoint Images. The spatial wavelet decompositions on a single viewpoint and on the inter-viewpoint images are performed using the biorthogonal Cohen-Debauchies-Feauveau 9 / 7 and 5 / 3 filter banks, respectively. All the resulting wavelet coefficients from application of the 3 D wavelet decomposition are <b>arithmetic</b> <b>encoded.</b> Simulations are performed on a set of different grey level 3 D Integral Images using a uniform scalar quantizer with deadzone. The results for the average of the four intensity distributions are presented and compared with previous use of 2 D DWT and 3 D-DCT based schemes. It was found that the algorithm achieves better rate-distortion performance and reconstructs the images with much better image quality at very low bit rates...|$|R
40|$|<b>Arithmetic</b> <b>encoding</b> is an {{essential}} class of coding techniques which {{have been widely used}} in various data compression systems and exhibited promising performance. One key issue of <b>arithmetic</b> <b>encoding</b> method is to predict the probability of the current symbol to be encoded from its context, i. e., the preceding encoded symbols, which usually can be executed by building a look-up table (LUT). However, the complexity of LUT increases exponentially with the length of context. Thus, such solutions are limited in modeling large context, which inevitably restricts the compression performance. Several recent convolutional neural network (CNN) and recurrent neural network (RNN) -based solutions have been developed to account for large context, but are still costly in computation. The inefficiency of the existing methods are mainly attributed to that probability prediction is performed independently for the neighboring symbols, which actually can be efficiently conducted by shared computation. To this end, we propose a trimmed convolutional network for <b>arithmetic</b> <b>encoding</b> (TCAE) to model large context while maintaining computational efficiency. As for trimmed convolution, the convolutional kernels are specially trimmed to respect the compression order and context dependency of the input symbols. Benefited from trimmed convolution, the probability prediction of all symbols can be efficiently performed in one single forward pass via a fully convolutional network. Experiments show that our TCAE attains better compression ratio in lossless gray image compression, and can be adopted in CNN-based lossy image compression to achieve state-of-the-art rate-distortion performance with real-time encoding speed...|$|E
40|$|This {{paper is}} {{intended}} to present a lossless image compression method based on multiple-tables arithmetic coding (MTAC) method to encode a gray-level image f. First, the MTAC method employs a median edge detector (MED) to reduce the entropy rate of f. The gray levels of two adjacent pixels in an image are usually similar. A base-switching transformation approach is then used to reduce the spatial redundancy of the image. The gray levels of some pixels in an image are more common than those of others. Finally, the <b>arithmetic</b> <b>encoding</b> method is applied to reduce the coding redundancy of the image. To promote high performance of the <b>arithmetic</b> <b>encoding</b> method, the MTAC method first classifies the data and then encodes each cluster of data using a distinct code table. The experimental results show that, in most cases, the MTAC method provides a higher efficiency in use of storage space than the lossless JPEG 2000 does. Copyright (c) 2009 Rung-Ching Chen et al...|$|E
40|$|Recently, {{biological}} techniques {{become more}} and more popular, as they are applied to many kinds of applications, authentication protocols, biochemistry, and cryptography. One of the most interesting biology techniques is deoxyribo nucleic acid and using it in such domains. Hiding secret data in deoxyribo nucleic acid becomes an important and interesting research topic. Some researchers hide the secret data in transcribed deoxyribo nucleic acid, translated ribo nucleic acid regions, or active coding segments where it doesn't mention to modify the original sequence, but others hide data in non-transcribed deoxyribo nucleic acid, non-translated ribo nucleic acid regions, or active coding segments. Unfortunately, these schemes either alter the functionalities or modify the original deoxyribo nucleic acid sequences. DNA has the ability to store large amount of digital data. This paper presents a method to hide an image in DNA sequence using <b>arithmetic</b> <b>encoding.</b> &# 13; &# 13; Keywords : DNA, mRNA, <b>Arithmetic</b> <b>Encoding</b> and Decodin...|$|E
40|$|This paper {{addresses}} {{the use of}} different coding methods for the <b>arithmetic</b> operators. Signal <b>encoding</b> is widely used to reduce the switching activity in buses. However, the signals need to be encoded and decoded since signal processing is executed in binary. To avoid this step, we investigate the viability of processing operators that use the same signal encoding as that used in the bus. Gray and...|$|R
30|$|For the m-ary (m is {{the number}} of symbols) {{adaptive}} <b>arithmetic</b> coding, the <b>encoded</b> symbol B is taken as the center; 2 δ symbols, which are located in the vicinity of B, would be chosen to add a large number λ {{on the basis of the}} original frequency, leading to rearrange the distribution of the model. λ is the cumulative counts of all symbols which can change the subinterval adaptively.|$|R
40|$|We {{describe}} an algebra of Edge-Valued Decision Diagrams (EVMDDs) to <b>encode</b> <b>arithmetic</b> functions and its implementation {{in a model}} checking library along with state-of-the-art algorithms for building the transition relation and the state space of discrete state systems. We provide efficient algorithms for manipulating EVMDDs and give upper bounds of the theoretical time complexity of these algorithms for all basic arithmetic and relational operators. We also demonstrate that the time complexity of the generic recursive algorithm for applying a binary operator on EVMDDs is no worse than that of Multi-Terminal Decision Diagrams. We have implemented a new symbolic model checker with the intention to represent in one formalism the best techniques available at the moment across a spectrum of existing tools: EVMDDs for <b>encoding</b> <b>arithmetic</b> expressions, identity-reduced MDDs for representing the transition relation, and the saturation algorithm for reachability analysis. We compare our new symbolic model checking EVMDD library with the widely used CUDD package and show that, in many cases, our tool is several orders of magnitude faster than CUDD...|$|R
40|$|This article proposes {{an image}} {{compression}} method based on multiple {{models for the}} probabilities of patterns (MMPP method) to encode a gray-level image f. First, the MMPP method employs a median edge detector (MED) to reduce the entropy of f. The intensities of two adjacent pixels in an image are usually close to each other. A base switching transformation (BST) is then used to lessen the spatial redundancy of f. Finally, the <b>arithmetic</b> <b>encoding</b> method is applied to further encode the data generated after the processing of MED and BST To reduce the memory space required to hold f, the MMPP method classifies the data and then compresses the data in each cluster by the <b>arithmetic</b> <b>encoding</b> method based on different probability tables. The experimental results show that mostly the MMPP method can provide better efficiency in memory space than the lossless JPEG 2000 method does. (C) 2009 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 19, 362 - 368, 2009; Published online in Wiley InterScience (www. interscience. wiley. com). DOI 10. 1002 /ima. 2021...|$|E
40|$|The {{growing trend}} of online image sharing and {{downloads}} today mandate {{the need for}} better encoding and decoding scheme. This paper looks into this issue of image coding. Multiple Description Coding is an encoding and decoding scheme that is specially designed in providing more error resilience for data transmission. The main issue of Multiple Description Coding is the lossy transmission channels. This work attempts {{to address the issue}} of re-constructing high quality image with the use of just one descriptor rather than the conventional descriptor. This work compare the use of Type I quantizer and Type II quantizer. We propose and compare 4 coders by examining the quality of re-constructed images. The 4 coders are namely JPEG HH (Horizontal Pixel Interleaving with Huffman Coding) model, JPEG HA (Horizontal Pixel Interleaving with <b>Arithmetic</b> <b>Encoding)</b> model, JPEG VH (Vertical Pixel Interleaving with Huffman Encoding) model, and JPEG VA (Vertical Pixel Interleaving with <b>Arithmetic</b> <b>Encoding)</b> model. The findings suggest that the use of horizontal and vertical pixel interleavings do not affect the results much. Whereas the choice of quantizer greatly affect its performance...|$|E
3000|$|... {{arithmetic}} classical decoding {{operations and}} depends only on q. The states number representing the <b>arithmetic</b> <b>encoding</b> machine of [15] increases for bigger values of L and U. Furthermore, the trellis construction needs {{the transmission of}} the source statistics as side information. Consequently, trellis-based decoding {{is very hard to}} apply with adaptive AC (the trellis changes with symbol probabilities). The proposed soft-input arithmetic decoder is very simple and can easily be extended to the adaptive context-based ACs.|$|E
40|$|AbstractBy using <b>arithmetic</b> circuits, <b>encoding</b> multivariate polynomials may be {{drastically}} {{more efficient}} than writing {{down the list of}} monomials. Via the study of two examples, we show however that such an encoding can be hard to handle with a Turing machine even if the degree of the polynomial is low. Namely we show that deciding whether the coefficient of a given monomial is zero is hard for P#P under strong nondeterministic Turing reductions. As a result, this problem does not belong to the polynomial hierarchy unless this hierarchy collapses. For polynomials over fields of characteristic k> 0, this problem is ModkP-complete. This gives a coNPModkP algorithm for deciding an upper bound on the degree of a polynomial given by a circuit in fields of characteristic k> 0...|$|R
3000|$|Given the encoded symbol in the {{previous}} frame, referred to as B, {{there is a large}} possibility of the input symbol C distributing around B. In G. 719, for the m-ary (m symbols) adaptive <b>arithmetic</b> coding, the <b>encoded</b> symbol B is the center; m/ 2 symbols, which are located in the range of B and m − B (provided by − B to avoid negative symbol), would be chosen to add [...]...|$|R
40|$|By using <b>arithmetic</b> circuits, <b>encoding</b> multivariate polynomials may be {{drastically}} {{more efficient}} than writing {{down the list of}} monomials. Via the study of two examples, we show however that such an encoding can be hard to handle with a Turing machine even if the degree of the polynomial is low. Namely we show that deciding whether the coefficient of a given monomial is zero is hard for P #P under strong nondeterministic Turing reductions. As a result, this problem does not belong to the polynomial hierarchy unless this hierarchy collapses. For polynomials over fields of characteristic k> 0, this problem is ModkP-complete. This gives a coNP ModkP algorithm for deciding an upper bound on the degree of a polynomial given by a circuit in fields of characteristic k> 0...|$|R
