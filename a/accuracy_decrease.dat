50|1374|Public
50|$|Technology of {{the guns}} {{themselves}} has {{had only a few}} innovations. Throughout the history of tank guns, they have almost exclusively been rifled weapons, however most modern tanks now use smoothbore guns. Rifling of the barrel imparts spin on the projectile, improving ballistic accuracy. The best traditional antitank weapons have been kinetic energy rounds, whose penetrating power and <b>accuracy</b> <b>decrease</b> with range. For longer ranges high explosive anti-tank rounds are more effective, but accuracy is limited; for extremely long ranges cannon-launched guided projectiles (CLGPs) are considered more accurate.|$|E
40|$|Abstract. This paper {{studies the}} basic {{principles}} of finite element method, then researches on the core algorithm of finite element method, according to Saint-Venant principle, introduce the concept of composite element, proves that if the element is composite element, the original integration is still able to calculate. Based on the feature that numerical integration solution is in fact the Gaussian integration points are calculated, we endow element internal with different material property values according to the different location artificially. This is the efficient and local <b>accuracy</b> <b>decrease</b> algorithm. The algorithm is able to simplify the work of mesh generation, improve the overall computing efficiency, is especially suitable for underground engineering, and we can quickly get the overall characteristics of the structure that we are most concerned about. Finally, builds the actual examples of underground engineering, uses the finite element software Ansys and the efficient and local <b>accuracy</b> <b>decrease</b> algorithm program to calculate the actual examples respectively, compares the results between them, expounds and set the analysis theory and relevant principles, makes conclusion: the efficient and local <b>accuracy</b> <b>decrease</b> algorithm can simplify mesh generation, improve the overall computing efficiency; and it can be used in underground engineering very well...|$|E
40|$|One of the {{drawbacks}} of airborne interferometric {{synthetic aperture radar}} is a relatively narrow swath compared to analogous space based systems. Increasing the swath with side view of the interferometer can be possible by increasing the flight altitude and angle of sight. At {{the same time the}} height measurement accuracy decreases due to slant range distance increase. Another possible way of swath increasing is using sector scan. The efficiency of sector scan using in interferometric synthetic aperture radar is analyzed in this paper. The mathematical model and geometry of height measurement at a sector scan have been discussed. There was made an analysis of the effect of terrain height and observation angle on received signal phase changing. Observation angle changing is shown to contribute to the phase changing. Potential height accuracy measurement was calculated. The calculation results show that increasing the observation angle reduces height accuracy measurement. The maximum <b>accuracy</b> <b>decrease</b> is obtained at the observation angle of 90 °. Despite height accuracy measurement decrease applying the sector scan allow to expand the swath. The <b>accuracy</b> <b>decrease</b> can be limited by selecting optimal parameters of scanning...|$|E
3000|$|... [34]. The {{algorithm}} was exhaustively {{tested for}} both high-density and long-term situations. Again, the tracking <b>accuracy</b> <b>decreases</b> {{if the number}} of targets and the number of frames increases simultaneously.|$|R
5000|$|Although the RVDT can {{theoretically}} operate between ±45°, <b>accuracy</b> <b>decreases</b> {{quickly after}} ±35°. Thus, its operational limits lie mostly within ±30°, but some up to ±40°. Certain types can operate up to ±60°.The {{advantages of the}} RVDT are: ...|$|R
5000|$|Carrasco, Evert, Chang and Katz’s 1995 {{research}} {{also included a}} free-viewing, fixed-viewing and fast-fixed-viewing condition. Free-viewing means that display was present until observers responded; fixed-viewing means that display was only present for 104msec, and finally fast-fixed means that the display was only present for 64msec. They found {{a main effect for}} display duration that performance is slower for the free-viewing than fixed-viewing conditions, but that <b>accuracy</b> <b>decreased</b> more in the fast-fixed than fixed viewing condition, while fixed-viewing was marginally less accurate than free-viewing. This indicates a main effect of eccentricity on accuracy across different display duration's, and indicates that the impact of the eccentricity effect on <b>accuracy</b> <b>decreases</b> when observers have longer to respond.|$|R
40|$|We {{consider}} {{the recognition of}} activities from passive enti-ties by analysing radio-frequency (RF) -channel fluctuation. In particular, {{we focus on the}} recognition of activities by active Software-defined-radio (SDR) -based Device-free Ac-tivity Recognition (DFAR) systems and investigate the lo-calisation of activities performed, the generalisation of fea-tures for alternative environments and the distinction be-tween walking speeds. Furthermore, we conduct case studies for Received Signal Strength (RSS) -based active and con-tinuous signal-based passive systems to exploit the <b>accuracy</b> <b>decrease</b> in these related cases. All systems are compared to an accelerometer-based recognition system...|$|E
40|$|The P 300 Speller {{has been}} {{proposed}} in 1988 by Farwell and Donchin [1]. In this Brain Computer Interface (BCI), a matrix of symbols is presented whose rows and columns are sequentially intensified. In this study, we investigated the influence of three stimulation parameters: the enhancement of the symbols while intensified, the inter-stimulus interval (ISI) and the reduction of flash duration. Results indicate that symbol enhancement increase P 300 amplitude and the ensuing classification accuracy by a Fisher LDA. P 300 amplitude and classification <b>accuracy</b> <b>decrease</b> with faster ISI. Finally, reduction of flash duration do not increase the P 300 amplitude but yielded a better classification accuracy. ...|$|E
40|$|A test on the {{numerical}} {{accuracy of the}} semiclassical approximation {{as a function of}} the principal quantum number has been performed for the Pullen [...] Edmonds model, a two [...] dimensional, non [...] integrable, scaling invariant perturbation of the resonant harmonic oscillator. A perturbative interpretation is obtained of the recently observed phenomenon of the <b>accuracy</b> <b>decrease</b> on the approximation of individual energy levels at the increase of the principal quantum number. Moreover, the accuracy provided by the semiclassical approximation formula is on the average the same as that provided by quantum perturbation theory. Comment: 12 pages, 5 figures (available upon request to the authors), LaTex, DFPD/ 93 /TH/ 47, to be published in Nuovo Cimento...|$|E
50|$|Comparative {{evaluations}} of plagiarism detection systems indicate that their performance {{depends on the}} type of plagiarism present (see figure). Except for citation pattern analysis, all detection approaches rely on textual similarity. It is therefore symptomatic that detection <b>accuracy</b> <b>decreases</b> the more plagiarism cases are obfuscated.|$|R
40|$|The aim of {{this paper}} is to {{simulate}} low Mach number unsteady viscous flows using a density-based finite volumes solver. This kind of solver is known to encounter some difficulties to simulate low Mach number flows in which the density is almost constant. Convergence fails or <b>accuracy</b> <b>decreases</b> when th...|$|R
30|$|Figure  5 {{demonstrates}} {{the value of}} q yields slight but discernible effect on average accuracy of 10 runs. As q increases to 0.8, the curve increases gradually till the highest. As q increases to 0.1, the average <b>accuracy</b> <b>decreases</b> sharply. The result again validates that WPTE (q =  0.8) is better than WPSE (q =  1).|$|R
40|$|We {{show that}} analysts who display more {{consistent}} forecast errors {{have a greater}} effect on stock prices than analysts who provide more accurate but less consistent forecasts. This result leads to three implications. First, consistent analysts {{are less likely to}} be demoted to a less prestigious brokerage house and are more likely to be named All Star analysts. Second, analysts strategically "lowball" (that is, deliver downward-biased forecasts) to increase their consistency. This is because lowballing gives management an easier target to beat and, in turn, management grants analysts greater access to company information. Finally, the benefits of both consistency and lowballing increase while those of <b>accuracy</b> <b>decrease</b> when institutional/sophisticated investors are more of a presence in the analyst’s audience...|$|E
40|$|International audienceIn this paper, {{we propose}} a Stochastic Selection {{strategy}} that ac- celerates the atom selection step of Matching Pursuit. This strategy consists of randomly selecting {{a subset of}} atoms and a subset of rows in the full dictionary at {{each step of the}} Matching Pursuit to obtain a sub-optimal but fast atom selection. We study the performance of the proposed algorithm in terms of approximation <b>accuracy</b> (<b>decrease</b> of the residual norm), of exact-sparse recovery and of audio declipping of real data. Numerical experiments show the relevance of the ap- proach. The proposed Stochastic Selection strategy is presented with Matching Pursuit but applies to any pursuit algorithms provided that their selection step is based on the computation of correlations...|$|E
40|$|An assay for rapidly {{detecting}} {{and determining}} sparse populations of vegetative bacteria or bacterial spores (minimum number about 500; 2 to 4 x Io-lOg. equivalent bacterial dry weight) is described. A sample is treated with 1251 -labelled purified homologous antibody, filtered and washed on a Millipore membrane filter, and the radioactivity of the separated labelled immune complex is measured. The assay is specific, accurate and completed within 8 to 10 min. Sensitivity and <b>accuracy</b> <b>decrease</b> if the assay {{is applied to}} samples that contain particulate matter that non-specifically attaches antibody and is retained by a membrane filter. This type of interference is decreased by pretreating samples with clarified normal rabbit serum {{for a few minutes}} before assay...|$|E
50|$|Under {{average flow}} conditions, the Palmer-Bowlus flume is {{accurate}} to within 3-5%. For lower flow rates - where the depth is low {{relative to the}} length of the flume - the <b>accuracy</b> <b>decreases</b> to 5-6%. This error, combined with typical installation / flow meter errors, means that overall site accuracy is somewhat less than other more common flumes.|$|R
40|$|This study {{investigated}} the effects of age of acquisition and syntactic complexity {{on the outcome of}} American Sign Language (ASL) acquisition. All the participants were ban deaf, had used ASL as a primary language for a minimum of 12 years, and began to acquire it at three different ages. The experimental task was grammatical judgement. In this task, the signer saw dynamic ASL sentences on a computer screen and decided whether they were grammatical or not. Response accuracy and latency were measured. The stimuli were 168 examples of six ASL syntactic structures ranging from simple to complex. Results showed that as age of acquisition increased, response <b>accuracy</b> <b>decreased</b> and response latency increased. Also, as ASL syntactic complexity increased, <b>accuracy</b> <b>decreased</b> and latency generally increased, independent of age of acquisition. The results provide additional evidence for the critical period for language acquisition and psycholinguistic evidence for previous linguistic descriptions of ASL syntax...|$|R
30|$|USPS: The best {{clustering}} {{performance was}} obtained with RICA feature learning and GNMF (NMI[*]=[*] 0.868, ACC[*]=[*] 0.926). The baseline GNMF performance was (NMI[*]=[*] 0.854, ACC[*]=[*] 0.828), which {{was higher than}} the baseline SPC-SYM performance (NMI[*]=[*] 0.842, ACC[*]=[*] 0.814). When the ICA BSS was applied to GNMF, {{the performance of the}} NMI increased, but the <b>accuracy</b> <b>decreased</b> (NMI[*]=[*] 0.854, ACC[*]=[*] 0.810).|$|R
40|$|A process corner {{monitoring}} circuit (PCMC) {{is presented in}} this work. The circuit generates a signal, the logical value of which depends on the process corner only. The signal {{can be used in}} both digital and analog circuits for testing and compensation of process variations (PV). The presented circuit uses only metal-oxide-semiconductor (MOS) transistors, which allow increasing its detection <b>accuracy,</b> <b>decrease</b> power consumption and area. Due to its simplicity the presented circuit can be easily modified to monitor parametrical variations of only n-type and p-type MOS (NMOS and PMOS, respectively) transistors, resistors, as well as their combinations. Post-layout simulation results prove correct functionality of the proposed circuit, i. e. ability to monitor the process corner (equivalently die-to-die variations) even in the presence of within-die variations...|$|E
40|$|In this paper, {{we propose}} a Stochastic Selection {{strategy}} that accelerates the atom selection step of Matching Pursuit. This strategy consists of selecting randomly {{a subset of}} atoms and a subset of rows in the full dictionary at {{each step of the}} Matching Pursuit to obtain a sub-optimal but fast atom selection. We study the performance of the proposed algorithm in terms of approximation <b>accuracy</b> (<b>decrease</b> of the residual norm), of exact-sparse recovery and of audio declipping of real data. Numerical experiments show the relevance of the approach. The proposed Stochastic Selection strategy is presented with Matching Pursuit but applies to any pursuit algorithms provided that their selection step is based on the computation of correlations. Index Terms — Sparsity; Pursuit Algorithm; Stochastic Procedure 1...|$|E
40|$|Permanent address A test on the {{numerical}} {{accuracy of the}} semiclassical approximation {{as a function of}} the principal quantum number has been performed for the Pullen– Edmonds model, a two–dimensional, non–integrable, scaling invariant perturbation of the resonant harmonic oscillator. A perturbative interpretation is obtained of the recently observed phenomenon of the <b>accuracy</b> <b>decrease</b> on the approximation of individual energy levels at the increase of the principal quantum number. Moreover, the accuracy provided by the semiclassical approximation formula is on the average the same as that provided by quantum perturbation theory. 2 Recently, there has been considerable renewed interest in the various aspects of the Semi–Classical Approximation (SCA), a powerful motivation behind that being the problem of the so–called quantum chaos (see for example references [1, 2, 3, 4, 5]). An important aspect is represented by the quantu...|$|E
30|$|Remote sensing {{image is}} photographed in complex environments, {{which not only}} {{includes}} light, viewpoint, and scale variant but also includes the influence of atmospheric and cloud cover, complex background, and similar ground environment. These disadvantages cause images blurring. The insufficient samples and obvious difference between the templates and the identified images will lead to detection <b>accuracy</b> <b>decreased</b> even a failure to match.|$|R
3000|$|Seventy-nine {{students}} completed pre- and post- Make-A-Dice tests (Grade 3 [*]=[*] 19, Grade 4 [*]=[*] 23; Grade 5 [*]=[*] 19; Grade 6 [*]=[*] 18; girls[*]=[*] 41; boys[*]=[*] 38). Attempt rates were high (pre-test M[*]=[*] 66 %, post-test M[*]=[*] 78 %; Table  2). Make-A-Dice <b>accuracy</b> <b>decreased</b> across tests (pre-test M[*]=[*] 66 %, post-test M[*]=[*] 60 %), F(1, 75)[*]=[*] 6.54, p[*]<[*] 0.05, η [...]...|$|R
40|$|Four {{groups of}} pigeons {{were trained to}} perform a delayed matching-to-sample task with a single delay of 0, 2, 4, or 6 s from the outset of {{training}}. The longer the training delay, the more sessions were required for all birds to reach {{the same level of}} response accuracy. Following initial training, five test sessions that included nonreinforced trials with delay intervals of 0, 2, 4, 6, 8, and 10 s were interspersed between training sessions. Unlike typical forgetting functions in which <b>accuracy</b> <b>decreases</b> monotonically with increasing delay, the forgetting functions from test sessions resembled generalization gradients with the peak of the functions occurring at the training delay. Following additional training for all birds with a 0 -s delay, forgetting functions decreased monotonically with increasing delay. The results suggested that remembering can be trained at a specific delay interval, and generalizes to similar delay intervals. Generalization along the temporal dimension of delay may contribute to typical forgetting functions in which <b>accuracy</b> <b>decreases</b> from 0 -s delay...|$|R
40|$|This paper aims to {{simultaneously}} accelerate and compress off-the-shelf CNN models via filter pruning strategy. The {{importance of each}} filter is evaluated by the proposed entropy-based method first. Then several unimportant filters are discarded to get a smaller CNN model. Finally, fine-tuning is adopted to recover its generalization ability which is damaged during filter pruning. Our method can {{reduce the size of}} intermediate activations, which would dominate most memory footprint during model training stage but is less concerned in previous compression methods. Experiments on the ILSVRC- 12 benchmark demonstrate the effectiveness of our method. Compared with previous filter importance evaluation criteria, our entropy-based method obtains better performance. We achieve 3. 3 x speed-up and 16. 64 x compression on VGG- 16, 1. 54 x acceleration and 1. 47 x compression on ResNet- 50, both with about 1 % top- 5 <b>accuracy</b> <b>decrease...</b>|$|E
40|$|The Gaussian {{graphical}} model (GGM) {{is one of}} the common dynamic modelling approaches in the construction of gene networks. In inference of this modelling the interaction between genes can be detected mainly via graphical lasso (glasso) or coordinate descent-based approaches. Although these methods are successful in moderate networks, their performances in <b>accuracy</b> <b>decrease</b> when the system becomes sparser. We here implement a particular type of polynomial transformations, called the Bernstein polynomials, of the network data in advance of their inference to raise the accuracy. From comparative Monte Carlo studies and real data analyses we show that these polynomials are successful in terms of the precision, specificity and F-measure when the scale-free networks are modelled via GGM and estimated by glasso, and accordingly they {{can be used as a}} preprocessing step in inference of these networks. (C) 2017 Statistical Society of Canad...|$|E
40|$|Neuroimaging {{research}} has shown localised brain activation to different facial expressions. This, along with the finding that schizophrenia patients perform poorly in their recognition of negative emotions, has raised the suggestion that patients display an emotion specific impairment. We propose that this asymmetry in performance reflects task difficulty gradations, rather than aberrant processing in neural pathways subserving recognition of specific emotions. A neural network model is presented, which classifies facial expressions {{on the basis of}} measurements derived from human faces. After training, the network showed an accuracy pattern closely resembling that of healthy subjects. Lesioning of the network led to an overall decrease in the network’s discriminant capacity, with the greatest <b>accuracy</b> <b>decrease</b> to fear, disgust and anger stimuli. This implies that the differential pattern of impairment in schizophrenia patients can be explained without having to postulate impairment of specific processing modules for negative emotion recognition...|$|E
5000|$|When {{analysing}} speech melody, {{rather than}} musical tones, <b>accuracy</b> <b>decreases.</b> This {{is not surprising}} given that speech does not stay at fixed intervals {{in the way that}} tones in music do. Johan 't Hart (1981) found that JND for speech averaged between 1 to 2 STs but concluded that [...] "only differences of more than 3 semitones play a part in communicative situations" [...] ('t Hart, 1981, page 811).|$|R
3000|$|... n {{considering}} {{an indoor}} navigation scenario. Tests {{were carried out}} to determine the accuracy and precision obtainable with internal and external sensors. In terms of the attitude and drift estimation with an updating interval equal to 1  s, 2 D accuracies of about 15  cm were obtained with the images. Residual benefits were also obtained, however, for large intervals, e.g. 2 and 5  s, where the <b>accuracies</b> <b>decreased</b> to 50  cm and 2.2  m, respectively.|$|R
40|$|ORIFI RIFICES, are {{potentially}} accurate furrow flow CES, devices. Flow measurement <b>accuracy</b> <b>decreases</b> with head loss. Orifice head loss will increase upstream infiltration. Thus, the head loss range must be constrained to maintain measurement accuracy. Design equations are developed for 3 -hole orifice plates which give a 23 : 1 flow range with a 5 : 1 head loss range. A portable differential point gauge can measure submerged flow head {{loss to the}} nearest millimeter...|$|R
40|$|Procedures for {{measuring}} the precision and accuracy with which the S- 193 scatterometer measured the background cross section of ground scenes are described. Homogeneous ground sites were selected, and data from Skylab missions were analyzed. The precision was expressed as the standard deviation of the scatterometer-acquired backscattering cross section. In special cases, inference of the precision of measurement was made by considering the total range from the maximum to minimum of the backscatter measurements within a data segment, rather than the standard deviation. For Skylab 2 and 3 missions a precision better than 1. 5 dB is indicated. This procedure indicates an accuracy of better than 3 dB for the Skylab 2 and 3 missions. The estimates of precision and accuracy given in this report are for backscattering cross sections from - 28 to 18 dB. Outside this range the precision and <b>accuracy</b> <b>decrease</b> significantly...|$|E
40|$|Novel generic {{frequency}} domain behavioural models are constructed for mixers and amplifiers. The modelling {{is based upon}} three concepts: time-invariant describing functions, linearisation and multidimensional curve fitting. The model parameters can be calculated based upon measured or simulated data. Model performance is illustrated on practical examples. I. Introduction RF/microwave circuit simulations can decrease the "time-to-market" and are as such valuable tools for circuit and system level designers. At present, the usefulness of simulators is limited in many practical cases by {{the characteristics of the}} component models that are used. If a model becomes too complex (e. g. many transistors in the component modelled) simulation speed as well as <b>accuracy</b> <b>decrease</b> significantly. It is also possible that {{it is very hard to}} construct an accurate model because of device physics, as is often the case with relatively new power transistor technologies (e. g. HBT). A solution is t [...] ...|$|E
40|$|Predicting the {{location}} of a mobile user in the near future can be used for {{a very large number of}} user-centered or crowd-centered ubiquitous applications. It is convenient for the discussion to think in terms of discrete locations driven by Points of Interest (POI) instead of absolute positions. We postulate that POI sequences are Markovian once the data is clustered by day of the week and time of the day. To prove our hypothesis we used a public dataset, used in a previous work [15]. In that paper the authors were able to predict {{the location}} of a user with 90 % to 70 % accuracy in five minutes and one hour time windows, respectively. With our approach, using Hidden Markov Models, we are able to predict the next POIs within seven hours without signifi-cant <b>accuracy</b> <b>decrease.</b> This result enables a large number of potential applications where the aggregate data of a single users conform the behavior of the crowd...|$|E
40|$|International audienceWe {{present a}} new {{approximate}} theory of scattering by randomly rough surfaces. The selvedge region {{is considered to}} be a layer with fluctuating index. A mean-field theory is used to derive the diffuse intensity. Comparisons with Monte Carlo simulations based on surface integral equations show that the technique can be used for rms heights as large as / 2 and for any type of slope for a dielectric constant of 2. 25. However, the <b>accuracy</b> <b>decreases</b> with increasing dielectric constant...|$|R
40|$|This paper {{presents}} {{a comparison of}} two versions of an Emergency Automated Response System (EARS), a fully manual version and a partially automated version. User evaluations involving both versions of the system were conducted using a low workload task and a high workload task. The {{results indicate that the}} automation employed by the partially automated system decreased overall response time and perceived workload for both tasks, but <b>accuracy</b> <b>decreased</b> and response times increased from low workload to high workload with both versions...|$|R
40|$|We have {{developed}} the notion of lexicon density as a metric to measure the expected accuracy of handwritten word recognizers. Thus far, researchers have used {{the size of the}} lexicon as a gauge for the di#culty of the handwritten word recognition task. For example, the literature mentions recognizers with accuracy for lexicons of sizes 10, 100, 1000, and so forth, implying that the di#culty of the task increases (and hence recognition <b>accuracy</b> <b>decreases)</b> with increasing lexicon sizes across recognizers. Lexico...|$|R
