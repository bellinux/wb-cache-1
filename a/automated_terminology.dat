2|5|Public
40|$|Methods: Literature {{review of}} the {{research}} published after 1995, based on PubMed, conference proceedings, and the ACM Digital Library, {{as well as on}} relevant publications referenced in papers already included. Results: 174 publications were selected and are discussed in this review in terms of methods used, pre-processing of textual documents, contextual features detection and analysis, extraction of information in general, extraction of codes and of information for decision-support and enrichment of the EHR, information extraction for surveillance, research, <b>automated</b> <b>terminology</b> management, and data mining, and de-identification of clinical text. Conclusions: Performance of information extraction systems with clinical text has improved since the last systematic review in 1995, but they are still rarely applied outside of the laboratory they have been developed in. Competitive challenges for information extraction from clinical text, along with the availability of annotated clinical text corpora, and further improvements in system performance are important factors to stimulate advances in this field and to increase the acceptance and usage of these systems in concrete clinical and biomedical research contexts...|$|E
40|$|Vandeghinste V., Vanallemeersch T., Van Eynde F., Heyman G., Moens S., Pelemans J., Wambacq P., Van der Lek - Ciudin I., Tezcan A., Macken L., Hoste V., Geurts E., Haesen M., ''Smart {{computer}} aided translation environment'', 18 th {{annual conference}} of the European Association for Machine Translation - EAMT 2015, May 11 - 13, 2015, Antalya, Turkey. We aim at improving the translators' efficiency through five different scientific objectives. Concerning improvements in translation technology, we are investigating syntax-based fuzzy matching in which we estimate fuzziness based on syntactic edit distance or similar measures. We are working on syntax-based MT using synchronous tree substitution grammars induced from parallel node-aligned treebanks, and are building a decoder to use these grammars in translation. Concerning improvements in evaluation of computer-aided translation, we have developed a taxonomy of typical MT errors and are constructing a manually annotated corpus of 3000 segments of Google Translate MT errors. Post-editing behaviour of translators is being monitored. Concerning improvements in <b>automated</b> <b>terminology</b> extraction from comparable corpora, we have developed C-BiLDA, a multilingual topic model. The key feature is that it no longer assumes linked documents to have identical topic distributions. We evaluated the model on the task of cross-lingual document categorization: we trained our topic model on a comparable corpus constructed from Wikipedia documents, and used it to infer cross-lingual document representations on a dataset for document categorization. The document representations and category labels are fed to an SVM classifier: we train on the documents of the source language and predict the labels for the documents of the target language. The results show that C-BiLDA outperforms state-of-the-art in multilingual topic modeling on two document categorization datasets. Concerning improvements in speech recognition accuracy, we have clustered words by their translations in multiple languages. If words share a translation in many languages, they are considered synonyms. By adding some context and by filtering out those that do not belong to the same part of speech, we find meaningful word clusters that look promising to incorporate into a language model. We found no improvements, and attribute this in part to errors made by the MT system, and to the employed incorporation technique (hard clustered class-based n-grams). We will take context into account during evaluation and/or further improve the word clusters by using the translations as features in vector space modeling techniques. Concerning improvements in work flows and personalised user interfaces, we reviewed existing translation systems, and created an inventory of the various features and configuration options of the system. Six Flemish companies are interviewed regarding their practices and their vision for future CAT tools. A worldwide survey is conducted with more than 135 responses. Detailed analyses of translators' practices are conducted by observing more than 7 translators by conducting a contextual inquiry. In the upcoming period, the results of the different studies will be analysed in order to obtain a model of how CAT tools can support workflows for specific translators. This model will be used as a base for the personalised visualisations as part of interfaces for translation work. In contrast with traditional engineering approaches, this model will also be usable by translators as part of the configuration of their personal CAT tool. status: publishe...|$|E
40|$|We {{investigate}} on the <b>automated</b> {{construction of}} <b>terminologies</b> from assertions in an expressive Description Logics representation like ACC. The overall unsupervised learning problem is decomposed into smaller supervised learning problems once clusters of disjoint concepts are detected. In turn, {{these problems are}} solved by means of refinement operators 1...|$|R
40|$|This note {{describes}} {{a new type}} of automaton that has been developed at CIS Munich for efficient recognition of phrases in large German corpora. The concept of a constraint-based automaton is tailored to sets of phrases where grammaticality depends on morphological agreement conditions. It incorporates features from three sides: traditional finite state techniques, methods from constraint programming, and, on the implementation side, technology from large-scale electronic dictionaries. We describe the main ideas behind the concept, and we give some experimental results that compare the performance of constraint based automata with Prolog. 1 Introduction In computational linguistics, efficient recognition of phrases is an important prerequisite for many ambitious goals, such as, e. g., <b>automated</b> extraction of <b>terminology,</b> part of speech disambiguation, and automated translation. If one wants to recognize a certain well-defined set of phrases, the question of which type of computational d [...] ...|$|R
40|$|The massive {{amount of}} {{statistical}} and text data available from Federal Agencies has created a set of daunting challenges to both research and analysis communities. These problems include heterogeneity, size, distribution, and control of terminology. At the Digital Government Research Center we are investigating solutions to three key problems, namely, (1) ontological mappings for terminology standardization; (2) data integration across data bases with high speed query processing; and (3) interfaces for query input and presentation of results. This collaboration between researchers from Columbia University and the Information Sciences Institute of the University of Southern California employs technology developed at both locations, in particular the SENSUS ontology, the SIMS multi-database access planner, the LKB <b>automated</b> dictionary and <b>terminology</b> analysis system, and others. The pilot application targets gasoline data from BLS, EIA, Census, and other agencies. • Introduction: The Digital Government Research Center As access to the web becomes a household commodity, the Government (and in particular Federal Agencies such as the Census Bureau, the Bureau of Labor Statistics, and others) has a mandate to make it...|$|R
40|$|Clinicians have {{traditionally}} documented patient data using natural language text. With the increasing prevalence of computer systems in health care, an increasing amount of medical record text will be stored electronically. However, for such textual documents to be indexed, shared, and processed adequately by computers, {{it will be}} important to be able to identify concepts in the documents using a common medical <b>terminology.</b> <b>Automated</b> methods for extracting concepts in a standard terminology would enhance retrieval and analysis of medical record data. This paper discusses a method for extracting concepts from medical record documents using the medical terminology SNOMED-III (Systematized Nomenclature of Human and Veterinary Medicine, Version III). The technique employs a linear least squares fit that maps training set phrases to SNOMED concepts. This mapping can be used for unknown text inputs in the same domain as the training set to predict SNOMED concepts that are contained in the document. We have implemented the method in the domain of congestive heart failure for history and physical exam texts. Our system has a reasonable response time. We tested the system over a range of thresholds. The system performed with 90 % sensitivity and 83 % specificity at the lowest threshold, and 42 % sensitivity and 99. 9 % specificity at the highest threshold...|$|R
40|$|The {{automatic}} {{extraction of}} collocations from large corpora or the Internet poses a daunting challenge to computational linguistics. Indeed, previous statistical methods based on bigram extraction have shown their limitations, {{and there is}} besides no theoretical consensus on the extension of parametric methods to trigrams or higher n-grams. This is a key issue, because the automatic extraction of significant n-grams {{has important implications for}} computer-aided translation, translation quality assessment, <b>automated</b> text correction, <b>terminology</b> and computational lexicography. This paper reports promising results that were obtained by using a totally different approach to the automatic extraction of significant n-grams of any size. Instead of having recourse to statistical scores, the method is based on the testing of proximity algorithms that corroborate the native speaker’s competence about existing collocations. It is argued that compound terminology and phraseology in the broad sense can be captured by algorithms based on linguistic co-occurrence phenomena. This is made possible by a subtle manipulation of the Application Programming Interface (API) of a Web search engine, in this case Yahoo. The algorithm presented here, the Web Proximity Measure (WPR), has been tested on about 4, 000 collocations mentioned in traditional dictionaries and on 340, 000 n-grams extracted from the Web 1 T or ‘Google n-grams’. The results show precision and recall scores superior to 0. 9...|$|R

