1692|1037|Public
5|$|HDMI (High-Definition Multimedia Interface) is a {{proprietary}} audio/video interface for transmitting uncompressed video data and compressed or uncompressed digital <b>audio</b> <b>data</b> from an HDMI-compliant source device, {{such as a}} display controller, to a compatible computer monitor, video projector, digital television, or digital audio device. HDMI is a digital replacement for analog video standards.|$|E
5|$|Introduced in HDMI 1.4, HDMI Ethernet and Audio Return Channel (HEAC) adds a {{high-speed}} bidirectional data communication link (HEC) {{and the ability}} to send <b>audio</b> <b>data</b> upstream to the source device (ARC). HEAC utilizes two lines from the connector: the previously unused Reserved pin (called HEAC+) and the Hot Plug Detect pin (called HEAC−). If only ARC transmission is required, a single mode signal using the HEAC+ line can be used, otherwise, HEC is transmitted as a differential signal over the pair of lines, and ARC as a common mode component of the pair.|$|E
5|$|The 2011 Chevrolet Volt comes {{standard}} with cruise control; remote vehicle start-up system; 17-inch 5-spoke forged painted aluminum wheels; Bluetooth {{wireless technology}} for select phones; audio and navigation {{system with a}} center console capacitive touch panel and DVD and MP3 playback, with voice recognition; OnStar with five years of service; BOSE premium speaker system, with six speakers and sub-woofer; 30 GB hard drive for <b>audio</b> <b>data</b> storage; USB port; three auxiliary, 12-volt, power outlets; power door locks and windows; power adjustable mirrors; programmable time of day charge control; and a 110-volt charge cord.|$|E
5000|$|Bluetooth {{wireless}} technology 2.0 with A2DP stereo <b>audio,</b> enhanced <b>data</b> rates (EDR) ...|$|R
5000|$|DP = Audio circuit (however {{transmitted}} over distance) {{presented as}} <b>Audio</b> for <b>Data</b> modem ...|$|R
5000|$|... 4. Development of <b>Audio</b> Meta <b>Data</b> Process for {{conforming}} audio the ATSC digital TV standard ...|$|R
5|$|Both HDMI and DVI use TMDS to send 10-bit {{characters}} that are encoded using 8b/10b encoding that {{differs from the}} original IBM form for the Video Data Period and 2b/10b encoding for the Control Period. HDMI adds the ability to send audio and auxiliary data using 4b/10b encoding for the Data Island Period. Each Data Island Period is 32 pixels in size and contains a 32-bit Packet Header, which includes 8 bits of BCH ECC parity data for error correction and describes {{the contents of the}} packet. Each packet contains four subpackets, and each subpacket is 64 bits in size, including 8 bits of BCH ECC parity data, allowing for each packet to carry up to 224 bits of <b>audio</b> <b>data.</b> Each Data Island Period can contain up to 18 packets. Seven of the 15 packet types described in the HDMI 1.3a specifications deal with <b>audio</b> <b>data,</b> while the other 8 types deal with auxiliary data. Among these are the General Control Packet and the Gamut Metadata Packet. The General Control Packet carries information on AVMUTE (which mutes the audio during changes that may cause audio noise) and Color Depth (which sends the bit depth of the current video stream and is required for deep color). The Gamut Metadata Packet carries information on the color space being used for the current video stream and is required for xvYCC.|$|E
25|$|FLAC handles archival {{and high}} {{fidelity}} <b>audio</b> <b>data.</b>|$|E
25|$|CD-DA, the {{standard}} audio CD, {{is said to}} have a data rate of 44.1kHz/16, meaning that the <b>audio</b> <b>data</b> was sampled 44,100 times per second and with a bit depth of 16. CD-DA is also stereo, using a left and right channel, so the amount of <b>audio</b> <b>data</b> per second is double that of mono, where only a single channel is used.|$|E
50|$|The Hi-MD format, {{introduced}} in 2004, marked {{a return to}} the data storage arena with its 1 GB discs and ability to act as a USB drive. Hi-MD units allow the recording and playback of <b>audio</b> and <b>data</b> on the same disc, and are compatible (both <b>audio</b> and <b>data)</b> with standard MiniDisc media - an 80-minute Minidisc blank could be formatted to store 305MB of data.|$|R
40|$|The {{structure}} of the Open Sound Control (OSC) content format is introduced with historical context. The needs for temporal synchronization and dynamic range of <b>audio</b> control <b>data</b> are {{described in terms of}} accuracy, precision, bit-depth, bit-rate, and sampling frequency. Specific details are given for the case of instrumental gesture control, spatial audio control and synthesis algorithm control. The consideration of various transport mechanisms used with OSC is discussed for datagram, serial and isochronous modes. A summary of design approaches for describing <b>audio</b> control <b>data</b> is shown, and the case is argued that multi-layered information-rich representations that support multiple strategies for describing semantic structure are necessary. Keywords <b>audio</b> control <b>data,</b> signal quality assurance, bes...|$|R
5000|$|SMPTE 388M: GC-AA (how {{to store}} A-law coded <b>audio</b> essence <b>data</b> in MXF using the Generic Container) ...|$|R
25|$|USB allows direct {{connection}} to PCs. Electronics in these microphones powered over the USB connection performs preamplification and AD conversion before the digital <b>audio</b> <b>data</b> is transferred via the USB interface.|$|E
25|$|Kevlar {{has also}} {{been found to have}} useful {{acoustic}} properties for loudspeaker cones, specifically for bass and midrange drive units. Additionally, Kevlar {{has been used as a}} strength member in fiber optic cables such as the ones used for <b>audio</b> <b>data</b> transmissions.|$|E
25|$|The HDMI {{interface}} is {{a compact}} audio/video interface for transferring uncompressed video data and compressed/uncompressed digital <b>audio</b> <b>data</b> from an HDMI-compliant device to a compatible computer monitor, video projector, digital television, or digital audio device. It is mainly {{used in the}} consumer area, but increasingly used in professional devices including uncompressed video, often called clean HDMI.|$|E
50|$|The Palm Multi-Connector (also Athena Connector) is a power, <b>audio</b> and <b>data</b> {{interface}} connector designed by Palm, Inc.|$|R
25|$|While the {{satellite}} receiver radio service was its primary product, XM operated several <b>audio</b> and <b>data</b> services, and advertising.|$|R
5000|$|SMPTE 382M: GC-AESBWF (how {{to store}} AES/EBU and Broadcast Wave <b>audio</b> essence <b>data</b> in MXF using the Generic Container) ...|$|R
25|$|Applications {{communicate}} with the audio driver through Sessions, and these Sessions are programmed through the Windows Audio Session API (WASAPI). In general, WASAPI operates in two modes. In exclusive mode (also called DMA mode), unmixed audio streams are rendered directly to the audio adapter and no other application's audio will play and signal processing has no effect. Exclusive mode is useful for applications that demand {{the least amount of}} intermediate processing of the <b>audio</b> <b>data</b> or those that want to output compressed <b>audio</b> <b>data</b> such as Dolby Digital, DTS or WMA Pro over S/PDIF. WASAPI exclusive mode is similar to kernel streaming in function, but no kernel mode programming is required. In shared mode, audio streams are rendered by the application and optionally applied per-stream audio effects known as Local Effects (LFX) (such as per-session volume control). Then the streams are mixed by the global audio engine, where a set of global audio effects (GFX) may be applied. Finally, they're rendered on the audio device.|$|E
25|$|Smartphone-based rich monitoring. Smartphones having various {{sensors can be}} used {{to track}} traffic speed and density. The {{accelerometer}} data from smartphones used by car drivers is monitored to find out traffic speed and road quality. <b>Audio</b> <b>data</b> and GPS tagging of smartphones enables identification of traffic density and possible traffic jams. This was implemented in Bangalore, India {{as a part of a}} research experimental system Nericell.|$|E
25|$|The {{reason why}} {{interleaving}} {{is used on}} OFDM is to attempt to spread the errors out in the bit-stream that is presented to the error correction decoder, because when such decoders are presented with {{a high concentration of}} errors the decoder is unable to correct all the bit errors, and a burst of uncorrected errors occurs. A similar design of <b>audio</b> <b>data</b> encoding makes compact disc (CD) playback robust.|$|E
5000|$|IEEE Edison Medal, for {{a career}} of {{creative}} contributions to the technologies of digital video, <b>audio,</b> and <b>data</b> recording. (USA, 1999) ...|$|R
5000|$|IEEE Medal of Honor, for {{pioneering}} {{contributions to}} video, <b>audio,</b> and <b>data</b> recording technology, including compact disc, DVD, and Blu-ray, 2017 ...|$|R
5000|$|... bit 1: A {{value of}} 0 {{indicates}} this is linear <b>audio</b> PCM <b>data.</b> A value of 1 indicates other (usually non-audio) data.|$|R
25|$|Besides {{providing}} the essential service of passing <b>audio</b> <b>data</b> {{to the sound}} card, DirectSound provides other essential capabilities such as recording and mixing sound, adding effects to sound (e.g., reverb, echo, or flange), using hardware accelerated buffers in Windows 95 through XP, and positioning sounds in 3D space. DirectSound also provides a means to capture sounds from a microphone or other input and controlling capture effects during audio capture.|$|E
25|$|Audio packets– {{also known}} as {{isochronous}} data packets, these packets are sent out by all CobraNet devices after they receive a beat packet. At standard latency settings, one audio packet is sent for each beat packet received, and each audio packet includes 64 samples of <b>audio</b> <b>data</b> per channel. At lower latency settings, audio packets may be sent twice or four times for each beat packet received. Bundles do not share packets; separate packets are sent in sequence for each bundle transmitted from the same device.|$|E
25|$|The MP3 lossy <b>audio</b> <b>data</b> {{compression}} algorithm {{takes advantage of}} a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described {{a complete set of}} auditory curves regarding this phenomenon. Ernst Terhardt et al. created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.|$|E
50|$|The {{majority}} of video cameras only provides {{the output of}} <b>audio</b> and video <b>data</b> via the FireWire bus ("DVout"). Additionally, some video cameras are able to record <b>audio</b> and video <b>data</b> ("DVout/DVin"). Video cameras exchange their data with computers and/or video recorders.|$|R
5000|$|The {{working group}} {{has broken down}} the problem of moving public safety {{communications}} <b>audio</b> over <b>data</b> (usually TCP/IP) networks into 5 specific interface points: ...|$|R
50|$|Slow {{transfer}} rates when transferring <b>audio</b> (and <b>data)</b> to {{and from}} the computer: Most Hi-MD units offer far lower speeds than flash memory-based and hard drive-based devices.|$|R
25|$|Compact Disc + Graphics is {{a special}} audio compact disc that {{contains}} graphics data {{in addition to the}} <b>audio</b> <b>data</b> on the disc. The disc can be played on a regular audio CD player, but when played on a special CD+G player, it can output a graphics signal (typically, the CD+G player is hooked up to a television set or a computer monitor); these graphics are almost exclusively used to display lyrics on a television set for karaoke performers to sing along with. The CD+G format takes advantage of the channels R through W. These six bits store the graphics information.|$|E
25|$|Real-time {{multimedia}} streaming applications require timely {{delivery of}} information and often can tolerate some packet loss to achieve this goal. For example, loss of a packet in audio application may result in loss of {{a fraction of a}} second of <b>audio</b> <b>data,</b> which can be made unnoticeable with suitable error concealment algorithms. The Transmission Control Protocol (TCP), although standardized for RTP use, is not normally used in RTP applications because TCP favors reliability over timeliness. Instead the majority of the RTP implementations are built on the User Datagram Protocol (UDP). Other transport protocols specifically designed for multimedia sessions are SCTP and DCCP, although, , they are not in widespread use.|$|E
25|$|Broadcasting {{began at}} the Empire State Building on December 22, 1931, when NBC and RCA began {{transmitting}} experimental television broadcasts from a small antenna erected atop the spire, with two separate transmitters for the visual and <b>audio</b> <b>data.</b> They leased the 85th floor and built a laboratory there. In 1934, RCA was joined by Edwin Howard Armstrong in a cooperative venture to test his FM system from the building's antenna. This setup, which entailed {{the installation of the}} world's first FM transmitter, continued only until October of the next year because of disputes between RCA and Armstrong. Specifically, NBC wanted to install more TV equipment in the room where Armstrong's transmitter was located.|$|E
50|$|CD-i Ready is {{a compact}} disc format for mixing <b>audio</b> and <b>data</b> content on a CD. It was {{developed}} by Phillips in 1991, based on the CD-i format.|$|R
5000|$|... • DSP {{implementation}} of 3D sound localizationA DSP-based {{implementation of}} a realtime 3D sound localization approach {{with the use of}} an embedded DSP can reduce the computational complexity As shown in the figure, the implementation procedure of this realtime algorithm is divided into three phases, (i) Frequency Division, (ii) Sound Localization, and (iii) Mixing. In the case of 3D sound localization for a monaural sound source, the <b>audio</b> input <b>data</b> are divided into two: left and right channels and the <b>audio</b> input <b>data</b> in time series are processed one after another.|$|R
5|$|There was {{an ongoing}} {{experimental}} DMB multiplex broadcast in London on L-Band and Cambridge on VHF Band III, used for video, <b>audio</b> and <b>data</b> applications which have since ceased.|$|R
