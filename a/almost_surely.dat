3945|18|Public
5|$|A {{series of}} glaciations further {{modified}} the region starting about 2 to 3 {{million years ago}} and ending sometime around 10,000 BP. At least four major glaciations {{have occurred in the}} Sierra Nevada, locally called the Sherwin (also called the pre-Tahoe), Tahoe, Tenaya, and Tioga. The Sherwin glaciers were the largest, filling Yosemite and other valleys, while later stages produced much smaller glaciers. A Sherwin-age glacier was <b>almost</b> <b>surely</b> responsible for the major excavation and shaping of Yosemite Valley and other canyons in the area.|$|E
25|$|By definition, we {{conclude}} that g(Xn) converges to g(X) <b>almost</b> <b>surely.</b>|$|E
25|$|For an ergodic transformation, {{the time}} average equals the space average <b>almost</b> <b>surely.</b>|$|E
25|$|A 3rd Arkansas Regiment, 30-Day Volunteers (cavalry), <b>almost</b> <b>surely</b> never {{completely}} organized—only the rosters of two mounted companies, under Captains Reves and Hooker, have survived.|$|E
25|$|And {{even if the}} {{probabilistic}} reasoning were rigorous, this would still imply only that the conjecture is <b>almost</b> <b>surely</b> true for any given integer, which does not necessarily imply that it is true for all integers.|$|E
25|$|Every {{positive}} number x is {{the product}} of two normal numbers. For instance if y is chosen uniformly at random from the interval (0,1) then <b>almost</b> <b>surely</b> y and x/y are both normal, and their product is x.|$|E
25|$|Then the {{simplest}} substantial mathematical {{conclusion is that}} if the average number of a man's sons is 1 or less, then their surname will <b>almost</b> <b>surely</b> die out, and if it is more than1, then there is more than zero probability that it will survive for any given number of generations.|$|E
25|$|The {{concept of}} almost sure {{convergence}} {{does not come from}} a topology on the space of random variables. This means there is no topology on the space of random variables such that the <b>almost</b> <b>surely</b> convergent sequences are exactly the converging sequences with respect to that topology. In particular, there is no metric of {{almost sure convergence}}.|$|E
25|$|It can {{be shown}} that for the output of Markov {{information}} sources, Kolmogorov complexity {{is related to the}} entropy of the information source. More precisely, the Kolmogorov complexity of the output of a Markov information source, normalized by the length of the output, converges <b>almost</b> <b>surely</b> (as the length of the output goes to infinity) to the entropy of the source.|$|E
25|$|The Rado graph arises <b>almost</b> <b>surely</b> in the Erdős–Rényi {{model of}} a random graph on countably many vertices. Specifically, one may form an {{infinite}} graph by choosing, independently and with probability 1/2 for each pair of vertices, whether to connect the two vertices by an edge. With probability 1 the resulting graph has the extension property, and is therefore isomorphic to the Rado graph.|$|E
25|$|According to intuition, {{the process}} will {{converge}} to the first exit point of the domain. However, this algorithm takes <b>almost</b> <b>surely</b> {{an infinite number of}} steps to end. For computational implementation, the process is usually stopped when it gets sufficiently close to the border, and returns the projection of the process on the border. This procedure is sometimes called introducing an -shell, or -layer.|$|E
25|$|In July 1800, Malartic sailed for {{a second}} cruise, {{capturing}} the ships Frederic North, Amboyna, Alkias, and Malava. She later captured Governor North (which is <b>almost</b> <b>surely</b> the same vessel as Frederic North, both being named for Governor Frederick North, who served between 1798 and 1805 as the first British civilian governor of Ceylon), Marquis de Wellesley, and a brig, before returning to Mauritius, where she arrived with her prizes on 21 September 1800.|$|E
25|$|The Wiener {{process can}} be {{constructed}} as the scaling limit of a random walk, or other discrete-time stochastic processes with stationary independent increments. This is known as Donsker's theorem. Like the random walk, the Wiener process is recurrent {{in one or two}} dimensions (meaning that it returns <b>almost</b> <b>surely</b> to any fixed neighborhood of the origin infinitely often) whereas it is not recurrent in dimensions three and higher. Unlike the random walk, it is scale invariant.|$|E
25|$|In {{probability}} theory, {{the expected}} {{value of a}} random variable, intuitively, is the long-run average value of repetitions of the experiment it represents. For example, the expected value in rolling a six-sided die is 3.5, because the average of all the numbers that come up in an extremely large number of rolls is close to 3.5. Less roughly, the law of large numbers states that the arithmetic mean of the values <b>almost</b> <b>surely</b> converges to the expected value {{as the number of}} repetitions approaches infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment.|$|E
25|$|Homeopathy {{achieved}} {{its greatest}} {{popularity in the}} 19th century. It {{was introduced to the}} United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the US opened in 1835, and in 1844, the first US national medical association, the American Institute of Homeopathy, was established. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States, and by 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors of the time. Homeopathic preparations, even if ineffective, would <b>almost</b> <b>surely</b> cause no harm, making the users of homeopathic preparations less likely to be killed by the treatment {{that was supposed to be}} helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and to have begun the move towards more effective, science-based medicine.|$|E
500|$|... holds. If two [...] and [...] are {{modifications}} {{of each other}} and are <b>almost</b> <b>surely</b> continuous, [...] then [...] and [...] are indistinguishable.|$|E
500|$|<b>Almost</b> <b>surely,</b> {{a sample}} {{path of a}} Wiener process is {{continuous}} everywhere but nowhere differentiable. It {{can be considered a}} continuous version of the simple random walk. [...] The process arises as the mathematical limit of other stochastic processes such as certain random walks rescaled, which is the subject of Donsker's theorem or invariance principle, also known as the functional central limit theorem.|$|E
500|$|With Elizabeth's death, James VI becomes {{ruler of}} both England and Scotland. James is distrustful of [...] "witchbreed" [...] (people born with magical powers and the mutants of this reality) and collaborates with Spanish High Inquisitor Enrique {{to blame the}} witchbreed of England, headed by Carlos Javier, for Elizabeth's death. Fury, a friend of Carlos and his students, is forced to take the witchbreed to the Tower. Strange, Javier, and Fury meet there and discuss how {{to save the world}} — an act which will <b>almost</b> <b>surely</b> lead to them being branded traitors by James. Strange has learned that the {{treasure}} of the Templars and its keeper Donal, and Murdoch, have been betrayed and are {{now in the hands of}} Doom. Strange also learns that Doom has been holding captive four heroes from the ship Fantastick, including Fury's friend Sir Richard Reed. Knowing that James will never give him an army to march on Latveria, he conspires with the witchbreed, taking a ship levitated by Javier and his page John Grey across the continent.|$|E
2500|$|If {{one wants}} to {{demonstrate}} that the ML estimator [...] converges to θ0 <b>almost</b> <b>surely,</b> then a stronger condition of uniform convergence <b>almost</b> <b>surely</b> has to be imposed: ...|$|E
2500|$|Under {{slightly}} stronger conditions, the estimator converges <b>almost</b> <b>surely</b> (or strongly) to: ...|$|E
2500|$|Convergence in {{probability}} implies {{there exists}} a sub-sequence [...] which <b>almost</b> <b>surely</b> converges: ...|$|E
2500|$|... then [...] converges <b>almost</b> <b>surely</b> if {{and only}} if [...] converges in probability.|$|E
2500|$|... (See <b>almost</b> <b>surely.)</b> That is, {{the smaller}} A is, {{the longer it}} takes to return to it.|$|E
2500|$|For generic random {{elements}} {Xn} on a metric space (S, d), convergence <b>almost</b> <b>surely</b> {{is defined}} similarly: ...|$|E
2500|$|Almost sure representation. Usually, {{convergence}} in distribution {{does not}} imply convergence <b>almost</b> <b>surely.</b> However for a given sequence {Xn} which converges in distribution to X0 it is always possible {{to find a new}} probability space (Ω, F, P) and random variables {Yn, n = 0, 1, ...} defined on it such that Yn is equal in distribution to [...] for each , and Yn converges to Y0 <b>almost</b> <b>surely.</b>|$|E
2500|$|... then we {{say that}} [...] converges almost completely, or almost in {{probability}} towards X. When [...] converges almost completely towards X then it also converges <b>almost</b> <b>surely</b> to X. [...] In other words, if [...] converges in probability to X sufficiently quickly (i.e. the above sequence of tail probabilities is summable for all [...] ), then [...] also converges <b>almost</b> <b>surely</b> to X. This is a direct implication from the Borel–Cantelli lemma.|$|E
2500|$|Express [...] {{in terms}} of the sample mean of [...] and show that it converges <b>almost</b> <b>surely</b> to Hk ...|$|E
2500|$|... are non-positive <b>almost</b> <b>surely</b> {{by setting}} α = nβ for any β > 1 and {{applying}} the Borel–Cantelli lemma.|$|E
2500|$|... {{are lower}} and upper bounded <b>almost</b> <b>surely</b> by H∞ and Hk {{respectively}} by {{breaking up the}} logarithms in the previous result.|$|E
2500|$|To {{say that}} the {{sequence}} [...] converges <b>almost</b> <b>surely</b> or almost everywhere or with probability 1 or strongly towards X means that ...|$|E
2500|$|Assume that [...] is {{well defined}} and finite valued for all [...] This implies {{that for every}} [...] the value [...] is finite <b>almost</b> <b>surely.</b>|$|E
2500|$|... where i is {{the time}} index, are {{stationary}} ergodic processes, whose sample means converge <b>almost</b> <b>surely</b> to some values denoted by Hk and H∞ respectively.|$|E
2500|$|Proof. The {{proof is}} straightforward, and {{essentially}} the same as the finitary version. If , then [...] is constant (and equal to [...] ) <b>almost</b> <b>surely,</b> so the inequality is trivial.|$|E
2500|$|An {{alternative}} characterisation of the Wiener {{process is}} the so-called Lévy characterisation that says that the Wiener process is an <b>almost</b> <b>surely</b> continuous martingale with W0 = 0 and quadratic variation [...]|$|E
2500|$|... is {{an inner}} product. In this case, [...] if {{and only if}} [...] (i.e., [...] <b>almost</b> <b>surely).</b> This {{definition}} of expectation as inner product can be extended to random vectors as well.|$|E
2500|$|For example, if [...] are {{independent}} Bernoulli random variables taking values 1 with probability p and 0 with probability 1-p, then [...] for all i, so that [...] converges to p <b>almost</b> <b>surely.</b>|$|E
