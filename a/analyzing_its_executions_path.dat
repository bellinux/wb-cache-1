0|10000|Public
40|$|Abstract. Kernel rootkits, {{as one of}} {{the most}} elusive types of malware, pose {{significant}} challenges for investigation and defense. Among the most notable are persistent kernel rootkits, a special type of kernel rootkits that implant persistent kernel hooks to tamper with the kernel execution to hide their presence. To defend against them, an effective approach is to first identify those kernel hooks and then protect them from being manipulated by these rootkits. In this paper, we focus on the first step by proposing a systematic approach to identify those kernel hooks. Our approach is based on two key observations: First, rootkits by design will attempt to hide its presence from all running rootkit-detection software including various system utility programs (e. g., ps and ls). Second, to manipulate OS kernel control-flows, persistent kernel rootkits by their nature will implant kernel hooks on the corresponding kernel-side <b>execution</b> <b>paths</b> invoked by the security programs. In other words, for any persistent kernel rootkit, either it is detectable by a security program or it has to tamper with one of the kernel hooks on the corresponding kernel-side <b>execution</b> <b>path(s)</b> of the security program. As a result, given an authentic security program, we only need to monitor and <b>analyze</b> <b>its</b> kernel-side <b>execution</b> <b>paths</b> to identify the related set of kernel hooks that could be potentially hijacked for evasion. We have built a proof-of-concept system called HookMap and evaluated it with a number of Linux utility programs such as ls, ps, and netstat in RedHat Fedora Core 5. Our system found that there exist 35 kernel hooks in the kernel-side <b>execution</b> <b>path</b> of ls that can be potentially hijacked for manipulation (e. g., for hiding files). Similarly, there are 85 kernel hooks for ps and 51 kernel hooks for netstat, which can be respectively hooked for hiding processes and network activities. A manual analysis of eight real-world rootkits shows that our identified kernel hooks cover all those used in them. ...|$|R
40|$|The authors propose an {{extension}} of SysML which enables description of continuous-time behavior. The authors also develop <b>its</b> <b>execution</b> tool integrated on Eclipse-based platform by exploiting co-simulation of SysML and MAT-LAB / Simulink. To demonstrate {{the effectiveness of the}} tool and the extension to SysML in verifying specifications of an embedded system, we create a sample model and <b>analyze</b> <b>its</b> <b>execution</b> results by checking constraints under a test case 1. 1...|$|R
5000|$|Gyroscope {{uses the}} Loader-Content-Handler-Handler (LCHH) {{architecture}} instead of explicit Model-View-Controller (MVC). In the LCHH architecture, a Loader, or a [...] "DIV" [...] container {{with a unique}} identifier is populated with default Content. Visual affordances, also known as [...] "triggers" [...] that {{are included in the}} Content invoke client-side handlers. These JavaScript handlers then sends XmlHTTPRequestObject (or AJAX) requests to Server-side Handlers. The Server-side scripts then perform updating, insertion or deletion functions before piggybacking the updated View back to the original Loader via AJAX callback and DOM update. The benefit of the LCHH architecture is that <b>its</b> <b>execution</b> <b>path</b> is identical to the life cycle of an HTTP request. This design has both performance and debugging advantages.|$|R
40|$|We {{present a}} new technique, failure-oblivious computing, that enables servers to execute through memory errors without memory corruption. Our safe {{compiler}} for C inserts checks that dynamically detect invalid memory accesses. Instead of terminating or throwing an exception, the generated code simply discards invalid writes and manufactures values {{to return for}} invalid reads, enabling the server to continue <b>its</b> normal <b>execution</b> <b>path...</b>|$|R
40|$|In {{this paper}} the authors {{contribute}} {{a new technique}} of failure-oblivious computing. It aims to enable the server to execute through memory errors without memory corruption. The failure-oblivious computing is a mechanism to discard invalid writes and manufacture values to return for the invalid reads, and enable the server to continue <b>its</b> normal <b>execution</b> <b>path,</b> instead of terminating or throwing an exception, that makes the server more available, secur...|$|R
40|$|Novel {{architectures}} such as Grids have dynamic characteristics. Faults, nonexclusive access, {{and maintenance}} operations make resources {{come and go}} even during the execution time of applications. In {{order to deal with}} these events and continue the execution as good as possible, applications must be able to adapt to those changes of their execution environment. To do so, we propose a model in which an application chooses a special point in the future of <b>its</b> <b>execution</b> <b>path</b> at which it is able to modify itself. In the specific case of parallel applications, the threads must agree on the point to choose. This article presents a negotiation-based approach for reaching such an agreement. It focuses on the distributed algorithm executed by the threads to find such a point. This algorithm is compared to works that has been done in areas of distributed consensus and dialogue protocols of agents...|$|R
40|$|In {{business}} processes, knowledge-intensive {{tasks are}} ones {{in which the}} people performing such tasks {{are involved in a}} fair degree of uncertainty. These people are required to apply and bring together their experience, training, expertise and judgement. In particular, they are concerned about issues or problems that might arise and how these are best dealt with or avoided. Current workflow technology does not support such tasks, as it deals only with predictable and easily automated decision making. In particular, it fails to deliver the right information to the user at the right time based on the context of the process instance, thus not taking the opportunity to forewarn users of potential problems. Context-aware workflows are a way to overcome shortcomings of workflow management systems. This paper proposes an approach for the dynamic integration of knowledge and workflow processes by offering proper support for the realtime handling of the both the current context of a process and <b>its</b> <b>execution</b> <b>path.</b> ...|$|R
40|$|In {{contrast}} to {{an increasing number}} of agent-based applica- tions in various domains, there has been very little work on maintenance and evolution of agent systems. This paper ad- dresses this gap with a focus on change impact analysis, i. e. estimating the potential eects of changes before they are made as an agent system evolves. We propose a technique for performing impact analysis in an agent system using dy- namic information about agent behaviour. Our approach builds a representation of an agent 2 ̆ 7 s behaviour by <b>analyzing</b> <b>its</b> <b>execution</b> traces which consist of goals and plans, and uses this representation to estimate impacts...|$|R
40|$|This paper {{presents}} a novel method {{for the analysis}} and representation of parallel program with MPI. Parallel programs are mapped onto graph-theoretical problems and are represented by DP*Tgraph, extension of T-graphs, timing graphs, which are similar to flow graphs. These graphs reflect the structure and the timing behavior of the code. The special merit of this new notation is that it uses a concise notation to characterize the static structure of a program and <b>its</b> possible <b>execution</b> <b>paths.</b> Eje: Programación concurrent...|$|R
40|$|Abstract. The {{branching}} {{space of}} a flow is the topological space of germs of <b>its</b> nonconstant <b>execution</b> <b>paths</b> beginning in the same way. However, there exist weakly S-homotopy equivalent flows having non weakly homotopy equivalent branching spaces. This topological space is then badly behaved from a computer-scientific viewpoint since weakly S-homotopy equivalent flows must correspond to higher dimensional automata having the same computer-scientific properties. To overcome this problem, the homotopy branching {{space of a}} flow is introduced as the left derived functor of the branching space functor fro...|$|R
40|$|Physical domains are {{notoriously}} hard to model completely and correctly, especially {{to capture the}} dynamics of the environment. Moreover, since environments change, it is even more important for the system to learn from its own experiences. Our work focusses on learning for the planning stages of a physical system, where our algorithm learns the costs and probabilities of operating the environment. Since actions may have dierent costs under dierent conditions, we introduce the concept of situation-dependent rules, in which situational features are attached to the costs or probabilities, re ecting patterns and dynamics encountered in the environment. In this article, we present Rogue, a robot that <b>analyzes</b> <b>its</b> <b>execution</b> experiences to detect patterns in the environment. Rogue extracts learning opportunities from massive, continual, probabilistic execution traces. It then correlates these learning opportunities with environmental features, creating situation-dependent costs for its actions. We present the development and use of these rules for a robotic path plan-ner. We present empirical data to show the eectiveness of Rogue's novel learnin...|$|R
40|$|Abstract We {{present a}} new technique, failure-oblivious comput-ing, that enables servers to execute through memory errors without memory corruption. Our safe {{compiler}} forC inserts checks that dynamically detect invalid memory accesses. Instead of terminating or throwing an excep-tion, the generated code simply discards invalid writes and manufactures values {{to return for}} invalid reads, en-abling the server to continue <b>its</b> normal <b>execution</b> <b>path.</b> We have applied failure-oblivious computing to aset of widely-used servers from the Linux-based opensource computing environment. Our results show thatour techniques 1) make these servers invulnerable to known security attacks that exploit memory errors, and 2) enable the servers to continue to operate successfully to service legitimate requests and satisfy the needs oftheir users even after attacks trigger their memory errors...|$|R
40|$|The {{branching}} {{space of}} a flow is the topological space of germs of <b>its</b> nonconstant <b>execution</b> <b>paths</b> beginning in the same way. However, there exist weakly Shomotopy equivalent flows having non weakly homotopy equivalent branching spaces. This topological space is then badly behaved from a computer-scientific viewpoint since weakly S-homotopy equivalent flows must correspond to higher dimensional automata having the same computer-scientific properties. To overcome this problem, the homotopy branching {{space of a}} flow is introduced as the left derived functor of the branching space functor from the model category of flows to the model category of topological spaces. As an application, we use this new functor to correct the notion of weak dihomotopy equivalence, which did not identify enough flows in its previous version...|$|R
40|$|Software {{maintenance}} and evolution {{is an important}} and lengthy phase in the software life-cycle which can account {{for as much as}} two-thirds of the total software development costs. Intelligent agent technology has evolved rapidly {{over the past few years}} as evidenced by the increasing number of agent systems in many different domains. Intelligent agent systems with their distinct characteristics and behaviours introduce new problems in software maintenance. However, in contrast to a substantial amount of work in providing methodologies for analysing, designing and implementing agent-based systems, there has been very little work on {{maintenance and}} evolution of agent systems. A critical issue in software maintenance and evolution is change impact analysis: estimating the potential effects of changes before they are made as an agent system evolves. In this paper, we propose two distinct approaches to change impact analysis for the well-known and widely-developed Belief-Desire-Intention agent systems. On the one hand, our static technique computes the impact of a change by analysing the source code and identifying various dependencies within the agent system. On the other hand, our dynamic technique builds a representation of an agent 2 ̆ 7 s behaviour by <b>analyzing</b> <b>its</b> <b>execution</b> traces which consist of goals and plans, and uses this representation to estimate impacts. We have implemented both techniques and in this paper we also report on the experimental results that compare their effectiveness in practice...|$|R
5000|$|The classic {{approach}} of performance prediction treats a {{program as a}} set of basic blocks connected by <b>execution</b> <b>path.</b> Thus the <b>execution</b> time of the whole program is the sum of execution time of each basic block multiplied by <b>its</b> <b>execution</b> frequency, as shown in the following formula: ...|$|R
40|$|Report {{published}} in the Proceedings of the National Conference on "Education and Research in the Information Society", Plovdiv, May, 2014 In this paper a software application, which provides personalized and adaptive elearning is presented. The application stores and supports data for each educational plan – examples for <b>its</b> <b>execution</b> (especially the <b>paths</b> for completion of educative goals by various students). A specific educated person is proposed a specific path {{in order to achieve}} more effective learning, based on a discovery of a similarity within the data of those who already graduated. The implementation makes use of a developed by the author Graph Database Management System. Association for the Development of the Information Society, Institute of Mathematics and Informatics Bulgarian Academy of Sciences, Plovdiv University "Paisii Hilendarski...|$|R
40|$|Trustworthy {{operation}} of industrial control systems depends on secure and real-time code execution on the embedded {{programmable logic controllers}} (PLCs). The controllers monitor and control the critical infrastructures, such as electric power grids and healthcare platforms, and continuously report back the system status to human operators. We present Zeus, a contactless embedded controller security monitor to ensure <b>its</b> <b>execution</b> control flow integrity. Zeus leverages the electromagnetic emission by the PLC circuitry during {{the execution of the}} controller programs. Zeus's contactless execution tracking enables non-intrusive monitoring of security-critical controllers with tight real-time constraints. Those devices often cannot tolerate the cost and performance overhead that comes with additional traditional hardware or software monitoring modules. Furthermore, Zeus provides an air-gap between the monitor (trusted computing base) and the target (potentially compromised) PLC. This eliminates the possibility of the monitor infection by the same attack vectors. Zeus monitors for control flow integrity of the PLC program execution. Zeus monitors the communications between the human-machine interface and the PLC, and captures the control logic binary uploads to the PLC. Zeus exercises <b>its</b> feasible <b>execution</b> <b>paths,</b> and fingerprints their emissions using an external electromagnetic sensor. Zeus trains a neural network for legitimate PLC executions, and uses it at runtime to identify the control flow based on PLC's electromagnetic emissions. We implemented Zeus on a commercial Allen Bradley PLC, which is widely used in industry, and evaluated it on real-world control program executions. Zeus was able to distinguish between different legitimate and malicious executions with 98. 9 % accuracy and with zero overhead on PLC execution by design...|$|R
40|$|The Implicit Path Enumeration Technique {{is often}} used to compute the WCET of control-intensive programs. This method does not {{consider}} <b>execution</b> <b>paths</b> as ordered sequences of basic blocks but instead as lists of basic blocks with their respective execution counts. This way of describing an <b>execution</b> <b>path</b> is adequate to compute <b>its</b> <b>execution</b> time, provided that safe individual WCETs for the blocks are known. Recently, a model for branch prediction has been integrated into WCET computation with IPET. This model generates safe estimations of the branch misprediction counts. However, we show in this paper that these counts can be over-estimated because IPET does consider simplified flow information that do not completely reflect the program semantics. We show how additional information on nested loops can be specified so that the model provides tighter WCET estimations...|$|R
40|$|We {{present a}} new technique, failure-oblivious computing, that enables servers to execute through memory errors without memory corruption. Our safe {{compiler}} for C inserts checks that dynamically detect invalid memory accesses. Instead of terminating or throwing an exception, the generated code simply discards invalid writes and manufactures values {{to return for}} invalid reads, enabling the server to continue <b>its</b> normal <b>execution</b> <b>path.</b> We have applied failure-oblivious computing {{to a set of}} widely-used servers from the Linux-based opensource computing environment. Our results show that our techniques 1) make these servers invulnerable to known security attacks that exploit memory errors, and 2) enable the servers to continue to operate successfully to service legitimate requests and satisfy the needs of their users even after attacks trigger their memory errors. We observed several reasons for this successful continued execution. When the memory errors occur in irrelevant computations, failure-oblivious computing enables the server to execute through the memory errors to continue on to execute the relevant computation. Even when the memory errors occur in relevant computations, failure-oblivious computing converts requests that trigger unanticipated and dangerous <b>execution</b> <b>paths</b> into anticipated invalid inputs, which the error-handling logic in the server rejects. Because servers tend to have small error propagation distances (localized errors in the computation for one request tend to have little or no effect on the computations for subsequent requests), redirecting reads that would otherwise cause addressing errors and discarding writes that would otherwise corrupt critical data structures (such as the call stack) localizes the effect of the memory errors, prevents addressing exceptions from terminating the computation, and enables the server to continue on to successfully process subsequent requests. The overall result is a substantial extension of the range of requests that the server can successfully process. ...|$|R
40|$|Abstract — Memory {{errors are}} a common cause of {{incorrect}} software execution and security vulnerabilities. We have developed two new techniques that help software continue to execute successfully through memory errors: failure-oblivious computing and boundless memory blocks. The foundation of both techniques is a compiler that generates code that checks accesses via pointers to detect out of bounds accesses. Instead of terminating or throwing an exception, the generated code takes another action that keeps the program executing without memory corruption. Failure-oblivious code simply discards invalid writes and manufactures values to return for invalid reads, enabling the program to continue <b>its</b> normal <b>execution</b> <b>path.</b> Code that implements boundless memory blocks stores invalid writes away in a hash table to return as the values for corresponding out of bounds reads. The net effect is to (conceptually) give each allocated memory block unbounded size and to eliminate out of bounds accesses as a programming error. We have implemented both techniques and acquired several widely used open source servers (Apache, Sendmail, Pine, Mutt, and Midnight Commander). With standard compilers, all of these servers are vulnerable to buffer overflow attacks as documented at security tracking web sites. Both failure-oblivious computing and boundless memory blocks eliminate these security vulnerabilities (as well as other memory errors). Our results show that our compiler enables the servers to execute successfully through buffer overflow attacks to continue to correctly service user requests without security vulnerabilities...|$|R
40|$|TL 2 {{and similar}} STM {{algorithms}} deliver high scalability based on write-locking and invisible readers. In fact, no modern STM design locks to read along <b>its</b> common <b>execution</b> <b>path</b> because {{doing so would}} require a memory synchronization operation that would greatly hamper performance. In this paper we introduce TLRW, a new STM algorithm intended for the single-chip multicore systems that are quickly taking over a large fraction of the computing landscape. We make {{the claim that the}} cost of coherence in such single chip systems is down to a level that allows one to design a scalable STM based on readwrite locks. TLRW is based on byte-locks, a novel read-write lock design with a low read-lock acquisition overhead and the ability {{to take advantage of the}} locality of reference within transactions. As we show, TLRW has a painfully simple design, one that naturally provides coherent state without validation, implicit privatization, and irrevocable transactions. Providing similar properties in STMs based on invisible-readers (such as TL 2) has typically resulted in a major loss of performance. In a series of benchmarks we show that when running on a 64 way single-chip multicore machine, TLRW delivers surprisingly good performance (competitive with and sometimes outperforming TL 2). However, on a 128 -way 2 -chip system that has higher coherence costs across the interconnect, performance deteriorates rapidly. We believe our work raises the question of whether on single-chip multicore machines, read-write lock-based STMs are the way to go. 1...|$|R
40|$|International audienceIn this paper, {{we report}} our ongoing {{investigations}} of the inherent non-determinism in contemporary execution environments that can potentially lead to divergence in state of a multi-channel hardware/software system. Our approach involved setting up of experiments to study <b>execution</b> <b>path</b> variability of a simple program by tracing <b>its</b> <b>execution</b> at the kernel level. In {{the first of the}} two experiments, we analyzed the <b>execution</b> <b>path</b> by repeated <b>execution</b> of the program. In the second, we executed in parallel two instances of the same program, each pinned to a separate processor core. Our results show that for a program executing in a contemporary hardware/software platform, there is sufcient path non-determinism in kernel space that can potentially lead to diversity in replicated architectures. We believe the execution non-determinism can impact the activation of residual systematic faults in software. If this is true, then the inherent diversity can be used together with architectural means to protect safety related systems against residual systematic faults in the operating systems...|$|R
40|$|The Implicit Path Enumeration Technique {{is often}} used to compute the WCET of control-intensive programs. This method does not {{consider}} <b>execution</b> <b>paths</b> as ordered sequences of basic blocks but instead as sets of basic blocks with their respective execution counts. This way of describing an <b>execution</b> <b>path</b> is adequate to compute <b>its</b> <b>execution</b> time, provided that safe individual WCETs for the blocks are known. Implicit path enumeration has also been used to analyze hardware schemes like instructions caches or branch predictors the behavior of which depends on the execution history. However, implicit paths do not completely capture the execution history since they do not express the order in which the basic blocks are executed. Then the estimated longest path might not be feasible and the estimated WCET might be overly pessimistic. This problem has been raised for cache analysis. In this paper, we show that it arises more acutely for branch prediction and we propose a solution to tighten the estimation of the misprediction counts. ...|$|R
5000|$|... #Caption: <b>Execution</b> <b>path</b> {{tree for}} this example. Three tests are {{generated}} {{corresponding to the}} three leaf nodes in the tree, and three <b>execution</b> <b>paths</b> in the program.|$|R
40|$|Abstract We {{propose a}} new way of {{automating}} statistical struc-tural testing, based on the combination of uniform generation of combinatorial structures, and of randomized con-straint solving techniques. More precisely, we show how to draw test cases which balance the coverage of programstructures according to structural testing criteria. The control flow graph is formalized as a combinatorial structurespecification. This provides a way of uniformly drawing <b>execution</b> <b>paths</b> which have suitable properties. Once a pathhas been drawn, the predicate characterizing those inputs which lead to <b>its</b> <b>execution</b> is solved using a constraint solv-ing library. The constraint solver is enriched with powerful heuristics {{in order to deal with}} resolution failures and ran-dom choice strategies...|$|R
40|$|This paper {{presents}} {{a method of}} intra-task dynamic voltage scaling (DVS) for SoC design with hierarchical FSM and synchronous dataflow model (in short, HFSM-SDF model). To have an optimal intra-task DVS, exact <b>execution</b> <b>paths</b> need to be determined in compile time or runtime. In general programs, since determining exact <b>execution</b> <b>paths</b> in compile time or runtime is not possible, existing methods assume worst/average-case <b>execution</b> <b>paths</b> and take static voltage scaling approaches. In our work, we exploit a property of HFSM-SDF model to calculate exact <b>execution</b> <b>paths</b> in runtime. With the information of exact <b>execution</b> <b>paths,</b> our DVS method can calculate exact remaining workload. The exact workload enables to calculate optimal voltage level which gives optimal energy consumption while satisfying the given timing constraint. Experiments show {{the effectiveness of the}} presented method in lowpower design of an MPEG 4 decoder system...|$|R
40|$|Trace-based {{compilation}} is {{a promising}} technique for language compilers and binary translators. It {{offers the potential}} to expand the compilation scopes that have traditionally been limited by method boundaries. Detecting repeating cyclic <b>execution</b> <b>paths</b> and capturing the detected repetitions into traces is a key requirement for trace selection algorithms to achieve good optimization and performance with small amounts of code. One important class of repetition detection is cyclic-path-based repetition detection, where a cyclic <b>execution</b> <b>path</b> (a path that starts and ends at the same instruction address) is detected as a repeating cyclic <b>execution</b> <b>path.</b> However, we found many cyclic paths that are not repeating cyclic <b>execution</b> <b>paths,</b> which we call false loops. A common class of false loops occurs when a method is invoked from multiple callsites...|$|R
3000|$|We first prepend {{an example}} {{regarding}} {{the interplay between}} input space coverage and <b>execution</b> <b>path</b> coverage to motivate our fuzzing algorithm. Consider a program which processes inputs from an input space I. Our aim is to generate a subset I' ⊂I of test cases (in finite amount of time) that yields maximal possible <b>execution</b> <b>path</b> coverage when processed by the target program. Further assume the program to reveal deep <b>execution</b> <b>paths</b> (covering long sequences of basic blocks) only for 3 [...]...|$|R
40|$|In this paper, {{we report}} our ongoing {{investigations}} of the inherent non-determinism in contemporary execution environments that can potentially lead to divergence in state of a multi-channel hardware/software system. Our approach involved setting up of experiments to study <b>execution</b> <b>path</b> variability of a simple program by tracing <b>its</b> <b>execution</b> at the kernel level. In {{the first of the}} two experiments, we analyzed the <b>execution</b> <b>path</b> by repeated <b>execution</b> of the program. In the second, we executed in parallel two instances of the same program, each pinned to a separate processor core. Our results show that for a program executing in a contemporary hardware/software platform, there is sufcient path non-determinism in kernel space that can potentially lead to diversity in replicated architectures. We believe the execution non-determinism can impact the activation of residual systematic faults in software. If this is true, then the inherent diversity can be used together with architectural means to protect safety related systems against residual systematic faults in the operating systems. Comment: in Yann Busnel. 11 th European Dependable Computing Conference (EDCC 2015), Sep 2015, Paris, France. 2015, Proceedings of Student Forum - EDCC 201...|$|R
40|$|We {{present a}} novel method for {{diagnosing}} configuration management errors. Our proposed approach deduces {{the state of}} a buggy computer by running predicates that test system correctness and comparing the resulting execution to that generated by running the same predicates on a reference computer. Our approach generates signatures that represent the <b>execution</b> <b>path</b> of a predicate by recording the causal dependencies of <b>its</b> <b>execution.</b> Our results show that comparisons based on dependency sets significantly outperform comparisons based on predicate success or failure, uniquely identifying the correct bug 86 – 100 % of the time. In the remaining cases, the dependency set method identifies the correct bug as one of two equally likely bugs. ...|$|R
40|$|We {{propose a}} new way of {{automating}} statistical structural testing, based on the combination of uniform generation of combinatorial structures, and of randomized constraint solving techniques. More precisely, we show how to draw test cases which balance the coverage of program structures according to structural testing criteria. The control flow graph is formalized as a combinatorial structure specification. This provides a way of uniformly drawing <b>execution</b> <b>paths</b> which have suitable properties. Once a path has been drawn, the predicate characterizing those inputs which lead to <b>its</b> <b>execution</b> is solved using a constraint solving library. The constraint solver is enriched with powerful heuristics {{in order to deal with}} resolution failures and random choice strategies...|$|R
50|$|A {{compiler}} often can, by rearranging its generated machine {{instructions for}} faster execution, improve program performance. It increases ILP (Instruction Level Parallelism) along the important <b>execution</b> <b>path</b> by statically predicting frequent <b>execution</b> <b>path.</b> Trace scheduling {{is one of}} many known techniques for doing so.|$|R
40|$|In this paper, {{we present}} a novel {{technique}} for modeling, checking, and enforcing temporal constraints in workflow processes containing conditionally executed activities. Existing workflow time modeling proposals either do not discriminate between time constraints that apply to disparate <b>execution</b> <b>paths,</b> or they treat every <b>execution</b> <b>path</b> independently. Consequently, superfluous time constraint violations may be detected at modeling time, even when each <b>execution</b> <b>path</b> does not violate any constraints. In addition, scheduling conflicts during process execution may not be detected for activities that are common to multiple <b>execution</b> <b>paths.</b> Our approach addresses these problems by (partially) unfolding the workflow graph associated with a process that contains conditionally executed activities and, then, incorporating the temporal constraints in the time calculations performed on the unfolded graph...|$|R
40|$|Linear Temporal Logic (LTL) {{is widely}} used for {{defining}} conditions on the <b>execution</b> <b>paths</b> of dynamic systems. In the case of dynamic systems that allow for nondeterministic evolutions, one has to specify, along with an LTL formula #, which are the paths that are required to satisfy the formula. Two extreme cases are the universal interpretation A. #, which requires to satisfy the formula for all the possible <b>execution</b> <b>paths,</b> and the existential interpretation E. #, which requires to satisfy the formula for some <b>execution</b> <b>paths...</b>|$|R
40|$|Abstract. In this paper, {{we present}} a novel {{technique}} for modeling, checking, and enforcing temporal constraints in work ow processes containing conditionally executed activities. Existing work ow time modeling proposals either do not discriminate between time constraints that apply to disparate <b>execution</b> <b>paths,</b> or they treat every <b>execution</b> <b>path</b> independently. Consequently, super uous time constraint violations may be detected at modeling time, even when each <b>execution</b> <b>path</b> does not violate any constraints. In addition, scheduling con icts during process execution may not be detected for activities that are common to multiple <b>execution</b> <b>paths.</b> Our approach addresses these problems by (partially) unfolding the work ow graph associated with a process that contains conditionally executed activities and, then, incorporating the temporal constraints in the time calculations performed on the unfolded graph. ...|$|R
40|$|International audienceWith the {{increasing}} performance demand in real-time systems {{it becomes more}} and more important to provide feedback to programmers and software development tools on the performance-relevant code parts of a real-time program. So far, this information was limited to an estimation of the worst-case execution time (WCET) and <b>its</b> associated worst-case <b>execution</b> <b>path</b> (WCEP) only. However, both, the WCET and the WCEP, only provide partial information. Only code parts that are on one of the WCEPs are indicated to the programmer. No information is provided for all other code parts. To give a comprehensive view covering the entire code base, tools in the spirit of program profiling are required. This work proposes an efficient approach to compute worst-case timing information for all code parts of a program using a complementary metric, called criticality. Every statement of a program is assigned a criticality value, expressing how critical the code is with respect to the global WCET. This gives valuable information how close the worst <b>execution</b> <b>path</b> passing through a specific program part is to the global WCEP. We formally define the criticality metric and investigate some of its properties with respect to dominance in control-flow graphs. Exploiting some of those properties, we propose an algorithm that reduces the overhead of computing the metric to cover complete programs. We also investigate ways to efficiently find only those code parts whose criticality is above a given threshold. Experiments using well-established real-time benchmark programs show an interesting distribution of the criticality values, revealing considerable amounts of highly critical as well as uncritical code. The metric thus provides ideal information to programmers and software development tools to optimize the worst-case execution time of these programs...|$|R
