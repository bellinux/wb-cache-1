0|10000|Public
50|$|The Java Speech API (JSAPI) is an {{application}} programming interface for cross-platform support of command and control recognizers, dictation <b>systems,</b> <b>and</b> <b>speech</b> synthesizers. Although JSAPI defines an interface only there are several implementations created by third parties, for example FreeTTS.|$|R
40|$|We {{present an}} {{overview}} of the development environment for Regulus, an Open Source platform for construction of grammar-based speech-enabled systems, focussing on recent work whose goal has been to introduce uniformity between text <b>and</b> <b>speech</b> views of Regulus-based applications. We argue the advantages of being able to switch quickly between text <b>and</b> <b>speech</b> modalities in interactive and offline testing, and describe how the new functionalities enable rapid prototyping of spoken dialogue <b>systems</b> <b>and</b> <b>speech</b> translators. ...|$|R
50|$|Rev. William Featherstone, {{headmaster}} from 1917 to 1930, {{introduced the}} prefects' system, a house <b>system</b> <b>and</b> <b>Speech</b> Day. He also moved the school from Bonham Road to a {{green field site}} in Mong Kok. Construction was completed in 1926. In February 1927, the British military authorities took the school {{for use as a}} hospital for one year.|$|R
40|$|The Teleprasenz-Consortium {{is an open}} {{group of}} {{currently}} 37 scientists of different disciplines who devote {{a major part of}} their research activities to the foundations of telepresence technology. Telepresence technology is basically understood as a means to bridge spatial and temporal gaps as well as certain kinds of concealment, inaccessibility and danger of exposure. The activities of the consortium are organized into three main branches: virtual environment, surveillance <b>and</b> control <b>systems,</b> <b>and</b> <b>speech</b> <b>and</b> language technology. A brief summary of the main activities in these areas is given...|$|R
50|$|CMUdict {{provides}} a mapping orthopraphic/phonetic for English words in their North American pronunciations. It {{is commonly used}} to generate representations for speech recognition (ASR), e.g. the CMU Sphinx <b>system,</b> <b>and</b> <b>speech</b> synthesis (TTS), e.g. the Festival system. CMUdict {{can be used as}} a training corpus for building statistical grapheme-to-phoneme (g2p) models that will generate pronunciations for words not yet included in the dictionary.|$|R
50|$|In text {{independent}} <b>systems</b> both acoustics <b>and</b> <b>speech</b> analysis {{techniques are}} used.|$|R
40|$|Abstract. Phonetic {{transcription}} is a core {{procedure of}} continuous <b>speech</b> recognition <b>systems</b> <b>and</b> <b>speech</b> synthesizers. The more correct phonetic translation {{is the more}} successful applications using phonetic transcription are. Phonological rules translate text to graphemes. These rules significantly reduce size of databases with words and it’s phonetic transcription and speeds up transcription process. This work utilizes grammatical evolution to improving original phonological rule set. New automatically learned phonological rules are added to original rule set. ...|$|R
40|$|AbstractConversion of Arabic script into phonetic {{rules is}} one of the major {{obstacles}} facing the researchers on Arabic text-to-speech <b>systems</b> <b>and</b> <b>speech</b> recognition. Although Arabic {{is one of}} the oldest languages that its sounds and phonological rules were extensively studied and documented (more than 12 centuries ago), these valuable studies need to be compiled from scattered literatures and formulated in a modern mathematical frame work. The objective of this paper is to present to the interested researchers a mathematical formulation for a comprehensive set of these rules...|$|R
40|$|J. C. Segura and C. G. Puntonet A robust {{algorithm}} for {{voice activity detection}} (VAD) is presented. It {{defines a}} likelihood ratio test (LRT) involving multiple and independent observations of the bispectra. The proposed VAD provides significant improvements in speech=pause discrimination when compared to standardised and recently reported VADs. Introduction: Voice activity detection (VAD) remains a challenging problem in <b>speech</b> processing <b>and</b> affects a number of applications including noise reduction for digital hearing aid devices, <b>speech</b> recognition <b>systems</b> <b>and</b> <b>speech</b> coding for discontinuous speech transmission (DTX) in mobile and IP networks. During the las...|$|R
40|$|SGStudio (Semantic Grammar Studio) is a grammar {{authoring}} tool that facilitates {{the development of}} spoken dialog <b>systems</b> <b>and</b> <b>speech</b> enabled applications. It enables regular software developers with little speech/linguistic background to rapidly create quality semantic grammars for automatic <b>speech</b> recognition (ASR) <b>and</b> spoken language understanding (SLU). This paper introduces {{the framework of the}} tool as well as the component technologies, including knowledge assisted example-based grammar learning, grammar controls and configurable grammar structures. Experimental results show that SGStudio not only greatly increases the productivity, but also improves the quality of the grammars developed. 1...|$|R
40|$|Abstract: The FearNot! {{application}} demonstrator, {{currently being}} {{developed for the}} EU framework V project VICTEC, uses empathic agents as a basic mechanism for engaging children in anti-bullying education. The empathic agents are driven by an affective system in interactional dramas in which both physical actions and language actions are required. This paper focuses on the different sets of Speech Act inspired language action lists developed for the project and discusses their use for an interactive language <b>and</b> action <b>system</b> for the elaboration of expressive characters. The paper also presents early development and implementation {{work as well as}} <b>system</b> <b>and</b> <b>speech</b> evaluation planning...|$|R
40|$|Decision tree {{classifiers}} (DTCs) {{are used}} successfully in many diverse {{areas such as}} radar signal classification, character recognition, remote sensing, medical diagnosis, expert <b>systems,</b> <b>and</b> <b>speech</b> recognition. Perhaps the most important feature of DTCs is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution which is often easier to interpret. A survey of current methods is presented for DTC designs and the various existing issues. After considering potential advantages of DTCs over single-state classifiers, subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed...|$|R
40|$|The aim of {{this study}} is to report the case of a patient with Fahr's Disease in order to {{describe}} the main stomatognathic and vocal changes that can be found in individuals with this disease. In order to establish the diagnosis, an assessment of the conditions of orofacial motor <b>system</b> <b>and</b> <b>speech</b> production, as well the efficiency of swallowing, was realized. Based on these assessments, there were difficulties in coordinating and sustaining muscle during <b>speech</b> <b>and</b> presence of oropharyngeal dysphagia. Speech disorders found in Fahr's disease manifest themselves in complex and cover various aspects of phonological knowledge and the diseases that affect the basal ganglia have similar frames of speech-language disorders of the stomatognathic system, being able to present a picture of dysarthria...|$|R
40|$|Abstract − The paper {{presents}} {{comparative assessment}} of Blind Source Separation methods for instantaneous mixtures. The study highlights the underlying principles of Principal Component Analysis (PCA) and Independent Component Analysis (ICA) in this context. These {{methods have been}} tested on instantaneous mixtures of synthetic periodic signals, monotonous noise from electromechanical <b>systems</b> <b>and</b> <b>speech</b> signals. In particular, methods based on Nonlinear PCA, Maximum Entropy, Mutual Information Minimization and Fast ICA, have been compared for their separation ability, processing time and accuracy. The quality of the output, {{the complexity of the}} algorithms and the simplicity (implementation) of the methods are some of the performance measures which are highlighted with respect to the above signals...|$|R
40|$|To {{facilitate}} {{the development of}} spoken dialog <b>systems</b> <b>and</b> <b>speech</b> enabled applications, we introduce SGStudio (Semantic Grammar Studio), a grammar authoring tool that enables regular software developers with little speech/linguistic background to rapidly create quality semantic grammars for automatic <b>speech</b> recognition (ASR) <b>and</b> spoken language understanding (SLU). We focus on the underlying technology of SGStudio, including knowledge assisted example-based grammar learning, grammar controls and configurable grammar structures. While the focus of SGStudio is to increase productivity, experimental results show that it also improves {{the quality of the}} grammars being developed. Key words: Automatic grammar generation, context free grammars (CFGs), example-based grammar learning, grammar controls, hidden Markov models (HMMs), n-gram model, automatic speech recognition (ASR), spoken language understandin...|$|R
30|$|Moreover, Li and Guan [17] make a {{link between}} CASA <b>system</b> <b>and</b> the <b>speech</b> quality. This {{combination}} enables a better selection of the segments which were not affected greatly by interference sources {{and use them to}} track the pitch contour which can be useful in the separation step.|$|R
40|$|Glottal {{waveform}} {{models have}} long been employed to improve quality in speech synthesis. In this paper the subjective importance of accuracy {{for some of the}} Rosenberg glottal source model parameters has been investigated through listener preference judgments. Two paired-comparison tests were conducted involving 14 and 13 listeners, respectively. The parameters, 1) relative opening duration, 2) relative closing duration and 3) pulse amplitude, were subjected to random variation, systematic compression/expansion and to quantization. The results show that if the pitch period is accurately conserved the other timing parameters and the pulse amplitude can be disturbed considerably without perceptible degradation of quality. 1. Introduction A prominent problem in the synthesis of speech, for example in text-to-speech <b>systems</b> <b>and</b> <b>speech</b> coding, is the quality or naturalness of the synthesized speech. Early LPC-based methods used impulse trains as excitation source for voiced <b>speech</b> <b>and</b> whit [...] ...|$|R
40|$|This thesis {{presents}} {{the design and}} implementation of a speech interface for bedside data entry in an intensive care unit. A speech interface is a system comprised of a speech recognition <b>system,</b> for <b>speech</b> input, <b>and</b> a <b>speech</b> generation or speech synthesis system, for speech output. These interfaces allow the operation of a computer system using voice commands providing the user with feedback via speech output. Such systems permit users to perform "hands-free" and "eyes-free" data entry or system operation in circumstances where the use of traditional manual input devices, such as a keyboard, cannot be used. This thesis begins with a literature sampling of contemporary computerized medical information <b>systems</b> <b>and</b> <b>speech</b> interface <b>systems</b> followed by {{a description of the}} hardware and software architecture of the speech interface implemented. Test results are then presented and discussed followed by an outline of future extensions for the system...|$|R
40|$|The eXtensible Markup Language (XML) (Bray, et al., 1998) is the {{emerging}} standard for data representation and exchange on the World Wide Web. The XML Framework includes very powerful mechanisms for accessing and manipulating XML documents {{that are likely}} to significantly impact the development of tools for processing natural language and annotated corpora. Introduction All language processing applications, including machine translation, information retrieval and extraction, text summarization, user/machine dialogue <b>systems,</b> <b>and</b> <b>speech</b> understanding <b>and</b> synthesis, manipulate language data represented in some electronic format. Some applications (e. g., machine translation, summarization, speech understanding) process streams of data more or less sequentially, while others (e. g., retrieval and extraction) rely more heavily on search and access over large bodies of data. In either case, processing exploits the markup in the data to assist in the analysis. For example, in textua [...] ...|$|R
30|$|Audio-to-audio alignment: Here, {{the text}} is {{converted}} to speech using a TTS <b>system,</b> <b>and</b> the synthesized <b>speech</b> is aligned with the audio [6, 7]. This method requires {{the existence of a}} TTS system.|$|R
40|$|Optimization is {{considered}} {{to be one of the}} pillars of statistical learning and also plays a major role in the design and development of intelligent systems such as search engines, recommender <b>systems,</b> <b>and</b> <b>speech</b> <b>and</b> image recognition software. Machine Learning is the study that gives the computers the ability to learn and also the ability to think without being explicitly programmed. A computer is said to learn from an experience with respect to a specified task and its performance related to that task. The machine learning algorithms are applied to the problems to reduce efforts. Machine learning algorithms are used for manipulating the data and predict the output for the new data with high precision and low uncertainty. The optimization algorithms are used to make rational decisions in an environment of uncertainty and imprecision. In this paper a methodology is presented to use the efficient optimization algorithm as an alternative for the gradient descent machine learning algorithm as an optimization algorithm...|$|R
40|$|This {{guide to}} recent sources of {{published}} inforMation in hearincj, <b>speech.</b> <b>and</b> con yntrnication disorders lists 759 references. The items {{are arranged in}} nine major sections and are annotated- except when reasons of processing prevent. The section on reviews covers hearing. language. <b>speech.</b> <b>and</b> additional reviews; the section on special serials contains review journals. annual and irregular review serials. status reports of laboratories, statistical survey series. i translation serials. and publishers' series. Indexing and abstracting publications litted indude " indexes, abstracts. research in progress. and meetings; gt tides and directories cover members and_ individuals, societies and associations: centers and services, professional training. and funding. Additional sections contain core and cognate journals; hearing. language. <b>speech.</b> <b>and</b> cognate Area bibliographies; English language. deaf language. and foreign language/English language dictionaries and glossaries; aucliolow. otolaryngology. <b>and</b> <b>speech</b> pa [...] . thology handbooks; and _ films and other media lists. M appendix cites publications from meetings dealing with hearing. the vesfibular <b>system.</b> <b>and</b> language <b>and</b> <b>speech.</b> (JD) C:= 1 C=...|$|R
50|$|OpenAIR {{was created}} to allow {{software}} components that serve their own purpose {{to communicate with each}} other in order to produce large scale, overall behavior of an intelligent system. A simple example would be to have a <b>speech</b> recognition <b>system,</b> <b>and</b> a <b>speech</b> synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue.|$|R
40|$|This paper {{presents}} {{exploration of}} speech enable operating <b>systems,</b> software, <b>and</b> applications. It {{begins with a}} description of how such <b>systems</b> work, <b>and</b> the level of accuracy that can be expected. It explains the applications of speech recognition technology in different areas education, medical, mobile computing, railway reservation, dictation, and web browsing. A brief comparison of the operating systems supported for voice, speech recognition software or tool. It gives the brief introduction about the potential of voice/speech recognition software. It explains the feature of different <b>speech</b> enable Operating <b>system</b> <b>and</b> <b>speech</b> recognition software. Windows speech recognition have many innovative features for Windows operating <b>system</b> <b>and</b> efficiently assist the computer to control, dictate, navigate, selecting the words, sending emails and correcting the words or sentences. It also explains the benefits and issue related to speech technology. In last era speech recognition technology grew tremendously. There are large number of companies who are working in these area and developing software {{for the people who are}} not able to control the system through keyboard or mouse such as physically impaired and senior citizens. This paper gives a brief introduction of <b>speech</b> enabled OS <b>and</b> <b>speech</b> recognition software. Comment: 9 pages, Proc. of the International Conference on <b>System</b> Modeling <b>and</b> Advancement in Research Trends (SMART), Teerthankar Mahaveer University, Moradabad, UP, Indi...|$|R
40|$|Stroke is {{the most}} comman cause of the speech {{disorders}} in adults. Dysarthria, as a motor speech disorder, is one of them. The treatement is based not only on vocal and articulatory exercices, but it requires the complex rehabilitation treatement. The benefit of the physiotherapeutic intervention beside speech therapy might be large. The treatement of the muskuloskeletal system allows to see the orofacial disorders {{in the context of}} the posture of the whole body. This work wants to make a view on the process of <b>speech</b> production <b>and</b> to describe relationship of the musculoskeletal <b>system</b> <b>and</b> <b>speech.</b> This work also summarises the basic informations about pathogenesis of dysarthria and particularly about the treatement. The swallowing disorders are also very comman complication of the stroke and occur very often with dysarthria. The aim of this work is to describe the specific role of the physiotherapy in the treatement of dysarthria and look for some possibility of the cooperation between physiotherapist <b>and</b> clinical <b>speech</b> therapiste. The form of the work is the research with case history. Powered by TCPDF (www. tcpdf. org...|$|R
40|$|Today's unrelenting {{increase}} {{in demand for}} information processing creates the need for novel computing concepts. Reservoir computing is such a concept that lends itself particularly well to photonic hardware implementations. Over recent years, these hardware implementations have gained maturity and now achieve state-of-the-art performance on several benchmark tasks. However, implementations so far are essentially all based on sequential data processing, leaving the inherent parallelism of photonics unexploited. Parallel implementations process all neurons simultaneously, and therefore have the potential of reducing computation time by a factor equal {{to the number of}} neurons, compared to sequential architectures. Here, we report a parallel reservoir computer that uses frequency domain multiplexing of neuron states. We illustrate its performance on standard benchmark tasks such as nonlinear channel equalization, the reproduction of a nonlinear 10 th-order <b>system,</b> <b>and</b> <b>speech</b> recognition, obtaining error rates similar to previous optical experiments. The present experiment is thus an important step towards high speed, low footprint, all optical photonic information processing...|$|R
50|$|The {{school was}} {{originally}} an all-boys grammar school, with girls being admitted {{for the first}} time in 1907. In the late 20th century the school ceased to be a grammar school, becoming one of the first neighbourhood comprehensive schools in the country. It became fully comprehensive in 1969, with partial selection (for more distant pupils) for a few years prior to that. The comprehensive school initially retained its Grammar school name and traditions such as the house <b>system</b> <b>and</b> <b>speech</b> night. These traditions were gradually scaled back, with the standard of uniform downgraded from blazers to sweaters in the 1990s. However, as of the academic year beginning 2011, the school has reverted to blazers and restored its traditional house system. Also in 2011, despite a long campaign to preserve them, the old buildings were entirely demolished or sold to as private housing by Barnsley council, and the school moved into a brand-new purpose-built building.|$|R
40|$|We review eorts in dening design {{principles}} and creating tools for building multimodal dialog sys-tems {{with emphasis on}} the speech modality. Gen-eral design {{principles and}} challenges are outlined. The focus is on <b>system</b> architecture, application <b>and</b> <b>speech</b> interface design, data collection and evaluation tools. We conclude that modularity, exibility, customizability, domain-independence and automatic dialog generation are some impor-tant features of successful dialog <b>systems</b> <b>and</b> de-sign tools. 1...|$|R
40|$|Audio {{presentation}} {{is an important}} modality in virtual storytelling. In this paper we present our work on audio presentation in our intelligent multimodal storytelling system, CONFUCIUS, which automatically generates 3 D animation <b>speech,</b> <b>and</b> non-speech audio from natural language sentences. We {{provide an overview of}} the <b>system</b> <b>and</b> describe <b>speech</b> <b>and</b> non-speech audio in virtual storytelling by using linguistic approaches. We discuss several issues in auditory display, such as its relation to verb and adjective ontology, concepts and modalities, and media allocation. Finally we conclude that introducing linguistic knowledge provides more intelligent virtual storytelling, especially in audio presentation. 1...|$|R
40|$|Sound <b>systems</b> <b>and</b> <b>speech</b> {{technologies}} {{can benefit}} greatly from {{a deeper understanding}} of how the auditory <b>system,</b> <b>and</b> particularly the auditory cortex, is able to parse complex acoustic scenes into meaningful auditory objects and streams under adverse conditions. In the current work, a biologically plausible model of this process is presented, where the role of cortical mechanisms in organizing complex auditory scenes is explored. The model consists of two stages: (i) a feature analysis stage that maps the acoustic input into a multidimensional cortical representation and (ii) an integrative stage that recursively builds up expectations of how streams evolve over time and reconciles its predictions with the incoming sensory input by sorting it into different clusters. This approach yields a robust computational scheme for speaker separation under conditions of speech or music interference. The model can also emulate the archetypal streaming percepts of tonal stimuli that have long been tested in human subjects. The implications of this model are discussed with respect to the physiological correlates of streaming in the cortex as well as the role of attention and other top-down influences in guiding sound organization...|$|R
40|$|Abstract. This paper {{describes}} the architecture {{and the performance}} of a new programmable 16 -bit Digital Signal Processor (DSP) engine. It is developed specifically for next generation wireless digital <b>systems</b> <b>and</b> <b>speech</b> applications. Besides providing a basic instruction set, similar to current day 16 -bit DSP’s, it contains distinctive architectural features and unique instructions, which make the engine highly efficient for compute-intensive tasks such as vector quantization and Viterbi operations. The datapath contains two Multiply-Accumulate units and one ALU. The external memory bandwidth is kept to two data busses and two corresponding address busses. Still, the internal bus network is designed such that all three units are operating in parallel. This parallelism is reflected in the performance benchmarks. For example, an FIR filter of N taps will take N/ 2 instruction cycles compared to N for a general purpose 16 -bit DSP, and it will require only half the number of memory accesses of a general purpose DSP. This efficiency is reflected in the very low MIPS requirement to implement cellular standards. 1...|$|R
40|$|Background: Children with {{specific}} <b>speech</b> <b>and</b> language difficulties (SSLD) pose {{a challenge to}} the education <b>system,</b> <b>and</b> to <b>speech</b> <b>and</b> language therapists who support them, {{as a result of their}} language needs and associated educational and social-behavioural difficulties. The development of inclusion raises questions regarding appropriate provision, whether the tradition of language units or full inclusion into mainstream schools. Aims: To gather the views of <b>speech</b> <b>and</b> language therapy service managers in England and Wales regarding approaches to service delivery, terminology and decision-making for educational provision, and the use of direct and indirect (consultancy) models of intervention. Method & Procedures: The study reports on a national survey of <b>speech</b> <b>and</b> language therapy (SLT) services in England and Wales (129 respondents, 72. 1...|$|R
50|$|Swype is {{a virtual}} {{keyboard}} for touchscreen smartphones and tablets originally developed by Swype Inc., founded in 2002, where the user enters words by sliding a finger or stylus from the first letter of a word to its last letter, lifting only between words. It uses error-correction algorithms and a language model to guess the intended word. It also includes a predictive text <b>system,</b> handwriting <b>and</b> <b>speech</b> recognition support. Swype was first commercially available on the Samsung Omnia II running Windows Mobile, and was originally pre-loaded on specific devices.|$|R
40|$|Presented at the 11 th International Conference on Auditory Display (ICAD 2005) Audio {{presentation}} {{is an important}} modality in virtual storytelling. In this paper we present our work on audio presentation in our intelligent multimodal storytelling system, CONFUCIUS, which automatically generates 3 D animation <b>speech,</b> <b>and</b> non-speech audio from natural language sentences. We {{provide an overview of}} the <b>system</b> <b>and</b> describe <b>speech</b> <b>and</b> non-speech audio in virtual storytelling by using linguistic approaches. We discuss several issues in auditory display, such as its relation to verb and adjective ontology, concepts and modalities, and media allocation. Finally we conclude that introducing linguistic knowledge provides more intelligent virtual storytelling, especially in audio presentation...|$|R
2500|$|Finally, {{alternative}} and augmentative communication approaches to treatment of apraxia are highly individualized for each patient. However, they often involve a [...] "comprehensive communication system" [...] that may include [...] "speech, a communication book aid, a spelling system, a drawing system, a gestural <b>system,</b> technologies, <b>and</b> informed <b>speech</b> partners".|$|R
40|$|We hereby present {{our first}} steps towards linking an {{embodied}} <b>speech</b> acquisition <b>system</b> <b>and</b> a <b>speech</b> production module, {{in order to}} provide the system with the ability to produce acquired speech representations. Due to the type of interaction planned with the system, we endowed it with a child-like voice, concretized with the use a vocoderlike technique for speech synthesis. The task in hand consists of finding and using a correspondence between configurations in the tutor's acoustic parameter space, which might be untangible for the <b>system's</b> voice, <b>and</b> phonologically equivalents in the robot's...|$|R
