687|1379|Public
25|$|The PPS {{approach}} can improve accuracy {{for a given}} sample size by concentrating sample on large elements that have {{the greatest impact on}} population estimates. PPS sampling is commonly used for surveys of businesses, where element size varies greatly and auxiliary information is often available—for instance, a survey attempting to measure the number of guest-nights spent in hotels might use each hotel's number of rooms as an <b>auxiliary</b> <b>variable.</b> In some cases, an older measurement of the variable of interest can be used as an <b>auxiliary</b> <b>variable</b> when attempting to produce more current estimates.|$|E
25|$|Still another {{possibility}} is that the code generated may employ an <b>auxiliary</b> <b>variable</b> as the loop variable, possibly held in a machine register, whose value {{may or may not be}} copied to I on each iteration. Again, modifications of I would not affect the control of the loop, but now a disjunction is possible: within the loop, references to the value of I might be to the (possibly altered) current value of I or to the <b>auxiliary</b> <b>variable</b> (held safe from improper modification) and confusing results are guaranteed. For instance, within the loop a reference to element I of an array would likely employ the <b>auxiliary</b> <b>variable</b> (especially if it were held in a machine register), but if I is a parameter to some routine (for instance, a print-statement to reveal its value), it would likely be a reference to the proper variable I instead. It is best to avoid such possibilities.|$|E
2500|$|... {{introduce}} the <b>auxiliary</b> <b>variable</b> [...] and to express [...] {{as a function}} of [...] Derivatives of [...] with respect to time may be rewritten as derivatives of [...] with respect to angle.|$|E
40|$|Imputation models {{sometimes}} use <b>auxiliary</b> <b>variables</b> that, {{though not}} part of the planned analysis, can improve the accuracy of imputed values and the efficiency of point estimates. A recent article, using evidence from simulations, argued that the use of <b>auxiliary</b> <b>variables</b> in imputation did not improve efficiency. We review the simulation results and find that the use of <b>auxiliary</b> <b>variables</b> did improve efficiency; under some conditions the efficiency gain was equivalent to increasing the sample size by a quarter. We give an example from our own research where the efficiency gained from <b>auxiliary</b> <b>variables</b> was equivalent to increasing the sample size by three quarters, and pushed some estimates from statistical insignificance to significance. For <b>auxiliary</b> <b>variables</b> to make a difference, there must be a lot of missing data, some estimates must be near the border of significance, and the <b>auxiliary</b> <b>variables</b> must be excellent predictors of the missing values. Comment: 10 pages, 2 tables, 0 figure...|$|R
40|$|Abstract Background Multiple {{imputation}} {{is becoming}} increasingly popular. Theoretical considerations as well as simulation {{studies have shown that}} the inclusion of <b>auxiliary</b> <b>variables</b> is generally of benefit. Methods A simulation study of a linear regression with a response Y and two predictors X 1 and X 2 was performed on data with n = 50, 100 and 200 using complete cases or multiple imputation with 0, 10, 20, 40 and 80 <b>auxiliary</b> <b>variables.</b> Mechanisms of missingness were either 100 % MCAR or 50 % MAR + 50 % MCAR. <b>Auxiliary</b> <b>variables</b> had low (r=. 10) vs. moderate correlations (r=. 50) with X’s and Y. Results The inclusion of <b>auxiliary</b> <b>variables</b> can improve a multiple imputation model. However, inclusion of too many variables leads to downward bias of regression coefficients and decreases precision. When the correlations are low, inclusion of <b>auxiliary</b> <b>variables</b> is not useful. Conclusion More research on <b>auxiliary</b> <b>variables</b> in multiple imputation should be performed. A preliminary rule of thumb could be that the ratio of variables to cases with complete data should not go below 1 : 3. </p...|$|R
40|$|This {{study is}} {{concerned}} in reducing high dimensionality problem of <b>auxiliary</b> <b>variables</b> in the calibration estimation {{with the presence}} of nonresponse. The calibration estimation is a weighting method assists to compensate for the nonresponse in the survey analysis. Calibration estimation using principal components (PCs) is new idea in the literatures. Principal component analysis (PCA) is used in reduction dimension of the <b>auxiliary</b> <b>variables.</b> PCA in calibration estimation is presented as an alternative method for choosing the <b>auxiliary</b> <b>variables.</b> In this study, simulation on the real data is used and nonresponse mechanism is applied on the sampled data. The calibration estimator is compared using different criteria such as varying the nonresponse rate and increasing the sample size. From the results, although the calibration estimation based on the principal components have reasonable outputs to use instead of the whole <b>auxiliary</b> <b>variables</b> for the means, the variance is very large compared with based on original <b>auxiliary</b> <b>variables.</b> Finally, we identified the principal component analysis is not efficient in the reduction of high dimensionality problem of <b>auxiliary</b> <b>variables</b> in the calibration estimation for large sample sizes...|$|R
2500|$|In {{some cases}} the sample {{designer}} has access to an [...] "auxiliary variable" [...] or [...] "size measure", believed to be correlated to the variable of interest, for each element in the population. These data {{can be used to}} improve accuracy in sample design. One option is to use the <b>auxiliary</b> <b>variable</b> as a basis for stratification, as discussed above.|$|E
2500|$|... where x* is the Galilean {{transformation}} x-vt. Except {{the additional}} [...] {{in the time}} transformation, this is the complete Lorentz transformation. While [...] is the [...] "true" [...] time for observers resting in the aether, [...] is an <b>auxiliary</b> <b>variable</b> only for calculating processes for moving systems. It is also important that Lorentz and later also Larmor formulated this transformation in two steps. At first an implicit Galilean transformation, and later the expansion into the [...] "fictitious" [...] electromagnetic system {{with the aid of}} the Lorentz transformation. In order to explain the negative result of the Michelson–Morley experiment, he (1892b) introduced the additional hypothesis that also intermolecular forces are affected in a similar way and introduced length contraction in his theory (without proof as he admitted). The same hypothesis was already made by George FitzGerald in 1889 based on Heaviside's work. While length contraction was a real physical effect for Lorentz, he considered the time transformation only as a heuristic working hypothesis and a mathematical stipulation.|$|E
2500|$|Lorentz's initial {{theory was}} created between 1892 and 1895 and {{was based on}} a {{completely}} motionless aether. It explained the failure of the negative aether drift experiments to first order in v/c by introducing an <b>auxiliary</b> <b>variable</b> called [...] "local time" [...] for connecting systems at rest and in motion in the aether. In addition, the negative result of the Michelson–Morley experiment led to the introduction of the hypothesis of length contraction in 1892. However, other experiments also produced negative results and (guided by Henri Poincaré's principle of relativity) Lorentz tried in 1899 and 1904 to expand his theory to all orders in v/c by introducing the Lorentz transformation. In addition, he assumed that also non-electromagnetic forces (if they exist) transform like electric forces. However, Lorentz's expression for charge density and current were incorrect, so his theory did not fully exclude the possibility of detecting the aether. [...] Eventually, it was Henri Poincaré who in 1905 corrected the errors in Lorentz's paper and actually [...] incorporated non-electromagnetic forces (including gravitation) within the theory, which he called [...] "The New Mechanics". [...] Many aspects of Lorentz's theory were incorporated into special relativity (SR) with the works of Albert Einstein and Hermann Minkowski.|$|E
40|$|The aim of {{this study}} is to {{investigate}} a novel method for dealing with incomplete scale scores in longitudinal data that result from missing item responses. This method includes item information as <b>auxiliary</b> <b>variables,</b> which is advantageous because it incorporates the observed item-level data while maintaining the scale scores as the focus of the analysis. These <b>auxiliary</b> <b>variables</b> do not change the analysis model, but improve missing data handling. The investigated novel method uses the item scores or some summary of a parcel of item scores as <b>auxiliary</b> <b>variables,</b> while treating the scale scores missing in a latent growth model. The performance of these methods was examined in several simulated longitudinal data conditions and analyzed through bias, mean square error, and coverage. Results show that including the item information as <b>auxiliary</b> <b>variables</b> results in rather dramatic power gains compared with analyses without <b>auxiliary</b> <b>variables</b> under varying conditions...|$|R
40|$|AbstractThe role of <b>auxiliary</b> <b>variables</b> in the {{specification}} of concurrent objects with multiple {{inputs and outputs}} is examined. <b>Auxiliary</b> <b>variables</b> needed in a specification are defined through logical assertions on the interface variables. Necessary conditions and generic rules for the definition of such <b>auxiliary</b> <b>variables</b> are presented. The method is illustrated by specifying a concurrent buffer. Based on this specification, a number of useful properties of concurrent buffers and systems composed of them are derived through formal manipulation...|$|R
40|$|To {{deal with}} missing data that arise due to {{participant}} nonresponse or attrition, methodologists have recommended an "inclusive" strategy where a large set of <b>auxiliary</b> <b>variables</b> {{are used to}} inform the missing data process. In practice, the set of possible <b>auxiliary</b> <b>variables</b> is often too large. We propose using principal components analysis (PCA) {{to reduce the number}} of possible <b>auxiliary</b> <b>variables</b> to a manageable number. A series of Monte Carlo simulations compared the performance of the inclusive strategy with eight <b>auxiliary</b> <b>variables</b> (inclusive approach) to the PCA strategy using just one principal component derived from the eight original variables (PCA approach). We examined the influence of four independent variables: magnitude of correlations, rate of missing data, missing data mechanism, and sample size on parameter bias, root mean squared error, and confidence interval coverage. Results indicate that the PCA approach results in unbiased parameter estimates and potentially more accuracy than the inclusive approach. We conclude that using the PCA strategy {{to reduce the number of}} <b>auxiliary</b> <b>variables</b> is an effective and practical way to reap the benefits of the inclusive strategy in the presence of many possible <b>auxiliary</b> <b>variables...</b>|$|R
5000|$|Introducing an <b>auxiliary</b> <b>variable</b> [...] {{the problem}} can be reformulated: ...|$|E
5000|$|Using {{the lens}} {{equation}} {{we can solve}} for the <b>auxiliary</b> <b>variable</b> f1: ...|$|E
5000|$|To {{sample a}} random {{variable}} X with density f(x) we introduce an <b>auxiliary</b> <b>variable</b> Y and iterate as follows: ...|$|E
40|$|In finite {{population}} sampling {{it is common}} to use information from one or several <b>auxiliary</b> <b>variables</b> through indirect estimators, such as ratio or product estimators. We propose an alternative estimator to the generalized multivariate estimator for cases where several <b>auxiliary</b> <b>variables</b> correlate either positively or negatively with the main variable. This new almost unbiased estimator based on the jackknife technique is always more precise than either the simple expansion estimator (the sample mean) or than the ratio, product, or ratio - product estimators built using any of the available <b>auxiliary</b> <b>variables...</b>|$|R
40|$|This {{paper is}} an {{extension}} of Hanif, Hamad and Shahbaz estimator [1] for two-phase sampling. The aim {{of this paper is to}} develop a regression type estimator with two <b>auxiliary</b> <b>variables</b> for two - phase sampling when we don’t have any type of information about <b>auxiliary</b> <b>variables</b> at population level. To avoid multi-collinearity, it is assumed that both <b>auxiliary</b> <b>variables</b> have minimum correlation. Mean square error and bias of proposed estimator in two-phase sampling is derived. Mean square error of proposed estimator shows an improvement over other well known estimators under the same case. </p...|$|R
40|$|Sample weights can be {{calibrated}} {{to reflect}} the known population to-tals {{of a set of}} <b>auxiliary</b> <b>variables.</b> Predictors of finite population totals calculated using these weights have low bias if the these variables are re-lated to the variable of interest, but can have high variance if too many <b>auxiliary</b> <b>variables</b> are used. This article develops an “adaptive calibra-tion ” approach, where the <b>auxiliary</b> <b>variables</b> to be used in weighting are selected using sample data. Adaptively calibrated estimators are shown to have lower mean squared error and better coverage properties than non-adaptive estimators in many cases...|$|R
50|$|The PPS {{approach}} can improve accuracy {{for a given}} sample size by concentrating sample on large elements that have {{the greatest impact on}} population estimates. PPS sampling is commonly used for surveys of businesses, where element size varies greatly and auxiliary information is often available—for instance, a survey attempting to measure the number of guest-nights spent in hotels might use each hotel's number of rooms as an <b>auxiliary</b> <b>variable.</b> In some cases, an older measurement of the variable of interest can be used as an <b>auxiliary</b> <b>variable</b> when attempting to produce more current estimates.|$|E
5000|$|Still another {{possibility}} is that the code generated may employ an <b>auxiliary</b> <b>variable</b> as the loop variable, possibly held in a machine register, whose value {{may or may not be}} copied to [...] on each iteration. Again, modifications of [...] would not affect the control of the loop, but now a disjunction is possible: within the loop, references to the value of [...] might be to the (possibly altered) current value of [...] or to the <b>auxiliary</b> <b>variable</b> (held safe from improper modification) and confusing results are guaranteed. For instance, within the loop a reference to element [...] of an array would likely employ the <b>auxiliary</b> <b>variable</b> (especially if it were held in a machine register), but if [...] is a parameter to some routine (for instance, a print-statement to reveal its value), it would likely be a reference to the proper variable [...] instead. It is best to avoid such possibilities.|$|E
5000|$|... where [...] is an <b>auxiliary</b> <b>variable</b> to {{hold the}} {{recursion}} value. (We take it that [...] still returns a Boolean {{even if it is}} given a non-Boolean argument.) By an eta-reduction, we obtain, ...|$|E
40|$|Abstract—Solvers for {{the maximum}} satisfiability (MaxSAT) problem—a {{well-known}} optimization variant of Boolean satis-fiability (SAT) —are finding {{an increasing number}} of applica-tions. Preprocessing has proven {{an integral part of the}} SAT-based approach to efficiently solving various types of real-world problem instances. It was recently shown that SAT preprocessing for MaxSAT becomes more effective by re-using the <b>auxiliary</b> <b>variables</b> introduced in the preprocessing phase directly in the SAT solver within a core-based hybrid MaxSAT solver. We take this idea of re-using <b>auxiliary</b> <b>variables</b> further by identifying them among variables already present in the input MaxSAT in-stance. Such variables can be re-used already in the preprocessing step, avoiding the introduction of multiple layers of new <b>auxiliary</b> <b>variables</b> in the process. Empirical results show that by detecting <b>auxiliary</b> <b>variables</b> in the input MaxSAT instances can lead to modest additional runtime improvements when applied before preprocessing. Furthermore, we show that by re-using <b>auxiliary</b> <b>variables</b> not only within preprocessing but also as assumptions within the SAT solver of the MaxHS MaxSAT algorithm can alone lead to performance improvements similar to those observed by applying SAT-based preprocessing. I...|$|R
5000|$|... we {{introduce}} <b>auxiliary</b> <b>variables</b> [...] and obtainthe equivalent equation ...|$|R
5000|$|In the {{formulation}} LP2, we {{have replaced the}} expression (...) with a continuous variable zi. Similarly, we can use standard solvers to solve the linearization problem. Note that Glover’s linearization only includes [...] <b>auxiliary</b> <b>variables</b> with [...] constraints while standard linearization requires [...] <b>auxiliary</b> <b>variables</b> and [...] constraints to achieve linearity.|$|R
5000|$|Our <b>auxiliary</b> <b>variable</b> Y {{represents}} a horizontal [...] "slice" [...] of the distribution. The rest of each iteration {{is dedicated to}} sampling an x value from the slice which {{is representative of the}} density of the region being considered.|$|E
5000|$|First, reorder the {{parameters}} [...] {{such that they}} are sorted in descending order (this is only to speed up computation and not strictly necessary). Now, for each trial, draw an <b>auxiliary</b> <b>variable</b> X from a uniform (0, 1) distribution. The resulting outcome is the component ...|$|E
5000|$|... 1904: Lorentz derived the {{previous}} relations {{in a more}} detailed way, namely {{with respect to the}} properties of particles resting in the system [...] and the moving system , with the new <b>auxiliary</b> <b>variable</b> [...] equal to [...] compared to the one in 1899, thus: ...|$|E
40|$|The use of weakest-precondition {{predicate}} tranformers in the derivation of sequential, process-control {{software is}} discussed. Only one extension to Dijkstra's calculus for deriving ordinary sequential programs {{was found to}} be necessary: function-valued <b>auxiliary</b> <b>variables.</b> These <b>auxiliary</b> <b>variables</b> are needed for reasoning about states of a physical process that exist during program transitions...|$|R
40|$|Many dual frame estimators {{have been}} {{proposed}} in the statistics literature. Some of these estimators are theoretically optimal but hard to apply in practice, whereas others are applicable but have larger variances than the first group. In this paper, a Joint Calibration Estimator (JCE) is proposed that is simple to apply in practice and meets many desirable properties for dual frame estimators. The JCE is asymptotically design unbiased conditional on the strong relationship between the estimation <b>variable</b> and the <b>auxiliary</b> <b>variables</b> employed in the calibration. The JCE achieves better performance when the <b>auxiliary</b> <b>variables</b> can fully explain the variability in the study variables or at least when the <b>auxiliary</b> <b>variables</b> are strong correlates of the estimation variables. As opposed to the standard dual frame estimators, the JCE does not require domain membership information. Even if included in the JCE <b>auxiliary</b> <b>variables,</b> {{the effect of the}} randomly misclassified domains does not exceed the random measurement error effect. Therefore, the JCE tends to be robust for the misclassified domains if included in the <b>auxiliary</b> <b>variables.</b> Meanwhile, the misclassified domains can significantly affect the unbiasedness of the standard dual frame estimators as proved theoretically and empirically in this paper...|$|R
40|$|Some {{improved}} estimators {{of population}} mean {{has been proposed}} in two phase and multiphase sampling using information on two and several <b>auxiliary</b> <b>variables.</b> The mini-mum variance of the proposed estimators has been obtained. Comparison of the proposed estimators has been done with some available estimators of two phase sampling that utilizes information of two and several <b>auxiliary</b> <b>variables...</b>|$|R
50|$|In 1985 and 1986 the SNCF BB 22200 {{locomotives}} numbers 22379 and 22380 were {{modified to}} test dual voltage electric traction equipment, the microprocessor control, and <b>auxiliary</b> <b>variable</b> speed induction motors. 20012 {{was used to}} test the combination of pneumatic brake system and electric brake system.|$|E
5000|$|In {{some cases}} the sample {{designer}} has access to an [...] "auxiliary variable" [...] or [...] "size measure", believed to be correlated to the variable of interest, for each element in the population. These data {{can be used to}} improve accuracy in sample design. One option is to use the <b>auxiliary</b> <b>variable</b> as a basis for stratification, as discussed above.|$|E
5000|$|For simplicity, the {{following}} algorithm (and the other algorithms in this article) uses parallel assignments. In {{a programming language}} which does not have this feature, the parallel assignments need to be simulated with an <b>auxiliary</b> <b>variable.</b> For example, the first one, (old_r, r) := (r, old_r - quotient *r)is equivalent to prov := r; r := old_r - quotient * prov; old_r := prov;and similarly for the other parallel assignments.This leads to {{the following}} code: ...|$|E
40|$|In this paper, we have {{proposed}} a class of mixture regression-cum-ratio estimator for estimating population mean by using information on multiple <b>auxiliary</b> <b>variables</b> and attributes simulta-neously in single-phase sampling and analyzed {{the properties of the}} estimator. An empirical was carried out to compare the performance of the proposed estimator with the existing estimators of finite population mean using simulated population. It was found that the mixture regression-cum-ratio estimator was more efficient than ratio and regression estimators using one auxiliary varia-ble and attribute, ratio and regression estimators using multiple <b>auxiliary</b> <b>variables</b> and attributes and regression-cum-ratio estimators using multiple <b>auxiliary</b> <b>variables</b> and attributes in single-phase sampling for finite population...|$|R
40|$|When using {{small area}} {{estimation}} models, {{the presence of}} outlying observations in the response and/or in the <b>auxiliary</b> <b>variables</b> can severely affect the estimates of the model parameters, which can in turn affect the small area estimates produced using these models. In this paper we propose an M-quantile estimator of the small area mean that is robust {{to the presence of}} outliers in the response variable and in the continuous <b>auxiliary</b> <b>variables.</b> To estimate the variability of this estimator we propose a non-parametric bootstrap estimator. The performance of the proposed estimator is evaluated by means of model- and design-based simulations and by an application to real data. In these comparisons we also include the extension of the Robust EBLUP able to down-weight the outliers in the <b>auxiliary</b> <b>variables.</b> The results show that in the presence of outliers in the <b>auxiliary</b> <b>variables</b> the proposed estimator outperforms its traditional version that takes into account the presence of outliers only in the response variable...|$|R
40|$|International audienceIn survey sampling, {{calibration}} is {{a popular}} tool used to make total estimators consistent with known totals of <b>auxiliary</b> <b>variables</b> and to reduce variance. When the number of <b>auxiliary</b> <b>variables</b> is large, calibration on all the variables may lead to estimators of totals whose mean squared error (MSE) {{is larger than the}} MSE of the Horvitz-Thompson estimator even if this simple estimator does not take account of the available auxiliary information. We study a new technique based on dimension reduction through principal components that can be useful in this large dimension context. Calibration is performed on the first principal components, which can be viewed as the synthetic variables containing {{the most important part of}} the variability of the <b>auxiliary</b> <b>variables.</b> When some <b>auxiliary</b> <b>variables</b> play a more important role than others, the method can be adapted to provide an exact calibration on these variables. Some asymptotic properties are given in which the number of variables is allowed to tend to infinity with the population size. A data driven selection criterion of the number of principal components ensuring that all the sampling weights remain positive is discussed. The methodology of the paper is illustrated, in a multipurpose context, by an application to the estimation of electricity consumption with the help of 336 <b>auxiliary</b> <b>variables...</b>|$|R
