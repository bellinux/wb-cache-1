7|30|Public
40|$|A new parallel-plate {{capacitor}} fixture {{has been}} designed and successfully used to measure dielectric loss of polyelectrolyte solutions with volumes as low as droplets of 13 – 26  μL. It is particularly useful when studying polypeptides that are either high-cost or can be synthesized only in limited quantities. The ease with which the fixture {{can be used to}} obtain preliminary dielectric loss data yields savings in time and cost. In this study capacitance measurements were performed {{in a wide range of}} frequencies between 1 and 800  MHz using an Agilent 4191 RF Impedance <b>Analyzer.</b> <b>Accuracy</b> of measurements was carefully examined through a comparison of measured conductivity of 1 M NaCl against Stogryn's equation for conductivity. A 0. 3 % difference between the experimentally measured and theoretically calculated results has been found, demonstrating the validity of the proposed analysis method...|$|E
40|$|Modern Vector Network Analyzers are {{powerful}} vector measurement systems for high frequency device characterization, {{in terms of}} scattering parameters. These instruments need a complex calibration procedure, aimed to remove {{the greater part of}} the errors associated to the hardware and to trace measurements to primary standards. Actually, these primary standards are only indirectly associated to SI fundamental electrical quantities. The more suitable standards are sections of transmission lines, characterized by means of dimensional measurements. The system accuracy is mainly related to the uncertainty of the standards, which propagates to the calibration coefficients of the network analyzer, according to the algorithms used by the calibration process. Systematic errors not removed by the calibration process, define a scattering matrix that must be anyway evaluated, for the final measurement uncertainty. Network <b>Analyzer</b> <b>accuracy</b> assessment after calibration process is widely described in the literature, but some clarifications are necessary, in order to avoid not consistent interpretations. For the same reasons, the measurand analysis would need a revision. Indeed, also in high-level comparisons, discrepancies arise among participants using instrumentation of equivalent performances, a result clearly due to different interpretation of the analysis rules. The authors suggest simple principles, in order to improve the harmonization of the measurement result...|$|E
40|$|BACKGROUND: Profiles from serum {{biochemical}} analyzers {{include the}} concentration of strong electrolytes (including l-lactate), total carbon dioxide (tCO 2), and total protein. These variables are associated with changes in acid-base balance. Application of physicochemical principles may allow predicting acid-base balance from serum biochemistry without measuring whole blood pH and pCO 2. OBJECTIVES: The {{purpose of the study}} was to determine if the acid-base status of critically ill horses could be accurately predicted using variables included in standard serum biochemical profiles. METHODS: Two jugular venous blood samples were prospectively obtained from critically ill horses and foals. Samples were analyzed using a whole blood gas and pH analyzer (BG) and a serum biochemistry multi analyzer system (AMAS). Linear regression, Deming regression, and Bland-Altman plots were used for method comparison and P <. 05 was considered significant. RESULTS: Values from 70 horses and foals for Na, K, Cl, and total protein concentrations, and consequently the calculated variables used for acid base interpretation, were different between the AMAS and BG analyzer. Using physicochemical principles, BG results accurately predicted pH, whereas the AMAS results did not when a fixed value for pCO 2 was used. CONCLUSIONS: Measurement of pCO 2 is required in critically ill horses for accurate prediction of whole blood pH. Differences in the measured values of Na and Cl concentration exist when measured in serum by the AMAS and in whole blood or plasma by BG, indicating that the accurate prediction of whole blood pH is analyzer-dependent. Application of physicochemical principles to plasma or serum provides a practical method to evaluate <b>analyzer</b> <b>accuracy...</b>|$|E
30|$|Carbon, hydrogen, nitrogen, and sulfur {{contents}} of asphaltene fractions were determined using a Thermo Finnigan EA 1112 elemental <b>analyzer.</b> Experimental <b>accuracy</b> of these measurements was 0.2  %.|$|R
40|$|We {{present an}} {{approach}} to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from {{the output of the}} <b>analyzer.</b> We obtain <b>accuracy</b> rates on all tasks in the high nineties. ...|$|R
30|$|Various {{levels of}} air were input {{to the pilot}} plant to {{increase}} the O 2 concentration in five increments from 225  ppm to 4.1  vol.% {{to determine the effect}} of O 2 on the concentration of gas-phase and particulate-phase contaminants in the syngas. The concentration of N 2 in the syngas was used to estimate the average concentration of O 2 that was input into the process, as air. O 2 concentrations were also measured in real-time using a Teledyne Model 3190 Oxygen <b>Analyzer.</b> The <b>accuracy</b> of these measurements was about ± 5  %. We define an O 2 -free environment when the O 2 is less than about 500  ppm.|$|R
40|$|We {{present an}} {{experimental}} investigation on {{the dissolution of}} uniaxially stressed crystals of NaClO 3 in contact with brine. The crystals are immersed in a saturated fluid, stressed vertically by a piston and monitored constantly in situ with a CCD camera. The experiments are temperature-controlled and uniaxial shortening of the sample is measured with a high-resolution capacitance analyzer. Once the crystal is stressed it develops dissolution grooves on its free surface. The grooves are oriented with their long axis perpendicular to the direction of compressive stress and the initial distance between the parallel grooves is {{in accordance with the}} Asaro-Tiller-Grinfeld instability. We observe a novel, transient evolution of this roughness: The grooves on the crystal surface migrate upwards (against gravity), grow in size and the inter-groove distance increases linearly with time. During the coarsening of the pattern this switches from a one-dimensional geometry of parallel grooves to a two-dimensional geometry with horizontal and vertical grooves. At the end of the experiment one large groove travels across the crystal and the surface becomes smooth again. Uniaxial shortening of the crystal by pressure solution creep decays exponentially with time and shows no long term creep within the range of the resolution of the capacitance <b>analyzer</b> (<b>accuracy</b> of 100 nm over a period of 14 days). This indicates that, while active, the fast transient processes on the free surface increase the solution concentration and thereby significantly slow down or stop pressure solution {{at the top of the}} crystal. This novel feedback mechanism can explain earlier results of cyclic pressure solution creep and demands development of a more complex theory of pressure-solution creep including processes that act on free surfaces...|$|E
40|$|Methane (CH 4 ) is {{considered}} to be the second main contributor to the global greenhouse gas effect, with major CH 4 emissions originating from livestock. Accurate measurements from ruminating herds are required to improve emission coefficients used in national emission inventories, and to evaluate mitigation strategies. Previous measurements of enteric methane emissions from domestic animals have been carried out in artificial conditions such as laboratory chambers, or by fitting individual animals with capillary tubes and using SF 6 as a tracer. Here we evaluated the reliability of eddy covariance technique (EC), already used for CO 2 fluxes, for continuous CH 4 measurements over a grazed field plot. <b>Analyzer</b> <b>accuracy</b> and reliability of eddy covariance technique were tested against field scale measurements with the SF 6 tracer technique, Gaussian plume model and emission factors (i. e. IPCC). Results indicate a better agreement between EC and SF 6 method when grazing heifers were parked close to the EC setup. However, a systematic underestimation of EC data appeared and even more when the distance between the source (ruminating heifers) and EC setup (mast) was increased. A two-dimensional footprint density function allowed to correct for the dilution effect on measured CH 4 and led to a good agreement with results based on the SF 6 technique (on average 231 and 252 g CH 4 ha− 1 over the grazing experiment, respectively). Estimations of the CH 4 budgets for the whole grazing season were in line with estimates (i. e. emission factor coefficients) based on feed intake and animal live weight as well as SF 6 technique. IPCC method Tier 2, however, led to an overestimation of CH 4 fluxes on our site...|$|E
40|$|Accurate CO 2 {{concentration}} gradient measurements {{are needed for}} the computation of advective flux terms, which {{are part of the}} full Net Ecosystem Exchange (NEE) budget equation. A typical draw back of current gradient measurement designs in advection research is the inadequate sampling of complex flow phenomena using too few observation points in space and time. To overcome this draw back, a new measurement design is presented which allows the parallel measurement of several sampling points at a high frequency. Due to the multi-analyzer nature of the design, inter-instrument bias becomes more of a concern compared to conventional setups. Therefore a statistical approach is presented which allows for accurate observations of {{concentration gradient}}s, which are typically small in relation to <b>analyzer</b> <b>accuracy,</b> to be obtained. This bias correction approach applies a conditional, time dependent signal correction. The correction depends on a mixing index based on cross correlation analysis, which characterizes the degree of mixing of the atmosphere between individual sample points. The approach assumes statistical properties of probability density functions (pdf) of concentration differences between a sample point and the field average which are common to the pdf's from several sample points. The applicability of the assumptions made was tested by Large Eddy Simulation (LES) using the model PALM and could be verified for a test case of well mixed conditions. The study presents concentration time series before and after correction, measured at a 2 m height in the sub-canopy at the FLUXNET spruce forest site Waldstein-Weidenbrunnen (DE-Bay), analyzes the dependence of statistical parameters of pdf's from atmospheric parameters such as stratification, quantifies the errors and evaluates the performance of the bias correction approach. The improvements that are achieved by applying the bias correction approach are one order of magnitude larger than possible errors associated with it, which is a strong incentive to use the correction approach. In conclusion, the presented bias correction approach is well suited for - but not limited to - horizontal gradient measurements in a multi-analyzer setup, which would not have been reliable without this approach. Finally, possible future improvements of the bias correction approach are outlined and further fields of application indicated...|$|E
40|$|Traditional and new {{techniques}} for locating and identifying faults in coax cables and waveguide runs are described. Particular {{emphasis is placed}} on Frequency Domain Reflectometry (FDR) using a software-enhanced scalar network <b>analyzer.</b> Measurement and <b>accuracy</b> considerations are discussed as are sources of error. This scalar technique is also compared with Time Domain analysis performed with the vector network analyzer. AUTHOR...|$|R
40|$|A {{methodology}} {{is developed}} to computationally assess the probabilistic composite material properties at all composite scale levels {{due to the}} uncertainties in the constituent (fiber and matrix) properties and in the fabrication process variables. The methodology is computationally efficient for simulating the probability distributions of material properties. The sensitivity of the probabilistic composite material property to each random variable is determined. This information {{can be used to}} reduce undesirable uncertainties in material properties at the macro scale of the composite by reducing the uncertainties in the most influential random variables at the micro scale. This methodology was implemented into the computer code PICAN (Probabilistic Integrated Composite <b>ANalyzer).</b> The <b>accuracy</b> and efficiency of this methodology are demonstrated by simulating the uncertainties in the material properties of a typical laminate and comparing the results with the Monte Carlo simulation method. The experimental data of composite material properties at all scales fall within the scatters predicted by PICAN...|$|R
40|$|This thesis {{proposes a}} method for {{automatic}} morphological analysis of Syriac - an under-resourced language for {{which there are no}} natural language processing tools such as morphological analyzers readily available. The proposed method uses a data-driven approach with automatically generated and weighted regular expression rules and patterns to cater for morphological attribute tagging and root- and lexeme derivation for dictionary linkage. The method is compared against a baseline, which it outperforms on all tests, and significantly outperforms for unknown words. When trained on all available training data, the <b>analyzer</b> achieves an <b>accuracy</b> of 95. 53 %...|$|R
40|$|Abstract — Recently, UWB (Ultra Wide Band) {{signal that}} has {{frequency}} bandwidth of 500 MHz or more {{is considered to}} be using high precision radar because of a very high resolution. The UWB is considered to have fewer influences on human being because of low transmitting power. In this paper, sensor network made up of some UWB radar terminals to estimate distance and to detect human being existence is studied. Detection performances are evaluated from the computer simulation by using the real propagation measurement result with the network <b>analyzer.</b> The positioning <b>accuracy</b> of the human body and the effectiveness of the sensor network with UWB radar are shown. UW...|$|R
40|$|This paper {{presents}} a complete network analyzer development for heterogeneous services in campus environment. The {{purpose of this}} study is to define the <b>accuracy</b> of network <b>analyzer</b> development with independent data, real network and OPNET simulation tool. This network analyzer software will test on traffic and utilization generated by the several services. The reliability of this network analyzer will test with web service, email service, video conference service, VoIP service, and voice conference service. The results show that network <b>analyzer</b> software has <b>accuracy</b> and same trend with independent data, real network and OPNET simulation tool. Finally, this software is able to measure the network resources during preparation, proposal and planning phases...|$|R
40|$|We {{describe}} a completely automated enzymatic meth-od for estimating serum triglycerides with a centrifugal <b>analyzer</b> (ENI-GEMSAEC). <b>Accuracy</b> and precision (CV, 2 to 4. 8 %) {{and the upper}} limit of linearity (400 mg / 100 ml) are monitored by use of newly developed, clear, aqueous triolein standards. A modified computer pro-gram (FOCAL 8) is used for data manipulation and cor-recting absorbance to zero reaction time. Results corn-pare well with those obtained by an automated fluoro-metric procedure and a manual enzymatic procedure. AdditIonal Keyphrase: “kft ” method This enzymatic procedure, that of Bucolo and David (1), was chosen because of its adaptability to the centrifugal analyzer, use of noncorrosive re-agents, specificity, accuracy, relatively high through-put (70 - 80 /h), and low reagent cost. The technique involves enzymatic hydrolysis of triglycerides to glycerol, which is quantitatively esti-mated by use of coupled reactions via glycerol kinase (EC 2. 7. 1. 30), pyruvate kinase (EC 2. 7. 1. 40), and lac-tate dehydrogenase (EC 1. 1. 1. 27), to the NADH to NAD+ reaction, on a 1 : 1 molar basis. Results are cal-culated from the molar absorptivity and the change in absorbance of the reaction mixture...|$|R
40|$|An {{integrated}} methodology {{is developed}} for computationally simulating the probabilistic composite material properties at all composite scales. The simulation requires minimum input {{consisting of the}} description of uncertainties at the lowest scale (fiber and matrix constituents) of the composite and in the fabrication process variables. The methodology allows {{the determination of the}} sensitivity of the composite material behavior to all the relevant primitive variables. This information is crucial for reducing the undesirable scatter in composite behavior at its macro scale by reducing the uncertainties in the most influential primitive variables at the micro scale. The methodology is computationally efficient. The computational time required by the methodology described herein is an order of magnitude less than that for Monte Carlo Simulation. The methodology has been implemented into the computer code PICAN (Probabilistic Integrated Composite <b>ANalyzer).</b> The <b>accuracy</b> and efficiency of the methodology/code are demonstrated by simulating the uncertainties in the heat-transfer, thermal, and mechanical properties of a typical laminate and comparing the results with the Monte Carlo simulation method and experimental data. The important observation is that the computational simulation for probabilistic composite mechanics has sufficient flexibility to capture the observed scatter in composite properties...|$|R
40|$|Abstract — This paper {{presents}} a complete network analyzer development for network delay in campus environment. The {{purpose of this}} study is to define the <b>accuracy</b> of network <b>analyzer</b> development with independent data, real network and OPNET simulation tool. The network delay will measure based on transmission delay and propagation delay. This network analyzer software will test on delay generated by the several services. The reliability of this network analyzer will test with email, text messaging and instant messaging over web service. The results show that network <b>analyzer</b> software has <b>accuracy</b> and same trend delay with independent data, real network and OPNET simulation tool. Finally, this software is able to measure the network delay during preparation, proposal and planning phases...|$|R
40|$|Radiometric {{identification}} is a recently coined term that describes a broad category of techniques {{for determining the}} identity of a wireless device based on unique characteristics of its transmitted signal that result from imperfections and variances in the device’s manufacturing processes. Existing techniques are based on extracting and classifying features from either the transient portion of a signal or, most recently, from patterns of modulation errors in a received signal, such as symbol phase and magnitude errors. While the latter approach was shown to be extremely successful in correctly identifying wireless devices using an expensive high-end signal <b>analyzer,</b> its <b>accuracy</b> has not been considered or evaluated under realistic deployment scenarios in the presence of an adversary who actively tries to manipulate his own radiometric signature. Using a software-defined radio platform and an implementation of the IEEE 802. 11 b PHY layer, we provide preliminary results that suggest a modulation-based radiometric identification system is both feasible and reasonably reliable on commodity hardware. We also experimentally evaluate the effectiveness of an attacker who actively tries to manipulate his radiometric signature in order to impersonate another 802. 11 b wireless device. We show that even a moderately sophisticated adversary can likely significantly reduce the accuracy of a modulation-based radiometric identification scheme based on a commodity RF hardware platform...|$|R
40|$|The {{purpose of}} this study was to measure the {{effectiveness}} of the Beatnik Rhythmic <b>Analyzer</b> on rhythmic <b>accuracy.</b> Non-percussion music majors (N = 19) were randomly divided to practice with either the Beatnik Rhythmic Analyzer (n = 9) or a metronome (n = 10). Five exercises were administered for one minute each over a three-week period. Pre- posttest scores were analyzed using a Mann-Whitney U to test for differences between the groups. While the mean posttest scores of the treatment group were higher than the control group, results indicated no significant difference between the groups (a=. 05). Lastly, two out of the five exercises resulted in large effect sizes in favor of the treatment group, suggesting that the Beatnik Rhythmic Analyzer is highly effective for developing specific fundamental techniques in snare drum playing...|$|R
40|$|Capillary blood {{sampling}} {{has been identified}} as a potentially suitable technique for use in diagnostic testing of the full blood count (FBC) at the point-of-care (POC), for which a recent need has been highlighted. In this study we assess the accuracy of capillary blood counts and evaluate the potential of a miniaturized cytometer developed for POC testing. Differential leukocyte counts in the normal clinical range from fingerprick (capillary) and venous blood samples were measured and compared using a standard hematology <b>analyzer.</b> The <b>accuracy</b> of our novel microfluidic impedance cytometer (MIC) was then tested by comparing same-site measurements to those obtained with the standard analyzer. The concordance between measurements of fingerprick and venous blood samples using the standard hematology analyzer was high, with no clinically relevant differences observed between the mean differential leukocyte counts. Concordance data between the MIC and the standard analyzer on same-site measurements presented significantly lower leukocyte counts determined by the MIC. This systematic undercount was consistent across the measured (normal) concentration range, suggesting that an internal correction factor could be applied. Differential leukocyte counts obtained from fingerprick samples accurately reflect those from venous blood, which confirms the potential of capillary {{blood sampling}} for POC testing of the FBC. Furthermore, the MIC device demonstrated here presents a realistic technology for the future development of FBC and related tests for use at the site of patient car...|$|R
40|$|An {{analysis}} is presented {{of a system}} for indirect calorimetric measurement. Emphasis has been placed upon problems asso-ciated {{with the use of}} large climate-controlled chambers. The dynamics of gas exchange and the influence of ambient condi-tions on various parameters have been subjected to physical analysis which has led to the following conclusions: I) Fast-response measurement of heat production can be achieved even if the size and hence washout time of the chamber is large. 2) Accurate continuous measurement of heat production over long periods, without correction for changes in barometric pressure, is possible if suitable instrumentation is chosen. 3) A method for calibration of a paramagnetic oxygen <b>analyzer</b> achieved high <b>accuracy</b> at low cost. 4) The entire system has been checked by simulation of gas exchange in the chamber, by injection of nitrogen at a known rate...|$|R
40|$|Cylinder {{pressure}} is measured simultaneously at several {{points in the}} combustion chamber of a high speed diesel engine with a toroidal piston cavity by means of small-sized piezo pressure transducers and a multichannel combustion <b>analyzer</b> with an <b>accuracy</b> of one fourth degree of crank angle. It has been clearly shown on the cylinder pressure behavior that the farther the distance of measuring point from the cavity center, the latter the rapid pressure rise and the larger the pressure fluctuation amplitude. The time difference of the rapid pressure rise between two measuring points {{is identical to the}} propagation time of a pressure wave originated in the piston cavity. The peak frequencies in the cylinder pressure spectrum, which correspond to the two kinds of wavelengths of cylinder bore and two times the root mean square diameter, are due to standing waves in the combustion chamber...|$|R
40|$|In {{developing}} a text-to-speech system, {{it is well}} known that the accuracy of information extracted from a text is crucial to produce high quality synthesized speech. In this paper, by transferring probabilistic natural language processing techniques into TTS system #eld, we develop a more robust text <b>analyzer</b> with high <b>accuracy</b> for Korean TTS systems. The proposed system is composed of #ve modules: a preprocessor, a morphological analyzer, a part-of-speech tagger, a grapheme-to-phoneme module, and a parser. Among these modules, the part-of-speech tagger and the parser are designed under probabilistic framework, and trained automatically. Given a text, our system produces the structures of word phrases, word pronunciations, and governor-dependent relationships that represents the structure of the sentence. Experimental results showed that the tagger got 90. 33 # correctness for #nding the structure of word phrases in the word level, and the parser, 80. 87 # for #nding governor-dependent relationships of sentences respectively...|$|R
30|$|The {{effects of}} low-salinity water and low-salinity nanofluids (adding SiO 2 {{nanoparticles}} to low-salinity water) on wettability alteration were investigated {{by measuring the}} contact angle between crude oil and brines or nanofluids on glass plates. Contact angle measurements were performed at room temperature and atmospheric pressure with a Kruss DSA- 100 contact angle <b>analyzer</b> with an <b>accuracy</b> of[*]±[*] 0.1 °. Measurements were conducted on glass plates because glass is of the same material as glass beads filled in sand packs. After being cleaned with acetone, 18 pieces of glass plate were immersed in brines at 333  K for 1  h to form a film of brine on the glass plates. The glass plates were then aged in crude oil at 333  K for 4  weeks. In order to measure the contact angle, the oil-wetted glass plate was immersed in the low-salinity water or the low-salinity nanofluid. Then, a crude oil drop {{was placed on the}} plate surface.|$|R
40|$|Mention {{any other}} presentations {{of this paper}} here, or delete this line. Abstract. Computer {{simulation}} provides a means of determining the relative effects of various factors on spray drift while field experiments to measure spray drift have the limitation that many variables cannot be controlled. A Windows Version computer program (DRIFTSIM) was developed to rapidly estimate the mean drift distances of water droplets discharged from atomizers on field sprayers. This program interpolates values from a large data base of drift distances originally calculated for single droplets with a flow simulation program (FLUENT). The simulations of drift distances up to 200 m (656 ft) included temperatures (10 - 30 °C; 50 - 86 °F), discharge heights (0 - 2. 0 m; 0 - 6. 56 ft), initial downward droplet velocities (0 - 50 m/s; 0 - 164 ft/s), relative humidity (10 - 100 %), wind velocities (0 - 10. 0 m/s; 0 - 32. 8 ft/s), droplet sizes (10 - 2000 µm), droplet size distribution in Dv. 1, Dv. 5 and Dv. 9, and 20 % turbulence intensity. Variables can be either in metric or English units. For the input of droplet size distribution, drift distances are reported along with portion of volume in each class such as provided by many droplet size <b>analyzers.</b> The <b>accuracy</b> of the program FLUENT was verified with a uniform droplet generator and wind tunnel. The program indicates the relative effects of the input variables on drift distances and should, especially for large droplets, provide reasonable accuracy for many field applications...|$|R
40|$|The {{focus of}} this project {{is to find a}} {{suitable}} method to determine the mixing ratio inbinary fluid mixtures in continuous-flow microfluidic systems because of thedifficulties in doing so for mixtures containing compressible fluids. Refractive indexand relative static permittivity are both properties that could be suitable, but methodsmeasuring the refractive index scales badly for microsystems. A microfluidic chip for measuring capacitance was placed on a PCB together with amixing structure with strain-relieved fluid and electrical interfaces. This PCB was builtinto a rig with two piston pumps and a backpressure regulator to makemeasurements of the relative static permittivity of air, ethanol, methanol, acetonitrile,liquid and gaseous carbon dioxide, as well as of several mixtures of ethanol andcarbon dioxide using a Network Analyzer. Several other measuring techniques were tried, but the Network <b>Analyzer</b> wassuperior in <b>accuracy,</b> stability and frequency range. It produced values within 4 % ofthe theoretical, and the discrepancy could be explained by the approximations in theparallel plate capacitor formula, the capacitance contributions of the external parts ofthe system and surface roughness. The Network Analyzer is a good tool to determinethe mixing ratio in binary fluid mixtures in continuous-flow microfluidic systems...|$|R
40|$|The {{ability to}} predict the power {{injected}} in the pins of an IC during EM susceptibility tests is key to enable affordable and effective design of robust circuits. Radiated susceptibility tests are especially difficult to model in the circuit simulator as they involve distributed effects which do not map easily onto the nodal framework used in those simulators, where nets are inherently local. In the case of TEM fields however, usage of the transmission-line formalism is well established and allows to model the coupling effects using concepts of distributed RLCG parameters. This paper presents a simulation framework allowing to evaluate how much power is injected into PCB tracks subjected to TEM-cell fields. The proposed models were validated against 3 D EM simulations {{as well as an}} extensive series of experiments, relying on the vector network <b>analyzer</b> for maximum <b>accuracy.</b> The simulation framework was implemented at ONSEMI in the CADENCE design environment. It uses one single PCB schematic to perform standard signal-integrity as well as TEM-cell field coupling simulations, and uses inherited connections to realize a ‘wireless coupling’ between all PCB tracks and the TEM-cell septum. status: accepte...|$|R
40|$|In this paper, {{we present}} an {{optimized}} analysis algorithm for non-dispersive infrared (NDIR) to in situ monitor stack emissions. The proposed algorithm simultaneously compensates for nonlinear absorption and cross interference among different gases. We present a mathematical derivation for the measurement error caused by variations in interference coefficients when nonlinear absorption occurs. The proposed algorithm {{is derived from}} a classical one and uses interference functions to quantify cross interference. The interference functions vary proportionally with the nonlinear absorption. Thus, interference coefficients among different gases can be modeled by the interference functions whether gases are characterized by linear or nonlinear absorption. In this study, the simultaneous analysis of two components (CO 2 and CO) serves as an example for the validation of the proposed algorithm. The interference functions in this case {{can be obtained by}} least-squares fitting with third-order polynomials. Experiments show that the results of cross interference correction are improved significantly by utilizing the fitted interference functions when nonlinear absorptions occur. The dynamic measurement ranges of CO 2 and CO are improved by about a factor of 1. 8 and 3. 5, respectively. A commercial <b>analyzer</b> with high <b>accuracy</b> was used to validate the CO and CO 2 measurements derived from the NDIR analyzer prototype in which the new algorithm was embedded. The comparison of the two analyzers show that the prototype works well both within the linear and nonlinear ranges...|$|R
40|$|Performance {{assessment}} and large-scale monitoring of terrestrial {{digital video broadcasting}} (DVB-T) transmission apparatuses are currently a pressing need due to {{the rapid growth of}} related broadcasting networks in many countries. Power measurements, in particular, play a very important role since RF and IF signal power, RF channel power, RF and IF power spectrum, noise (or unwanted) power, and power efficiency are relevant parameters to be measured as accurately as possible. Although modern spectrum analyzers and high-performance vector signal <b>analyzers</b> exhibit satisfying <b>accuracy</b> and repeatability, their cost, weight, and size make them unsuitable for the purpose. This paper is focused on the design and realization of a digital signal processing-based meter for power measurement in DVB-T systems that is capable of granting good accuracy, satisfying repeatability, reduced measurement time, and cost effectiveness. This paper deals with the digital signal processing algorithm to be implemented in the meter, paying attention to parametric power spectral density (PSD) estimators for their reduced memory requirement and potentially limited computational burden. In particular, a parametric estimation algorithm that is capable of assuring a fast measurement rate is implemented and made operative. The simulation and emulation stages are properly designed to regulate the most relevant parameters of the adopted PSD estimators according to the specific features of the signals involved; a number of experiments on actual DVB-T signals are conducted, the results of which are compared to those provided by competitive measurement solutions...|$|R
40|$|Our goal is {{to develop}} an {{effective}} work flow for analysis of intact proteins in a complex mixture using the LC-LTQ-Orbitrap XL. Intact protein analysis makes the entire sequence available for characterization, which allows for the identification of isoforms and post translational modifications. We focus on {{developing a method for}} top-down proteomics using a high-resolution, high mass <b>accuracy</b> <b>analyzer</b> coupled with bioinformatics tools. The complex mixtures are fractionated using 1 -dimensional reversed-phase chromatography and basic reversed- phase, and open tubular electrophoresis. The analysis of intact proteins requires various fragmentation methods such as collisional induced dissociation, high energy collisional dissociation, and electron transfer dissociation. This overall method enables us to analyze intact proteins, providing a better understanding of protein expression levels and post transitional modification information. We have used standard proteins to optimize HPLC conditions and to compare three methods for ion activation and dissociation. Furthermore, we have extended the method to analyze low mass proteins in MCF 7 cytosol and in E. coli lysate as a model complex mixture. We have applied this strategy to identify and characterize proteins from extracellular vesicles (EVs) shed by murine myeloid-derived suppressor cells (MDSC). MDSCs suppress both innate and adaptive immune responses to tumor growth and prevent effective immunotherapy. Recently some of the intercellular immunomodulatory effects of MDSC {{have been shown to be}} propagated by EVs. Top-down analysis of intact proteins from these EVs was undertaken to identify low mass protein cargo, and to characterize post-translational modifications...|$|R
40|$|BACKGROUND: The results {{obtained}} from various point-of-care (POC) test devices for estimating C-reactive protein (CRP) levels {{in a laboratory}} setting differ when compared to a laboratory reference test. We aimed to determine whether such differences meaningfully affect the accuracy and added diagnostic value in predicting radiographic pneumonia in adults presenting with acute cough in primary care. METHODS: A nested case control study of adult patients presenting with acute cough in 12 different European countries (the Genomics to combat Resistance against Antibiotics in Community-acquired LRTI in Europe [GRACE] Network). Venous blood samples from 100 patients with and 100 patients without pneumonia were tested with five different POC CRP tests and a laboratory <b>analyzer.</b> Single test <b>accuracy</b> values and the added value of CRP to symptoms and signs were calculated. RESULTS: Single test accuracy values showed similar results for all five POC CRP tests and the laboratory analyzer. The area under {{the curve of the}} different POC CRP tests and the laboratory analyzer (range 0. 79 - 0. 80) were all comparable and higher than the clinical model without CRP (0. 70). Multivariable odds ratios were the same (1. 2) for all CRP tests. CONCLUSIONS: Five POC CRP test devices and the laboratory analyzer performed with similar accuracy in detecting pneumonia both as single test, and when used in addition to clinical findings. Variability in {{results obtained}} from standard CRP laboratory and POC test devices do not translate into clinically relevant differences when used for prediction of pneumonia in patients with acute cough in primary care...|$|R
40|$|Abstract. Contact stylus {{instrument}} has difficulties in tracing large slope angle (LSA) lens for surface profile measurement {{due to the}} dilation of measured profile image by the shape of stylus tip. This research {{is to develop a}} surface profile reconstruction method for tip profile with LSA lens and then verified the result by estimating the form error. Experimental set-up is configured of a surface profile and roughness <b>analyzer,</b> Panasonic Ultra <b>Accuracy</b> 3 -D Profilometer (UA 3 P), and the feasibility of the developed program is compared with the results of a commercial operation software, UA 3 P software. The figure form error (FFE) is used to evaluate the developed method. Experiments have been performed by measuring the LSA lens of a s 55 ml spherical gage with measuring ranges increasing from ± 30 °, ± 45 ° to ± 60 ° and measuring an aspheric lens with range ± 50 °. Experimental results showed that the FFE of the developed method has smaller FFE values than those obtained by the UA 3 P software as the measuring range of ± 30 ° and ± 45. Similar result of FFE is obtained as 0. 891 µm for the aspheric lens by the developed method than FFE 0. 896 µm by the UA 3 P software. Therefore, the developed method has been verified for estimating the FFE of LSA lens with variant measuring ranges. Future work is to develop related compensation methodology of FFE for LSA lens measurement and also to find appreciated tip profiles for measuring LSA lens of higher order nonlinear terms...|$|R
40|$|Introduction: Cobas 6000 (Roche, Germany) is {{biochemistry}} analyzer for spectrophotometric, immu-noturbidimetric and ion-selective {{determination of}} biochemical analytes. Hereby we present analytical validation {{with emphasis on}} method performance judgment for routine operation. Materials and methods: Validation was made for 30 analytes (metabolites, enzymes, trace ele-ments, specific proteins and electrolytes). Research included determination of within-run (N = 20) and between-run imprecision (N = 30), inaccuracy (N = 30) and method comparison with routine analyzer (Beckman Coulter AU 640) (N = 50). For validation of complete analytical process we calculated total error (TE). Results were judged according to quality specification criteria given by European Working Group. Results: Within-run imprecision CVs were all below 5 % except for cholesterol, triglycerides, IgA and IgM. Between-run CVs for all analytes were below 10 %. Analytes that {{did not meet the}} required speci-fications for imprecision were: total protein, albumin, calcium, sodium, chloride, immunoglobulins and HDL cholesterol. Analytes that did not fulfill requirements for inaccuracy were: total protein, calcium, sodium and chloride. Analytes that deviated from quality specifications for total error were: total pro-tein, albumin, calcium, sodium, chloride and IgM. Passing-Bablok regression analysis provided linear equation and 95 % confidence interval for intercept and slope. Complete accordance with routine ana-lyzer Beckman Coulter AU 640 showed small number of analytes. Other analytes showed small pro-portional and/or small constant difference and therefore need to be adjusted for routine operation. Conclusions: Regarding low CV values, tested <b>analyzer</b> has satisfactory <b>accuracy</b> and precision and is extremely stable. Except for analytes that are coherent on both analyzers, some analytes require adjustments of slope and intercept for complete accordance...|$|R
40|$|In the {{biodefense}} {{and medical}} diagnostic fields, MALDI mass spectrometry-based systems {{are used for}} rapid characterization of microorganisms generally by detecting and discriminating the highly abundant protein mass-to-charge peaks. It is important that these peaks eventually are identified, but few bacteria have publicly available, annotated genome or proteome from which this identification can be made. This dissertation proposes a method of top-down proteomics using a high-resolution, high mass <b>accuracy</b> <b>analyzer</b> coupled with bioinformatics tools to identify proteins from bacteria with unavailable genome sequences by comparison to protein sequences from closely-related microorganisms. Once these proteins are identified and {{a link between the}} unknown target bacteria and the annotated related bacteria is established, phylogenetic trees can be constructed to characterize where the target bacteria relates to other members of the same phylogenetic family. 	First, the top-down proteomic approach using an Orbitrap mass analyzer is tested using a well known, well studied single protein. After this is demonstrated to be successful, the approach is demonstrated on a bacterium without a sequenced genome, only matching proteins from other organisms which are thought to have 100 % homology with the proteins studied by the top-down approach. Finally, the proposed method is changed slightly to be more inclusive and the proteins from two other bacteria without publicly available genomes or proteomes are matched to known proteins that differ in mass and may not be 100 % homologous to the proteins of the studied bacteria. This more inclusive method is shown to also be successful in phylogenetically characterizing the bacteria lacking sequence information. Furthermore, some of the mass differences are localized to a small window of amino acids and proposed changes are made that increase confidence in identification while lowering the mass difference between the studied protein and the matched, homologous, known protein...|$|R
40|$|Calculation and {{measurement}} of performance parameters of public lighting are {{performed in a}} grid of points defined by the CIE publication 140 [1] and European standard EN 13201 - 3. [2] Calculation grid defined in these documents was created {{at a time of}} less powerful computers and less precise measuring technology. Distance between two adjacent calculating points may be even 3 m, depending on the geometry of the road. This arrangement is too sparse {{in view of the fact}} that presently used computers can cope with much more denser grids in a shorter operation time and the measurement can be performed by luminance <b>analyzers</b> with more <b>accuracy</b> as well. Between the results of the calculation with different grid densities of calculation points there can be significant differences. This fact can be influenced by new optical systems and some LED luminaires which, as it was found out by measurements, may contain various local extremes in the LIDC. When calculating the performance parameters of public lighting with given grid density these extremes may fall between calculation points and thus they are not included in the calculation. The solution is to propose a new density of calculation points which would be designed with respect to current possibilities of computer and measuring technology. The paper deals with the influence of grid density to the resulting photometric parameters of public lighting. In computing software, calculation grid of points according to EN 13201 - 3 and calculation grid with half and 2 to 10 -times densification of calculation points will be used in order to capture the impact of given local extremes to the resulting parameters using luminaires with different optical systems. The results of these calculations will be used as the basis to determine an appropriate density of calculation {{and measurement}} grid. The results will be used in the preparation of future revisions of normative documents, which provide a method for determining points of calculation and measurement grid...|$|R
40|$|While {{thermochemical}} syngas {{production facilities}} for biomass utilization are already employed worldwide, exploitation {{of their potential}} has been inhibited by technical limitations encountered when attempting to obtain real-time syngas compositional data required for process optimization, reliability, and syngas quality assurance. To address these limitations, the Gas Technology Institute (GTI) carried out two companion projects (under US DOE Cooperative Agreements DE-FC 36 - 02 GO 12024 and DE-FC 36 - 03 GO 13175) to develop and demonstrate the equipment and methods required to reliably and continuously obtain accurate and representative on-line syngas compositional data. These objectives were proven through a stepwise series of field tests of biomass and coal gasification process streams. GTI developed the methods and hardware for extractive syngas sample stream delivery and distribution, necessary {{to make use of}} state-of-the-art on-line analyzers to evaluate and optimize syngas cleanup and conditioning. The primary objectives of Cooperative Agreement DE-FC 36 - 02 GO 12024 were the selection, acquisition, and application of a suite of gas analyzers capable of providing near real-time gas analyses to suitably conditioned syngas streams. A review was conducted of sampling options, available analysis technologies, and commercially available analyzers, that could be successfully applied to the challenging task of on-line syngas characterization. The majority of thermochemical process streams comprise multicomponent gas mixtures that, prior to crucial, sequential cleanup procedures, include high concentrations of condensable species, multiple contaminants, and are often produced at high temperatures and pressures. Consequently, GTI engaged in a concurrent effort under Cooperative Agreement DE-FC 36 - 03 GO 13175 to develop the means to deliver suitably prepared, continuous streams of extracted syngas to a variety of on-line gas analyzers. The review of candidate analysis technology also addressed safety concerns associated with thermochemical process operation that constrain the location and configuration of potential gas analysis equipment. Initial <b>analyzer</b> costs, reliability, <b>accuracy,</b> and operating and maintenance costs were also considered prior to the assembly of suitable analyzers for this work. Initial tests at GTI’s Flex-Fuel Test Facility (FFTF) in late 2004 and early 2005 successfully demonstrated the transport and subsequent analysis of a single depressurized, heat-traced syngas stream to a single analyzer (an Industrial Machine and Control Corporation (IMACC) Fourier-transform infrared spectrometer (FT-IR)) provided by GTI. In March 2005, our sampling approach was significantly expanded when this project participated in the U. S. DOE’s Novel Gas Cleaning (NGC) project. Syngas sample streams from three process locations were transported to a distribution manifold for selectable analysis by the IMACC FT-IR, a Stanford Research Systems QMS 300 Mass Spectrometer (SRS MS) obtained under this Cooperative Agreement, and a Varian micro gas chromatograph with thermal conductivity detector (μGC) provided by GTI. A syngas stream from a fourth process location was transported to an Agilent Model 5890 Series II gas chromatograph for highly sensitive gas analyses. The on-line analyses made possible by this sampling system verified the syngas cleaning achieved by the NGC process. In June 2005, GTI collaborated with Weyerhaeuser to characterize the ChemrecTM black liquor gasifier at Weyerhaeuser’s New Bern, North Carolina pulp mill. Over a ten-day period, a broad range of process operating conditions were characterized with the IMACC FT-IR, the SRS MS, the Varian μGC, and an integrated Gas Chromatograph, Mass Selective Detector, Flame Ionization Detector and Sulfur Chemiluminescence Detector (GC/MSD/FID/SCD) system acquired under this Cooperative Agreement from Wasson-ECE. In this field application, a single sample stream was extracted from this low-pressure, low-temperature process and successfully analyzed by these devices. In late 2005, GTI conducted intensive field characterizations of biomass-derived syngas at GTI’s FFTF during a concurrent test of pelletized wood-fueled gasification and catalyst performance investigated under Cooperative Agreement DE-FG 36 - 04 GO 14314. In 2006 GTI continued its sampling development and verification activities at GTI’s FFTF with a follow-on set of calibration measurements. The combination of the sample conditioning and sample stream transport methods developed under Cooperative Agreement DE-FC 36 - 03 GO 13175, and the assembly and coordination of gas analyzers and data collection and analyses under Cooperative Agreement DE-FC 36 - 02 GO 12024, have provided a new, powerful, enabling capability for on-line data characterizations of biomass- and coal-derived syngas from thermochemical conversion process streams...|$|R
