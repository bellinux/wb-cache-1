9|10000|Public
40|$|WAOA Session 7 LNCS v. 5893 is Proceedings of the 7 th International Workshop, WAOA 2009 In an <b>asynchronous</b> <b>data</b> <b>stream,</b> {{the data}} items {{may be out}} of order with respect to their {{original}} timestamps. This paper gives a space-efficient data structure to maintain such a data stream so that it can approximate the frequent item set over a sliding time window with sufficient accuracy. Prior to our work, Cormode et al. [3] have the best solution, with space complexity O(1 /ε log W log (ε B/log W) min{log W, 1 /ε } log U), where ε is the given error bound, W and B are parameters of the sliding window, and U is the number of all possible item names. Our solution reduces the space to O(1 /ε log W log (ε B/log W)). We also unify the study of synchronous and <b>asynchronous</b> <b>data</b> <b>stream</b> by quantifying the delay of the data items. When the delay is zero, our solution matches the space complexity of the best solution to the synchronous data streams [8]. © 2010 Springer-Verlag Berlin Heidelberg. link_to_subscribed_fulltextThe 7 th Workshop on Approximation and Online Algorithms (WAOA 2009), University of Copenhagen, Denmark, 10 - 11 September 2009. In Lecture Notes in Computer Science, 2009, v. 5893, p. 49 - 6...|$|E
30|$|Complex event {{processing}} (CEP) {{is a set}} {{of techniques}} and tools that provides an in-memory processing model for an <b>asynchronous</b> <b>data</b> <b>stream</b> in real time (i.e., minimum delay) for online detection of situations of interest [28]. Complex event processing offers [28]: (i) situation awareness through the use of continuous queries that correlate data from different sensors data streams; (ii) context awareness by subdividing data streams into different views, such as temporal windows or key partitions; and (iii) flexibility, since it can specify events at any time, that is, the specification of events can be dynamically changed while a system is running (i.e., on-the-fly).|$|E
40|$|Many {{real world}} data {{naturally}} arrive as rapid paced and virtually unbounded streams. Examples of such streams include network traffic at a router, events observed by a sensor network, accesses to a web server and transactional updates {{to a large}} database. Such streaming data need to be monitored online to collect traffic statistics, detect trends and anomalies, tune system performance and help make business decisions. However, {{because of the large}} size and rapid pace of the data, as well as the online processing requirement, conventional data processing methods, such as storing the data in a database and issuing offline SQL queries thereafter, are not feasible. Data stream processing is a new diagram of massive data set processing and creates new challenges in the algorithm design and implementation. In this thesis, we consider time-decayed data aggregation for data streams, where the importance or contribution of each data element decays over time, since recent data are usually considered of more importance in applications, and therefore are given heavier weights. We design small space data structures and algorithms for maintaining fundamental aggregates of the streams if it is possible and otherwise show large space lower bounds. We consider the data aggregation over a robust data stream model called <b>asynchronous</b> <b>data</b> <b>stream,</b> motivated by the streaming data transmitted in distributed systems, including computer networks, where the asynchrony in the data transmission is inevitable. In <b>asynchronous</b> <b>data</b> <b>stream,</b> the arrival order of the data elements at the receiver side is not necessarily the same as the order in which the data elements were generated. <b>Asynchronous</b> <b>data</b> <b>stream</b> is a robuster and generalized model of the previous synchronous data stream model. In summary, this thesis presents the following results: 1. We formalize the model of <b>asynchronous</b> <b>data</b> <b>stream</b> and the notion of timestamp sliding window. We propose the first small space sketch for summarizing the data elements over timestamp sliding windows of multiple geographically distributed asynchronous data streams. The sketch can return accuracy guaranteed estimates for basic aggregates, such as: Sum, Median and Quantiles. 2. We design the first small space sketch for general purpose network streaming data aggregation. The sketch has the following properties that make it useful in communication-efficient aggregation in distributed streaming scenarios: (1) The sketch can handle multiple geographically distributed asynchronous data streams. (2) The sketch is duplicate-insensitive, i. e. reinsertions of the same data will not affect the sketch, and hence the estimates of aggregates. (3) The sketch is also time-decaying, so that the weight of each data element summarized in the sketch decreases over time. (4) The sketch returns accuracy guaranteed estimates for a variety of core aggregates, including the sum, median, quantiles, frequent elements and selectivity. 3. We conduct a comprehensive study on the time-decayed correlated data aggregation over asynchronous data streams. For each class of time decay function, we either propose space efficient algorithms or show large space lower bounds. We not only closes the open problem of correlated data aggregation under sliding windows decay, but also presents negative results for the case of exponential decay, which however is highly used in the non-correlated scenarios. 4. We propose the forward decay model to simplify the time-decayed data stream aggregation and sampling. Forward decay captures a variety of usual decay functions (or called backward decay), such as exponential decay. We design efficient algorithms for data aggregation and sampling under the forward decay model, and show that they are easy to implement scalably...|$|E
5000|$|... <b>Asynchronous</b> <b>data</b> <b>streaming</b> in a {{peer to peer}} network. Issued March 1, 2012 ...|$|R
50|$|Data Space, when present, carries application-specific {{information}} such as isochronous, and <b>asynchronous</b> <b>data</b> <b>streams.</b>|$|R
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1989 / Town & Country Hotel & Convention Center, San Diego, CaliforniaRecent {{developments}} in telemetry {{have resulted in}} an increased variety of data sources. As a result, <b>data</b> <b>streams</b> are incorporating such complexities as embedded <b>asynchronous</b> <b>data</b> <b>streams,</b> packets, and multiple formats. These <b>data</b> <b>streams</b> must be acquired and processed in real-time by telemetry ground stations. Most modern telemetry systems use a distributed architecture to accomplish these complex decommutation and preprocessing tasks. It is usually desirable to verify the data base setup and functional operation of the system before critical tests, as well as during test development. Most of the telemetry simulator products available today can do only a very limited simulation of the incoming <b>data</b> <b>stream.</b> This often fails to exercise many key components of the system. A new product will be described which can simulate a <b>data</b> <b>stream</b> with multiple formats, embedded <b>asynchronous</b> <b>data</b> <b>streams,</b> unlimited special words, and other useful functions. This product will enable the user to perform a more complete test {{of all of the}} components of the telemetry system...|$|R
40|$|International Telemetering Conference Proceedings / November 04 - 07, 1991 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe use of {{embedded}} {{asynchronous data}} streams {{is becoming a}} popular means of expanding existing telemetry systems and acquiring subsystem data. In such systems, synchronization between the primary and secondary system(s) clocks is usually considered a prerequisite. The Phillips Laboratory has developed a software/hardware approach {{to the problem of}} decommutating an embedded <b>asynchronous</b> <b>data</b> <b>stream</b> without primary and secondary frame and clock synchronization. The methodology employed is easily implemented and adapted to many system configurations, and represents a low-cost option in the acquisition of subsystem data. More importantly, the use of such a system greatly reduces the amount of systems integration effort required to incorporate multiple subsystems into a host telemetry system...|$|E
40|$|In an <b>asynchronous</b> <b>data</b> <b>stream,</b> {{the data}} items {{may be out}} of order with respect to their {{original}} timestamps. This paper studies the space complexity required by a data structure to maintain such a data stream so that it can approximate the set of frequent items over a sliding time window with sufficient accuracy. Prior to our work, the best solution is given by Cormode et al. [1], who gave an O (1 /∈ logW log(∈B/logW) min{logW; 1 /∈} log |U|) -space data structure that can approximate the frequent items within an ∈ error bound, where W and B are parameters of the sliding window, and U is the set of all possible item names. We gave a more space-efficient data structure that only requires O (1 /∈log W log(∈B/logW) log logW) space. © 2011 by the authors. link_to_OA_fulltex...|$|E
40|$|International Telemetering Conference Proceedings / November 04 - 07, 1991 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe {{growth of}} {{personal}} computer use was explosive {{in the last}} decade. In the telemetry industry, however, the adaptation and utilization of a PC-based telemetry instrument for high-speed data processing and display did not come about until the Intel 80386 ™ or equivalent processors were widely used in the late 1980 s. At this time, the power of these processors finally began to meet the requirement to display, store, and play back the high-speed data (such as 10 Mbps with an embedded <b>asynchronous</b> <b>data</b> <b>stream)</b> that is typical in telemetry applications. Many users are still hesitant to use PCS for their telemetry applications because of the real-time limitations of these instruments. This paper will examine {{the advantages and disadvantages}} of PC-based test equipment, the performance these instruments, and the future of PC-based telemetry instrumentation. This paper will also focus on Loral Instrumentation’s d*STAR as an example of a PC-based telemetry system...|$|E
40|$|International Telemetering Conference Proceedings / September 28 - 30, 1982 / Sheraton Harbor Island Hotel and Convention Center, San Diego, CaliforniaTelemetered data {{generated}} by missile systems {{has become increasingly}} complex with the inclusion of <b>asynchronous</b> <b>data</b> <b>streams,</b> variable word lengths, and discrete encoding. The display of this data for analysis purposes requires sophisticated equipment, usually designed with a programmable architecture. This paper describes software support that was developed for a stored program PCM decommutator. The software includes a cross assembler and supports downline loading of the decommutator from a host computer...|$|R
40|$|In this paper, the {{development}} of a framework based on the Realtime Database (RTDB) for processing multimodal data is presented. This framework allows readily integration of input and output modules. Furthermore the <b>asynchronous</b> <b>data</b> <b>streams</b> from different sources can be approximately processed in a synchronous manner. Depending on the included modules, online as well as offline data processing is possible. The idea is to establish a real multimodal interaction system that is able to recognize and react to those situations that are relevant for human-robot interaction. 1 Index Terms — multimodal data communication, human-robot interaction, real-time data processing, augmented realit...|$|R
40|$|Abstract—Related <b>data</b> <b>streams</b> {{refer to}} <b>data</b> <b>streams</b> {{that can be}} joined {{together}} by matching their join attributes. Existing research on learning from related <b>data</b> <b>streams</b> {{is based on an}} assumption that all streams arrive at a central processing unit in a synchronous way, such that in an arbitrary sliding window, all tuples of the streams can be perfectly joined together. This assumption, however, does not hold when related <b>data</b> <b>streams</b> are generated or transferred at different speeds, and thus may arrive in the central processing unit in an asynchronous manner. In this paper, we argue that for <b>asynchronous</b> <b>data</b> <b>streams,</b> there exist a small portion of perfectly joined examples (i. e., complete examples) and a large portion of partially joined examples (i. e., incomplete examples). Accordingly, we present a new Learning from Complete and Fixed Examples (LCFE) framework that can fix incomplete examples to boost the learning. Experiments on both synthetic and real-world <b>data</b> <b>streams</b> demonstrate that LCFE is able to achieve a higher prediction accuracy for learning from related <b>data</b> <b>streams</b> than other simple solutions can offer. I...|$|R
40|$|Abstract. We {{consider}} {{the problem of}} maintaining aggregates over re-cent elements of a massive data stream. Motivated by applications in-volving network data, we consider asynchronous data streams, where the observed order of data may be dierent from {{the order in which}} the data was generated. The set of recent elements is modeled as a sliding times-tamp window of the stream, whose elements are changing continuously with time. We present the rst deterministic algorithms for maintaining a small space summary of elements in a sliding timestamp window of an <b>asynchronous</b> <b>data</b> <b>stream.</b> The summary can return approximate an-swers for the following fundamental aggregates: basic count, the number of elements within the sliding window, and sum, the sum of all element values within the sliding window. For basic counting, the space taken by our summary is O(log W log B (log W + log B) =) bits, where B is an upper bound on the value of the basic count, W is an upper bound on the width of the timestamp window, and is the desired relative error. Our algorithms are based on a novel data structure called splittable his-togram. Prior to this work, randomized algorithms were known for this problem, which provide weaker guarantees than those provided by our deterministic algorithms. ...|$|E
40|$|International Telemetering Conference Proceedings / October 26 - 29, 1987 / Town and Country Hotel, San Diego, CaliforniaTraditional space {{telemetry}} {{has generally been}} handled as <b>asynchronous</b> <b>data</b> <b>stream</b> fed into a time division multiplexed channel on a point-to-point radio frequency (RF) link between space and ground. The data handling concepts emerging for the Space Station challenge each of these precepts. According to current concepts, telemetry data on the Space Station will be packetized. It will be transported asynchronously through onboard networks. The space-to-ground link will not be time division multiplexed, but rather will have flexibly managed virtual channels, and finally, the routing of telemetry data must potentially traverse multiple ground distribution networks. Appropriately, the communication standards for handling telemetry are changing to support the highly networked Space Station environment. While a companion paper (1. W. Marker, "Telemetry Formats for the Space Station RF Links") examines the emerging telemetry concepts and formats for the RF link, this paper focuses {{on the impact of}} telemetry handling on the design of the onboard networks {{that are part of the}} Data Management System (DMS). The DMS will provide the connectivity between most telemetry sources and the onboard node for transmission to the ground. By far the bulk of data transported by DMS will be telemetry, however, not all telemetry will place the same demands on the communication system and DMS must also satisfy a rich array of services in support of distributed Space Station operations. These services include file transfer, data base access, application messaging and several others. The DMS communications architecture, which will follow the International Standards Organization (ISO) Reference Model, must support both the high throughput needed for telemetry transport, as well as the rich services needed for distributed computer systems. This paper discusses an architectural approach to satisfying the dual set of requirements and discusses several of the functionality vs. performance trade-offs that must be made in developing an optimized mechanism for handling telemetry data in the DMS...|$|E
40|$|This paper {{describes}} {{the design and}} performance of a highly versatile modulator and demodulator recently developed to facilitate the evaluation of various digital communications links. The modem is capable of either PSK or QPSK operation and can accommodate a very wide range of continuously tunable data rates (1 kbps to 30 Mbps in each of two channels). In the QPSK mode, operation is possible using either a single serial <b>data</b> <b>stream</b> (single channel operation) or using two mutually independent, unrelated, and <b>asynchronous</b> <b>data</b> <b>streams</b> (dual-channel operation). Integrate and dump detectors are used at the demodulator for regeneration of the <b>data</b> <b>stream(s).</b> Measurements indicate that {{the performance of the}} overall system (including the bit detectors) is within 2 dB of the theoretically optimum performance of either PSK or QPSK at any rate within the range of rates provided by the modem, and is within 1 dB of theoretical over most of the range of rates...|$|R
40|$|Modern AI systems need {{theoretically}} sound, easy to use, communication {{models in}} order to be able to explore distributed computing and agent-oriented operation. ICE (Intarc Communication Environment) represents such a system. It grounds on the theoretical framework of CSP (Communicating Sequential Processes) and implements channels as bidirectional, <b>asynchronous</b> <b>data</b> <b>streams</b> that can be configured in various ways. On top of PVM (Parallel Virtual Machine), a defacto standard of message passing systems, software layers have been built that implement the channel operation modes together with interfaces for programming languages most often used in the AI community. A separate layer supports the use of complex data types, as they often arise in speech processing. We describe the design of ICE with a focus on configuration and synchronization during the creation of channels...|$|R
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1989 / Town & Country Hotel & Convention Center, San Diego, CaliforniaThis paper {{explores the}} effects {{the new breed}} of off-theshelf {{integrated}} telemetry front end (TFE) packages have on the cost and schedule of the development cycle associated with real-time telemetry acquisition/processing systems. A case study of an actual project involving replacement of the Holloman AFB sled track telemetry processing system (TPS) with a system capable of simultaneously supporting up to twenty (20) <b>asynchronous</b> <b>data</b> <b>streams</b> is profiled. Notable among the capabilities of the system are; support for PCM, PAM, FM, IRIG and Local time streams; incoming data rates up to 10 Megabits/sec/stream; data logging rates over 16 MegaBytes/sec and the use of local area networks for distribution of data to real-time displays. To achieve these requirements within a manageable cost/schedule framework, the system was designed around an integrated TFE sub-system. Comparisons are drawn between several aspects of this projects development and that of an earlier developmental system which was completed by PSL within the last 16 months...|$|R
40|$|International Telemetering Conference Proceedings / September 27 - 29, 1971 / Washington Hilton Hotel, Washington, D. C. In {{selecting}} a modulation format and modulation/demodulation techniques for a 1 -Gbit/sec intersatellite data link, a judicious balance must be struck between {{a number of}} more or less contradictory factors for the choice to be an optimum one. Particularly important are the factors that determine the average transmitter input power required, as this has a large influence on launch weight and hence on cost. In this connection, both the inherent efficiency of the modulation format and the power losses involved in the implementation chosen obviously {{must be taken into}} account. The latter choice depends, in turn, upon the status of component development assumed, or, alternatively, on the amount of development risk that can be tolerated. Finally, in some applications, the ability of the system chosen to handle a variety of data signal formats and, in particular, to handle <b>asynchronous</b> <b>data</b> <b>streams</b> without requiring complex onboard signal processing such as bit stuffing and bit stream reclocking must be considered...|$|R
40|$|International Telemetering Conference Proceedings / October 13 - 16, 1986 / Riviera Hotel, Las Vegas, NevadaDue to the {{increasing}} complexity of systems on which telemetry is used, the method of embedding a <b>data</b> <b>stream</b> within the main PCM <b>data</b> <b>stream</b> is becoming a popular means of acquiring subsystem data. However, this technique increases {{the complexity of the}} decommutating requirement in that the embedded stream must first be extracted from the main stream and then decommutated. One obvious solution would be to re-serialize the extracted <b>data</b> <b>stream</b> to be decommutated by a second set of frame and subframe synchronizers. This method suffers from increased system cost due to the additional hardware needed solely to support the embedded <b>data</b> <b>stream.</b> This paper will address an alternate method developed by Loral Instrumentation and an actual application at General Dynamics, Convair Division. This technique decommutates <b>asynchronous</b> embedded <b>data</b> <b>streams</b> via standard algorithms in a data compressor...|$|R
40|$|Many stream-based {{applications}} have sophisticated {{data processing}} requirements and real-time performance expectations {{that need to}} be met under <b>asynchronous,</b> time-varying <b>data</b> <b>streams.</b> In order to address these challenges, we propose novel operator scheduling approaches that specify (1) which operators to schedule (2) in which order to schedule the operators, and (3) how many tuples to process at each execution; and study them {{in the context of the}} Aurora <b>data</b> <b>stream</b> manager. We argue and provide experimental evidence that a fine-grained scheduling approach in combination with various scheduling techniques (such as batching of operators and tuples) can significantly improve the efficiency by reducing various system overheads. We also discuss application-aware extensions that address Quality of Service (QoS) issues by making scheduling decisions according to tuple processing delays and per-application QoS specifications. Finally, we present prototype-based experimental results that characterize the efficiency and effectiveness of our approaches under various stream workloads and processing scenarios. ...|$|R
40|$|We {{present a}} {{programming}} methodology and runtime performance case study comparing the declarative data flow coordination language S-Net with Intel's Concurrent Collections (CnC). As a coordination language S-Net achieves a near-complete separation of concerns between sequential software components implemented {{in a separate}} algorithmic language and their parallel orchestration in an <b>asynchronous</b> <b>data</b> flow <b>streaming</b> network. We investigate the merits of S-Net and CnC {{with the help of}} a relevant and non-trivial linear algebra problem: tiled Cholesky decomposition. We describe two alternative S-Net implementations of tiled Cholesky factorization and compare them with two CnC implementations, one with explicit performance tuning and one without, that have previously been used to illustrate Intel CnC. Our experiments on a 48 -core machine demonstrate that S-Net manages to outperform CnC on this problem. Comment: 9 pages, 8 figures, 1 table, accepted for PLC 2014 worksho...|$|R
40|$|Abstract: Mining {{asynchronous}} coincidence {{pattern is}} a difficult task in multi-data streams. The main contributions of this work included: (1) The filter technique of Haar Wavelet is investigated and applied to mining asynchronous coincidence pattern in multi-streams; (2) The Wavelet coefficient series are applied to the measurement of <b>asynchronous</b> coincidence between <b>data</b> <b>streams.</b> A series of theorems are proved to ensure the validity of measuring asynchronous coincidence; (3) The anti-noise increment algorithms are designed on loop sliding windows to mine asynchronous coincidence pattern and implemented with complexity O(n 2); (4) The extensive experiments on real data are given to validate algorithms...|$|R
40|$|A burst {{compression}} and expansion technique is described for asynchronously interconnecting variable-data-rate users with cost-efficient ground terminals in a satellite-switched, time-division-multiple-access (SS/TDMA) network. Compression and expansion buffers in each ground terminal convert between lower rate, <b>asynchronous,</b> continuous-user <b>data</b> <b>streams</b> and higher-rate TDMA bursts synchronized with the satellite-switched timing. The technique described uses a first-in, first-out (FIFO) memory approach which enables {{the use of}} inexpensive clock sources by both the users and the ground terminals and obviates the need for elaborate user clock synchronization processes. A continuous range of data rates from kilobits per second to that approaching the modulator burst rate (hundreds of megabits per second) can be accommodated. The technique was developed for use in the NASA Lewis Research Center System Integration, Test, and Evaluation (SITE) facility. Some key features of the technique have also been implemented in the ground terminals developed at NASA Lewis for use in on-orbit evaluation of the Advanced Communications Technology Satellite (ACTS) high burst rate (HBR) system...|$|R
40|$|International Telemetering Conference Proceedings / October 20 - 23, 2003 / Riviera Hotel and Convention Center, Las Vegas, NevadaDisk Recorders now {{represent}} a high performance, low cost and reliable alternative to traditional tape recorders {{for a wide}} range of platform data recording applications. This paper discusses the latest advances in disk-based recording technology in the context of multi-channel Telemetry applications, showing the degree of flexibility that is now possible in terms of both channel count and the ability to record synchronous and <b>asynchronous</b> digital <b>data</b> <b>streams</b> alongside multiple wideband analog channels. The techniques described are equally applicable to Acoustic, SIGINT and Telecommunications data capture and analysis applications aboard static, airborne and maritime platforms. Topics covered include how new disk-based data capture technologies have been able to extend bandwidth, storage capacity, signal fidelity and the overall capability of mission recorders. Advanced operational issues, including true ‘read-after-write’, data security, portability and archiving, enhanced data management and analysis strategies are also covered. The Paper includes detailed test results from COTS Disk Recorders already in service as well as an informative Road Map for this exciting new technology...|$|R
40|$|International Telemetering Conference Proceedings / September 28 - 30, 1976 / Hyatt House Hotel, Los Angeles, CaliforniaMany future space {{programs}} such as Space Shuttle contain a number of payloads, each generating its own digital <b>data</b> <b>stream.</b> It is frequently desirable to combine these <b>data</b> <b>streams</b> in a composite serial stream for the telemetry down link. The circuitry for combining these <b>data</b> <b>streams</b> should be as transparent {{as possible to the}} design and operation of individual data source subsystems. A concept is described for interleaving the data of several sources without any subsystem synchronization, few limitations on data rates, a no restrictions on formats. All data are accepted without loss and the composite stream is formatted in accordance with IRIG standards. The interleaver requires the use of artificial fill data to assure the possibility of accepting and formatting <b>asynchronous</b> <b>data</b> symmetrically; therefore, methods of error detection and correction of fill words are discussed to ensure nonambiguity of data and fill work...|$|R
40|$|Recent {{advances}} in image and volume acquisition {{as well as}} computational {{advances in}} simulation have led to an explosion {{of the amount of}} data that must be visualized and analyzed. Modern techniques combine the parallel processing power of GPUs with out-of-core methods and <b>data</b> <b>streaming</b> to enable the interactive visualization of giga- and terabytes of image and volume data. A major enabler for interactivity is making both the computational and the visualization effort proportional to the amount of data that is actually visible on screen, decoupling it from the full data size. This leads to powerful display-aware multi-resolution techniques that enable the visualization of data of almost arbitrary size. The course consists of two major parts: An introductory part that progresses from fundamentals to modern techniques, and a more advanced part that discusses details of ray-guided volume rendering, novel data structures for display-aware visualization and processing, and the remote visualization of large online data collections. You will learn how to develop efficient GPU data structures and large-scale visualizations, implement out-of-core strategies and concepts such as virtual texturing that have only been employed recently, as well as how to use modern multi-resolution representations. These approaches reduce the GPU memory requirements of extremely large data to a working set size that fits into current GPUs. You will learn how to perform ray-casting of volume data of almost arbitrary size and how to render and process gigapixel images using scalable, display-aware techniques. We will describe custom virtual texturing architectures as well as recent hardware developments in this area. We will also describe client/server systems for distributed visualization, on-demand <b>data</b> processing and <b>streaming,</b> and remote visualization. We will describe implementations using OpenGL as well as CUDA, exploiting parallelism on GPUs combined with additional <b>asynchronous</b> processing and <b>data</b> <b>streaming</b> on CPUs...|$|R
40|$|In a <b>data</b> <b>streaming</b> system, each {{component}} consumes one or several <b>streams</b> of <b>data</b> {{on the fly}} and produces one or several <b>streams</b> of <b>data</b> for other components. The entire Internet {{can be viewed as}} a giant <b>data</b> <b>streaming</b> system. Other examples include real-time exploratory data mining and high performance transaction processing. In this thesis we study several measurement and resource allocation optimization problems of <b>data</b> <b>streaming</b> systems. Measuring quantities associated with one or several <b>data</b> <b>streams</b> is often challenging because the sheer volume of data makes it impractical to store the streams in memory or ship them across the network. A <b>data</b> <b>streaming</b> algorithm processes a long <b>stream</b> of <b>data</b> in one pass using a small working memory (called a sketch). Estimation queries can then be answered from one or more such sketches. An important task is to analyze the performance guarantee of such algorithms. In this thesis we describe a tail bound problem that often occurs and present a technique for solving it using majorization and convex ordering theories. We present two algorithms that utilize our technique. The first is to store a large array of counters in DRAM while achieving the update speed of SRAM. The second is to detect global icebergs across distributed <b>data</b> <b>streams.</b> Resource allocation decisions are important for the performance of a <b>data</b> <b>streaming</b> system. The processing graph of a <b>data</b> <b>streaming</b> system forms a fork and join network. The underlying data processing tasks consists of a rich set of semantics that include synchronous and <b>asynchronous</b> <b>data</b> fork and data join. The different types of semantics and processing requirements introduce complex interdependence between various <b>data</b> <b>streams</b> within the network. We study the distributed resource allocation problem in such systems with the goal of achieving the maximum total utility of output streams. For networks with only synchronous fork and join semantics, we present several decentralized iterative algorithms using primal and dual based optimization techniques. For general networks with both synchronous and asynchronous fork and join semantics, we present a novel modeling framework to formulate the resource allocation problem, and present a shadow-queue based decentralized iterative algorithm to solve the resource allocation problem. We show that all the algorithms guarantee optimality and demonstrate through simulation that they can adapt quickly to dynamically changing environments. Ph. D. Committee Chair: Xu, Jun; Committee Member: Ammar, Mostafa; Committee Member: Ma, Xiaoli; Committee Member: Xia, Cathy; Committee Member: Zegura, Elle...|$|R
40|$|International Telemetering Conference Proceedings / October 21, 2002 / Town & Country Hotel and Conference Center, San Diego, CaliforniaThe Joint Advanced Missile Instrumentation (JAMI) {{program is}} {{developing}} a Time, Space, and Position Information (TSPI) unit for high dynamic missile platforms by employing the use of Global Position System (GPS) and inertial sensors. The GPS data is uncoupled from the inertial data. The output of the JAMI TSPI unit follows the packet telemetry protocol and is called the TSPI unit message structure (TUMS). The packet format allows the <b>data</b> <b>stream</b> to stand on its own, be integrated into a packet telemetry system or be an <b>asynchronous</b> <b>data</b> channel in a PCM <b>data</b> <b>stream.</b> On the ground, the JAMI data processor (JDP) Kalman filters the GPS and inertial data to provide a real time TSPI solution to the ranges for display. This paper gives {{an overview of the}} message format, the timing relationships between the GPS data and inertial data, and how TUMS is to be handled by the telemetry receiving site to hand it off to the JDP...|$|R
40|$|Includes bibliographical {{references}} (leaves 47 - 51). Cataloged from PDF {{version of}} thesis. Thesis (M. S.) : Bilkent University, The Department of Computer Engineering and the Graduate School of Engineering and Science of Bilkent University, 2014. There is an ever increasing rate of digital information {{available in the}} form of online <b>data</b> <b>streams.</b> In many application domains, high throughput processing of such data is a critical requirement for keeping up with the soaring input rates. <b>Data</b> <b>stream</b> processing is a computational paradigm that aims at addressing this challenge by processing <b>data</b> <b>streams</b> in an on-the-fly manner. In this thesis, we study the problem of automatically parallelizing <b>data</b> <b>stream</b> processing applications to improve throughput. The parallelization is automatic in the sense that stream programs are written sequentially by the application developers and are parallelized by the system. We adopt the <b>asynchronous</b> <b>data</b> flow model for our work, where operators often have dynamic selectivity and are stateful. We solve the problem of pipelined fission, in which the original sequential program is parallelized by taking advantage of both pipeline and data parallelism at the same time. Our solution supports partitioned stateful data parallelism with dynamic selectivity and is designed for shared-memory multi-core machines. We first develop a cost-based formulation to express pipelined fission as an optimization problem. The bruteforce solution of this problem takes a very long time for moderately sized stream programs. Accordingly, we develop a heuristic algorithm that can quickly, but approximately, solve this problem. We provide an extensive evaluation studying the performance of our solution, including simulations and experiments with an industrial-strength <b>Data</b> <b>Stream</b> Processing Systems (DSPS). Our results show good scalability for applications that contain sufficient parallelism, closeness to optimal performance for the algorithm. by Habibe Güldamla Özsema. M. S...|$|R
5000|$|OpenPuff is used {{primarily}} for anonymous <b>asynchronous</b> <b>data</b> sharing: ...|$|R
40|$|This paper {{presents}} a modified interpolation algorithm for signals with variable <b>data</b> rate from <b>asynchronous</b> ADCs. The Adaptive weights Conjugate gradient Toeplitz matrix (ACT) algorithm is extended {{to operate with}} a continuous <b>data</b> <b>stream.</b> An additional preprocessing of data with constant and linear sections and a weighted overlap of step-by-step into spectral domain transformed signals improve {{the reconstruction of the}} asycnhronous ADC signal. The interpolation method can be used if <b>asynchronous</b> ADC <b>data</b> is fed into synchronous digital signal processing...|$|R
40|$|According to the <b>asynchronous</b> {{transmission}} of <b>data</b> for the SINS/WSN integrated positioning system, this paper proposes a novel <b>asynchronous</b> <b>data</b> fusion method using Unscented Kalman Filter for SINS/WSN integrated positioning {{system based on}} indoor mobile target. The state equation of the integrated system is built with the motion characteristic of mobile target. The pseudo measurement equation {{is built based on}} the time sequence of SINS/WSN measured value through detecting the measurement of WSN in every fusion period. Considering that the improved state-space model, comprised of the state equation and pseudo measurement equation, is the nonlinear equations, the Unscented Kalman Filter is applied to estimate the state value of the state-space model. Hence the <b>asynchronous</b> <b>data</b> fusion method for SINS/WSN integrated positioning system can be achieved. Simulation results and experimental tests show that the positioning system with proposed <b>asynchronous</b> <b>data</b> fusion algorithm performs feasibility and stability under circumstances of the asynchronous time, and it is superior to the traditional <b>asynchronous</b> <b>data</b> fusion and synchronous data fusion methods...|$|R
5000|$|IEC 62056-42:2002 Physical layer {{services}} {{and procedures for}} connection-oriented <b>asynchronous</b> <b>data</b> exchange ...|$|R
50|$|Commutated frames {{may also}} contain <b>asynchronous</b> <b>data,</b> which require further {{processing}} to extract.|$|R
3000|$|... <b>data</b> <b>streams,</b> the RS has to {{separate}} the two <b>data</b> <b>streams</b> from the other received <b>data</b> <b>streams.</b> The superposed <b>data</b> <b>stream</b> needs to be transmitted simultaneously to [...]...|$|R
30|$|In the following, {{we explain}} {{the design of}} BCSA transceive beamforming. Based on the chosen BC strategy, the RS {{separates}} the <b>data</b> <b>streams</b> which {{are going to be}} transmitted in the BC phase and transmits to the corresponding node or nodes. For unicasting strategy, the RS separates all <b>data</b> <b>streams</b> and transmits each <b>data</b> <b>stream</b> to each corresponding receiving node. For hybrid uni/multicasting, for each group, the RS separates the unicasted <b>data</b> <b>stream</b> from the other <b>data</b> <b>streams</b> and transmits it to the corresponding node whose <b>data</b> <b>stream</b> is multicasted. The RS also separates the multicasted <b>data</b> <b>stream</b> from the other <b>data</b> <b>streams</b> and transmits it to the remaining nodes in the corresponding group. For multicasting strategy, the RS separates the superposition of two <b>data</b> <b>streams</b> from the others and transmits the superposed <b>data</b> <b>stream</b> to all nodes in the group.|$|R
