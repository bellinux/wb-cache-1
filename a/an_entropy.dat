2228|6371|Public
25|$|Entropy {{is defined}} {{in the context of}} a {{probabilistic}} model. Independent fair coin flips have <b>an</b> <b>entropy</b> of 1 bit per flip. A source that always generates a long string of B's has <b>an</b> <b>entropy</b> of 0, since the next character will always be a 'B'.|$|E
25|$|In Elements the Game, Schrödinger's Cat is <b>an</b> <b>Entropy</b> {{creature}} {{that has an}} ability Dead and Alive. This activates death effects without killing the cat.|$|E
25|$|Like any {{physical}} system, {{a black hole}} has <b>an</b> <b>entropy</b> {{defined in terms of}} the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.|$|E
25|$|Brudno, <b>A.</b> <b>Entropy</b> and the {{complexity}} of the trajectories of a dynamical system., Transactions of the Moscow Mathematical Society, 2:127{151, 1983.|$|R
25|$|Other {{important}} information theoretic quantities include Rényi <b>entropy</b> (<b>a</b> generalization of <b>entropy),</b> differential <b>entropy</b> (<b>a</b> generalization of quantities {{of information to}} continuous distributions), and the conditional mutual information.|$|R
40|$|Using a sharp {{version of}} the reverse Young inequality, and <b>a</b> Rényi <b>entropy</b> {{comparison}} result due to Madiman and Wang, the authors are able to derive <b>a</b> Rényi <b>entropy</b> power inequality for log-concave random vectors when Rényi parameters belong to (0, 1). Furthermore, the estimates are shown to be somewhat sharp...|$|R
25|$|But Jacob Bekenstein {{noted that}} {{this leads to a}} {{violation}} of the second law of thermodynamics. If one throws a hot gas with entropy into a black hole, once it crosses the event horizon, the entropy would disappear. The random properties of the gas would no longer be seen once the black hole had absorbed the gas and settled down. One way of salvaging the second law is if black holes are in fact random objects with <b>an</b> <b>entropy</b> that increases by an amount greater than the entropy of the consumed gas.|$|E
500|$|R is the gas {{constant}} and T {{is the absolute}} temperature. Note that [...] and [...] At 25°C, ΔG in kJ·mol−1 ≈ 5.708 pKa (1 kJ·mol−1 = 1000 joules per mole). Free energy {{is made up of}} an enthalpy term and <b>an</b> <b>entropy</b> term.|$|E
2500|$|For example, the Fibonacci {{sequence}} is 1, 1, 2, 3, 5, 8, 13, …. Treating the sequence as {{a message and}} each number as a symbol, there are almost as many symbols as there are characters in the message, giving <b>an</b> <b>entropy</b> of approximately [...] So the first 128 symbols of the Fibonacci sequence has <b>an</b> <b>entropy</b> of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [...] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.|$|E
2500|$|Isentropic process: a {{reversible}} adiabatic process, {{occurs at}} <b>a</b> constant <b>entropy</b> ...|$|R
40|$|<b>A</b> maximum <b>entropy</b> copula is the copula {{associated}} with the joint distribution, with prescribed marginal distributions on [0, 1], which maximizes the Tsallis–Havrda–Chavát entropy with q = 2. We find necessary and sufficient conditions for each maximum entropy copula to be a copula in the class introduced in Rodríguez-Lallena and Úbeda-Flores (2004), and we also show that each copula in that class is <b>a</b> maximum <b>entropy</b> copula...|$|R
40|$|Following [6, 12], {{we study}} coupled map {{networks}} over arbitrary finite graphs. An estimate from below for <b>a</b> topological <b>entropy</b> of <b>a</b> perturbed coupled map network via <b>a</b> topological <b>entropy</b> of <b>an</b> unperturbed network by {{making use of}} the covering relations for coupled map networks is obtained. The result is quite general, particularly no assumptions on hyperbolicity of a local dynamics or linearity of coupling are made...|$|R
2500|$|For highly {{communicable}} epidemics, such as SARS in 2003, if publication {{intervention is}} involved, {{the number of}} hospitalized cases is shown to satisfy the lognormal distribution with no free parameters if <b>an</b> <b>entropy</b> is assumed and the standard deviation {{is determined by the}} principle of maximum rate of entropy production.|$|E
2500|$|The Theil {{index is}} [...] where [...] is the {{theoretical}} maximum entropy that is reached when all incomes are equal, i.e. [...] for all [...] This is substituted into [...] to give , a constant determined solely by the population. So the Theil index gives a value {{in terms of}} <b>an</b> <b>entropy</b> that measures how far [...] is away from the [...] "ideal" [...] The index is a [...] "negative entropy" [...] {{in the sense that}} it gets smaller as the disorder gets larger, hence it is a measure of order rather than disorder.|$|E
2500|$|Although entropy {{is often}} used as a {{characterization}} of the information content of a data source, this information content is not absolute: it depends crucially on the probabilistic model. A source that always generates the same symbol has <b>an</b> <b>entropy</b> rate of 0, but the definition of what a symbol is depends on the alphabet. Consider a source that produces the string ABABABABAB… in which A is always followed by B and vice versa. If the probabilistic model considers individual letters as independent, the entropy rate of the sequence is 1 bit per character. But if the sequence is considered as [...] "AB AB AB AB AB …" [...] with symbols as two-character blocks, then the entropy rate is 0 bits per character.|$|E
40|$|The paper {{presents}} <b>a</b> maximum <b>entropy</b> Chinese character-based parser {{trained on}} the Chinese Treebank ("CTB" henceforth). Word-based parse trees in CTB are first converted into characterbased trees, where word-level part-ofspeech (POS) tags become constituent labels and character-level tags are derived from word-level POS tags. <b>A</b> maximum <b>entropy</b> parser is then {{trained on the}} character-based corpus. The parser does word-segmentation, POStagging and parsing in a unified framework...|$|R
30|$|This paper {{means no}} {{prejudice}} on this oil-oriented economy structure or else. That is because {{when talking about}} one structure is superior to others, specific situations {{must be taken into}} account. What is worth questioning in this paper is that whether it is still appropriate for Venezuela to use this structure under the current scenario. <b>As</b> <b>entropy,</b> or more directly, the disorderliness increases, it is really worth reconsideration.|$|R
40|$|AbstractIn this paper, {{we study}} coupled map {{networks}} over arbitrary finite graphs. An estimate from below for <b>a</b> topological <b>entropy</b> of <b>a</b> perturbed coupled map network is obtained via <b>a</b> topological <b>entropy</b> of <b>an</b> unperturbed network by {{making use of}} the covering relations for coupled map networks. The result is quite general; in particular, nonlinear coupling is allowed and no assumptions of hyperbolicity of the local dynamics are made...|$|R
2500|$|The first form can {{be termed}} a density-of-states based entropy. The Fermi–Dirac {{distribution}} implies that each eigenstate of a system, , is occupied {{with a certain}} probability, [...] As the entropy is given by a sum over the probabilities of occupation of those states, there is <b>an</b> <b>entropy</b> associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior. A second form of electronic entropy {{can be attributed to}} the configurational entropy associated with localized electrons and holes.FePO4 ...|$|E
50|$|Entropy {{is defined}} {{in the context of}} a {{probabilistic}} model. Independent fair coin flips have <b>an</b> <b>entropy</b> of 1 bit per flip. A source that always generates a long string of B's has <b>an</b> <b>entropy</b> of 0, since the next character will always be a 'B'.|$|E
5000|$|... #Caption: Foveated {{image with}} {{fixation}} selected using <b>an</b> <b>entropy</b> minimization algorithm ...|$|E
40|$|Two {{cellular}} automata {{are strongly}} conjugate if {{there exists a}} shift-commuting conjugacy between them. We prove that the following two sets of pairs $(F,G) $ of one-dimensional one-sided cellular automata over a full shift are recursively inseparable: (i) pairs where $F$ has strictly larger topological entropy than $G$, and (ii) pairs that are strongly conjugate and have zero topological entropy. Because there is no factor map from <b>a</b> lower <b>entropy</b> system to <b>a</b> higher <b>entropy</b> one, {{and there is no}} embedding of <b>a</b> higher <b>entropy</b> system into <b>a</b> lower <b>entropy</b> system, we also get as corollaries that the following decision problems are undecidable: Given two one-dimensional one-sided cellular automata $F$ and $G$ over a full shift: Are $F$ and $G$ conjugate? Is $F$ a factor of $G$? Is $F$ a subsystem of $G$? All of these are undecidable in both strong and weak variants (whether the homomorphism is required to commute with the shift or not, respectively). It also immediately follows that these results hold for one-dimensional two-sided cellular automata. Comment: 12 pages, 2 figures, accepted for SOFSEM 201...|$|R
50|$|Every {{distribution}} with log-concave {{density is}} <b>a</b> maximal <b>entropy</b> distribution with specified mean μ and Deviation risk measure D.|$|R
5000|$|The Wehrl entropy of [...] for Bloch {{coherent}} {{states is}} defined as <b>a</b> classical <b>entropy</b> of the density distribution , ...|$|R
5000|$|<b>An</b> <b>entropy</b> {{maximization}} {{problem is}} a convex optimization problem of the form ...|$|E
5000|$|Nameles - <b>an</b> <b>entropy</b> based {{detection}} and filtering solution to counter ad fraud ...|$|E
5000|$|I’d {{look for}} <b>an</b> <b>entropy</b> reduction, since {{this must be}} a general {{characteristic}} of life.|$|E
3000|$|..., {{we propose}} <b>a</b> cross <b>entropy</b> (CE) based {{algorithm}} together with analytical pilot power distribution technique to determine pilot set [...]...|$|R
3000|$|Now {{we aim at}} {{defining}} the quantization point density functions which minimize D for <b>a</b> target <b>entropy</b> denoted H [...]...|$|R
40|$|Quantum Molecular Dynamics (QMD) {{calculations}} {{are used}} to study the expansion phase in central collisions between heavy nuclei. The final state of such a reaction {{can be understood as}} the result of <b>a</b> <b>entropy</b> conserving expansion starting from a compact source. The properties of this hypothetic source, however, are in conflict with the assumptions used in fireball models. Moreover, this hypothetical source is not formed in the dynamical evolution of the system. ...|$|R
5000|$|Just like in {{the balance}} laws in the {{previous}} section, we assume {{that there is a}} flux of a quantity, a source of the quantity, and an internal density of the quantity per unit mass. The quantity of interest in this case is the entropy. Thus, we assume that there is <b>an</b> <b>entropy</b> flux, <b>an</b> <b>entropy</b> source, and an internal entropy density per unit mass (...) in the region of interest.|$|E
50|$|You, L., and S. Wood. 2006. <b>An</b> <b>entropy</b> {{approach}} to spatial disaggregation of agricultural production. Agricultural Systems 90(1-3): 329-347.|$|E
50|$|The {{design of}} Yarrow {{consists}} of four major components including <b>an</b> <b>entropy</b> accumulator, reseed mechanism, generation mechanism and reseed control.|$|E
40|$|This paper {{presents}} <b>a</b> novel <b>entropy</b> descriptor in {{the sense}} of geo-metric manifolds. With this descriptor, entropy cycles can be easily designed for image classification. Minimizing this <b>entropy</b> leads to <b>an</b> optimal <b>entropy</b> cycle where images are connected in the se-mantic order. During classification, the training step is to find <b>an</b> optimal <b>entropy</b> cycle in each class. In the test step, an unknown image is grouped into a class if the entropy increase as the result of inserting the image into the cycle of this class is relatively least. The proposed approach can generalize well on difficult image clas-sification problems where images with same objects are taken in multiple views. Experimental results show that this entropy de-scriptor performs well in image classification and has potential in the image-based modeling retrieval...|$|R
30|$|Theorem 3.3 Assume that (3) holds. Then Cauchy problem (7) has <b>a</b> unique <b>entropy</b> weak {{solution}} {{in the sense}} of Definition  2.2.|$|R
50|$|<b>A.</b> Castellanos. <b>Entropy</b> {{production}} and the temperature equation in electrohydrodynamics. IEEE Transactions on Dielectrics and Electrical Insulation, vol. 10, pp. 22-26, 2003.|$|R
