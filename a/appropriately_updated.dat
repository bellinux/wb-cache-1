17|38|Public
50|$|Each {{statement}} {{having at}} least one valid justification is made {{a part of the}} current belief set. When a contradiction is found, the statement(s) responsible for the contradiction are identified and the records are <b>appropriately</b> <b>updated.</b> This process is called dependency-directed backtracking.|$|E
5000|$|The game is {{historically}} detailed; one {{can play the}} [...] "grand campaign" [...] starting in 1453 or 1399, but any date before the French Revolution (1821 with the Napoleon's Ambition expansion) is a valid starting point as well, with historical leaders and countries <b>appropriately</b> <b>updated.</b> Major {{events such as the}} War of the League of Cambrai are playable this way. Often the game diverges from reality after some time in-game, with unhistorical events such as Portugal colonizing North America, or Poland-Lithuania surviving to bully its neighbors.|$|E
5000|$|In 2007 Kuczynski {{took part}} in the theatre project, [...] "Karl Marx: Das Kapital - Erster Band" [...] ("Karl Marx: Das Kapital - First volume"), a show {{presenting}} a range of experts, mostly not normally to be found on a theatre stage, from various political and business backgrounds, giving their perspectives on Marxian philosophy in general and on Das Kapital in particular. [...] Kuczynski claims to eschew [...] "-isms", {{but it is hard to}} avoid concluding that much of his academic work is undertaken through an <b>appropriately</b> <b>updated</b> Marxist prism. [...] He is a member of the advisory council of the Bildungsgemeinschaft SALZ (Soziales, Arbeit, Leben & Zukunft), a left-of-centre think tank.|$|E
30|$|Another {{shortcoming}} in TCP is {{that its}} CWND update mechanism does not fit the distinct characteristics of different networks. For example, while satellite networks demand an aggressive CWND increase, WANETs perform better under a conservative approach. Table  12 outlines several solutions that have used RL techniques to <b>appropriately</b> <b>update</b> CWND according to the network conditions. Half of these ML-based approaches apply FALA, CALA, or Q-learning (including the FK function approximation) on resource-constrained networks (i.e. WANET and IoT). Whereas, the other half either use CALA or an own RL design on {{a wider range of}} network architectures, including satellite, cellular, and data center networks.|$|R
40|$|Three {{automatic}} {{gain control}} (AGC) strategies and their potential capabilities for mitigation of pulsed radio frequency interference (RFI) over a coded, nonlinear channel are examined. The first updates the AGC control voltage once each symbol interval in accordance with a prespecified criterion. The second implementation is designed to first detect high level RFI and then <b>appropriately</b> <b>update</b> its gain. The final implementation is a wideband device which updates its gain once each symbol interval by reducing the gain {{in direct proportion to}} a suitable power measurement. Computed results treat pulsed noise and pulsed CW RFI and compare performances in terms of bit error rate (BER) at the output of a Viterbi decoder...|$|R
5000|$|If B is a leaf node, scan {{through every}} point {{enumerated}} in B and update the nearest-neighbor queue <b>appropriately.</b> Return the <b>updated</b> queue.|$|R
40|$|This report {{covers all}} {{nondestructive}} development {{effort on the}} Roll Control Program for the period June 1, 1976 through December 15, 1976. Information previously reported will be re-presented here and <b>appropriately</b> <b>updated.</b> The samples consist of solid propellant and composites of propellant and inhibitor that are currently being examined in the accelerated aging and shelf life prediction studies of materials involved in the B- 77 Roll Control Program...|$|E
30|$|Alarcon and Lopez [8] applied PFs for {{tracking}} {{the lines of}} a road in real time. For each image, the presence of the lane lines is detected and their center of mass is calculated. Three consecutive images frame 0, frame 1, and frame 2 are used for prediction and full tracking of the lane lines. The architecture is designed such that each particle is evaluated and <b>appropriately</b> <b>updated</b> independent of the rest.|$|E
40|$|The self-tuning {{strategy}} {{concerning the}} stabilized control {{system of the}} rotational inverted pendulum is considered. In {{the case of the}} pendulum with expansion and contraction under the stabilization, the system parameter is recursively estimated and the redesigned controller is <b>appropriately</b> <b>updated.</b> The available disturbance torque is appended to the control signal because the inverted pendulum does not have sufficient information for the estimation of the system parameter under the stabilization. It is verified by the practical experiment that the proposed self-tuning is very useful as one of the on-line tuning...|$|E
40|$|Abstract. The key task in {{probabilistic}} reasoning is to <b>appropriately</b> <b>update</b> one’s beliefs as one obtains new {{information in the}} form of evidence. In many application settings, however, the evidence we obtain as input to an inference problem may be uncertain (e. g. owing to unreliable mechanisms with which we obtain the evidence) or may correspond to (soft) degrees of belief rather than hard logical facts. So far, methods for updating beliefs in the light of soft evidence have been centred around the iterative proportional fitting procedure and variations thereof. In this work, we propose a Markov chain Monte Carlo method that allows to directly integrate soft evidence into the inference procedure without generating substantial computational overhead. Within the framework of Markov logic networks, we demonstrate the potential benefit of this method over standard approaches in a series of experiments on synthetic and real-world applications. ...|$|R
40|$|In many {{practical}} situations, we do {{not have}} full information about which alternatives are possible and which are not. In such situations, an expert can estimate, for each alternative, the degree to which this alternative is possible. Sometimes, experts can produce numerical estimates of their degrees, but often, they can only provide us with qualitative estimates: they inform us which degrees are higher, but do not provide us with numerical values for these degrees. After we get these degrees from the experts, we often gain additional information, because of which some alternatives which were previously considered possible are now excluded. To take this new information into account, we need to <b>appropriately</b> <b>update</b> the corresponding possibility degrees. In this paper, we prove that under several natural requirements on such an update procedure, there is only one procedure that satisfies all these requirements [...] namely, the min-based conditioning...|$|R
40|$|We {{present an}} ecient {{algorithm}} {{for determining the}} number of spanning trees {{in the class of}} P 4 -reducible graphs, which are perfect graphs and generalize both the well-known class of cographs and the class of quasi-threshold graphs. In particular, for a P 4 -reducible graph G on n vertices and m edges, our algorithm computes the number of spanning trees of G in O(n+m) time and space, where the complexity of arithmetic operations is measured under the uniform cost criterion. The algorithm takes advantage of the modular decomposition tree of the input graph which it gradually shrinks in a systematic fashion until it becomes a single vertex {{while at the same time}} <b>appropriately</b> <b>updating</b> certain parameters whose product gives the desired number of spanning trees. The correctness of the algorithm is established through the Kirchho matrix tree theorem, and is also based on structural and algorithmic properties of the graphs with few P 4 s. Our results generalize previous results and extend the family of graphs admitting linear-time algorithms for the number of their spanning trees...|$|R
40|$|These {{notes are}} based on lectures given at the Les Houches Summer School in 2011, which was {{centered}} on the general topic "Theoretical Physics to {{face the challenge of}} LHC". In these lectures I reviewed a number of topics in the field of String Phenomenology, focusing on orientifold/F-theory models yielding semi-realistic low-energy physics. The emphasis was on the extraction of the low-energy effective action and the possible test of specific models at LHC. These notes are a brief summary, <b>appropriately</b> <b>updated,</b> of some of the main topics covered in the lectures. Comment: 28 pages, 9 figure...|$|E
40|$|Coordinating the {{cumulative}} use of distributed resources in a grid environment so that users do not consume {{too much is}} a difficult task. This paper presents one approach that we have implemented in Globus Toolkit version 4 (GT 4), that uses an SQL database to hold “coordination ” data, and policy decision points (PDPs) to make access control decisions about whether the user’s request for more resources can be granted or denied. When access is granted, obligations in the policy ensure that the coordination database is <b>appropriately</b> <b>updated.</b> In our initial implementation, the coordination service is imbedded into the GT 4 authorization chain as a custom PDP so that any web service can be provided with a security policy that provides a coordination capability. In the final section we describe how coordinated decision making could be more tightly integrated into a future version of GT...|$|E
40|$|Active {{knowledge}} bases {{result from}} the addition of action rules to ordinary knowledge bases, like relational or deductive databases and their (for instance, disjunctive) extensions. Action rules provide, e. g., for knowledge base updates on certain conditions, thus beeing able to enforce integrity constraints. But they can also call procedures, like running certain diagnostic tests, or turning switches on and off. In this paper, we shall focus on update actions. When updating an active KB, all action rules have to be checked and fired if their condition is satisfied, leading to subsequent updates triggering further action rules. This recursive process is captured declaratively by {{the notion of an}} active closure of a KB, being an <b>appropriately</b> <b>updated</b> KB closed under all action rules. Since there might be several closures, we define the notion of a stable closure of an active knowledge base in order to distinguish the preferred, resp. intended, ones from the non-intended ones. We are the [...] ...|$|E
40|$|Color-code quantum {{computation}} seamlessly combines Majorana-based hardware with topological error correction. Specifically, as Clifford gates are transversal in two-dimensional color codes, they enable {{the use of}} the Majoranas' nonabelian statistics for gate operations at the code level. Here, we discuss the implementation of color codes in arrays of Majorana nanowires that avoid branched networks such as T-junctions, thereby simplifying their realization. We show that, in such implementations, nonabelian statistics can be exploited without ever performing physical braiding operations. Physical braiding operations are replaced by Majorana tracking, an entirely software-based protocol which <b>appropriately</b> <b>updates</b> the Majoranas involved in the color-code stabilizer measurements. This approach minimizes the required hardware operations for single-qubit Clifford gates. For Clifford completeness, we combine color codes with surface codes, and use color-to-surface-code lattice surgery for long-range multi-target CNOT gates which have a time overhead that grows only logarithmically with the physical distance separating control and target qubits. With the addition of magic state distillation, our architecture describes a fault-tolerant universal quantum computer in systems such as networks of tetrons, hexons, or Majorana box qubits, but can also be applied to non-topological qubit platforms. Comment: 11 pages, 9 figure...|$|R
40|$|The Employee Retirement Income Security Act of 1974 (ERISA) {{was enacted}} {{in large part}} to protect {{employee}} benefit plan participants. Yet ERISA’s broad preemption clause may actually thwart this goal in certain cases by imposing unexpected consequences on participants who die before <b>appropriately</b> <b>updating</b> their beneficiary designations. For instance, although many state laws presume divorce to revoke the former spouse’s beneficiary status, the U. S. Supreme Court’s 2001 decision in Egelhoff v. Egelhoff ex rel. Breiner made clear that ERISA preempts such state-level wills doctrines, enabling a former spouse to benefit when the deceased participant likely intended otherwise. The rationale behind this broad preemption provision applies equally to other wills doctrines such as “slayer statutes,” which prevent murderers from benefitting from their crimes. Therefore, it is likely impossible to confine the impact of this pre-emptive effect to the divorce realm. Moreover, in the wake of Kennedy v. Plan Administrator for DuPont Savings 2 ̆ 6 Investment Plan, decided by the U. S. Supreme Court in 2009, the federal common law no longer appears to be a valid solution to this problem of effecting the likely intent of deceased plan participants. Congressional action to amend ERISA is therefore necessary to avoid these inequitable results...|$|R
30|$|Isochores were {{calculated}} for each zone between stratigraphic well tops from the “Convert to isochore points” command by right clicking the lower stratigraphic well tops of the zone of interest. TVT was also calculated from the well top data in the “Well settings” window. This ensures that the zone spreadsheet <b>updates</b> <b>appropriately</b> with thickness values. Isochore data were used directly to generate thickness maps.|$|R
40|$|This work {{is focused}} on {{improving}} the computational efficiency of evolutionary algorithms implemented in large-scale structural optimization problems. Locating optimal structural designs using evolutionary algorithms is a task associated with high computational cost, since a complete finite element (FE) analysis needs {{to be carried out}} for each parent and offspring design vector of the populations considered. Each of these FE solutions facilitates decision making regarding the feasibility or infeasibility of the corresponding structural design by evaluating the displacement and stress constraints specified for the structural problem at hand. This paper presents a neural network (NN) strategy to reliably predict, in the framework of an evolution strategies (ES) procedure for structural optimization. the feasibility or infeasibility of structural designs avoiding computationally expensive FE analyses, The proposed NN implementation is adaptive {{in the sense that the}} utilized NN configuration is <b>appropriately</b> <b>updated</b> as the FS process evolves by performing NN retrainings using information gradually accumulated during the ES execution, The prediction capabilities and the computational advantages offered by this adaptive NN scheme coupled with domain decomposition solution techniques are investigated in the context of design optimization of skeletal structures On both sequential and parallel computing environments. (c) 2005 Elsevier B. V. All rights reserved...|$|E
40|$|While the {{information}} technologies provide organizational members with explicit concepts, such as writing instruction manuals, the ‘organizational memory’ provides individuals with tacit knowledge, such as systematic sets, routines and shared visions. This means that individuals within an organization learn by using both the organizational memory and {{the information}} technologies. They interact to reduce organizational information needs contributing to improve organizational innovativeness. However, the utilization of the organization memory or the technology infrastructure does not guarantee that appropriate information is used in appropriate circumstances or that information is <b>appropriately</b> <b>updated.</b> In other words, previous memories reflect {{a world that is}} only partially understood and assimilated, which might lead individuals to doing the wrong things right or the right things wrong. This paper examines the relative importance and significance of the existence of unlearning to the presence and nature of ‘organizational memory and technology’. We further examine the effect of the existence of organizational memory and information technology on conditions that promote organizational innovativeness. These relationships are examined through an empirical investigation of 291 large Spanish companies. Our analysis found that if the organization considers the establishment of an unlearning culture as a prior step in the utilization of organization memory or the technology infrastructure through organizational innovativeness, then organization memory and technology have a positive influence on the conditions that stimulate organizational innovativeness...|$|E
40|$|Abstract—Network {{services}} are often provided by server clusters. From {{the perspective of}} operational expenditure and the global environment, the power consumption of server clusters should be decreased. This is possible by operating the minimum number of computers required to realize a sufficiently good performance against changes in load. To do this, {{it is necessary to}} accurately determine the number of computers that should be turned on or off for the measured load metrics. This number should be determined by estimating multiple load metrics because a single metric does not adequately represent the statuses of various bottlenecked resources. In addition, decision rules should be <b>appropriately</b> <b>updated</b> if there are changes in the service content or computer specifications. To satisfy these requirements, this study proposes a machine learning approach as a method of determining the number of server computers. Another technical requirement for power management is that the load metrics should be measured nonintrusively for the OS or hardware of the cluster computers. From this viewpoint, we employ traffic parameters as the metrics that reflect resource consumptions. These traffic parameters are passively measured on a machine that is separate from the server cluster. This paper first explains the machine learning approach to determine the number of computers. The implementation of the approach is then presented. The effectiveness of the scheme is confirmed experimentally. Index Terms — machine learning; power management; server cluster; measurement; traffic T I...|$|E
50|$|Induction {{variable}} analysis: Roughly, if {{a variable}} in a loop {{is a simple}} linear function of the index variable, such as , it can be <b>updated</b> <b>appropriately</b> each time the loop variable is changed. This is a strength reduction, and also may allow the index variable's definitions to become dead code. This information is also useful for bounds-checking elimination and dependence analysis, among other things.|$|R
40|$|In this paper, {{we present}} the next-to-leading order {{predictions}} for three photon {{production in the}} Standard Model, matched to the parton shower using the MC@NLO formalism. We have studied the role of parton shower on various observables and shown a selection of results for the 14 TeV LHC. Comment: 17 pages, 7 figures, 2 tables, v 2 : The results have been <b>updated</b> <b>appropriately</b> after changing the cut for the generation of the event...|$|R
40|$|Schizophrenia is {{the ninth}} {{leading cause of}} {{disability}} worldwide (e. g., Lopez et al., 2006), and is a devastating psychiatric illness. Although diagnosis is made based upon the occurrence {{of positive and negative}} symptoms (First, Spitzer, Gibbon & Williams, 1995), it is the cognitive symptoms that are most strongly associated with functional outcome (Green, 1996). Cognitive control, including the ability to <b>appropriately</b> <b>update</b> relevant information and resist interference from irrelevant information, is critical for flexible and adaptive goal-directed behavior, and is among the most frequently noted of the cognitive symptoms in schizoprenia (Barch, 2005; Barch & Smith, 2008). Despite this, deficits in cognitive control are unaffected by medications used to treat the clinical symptoms of the disorder (Greene et al, 2008). Understanding both the behavioral and the neural mechanisms that comprise this deficit is thus of paramount importance. Although deficits in cognitive control in schizoprenia have been extensively studied, a number of questions still remain. Here, I ask two main questions: First, is cognitive control impaired globally, or are only certain aspects of cognitive control impaired in schizophrenia? I found that that there are (at least) two different selection mechanisms, and that people with schizophrenia are impaired in only one of these: dysregulation in left posterior ventrolateral prefrontal cortex correlates with impaired behavioral performance on a working memory task, suggesting that deficits in inhibiting irrelevant information from working memory is the crux of the deficit. Second, I asked whether the nature of the information affects cognitive control. I found that people with schizophrenia are able to deploy cognitive control processes more effectively than healthy controls in cases in which salient, emotional information competes with active cognitive goals, suggesting specific underlying deficits in emotional processing...|$|R
40|$|Phosphorus {{transfer}} from agricultural soils to surface waters {{is an important}} environmental issue. Commonly used computer models like EPIC {{have not always been}} <b>appropriately</b> <b>updated</b> to reflect our improved understanding of soil P transformations and transfer to runoff. Our objectives were to determine if replacing EPIC’s constant sorption and desorption rate factor (0. 1) with more dynamic rate factors can more accurately predict changes in soil labile P on addition to and depletion of P from soils. From published data, methods were developed to easily determine dynamic sorption and desorption rate constants from soil properties. These methods were tested with data from new soil P incubation experiments where changes in soil labile P after P addition to and depletion from nine U. S. soils were measured. Replacing constant 0. 1 P sorption rate factors with dynamic factors improved prediction of soil labile P with time after P additions but more so for high-clay than low-clay soils. EPIC’s constant 0. 1 P desorption rate factor greatly underpredicted soil P desorption. Increasing the constant to 0. 6 improved predictions, whereas dynamic P desorption rate factors most accurately predicted P desorption. Soil P simulations showed that replacing constant P sorption and desorption rate factors with dynamic ones may change dissolved P loads (kg ha 21) in runoff for common soil, cropping, and runoff scenarios by only 1 to 8 % in the long term but by 8 to 30 % in the short term. These improvements are recommended given the simplicity of making EPIC’s sorption and desorption rate factors dynamic...|$|E
40|$|In this paper, {{we propose}} a model {{reference}} adaptive control (MRAC) strategy for continuous-time singleinput single-output (SISO) linear time-invariant (LTI) systems with unknown parameters, performing repetitive tasks. This is achieved through {{the introduction of}} a discrete-type parametric adaptation law in the ‘iteration domain’, which is directly obtained from the continuous-time parametric adaptation law used in standard MRAC schemes. In fact, at the first iteration, we apply a standard MRAC to the system under consideration, while for the subsequent iterations, the parameters are <b>appropriately</b> <b>updated</b> along the iteration-axis, in order to enhance the tracking performance from iteration to iteration. This approach {{is referred to as the}} model reference adaptive iterative learning control (MRAILC). In the case of systems with relative degree one, we obtain a pointwise convergence of the tracking error to zero, over the whole finite time interval, when the number of iterations tends to infinity. In the general case, i. e. systems with arbitrary relative degree, we show that the tracking error converges to a prescribed small domain around zero, over the whole finite time interval, when the number of iterations tends to infinity. It is worth noting that this approach allows: (1) to extend existing MRAC schemes, in a straightforward manner, to repetitive systems; (2) to avoid the use of the output time derivatives, which are generally required in traditional iterative learning control (ILC) strategies dealing with systems with high relative degree; (3) to handle systems with multiple tracking objectives (i. e. the desired trajectory can be iteration-varying). Finally...|$|E
40|$|Landslide hazard {{and risk}} are growing as a {{consequence}} of climate change and demographic pressure. Land‐use planning represents a powerful tool to manage this socio‐economic problem and build sustainable and landslide resilient communities. Landslide inventory maps are a cornerstone of land‐use planning and, consequently, their quality assessment represents a burning issue. This work aimed to define the quality parameters of a landslide inventory and assess its spatial and temporal accuracy with regard to its possible applications to land‐use planning. In this sense, I proceeded according to a two‐steps approach. An overall assessment of the accuracy of data geographic positioning was performed on four case study sites located in the Italian Northern Apennines. The quantification of the overall spatial and temporal accuracy, instead, focused on the Dorgola Valley (Province of Reggio Emilia). The assessment of spatial accuracy involved a comparison between remotely sensed and field survey data, as well as an innovative fuzzylike analysis of a multi‐temporal landslide inventory map. Conversely, long‐ and short‐term landslide temporal persistence was appraised over a period of 60 years with the aid of 18 remotely sensed image sets. These results were eventually compared with the current Territorial Plan for Provincial Coordination (PTCP) of the Province of Reggio Emilia. The outcome of this work suggested that geomorphologically detected and mapped landslides are a significant approximation of a more complex reality. In order to convey to the end‐users this intrinsic uncertainty, a new form of cartographic representation is needed. In this sense, a fuzzy raster landslide map may be an option. With regard to land‐use planning, landslide inventory maps, if <b>appropriately</b> <b>updated,</b> confirmed to be essential decision‐support tools. This research, however, proved that their spatial and temporal uncertainty discourages any direct use as zoning maps, especially when zoning itself is associated to statutory or advisory regulations. ...|$|E
40|$|ABSTRACT An Important {{aspect of}} {{cognitive}} control {{is the ability}} to <b>appropriately</b> select, <b>update,</b> and maintain contextual information related to behavioral goals, and to me this information to coordinate processing over extended periods. In our novel, neurobiologicaUy based, connectionist computational model, the selection, updating, and maintenance of context occur through Interactions between the prefrontal cortex @'PC) and dofamlne (DA) neuiotiuismitter system. Phasic DAactivity serves two simultaneous and synergistic functions: (1) a gating function, which regulates the access of information to active memory mechanisms subserved by PFC; and (2) a learning function, whlcn auowa me system to discover wast uifonnation is relevant for selection as context. We present a simulation that establishes the computational viability of these postulated neurobiological mechanisms for subserving control functions. The need for a control mechanism in cognition has been long noted within psychology. Virtually all theorists agree that some mechanism is needed to guide, coordinate, and update behavior in a flexible fashionparticularly in novel or complex tasks (Norman and Shallice 1986). I...|$|R
40|$|In this paper, {{we study}} {{the problem of}} {{large-scale}} Kernel Logistic Regression (KLR). A straightforward approach is to apply stochastic approximation to KLR. We refer to this approach as non-conservative online learning algorithm because it updates the kernel classifier after every received training example, leading to a dense classifier. To improve the sparsity of the KLR classifier, we propose two conservative online learning algorithms that update the classifier in a stochastic manner and generate sparse solutions. With <b>appropriately</b> designed <b>updating</b> strategies, our analysis shows that the two conservative algorithms enjoy similar theoretical guarantee {{as that of the}} non-conservative algorithm. Empirical studies on several benchmark data sets demonstrate that compared to batch-mode algorithms for KLR, the proposed conservative online learning algorithms are able to produce sparse KLR classifiers, and achieve similar classification accuracy but with significantly shorter training time. Furthermore, both the sparsity and classification accuracy of our methods are comparable to those of the online kernel SVM...|$|R
40|$|We {{propose a}} {{sequential}} Markov chain Monte Carlo (SMCMC) algorithm to sample from {{a sequence of}} probability distributions, corresponding to posterior distributions at different times in on-line applications. SMCMC proceeds as in usual MCMC but with the stationary distribution <b>updated</b> <b>appropriately</b> each time new data arrive. SMCMC has advantages over sequential Monte Carlo (SMC) in avoiding particle degeneracy issues. We provide theoretical guarantees for the marginal convergence of SMCMC under various settings, including parametric and nonparametric models. The proposed approach is compared to competitors in a simulation study. We also consider an application to on-line nonparametric regression...|$|R
40|$|The {{difficulty}} of exchanging information between heterogeneous medical databases {{remains one of}} the chief obstacles in achieving a unified patient medical record. Although methods have been developed to address differences in data formats, system software, and communication protocols, automated data exchange between disparate systems still remains an elusive goal. The Medical Information Acquisition and Transmission Enabler (MEDIATE) system identifies semantically equivalent concepts between databases to facilitate information exchange. MEDIATE employs a semantic network representation to model underlying native databases and to serve as an interface for database queries. This representation generates a semantic context for data concepts that can subsequently be exploited to perform automated concept matching between disparate databases. To test the feasibility of this system, medical laboratory databases from two different institutions were represented within MEDIATE and automated concept matching was performed. The experimental results show that concepts that existed in both laboratory databases were always correctly recognized as candidate matches. (cont.) In addition, concepts which existed in only one database could often be matched with more "generalized" concepts in the other database that could still provide useful information. The architecture of MEDIATE offers advantages in system scalability and robustness. Since concept matching is performed automatically, the only work required to enable data exchange is construction of the semantic network representation. No pre-negotiation is required between institutions to identify data that is compatible for exchange, and there is no additional overhead to add more databases to the exchange network. Because the concept matching occurs dynamically at the time of information exchange, the system is robust to modifications in the underlying native databases as long as the semantic network representations are <b>appropriately</b> <b>updated.</b> by Yao Sun. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, February 2002. "February 2002. "Includes bibliographical references (p. 123 - 127) ...|$|E
40|$|The {{cooperative}} {{enterprises in}} the Province of Modena are ‘long life’ economic species. This characteristic is more marked, as their ability {{in developing a}} network interactions increases. These results are compatible with other studies — theoretical and empirical — which attribute to the cooperative enterprise the the highest probability of survival compared to other firms. Unfortunately, the longevity of the enterprises is not a unequivocal indicator of economic efficiency. On the contrary, {{the relationship between the}} concept (historical) of “longevity” and the standard concept (a-temporal) of efficiency in the economic theory, is not only ambiguous, but also contradictory on the epistemological level. This paper uses an institutional perspective in its attempts to identify a connecting path from the traditional to the evolutionary-historical point of view. It is in this framework that the implications of the definition of economic efficiency of the cooperative enterprise are discussed. The key-concept examined is that of transaction costs as the costs of altering the division of labour. In such a theoretical perspective, the cooperative enterprise appears as both a individual firm and as aggregate movement of firms, that is a network of firm relationships, which— focus on economising on such costs. In fact, during its history, particularly in case of the Modena experience, the cooperative movement has deeply reduced the number and the cost of many transactions in the economic system. It has done this by transferring information between firms and agents; creating a quality standard; reducing search costs in the goods and labour markets. It also generated new contractual relationships/institutions; contributed to developing a culture of the training and public administration. Overall cooperatives like enterprises profited from reduction of such costs, which are often present in other areas and economic contexts. The present paper finds that these all of these factors tend to indicate an evolutionarily, efficient path of development; and are the principal reasons for the greater longevity of this enterprise form. The data, <b>appropriately</b> <b>updated</b> and adjusted, used in the paper are from the files on cooperative enterprises of the Ufficio Provinciale del Lavoro (Provincial Labour Agency). These files contain detailed information on 2760 cooperative enterprises (birth date, number of members, sector of activity, exit date, ecc.). The multivariate and other statistical analysis on firm mortality do not reject our basic hypotheses...|$|E
40|$|In this paper, {{we present}} an {{improved}} Direct Adaptive Fuzzy (IDAF) controller applied to general control DC motor speed system. In particular, an IDAF algorithm {{is designed to}} control an uncertain DC motor speed to track a given reference signal. In fact, {{the quality of the}} control system depends significantly on the amount of fuzzy rules-fuzzy sets and the updating coefficient of the adaptive rule. This can be observed clearly by the system error when the reference input is constant and out of a particular range {{or in the case of}} it varies with nonzero acceleration. So, in order to enhance quality of the system, increasing the amount of fuzzy sets and adjusting <b>appropriately</b> the <b>updating</b> coefficient of controller based on value of state error vector are needed. In addition, the proposed IDAF algorithm can control the DC motor speed under unstable supply voltages and varying loads. The control system is implemented on a dsPIC 33 FJ 256 MC 710 A 16 -bit DSC (Digital Signal Processing Controller) board. Experimental results demonstrate the effectiveness of the proposed method.  </p...|$|R
40|$|Sometimes an {{optimization}} {{problem can be}} simplified to a form that is faster to solve. Indeed, sometimes it is convenient to state a problem {{in a way that}} admits some obvious simplifications, such as eliminating fixed variables and removing constraints that become redundant after simple bounds on the variables have been <b>updated</b> <b>appropriately.</b> Because of this convenience, the AMPL modeling system includes a "presolver" that attempts to simplify a problem before passing it to a solver. The current AMPL presolver carries out all the primal simplifications described by Brearely et al. in 1975. This paper describes AMPL's presolver, discusses reconstruction of dual values for eliminated constraints, and presents some computational results...|$|R
50|$|The {{endpoints}} {{of both of}} {{the sets}} of x-intervals can be maintained in a kinetic sorted list. When points swap, the list of antipodal edge-point pairs are <b>updated</b> <b>appropriately.</b> The upper and lower envelopes can be maintained using the standard data structure for kinetic convex hull. The minimum distance between edge-point pairs can be maintained with a kinetic tournament. Thus, using kinetic convex hull to maintain {{the upper and lower}} envelopes, a kinetic sorted list on these intervals to maintain the antipodal edge-vertex pairs, and a kinetic tournament to maintain the pair of minimum distance apart, the diameter of a moving point set can be maintained.|$|R
40|$|Abstract — Multiple-Access Interference (MAI) {{has been}} {{considered}} as a major performance-limiting factor in the new generation CDMA systems. Multiuser detection (MUD) methods have been proposed to mitigate the MAI from the co-channel users by incoporating the cross-correlation properties between users. Recently, two classes of emerging techniques, probabilistic data association (PDA) and Markov Chain Monte Carlo (MCMC) methods, have been applied to the multiuser detection. In this paper, we present a new method, named Monte Carlo PDA (MC-PDA), that incorporates the concepts of both to give a more reliable inference of the CDMA symbols by <b>appropriately</b> modelling and <b>updating</b> the MAI. The methodology is general and {{can be applied to}} other communication channels. I...|$|R
