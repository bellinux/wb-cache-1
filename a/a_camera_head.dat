15|10000|Public
5000|$|PackBot Explorer Carries an {{integrated}} payload {{which can be}} elevated to allow peeking over obstacles or cover. Payload has <b>a</b> <b>camera</b> <b>head</b> equipped with multiple cameras, laser pointers, audio and other sensors.|$|E
5000|$|The {{earliest}} {{video cameras}} were mechanical flying-spot scanners {{which were in}} use in the 1920s and 1930s {{during the period of}} mechanical television. Improvements in video camera tubes in the 1930s ushered in the era of electronic television. Earlier, cameras were very large devices, almost always in two sections. The camera section held the lens and tube pre-amplifiers and other necessary electronics, and was connected to a large diameter multicore cable to the remainder of the camera electronics, usually mounted in a separate room in the studio, or a remote truck. The camera head could not generate a video picture signal on its own. The video signal was output to the studio for switching and transmission. By the fifties, electronic miniaturization had progressed to the point where some monochrome cameras could operate stand alone and even be handheld. But the studio configuration remained, with the large cable bundle transmitting the signals back to the camera control unit (CCU). The CCU in turn was used to align and operate the camera's functions, such as exposure, system timing, video and black levels.The first color cameras (1950s in the US, early 1960s in Europe), notably the RCA TK-40/41 series, were much more complex with their three (and in some models four) pickup tubes, and their size and weight drastically increased. Handheld color cameras did not come into general use until the early 1970s - the first generation of cameras were split into <b>a</b> <b>camera</b> <b>head</b> unit (the body of the camera, containing the lens and pickup tubes, and held on the shoulder or a body brace in front of the operator) connected via a cable bundle to a backpack CCU. The Ikegami HL-33, the RCA TK45 and the Thomson Microcam were portable two piece color cameras introduced in the early 1970s. For field work a separate VTR was still required to record the camera's video output. Typically this was either a portable 1" [...] reel to reel VTR, or a portable 3/4" [...] U-matic VCR. Typically, the two camera units would be carried by the camera operator, while a tape operator would carry the portable recorder. With the introduction of the RCA TK76 in 1976, camera operators were finally able to carry on their shoulders a one piece camera containing all the electronics to output a broadcast quality composite video signal. A separate videotape recording unit was still required.|$|E
40|$|This paper {{describes}} about spherical immersive display using convex mirror. Our display provides seamless {{image that}} totally surrounds a viewer. In {{order to construct}} the system, we designed optical configuration of display through the virtual projector that simulates behavior of projected light. The prototype display based on the optical design provides 360 degree horizontal and 115 degree vertical image. This optical configuration has an advantage in displaying video image. Then we also developed <b>a</b> <b>camera</b> <b>head</b> with convex mirror, which corrects distortion of displayed image...|$|E
40|$|This paper {{describes}} {{a method for}} calibrating <b>a</b> pan-tilt <b>camera</b> <b>head.</b> The key feature of the method is that the camera, the pan-tilt unit, {{and the relationship between}} the camera and the pan-tilt unit, are calibrated independently. This paper develops a geometric model of the pan-tilt <b>camera</b> <b>head</b> and presents procedures to estimate the values of the unknown model parameters. Target image repositioning is discussed as an application of using <b>a</b> calibrated pan-tilt <b>camera</b> <b>head...</b>|$|R
5000|$|... #Caption: Sony Betacam SP BVV-5 VTR, docked to <b>a</b> JVC KY-D29 <b>camera</b> <b>head.</b>|$|R
40|$|Abstract: This paper {{addresses}} {{the problem of}} initialization of a vision-based human tracking system. Most active binocular systems built for tracking require manual initialization. Our approach uses color skin-tone segmentation to initialize monochrome binocular tracking. <b>A</b> trinocular <b>camera</b> <b>head</b> designed to simplify the integration is used. We demonstrate {{the results of our}} color segmentation algorithms...|$|R
40|$|The NEAT (Near Earth Asteroid Tracking) {{camera system}} {{consists}} of <b>a</b> <b>camera</b> <b>head</b> with a 6. 3 cm square 4096 x 4096 pixel CCD, fast electronics, and a Sun Sparc 20 data and control computer with dual CPUs, 256 Mbytes of memory, and 36 Gbytes of hard disk. The system {{was designed for}} optimum use with an Air Force GEODSS (Ground-based Electro-Optical Deep Space Surveillance) telescope. The GEODSS telescopes have 1 m f/ 2. 15 objectives of the Ritchey-Chretian type, designed originally for satellite tracking. Installation of NEAT began July 25 at the Air Force Facility on Haleakala, a 3000 m peak on Maui in Hawaii...|$|E
30|$|The {{slave system}} is {{composed}} of two 7 -DOF manipulators with a serial link mechanism and <b>a</b> <b>camera</b> <b>head</b> system for monitoring the working environment (see Fig.   3). The camera head can pan and tilt to see the workspace. A wrist camera, a 6 -axis force sensor and a BarrettHand™ (Barrett Technology, LLC) for grasping the various payloads are mounted on each end effector. The 6 -axis force sensor acquires the force generated at the wrist of the slave arm. The BarrettHand is able to open and close fingers to grip a payload. The end effector camera is used to obtain detailed information with a different field of view angle of the camera head.|$|E
40|$|Imagination {{or mental}} imagery is the {{capability}} of generating sensory experiences without actual sensory inflow. These sen-sory experiences might be the results of actions that are not carried out physically but only simulated within the cognitive system of an agent. We propose a model architecture for men-tal imagery that enables an artificial agent to generate views of parts of himself based {{on a set of}} given pose parameters and according to a specific viewing direction. The architecture is evaluated in a robotic study with an agent consisting of a robotic manipulator and <b>a</b> <b>camera</b> <b>head.</b> The agent adaptively learns to associate views of its gripper with its current pose. Furthermore, a visual forward model predicts visual changes according to the agent’s gaze direction...|$|E
5000|$|Chadwell OConnor (October 9, 1914 [...] - [...] September 5, 2007) was an American {{inventor}} {{and steam}} engine enthusiast. He is most {{remembered as the}} inventor of <b>an</b> improved fluid-damped <b>camera</b> <b>head,</b> for which he won Academy Awards in 1975 and 1992.|$|R
40|$|This paper details {{experiments}} in which neural network controllers were evolved in simulation that allowed <b>a</b> simple panning <b>camera</b> <b>head</b> to track patterned objects moving against similarly patterned backgrounds in reality. It {{begins with a}} discussion of minimal simulations: fast-running, easy-to-build simulations for the evolution of real robot controllers (Jakobi, 1998 a; Jakobi, 1998 b). The minimal simulation with which the motion-tracking controllers were evolved (along {{with the rest of the}} evolutionary machinery) is then described. Experimental results are offered in which an evolved controller was downloaded onto <b>a</b> simple panning <b>camera</b> <b>head</b> and satisfactorily tracked two very differently patterned objects as they moved against similarly patterned backgrounds in a random fashion. 1. Introduction Various different types of simulation have been used in the past to evolve controllers for robots: in (Jakobi et al., 1995), an empirically verified model of the underlying physics was co [...] ...|$|R
40|$|When visual {{behaviors}} are combined {{to provide a}} speci c functionality needed for a task, the combination is often based on heuristic rules. In this paper we showthat by adopting the Discrete Event Systems (DES) formalism for describing the interaction between visual behaviors {{it is possible to}} provide systems which have well de ned properties in terms of observability and controllability. The method is in particular suited for describing the coupling between action and perception. An introduction to the use of DES is provided and it is demonstrated how DES are used for modeling behaviors and controlling a mobile robot equipped with <b>a</b> binocular <b>camera</b> <b>head</b> and some additional sensors...|$|R
40|$|A slow-scan {{television}} camera called the solid-state imaging subsystem (SSI), {{built for the}} Galileo Jupiter Orbiter, is described. The SSI consists of a 1500 -mm focal-length telescope coupled to <b>a</b> <b>camera</b> <b>head</b> housing a 800 x 800 -element charge-coupled device (CCD) detector based on 'virtual-phase' charge transfer technology. The CCD detector provides broadband sensitivity over 100 times that of a comparable vidicon-tube camera, while also yielding improved resolution, linearity, geometric fidelity, and spectral range. The system noise floor is 30 electrons, which results in a dynamic range of about 3500. Saturation of the detector with 9000 -A light, followed by a high-speed erasure cycle prior to exposing each image, stabilizes the detector quantum efficiency at its maximum level for wavelengths beyond 7000 A. An optical schematic diagram of the SSI is included...|$|E
3000|$|IDP Express {{includes}} <b>a</b> <b>camera</b> <b>head</b> and an FPGA {{image processing}} board (IDP Express board). The camera head has a 512 × 512 pixel CMOS image sensor, the sensor and pixel size {{of which are}} 5.12 × 5.12 mm and 10 × 10 [...] m, respectively. The camera head was mounted on the camera port of the CCTV zoom lens. The IDP Express board was designed for high-speed video processing and recording, and we could implement image processing algorithms by hardware logic on the FPGA (Xilinx XC 3 S 5000); it was mounted using a PCI-e 2.0 × 16 bus I/F on the PC. The 8 -bit grayscale 512 × 512 images and processed results could be simultaneously transferred at 2000 fps to the allocated memory in the PC.|$|E
40|$|An {{absolute}} optical linear or {{rotary encoder}} which encodes {{the motion of}} an object (3) with increased resolution and encoding range and decreased sensitivity to damage to the scale includes a scale (5), which moves with the object and is illuminated by a light source (11). The scale carries a pattern (9) which is imaged by a microscope optical system (13) on a CCD array (17) in <b>a</b> <b>camera</b> <b>head</b> (15). The pattern includes both fiducial markings (31) which are identical for each period of the pattern and code areas (33) which include binary codings of numbers identifying the individual periods of the pattern. The image of the pattern formed on the CCD array is analyzed by an image processor (23) to locate the fiducial marking, decode the information encoded in the code area, and thereby determine {{the position of the}} object...|$|E
40|$|Good {{observation}} of a manipulation presentation {{performed by a}} human teacher is crucial to further processing steps in Programing by demonstration (Pbd) which has reached a prime of importance in interactive robot programing. This paper outlines a sensor fusion concept for hand action tracking observing hand posture, position and applied forces. As input sources serve {{on the one hand}} a data glove which will classify several gestures and grasps, <b>a</b> stereo <b>camera</b> <b>head</b> and several force sensors mounted on the finger tips. The hardware used at our institute is presented as well as first implementations of measurement and fusion approaches. Accuracies of first experiments are also given...|$|R
40|$|Background {{subtraction}} is {{a simple}} and effective method to detect anomalous regions in images. In spite of the effectiveness, it cannot be used with <b>an</b> active (moving) <b>camera</b> <b>head</b> because the background image varies with camera-parameter control. This paper presents a background subtraction method with pan-tilt-zoom control. The proposed method consists of an omnidirectional background model called appearance sphere and parallax free sensing. Based on this model, precise background images can be generated and background subtraction can be performed for any combination of pan-tilt-zoom parameters without restoring 3 D scene information</p...|$|R
40|$|The office {{delivery}} system is organized as a hybrid architecture, using simple reactive behaviors for safe navigation, being {{guided by a}} high-level strategic executive, running concurrently with the reactive subsystem, and performing goal-directed task-achieving planning. The implemented system is capable of autonomously navigating, with no human intervention, from one room to another designated by the operator. This involves: navigation in a room, where the robot dynamically detects and avoids obstacles; and door traversing, where the robot autonomously finds the door and passes safely through it. The only a priori knowledge used by the system consists of a graph representation of the navigation environment, containing door positions; and a floor map containing the static structures, specifically walls. A priori, no knowledge is provided about the position, physical or geometrical properties of other objects in the environment. The hardware platform of the implemented system consists of: i) the Robuter, a wheeled mobile robot with ultrasonic sonars; ii) the Aalborg <b>Camera</b> <b>Head,</b> <b>a</b> stereo <b>camera</b> <b>head</b> for stereo vision; and iii) a computer network for processing, all available at LIA...|$|R
40|$|Endoscopes {{consisting}} of a probe mounted with <b>a</b> <b>camera</b> <b>head,</b> are frequently used in medicine for inspection and vi-sualization of human body cavities (knee and shoulder articu-lations, bronchi, nose, brain, etc). However the images often suffer from strong lens distortions. Estimation and correction of image distortion is important not only to improve the prac-titioner’s perception of the inspected area but also to develop systems for 3 D navigation and computer assisted surgery. In this paper we compare various conventional calibration methods against the new parameter free method proposed by Hartley and Kang. We believe the non-parametric method is more suitable for endoscopic imaging. We present nu-merical analysis {{of the goodness of}} fit of other conventional approaches as well as calibration results on real images. It should be noted that our results are directly applicable to all vision applications using wide-angle lenses. Index Terms — Calibration, lens distortion, radial distor-tions, non-perspective, endoscope. 1...|$|E
40|$|This paper {{describes}} the advances to the Indigo Systems family of Commercial-off-theshelf (COTS) hardware and software. Indigo Phoenix ™ camera family is being advanced {{to meet the}} demanding needs of advanced range & phenomenology applications by including things like IRIG-B time stamping, fiber optic communications, environmental enclosures, and the new RTools ™ radiometric software suite. RTools ™ is a highly sophisticated software package developed for engineers and scientists to acquire, radiometrically calibrate, process, and analyze data from digital infrared camera systems. The RTools ™ toolkit is comprised of several stand-alone modules named RDac ™ for camera acquisition, RCal ™ for IR camera calibration, REdit ™ for file archival and maintenance, and RView ™ for data review and analysis. Created for flexible and extensible use in data archiving RTools ™ utilizes the Air Force’s Standard Archive Format (SAF). The complete camera system consists of <b>a</b> <b>camera</b> <b>head</b> and back-end electronics. The camera head supports 640 x 512 and 320 x 256 formats, {{a wide selection of}} ROIC’s...|$|E
40|$|The {{interest}} in camera heads has been ever increasing {{over the past}} years. This {{interest in}} agile sensor systems seems promising for guiding the vision research in new useful directions. At Aalborg University, we have implemented the second and much improved version of <b>a</b> <b>camera</b> <b>head,</b> which specifically has lead to research in how to control the gaze mechanism. We propose a combination of multiple visual cues for controlling the camera head, enabling it to cope with less restricted scenarios than otherwise possible. By integrating accommodation and disparity cues {{we are able to}} control the vergence of the camera head separate from the version mechanism, enabling reliable smooth pursuit and gaze shift. 1 Introduction Since the late sixties much research have been concentrated on extraction of robust and accurate 3 D information. Despite the intensive efforts put into extraction of 3 D information from visual input, no one has so far been {{able to come up with}} an algorithm, acquiring the r [...] ...|$|E
40|$|Abstract. We {{present a}} method for {{estimating}} the distance between <b>a</b> <b>camera</b> and <b>a</b> human <b>head</b> in 2 D images from <b>a</b> calibrated <b>camera.</b> Leading <b>head</b> pose estimation algorithms focus mainly on head orienta-tion (yaw, pitch, and roll) and translations perpendicular to the cam-era principal axis. Our contribution {{is a system that}} can estimate head pose under large translations parallel to the camera’s principal axis. Our method uses a set of exemplar 3 D human heads to estimate the dis-tance between <b>a</b> <b>camera</b> and <b>a</b> previously unseen head. The distance is estimated by solving for the camera pose using Effective Perspective n-Point (EPnP). We present promising experimental results using the Texas 3 D Face Recognition Database. ...|$|R
50|$|In photography, {{a tripod}} {{is used to}} {{stabilize}} and elevate <b>a</b> <b>camera,</b> <b>a</b> flash unit, or other photographic equipment. All photographic tripods have three legs and a mounting head to couple with <b>a</b> <b>camera.</b> The mounting <b>head</b> usually includes a thumbscrew that mates to a female threaded receptacle on the camera, {{as well as a}} mechanism to be able to rotate and tilt the camera when it is mounted on the tripod. Tripod legs are usually made to telescope, in order to save space when not in use. Tripods are usually made from aluminum, carbon fiber, steel, wood or plastic.|$|R
40|$|A {{major goal}} for the {{realization}} {{of a new generation}} of intelligent robots is the capability of instructing work tasks by interactive demonstration. To make such a process efficient and convenient for the human user requires that both the robot and the user can establish and maintain a common focus of attention. We describe a hybrid architecture that combines neural networks and finite state machines into a flexible framework for controlling the behaviour of a vision based robot called GRAVIS-robot (Gestural Recognition Active VIsion System robot). It consists of <b>a</b> binocular <b>camera</b> <b>head,</b> <b>a</b> 6 DOF robot arm and a 9 DOF multifingered hand. We focus primarily on non-verbal communication based on gestural commands of a human instructor which will at a later stage be complemented by spoken instructions. ...|$|R
40|$|A new {{detector}} {{system with}} high time resolution (1 ms) {{has been developed}} and applied for the continuous measurement of spectra in the vacuum ultraviolet (VUV) and extreme ultraviolet (EUV) wavelength region at the fusion plasma experiment Torus Experiment for Technology-Oriented Research (TEXTOR). The system consists of an open multichannel-plate (MCP) detector with subsequent first generation (Gen I) light amplifier and <b>a</b> <b>camera</b> <b>head</b> {{which is based on}} a linear photodiode array with 1024 elements (pixels). The camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a PC-based data acquisition system. Three vacuum spectrometers operating in the VUV/EUV region (10 - 130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at TEXTOR has been performed. Spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolution corresponding to a width of 3 - 4 pixels. (C) 2004 American Institute of Physics...|$|E
40|$|In {{order to}} obtain an {{objective}} and permanent record of motility of human spermatozoa, a spermatozoon was followed under a microscope and its movement was traced on paper. Instruments for recording spermatozoal movement consisted of a phase-contrast microscope, <b>a</b> <b>camera</b> <b>head,</b> a camera control unit, an X-Y tracker, a TV monitor and an X-Y recorder. These are assembled as illustrated in Fig's. 1 and 2. The X-Y tracker, {{one of the most}} important instruments in this system, was devised for use in aeronautics or architecture, such as to monitor the orbit of a rocket, to measure the sway of skyscrapers to predict when it might crumble down during an earthquake and to measure the sway of a big bridge to determine whether it would hold during a storm. In the spermatozoal trackogram the spermatozoon progressed in a roughly straight line in a sine curve, and at times in a rectangular wave (Fig. 2 to 5). The speed of spermatozoa movement in the X and Y directions can be immediately and automatically calculated if a computer is connected to this system. The system will be useful for recording the movement of a single free cell...|$|E
40|$|A novel way for {{capturing}} native 1080 P, 1080 I and 720 P at 16 : 9 {{aspect ratio}} and a CineScope aspect ratio in 1080 P of 2. 37 : 1 will be presented. The {{architecture of the}} multi-format HDTV camera head {{is based on a}} newly designed Frame Transfer CCD-imager. It enables the design of <b>a</b> <b>camera</b> <b>head</b> that has multiple formats available at the HD-SDI (SMPTE 292 M) output of the camera head itself. The camera head utilizes three Frame Transfer (2 / 3 inch) CCDs with 9. 2 Mpixels each (including overscan). Through the use of a 12 -phase system all the vertical resolutions of the SMPTE 274 M and SMPTE 296 M standards are possible. Under software control (Dynamic Pixel Management ™) these pixels are pre-arranged, at the CCD, into image cells. The image diagonal (11 mm) is independent of spatial resolution and therefore the same for all scanning formats. The video processing chain consists of three 12 -bit ADCs followed by two newly designed ASICs that allow multi-format video processing. Amongst many other outputs, the ASICs have an SMPTE 292 M-compatible 20 -bit parallel output can be serialized to obtain a HD-SDI output at the camera head. The above video chain offers full digital processing capability. A dockable concept is exploited in full to enable: • A multi-purpose adapter for “stand-alone ” operation with battery power input, genlock input and HD-SDI output (1. 5 Gb/s) • A wide-band analog Triax adapter (Y = 30 MHz, Cr, Cb = 15 MHz) for transmission up to 3000 ft. of cable. • Future adapters to offer the possibility of optical fiber transmission with longer cable lengths or a (DVCPRO HD) camcorde...|$|E
40|$|Recent {{research}} projects {{have demonstrated that}} it is possible to make robots move in formation. The approaches dier by the various assumptions about what can be perceived and communicated by the robots, the strategies used to make the robots move in formation, the ability to deal with obstacle and to switch formations. After suggesting criteria to char-acterize problems associated with robot formations, this paper presents a distributed approach based on directional visual perception and inter-robot commu-nication. Using <b>a</b> pan <b>camera</b> <b>head,</b> sonar readings and wireless communication, we demonstrate that robots are not only able to move in formation, avoid obstacles and switch formations, but also initialize and determine by themselves their positions in the formation. Validation of our work is done in simu-lation and with Pioneer 2 robots. ...|$|R
40|$|In {{this article}} we {{describe}} a calibration procedure of <b>a</b> binocular <b>camera</b> <b>head</b> with two independent pan and tilt axes. For the field of computer vision the main benefit of the proposed method is the easy computation of the epipolar line in the second <b>camera</b> when selecting <b>a</b> pixel coordinate in the first {{with respect to the}} current commanded position of the two movable cameras. Photogrammetrists may be interested in the proposed unified calibration idea. For the calibration procedure itself no separate rotations of the respective axes and no fixation is required. To get reliable calibration data just a plane surface with known 2 D coordinates of distinguishable target points is needed. key words: active computer vision, calibration, epipolar geometry 1 Mr. Spiess is supported by the Austrian Science Foundation FWF under grant 11420 -OEMA. Spiess & Li: Kinematic Calibration and Online Computation of Epipolar Geometry 1 1 Introduction <b>A</b> binocular <b>camera</b> system usually comprises a [...] ...|$|R
40|$|We {{present a}} robotic system capable of {{manipulating}} arbitrary objects using visual and tactile object representations learned in advance. Objects are learned from appearance {{by using a}} collection of example images and by gauging their surfaces, respectively. The whole system consists of <b>a</b> stereo <b>camera</b> <b>head</b> and <b>a</b> robot arm with seven degrees of freedom with a two-plate gripper equipped with tactile sensors of novel type. The visual representation is set up using an approach based on information theory resulting in a linear discriminant. The tactile representation is derived from a geometric model based on three dimensional α-shapes. The underlying cloud of points is collected using an array of tactile sensors highly sensitive to dynamic force changes which is mounted to the robot’s gripper. Experiments {{have shown that the}} system can deal with all objects manageable with the two-plate gripper...|$|R
40|$|The recent {{availability}} of large format CCD's with high quantum efficiency {{makes it possible}} to achieve significant advances in high dispersion astronomical spectroscopy. An echelle CCD combination excels or equals other techniques presently available, and offers the advantage of complete spectral coverage of several thousand Angstroms in a single exposure. Attention is given to experiments which were conducted with <b>a</b> CCD <b>camera</b> <b>head</b> and <b>an</b> echelle spectrograph on a 4 -meter telescope. It was found possible to achieve a signal-to-noise ratio of 150 / 1 on a 13 th magnitude star at 6000 A in a two-hour exposure at 0. 16 A/pixel, limited primarily by photon statistics. For fainter objects, readout noise is the limiting factor in precision. For 20 electron rms readout noise, an S/N = 15 / 1 at 18 th magnitude is expected, all other things being equal...|$|R
50|$|The eye {{movement}} of {{two groups of}} drivers have been filmed with <b>a</b> special <b>head</b> <b>camera</b> by <b>a</b> team of the Swiss Federal Institute of Technology: Novice and experienced drivers had their eye-movement recorded while approaching a bend of a narrow road.The series of images has been condensed from the original film frames to show 2 eye fixations per image for better comprehension.|$|R
50|$|Arizona State {{psychology}} professor Michael McBeath {{has proposed a}} simple model to explain how dogs play Fetch. By mounting <b>a</b> <b>camera</b> on the <b>head</b> of a dog, {{he found that the}} dog changed its speed and direction {{in order to keep the}} frisbee's image in a constant position on its retina. This approach, called the Linear Optical Trajectory, makes the frisbee appear to move in a linear path at a constant speed. McBeath had previously noticed this interception strategy in professional baseball players pursuing fly balls.|$|R
40|$|This {{dissertation}} {{presents an}} architecture for controlling <b>an</b> agile <b>camera</b> <b>head</b> with multiple degrees of freedom. The {{purpose of the}} architecture is to provide control of the different degrees of freedom {{in the head by}} applying it in a fixation based scheme. This includes control of the optical degrees of freedom, an often neglected property in most current head control architectures. The basic thesis is that a tight connection between the lowest visual processes and the sensing apparatus is imperative for successful operation in dynamic environments. This has lead to the introduction of a basic system which facilitates three low level processes, which are; fixation, tracking and attention selection and shifting, which can provide fully data driven responses. The basic system carries the possibility of servicing requests from higher level visual processes, whereby model driven responses in a hypothesis verification/rejection scheme may be implemented. The interaction between the basic [...] ...|$|R
40|$|In {{this paper}} {{we present a}} robust method to recover 3 D {{position}} and orientation (pose) of a moving head using <b>a</b> single stationary <b>camera.</b> <b>Head</b> pose is recovered via <b>a</b> <b>camera</b> pose estimation formulation. 3 D feature points (artificial or natural occurring) are acquired from the head prior to tracking and used as a model. Pose is estimated by solving a robust version of ”Perspective n Point ” problem. The proposed algorithm can handle self occlusions, outliers and recover from tracking failures. Results were validated by simultaneous tracking using our system and an accurate magnetic field 3 D measuring device. Our contribution {{is a system that}} is not restricted to track only human heads, and is accurate enough {{to be used as a}} measuring device. To demonstrate the applicability of our method, three types of heads (human, barn owl, chameleon) were tracked in a series of biological experiments. 1...|$|R
40|$|This paper {{introduces}} a machine vision system, which {{is suitable for}} cooperative works between the human and computer. This system provides images inputted from <b>a</b> stereo <b>camera</b> <b>head</b> {{not only to the}} processor but also to the user’s sight as binocular wide-angle foveated (WAF) information, thus it is applicable for Virtual Reality (VR) systems such as tele-existence or training experts. The stereo <b>camera</b> <b>head</b> plays <b>a</b> role to get required input images foveated by special wide-angle optics under camera view direction control and 3 D head mount display (HMD) displays fused 3 D images to the user. Moreover, an analog video signal processing device much inspired from a structure of the human visual system realizes a unique way to provide WAF information to plural processors and the user. Therefore, this developed vision system is also much expected to be applicable for the human brain and vision research, because the design concept is to mimic the human visual system. Further, an algorithm to generate features using Discrete Fourier Transform (DFT) for binocular fixation in order to provide well-fused 3 D images to 3 D HMD is proposed. This paper examines influences of applying this algorithm to space variant images such as WAF images, based on experimental results...|$|R
