71|7851|Public
40|$|In this paper, we {{investigate}} a sequential test for binary hypothesis testing for stationary, first-order Markov dependent observations in steady state. Wald's {{first and second}} lemmas are generalized. For a Markov chain with symmetric transition probability matrix the <b>average</b> <b>sample</b> <b>number</b> required by the test to decide a hypothesis is derived. Numerical analysis shows that accounting for a positive correlation in the observations results in {{a significant decrease in}} the <b>average</b> <b>sample</b> <b>number</b> for fixed error probabilities...|$|E
40|$|In this paper, {{we present}} the {{designing}} of the skip-lot sampling plan including the re-inspection called SkSP-R. The plan {{parameters of the}} pro-posed plan are determined through a nonlinear optimization problem by minimizing the <b>average</b> <b>sample</b> <b>number</b> satisfying both the producer’s risk and the consumer’s risks. The proposed plan is shown to perform better than the existing sampling plans {{in terms of the}} <b>average</b> <b>sample</b> <b>number.</b> The application of the proposed plan is explained with the help of illustra-tive examples...|$|E
40|$|A double {{sampling}} plan based on truncated life tests is proposed and designed under a general life distribution. The design parameters such as sample sizes and acceptance {{numbers for the}} first and the second samples are determined so as to minimize the <b>average</b> <b>sample</b> <b>number</b> subject to satisfying the consumer's and producer's risks at the respectively specified quality levels. The resultant tables can be used regardless of the underlying distribution as long as the reliability requirements are specified at two risks. In addition, Gamma and Weibull distributions are particularly considered to report the design parameters according to the quality levels in terms of the mean ratios. acceptance probability, <b>average</b> <b>sample</b> <b>number,</b> consumer's risk, life distribution, life test, producer's risk,...|$|E
40|$|In this paper, we have {{established}} a new framework of multistage hypothesis tests. Within the new framework, we have developed specific multistage tests which guarantee prescribed level of power and are more efficient than previous tests in terms of <b>average</b> <b>sampling</b> <b>number</b> and the <b>number</b> of <b>sampling</b> operations. Without truncation, the maximum <b>sampling</b> <b>number...</b>|$|R
40|$|In this paper, we have {{developed}} new multistage tests which guarantee prescribed level of power and are more efficient than previous tests in terms of <b>average</b> <b>sampling</b> <b>number</b> and the <b>number</b> of <b>sampling</b> operations. Without truncation, the maximum <b>sampling</b> <b>numbers</b> of our testing plans are absolutely bounded. Based on geometrical arguments, we have derived extremely tight bounds for the operating characteristic function. ...|$|R
40|$|Methodology is {{proposed}} {{for the design of}} sequential methods when data are obtained by gauging articles into groups. Exact expressions are obtained for the operating characteristics and <b>average</b> <b>sampling</b> <b>number</b> of Wald Sequential Probability Ratio Tests (SPRTs), and for the average run length of Cumulative Sum (CUSUM) schemes based on grouped data. Step by ste...|$|R
40|$|Sequential {{probability}} ratio test {{is developed}} for testing the hypothesis regarding the parameter of Pareto distribution. The expression for the operating characteristics (OC) and <b>average</b> <b>sample</b> <b>number</b> (ASN) functions are derived. For {{the purpose of}} the plotting the OC and ASN functions different approaches are used. These different approaches give quite satisfactorily results...|$|E
40|$|This article {{proposes a}} {{variables}} two-plan sampling system called tightened-normal-tightened (TNT) sampling inspection scheme where the quality characteristic follows a normal distribution or a lognormal distribution {{and has an}} upper or a lower specification limit. The TNT variables sampling inspection scheme will be useful when testing is costly and destructive. The advantages of the variables TNT scheme over variables single and double sampling plans and attributes TNT scheme are discussed. Tables are also constructed for the selection of parameters of known and unknown standard deviation variables TNT schemes for a given acceptable quality level (AQL) and limiting quality level (LQL). The problem is formulated as a nonlinear programming where the objective function to be minimized is the <b>average</b> <b>sample</b> <b>number</b> and the constraints are related to lot acceptance probabilities at AQL and LQL under the operating characteristic curve. <b>average</b> <b>sample</b> <b>number,</b> OC curve, sampling system, two-plan system,...|$|E
40|$|Let [lambda] > 0 be {{the unknown}} {{parameter}} {{of the common}} exponential distribution of the i. i. d. sequence X 1, X 2 [...] . Exact formulae of a simple from for the error probabilities and {{the average number of}} the SPRT of against H 1 : [lambda] [greater-or-equal, slanted] [lambda] 0 ([lambda] 0 exponential distribution sequential probability ratio test error probabilities <b>average</b> <b>sample</b> <b>number...</b>|$|E
40|$|In this paper, we have {{established}} a new framework of multistage parametric estimation. Specially, we have developed sampling schemes for estimating parameters of common important distributions. Without any information of the unknown parameters, our sampling schemes rigorously guarantee prescribed levels of precision and confidence, while achieving unprecedented efficiency {{in the sense that}} the <b>average</b> <b>sample</b> <b>numbers</b> are virtually the sam...|$|R
40|$|A {{generalized}} modified {{method is}} proposed {{to control the}} sum of error probabilities in sequential probability ratio test to minimize the weighted average of the two <b>average</b> <b>sample</b> <b>numbers</b> under a simple null hypothesis and a simple alternative hypothesis with the restriction that the sum of error probabilities is a pre-assigned constant to find the optimal sample size and finally a comparison is done with the optimal sample size found from fixed sample size procedure. The results are applied to the cases when the random variate follows a normal law as well as Bernoullian law...|$|R
40|$|A random walk, {Sn}∞n = 0, having {{positive}} drift {{and starting}} at the origin, is stopped the first time Sn > t ≧ 0. The present paper studies the "excess," Sn - t, when the walk is stopped. The main result is an upper bound on {{the mean of the}} excess, uniform in t. Through Wald's equation, this gives an upper bound on the mean stopping time, as well as upper bounds on the <b>average</b> <b>sample</b> <b>numbers</b> of sequential probability ratio tests. The same elementary approach yields simple upper bounds on the moments and tail probabilities of residual and spent waiting times of renewal processes...|$|R
40|$|NOMBAS is {{an acronym}} for NOrmal Myopic Bayes Sequential, and {{is the name of}} a Bayesian {{procedure}} for selecting the category with the greatest mean. This paper describes NOMBAS in detail and then compares it with other procedures on the basis of Bayes risk versus <b>average</b> <b>sample</b> <b>number.</b> This research was supported by the Office of Naval Research through the NPS Foundation Research Program. [URL]...|$|E
40|$|The mixed variables-attributes test {{plans for}} single {{acceptance}} sampling are proposed to protect “good lots” from attributes aspect and to optimize sample sizes from variables aspect. For the single and double mixed plans, exact formulas {{of the operating}} characteristic and <b>average</b> <b>sample</b> <b>number</b> are developed for the exponential distribution. Numerical illustrations show that the mixed sampling plans have some advantages over the variables plans or attributes plans alone...|$|E
40|$|As {{a helpful}} guide for applications, the {{alternative}} hypotheses of the three-hypothesis test problems are designed under the required error probabilities and <b>average</b> <b>sample</b> <b>number</b> in this paper. The asymptotic formulas {{and the proposed}} numerical quadrature formulas are adopted, respectively, to obtain the hypothesis designs and the corresponding sequential test schemes under the Koopman-Darmois distributions. The example of the normal mean test shows that our methods are quite efficient and satisfactory for practical uses...|$|E
40|$|Methodology is {{presented}} {{for the design}} of single and double compressed limit Sequential Probability Ratio Tests (SPRT) and Cumulative Sum (CUSUM) control charts to detect one-sided mean shifts in a symmetric probability distribution. We also show how to evaluate the average run length properties with the Fast Initial Response (FIR) feature. The resulting CUSUM plans have a simple scoring procedure, and are extremely simple to derive and implement. The use of two compressed limit gauges is more efficient than a single compressed limit gauge. In the case of SPRTs, the use of two compressed limit gauges minimizes the <b>average</b> <b>sampling</b> <b>number</b> required for specified operating characteristics. In th...|$|R
40|$|In this paper, a {{variable}} multiple dependent state (MDS) sampling plan is developed {{based on the}} process capability index using Bayesian approach. The optimal parameters of the developed sampling plan with respect to constraints related {{to the risk of}} consumer and producer are presented. Two comparison studies have been done. First, the methods of double sampling model, sampling plan for resubmitted lots and repetitive group sampling (RGS) plan are elaborated and <b>average</b> <b>sample</b> <b>numbers</b> of the developed MDS plan and other classical methods are compared. A comparison study between the developed MDS plan based on Bayesian approach and the exact probability distribution is carried out...|$|R
40|$|A further {{generalization}} {{of the family}} of 'two-stage' chain sampling inspection plans is developed - viz, the use of different sample sizes in the two stages. Evaluation of the operating characteristics is accomplished by the Markov chain approach of the earlier work, modified to account for the different sample sizes. Markov chains for a number of plans are illustrated and several algebraic solutions are developed. Since these plans involve a variable amount of sampling, an evaluation of the <b>average</b> <b>sampling</b> <b>number</b> (ASN) is developed. A number of OC curves and ASN curves are presented. Some comparisons with plans having only one sample size are presented and indicate that improved discrimination is achieved by the two-sample-size plans...|$|R
40|$|Sequential testing {{procedures}} are {{developed for the}} parameters of generalized life distributions. Robustness of the {{testing procedures}} is studied, when the distribution under consideration has undergone a change. In order to apply Newton-Raphson method {{for the purpose of}} plotting the operating characteristic (OC) and <b>average</b> <b>sample</b> <b>number</b> (ASN) functions, a method of choosing the initial values is provided. Key words: Generalized life distributions; OC and ASN functions; robustness; sequential probability ratio tests. ...|$|E
40|$|A {{test was}} {{described}} for two systems, long term and short term with an exponentially distributed time between failures. The test {{is intended for}} checking the ratio MTBFl /MTBFs exceeds or equals a prescribed value, versus one that it {{is less than the}} prescribed value, by means of long term tests with large <b>average</b> <b>sample</b> <b>number</b> in the earlier system. Our proposed system focus on improving test by using low <b>average</b> <b>sample</b> <b>number</b> in short term which is having the advantage of economy in time requirement and cost. It produces optimum truncated test called binomial Sequential Probability Ratio Test. Criteria are proposed for determining the characteristics of truncated test followed with the discretizing effect of truncation on error probabilities with a view to optimization of its parameters. The search algorithm for truncation apex used in this system achieves closeness to the optimum which depends on successful choice of the initial approximation, search boundaries and on the search step. The enhanced reliability of modern technological systems, combined with the reduced time quotas allotted for creating new system is capable of yielding a highly efficacious test which increases reliability and feasibility of decisions...|$|E
40|$|Acceptance {{sampling}} plans {{are used to}} assess the quality of an ongoing production process, in addition to the lot acceptance. In this paper, we consider sampling inspection plans for monitoring the Markov-dependent production process. We construct sequential plans that satisfy the usual probability requirements at acceptable quality level and rejectable quality level and. in addition, possess the minimum <b>average</b> <b>sample</b> <b>number</b> under semicurtailed inspection. As these plans result in large sample sizes, especially when the serial correlation is high, we suggest new plans called "systematic {{sampling plans}}. " The minimum <b>average</b> <b>sample</b> <b>number</b> systematic plans that satisfy the probability requirements are constructed. Our algorithm uses some simple recurrence relations to compute the required acceptance probabilities. The optimal systematic plans require much smaller sample sizes and acceptance numbers, compared to the sequential plans. However, they need larger production runs to make a decision. Tables for choosing appropriate sequential and systematic plans are provided. The problem of selecting the best systematic sampling plan is also addressed. The operating characteristic curves of some of the sequential and the systematic plans are compared, and are observed to be almost identical. (C) 2001, Inc...|$|E
40|$|Let X 1,X 2, [...] . be {{independent}} random variables, and set Wn = max(0,Wn- 1 + Xn), W 0 = 0, n [greater-or-equal, slanted] 1. The so-called cusum (cumulative sum) procedure uses the {{first passage time}} T(h) = inf{n [greater-or-equal, slanted] 1 : Wn[greater-or-equal, slanted]h}for detecting changes in the mean [mu] of the process. It is shown that limh [...] >[infinity] [mu]ET(h) /h = 1 if [mu] > 0. Also, a cusum procedure for detecting changes in the normal mean is derived when the variance is unknown. An asymptotic approximation to the average run length is given. Detection normal mean cusum procedure likelihood ratio average run lenth (ARL) asymptotic <b>average</b> <b>sampler</b> <b>number...</b>|$|R
40|$|The paper {{deals with}} active change {{detection}} problem in linear stochastic discrete time systems. In contrast to standard change detection problem [1], {{the aim is}} to design an active detector, which generates an auxiliary input signal exciting the observed system and provides decisions about changes in this system. The idea of the auxiliary input signal has come from identification experiment design [2], where the properly designed input signal can lead to better estimates of the system parameters. Known active change detection methods are based entirely on using multiple linear models for description of the observed system. The stochastic multiple linear models and the sequential probability ratio test (SPRT) are considered in [3, 4]. Firstly, only two models are considered and the SPRT is used to detect a change. The auxiliary input signal is designed to optimize a selected property of the SPRT, namely <b>average</b> <b>sampling</b> <b>number</b> (ASN) and probability of wrong decision. In case of more than two models the SPRT must be modifie...|$|R
40|$|The {{behaviour}} {{of group}} sequential {{tests in the}} two-sample problem is investigated if one replaces the classical non-robust estimators in the t-test statistic by modern robust estimators of location and scale. Hampel's 3 -part redescending M-estimator 25 A used in the Princeton study and the robust scale estimators length of the shortest half proposed by Rousseeuw & Leroy and Q proposed by Rousseeuw & Croux are considered. Of special interest are level, power and the <b>average</b> <b>sample</b> size <b>number</b> of the tests. It is investigated, whether commerical software {{can be used to}} apply these tests...|$|R
40|$|AbstractIn this paper, a time {{truncated}} {{life test}} {{based on two}} stage group acceptance sampling plan for the percentile lifetime following half-normal distribution is proposed. The optimal parameters for the proposed plan are determined such that both producer’s and consumer’s risks are satisfied simultaneously for the given experimentation time and sample size. The efficiency of the proposed sampling plan is {{discussed in terms of}} <b>average</b> <b>sample</b> <b>number</b> with the existing sampling plan. The proposed sampling plan is explained with the help of industrial examples...|$|E
40|$|Consider {{the problem}} of testing H 0 : p = p 0 vs. H 1 : p = p 1 where p 1 > p 0 and p is the {{parameter}} of a Bernoulli distribution. The triangular test is a sequential test that has the attractive properties of a reasonable <b>average</b> <b>sample</b> <b>number</b> (ASN) at (p 0 + p 1) / 2 and an upper bound {{on the number of}} required observations. We present an algorithm {{that can be used to}} design ASN-minimizing triangular tests...|$|E
40|$|In this paper, a time {{truncated}} {{life test}} {{based on two}} stage group acceptance sampling plan for the percentile lifetime following half-normal distribution is proposed. The optimal parameters for the proposed plan are determined such that both producer’s and consumer’s risks are satisfied simultaneously for the given experimentation time and sample size. The efficiency of the proposed sampling plan is {{discussed in terms of}} <b>average</b> <b>sample</b> <b>number</b> with the existing sampling plan. The proposed sampling plan is explained with the help of industrial examples...|$|E
40|$|Abstract: Early {{stopping}} {{of clinical}} trials either {{in case of}} beneficial or dele-terious effect of treatment on quality of life (QoL) is an important issue. QoL is usually evaluated using self-assessment questionnaires and responses to the items are combined into scores assumed to be normally distributed (which is rarely the case). An alternative is to use item response theory (IRT) models such as the Partial Credit Model (PCM) for polytomous items which {{takes into account the}} categorical nature of the items. Sequential analysis and mixed Partial Credit Models were combined in the context of phase II non-comparative trials. The statistical properties of the Sequential Probability Ratio Test (SPRT) and of the Triangular Test (TT) were compared using mixed PCM and traditional average scores methods (ASM) by means of simulations. The type I error of the sequential tests was correctly maintained for both methods, the mixed PCM being more conservative than the ASM. While re-maining a bit underpowered, the mixed PCM displayed higher power than the ASM for both sequential tests. Both methods allowed substantial reductions in <b>average</b> <b>sample</b> <b>numbers</b> as compared with fixed sample designs. Overlapping of item category particularly affected the ASM by inflating the type I error and power. The use of IRT models in sequential analysis of QoL endpoints is promising and should provide a more powerful method to detect therapeutic effects than the traditional ASM...|$|R
40|$|Electrocardiogram {{recordings}} {{are very}} often contaminated by high-frequency noise usually power-line interference and EMG disturbances (tremor). Filtering out the tremor remains a priori partially successful {{since it has}} a relatively wide spectrum, which overlaps the useful ECG frequency band by aperiodic noise. The proposed simple approach for tremor suppression uses heuristic relations between the ECG signal parts and parameters of the applied moving averaging. The results obtained are assessed and compared to tremor suppression obtained by moving <b>averaging</b> with constant <b>sample</b> <b>numbers</b> throughout the signal...|$|R
40|$|Background For {{studies with}} two-by-two {{factorial}} designs, {{the complexity of}} deter-mining an appropriate futility analysis plan is increased as compared to studies where patients are randomized to one treatment. Issues {{that must be addressed}} include the possibility of a significant interaction and the need to determine how to proceed given evidence of futility in one arm. Suggested approaches include a two-stage plan, which first assesses futility of the interaction term and proceeds to exam-ine the main effects, given sufficient evidence that no interaction is present, and var-iations on one-stage plans, which assume the trial will not be stopped for futility in the interaction. Purpose To discuss different approaches to monitoring futility in two-by-two factor-ial clinical trials and compare their properties. Methods We utilized a simulation study, designed to mimic the Secondary Preven-tion of Small Subcortical Strokes (SPS 3) Study, to determine which approach to monitoring futility in two-by-two factorial studies had the most desirable statistical properties. Results We found that in most scenarios typical of clinical trials, monitoring futility in each arm simultaneously was superior to or as good as monitoring the interaction and then assessing futility in each arm only when the interaction was deemed futile. Monitoring each arm simultaneously lead to early stopping more often when no treatment effect was present, and lower <b>average</b> <b>sample</b> <b>numbers</b> (ASNs). The exception to this was the unlikely case when a qualitative interaction was present. Limitations We assumed that one-sided tests were to be performed, and only assessed some of the possible methods for monitoring futility under the study design. Conclusions Futility monitoring in two-by-two factorial studies should proceed by assessing each arm simultaneously, rather than monitoring the interaction first. If sizeable interactions are anticipated, study design, rather than study monitoring, should account for this. Clinical Trials 2013; 0 : 1 – 7...|$|R
40|$|Abstract: All {{classical}} {{acceptance sampling}} plans were constructed with {{the proportion of}} defective that is crisp value. In the manufacturing processes, this parameter in a production lot may not be precis ely known. Hence it is meaningful to treat such parameter as fuzzy quantities. In this paper we have argued for the acceptance double sampling plan when the proportion of defective items is a fuzzy number and it is being modeled based on the fuzzy Poisson distribution. The calculation of the Operating Characteristic (OC) curves, <b>Average</b> <b>Sample</b> <b>Number</b> (ASN) curves, Average Outgoing Quality (AOQ) curves and Average Total Inspection (ATI) curves of the plan will be presented for double plan by using the concept of fuzzy probability. The results show that these four curves are like band having high and low bounds with their width depending on the ambiguity proportion parameter in the lot, when the size and acceptance numbers of the samples are fixed. We have als o shown that in this plan, if the process quality is perfect or poor, then the ASN and AOQ bands will be of lower value. Key words: Statistical quality control • acceptance double sampling • fuzzy numbers • <b>average</b> <b>sample</b> <b>number</b> • average outgoing quality • average total inspectio...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe {{implementation of a}} Naval Supply Systems Command Quality Control Program is intended to promote improved performance at U. S. Naval stockpoints. this paper examines current quality control procedures, compares current practice to quality control theory, and recommends that sequential sampling techniques be adopted. Sequential sampling plans and their associated operating characteristics curves and <b>average</b> <b>sample</b> <b>number</b> curves are provided. Implementation of the recommended procedures {{could result in a}} more flexible and efficient Quality Control Program at Naval Supply System stockpoints. [URL] Commander, United States Nav...|$|E
40|$|In this article, {{a general}} problem of {{sequential}} statistical inference for general discrete-time stochastic processes is considered. The {{problem is to}} minimize an <b>average</b> <b>sample</b> <b>number</b> given that Bayesian risk due to incorrect decision does not exceed some given bound. We characterize the form of optimal sequential stopping rules in this problem. In particular, we have a characterization of the form of optimal sequential decision procedures when the Bayesian risk includes both the loss due to incorrect decision {{and the cost of}} observations. Comment: Shortened version for print publication, 17 page...|$|E
40|$|International audienceBased on {{previous}} results on periodic non-uniform sampling (Multi-Coset) {{and using the}} well known Non-Uniform Fourier Transform through Bartlett's method for Power Spectral Density estimation, we propose a new smart sampling scheme named the Dynamic Single Branch Non-uniform Sampler. The idea of our scheme {{is to reduce the}} <b>average</b> <b>sampling</b> frequency, the <b>number</b> of <b>samples</b> collected, and consequently the power consumption of the Analog to Digital Converter. In addition to that our proposed method detects the location of the bands in order to adapt the sampling rate. In this paper, through we show simulation results that compared to classical uniform sampler or existing multi-coset based samplers, our proposed sampler, in certain conditions, provides superior performance, in terms of sampling rate or energy consumption. It is not constrained by the inexibility of hardware circuitry and is easily recongurable. We also show the eect of the false detection of active bands on the <b>average</b> <b>sampling</b> rate of our new adaptive non-uniform sub-Nyquist sampler scheme...|$|R
40|$|The {{public access}} defibrillators (PADs) are {{introduced}} {{on the market}} for more efficient treatment of out-of-hospital sudden cardiac arrests. PADs are used normally by untrained people at streets, sports centers, airports, and other public scenes. Therefore, the automatic fibrillation detection is of high importance. Special case represents the railway stations. Some countries use 16. 7 Hz AC power-line, which introduces largely frequency-varying interference that may corrupt the fibrillation detection. Moving signal averaging is often applied for 50 / 60 Hz interference suppression if the affect on the ECG spectrum has no importance (no morphological analysis is performed). This approach may be also applied on the railway interference if its frequency is ongoing detected to synchronize the analog-to-digital conversion (ADC) for introducing of variable inter-sample intervals. A better solution consists of rated ADC, frequency measuring, internal irregular re-sampling according to the interference frequency, moving <b>averaging</b> over constant <b>sample</b> <b>number</b> followed by regular back re-sampling...|$|R
40|$|Abstract Background In {{clinical}} trials, both unequal randomization {{design and}} sequential analyses have ethical and economic advantages. In the single-stage-design (SSD), however, if {{the sample size}} is not adjusted based on unequal randomization, {{the power of the}} trial will decrease, whereas with sequential analysis the power will always remain constant. Our aim was to compare sequential boundaries approach with the SSD when the allocation ratio (R) was not equal. Methods We evaluated the influence of R, the ratio of the patients in experimental group to the standard group, on the statistical properties of two-sided tests, including the two-sided single triangular test (TT), double triangular test (DTT) and SSD by multiple simulations. The <b>average</b> <b>sample</b> size <b>numbers</b> (ASNs) and power (1 -β) were evaluated for all tests. Results Our simulation study showed that choosing R = 2 instead of R = 1 increases the sample size of SSD by 12 % and the ASN of the TT and DTT by the same proportion. Moreover, when R = 2, compared to the adjusted SSD, using the TT or DTT allows to retrieve the well known reductions of ASN observed when R = 1, compared to SSD. In addition, when R = 2, compared to SSD, using the TT and DTT allows to obtain smaller reductions of ASN than when R = 1, but maintains the power of the test to its planned value. Conclusion This study indicates that when the allocation ratio is not equal among the treatment groups, sequential analysis could indeed serve as a compromise between ethicists, economists and statisticians. </p...|$|R
