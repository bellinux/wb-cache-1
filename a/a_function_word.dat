31|10000|Public
5000|$|... m̀h (adv. not, no, cannot; {{originally}} <b>a</b> <b>function</b> <b>word)</b> Standard Chinese: ...|$|E
50|$|That is <b>a</b> <b>function</b> <b>word</b> {{used in the}} English {{language}} for several grammatical purposes.|$|E
50|$|The {{descriptor}} lexical {{is applied}} to the words of a language's lexicon, often to indicate a content word, as distinct from <b>a</b> <b>function</b> <b>word.</b>|$|E
5000|$|In late Quenya moods (other {{than the}} indicative) are {{expressed}} by particles, <b>a</b> short <b>function</b> <b>word</b> {{that does not}} belong to any of the inflected grammatical word classes: ...|$|R
50|$|The word functor was {{borrowed}} by mathematicians {{from the}} philosopher Rudolf Carnap, {{who used the}} term in <b>a</b> linguistic context;see <b>function</b> <b>word.</b>|$|R
40|$|BACKGROUND AND PURPOSE: Changes {{in brain}} {{activation}} as <b>a</b> <b>function</b> of continuous multiparametric word recognition {{have not been}} studied before by using functional MR imaging (fMRI), to our knowledge. Our aim was to identify linear changes in brain activation and, what is more interesting, nonlinear changes in brain activation as <b>a</b> <b>function</b> of extended <b>word</b> repetition. MATERIALS AND METHODS: Fifteen healthy young right-handed individuals participated in this study. An event-related extended continuous word-recognition task with 30 target words was used to study the parametric effect of word recognition on brain activation. Word-recognition–related brain activation was studied as <b>a</b> <b>function</b> of 9 <b>word</b> repetitions. fMRI data were analyzed with a general linear model with regressors for linearly changing signal intensity and nonlinearly changing signal intensity, accord-ing to group average reaction time (RT) and individual RTs. RESULTS: A network generally associated with episodic memory recognition showed either constant or linearly decreasing brain activation as <b>a</b> <b>function</b> of <b>word</b> repetition. Furthermore, both anterior and posterior cingulate cortices and the left middle frontal gyrus followed the nonlinear curve of the group RT, whereas the anterior cingulate cortex was also associated with individual RT. CONCLUSION: Linear alteration in brain activation as <b>a</b> <b>function</b> of <b>word</b> repetition explained mos...|$|R
5000|$|In grammar {{the term}} {{particle}} (abbreviated [...] ) has a traditional meaning, {{as a part}} of speech that cannot be inflected, and a modern meaning, as <b>a</b> <b>function</b> <b>word</b> associated with another word or phrase to impart meaning.|$|E
5000|$|There {{are very}} few counterexamples that proceed further, and they {{required}} special circumstances to occur. One {{is found in the}} development of Irish Gaelic with the origin of the first-person-plural pronoun muid (<b>a</b> <b>function</b> <b>word)</b> from the inflectional suffix -mid (as in táimid [...] "we are") because of a reanalysis based on the verb-pronoun order of the other persons of the verb.|$|E
5000|$|The {{original}} {{motivation for}} the DP-analysis {{came in the form}} of parallelism across phrase and clause. The DP-analysis provides a basis for viewing clauses and phrases as structurally parallel. The basic insight runs along the following lines: since clauses have functional categories above lexical categories, noun phrases should do the same. The traditional NP-analysis has the drawback that it positions the determiner, which is often a pure function word, below the lexical noun, which is usually a full content word. The traditional NP-analysis is therefore unlike the analysis of clauses, which positions the functional categories as heads over the lexical categories. The point is illustrated by drawing a parallel to the analysis of auxiliary verbs. Given a combination such as will understand, one views the modal auxiliary verb will, <b>a</b> <b>function</b> <b>word,</b> as head over the main verb understand, a content word. Extending this type of analysis to a phrase like the car, the determiner the, <b>a</b> <b>function</b> <b>word,</b> should be head over car, a content word. In so doing, the NP the car becomes a DP. The point is illustrated with simple dependency-based hierarchies: ...|$|E
40|$|The {{generation}} of precise and comprehensible translations {{is still a}} challenge in the patent and scientific domain. In particular, <b>function</b> <b>words</b> are often poorly translated in standard machine translation systems, particularly across language pairs with greatly differing syntax. In this paper we exploit the target-side structure in tree-to-tree machine translation to post-edit <b>function</b> <b>words</b> automatically using <b>a</b> tree-based <b>function</b> <b>word</b> language model. We show that a significant improvement in human evaluation can be achieved with our proposed method. ...|$|R
50|$|Lexical {{choice is}} the subtask of Natural {{language}} generation that involves choosing the content words (nouns, verbs, adjectives, and adverbs) in <b>a</b> generated text. <b>Function</b> <b>words</b> (determiners, for example) are usually chosen during realisation.|$|R
40|$|Abstract—A {{method for}} authorship {{attribution}} based on func-tion word adjacency networks (WANs) is introduced. <b>Function</b> <b>words</b> {{are parts of}} speech that express grammatical relationships between other words but do not carry lexical meaning on their own. In the WANs in this paper, nodes are <b>function</b> <b>words</b> and directed edges from <b>a</b> source <b>function</b> <b>word</b> to <b>a</b> target <b>function</b> <b>word</b> stand in for the likelihood of finding the latter in the ordered vicinity of the former. WANs of different authors {{can be interpreted as}} transition probabilities of a Markov chain and are therefore compared in terms of their relative entropies. Optimal selection of WAN parameters is studied and attribution accuracy is benchmarked across a diverse pool of authors and varying text lengths. This analysis shows that, since <b>function</b> <b>words</b> are independent of content, their use tends to be specific to an author and that the relational data captured by <b>function</b> WANs is <b>a</b> good summary of stylometric fingerprints. Attribution accuracy is observed to exceed the one achieved by methods that rely on word frequencies alone. Further combining WANs with methods that rely on word frequencies, results in larger attribution accuracy, indicating that both sources of information encode different aspects of authorial styles. I...|$|R
50|$|An {{interrogative}} word or {{question word}} is <b>a</b> <b>function</b> <b>word</b> {{used to ask}} a question, such as what, when, where, who, whom, why, and how. They are sometimes called wh-words, because in English most of them start with wh- (compare Five Ws). They {{may be used in}} both direct questions (Where is he going?) and in indirect questions (I wonder where he is going). In English and various other languages the same forms are also used as relative pronouns in certain relative clauses (The country where he was born) and certain adverb clauses (I go where he goes).|$|E
5000|$|Since {{there is}} no need to mark the {{stressed}} syllable of a monosyllabic word, most of them do not have an accent. Exceptions to this are those with a diacritical accent that differentiates some cases of words that would otherwise be homographic. Example: es [...] (it impersonal) vs és [...] (is), te [...] (you clitic) vs té [...] (s/he has), mes [...] (month) vs més [...] (more), dona [...] (woman) vs dóna [...] (s/he gives). In most cases, the word bearing no accent is either unstressed (as in the case of es and te), or the word without the accent is more common, usually <b>a</b> <b>function</b> <b>word.</b>|$|E
50|$|This {{analysis}} of noun phrases is widely {{referred to as}} the DP hypothesis. It has been the preferred {{analysis of}} noun phrases in the minimalist program from its start (since the early 1990s), though the arguments in its favor tend to be theory-internal. By taking the determiner, <b>a</b> <b>function</b> <b>word,</b> to be head over the noun, a structure is established that is analogous to the structure of the finite clause, with a complementizer. Apart from the minimalist program, however, the DP hypothesis is rejected by most other modern theories of syntax and grammar, in part because these theories lack the relevant functional categories. Dependency grammars, for instance, almost all assume the traditional NP analysis of noun phrases.|$|E
40|$|In this {{experiment}} using the conditioned head-turn procedure, 18 -month-old French-learning toddlers {{were trained to}} respond to either a target noun (“la balle”/ the ball) or a target verb (“je mange”/ I eat). They were then tested on target word recognition in two syntactic contexts: the target word was preceded either by <b>a</b> correct <b>function</b> <b>word</b> (“une balle”/ <b>a</b> ball or “on mange”/ they eat), or by <b>an</b> incorrect <b>function</b> <b>word,</b> signaling <b>a</b> word from the other category (*“on balle”/ they ball or *“une mange”/ a eat). We showed that 18 -month-olds exploit the syntactic context on-line to recognize the target word: verbs were recognized when preceded by a personal pronoun but not when preceded by a determiner and vice-versa for nouns. These results suggest that 18 -month-olds already know noun and verb contexts. As a result, {{they might be able}} to exploit them to categorize unknown words and constrain their possible meaning (nouns typically refer to objects whereas verbs typically refer to actions) ...|$|R
40|$|In {{statistical}} word alignment for machine translation, <b>function</b> <b>words</b> usually cause poor aligning performance {{because they}} do not have clear correspondence between different languages. This paper proposes a novel approach to improve word alignment by pruning alignments of <b>function</b> <b>words</b> from <b>an</b> existing alignment model with high precision and recall. Based on monolingual and bilingual frequency characteristics, <b>a</b> language-independent <b>function</b> <b>word</b> recognition algorithm is first proposed. Then a group of carefully defined syntactic structures combined with content word alignments are used for further <b>function</b> <b>word</b> alignment pruning. The experimental results show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, Germanto-English and French-to-English language pairs. ...|$|R
50|$|The {{indirect}} case is {{used when}} <b>a</b> <b>word</b> <b>functions</b> as <b>an</b> attribute before a word, {{and is not}} considered a case in some literature.|$|R
5000|$|A {{more unusual}} case of anthimeria is {{displayed}} not through {{a change in}} lexical category but a change in form altogether. The punctuation mark '/' was originally implemented to juxtapose two similarly related words or phrases, such as a 'friend/roommate', meaning that the referred person is both a friend and roommate to the speaker. However, younger generations have come to morph the symbol '/' into the written and spoken word of 'slash'. Anne Curzan, a professor of English at the University of Michigan, notes that the [...] "emergence of a new conjunction/conjunctive adverb (let alone one stemming from a punctuation mark) is like a rare-bird sighting {{in the world of}} linguistics: an innovation in the slang of young people embedding itself as <b>a</b> <b>function</b> <b>word</b> in the language".|$|E
5000|$|Although unidirectionality is a {{key element}} of grammaticalization, it is not absolute. Indeed, the {{possibility}} of counterexamples, coupled with their rarity, is given as evidence for the general operating principle of unidirectionality. According to Lyle Campbell, however, advocates often minimize the counterexamples or redefine them as not being part of the grammaticalization cline. He gives the example of Hopper and Traugott (1993) who treat some putative counterexamples as cases of lexicalization in which a grammatical form is incorporated into a lexical item but does not itself become a lexical item. An example is the phrase to up the ante, which incorporates the preposition up (<b>a</b> <b>function</b> <b>word)</b> in a verb (a content word) but without up becoming a verb outside of this lexical item. Since it is the entire phrase to up the ante that is the verb, Hopper and Traugott argue that the word up itself cannot be said to have degrammaticalized, a view that is challenged to some extent by parallel usages such as to up the bid, to up the payment, to up the deductions, to up the medication, by the fact that in all cases the can be replaced by a possessive (my, your, her, Bill's, etc.), and by further extensions still: he upped his game 'he improved his performance'.|$|E
5000|$|Categorial grammar is an {{approach}} that attributes the syntactic structure not to rules of grammar, but to {{the properties of the}} syntactic categories themselves. For example, rather than asserting that sentences are constructed by a rule that combines a noun phrase (NP) and a verb phrase (VP) (e.g., the phrase structure rule S → NP VP), in categorial grammar, such principles are embedded in the category of the head word itself. So the syntactic category for an intransitive verb is a complex formula representing the fact that the verb acts as <b>a</b> <b>function</b> <b>word</b> requiring an NP as an input and produces a sentence level structure as an output. This complex category is notated as (NP\S) instead of V. NP\S is read as [...] "a category that searches to the left (indicated by \) for an NP (the element on the left) and outputs a sentence (the element on the right)." [...] The category of transitive verb is defined as an element that requires two NPs (its subject and its direct object) to form a sentence. This is notated as (NP/(NP\S)) which means [...] "a category that searches to the right (indicated by /) for an NP (the object), and generates a function (equivalent to the VP) which is (NP\S), which in turn represents a function that searches to the left for an NP and produces a sentence." ...|$|E
50|$|In linguistics, a pro-form is <b>a</b> type of <b>function</b> <b>word</b> or {{expression}} {{that stands in}} for (expresses the same content as) another word, phrase, clause or sentence where the meaning is recoverable from the context. They are used either to avoid repetitive expressions or in quantification (limiting the variables of a proposition).|$|R
40|$|In this paper, we {{show some}} {{properties}} of <b>function</b> <b>words</b> in dependency trees. <b>Function</b> <b>words</b> are grammatical words, such as articles, prepositions, pronouns, conjunctions, or auxiliary verbs. These words are often short and very frequent in texts and therefore {{many of them}} can be easily recognized. We formulate <b>a</b> hypothesis that <b>function</b> <b>words</b> tend to have a fixed number of dependents and we prove this hypothesis on treebanks. Using this hypothesis, we are able to improve unsupervised dependency parsing and outperform previously published state-of-the-art results for many languages...|$|R
40|$|This paper proposes an {{approach}} to the secondary task in the TREC Genomics Track. We regard the task as identification of the sentences describing gene functions (i. e., GeneRIFs) and propose a method considering two factors: topicality and relevance. The former refers to the topicality of a sentence and is measured based on location information and word frequencies in the article. The latter refers to the relevance as a GeneRIF based on the vocabulary used in the article. We formalize a probabilistic model combining these features. Our method is evaluated on the test set of 139 MEDLINE abstracts, and the results demonstrate that (<b>a)</b> <b>function</b> <b>words</b> in input could help to identify gene function descriptions and that (b) there is a vocabulary peculiar to GeneRIFs and that (c) location information shows the highest predictive power for this particular task despite its simplicity. Additionally, we examine some alternative methods in comparison with our method...|$|R
40|$|This paper {{outlines}} experiments {{conducted to}} determine {{the contribution of the}} traditional bilingual dictionary in the automatic alignment process to learn translation patterns, and at runtime. We found that by using automatically derived translation word pairs combined with <b>a</b> <b>function</b> <b>word</b> only lexicon, we were able to either match or nearly match the translation quality of the system that used a full traditional bilingual lexicon in addition. The language pairs studied were French-English and Spanish-English...|$|E
40|$|This study {{investigates the}} {{function}} word deficits in aphasic patients and, in particular, agrammatic Broca's aphasics. Several {{explanations for the}} function word problem are addressed including <b>a</b> <b>function</b> <b>word</b> vocabulary deficit theory, a general syntactic deficit theory, and an abstract word deficit theory. Subjects {{with varying degrees of}} agrammatism were tested on a variety of production, comprehension, and reading, and syntactic tests which isolate semantic and syntactic aspects of both function and content words in order to better define the nature of the function word deficits in agrammatism...|$|E
40|$|This paper {{presents}} <b>a</b> <b>Function</b> <b>Word</b> centered, Syntax-based (FWS) {{solution to}} address phrase ordering {{in the context}} of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words ’ arguments and improving translation quality in both perfect and noisy word alignment scenarios. ...|$|E
5000|$|Requiring a {{brand name}} for her product, Blakely was {{frustrated}} after failing to settle on a title that satisfied her, {{after a year and}} a half of ideation. At the time she finalized her brand name, Blakely knew that Coca-Cola and Kodak were the two most recognized brand names in the world, and that both contained a strong [...] "k" [...] sound. Blakely read that the founder of Kodak liked the sound so much that he used it as the beginning and end of his brand name and then proceeded to create <b>a</b> <b>functioning</b> <b>word</b> based upon this foundation. The name [...] "Spanks" [...] eventually came to Blakely, and she decided to replace the [...] "ks" [...] with an [...] "x", as her research had shown that constructed names were more successful and were easier to register as a trademark. Blakely then used her credit card to purchase the [...] "Spanx" [...] trademark on the USPTO website for US$350.|$|R
30|$|The PSTs {{displayed}} {{their knowledge}} of word classes particularly in relation to content words, {{in the context of}} development of the Field (for example, Student <b>A).</b> They found <b>function</b> <b>words</b> more complex, and even the best assignments did not display this, although some (not part of this sample) mentioned prepositions in association with describing location in barrier games.|$|R
50|$|A ploce is {{a figure}} of speech in which a word is {{separated}} or repeated by way of emphasis; the repetition of <b>a</b> <b>word</b> <b>functions</b> as <b>a</b> different part of speech or in different contexts.|$|R
40|$|International audienceFrench accentual phrases (APs) are {{characterized}} {{by the presence of}} a typical final fo rise (LH*) and an optional/additional initial fo rise (LHi). This study tested whether between-speaker speech imitation influenced the realization of APs tonal patterns. The experiment was based on APs containing <b>a</b> <b>function</b> <b>word</b> plus a bisyllabic content word, whose tonal patterns differed in the potential placement of an optional/initial high tone (Hi). In two shadowing tasks (without/with explicit instructions to imitate the speaker's way of pronouncing the stimuli), participants produced more initial high tones when they heard a stimulus including both initial and final high tones relative to stimuli which only a final high tone was present. Thus, imitation influences the realization of APs tonal patterns in French...|$|E
40|$|This study {{tests the}} {{hypothesis}} that function words are among the earliest word forms segmented by preverbal infants. In a visual fixation procedure, French-learning 8 -month-old infants were familiarized to <b>a</b> <b>function</b> <b>word,</b> mes or ta. All infants were then tested with passages containing mes vs. ta. Looking times during {{the presentation of the}} two passage types were expected to differ if infants segmented the target functor. The results showed a significant interaction of passage type and sex. Although the direction of the looking preference is different for the two sexes, both groups showed a significant difference in listening times to the passage containing the target versus that containing the non-target. This suggests that both groups segmented the function words. The implications of functional elements for early lexical and syntactic acquisition are discussed...|$|E
40|$|The {{conjunction}} and {{is treated}} :in early and modern grammatical writings from the poimt {{of view of}} its categorization, its function, and evaluation in terms of style. Syntactico-grammatical and logico-semantic criteria are employed by early as well as modern authors. And is defined as <b>a</b> <b>function</b> <b>word</b> marking structural relations called coordination, and semantic relations called addition. Rules governing coordination are discussed, {{as well as the}} semantic implications of the structure in question. On levels above sentence and functions as a text-forming device producting cohesion of the discourse. The paper suggests that the only undisputed function of and is that of connector. The idea of some early grammariams of coordination reduction has been usefully developed in transformational syntax. Further studies in frequency, variety of stylistic applications and change of stylistic range should provide more interesting views of constructions with "and"...|$|E
40|$|In {{this paper}} {{we present a}} new {{approach}} for horizontal object oriented database fragmentation combined with fine-grained object level replication in one step. We build our fragmentation/replication method using AI probabilis-tic clustering (fuzzy clustering). Fragmentation quality evaluation is provided using <b>an</b> evaluator <b>function.</b> KEY <b>WORD...</b>|$|R
50|$|Two primary {{hypotheses}} {{tried to}} explain the missing letter effect: Healy (1994) emphasized identification processes playing a crucial role, almost entirely focusing on word frequency. However, Koriat & Greenberg (1994) viewed the structural role of the word within <b>a</b> sentence (i.e. <b>function</b> <b>words</b> vs. content words) to be crucial. Both accounts were thoroughly investigated, but neither could completely explain the effect.|$|R
40|$|AbstractIt {{is shown}} by means of example that for each n > 3, there is a minimal n-state {{stochastic}} automaton whose output behavior is <b>a</b> <b>word</b> <b>function</b> of rank 3. Thus, there are stochastic <b>word</b> <b>functions</b> whose natural representation may be quite unwieldy, but whose values nevertheless can be computed in a rapid, compact fashion...|$|R
