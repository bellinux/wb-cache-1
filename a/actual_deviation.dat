15|77|Public
5000|$|With [...] {{representing}} the deviation {{from an individual}} to the sample mean, and [...] {{representing the}} deviation from the sample mean to the population mean. Note that we've simply decomposed the <b>actual</b> <b>deviation</b> from the (unknown) population mean into two components: the deviation to the sample mean, which we can compute, and the additional deviation to the population mean, which we can not. Now apply that identity to the squares of deviations from the population mean: ...|$|E
3000|$|... is {{the mean}} {{value of the}} {{measured}} data, ESS is the explained sum of squares and TSS is the total sum of squares. The MBE determines whether the model underestimates or overestimates experimental data, the RMSE gives {{a measure of how}} well the model can predict the experimental data (it provides information on the short-term performance of the model by allowing a term-by-term comparison of the <b>actual</b> <b>deviation</b> between the estimated and the measured values), and R 2 provides a measure of how well the model explains observed phenomena (its value shows how well future outcomes can be predicted by the model).|$|E
40|$|The use of exergy to {{correlate}} energy-utilization efficiencies and energy research investments is described. Specifically, energy and exergy losses are compared with energy {{research and development}} expenditures, demonstrating that the latter correlates with energy losses, even though {{it would be more}} sensible to allocate energy research and development funding in line with exergy losses, as they represent the <b>actual</b> <b>deviation</b> of efficiency from the ideal. The methodology is outlined and illustrated with two case studies. The case studies consider the province of Ontario, Canada and the United States. The investigation utilizes data on the energy utilization in a country or region, including flows of energy and exergy through the main sectors of the economy. The results are expected to be of use to government and public authorities that administer research and development funding and resources and should help improve the effectiveness of such investments...|$|E
50|$|Control: The {{final element}} of {{management}} involves {{the comparison of}} {{the activities of the}} personnel to the plan of action, it is the evaluation component of management. Monitoring function that evaluates quality in all areas and detects potential or <b>actual</b> <b>deviations</b> from the organization's plan, ensuring high-quality performance and satisfactory results while maintaining an orderly and problem-free environment. Controlling includes information management, measurement of performance, and institution of corrective actions.|$|R
40|$|When seeking for the {{possible}} emendation of the errors in a map based on Ptolemy's co-ordinates, {{we have to}} understand the measuring process of his sources first. Ptolemy possibly used the official formulae provinciae, which were created with the help of gromatic surveys carried out with due precision, but the author had to deal with different formulae, in Pannonia possibly four, which were not consistent with one another. Carefully examining the <b>actual</b> <b>deviations,</b> we can restore the ways and the starting-points of the surveys, and so the methods of eliminating the inconsistencies can be identified...|$|R
50|$|James Joule {{tried to}} measure the {{internal}} pressure of air in his expansion experiment by isothermally pumping high pressure air from one metal vessel into another evacuated one. The water bath in which the system was immersed did not change its temperature, signifying that {{no change in the}} internal energy occurred, the internal pressure of the air was equal to zero and the air was a perfect gas. The <b>actual</b> <b>deviations</b> from the perfect behaviour were not observed since they are very small and the specific heat capacity of water is relatively high.|$|R
30|$|Most {{existing}} {{end-to-end service}} QoE evaluation methods {{are derived from}} the evaluation methods based on linear evaluation model. These methods depend on the majority of evaluation information composition rules of setting the model and linear model with great proximity. This process is called qualitative data analysis. The results of qualitative data analysis are usually restricted in a certain range. However, not all evaluation information can be evaluated accurately, which falls on the qualitative model, compared with the evaluation {{on the presence of}} the <b>actual</b> <b>deviation</b> value. The advantages of clustering model are intuitive, and the conclusion is simple. Fuzzy clustering analysis is to establish fuzzy similar relation based on the characteristics, intimacy, and similarity, realizing clustering analysis method for objective things. The QoE evaluation method of fuzzy clustering heuristic algorithm begins with the service characteristics. For each service periodic evaluation parameter, accurate service score should be given, the deviation degree of evaluation reduced, and the precision of calculation improved.|$|E
40|$|As the two {{important}} form of financial market, {{the risk of}} financial securities, such as stocks and bonds, has been a hot topic in the financial field; at the same time, {{under the influence of}} many factors of financial assets, the correlation between portfolio returns causes more research. This paper presents Copula-SV-t model that it uses SV-t model to measure the edge distribution, and uses the Copula-t method to obtain the high-dimensional joint distribution. It not only solves the <b>actual</b> <b>deviation</b> with using the ARCH family model to calculate the portfolio risk, but also solves the problem to overestimate the risk with using extreme value theory to study financial risk. Through the empirical research, the conclusion shows that the model describes better assets and tail characteristics of assets, and is more in line with the reality of the market. Furthermore, Empirical evidence also shows that if the portfolio is relatively large degree of correlation, the ability to disperse portfolio risk is relatively weakness...|$|E
40|$|This paper {{presents}} {{an analysis of}} optimal impact strategies to deflect potentially dangerous asteroids. To compute {{the increase in the}} minimum orbit intersection distance of the asteroid due to an impact with a spacecraft, simple analytical formulas are derived from proximal motion equations. The proposed analytical formulation allows for an analysis of the optimal direction of the deviating impulse transferred to the asteroid. This ideal optimal direction cannot be achieved for every asteroid at any time; therefore, an analysis of the optimal launch opportunities for deviating a number of selected asteroids was performed {{through the use of a}} global optimization procedure. The results in this paper demonstrate that the proximal motion formulation has very good accuracy in predicting the <b>actual</b> <b>deviation</b> and can be used with any deviation method because it has general validity. Furthermore, the characterization of optimal launch opportunities shows that a significant deviation can be obtained even with a small spacecraf...|$|E
40|$|With {{present day}} grain drills {{the number of}} plants per unit area is {{controlled}} via the seed-mass per unit area in kg/ha. This method causes substantial deviations from the target. Instead of the seed-mass the seed-numbers should be controlled. This can be attained by recording the number of seeds passing through the seed tube. A method is presented which includes a compensating program for recording errors due to seed clusters falling through the tube. It is shown that the <b>actual</b> <b>deviations</b> {{in the number of}} seeds recorded with the present day seed rates can be kept below 2, 5 %. The method is suitable for automatic closed loop computer control and for side specific sowing...|$|R
40|$|Neglect of {{end effects}} in Couette {{rotational}} viscometry introduces 10 - 30 % {{error in the}} estimate of shear stress at spindle surface. <b>Actual</b> <b>deviations</b> depend on the shear-thinning level of a given sample. We tackle the end effect for the standard sensor Z 40 DIN according to ISO 3219 by solving the related 2 D boundary-value problem for a class of shear-thinning viscosity functions. Pseudosimilarity method of treating the primary data leaves an error in shear stresses about 0. 5 %. Further reduction of the errors needs a full numerical simulation for each point of primary data, based on a suitable wide-range representation of the viscosity function. To support high accuracy of torque calibrations, the effect of inertia on torque for Newtonian liquids in standard sensor Z 40 DIN at Re < 500 is calculated using the FLUENT 6. 2 commercial software...|$|R
40|$|International audienceMaking {{a product}} {{conform to the}} {{functional}} requirements indicated by the customer suppose {{to be able to}} manage the manufacturing process chosen to realise the parts. A simulation step is generally performed to verify that the expected generated deviations fit with these requirements. It is then necessary to assess the <b>actual</b> <b>deviations</b> of the process in progress. This is usually done by the verification of the conformity of the workpiece to manufacturing tolerances {{at the end of each}} set-up. It is thus necessary to determine these manufacturing tolerances. This step is called "manufacturing tolerance synthesis". In this paper, a numerical method is proposed to perform 3 D manufacturing tolerances synthesis. This method uses the result of the numerical analysis of tolerances to determine influent mall displacement of surfaces. These displacements are described by small displacements torsors. An algorithm is then proposed to determine suitable ISO manufacturing tolerances...|$|R
40|$|A multi-view dense {{matching}} algorithm of high resolution aerial images based on graph network was presented. Overlap ratio and direction between adjacent images {{was used to}} find the candidate stereo pairs and build the graph network, then a Coarse-to-Fine strategy based on modified semi-global {{matching algorithm}} (SGM) was adapted to calculate the disparity map of stereo pairs. Finally, dense point cloud was generated and fused using a multi-triangulation method based on graph network. In the experiment part, the Vaihingen aerial image dataset and the oblique nadir image block of Z&# 252;rich in ISPRS/EuroSDR project were {{used to test the}} algorithm above. Experiment results show that out method is effective for multi-view dense matching of high resolution aerial images in consideration of completeness, efficiency and precision, while the RMS of average reprojection error can reach subpixel level and the <b>actual</b> <b>deviation</b> is better than 1. 5 GSD. Due to the introduction of guided median filter, regions of sharp discontinuities, weak textureness or repeat textureness like buildings, vegetation and water body can also be matched well...|$|E
40|$|In {{this paper}} a {{mechanism}} {{is designed for}} the desired performance output of the machine and these mechanisms are being used in case of climbing wheels. In cities the buildings are generally three or four storied {{and it is not}} convenient and also financially not easy to fit electric lifts everywhere. A four bar lift can be used by old or disabled person to climb one floor as subjected to lift. Hence a four bar mechanism to be implemented on every wheel to make the frame to be moved and to make synthesis and simulation for same mechanism to track its actual path and to understand its motion of stair of height of 220 mm. Taking into consideration the available dimensions of the four links a software is prepared with help of Fraudenstein’s equation being compared graphically with existing dimensions so as to calculate the <b>actual</b> <b>deviation</b> graphically and programmatically. Also the sensitivity of each link being carried out for variation of + 1 mm and – 1 mm in actual dimensions available...|$|E
40|$|The {{effect of}} setting all T= 0 two body {{interaction}} matrix elements equal to a constant (or zero) in shell model calculations (designated as = 0) are investigated. Despite the apparent severity {{of such a}} procedure, one gets fairly reasonable spectra. We find that using = 0 in single j shell calculations degeneracies appear e. g. the I = 1 − 13 − 2 and 2 states in 43 Sc are at the same excitation energies as are the I= 3 + 2, 7 + 2, 9 + 1 and 10 + 1 states in 44 Ti. The above degeneracies involve the vanishing of certain 6 j and 9 j symbols. The symmetry relations of Regge are used to explain why these vanishings are not accidental. Thus for these states the <b>actual</b> <b>deviation</b> from degeneracy are good indicators {{of the effects of}} the T= 0 matrix elements. A further indicator {{of the effects of the}} T= 0 interaction in an even- even nucleus is to compare the energies of states with odd angular momentum with those that are even. 1 I...|$|E
25|$|However critics {{claim to}} have {{identified}} statistical errors in the conclusions published in Nature: including: the <b>actual</b> standard <b>deviation</b> for the Tucson study was 17 years, not 31, as published; the chi-square distribution value is 8.6 rather than 6.4, and the relative significance level (which measures {{the reliability of the}} results) is close to 1% – rather than the published 5%, which is the minimum acceptable threshold.|$|R
40|$|In this note, {{we focus}} on the measure of Competitive Balance in sport league when the size of the league varies. We {{construct}} a Competitive Balnce index defined as the ratio of the <b>actual</b> standard <b>deviation</b> to the maximal standard deviation, the value of the demoninator {{depending on the size of}} the league. On the basis of this ratio we then construct Iso Competitive Balance curves...|$|R
50|$|For a {{graphical}} {{representation of the}} bearing tolerance differences by grade, please see the table. The table shows the <b>actual</b> allowable <b>deviations,</b> in micrometers (μm), from nominal for a 20 mm inner diameter bearing. As you can see an ABEC 7 bearing only has a 5 μm tolerance window whereas an ABEC 1 has double that at 10 μm. Please see the tables and PDF’s above for more specific precision bearing tolerancing.|$|R
40|$|The {{effect of}} setting all T= 0 two body {{interaction}} matrix elements equal to a constant (or zero) in shell model calculations in the $f_{ 7 / 2 }$ region are investigated. Despite the apparent severity {{of such a}} procedure, one gets fairly reasonable spectra. It is noted that using $V^{T= 0 }= 0 $ in single j shell calculations degeneracies appear e. g. the $I={ 1 / 2 } ^{-}$ and ${ 13 / 2 }^{-}$ states in $^{ 43 }$Sc are at the same excitation energies as are the I=$ 3 _{ 2 }^{+}$,$ 7 _{ 2 }^{+}$, 9 $^{+}_{ 1 }$ and 10 $^{+}_{ 1 }$ states in $^{ 44 }$Ti. Thus for these states the <b>actual</b> <b>deviation</b> from degeneracy are good indicators {{of the effects of}} the T= 0 matrix elements. The best way of seeing the effects of the T= 0 interaction in an even - even nucleus is to compare the energies of state with states of odd angular momentum with those that are even...|$|E
40|$|The {{effects of}} setting all T= 0 two body {{interaction}} matrix elements equal to a constant (or zero) in shell model calculations (designated as $ = 0 $) are investigated. Despite the apparent severity {{of such a}} procedure, one gets fairly reasonable spectra. We find that using $ = 0 $ in single j shell calculations degeneracies appear e. g. the $I={ 1 / 2 } ^{-}$ and ${ 13 / 2 }^{-}$ states in $^{ 43 }$Sc are at the same excitation energies; likewise the I=$ 3 _{ 2 }^{+}$,$ 7 _{ 2 }^{+}$, 9 $^{+}_{ 1 }$ and 10 $^{+}_{ 1 }$ states in $^{ 44 }$Ti. The above degeneracies involve the vanishing of certain 6 j and 9 j symbols. The symmetry relations of Regge are used to explain why these vanishings are not accidental. Thus for these states the <b>actual</b> <b>deviation</b> from degeneracy are good indicators {{of the effects of}} the T= 0 matrix elements. A further indicator {{of the effects of the}} T= 0 interaction in an even - even nucleus is to compare the energies of states with odd angular momentum with those that are even...|$|E
40|$|The mission {{architecture}} of SHEFEX II features a two-stage solid propellant sounding rocket vehicle on a suppressed trajectory, which is induced by a cold gas pointing maneuver {{of the vehicle}} before second stage ignition. The impact point is subject to a 3 -� dispersion of roughly� 110 kmin downrange and� 90 kmin crossrange, which makes a recovery of the vehicle particularly difficult, as the whole impact area is located off shore and the vehicle needs to be recovered by ship. As the major part to dispersion is contributed during the atmospheric ascent of the vehicle, a control algorithm is developed that considers the <b>actual</b> <b>deviation</b> from the nominal trajectory after atmospheric exit and recommends a vehicle pointing that corrects for this deviation. The analytic control algorithm is found by linear/quadratic approximation of the impact point sensitivity towards the deviations after atmospheric exit and to the pointing angles. The effectiveness of the algorithm is tested by implementing it in a full six-degree-offreedom simulation and applying dispersion factors in a Monte Carlo simulation. The result is a reduction of the impact point dispersion area by about 78...|$|E
40|$|Investment project {{management}} information technology was developed. The information technology {{is intended for}} failure project risk prognostication and <b>actual</b> project results <b>deviation</b> (from planned project results) prognostication in the pre-project planning process, based on the methods, developed by the authors...|$|R
40|$|A {{well known}} problem of time-interleaved analogto-digital {{converters}} is the matching {{between the individual}} channels of the converter. Any mismatch between the channels affects {{the accuracy of the}} converter adversely. The random mismatch between the channels originates mainly from the mismatch of components like transistors and capacitors. To achieve a certain degree of matching between the channels, the sizes of the individual components have to be chosen accordingly. Especially for high-resolution converters, this means that physically large transistors are required, resulting in a large chip area, increased power consumption and reduced conversion speed. Instead of increasing sizes to achieve a certain accuracy, one can also start with an analog circuit that is relatively inaccurate from itself (allowing physically small devices), and use a digital post-correction technique afterwards to correct for the <b>actual</b> <b>deviations</b> of each component. With this method, a high accuracy can be obtained while the requirements for the components are relaxed significantly. Although these techniques have been available for single-channel converters for many years, techniques correcting the mismatch between several channels are scarce. In this paper, an existing algorithm for single-channel pipelined converters is extended to include inter-channel correction as well, requiring almost no additional hardware...|$|R
40|$|We compare {{theoretical}} and empirical forecasts computed by rational agents living in a model economy to those produced by professional forecasters. We focus on the variance of the prediction errors {{as a function of}} the forecast horizon and analyze the speed with which it converges to a constant (which can be seen as a measure of the speed of convergence of the economy to the steady state). We look at a standard sticky-prices-wages model, concluding that it delivers a strong theoretical forecastability of the variables under scrutiny, at odds with the data (professional forecasts). The flexible prices-wages version delivers a forecastability closer to the data and performs relatively better empirically (with actual data), but mainly because forecasts deviate little from the unconditional mean. These results can be interpreted in at least two ways: first, <b>actual</b> <b>deviations</b> from the steady-state are not persistent, in which case the implications of the specific formulation of nominal rigidities for short-run dynamics are unrealistic; second, and still looking through the lens of a model, exogenous (or unmodelled) steady-state shifts attributable to, e. g., changes in monetary-policy, taxation, regulation or in the growth of the technological frontier, occur in such a way as to strongly limit the performance of professional forecasters. �...|$|R
40|$|Intelligence {{testing in}} {{children}} with intellectual disabilities (ID) has significant limitations. The normative samples of widely used intelligence tests, such as the Wechsler Intelligence Scales, rarely include an adequate number of subjects with ID needed to provide sensitive measurement in the very low ability range, and they are highly subject to floor effects. The IQ measurement problems in these children prevent characterization of strengths and weaknesses, poorer estimates of cognitive abilities in research applications, and in clinical settings, limited utility for assessment, prognosis estimation, and planning intervention. Here, we examined {{the sensitivity of the}} Wechsler Intelligence Scale for Children (WISC-III) in a large sample of children with fragile X syndrome (FXS), {{the most common cause of}} inherited ID. The WISC-III was administered to 217 children with FXS (age 6 – 17  years, 83 girls and 134 boys). Using raw norms data obtained with permission from the Psychological Corporation, we calculated normalized scores representing each participant’s <b>actual</b> <b>deviation</b> from the standardization sample using a z-score transformation. To validate this approach, we compared correlations between the new normalized scores versus the usual standard scores with a measure of adaptive behavior (Vineland Adaptive Behavior Scales) and with a genetic measure specific to FXS (FMR 1 protein or FMRP). The distribution of WISC-III standard scores showed significant skewing with floor effects in a high proportion of participants, especially males (64. 9 %– 94. 0 % across subtests). With the z-score normalization, the flooring problems were eliminated and scores were normally distributed. Furthermore, we found correlations between cognitive performance and adaptive behavior, and between cognition and FMRP that were very much improved when using these normalized scores in contrast to the usual standardized scores. The results of this study show that meaningful variation in intellectual ability {{in children with}} FXS, and probably other populations of children with neurodevelopmental disorders, is obscured by the usual translation of raw scores into standardized scores. A method of raw score transformation may improve the characterization of cognitive functioning in ID populations, especially for research applications...|$|E
40|$|Statement of the problem. The {{main points}} of {{contractual}} obligations is their timely performance with ensuring the desired level of investments {{of the investment}} funds provided for in this contract. The longer {{the execution of the}} works under the contract, the higher the probability of violation of these terms. Analysis of construction projects over the past decade has shown that the situation has not changed significantly, according to [8] contemporary data on the construction of a number of objects from which it follows that the larger the object, and accordingly, the longer construction period, the more the <b>actual</b> <b>deviation</b> of the actual terms of the construction of the planned, up to 50 [...] . 100 % in some cases. The comparison of these data shows that the problem of ensuring reliable operation of the construction company on the stage of implementation of a specific project is relevant in the present time. Analysis of recent research. The analysis of researches in the field of the rational justification of organizational and technological reliability values shows that its range is in the range from 0. 35 to 0. 9, it indicates the absence of a reasoned approach to this issue. Of course, for a more reliable implementation of the plan one needs to have a certain amount of appropriate material and financial resources, but in the management process is another important resource that should be in possession of the subject of management this is information. The purpose and objectives of work. The aim of this work is the study of the rational level of organizational and technological reliability (OTR) based on analysis of the need for this information. To achieve the goal of the article were set and solve the following tasks: - to establish the relationship between OTR and the right amount of information; - to determine the influence of the accuracy of determining {{the current state of the}} controlled parameter and the level of information; - to justify the rational criterion level OTR. Conclusions. The studies, which were based on the theory of information found that the rational level of OTR is in the range of 75 % [...] . 85 %. The accuracy of the determination of parameter values for which management is acted has significant influence on the amount of administrative work associated with analysis of the information that enables you to set high of the reliability in construction projects during the implementation of construction projects...|$|E
40|$|During {{the last}} decade {{there has been a}} {{tremendous}} revolution in quality management practices. These changes have come {{as a result of the}} necessity to change in a globally competitive environment rather as a result of any major breakthrough in science or technology. This dissertation focuses on problems that will assist the firm in increasing its productivity and improving the quality of the product. We first consider the problem of optimal allocation of work in an assembly system, we examine a PUSH system. The problem corresponds to moving work between feeder stations and the assembly station to achieve the optimal workload that will maximize expected throughput. For this configuration, earlier studies showed that optimal expected throughput decreased as the number of feeder stations increased for exponential and log-normal processing times. However, the pattern was not similar for uniform processing times, where, the optimal throughput initially decreased and then increased as the number of components increased. We provide insights as to the causes of this seemingly anomalous behavior described for the uniform processing time distribution. We also investigate the impact of unbalancing on the variability of interdeparture time (reciprocal of throughput) and observe that the variability of interdeparture time decreases due to unbalancing. Further, we investigate a more realistic case that considers the assembly time as a function of the number of components. Next, we consider a manufacturing environment where process improvement activities require use of the productive capacity of the firm in addition to other investments. Thus, the firm must allocate its productive capacity between production activities and improvement activities. The output of production activities is used to meet customer demand. Process improvement activities improve the quality of the output, which in turn leads to lower quality related costs (both internal and external) and possibly lower per-unit production cost. A continuous time, finite horizon, profit maximization, resource allocation model is developed to find an optimal time path for process improvement activities and production activities. Lastly, we consider acceptance sampling plans using Bayesian techniques and subject to measurement error. Prior work has shown that Taguchi 2 ̆ 7 s quadratic loss function is superior to the step-loss function and Bayesian sampling plans are more appropriate. Impact of measurement error is significant and the assumption of no measurement error should be reconsidered. In this section, we develop two Bayesian lot-by-lot variable acceptance sampling plans, subject to measurement error, using the Taguchi loss function for measuring the loss due to imperfect quality. We conduct a sensitivity analysis of the models with respect to the cost of rejection or rework, the quadratic cost of quality, the correlation between the observed deviation and the <b>actual</b> <b>deviation</b> of the performance variable, and the measurement error, and obtain some interesting counterintuitive results. These models incorporate the concept of continuous improvement of productivity and quality through learning. ...|$|E
40|$|The diploma work {{presents}} the results of analysis of two geological cross sections of the Trojane tunnel: the cross section 80 + 300 in km and the cross section 80 + 250 in km, both located in the area near the Trojane restaurant. The numerical analysis of both geological cross sections was carried out by the finite element method with the programme Phase 2 v. 6. 0 using the Mohr-Coulomb's constitutive material model. At the beginning of the diploma work the tunnel Trojane is presented, the results of the actual geological-geotechnical research on the area of the tunnel are summarized, the technology for the construction of the Trojane tunnel is described, and the procedure of geotechnical measurements accompanying the construction is presented. The first part of the numerical analysis was carried out with the purpose of closely approaching the measurement deviation in the profile of 80 + 300. By taking into account the characteristics of soil and rock which best describe the <b>actual</b> <b>deviations</b> in the profile 80 + 300, the second part analysed also the profile 80 + 250 with a bit different geometry. The achieved results are compared with measured deviations reached by Saša Miklavžin's diploma work using the programme Plaxis...|$|R
40|$|International audienceThis {{chapter is}} an {{introduction}} to the semiclassical approach for the Helmholtz equation in complex systems originating in the field of quantum chaos. A particular emphasis will be made on the applications of trace formulae in paradigmatic wave cavities known as wave billiards. Its connection with random matrix theory (RMT) and disordered scattering systems will be illustrated through spectral statistics. In this chapter we will particularly discuss how the global knowledge about ray dynamics in a chaotic billiard may be used to explain universal statistical features of the corresponding wave cavity, concerning spatial wave patterns of modes, as well as frequency spectra. These features are for instance embodied in notions such as the spatial ergodicity of modes and the spectral rigidity, which are indicators of particular spatial and spectral correlations. The spectral study can be done through the so-called trace formula based on the periodic orbits of chaotic billiards. From the latter we will derive universal spatial and spectral features in agreement with predictions of Random Matrix Theories, but also see that <b>actual</b> <b>deviations</b> from a universal behavior can be found, which carry information about the specific geometry of the cavity. Finally, as a first step towards disordered scattering systems, a description of spectra of cavities dressed with a point scatterer, will be given in terms of diffractive orbits...|$|R
5000|$|The new sum of squared {{deviations}} is computed {{by adding}} the previous respective sums of squared deviations. However, a third [...] "interaction term" [...] is needed because {{the two sets of}} squared deviations were computed with respect to different means, and hence the sum of the two underestimates the <b>actual</b> total squared <b>deviation.</b>|$|R
30|$|SUVmax are {{commonly}} used for threshold-dependent volume definition in a clinical setting. If the delineation is strictly based on SUVmax (i.e., relative threshold without background correction), these differences would also implicate corresponding MTV deviations as reported previously [12]. However, we refrained from volumetric analyses as such delineation methods may not reflect the clinical practice where more sophisticated algorithms with background correction or manual MTV delineation are required - especially in hepatic lesions [14, 25, 26]. Thus, the <b>actual</b> MTV <b>deviations</b> may be lower and less dependent on the TBR than the current results on SUVmax deviations suggest.|$|R
40|$|Manufacturers often {{express the}} {{performance}} of a 3 D imaging device in various non-uniform ways for the lack of internationally recognized standard requirements for metrological parameters able to identify the capability of capturing a real scene. For this reason several national and international organizations {{in the last ten years}} have been developing protocols for verifying such performance. Ranging from VDI/VDE 2634, published by the Association of German Engineers and oriented to the world of mechanical 3 D measurements (triangulation-based devices), to the ASTM technical committee E 57, working also on laser systems based on direct range detection (TOF, Phase Shift, FM-CW, flash LADAR), this paper shows the state of the art about the characterization of active range devices, with special emphasis on measurement uncertainty, accuracy and resolution. Most of these protocols are based on special objects whose shape and size are certified with a known level of accuracy. By capturing the 3 D shape of such objects with a range device, a comparison between the measured points and the theoretical shape they should represent is possible. The <b>actual</b> <b>deviations</b> can be directly analyzed or some derived parameters can be obtained (e. g. angles between planes, distances between barycenters of spheres rigidly connected, frequency domain parameters, etc.). This paper shows theoretical aspects and experimental results of some novel characterization methods applied to different categories of active 3 D imaging devices based on both principles of triangulation and direct range detection...|$|R
40|$|This paper {{examines}} {{the impact of}} competitive balance on attendance in Major League Baseball. Two types of competitive balance are included in a single-equation model of attendance: intra-seasonal balance and inter-seasonal balance. The metric used to calibrate {{the first is the}} ratio of the <b>actual</b> standard <b>deviation</b> of season win percents divided by the ideal standard deviation. Inter-seasonal balance is calibrated with Markov transitional probabilities of teams making the playoffs in consecutive seasons. The results indicate that intra-seasonal balance does not significantly impact attendance, and that inter-seasonal balance has significant but small impacts on attendance in the American League. Copyright © 2006 John Wiley & Sons, Ltd. ...|$|R
40|$|The aim of {{the present}} paper is to {{investigate}} the accuracy of levelling {{along the lines of}} the First Order Levelling Network of Greece as deduced from the misclosures of its loops. <b>Actual</b> standard <b>deviation</b> of the standardized circuit misclosures significantly larger than 1 reveals correlation of the individual height differences between consecutive bench marks along a given levelling line accompanied by various other systematic effects. Correlation also exists between forward and backward levelling runs. The detection of correlations neither leads to their source nor eliminates them. However, especially for a high degree of correlation, the investigation of their causes, and their inclusion in the network adjustment may be helpful for the analysis and an improvement of the results...|$|R
40|$|This paper {{discusses}} a novel wind generation balancing {{technique to}} improve renewable energy integration to the system. Novel individual hot water heater controllers were modelled {{with the ability}} to forecast and look ahead the required energy, while responding to electricity grid imbalance. Artificial intelligence and machine learning techniques were used to learn and predict energy usage. In this research wind power data was used in most cases to represent the supply side, where focus was on the <b>actual</b> generation <b>deviation</b> from plan. It proved to be possible to balance the generation and increase system efficiency while maintaining user satisfaction. The methods developed in this research are not limited to wind power balancing and can also be used with any other type of renewable generation source...|$|R
40|$|This work {{systematically}} {{investigates the}} factors that could pose negative influence on the transmit pulse performance at 9. 4 tesla. These factors are B 1 mapping inaccuracy, B 0 inhomogeneity, actual RF and gradient pulse deviations, among which B 1 mapping inaccuracy and <b>actual</b> RF pulse <b>deviations</b> are deemed as the major influence factor. Comparative measurements with and without consideration of these factors during the pulse design process, including one in vivo experiment, are performed...|$|R
