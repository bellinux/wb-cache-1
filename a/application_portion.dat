14|123|Public
25|$|Persuasive {{writing is}} the most rhetorically stylized. So {{although}} a brief states the legal issues, describes authorities, and applies authorities to the question—as does a memorandum—the brief's <b>application</b> <b>portion</b> is framed as an argument. The author argues for one approach to resolving the legal matter and does not present a neutral analysis.|$|E
5000|$|Under the Brazilian {{military}} regime from 1964 to 1985, torture {{was not only}} condoned, but also institutionalized. Methods and procedures for torture became such {{an integral part of}} the military government’s oppressive tactics that the military integrated classes on torture into their training program. Some of the defendants from the court documents were forced to participate in the practical <b>application</b> <b>portion</b> of the class; these individuals were tortured in front of the classroom of students as a pedagogical approach.The following list provides some of the methods the interrogators used while torturing the defendants:• The Parrot’s Perch• Electric Shock• Drowning• The Dragon’s Chair• The Ice Box• Insects and Animals (such as snakes, alligators, cockroaches) • Chemical Products• Physical Injury ...|$|E
5000|$|Along {{with the}} <b>application</b> <b>portion</b> of the framework, Mojo {{provides}} graphics, sound, and device input functionality on several targets. Mojo's device-input functionality is built to be [...] "virtualizable" [...] through its native implementations. An {{example of this}} being the framework's [...] "touch" [...] input functionality, which some targets [...] "virtualize" [...] using traditional mouse input. The same goes for various targets when handling mouse input. Similar examples {{of this can be}} found on mobile platforms for keyboard input. Mojo's input functionality also provides several forms of keyboard functionality, accelerometer support, and game controller support. Because of this framework's design philosophies, games made using Mojo tend to look identical when deployed to other platforms, despite sometimes being based on drastically different technologies.|$|E
40|$|Quantitative {{exploitation}} of meteorological data from geosynchronous satellites {{is starting to}} move from the laboratory to operational practice. Investigations of the data <b>applications</b> <b>portion</b> of the total meteorological satellite system include: (1) tropospheric wind shear and the related severe storm circulations; (2) kinematic properties of the tropical atmosphere as derived from cloud motion vectors; (3) application of a geostationary satellite rake system to measurements of rainfall; and (4) pointing error analysis of geosynchronous satellites...|$|R
40|$|The Computational Aerosciences (CAS) {{project of}} the High Performance Computing and Communications program is {{presented}} and discussed. The main emphasis of this presentation is on the <b>applications</b> <b>portion</b> of the CAS program, which includes a High-Speed Civil Transport element, a High Performance Aircraft element, a NASP-Derived Vehicle element, and an Aerobraking element. Two major thrusts of this program are the enhancement of simulation capabilities using multidiscipline formulations and the improvement in processing efficiency via massive parallel computer hardware. Current activities in these two areas at Ames, Langley and Lewis, are presented and discussed...|$|R
40|$|Self-reference, {{far from}} being just a logician's and a philosopher's puzzle is, in fact, a central feature of human {{language}} and reason. It, thus, seems natural that intelligent machines {{will also have to}} deal with the issue of self-reference. We discuss some of the formal problems, and potential solutions and <b>applications.</b> <b>Portions</b> of this essay are descriptive in nature, portions prescriptive. We are involved in the development of some of the ideas in the relevant literature, and make no apology for injecting a certain subjective note into the text, as opposed to forcing a false objectivity. We have also freely drawn on portions of essays written by one of the authors...|$|R
40|$|The method {{involves}} {{forming a}} core layer (1) comprising a support portion (40) and a functional portion (11) in an additive manufacturing process. The function portion {{is comprised of}} a full cross-section or a hollow cross-section (13), with a force <b>application</b> <b>portion</b> (50). The support portion is integrally connected with the functional portion. A bore (100) is formed of a triangular, honeycomb, oval, circular and/or rectangular shaped open-cell foam housing for receiving a functional element in the functional portion. An independent claim is included for a light component...|$|E
40|$|The Visual Staging of Audio Plays {{explores the}} {{directing}} practice of radio dramas that are staged for viewing purposes {{rather than their}} typical solo-auditory purposes. The thesis is comprised of three separate parts: {{a brief history of}} theatrical sound, an introduction to radio drama theory and practice, and application. The <b>application</b> <b>portion</b> is a detailed first-person account of my personal experience staging It’s a Wonderful Life: A Live Radio Play by Joe Landry for TheatreVCU’s Mainstage winter special event in 2015. It is also in this section where I integrate history, theory, and practice to formulate technique for directing the genre for stage...|$|E
40|$|The magnetically {{suspended}} {{reaction wheel}} assembly (MSRWA) {{is the product}} of a development effort funded by the Air Force Materials Laboratory (AFML) at Wright Patterson AFB. The specific objective of the project was to establish the manufacturing processes for samarium cobalt magnets and demonstrate their use in a space application. The development was successful on both counts. The <b>application</b> <b>portion</b> of the program, which involves the magnetically suspended reaction wheel assembly, is emphasized. The requirements for the reaction wheel were based on the bias wheel requirements of the DSP satellite. The tasks included the design, fabrication, and test of the unit to the DSP program qualification requirements...|$|E
40|$|This volume {{has been}} {{divided into two}} parts: Geometry and <b>Applications.</b> The {{geometry}} <b>portion</b> of the book relates primarily to geometric flows, laminations, integral formulae, geometry of vector fields on Lie groups, and osculation; the articles in the <b>applications</b> <b>portion</b> concern some particular problems {{of the theory of}} dynamical systems, including mathematical problems of liquid flows and a study of cycles for non-dynamical systems. This Work is based on the second international workshop entitled "Geometry and Symbolic Computations," held on May 15 - 18, 2013 at the University of Haifa and is dedicated to modeling (using symbolic calculations) in differential geometry and its applications in fields such as computer science, tomography, and mechanics. It is intended to create a forum for students and researchers in pure and applied geometry to promote discussion of modern state-of-the-art in geometric modeling using symbolic programs such as Maple™ and Mathematica®, as well as presentation of new results. ...|$|R
30|$|The {{database}} in each stratum {{handles the}} basic information of sensor values {{themselves and the}} information on road surfaces, enabling the description of an application {{in accordance with the}} response speed and the computer power in each stratum. When considering the construction of several applications in the system configuration described above, it becomes possible to share the data management unit of the main server and make responses efficiently only by building <b>application</b> <b>portions</b> of each stratum. Furthermore, we consider that this system can make responses flexibly when there is a variation in measurement characteristics such as sampling time due to a change in terminal types, when the function as the application is improved by adding new information, or when the function deployment is modified among the client, the local server, and the main server.|$|R
40|$|Performance-based {{earthquake}} engineering requires a probabilistic treatment of potential failure modes {{in order to}} accurately quantify the overall stability of the system. This paper is {{a summary of the}} <b>application</b> <b>portions</b> of the probabilistic liquefaction triggering correlations proposed recently proposed by Moss and co-workers. To enable probabilistic treatment of liquefaction triggering, the variables comprising the seismic load and the liquefaction resistance were treated as inherently uncertain. Supporting data from an extensive Cone Penetration Test (CPT) -based liquefaction case history database were used to develop a probabilistic correlation. The methods used to measure the uncertainty of the load and resistance variables, how the interactions of these variables were treated using Bayesian updating, and how reliability analysis was applied to produce curves of equal probability of liquefaction are presented. The normalization for effective overburden stress, the magnitude correlated duration weighting factor, and the non-linear shear mass participation factor used are also discussed...|$|R
40|$|Computational {{simulation}} plays {{a central}} role in the engineering analysis and design of major bridge structures and accurate simulations are essential for the development of earthquake resistant and economical structural designs. This paper describes new methodologies and computational tools which have recently been developed for simulating earthquake ground motions and the seismic response of cable supported bridges. The simulation tools are described and an example application for an important long-span suspension bridge is demonstrated. The <b>application</b> <b>portion</b> of the study has particular focus on the potential damaging effects of long period displacement pulses and permanent ground displacements which can occur when a bridge is located in the near-field of a major earthquake fault...|$|E
40|$|AbstractThis paper {{provides}} two {{contributions to}} the study of developing and applying domain-specific modeling languages (DSMLS) to distributed real-time and embedded (DRE) systems—particularly those systems using standards-based QoS-enabled component middleware. First, it describes the Platform-Independent Component Modeling Language (PICML), which is a DSML that enables developers to define component interfaces, QoS parameters and software building rules, and also generates descriptor files that facilitate system deployment. Second, it applies PICML to an unmanned air vehicle (UAV) <b>application</b> <b>portion</b> of an emergency response system to show how PICML resolves key component-based DRE system development challenges. Our results show that the capabilities provided by PICML—combined with its design- and deployment-time validation capabilities—eliminates many common errors associated with conventional techniques, thereby increasing the effectiveness of applying QoS-enabled component middleware technologies to the DRE system domain...|$|E
40|$|This paper {{provides}} two {{contributions to}} the study of developing and applying domain-specific modeling languages (DSMLS) to distributed real-time and embedded (DRE) systems – particularly those systems using standards-based QoS-enabled component middleware. First, it describes the Platform-Independent Component Modeling Language (PICML), which is a DSML that enables developers to define component interfaces, QoS parameters and software building rules, and also generates descriptor files that facilitate system deployment. Second, it applies PICML to an unmanned air vehicle (UAV) <b>application</b> <b>portion</b> of an emergency response system to show how PICML resolves key component-based DRE system development challenges. Our results show that the capabilities provided by PICML – combined with its design- and deployment-time validation capabilities – eliminates many common errors associated with conventional techniques, thereby increasing the effectiveness of applying QoS-enabled component middleware technologies to the DRE system domain...|$|E
50|$|NovaThor was a {{platform}} consisting of integrated System on Chips (SoC) and modems for smartphones and tablets developed by ST-Ericsson, a 50/50 joint venture of Ericsson and STMicroelectronics established on February 3, 2009. ST-Ericsson also sold the SoCs (Nova) and the modems (Thor) separately. The <b>application</b> processor <b>portion</b> {{of the system}} was the successor of the previous Nomadik line from STMicroelectronics.|$|R
50|$|The optics in CPV modules {{accept the}} direct {{component}} of the incoming light and therefore must be oriented appropriately to maximize the energy collected. In low concentration <b>applications</b> a <b>portion</b> of the diffuse light from the sky can also be captured. The tracking functionality in CPV modules is used to orient the optics such that the incoming light is focused to a photovoltaic collector.|$|R
50|$|As in a two-winding transformer, {{the ratio}} of {{secondary}} to primary voltages is equal to {{the ratio of}} the number of turns of the winding they connect to. For example, connecting the load between the middle and bottom of the autotransformer will reduce the voltage by 50%. Depending on the <b>application,</b> that <b>portion</b> of the winding used solely in the higher-voltage (lower current) portion may be wound with wire of a smaller gauge, though the entire winding is directly connected.|$|R
40|$|The {{major focus}} of this two-part study will be to {{identify}} employable technologies for application in establishing a sustainable food-producing system. Part One: Research will explore the realm of sustainable agriculture [...] from mini-farming, to appropriate technologies, to deep ecology [...] {{in an attempt to}} relay its concepts and portray a design methodology for bringing about a comprehensive, wholely functional, food-production program as an agriculturally-oriented means of sustainability. Part Two: Application will demonstrate the results of employing these concepts at various levels of a sustainable community in Henry County, Indiana. It {{is important to note that}} the <b>application</b> <b>portion</b> of the study will be a team effort [...] a charrette headed by the Department of Landscape Architecture at Ball State University. And while my emphasis in the research portion of the study will be single, small-scale, self-reliant, predominately vegetative, food-producing systems; the objectives of the charrette have been predetermined, and will ultimately exceed those of the research portion to include three different scales and orientations towards sustainable agriculture-community demonstration site, sustainable farmstead site, and research-housing site. College of Architecture and PlanningThesis (B. L. A. ...|$|E
40|$|This {{study is}} {{performed}} {{to show the}} effects of Facebook in French language instruction as a tool. It seeks to analyze the activities of using Facebook {{in a position of}} learning French. To conduct this study, we set up a Facebook group with 23 students in French Preparatory Program and three teachers who lead the program in the Department of Translation and Interpreting, Faculty of Letters and Sciences, University of Mersin, in the study period 2014 - 2015 fall semester. This closed group, teachers shared topics of their courses, images, writings, videos and they want their students to answer to their questions in the group. The students are also participated to the activities making comments under the sharing, giving answers to asked questions, making themselves sharing on this platform. The course announcements were also shared in the group. Moreover, in the current <b>application</b> <b>portion</b> exemplary, extracts the students' work were made anonymous to protect the right to privacy. For the protection of the names and photos profiles have hidden. In the period of the application, the observation technique was used. At the end of the application a student survey was submitted as part of the copy of this study. For the analysis of survey data the content analysis technique was used...|$|E
40|$|First and {{foremost}} I {{have to acknowledge}} and thank my loving parents and my sister. Without their support – not only financial but emotional and spiritual – this whole adventure that is graduate school would have been impossible. Secondly, I must express my most sincere gratitude towards my advisor Dr. Manfred Huber and The University of Texas at Arlington’s Computer Science Department for believing in me and giving {{me the opportunity to}} succeed. I could not have asked for a better advisor than Dr. Huber, whose patience, wit and support have been paramount. It has been an honor and a privilege to collaborate and learn with him. I must thank both members of my committee, Drs. David Levine and Gergely Zaruba for devoting their time and energy to me in this process. I would like to thank all of my friends, both at UTA and at other universities, for the gift of intelligent conversation, discussion and discourse. I was able to bounce ideas off of them and commiserate with them over our shared trials and tribulations. Specifically I owe a debt of gratitude to Giles D’Silva, not only for pointing me towards Microsoft Robotics Studio, which ended up being an invaluable tool in the <b>application</b> <b>portion</b> of this project, but also for the many games of FIFA soccer and Wii Tennis, which kept me sane throughout the final semester. iv I would be remiss if I didn’t thank various professors from my alma mate...|$|E
50|$|To the end-user, folder {{redirection}} generally {{does not}} appear to function any differently from using a normal standalone computer. Redirecting the user's My Documents and Desktop to be accessed directly on a file server are the first two big steps for speeding up roaming profiles. However, as 3rd party software have begun to store more and more data in the <b>Application</b> Data <b>portion</b> of the roaming profile, it has also become useful to redirect that to also be accessed directly on the server.|$|R
2500|$|Infighting {{at the new}} {{joint company}} was legendary, and the {{problems}} with Pink within Apple soon appeared to be minor in comparison. Apple employees made T-shirts, graphically displaying their prediction that {{the result would be}} an IBM-only project, a prediction which came true on December 19, 1995, when Apple officially pulled out of the project. IBM continued working with Taligent, and eventually released its <b>application</b> development <b>portions</b> under the new name [...] "CommonPoint". This saw little interest and the project disappeared from IBM's catalogs within months.|$|R
40|$|RoboCup {{simulated}} soccer presents {{many challenges}} to machine learning (ML) methods, including a large state space, hidden and uncertain state, multiple agents, and long and variable {{delays in the}} effects of actions. While {{there have been many}} successful ML <b>applications</b> to <b>portions</b> of the robotic soccer task, it appears to be still beyond the capabilities of modern machine learning techniques to enable a team of 11 agents to successfully learn the full robotic soccer task from sensors to actuators. Because the successful <b>applications</b> to <b>portions</b> of the task have been embedded in dierent teams and have often addressed dierent subtasks, they have been dicult to compare. We put forth keepaway soccer as a domain suitable for directly comparing dierent machine learning approaches to robotic soccer. It is complex enough that it can't be solved trivially, yet simple enough that complete machine learning approaches are feasible. In keepaway, one team, keepers," tries to keep control of the ball {{for as long as possible}} despite the eorts of takers. " The keepers learn individually when to hold the ball and when to pass to a teammate, while the takers learn when to charge the ball-holder and when to cover possible passing lanes. We fully specify the domain and summarize some initial, successful learning results...|$|R
40|$|This {{article is}} a {{presentation}} of the method and application of the Clarifi-cation Group (C-group). The C-group is based on social-psychological and sociological knowledge and is focused on intergroup phenomena, socio-identities, stereotypy, and prejudice. In {{the first part of this}} article, the authors delineate the nature of the C-group; its philosophical and societal value bases; and the major needs and concerns met by the C-group approach (in and of itself and in comparison with other human relations approaches). The major goals of the C-group are explicated in detail. A major part of the paper is concerned with C-group application for the use of trainers and group practitioners. The <b>application</b> <b>portion</b> includes a description and analysis of the major structured activities utilized in the C-group laboratory and some general considerations con-cerning C-group design and application. This paper was written to serve two purposes: (1) to acquaint the theoretically inclined practitioner and the applied social psychologist with the Clarification-Group (C-group) approach to laboratory learn-ing, its background, rationale, and scope of application and (2) to enable professionals in the training field to consider, evaluate, and utilize the C-group approach in their work. The C-group was developed at the Boston University Center for Applied Social Science by Max Birnbaum in collaboration with Ken-neth D. Benne, James Small, and various other colleagues. Its focus is on the dynamics of intergroup relations, and in this respect it differs from other small-group methodologies that focus on individual and interpersonal dynamics. Throughout the last twelve years, the C-group and a family of supporting technologies consistent with its learning objectives have been utilized in each of the Center’s annua...|$|E
40|$|Abstract. RoboCup {{simulated}} soccer presents {{many challenges}} to machine learning (ML) methods, including a large state space, hidden and uncertain state, multiple agents, and long and variable {{delays in the}} effects of actions. While {{there have been many}} successful ML <b>applications</b> to <b>portions</b> of the robotic soccer task, it appears to be still beyond the capabilities of modern machine learning techniques to enable a team of 11 agents to successfully learn the full robotic soccer task from sensors to actuators. Because the successful <b>applications</b> to <b>portions</b> of the task have been embedded in different teams and have often addressed different subtasks, they have been difficult to compare. We put forth keepaway soccer as a domain suitable for directly comparing different machine learning approaches to robotic soccer. It is complex enough that it can’t be solved trivially, yet simple enough that complete machine learning approaches are feasible. In keepaway, one team, “the keepers, ” tries to keep control of the ball {{for as long as possible}} despite the efforts of “the takers. ” The keepers learn individually when to hold the ball and when to pass to a teammate, while the takers learn when to charge the ball-holder and when to cover possible passing lanes. We fully specify the domain and summarize some initial, successful learning results. ...|$|R
40|$|Partitioning {{is often}} used to support better graph drawing; in this paper, we {{describe}} an interactive system in which graph drawing is used to support better partitioning. In our system the user is presented with a drawing of a current network partitioning, and is responsible for choosing appropriate optimization procedures and for focusing their <b>application</b> on <b>portions</b> of the network. Our pilot experiments show that our network drawings succeed in conveying some of the information needed by the human operator to steer the computation effectively, and suggest that interactive, human-guided search may be a useful alternative to fully automatic methods for network and graph partitioning. ...|$|R
40|$|The {{clinical}} {{applications of}} jet ventilation (JV) in ear, nose, and throat surgery can be best {{understood by the}} characteristics that distinguish this form of ventilation from conventional positive pressure ventilation. By definition, JV {{is based on the}} <b>application</b> of gas <b>portions</b> under high pressure through an unblocked catheter into the airway, which is open to the ambient air. Beneficial opportunities arise in JV, which otherwise are not available in regular ventilation...|$|R
40|$|Background: Tunnel {{widening}} (TW) after {{anterior cruciate}} ligament (ACL) reconstruction can be a serious complication, and there is controversy over how to prevent it. This study aimed to suggest surgical approaches to prevent TW using an allo-Achilles tendon graft, and then to evaluate TW after these surgical tips were applied. Materials and Methods: Sixty two patients underwent ACL reconstruction with an allo-Achilles tendon graft. Four surgical approaches were used: Making a tibial tunnel by bone impaction, intraarticular reamer <b>application,</b> bone <b>portion</b> <b>application</b> for the femoral tunnel, and an additional bone plug application for the tibial tunnel. After more than 1 -year, followup radiographs including anteroposterior and lateral views were taken in 29 patients encompassing thirty knees. The diameter of the tunnels at postoperation day 1 (POD 1) and at followup was measured and compared. Results: In 18 knees (60 %), there were no visible femoral tunnel margins on the radiographs at POD 1 or followup. In the other 12 cases, which had visible femoral tunnel margins on followup radiographs, the mean femoral tunnel diameter was 8. 6 mm. In the tibial tunnel, the mean diameters did not increase on all three levels (proximal, middle, and distal), {{and there was no}} statistically significant difference between the diameters at POD 1 and followup. Conclusion: The suggested tips for surgery involving an allo-Achilles tendon graft can effectively prevent TW after ACL reconstruction according to this case series. These surgical tips can prevent TW...|$|R
50|$|Because a kernel {{implementation}} of the TCP stack {{can be seen as}} a bottleneck, the protocol is typically implemented in hardware RDMA network interface controllers (rNICs). As simple data losses are rare in tightly coupled network environments, the error-correction mechanisms of TCP may be performed by software while the more frequently performed communications are handled strictly by logic embedded on the rNIC. Similarly, connections are often established entirely by software and then handed off to the hardware. Furthermore, the handling of iWARP specific protocol details is often isolated from the TCP implementation, allowing rNICs to be used for both as RDMA offload and TCP offload (in support of traditional sockets based TCP/IP <b>applications).</b> The <b>portion</b> of the hardware implementation used for implementing the TCP protocol is known as the TCP Offload Engine (TOE).|$|R
40|$|A {{laminated}} piezoelectric transformer {{is provided}} using the longitudinal vibration modes for step-up voltage conversion <b>applications.</b> The input <b>portions</b> are polarized to deform in a longitudinal plane and are bonded to an output portion. The deformation of the input portions is mechanically coupled to the output portion, which deforms {{in the same}} longitudinal direction relative to the input portion. The output portion is polarized in the thickness direction relative its electrodes, and piezoelectrically generates a stepped-up output voltage...|$|R
40|$|A {{modular design}} for {{combining}} piezoelectric transformers is provided for high voltage and high power conversion <b>applications.</b> The input <b>portions</b> of individual piezoelectric transformers are driven {{for a single}} power supply. This created the vibration and the conversion of electrical to electrical energy from the input to {{the output of the}} transformers. The output portions of the single piezoelectric transformers are combining in series and/or parallel to provide multiple outputs having different rating of voltage and current...|$|R
40|$|This preclosure {{work plan}} {{presents}} {{a description of}} the PUREX Facility, the history of the waste managed, and addresses transition phase activities that position the PUREX Facility into a safe and environmentally secure configuration. For purposes of this documentation, the PUREX Facility does not include the PUREX Storage Tunnels (DOE/RL- 90 / 24). Information concerning solid waste management units is discussed in the Hanford Facility Dangerous Waste Permit <b>Application,</b> General Information <b>Portion</b> (DOE/RL- 91 - 28, Appendix 2 D) ...|$|R
40|$|The overall {{performance}} level for an NDE operation {{is dependent on}} the NDE material, equipment, processes (methodology) and human skills applied to the operation. It is important to understand and consider human factors elements and contributions to NDE applications in the improvement of applications, in the design and validation of new <b>applications,</b> in automating <b>portions</b> of task performance, and in the development of modeling tools for the prediction of task performance for existing and new applications...|$|R
30|$|CAVLD is {{considered}} to be a bottleneck in both parallel and sequential decoders of H. 264 /AVC because of its bit-by-bit dependency. Furthermore, as video resolution is increasing based on market demands, the common bitrate for high-resolution videos such as full high definition (full HD) now ranges from 10 to 20 megabits per second (Mbps) for high-quality applications. For such high-bitrate <b>applications,</b> a <b>portion</b> of the complexity for the entropy decoding would significantly increase. For fast CAVLD of H. 264 /AVC, there are several algorithms such as table mapping, which can improve the overall latency of entropy decoding without parallelism. In order to develop an efficient table mapping approach for multi-stage pipelined processors, it is necessary to consider not only the memory requirements and the number of memory accesses, but also the number of conditional branches.|$|R
40|$|INTRODUCTION Several tools may be {{used for}} {{performance}} analysis. Coverage analysis tools [1] obtain a precise count of instructions executed in a program, a strong indication of the processing time required. Profilers [2] use sampling at regular time intervals to statistically determine the time spent in each function. Some profilers use hardware performance counters [3] to obtain program counter samples correlated to metrics other than CPU time, for instance cache misses, mispredicted branches or pipeline stalls. For CPU intensive applications, these tools produce useful and reasonably accurate measurements. However, this may not be sufficient for <b>applications,</b> or <b>portions</b> thereof, where interactions between one or more processes and the operating system impact significantly the performance. In many cases, resources managed by the operating system may indeed strongly affect the performance. This includes disk heads scheduling, I/O buffer cache, virtual memory management, signal del...|$|R
