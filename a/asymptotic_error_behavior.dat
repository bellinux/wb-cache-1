5|1522|Public
40|$|Abstract. In {{this project}} we {{initiate}} {{an investigation of}} the applicability of Quasi-Monte Carlo methods to lattice field theories in order to improve the <b>asymptotic</b> <b>error</b> <b>behavior</b> of observables for such theories. In most cases the error of an observable calculated by averaging over random observations generated from an ordinary Monte Carlo simulation behaves like N− 1 / 2, where N is the number of observations. By means of Quasi-Monte Carlo methods it is possible to improve this behavior for certain problems to up to N− 1. We adapted and applied this approach to simple systems like the quantum harmonic and anharmonic oscillator and verified an improved error scaling. 1...|$|E
40|$|In {{this project}} we {{initiate}} {{an investigation of}} the applicability of Quasi-Monte Carlo methods to lattice field theories in order to improve the <b>asymptotic</b> <b>error</b> <b>behavior</b> of observables for such theories. In most cases the error of an observable calculated by averaging over random observations generated from an ordinary Monte Carlo simulation behaves like 1 /sqrt(N), where N is the number of observations. By means of Quasi-Monte Carlo methods it is possible to improve this behavior for certain problems to up to 1 /N. We adapted and applied this approach to simple systems like the quantum harmonic and anharmonic oscillator and verified an improved error scaling. Comment: 12 pages, 2 figures, conference proceedings CCP 201...|$|E
40|$|We {{investigate}} {{the applicability of}} quasi-Monte Carlo methods to Euclidean lattice systems for quantum mechanics {{in order to improve}} the <b>asymptotic</b> <b>error</b> <b>behavior</b> of observables for such theories. In most cases the error of an observable calculated by averaging over random observations generated from an ordinary Markov chain Monte Carlo simulation behaves like N− 1 / 2 N− 1 / 2, where NN is the number of observations. By means of quasi-Monte Carlo methods it is possible to improve this behavior for certain problems to N− 1 N− 1, or even further if the problems are regular enough. We adapted and applied this approach to simple systems like the quantum harmonic and anharmonic oscillator and verified an improved error scaling...|$|E
40|$|AbstractThis paper gives {{a survey}} of the results known to date about {{quadrature}} formulas obtained by variable transformation followed by an application of the trapezoidal rule with an equal mesh size. It has been shown that a formula obtained by an appropriate transformation is in general efficient and also robust against the end point singularity. Various kinds of useful transformations together with the <b>asymptotic</b> <b>error</b> <b>behaviors</b> of the resulting quadrature formulas are summarized. In particular special emphasis is placed on an asymptotically optimal formula called the double exponential formula, abbreviated as the DE-rule, which is characterized by the double exponential decrease of its weights in the neighborhood of the end points of the transformed interval of integration...|$|R
40|$|For {{an integer}} p [greater-or-equal, slanted] 0, Singh has {{considered}} {{a class of}} kernel estimators [latin small letter f with hook]~(p) of the pth order derivative [latin small letter f with hook](p) of a density [latin small letter f with hook] and showed how specializations {{of some of the}} results there improve the corresponding existing results. In this paper these improved estimators are examined on a global measure of quality of an estimator, namely, the mean integrated square <b>error</b> (MISE) <b>behavior.</b> An upper bound, which can not be tightened any further for a wide class of kernels, is obtained for MISE ([latin small letter f with hook]~(p)). The exact asymptotic value for the same is also obtained. Under two alternative conditions, weaker than those assumed for the two results mentioned above, convergence of MISE ([latin small letter f with hook]~(p)) to zero is proved. Specializations of some of the results here improve the corresponding existing results by weakening the conditions, sharpening the rates of convergence or both. estimates density derivatives of a density mean integrated square <b>error</b> <b>asymptotic</b> <b>behavior...</b>|$|R
50|$|Discriminative {{classifiers}} {{have lower}} <b>asymptotic</b> <b>error</b> than generative ones; however, research by Ng and Jordan {{has shown that}} in some practical cases naive Bayes can outperform logistic regression because it reaches its <b>asymptotic</b> <b>error</b> faster.|$|R
40|$|Compressive sensing {{theory has}} {{demonstrated}} that sparse signals can be re-covered from {{a small number of}} random linear measurements. However, for practical purposes, like storage, transmission, or processing with modern dig-ital equipment, continuous-valued compressive sensing measurements need to be quantized. In this thesis we examine the topic of optimal quantization of compressive sensing measurements under reconstruction with message-passing algorithms by following the work on generalization of relaxed belief propagation (BP) for arbitrary measurement channels. Relaxed BP is an iterative reconstruction algorithm proposed for the task of estimation from random linear measurements. It was inspired by the traditional belief propa-gation algorithm widely used in decoding of low-density parity-check (LDPC) codes. One of the aspects that makes relaxed belief propagation so appealing is the state evolution framework, which predicts <b>asymptotic</b> <b>error</b> <b>behavior</b> of the algorithm. We utilize the predictive capability of the framework to design mean-square optimal scalar quantizers under relaxed BP signal re-construction. We demonstrate that error performance of the reconstruction can be significantly improved by using state evolution optimized quantiz-ers, compared to quantizers obtained via traditional design schemes. We finally propose relaxed BP as a practical algorithm for reconstruction from measurements digitized with binned quantizers, which further improve error performance of the reconstruction...|$|E
40|$|The {{mean square}} error {{performance}} of simple polynomial interpolators is analyzed for wide-sense stationary signals subjected to randomly timed sampling represented by stationary point processes. This performance is expressed in dimensionsless parametric terms, with emphasis on <b>asymptotic</b> <b>error</b> <b>behavior</b> at high dimensionless sampling rates [gamma]. The form of the asymptotic error expression, and particularly its dependence on [gamma], is shown to {{vary according to the}} number of points utilized, together with the differentiability properties of the signal. One point extrapolation yields a {{mean square error}} varying with [gamma]- 2 if the signal is differentiable, and as [gamma]- 1 if the signal is not. Similarly, two-point (polygonal) interpolation error exhibits linearity in [gamma]- 4, [gamma]- 3 or [gamma]- 2, according as the signal is twice, exactly once, or nondifferentiable. Specific examples are offered to furnish insight into actual error magnitudes. It is shown, for instance, that introduction of jitter in the sampling sequence increases the error by only a negligible amount. Exponential decay of the sample values is compared with stepwise holding; little is gained for a nondifferentiable signal, while for a differentiable signal the error performance deteriorates from [gamma]- 2 to [gamma]- 1 at high sampling rates. When more than two points are used in a polynomial fitting recovery scheme, specific computations or error become excessively difficult. However, it is proved that the asymptotic mean square error varies with [gamma]- 2 n when n points are utilized, and the signal is continuously differentiable at least n times. Finally, we compare the mean square errors of one and two sample schemes as described above with those attained by causal (extrapolating) and noncausal (interpolating) Wiener-Kolmogorov optimal filters. We demonstrate nontrivial instances in which the Wiener-Kolmogorov mean square error varies as [gamma]- 1 / 2, so that any of the simple recovery schemes considered exhibits superior performance at high sampling rates. This is explained by noting that the latter represent time-varying filters, whereas the Wiener-Kolmogorov filter is timeinvariant...|$|E
40|$|AbstractThe Laguerre {{family of}} {{iteration}} functions for finding multiple zeros is considered. This family is algebraically {{equivalent to the}} multiple zero counterpart of Hansen–Patrick family. The <b>asymptotic</b> <b>error</b> constant for the Laguerre family is given. The magnitude of <b>asymptotic</b> <b>error</b> constants for cubically convergent iteration functions for finding multiple real zeros of real functions are compared. Numerical examples are given...|$|R
40|$|AbstractIn this paper, the {{eigenvalue}} {{approximation of}} a compact integral operator {{with a smooth}} kernel is discussed. We propose <b>asymptotic</b> <b>error</b> expansions of the iterated discrete Galerkin and iterated discrete collocation methods, and <b>asymptotic</b> <b>error</b> expansion of approximate eigenvalues. We then apply Richardson extrapolation to obtain higher order super-convergence of eigenvalue approximations. Numerical examples are presented to illustrate the theoretical estimate...|$|R
2500|$|... then [...] as [...] {{goes from}} [...] to [...] converges to [...] of order , with <b>asymptotic</b> <b>error</b> {{constant}} ...|$|R
30|$|In our work, {{we first}} present a {{theoretical}} {{comparison of the}} mean-squared error of MH-MCMC and RDS estimators. It was observed that in many practical cases RDS outperforms MH-MCMC in terms of <b>asymptotic</b> <b>error.</b> We confirm this observation here using theoretical expressions for the <b>asymptotic</b> mean-squared <b>errors</b> of the two estimators. Then, we introduce a novel estimator for the network average based on reinforcement learning (RL). By way of simulations on real networks, we demonstrate that, with a good choice of cooling schedule, RL can achieve similar <b>asymptotic</b> <b>error</b> performance to RDS but its trajectories have smaller fluctuations.|$|R
40|$|AbstractGiven {{alternative}} methods with identical order of convergence for solving the polynomial equation -(z) = 0, the method with the smaller <b>asymptotic</b> <b>error</b> constant might {{be assumed to}} be superior {{in terms of the}} number of iterations required for convergence. We present empirical evidence for a parameterized class of methods of second order showing that a parameter choice which does not correspond to the minimal <b>asymptotic</b> <b>error</b> constant may nevertheless be superior in practice...|$|R
40|$|AbstractWe study <b>asymptotic</b> <b>errors</b> of {{algorithms}} for computing {{the global}} maximum {{of any real}} function defined on the s-dimensional unit cube whose (r − 1) st derivative exists and satisfies a Lipschitz condition. We prove that the <b>asymptotic</b> <b>error</b> of any algorithm that uses adaptive linear information cannot tend to zero essentially faster than n−r/s. This rate of convergence {{can be achieved by}} spline-type algorithms which use nonadaptive function evaluations at equispaced points...|$|R
40|$|A {{framework}} for Iterative Learning Control (ILC) is {{proposed for the}} situation when the ILC algorithm {{is based on an}} estimate of the controlled variable obtained from an observer-based estimation procedure. Under the assumption that the ILC input converges to a bounded signal, a general expression for the <b>asymptotic</b> <b>error</b> of the controlled variable is given. The <b>asymptotic</b> <b>error</b> is then exemplified by an ILC algorithm applied to a flexible two-mass model of a robot joint...|$|R
40|$|AbstractWe {{consider}} integral approximations using equidistributed point sequences, which converge for Riemann integrable functions. Under certain conditions, Sobol (Soviet Math. Dokl. 14 (1973) 734) and Klinger (Computing 59 (1997) 223) showed convergence {{for some}} classes of singular functions. We focus on <b>asymptotic</b> <b>error</b> bounds {{and give a}} scheme for extensions of Sobol's results, thereby obtaining insight in the <b>asymptotic</b> <b>error</b> structure. Numerical examples are presented, validating the principle of “ignoring the singularity” and illustrating the use of extrapolation...|$|R
50|$|An <b>asymptotic</b> <b>error</b> {{estimate}} for N → ∞ is given byFurther terms in this error estimate are {{given by the}} Euler-Maclaurin summation formula.|$|R
40|$|We compare discriminative and {{generative}} {{learning as}} typied by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classiers {{are almost always}} to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation|which is borne out in repeated experiments|that while discriminative learning has lower <b>asymptotic</b> <b>error,</b> a generative classier may also approach its (higher) <b>asymptotic</b> <b>error</b> much faster. ...|$|R
40|$|A regularized {{classifier}} {{is proposed}} for a two-population classification problem of mixed continuous and categorical variables {{in a general}} location model(GLOM). The limiting overall expected error for the classifier is given. It {{can be used in}} an optimization search for the regularization parameters. For a heteroscedastic spherical dispersion across all locations, an <b>asymptotic</b> <b>error</b> is available which provides an alternative criterion for the optimization search. In addition, the <b>asymptotic</b> <b>error</b> can serve as a baseline for practical comparisons with other classifiers. Results based on a simulation and two real datasets are presented. Regularized discrimination Location linear discriminant function Spherically symmetric across-location dispersion Limiting expected overall <b>error</b> <b>Asymptotic</b> expansion...|$|R
40|$|AbstractA regularized {{classifier}} {{is proposed}} for a two-population classification problem of mixed continuous and categorical variables {{in a general}} location model(GLOM). The limiting overall expected error for the classifier is given. It {{can be used in}} an optimization search for the regularization parameters. For a heteroscedastic spherical dispersion across all locations, an <b>asymptotic</b> <b>error</b> is available which provides an alternative criterion for the optimization search. In addition, the <b>asymptotic</b> <b>error</b> can serve as a baseline for practical comparisons with other classifiers. Results based on a simulation and two real datasets are presented...|$|R
40|$|This article {{considers}} {{the use of}} adaptive ridge classification rules for classifying an observation as coming from one of two multivariate normal distributionsN([mu](1),Â [Sigma]) andN([mu](2),Â [Sigma]). In particular, the <b>asymptotic</b> expected <b>error</b> rates for a general class of these rules are obtained and are {{compared with that of}} the usual linear discriminant rule. adaptive ridge classification rule <b>asymptotic</b> <b>error</b> rate expansion linear discrimination multivariate normal distribution...|$|R
40|$|In [14, 8] Kurtz and Protter resp. Jacod and Protter {{specify the}} <b>asymptotic</b> <b>error</b> {{distribution}} of the Euler method for stochastic differential equations (SDEs) with smooth coefficients growing at most linearly. The required differentiability and linear growth of the coefficients rule out some popular SDEs as for instance the Cox-Ingersoll-Ross (CIR) model, the Heston model, or the stochastic Brusselator. In this article, we partially extend {{one of the fundamental}} results in [8], so that also the mentioned examples are covered. Moreover, we compare by means of simulations the <b>asymptotic</b> <b>error</b> distributions of the CIR model and the geometric Brownian motion with mean reversion...|$|R
40|$|<b>Asymptotic</b> <b>error</b> {{distribution}} for approximation of a stochastic integral {{with respect to}} continuous semimartingale by Riemann sum with general stochastic partition is studied. Effective discretization schemes of which <b>asymptotic</b> conditional mean-squared <b>error</b> attains a lower bound are constructed. Two applications are given; efficient delta hedging strategies with transaction costs and effective discretization schemes for the Euler-Maruyama approximation are constructed. ...|$|R
40|$|Abstract — In this work, {{we study}} the {{convergence}} {{behavior of a}} modified Newton's method based on Simpson integral rule. The convergence properties of this method for solving equations which have simple roots have been discussed {{and it has been}} shown that it converges cubically to simple roots. And the values of the corresponding <b>asymptotic</b> <b>error</b> constants of convergence are determined. Theoretical results have been verified on the relevant numerical problems. A comparison of the efficiency of this method with other mean-based Newton's methods, based on the arithmetic, harmonic means and geometric means, is also included. Index Terms—Newton's method, mean-based method, order of convergence, <b>asymptotic</b> <b>error</b> constant, Simpson integral rule I...|$|R
40|$|AbstractIn this paper, {{we first}} discuss the constructions of three-point finite-difference {{approximation}} and a spline approximation {{for a class}} of singular two-point boundary value problems: x−α(xαu′) ′=f(x,u),u′(0 +) = 0,u(1) =A,α⩾ 1. The <b>asymptotic</b> <b>error</b> expansions of the numerical solutions of these problems are obtained. From these <b>asymptotic</b> <b>error</b> expansions {{we find that the}} finite-difference solution and the spline approximation solution approximate the exact solution from two sides. So we can obtain correct solution of high-order accuracy. Richardson's extrapolation can also be done and the accuracy of numerical solution can be improved greatly. We also present numerical examples which show the performance and efficiency of our methods...|$|R
40|$|The polynomial-trigonometric {{interpolation}} {{based on}} the Krylov approach for a smooth function given on [- 1, 1] is defined on the union of m shifted each other uniform grids with the equal number of points. The <b>asymptotic</b> <b>errors</b> of the interpolation in both uniform and L 2 metrics are investigated. It {{turned out that the}} corresponding errors can be minimized due to an optimal choice of the shift parameters. The study of <b>asymptotic</b> <b>errors</b> is {{based on the}} concept of the ”limit function ” proposed by Vallee-Poussin. In particular cases of unions of two and three uniform grids the limit functions are found explicitly and the optimal shift parameters are calculated using MATHEMATICA 4. 1 computer system. The parallel processing is investigated...|$|R
40|$|Accurate fault {{models are}} {{required}} to conduct the experiments defined in validation methodologies for highly reliable fault-tolerant computers (e. g., computers with a probability of failure of 10 to the - 9 for a 10 -hour mission). Described is a technique by which a researcher can evaluate the capability of the pin-level stuck-at fault model to simulate true <b>error</b> <b>behavior</b> symptoms in very large scale integrated (VLSI) digital circuits. The technique {{is based on a}} statistical comparison of the <b>error</b> <b>behavior</b> resulting from faults applied at the pin-level of and internal to a VLSI circuit. As an example of an application of the technique, the <b>error</b> <b>behavior</b> of a microprocessor simulation subjected to internal stuck-at faults is compared with the <b>error</b> <b>behavior</b> which results from pin-level stuck-at faults. The <b>error</b> <b>behavior</b> is characterized by the time between errors and the duration of errors. Based on this example data, the pin-level stuck-at fault model is found to deliver less than ideal performance. However, with respect to the class of faults which cause a system crash, the pin-level, stuck-at fault model is found to provide a good modeling capability...|$|R
40|$|In {{this paper}} we study the performance, {{in terms of}} the <b>asymptotic</b> <b>error</b> probability, of a user which {{communicates}} with a destination {{with the aid of a}} full-duplex in-band relay. We consider that the network is interference-limited and interfering users are distributed as a Poisson point process. In this case the <b>asymptotic</b> <b>error</b> probability is upper bounded by the outage probability (OP). We investigate the outage behavior for well-known cooperative schemes, namely, decode-and-forward (DF) and compress-and-forward (CF) considering fading and path loss. For DF we determine the exact OP and develop upper bounds which are tight in typical operating conditions. Also, we find the correlation coefficient between source and relay signals which minimizes the OP when the density of interferers is small. For CF, the achievable rates are determined by the spatial correlation of the interferences, and a straightforward analysis isn’t possible. To handle this issue, we show the rate with correlated noises is at most one bit worse than with uncorrelated noises, and thus find an upper bound on the performance of CF. These results are useful to evaluate the performance and to optimize relaying schemes in the context of full-duplex wireless networks. Index Terms Cooperative communication, interference, <b>asymptotic</b> <b>error</b> probability, outage probability, decod...|$|R
40|$|A multi-parameter {{family of}} three-step eighth-order {{iterative}} methods free from second derivatives are proposed {{in this paper}} to find a simple root of nonlinear algebraic equations. Convergence analysis as well as numerical experiments confirms the eighth-order convergence and <b>asymptotic</b> <b>error</b> constants. (C) 2009 Elsevier Inc. All rights reserved...|$|R
40|$|Abstract—A {{generalized}} {{concept of}} interference suppression is introduced for multiuser systems with a known memoryless transmit non-linearity, {{such as in}} the downlink amplifier of a satellite communication system. By considering the operation of commonly used multiuser detection techniques from a viewpoint of constellation structure, we extend these notions to nonlinear multiuser systems. We also analyze the performance of such multiuser detectors in terms of their <b>asymptotic</b> multiuser <b>error</b> exponents, which reduce to the asymptotic multiuser effective energies for the respective detectors in the absence of nonlinearity. The <b>asymptotic</b> conditional <b>error</b> exponent is introduced as a quantitative measure for evaluating nonlinear multiuser detectors based on the conditional probability of error at high signal-to-noise ratio (SNR). The optimal affine detector that maximizes the <b>asymptotic</b> <b>error</b> exponent for a given user is derive...|$|R
40|$|The CFC# {{algorithm}} is a recently introduced analytical {{solution for the}} blind MIMO channel identification problem, provided a certain spectral diversity holds for the stochastic inputs of the MIMO system. Here, we develop a theoretical study to derive the asymptotic performance of the CFC# algorithm, in terms of meansquare <b>error.</b> <b>Asymptotic</b> normality of the MIMO channel estimate is proved, and the <b>asymptotic</b> <b>error</b> covariance matrix derived. Computer simulation results are included to validate the theoretical expressions...|$|R
40|$|Abstract. A {{symmetric}} {{finite volume}} element scheme on quadrilateral grids is established {{for a class}} of elliptic problems. The <b>asymptotic</b> <b>error</b> expan-sion of finite volume element approximation is obtained under rectangle grids, which in turn yields the error estimates and superconvergence of the averaged derivatives. Numerical examples confirm our theoretical analysis...|$|R
40|$|AbstractWe propose an IMT-type {{quadrature}} formula which achieves {{the same}} <b>asymptotic</b> <b>error</b> estimate as the DE formula. The {{point of the}} idea is to optimize the parameters of the IMT-type transformation depending on the number of sampling points. We also show the performance of our IMT-type quadrature formula by numerical examples...|$|R
40|$|This paper {{shows how}} to compute <b>asymptotic</b> {{standard}} <b>errors</b> of the characteristic roots of a nonlinear econometric model. The system of simultaneous equations is linearized {{in the neighborhood}} of a given point, then characteristic roots and related standard errors are computed. Nonlinear econometric models; characteristic roots; eigenvalues; <b>asymptotic</b> standard <b>errors...</b>|$|R
40|$|We {{construct}} a biparametric family of fourth-order iterative methods to compute multiple roots of nonlinear equations. This method is verified to be optimally convergent. Various nonlinear equations confirm our proposed method with order of convergence {{of four and}} show that the computed <b>asymptotic</b> <b>error</b> constant agrees with the theoretical one...|$|R
40|$|Some upper bounds for MISE of multivariate kernel density estimators are obtained. It is shown, in particular, {{that under}} some {{regularity}} conditions, the actual error is always {{less than the}} <b>asymptotic</b> <b>error.</b> Key words: Density estimation, multivariate kernel estimator, mean integrated squared error, inequalities for characteristic functions, empirical characteristic functio...|$|R
