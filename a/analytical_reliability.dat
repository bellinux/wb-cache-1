60|83|Public
50|$|In 2003, Péan {{published}} La Face cachée du Monde (The Hidden Face of Le Monde) with Philippe Cohen. The book criticised the French newspaper's editors, {{claiming that}} they had purposefully {{turned their backs on}} Le Monde's past ethics. In particular, they alleged that Jean-Marie Colombani and Edwy Plenel had, amongst other things, shown partisan bias (concerning Corsica, for example) and engaged in financial dealings that compromised the paper's independence. These findings remain controversial, but attracted much attention in France and around the world {{at the time of their}} publication, not least because they impugned the <b>analytical</b> <b>reliability</b> of a paper whose emphasis is precisely on analysis and not simply straight reporting. Le Monde's subsequent difficulties have been attributed in part to this book.|$|E
40|$|This paper proposes {{for wind}} {{turbines}} (WTs) an <b>analytical</b> <b>reliability</b> method, used on other engineering systems, {{to compare the}} reliability of different turbine concepts. The main focus of the paper is to compare the reliability of geared generator and direct-drive concept WTs. Modification methods are also recommended for improving the availability of WTs and geared generator concept incorporating doubly fed induction generator...|$|E
40|$|<b>Analytical</b> <b>reliability</b> and {{clinical}} significance of laboratory data allows to consider {{them as a}} basis for diagnosis and treatment monitoring in therapeutic clinic. Laboratory biomarkers and predictors of use for risk assessment in clinical practice and disease prevention. Pre- and postanalytical stages of laboratory studies a large proportion of errors related to the actions of clinicians. Clinical laboratory diagnostics as a branch of medical science and the element {{of the health care system}} evolves, and this requires constant updating of knowledge in the discipline. </strong...|$|E
5000|$|Kröger, W. Critical Infrastructures at Risk, A Need for a New Conceptual Approach and Extended <b>Analytical</b> Tools, <b>Reliability</b> Engineering & System Safety, 93(12), 2008 ...|$|R
40|$|The paper {{considers}} {{calculation methods}} for reliability of  agricultural distribution power networks while using Boolean algebra functions and <b>analytical</b> method. <b>Reliability</b> of 10 kV overhead line circuits with automatic sectionalization points and automatic standby activation {{has been investigated}} in the paper. </p...|$|R
40|$|Flight {{critical}} {{computer based}} control systems designed for advanced aircraft must exhibit ultrareliable performance in lightning charged environments. Digital system upset can {{occur as a}} result of lightning induced electrical transients, and a methodology was developed to test specific digital systems for upset susceptibility. Initial upset data indicates that there are several distinct upset modes and that the occurrence of upset is related to the relative synchronization of the transient input with the processing sate of the digital system. A large upset test data base will aid in the formulation and verification of <b>analytical</b> upset <b>reliability</b> modeling techniques which are being developed...|$|R
40|$|A {{framework}} is proposed which addresses traditional reliability validation approaches consisting of life testing techniques which are inapplicable for digital flight control systems. A specific validation methodology is identified based on logical proofs, analytical modeling, and experimental testing. Research activities required to support continued development of validation technology are identified, and the validation procedure {{is driven by}} the reliability model obtained from the system description. The <b>analytical</b> <b>reliability</b> model is shown to be a proper abstraction of the system under consideration, and a proof of correctness of system design and system scheduler performance is proposed...|$|E
40|$|The use of {{simulation}} {{is described}} {{and it is}} contrasted to analytical solution techniques for evaluation of <b>analytical</b> <b>reliability</b> models. The role importance sampling plays in simulation of models of this type was also discussed. The simulator tool used for our analysis is described. Finally, {{the use of the}} simulator tool was demonstrated by applying it to evaluate the reliability of a fault tolerant hypercube multiprocessor intended for spacecraft designed for long duration missions. The reliability analysis was used to highlight the advantages and disadvantages offered by simulation over analytical solution of Markovian and non-Markovian reliability models...|$|E
40|$|Temperature {{control and}} volume {{measurement}} are two instrumental factors that, {{at different stages}} of the analytical cycle (both in manual and automatic work), {{have a significant impact on}} the quality (accuracy and precision) of the analytical result. This paper reports work carried out by two committees which prepared guidelines on the definition and control of these two instrumental factors of <b>analytical</b> <b>reliability.</b> "Temperature control " is the subject of an "ad hoc ’ committee on temperature control (designated AHCTC) set up under the auspices of the Standing Action Committee on Instrumentation (SA CI), a technical committee of the European Committee fo...|$|E
40|$|NASA {{made use}} of the <b>analytical</b> outputs of <b>reliability</b> people to make {{management}} decisions on the Apollo program. Such decisions affected {{the amount of the}} incentive fees, how much acceptance testing was necessary, how to optimize development testing, whether to approve engineering changes, and certification of flight readiness. Examples of such analysis are discussed and related to programmatic decisions. ...|$|R
40|$|International audienceA {{continuous}} liquid-solid interface {{model is}} introduced to calculate thermoelectric current. For comparison, the governing equations are solved by both a home-made code based on finite difference scheme and a commercial code COMSOL. The results are well {{in agreement with}} <b>analytical</b> calculation. The <b>reliability</b> of the model and the method are proved through a computation of some examples...|$|R
40|$|Accurate {{reliability}} estimates can {{be obtained}} by using software reliability models only in the later phase of software testing. However, for cost effective and timely, management prediction in the early phase is important. Non-homogenerous Poisson process (NHPP) models and Artificial Neural Network (ANN) models are the most important <b>Analytical</b> software <b>reliability</b> growth models. In this paper we study an approach to using past fault-related data with Wavelet Networks model to improve reliability predictions in the early testing phase. A numerical example is shown with both actual and simulated datasets. The analysis with example shows that the proposed approach works effectively in the early phase of software testing...|$|R
40|$|This paper {{proposes a}} {{probabilistic}} {{model for the}} reliability analysis of the creep and fatigue of materials. A new continuous creep-fatigue failure criterion function is introduced based on experimental data to facilitate <b>analytical</b> <b>reliability</b> approximations. This function is derived based on experimental data for any material, and is not restricted by any symmetry assumption used in current studies. A linear damage accumulation rule is used. The first-order reliability method and Monte Carlo simulation are used for probabilistic analysis. The influence of different random variables is measured with sensitivity index. The effects of the scatter of different random variables on the creep-fatigue life are studied...|$|E
40|$|Summary: A new enzymatic {{method for}} the {{determination}} of cholesterol in serum and plasma was evaluated in 8 separate laboratories in comparison with routine and reference methods. Investigation of the <b>analytical</b> <b>reliability</b> in the 2 - 26 mmol/ 1 measurement ränge showed the following results: 1. At the set reading points (10 min at 25 °C and 5 min at 37 °C) the reaction shows complete Substrate conversion. The colour complex is stable {{over a period of}} 60 min. 2. The response to cholesterol is linear up to 26 mmol/ 1. 3. Precision within the series was 0. 6 — 2. 8 % in 20 determinations (coefficient of Variation) ...|$|E
40|$|Address for correspondence: 	V. Vassilev; 	Department of Clinical Laboratory and 	Clinical Immunology; 	Medical University; 	 1 Sv. G. Sofiiski str.; 	 1431 Sofia; 	Bulgaria; tel. 02 - 92 - 30 / 923 Tin {{is spread}} all over the world. Many studies have been related to the {{toxicity}} of the element and its compounds. In the last few decades, the essential character of tin has been established. The aim {{of the present study}} is to develop a direct method for tin analyses in human plasma using graphite furnace with Zeeman correction and matrix modification. The optimized furnace conditions and evaluation of the <b>analytical</b> <b>reliability</b> indicate that the atomic absorption assay is suitable for the purposes of toxicological analyses...|$|E
40|$|Reliability of {{transport}} equipment plays {{a crucial role}} in providing safety for passengers. Safety systems {{of transport}} equipment perform safety functions with assigned safety integrity levels (SIL). If the reliability of a safety system is not sufficient, it has to be improved till the required level. This can be done by improving maintenance, enhancement of diagnostics or by applying redundancy. To conclude that reliability value is sufficient (or not), it is necessary to calculate its value before and after reliability improvement. Such calculations can be done analytically or by a simulation approach. Usually simulation approach is time consuming for a large number of simulations. Small number of simulations leads to an error in the results. Therefore analytical methods are often welcomed by both – scientists and practitioners. This thesis investigates <b>analytical</b> methods of <b>reliability</b> calculation focusing on systems with degradation. <b>Analytical</b> formulas of <b>reliability</b> calculation have limitations for systems with degradation due to non-constant failure rates (in this thesis they are modelled by Weibull distribution). These limitations have been shown in the example of a braking system of moving walks in Chapter 3 : analytical methods are mainly applicable only to systems with constant failure rates {{especially in the case of}} redundant systems. Transport Engineering and Logistic...|$|R
40|$|A {{number of}} <b>analytical</b> {{software}} <b>reliability</b> {{models have been}} proposed for estimating the reliability growth of a software product. In this paper we present an Enhanced non-homogeneous Poisson process (ENHPP) model and show that previously reported NonHomogeneous Poisson Process (NHPP) based models, with bounded mean value functions, are special cases of the ENHPP model. The ENHPP model differs from previous models in that it incorporates explicitly the timevarying test-coverage function in its analytical formulation, and provides for defective fault detection and test coverage during the testing and operational phases. The ENHPP model is validated using several available failure data sets. This work {{was supported in part}} by the US AIR FORCE Rome Laboratory as a core project in the Center for Advanced Computing and Communication and by a contract from the Charles Stark Draper Laboratory. 1 Introduction The past several years have seen the formulation of numerous analytical models fo [...] ...|$|R
40|$|Fatigue {{reliability}} calculations during iterations of {{the design}} process have been demonstrated as practical using new analytical and numerical methods based on <b>analytical</b> cumulative-damage <b>reliability</b> tools (ܽ-functions). Effects of fatigue strength and usage distribution may be studied {{in the presence of}} load variation. However, to date, the development of the ܽ-functions has been centered on use of the Palmgren-Miner cycle-ratio summation rule for cumulative damage, also known as the Linear Damage Rule (LDR). This paper extends the ܽ-function toolbox for use in determining reliability based on the Manson-Halford Double Linear Damage Rule (DLDR). Examples demonstrate use of the method and charts are provided to illustrate the sensitivity of the cumulative-damage reliability problem to load and strength variations. For a given reliability, probabilistic DLDR methods demonstrate significant reductions in life when compared to LDR methods. The importance of understanding and applying the appropriate LDR, DLDR, or other material characterization is apparent...|$|R
40|$|Considering the {{prevalence}} of diabetes mellitus, the possibility of early and rapid progress of complications, {{a large number of}} undiagnosed cases and disappointing forecasts of the WHO on the prospects of diabetes mellitus spreading in the world, timely and accurate diagnosis of carbohydrate metabolism disorders is important. The glycated hemoglobin — an indicator that, while using the standardized methods, provides an integrated view of the glycemia level during {{a long period of time}} and allows timely detection of carbohydrate metabolism disorders. When using the glycated hemoglobin as a diagnostic criterion for detecting carbohydrate metabolism disorders or degree of diabetes mellitus compensation, the correct approach to the choice of the method of this index determination, considering its <b>analytical</b> <b>reliability,</b> is important...|$|E
40|$|A poly[dibenzo- 18 -crown- 6] {{exhibits}} good chemical stability, reusability, {{and faster}} rate equilibrium for {{the separation of}} Gd(III). Both uptake and stripping of metal ions were rapid, indicating a better accessibility of the complexing sites. The proposed method has been applied for chromatographic separation of Gd(III) by using picric acid as medium and poly[dibenzo- 18 -crown- 6] as stationary phase. The influences of picric acid concentration, different eluting agents, and so forth, were discussed and the optimum conditions were established. The breakthrough capacity of poly[dibenzo- 18 -crown- 6] for Gd(III) was 0. 572 ± 0. 01 [*]mmolg- 1 of crown polymer. The proposed method {{has been applied to}} sequential chromatographic separation of their binary and multicomponent mixtures. Gd(III) has been determined from real samples with good <b>analytical</b> <b>reliability...</b>|$|E
40|$|Abstract: Thermal {{ionization}} {{mass spectrometry}} (TIMS) {{was used to}} determine the concentration and isotope ratio of uranium contained in samples of soil and groundwater collected from Korea. Quantification of uranium in ground water samples was per-formed by isotope dilution mass spectrometry. A series of chemical treatment processes, including chemical separation using extraction chromatography, was applied to the soil samples to extract the uranium. No treatments other than filtration were applied to the groundwater samples. Isotopic analyses by TIMS showed that the isotope ratios of uranium in both the soil and water samples were indistinguishable from those of naturally abundant uranium. The concentration of uranium in the groundwa-ter samples was within the U. S. acceptable standards for drinking water. These results demonstrate the utility of TIMS for moni-toring uranium in environmental samples with high <b>analytical</b> <b>reliability...</b>|$|E
40|$|This is {{the second}} paper in a two-part series {{discussing}} how Regulator requirements for continuity of supply could be incorporated in the reliability analysis of existing electricity networks and future “smart grids”. Part 1 paper presents input data, parameters and models required for a comprehensive assessment of system reliability performance, including {{an overview of the}} overall and guaranteed standards of performance in the UK and Italy. This paper presents scenarios and results of both <b>analytical</b> and probabilistic <b>reliability</b> assessment procedures for the test network introduced in Part 1 paper...|$|R
40|$|A {{number of}} <b>analytical</b> {{software}} <b>reliability</b> {{models have been}} proposed to estimate the reliability growth of a software product. In this paper we present an Enhanced non-homogeneous Poisson process (ENHPP) model and assert that previously reported Non-Homogeneous Poisson Process (NHPP) based models, with bounded mean value functions, are special cases of the ENHPP model. The ENHPP model differs from previous models in that it incorporates explicitly the time-varying test-coverage function in its analytical formulation. The model also provides for defective fault detection and test coverage during the testing and operational phases. The ENHPP model thus provides a framework for the unification of the finite failure NHPP models through test coverage. In addition we validate the ENHPP model for different coverage functions by comparing their This work {{was supported in part}} by the US AIR FORCE Rome Laboratory as a core project in the Center for Advanced Computing and Communication and by a [...] ...|$|R
40|$|Abstract. One of today’s major {{problems}} {{is the lack}} of detailed and current information on the spot for fire brigades and other action forces (e. g. police). However, access to the distributed information (e. g. location maps of fire hydrants, building plans, air photos) is not always available and has to be requested via paper mail in advance. Particularly, preventive fire protection and reactive fire prevention depending on a high-quality information management, which allows them to arrange a more efficient operation. In order to solve this problem, this paper proposes a design of a secure and reliable role based concept, which permits fast access to confidential information. Furthermore, the technical challenge of the development of such a communication infrastructure is illustrated. A performance analysis of an exemplary Monitoring Service for a distributed IT-Federation is introduced, in which an <b>analytical</b> Boolean <b>Reliability</b> Model is presented as an approach to visualize the bottleneck of a decentralized system...|$|R
40|$|Complex topology, multi-state behaviour, {{component}} interdependencies {{and interactions}} with external phenomena are prominent attributes of many realistic systems. <b>Analytical</b> <b>reliability</b> evaluation techniques have limited applicability to such systems, and efficient simulation models are therefore required. In this paper, {{we present a}} simulation framework to simplify the availability assessment of these systems. It allows the tracking {{of changes in the}} performance levels of components, from which system performance is deduced by solving a set of flow equations. This framework is adapted to the availability modelling of an offshore plant with interdependencies, operated in the presence of limited maintenance teams and operational loops. The result is compared to a Monte Carlo simulation-based solution in literature that required the enumeration of the plant's cut sets. The proposed approach is shown to be more intuitive, robust to errors and require less human effort...|$|E
40|$|Evaluating the {{reliability}} of slopes against sliding failure {{is complicated by the}} fact that most slope soils are spatially variable. This means that instead of nice circular failure surfaces, slope failures tend to be more complex, following the weakest path or zones through the material. The finite element method is well suited to slope stability calculations since it allows the failure surface to seek out the weakest path through the soil. This paper weds the finite element method with random field simulations to perform a `random finite element method' (RFEM) analysis of a slope. The simulation results are used to validate a simplified <b>analytical</b> <b>reliability</b> model for slope stability which is based on harmonic averaging. The harmonic average is low-strength dominated, so that it captures the weakest path characteristics of slope failure and provides good estimates of the failure probability...|$|E
40|$|We have {{developed}} and validated an automated kinetic method for angiotensin-converting enzyme (EC 3. 4. 15. 1) on the Olli C + 0 analyzer, modified {{from that of}} Ronca-Testoni (Clin Chem 1983; 29 : 1 093 - 6) with N-[3 -(2 -furyl) -acryloyl]-L-phenylalanylglycylglycine used as substrate. We have deter-mined appropriate reaction conditions for the assay, verified the principal <b>analytical</b> <b>reliability</b> criteria (repeatibility, repro-ducibility, sensitivity), and established normal reference inter-vals (mean ± SD) for the enzyme’s activity, using serum of normal adults (100 ± 35 U/L, n = 150), newborns (130 ± 27 U/L, n = 10), women taking oral contraceptives (103 ± 30 UI L, n = 10), smokers (109 ± 38 UIL, n = 27), and patients with sarcoidosis (220 ± 48 U/L, n = 15). AddItIonal Keyphrases: enzyme activity. reference interval sarcoidosis variation,sourceof newborn...|$|E
40|$|Evaluation of <b>analytical</b> results <b>reliability</b> is of core {{importance}} as crucial decisions are taken with them. From the various methodologies {{to evaluate the}} fitness of purpose of analytical methods, overall measurement uncertainty estimation {{is more and more}} applied. Overall measurement uncertainty allows to combine simultaneously the remaining systematic influences to the random sources of uncertainty and allows assessing the reliability of results generated by analytical methods. However there are various interpretations on how to estimate overall measurement uncertainty, and thus various models for estimating it. Each model together with its assumptions has great impacts on the risks to abusively declare that analytical methods are suitable for their intended purpose. This review paper aims at i) summarizing the various models used to estimate overall measurement uncertainty, ii) provide their pros and cons, iii) review the main areas of application and iv) as a conclusion provide some recommendations when evaluating overall measurement uncertainty. Peer reviewe...|$|R
40|$|We {{evaluated}} {{the levels of}} essential elements as Cu, Cr, Fe and Zn, and toxic elements as Al, Ni, Pb and Cd {{in a total of}} 40 samples of different legumes and 56 samples of different nuts, that are widely consumed in Spain. These elements were determined in the samples mineralized with HNO 3 and V 2 O 5, using electrothermal atomic absorption spectrometry (ETAAS) as the <b>analytical</b> technique. <b>Reliability</b> of the procedure was checked by analysis of a certified reference material. No matrix effects were observed and aqueous standard solutions were used for calibration. In legumes, the levels ranged from 1. 5 – 5. 0 mg Cuyg, 0. 05 – 0. 60 mg Cryg, 18. 8 – 82. 4 mg Feyg, 32. 6 – 70. 2 mg Znyg, 2. 7 – 45. 8 mg Alyg, 0. 02 – 0. 35 mg Niyg, 0. 32 – 0. 70 mg Pbygand not detectable— 0. 018 mg Cdyg. In nuts, the level...|$|R
40|$|Abstract: Our {{research}} was conducted based {{on the development of}} some antitank missile CCD observer test system. Combining the function and working principal of a typical CCD observer, we undertook an in-depth study on the collection of the core output signal of the CCD observer, naming the video signal information, and the design of key software and hardware. Utilizing both digital and video collection technology, our study provides handling method of measuring angle precision, interference rejection as well as sensitivity. Using the most advanced technology, this system is characterized with a user-friendly interface, <b>analytical</b> visualization, and <b>reliability...</b>|$|R
40|$|Summary: The <b>analytical</b> <b>reliability</b> of any {{measurement}} procedure requires a specifically designed, coherent reference measurement System, including interrelated {{measurement procedure}}s and measurement Standards such äs reference materials. The latter may {{be characterized by}} three sets of characteristics. The general characteristics comprise origin, mode of production, physical state and phase, homogeneity, physical form, Container, additives, storage conditions, stability, and dangerous properties. The specific characteristics describe molecular composition, analyte, purity, matrix, quantity of interest (including scale), assigned value, and uncertainty of measurement. The additional characteristics concern {{the way in which}} values for other characteristics were obtained, the hierarchical position of the material, certificate, instructions for use, and intended function. The problem areas of reference materials comprise definition of the appropriate analyte, purification, matrix, assignment of values, and nomenclature. The present ambiguous terminology is presented and a systematic structure of descriptive names is proposed (tab. 1) ...|$|E
40|$|International audienceLow voltage {{operation}} and small device sizes reduce the critical charge {{stored in a}} SRAM cell making caches more susceptible to soft errors. To counter the problem, a plethora of protection schemes are being proposed to reduce the power and area overheads of cache protection. Accurate methods to measure and compare the effectiveness of such schemes are sorely needed. This paper introduces a unified reliability benchmarking framework with PARMA (Precise <b>Analytical</b> <b>Reliability</b> Model for Architecture). PARMA is a rigorous analytical framework to measure the failure rate {{in the presence of}} soft errors under any protection scheme. PARMA continuously and accurately accounts for the increase of failure density due to soft errors affecting program behavior during an execution. We have implemented the PARMA framework on top of a cycle-accurate simulator to benchmark Level- 2 cache failure rates for a set of CPU 2000 benchmarks. Three protection schemes are compared: parity, word-level 1 -bit ECC and block-level 1 -bit ECC...|$|E
40|$|One of {{the motivations}} for specifying {{software}} architectures explicitly {{is the better}} prediction of system quality attributes. In this chapter we present an approach for determining the reliability of component- based software architectures. Our method is based on RADL (Rich Architecture Definition Language), an extension of DARWIN [16]. RADL places special emphasis on component interoperation and, in particular, on accounting {{for the effects of}} interoperation on system reliability. To achieve this, our methods use a notion of design-by-contract [19] for components, called parameterized contracts [26]. Our contracts involve finite state machines that allow software architects to define how a component&# 039;s reliability will react to a deployment environment. We show how a system, built from contractually specified components, can be understood in terms of Markov models, facilitating system reliability analysis. We illustrate our approach with an e-commerce example and report about empirical measurements which confirm our <b>analytical</b> <b>reliability</b> prediction by means of monitoring in our reliability testbed...|$|E
40|$|This {{paper is}} part {{one of a}} two-part series {{discussing}} how Regulator requirements for continuity of supply could be incorporated in the reliability analysis of existing electricity networks and future “smart grids”. The paper uses examples of overall and guaranteed standards of performance from the UK and Italy, specifying requirements that network operators should satisfy with respect to excessively long and/or too frequent supply interruptions. Besides the relevant Regulator requirements, this paper presents input data, parameters and models required for comprehensive reliability assessment, while Part 2 paper presents scenarios and results for test network based on both <b>analytical</b> and probabilistic <b>reliability</b> procedures...|$|R
40|$|Summary: The Boehringer Mannheim Hitachi 911 is a {{selective}} analyzer for 35 different methods including 3 ion-selective electrode (ISE) methods. We have evaluated this analyzer primarily to obtain objective information on its applicability for routine urine analyses in our laboratory. We also implemented appropriate assays for various special serum- and whole blood-tests, some {{for the first}} time on the Hitachi 911 and some with modified settings. Analytical evaluation involved NCCLS EP 5 -T 2 (imprecision), NCCLS EP 6 -P (linearity), Krouwer 27 (multifactor) and Passing & Bablok (method comparison) evaluation protocols. With the exception of evidence of systematic erroneous sample predilution, overall results were favourable. Practicability of the Hitachi 911 was judged by simulating daily routine. During a period of two weeks, daily urine samples were rerun on the Hitachi 911, leading to a gain of about 50 % in total processing time. It was concluded that the Hitachi 911 meets the requirements in terms of <b>analytical</b> performance, <b>reliability,</b> versatility and speed for an analyzer to be used in a routine (urine) setting, while having a distinct role in special (serum/whole blood) measurements...|$|R
40|$|This paper {{presents}} a new method for predicting shape sensitivity of J-integral for {{a crack in}} a homogeneous, isotropic, and linear-elastic body subject to model-I loading condition. The method involves variational formulation of continuum mechanics, domain integral representation of J, and shape sensitivity analysis by direct differentiation. Numerical examples are presented to calculate the sensitivity of J using the proposed method. The maximum error in calculating the sensitivity of J is less than two percent. Based on these <b>analytical</b> sensitivities, standard <b>reliability</b> method was formulated to perform probabilistic fracture-mechanics analysis. A significant reduction of the computational effort can be achieved when solving fracture reliability problems using the analytical sensitivities...|$|R
