27|109|Public
40|$|Theoretically, the Delft {{approach}} of surface-related multiple eliminationcan be appliedin three dimension, {{as long as}} the sourceand receiver coverage is dense enough. In reality, such a dense coverage is still far from reach, using the available multi-streamer acquisition system. One way to fill the gap is to massively interpolate the missing sources and receivers in the survey, which requires huge computational cost. In this paper, I propose a more practical approach for the multi-streamer system. Instead of the large-volume missing-streamer interpolation, this approach tries to find the most reasonable proxy from the collected dataset for each missing trace needed in the multiple prediction. Although the missing-streamer interpolation is avoided, another problem pops up in the multi-streamer case, the <b>aliasing</b> <b>noise</b> caused by the sparse sampling in the cross-line direction. To solve this problem, I introduce a new concept, partially-stacked multiple contribution gather (PSMCG). Using the multi-scalePEF theory, this approachinterpolates the PSMCG in the cross-line direction to remove the <b>aliasing</b> <b>noise...</b>|$|E
40|$|We {{describe}} {{the work of}} detection and identication of frequency lines in the Virgo dark fringe data from run C 7. Among the topics of this note we highlight: (i) the new list of line candidates from the pulsars search analysis; (ii) investigation of the 10 Hz harmonics; (iii) violin modes; (iv) noise from the NE and WE buildings ' air conditioners; (v) sidebands in calibration lines; (vi) <b>aliasing</b> <b>noise</b> in the 4 kHz reconstructed data...|$|E
30|$|As {{can be seen}} in Figure 9, {{the real}} AC 30 (first column) has a {{relatively}} high noise level due to the 50 [*]Hz power supply hum and its harmonic components. This power supply noise is clearly audible also in quiet parts when using the amplifier in a real playing situation. Interestingly, the aliasing analysis plots for both virtual AC 30 plugins (second and third column) show severe aliasing phenomenon. The aliased component at 1 [*]kHz is roughly 40 [*]dB above the auditory spectrum estimate at full gain settings, making it clearly audible. In fact, the heavy aliasing behavior can strongly be heard also by listening to the logsweep responses of the AC 30 plugins. It {{should be noted that the}} power supply noise is generally less irritating than <b>aliasing</b> <b>noise,</b> since the former stays largely constant regardless of the input signal. <b>Aliasing</b> <b>noise,</b> in turn, changes radically according to the input signal, and can it be an especially undesired phenomenon when playing high bent notes, since when the original tone moves up in frequency, the strongest aliased components move down, creating an unpleasant effect.|$|E
50|$|Apart from {{composing}} electronic music, {{they also}} produce music for commercial spots, image films and sound logos under the <b>alias</b> <b>Noise</b> Design. Furthermore, they develop sample libraries and demo songs for Steinberg's DAW Cubase, Halion, Halion Sonic and Sequel.|$|R
40|$|The {{magnitude}} of <b>aliased</b> <b>noise</b> that degrades {{the accuracy of}} continuous reconstructions of discrete radiometric measurements was evaluated {{as a function of}} the spatial response and sampling intervals of the radiometer, and of the resolution of the reconstructed measurements. A Wiener spectrum, representative of a wide range of scenes, was used to characterize the radiance fluctuations...|$|R
40|$|We {{present a}} {{theoretical}} analysis of <b>aliasing</b> <b>noises</b> that might appear in cold atom Mach-Zehnder interferometers {{used for the}} measurement of various physical quantities. We focus more specifically on single cold atom gyroscopes. To evaluate the level of <b>aliasing</b> <b>noises,</b> we have developed a model based on the power spectral densities of the different identified noise sources as input parameters and which makes use of a servo-loop to realize a precise measurement of the rotation rate. The model allows one {{to take into account}} different modes of operation, like a continuous as well as a pulsed or even a multi-ball operation. For monokinetic atoms, we show that the intermodulation noise can be completely filtered out with a continuous mode of operation and an optimum modulation scheme for any modulation frequency but also with a pulsed operation however only for specific launching frequencies. In the case of a real continuous atomic beam having a velocity distribution, it comes out that a high attenuation can be reached which indicates clearly the potential stability improvement that can be expected from a continuous operation...|$|R
40|$|Abstract. In {{this paper}} we address the {{parallel}} beam 2 D computer tomography reconstruction. The proposed method belongs {{to the field of}} analytic reconstruction methods and is compared to several methods known in the field, among other the two-step Hilbert-transform method. In contrast to the latter, the derivative data is multiplied with an orientation vector and the Hilbert transform is replaced with the Riesz transform. Experimental results show that the new method is superior to established ones concerning <b>aliasing,</b> <b>noise,</b> and DC errors. ...|$|E
40|$|Nonuniform {{sampling}} of images {{is a useful}} technique in computer graphics, because a properly designed pattern of samples can make aliasing {{take the form of}} high-frequency random noise. In this paper, the tech-nique of nonuniform sampling is extended from two dimensions to include the extra parameter dimensions of distribution ray tracing. A condition for optimality is suggested, and algorithms for approximating opti-mal sampling are developed. The technique is demonstrated at low sampling densities, so the characteris-tics of <b>aliasing</b> <b>noise</b> are clearly visible. At supersampling rates, this technique should move noise into fre-quencies above the passband of the pixel-reconstruction filter...|$|E
40|$|In {{this paper}} we address the {{parallel}} beam 2 D computer tomography reconstruction. The proposed method belongs {{to the field of}} analytic reconstruction methods and is compared to several methods known in the field, among other the two-step Hilbert-transform method. In contrast to the latter, the derivative data is multiplied with an orientation vector and the Hilbert transform is replaced with the Riesz transform. Experimental results show that the new method is superior to established ones concerning <b>aliasing,</b> <b>noise,</b> and DC errors. Original Publication: Michael Felsberg, A Novel two-step Method for CT Reconstruction, 2008, Bildverarbeitung für die Medizin, 303 - 307. Copyright: Springer</p...|$|E
50|$|In addition, filters {{employed}} in the frequency conversion processes of the transmitter and receiver all contribute to gain and phase variations across the system passband, especially near to band edges. In particular, major contributors to overall non-linear phase characteristics are the low-pass filters preceding the A/D converters, which are usually sharp-cut filters chosen to ensure maximum bandwidth while minimizing <b>aliased</b> <b>noise.</b> The transient response characteristics of these filters contribute another (unwanted) source of time sidelobes.|$|R
40|$|The {{ability to}} tune an imaging {{system to be}} optimal for a {{specific}} task is {{an essential component of}} image quality. This article discusses the ability to tune the noise-equivalent quanta (NEQ) of cone-beam computed tomography (CBCT) by managing <b>noise</b> <b>aliasing</b> through binning of data at different points in the reconstruction cascade. The noise power spectrum, modulation transfer function, and NEQ for CBCT are calculated using cascaded systems analysis. Binning is treated as a modular process, insertable between any two stages (in both the 2 D projection domain and in the 3 D reconstruction domain), consisting of the application of an aperture, followed by the resampling of data (which introduces <b>noise</b> <b>aliasing).</b> Several conditions were examined to demonstrate the validity of the model and to describe the effect on the image quality of some common reconstruction and visualization techniques. It was found that when downsampling data for increased reconstruction speed, binning in 2 D results in a superior low-frequency NEQ, while binning in 3 D results in a superior high-frequency NEQ. Furthermore, visualization procedures such as slice averaging were found not to degrade the NEQ provided the sampling interval is unchanged. Finally methods for reducing <b>noise</b> <b>aliasing</b> by oversampling are examined, and a method to eliminate <b>noise</b> <b>aliasing</b> without increasing reconstruction time is proposed. These results demonstrate the ease with which the NEQ of CBCT can be modified and thus optimized for specific tasks and show how such analysis can be used to improve image quality...|$|R
40|$|International audienceWe {{propose a}} new color filter array (CFA) with optimal {{characteristics}} for {{the acquisition of}} color scenes, since the luminance and chrominance information is encoded in the mosaicked image with the best achievable robustness to <b>aliasing</b> and <b>noise.</b> Our 2 × 3 pattern {{is based on the}} paradigm recently introduced by Hirakawa et al. [1], which focuses on the spectral properties of CFAs. Moreover, these superior properties are fully exploited by a simple, linear and efficient demosaicking method...|$|R
40|$|I {{present a}} method for {{anti-aliasing}} Kirchhoff imaging operators that improves {{the resolution of the}} image by properly imaging aliased components of the recorded data that would be suppressed by standard anti-aliasing methods for Kirchhoff operators. The proposed method succeeds in imaging “beyond aliasing ” without generating <b>aliasing</b> <b>noise</b> because it exploits a priori knowledge on the dipspectrum of the data. Therefore, the proposed method is not of general applicability but it successfully improves the image resolution when a priori assumptions on the dip-spectrum of the data are realistic. The imaging of a salt-dome flanks in the Gulf of Mexico has been enhanced by the application of the proposed method...|$|E
40|$|Multi-resolution {{techniques}} {{have been used}} {{in a wide range of}} vision applications. Unfortunately, the costly operation of building a proper pyramid strongly reduces its value as a tool for reducing computational cost. A new approach, physical panoramic pyramid, is introduced in this paper. Physical panoramic pyramid measures multiple resolutions simultaneously resulting in multi-resolution panoramic images. No computation is needed to construct these image pyramids. We also analyze general noise sensitivity in image pyramids, including the interaction of the loss of resolution, random background noise and <b>aliasing</b> <b>noise.</b> The paper also discusses the issue of indexing between the neighboring layer, the viewpoint variation and the applications of the physical panoramic pyramid...|$|E
30|$|The {{usage of}} the DATK was {{illustrated}} with a case study, where a custom-built AC 30 guitar tube amplifier and two commercial software simulations, TH 1 AC 30 and ReValver AC 30, were measured and compared. The amount and type of distortion on the real AC 30 {{were found to be}} strongly dependent on the channel gain, as well as input signal frequency. Especially the amplitude ratios between the first few harmonic components show a complex nonmonotonous dependency from the channel gain. Although both software plugins successfully simulate the overall distortion characteristics in a broad sense, their response is more static than the real AC 30 response. Waveform responses to a transient input signal reveal that the nonlinearity in the real AC 30 is dynamic. Also the tested TH 1 AC 30 plugin shows strong dynamic behavior, while the ReValver AC 30 plugin is more static. Intermodulation distortion analysis did not reveal any major differences between the three AC 30 variants for low- and medium-gain settings, although some differences could be observed under full gain operation. Clearly audible <b>aliasing</b> <b>noise</b> was observed for both software plugins with sinusoid inputs when running a 48 [*]kHz sample rate, whereas the real AC 30 suffered from power supply hum. In a practical guitar playing condition, the <b>aliasing</b> <b>noise</b> on the tested software plugins is in most cases negligible due to the complex spectrum of the guitar. High bent notes can, however, reveal audible aliasing artifacts even when operating at a 96 [*]kHz sample rate.|$|E
40|$|Ray tracing is a {{straightforward}} and powerful image synthesis technique but usually requires many rays per pixel {{to eliminate the}} <b>aliasing</b> or <b>noise</b> in the nal image. However, not all the pixels in the image require the same quantity of rays. In this paper we introduce new measures for supersampling re nement criteria, based on colour and geometry. These measures use the entropy, {{one of the most}} relevant Information Theory concepts. All the new measures are compared with other classic contrast measures...|$|R
40|$|The Cone-Kernel {{representation}} (CKR) and the {{instantaneous power}} spectrum (IPS) are two time-frequency representations (TFRs) where the cross-terms are localized {{in the region}} of auto-terms. Exploring their relationship for discrete sequences, we show that the IFS kernel is only two extreme terms of the CKR kernel. Further comparing other properties of these two TFRs, viz., invertibility, <b>aliasing,</b> and <b>noise</b> robustness, we show that there is aliasing in some transform domains for signals sampled at Nyquist rate and that the CKR has better noise robustness than the IFS...|$|R
5000|$|Distortion {{in music}} is often {{intentionally}} {{used as an}} effect when applied to an electric guitar signal in styles of rock music such as heavy metal and punk rock (see also overdrive and distortion synthesis). Other forms of audio distortion that may be referred to are non-flat frequency response, compression, modulation, <b>aliasing,</b> quantization <b>noise,</b> wow and flutter from analog media such as vinyl records and magnetic tape. The human ear cannot hear phase distortion, except that it may affect the stereo imaging. (See also: Audio system measurements.) ...|$|R
40|$|Many seismic data {{processing}} and imaging processes require densely and regularly sampled data, whereas the actual measurements are mostly irregularly and sparsely sampled. Therefore, seismic data reconstruction methods are utilised as a pre-processing step. Within {{the class of}} transformation-based reconstruction techniques, observed seismic data is decomposed into certain basis functions, such as plane waves, parabolas or curvelets. In the corresponding model space the <b>aliasing</b> <b>noise</b> is assumed to have different properties than the seismic signal and can be suppressed. However, in many cases subsurface information is available that cannot be used in these traditional reconstruction methods. Therefore, the double focal transformation was derived {{as a way to}} incorporate knowledge about the subsurface in the reconstruction algorithm. The basic principle of the double focal transformation is to focus seismic energy by a back-propagation of the seismic data at the source and receiver side to certain depth levels. As a result, the seismic data are represented by a limited number of samples in the focal domain in a localised area, whereas <b>aliasing</b> <b>noise</b> spreads out. By imposing a sparse solution in the focal domain, <b>aliasing</b> <b>noise</b> is suppressed and data reconstruction beyond aliasing is achieved. To facilitate the process, only a few effective depth levels need to be included, preferably along the major boundaries in the subsurface. Propagation operators from these boundaries to the surface (focal operators) serve as the basis functions of this data decomposition method. Including more depth levels allows a sparser data representation, and hence, increases the reconstruction capability. The more precise the subsurface information is known, the more accurate these propagators can be computed. However, very precise operators are not necessary for a good reconstruction result, because in the reconstruction step (the inverse focal transformation) the effect of these operators is again removed. The calculation of the double focal transformation requires a non-linear inversion process, where the samples in the focal domain are estimated such that they - after inverse transformation - match the input data at the measurement locations. Because the inversion process is under-determined, an extra constraint on the focal domain is applied, for which the minimum L 1 norm is chosen. This forces the distribution in the focal domain to be sparse and - thereby - suppresses the <b>aliasing</b> <b>noise.</b> For the inversion a so-called spgl 1 solver has been used that is guaranteed to converge to the desired minimum of the defined objective function. It utilises a steepest decent type iterative process, called Spectral Projected Gradient. Seismic data reconstruction via the double focal transform method appears to be robust against inaccuracies in the focal operators up to roughly ten percent velocity error. Furthermore, the method was extended to the full 3 D case, where each focal transform sub-domain in principle contains a 5 D data space. In addition to the basic focal transformation, the method can be combined with other transforms in order to increase data compression. As an example, the double focal transformation can be combined with the linear Radon transformation, such that the seismic data can be represented sparser and fewer focal operators are necessary. Satisfactory results of focal domain data reconstruction beyond aliasing on 2 D and 3 D synthetic and 2 D field data illustrate the method’s virtues. Department of Imaging PhysicsApplied Science...|$|E
40|$|We {{describe}} {{a method for}} perfect signal reconstruction from spiking neuron models such as integrate-and-fire or leaky integrate-and-fire neurons. These neural models encode a single analog signal in the timing of asynchronous digital pulses. We show that using only the output firing times of these neurons, we can recover a bandlimited input signal to within machine precision. A major application of this work is for a replacement of conventional analog-to-digital converters in some applications where simpler analog hardware is traded off for more complex reconstruction {{on the part of}} the subsequent digital processor. Realistic SPICE simulations of CMOS spiking neurons show that accurate reconstruction with more than 12 -bit precision can be achieved. The effects of frequency <b>aliasing,</b> <b>noise,</b> and temporal quantization are considered. 1...|$|E
40|$|Maloney and Ahumada [9] have {{proposed}} a network learning algorithm that allows the visual system to compensate for irregularities in the positions of its photoreceptors. Weights in the network are adjusted by a process tending to make the internal image representation translationinvariant. We report {{on the behavior of}} this translation-invariance algorithm calibrating a visual system that has lost receptors. To attain robust performance in the presence of <b>aliasing</b> <b>noise,</b> the learning adjustment was limited to the receptive field of output units whose receptors were lost. With this modification the translation-invariance learning algorithm provides a physiologically plausible model for solving the recalibration problem posed by retinal degeneration. 1. 1 Introduction During the course of the degenerative disease retinitis pigmentosa (RP), patients experience progressive visual field loss, raised luminance and contrast thresholds, and nightblindness. Visual field loss typically begins in [...] ...|$|E
40|$|We {{present a}} model-based {{argument}} that, {{for the purposes}} of system design and digital image processing, aliasing should be treated as signal-dependent additive noise. By using a computational simulation based on this model, we process (high resolution images of) natural scenes in a way which enables the 'aliased component' of the reconstructed image to be isolated unambiguously. We demonstrate that our model-based argument leads naturally to system design metrics which quantify the extent of aliasing. And, by illustrating several aliased component images, we provide a qualitative assessment of <b>aliasing</b> as <b>noise...</b>|$|R
40|$|We {{propose a}} new color filter array (CFA) with optimal {{characteristics}} for {{the acquisition of}} color scenes, since the luminance and chrominance information is encoded in the mosaicked image with the best achievable robustness to <b>aliasing</b> and <b>noise.</b> Our 2 × 3 pattern {{is based on the}} paradigm recently introduced by Hirakawa et al. [1], which focuses on the spectral properties of CFAs. Moreover, these superior properties are fully exploited by a simple, linear and efficient demosaicking method. Index Terms — Color filter array (CFA), demosaicking, spatiospectral sampling, noise sensitivity. 1...|$|R
50|$|It is {{a subject}} of debate whether analog audio is {{superior}} to digital audio or vice versa. The question is highly dependent {{on the quality of}} the systems (analog or digital) under review, and other factors which are not necessarily related to sound quality. Arguments for analog systems include the absence of fundamental error mechanisms which are present in digital audio systems, including <b>aliasing,</b> quantization <b>noise,</b> and the absolute limitation of dynamic range. Advocates of digital point to the high levels of performance possible with digital audio, including excellent linearity in the audible band and low levels of noise and distortion.|$|R
40|$|In {{this paper}} we develop a {{computational}} model of visual masking based on psychophysical data. The model predicts how {{the presence of}} one visual pattern affects the detectability of another. The model allows us to choose texture patterns for computer graphics images that hide the effects of faceting, banding, <b>aliasing,</b> <b>noise</b> and other visual artifacts produced by sources of error in graphics algorithms. We demonstrate {{the utility of the}} model by choosing a texture pattern to mask faceting artifacts caused by polygonal tesselation of a flat-shaded curved surface. The model predicts how changes in the contrast, spatial frequency, and orientation of the texture pattern, or changes in the tesselation of the surface will alter the masking effect. The model is general and has uses in geometric modeling, realistic image synthesis, scientific visualization, image compression, and image-based rendering...|$|E
40|$|Interpolation of data beyond {{aliasing}} {{limits and}} removal of noise that occurs within the seismic bandwidth are still important problems in seismic processing. The focal transform is introduced as a promising tool in data interpolation and noise removal, allowing {{the incorporation of}} macroinformation about the involved wavefields. From a physical point of view, the principal action of the forward focal operator is removing the spatial phase of the signal content from the input data, and the inverse focal operator restores what the forward operator has removed. The strength of the method {{is that in the}} transformed domain, the focused signals at the focal area can be separated from the dispersed noise away from the focal area. Applications of particular interest in preprocessing are interpolation of missing offsets and reconstruction of signal beyond aliasing. The latter {{can be seen as the}} removal of <b>aliasing</b> <b>noise.</b> GeotechnologyCivil Engineering and Geoscience...|$|E
40|$|A recent proof-of-principle study {{proposes a}} {{nonlinear}} electrostatic implicit particle-in-cell (PIC) algorithm in one dimension (Chen, Chacon, Barnes, J. Comput. Phys. 230 (2011) 7018). The algorithm employs a kinetically enslaved Jacobian-free Newton-Krylov (JFNK) method, and conserves energy and charge to numerical round-off. In this study, we generalize the method to electromagnetic simulations in 1 D using the Darwin approximation of Maxwell's equations, which avoids radiative <b>aliasing</b> <b>noise</b> issues by ordering {{out the light}} wave. An implicit, orbit-averaged time-space-centered finite difference scheme is applied to both the 1 D Darwin field equations (in potential form) and the 1 D- 3 V particle orbit equations to produce a discrete system that remains exactly charge- and energy-conserving. Furthermore, enabled by the implicit Darwin equations, exact conservation of the canonical momentum per particle in any ignorable direction is enforced via a suitable scattering rule for the magnetic field. Several 1 D numerical experiments demonstrate the accuracy and the conservation properties of the algorithm. Comment: 24 pages, 4 figure...|$|E
40|$|In {{communication}} {{the signals}} area unit processed by the sampling devices while not loss of data. As associate in nursing interface between radio front-ends and digital signal process blocks. Digital radiocommunications completed by sampling devices. In {{the idea of}} software defined radio, radio systems areaunit that mixes analog, digital and software technology. One goal of software defined radio is to place theanalog to digital convertor as nearest as double to the antenna. Band pass sampling allows one to own aninterface between the radio frequency or the upper intermediate frequency signal and also the analog todigital convertor, {{and it might be}} an answer to software defined radio. Three types of sources performdegradation present in harmful signal spectral overlapping, <b>noise</b> <b>aliasing,</b> rate systems and samplingtemporal order noise. In this Research Paper, Optimized Construction BandPass Sampling (OCBPS) iscompletely studied with specialise in the <b>noise</b> <b>aliasing</b> drawback in software defined Radio...|$|R
40|$|Ray tracing usually needs supersampling {{to reduce}} <b>aliasing</b> or <b>noise</b> {{in the final}} image. Not all the pixels in the image require the same {{quantity}} of rays, thus adaptive supersampling is implemented by adaptive subdivision of the sampling region, resulting in a refinement tree. We present here a theoretically sound adaptive supersampling method based on entropy, an information theory approach with strong analogies to the decision tree problem where entropy is frequently used as a decision criterion. Our adaptive supersampling algorithm is implemented within a path tracing method and we show that our results compare well to the ones obtained by a classic strategy...|$|R
40|$|International audienceDigital color cameras acquire color images {{by means}} of a sensor on which a color ﬁlter array (CFA) is overlaid. The Bayer CFA dominates the {{consumer}} market, but there has been recently a renewed interest for the design of CFAs. However, robustness to noise is often neglected in the design, though it is crucial in practice. In this work, we present a new 2 × 3 -periodic CFA which provides, by construction, the optimal tradeoff between robustness to <b>aliasing,</b> chrominance <b>noise</b> and luminance noise. Moreover, a simple and efﬁcient linear demosaicking algorithm is described, which fully exploits the spectral properties of the CFA. Practical experiments conﬁrm the superiority of our design, both in noiseless and noisy scenarios...|$|R
40|$|A {{differential}} single-port switched-RC N-path filter with band-pass characteristic is proposed. The {{switching frequency}} defines the center frequency, while the RC-time and duty {{cycle of the}} clock define the bandwidth. This allows for high-Q highly tunable filters which can for instance be useful for cognitive radio. Using a linear periodically time-variant (LPTV) model, exact expressions for the filter transfer function are derived. The behavior of the circuit including non-idealities such as maximum rejection, spectral <b>aliasing,</b> <b>noise</b> and effects due to mismatch in the paths is modeled and verified via measurements. A simple RLC equivalent circuit is provided, modeling bandwidth, quality factor and insertion loss of the filter. A 4 -path architecture is realized in 65 nm CMOS. An off-chip transformer acts as a balun,improves filter-Q and realizes impedance matching. The differential architecture reduces clock-leakage and suppresses selectivity around even harmonics of the clock. The filter has a constant 3 dB bandwidth of 35 MHz and can be tuned from 100 MHz up to 1 GHz. Over the whole band, IIP 3 is better than 14 dBm, dBm and the noise figure is 3 – 5 dB, while the power dissipation increases from 2 mW to 16 mW (only clocking power) ...|$|E
40|$|We {{investigate}} {{the possibility of}} constraining primordial non-Gaussianity using the 3 D bispectrum of Ly-alpha forest. The strength of the quadratic non-Gaussian correction to an otherwise Gaussian primordial gravitational field {{is assumed to be}} dictated by a single parameter fnl. We present the first prediction for bounds on fnl using Ly-alpha flux spectra along multiple lines of sight. The 3 D Ly-α transmitted flux field is modeled as a biased tracer of the underlying matter distribution sampled along 1 D skewers corresponding to quasars sight lines. The precision to which fnl can be constrained depends on the survey volume, pixel noise and <b>aliasing</b> <b>noise</b> (arising from discrete sampling of the density field). We consider various combinations of these factors to predict bounds on fnl. We find that in an idealized situation of full sky survey and negligible Poisson noise one may constrain fnl 23 in the equilateral limit. Assuming a Ly-alpha survey covering large parts of the sky (k_min = 8 * 10 ^- 4 Mpc^- 1) and with a quasar density of n̅ = 5 * 10 ^- 3 Mpc^- 2 it is possible to constrain fnl 100 for equilateral configurations. The possibility of measuring fnl at a precision comparable to LSS studies maybe useful for joint constraining of inflationary scenarios using different data sets. Comment: 4 pages, 1 figure, 1 table. Accepted for publication in Physical Review Letter...|$|E
40|$|For {{the last}} two decades, two related {{approaches}} have been studied independently in conjunction with limitations of image sensors. The one is to reconstruct a high-resolution (HR) image from multiple low-resolution (LR) observations suffering from various degradations such as blur, geometric deformation, <b>aliasing,</b> <b>noise,</b> spatial sampling and so on. The other one is to reconstruct a high dynamic range (HDR) image from differently exposed multiple low dynamic range (LDR) images. LDR {{is due to the}} limitation of the capacitance of analogue-to-digital converter and the nonlinearity of the imaging system’s response function. In practical situations, since observations suffer from limitations of both spatial resolution and dynamic range, it is reasonable to address them in a unified context. Most super-resolution (SR) image reconstruction methods that enhance the spatial resolution assume that the dynamic ranges of observations are the same or the imaging system’s response function is already known. In this paper, the conventional approaches are overviewed and the SR image reconstruction, which simultaneously enhances spatial resolution and dynamic range, is proposed. The image degradation process including limited spatial resolution and limited dynamic range is modelled. With the observation model, the maximum a posteriori estimates of the response function of the imaging system as well as the single HR image and HDR image are obtained. Experimental results indicate that the proposed algorithm outperforms the conventional approaches that perform the HR and HDR reconstructions sequentially with respect to both objective and subjective criteria...|$|E
40|$|Ray tracing {{techniques}} need supersampling {{to reduce}} <b>aliasing</b> and/or <b>noise</b> {{in the final}} image. Since not all the pixels in the image require {{the same number of}} rays, supersampling can be implemented by adaptive subdivision of the sampling region, resulting in a refinement tree. In this paper we present a theoretically sound adaptive sampling method based on entropy, the classical measure of information. Our algorithm is orthogonal to the method used for sampling the pixel or for obtaining the radiance of the hitpoint in the scene. Results will be shown for our implementation within the context of stochastic ray tracing and path tracing. We demonstrate that our approach compares well to the ones obtained by using classic strategies based on contrast and variance...|$|R
40|$|Optical-mechanical line-scan {{techniques}} {{have been applied}} to earth satellite multispectral imaging systems. The capability of the imaging system is generally assessed by its information capacity. An approach based on information theory is developed to formulate {{the capacity of the}} line-scan process. Included are the effects of blurring of spatial detail, photosensor <b>noise,</b> <b>aliasing,</b> and quantization. The information efficiency is shown to be dependent on sampling rate, system frequency response shape, SNR, and quantization interval...|$|R
40|$|Many modeling, {{simulation}} {{and performance}} analysis studies of sampled imaging systems are inherently incomplete {{because they are}} conditioned on a discrete-input, discrete-output model that only accounts for blurring during image acquisition and additive noise. For those sampled imaging systems where the effects of digital image acquisition, digital filtering and reconstruction are significant, the modeling, simulation and performance analysis {{should be based on}} a more comprehensive continuous-input, discrete-processing, continuous-output end-to-end model. This more comprehensive model should properly account for the low-pass filtering effects of image acquisition prior to sampling, the potentially important noiselike effects of the aliasing caused by sampling, additive noise due to device electronics and quantization, the generally high-boost filtering effects of digital processing, and the low-pass filtering effects of image reconstruction. This model should not, however, be so complex as to preclude significant mathematical analysis, particularly the mean-square (fidelity) type of analysis so common in linear system theory. We demonstrate that, although the mathematics of such a model is more complex, the increase in complexity is not so great as to prevent a complete fidelity-metric analysis at both the component level and at the end-to-end system level: that is, computable mean-square-based fidelity metrics are developed by which both component-level and system-level performance can be quantified. In addition, we demonstrate that system performance can be assessed qualitatively by visualizing the output image as the sum of three component images, each of which relates to a corresponding fidelity metric. The cascaded, or filtered, component accounts for the end-to-end system filtering of image acquisition, digital processing, and image reconstruction; the random noise component accounts for additive random noise, modulated by digital processing and image reconstruction filtering; and the <b>aliased</b> <b>noise</b> component accounts for the frequency folding effect of sampling, modulated by digital processing and image reconstruction filtering...|$|R
