9|676|Public
5000|$|Native <b>access</b> <b>logging</b> with {{advanced}} details: browser, IP and more.|$|E
5000|$|Some {{versions}} of the Squid web proxy will pipeline up to two outgoing requests. This functionality has been disabled by default {{and needs to be}} manually enabled for [...] "bandwidth management and <b>access</b> <b>logging</b> reasons." [...] Squid supports multiple requests from clients.|$|E
50|$|Change and <b>access</b> <b>logging</b> records who {{accessed}} which attributes, {{what was}} changed, {{and when it}} was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.|$|E
30|$|Most {{websites}} maintain <b>access</b> <b>log</b> tables, {{which are}} responsible of recording all visitors’ accesses {{information such as}} IP address, the page URL being visited, and timestamp. In order to save more information than the normal <b>access</b> <b>log,</b> PathMarker creates a new table in the database. We call the new table as Extended <b>Access</b> <b>Log</b> Table. When a new web request is received, the server decrypts the encrypted URL and parses the plaintext to get the URL marker. Then PathMarker extracts {{the information in the}} server’s normal <b>access</b> <b>log</b> and insert them into the extended <b>access</b> <b>log</b> table with the visitor’s user ID and corresponding marker’s information.|$|R
5000|$|Transaction journaling, redo <b>logs,</b> <b>access</b> <b>logs,</b> error {{logs and}} audit logs enabled by default ...|$|R
5000|$|The {{earliest}} preliminary IDS {{concept was}} delineated in 1980 by James Anderson at the National Security Agency {{and consisted of}} a set of tools intended to help administrators review audit trails. [...] User <b>access</b> <b>logs,</b> file <b>access</b> <b>logs,</b> and system event logs are examples of audit trails.|$|R
5000|$|The Microsoft Enterprise Library {{is a set}} {{of tools}} and {{programming}} libraries for the Microsoft [...]NET Framework. It provides APIs to facilitate proven practices in core areas of programming including data <b>access,</b> <b>logging,</b> exception handling and others. Enterprise Library is provided as pluggable binaries and source code, which can be freely used and customized by developers for their own purposes. It also ships with test cases and quickstarts.|$|E
50|$|In Aboriginal history, the Mekinac River and Missionary Lake {{together}} {{served as}} paths between the Saint-Maurice River and Batiscan River. In winter, this route {{was also very}} useful for forest contractors using horse-drawn sleighs to <b>access</b> <b>logging</b> areas around Missionary Lake or Mekinac Lake.The need for this path for forestry was greatly reduced when a railway was completed in 1908 in the nearby Tawachiche sector connecting Hervey-Jonction to La Tuque. Today, this Mekinac/Missionary route is used recreationally by snowmobiles and ATVs between early December and late March.|$|E
50|$|The Storm's stork is {{possibly}} not strongly {{directly affected by}} habitat fragmentation through deforestation. It may actually be somewhat tolerant of fragmentation because it could fly great distances in search of new habitat and be relatively unaffected by the open land matrix which it overflies. However, this is probably not a favourable situation for the species, and the exact maximum distances it will travel to reach new habitat are currently unknown. A larger impact of deforestation on the Storm's stork {{is more likely to}} be the decrease in freshwater faunal prey abundance and diversity resulting from increased sedimentation, nutrient loads and water temperatures after logging. This loss of freshwater taxa would in turn decrease food availability for this stork at its foraging sites. Road building through the forests to <b>access</b> <b>logging</b> areas creates similar problems by contributing to soil erosion, thereby also decreasing freshwater prey diversity. The large canopy gaps created through logging also lead to drier abiotic conditions in the cleared areas than under dense canopy, which would render these areas unsuitable for food taxa of the Storm's stork such as amphibians and invertebrates that require wet substrates to live on. This is another likely contributor to the decrease in food taxa abundance after forest clearing.|$|E
40|$|Abstract. Web <b>access</b> <b>logs,</b> usually {{stored in}} {{relational}} databases, {{are commonly used}} for various data mining and data analysis tasks. The tasks typically consist in searching the web <b>access</b> <b>logs</b> for event sequences that support a given sequential pattern. For large data volumes, this type of searching is extremely time consuming and is not well optimized by traditional indexing techniques. In this paper we present a new index structure to optimize pattern search queries on web <b>access</b> <b>logs.</b> We focus on its physical structure, maintenance and performance issues. ...|$|R
40|$|Web server <b>access</b> <b>logs</b> {{analyses}} are frequently used for re-structuring Web sites, such as simplifying most-wanted access paths or pruning unwanted parts. In addition, information found in <b>access</b> <b>logs</b> {{can be used}} to learn how a Web site is used in context. In this paper, we present findings of an informal investigation of Web server <b>access</b> <b>logs</b> and we discuss how these insights can used to enhance Web sites from a community-oriented perspective. 1. Introduction Web server <b>access</b> <b>logs</b> are frequently used for traffic analyses to estimate the usage of a Web site. The information gained is required for various activities ranging from accounting to load distribution. The raw format of <b>access</b> <b>logs</b> is difficult to understand as single entries contain a lot of information. Consider, for example, the following entry generated by an Apache 1. 3 Web server: j 110. inktomi. com - - [01 /Jul/ 2000 : 08 : 24 : 10 + 0200] "GET /staff/lueg/abstracts/chi 98 late. html HTTP/ 1. 0 " 200 1822 "-" "Slurp. so/ 1. 0 [ [...] . ]" A [...] ...|$|R
40|$|This paper {{introduces}} a novel clustering scheme employing {{a combination of}} rough set theory and fuzzy set theory to generate meaningful abstractions from web <b>access</b> <b>logs.</b> Our experimental {{results show that the}} proposed scheme is capable of capturing the semantics involved in web <b>access</b> <b>logs</b> at an acceptable computational expense...|$|R
40|$|In the {{framework}} of GOODSTEP, rules have been introduced {{as a means to}} support the implementation of next generation of Software Development Environments (SDE), mainly for: (i) notifying users, i. e., programmers of SDE or end-users, (ii) application <b>access</b> <b>logging,</b> (iii) organizing related application programs, (iv) tools communication[22], (v) change propagation, and (vi) maintaining data consistency. Such rules are comprised of three parts : an Event(E) part, a Condition(C) part and an Action(A) part. The general semantics of an ECA rule is the following: "Whenever the event E occurs, if the Condition C holds, then execute Action A". This paper first presents the decisions we made for integrating rules in the object-oriented database system O 2, the basis of the GOODSTEP platform. Then we concentrate on the Event part definition of a rule and finally we discuss the rule execution model. 1 This report has been accepted to the 2 nd International Workshop on Database and Software Engin [...] ...|$|E
40|$|Recent {{advances}} in speech and language technology have made spoken dialogue systems mainstream in many industries. They allow customers {{to engage in}} natural speech interactions with machines instead of being compelled to navigate menus of options with touch tones inputs. VoiceXML was a major milestone for the process of using automated speech applications to expose business portals to ubiquitous telephone access. By {{the introduction of a}} uniform and universally accepted client-server browser model, the VoiceXML programming model greatly simplified previously dominant proprietary computer telephony interfaces. However, natural language spoken dialogue systems entail more complex interactions with the user which, depending upon the application domain, may require computational models that are difficult to express directly in VoiceXML. This paper describes Florence, a dialogue manager with a more general approach that uses an extensible and flexible framework to combine interchangeable and interoperable dialogue strategies as appropriate to the task. Florence’s declarative XML-based language facilitates the development of natural language applications and allows the dialogue author to encapsulate and reuse different algorithms between applications. Moreover, it addresses large-scale natural language issues related to enterprise backend <b>access,</b> <b>logging,</b> distributed deployment, and fail-over support. These issues must be addressed in a modern, industrial-strength application server environment. 1...|$|E
40|$|Time-series of count {{data are}} {{generated}} {{in many different}} contexts, such as web <b>access</b> <b>logging,</b> freeway traffic monitoring, and security logs associated with buildings. Since this data measures the aggregated behavior of individual human beings, it typically exhibits a periodicity in time {{on a number of}} scales (daily, weekly, etc.) that reflects the rhythms of the underlying human activity and makes the data appear non-homogeneous. At the same time, the data is often corrupted by a number of bursty periods of unusual behavior such as building events, traffic accidents, and so forth. The data mining problem of finding and extracting these anomalous events is made difficult by both of these elements. In this paper we describe a framework for unsupervised learning in this context, based on a time-varying Poisson process model that can also account for anomalous events. We show how the parameters of this model can be learned from count time series using statistical estimation techniques. We demonstrate the utility of this model on two data sets for which we have partial ground truth in the form of known events, one from freeway traffic data and another from building access data, and show that the model performs significantly better than a non-probabilistic, threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-ofday effects, and make inferences about the detected events (e. g., popularity or level of attendance). Our experimental results indicate that the proposed time-varying Poisson model provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity. 1...|$|E
30|$|The first {{table is}} “Extended <b>Access</b> <b>Log</b> Table”, which saves all {{extended}} <b>access</b> <b>logs.</b> It has six columns: Log ID, User ID, User IP, URL, URL marker, and timestamp. For our online forum, whenever the main controller confirms the current user ID {{is not known}} crawler, PathMarker inserts one entry into this table.|$|R
5000|$|NADDIS <b>logs</b> the <b>access</b> of NADDIS by any {{authorized}} DEA {{employee and}} records the time, date, location and {{identity of the}} employee who requested access to a particular file. These <b>access</b> <b>logs</b> or [...] "detail reports" [...] have been utilized by DEA Headquarters to identify DEA employees who provided NADDIS records to criminal organizations or who have improperly accessed NADDIS21. Agencies' <b>access</b> <b>logs</b> have also been considered by district courts in resolving legal issues over the timing of agents' and prosecutors' awareness of investigative records on a subject. Thus, the use of NADDIS detail reports to detect government improprieties {{is similar to the}} use of other law enforcement database <b>access</b> <b>logs</b> in demonstrating police misconduct. For example, in Kansas the FBI's Interstate Identification Index (III) <b>access</b> <b>log</b> confirmed improper <b>access</b> by a sheriff who conducted criminal history checks against political opponents.|$|R
40|$|AbstractAs the Internet is {{widespread}} {{and there are}} many online shops in the Internet, many persons buy products in the online shops. Customer's behavior in the online shops is a sequence of customer driven activities intrinsically because his/her movement in an online shop occurs according to only his/her decision. Hence, to achieve satisfactory purchase experiments it is important how the shop supports them. Online shops have to predict visitors’ intents correctly to support them effectively. One of information resources the shops can use is an <b>access</b> <b>log</b> including information on customer's movement in the online shop. If they are histories of customer's behaviors in online shops and the behaviors depend on customer's intents, we can extract new knowledge on them from the <b>access</b> <b>logs.</b> Speaking concretely, we can predict customers’ intents from the <b>access</b> <b>logs</b> since their internal intents affect their activities. We can realized more appropriate recommendation service by changing recommendation strategy depending on customer's intents. In this paper, we propose a method to predict customer's intents from <b>access</b> <b>logs</b> in a real online shop. We adopt a Topic Tracking Model (TTM) to analyze the <b>access</b> <b>logs...</b>|$|R
40|$|In this paper, {{we present}} a system based on an Unsupervised Fuzzy Divisive Hierarchical Clustering (UFDHC) {{algorithm}} to determine a hierarchy of profiles of web site typical users from the web <b>access</b> <b>log.</b> These profiles can be extremely useful, for instance, to customize the web site, or to send personalized advertisements. After eliminating categories {{that have not been}} accessed by a significant percentage of users and removing the occasional users, the <b>access</b> <b>log</b> data are input to the UFDHC algorithm which clusters the users of the web site into a hierarchy of groups characterized by a set of common interests and represented by a prototype, which defines the profile of the group typical member. To show the effectiveness of our system, we describe how the profiles determined by the UFDHC algorithm from <b>access</b> <b>log</b> data collected along a period of 15 days allow classifying approximately 95 % of the users defined by <b>access</b> <b>log</b> data collected during subsequent 60 days...|$|R
40|$|Determining {{profiles}} of web portal typical users {{can be extremely}} useful, for instance, to personalize the web portal, to provide customized guide and to send tailored advertisements. In this work, we present a system to produce {{a small number of}} user profiles from the web <b>access</b> <b>log</b> and to associate each user with one of these profiles. The system is based on a version of the fuzzy C-means (FCM) algorithm which uses the cosine distance rather than the classical Euclidean distance. After filtering the <b>access</b> <b>log,</b> for instance, by removing occasional and undecided users, the FCM algorithm clusters the users into groups characterized by a set of common interests and represented by a prototype, which defines the profile of the group typical member. To attest the validity of these profiles, we extract a set of association rules from the raw <b>access</b> <b>log</b> data by applying the well-known A-priori algorithm and show how the profiles are a concise representation of the association rules. Finally, to test the effectiveness of the overall fuzzy system, we illustrate how the profiles determined by the FCM algorithm from <b>access</b> <b>log</b> data collected along a period of 30 days allow classifying approximately 93 % of the users defined by <b>access</b> <b>log</b> data collected during subsequent 30 days...|$|R
40|$|Abstract: Web usage mining {{deals with}} {{understanding}} the Visitor’s behaviour with a Website. It helps {{in understanding the}} concerns such as present and future probability of every website user, relationship between behaviour and website usability. It has different branches such as web content mining, web structure and web usage mining. The focus {{of this paper is}} on web mining usage patterns of an educational institution web log data. There are three types of web related log data namely web <b>access</b> <b>log,</b> error log and proxy log data. In this paper web <b>access</b> <b>log</b> data has been used as dataset because the web <b>access</b> <b>log</b> data is the typical source of navigational behaviour of the website visitor. The study of web server log analysis is helpful in applying the web mining techniques...|$|R
40|$|Finding {{relevant}} {{information on the}} World Wide Web is becoming highly challenging day by day. Web usage mining {{is used for the}} extraction of relevant and useful knowledge, such as user behaviour patterns, from web <b>access</b> <b>log</b> records. Web <b>access</b> <b>log</b> records all the requests for individual files that the users have requested from the website. Web usage mining is important for Customer Relationship Management (CRM), as it can ensure customer satisfaction as far as the interaction between the customer and the organization is concerned. Web usage mining is helpful in improving website structure or design as per the user’s requirement by analyzing the <b>access</b> <b>log</b> file of a website through a log analyzer tool. The focus {{of this paper is to}} enhance the accessibility and usability of a guitar selling web site by analyzing their <b>access</b> <b>log</b> through Deep Log Analyzer tool. The results show that the maximum number of users is from the United States and that they use Opera 9. 8 web browser and the Windows XP operating system...|$|R
3000|$|For {{the cases}} that user’s {{identity}} is normal, before_routing (...) passes the part before “/mk:” to the Website Router to find the corresponding sub-controller and we record an extended <b>access</b> <b>log</b> in the database. After recording the extended <b>access</b> <b>log,</b> we get the timestamp and IP address from the system default log. before_routing (...) conducts heuristic detection of current request. If any field is abnormal, we increment the current user’s Wrong Heuristic Logs’s value in the “User Information Table”.|$|R
5000|$|Web servers: Apache HTTP Server (<b>access</b> <b>log</b> {{and error}} log), IIS web server (NSCA and W3C extended), and Zeus Web Server errors log ...|$|R
40|$|In this paper, {{we present}} a system based on an {{appropriately}} targeted version of the well-known fuzzy C-means (FCM) algorithm to determine {{a small number of}} profiles of typical Web site users from the Web <b>access</b> <b>log.</b> These profiles can be extremely useful, for instance, to customize the Web site, or to send personalized advertisements. After filtering the <b>access</b> <b>log,</b> for instance, by eliminating occasional users, the FCM algorithm clusters the users of the Web site into groups characterized by a set of common interests and represented by a prototype, which defines the profile of the group typical member. To show the effectiveness of our system, we describe how the profiles determined by the FCM algorithm are a concise representation of the association rules discovered applying the well-known A-priori algorithm to the raw <b>access</b> <b>log</b> dat...|$|R
30|$|To prepare {{these data}} for web site automation, a {{significant}} amount of pre-processing is necessary. Usage data can be obtained from web <b>access</b> <b>logs</b> and/or page tagging, which consist in pieces of code on a page to notify when the page is accessed [13, 20, 21]. Here, we focus on web <b>access</b> <b>logs</b> as usage data. The pre-processing of web logs is likely the most difficult task in the pre-processing of web data due to the quality (incompleteness, noise, etc.) of the available data [13, 20].|$|R
40|$|Web usage mining is to {{analysis}} Web log files to discover user accessing patterns of Web pages. The user <b>access</b> <b>log</b> files present very significant {{information about a}} web server. This paper is deal with finding information about a web site, top errors, link errors between the pages, etc. from the web server <b>access</b> <b>log</b> files. The {{aim of this study}} is {{to analysis}} the web server user <b>access</b> <b>logs</b> of Firat University to help system administrator and Web designer to improve their system by determining occured systems errors, corrupted and broken links by using web using mining. We found useful information about activity statistics like top errors, client errors, server errors within the visited pages etc. in a web server. The obtained results of the study will be used in the further development of the web site in order to increase its effectiveness...|$|R
40|$|One of {{the most}} {{difficult}} and complicated problems for the Web Masters is obtaining feedback for their web sites. Web server <b>access</b> <b>logs</b> are originally intended for being used by Web Masters. In fact these files are the ideal source of feedback. On the other hand these files may reach to extreme sizes {{in a very short time}} periods and it becomes impossible to draw results without using advanced tools. As a result web server <b>access</b> <b>logs</b> are nice raw material for Data Mining applications but advanced techniques are necessary. In this paper I handle the problem of discovering paths traversed by visitors of a web site. I have been developing the path discovery tool, PathFinder 2 K, mining web server <b>access</b> <b>logs</b> for paths traversed by visitors. I present the design of the PathFinder 2 K, examine the performance of the algorithm, and give some outputs of the tool. 1...|$|R
40|$|Recently, the {{shortcomings}} of current security solutions in protecting web servers and web applications against web-based attacks have encouraged many researchers to work on web intrusion detection systems (WIDSs). In this paper, a host-based web anomaly detection system is presented which analyzes the POST and GET requests processed and logged in web servers <b>access</b> <b>log</b> files. A special kind of web <b>access</b> <b>log</b> file is introduced which eliminates {{the shortcomings}} of common log files for defining legitimate users sessions boundaries. Different features are extracted from this <b>access</b> <b>log</b> file in order to model {{the operations of the}} system. For the detection task, we propose the use of a novel approach inspired by the natural immune system. The capability of the proposed mechanism is evaluated by comparing the results to some well-known neural networks. The results indicate high ability of the immune inspired system in detecting suspicious activities...|$|R
40|$|As {{the size}} of web {{increases}} along with number of users, {{it is very much}} essential for the websiteowners to better understand their customers so that they can provide better service, and also enhance thequality of the website. To achieve this they depend on the web <b>access</b> <b>log</b> files. The web <b>access</b> <b>log</b> files can bemined to extract interesting pattern so that the user behavior can be understood. This paper presents anoverview of web usage mining and also provides a survey of functionalities that are associated with web logfiles used for web usage mining...|$|R
50|$|Referrer spam (also {{known as}} {{referral}} spam, log spam or referrer bombing) {{is a kind}} of spamdexing (spamming aimed at search engines). The technique involves making repeated web site requests using a fake referrer URL to the site the spammer wishes to advertise. Sites that publish their <b>access</b> <b>logs,</b> including referer statistics, will then inadvertently link back to the spammer's site. These links will be indexed by search engines as they crawl the <b>access</b> <b>logs,</b> improving the spammer's search engine ranking. Except for polluting their statistics, the technique does not harm the affected sites.|$|R
3000|$|By observing alerts {{coming from}} {{different}} sources (IDSs, system calls, web server <b>access</b> <b>logs,</b> Windows security events and firewall events), we have divided them into three main groups: [...]...|$|R
50|$|SecureLog is used {{to secure}} {{different}} types of data <b>logs</b> like <b>access</b> <b>logs,</b> email archives or transaction logs and is primarily in use where compliance might be an issue.|$|R
40|$|With {{the growing}} {{popularity}} of the World Wide Web (Web), large volumes of data such as user address or URL requested are gathered automatically by Web servers and collected in <b>access</b> <b>log</b> les. Exhibiting relationships and global patterns that exist in these large les, but are hidden among the vast amounts of data is usually called Web usage mining. Recently, many interesting works have been published in this context. Nevertheless, the large amount of input data poses a maintenance problem. In fact, maintening global patterns is a non-trivial task after <b>access</b> <b>log</b> le update because new data may invalidate old client behavior and creates new ones. In this paper we address the problem of incremental web usage mining, i. e. the problem of mining user patterns when new transactions or new clients are added to the original <b>access</b> <b>log</b> le. keywords: data mining, Web usage mining, sequential patterns, incremental mining. 1 Introduction With {{the growing popularity}} of the Worl [...] ...|$|R
40|$|International audienceThe {{performance}} of Web sites {{continues to be}} an important research topic. Such studies are invariably based on the <b>access</b> <b>logs</b> from the servers comprising the Web site. A problem with existing <b>access</b> <b>logs</b> is the coarse granularity of the timestamps, e. g., arrival times. In this study we demonstrate and quantify the significant differences in performance obtained under diverse assumptions about the arrival process of user requests derived from the <b>access</b> <b>logs,</b> where the corresponding user response times can differ by more than an order of magnitude. This motivates the need for a general methodology to construct accurate representations of the actual arrival process of user requests from existing coarse-grained access-log data. Our analysis of the <b>access</b> <b>logs</b> from representative commercial Web sites illustrates self-similar behavior of the arrival process. We propose a drill-down methodology for constructing the arrival process at finer time scales based on the self-similar properties of the arrival process observed at coarse logging time scales. The advantage of our approach is that it maintains consistency between the properties of the arrival processes at both coarser and finer time scales. In addition, our analysis of the request size distribution from commercial Web sites demonstrates a subexponential, but not heavy-tail (power-law) distribution. Through simulations, we investigate the impact of these different traffic models on user response times...|$|R
5000|$|Nasraoui O., Frigui H., Joshi A., and Krishnapuram R., “Mining Web <b>Access</b> <b>Logs</b> Using Relational Competitive Fuzzy Clustering”, Proceedings of the Eighth International Fuzzy Systems Association Congress, Hsinchu, Taiwan, August 1999 ...|$|R
