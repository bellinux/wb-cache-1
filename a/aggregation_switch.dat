6|42|Public
50|$|BLADE RackSwitch {{products}} included: RackSwitch G8100 and G8124 10G {{low latency}} Switches and RackSwitch G8000 1-10G <b>aggregation</b> <b>switch.</b>|$|E
50|$|Bandwidth pooling {{is used in}} network {{switches}} to optimize the use of network resources. It allows switch processing cards to be shared by physical interface cards. This innovation frees switch resources {{that would otherwise be}} stranded when lower-rate interface cards are deployed in an <b>aggregation</b> <b>switch.</b>|$|E
5000|$|The split {{may be at}} one or at {{both ends}} of the MLT. If both ends of the link are split, the {{resulting}} topology is referred to as an [...] "SMLT square" [...] when there is no cross-connect between diagonally opposite aggregation switches, or an [...] "SMLT mesh" [...] when each <b>aggregation</b> <b>switch</b> has a SMLT connection with both aggregation switches in the other pair. If only one end is split, the topology is referred to as an SMLT triangle.|$|E
50|$|For each SMLT connection, the <b>aggregation</b> <b>switches</b> have a {{standard}} MLT or individual port with which an SMLT identifier is associated. For a given SMLT connection, the same SMLT ID must be configured {{on each of}} the peer <b>aggregation</b> <b>switches.</b>|$|R
5000|$|N series: Campus {{access and}} <b>aggregation</b> <b>switches</b> with models for PoE+ {{offering}} 10Gb or 40Gb uplinks to core. N-series switches run DNOS6.x on a Linux kernel.|$|R
50|$|The two {{switches}} between {{which the}} SMLT is split {{are known as}} <b>aggregation</b> <b>switches</b> and form a logical cluster which appears {{to the other end}} of the SMLT link as a single switch.|$|R
40|$|IPTV {{services}} delivered traditional television channels via IP to customers. IGMP protocol is {{the control}} mechanism {{used to control}} the delivery of multicast traffic to interested and authorized users. IGMP commands notify the upstream equipment to stop sending (“leave”) one channel or begin sending (“join”) another channel. Depending on the architectural choices, this process occurs in the DSLAM, an <b>aggregation</b> <b>switch,</b> or at an edge router. All channels are sent by a multicast source to DSLAM even if no user in the DSLAM has requested it. This leads to sub-optimal use of bandwidth. This paper studies and investigates the performance of IPTV controlled by CGMP Protocol to save bandwidth using OPNET package. It shows that when CGMP is enabled on edge routers and switches, hosts receive only the channels demanded and therefore the bandwidth is optimized and also the queuing delay is reduced...|$|E
40|$|AbstractIn this paper, we {{proposed}} a testing method to evaluate the system which is developed by IPv 6 multicasting for conform the international and industry standards, implementing the functionality as well as meeting business requirements during practical use. The paper also presents a design and implements method for hybrid multiplex integrated network access equipment System by IPv 6 multicasting under AN 6016 - 01 system research project context. Moreover, it conducts an IPv 6 multicast simulation test on system basic factors through Spirent Testcenter. Testing result shows, {{it is difficult for}} instrument ports of Testcenter to meet testing requirements when handle massive users in an application senario. But long time performance testing among massive users may be conducted via <b>aggregation</b> <b>switch.</b> IPv 6 multicast simulation testing may effectively discover problems in system development and application. But this testing method does not suitable for the actual application scenarios (ONU+STB+TV), it is only suitable in laboratory. Finally, the paper proposes a common solution about laboratory testing...|$|E
40|$|Group {{communication}} systems (e. g., multicast, DHTs) {{have emerged as}} basic primitives for several large-scale distributed systems. Most existing systems that implement these primitives often assume a flat topology of overlay nodes. For instance, many DHTs assume that all overlay links are often homogeneous in their capacities, costs and other such characteristics. Similarly, multicast protocols create overlay trees without {{taking into account the}} physical location of nodes. While a few systems do consider latency between different overlay nodes, the fact that metrics such as latency change continuously often translates into additional complexity in constantly measuring and reorganizing nodes. Modern trends in hierarchical data center designs and global services running across several geographically disparate data centers pose unique challenges and opportunity to revisit the design of group {{communication systems}} with location awareness in-built into these systems from the ground up. In this paper, we present the design and architecture of one such system called DC 2, in which nodes are aware of their location within the data center (e. g., rack, <b>aggregation</b> <b>switch)</b> and organize group communications in order to minimize the expensive cross-data center links or cross-hierarchy links as much as possible. In our experiments using a real prototype deployed over 700 virtual nodes running over 15 physical machines, we found that DC 2 minimizes message latencies by several orders of magnitude, and reduces node and link stress by a factor of 2 to 3 ×...|$|E
50|$|The {{key to the}} {{operation}} of SMLT is the Inter-Switch Trunk (IST). The IST is a (standard) MLT connection between the <b>aggregation</b> <b>switches</b> which allows the exchange of information regarding traffic forwarding {{and the status of}} individual SMLT links.|$|R
50|$|In general, normal {{network traffic}} does not {{traverse}} the IST unless {{this is the}} only path to reach a host which is connected only to the peer switch. By ensuring all devices have SMLT connections to the <b>aggregation</b> <b>switches,</b> traffic never needs to traverse the IST and the total forwarding capacity of the switches in the cluster is also aggregated.|$|R
40|$|In Data Centers each rack uses a Top-of-Rack (ToR) as a {{first level}} switch to connect servers to the <b>aggregation</b> <b>switches.</b> The current paper focuses on a hybrid electrical/optical ToR design, which first, adapts the servers Ethernet traffic to the optical TDMA {{operation}} of the core network for supporting optical switching in the Data Center’s upper layer and second, it employs optical switching of traffic at the rack level. The proposed ToR architecture {{is based on an}} Ethernet switch and FPGA port extensions realizing the required functions to support 20 10 Gbps connections, exploit the network routing resources and handle effectively virtual queues...|$|R
5000|$|Points of interconnections: are {{the border}} {{physical}} connections between licensed Internet Service Provider, and the Access Network of a Telecommunication Network, marked as the demarcation zone, and {{the aim is}} to allow the licensed Internet Service Providers to acquire logical port capacity to serve IP stream and access services provided on a Public Telecommunications Network as appropriate. Points of Internet connection are physically represented by <b>aggregation</b> <b>switches</b> which allows port bundling, link aggregation to form logical capacity to serve the backhauling connectivity. Point of Interconnection, are the major POPs identified by the telecommunication provider at the feasible flexible to connect points [...]|$|R
30|$|The Spine-Leaf topology, {{proposed}} by Cisco [5, 7], {{is used in}} massively scalable data centers. As shown in Fig.  1, the Spine-Leaf topology has two types of switches: Spine switch and Leaf switch. Spine <b>switches</b> work as <b>aggregation</b> <b>switches</b> in the traditional 3 -tier network architecture. They only connect with Leaf switches and do not connect directly with servers. Every Spine switch connects with all Leaf switches. Leaf switches are access switches. They connect with Spine switches {{and a number of}} servers. As shown in Fig.  1, there are also some Leaf switches called Border Leaf switches which are responsible for connecting to public networks.|$|R
40|$|Abstract. Aiming at {{reduction}} of complexity, {{this work is}} concerned with two-time-scale Markov chains and applications to quasi-birth-death queues. Asymptotic expansions of probability vectors are constructed and justified. Lumping all states of the Markov chain in each subspace into a single state, an aggregated process is shown to converge to a continuous-time Markov chain whose generator is an average {{with respect to the}} stationary measures. Then a suitably scaled sequence is shown to converge to a switching diffusion process. Extensions of the results are presented together with examples of quasi-birth-death queues. Key words. Markov chain, singular perturbation, countable state space, asymptotic expansion, occupation measure, <b>aggregation,</b> <b>switching</b> diffusion, quasi-birth-death queu...|$|R
40|$|Abstract. As {{networks}} to be flattened, a direction {{has been put}} forward clearly by Telecom operators that Optical Line Terminals (OLT) of Gigabit Passive Optical Network (GPON) should become aggregation[1] OLTs, which makes them to work as both <b>aggregation</b> <b>switches</b> and edge access devices. According to this requirement, a scheme that realizes Routing[2] Information Protocol (RIP) on GPON OLT is put forward after deep analysis of GPON system[3] [4] [5] and RIP. Results shows that IP routing on GPON OLT is realized, which allows OLTs {{to work on the}} network layer, at the same time simplifies the broadband network levels and increases the reliability of the network...|$|R
5000|$|The Juniper EX2200-C is a compact, affordable, {{and energy}} {{efficient}} switch in a 1 U high and 10.8 inch wide formfactor. The EX2200-C {{does not have}} dual redundant power supplies nor does it have fans and instead the small unit stays cool with a heat sink that sits {{on the back of}} the unit. Even the PoE version is fanless. The EX2200-C is a small 19" [...] rack mountable (using optional brackets) or desktop switch with 12 gigabit ports with a choice of PoE or non-PoE. The switch also has 2 SFP uplink ports for virtual chassis (stacking), link aggregation or as uplinks to <b>aggregation</b> <b>switches.</b> The EX2200-C has a data rate of 28 Gbit/s.|$|R
40|$|This paper proposes an {{algorithm}} {{to minimize}} weighted service latency for different classes of tenants (or service classes) in a data center network where erasure-coded files are stored on distributed disks/racks and access requests are {{scattered across the}} network. Due to limited bandwidth available at both top-of-the-rack and <b>aggregation</b> <b>switches</b> and tenants in different service classes need differentiated services, network bandwidth must be apportioned among different intra- and inter-rack data flows for different service classes in line with their traffic statistics. We formulate this problem as weighted queuing and employ a class of probabilistic request scheduling policies to derive a closed-form upper-bound of service latency for erasure-coded storage with arbitrary file access patterns and service time distributions. The result enables us to propose a joint weighted latency (over different service classes) optimization over three entangled "control knobs": the bandwidth allocation at top-of-the-rack and <b>aggregation</b> <b>switches</b> for different service classes, dynamic scheduling of file requests, and the placement of encoded file chunks (i. e., data locality). The joint optimization is {{shown to be a}} mixed-integer problem. We develop an iterative algorithm which decouples and solves the joint optimization as 3 sub-problems, which are either convex or solvable via bipartite matching in polynomial time. The proposed algorithm is prototyped in an open-source, distributed file system, Tahoe, and evaluated on a cloud testbed with 16 separate physical hosts in an Openstack cluster using 48 -port Cisco Catalyst switches. Experiments validate our theoretical latency analysis and show significant latency reduction for diverse file access patterns. The results provide valuable insights on designing low-latency data center networks with erasure coded storage...|$|R
50|$|However, {{almost all}} vendors have {{proprietary}} extensions that resolve {{some of this}} issue: they aggregate multiple physical switches into one logical switch. In 2012, the IEEE standardized this feature in IEEE 802.1aq. The Split multi-link trunking (SMLT) protocol allows multiple Ethernet links to be split across multiple switches in a stack, preventing any single point of failure, and additionally allowing all switches to be load balanced across multiple <b>aggregation</b> <b>switches</b> from the single access stack. These devices synchronize state across an Inter-Switch Trunk (IST) such that they appear to the connecting (access) device to be a single device (switch block) and prevent any packet duplication. SMLT provides enhanced resiliency with sub-second failover and sub-second recovery for all speed trunks (10 Mbit/s, 100 Mbit/s, 1,000 Mbit/s, and 10 Gbit/s) while operating transparently to end-devices.|$|R
40|$|AbstractSoftware defined {{networking}} {{becomes more}} and more popular in the networking community, but is still missing its triumphal procession into existing networks. Especially data-centers could benefit from this evolutionary network paradigm and get rid of many legacy parts with are still blocking the evolution how their networks are working in general. In this paper we will present an approach, use-case and performance evaluation as well, for the integration of software defined networking applications in data-centers. This approach addresses a sub topic or more precisely a single software defined networking application for enhancing inter rack communication by involving the top-of-rack and <b>aggregation</b> <b>switches</b> in a common fat-tree network topology. The basic aim is to provide some sort of load distribution mechanism on the afore mentioned network layers by using a flow based load sharing procedure with layer two multi-path. The evaluation will show that this is increasing the throughput in a rack-to-rack data exchange scenario...|$|R
50|$|There {{are three}} chassis options; a 3-slot chassis most {{commonly}} used for access or distribution / <b>aggregation</b> of <b>switches</b> which has a MTBF of 2,043,676hr., a 6-slot chassis for backbones of low density or high space premium environments, and a 10-slot chassis for high availability and high scalability. The chassis can be configured {{with one or two}} CPU modules and is normally configured with two or three load balancing power supplies.|$|R
3000|$|Data center network {{architectures}} {{are typically}} classified into two categories: switch-centric and server-centric. In switch-centric DCNs, the routing intelligence {{is placed on}} switches and each server connects to the network through a single port. In server-centric DCNs, the routing intelligence is placed on servers which connect to the network through multiple ports while switches serve merely as cross-bars. Although a number of architectures in both categories have been proposed {{in order to achieve}} scalability, efficiency, reliability, cost minimization etc. in addition to some dual-centric architectures combining the best of both categories, the legacy three-tier tree-based architecture continues to be the most widely deployed and fat-tree being the most promising in terms of scalability, robustness and cost (Bilal et al. 2013). Both of these architectures are switch-centric. A typical data center network consists of access layer, aggregation layer and core layer. It consists of routers and switches, in two-level or three-level hierarchy. In two-level hierarchy there are no <b>aggregation</b> <b>switches</b> while three-level hierarchy includes all three layers. The most realistic and practical DCN simulation involves three-level architectures. We briefly explain the legacy three-tier and fat-tree architectures below: [...]...|$|R
50|$|The Juniper EX2200 line of 1Gb Ethernet {{switches}} are compact gigabit switches. They {{are small}} and energy efficient making them environmentally friendly and great for office use. PoE variants in the EX2200 family support high-power 802.3at PoE+ (up to 30W/port) on all ports, up {{to a maximum of}} 405 Watts, which is the maximum power budget for the switch. The power budget is the lowest of all EX series PoE switches, and notably insufficient to run high-power Powered Devices (PD's) on all ports simultaneously. They {{come in a variety of}} forms including a 24 port plus 4 SFP PoE and non-PoE, a 48 port plus 4 SFP PoE and non-PoE, and the EX2200-C which is a 12 port plus 2 SFP Poe and non-PoE models. The 4 SFP ports allow link aggregation and uplink to <b>aggregation</b> <b>switches</b> without sacrificing the base ports. This model does not have dual power supplies. However, as of September 24, 2012 (JunOS 12.2R1 release date), it supports Juniper's virtual chassis technology. The EX2200-24T/24P has a data rate of 56 Gbit/swhile the EX2200-48T/48P has a data rate of 104 Gbit/s.|$|R
40|$|International audienceThe {{increasing}} {{adoption of}} virtualization techniques has recently favored {{the emergence of}} useful switching functions at the hypervisor level, {{commonly referred to as}} virtual bridging. In the context of data center network (DCN) consolidations, for VMs colocated in the same virtualization server, virtual bridging becomes very useful to offload inter-VM traffic from access and <b>aggregation</b> <b>switches,</b> at the expense of an additional computing load. DCN consolidations typically chase traffic engineering (TE) and energy efficiency (EE) objectives, and both should be affected by virtual bridging, but it is not intuitive to assert whether virtual bridging acts positively or negatively with respect to TE and EE that should also depend on the DCN topology and forwarding techniques. In this paper, we bring additional understanding about the impact of virtual bridging on DCN consolidations. First, we present a repeated matching heuristic for the generic multi-objective DCN optimization problem, with possible multipath and virtual bridging capabilities, accounting for both TE and EE objectives. Second, we assess the impact of virtual bridging on TE and EE in DCN consolidations. Extensive simulations show us that enabling virtual bridging has a negative impact when EE is the goal and multipath forwarding is adopted, while it leads to important gains, halving the maximum link utilization, when TE is the DCN consolidation goal. I. INTRODUCTION The recent achievement of x 86 virtualization by advanced software techniques allows attaining virtualization of server and network functions at competitive performance-cost trade-offs with respect to legacy solutions. The increasing adoption of virtualization techniques has recently favored the emergence of useful switching functions at the hypervisor level, commonly referred to as virtual bridging. In the context of data center network (DCN) consolidations, for VMs colocated in the same virtualization server, virtual bridging becomes very useful to offload inter-VM traffic from access and <b>aggregation</b> <b>switches,</b> at the expense of an additional computing load on the physical server. DCN consolidations typically chase traffic engineering (TE) and energy efficiency (EE) objectives, and both should be affected by virtual bridging, but it is not intuitive to assert whether virtual bridging acts positively or negatively with respect to TE and EE that should also depend on the DCN topology and forwarding techniques. In this paper, we bring additional understanding about the impact of virtual bridging on DCN consolidations. The literature on DCN consolidation problems is extensive. Often referred to as DCN optimization, VM placement or virtual network embedding, the various propositions at {{the state of the art}} often have a narrow scope, with a set of constraints so that it is not possible to jointly adopt E...|$|R
40|$|Abstract—The {{increasing}} {{adoption of}} virtualization techniques has recently favored {{the emergence of}} useful switching functions at the hypervisor level, {{commonly referred to as}} virtual bridging. In the context of data center network (DCN) consolidations, for VMs colocated in the same virtualization server, virtual bridging becomes very useful to offload inter-VM traffic from access and <b>aggregation</b> <b>switches,</b> at the expense of an additional computing load. DCN consolidations typically chase traffic engineering (TE) and energy efficiency (EE) objectives, and both should be affected by virtual bridging, but it is not intuitive to assert whether virtual bridging acts positively or negatively with respect to TE and EE that should also depend on the DCN topology and forwarding techniques. In this paper, we bring additional understanding about the impact of virtual bridging on DCN consolidations. First, we present a repeated matching heuristic for the generic multi-objective DCN optimization problem, with possible multipath and virtual bridging capabilities, accounting for both TE and EE objectives. Second, we assess the impact of virtual bridging on TE and EE in DCN consolidations. Extensive simulations show us that enabling virtual bridging has a negative impact when EE is the goal and multipath forwarding is adopted, while it leads to important gains, halving the maximum link utilization, when TE is the DCN consolidation goal. I...|$|R
5000|$|A {{commonly}} deployed three-tier LAN network design {{includes the}} access layer, which provides initial connectivity for devices to the network. At the next tier, the aggregation layer (sometimes {{referred to as}} distribution layer) concentrates the connectivity of multiple access-layer switches to higher-port-count and typically higher-performance Layer 3 <b>switches.</b> The <b>aggregation</b> layer <b>switches</b> are in turn connected to the network core layer switches, which centralize all connectivity in the network. The trinity of access, aggregation, and core layers enables the network to scale over time to accommodate an ever greater number of end devices ...|$|R
40|$|Most {{datacenter}} network (DCN) designs {{focus on}} maximizing bisection bandwidth rather than minimizing server-to-server latency. They are, therefore, ill-suited for important latency-sensitive applications, {{such as high}} performance computing, realtime analytic systems and high-frequency financial trading. Although {{there are a number}} of existing approaches to reduce network latency, they are only partially effective, workload dependent, and often require network protocol changes. In this thesis, we explore architectural approaches to building a low-latency DCN and introduce Quartz, a new optical design element consisting of a full mesh of switches connected by an optical ring. We can reduce the network latency of a hierarchical or random network by replacing portions of it with a Quartz ring. Our analysis shows that, in a standard 3 -tier DCN, replacing high port-count core switches with Quartz can significantly reduce switching delays, and replacing groups of top-of-rack and <b>aggregation</b> <b>switches</b> with Quartz can significantly reduce congestion-related delays from cross-traffic. We overcome the complexity of wiring a complete mesh by using low-cost optical multiplexers that enable us to efficiently implement a logical mesh as a physical ring. We evaluate our performance using both simulations and a small working prototype. Our evaluation results confirm our analysis, and demonstrate that it is possible to build low-latency DCNs using inexpensive commodity elements without significant concessions to cost, scalability, or wiring complexity...|$|R
40|$|Abstract—Data center {{networks}} (DCNs) generally adopt Clos {{network with}} crossbar middle switches to achieve non-blocking data switching among the servers, {{and the number}} of middle switches is proportional to the number of ports of the <b>aggregation</b> <b>switches</b> in a fixed manner. Besides, reconfiguration overhead of the switches is generally ignored, which may contradict the engineering practice. In this paper, we consider batch scheduling based packet switching in DCNs with reconfiguration overhead at each middle switch, which inevitably leads to packet delay. With existing state-of-the-art traffic matrix decomposition algorithms, we can generate a set of permutations, each of which stands for the configuration of a middle switch. By reconfiguring each middle switch to fulfill multiple configurations in parallel with others, we reveal that a tradeoff exists between packet delay and switch cost (denoted by the number of middle switches), while performance guaranteed switching with bounded packet delay can be achieved without any packet loss. Based on the tradeoff, we can minimize the number of middle switches (under a given packet delay bound) and an overall cost metric (by translating delay into a comparable cost factor), as well as formulating criterions for choosing a matrix decomposition algorithm. This provides a flexible way {{to reduce the number of}} middle switches by slightly enlarging the packet delay bound. Keywords-Clos network, Data center networks (DCNs), matrix decomposition, packet switching, scheduling. I...|$|R
40|$|This paper {{analyzes}} {{the performance of}} an optical packet switch architecture with highly distributed control, designed for interconnection of cluster switches in a simulated data center traffic environment. The system under development can be scaled up to a very large ports count, in the thousand order, enabling interconnection of {{a great number of}} servers. An important feature of this optical packet switch is the few nanoseconds reconfiguration time, regardless of the port count. This characteristic is essential to minimize the end-to-end latency. Flow control is employed to regulate packets transmission between the electronic buffers of the ingress and egress ports. The limited contention resolution capability of the optical packet switch is compensated by these electronic buffers and a retransmissions algorithm. We investigate the performance of the switch with 1024 in/out ports in terms of data loss, throughput and latency with a data center-like traffic model. The results show that increasing the electronic input buffer size allows lower packet loss at the expense of higher latency. A buffer size in the order of 20 times the average packet length proves to be adequate to obtain a packet loss lower than 10 - 5 and latency below 1 µs when managing a sustained normalized input load of 0. 3. This traffic intensity is more than triple the average utilization of current data centers <b>aggregation</b> <b>switches.</b> Packet loss lower than 10 - 4 can be achieved for input load up to 0. 4, keeping latency at 1. 4 µs...|$|R
50|$|Port Aggregation Protocol (PAgP) is a Cisco Systems {{proprietary}} networking protocol, {{which is}} used for the automated, logical <b>aggregation</b> of Ethernet <b>switch</b> ports, known as an ether channel. The PAgP is proprietary to Cisco Systems. A similar protocol known as LACP— released by the IEEE and known as 802.3ad or 802.1ax recently — is an industry standard and is not tied to a specific vendor.|$|R
50|$|HP uses IRF {{technology}} {{without the}} 2 box limitation (they allow 4 to 9 boxes in single stack). Similarly Avaya's SMLT protocol also removes this limitation {{by allowing the}} physical ports to be split between two switches in a triangle configuration or 4 or more switches in a mesh configuration. Extreme Networks may do this functionality via M-LAG Multilink <b>Aggregation.</b> Cisco Nexus <b>switches</b> also allow this, using Virtual Port Channel.|$|R
40|$|Abstract — Today’s metro {{networks}} have {{evolved from the}} need to support traditional voice and private line services. However, the tremendous growth in access to Frame Relay, ATM, IP and Ethernet services, coupled with the desire of enterprise customers to interconnect via Ethernet interfaces, suggests {{the need for a}} new approach. This paper proposes a new architecture for Packet-Aware Transport Networks (PATN) which supports both packet and traditional TDM services and which leverages an assemblage of emerging technologies to provide efficient <b>aggregation</b> and <b>switching</b> of packet traffic in metro networks. The PATN has the potential to provide significant cost savings to carriers by reducing the number of network elements, reducing transport costs through statistical multiplexing, and eliminating the need for redundant multiplexing operations...|$|R
40|$|Smart, or {{responsive}} polymers can reversibly {{change their}} state of <b>aggregation,</b> thus <b>switching</b> from water-soluble to insoluble state, {{in response to}} minor changes in temperature, pH or solvent composition. Grafting of these polymers to solid surfaces imparts the surfaces with controllable wettability and adsorption behaviour. The review summarizes the theoretical models {{and the results of}} physical measurements of the conformational transitions in grafted polymer chains and polymer brushes. Primary attention is paid to the grafting density and the length and spatial arrangement of grafted chains, the role of polystyrene, organosilane or alkanethiol sublayers and their effects on adsorption of proteins and adhesion of cells. The key applications of grafted smart polymers such as cell culture and tissue engineering, cell and protein separation, biosensing and targeted drug delivery are surveyed. The bibliography includes 174 references...|$|R
40|$|This paper {{provides}} an overview of different experimental SDN/NFV control and orchestration architectures aiming at integrating the multi-layer (packet/optical) transport networks and distributed DC infrastructures (core cloud, edge computing) of the ADRENALINE testbed. The proposed SDN/NFV solutions enable the development and testing of end-to-end 5 G and IoT services supporting a wide variety of use cases from different vertical industries, such as automotive, e-health, energy, media or smart cities. To this end, the ADRENALINE testbed is adopting the following technologies to meet the stringent requirements of 5 G and IoT; i) high-capacity, flexible and cost/energy-efficient software-defined optical transmission technologies for access, metro and core networks; ii) highly-scalable packet technologies for <b>aggregation</b> and <b>switching</b> of flows with quality of service (QoS); iii) core datacenters and edge computing for the deployment of virtualized network functions (VNFs) and cloud service...|$|R
40|$|Mathematical {{decision}} support for operative planning in water supply systems is highly desirable but leads to very difficult optimization problems. We propose a nonlinear programming approach that yields practically satisfactory operating schedules in acceptable computing time even for large networks. Based on a carefully designed model supporting gradient-based optimization algorithms, this approach employs a special initialization strategy for convergence acceleration, special minimum {{up and down}} time constraints together with pump <b>aggregation</b> to handle <b>switching</b> decisions, and several network reduction techniques for further speed-up. Results for selected application scenarios at Berliner Wasserbetriebe demonstrate {{the success of the}} approach...|$|R
40|$|Significant {{research}} {{efforts have been}} devoted {{over the last decade}} to design efficient data centre networks. However, major concerns are still raised about the power consumption of data centres and its impact on global warming {{in the first place and}} on the electricity bill of data centres in the second place. Passive Optical Network (PON) technology with its proven performance in residential access networks can provide energy efficient, high capacity, low cost, scalable, and highly elastic solutions to support connectivity inside modern data centres. Here, we focus on introducing PONs in the architecture of data centres to resolve many issues in current data centre designs such as high cost and high power consumption resulting from the large number of access and <b>aggregation</b> <b>switches</b> needed to interconnect hundreds of thousands of servers. PONs can also overcome the problems of switch oversubscription and unbalanced traffic in data centres where PON architectures and protocols have historically been optimised to deal with these problems and handle bursty traffic efficiently. In this thesis, five novel PON data centre designs are proposed and compared to facilitate intra and inter rack communications. In addition to maximising the use of only passive optical devices, other challenges have to be addressed by these designs including off-loading the inter-rack traffic from the Optical Line Terminal (OLT) switch to avoid undesired power consumption and delays, facilitating multi-path routing, and reducing or eliminating the need for expensive tuneable lasers. The Scalability of the proposed architectures in terms of efficiently accommodating hundreds of thousands of servers is discussed. CAPEX and energy consumption of the proposed architectures are also investigated and savings compared to conventional architectures, such as the Fat-Tree and BCube, are demonstrated. The Routing and Wavelength Assignment (RWA) in intra and inter rack communication and the resource provisioning needed to cater for different applications that can be hosted in data centre are optimised using Mixed Integer Linear Programming (MILP) models to minimise the PON designs power consumption. Furthermore, real-time energy-efficient routing and resource provisioning algorithms are developed. In addition to optimising the power consumption, delay is also considered for the delay sensitive applications that can be hosted in the proposed data centre architectures. To further reduce power consumption and overcome issues related to link oversubscription and multi-path routing, Software Defined Network (SDN) based design is proposed...|$|R
