5|10000|Public
40|$|Limits on the {{effective}} resolution of many optical near-field experiments are investigated. The results are <b>applicable</b> <b>to</b> <b>variants</b> of total-internal-reflection microscopy (TIRM), photon-scanning-tunneling microscopy (PSTM), and near-field-scanning-optical microscopy (NSOM) {{in which the}} sample is weakly scattering and the direction of illumination may be controlled. Analytical expressions for the variance of the estimate of the complex susceptibility of an unknown two-dimensional object {{as a function of}} spatial frequency are obtained for Gaussian and Poisson noise models, and a model-independent measure is examined. The results are used to explore the transition from near-zone to far-zone detection. It is demonstrated that the information content of the measurements made at a distance of even one wavelength away from the sample is already not much different from the information content of the far field. Copyright 2004 Optical Society of Americ...|$|E
40|$|Copy number {{variants}} (CNVs) play {{an important}} role in the etiology of many diseases such as cancers and psychiatric disorders. Due to a modest marginal effect size or the rarity of the CNVs, collapsing rare CNVs together and collectively evaluating their effect serves as a key approach to evaluating the collective effect of rare CNVs on disease risk. While a pleth-ora of powerful collapsing methods are available for sequence variants (e. g., SNPs) in asso-ciation analysis, these methods cannot be directly applied to rare CNVs due to the CNV-specific challenges, i. e., the multi-faceted nature of CNV polymorphisms (e. g., CNVs vary in size, type, dosage, and details of gene disruption), and etiological heterogeneity (e. g., heterogeneous effects of duplications and deletions that occur within a locus or in different loci). Existing CNV collapsing analysis methods (a. k. a. the burden test) tend to have subop-timal performance due to the fact that these methods often ignore heterogeneity and evalu-ate only the marginal effects of a CNV feature. We introduce CCRET, a random effects test for collapsing rare CNVs when searching for disease associations. CCRET is <b>applicable</b> <b>to</b> <b>variants</b> measured on a multi-categorical scale, collectively modeling the effects of multipl...|$|E
40|$|This thesis {{presents}} {{an investigation into}} the applications of the maximum-entropy principle as a heuristic for the multiple travelling salesman problem. This is a computationally complex problem which requires special treatment by conventional optimization techniques. Specific focus is given to developing a generalized framework for this problem that can be applied to any number of variants on the basic formulation. Additional consideration is given to the applications of this generalized framework to other variants on the travelling salesman problems such as the close enough travelling salesman problem. The heuristic framework developed here is shown to provide flexibility in addressing the multiple salesman variation on the travelling salesman problem as well as a several other variants on the travelling salesman problem. Additionally, this framework is shown to be effective in determining solutions to this class of problems, and it is especially effective for the close-enough travelling salesman problems which is particularly challenging for most conventional combinatorial algorithms. Concrete steps are presented by which to further extend and improve this framework to become both more widely <b>applicable</b> <b>to</b> <b>variants</b> on the travelling salesman problem, and more computationally efficient in solving such problems...|$|E
40|$|<b>Variant</b> design refers <b>to</b> the {{technique}} of adapting existing design specifications to satisfy new design goals and constraints. Specific support of variant design techniques in current computer aided design systems would help to realize a rapid response manufacturing environment. A survey of approaches supporting variant design is presented. Capabilities used in current commercial computer aided design systems are discussed along with approaches used in recent research efforts. Information standards <b>applicable</b> <b>to</b> <b>variant</b> design are also identijed. Barriers <b>to</b> <b>variant</b> design in current systems are ident#ed and ideas are presented for augmentation of current systupzs to support uariant design...|$|R
40|$|Driven by {{the wide}} range of applications, scene text de-tection and {{recognition}} have become active research topics in computer vision. Though extensively studied, localizing and reading text in uncontrolled environments remain ex-tremely challenging, due to various interference factors. In this paper, we propose a novel multi-scale representation for scene text recognition. This representation consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at differ-ent granularities. Strokelets possess four distinctive advan-tages: (1) Usability: automatically learned from bounding box labels; (2) Robustness: insensitive to interference fac-tors; (3) Generality: <b>applicable</b> <b>to</b> <b>variant</b> languages; and (4) Expressivity: effective at describing characters in natu-ral scenes. Extensive experiments on standard benchmarks verify the advantages of strokelets and demonstrate that the proposed algorithm outperforms the state-of-the-art meth-ods in the literature. 1...|$|R
40|$|The dynamic {{investigation}} of helical planetary gears {{plays an important}} role in structure design as the vibration and noise are perceived negatively to the transmission quality. With consideration of the axial deformations of members, the gyroscopic effects, the time-variant meshing stiffness, and the coupling amongst stages, a three-dimensional dynamic model of the two-stage helical planetary gears is established by using of the lumped-parameter method in this paper. The model is <b>applicable</b> <b>to</b> <b>variant</b> number of planets in two stages, different planet phasing, and spacing configurations. Numerical simulation is conducted to detect the structured vibration modes of the equally spaced systems. Furthermore, the unique properties of these vibration modes are mathematically proved. Results show that the vibration modes of the two-stage helical planetary gears can be categorized as five classes: the rigid body mode, the axial translational-rotational mode, the radical translational mode, and the 1 st-stage and the 2 nd-stage planet mode...|$|R
40|$|Next {{generation}} sequencing methodologies are {{facilitating the}} rapid characterisation of novel structural variants at nucleotide resolution. These approaches are particularly <b>applicable</b> <b>to</b> <b>variants</b> initially identified using alternative molecular methods. We report {{a child born}} with bilateral postaxial syndactyly of the feet and bilateral fifth finger clinodactyly. This was presumed to be an autosomal recessive syndrome, due to the family history of consanguinity. Karyotype analysis revealed a homozygous pericentric inversion of chromosome 7 (46,XX,inv(7) (p 15 q 21) x 2) which was confirmed to be heterozygous in both unaffected parents. Since {{the resolution of the}} karyotype was insufficient to identify any putatively causative gene, we undertook medium-coverage whole genome sequencing using paired-end reads, in order to elucidate the molecular breakpoints. In a two-step analysis, we first narrowed down the region by identifying discordant read-pairs, and then determined the precise molecular breakpoint by analysing the mapping locations of "soft-clipped" breakpoint-spanning reads. PCR and Sanger sequencing confirmed the identified breakpoints, both of which were located in intergenic regions. Significantly, the 7 p 15 breakpoint was located 523 kb upstream of HOXA 13, the locus for hand-foot-genital syndrome. By inference from studies of HOXA locus control in the mouse, we suggest that the inversion has delocalised a HOXA 13 enhancer to produce the phenotype observed in our patient. This study demonstrates how modern genetic diagnostic approach can characterise structural variants at nucleotide resolution and provide potential insights into functional regulation...|$|E
40|$|This report {{describes}} a Master’s Thesis {{research that has}} been carried out to gain insight in the possible improvements of sound insulation of membrane structures, which are used in practice for temporary structures, e. g. festival tents, and to give practical solutions. This research concentrated on triple-leaf membrane systems with filled cavities. From a state-of-the-art review can be concluded that triple-leaf membrane systems, when filled, perform better than double-leaf and single-leaf membrane structures. From literature research it was concluded as well that tension in the membrane has a negligible effect on the sound insulation and that, on the other hand the flow resistance both of the filling and of the membrane material has large influence. Three different kind of filling materials were used in the present study: (lightweight) glass wool, polyester wool and aerogel. Acoustical measurements were carried out in a laboratory, of which the outcomes were compared to a number of computer and mathematical models. The Multiple Layer Model appears to give good prediction for filled triple-layer membrane systems and this model therefore was used to optimise the important parameters. A well performing triple-layer membrane system was discussed, which met the restriction of 7 kg/m 2 for the surface density of the membrane package. This system includes one layer of aerogel for reasonable sound insulation at low frequencies, and one thicker layer of glass wool yielding good sound insulation at higher frequencies. This system is only investigated theoretically for nog and not empirically (yet). Details have been worked out for a number of practical membrane structure applications for this result (also <b>applicable</b> <b>to</b> <b>variants</b> using only glass wool), focussing on temporary (festival) tent structures. Building PhysicsBuilding EngineeringCivil Engineering and Geoscience...|$|E
40|$|We {{present a}} robotic system that watches a human using a Kinect v 2 RGB-D sensor, detects what {{he forgot to}} do while {{performing}} an activity, and if necessary reminds the person using a laser pointer {{to point out the}} related object. Our simple setup can be easily deployed on any assistive robot. Our approach is based on a learning algorithm trained in a purely unsupervised setting, which does not require any human annotations. This makes our approach scalable and <b>applicable</b> <b>to</b> <b>variant</b> scenarios. Our model learns the action/object co-occurrence and action temporal relations in the activity, and uses the learned rich relationships to infer the forgotten action and the related object. We show that our approach not only improves the unsupervised action segmentation and action cluster assignment performance, but also effectively detects the forgotten actions on a challenging human activity RGB-D video dataset. In robotic experiments, we show that our robot is able to remind people of forgotten actions successfully...|$|R
40|$|Abstract. In {{the present}} work the {{application}} of hierarchical grid refinement for the Lattice-Boltzmann Method (LBM) to simulate turbulent flows has been investigated. A transformation and interpolation scheme has been developed based on a volumetric approach, independent of a specific collision model and therefore, generally <b>applicable</b> <b>to</b> all <b>variants</b> of the LBM. Besides a short review of the LBM the developed scheme is presented. Furthermore, {{the validity of the}} method is demonstrated by solutions for Poiseuille, cavity, and turbulent channel flow. ...|$|R
40|$|The {{final step}} in the {{solution}} of contact problems of elasticity by FETI-based domain decomposition methods is the reconstruction of displacements corresponding to the Lagrange multipliers for ''gluing'' of subdomains and non-penetration conditions. The rigid body component of the displacements is usually obtained {{by means of a}} well known but quite complex formula, the application of which requires reassembling and factorization of some large matrices. Here we propose a simple formula which is <b>applicable</b> <b>to</b> many <b>variants</b> of the FETI based algorithms for contact problems. The method takes a negligible time and avoids reassembling or factorization of any matrices...|$|R
40|$|Artículos en revistasIn a {{competitive}} environment with bid-based markets, power generation companies desire to develop bidding strategies that maximize their revenue. In this paper we ask: What approaches and methodologies {{have been used}} to model the bidding problem for hydro-electric producers? We present the problem's developments over time and, through reviewing different variants of the problem, progressively build to the case in which the agent is a price-maker hydro-electric producer. In each variant of the bidding problem, we examine how the approaches used to solve {{it may or may not}} be <b>applicable</b> <b>to</b> other <b>variants.</b> Last, for the price-maker hydro-electric producer's bidding problem, we recognize the most recent developments and illuminate a path for future efforts. info:eu-repo/semantics/publishedVersio...|$|R
40|$|This paper {{presents}} an efficient biorthogonal wavelet construction with the generalized Catmull-Clark subdivision {{based on the}} lifting scheme. The subdivision wavelet construction scheme is <b>applicable</b> <b>to</b> all <b>variants</b> of Catmull-Clark subdivision, so it is more universal than the previous wavelet construction for the generalized bicubic B-spline subdivision. Because the analysis and synthesis algorithms of the wavelets are composed {{of a series of}} local and in-place lifting operations, they can be performed in linear time. The experiments have demonstrated the stability of the proposed wavelet analysis based on the ordinary Catmull-Clark subdivision. Moreover, the resulting Catmull-Clark subdivision wavelets have better fitting quality than the generalized bicubic B-spline subdivision wavelets at a similar computation cost...|$|R
40|$|We {{introduce}} a parametric finite element approximation for the Stefan {{problem with the}} Gibbs–Thomson law and kinetic undercooling, which mimics the underlying energy structure of the problem. The proposed method is also <b>applicable</b> <b>to</b> certain quasi-stationary <b>variants,</b> such as the Mullins–Sekerka problem. In addition, fully anisotropic energies are easily handled. The approximation has good mesh properties, leading to a well-conditioned discretization, even in three space dimensions. Several numerical computations, including for dendritic growth and for snow crystal growth, are presented. ...|$|R
40|$|Haemoglobin is {{the best}} studied of all proteins. Its {{detailed}} structure is well understood and it is easily obtained from humans in whom over 100 variant forms have been identified. These variants have usually been {{discussed in terms of}} the primary structure or amino-acid sequence of haemoglobin, the amino acids being listed in a linear fashion. With the increase in knowledge of the three-dimensional structure of the haemoglobin molecule, it is now possible <b>to</b> discuss the <b>variants</b> in terms of the ordered structure of the molecule. As will be shown, this provides a more rational approach to the consideration of the type and frequency of variants, and in particular {{to the understanding of the}} types of variation which result in protein dysfunction. From these considerations a number of conclusions can be reached which are probably <b>applicable</b> <b>to</b> protein <b>variants</b> in general...|$|R
40|$|Abstract—In this paper, {{we study}} {{estimator}} inconsistency in vision-aided {{inertial navigation systems}} (VINS) from the stand-point of system’s observability. We postulate that {{a leading cause of}} inconsistency is the gain of spurious information along unob-servable directions, which results in smaller uncertainties, larger estimation errors, and divergence. We develop an observability constrained VINS (OC-VINS), which explicitly enforces the un-observable directions of the system, hence preventing spurious information gain and reducing inconsistency. This framework is <b>applicable</b> <b>to</b> several <b>variants</b> of the VINS problem such as vi-sual simultaneous localization and mapping (V-SLAM), as well as visual-inertial odometry using the multi-state constraint Kalman filter (MSC-KF). Our analysis, along with the proposed method to reduce inconsistency, are extensively validated with simulation trials and real-world experimentation. Index Terms—Consistency, nonlinear estimation, observability analysis, vision-aided inertial navigation. I...|$|R
40|$|The ultrastructural {{characterization}} of neuronal compartments in intact tissue labeled with {{green fluorescent protein}} (GFP) remains a frequently encountered challenge, despite work establishing photooxidation of GFP in cultured cells. However, most applications require the detection of GFP or GFP fusion proteins expressed in intact tissue. Here, we report that illumination of GFP variants in oxygen-enriched environment reliably generated electron-dense 3, 3 '-diaminobenzidine (DAB) precipitates in slices from rat brain. The method is <b>applicable</b> <b>to</b> GFP <b>variants</b> tagged <b>to</b> presynaptic proteins {{as well as to}} soluble GFP in various brain regions. Serial section scanning electron microscopy was used to examine genetically labeled presynaptic terminals at high resolution and to generate three-dimensional representations of the synapses. Thus, we introduce a generally applicable correlative approach for the identification of presynaptic terminals genetically labeled with green fluorescent proteins in tissue slices and their ultrastructural characterization...|$|R
40|$|We explore two {{complementary}} {{modifications of}} the hybridization-expansion continuous-time Monte Carlo method, aiming at large multi-orbital quantum impurity problems. One {{idea is to}} compute the imaginary-time propagation using a matrix product state representation. We show that bond dimensions considerably smaller than the dimension of the Hilbert space are sufficient to obtain accurate results and that this approach scales polynomially, rather than exponentially {{with the number of}} orbitals. Based on scaling analyses, we conclude that a matrix product state implementation will outperform the exact-diagonalization based method for quantum impurity problems with more than 12 orbitals. The second idea is an improved Monte Carlo sampling scheme which is <b>applicable</b> <b>to</b> all <b>variants</b> of the hybridization expansion method. We show that this so-called sliding window sampling scheme speeds up the simulation by at least an order of magnitude for a broad range of model parameters, with the largest improvements at low temperature...|$|R
40|$|In {{this paper}} we study the {{influence}} of key scheduling algorithms {{on the strength of}} blockciphers. We show that the key scheduling algorithms of many blockciphers inherit obvious relationships between keys, and use these key relations to attack the blockciphers. Two new types of attacks are described: New chosen plaintext reductions of the complexity of exhaustive search attacks (and the faster variants based on complementation properties), and new lowcomplexity chosen key attacks. These attacks are independent of the number of rounds of the cryptosystems and of the details of the F -function and may have very small complexities. These attacks show that the key scheduling algorithm should be carefully designed and that its structure should not be too simple. These attacks are <b>applicable</b> <b>to</b> both <b>variants</b> of LOKI and to Lucifer. DES is not vulnerable to the related keys attacks since the shift pattern in the key scheduling algorithm is not the same in all the rounds. Key words: Key sche [...] ...|$|R
40|$|The ongoing {{scaling of}} {{semiconductor}} technology is causing se-vere increase of on-chip power density and temperature in micro-processors. This has raised urgent requirement for both power and thermal management during {{each level of}} system design. In this paper, we propose a formal technique based on model checking us-ing extended timed automata to solve the processor frequency as-signment problem in a temperature- and energy- constrained multi-tasking system. The state space explosion problem is alleviated by transforming and solving a Pseudo-Boolean satisfiability problem. Our approach is capable of finding efficient solutions under vari-ous constraints and <b>applicable</b> <b>to</b> other problem <b>variants</b> as well. Our method is independent of any system and task characteristics. Experimental results demonstrate the usefulness of our approach...|$|R
40|$|X-ray {{diffraction}} contrast tomography (DCT) is {{a technique}} for mapping grain shape and orientation in plastically undeformed polycrystals. In this paper, we describe a modified DCT data acquisition strategy which permits the incorporation of an innovative Friedel pair method for analyzing diffraction data. Diffraction spots are acquired during a 360 degree rotation of the sample and are analyzed {{in terms of the}} Friedel pairs ((hkl) and (hkl -) reflections, observed 180 degrees apart in rotation). The resulting increase in the accuracy with which the diffraction vectors are determined allows the use of improved algorithms for grain indexing (assigning diffraction spots to the grains from which they arise) and reconstruction. The accuracy of the resulting grain maps is quantified with reference to synchrotron microtomography data for a specimen made from a beta titanium system in which a second phase can be precipitated at grain boundaries, thereby revealing the grain shapes. The simple changes introduced to the DCT methodology are equally <b>applicable</b> <b>to</b> other <b>variants</b> of grain mapping. Copyright 2009 American Institute of Physics...|$|R
40|$|Anthropologists {{have adopted}} methods from {{population}} genetics to study modes of cultural transmission in time-transgressive cultural data sets. However, {{it remains unclear}} to what extent methods originally developed to assess neutrality in genes sampled from a population at a single point in time are <b>applicable</b> <b>to</b> cultural <b>variants</b> sampled from assemblages. This report applies a suite of previously published methods to assemblages formed under unbiased cultural transmission. The results show that, even under the controlled conditions afforded by computer simulation, all but one method (the variants frequency approach) fail to identify unbiased cultural transmission in samples collected from moderately to severely time-averaged assemblages. While it is encouraging that the method that is best at identifying unbiased cultural transmission in simulated time-averaged data also is relatively robust to relative sample size, additional work {{is needed to determine}} whether even the variants frequency approach is powerful enough to identify “weakly” biased forms of cultural transmission in real-world assemblages. Premo, L. S. (2014) Cultural transmission and diversity in time-averaged assemblages. Current Anthropology 55 (1) : 105 - 114. Available at [URL]...|$|R
40|$|In August 1999, Knudsen and Meier {{proposed}} an attackto the block cipher RC 6 by using correlations derived from 2 tests. In this paper, we improve {{the attack and}} apply this method to the block cipher RC 5 and simplified variants of RC 6, and show some experimental results. We show this approach distinguish the random permutation and RC 5 with of up to 20 rounds by using chosen ciphertexts attack. We also show our approach for deriving the last round key of up to 17 rounds RC 5 by using chosen plaintext attack. Moreover, we show full rounds RC 5 with some weak key can be broken by using lesser complexity {{than that of the}} exhaustive search. Additionally, this method can be <b>applicable</b> <b>to</b> simplified <b>variants</b> of RC 6, that is, RC 6 -INFR, RC 6 -NFR, RC 6 -I, we observe the attack to these block ciphers. 1 Introduction RC 5 is a block cipher designed by R. Rivest in 1994 [10]. One of the reason that many cryptographers were interested in cryptanalysis of RC 5 comes from its simple structure. Kaliski a [...] ...|$|R
40|$|In this work, {{we present}} an {{approach}} for implementing an implicit scheme for the numerical {{solution of the}} partial differential equation {{of the evolution of}} an active contour/surface. The proposed scheme is <b>applicable</b> <b>to</b> any <b>variant</b> of the traditional active contour (AC), irrespectively of the calculation of the image-based force field and it is readily <b>applicable</b> <b>to</b> explicitly parameterized active surfaces (AS). The proposed approach is formulated as an infinite impulse response (IIR) filtering of the coordinates of the contour/surface points. The poles of the filter are determined by the parameters controlling the shape of the active contour/surface. We show that the proposed IIR-based implicit evolution scheme has very low complexity. Furthermore, the proposed scheme is numerically stable, thus it allows the convergence of the AC/AS with significantly fewer iterations than the explicit evolution scheme. It also possesses the separability property along the two parameters of the AS, thus it may be applied to deformable surfaces, without the need to store and invert large sparse matrices. We implemented the proposed IIR-based implicit evolution scheme in the Vector Field Convolution (VFC) AC/AS using synthetic and clinical volumetric data. We compared the segmentation results with those of the explicit AC/AS evolution, in terms of accuracy and efficiency. Results show that the VFC AC/AS with the proposed IIR-based implicit evolution scheme achieves the same segmentation results with the explicit scheme, with considerably less computation time. (C) 2014 Elsevier Ltd. All rights reserved...|$|R
40|$|The {{alternating}} direction {{method of}} multipliers (ADMM) {{is widely used}} to solve large-scale linearly constrained optimization problems, convex or nonconvex, in many engineering fields. However {{there is a general}} lack of theoretical understanding of the algorithm when the objective function is nonconvex. In this paper we analyze the convergence of the ADMM for solving certain nonconvex consensus and sharing problems, and show that the classical ADMM converges to the set of stationary solutions, provided that the penalty parameter in the augmented Lagrangian is chosen to be sufficiently large. For the sharing problems, we show that the ADMM is convergent regardless of the number of variable blocks. Our analysis does not impose any assumptions on the iterates generated by the algorithm, and is broadly <b>applicable</b> <b>to</b> many ADMM <b>variants</b> involving proximal update rules and various flexible block selection rules. Comment: Accepted by SIOP...|$|R
40|$|Digital {{signature}} {{is one of}} {{the basic}} primitives in cryptography. A common paradigm of obtaining signatures, known as the Fiat-Shamir (FS) paradigm, is to collapse any Σ-protocol (which is 3 -round public-coin honest-verifier zero-knowledge) into a non-interactive scheme with hash functions that are modeled to be random oracles (RO). The Digital Signature Standard (DSS) and Schnorr’s signature schemes are two salient examples following the FS-paradigm. In this work, we present a modified Fiat-Shamir paradigm, named challenge-divided Fiat-Shamir paradigm, which is <b>applicable</b> <b>to</b> a <b>variant</b> of Σ-protocol with divided random challenges. This new paradigm yields a new family of (online/offline efficient) digital signatures from challenge-divided Σ-protocols, including in particular a variant of Schnorr’s signature scheme called challenge-divided Schnorr signature. We then present a formal analysis of the challenge-divided Schnorr signature in the random oracle model. Finally, we give comparisons between the challenge-divided Schnorr signature and DSS and Schnorr’s signature, showing that the newly developed challenge-divided Schnorr signature can enjoy better (online/offline) efficiency (besides provable security in the random oracle model). Of independent interest is a new forking lemma, referred to as divided forking lemma, for dealing with multiple ordered rewinding points in the RO model, which is of independent interest and can be applied to analyzing other cryptographic schemes in the RO model. ...|$|R
5000|$|The Exora has an {{equivalent}} 4-star EuroNCAP rating after 32 Exoras were crashed at the Applus+ IDIADA crash testing facility in Spain. Additionally, the Exora {{was awarded a}} 4-star rating by Malaysia's own MyVAP evaluation program. The Exora was given a four-star ANCAP safety rating, <b>applicable</b> <b>to</b> all <b>variants.</b> The MPV, which is [...] with dual frontal and side airbags with thorax and head protection for front occupants, scored 26.37 {{out of a possible}} 37 points. In the offset crash test, the Exora scored 10.55 out of 16 points - driver chest protection was acceptable and leg protection was marginal. In the side impact crash test, it managed 14.82 out of 16 points - driver chest protection was listed as acceptable. The Exora result - with testing carried out in July 2013 - reflected its performance in the 40% frontal offset test, where the risk of serious injury to the driver’s legs was high. Side impact performance was good, but overall pedestrian test results were poor, according to ANCAP. One of the key features that made these ratings possible is the use of high tensile steel for the body cage, providing better stability and increased impact absorption during a collision. Side impact bars are also installed to reinforce the door frames and to absorb impacts from both sides.|$|R
40|$|Abstract—In this paper, {{we propose}} a simple yet {{accurate}} analytical {{model for the}} slotted non-persistent {{carrier sense multiple access}} protocol with binary exponential backoff, as specified in the medium access control (MAC) protocol of the IEEE 802. 15. 4 standard for the contention access period. The model is based on a three-level renewal process, which leads to a general analytical framework <b>applicable</b> <b>to</b> the protocol <b>variants</b> of either single or double sensing, in a saturated or unsaturated case, under a general traffic arrival distribution and with various backoff policies. The analytical model can be used to obtain some important performance metrics, such as MAC throughput and average frame service time. The accuracy of the analytical model is demonstrated by extensive simulation results. The applicability of this model to the performance analysis of other slotted MAC protocols is also briefly discussed. Index Terms—Analytical model, CSMA/CA, IEEE 802. 15. 4, MAC, renewal theory, wireless networks...|$|R
40|$|Abstract – Knowing the {{capacity}} of an Internet path is important for efficient network utilization, pricing, and management. Using {{the capacity}} information, one can provide better TCP congestion control, multimedia streaming, P 2 P peer selection, and overlay structuring. Capacity estimation has been extensively studied. Though current approaches {{have been able to}} provide fast and accurate capacity estimates, they are mostly “active ” in nature (ie, they utilize extra probing packets) and thus tend to be intrusive. In this paper, we propose TCP Probe, a “passive ” capacity estimation extension to TCP, to accurately estimate bottleneck link capacity of an Internet path. The TCP Probe extension is packet-pair based, and it is <b>applicable</b> <b>to</b> all TCP <b>variants.</b> Using simulation and Internet measurements, we show that TCP Probe is able to correctly measure bottleneck capacity of a path. Moreover, we present a simple application of TCP Probe where TCP is forced to enter slow start phase when a drastic capacity change from LOW to HIGH is detected. The results show that the capacity estimate provided by TCP Probe enables it to take better advantages of the capacity increase than the original TCP. In summary, TCP Probe is simple, passive, and accurate, and it is <b>applicable</b> <b>to</b> a broad 1 2 variety of TCP variants...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2016 - 12 Population {{structure}} is systematic {{variation in the}} human genome due to non-random mating because of physical or cultural barriers. Population {{structure is}} of interest in several fields of medicine, including population genetics, medical genetics, and personalized genomics. Advances in sequencing technology have lead to a precipitous drop in the cost to sequence the human genome, which has lead to a plethora of sequencing studies in recent years. This increase in the availability of genotype data {{has led to a}} commensurate {{increase in the number of}} statistical methods for analyzing sequence data. To date, the majority of these new methods have focused on association testing, with relatively little work on inferring population structure, despite the importance of population structure inference. There are several challenges to inferring population structure with sequencing data, including: an abundance of rare variants (loci where there is little variation across human populations) and the large number of loci. Existing methods are not directly <b>applicable</b> <b>to</b> rare <b>variants</b> and few computationally feasible methods exist. This dissertation considers the problem of inferring population structure with human genome sequence data. We present new statistical methods, with theoretical justification, extensive simulation studies, and applications to the 1000 Genomes Project data. We also develop extensions of the methods that are computationally feasible for large sequencing data sets and that allow for the use of reference population samples to better elucidate population structure from sequence data...|$|R
40|$|Abstract. This paper {{presents}} an improved impossible differential {{attack on the}} new block cipher CLEFIA which is proposed by Sony Corporation at FSE 2007. Combining some observations with new tricks, we can filter out the wrong keys more efficiently, and improve the impossible differential attack on 11 -round CLEFIA- 192 / 256, which also firstly works for CLEFIA- 128. The complexity is about 2 103. 1 encryptions and 2 103. 1 chosen plaintexts. By putting more constraint conditions on plaintext pairs, we give the first attack on 12 -round CLEFIA for all three key lengths with 2 119. 1 encryptions and 2 119. 1 chosen plaintexts. For CLEFIA- 192 / 256, our attack is <b>applicable</b> <b>to</b> 13 -round <b>variant,</b> of which the time complexity is about 2 181, and the data complexity is 2 120. We also extend our attack to 14 -round CLEFIA- 256, with about 2 245. 4 encryptions and 2 120. 4 chosen plaintexts. Moreover, a birthday sieve method is introduced to decrease {{the complexity of the}} core precomputation. Key words: Block ciphers, cryptanalysis, impossible differential attack, CLEFIA...|$|R
40|$|In {{spite of}} the success of {{genome-wide}} association studies (GWASs), only a small proportion of heritability for each complex trait has been explained by identified genetic variants, mainly SNPs. Likely reasons include genetic heterogeneity (i. e., multiple causal genetic variants) and small effect sizes of causal variants, for which pathway analysis has been proposed as a promising alternative to the standard single-SNP-based analysis. A pathway contains a set of functionally related genes, each of which includes multiple SNPs. Here we propose a pathway-based test that is adaptive at both the gene and SNP levels, thus maintaining high power across a wide range of situations with varying numbers of the genes and SNPs associated with a trait. The proposed method is <b>applicable</b> <b>to</b> both common <b>variants</b> and rare variants and can incorporate biological knowledge on SNPs and genes to boost statistical power. We use extensively simulated data and a WTCCC GWAS dataset to compare our proposal with several existing pathway-based and SNP-set-based tests, demonstrating its promising performance and its potential use in practice...|$|R
40|$|Our {{ability to}} analyze the {{microstructure}} of biological tissue in three dimensions (3 D) has proven invaluable in modeling its functionality, and therefore providing {{a better understanding of}} the basic mechanisms of life. Volumetric imaging of tissue at the cellular level, using serial imaging of consecutive tissue sections, provides such ability to acquire microstructure in 3 D. Three-dimensional light microscopy in biology can be broadly classified as using either optical sectioning or physical sectioning. Due to the inherent limitations on the depth resolution in optical sectioning, and the recent introduction of novel techniques, physical sectioning has become the sought-out method to obtain high-resolution volumetric tissue structure data. To meet this demand with increased processing speed in 3 D biological imaging, this thesis provides an engineering study and formulation of the tissue sectioning process. The knife-edge scanning microscopy (KESM), a novel physical sectioning and imaging instrument developed in the Brain Networks Laboratory at Texas A&M University, has been used for the purpose of this study. However, the modes of characterizing chatter and its measurement are equally <b>applicable</b> <b>to</b> all current <b>variants</b> of 3 D biological microscopy using physicali...|$|R
40|$|The {{critical}} exponent of {{an infinite}} word is defined {{to be the}} supremum of the exponent of each of its factors. For k-automatic sequences, we show that this critical exponent is always either a rational number or infinite, and its value is computable. Our results also apply <b>to</b> <b>variants</b> of the critical exponent, such as the initial critical exponent of Berthe, Holton, and Zamboni and the Diophantine exponent of Adamczewski and Bugeaud. Our work generalizes or recovers previous results of Krieger and others, and is <b>applicable</b> <b>to</b> other situations; e. g., the computation of the optimal recurrence constant for a linearly recurrent k-automatic sequence. Comment: In Proceedings WORDS 2011, arXiv: 1108. 341...|$|R
5000|$|In {{addition}} <b>to</b> these holidays, <b>applicable</b> <b>to</b> {{the whole}} population, {{there are four}} official public holidays <b>applicable</b> <b>to</b> specific sections of the population: ...|$|R
5000|$|The {{expression}} [...] "hybrid offence" [...] was <b>applicable</b> <b>to</b> {{an offence}} triable either on indictment or summarily. It was <b>applicable</b> <b>to</b> offences to which section 18 of the Magistrates' Courts Act 1952 applied.|$|R
