20|76|Public
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Coarse aggregates are the major constituents of concrete or asphalt mixtures and are widely used in various construction purposes. A classification system for these aggregates would provide a systematic means of aggregate identification which could be used in the selection of aggregates for different construction activities. The objectives of this research were: (1) to characterize the aggregates based on their properties (2) to develop a framework for an <b>Aggregate</b> <b>Classification</b> System (3) to provide basis for implementation of the classification system and (4) to recommend and list test procedures and equipment needed to carry out the tests on aggregates. Extensive field and laboratory investigations have been carried to study the performance of pavements made with different types of coarse aggregates and the properties which affected the performance have been incorporated in the <b>aggregate</b> <b>classification</b> system. The framework for the classification system is developed based on various physical, chemical, mechanical and thermal properties of aggregates. The classification system is developed in two stages: (1) A comprehensive system of <b>aggregate</b> <b>classification</b> incorporating all significant aggregate properties affecting performance and (2) A simplified version is arrived at from the first stage classification system. The classification system is recommended for implementation at three levels. The first level of implementation consists of aggregate identification and the second level provides detailed aggregate evaluation. The third level is aimed at supplementing the findings of the first two levels by providing a detailed evaluation of aggregates as needed. The basic tests recommended for aggregate evaluation are discussed and the required equipment for these tests are also listed. Aggregate properties and respective performance indicators are tabulated. Various areas of further research are identified and recommendations are made for implementation of the proposed classification system...|$|E
40|$|Brain {{computer}} interfaces (BCIs) offer individuals {{suffering from}} major disabilities an alternative method {{to interact with}} their environment. Sensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks; however, the traditional SMR paradigms intuitively disconnect the control and real task, making them non-ideal for complex control scenarios. In this study, we design a new, intuitively connected motor imagery (MI) paradigm using hierarchical common spatial patterns (HCSP) and context information to effectively predict intended hand grasps from electroencephalogram (EEG) data. Experiments with 5 participants yielded an <b>aggregate</b> <b>classification</b> accuracy [...] intended grasp prediction probability [...] of 64. 5 % for 8 different hand gestures, more than 5 times the chance level. Comment: This work has been submitted to EMBC 201...|$|E
40|$|Constructing {{accurate}} classifier {{based on}} association rule {{is an important}} and challenging task in data mining. In this paper, a novel combination strategy based on rough sets (RST) and evidence theory (DST) for associative classification (RSETAC) is proposed. In RSETAC, rules are regarded as classification experts, after the calculation of the basic probability assignments (bpa) according to rule confidences and evidence weights employing RST, Yang's rule of combination is employed to combine the distinct evidences to realize an <b>aggregate</b> <b>classification.</b> A numerical example is shown to highlight the procedure of the proposed method. The comparison with popular methods like CBA, C 4. 5, RIPPER and MCAR indicates that RSETAC is a competitive method for classification based on association rule...|$|E
30|$|A RF {{algorithm}} is basically an ensemble nonlinear classification and regression machine-learning algorithm, which was first proposed by Breiman (2001). The algorithm improves model robustness and prediction accuracy by <b>aggregating</b> <b>classification</b> or regression trees. The algorithm also increases diversity and predictive power by modifying the tree construction method. Each node {{of the tree}} is split by the best variable, instead {{of all of the}} input variables, from several randomly selected variables.|$|R
50|$|Only {{the best}} four results count towards the championship. The race {{classification}} {{is given by}} the <b>aggregate</b> points <b>classification</b> of the round.|$|R
40|$|Judgment (or logical) {{aggregation}} {{theory is}} logically {{more powerful than}} social choice theory and has been put to use to recover some classic results of this field. Whether it could also enrich it with genuinely new results is still controversial. To support a positive answer, we prove a social choice theorem by using the advanced nonbinary form of judgment aggregation theory developed by Dokow and Holzman (2010 c). This application involves <b>aggregating</b> <b>classifications</b> (specifically assignments) instead of preferences, and this focus justifies shifting away from the binary framework of standard judgement aggregation theory to a more general one...|$|R
40|$|This paper updates {{earlier work}} by the authors (Elias and Purcell 2004) to create a {{statistical}} classification for analysis {{of the relationship between}} higher education (HE) and employment. Based on the Standard Classification of Occupations (SOC 2010), occupation unit groups are allocated to four categories via analysis of the tasks associated with typical jobs in each unit group. For three of the four categories (Experts, Orchestrators and Communicators) we postulate a link between the constituent tasks and the skills/knowledge provided via higher education. Validation of the new <b>aggregate</b> <b>classification</b> [SOC(HE) 2010] is then undertaken, using information from the 2011 and 2012 UK Labour Force Surveys and Futuretrack – a longitudinal study of applicants to HE in 2006...|$|E
40|$|Electronic Tongue {{is a kind}} of {{intelligent}} equipment which is used to distinguish tastes. An electronic tongue made by a sensor array of ion-selective electrodes (ISE) has been developed and used for the qualitative analysis of five different kinds of mineral water. The acquired original data has been optimized by the principle component analysis (PCA) and independent component analysis (ICA). Then a wavelet neural network (WNN) model was designed based on the local optimalizing searching characteristic of BP neural network and an appropriate set of the parameters. The application results show that the performance of the proposed method surpasses the traditional BP algorithm. It can improve convergence and the learning capability of the network, and gives the Electronic Tongue a higher <b>aggregate</b> <b>classification</b> rate...|$|E
40|$|The main {{challenge}} of designing classification algorithms for sensor networks {{is the lack}} of labeled sensory data, due to the high cost of manual labeling in the harsh locales where a sensor network is normally deployed. Moreover, delivering all the sensory data to the sink would cost enormous energy. Therefore, although some classification techniques can deal with limited label information, they cannot be directly applied to sensor networks since they are designed for centralized databases. To address these challenges, we propose a hierarchical <b>aggregate</b> <b>classification</b> (HAC) protocol which can reduce the amount of data sent by each node while achieving accurate classification in the face of insufficient label information. In this protocol, each sensor node locally makes cluster analysis and forwards only its decision to th...|$|E
40|$|This paper aims {{to analyze}} {{occupational}} and industrial segregation in the Spanish labor market {{by using the}} alternative tools proposed by Alonso-Villar and Del Río (2007), along with some new extensions put forward here. In particular, two decompositions of their segregation curves are proposed. The approach followed in this article allows measuring segregation {{of women and men}} separately, since the distribution of each group of workers across occupations and industries is compared with the distribution of total employment. To analyze industrial segregation, an <b>aggregated</b> <b>classification</b> of industries in four large groups (agriculture-fishing, industry, construction and services) and another by branches of activity are considered while to study occupational segregation, several partitions of individuals and of occupations are included. Occupational and industrial segregation; Segregation curves; Gender...|$|R
40|$|Objectives: Demonstration of the {{applicability}} of a framework called indirect classification to the example of glaucoma classification. Indirect classification combines medical a priori knowledge and statistical classification methods. The method is compared to direct classification approaches {{with respect to the}} estimated misclassification error. Methods: Indirect classification is applied using classification trees and the diagnosis of glaucoma. Misclassification errors are reduced by bootstrap aggregation. As direct classification methods linear discriminant analysis, classification trees and bootstrap <b>aggregated</b> <b>classification</b> trees are utilized in the problem of glaucoma diagnosis. Misclassification rates are estimated via 10 -fold cross-validation. Results: Indirect classification techniques reduce the misclassification error in the context of glaucoma classification compared to direct classification methods. Conclusions: Embedding a priori knowledge into statistical classification techniques can improve misclassification results. Indirect classification offers a framework to realize this combination...|$|R
40|$|Abstract: Alkali-silica {{reaction}} {{is one of}} the most recognized deleterious phenomenon in concrete that results in excessive expansion, cracks, loss in mechanical properties and serviceability problems. This paper reports overview of alkali-silica reactivity (ASR) in concrete including background, chemistry behind ASR, factors affecting ASR, and symptoms of ASR. The aggregates susceptible to ASR were evaluated using field performance, petrographic analysis, aggregate mineralogy and the standard and modified methods of ASTM C 1260 and C 1293, and their modifications. <b>Aggregate</b> <b>classifications</b> into innocuous and reactive based on the stated mineralogy tests, and the expansion limits of the standard methods and their modifications were compared. The study demonstrated that none of the single method is an ideal approach to evaluate the alkali-silica reactivity of an aggregate, and a suitable combination of various methods can be utilized to better predict the potential ASR reactivity of an aggregate...|$|R
40|$|Ensemble {{learning}} algorithms {{combine the}} results of several classifiers to yield an <b>aggregate</b> <b>classification.</b> We present a normative evaluation of combination methods, applying and extending existing axiomatizations from social choice theory and statistics. For the case of multiple classes, we show that several seemingly innocuous and desirable properties are mutually satisfied only by a dictatorship. A weaker set of properties admit only the weighted average combination rule. For the case of binary classification, we give axiomatic justifications for majority vote and for weighted majority. We also show that, even when all component algorithms report that an attribute is probabilistically independent of the classification, common ensemble algorithms often destroy this independence information. We exemplify these theoretical results with experiments on stock market data, demonstrating how ensembles of classifiers can exhibit canonical voting paradoxes. 1. Introduct [...] ...|$|E
40|$|Abstract—Electronic Tongue {{is a kind}} of {{intelligent}} equipment which is used to distinguish tastes. An electronic tongue made by a sensor array of ion-selective electrodes (ISE) has been developed and used for the qualitative analysis of five different kinds of mineral water. The acquired original data has been optimized by the principle component analysis (PCA) and independent component analysis (ICA). Then a wavelet neural network (WNN) model was designed based on the local optimalizing searching characteristic of BP neural network and an appropriate set of the parameters. The application results show that the performance of the proposed method surpasses the traditional BP algorithm. It can improve convergence and the learning capability of the network, and gives the Electronic Tongue a higher <b>aggregate</b> <b>classification</b> rate. Index Terms—electronic tongue, mineral water, principal component analysis, independent component analysis, wavelet neural network I...|$|E
40|$|The Iowa D. O. T. has a {{classification}} {{system designed to}} rate coarse aggregates as to their skid resistant characteristics. Aggregates have been classified into five functional types, with a Type 1 being the most skid resistant. A complete description of the classification system {{can be found in}} the Office of Materials Instructional Memorandum T- 203. Due to the variability of ledges within any given quarry the classification of individual ledges becomes necessary. The type of aggregate is then specified for each asphaltic concrete surface course. As various aggregates become used in a. c. paving, there is a continuing process of evaluating the frictional properties of the pavement surface. It is primarily through an effort of this sort that information on aggregate sources and individual ledges becomes more refined. This study is being conducted to provide that needed up-to-date information that can be used to monitor the <b>aggregate</b> <b>classification</b> system...|$|E
40|$|Credit {{is one of}} the {{facilities}} provided by banks to lend money to someone or a business entity within the prescribed period. The smooth repayment of credit is essential for the bank because it influences the performance as well as its presence in daily life. Acceptance of prospective credit customers should be considered to minimize the occurrence of bad credit. Classification and Regression Trees (CART) is a statistical method {{that can be used to}} identify potency of credit customer status such as current credit and bad credit. The predictor variables used in this study are gender, age, marital status, number of children, occupation, income, tenor / period, and home ownership. To improve the stability and accuracy of the prediction were used the Bootstrap <b>Aggregating</b> <b>Classification</b> and Regression Trees (Bagging CART) method. The classification of credit customers using Bagging CART gives the classification accuracy 81, 44 %...|$|R
25|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example. However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. Nonetheless, the coherence of AUC as a measure of <b>aggregated</b> <b>classification</b> performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|R
30|$|A {{lesson from}} the {{examples}} in Fig. 1 a and b is that {{the heart of the}} complexity of service level agreements is far beyond the simple configuration of arrivals at the port. Both figures show the overlap of supply chains beyond the port. Furthermore, not only do multiple supply chains use these resources, but individual chains overlap, and even in the same chain have different requirements for different cargoes. While integration with the port terminal itself is a start, there are potentially many uncoordinated factors. The risk of delay or error goes up with the number of partners and the overlap of individual customer supply chains implies that factors along the way may have an agency conflict that might create preferred treatment for one or the other chain on quixotic grounds. A standard Terms of Service which applies to the whole chain would assist these intermediate partners downstream to determine how to coordinate with each other through an <b>aggregated</b> <b>classification</b> of cargos.|$|R
40|$|PHBs {{defined in}} the IETF, the Expedited Forwarding (EF) PHD [2, 3] and the Assured Forwarding (AF) PHB [4]. The basic {{features}} of a DiffServ architecture are: (i) multiple flows are mapped to aggregate service levels, (ii) qualitative QoS assurances can be provided to applications using various service levels, and (iii) state information about every flow need not be maintained along the path. DiffServ performs <b>aggregate</b> <b>classification</b> 1 Table 1 : Differentiated Services Code Point Space Pool Code point Space Assignment Policy 1 xxxxx 0 Standard Action 2 xxxx 11 Experimental or local use 3 xxxx 01 Experimental or local use of packets in contrast to IntServ, which provides a per-flow classification. In principal, Differentiated Services will support QoS based on flows and aggregated flows by differentiation based on a certain code point. The code points are divided into three code point pools (Table 1. One is for standards {{and the other two}} are for experimental or local use. One o...|$|E
40|$|ABSTRACT: This paper {{discusses}} {{a quality}} control method, based on artificial neural networks, that enables a plant operator to quickly detect property variations during {{the production of}} stone aggregates. The group texture concept in digital image analyses, two-dimensional wavelet transforms, and artificial neural networks are reviewed first. An artificial intelligence based <b>aggregate</b> <b>classification</b> system is then described. This system relies on three-dimensional aggregate particle surface data, acquired with a laser profiler, and conversion of this data into digital images. Two-dimensional wavelet transforms are applied to the images and used to extract important features that can help to differentiate between in-spec and out-of-spec aggregates. These wavelet-based features are used as inputs to an artificial neural network, {{which is used to}} assign a predefined class to the aggregate sample. Verification tests show that this approach can potentially help a plant operator determine, in a fast and accurate manner, if the aggregates currently being produced are in-spec or out-of-spec...|$|E
40|$|The {{performance}} of hot mix asphalt, Portland cement concrete, unbound base, and subbase layers in a pavement are significantly affected by aggregate shape characteristics. Classification of coarse and fine aggregate shape properties such as shape (form), angularity, and texture, {{are important in}} predicting the {{performance of}} pavements. Consequently, {{there is a need}} to implement a system that can characterize aggregates without the limitations of the current <b>aggregate</b> <b>classification</b> standards. The Aggregate Image Measurement System (AIMS) was developed as a comprehensive and capable means of measuring aggregate shape properties. A new design of AIMS will be introduced with several modifications to improve the operational and physical components. The sensitivity, repeatability, and reproducibility are analyzed to evaluate the quality of AIMS measurements. The sensitivity of AIMS is evaluated and found to be good for several operational and aggregate parameters. Important operational and environmental factors that could affect the AIMS results are identified and appropriate limits are recommended. AIMS is able to control normal variations in the system without affecting the results. A comprehensive analysis is conducted to determine the repeatability and reproducibility of AIMS for multiple users and laboratories. Single-operator and multi-laboratory precision statements are developed for the test method in order to be implemented into test standards...|$|E
40|$|To enable {{efficient}} browsing {{and interactive}} querying of very large collections, {{such as those}} found in digital libraries, {{it is essential to}} provide users with summaries of query result sets. Smart indexes can be used to generate summary statistics, <b>aggregated</b> <b>classification</b> information, and/or <b>aggregated</b> contentbased information for the result sets of arbitrary queries. We present the basic model of a smart index, as well as variations of smart indexes that are suitable when the size of summaries is large. An algorithm for generating summaries of the results of arbitrary queries is given, and algorithms for updating various summaries are discussed. Experimental results show that smart indexes generate summaries much more efficiently than traditional trees for all query areas greater than 1 %- 2 % of the data space, with a relatively small additional storage overhead. Contrary to traditional trees, smart indexes in general perform better as the query area grows larger. 1. Introduction D [...] ...|$|R
50|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example. However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. Nonetheless, the coherence of AUC as a measure of <b>aggregated</b> <b>classification</b> performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|R
30|$|When <b>aggregating</b> the <b>classification</b> {{results for}} both {{observers}} {{in order to}} compare inter-method agreement between quantitative and visual assessment of planar scintigraphy, k was 0.42 (p[*]<[*] 0.01). When comparing the same quantitative results to visual assessment of the SPECT/CT, k was 0.62 (p[*]<[*] 0.01). The results of the aggregated comparison of visual and quantitative assessment and the uptake ratios {{can be found in}} Table  8.|$|R
40|$|It is well {{understood}} that binary classifiers have two implicit objective functions (sensitivity and specificity) describing their performance. Traditional methods of classifier training attempt to combine these two objective functions (or two analogous class performance measures) into one so that conventional scalar optimization techniques can be utilized. This involves incorporating a priori information into the aggregation method {{so that the}} resulting performance of the classifier is satisfactory for the task at hand. We have investigated {{the use of a}} niched Pareto multiobjective genetic algorithm (GA) for classifier optimization. With niched Pareto GA's, an objective vector is optimized instead of a scalar function, eliminating the need to <b>aggregate</b> <b>classification</b> objective functions. The niched Pareto GA returns a set of optimal solutions that are equivalent {{in the absence of any}} information regarding the preferences of the objectives. The a priori knowledge that was used for aggregating the objective functions in conventional classifier training can instead be applied post-optimization to select from one of the series of solutions returned from the multiobjective genetic optimization. We have applied this technique to train a linear classifier and an artificial neural network (ANN), using simulated datasets. The performances of the solutions returned from the multiobjective genetic optimization represent a series of optimal (sensitivity, specificity) pairs, which can be thought of as operating points on a receiver operating characteristic (ROC) curve. All possible ROC curves for a given dataset and classifier are less than or equal to the ROC curve generated by the niched Pareto genetic optimization...|$|E
40|$|This paper {{summarizes}} the initial formulation of a micro-simulation model for activity-based travel demand forecasting that integrates household activities, land use distributions, regional demographics and transportation networks in an explicitly time-dependent fashion. Intended {{to form the}} initial elements of {{an alternative to the}} conventional four-step transportation planning process, the prototype model incorporates an activity-based travel behavior model in a micro-simulation approach utilizing a geographic information system platform to manipulate survey, demographic, land use and network databases. An <b>aggregate</b> <b>classification</b> using travel diaries produces representative activity patterns that are implicitly specified in terms of temporal information, activity purpose and sequencing. The classification also provides probability distributions of activity dimensions such as purpose and duration. Additional households are sampled and, based on demographic, land use and network characteristics provided by the GIS, a target representative activity pattern is specified as are ambient activity densities. Activity characterists such as purpose and duration are drawn from the distributions associated with the target pattern: trips are sequentially simulated based on a Monte Carlo approach of potential activity-specific destinations within a range of travel times from the prior and the home locations. The nature of the simulation is such that the simulated pattern, while maintaining the general characteristics of the target representative pattern, reflects the activity distributions and network characteristics of the household being simulated. The resultant set of activity patterns may be aggregated for any defined spatial-temporal limits. The model provides an activity-based method for estimating dynamic, linked-trip, origin-destination demand matrices. Effectively replacing the generation and distribution components of the conventional process, the model represents a potentially important step toward the development of alternative transportation planning methods...|$|E
40|$|In this paper, {{we develop}} a novel {{methodology}} within the IDCP measuring framework for comparing normalization procedures based on different classification systems of articles into scientific disciplines. Firstly, {{we discuss the}} properties of two rankings, based on a graphical and a numerical approach, for the comparison of any pair of normalization procedures using a single classification system for evaluation purposes. Secondly, when the normalization procedures are based on two different classification systems, we introduce two new rankings following the graphical and the numerical approaches. Each ranking {{is based on a}} double test that assesses the two normalization procedures in terms of the two classification systems on which they depend. Thirdly, we also compare the two normalization procedures using a third, independent classification system for evaluation purposes. In the empirical part of the paper we use: (i) a classification system consisting of 219 sub-fields identified with the Web of Science subject-categories; an <b>aggregate</b> <b>classification</b> system consisting of 19 broad fields, as well as a systematic and a random assignment of articles to sub-fields with the aim of maximizing or minimizing differences across sub-fields; (ii) four normalization procedures that use the field or sub-field mean citations of the above four classification systems as normalization factors; and (iii) a large dataset, indexed by Thomson Reuters, in which 4. 4 million articles published in 1998 – 2003 with a five-year citation window are assigned to sub-fields using a fractional approach. The substantive results concerning the comparison of the four normalization procedures indicate that the methodology can be useful in practice. The authors acknowledge financial support by Santander Universities Global Division of Banco Santander. Ruiz-Castillo also acknowledges financial help from the Spanish MEC through grant ECO 2011 - 29762...|$|E
40|$|The paper {{deals with}} {{measurement}} error, and its potentially distorting role, in information on industry and professional status collected by labour force surveys. The {{focus of our}} analyses is on inconsistent information on these employment characteristics resulting from yearly transition matrices for workers who were continuously employed over the year and who did not change job. As a case-study we use yearly panel data for the period from April 1993 to April 2003 collected by the Italian Quarterly Labour Force Survey. The analysis goes through four steps: (i) descriptive indicators of (dis) agreement; (ii) testing whether the consistency of repeated information significantly increases {{when the number of}} categories is collapsed; (iii) examination of the pattern of inconsistencies among response categories by means of Goodman’s quasi-independence model; (iv) comparisons of alternative classifications jointly by professional status and occupation. Results document sizable measurement error, which is only moderately reduced by more <b>aggregated</b> <b>classifications.</b> They suggest that even cross-section estimates of employment by industry and/or professional status are affected by non-random measurement erro...|$|R
40|$|The talk {{introduces}} a new <b>aggregated</b> <b>classification</b> scheme aimed {{to support the}} implementation of text analysis methods in contexts characterised {{by the presence of}} rare text categories. This approach starts from the aggregate supervised text classifier developed by Hopkins and King and moves forward relying on rare event sampling methods. In details, it enables the analyst to enlarge the number of text categories whose proportions can be estimated preserving the estimation accuracy of standard aggregate supervised algorithms and reducing the working time w. r. t. to unconditionally increase the size of the random training set. The approach is applied to study the daily evolution of the web reputation of Expo Milano 2015, before, during and after the event. The data set is constituted by about 900, 000 tweets in Italian and 260, 000 tweets in English, posted about the event between March 2015 and December 2015. The analysis provides an interesting portray of the evolution of Expo stakeholders’ opinions over time and allow to identify the main drivers of Expo reputation. The algorithm will be implemented as a running option of the next release of R package ReadM...|$|R
30|$|F 2 topic-interesting-me {{reflects}} {{the extent to}} which the retweeter is interested in the article’s topic. To compute it, we create, for each user, his interest-vector by considering each article the user posts, classifying the article’s categories, and <b>aggregating</b> the <b>classifications</b> of all the user’s articles into a unique interest-vector. The classification consists of 12 categories and is performed by the Alchemy Application Programming Interface ([URL] which is a popular text-mining web service that classifies news articles in a number of topic.|$|R
40|$|Shape, texture, and {{angularity}} {{are among}} the properties of aggregates that {{have a significant effect}} on the performance of hot-mix asphalt, hydraulic cement concrete, and unbound base and subbase layers. Consequently, there is a need to develop methods that can quantify aggregate shape properties rapidly and accurately. In this study, an improved version of the Aggregate Imaging System (AIMS) was developed to measure the shape characteristics of both fine and coarse aggregates. Improvements were made in the design of the hardware and software components of AIMS to enhance its operational characteristics, reduce human errors, and enhance the automation of test procedure. AIMS was compared against other test methods that have been used for measuring aggregate shape characteristics. The comparison was conducted based on statistical analysis of the accuracy, repeatability, reproducibility, cost, and operational characteristics (e. g. ease of use and interpretation of the results) of these tests. Aggregates that represent a wide range of geographic locations, rock type, and shape characteristics were used in this evaluation. The comparative analysis among the different test methods was conducted using the Analytical Hierarchy Process (AHP). AHP is a process of developing a numerical score to rank test methods based on how each method meets certain criteria of desirable characteristics. The outcomes of the AHP analysis clearly demonstrated the advantages of AIMS over other test methods as a unified system for measuring the shape characteristics of both fine and coarse aggregates. A new <b>aggregate</b> <b>classification</b> methodology based on the distribution of their shape characteristics was developed in this study. This methodology offers several advantages over current methods used in practice. It is based on the distribution of shape characteristics rather than average indices of these characteristics. The coarse aggregate form is determined based on three-dimensional analysis of particles. The fundamental gradient and wavelet methods are used to quantify angularity and surface texture, respectively. The classification methodology can be used for the development of aggregate shape specifications...|$|E
40|$|The {{objective}} {{of this study is}} to characterize and predict the permanent deformation properties of unbound granular materials (UGMs) for Pavement ME Design. First, laboratory repeated load triaxial (RLT) tests are conducted on the UGMs from 11 quarries in Texas to measure the permanent strain curves. The shakedown theory is applied to evaluate the permanent deformation behavior of the selected UGMs. It is found that using Werkmeister's criteria to define the shakedown range boundaries is not suitable for the selected UGMs. Under this circumstance, new criteria are proposed to redefine the shakedown range boundaries for the flexible base materials in Texas. The new criteria are consistent with the current Texas flexible base specification in terms of <b>aggregate</b> <b>classification.</b> Second, the mechanistic-empirical design guide (MEPDG) model is used to determine the permanent deformation properties of the selected UGMs on the basis of the measured permanent strain curves. The determined permanent deformation properties are assigned as target values for the development of permanent deformation prediction models. Third, a series of performance-related base course properties are used to comprehensively characterize the UGMs, which include the dry density, moisture content, aggregate gradation, morphological properties, percent fines content, and methylene blue value. These performance-related base course properties are assigned as the inputs of the permanent deformation prediction models. Fourth, a multiple regression analysis is conducted to develop the prediction models for permanent deformation properties using these performance-related properties. The developed models are capable of accurately predicting the permanent deformation properties of UGMs. Compared to other prediction models (e. g., simple indicators-based models and Pavement ME Design models), the developed models have the highest prediction accuracy. It is also found that the Pavement ME model-predicted permanent strains are much lower than those measured from the RLT tests. This demonstrates that the current Pavement ME Design software substantially underestimates the rutting that occurs in base course. Finally, the developed prediction models are validated by comparing the predicted and measured permanent strains of other four base materials. The obtained R-squared value of 0. 81 indicates that the developed models have a desirable accuracy in the prediction of permanent deformation properties of UGMs...|$|E
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 120 - 123). Issued also on microfiche from Lange Micrographics. Skid resistance on wet pavements is influenced by friction at the tire-pavement interface as well as overall hot mix asphalt (HMA) performance. It is important to control aggregate, asphalt, and mix properties to achieve desirable frictional properties on HMA during its service life. Aggregate consensus and source properties influence frictional properties at the surface as well as aggregate matrix properties that affect overall skid performance. Thus, it is important to identify and control these properties through an effective testing and monitoring program. Research studies have indicated that current testing protocol for pre-qualification of aggregates being used by DOT's is tenuous and needs definitive evaluation. The validity of some tests currently being used for pre-qualification of aggregates is being questioned due to poor field correlation. Thus, {{there is a need for}} upgrading current testing criteria and <b>aggregate</b> <b>classification</b> system in view of new techniques that can be used either as replacements and/or supplements to current tests. This study, a part of the Texas Department of Transportation (TxDOT) current research program to evaluate inadequacies of current tests to skid performance, focuses on tests evaluating aggregate shape and distribution parameters. In this study, a wet weather test selection criteria was developed to evaluate the effectiveness of current and new testing techniques to monitor aggregate shape, texture, and distribution characteristics. Extensive tests were conducted on forty aggregates selected from TxDOT Quality Material Catalogue covering various parts of U. S. A. Fine aggregates tests including the Uncompacted Void Content, the Compacted Aggregate Resistance, the Methylene Blue, and the Particle Size Analysis were performed to evaluate angularity, texture, and distribution characteristics within fine aggregates. Flat and elongated tests on coarse aggregates were also performed using both conventional and automated techniques to analyze shape and size distribution characteristics. A statistical analysis was performed to select tests that would enable monitoring of aggregate shape and distribution properties enhancing skid performance. The evaluation criteria were based upon a sensitivity and correlation analysis to evaluate consistency, reproducibility, and ability of tests to effectively discern aggregates with good and marginal performance...|$|E
30|$|Companies adopt {{a variety}} of {{financial}} performance measures in executive compensation plans. Prior literature shows that performance measure choice affects managerial decisions (e.g., Marquardt and Wiedman, 2005; Young and Yang, 2011; Huang et al., 2014). Performance measures adopted in executive compensation contracts are important in communicating corporate objectives to managers and evaluating managerial performance. Due to data limitations, prior studies in executive compensation have generally investigated <b>aggregate</b> performance measure <b>classifications,</b> such as net income and stock returns, rather than specific performance measures (Ittner and Larcker, 2002). However, Ittner and Larcker (2002) assert that the <b>aggregate</b> performance measure <b>classifications,</b> such as accounting vs. market performance measures, commonly used in compensation research provide somewhat misleading inferences regarding performance measure choices since factors influencing the use of specific measures vary.|$|R
40|$|Poster Session: UltrasoundMitral valve repair {{is one of}} {{the most}} {{prevalent}} operations for various mitral valve conditions. Echocardiography, being famous for its low-cost, non-invasiveness and speediness, is the dominant imaging modality used for carrying out mitral valve condition analysis in both pre-operative and intra-operative examinations. In order to perform analysis on different phases of a cardiac cycle, it is necessary to first classify the echocardiograhic data into volumes corresponding to the systole and diastole phases. This often requires tedious manual work. This paper presents a fully-automatic method for systole-diastole classification of real-time three-dimensional transesophageal echocardiography (RT- 3 D-TEE) data. The proposed method first resamples the data with radial cutting planes, segments the mitral valve by thresholding, and removes noise by median filtering. Classification is then carried out based on the number of identified mitral valve regions. A multiresolution processing scheme is proposed to further improve the <b>classification</b> accuracy by <b>aggregating</b> <b>classification</b> results obtained from different image resolution scales. The proposed method was evaluated against the classification results produced by a cardiologist. Experimental results show that the proposed method, without the use of computationally intensive algorithms or the use of any training database, can achieve a classification accuracy of 91. 04 %. published_or_final_versio...|$|R
40|$|Numerous {{methods are}} {{currently}} available for motion detection using background modeling and subtraction. However, {{there are still}} many challenges to take into account such as moving shadows, illumination changes, moving background, relocation of background objects, and initialization with moving objects. This paper provides a new background subtraction algorithm that <b>aggregates</b> the <b>classification</b> results of several foreground extraction techniques based on UV color deviations, probabilistic gradient information and vector deviations, in order to produce a single decision that is more robust to those challenges. Categories and Subject Descriptors I. 4 [Image processing and computer vision]: Segmentation- pixe...|$|R
