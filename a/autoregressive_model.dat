2742|5402|Public
2500|$|The {{notation}} AR(p) {{refers to}} the <b>autoregressive</b> <b>model</b> of order p. The AR(p) model is written ...|$|E
2500|$|Another {{generalization}} is the multiscale autoregressive (MAR) model. [...] A MAR {{model is}} indexed by the nodes of a tree, whereas a standard (discrete time) <b>autoregressive</b> <b>model</b> is indexed by integers.|$|E
5000|$|... #Subtitle level 3: SETAR as an Extension of the <b>Autoregressive</b> <b>Model</b> ...|$|E
40|$|We {{compare the}} {{forecasting}} performance of linear <b>autoregressive</b> <b>models,</b> <b>autoregressive</b> <b>models</b> with structural breaks, self-exciting threshold <b>autoregressive</b> <b>models,</b> and Markov switching <b>autoregressive</b> <b>models</b> {{in terms of}} point, interval, and density forecasts for h-month growth rates of industrial production of the G 7 countries, for the period January 1960 -December 2000. The results of point forecast evaluation tests support the established notion in the forecasting literature on the favorable performance of the linear AR model. By contrast, the Markov switching models render more accurate interval and density forecasts than the other models, including the linear AR model. This encouraging finding supports the idea that non-linear models may outperform linear competitors in terms of describing the uncertainty around future realizations of a time series. nonlinearity;structural change;density forecasts;forecast evaluation tests;interval forecasts...|$|R
40|$|Full <b>autoregressive</b> <b>models</b> {{are always}} {{characterize}} by many parameters {{and this is}} a problem. Some of these parameters are redundant that is close to zero and there is the need to eliminate these parameters through the concept of subsetting. Subsets <b>autoregressive</b> <b>models</b> are free from redundant parameters thereby lowering the residual variance and forecasting with such models will always give a better forecast. Likewise auto projective models calculate on the basis of current knowledge what the errors would have been which gives us some guide to errors of the future. It is {{in the light of the}} above we considered the subsets <b>autoregressive</b> <b>models</b> and auto projective models, to see how these models will perform with regard to forecast. Exponential smoothening was used to forecast the future value in auto projective models while conditional least square predictor was used to forecast the future value in subset <b>autoregressive</b> <b>models.</b> An algorithm was proposed to eliminate redundant parameters from the full order <b>autoregressive</b> <b>models</b> and the parameters were estimated. To determine optimal models, residual variance, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) were adopted. Results revealed that the residual variance attached to the subset <b>autoregressive</b> <b>models</b> is smaller than the residual variance attached to the auto projective models. We conclude that the forecast for subset autoregressive is preferred to the forecast for auto projective...|$|R
5000|$|MATLAB's Econometrics Toolbox [...] and System Identification Toolbox [...] {{includes}} <b>autoregressive</b> <b>models</b> ...|$|R
5000|$|The {{notation}} [...] {{indicates an}} <b>autoregressive</b> <b>model</b> of order p. The AR(p) model {{is defined as}} ...|$|E
5000|$|The {{notation}} AR(p) {{refers to}} the <b>autoregressive</b> <b>model</b> of order p. The AR(p) model is written ...|$|E
5000|$|As another example, {{consider}} a first-order <b>autoregressive</b> <b>model,</b> defined byxi = c + φxi−1 + εi, with the εi being i.i.d. Gaussian (with zero mean).For this model, {{there are three}} parameters: c, φ, and the variance of the εi. More generally, a pth-order <b>autoregressive</b> <b>model</b> has p + 2 parameters.(If, however, c is not estimated, but given in advance, then there are only p + 1 parameters.) ...|$|E
50|$|In statistics, Smooth Transition <b>Autoregressive</b> (STAR) <b>models</b> are {{typically}} applied to time series data {{as an extension}} of <b>autoregressive</b> <b>models,</b> in order to allow for higher degree of flexibility in model parameters through a smooth transition.|$|R
50|$|In statistics, Self-Exciting Threshold <b>AutoRegressive</b> (SETAR) <b>models</b> are {{typically}} applied to time series data {{as an extension}} of <b>autoregressive</b> <b>models,</b> in order to allow for higher degree of flexibility in model parameters through a regime switching behaviour.|$|R
40|$|A {{solution}} to the problem of parameter estimation of linear <b>autoregressive</b> <b>models</b> of time series is proposed based on a Kalman estimator. Two cases are considered - when the data are stationary time series and nonstationary series with unknown drift. The proposed algorithm for estimation of the parameters of adaptive <b>autoregressive</b> <b>models</b> is compared to the widely used Yule-Walker estimator...|$|R
5000|$|<b>Autoregressive</b> <b>model</b> (AR) estimation, which {{assumes that}} the nth sample is {{correlated}} with the previous p samples.|$|E
50|$|Akaike, H. (1979). A Bayesian {{extension}} of the minimum AIC procedure of <b>autoregressive</b> <b>model</b> fitting. Biometrika 66, 237-242.|$|E
50|$|Contrary to the {{moving-average}} model, the <b>autoregressive</b> <b>model</b> is {{not always}} stationary as it may contain a unit root.|$|E
50|$|Several {{classes of}} {{nonlinear}} <b>autoregressive</b> <b>models</b> formulated for time series applications have been threshold models.|$|R
40|$|The hybrid {{coding scheme}} is modified. The {{discrete}} cosine transform used for encoding displaced frame differences {{is replaced by}} a predefined set of transform matrices based on <b>autoregressive</b> <b>models</b> up to the fourth order. The <b>autoregressive</b> <b>models</b> are parameterized using reflection coefficients. It is shown that the coding efficiency can be improved though side information for the chosen transform has to be transmitted to the decoder...|$|R
40|$|This paper {{features}} {{an analysis of}} major currency exchange rate movements {{in relation to the}} US dollar, as constituted in US dollar terms. Euro, British pound, Chinese yuan, and Japanese yen are modelled using a variety of non- linear models, including smooth transition regression models, logistic smooth transition regressions <b>models,</b> threshold <b>autoregressive</b> <b>models,</b> nonlinear <b>autoregressive</b> <b>models,</b> and additive nonlinear <b>autoregressive</b> <b>models,</b> plus Neural Network models. The results suggest that there is no dominating class of time series models, and the different currency pairs relationships with the US dollar are captured best by neural net regression models, over the ten year sample of daily exchange rate returns data, from August 2005 to August 2015...|$|R
50|$|An <b>autoregressive</b> <b>model</b> {{can thus}} {{be viewed as the}} output of an {{all-pole}} infinite impulse response filter whose input is white noise.|$|E
5000|$|The {{first order}} <b>autoregressive</b> <b>model,</b> , has a unit root when [...] In this example, the {{characteristic}} equation is [...] The {{root of the}} equation is [...]|$|E
50|$|Another {{generalization}} is the multiscale autoregressive (MAR) model. A MAR {{model is}} indexed by the nodes of a tree, whereas a standard (discrete time) <b>autoregressive</b> <b>model</b> is indexed by integers.|$|E
5000|$|Matlab and Octave: the TSA toolbox {{contains}} several estimation {{functions for}} uni-variate, multivariate and adaptive <b>autoregressive</b> <b>models.</b>|$|R
40|$|Mixture {{periodic}} <b>autoregressive</b> <b>models</b> {{are introduced}} to fit periodic time series with asymmetric or multimodal distributions. The stationary conditions of such series are derived, the asymptotic property of maximum likelihood estimators is obtained, {{and the application of}} EM algorithm is discussed. The new model class is illustrated by analyzing the particulate matter concentrations in Cleveland, OH. Periodically correlated time series Periodic autocovariances Mixture periodic <b>autoregressive</b> <b>models</b> EM algorithm...|$|R
40|$|The paper {{provides}} a proof of {{consistency of the}} ridge estimator for regressions where the number of regressors tends to infinity. Such result is obtained without assuming a factor structure. A Monte Carlo study suggests that shrinkage <b>autoregressive</b> <b>models</b> can lead to very substantial advantages compared to standard <b>autoregressive</b> <b>models.</b> An empirical application focusing on forecasting inflation and GDP growth in a panel of countries confirms this finding. Shrinkage, Forecasting...|$|R
5000|$|The <b>Autoregressive</b> <b>model</b> {{is one of}} a {{group of}} linear {{prediction}} formulas that attempt to predict an output y_n of a system based on previous set of outputs {y_k} where k < n and inputs x_n and {x_k} where k < n. There exist minor changes in the way the predictions are computed based on which, several variations of the model are developed. Basically, when the model depends only on the previous outputs of the system, it is referred to as an auto-regressive model. It is referred to as a Moving Average Model (MAM), if it depends on only the inputs to the system. Finally, Autoregressive-Moving Average models are those that depend both on the inputs and the outputs, for prediction of current output. <b>Autoregressive</b> <b>model</b> of order p, denoted as AR(p), has the following form:Xt = R1 Xt-1 + R2 Xt-2 + ... + Rp Xt-p + Wtwhere Wt is the white noise, Ri are real numbers and Xt are prescribed correlated random numbers. The auto-correlation function of the AR(p) process consists of damped sine waves depending on whether the roots (solutions) of the model are real or imaginary. Discrete <b>Autoregressive</b> <b>Model</b> of order p, denoted as DAR(p), generates a stationary sequence of discrete random variables with a probability distribution and with an auto-correlation structure similar to that of the <b>Autoregressive</b> <b>model</b> of order p.3 ...|$|E
5000|$|The {{most common}} form of {{parametric}} SDF estimate uses as a model an <b>autoregressive</b> <b>model</b> [...] of order [...] A signal sequence [...] obeying a zero mean [...] process satisfies the equation ...|$|E
5000|$|In {{time series}} modeling, a {{nonlinear}} autoregressive exogenous model (NARX) is a nonlinear <b>autoregressive</b> <b>model</b> which has exogenous inputs. This {{means that the}} model relates the current value of a time series to both: ...|$|E
40|$|In this paper, we {{extend the}} minimum {{distance}} method of Beran (1993) to random coefficient <b>autoregressive</b> (RCA) <b>models.</b> After stating the necessary assumptions the asymptotic {{properties of the}} minimum distance estimator are derived. Random coefficient <b>autoregressive</b> <b>models</b> Minimum distance estimator Absolute regularity Geometric ergodicity...|$|R
40|$|A kernel-based {{approach}} for nonlinear modeling of time series data is proposed in this paper. <b>Autoregressive</b> <b>modeling</b> is achieved in a feature space {{defined by a}} kernel function using a linear algorithm. The method extends {{the advantages of the}} conventional <b>autoregressive</b> <b>models</b> to characterization of nonlinear signals through the intelligent use of kernel functions. Experiments with synthetic signals demonstrate that this method seems to be a promising alternative to nonlinear modeling schemes. 1...|$|R
5000|$|Constants of <b>autoregressive</b> <b>models</b> {{that are}} based on the fast Fourier {{transform}} algorithm (FFT). These coefficients are ARC, ARF and ARE.|$|R
5000|$|The {{testable}} {{definition of}} causality {{was introduced by}} Granger. Granger causality principle states that if some series Y(t) contains information in past terms that helps in the prediction of series X(t), then Y(t) is said to cause X(t). Granger causality principle can be {{expressed in terms of}} two-channel multivariate <b>autoregressive</b> <b>model</b> (MVAR). Granger in his later work [...] pointed out that the determination of causality is not possible when the system of considered channels is not complete.The measures based on Granger causality principle are: Granger Causality Index (GCI), Directed Transfer Function (DTF) and Partial Directed Coherence (PDC). These measures are defined in the framework of Multivariate <b>Autoregressive</b> <b>Model.</b>|$|E
5000|$|The third {{monitoring}} tool in RODS is a recursive least squares (RLS) algorithm, which fits an <b>autoregressive</b> <b>model</b> to the counts and updates estimates continuously by minimizing prediction error. A Shewhart I-chart is then {{applied to the}} residuals, using a threshold of 4 standard deviations.|$|E
50|$|The test is {{more general}} than the Durbin-Watson {{statistic}} (or Durbin's h statistic), {{which is only}} valid for nonstochastic regressors and for testing {{the possibility of a}} first-order <b>autoregressive</b> <b>model</b> (e.g. AR(1)) for the regression errors. The BG test has none of these restrictions, and is statistically more powerful than Durbin's h statistic.|$|E
40|$|One of {{the most}} {{challenging}} problems in intensive care is the process of discontinuing mechanical ventilation, called weaning process. An unnecessary delay in the discontinuation process and an early weaning trial are undesirable. This paper proposes to analysis the respiratory pattern variability of these patients using <b>autoregressive</b> <b>modeling</b> techniques: <b>autoregressive</b> <b>models</b> (AR), <b>autoregressive</b> moving average <b>models</b> (ARMA), and <b>autoregressive</b> <b>models</b> with exogenous input (ARX). A total of 153 patients on weaning trials from mechanical ventilation were analyzed: 94 patients with successful weaning (group S); 38 patients that failed to maintain spontaneous breathing(group F), and 21 patients who had successful weaning trials,but required reintubation in less than 48 h (group R). The respiratory pattern was characterized by their time series. The results show that significant differences were obtained with parameters as model order and first coefficient of AR model, and final prediction error by ARMA model. An accuracy of 86...|$|R
40|$|Respiratory rate is {{recognised}} as {{a valuable}} predictor {{of the severity of}} illness in children, but it is not currently feasible to measure this automatically in a triage environment. <b>Autoregressive</b> <b>modelling</b> on data from the pulse oximeter photoplethysmogram has the potential to introduce automated breathing measurement into the realm of paediatric triage. Using <b>autoregressive</b> <b>modelling,</b> it is shown that respiratory rate can be extracted from the paediatric photoplethysmogram with a mean error of 3. 4 breaths per minute...|$|R
40|$|In {{this work}} we propose {{the use of}} Vector <b>Autoregressive</b> <b>Models</b> (ST-VAR) and Generalized Vector <b>Autoregressive</b> <b>Models</b> (ST-GVAR) for spatio-temporal data in which {{relations}} among different sites of a monitoring network are not exclusively dependent on the distance among them but take also into account intensity and direction that a phenomenon measured on the whole network has on a single site of the network. We present an application of these models using data of fifteen italian meteorological locations of Mediterranean Basin...|$|R
