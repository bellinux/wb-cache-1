0|10000|Public
40|$|CAD, or {{computer-aided}} design, is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> that technical professionals use. This {{course will}} introduce {{the student to}} the process of modeling 2 - and 3 -D objects with computer-aided design; the student will learn the basics of drafting by hand as well. This free course may be completed online at any time. See course site for detailed overview and learning outcomes. (Mechanical Engineering 104) ...|$|R
40|$|Abstract. State-space <b>models</b> offer <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> {{for time}} series prediction. However, as most {{algorithms}} are not optimized for longterm prediction, {{it may be}} hard to achieve good prediction results. In this paper, we investigate Gaussian linear regression filters for parameter estimation in state-space models and we propose new long-term prediction strategies. Experiments using the EM-algorithm for training of nonlinear state-space models show that significant improvements are possible with no additional computational cost. ...|$|R
40|$|Mesh {{morphing}} {{techniques are}} capable of producing a sequence of meshes, gradually changing from a source to a target shape. However, current techniques do not allow to describe the local behavior of the morph. A {{solution to this problem}} is presented. The main idea is to describe mesh geometry in a differential way, thus, insertion of local features from one shape into another does not suffer from difference in absolute coordinates. Besides interesting possibilities for animation the technique proves to be <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool...</b>|$|R
40|$|Abstract — Although chaotic {{systems have}} {{received}} in-creasing attention {{over the past}} decades, traditional mod-eling tools have always encountered considerable analytical and numerical difficulties in modeling and predicting the behavior of chaotic systems. Neural networks, on the other hand, {{seem to be able}} to introduce <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> relying on their nonlinear nature for the task of modeling. In this paper, we introduce a novel scheme for the modeling task of multi-dimensional discrete-time chaotic maps rely-ing on the capabilities of perceptron neural networks and present some of the related experimental results...|$|R
40|$|Abstract—Though hybrid {{dynamical}} {{systems are}} <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool,</b> it has proven difficult to accurately simulate their trajectories. In this paper, we develop a provably convergent numerical integration scheme for approximating trajectories of hybrid dynamical systems. This {{is accomplished by}} first relaxing hybrid systems whose continuous states reside on manifolds by attaching epsilon-sized strips to portions of the boundary and then extending the dynamic and distance metric onto these strips. On this space we develop a numerical integration scheme and prove that discrete approximations converge to trajectories of the hybrid system. An example is included to illustrate the approach. I...|$|R
40|$|AbstractStrategic asset {{management}} of urban water infrastructures jointly deals with assets of diverse nature, useful life, cost, age and condition. Service sustainability requires a sound long-term planning, which needs assessing, among other aspects: {{the value of}} the infrastructure over time; the need for reinvestments; the impact of long-term re-investment policies. The infrastructure value index (IVI) was proven to be <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> for combined long-term planning of linear and vertical assets. An open-source software enables IVI assessment for both asset-by-asset detailed inventory and for simplified cohort-based infrastructure description. This paper presents the formulation, discusses the underlying assumptions and applicability, and illustrates its use for strategic plannin...|$|R
40|$|Abstract:- Fuzzy {{cognitive}} map is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool.</b> It has several desirable properties on control. In this paper, we utilize the feature and the inference mechanism of fuzzy {{cognitive map}}, and present a control method, which study combines control theory with fuzzy cognitive map theory. The causal relationship of variables is constructed by online learning or offline learning, {{the values of}} control variables are given by the inference of FCM model, and the control variables are used to adjust the value of controlled variables in actual process and carries out the control of multi-input and multi-out. Key-Words:- fuzzy cognitive map, system control, system modeling, control framewor...|$|R
40|$|In {{the context}} of optimal control, we {{consider}} the inverse problem of Lagrangian identification given system dynamics and optimal trajectories. Many of its theoret-ical and practical aspects are still open. Potential applications are very broad as a reliable {{solution to the problem}} would provide <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> in many areas of experimental science. We propose to use the Hamilton-Jacobi-Bellman sufficient optimality conditions for the direct problem as a tool for analyzing the inverse problem and propose a general method that attempts at solving it numeri-cally with techniques of polynomial optimization and linear matrix inequalities. The relevance of the method is illustrated based on simulations on academic examples under various settings. ...|$|R
40|$|Despite the {{benefits}} deriving from explicitly modeling concept disjointness {{to increase the}} quality of the ontologies, the number of disjointness axioms in vocabularies for the Web of Data is still limited, thus risking to leave important constraints underspecified. Automated methods for discovering these axioms may represent <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for knowledge engineers. For the purpose, we propose a machine learning solution that combines (unsupervised) distance-based clustering and the divide-and-conquer strategy. The resulting terminological cluster trees can be used to detect candidate disjointness axioms from emerging concept descriptions. A comparative empirical evaluation on different types of ontologies show the feasibility and the effectiveness of the proposed solution that may be regarded as complementary to the current methods which require supervision or consider atomic concepts onl...|$|R
40|$|Taxonomies are <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> when {{building}} interfaces for disclosing large information repositories. However, {{their actual}} use {{is far from}} trivial; tasks such as creation, instantiation and maintenance of taxonomies are often difficult and time-consuming. We present {{a number of ways}} in which the Cluster Map, a component for the visualization of instantiated taxonomies, can help in these tasks. Using the proposed visualizations, a user gains insight in the information, detects anomalies, monitors the information as it evolves over time and assesses the quality of the output of automatic classification tools. The proposed visualizations are presented in the context of one of our customers, for which we create web portals based on taxonomies, providing access to a large document collection. 1...|$|R
50|$|Some form of Nengo {{has existed}} since 2003. Originally {{developed}} as a Matlab script under the name NESim (Neural Engineering Simulator), it was later moved to a Java implementation under the name NEO, and then eventually Nengo. The first three generations of Nengo developed {{with a focus on}} developing <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> with <b>a</b> simple interface, and scripting system. As the tool became increasingly useful the limitations of the system in terms of speed led to development of a back-end agnostic API. This most recent iteration of Nengo defines a specific Python-based scripting API with back-ends targetting Numpy, OpenCL and Neuromorphic hardware such as Spinnaker. This newest iteration also comes with an interactive GUI (video demo) to help with the quick prototyping of neural models.|$|R
40|$|DEEP -A Statistical System in Dental Epidemiology is a {{monograph}} {{going to}} the root of the key issues in epidemiological data analysis: a sensible data management system, <b>a</b> <b>powerful</b> statistical <b>modelling</b> <b>tool</b> and <b>an</b> easily accessible tool for graphical representation to ease the interpretation and understanding of statistical results...|$|R
40|$|Integer {{optimization}} is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> {{both for}} problems of practical and more abstract origin. Since the 1970 s {{we have seen}} huge progress {{in the size of}} problem instances that can be tackled. This progress is mostly due to the many results in polyhedral combinatorics and to algorithms and implementations related to the polyhedral results. In the theory of integer optimization we have also seen exciting results related to the algebraic structure of the set of integer points in polyhedra together with algorithms that exploit them. This thesis presents results that make a step in the direction of merging the approach of polyhedral combinatorics with a reformulation technique built on lattices, an algebraic concept generalizing the structure of the integer points. Applied mathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|In recent years, the variational {{inequality}} framework {{has been}} recognized as <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> in operations research and economics, whenever equilibrium relationships among several agents occur. This interest has prompted researchers to develop efficient algorithms for solving variational inequalities in finite dimensional spaces. In this presentation {{we will try to}} assess the advantages and possible drawbacks of variational inequality formulations, focusing on four problems: oligopoly models, traffic assignment, bilevel programming, multicriterion equilibrium. Each topic will be analyzed from the modelling and computational points of view. In particular we will try to assess whether the variational inequality formulations of these models has led {{to a better understanding of}} their inner structure and to efficient solution algorithms. Key Words: Variational inequalities, oligopoly, traffic assignment, bilevel programming, multicriterion modelling. 1 Research supported by NS [...] ...|$|R
40|$|The {{quantitative}} structure-retention relationship (QSRR) of nanoparticles in roadside atmosphere {{against the}} comprehensive two-dimensional gas chromatography which was coupled to high-resolution time-of-flight mass spectrometry was studied. The genetic algorithm (GA) {{was employed to}} select the variables {{that resulted in the}} best-fitted models. After the variables were selected, the linear multivariate regressions [e. g. the partial least squares (PLS) ] as well as the nonlinear regressions [e. g. the kernel PLS (KPLS) and Levenberg- Marquardt artificial neural network (L-M ANN) ] were utilized to construct the linear and nonlinear QSRR models. The correlation coefficient cross validation (Q 2) and relative error for test set L-M ANN model are 0. 939 and 4. 89, respectively. The resulting data indicated that L-M ANN could be used as <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for the QSPR studies...|$|R
40|$|The {{mechanisms}} of force generation and transference via microfilament networks {{are crucial to}} the understandings of mechanobiology of cellular processes in living cells. However, there exists an enormous challenge for all-atom physics simulation of real size microfilament networks due to scale limitation of molecular simulation techniques. Following biophysical investigations of constitutive relations between adjacent globular actin monomers on filamentous actin, a hierarchical multiscale model was developed to investigate the biomechanical properties of microfilament networks. This model was validated by previous experimental studies of axial tension and transverse vibration of single F-actin. The biomechanics of microfilament networks can be investigated at the scale of real eukaryotic cell size (10 [*]μm). This multiscale approach provides <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> which {{can contribute to the}} understandings of actin-related cellular processes in living cells...|$|R
40|$|Spreadsheets {{constitute}} <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> and <b>an</b> easy {{computer programming}} language. Spreadsheets, ubiquity, their low cost, their flexible programmable environment, {{as well as}} their plotting capabilities, make them attractive as an educational tool. Their capability is illustrated bellow through several examples ranging from classical control theory to more advanced topics such as optimal control or state-observer. Some of these examples are developed as classroom activities, whereas others are designed as interactive learning modules for illustrating specific control topics. Our approach focuses exclusively on the standar use of spreadsheets; that is, no macros no macro languaje support are used. Pedagogical issues related to the use of spreadsheets in the classroom are also discussed. 0. 693 SJR (2009) Q 1, 67 / 918 Engineering (miscellaneous), 139 / 712 Educatio...|$|R
40|$|In {{this paper}} we propose a new {{clustering}} algorithm based on copula functions. Copula functions (CF, hereafter) were introduced in Sklar (1959) in a probabilistic context and have become <b>a</b> <b>powerful</b> multivariate <b>modeling</b> <b>tool</b> in many fields. Among their advantages CFs allow to i) overcome the limitations of the linear correlation coefficien...|$|R
40|$|Although chaotic {{systems have}} {{received}} increasing attention {{over the past}} two decades, traditional <b>modeling</b> <b>tools</b> have always encountered considerable analytical and numerical difficulties in modeling and predicting the behavior of chaotic systems. Neural networks, on the other hand, {{seem to be able to}} introduce <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> relying on their nonlinear nature. This paper contains a brief discussion on the properties of one- and two-dimensional discrete maps, an introduction to the operation of perceptron neural networks, neural network modeling and prediction of consecutive samples of one- and two-dimensional discrete-time chaotic arrays, and several experimental results. Key Words: Chaos, Discrete-Time Chaotic Maps, Fractals, Perceptron Neural Networks, Back Propagation Learning Algorithm. 1 Introduction Chaos is a nonlinear phenomenon that manifests itself in many fields of science. Despite being singled out as an important research area only recently, He is currently [...] ...|$|R
40|$|The {{basic idea}} behind the {{proposed}} hierarchical phoneme recognition is that phonemes can be classified into specific phoneme types which can be organized within a hierarchical tree structure. The recognition principle is based on “divide and conquer ” in which a large problem is divided into many smaller, easier to solve problems whose solutions can be combined to yield {{a solution to the}} complex problem. Fuzzy Petri net (FPN) is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for fuzzy production rules based knowledge systems. For building hierarchical classifier using Neural Fuzzy Petri net (NFPN), Each node of the hierarchical tree is represented by a NFPN. Every NFPN in the hierarchical tree is trained by repeatedly presenting a set of input patterns along with the class to which each particular pattern belongs. The feature vector used as input to the NFPN is the LPC parameters...|$|R
40|$|Interval Markov {{decision}} processes (IMDPs) generalise classical MDPs {{by having}} interval-valued transition probabilities. They provide <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> for probabilistic systems {{with an additional}} variation or uncertainty that prevents {{the knowledge of the}} exact transition probabilities. In this paper, we consider the problem of multi-objective robust strategy synthesis for interval MDPs, where the aim is to find a robust strategy that guarantees the satisfaction of multiple properties {{at the same time in}} face of the transition probability uncertainty. We first show that this problem is PSPACE-hard. Then, we provide a value iteration-based decision algorithm to approximate the Pareto set of achievable points. We finally demonstrate the practical effectiveness of our proposed approaches by applying them on several case studies using a prototypical tool. Comment: This article is a full version of a paper accepted to the Conference on Quantitative Evaluation of SysTems (QEST) 201...|$|R
40|$|Nichols Research Corporation is {{developing}} the BM/C 3 Requirements Analysis Tool (BRAT) for the U. S. Army Strategic Defense Command. BRAT uses embedded CLIPS/Ada {{to model the}} decision making processes used by the human commander of a defense system. Embedding CLlPS/Ada in BRAT allows the user to explore {{the role of the}} human in Command and Control (C 2) and the use of expert systems for automated C 2. BRAT models assert facts about {{the current state of the}} system, the simulated scenario, and threat information into CLIPS/Ada. A user-defined rule set describes the decision criteria for the commander. We have extended CLIPS/Ada with user-defined functions that allow the firing of a rule to invoke a system action such as weapons release or a change in strategy. The use of embedded CLIPS/Ada will provide <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for our customer at minimal cost...|$|R
40|$|Construction {{industry}} {{suffers from}} the lack of efficiency and practicability in the planning process. In this paper, the reasons standing behind construction planning problems are investigated. It is claimed that the conventional production theory (mass production), which the construction management techniques stand on, is the source of planning inefficiency and impracticability. Accordingly, the new production theory (lean production) is reviewed, and its main principles are presented, to evaluate the potential benefits that can be gained from adopting it in the construction domain. It is concluded that lean production theory can offer practical solutions for construction planning problems, but two requirements must be fulfilled. The techniques of this theory must be transformed from other production domains to the construction industry at a high level of abstraction, and <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> must be used, instead of the traditional network techniques, in order to implement the lean principles and concepts...|$|R
40|$|Free-form {{deformation}} (FFD) is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool,</b> but {{controlling the}} shape of an object under complex deformations is often difficult. The interface to FFD in most conventional systems simply represents the underlying mathematics directly; users describe deformations by manipulating control points. The difficulty in controlling shape precisely is largely due to the control points being extraneous to the object; the deformed object does not follow the control points exactly. In addition, the number of degrees of freedom presented to the user can be overwhelming. We present a method that allows a user to control a free-form deformation of an object by manipulating the object directly, leading to better control of the deformation and a more intuitive interface. CR Categories: I. 3. 5 [Computer Graphics]: Computational Geometry and Object Modeling - Curve, Surface, Solid, and Object Representations; I. 3. 6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques. Ad [...] ...|$|R
40|$|This paper {{describes}} a partial parser that assigns syntactic structures to sequences of partof -speech tags. The program uses the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources: the hierarchical structure, {{parts of speech}} and phrasal categories. In effect, the parser goes beyond simple bracketing and recognises even fairly complex structures. We give accuracy figures for different applications of the parser. 1 Introduction The maximum entropy framework {{has proved to be}} <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> in many areas of natural language processing. Its applications range from sentence boundary disambiguation (Reynar and Ratnaparkhi, 1997) to part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997) and machine translation (Berger et al., 1996). In the present paper, we describe a partial parser based on the maximum entropy modelling method. After a synopsis of the maximum entropy framework in section 2, we present [...] ...|$|R
40|$|Stochastic {{differential}} equations {{have been shown}} useful in describing random continuous time processes. Biomedical experiments often imply repeated measurements {{on a series of}} experimental units and differences between units can be represented by incorporating random effects into the model. When both system noise and random effects are considered, stochastic differential mixed-effects models ensue. This class of models enables the simultaneous representation of randomness in the dynamics of the phenomena being considered and variability between experimental units, thus providing <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> with immediate applications in biomedicine and pharmacokinetic/pharmacodynamic studies. In most cases the likelihood function is not available, and thus maximum likelihood estimation of the unknown parameters is not possible. Here we propose a computationally fast approximated maximum likelihood procedure for the estimation of the non-random parameters and the random effects. The method is evaluated on simulations from some famous diffusion processes and on real data set...|$|R
40|$|A {{returning}} idea {{among some}} Bayesians {{in research on}} human visual perceptual organization is that the surprisal of something (i. e., the negative logarithm of its probability) expresses its complexity (i. e., the length of its shortest description). Bayes' rule is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> and descriptive simplicity is a rich concept, but this idea is wishful thinking at best: If true, it would unify the simplicity and likelihood principles, which reflect two traditionally opposed schools of thought on perceptual organization. Some rapprochement between the two principles can certainly be discerned, but the aforementioned idea lacks formal underpinning and confounds otherwise perfectly good ideas. Here, this idea is revisited and its latest version is debunked step by step. In addition, I argue that its likely origin lies, inadvertently, in a standard Bayesian textbook: The author made (a) a pivotal mistake and (b) a compelling argument that was overinterpreted by others. status: publishe...|$|R
40|$|International audienceState {{estimation}} of nonlinear systems {{plays an important}} role in several control engineering problems. Multiple model approach is an interesting way to cope with this relevant problem. Indeed, multiple models are recognized as <b>a</b> <b>powerful</b> <b>modelling</b> <b>tool</b> for nonlinear dynamic systems. In this framework, several realisations of multiple models can be considered for submodel interconnections. In contrast to the most popular results found in the multiple model literature, we consider here heterogeneous multiple models which allow to use submodels of different state space dimensions. Thanks to this fact, flexibility and generality can be introduced in the modelling stage. This paper provides survey of recent results in state estimation strategies based on heterogeneous multiple models. Different kinds of observers are investigated in order to improve the state estimation with respect to disturbance as well as unknown inputs. Theoretical results on the observers design and the state estimation error convergence are presented. Discussion and criticisms of the suggested approaches are also proposed and further research are pointed out...|$|R
40|$|Associate Professor Allan HolmgrenIf {{one does}} not really {{understand}} what is meant, Modeling surface adsorption requires systems of hundreds of atoms. To model such systems at an ab initio level successfully, we need to avoid traditional quantum chemical methods. In the present work we have shown that density functional theory is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for large chemical systems especially in combinations with pseudopotentials This is validated by an initial study of ethyl and heptyl xanthates and their sodium/potassium salts. In this study, all electron calculations using both Hartree-Fock and density functional theory methods are compared with experimental infrared results. To do this the influence of basis sets and modeling approaches on the geometrical structure and the vibrational modes are examined. This includes comparing the pseudopotential and full electron potential approaches. Results obtained from pseudopotential methods are {{in close agreement with}} both all electron calculations as well as experimental results, here used to study adsorption o...|$|R
40|$|AbstractIn this paper, a fuzzy Petri net (FPN) {{approach}} to modeling fuzzy rule-based reasoning is proposed to determining confidence values for bases called in DNA sequencing. The proposed {{approach is to}} bring DNA bases-called {{within the framework of}} <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> FPN. The three input features in our fuzzy model-the height, the peakness, and the spacing of the first most likely candidate (the base called) and the peakness and height for the second likely candidate can be formulated as uncertain fuzzy tokens to determines the confidence values. The FPN components and functions are mapped from the different type of fuzzy operators of If-parts and Then-parts in fuzzy rules. The validation was achieved by comparing the results obtained with the FPN model and fuzzy logic using the MATLAB Toolbox; both methods have the same reasoning outcomes. Our experimental results suggest that the proposed models, can achieve the confidence values that matches, of available software...|$|R
40|$|Analytical {{methods in}} {{reliability}} analysis {{are useful for}} studying simple problems. For complex networks with cross-linked (non-series/parallel) component configurations, {{it is difficult to}} use mathematical reliability analysis. Powerful methods for reliability analysis of such systems have been developed using discrete event simulation. The main drawback of these methods is that they are computer time intensive. In this paper, the main idea behind these methods is further explored and modified {{in order to reduce the}} computational loads. The modified approach presented here leads to a great time saving which is very important for reliability analysis of large scale systems. This modified method is then modeled by Petri net, which is <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool.</b> The network reliability modeling technique developed in the paper has two main advantages. First, it can be easily implemented through a systematic and standard approach. Second, the developed model will greatly help solving the reliability analysis problem since it is simple and graphical...|$|R
40|$|In {{this paper}} {{we present a}} {{methodology}} of study of complex phenomena emerging in stock markets. This methodology {{is based on the}} use of distributed, multi-agent models with minimal knowledge representation and reasoning capabilities that has proven <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for complex biological systems. Unlike neural and "neo-connectionist" models, ours' allow a comparative and incremental evaluation of their validity and relevance to the observed phenomena. The possibility of their application to the modeling and study of stock market phenomena is demonstrated on a simple example of a central agency that regulates the behavior of the investors : we show how a "blind" or myopic behavioral model reproduces results found in the literature and how the mutation of the model according to the parameters' values or the adaptation structures gives rise to a series of complex phenomena comparable to those observed in reality. 1 Introduction Why do prices fluctuate and under what circumstance [...] ...|$|R
40|$|The coalescent {{process is}} <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for {{population}} genetics. The allelic states of all homologous gene copies {{in a population}} {{are determined by the}} genealogical and mutational history of these copies. The coalescent approach is based on the realization that the genealogy is usually easier to model backward in time, and that selectively neutral mutations can then be superimposed afterwards. A wide range of biological phenomena can be modeled using this approach. Whereas almost all of classical population genetics considers the future of a population given a starting point, the coalescent considers the present, while taking the past into account. This allows the calculation of probabilities of sample configurations under the stationary distribution of various population genetic models, and makes full likelihood analysis of polymorphism data possible. It also leads to extremely efficient computer algorithms for generating simulated data from such distributions, data which can then be compared with observations as a form of exploratory data analysis...|$|R
30|$|Several {{interesting}} and important results concerning existence and uniqueness of solutions, stability properties of solutions, analytic and numerical methods of solutions for fractional differential equations {{can be found}} in the recent literature on the topic and the search for more and more results is in progress. Fractional-order operators are nonlocal in nature and take care of the hereditary properties of many phenomena and processes. Fractional calculus has also emerged as <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for many real world problems. For examples and recent development of the topic, see [1]–[14]. However, it has been observed that most of the work on the topic involves either Riemann-Liouville or Caputo type fractional derivatives. Besides these derivatives, the Hadamard fractional derivative is another kind of fractional derivative that was introduced by Hadamard in 1892 [15]. This fractional derivative differs from the other ones in the sense that the kernel of the integral (in the definition of the Hadamard derivative) contains a logarithmic function of an arbitrary exponent. For background material of the Hadamard fractional derivative and integral, we refer to [2], [16]–[22].|$|R
40|$|My {{research}} {{focuses on}} statistical approaches to natural language processing, especially machine translation. My research interests also include computational morphology and unsupervised machine learning for NLP tasks. My previous works fall into three broad categories: Improving synchronous context free grammar (SCFG) -based machine translation, using latent information to improve NLP tasks, and improving optimization in natural language processing. Contributions My contributions in machine translation extends to all stages of machine translation pipeline. Starting from tokenization [9], alignment [7], rule extraction [3] [5] [6], decoding [5], {{and all the way}} to parameter tuning [1] [2]. • SCFG-based machine translation Although SCFG with rich representation can be <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for translation, decoding SCFG suffers from many practical issues such as managing complexity. I have worked with collaborators to make SCFG-based machine translation to be more efficient. I found that enforcing SCFG rules to always have source-side lexical items by modifying rule extraction improves translation accuracy [5]. In the same publication, I discuss several practical solutions that make SCFG decodin...|$|R
40|$|Packing and {{covering}} linear programs {{belong to the}} narrow class of linear programs that are efficiently solvable in parallel and distributed models of computation, yet are <b>a</b> <b>powerful</b> <b>modeling</b> <b>tool</b> for <b>a</b> wide range of fundamental problems in theoretical computer science, operations research, and many other areas. Following recent progress in obtaining faster distributed and parallel algorithms for packing {{and covering}} linear programs, we present a simple algorithm whose iteration count matches the best known Õ(1 /ϵ^ 2) for this class of problems. The algorithm {{is similar to the}} algorithm of [Allen-Zhu and Orecchia, 2015], it can be interpreted as Nesterov's dual averaging, and it constructs approximate solutions to both primal (packing) and dual (covering) problems. However, the analysis relies on the construction of an approximate optimality gap and a primal-dual view, leading to a more intuitive interpretation. Moreover, our analysis suggests that all existing algorithms for solving packing and covering linear programs in parallel/distributed models of computation are, in fact, unaccelerated, and raises the question of designing accelerated algorithms for this class of problems...|$|R
