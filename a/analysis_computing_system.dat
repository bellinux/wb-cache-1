0|10000|Public
40|$|This report {{contains}} {{contributions to}} the first international workshop on Overture, a community project to develop open source interoperable tools for the formal modelling and <b>analysis</b> of <b>computing</b> <b>systems,</b> based on extensions to the Vienna Development Method (VDM and VDM++) modelling languages. The contributions address the industry-strength basic tools such as parsersas well as advanced capabilities including modelling real-time and embedded systems, and test generation. © 2006 University of Newcastle upon Tyne. Printed and published by the University of Newcastle upon Tyne...|$|R
30|$|In the literature, several {{simulators}} {{have been}} developed for performance <b>analysis</b> of cloud <b>computing</b> <b>systems,</b> such as GridSim, MicroGrid, GangSim and CloudSim [32]. The first three focus on Grid <b>computing</b> <b>systems</b> to evaluate costs of executing distributed applications in Cloud infrastructures. However, CloudSim is a generalized and extensible simulation toolkit and application which enables seamless modeling, simulation, and experimentation of emerging cloud <b>computing</b> <b>system,</b> infrastructures and application environments for single and internetworked clouds [33, 34, 35].|$|R
30|$|In this Appendix we {{formulate}} {{the predictive}} analysis problem {{in terms of}} reachability assessment, show that these reachability questions can be addressed through an “altitude function” <b>analysis</b> without <b>computing</b> <b>system</b> trajectories, and apply this theoretical framework to demonstrate that predictability of a broad class of social diffusion models depends crucially upon the meso-scale topological structures of the underlying networks. For convenience of exposition, in this Appendix we focus on network communities as a representative meso-scale structure; however, all results derived here are also applicable to the more general {{case in which the}} “network partition” (see Section 2.2) includes both community and core-periphery structures.|$|R
40|$|AbstractThis paper {{presents}} a very first {{step on the}} road to applying the theory of discrete dynamical systems (DDSs) in the <b>analysis</b> of concurrent <b>computing</b> <b>systems.</b> In order to proceed, Petri nets (PNs) are appropriately modeled as DDSs, so defining the corresponding phase space with its metric structure and the evolution operator of the system. We conclude this study by showing results for some identifiable problems...|$|R
40|$|The HiPEAC roadmap {{describes}} the HiPEAC vision on high-performance embedded architecture and compilation {{for the coming}} decade. It starts from societal challenges, application and industry trends, and technological constraints which lead to 7 technical challenges. This forms {{the basis for the}} HiPEAC vision "keep it simple for humans, and let the computer do the hard work" and its consequences. The roadmap ends with a SWOT <b>analysis</b> of the <b>computing</b> <b>systems</b> industry in Europe, and 6 research recommendations...|$|R
40|$|Performance <b>analysis</b> of <b>computing</b> <b>systems,</b> in {{particular}} distributed <b>computing</b> <b>systems,</b> {{is a complex}} process. Analysing the complex flows and interactions between a set of distributed processing nodes is a non-trivial task. The problem is exacerbated {{by the addition of}} continuous system functions that are time dependent, such as communication between components in the form of multimedia streams of video and audio data. Quality-of-Service (QoS) specifications define constraints on such communications and describe the required patterns of data transfer. By making use of these specifications as part of the performance analysis process it is possible to add significant confidence to predictions about the correct (required) operation of a distributed system. This paper presents a method for designing distributed multimedia systems, including the specification of QoS, using the ODP framework and UML and describes a technique for verifying the QoS specification against the designed functional behaviour of the system using Timed Automata...|$|R
40|$|This {{monograph}} {{presents the}} Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and <b>analysis</b> of timed (<b>computing)</b> <b>systems.</b> Timed systems are systems in which desirable correctness or performance {{properties of the}} system depend on the timing of events, {{not just on the}} order of their occurrence. Timed systems are employed {{in a wide range of}} domains including communications, embedded systems, real-time operating systems, and automated control. Many applications involving timed systems have strong safety, reliability, and predictability re...|$|R
40|$|The {{application}} of formal methods to the <b>analysis</b> of <b>computing</b> <b>systems</b> promises to provide {{higher and higher}} levels of assurance as the sophistication of our tools and techniques increases. Improvements in tools and techniques come about as we pit {{the current state of the}} art against new and challenging problems. A promising area for the {{application of}} formal methods is in real-time and distributed computing. Some of the algorithms in this area are both subtle and important. In response to this challenge and as part of an ongoing attempt to verify an implementation of the Interactive Convergence Clock Synchronization Algorithm (ICCSA), we decided to undertake a proof of the correctness of the algorithm using the Boyer-Moore theorem prover. This paper describes our approach to proving the ICCSA using the Boyer-Moore prover...|$|R
40|$|An {{increasing}} {{amount of}} research activity in computational fluid dynamics {{has been devoted to}} the development of efficient algorithms for parallel <b>computing</b> <b>systems.</b> The increasing performance to price ratio of engineering workstations has led to research to development procedures for implementing a parallel <b>computing</b> <b>system</b> composed of distributed workstations. This thesis proposal outlines an ongoing research program to develop efficient strategies for performing three-dimensional flow <b>analysis</b> on distributed <b>computing</b> <b>systems.</b> The PVM parallel programming interface was used to modify an existing three-dimensional flow solver, the TEAM code developed by Lockheed for the Air Force, to function as a parallel flow solver on clusters of workstations. Steady flow solutions were generated for three different wing and body geometries to validate the code and evaluate code performance. The proposed research will extend the parallel code development to determine the most efficient strategies for unsteady flow simulations...|$|R
40|$|Simulation {{is often}} used in the <b>analysis</b> of complex <b>computing</b> <b>systems.</b> The design and {{validation}} of complex real-time systems demands capabilities not found in existing simulation environments. We categorize these capabilities into three major functional areas: search control, the execution engine, and output analysis. Search control is an imporant topic often treated lightly in simulation environments for real-time systems. The search controller chooses values for variable input parameters and launches simulation sto determine {{the performance of the}} system under different sets of parameter values. The search control framework presented in this thesis allows both conventional design and search and also validation searc...|$|R
40|$|A {{reliability}} <b>analysis</b> {{method for}} <b>computing</b> <b>systems</b> is considered {{in which the}} underlying criteria for 'success' {{are based on the}} computations the system must perform in the use environment. Beginning with a general model of a 'computer with faults', intermediate concepts of a 'tolerance relation' and an 'environment space' are introduced which account for the computational needs of the user and the probabilistic nature of the use environment. These concepts are then incorporated to obtain a precisely defined class of computation-based reliability measures. Formulation of a particular measure is illustrated and results, applying this measure, are compared with those of a typical structure-based analysis...|$|R
40|$|A {{model of}} a {{distributed}} fault-tolerant avionic system of a transport aircraft is presented {{as an example of}} a phased-mission <b>analysis</b> for aerospace <b>computing</b> <b>systems.</b> The avionic system consists of a flight control, inertial navigation system, radio navigational aids, fuel management, and autolanding system computers. A demand profile for a flight is constructed for fuel efficient flight, constant failure rates for all computers, and each phase having an s-coherent structure. The probabilities of accomplishing desired levels are calculated. An evaluation is made of the bounds of effectiveness for the <b>computing</b> <b>systems,</b> with considerations of model development for a multistate system, an analysis of the system components, and the use of a hazard transform bound. An example is provided which includes a provision for the functions of a failed computer to be carried out by the remaining systems...|$|R
40|$|This paper {{addresses}} {{the problem of}} performance modeling for large-scale heterogeneous distributed systems with emphases on multi-cluster <b>computing</b> <b>systems.</b> Since the overall performance of distributed systems is often depends {{on the effectiveness of}} its communication network, the study of the interconnection networks for these systems is very important. Performance modeling is required to avoid poorly chosen components and architectures as well as discovering a serious shortfall during system testing just prior to deployment time. However, the multiplicity of components and associated complexity make performance <b>analysis</b> of distributed <b>computing</b> <b>systems</b> a challenging task. To this end, we present an analytical performance model for the interconnection networks of heterogeneous multi-cluster systems. The analysis is based on a parametric family of fat-trees, the m-port n-tree, and a deterministic routing algorithm, which is proposed in this paper. The model is validated through comprehensive simulation, which demonstrated that the proposed model exhibits a good degree of accuracy for various system organizations and under different working conditions. <br /...|$|R
40|$|Goal of {{my thesis}} {{is to show}} and compare various ways of {{graphical}} data <b>analysis</b> in statistical <b>computing</b> <b>systems.</b> This area is still developing, new techniques are being discovered and the already known are being developed. This work is split into three parts, one theoretical, one comparing and one general with new ideas. The first part is theoretical and is dedicated to classification of statistical variables. For statistical <b>computing</b> <b>systems</b> I offer a simplified classification {{because of the way}} they treat some types of variables. Second part of my work compares graphical outputs of three most used statistical <b>computing</b> <b>systems</b> SAS, SPSS and STATISTICA. This comparison is based on some of the most commonly used graphical methods. The outputs are tables with detailed description and examples of graphs from each system are attached. The last part is focused on the general graphical data exploration and follows the simplified classification of variables. In this part I use common graphical techniques and introduce some new or fewer used techniques. Many of them haven't been described in any Czech literature yet. Almost all the graphs presented in this work were created by me in various statistical <b>computing</b> <b>systems.</b> Two were copied from literature because of serious problems with the process of their creation...|$|R
40|$|The {{design and}} <b>analysis</b> of <b>computing</b> <b>systems</b> {{presents}} a significant challenge: systems {{need to be}} understood at many different levels of abstraction, and examined from many different perspectives. Formal methods—languages, tools, and techniques with a sound, mathematical basisandemdash;can be used to develop a thorough understanding, and to support rigorous examination. Further research into effective integration is required if these methods are {{to have a significant}} impact outside academia. The Integrated Formal Methods (IFM) series of conferences seeks to promote that research, to bring together the researchers carrying it out, and to disseminate the results of that research among the wider academic and industrial community. Earlier meetings in the series were held at: York (1999); Dagstuhl (2000); Turku (2002); Kent (2004); Eindhoven (2005). IFM 2007 is the largest to date, with 32 technical papers (from 85 submissions), 3 invited talks, 3 workshops, and a tutorial. The success of the series reflects the enthusiasm and efforts of the IFM community, and the organisers {{would like to thank the}} speakers, the committee, and the reviewers for their contributions. </p...|$|R
40|$|Abstract—The {{environmental}} need to curb {{distribution network}} losses and utilize {{renewable energy sources}} has created new chal-lenges in estimation. High fidelity estimates are required even {{in the presence of}} significant uncertainty. Herein, we develop a new analytical probabilistic load flow method that, unlike existing an-alytical methods, is not based on a Taylor series approximation of the power equations. The method is exact for a set of distributions that includes the multivariate normal distribution. The method implementation is made scalable by casting all formulas into the framework of the popular backward/forward algorithm. The ad-vantages of this approach are illustrated on a radial IEEE 32 -bus test system. Significant improvements are observed in the presence of large power uncertainties and near the network power limits. Uniformly better estimation of power losses is achieved. Index Terms—Load flow, power <b>system</b> <b>analysis</b> <b>computing,</b> power <b>system</b> modeling, uncertainty. NOMENCLATURE Complete set of node indices. Complete set of network branches. Set of nodes upstream of node. Estimated voltage at node. Actual base voltage. Estimated current through branch. Estimated loop current at node. Estimated load at node. Actual load at node. Estimated power loss across branch. Impedance of branch. Correlation parameters of loop currents...|$|R
40|$|Abstract—There exists no {{universal}} tool {{to analyze the}} increas-ing complexity in smart grids. Domain specific simulation and engineering tools partly address the challenges of complex system behavior. Different component technologies, customer behavior and controls in the power networks are interacting in a highly dynamic manner. Results of isolated simulations may be not accurate enough on the system level. Free and open available tools like GridLAB-D, PSAT, OpenModelica and 4 DIAC are well known and widely used because of their excellent domain specific expertise. With co-simulation approaches the individual strengths of each tool can be exploited to model and simulate the various aspects of complex smart grids. The achieved level of detail and realism potentially surpasses the results that the individual analyses would gain. This paper demonstrates a local smart charging control strategy implemented with the IEC 61499 -based standard for distributed control systems. It is simulated with different electric vehicle driving patterns, modeled with the multi-agent environ-ment GridLAB-D. Battery models are defined in OpenModelica and embedded as individual dynamic loads. The power system is simulated using PSAT. This work shows that boundaries and restriction in terms of modeling cross-domain specific problems can be overcome by coupling these open source applications. Index Terms—Open source software, Power <b>system</b> <b>analysis</b> <b>computing,</b> Power <b>system</b> simulation, Smart grids...|$|R
40|$|Colloque avec actes et comité de lecture. internationale. International audienceAs an {{emerging}} field, pervasive computing {{has not had}} the opportunity to develop a conceptual model similar to the OSI Reference Model used to describe computer networks. Such a model would be useful in properly classifying design issues and providing needed context. Inspired by the layers of abstraction provided by the OSI Reference Model, we present our Layered Pervasive Computing (LPC) model to facilitate discussion and <b>analysis</b> of pervasive <b>computing</b> <b>systems</b> by providing a much needed conceptual framework. A key feature of our model is its representation of the human user at each layer of abstraction of the model. We will then use our model to analyze a research prototype created as part of our Aroma pervasive <b>computing</b> project. This <b>analysis</b> is illustrative because it quickly reveals issues that must be addressed to realize our research prototype as a commercial product...|$|R
40|$|This paper {{provides}} an interdisciplinary {{reflection on the}} nature meaning-making involving users and animated gestural interfaces. In particular, we propose a new model for <b>analysis</b> of creative <b>computing</b> <b>systems</b> incorporating gestural input into dynamically animated interfaces. Our contributions {{are based on a}} theoretical framework synthesizing embodied cognition approaches in cognitive science, phenomenology in philosophy, and user interface design in computing. We introduce the term enduring interaction to refer to the phenomenon of bodily and conceptually engaging interaction within constantly changing computational environments. Our construct centralizes the issue of how users' motor-sensory experiences inform their construction of meaning in the design of interactive systems. We argue that creative <b>computing</b> <b>systems,</b> a class of artifacts including types of hobbyist websites, video games, and computer-based artworks, require a new design perspective quite distinct from user-centric interface design approaches focused on productivity-oriented applications. Using examples including outcomes of the Gestural Narrative and Interactive Expression (GeNIE) project (Harrell, PI; Chow and Erik Loyer collaborators) along with existing prevalent, exceptional, or historically significant artifacts, we articulate a continuum of various kinds of engagement, showing design implications of our perspective, enabling users to use gestural interaction (through multi-touch and gyroscope/accelerometer-based input devices) to result in narratively salient, evocative, and even intimate interaction mechanisms in interactive narrative environments. School of DesignRefereed conference pape...|$|R
40|$|Abstract−Power flow {{studies are}} {{typically}} {{used to determine}} the steady state or operating conditions of power systems for specified sets of load and generation values, {{and is one of the}} most intensely used tools in power engineering. When the input conditions are uncertain, numerous scenarios need to be analyzed to cover the required range of uncertainty, and hence reliable solution algorithms that incorporate the effect of data uncertainty into the power flow analysis are needed. To address this problem, this paper proposes a new solution methodology based on the use of optimization techniques and worst-case scenario analysis. The application of these techniques to the power flow problem with uncertainties is explained in detail, and several numerical results are presented and discussed, demonstrating the effectiveness of the proposed methodology Index Terms−Power flow <b>analysis,</b> reliable <b>computing,</b> uncertain <b>systems...</b>|$|R
40|$|We {{consider}} a “k-out-of-N” system with different standby modes. Each of the N components consists of multiple part types. Upon failure, a component can be repaired {{within a certain}} time by switching the failed part by a spare, if available. We develop both an exact and a fast approximate <b>analysis</b> to <b>compute</b> the <b>system</b> availability. Next, we jointly optimize the component redundancy level with the inventories of the various spare parts. We find that our approximations are very accurate and suitable for large systems. We apply our model to a case study at a public organization in Qatar, and find that we can improve the availability-to-cost ratio by reducing the redundancy level and increasing the spare part inventories. In general, high redundancy levels appear to be useful only when components are relatively cheap and part replacement times are high...|$|R
40|$|The energy {{consumption}} of DRAM {{is a critical}} concern in modern <b>computing</b> <b>systems.</b> Improvements in manufacturing process technology have allowed DRAM vendors to lower the DRAM supply voltage conservatively, which reduces some of the DRAM {{energy consumption}}. We would like to reduce the DRAM supply voltage more aggressively, to further reduce energy. Aggressive supply voltage reduction requires {{a thorough understanding of}} the effect voltage scaling has on DRAM access latency and DRAM reliability. In this paper, we take a comprehensive approach to understanding and exploiting the latency and reliability characteristics of modern DRAM when the supply voltage is lowered below the nominal voltage level specified by DRAM standards. Using an FPGA-based testing platform, we perform an experimental study of 124 real DDR 3 L (low-voltage) DRAM chips manufactured recently by three major DRAM vendors. We find that reducing the supply voltage below a certain point introduces bit errors in the data, and we comprehensively characterize the behavior of these errors. We discover that these errors can be avoided by increasing the latency of three major DRAM operations (activation, restoration, and precharge). We perform detailed DRAM circuit simulations to validate and explain our experimental findings. We also characterize the various relationships between reduced supply voltage and error locations, stored data patterns, DRAM temperature, and data retention. Based on our observations, we propose a new DRAM energy reduction mechanism, called Voltron. The key idea of Voltron is to use a performance model to determine by how much we can reduce the supply voltage without introducing errors and without exceeding a user-specified threshold for performance loss. Voltron reduces the average system energy by 7. 3 % while limiting the average system performance loss to only 1. 8 %, for a variety of workloads. Comment: 25 pages, 25 figures, 7 tables, Proceedings of the ACM on Measurement and <b>Analysis</b> of <b>Computing</b> <b>Systems</b> (POMACS...|$|R
40|$|The {{purpose of}} this study was to develop a sngle {{procedure}} for comparing population variances which could be used for distribution forms. Bootstrap methodology was used to estimate the variability of the sample variance statiatic when the population distribution WaS normal, platykurtic and leptokurtic. The data for the study were generated and analyzed using the Statistical <b>Analysis</b> <b>System</b> <b>computing</b> package. The bootstrap estimates of variability underestimated the theoretical variance value, and the mean square error of the estimated variance was small for both the normal and platykurtic distributions,. but large for the leptokurtic distribution. The F-ratio and chi-square test statistics were computed for comparing the variability of two popuiations using the bootstrap estimates of variance. Observed Type I error rates within two standard errors of the nominal significance level were obtaine...|$|R
40|$|This paper {{discusses}} {{a systematic}} new methodology for analyzing and predicting {{the performance of}} <b>computing</b> <b>systems.</b> The approaches considered address <b>analysis</b> of the <b>computing</b> <b>system</b> viewed {{in terms of an}} architectural framework consisting of the applications, the system soft-ware, and the underlying hardware. The methodology dis-cussed enables analysis of the interdependent effects of these layers to the behavior of the system. To enable that, the layers and components of the system are described in multiple levels of detail and in multiple modes (analytical and simulation approaches as well as integrated perfor-mance measurements). Key approaches will be the ability to use these multilevel and multimodal methods of de-scribing such systems and their subcomponents and incorporate them into performance frameworks in a plug-and-play fashion, to enable the capability to describe sys-tem behavior. In addition, it is envisioned that this technol-ogy will not only support the design of the complex systems but also be used in the runtime (control) and man-agement cycles of these systems. These new methods will allow the design of individual components and also pro-vide capabilities for system analysis and design, as well as provide a path from understanding component behavior to understanding system behavior and from understanding component behavior to predicting system behavior. The term performance engineering technology is used here to describe the technology that supports the capabilities dis-cussed here...|$|R
40|$|Research {{on tests}} for scale {{equality}} have focused exclusively on an overall test statistic {{and have not}} examined procedures for identifying specific differences in multiple group designs. The present study compares four contrast analysis procedures for scale differences in the single factor four-group design: (1) Tukey HSD; (2) Kramer-Tukey; (3) Games-Howell; and (4) Dunnett T- 3. Two data transformations are considered under several combinations of variance difference, sample sizes, and distributional forms. The data for the study were generated using the Statistical <b>Analysis</b> <b>System</b> <b>computing</b> package. The results indicate that no single transformation or analysis procedure is uniformly superior in controlling the family-wise error rate or in statistical power. The relationship between sample size and variances {{is a major factor}} in selecting a contrast analysis procedure. Adopting a multiple analysis strategy is recommended. Four tables show comparisons in these methods. (Author/SLD...|$|R
40|$|In {{the last}} years, a {{progressive}} migration from single processor chips to multi-core computing devices {{has taken place}} in the general-purpose and embedded system market. The development of multi-processor systems is already a core activity for the most important hardware companies. A lot of different solutions have been proposed to overcome the physical limits of single core devices and to address the increasing computational demand of modern multimedia applications. The real-time community followed this trend with an increasing number of results adapting the classical scheduling <b>analysis</b> to parallel <b>computing</b> <b>systems.</b> This paper will contribute to refine the schedulability analysis for symmetric multi-processor (SMP) real-time systems composed by a set of periodic and sporadic tasks. We will focus on both fixed and dynamic priority global scheduling algorithms, where tasks can migrate from one processor to another during execution. By increasing the complexity of the analysis, we will show that an improvement is possible over existing schedulability tests, significantly increasing the number of schedulable task sets detected. The added computational effort is comparable to the cost of techniques widely used in the uniprocessor case. We believe this is a reasonable cost to pay, given the intrinsically higher complexity of multi-processor devices...|$|R
40|$|The {{problem of}} {{executing}} large BLAS (basic linear algebra subprograms) Level- 2 operations, such as matrix-vector products, in a network-based distributed computing environment {{composed of a}} bus-oriented workstation cluster is considered. Unlike previous contributions, we {{take into account the}} fact that workstations, as against mainframe computers, are not equipped with communication coprocessors or front-ends, precluding any possibility of communication off-loading. Communication delays, which are significant in workstation clusters due to limited bandwidth availability, are specifically accounted for. This aspect is generally ignored in most performance <b>analysis</b> of parallel <b>computing</b> <b>systems.</b> The important contribution {{of this study is to}} show that the optimal load partitioning, and the subsequent performance of the network, depends critically on network bandwidth, computing capacity, and load characteristics. We design load distribution strategies for three cases (no communication, broadcast communication, and multicast communication) based on closed-form solutions of the optimal load partitioning problem and also present extensive and complete asymptotic analysis with respect to several parameters of the load and the system. Necessary and sufficient conditions for feasible and optimal load sharing are also derived. A trade-off study between the optimal number of workstations and the bandwidth of the bus is also presented...|$|R
40|$|Reliability and {{dependability}} modeling can {{be employed}} during many stages of <b>analysis</b> of a <b>computing</b> <b>system</b> to gain insights into its critical behaviors. To provide useful results, realistic models of systems are often necessarily large and complex. Numerical analysis of these models presents a formidable challenge because the sizes of their state-space descriptions grow exponentially {{in proportion to the}} sizes of the models. On the other hand, simulation of the models requires analysis of many trajectories in order to compute statistically correct solutions. This dissertation presents a novel framework for performing both numerical analysis and simulation. The new numerical approach computes bounds on the solutions of transient measures in large continuous-time Markov chains (CTMCs). It extends existing path-based and uniformization-based methods by identifying sets of paths that are equivalent with respect to a reward measure and related to one another via a simple structural relationship. This relationship makes it possible for the approach to explore multiple paths at the same time, thus significantly increasing the number of paths that can be explored in a given amount of time. Furthermore, the use of a structured representation for the state space and the direct computation of the desire...|$|R
40|$|Abstract — The {{rapid growth}} of {{handheld}} computing devices such as mobile phones, PDAs or palmtops is {{paving the way for}} the emergence of pervasive <b>computing</b> <b>systems.</b> Just {{as in the case of}} traditional <b>computing</b> <b>systems,</b> perva-sive <b>computing</b> <b>systems</b> need to be tested in the large before they can be deployed in the field. As opposed to traditional <b>computing</b> <b>systems,</b> however, large-scale testing of pervasive <b>computing</b> <b>systems</b> requires the presence of dozens (perhaps hundreds) of physical devices, arranged together in a network, executing a vari-ety of complex scenarios. In order to reduce the cost of such testing, it would be better to simulate the operation of a pervasive <b>computing</b> <b>system</b> using well-known techniques from multi-agent simulation, by representing each (hardware or software) component of the system as a software agent. In this paper we de-scribe our ongoing work, where we extend our earlier work on multi-agent simu-lation, for pervasive <b>computing</b> <b>systems.</b> Since adaptation in pervasive <b>computing</b> <b>systems</b> is expected to be common, we also show that our simulation technique can model adaptation. ...|$|R
40|$|This paper {{deals with}} {{analysis}} of existing approaches to tasks mapping on reconfigurable <b>computing</b> <b>systems</b> with special {{attention paid to}} mapping methods for coarse grained reconfigurable <b>computing</b> <b>systems.</b> The purpose and objectives of a new heuristic method for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> are produced {{on the base of}} the carried out analysis. This novel method for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> is based on the method of graph partitioning with pushing vertices, graph covering algorithm, a heuristic approach to optimizing and packaging for a particular graph on coarse grained reconfigurable <b>computing</b> <b>system</b> and displaying methods of data flow graph on resources of coarse grained reconfigurable <b>computing</b> <b>system.</b> The simulation was conducted on system model with coarse grained reconfigurable hardware accelerator MATRIX. Experimental results are given. They prove the effectiveness of the proposed approach as compared with the widely used methods for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> and the ability to use dynamic functional parameters of coarse grained reconfigurable <b>computing</b> <b>system</b> to further improvement of the mapping results...|$|R
40|$|<b>Computing</b> <b>Systems</b> have a {{tremendous}} impact on everyday life in all domains, from the Internet to consumer electronics, transportation to manufacturing, medicine, energy, and scientific computing. In the future, <b>computing</b> <b>systems</b> will continue to be one of our most powerful tools for taking on the societal challenges shaping Europe, its values, and its global competitiveness. The FP 7 HiPEAC network of excellence is Europe’s premier organization for coordinating research, improving mobility, and enhancing visibility in the <b>computing</b> <b>system</b> field. HiPEAC covers all computing market segments: embedded <b>systems,</b> general purpose <b>computing</b> <b>systems,</b> data centers and high performance computing. Created in 2004, HiPEAC today gathers over 250 leading European academic and industrial <b>computing</b> <b>system</b> research- ers from about 100 universities and 50 companies in one virtual centre of excellence. To encourage <b>computing</b> <b>systems</b> innovation in Europe, HiPEAC provides collaboration grants, internships, sabbaticals, and improves networking through the yearly HiPEAC conference, ACACES summer school, and the semiannual <b>computing</b> <b>systems</b> week. In this roadmap document, HiPEAC leverages the broad expertise of its members to identify and analyze the key challenges for <b>computing</b> <b>systems</b> in Europe over the next decade. While advances in <b>computing</b> <b>systems</b> have been consistent and dra- matic over the past fifty years, its future today is not as certain. To continue to be a tool for providing new and innovative solu- tions, the <b>computing</b> <b>systems</b> community must face serious challenges in efficiency, complexity, and dependability. upmarchipea...|$|R
40|$|Abstract- In {{this paper}} we {{described}} four layer architecture of Grid <b>Computing</b> <b>System,</b> analyzes security requirements and problems existing in Grid <b>Computing</b> <b>System.</b> This paper presents a new approch of five layer security architecture of Grid <b>Computing</b> <b>System,</b> defines {{a new set}} of security policies & gives the representation...|$|R
40|$|Subject of investigation: <b>computing</b> <b>systems</b> {{and methods}} to ensure failure {{stability}} of the parallel real-time <b>computing</b> <b>systems.</b> Purpose of the work: development of methodological and practical recommendations for the analysis and maintenance of the required levels of the failure {{stability of the}} parallel real-time <b>computing</b> <b>systems</b> under the conditions of limitations for the structural redundancy and use of built-in control facilities. Methods and a mathematical model complex are developed {{for the analysis of}} the reliability of the parallel <b>computing</b> <b>systems</b> with active protection. An engineering method and software are developed for calculating the reliability of the <b>computing</b> <b>systems.</b> Technical concepts are proposed on the construction of the failure-stable parallel <b>computing</b> <b>systems.</b> The ingineering method of calculation of the <b>computing</b> <b>system</b> reliability is put into practice in the scientific and research centre of the computer equipment and is included in the standards of the intergovernmental commission on the computer equipment. Field of application: microprocessor computing systemsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Two {{design rules}} which aid the {{construction}} of distributed <b>computing</b> <b>systems</b> {{and the provision of}} fault tolerance are described, namely that: (i) a distributed <b>computing</b> <b>system</b> should be functionally equivalent to the individual <b>computing</b> <b>systems</b> of which it is composed, and (ii) fault tolerant systems should be constructed from generalised fault tolerant components. The reasonin...|$|R
40|$|Recently, {{applications}} using Peer-to-Peer and Grid-computing {{techniques have}} been gaining much attention {{due to the}} growing requirement for data processing. In this work we focus on public-resource <b>computing</b> <b>systems</b> known also as global <b>computing</b> <b>systems</b> or peer-to-peer <b>computing</b> <b>systems.</b> We {{address the problem of}} data transmission in overlay-based public-resource <b>computing</b> <b>systems</b> and examine two approaches: peer-to-peer (P 2 P) and unicast. As the objective we use cost of the system. Optimization model of public-resource <b>computing</b> <b>system</b> formulated as Integer Programming problem is applied to obtain optimal solutions and approximate results of effective heuristics and random approaches. Extensive simulations show that P 2 P approach enables significant reduction of the system cost. 1...|$|R
40|$|Purpose of the work: {{investigation}} of approaches {{and methods of}} organization of synchronization algorithms and development of algorithms for the models and trunk-module <b>computing</b> <b>systems.</b> The synchronization algorithms are classified and compared. An algorithm is developed for the models and for the trunk-module <b>computing</b> <b>systems.</b> The distributed simulation modelling system is implemented for trunk-module <b>computing</b> <b>system</b> "Stand". Field of application: software for the distributed <b>computing</b> <b>systems,</b> distributed simulation modelling, half-natural modellingAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
