136|33|Public
2500|$|The {{cotton-top}} tamarin vocalizes with bird-like whistles, soft chirping sounds, high-pitched trilling, and staccato calls. Researchers describe its repertoire of 38 distinct sounds as unusually sophisticated, conforming to grammatical rules. Jayne Cleveland and Charles Snowdon performed an in-depth feature analysis to classify the cotton-top's repertoire of vocalizations in 1982. They {{concluded that it}} uses a simple grammar consisting of eight phonetic variations of short, frequency-modulated [...] "chirps"—each representing varying messages—and five longer constant frequency [...] "whistles". They hypothesize {{that some of these}} calls demonstrate that the {{cotton-top tamarin}} uses phonetic syntax, while other calls may be exemplars of lexical syntax usage. Each type of call is given a letter signifier; for example, C-calls are associated with finding food and D-calls are associated with eating. Further, these calls can be modified to better deliver information relevant to <b>auditory</b> <b>localization</b> in call-recipients. Using this range of vocalizations, the adults may be able to communicate with one another about intention, thought processes, and emotion, including curiosity, fear, dismay, playfulness, warnings, joy, and calls to young.|$|E
5000|$|Subsequently, between 1776 and 1802, Giovanni Battista Venturi (1746-1822), an Italian physicist, savant, man of letters, diplomat, and {{historian}} of science, conducted and described {{a series of}} experiments intended to elucidate the nature of binaural hearing.It was in an appendix to a monograph on color that Venturi described experiments on <b>auditory</b> <b>localization</b> using one or two ears, concluding that [...] "the inequality of the two impressions, which are perceived {{at the same time}} by both ears, determines the correct direction of the sound".|$|E
50|$|Payne {{was born}} in New York, New York, and {{received}} his BA degree at Harvard University and his Ph.D. at Cornell. He spent {{the early years of}} his career studying echolocation in bats (and how their food, moths, avoid them) and <b>auditory</b> <b>localization</b> in owls. Desiring to work with something more directly linked to conservation he later focused his research on whales where, together with researcher Scott McVay, in 1967 they discovered the complex sonic arrangements performed by the male humpback whales during the breeding season.|$|E
50|$|Since CASA is {{modeling}} human auditory pathways, binaural CASA systems {{better the}} human model by providing sound <b>localization,</b> <b>auditory</b> grouping and robustness to reverberation by including 2 spatially separated microphones. With methods similar to cross-correlation, systems {{are able to}} extract the target signal from both input microphones.|$|R
40|$|Bilateral {{cochlear}} implants aim {{to improve}} sound localization compared to monaural implants, among other potential benefits. Monaural cochlear implants should not support localization in the horizontal plane {{as there are}} no interaural level and time difference cues available, although some previous studies have suggested limited capability. As background to other studies of bilateral implantation, the localization abilities of 18 monaural cochlear implantees were investigated experimentally in an anechoic chamber, using various sound stimuli with different amounts of temporal information. The effects of head movement and reverberation were also investigated. Localization performance {{was found to be}} close to chance for all stimuli. It is confirmed that monaural cochlear implants are unable to support useful <b>auditory</b> sound <b>localization,</b> even when head movements are allowed...|$|R
40|$|This paper {{introduces}} a functional model of <b>auditory</b> sound <b>localization</b> {{based on the}} interaural time difference (ITD). The signals in the nervous system such as action potentials and synaptic transmission are modeled computationally and these applied to a coincidence detector circuit model to detect ITD. Then impulse trains fluctuating in time are used as input. The incorporated model outputs a spike histogram of which {{the peak of the}} envelope will indicate the ITD. The simulation results show that a peak indication the ITD in the azimuth clearly sharpens when using impulses fluctuating in time as input. This suggests that impulse fluctuation does not behave like noise and it can contribute to the detection of ITDs in the temporally redundant process and the nonlinear output mechanism...|$|R
5000|$|After {{growing up}} in wartime Kyoto, Konishi moved to study at Sapporo Agricultural College, Hokkaido University. Konishi studied for his {{doctoral}} thesis on properties of birdsong under Peter Robert Marler at University of California, Berkeley. He {{was elected to the}} National Academy of Sciences (USA) in 1985. The NAS membership directory notes that [...] "Konishi is a world leader in linking ethology and neurophysiology. He has shown the crucial role of auditory feedback in bird songs, has elucidated the role of sound frequency and spatial characteristics in <b>auditory</b> <b>localization</b> by barn owl, and has discovered the role of hormones in early differentiation of vocal control areas in the male avian brain." ...|$|E
5000|$|Rhodes [...] {{sought to}} {{identify}} whether audiospatial attention was represented analogically, that is, if the mental representation of auditory space was arranged {{in the same}} fashion as physical space. If this is the case, then the time to move the focus of auditory attention should be related to the distance to be moved in physical space. Rhodes notes that previous work by Posner, among others, had not found behavioral differences in an auditory attention task that merely requires stimulus detection, possibly due to low-level auditory receptors being mapped tonotopically rather than spatially, as in vision. For this reason, Rhodes utilized an <b>auditory</b> <b>localization</b> task, finding that the time to shift attention increases with greater angular separation between attention and target, although this effect reached asymptote at locations more than of 90° from the forward direction.|$|E
5000|$|Traditional [...] "activation studies" [...] {{focus on}} {{determining}} distributed patterns of brain activity associated with specific tasks. However, scientists {{are able to}} more thoroughly understand brain function by studying the interaction of distinct brain regions, as {{a great deal of}} neural processing is performed by an integrated network of several regions of the brain. An active area of neuroimaging research involves examining the functional connectivity of spatially remote brain regions. Functional connectivity analyses allow the characterization of interregional neural interactions during particular cognitive or motor tasks or merely from spontaneous activity during rest. FMRI and PET enable creation of functional connectivity maps of distinct spatial distributions of temporally correlated brain regions called functional networks. Several studies using neuroimaging techniques have also established that posterior visual areas in blind individuals may be active during the performance of nonvisual tasks such as Braille reading, memory retrieval, and <b>auditory</b> <b>localization</b> as well as other auditory functions.|$|E
40|$|Localization {{of objects}} and events in the {{environment}} is critical for survival, as many perceptual and motor tasks rely on estimation of spatial location. Therefore, {{it seems reasonable to}} assume that spatial localizations should generally be accurate. Curiously, some previous studies have reported biases in visual and <b>auditory</b> <b>localizations,</b> but these studies have used small sample sizes and the results have been mixed. Therefore, it is not clear (1) if the reported biases in localization responses are real (or due to outliers, sampling bias, or other factors), and (2) whether these putative biases reflect a bias in sensory representations of space or a priori expectations (which {{may be due to the}} experimental setup, instructions, or distribution of stimuli). Here, to address these questions, a dataset of unprecedented size (obtained from 384 observers) was analyzed to examine presence, direction, and magnitude of sensory biases, and quantitative computational modeling was used to probe the underlying mechanism(s) driving these effects. Data revealed that, on average, observers were biased towards the center when localizing visual stimuli, and biased towards the periphery when localizing auditory stimuli. Moreover, quantitative analysis using a Bayesian Causal Inference framework suggests that while pre-existing spatial biases for central locations exert some influence, biases in the sensory representations of both visual and auditory space are necessary to fully explain the behavioral data. How are these opposing visual and auditory biases reconciled in conditions in which both auditory and visual stimuli are produced by a single event? Potentially, the bias in one modality could dominate, or the biases could interact/cancel out. The data revealed that when integration occurred in these conditions, the visual bias dominated, but the magnitude of this bias was reduced compared to unisensory conditions. Therefore, multisensory integration not only improves the precision of perceptual estimates, but also the accuracy...|$|R
40|$|Abstract — The {{contribution}} {{of this paper}} is twofold. First, we present a new conceptual framework for modeling incremental hierarchical behavior control systems for humanoids. The biological motivation and the key elements are discussed. Second, we show our current instance of such a behavior control system, called ALIS. It is designed according to the concepts presented within the framework. The system is integrated with the humanoid ASIMO and comprises visual saliency computation and <b>auditory</b> source <b>localization</b> for gaze selection, a visual protoobject based fixation and short term memory of the current visual field of view, the online learning of visual appearances of such proto-objects and an interaction oriented control of the humanoid body including walking. Humans can freely interact with the system in real-time. Experiments show the feasibility of the chosen ansatz. I...|$|R
40|$|This paper investigates {{two types}} of human <b>auditory</b> perceptions, <b>localization</b> and pitch perception, {{to see if there}} could be any common {{mechanism}} underlying both abilities. A brief description of the theories will be listed. From those theories it will be shown that, for lower frequencies, both perceptions seem to be based on timing information from the hair cells in the cochlea. One experiment will also be outlined and performed to investigate our ability to localize harmonic complex tones, depending on how many partials the tones are built of. The results from the experiment are compared with earlier reported experiments on pitch perception and some interesting similarities are enlightened that implies there could be a common mechanism underlying both perceptions. However, more subjects are needed to be able to draw reliable conclusions. Validerat; 20101217 (root...|$|R
5000|$|The {{cotton-top}} tamarin vocalizes with bird-like whistles, soft chirping sounds, high-pitched trilling, and staccato calls. Researchers describe its repertoire of 38 distinct sounds as unusually sophisticated, conforming to grammatical rules. Jayne Cleveland and Charles Snowdon performed an in-depth feature analysis to classify the cotton-top's repertoire of vocalizations in 1982. They {{concluded that it}} uses a simple grammar consisting of eight phonetic variations of short, frequency-modulated [...] "chirps"—each representing varying messages—and five longer constant frequency [...] "whistles". They hypothesize {{that some of these}} calls demonstrate that the {{cotton-top tamarin}} uses phonetic syntax, while other calls may be exemplars of lexical syntax usage. Each type of call is given a letter signifier; for example, C-calls are associated with finding food and D-calls are associated with eating. Further, these calls can be modified to better deliver information relevant to <b>auditory</b> <b>localization</b> in call-recipients. Using this range of vocalizations, the adults may be able to communicate with one another about intention, thought processes, and emotion, including curiosity, fear, dismay, playfulness, warnings, joy, and calls to young.|$|E
50|$|Neuronal plasticity, or the {{capability}} of the brain to adapt to new requirements, {{is a prime example}} of plasticity stressing that the individual’s ability to change is a lifelong process. Recently, researchers have been analyzing how the spared senses compensate for the loss of vision. Without visual input, blind humans have demonstrated that tactile and auditory functions still fully develop. A superiority of the blind has even been observed when they are presented with tactile and auditory tasks. This superiority may suggest that the specific sensory experiences of the blind may influence the development of certain sensory functions, namely tactile and auditory. One experiment was designed by Röder and colleagues to clarify the <b>auditory</b> <b>localization</b> skills of the blind in comparison to the sighted. They examined both blind human adults’ and sighted human adults’ abilities to locate sounds presented either central or peripheral (lateral) to them. Both congenitally blind adults and sighted adults could locate a sound presented in front of them with precision but the blind were clearly superior in locating sounds presented laterally. Currently, brain-imaging studies have revealed that the sensory cortices in the brain are reorganized after visual deprivation. These findings suggest that when vision is absent in development, the auditory cortices in the brain recruit areas that are normally devoted to vision, thus becoming further refined.|$|E
40|$|This report {{summarizes}} ttnree {{studies of}} unaided <b>auditory</b> <b>localization</b> of fixed noise sources. Pointing wa:, {{as accurate as}} aiming at auditory targets in dark-ness. Elevation errors were not significantly larger than azimuth errors. Subjects with hearing deviations (defects) performed as well as non-deviant subjects (normals) in <b>auditory</b> <b>localization...</b>|$|E
40|$|Neural {{structures}} and mechanisms mediating the detection, localization, {{and recognition of}} sounds. Discussion of how acoustic signals are coded by auditory neurons, {{the impact of these}} codes on behavorial performance, and the circuitry and cellular mechanisms underlying signal transformations. Topics include temporal coding, neural maps and feature detectors, learning and plasticity, and feedback control. General principles are conveyed by theme discussions of <b>auditory</b> masking, sound <b>localization,</b> musical pitch, speech coding, and cochlear implants, and auditory scene analysis...|$|R
40|$|The paper {{presents}} {{our recent}} results obtained {{with a new}} <b>auditory</b> spatial <b>localization</b> based BCI paradigm in which the ERP shape differences at early latencies are employed to enhance the traditional P 300 responses in an oddball experimental setting. The concept relies on the recent results in auditory neuroscience showing a possibility to differentiate early anterior contralateral responses to attended spatial sources. Contemporary stimuli-driven BCI paradigms benefit mostly from the P 300 ERP latencies in so called "aha-response" settings. We show the further enhancement of the classification results in spatial auditory paradigms by incorporating the N 200 latencies, which differentiate the brain responses to lateral, {{in relation to the}} subject head, sound locations in the auditory space. The results reveal that those early spatial auditory ERPs boost online classification results of the BCI application. The online BCI experiments with the multi-command BCI prototype support our research hypothesis with the higher classification results and the improved information-transfer-rates. Comment: APSIPA ASC 201...|$|R
40|$|Three {{experiments}} {{investigated the}} impact of working memory load on online plan adjustment during a test of multitasking in young, nonexpert, adult participants. Multitasking was assessed using the Edinburgh Virtual Errands Test (EVET). Participants were asked to memorize either good or poor plans for performing multiple errands and were assessed both on task completion and {{on the extent to}} which they modified their plans during EVET performance. EVET was performed twice, with and without a secondary task loading a component of working memory. In Experiment 1, articulatory suppression was used to load the phonological loop. In Experiment 2, oral random generation was used to load executive functions. In Experiment 3, spatial working memory was loaded with an <b>auditory</b> spatial <b>localization</b> task. EVET performance for both good- and poor-planning groups was disrupted by random generation and sound localization, but not by articulatory suppression. Additionally, people given a poor plan were able to overcome this initial disadvantage by modifying their plans online. It was concluded that, in addition to executive functions, multiple errands performance draws heavily on spatial, but not verbal, working memory resources but can be successfully completed on the basis of modifying plans online, despite a secondary task load...|$|R
40|$|We {{have used}} {{positron}} emission tomography (PET) to measure regional cerebral blood flow (rCBF) in sighted and congenitally blind subjects performing <b>auditory</b> <b>localization</b> tasks. During scanning, the spectral and binaural cues of localized sound were reproduced by a sound system and delivered via headphones. During tasks that required <b>auditory</b> <b>localization</b> both the sighted and blind subjects strongly activated posterior parietal areas. In addition, the blind subjects activated association areas in the right occipital cortex, the foci of which were similar to areas previously identified in visual location and motion detection experiments in sighted subjects. The blind subjects, therefore, demonstrated visual to auditory crossmodal plasticity with <b>auditory</b> <b>localization</b> activating occipital association areas originally intended for dorsal-stream visual processing...|$|E
40|$|Doctor of PhilosophyThis {{dissertation}} {{provides an}} overview of my research {{over the last five years}} into the spectral analysis involved in human sound localization. The work involved conducting psychophysical tests of human <b>auditory</b> <b>localization</b> performance and then applying analytical techniques to analyze and explain the data. It is a fundamental thesis of this work that human <b>auditory</b> <b>localization</b> response directions are primarily driven by the <b>auditory</b> <b>localization</b> cues associated with the acoustic filtering properties of the external auditory periphery, i. e., the head, torso, shoulder, neck, and external ears. This work can be considered as composed of three parts. In the first part of this work, I compared the <b>auditory</b> <b>localization</b> performance of a human subject and a time-delay neural network model under three sound conditions: broadband, high-pass, and low-pass. A “black-box” modeling paradigm was applied. The modeling results indicated that training the network to localize sounds of varying center-frequency and bandwidth could degrade localization performance results in a manner demonstrating some similarity to human <b>auditory</b> <b>localization</b> performance. As the data collected during the network modeling showed that humans demonstrate striking localization errors when tested using bandlimited sound stimuli, the second part of this work focused on human sound localization of bandpass filtered noise stimuli. Localization data was collected from 5 subjects and for 7 sound conditions: 300 Hz to 5 kHz, 300 Hz to 7 kHz, 300 Hz to 10 kHz, 300 Hz to 14 kHz, 3 to 8 kHz, 4 to 9 kHz, and 7 to 14 kHz. The localization results were analyzed using the method of cue similarity indices developed by Middlebrooks (1992). The data indicated that the energy level in relatively wide frequency bands could be driving the localization response directions, just as in Butler’s covert peak area model (see Butler and Musicant, 1993). The question was then raised as to whether the energy levels in the various frequency bands, as described above, are most likely analyzed by the human <b>auditory</b> <b>localization</b> system on a monaural or an interaural basis. In the third part of this work, an experiment was conducted using virtual auditory space sound stimuli in which the monaural spectral cues for <b>auditory</b> <b>localization</b> were disrupted, but the interaural spectral difference cue was preserved. The results from this work showed that the human <b>auditory</b> <b>localization</b> system relies primarily on a monaural analysis of spectral shape information for its discrimination of directions on the cone of confusion. The work described in the three parts lead to the suggestion that a spectral contrast model based on overlapping frequency bands of varying bandwidth and perhaps multiple frequency scales can provide a reasonable algorithm for explaining much of the current psychophysical and neurophysiological data related to human <b>auditory</b> <b>localization...</b>|$|E
40|$|It is {{well known}} that {{discrepancies}} in the location of synchronized auditory and visual events can lead to mislocalizations of the auditory source, so-called ventriloquism. In two experiments, we tested whether such cross-modal influences on <b>auditory</b> <b>localization</b> depend on deliberate visual attention to the biasing visual event. In Experiment 1, subjects pointed to the apparent source of sounds in {{the presence or absence of}} a synchronous peripheral flash. They also monitored for target visual events, either at the location of the peripheral flash or in a central location. <b>Auditory</b> <b>localization</b> was attracted toward the synchronous peripheral flash, but this was unaffected by where deliberate visual attention was directed in the monitoring task. In Experiment 2, bilateral flashes were presented in synchrony with each sound, to provide competing visual attractors. When these visual events were equally salient on the two sides, <b>auditory</b> <b>localization</b> was unaffected by which side subjects monitored for visual targets. When one flash was larger than the other, <b>auditory</b> <b>localization</b> was slightly but reliably attracted toward it, but again regardless of where visual monitoring was required. We conclude that ventriloquism largely reflects automatic sensory interactions, with little or no role for deliberate spatial attention. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|Some past {{studies suggest}} that when sound {{elements}} are heard as one object, the spatial cues in the component elements are integrated to determine perceived location, and that this integration is reduced when the elements are perceived in separate objects. The current study explored how object localization depends on the spatial, spectral, and temporal configurations of sound elements in an <b>auditory</b> scene. <b>Localization</b> results are interpreted in light of results {{from a series of}} previous experiments studying perceptual grouping of the same stimuli, e. g., Shinn-Cunningham et al. [Proc. Natl. Acad. Sci. U. S. A. 104, 12223 – 12227 (2007) ]. The current results suggest that the integration (pulling) of spatial information across spectrally interleaved elements is obligatory when these elements are simultaneous, even though past results show that these simultaneous sound elements are not grouped strongly into a single perceptual object. In contrast, perceptually distinct objects repel (push) each other spatially with a strength that decreases as the temporal separation between competing objects increases. These results show that the perceived location of an attended object is not easily predicted by knowledge of how sound elements contribute to the perceived spectro-temporal content of that object...|$|R
40|$|A {{sound that}} we hear in a natural setting allows us to {{identify}} the sound source and localize it in space. The two aspects can be disrupted independently as shown {{in a study of}} 15 patients with focal right-hemispheric lesions. Four patients were normal in sound recognition but severely impaired in sound localization, whereas three other patients had difficulties in recognizing sounds but localized them well. The lesions involved the inferior parietal and frontal cortices, and the superior temporal gyrus in patients with selective sound localization deficit; and the temporal pole and anterior part of the fusiform, inferior and middle temporal gyri in patients with selective recognition deficit. These results suggest separate cortical processing pathways for <b>auditory</b> recognition and <b>localization...</b>|$|R
40|$|The anteroventral {{cochlear}} nucleus (AVCN) acts as {{the first}} relay center in the conduction of auditory information from the ear to the brain, and it probably performs {{a crucial role in}} sound <b>localization.</b> <b>Auditory</b> nerve input to the principal neurons of the AVCN, the spherical bushy cells, appears to be mediated by an excitatory amino acid such as glutamate, which acts at a specialized, large synaptic ending called an endbulb of Held. Presumably, endbulb synapses contain some specific combination of glutamate receptors to facilitate rapid neurotransmission of auditory signals. AMPA glutamate receptor composition at the endbulb synapses was examined with both light and electron microscope immunocytochemistry. Electron microscope localization of AMPA receptors was examined with two techniques, preembedding immunoperoxidase and postembedding immunogold, which provide maximum sensitivity an...|$|R
40|$|Multisensory {{integration}} {{allows us}} to integrate information from different senses, enhancing the ability to detect, locate and discriminate objects and events in the surroundings. The {{purpose of the present}} study is to examine whether the location and relative intensity of a seemingly irrelevant visual stimulus influences <b>auditory</b> <b>localization.</b> Subjects were asked to localize by pointing a sound in a condition in which a neutral visual stimulus was either suprathreshold (Experiment 1), or subthreshold (Experiment 2). In the crossmodal condition, the spatial disparity of the audio-visual stimuli was systematically varied. The results show that the efficacy of the visual information has substantially different effects on <b>auditory</b> <b>localization</b> with respect to visual capture and adherence to the spatial principle of multisensory integration. When the visual stimulus is presented suprathreshold, vision dominates and captures sound, regardless of the location of the visual stimulus. By contrast, a visual stimulus at threshold level improves <b>auditory</b> <b>localization</b> accuracy only when the two stimuli are spatially coincident...|$|E
40|$|Four {{experiments}} explored possible {{roles for}} working memory in sound localization. In each experiment, the angular error of localization was assessed when performed alone, or concurrently with a working-memory task. The {{role of the}} phonological slave systems in <b>auditory</b> <b>localization</b> was ruled out by Experiments 1 and 2, while an engagement of central resources was suggested {{by the results of}} Experiment 3. Experiment 4 examined the involvement of visuo-spatial systems in <b>auditory</b> <b>localization</b> and revealed impairment of localization by the concurrent spatial workingmemory task. A comparison of dual-task decrement across all four studies suggests that localization places greater demand on central than on spatial resources...|$|E
40|$|In {{response}} to Rhodes' (1987; personal communication, April 22, 1990) assertion that reaction time data in <b>auditory</b> <b>localization</b> must be corrected for eccentricity {{of the sound}} source from straight ahead of the participant, data from <b>auditory</b> <b>localization</b> in the horizontal plane was collected and analyzed. Multiple regression analysis indicates that the eccentricity of the sound to be localized (from straight ahead of the observer) has an interactive effect on reaction time with the angle of required attentional shift. Although the effect of speaker eccentricity cannot be ignored as in Clark, Covey & Hines (1995), the simple correction factor suggested by Rhodes (1987) is also not appropriate to interpret the interaction. Department of Psychological ScienceThesis (M. A. ...|$|E
40|$|Abstract A {{sound that}} we hear in a natural setting allows us to {{identify}} the sound source and localize it in space. The two aspects can be disrupted independently as shown {{in a study of}} 15 patients with focal right-hemispheric lesions. Four patients were normal in sound recognition but severely impaired in sound localization, whereas three other patients had difficulties in recognizing sounds but localized them well. The lesions involved the inferior parietal and frontal cortices, and the superior temporal gyrus in patients with selective sound localization deficit; and the temporal pole and anterior part of the fusiform, inferior and middle temporal gyri in patients with selective recognition deficit. These results suggest sepa-rate cortical processing pathways for <b>auditory</b> recognition and <b>localization...</b>|$|R
40|$|This {{meeting of}} multi-disciplinary {{researchers}} (Santa Barbara, CA, February 1999) explored the integration and {{problems associated with}} multiple modalities and multiple frames of reference, as represented in spatial language and human decision making. Modality themes included learning environments via maps, navigation, and virtual navigation; tactile, <b>auditory,</b> and visual <b>localization</b> and navigation; and learning environments from spatial descriptions. Reference frames under consideration included relative, intrinsic, and absolute reference frames for describing locations; orientation-free vs. orientation-specific representations; heads-up and north-up maps in navigation systems; mixing gaze, route, and survey perspectives in descriptions; expressing differing modalities or frames through language; and cross-cultural differences {{in the use of}} reference frames. This report summarizes the discussions and plenary presentation, and presents a set of research questions for further investigation...|$|R
50|$|Spence and Driver, {{noting that}} {{previous}} findings of audiospatial attentional effects including the aforementioned study by Rhodes could be confounded with response-priming, instead utilized several cuing paradigms, both exogenous and endogenous, {{over the course}} of 8 experiments. Both endogenous (informative) and exogenous (un-informative) cues increased performance in an <b>auditory</b> spatial <b>localization</b> task, consistent with the results previously found by Rhodes. However, only endogenous spatial cues improved performance on an auditory pitch discrimination task; exogenous spatial cues had no effect on the performance of this non-spatial pitch judgement. In light of these findings, Spence and Driver suggest that exogenous and endogenous audiospatial orientating may involve different mechanisms, with the colliculus possibly playing a role in both auditory and visual exogenous orienting, and the frontal and parietal cortex playing a similar part for endogenous orienting. It is noted that the lack of orientation effects to pitch stimuli for exogenous spatial cuing {{may be due to the}} connectivity of these structures, Spence and Driver note that while frontal and parietal cortical areas have inputs from cells coding both pitch and sound location, colliculus is only thought to be sensitive to pitches above 10 kHz, well above the ~350 Hz tones used in their study.|$|R
40|$|The {{visual and}} {{auditory}} systems often concur {{to create a}} unified perceptual experience and to determine the localization of objects in the external world. Co-occurring auditory and visual stimuli in spatial coincidence are known to enhance performance of <b>auditory</b> <b>localization</b> due to the integration of stimuli from different sensory channels (i. e. multisensory integration). However, <b>auditory</b> <b>localization</b> of audiovisual stimuli presented at spatial disparity might also induce a mislocalization of the sound towards the visual stimulus (i. e. ventriloquism effect). Using repetitive transcranial magnetic stimulation we tested the role of right temporoparietal (rTPC), right occipital (rOC) and right posterior parietal (rPPC) cortex in an <b>auditory</b> <b>localization</b> task in which indices of ventriloquism and multisensory integration were computed. We found that suppression of rTPC excitability by means of continuous theta-burst stimulation (cTBS) reduced multisensory integration. No similar effect was found for cTBS over rOC. Moreover, inhibition of rOC, but not of rTPC, suppressed the visual bias in the contralateral hemifield. In contrast, cTBS over rPPC did not produce any modulation of ventriloquism or integrative effects. The double dissociation found {{in the present study}} suggests that ventriloquism and audiovisual multisensory integration are functionally independent phenomena and may be underpinned by partially different neural circuits...|$|E
40|$|Multisensory {{integration}} {{is a powerful}} mechanism for maximizing sensitivity to sensory events. We examined its effects on <b>auditory</b> <b>localization</b> in healthy human subjects. The specific objective was to test whether the relative intensity and location of a seemingly irrelevant visual stimulus would influence <b>auditory</b> <b>localization</b> {{in accordance with the}} inverse effectiveness and spatial rules of multisensory integration that have been developed from neurophysiological studies with animals [Stein and Meredith, 1993 The Merging of the Senses (Cambridge, MA: MIT Press) ]. Subjects were asked to localize a sound in one condition in which a neutral visual stimulus was either above threshold (supra-threshold) or at threshold. In both cases the spatial disparity of the visual and auditory stimuli was systematically varied. The results reveal that stimulus salience is a critical factor in determining the effect of a neutral visual cue on <b>auditory</b> <b>localization.</b> Visual bias and, hence, perceptual translocation of the auditory stimulus appeared when the visual stimulus was supra-threshold, regardless of its location. However, {{this was not the case}} when the visual stimulus was at threshold. In this case, the influence of the visual cue was apparent only when the two cues were spatially coincident and resulted in an enhancement of stimulus localization. These data suggest that the brain uses multiple strategies to integrate multisensory information...|$|E
40|$|AbstractTheory {{indicates}} that neural networks can derive considerable computational power {{from a simple}} multiplication of their inputs, but {{the extent to which}} real neurons do this is unclear. A recent study of the <b>auditory</b> <b>localization</b> pathway of the barn owl has shed new light on this important question...|$|E
40|$|International audienceObjective: We report two psychoacoustical {{experiments}} that assessed {{the relationship between}} <b>auditory</b> azimuthal <b>localization</b> performance in water and duration of prior exposure to the milieu. Background: The adaptability of spatial hearing abilities has been demonstrated in air for both active and passive exposures to altered localization cues. Adaptability occurred faster and was more complete for elevation perception than for azimuth perception. In water, spatial hearing is believed to solely rely on smaller-than-normal cues-to-azimuth: interaural time differences. This should produce a medial bias in localization judgments towards {{the center of the}} horizontal plane, unless the listeners have adapted to the environment. Method: Azimuthal localization performance was measured in seawater for eight azimuthal directions of a low-frequency (< 500 Hz) auditory target. Seventeen participants performed a forced-choice task in Experiment 1. Twenty-eight other participants performed a pointing task in Experiment 2. Results: In both experiments we observed poor front/back discrimination but accurate left/right discrimination, regardless of prior exposure. A medial bias was found in azimuth perception, whose size decreased as the exposure duration of the participant increased. Conclusion: The study resembles earlier results showing that passive exposure to altered azimuth cues elicits the adaptability of internal audio-spatial maps, that is, the behavioral plasticity of spatial hearing abilities. Application: Studies of the adaptability of the auditory system to altered spatial information may yield practical implications for scuba divers, hearing-impaired listeners with reduced sensitivity to spatial cues and various normal-hearing users of virtual auditory displays...|$|R
40|$|The {{consequence}} of blindness on <b>auditory</b> spatial <b>localization</b> {{has been an}} interesting issue {{of research in the}} last decade providing mixed results. Enhanced auditory spatial skills in individuals with visual impairment have been reported by multiple studies, while some aspects of spatial hearing seem to be impaired in the absence of vision. In this study, the ability to encode the trajectory of a 2 dimensional sound motion, reproducing the complete movement, and reaching the correct end-point sound position, is evaluated in 12 early blind individuals, 8 late blind individuals, and 20 age-matched sighted blindfolded controls. Early blind individuals correctly determine the direction of the sound motion on the horizontal axis, but show a clear deficit in encoding the sound motion in the lower side of the plane. On the contrary, late blind individuals and blindfolded controls perform much better with no deficit in the lower side of the plane. In fact the mean localization error resulted 271 ± 10 mm for early blind individuals, 65 ± 4 mm for late blind individuals, and 68 ± 2 mm for sighted blindfolded controls. These results support the hypothesis that i) it exists a trade-off between the development of enhanced perceptual abilities and role of vision in the sound localization abilities of early blind individuals, and ii) the visual information is fundamental in calibrating some aspects of the representation of auditory space in the brain...|$|R
40|$|The {{long-term}} {{goal of this}} research is to understand the neural mechanisms that mediate the ability of normal-hearing people to understand speech and localize sounds in complex acoustic environments comprising reverberation and competing sound sources. In the past year, we continued work on two research projects: (1) Physiological studies of sound localization in reverberant environments; (2) Spatio-temporal representation of pitch in the auditory nerve and cochlear nucleus. We also started a new project on the dynamic range problem, which impacts all aspects of <b>auditory</b> perception. Sound <b>localization</b> in reverberant environments Most listening environments contain acoustically reflective boundary surfaces e. g., ground, walls, trees, and rocks. Listeners are thus faced with the task of localizing sound sources in the presence of interfering reflections and reverberation. Despite this interference, normal-hearing listeners localize sounds quite accurately in moderate reverberation. We showed previously that inferior-colliculus (IC) neurons sensitive to interaural time differences (ITD) are more robust to reverberation than predicted by current models of binaural processing based on interaural crosscorrelation [4]. This work was done in anesthetized animals, and focused on low-frequenc...|$|R
