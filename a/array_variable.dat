34|601|Public
2500|$|... vector v1 [...] := (1,2,3); [...] # <b>array</b> <b>variable</b> {{initially}} (1,2,3) [...] # ...|$|E
5000|$|... {{the lower}} bounds for [...] are {{determined}} as if [...] {{was applied to}} [...] Thus, when a pointer is assigned to a whole <b>array</b> <b>variable,</b> it inherits the lower bounds of the variable, otherwise, the lower bounds default to 1.|$|E
5000|$|A {{variable}} named {{with just}} an underscore often has special meaning. In many interactive shells, {{such as those}} of Python, Ruby, and Perl, [...] or [...] is the previous command or result. In Perl, [...] is a special <b>array</b> <b>variable</b> that holds the arguments to a function.|$|E
5000|$|Sections {{are parts}} of the <b>array</b> <b>variables,</b> and are <b>arrays</b> themselves: ...|$|R
5000|$|One- or {{two-dimensional}} matrix (<b>array)</b> <b>variables</b> of {{the form}} [...] "Ax" [...] or [...] "Ax,y" ...|$|R
40|$|ABSTRACT. Stratght line {{programs}} with assignment statements involving both simple and <b>array</b> <b>variables</b> are considered Two such programs are equivalent if they compute {{the same values}} {{as a function of}} the inputs. Testing the equivalence of array programs ts shown to be NP-hard If <b>array</b> <b>variables</b> are updated but never subsequently referenced, equivalence can be tested in polynomial time Programs without array varmbles can be tested for equivalence in expected linear t~me KEY WORDS AND PHRASES semanttcs, array asstgnments, data structures, NP-complete CR CATEGORIES 5. 24, 5 2...|$|R
5000|$|Dynamic {{lists are}} also more common and easier to {{implement}} than dynamic arrays. Array types are distinguished from record types mainly because they allow the element indices to be computed at run time, as in the Pascal assignment [...] Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an <b>array</b> <b>variable.</b>|$|E
5000|$|The {{overhead}} in [...] "tight" [...] loops often {{consists of}} instructions to increment a pointer or index {{to the next}} element in an array (pointer arithmetic), as well as [...] "end of loop" [...] tests. If an optimizing compiler or assembler is able to pre-calculate offsets to each individually referenced <b>array</b> <b>variable,</b> these can be built into the machine code instructions directly, therefore requiring no additional arithmetic operations at run time.|$|E
5000|$|... in {{computer}} science, an array type is a data type that {{is meant to}} describe a collection of elements (values or variables), each selected {{by one or more}} indices (identifying keys) that can be computed at run time by the program. Such a collection is usually called an <b>array</b> <b>variable,</b> array value, or simply array. [...] By analogy with the mathematical concepts of vector and matrix, array types with one and two indices are often called vector type and matrix type, respectively.|$|E
50|$|Some {{languages}} allow dynamic arrays (also called resizable, growable, or extensible): <b>array</b> <b>variables</b> whose index ranges may {{be expanded}} {{at any time}} after creation, without changing the values of its current elements.|$|R
40|$|Static single {{assignment}} (SSA) form for scalars {{has been a}} significant advance. It has simplified {{the way we think about}} scalar variables. It has simplified the design of some optimizations and has made other optimizations more effective. Unfortunately none of this can be said for SSA form for arrays. The current SSA processing of arrays views an array as a single object. But the kinds of analyses that sophisticated compilers need to perform on arrays, for example those that drive loop parallelization, are at the element level. Current (scalar) SSA form does not provide the element-level data flow information required for such analyses. In this paper, we introduce an Array SSA form that captures precise element-level data flow information for <b>array</b> <b>variables.</b> It is general and simple, and coincides with scalar SSA form when applied to scalar <b>variables.</b> <b>Array</b> SSA form provides for renaming of <b>array</b> <b>variables</b> and uses a OE function to identify the defining assignment for each array elem [...] ...|$|R
40|$|A {{report on}} the {{computer}} routine INTERP 3 is presented. The routine is designed to linearly interpolate a variable which {{is a function of}} three independent variables. The variables within the parameter arrays {{do not have to be}} distinct, or equally spaced, and the <b>array</b> <b>variables</b> can be in increasing or decreasing order...|$|R
5000|$|Traditional compilers used in parallelization {{were able}} to perform {{privatization}} on scalar elements only. In order to exploit parallelism that occurs across loops within a parallel program (loop-level parallelism), the need rose for compilers {{that are able to}} perform <b>array</b> <b>variable</b> privatization as well. Hence, most of today's compilers are capable of performing array privatization with more features and functions to enhance the performance of the parallel program in general. An example of such compiler is the Polaris parallelizing compiler ...|$|E
50|$|A {{common use}} of {{information}} hiding is to hide the physical storage layout for data so that if it is changed, the change is restricted to a small subset of the total program. For example, if a three-dimensional point (x,y,z) is represented in a program with three floating point scalar variables and later, the representation is changed to a single <b>array</b> <b>variable</b> of size three, a module designed with information hiding in mind would protect {{the remainder of the}} program from such a change.|$|E
50|$|In {{order to}} {{effectively}} implement variables of such types as array structures (with indexing done by pointer arithmetic), many languages restrict the indices to integer data types (or other types {{that can be}} interpreted as integers, such as bytes and enumerated types), and require that all elements have the same data type and storage size. Most of those languages also restrict each index to a finite interval of integers, that remains fixed throughout the lifetime of the <b>array</b> <b>variable.</b> In some compiled languages, in fact, the index ranges may have to be known at compile time.|$|E
50|$|C99 {{standardised}} variable-length arrays (VLAs) within block scope. Such <b>array</b> <b>variables</b> are allocated {{based on}} the value of an integer value at runtime upon entry to a block, and are deallocated {{at the end of the}} block. As of C11 this feature is no longer required to be implemented by the compiler.|$|R
40|$|Image {{and video}} {{processing}} applications involve large multi-dimensional signals which {{have to be}} stored in memory modules. In (application-specific) architectures for real-time multidimensional signal processing, a significant cost in terms of chip area and power consumption is due to these background memory units. The multi-dimensional signals are usually modeled in behavioral descriptions with <b>array</b> <b>variables.</b> In the algorithmic specifications of our target applications, many of the array references cover large amounts of scalars. Therefore, the efficient handling of array references in the specifications for image and video processing is crucial for obtaining low cost memory allocation solutions. This paper addresses a central problem which arises when handling the <b>array</b> <b>variables</b> in behavioral specifications: the computation {{of the number of}} scalars covered by an array reference. This problem is closely related to the computation of dependences in data-flow analysis. The novel algo [...] ...|$|R
50|$|The {{other major}} cause of a stack {{overflow}} results from an attempt to allocate more memory on the stack than will fit, for example by creating local <b>array</b> <b>variables</b> that are too large. For this reason some authors recommend that arrays larger than a few kilobytes should be allocated dynamically instead of as a local variable.|$|R
5000|$|ALGOL 68 {{supports}} arrays {{with any}} number of dimensions, and it allows for the slicing of whole or partial rows or columns. mode vector = 1:3 real; # vector mode declaration (typedef) # mode matrix = 1:3,1:3real; # matrix mode declaration (typedef) # vector v1 := (1,2,3); # <b>array</b> <b>variable</b> initially (1,2,3) # real v2 = (4,5,6); # constant array, type equivalent to vector, bounds are implied # op + = (vector a,b) vector: # binary operator definition # (vector out; for i from ⌊a to ⌈a do outi := ai+bi od; out); matrix m := (v1, v2, v1+v2); print ((m,2:)); # {{a slice of the}} 2nd and 3rd columns # ...|$|E
50|$|Palo Alto Tiny BASIC was {{the fourth}} version of Tiny BASIC that {{appeared}} in Dr. Dobb's Journal of Computer Calisthenics & Orthodontia, but probably the most influential. It appeared in the May 1976 Vol 1, No. 5 issue, and distinguished itself from other versions of Tiny BASIC through a novel means of abbreviating commands to save memory, and the inclusion of an <b>array</b> <b>variable</b> ("@"). The interpreter occupied 1.77 kilobytes of memory and assumed {{the use of a}} Teletype Machine (TTY) for user input/output. An erratum to the original article appeared in the June/July issue of Dr. Dobb's (Vol. 1, No 6.) This article also included information on adding additional I/O devices, using code for the VDM video display by Processor Technology as an example.|$|E
50|$|Tiling arrays {{differ from}} {{traditional}} microarrays {{in the nature}} of the probes. Instead of probing for sequences of known or predicted genes that may be dispersed throughout the genome, tiling arrays probe intensively for sequences which are known to exist in a contiguous region. This is useful for characterizing regions that are sequenced, but whose local functions are largely unknown. Tiling arrays aid in transcriptome mapping as well as in discovering sites of DNA/protein interaction (ChIP-chip, DamID), of DNA methylation (MeDIP-chip) and of sensitivity to DNase (DNase Chip) and array CGH. In addition to detecting previously unidentified genes and regulatory sequences, improved quantification of transcription products is possible. Specific probes are present in millions of copies (as opposed to only several in traditional arrays) within an array unit called a feature, with anywhere from 10,000 to more than 6,000,000 different features per <b>array.</b> <b>Variable</b> mapping resolutions are obtainable by adjusting the amount of sequence overlap between probes, or the amount of known base pairs between probe sequences, as well as probe length. For smaller genomes such as Arabidopsis, whole genomes can be examined. Tiling arrays are a useful tool in genome-wide association studies.|$|E
40|$|Abstract—Dynamic {{invariant}} analysis identifies likely properties over variables from observed program traces. These properties can aid programmers in refactoring, documenting, and debugging tasks {{by making}} dynamic patterns visible statically. Two useful forms of invariants involve relations among polynomials over program variables and relations among <b>array</b> <b>variables.</b> Current dynamic analysis methods support such invariants in only very limited forms. We combine mathematical techniques {{that have not}} previously been applied to this problem, namely equation solving, polyhedra construction, and SMT solving, to bring new capabilities to dynamic invariant detection. Using these methods, we show how to find equalities and inequalities among nonlinear polynomials over program variables, and linear relations among <b>array</b> <b>variables</b> of multiple dimensions. Preliminary experiments on 24 mathematical algorithms and an implementation of AES encryption provide evidence that the approach is effective at finding these invariants. Keywords-program analysis; dynamic analysis; invariant generation; nonlinear invariants; array invariants I...|$|R
5000|$|Dynamic - an <b>array</b> of <b>variable</b> phase {{shifters}} {{are used to}} move the beam ...|$|R
50|$|Compared to {{the first}} {{generation}} models these have many more commands including: For and While Loops, If.. Then structures and the ability for real-time user interaction with the Getkey command {{and the ability to}} place characters anywhere on the screen with the Locate and Text commands. Also the method for using <b>array</b> <b>variables</b> was changed to using lists and matrices.|$|R
40|$|A {{low-power}} high-performance scratch-pad {{memory system}} for an embedded VLIW processor is presented. It uses a simple stream address generator capable of implementing {{a variety of}} addressing modes. <b>Array</b> <b>variable</b> rotation, a technique that can replace register renaming and rotating register files is discussed. Factor of 135 x energy-delay advantage is demonstrated using Spice simulations of the processor running speech, vision and signal processing algorithms...|$|E
40|$|The energy {{consumption}} for Mobile Embedded Systems is a limiting factor because of today's battery capacities. The memory subsystem consumes {{a large chunk}} of the energy, necessitating its efficient utilization. Energy efficient scratchpads are thus becoming common, though unlike caches they require to be explicitly utilized. In this paper, an algorithm integrated into a compiler is presented which analyzes the application, partitions an <b>array</b> <b>variable</b> whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Results show an energy improvement between 5. 7 % and 17. 6 % for a variety of applications against a previously known algorithm...|$|E
40|$|AbstractMany {{interesting}} {{systems can}} be seen as having two kinds of state variables: array variables, which are mappings from one data type into another; and basic variables, which are used to control the system, to perform basic computations, and for operations involving arrays. We investigate such systems where:•the type of each basic variable is built from type variables using product and sum constructs;•the type of each <b>array</b> <b>variable</b> is B→B′, where B and B′ are types as for basic variables;•on any type variable, either no operations are available, or only the equality predicate, or only a linear-order predicate;•type variables denote arbitrary non-empty finite sets. We present a complete classification of reachability decision problems for these systems into decid- able or undecidable...|$|E
40|$|Method Declarations................................. 189 9. 4. 1 Inheritance and Overriding............................ 189 9. 4. 2 Overloading........................................ 190 9. 4. 3 Examples of Abstract Method Declarations............... 190 9. 4. 3. 1 Example: Overriding........................ 190 9. 4. 3. 2 Example: Overloading....................... 191 10 Arrays............................................... 193 10. 1 Array Types............................................... 194 10. 2 <b>Array</b> <b>Variables.............................................</b> 194 xii 10. 3 Array Creation.............................................. 195 10. 4 Array Access [...] ...|$|R
40|$|This article {{describes}} and evaluates DIG, a dynamic invariant generator that infers invariants from observed program traces, focusing on numerical and <b>array</b> <b>variables.</b> For numerical invariants, DIG supports both nonlinear equalities and inequalities of arbitrary degree defined over numerical program <b>variables.</b> For <b>array</b> invariants, DIG generates nested relations among multidimensional <b>array</b> <b>variables.</b> These properties are nontrivial and challenging for current static and dynamic invariant analysis methods. The key difference between DIG and existing dynamic methods is its generative technique, which infers invariants directly from traces, {{instead of using}} traces to filter out predefined templates. To generate accurate invariants, DIG employs ideas and tools from the mathematical and formal methods domains, including equation solving, polyhedra construction, and theorem proving; for example, DIG represents and reasons about polynomial invariants using geometric shapes. Experimental results on 27 mathematical algorithms and an implemen-tation of AES encryption provide evidence that DIG is effective at generating invariants for these programs...|$|R
40|$|Abstract — Image {{and video}} {{processing}} applications involve large multidimensional signals which {{have to be}} stored in memory modules. In application-specific architectures for real-time multidimensional signal processing, a significant cost in terms of chip area and power consumption is due to these background memory units. The multidimensional signals are usually modeled in behavioral descriptions with <b>array</b> <b>variables.</b> In the algorithmic specifications of our target applications, many of the array references cover large amounts of scalars. Therefore, the efficient handling of array references in the specifications for image and video processing is crucial for obtaining low cost memory allocation solutions. This paper addresses a central problem which arises when handling the <b>array</b> <b>variables</b> in behavioral specifications: the computation {{of the number of}} scalars covered by an array reference. This problem is closely related to the computation of dependences in data-flow analysis. The novel algorithms proposed in this paper are embedded in the ATOMIUM environment—a memory management system for multidimensional signal processing. Index Terms—Data dependence, data-flow analysis, digital signal processing (DSP), linearly bounded lattice, polytope...|$|R
40|$|Abstract—Parallel netCDF (PnetCDF) is {{a popular}} library used in many {{scientific}} applications to store scientific datasets. It provides high-performance parallel I/O while maintaining file-format compatibility with Unidata’s netCDF. Array variables comprise {{the bulk of the}} data in a netCDF dataset, and for accesses to large regions of single array variables, PnetCDF attains very high performance. However, the current PnetCDF interface only allows access to one <b>array</b> <b>variable</b> per call. If an application instead accesses a large number of small-sized array variables, this interface limitation can cause significant performance degradation, because high end network and storage systems deliver much higher performance with larger request sizes. Moreover, the record variables data is stored interleaved by record, and the contiguity information is lost, so the existing MPI-IO collective I/O optimization can no...|$|E
40|$|Rapidly responding, reversible, sensitive, and {{selective}} porous silicon gas sensors, operating at room temperature, are formed {{with a highly}} efficient electrical contact to a nanopore covered microporous <b>array.</b> <b>Variable</b> surface sensitivities are facilitated {{for a variety of}} gases, based on a complementary concept to that of strong and weak acid-base (HSAB) interactions and commensurate with a basis in physisorption. A range of highly selective surface deposits have been placed on nanopore covered porous silicon micropores with the application of nanostructured metals, metal oxides, and nanoparticle catalytic coatings, providing for notably higher sensitivities and selectivity. Single nanostructure depositions, which include electroless gold, tin, copper, and nickel, as well as nano-alumina, magnesia, and titania provide for the detection of gases including NO, NO 2, CO, NH 3, PH 3, SO 2, H 2 S, HCl, and toluene in an array-based format at the sub-ppm level...|$|E
40|$|Currently {{dataflow}} architectures are programmed using applicative languages to {{ease the}} task of deriving the dataflow graph during compilation. We summarise our experience gained in prototyping a FORTRAN compiler for a pipeline ring dataflow architecture. We present {{the status of the}} current implementation and future directions which the development of the compiler will take. Current evidence suggests {{that it is possible to}} efficiently compile FORTRAN nested loop kernels directly onto dataflow architectures, without the need for additional run-time support mechanisms. We present a scheme for deriving the dataflow graph from the analysis of "carried" <b>array</b> <b>variable</b> subscript expressions, and a scheme to map the actors in the dataflow graph onto a pipeline ring dataflow architecture. Categories and Subject Descriptors: [Programming Languages]: Processors [...] compilers; code [...] generation; optimization General Terms: Languages Additional Key Words and Phrases: Parallelizing compiler, datafl [...] ...|$|E
50|$|Lists are {{typically}} implemented either as linked lists (either singly or doubly linked) or as <b>arrays,</b> usually <b>variable</b> length or dynamic arrays.|$|R
40|$|Communication (data movement) often dominates a computation's runtime {{and energy}} costs, {{motivating}} organizing an algorithm's operations to minimize communication. We study communication {{costs of a}} class of algorithms including many-body and matrix/tensor computations and, more generally, loop nests operating on <b>array</b> <b>variables</b> subscripted by linear functions of the loop iteration vector. We use this algebraic relationship between variables and operations to derive communication lower bounds for these algorithms. We also discuss communication-optimal implementations that attain these bounds...|$|R
40|$|This paper {{describes}} "Variable Zo," a {{novel and}} proprietary approach to antenna design and optimization. The new methodology {{is illustrated by}} applying it {{to the design of}} a resistively-loaded bowtie antenna and to two broadband Yagi-Uda <b>arrays.</b> <b>Variable</b> Zo is applicable to any antenna design or optimization methodology. Using it will result in generally better antenna designs across any user-specified set of performance objectives. Comment: Ver. 2 (14 July 2011). Adds Yagi-Uda array design example. Updates source cod...|$|R
