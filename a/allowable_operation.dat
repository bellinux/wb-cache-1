7|44|Public
40|$|LectureLecture 14 : Centrifugal {{compressors}} are {{at times}} required {{to operate in}} or near the choke region. Various limits of the degree of <b>allowable</b> <b>operation</b> in choke have been established. Based on test data and numerical data, the behavior of centrifugal compressors in the choke region is studied. Changes in aerodynamic performance, thrust load, volute behavior and radial loading are considered. The issue of excitation of impeller vanes is addressed. Particular consideration is given to multistage machines, as well as dual compartment machines, in particular regarding the effects of impeller mismatch during operating conditions at flows significantly higher than the design flow. Limitations in the overload operating range of a compressor not only impact the operational flexibility, but also can require more complicated control systems. The paper addresses aerodynamic, structural as well as rotordynamic issues related to the operation in choke...|$|E
40|$|Abstract — This paper {{identifies}} {{and addresses}} the control challenges associated with simultaneous power and thermal management of a 5 -kW-class solid oxide fuel cell and gas turbine combined cycle system. A model predictive controller (MPC) is developed to achieve improved system performance subject to input and state constraints. The subsystem dynamic couplings and control authority limitations under thermal constraints are investigated by both static and dynamic analysis. Through judicious allocation of penalty parameters {{in the cost}} function associated with the fuel cell current, anode fuel flow rate, and generator load, several different MPC implementations are derived to explore different control design degree of freedom and their performance is evaluated. Simulation results show {{the efficacy of the}} MPC design by demonstrating the fast load transition while maintaining the stack temperature within the <b>allowable</b> <b>operation</b> limits. Index Terms — Model predictive control, power and thermal management, SOFC/GT hybrid system...|$|E
40|$|A Rh-based {{catalyst}} for low temperature hydrogen generation in membrane microreactor applications {{has been developed}} and characterized. A RhZrO 2 catalyst with 1. 4 wt% Rh was prepared by incipient wetness impregnation and was tested for both methane reforming and autothermal reforming at temperatures interesting for membrane reactor applications (i. e. temperatures below 700 °C and steam-to-carbon ratio of 2). The kinetic parameters to describe the reaction rate of both methane steam reforming (SMR) and auto-thermal reforming (ATR) over the RhZrO 2 catalyst have been determined using a 1 D heterogeneous packed bed reactor model to properly account for mass and heat transfer resistances. The experimental results demonstrate that the RhZrO 2 catalyst is extremely active for ATR and resistant to coke formation at much lower temperatures and steam-to-carbon ratios compared with conventional Ni-based catalysts. This makes the new catalyst especially suitable for integration in a Pd-based membrane microreactor with a maximum <b>allowable</b> <b>operation</b> temperature of about 650 °C dictated by the membrane stability...|$|E
5000|$|Security attributes, such as {{the process}} owner and the process' set of permissions (<b>allowable</b> <b>operations).</b>|$|R
50|$|The Damerau-Levenshtein {{distance}} {{differs from}} the classical Levenshtein distance by including transpositions among its <b>allowable</b> <b>operations</b> {{in addition to the}} three classical single-character edit operations (insertions, deletions and substitutions).|$|R
50|$|It is {{said that}} an object is {{constructed}} from primitives by means of <b>allowable</b> <b>operations,</b> which are typically Boolean operations on sets: union, intersection and difference, as well as geometric transformations of those sets.|$|R
40|$|ABSTRACTEvaluation of the {{performance}} of aging structures is essential in the oil and gas industry, where the inaccurate prediction of structural performance can have significantly hazardous consequences. The effects of structure failure due to the significant reduction in wall thickness, which determines the burst strength, make it very complicated for pipeline operators to maintain pipeline serviceability. In other words, the serviceability of gas pipelines and elbows needs to be predicted and assessed to ensure that the burst or collapse strength capacities of the structures remain less than the maximum <b>allowable</b> <b>operation</b> pressure. In this study, several positions of the corrosion in a subsea elbow made of API X 42 steel were evaluated using both design formulas and numerical analysis. The most hazardous corrosion position of the aging elbow was then determined to assess its serviceability. The results of this study are applicable to the operational and elbow serviceability needs of subsea pipelines and can help predict more accurate replacement or repair times...|$|E
40|$|M. S. University of Hawaii at Manoa 2012. Includes bibliographical references. The {{research}} in this thesis was performed at the Hawaii Natural Energy Institute based at the University of Hawaii at Manoa; two principal foci were addressed: (1) to design and investigate the performance of an aqueous-alkaline biocarbon fuel cell that generates power at temperatures ~ 500 K, (2) to determine the electrolyte chemistry at conditions {{similar to those of}} the fuel cell. Our Lab has been working in the first focus, the design of an aqueous-alkaline biocarbon fuel cell, since 2000. In 2007 T. Nunoura et al. [12] published the first paper in this topic. They studied the thermodynamics of the anode and cathode reactions and showed the experimental results of a firstgeneration, aqueous-alkaline biocarbon fuel cell built in the Lab. They showed that an aqueous-alkaline fuel cell operating at 518 K and 35. 8 bar was able to realize an open-circuit voltage of 0. 57 V, a short circuit current density of 43. 6 mA/cm 2 and a maximum power of 19 mW, using a 6 M KOH/ 1 M LiOH mixed electrolyte with a catalytic silver screen/platinum foil cathode and an anode composed of 0. 5 g of compacted corncob charcoal previously carbonized at 950 °C. A second paper published by M. Antal and G. Nihous [13] proved that the reactions of a moderate temperature aqueous-alkaline biocarbon fuel cell may be favored at temperatures as high as 300 °C and the carbonate electrolyte may be as effective as the hydroxide electrolyte. Based on these promising findings, the work in the biocarbon fuel cell continued. Different fuel cells were designed and built to overcome the problems that arose. The fuel cell working with high concentrations of potassium carbonate solution as electrolyte showed the best performance. However, unexpected crystals determined by TG-MS as potassium bicarbonate appeared in this fuel cell that caused our research focus to take another direction: The study of the electrolyte. We decided to focus on the carbonate/bicarbonate chemistry and the understanding of the formation of these potassium bicarbonate crystals. To study the formation of the crystals, a "tubing bomb" that can stand pressures as high as 2000 psi and can be quickly heated in a sand bath was built. Previous literature [14] indicates the decomposition of dry bicarbonate into carbonate and CO 2. This research focuses on the thermodynamics of this decomposition reaction in solution at conditions close to the fuel cell conditions. By determining the equilibrium constant, the thermodynamic properties (enthalpy, entropy and Gibbs free energy) of the bicarbonate decomposition reaction and the temperature at which the potassium bicarbonate in solution will completely decompose, we can determine the <b>allowable</b> <b>operation</b> temperature for the fuel cell...|$|E
40|$|We {{develop a}} {{framework}} for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides {{an alternative to the}} lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the <b>allowable</b> <b>operations</b> in order to capture basic generalizations and improve maintainability. ...|$|R
50|$|In {{the theory}} of computation, {{abstract}} machines are often used in thought experiments regarding computability or to analyze the complexity of algorithms (see computational complexity theory). A typical abstract machine consists of a definition in terms of input, output, and the set of <b>allowable</b> <b>operations</b> used to turn the former into the latter. The best-known example is the Turing machine.|$|R
40|$|Abstract—In this paper, {{we develop}} {{a theory of}} computable types {{suitable}} {{for the study of}} control systems. The theory uses type-two effectivity as the underlying computational model, but we quickly develop a type system which can be manipulated abstractly, but for which all <b>allowable</b> <b>operations</b> are guaran-teed to be computable. We apply the theory to the study of hybrid systems, reachability analysis, and control synthesis. I...|$|R
50|$|In {{computability theory}} and {{computational}} complexity theory, {{a model of}} computation {{is the definition of}} the set of <b>allowable</b> <b>operations</b> used in computation and their respective costs. It is used for measuring the complexity of an algorithm in execution time and or memory space: by assuming a certain model of computation, it is possible to analyze the computational resources required or to discuss the limitations of algorithms or computers.|$|R
5000|$|There {{are other}} popular {{measures}} of edit distance, which are calculated {{using a different}} set of <b>allowable</b> edit <b>operations.</b> For instance, ...|$|R
5000|$|For example, in the Java {{programming}} language, the [...] "int" [...] type {{represents the}} set of 32-bit integers ranging in value from -2,147,483,648 to 2,147,483,647, {{as well as the}} operations that can be performed on integers, such as addition, subtraction, and multiplication. Colors, on the other hand, are represented by three bytes denoting the amounts each of red, green, and blue, and one string representing that color's name; <b>allowable</b> <b>operations</b> include addition and subtraction, but not multiplication.|$|R
5000|$|In Microsoft's [...]NET Framework, the Common Type System (CTS) is a {{standard}} that specifies how type definitions and specific values of types are represented in computer memory. It is intended to allow programs written in different programming languages to easily share information. As used in programming languages, a type {{can be described as}} a definition of a set of values (for example, [...] "all integers between 0 and 10"), and the <b>allowable</b> <b>operations</b> on those values (for example, addition and subtraction).|$|R
40|$|Computer {{systems for}} data {{management}} are increasingly {{concerned with the}} authorization problem: the problem of creating and managing the matrix of users, objects, and <b>allowable</b> <b>operations.</b> This paper discusses the authorization problem and explains why the problem {{is likely to become}} more complex in the future. We then present the use of database normalization theory to formalize and address the authorization problem. A variety of issues related to authorization that arise in practical data management deployments are discussed. Finally, recommendations are made for further research on authorization...|$|R
40|$|Vector {{algorithms}} {{allow the}} computation of an output vector r = r 1 r 2 : : : rm given an input vector e = e 1 e 2 : : : em in a bounded number of operations, independent of m {{the length of}} the vectors. The <b>allowable</b> <b>operations</b> are usually restricted to bit-wise operations available in processors, including shifts and binary addition with carry. These restrictions imply that the existence of a vector algorithm for a particular problem opens the way to extremely fast implementations, using the inherent parallelism of bit-wise operations...|$|R
2500|$|Rather than {{inferring}} {{the ancestral}} DNA sequence, {{one may be}} interested in the larger-scale molecular structure and content of an ancestral genome. This problem is often approached in a combinatorial framework, by modelling genomes as permutations of genes or homologous regions. Various operations are allowed on these permutations, such as an inversion (a segment of the permutation is reversed in-place), deletion (a segment is removed), transposition (a segment is removed from one part of the permutation and spliced in somewhere else), or gain of genetic content through recombination, duplication or horizontal gene transfer. The [...] "genome rearrangement problem", first posed by Watterson and colleagues, asks: given two genomes (permutations) and a set of <b>allowable</b> <b>operations,</b> what is the shortest sequence of operations that will transform one genome into the other? [...] A generalization of this problem applicable to ancestral reconstruction is the [...] "multiple genome rearrangement problem": given a set of genomes and a set of <b>allowable</b> <b>operations,</b> find (i) a binary tree with the given genomes as its leaves, and (ii) an assignment of genomes to the internal nodes of the tree, such that the total number of operations across the whole tree is minimized. [...] This approach is similar to parsimony, except that the tree is inferred along with the ancestral sequences. [...] Unfortunately, even the single genome rearrangement problem is NP-hard, although it has received much attention in mathematics and computer science (for a review, see Fertin and colleagues).|$|R
50|$|These proofs {{require an}} {{assumption}} {{of a particular}} model of computation, i.e., certain restrictions on <b>operations</b> <b>allowable</b> with the input data.|$|R
40|$|Vector {{algorithms}} {{allow the}} computation of an output vector r = r 1 r 2 :::rm given an input vector e = e 1 e 2 :::em inaboundednumber of operations, independent of m {{the length of}} the vectors. The <b>allowable</b> <b>operations</b> are usually restricted to bit-wise operations available in processors, including shifts and binary addition with carry. These restrictions imply that the existence of a vector algorithm for a particular problem opens the way to extremely fast implementations, using the inherent parallelism of bit-wise operations. This paper presents general results on the existence and construction of vector algorithms, with a particular focus on problems arising from computational biology. We show that efficient vector algorithms exist for the problem of approximate string matching with arbitrary weighted distances, generalizing a previous result by G. Myers. We also characterize a class of automata for which vector algorithms can be automatically derived from the transition table of the automata...|$|R
40|$|Tabled logic {{programming}} has become important to {{logic programming}} {{in part because}} it opens new application areas, such as model checking, to logic programming techniques. However, the development of new extensions of tabled logic programming is becoming restricted by the formal methods that underly it. Formalisms for tabled evaluations, such as SLG [2], are generally developed with a view to a specific set of <b>allowable</b> <b>operations</b> that can be performed in an evaluation. In the case of SLG, tabling operations are based on a variance relation between atoms. While the set of SLG tabling operations has proven useful for a number of applications, other types of operations, such as those based on a subsumption relation between atoms, can have practical uses. In this paper, SLG is reformulated in two ways: {{so that it can be}} parameterized using different sets of operations; and to use a forest of trees paradigm. Equivalence to SLG of the new formulation, SLGX, is shown when it is parameterize [...] ...|$|R
40|$|The {{maximum number}} of strands used is an {{important}} measure of a molecular algorithm's complexity. This measure is also called the space used by the algorithm. We show that every NP problem that can be solved with b(n) bits of nondeterminism can be solved by molecular computation in a polynomial number of steps, with four test tubes, in space 2 b(n). In addition, we identify a large class of recursive algorithms that can be implemented using bounded nondeterminism. This yields improved molecular algorithms for important problems like 3 -SAT, independent set, and 3 -colorability. 1. A model of molecular computing Molecular computation was first studied in [1, 17]. The models we define were inspired as well {{by the work of}} [3, 23]. A molecular sequence is a string over an alphabet Σ (we can use any alphabet we like, encoding characters of Σ by finite sequences of base pairs). A test tube is a multi-set of molecular sequences. We describe the <b>allowable</b> <b>operations</b> below. Where se [...] ...|$|R
40|$|Abstract — Security is {{emerging}} as an important requirement {{for a number of}} distributed applications such as online banking, social networking etc. due to the private nature of the data being involved. Further more, the wide spread use of portable devices such as laptops, PDAs etc. allows users to make meaningful ad hoc collaborations. Traditional security solutions are not feasible for these scenarioes due to the varying nature of the collaborations in terms of entities involved and their roles, available resources etc. Under these circumstances, we need generic solutions that take into account the semantics of the collaborations in determining the set of <b>allowable</b> <b>operations.</b> In this paper, we propose an extensible framework that uses semantics driven policies for enforcing security. Our policies are rooted in semantic web languages which makes amenable to interoperability, and also enables high level reasoning for conflict resolution and policy adaptation. We describe our policy based network that uses packet content semantics to best handle different streams, and show how our framework can be used to secure enterprise networks and the BGP routing process. I...|$|R
40|$|An image hash {{should be}} (1) robust to <b>allowable</b> <b>operations</b> and (2) {{sensitive}} to illegal manipulations (like image tampering) and distinct queries. Although existing methods try {{to address the}} first issue, the second issue has not been adequately looked into, primarily the issue of localizing tampering. This is {{primarily because of the}} difficulty in meeting two contradictory requirements. First, the hash should be small and second, to detect localized tampering, the amount of information in the hash about the original should be as large as possible. The desynchronization of the query with the original further aggravates the problem. Hence a tradeoff between these factors needs to be found. This paper presents an image hashing approach that is both robust and sensitive to not only detect but also localize tampering using a small signature (< 1 kB). To our knowledge this is the first hashing method that can localize image tampering using a small signature that is not embedded into the image, like in watermarking. Index Terms — Locality preserving hashing, edge histogram, local region descriptors. 1...|$|R
40|$|Despite the {{simplicity}} of this example, it demonstrates several features of the MOODS data model. First, the semantics of the data at each stage continuously tracks the current meaning {{of the information that}} has been identified. The newly identified semantic information can then be queried and searched. The semantics at each stage also restrict the <b>allowable</b> <b>operations</b> at that stage, preventing users from applying inappropriate operations. Second, extracting the arms and legs requires computing the difference between the results of two distinct processing stages which illustrates the power and necessity of the combine operation. Third, both backtracking and interactive refinement allow users to follow multiple processing paths out of a single stage and also allows the user to repeat operations which fail to produce the desired results (e. g., the wrong parameters were selected for an operation such as dilation or erosion). Fourth, MOODS history and path saving abilities allow users to define and then automatically replay a processing path (e. g., to extract the arms and legs) on new images. ...|$|R
40|$|In {{this article}} I {{will discuss the}} {{semantics}} of predication in English {{and some of its}} implications for syntax. Consider (1) : (1) John is crazy. (1) says that John has (or instantiates) a certain property (or state), the property of being crazy. This, of course, is informative {{only to the extent that}} we know what properties are and what operations we can perform on them (for example, under what conditions we can conjoin two properties or attribute a property to an individual). Suppose it is possible to determine the set of operations we can perform on properties and to use the resulting structure in spelling out the semantics of predicative constructions. Presumably, there will be just a finite set of such <b>allowable</b> <b>operations.</b> Since there must exist some systematic relation between syntax and semantics, it follows that this will make pre-dictions about the behavior of predicative expressions. For example, it may set up a limited range of patterns that we should expect to find. Thus, a (semantic) theory of properties might be a significant step toward defining what a possible grammar for natural languages can be...|$|R
40|$|Levenshtein {{distance}} is a metric {{for measuring the}} amount of difference between two sequences. A metric, in this case, measure similarity or dissimilarity (distance) between two text strings for approximate matching (fuzzy string search) or comparison. The term edit {{distance is}} often used to refer specifically to Levenshtein distance. The Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with the <b>allowable</b> edit <b>operations</b> being insertion, deletion, or substitution of a single characte...|$|R
40|$|AbstractMost {{research}} on the edit distance problem and thek-differences problem considered the set of edit operations consisting of changes, insertions, and deletions. In this paper we include theswapoperation that interchanges two adjacent characters into the set of <b>allowable</b> edit <b>operations,</b> and we present anO(tmin(m,n)) -time algorithm for the extended edit distance problem, wheretis the edit distance between the given strings, and anO(kn) -time algorithm for the extendedk-differences problem. That is, we add swaps into the set of edit operations without increasing the time complexities of previous algorithms that consider only changes, insertions, and deletions for the edit distance andk-differences problems...|$|R
40|$|Part 3 : Secure Devices and Execution EnvironmentInternational audienceA {{challenging}} {{problem in}} managing large networks is {{the complexity of}} security administration. Role Based Access Control (RBAC) is the most well-known access control model in diverse enterprises of all sizes because of its ease of administration {{as well as economic}} benefits it provides. Deploying such system requires identifying a complete set of roles which are correct and efficient. This process, called role engineering, has been identified {{as one of the most}} expensive tasks in migrating to RBAC. Numerous bottom-up, top-down, and hybrid role mining approaches have been proposed due to increased interest in role engineering in recent years. In this paper, we propose a new top-down role engineering approach and take the first step towards extracting access control policies from unrestricted natural language requirements documents. Most organizations have high-level requirement specifications that include a set of access control policies which describes <b>allowable</b> <b>operations</b> for the system. It is very time consuming, labor-intensive, and error-prone to manually sift through these natural language documents to identify and extract access control policies. We propose to use natural language processing techniques, more specifically Semantic Role Labeling (SRL) to automatically extract access control policies from these documents, define roles, and build an RBAC system. By successfully applying semantic role labeling to identify predicate-argument structure, and using a set of predefined rules on the extracted arguments, we were able correctly identify access control policies with a precision of 79 %, recall of 88 %, and F 1 score of 82 %...|$|R
40|$|Data {{types are}} a well known concept in {{computer}} science (for example, in programming languages or in database systems). A data type defines a set of homogeneous values and the <b>allowable</b> <b>operations</b> on those values. An example is a type integer representing the set of 32 -bit integers and including operations such as addition, subtraction, and multiplication that can be performed on integers. Spatial data types or geometric data types provide a fundamental abstraction for modeling the geometric structure of objects in space {{as well as their}} relationships, properties, and operations. They are of particular interest in spatial databases [5, 8, 13] and Geographical Information Systems [14]. One speaks of spatial objects as values of spatial data types. Examples are two-dimensional data types for points (for example, representing the locations of lighthouses in the U. S.), lines (for example, describing the ramifications of the Nile Delta), regions (for example, depicting air-polluted zones), spatial networks (for example, representing the routes of the Metro in New York), and spatial partitions (for example, describing the 50 states of the U. S. and their exclusively given topological relationships of adjacency or disjointedness) as well as three-dimensional data types for surfaces (for example, modeling the shape of landscapes) or volumes (for example, representing urban areas). Operations on spatial data types include spatial operations like the geometric intersection, union, and difference of spatial objects, numerical operations like the length of a line or the area of a region, topological relationships checking the relative position of spatial objects to each othe...|$|R
40|$|In {{incremental}} development strategies, modelers frequently refine Statecharts models to satisfy requirements and changes. Although several solutions exist {{to the problem}} of Statecharts refinement, they provide such levels of freedom that a statechart cannot make assumptions or guarantees about its future structure. In this paper, we propose a set of bounding rules to limit the <b>allowable</b> Statecharts refinement <b>operations</b> such that certain assumptions will hold. Comment: In Poster Proceedings of 6 th Conference on Software Language Engineering (SLE) 2013 ([URL]...|$|R
40|$|In this report, {{we apply}} {{constraint}} least squares solution (CLS) {{to the problem}} of reducing the number of operations in FIR digital filters with a motivation of reducing its power consumption. The constraints are defined by the maximum <b>allowable</b> add/subtract <b>operations</b> in forming the products which are used in computing the output. We show that truncation and rounding 0. f coefficients can be viewed as power constrained least squares (PCLS) solutions. Further, we show that in dedicated DSP processor based architectures it is possible to reduce power by using PCLS coefficients along with appropriately modified multipliers. It is also shown that Booth multiplier effectively reduces the complexity of such filters, thereby increasing power savings. Finally, we show that typically 30 % and 45 % retluction in number of operations can be obtained for systems employing uncoded and Booth recoded multipliers, respectively...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of the MATE workbench, a program which provides support for the annotation of speech and text. It provides facilities for exible display and editing of such annotations, and complex querying of a resulting corpus. The workbench oers a more exible approach than most existing annotation tools, which were often designed with a specic annotation scheme in mind. Any annotation scheme can be used with the MATE workbench, provided it is coded using XML markup (linked to the speech signal, if available, using certain conventions). The workbench uses a transformation language to dene specialised editors optimised for particular annotation tasks, with suitable display formats and <b>allowable</b> editing <b>operations</b> tailored to the task. The workbench is written in Java, which means that it is platform-independent. This paper outlines the design of the workbench software and compares it with other annotation programs. Zusammenfassung Dieser Beitr [...] ...|$|R
40|$|In this paper, {{we apply}} {{constrained}} least squares solution (CLS) {{to the problem}} of reducing the number of operations in FIR digital filters with a motivation of reducing its power consumption. The constraints are defined by the maximum <b>allowable</b> add/subtract <b>operations</b> in forming the products which are used in computing the output. We show that truncation and rounding of coefficients can be viewed as power constrained least squares (PCLS) solutions. Further, we show that in dedicated DSP processor based architectures it is possible to reduce power by using PCLS coefficients along with appropriately modified multipliers. It is also shown that Booth multiplier effectively reduces the complexity of such filters, thereby increasing power savings. Finally, we show that typically 30 % to 45 % reduction in number of operations can be obtained for systems employing uncoded and Booth recoded multipliers, respectively. I. INTRODUCTION Recently, constrained least square (CLS) design of FIR filter [...] ...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of the MATE workbench, a program which provides support for flexible display and editing of XML annotations, and complex querying of a set of linked files. The workbench was designed to support the annotation of XML coded linguistic corpora, but it could be used to annotate any kind of data, as it is not dependent on any particular annotation scheme. Rather than being a general purpose XMLaware editor it is a system for writing specialised editors tailored to a particular annotation task. A particular editor is defined using a transformation language, with suitable display formats and <b>allowable</b> editing <b>operations.</b> The workbench is written in Java, which means that it is platform-independent. This paper outlines the design of the workbench software and compares it with other annotation programs. 1. Introduction The annotation or markup of files with linguistic or other complex information usually requires either human coding or human [...] ...|$|R
40|$|In the {{previous}} chapters, we discussed problems involving an exact match of string patterns. We {{now turn to}} problems involving similar but not necessarily exact pattern matches. There {{are a number of}} similarity or distance measures, {{and many of them are}} special cases or generalizations of the Levenshtein metric. The problem of evaluating the measure of string similarity has numerous applications, including one arising in the study of the evolution of long molecules such as proteins. In this chapter, we focus on the problem of evaluating a longest common subsequence, which is expressively equivalent to the simple form of the Levenshtein distance. 4. 1 Levenshtein distance and the LCS problem The Levenshtein distance is a metric that measures the similarity of two strings. In its simple form, the Levenshtein distance, D(x; y), between strings x and y is the minimum number of character insertions and/or deletions (indels) required to transform string x into string y. A commonly used generalization of the Levenshtein distance is the minimum cost of transforming x into y when the <b>allowable</b> <b>operations</b> are character insertion, deletion, and substitution, with costs (;); (;), and (1; 2), that are functions of the involved character(s). There are direct correspondences between the Levenshtein distance of two strings, the length of the shortest edit sequence from one string to the other, and the length of the longest common subsequence (LCS) of those strings. If D is the simple Levenshtein distance between two strings having lengths m and n, SES is the length of the shortest edit sequence between the strings, and L is the length of an LCS of the strings, then SES = D and L = (m + n 0 D) = 2. We will focus on the problem of determining the length of an LCS and also on the related problem of recovering an LCS. Another related problem, which will be discussed in Chapter 7, is that of approximate string matching, in which it is desired to locate all positions within string y which begin an approximation to string x containing at most D errors (insertions or deletions). 124 SERIAL COMPUTATIONS OF LEVENSHTEIN DISTANCES procedure CLASSIC (x, m, y, n, C, p) : begi...|$|R
