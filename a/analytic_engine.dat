18|111|Public
5|$|The {{origins of}} true (MIMD) {{parallelism}} {{go back to}} Luigi Federico Menabrea and his Sketch of the <b>Analytic</b> <b>Engine</b> Invented by Charles Babbage.|$|E
25|$|Sydney Padua created The Thrilling Adventures of Lovelace and Babbage, {{a cartoon}} {{alternate}} history in which Babbage and Lovelace succeed {{in building the}} <b>analytic</b> <b>engine.</b> It quotes heavily from the writings of Lovelace, Babbage and their contemporaries.|$|E
50|$|The {{origins of}} true (MIMD) {{parallelism}} {{go back to}} Luigi Federico Menabrea and his Sketch of the <b>Analytic</b> <b>Engine</b> Invented by Charles Babbage.|$|E
5000|$|Ooyala built a scalable, flexible, {{real-time}} <b>analytics</b> <b>engine</b> using Cassandra ...|$|R
5000|$|... 2012 Paragon Global Technology Inc (PGTI) was {{acquired}} by CA Technologies - including iDash Workload <b>Analytics</b> <b>Engine</b> for AutoSys.|$|R
50|$|Ninja Metrics {{launched}} its primary service called the Katana Social <b>Analytics</b> <b>Engine</b> in 2013, a Cloud computing Social analytics platform for video games.|$|R
5000|$|Sydney Padua created The Thrilling Adventures of Lovelace and Babbage, {{a cartoon}} {{alternate}} history in which Babbage and Lovelace succeed {{in building the}} <b>analytic</b> <b>engine.</b> It quotes heavily from the writings of Lovelace, Babbage and their contemporaries.|$|E
5000|$|In 2000, McGregor joined Data Digest, {{where he}} led the {{engineering}} team developing of an Bayesian Network predictive <b>analytic</b> <b>engine</b> which analyzed arbitrary databases or tables of transactional data to find hidden predictive relationships. Initially named [...] "Business Navigator / BN-5", the product was later sold to Decision-Q Corporation who renamed it FasterAnalytics. The <b>analytic</b> <b>engine</b> is general purpose and was initially used to for analyze data {{for a wide range}} of problems, from predicting response levels to offers for a mail order record and video club to predicting protein-folding in drug discovery research. It is notable that with this work, McGregor returned to the roots of his earlier Prescient Agent work of a decade earlier which also relied on Bayesian prediction. However, a decade of computer performance improvements now made technology that had once challenged $100,000 workstations well within the capabilities of desktop office machines.|$|E
5000|$|In data mining, {{the process}} of using a model to derive {{predictions}} or descriptions of behavior that is yet to occur is called [...] "scoring". In traditional analytic workbenches, a model built in the <b>analytic</b> <b>engine</b> has to be deployed in a mission-critical system to score new data, or the data is moved from relational tables into the analytical workbench - most workbenches offer proprietary scoring interfaces. ODM simplifies model deployment by offering Oracle SQL functions to score data stored right in the database. This way, the user/application-developer can leverage the full power of Oracle SQL - {{in terms of the}} ability to pipeline and manipulate the results over several levels, and in terms of parallelizing and partitioning data access for performance.|$|E
5000|$|<b>Analytics</b> <b>engines</b> for capturing, {{measuring}} {{and reporting}} on all internal {{aspects of the}} system including network bandwidth, read/write error rates, data storage profiles, etc.|$|R
50|$|Apache Kylin is an {{open source}} {{distributed}} <b>analytics</b> <b>engine</b> {{designed to provide a}} SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets.|$|R
5000|$|... 30 January 2014: {{the company}} {{announced}} {{that one of its}} partners, Kainos, integrated HP IDOL 10.5, the new version of HP Autonomy’s information <b>analytics</b> <b>engine,</b> into Kainos's electronic medical record platform, Evolve.|$|R
40|$|Online social {{networks}} become a bridge toconnect our physical {{daily life and}} the virtual Web space, which not only provides rich data for mining, but also brings many new challenges. In this paper, we present a novel Social <b>Analytic</b> <b>Engine</b> (SAE) for large online {{social networks}}. The key issues we pursue in the <b>analytic</b> <b>engine</b> {{are concerned with the}} following problems: 1) at the micro-level, how do people form different types of social ties and how people influence each other? 2) at the meso-level, how do people group into communities? 3) at the macro-level, what are the hottest topics in a social network and how the topics evolve over time? We propose methods to address the above questions. The methods are general and can be applied to various social networking data. We have deployed and validated the proposed <b>analytic</b> <b>engine</b> over multiple different networks and validated the effectiveness and efficiency of the proposed methods...|$|E
40|$|Abstract—With the {{increasing}} complexity of cyber-physical systems, {{it is essential}} to enhance their self-management capabilities (e. g., self-protection, self-optimization). This paper presents a data-oriented approach to achieving that goal, given that a large amount of measurements can be collected in current systems. We investigate typical data characteristics in physical systems, and identify that the collected data from those systems exhibit a wide range of diversities. Following those observations, a new <b>analytic</b> <b>engine</b> is proposed and developed to extract knowledge from measurement data streams in physical systems. The engine treats each attribute in measurements as a time series and contains an ensemble of models, each attempting to discover a specific data property accordingly, such as periodicity, pairwise dependency and so on. Therefore time series are profiled based on their properties captured by engine models. The extracted data profiles can be further used to facilitate several management tasks of system status monitoring and online anomaly detection. Our experimental results in a real power plant have demonstrated that our <b>analytic</b> <b>engine</b> can correctly profile heterogeneous time series in the system, and successfully detect a number of abnormal situations in the system operation including some system inspection events as well as component faults. I...|$|E
30|$|An <b>analytic</b> <b>engine</b> in {{top layer}} {{processes}} {{the data for}} application specific purposes. The engine utilises the data that {{is available in the}} linked data layer and helps users in submitting queries, application specific algorithms and workflows to find information from the data repositories. In this respect, Big Data Mining is recently a new trend used to identify large data sets due to complexity, cardinality and continuality [25, 26]. Big Data Mining techniques are increasingly becoming an important and effective way in various data driven applications such as network traffic risk analysis, business data analysis etc. These techniques will be extremely useful to generate non-obvious relations and associations from huge data available from public services of smart future cities.|$|E
30|$|Urban {{information}} systems collect, analyze, produce, and manage various information products pertaining to social, economic, and environmental phenomena in urban areas. Potentially, UIS {{can serve as}} data providers, <b>analytic</b> <b>engines,</b> and information-sharing platforms for the measurement and monitoring of the SDGs.|$|R
50|$|MariaDB Corporation {{announced}} on April 5, 2016 {{the release of}} its first big data <b>analytics</b> <b>engine,</b> MariaDB ColumnStore. It is based both on a fork of InfiniDB and open-source community contributions. ColumnStore supports use cases including real-time, batch and algorithmic.|$|R
5000|$|In 2007, Hanweck {{launched}} Volera Options <b>Analytics</b> <b>Engine,</b> {{the first}} GPU-based software product {{for the financial}} services industry. In 2011, Hanweck introduced a new TIMS Scenario Feed of real-time equity option scenario calculations, based on the Options Clearing Corp's Theoretical Intermarket Margining System.|$|R
40|$|In {{this paper}} {{we present a}} new server {{monitoring}} method based on a new and powerful approach to dynamic data analysis: Process Query Systems (PQS). PQS enables userspace monitoring of servers and, by using advanced behavioral models, makes accurate and fast decisions regarding server and service state. Data to support state estimation come from multiple sensor feeds located within a server network. By post-processing a system’s state estimates, it becomes possible to identify, isolate and/or restart anomalous systems, thus avoiding cross-infection or prolonging performance degradation. The PQS system we use is a generic process detection software platform. It builds on {{the wide variety of}} system-level information that past autonomic computing research has studied by implementing a highly flexible, scalable and efficient process-based <b>analytic</b> <b>engine</b> for turning raw system information into actionable system and service state estimates. ...|$|E
40|$|This paper {{describes}} a comprehensive biomathematical cancer modeling facility, {{and its use}} in meeting the challenges of developing better cancer treatment strategies by exploiting the cancer biology knowledge explosion thoroughly. Using information about the biology of cancer cells {{in the treatment of}} patients requires synthesizing this information to draw conclusions about the whole tumor/patient relationship, hopefully pointing to better treatments. Oncology Thinking Cap (OncoTCAP) does the required synthesis by providing both a Monte Carlo simulation engine and an <b>analytic</b> <b>engine</b> providing the joint probability generating function and therefore specifically the probability of cure. 1. INTRODUCTION In this paper we describe a cancer modeling facility, OncoTCAP, and how {{it can be used to}} develop better cancer treatment strategies. 1. 1. Rapidly developing knowledge in cancer biology and treatment The ongoing enormous explosion of research providing detailed information about the ge [...] ...|$|E
40|$|Pascal’s {{mechanical}} calculator and Babbage’s <b>Analytic</b> <b>Engine</b> set {{the stage}} for a great deal of modern computing machinery. Modern versions of these concepts were finally realized in von Neumann’s architecture in about 1950. However, the science of computing does not depend on physical implementations of these ideas, but rather on virtual machines—that is, abstractions that perform computations with a well defined semantics. In 1937, well before the advent of the stored program computer, Turing gave a complete description of a computational device which could be programmed and executed and whose properties could be inspected at runtime. This Turing Machine, and the program for the Universal Turing Machine were the first formal descriptions of virtual machines. Wolfram generalized and simplified the idea of a program, depicting a class of virtual machines known as Cellular Automata. The Java Virtual Machine was invented as a high-level conceptual machine with a great deal of internal structure, homogeneity and metadata. The Turing Machine, Cellular Automata and Java are all optimized for different aspects of programming. This paper describes each of these virtual machines, their histories and th...|$|E
40|$|Internet of Things <b>analytics</b> <b>engines</b> {{are complex}} {{to use and}} often {{optimized}} for a single domain or limited to proprietary data. A prototype system shows that existing Web analytics technologies can successfully be repurposed for IoT applications including sensor monitoring and user engagement tracking...|$|R
50|$|Anvita Health is {{a health}} care {{analytics}} company serving doctors, health plans, pharmacy benefit managers, disease management companies, point-of-care IT systems, personal health record (PHR) providers, and other clinical providers. Anvita Health provides the <b>analytics</b> <b>engine</b> for Google Health. The company is headquartered in San Diego, California.|$|R
50|$|The PivotLink Business Intelligence Platform is {{designed}} for business users to quickly explore data to make decisions with limited IT involvement. The PivotLink solution uses a highly simplified data structure and in-memory <b>analytics</b> <b>engine</b> to power dynamic reports and collaborative dashboards in a highly secure manner.|$|R
40|$|This {{dissertation}} {{is focused}} on the topic of service innovation and explores economies of scale and strategic differentiation in services via an inductive field-based case study of the world’s largest casino gaming company, Harrah’s Entertainment. It includes comparisons to services firms in other industries such as distribution/logistics (UPS) and for-profit/online education (Apollo Group/University of Phoenix). The findings suggest that scale and differentiation (considered by many to be mutually exclusive in services) can be combined through the strategic use of information technology in a manner that increases customer switching costs, resulting in improved profitability and returns. The limitations of standardization-only scale-oriented strategies are discussed, and the dissertation concludes with a description of the three key components needed by any firm seeking to employ a strategy of scalable service differentiation: (1) a loyalty program, or other means of linking specific transaction data with specific customers, (2) an <b>analytic</b> <b>engine</b> that determines the ranking/prioritization of customers and the criteria upon which to differentiate services, and (3) a set of information technology tools that automate consistent differentiated service delivery across a company’s touch-points with its customers...|$|E
30|$|To {{demonstrate}} {{the real world}} applicability of the proposed Big data analysis architecture, a basic prototype application (mainly <b>analytic</b> <b>engine)</b> using MapReduce [11] shown in Figure 4 was developed using Hadoop and Spark. The survey questionnaires used by the Bristol City Council give an indication of what the citizens think about the various indicators. However, there is no implicit quantifiable measure of the various indicators. Such a quantifiable measure can be helpful for decision makers when analysing the Quality of Life in Bristol. For example, planners can use the quantitative measures for the indicators spread over years to assess positive or negative trends or effects of certain policies. Moreover, they can also find statistical correlations between the various indicators to determine whether, and to what extent, one indicator affects another. However, since {{there are a number}} of questions for each indicator, and each indicator is measured separately for each geographical unit, the size of data is significant. For example, the 2007 questionnaire for assessing the public perception of Crime and Safety consists of approximately 20 questions. Moreover, each question was asked for approximately 40 wards of Bristol which makes 800 questions in total. Since there are responses for 8 years in total for 2 indicators, the total number of questions comes to 12, 800 (800 x 2 x 8).|$|E
40|$|Although {{medicine}} {{possesses the}} knowledge and technology for preventing or relieving most pain, poor pain control is still widespread. Unrelieved pain causes unnecessary suffering and increases health care expenditures. Among the barriers to improving pain control are poor provider education in pain management, misguided beliefs about the inevitability of pain and the dangers of pain medication, provider resistance to changing practice patterns, and administrative resistance to implementing improvements that incur short-term costs but lead to long-term savings. In short, poor pain relief in America 2 ̆ 7 s health care institutions is a system issue, and improvement requires a system-wide change. An effective program for improving pain management requires a multidisciplinary team committed to the task, ideally a triad consisting of a physician, a nurse, and a pharmacist. The triad needs administrative support in order to undertake needs assessment, offer provider and patient education, and perform continuous cycles of assessment, intervention, and reassessment of pain management. A strong information management base and an <b>analytic</b> <b>engine</b> are essential so that the team can evaluate outcomes from multiple perspectives (provider, payer, patient). The triad should identify a service area with clear pain problems, demonstrate improvements in this area, and then systematically move to other service areas. Educating providers and patients about pain and its control is essential for bringing about change. Improved pain management is a win-win situation for patients and institutions alike. Patients and families benefit from reduced suffering and improved quality of life, while institutions can offer more cost-effective care to patients...|$|E
50|$|LiveChat Software {{owns the}} {{technology}} it uses in the product, however in areas unrelated directly to chat, the company relies on 3rd party services. Technology partners include: Elastic for search and <b>analytics</b> <b>engine,</b> Postmark for delivery of transactional emails, Recurly for subscription billing and Pingdom for performance monitoring and uptime tracking.|$|R
50|$|In 2011, {{the company}} {{expanded}} its existing ethics and compliance training offerings with interactive web-based training offerings, which cover {{topics such as}} codes of conduct, conflicts of interest, anti-corruption, discrimination and harassment, insider training, information security and ethics reporting. That same year, the company introduced an enhanced version of its platform-level Reporting & <b>Analytics</b> <b>engine.</b>|$|R
50|$|Cloudant's service {{provides}} {{integrated data}} management, search, and <b>analytics</b> <b>engine</b> designed for web applications. Cloudant scales databases on the CouchDB framework and provides hosting, administrative tools, analytics and commercial support for CouchDB and BigCouch. Cloudant's distributed CouchDB service is {{used the same}} way as standalone CouchDB, with the added advantage of data being redundantly distributed over multiple machines.|$|R
40|$|As vehicle {{maneuver}} data becomes abundant for assisted or autonomous driving, their {{implication of}} privacy invasion/leakage {{has become an}} increasing concern. In particular, the surface for fingerprinting a driver will expand significantly if the driver's identity can be linked with the data collected from his mobile or wearable devices which are widely deployed worldwide and have increasing sensing capabilities. In line with this trend, this paper investigates a fast emerging driving data source that has driver's privacy implications. We first show that such privacy threats can be materialized via any mobile device with IMUs (e. g., gyroscope and accelerometer). We then present Dri-Fi (Driver Fingerprint), a driving data <b>analytic</b> <b>engine</b> that can fingerprint the driver with vehicle turn(s). Dri-Fi achieves this based on IMUs data taken only during the vehicle's turn(s). Such an approach expands the attack surface significantly compared to existing driver fingerprinting schemes. From this data, Dri-Fi extracts three new features [...] - acceleration along the end-of-turn axis, its deviation, and the deviation of the yaw rate [...] - and exploits them to identify the driver. Our extensive evaluation shows that an adversary equipped with Dri-Fi can correctly fingerprint the driver within just one turn with 74. 1 %, 83. 5 %, and 90. 8 % accuracy across 12, 8, and 5 drivers [...] - typical of an immediate family or close-friends circle [...] - respectively. Moreover, with measurements {{on more than one}} turn, the adversary can achieve up to 95. 3 %, 95. 4 %, and 96. 6 % accuracy across 12, 8, and 5 drivers, respectively. Comment: 15 pages, 15 figure...|$|E
40|$|Non-residential sectors offer many {{promising}} {{applications for}} electrical storage (batteries) and photovoltaics (PVs). However, choosing and operating storage under complex tariff structures poses a daunting technical and economic problem that may discourage potential customers {{and result in}} lost carbon and economic savings. Equipment vendors are unlikely to provide adequate environmental analysis or unbiased economic results to potential clients, and are even less likely to completely describe the robustness of choices {{in the face of}} changing fuel prices and tariffs. Given these considerations, researchers at Lawrence Berkeley National Laboratory (LBNL) have designed the Storage Viability and Optimization Web Service (SVOW) : a tool that helps building owners, operators and managers to decide if storage technologies and PVs merit deeper analysis. SVOW is an open access, web-based energy storage and PV analysis calculator, accessible by secure remote login. Upon first login, the user sees an overview of the parameters: load profile, tariff, technologies, and solar radiation location. Each parameter has a pull-down list of possible predefined inputs and users may upload their own as necessary. Since the non-residential sectors encompass a broad range of facilities with fundamentally different characteristics, the tool starts by asking the users to select a load profile from a limited cohort group of example facilities. The example facilities are categorized according to their North American Industry Classification System (NAICS) code. After the load profile selection, users select a predefined tariff or use the widget to create their own. The technologies and solar radiation menus operate in a similar fashion. After these four parameters have been inputted, the users have to select an optimization setting as well as an optimization objective. The <b>analytic</b> <b>engine</b> of SVOW is LBNL?s Distributed Energy Resources Customer Adoption Model (DER-CAM), which is a mixed-integer linear program (MILP) written and executed in the General Algebraic Modeling System (GAMS) optimization software. LBNL has released version 1. 2. 0. 11 of SVOW. Information can be found at [URL]...|$|E
40|$|Engineering {{feedstock}} {{supply systems}} that deliver affordable, high-quality biomass remains {{a challenge for}} the emerging bioenergy industry. Cellulosic biomass is geographically distributed and has diverse physical and chemical properties. Because of this feedstock supply systems that deliver cellulosic biomass resources to biorefineries require integration of a broad set of engineered unit operations. These unit operations include harvest and collection, storage, preprocessing, and transportation processes. Design decisions for each feedstock supply system unit operation impact the engineering design and performance of the other system elements. These interdependencies are further complicated by spatial and temporal variances such as climate conditions and biomass characteristics. This paper develops an integrated model that couples a SQL-based data management engine and systems dynamics models to design and evaluate biomass feedstock supply systems. The integrated model, called the Biomass Logistics Model (BLM), includes a suite of databases that provide 1) engineering performance data for hundreds of equipment systems, 2) spatially explicit labor cost datasets, and 3) local tax and regulation data. The BLM <b>analytic</b> <b>engine</b> is built in the systems dynamics software package PowersimTM. The BLM is designed to work with thermochemical and biochemical based biofuel conversion platforms and accommodates a range of cellulosic biomass types (i. e., herbaceous residues, short- rotation woody and herbaceous energy crops, woody residues, algae, etc.). The BLM simulates the flow of biomass through the entire supply chain, tracking changes in feedstock characteristics (i. e., moisture content, dry matter, ash content, and dry bulk density) as influenced by the various operations in the supply chain. By accounting {{for all of the}} equipment that comes into contact with biomass from the point of harvest to the throat of the conversion facility and the change in characteristics, the BLM evaluates economic performance of the engineered system, as well as determining energy consumption and green house gas performance of the design. This paper presents a BLM case study delivering corn stover to produce cellulosic ethanol. The case study utilizes the BLM to model the performance of several feedstock supply system designs. The case study also explores the impact of temporal variations in climate conditions to test the sensitivity of the engineering designs. Results from the case study show that under certain conditions corn stover can be delivered to the cellulosic ethanol biorefinery for $ 35 /dry ton...|$|E
50|$|It {{provides}} a developer portal for application developers and to view published APIs. An administration portal {{allows users to}} establish policies for APIs such as self-registration, quotas, key management and security policies. An <b>analytics</b> <b>engine</b> provides role-based <b>analytics</b> for API owners, solution administrators and application developers in order to manage APIs and ensure service levels are being achieved.|$|R
50|$|Openbravo Analytics is an {{embedded}} <b>analytics</b> <b>engine</b> {{that provides}} business intelligence capabilities to Openbravo solutions. It provides the infrastructure necessary to create analytical cubes (OLAP) and reports. It combines {{the power of}} high performing open source business intelligence components from Pentaho (use of Mondrian) and Saiku allied with the Openbravo user interface to provide a fully integrated analytics solution.|$|R
5000|$|Predixion RIOT Nano is a complete, self-contained, small {{foot-print}} real-time visual <b>analytics</b> <b>engine</b> {{which runs}} {{on a single}} device and provides immediate visual analytics and insights from device sensors. Predixion RIOT Nano provides on-device visual analytics or stream analytics to the gateway or cloud so a user can easily have visual insights from all connected devices.|$|R
