819|212|Public
5|$|Another {{approach}} to rate-enhancement is Dasher, which uses language models and <b>arithmetic</b> <b>coding</b> to present alternative letter targets {{on the screen}} with size relative to their likelihood given the history.|$|E
25|$|The JPEG {{standard}} also allows, {{but does}} not require, decoders to {{support the use of}} <b>arithmetic</b> <b>coding,</b> which is mathematically superior to Huffman coding. However, this feature has rarely been used, as it was historically covered by patents requiring royalty-bearing licenses, and because it is slower to encode and decode compared to Huffman coding. <b>Arithmetic</b> <b>coding</b> typically makes files about 5–7% smaller.|$|E
25|$|Entropy {{effectively}} bounds {{the performance}} of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or <b>arithmetic</b> <b>coding.</b> See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.|$|E
5000|$|AN {{codes are}} error-correcting code {{that are used}} in <b>arithmetic</b> applications. <b>Arithmetic</b> <b>codes</b> were {{commonly}} used in computer processors to ensure the accuracy of its arithmetic operations when electronics were more unreliable. <b>Arithmetic</b> <b>codes</b> help the processor to detect when an error is made and correct it. Without these codes, processors would be unreliable since any errors would go undetected. AN <b>codes</b> are <b>arithmetic</b> <b>codes</b> that are named for the integers [...] and [...] that are used to encode and decode the codewords.|$|R
40|$|Published report {{discusses}} {{fault location}} properties of <b>arithmetic</b> <b>codes.</b> Criterion for effectiveness of given code is detection probability of local fault by application of checking algorithm to results of entire set of algorithms of processor. Report also presents analysis of <b>arithmetic</b> <b>codes</b> with low-cost check algorithm which possesses partial fault-location properties...|$|R
5000|$|This {{is one of}} {{the primary}} metrics used when {{analyzing}} <b>arithmetic</b> <b>codes.</b>|$|R
25|$|Markov {{chains are}} used {{throughout}} information processing. Claude Shannon's famous 1948 paper A Mathematical Theory of Communication, {{which in a}} single step created the field of information theory, opens by introducing the concept of entropy through Markov modeling of the English language. Such idealized models can capture many of the statistical regularities of systems. Even without describing the full structure of the system perfectly, such signal models can make possible very effective data compression through entropy encoding techniques such as <b>arithmetic</b> <b>coding.</b> They also allow effective state estimation and pattern recognition. Markov chains also {{play an important role}} in reinforcement learning.|$|E
25|$|Some {{standard}} {{but rarely}} used options already exist in JPEG {{to improve the}} efficiency of coding DCT coefficients: the <b>arithmetic</b> <b>coding</b> option, and the progressive coding option (which produces lower bitrates because values for each coefficient are coded independently, and each coefficient has a significantly different distribution). Modern methods have improved on these techniques by reordering coefficients to group coefficients of larger magnitude together; using adjacent coefficients and blocks to predict new coefficient values; dividing blocks or coefficients up among {{a small number of}} independently coded models based on their statistics and adjacent values; and most recently, by decoding blocks, predicting subsequent blocks in the spatial domain, and then encoding these to generate predictions for DCT coefficients.|$|E
50|$|LHarc {{compresses}} files {{using an}} algorithm from Yoshizaki's earlier LZHUF product, which was modified from LZARI developed by Haruhiko Okumura (Okumura Haruhiko), but uses Huffman coding instead of <b>arithmetic</b> <b>coding.</b> LZARI uses Lempel-Ziv-Storer-Szymanski with <b>arithmetic</b> <b>coding.</b>|$|E
40|$|In recent {{publications}} about data compression, <b>arithmetic</b> <b>codes</b> {{are often}} suggested {{as the state}} of the art, rather than the more popular Huffman codes. While it is true that Huffman codes are not optimal in all situations, we show that the advantage of <b>arithmetic</b> <b>codes</b> in compression performance is often negligible. Referring also to other criteria, we conclude that for many applications, Huffman codes should still remain a competitive choice...|$|R
3000|$|..., are generated, {{we apply}} <b>arithmetic</b> <b>code</b> to encode each symbol to the {{specific}} codeword using the corresponding codebook, Q [...]...|$|R
40|$|The {{problem of}} finding the {{covering}} radius and minimum distance of algebraic and <b>arithmetic</b> <b>codes</b> is shown {{to be related to}} Waring's problem in a finite field and to the theory of cyclotomic numbers. The methods developed lead to new results for the covering radius of certain t-error-correcting BCH codes. Further, new results are given for the covering radius and the minimum distance of some classes of <b>arithmetic</b> <b>codes</b> generated by prime numbers...|$|R
50|$|Because <b>arithmetic</b> <b>coding</b> doesn't {{compress}} one datum at a time, {{it can get}} arbitrarily {{close to}} entropy. By contrast, the older Huffman coding does not reach entropy unless all probabilities are powers of two, in which case both Huffman and <b>arithmetic</b> <b>coding</b> achieve entropy.|$|E
5000|$|At {{least one}} {{significant}} compression software program, bzip2, deliberately discontinued {{the use of}} <b>arithmetic</b> <b>coding</b> in favor of Huffman coding due to the perceived patent situation at the time. Also, encoders and decoders of the JPEG file format, which has options for both Huffman encoding and <b>arithmetic</b> <b>coding,</b> typically only support the Huffman encoding option, which was originally because of patent concerns; {{the result is that}} nearly all JPEG images in use today use Huffman encoding although JPEG's <b>arithmetic</b> <b>coding</b> patents have expired due to the age of the JPEG standard (the design of which was approximately completed by 1990). There are some archivers like PackJPG, that can losslessly convert Huffman encoded files to ones with <b>arithmetic</b> <b>coding</b> (with custom file name [...]pjg), showing up to 25% size saving.|$|E
5000|$|... #Subtitle level 2: p-adic {{interpretation}} of <b>arithmetic</b> <b>coding</b> algorithm ...|$|E
40|$|In this paper, {{we address}} Joint Source-Channel (JSC) {{decoding}} with low decoding complexity over wireless channel. We propose a unity rate accumulator based design for soft-input soft-out decoding for low complexity Chase-like decoding of <b>arithmetic</b> <b>codes.</b> Chase-like decoding {{is a low}} complexity algorithm, where a maximum a posteriori sequence estimation criterion is employed for maximum likelihood decoding of variable length <b>codes</b> like <b>arithmetic</b> <b>codes.</b> Previous contributions propose iterative decoding SISO <b>arithmetic</b> <b>codes</b> with convolutional codes and LDPC codes. We propose application of unity rate accumulator as inner encoder and decoder in the system, which improves the bit error performance of the system by 1. 25 dB with same number of decoding iterations. We evaluate {{the performance of the}} proposed scheme for image transmission application. General Terms Wireless communication, image transmission, source-channel coding...|$|R
3000|$|... are generated, {{we apply}} <b>arithmetic</b> <b>code</b> to encode each symbol to the {{specific}} codeword using the corresponding codebook at each stage. Cardinalities of different codebooks Q [...]...|$|R
30|$|<b>Arithmetic</b> <b>codes</b> can {{be viewed}} as tree codes. Sequential {{decoding}} is a general decoding algorithm for tree codes. It was introduced by Wozencraft and Reiffen to decode convolutional codes in [15]. Fano [16] presented an improved sequential algorithm in 1963, which is now known as the Fano algorithm. Pettijohn et al. [17, 18] proposed two sequential decoding algorithms, depth first and breadth first, for decoding <b>arithmetic</b> <b>codes</b> in the presence of channel errors. We can use these decoding algorithms with the same key for decoding the output of our proposed scheme.|$|R
50|$|Some US patents {{relating}} to <b>arithmetic</b> <b>coding</b> are listed below.|$|E
5000|$|FLIF (Free Lossless Image Format) - a {{work-in-progress}} lossless {{image format}} which claims to outperform PNG, lossless WebP, lossless BPG and lossless JPEG2000 {{in terms of}} compression ratio. It uses the MANIAC (Meta-Adaptive Near-zero Integer <b>Arithmetic</b> <b>Coding)</b> entropy encoding algorithm, {{a variant of the}} CABAC (context-adaptive binary <b>arithmetic</b> <b>coding)</b> entropy encoding algogithm.|$|E
5000|$|... #Subtitle level 2: <b>Arithmetic</b> <b>coding</b> as a {{generalized}} change of radix ...|$|E
40|$|The {{need for}} {{fault-tolerant}} computing is addressed from the viewpoints of (1) {{why it is}} needed, (2) how to apply it in {{the current state of}} technology, and (3) what it means {{in the context of the}} Phoenix computer system and other related systems. To this end, the value of concurrent error detection and correction is described. User protection, program retry, and repair are among the factors considered. The technology of algebraic codes to protect memory systems and <b>arithmetic</b> <b>codes</b> to protect memory systems and <b>arithmetic</b> <b>codes</b> to protect <b>arithmetic</b> operations is discussed...|$|R
40|$|In this paper, we {{consider}} the use of source codes and channel codes for asymmetric distributed source coding of non uniform correlated sources. In particular, we use distributed <b>arithmetic</b> <b>codes</b> as source codes and syndrome based turbo codes as channel codes. We compare the advantages and drawbacks of the two systems for different source probabilities and different compression ratio. We show that prior knowledge of the source distribution improves the performance of both approaches {{in terms of their}} distances to the Slepian-Wolf bound. Turbo codes are better when the puncturing is low, while distributed <b>arithmetic</b> <b>codes</b> are less impacted by the change of compression rate. 1...|$|R
40|$|In this paper, a novel maximum a posteriori (MAP) {{technique}} for the decoding of <b>arithmetic</b> <b>codes</b> {{in the presence}} of transmission errors is presented. <b>Arithmetic</b> <b>codes</b> with a forbidden symbol and trellis search techniques are employed in order to estimate the best transmitted codeword. The viability of the proposed approach is demonstrated in the binary symmetric channel case in terms of both performance and decoding complexity. The results are compared with a traditional separated approach based on rate compatible convolutional codes. The MAP decoding is applied to the progressive transmission of SPIHT compressed images and competitive results in terms of average decoded quality are reported...|$|R
5000|$|... {{conversion}} between Huffman and <b>arithmetic</b> <b>coding</b> in the {{entropy coding}} layer.|$|E
5000|$|Bitstream {{processing}} (Context-adaptive variable-length coding/Context-adaptive binary <b>arithmetic</b> <b>coding)</b> {{and perfect}} pixel positioning.|$|E
5000|$|Huffman coding a two-symbol {{alphabet}} (such as {{just the}} bits 0 and 1) cannot compress at all, whereas <b>arithmetic</b> <b>coding</b> compresses bits well. If such an alphabet ({0, 1}) has probabilities {0.95, 0.05}, Huffman encoding assigns 1 bit to each value, leaving input (and thus its length) unchanged. <b>Arithmetic</b> <b>coding,</b> by contrast, approaches the optimal compression ratio of ...|$|E
40|$|Abstract. In {{this article}} we present three <b>arithmetic</b> methods for <b>coding</b> {{oriented}} geodesics on the modular surface using various continued fraction expansions and show that the space of admissible coding sequences for each coding is a one-step topological Markov chain with countable alphabet. We also present conditions under which these <b>arithmetic</b> <b>codes</b> coincide with the geometric code obtained by recording oriented excursions into the cusp of the modular surface...|$|R
40|$|This paper clarifies two variable-to-fixed length codes which achieve optimum large {{deviations}} {{performance of}} empirical compression ratio. One is Lempel-Ziv code with fixed number of phrases, {{and the other}} is an <b>arithmetic</b> <b>code</b> with fixed codeword length. It is shown that Lempel-Ziv code is asymptotically optimum in the above sense, for the class of finite-alphabet and finite-state sources, and that the <b>arithmetic</b> <b>code</b> is asymptotically optimum for the class of finite-alphabet unifilar sources. key words: source coding, variable-to-fixed length code, empirical compression rate, finite state source 1. Introduction Comparisons between lossless variable-to-fixed (V-F) length codes and fixed-to-variable (F-V) length codes have been done by many researchers. Especially, Ziv [1] has shown that for Markovian sources with long memory there exists a V-F length code that provides a better compression ratio than any F-V length code with the same number of codewords. Tja...|$|R
40|$|We {{study the}} effects of finite-precision {{representation}} of source's probabilities on the efficiency of classic source coding algorithms, such as Shannon, Gilbert-Moore, or <b>arithmetic</b> <b>codes.</b> In particular, we establish the following simple connection between the redundancy R and the number of bits W necessary for representation of source's probabilities in computer's memory (R is assumed to be small) :...|$|R
50|$|The JPEG {{standard}} also allows, {{but does}} not require, decoders to {{support the use of}} <b>arithmetic</b> <b>coding,</b> which is mathematically superior to Huffman coding. However, this feature has rarely been used, as it was historically covered by patents requiring royalty-bearing licenses, and because it is slower to encode and decode compared to Huffman coding. <b>Arithmetic</b> <b>coding</b> typically makes files about 5-7% smaller.|$|E
50|$|The primary {{encoding}} {{algorithms used}} to produce bit sequences are Huffman coding (also used by DEFLATE) and <b>arithmetic</b> <b>coding.</b> <b>Arithmetic</b> <b>coding</b> achieves compression rates close to the best possible for a particular statistical model, which is given by the information entropy, whereas Huffman compression is simpler and faster but produces poor results for models that deal with symbol probabilities close to 1.|$|E
50|$|Some {{examples}} of well-known variable-length coding strategies are Huffman coding, Lempel-Ziv coding and <b>arithmetic</b> <b>coding.</b>|$|E
40|$|I is 'hown that a non-separate <b>arithmetic</b> <b>code</b> that {{preserves}} bI th addition and multiplication {{must be an}} AN code {{where the}} nerator A is an idempotent element of the ring being used An idempotent element is one that satisfies the equation x i x. Given this type of code, its ability to detect errors in arithmetic expressions is explored and shown to be poor, due to error masking in multipliers. The constraints placed on a non-separate multiplication-preserving <b>arithmetic</b> <b>code</b> that avoids such problems are discussed. The simplest code satisfying these conditions {{turns out to be}} an AN+B code where both A and B are idempotent elements. Conditions for the existence of this type of code are given along with a list of examples. The fault tolerance provided by these codes is then considered for a specific example [...] L 7 Aooess on For DTIC TA...|$|R
40|$|Abstract-Arithmetic {{error codes}} {{constitute}} {{a class of}} error codes that are preserved during most arithmetic operations. Effectiveness studies for <b>arithmetic</b> error <b>codes</b> have shown their value for concurrent detection of faults in arithmetic processors, data transmission subsystems, and main storage units in fault-tolerant computers In this paper, it is shown that the same class of codes is also quite effective for detecting storage errors in both shift-register and magnetic-recording mass memories. Some of the results are more general and deal with properties of <b>arithmetic</b> error <b>codes</b> in detecting unidirectional failures. For example, it is shown that a low-cost <b>arithmetic</b> error <b>code</b> with check modulus A = 2 - 1 can detect any unidirectional failure which affects fewer than N bits. The use of <b>arithmetic</b> error <b>codes</b> for checking of mass memories is further justified since it {{eliminates the need for}} hard-core or self-checking code translators and reduces the number of different types of cod...|$|R
40|$|In {{this paper}} we {{consider}} transmission of image over hybrid networks, i. e., in part consisting of packet switched backbone networks and last hop delivery consisting of wireless links. Multiple descriptions of wavelet transformed image are generated and coded using SPIHT compression algorithm for achieving robust transmission over packet-loss channel. We consider two different schemes {{in which we}} apply softinput soft-output (SISO) Chase-type arithmetic decoder for noise robust reception of the multiple descriptions over additive white Gaussian channel (AWGN) in contrast to earlier works which consider reception over binary symmetric channels. We utilize ‘built-in’ arithmetic coder of SPIHT algorithm for achieving noise robust transmission of multiple descriptions without increasing encoder complexity and utilize low complexity SISO decoding of <b>arithmetic</b> <b>codes.</b> We evaluate the performance of these schemes with proposed improvements and compare their performance with existing works. An improvement of 9 dB in PSNR is obtained by employing SISO decoding of <b>arithmetic</b> <b>code...</b>|$|R
