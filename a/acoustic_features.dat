1834|1186|Public
25|$|Stress is {{a complex}} of various <b>acoustic</b> <b>features,</b> {{particularly}} loudness (strength and intensity). Other features such as duration, spectrum and pitch, are of lesser importance in normal speech.|$|E
25|$|An accent {{represents}} {{a complex of}} <b>acoustic</b> <b>features</b> such as, sound quality (timbre), quantity, strength (intensity), fundamental frequency (pitch), and degree of pitch separation. These features may be varied {{in order to produce}} dialect and in order to differentiate between two accents when they are present.|$|E
25|$|South American {{sea lions}} are {{observed}} to make various vocalizations and calls which differ between sexes and ages. Adult males make high-pitched calls during aggressive interactions, barks when establishing territories, growls when interacting with females, and exhalations after antagonistic encounters. Females with pups make a mother primary call when interacting with their pups, and grunts during aggressive encounters with other females. Pups make pup primary calls. Some of those vocalizations and <b>acoustic</b> <b>features</b> may support individuality.|$|E
40|$|The {{quality of}} unit {{selection}} speech synthesisers depends significantly {{on the content}} of the speech database being used. In this paper a technique is introduced that can highlight mispronunciations and abnormal units in the speech synthesis voice database through the use of articulatory <b>acoustic</b> <b>feature</b> extraction to obtain an additional layer of annotation. A set of articulatory <b>acoustic</b> <b>feature</b> classifiers help minimise the selection of inappropriate units in the speech database and are shown to significantly improve the word error rate of a diphone synthesiser. Index Terms: speech synthesis, unit selection, articulatory <b>acoustic</b> <b>feature</b> extractio...|$|R
5000|$|... "Nothing to Lose" [...] is {{a country}} rock ballad, backed mostly by {{acoustic}} guitar. The narrator describes a slow-churning declaration of codependence. [...] "Nothing To Lose" [...] features five recorded versions of the song; solo, solo <b>acoustic,</b> <b>featuring</b> Miley Cyrus, <b>acoustic</b> <b>featuring</b> Miley Cyrus and a country version also featuring the multi-platinum artist.|$|R
40|$|I {{would like}} to express my {{greatest}} gratitude {{to the people who}} have helped and supported me. First, I {{would like to}} thank Professor Wendi Heinzelman for her continuous guidance and invaluable advice on my thesis. Many thanks to my parents for their undivided support and encouragement. I {{would also like to thank}} Na Yang and He Ba from the Wireless Communication and Networking Group for providing advice on my thesis. This research was part of the Bridge project, which was supported by funding from National Institute of Health NICHD (Grant R 01 HD 060789). iv <b>Acoustic</b> <b>feature</b> extraction algorithms play a central role in many speech and music processing applications. However, noise usually prevents <b>acoustic</b> <b>feature</b> extraction algorithms from obtaining the correct information from speech and music signals. Thus, the robustness of <b>acoustic</b> <b>feature</b> extraction algorithms is an area worth studying. In this thesis, we consider two important acoustic features: pitch and speaking rate. For each <b>acoustic</b> <b>feature,</b> we introduce several classic and state-of-the-art feature extraction algorithms and evaluate the performance of each of them in noisy environments. We analyze the results and provide possible explanations why some feature extraction algorithms outperform the others in noisy environments. ...|$|R
25|$|If {{we accept}} this theory, the {{emergence}} of speech becomes theoretically impossible. Communication of this kind just cannot evolve. The problem is that words are cheap. Nothing about their <b>acoustic</b> <b>features</b> can reassure listeners that they are genuine and not fakes. Any strategy of reliance on someone else's tongue— perhaps the most flexible organ in the body— presupposes unprecedented levels of honesty and trust. To date, Darwinian thinkers have {{found it difficult to}} explain the requisite levels of community-wide cooperation and trust.|$|E
500|$|In the table, when obstruents (stops, affricates, and fricatives) {{appear in}} pairs, such as , , and , {{the first is}} fortis (strong) {{and the second is}} lenis (weak). Fortis obstruents, such as [...] are {{pronounced}} with more muscular tension and breath force than lenis consonants, such as , and are always voiceless. Lenis consonants are partly voiced {{at the beginning and end}} of utterances, and fully voiced between vowels. Fortis stops such as [...] have additional articulatory or <b>acoustic</b> <b>features</b> in most dialects: they are aspirated [...] when they occur alone at the beginning of a stressed syllable, often unaspirated in other cases, and often unreleased [...] or pre-glottalised [...] at the end of a syllable. In a single-syllable word, a vowel before a fortis stop is shortened: thus nip has a noticeably shorter vowel (phonetically, but not phonemically) than nib [...] (see below).|$|E
500|$|The {{mismatch}} negativity (MMN) is a rigorously documented ERP component {{frequently used}} in neurolinguistic experiments. [...] It is an electrophysiological response {{that occurs in}} the brain when a subject hears a [...] "deviant" [...] stimulus {{in a set of}} perceptually identical [...] "standards" [...] (as in the sequence s s s s s s s d d s s s s s s d s s s s s d). [...] Since the MMN is elicited only in response to a rare [...] "oddball" [...] stimulus in a set of other stimuli that are perceived to be the same, it has been used to test how speakers perceive sounds and organize stimuli categorically. For example, a landmark study by Colin Phillips and colleagues used the mismatch negativity as evidence that subjects, when presented with a series of speech sounds with acoustic parameters, perceived all the sounds as either /t/ or /d/ in spite of the acoustic variability, suggesting that the human brain has representations of abstract phonemesin other words, the subjects were [...] "hearing" [...] not the specific <b>acoustic</b> <b>features,</b> but only the abstract phonemes. [...] In addition, the mismatch negativity has been used to study syntactic processing and the recognition of word category.|$|E
50|$|Lynch is {{presently}} signed with ESP Guitars, which {{has resulted in}} ESP's creation of the Lynch Jumbo <b>acoustic,</b> <b>featuring</b> graphics designed by Stephen Jensen.|$|R
30|$|When used {{without the}} DAE-based front-end, the {{accuracy}} with the DNN-HMM (mc) was degraded significantly by using MFCC as the <b>acoustic</b> <b>feature</b> (from row 3 to row 1). More importantly, when using MFCC, the DAE front-end yielded little improvement (from row 1 to row 2), while the improvement by the DAE was {{significant in the}} LMFB-based system (from row 3 to row 4). From these results, we {{understand that it is}} essential to use the LMFB <b>feature</b> as the <b>acoustic</b> <b>feature</b> in the combined system.|$|R
5000|$|Nandi, D. & Balakrishnan, R. (2013). Call {{intensity}} is a repeatable and dominant <b>acoustic</b> <b>feature</b> determining male call attractiveness {{in a field}} cricket. Animal Behaviour 86, 1003-1012.|$|R
5000|$|<b>Acoustic</b> <b>features</b> : {{physical}} characteristics of speech sounds such as, loudness, amplitude, frequency etc.|$|E
5000|$|Lisker, L. [...] "Voicing" [...] in English: A {{catalogue}} of <b>acoustic</b> <b>features</b> signaling /b/ versus /p/ in trochees. Language and Speech, 1986, 29, 3-11.|$|E
50|$|Stress is {{a complex}} of various <b>acoustic</b> <b>features,</b> {{particularly}} loudness (strength and intensity). Other features such as duration, spectrum and pitch, are of lesser importance in normal speech.|$|E
40|$|We {{explore the}} use of sequential, mistake-driven updates for online {{learning}} and <b>acoustic</b> <b>feature</b> adaptation in large margin hidden Markov models (HMMs). The updates are applied to the parameters of acoustic models after the decoding of individual training utterances. For large margin training, the updates attempt to separate the log-likelihoods of correct and incorrect transcriptions by an amount proportional to their Hamming distance. For <b>acoustic</b> <b>feature</b> adaptation, the updates attempt to improve recognition by linearly transforming the features computed by the front end. We evaluate acoustic models trained in this way on the TIMIT speech database. We find that online updates for large margin training not only converge faster than analogous batch optimizations, but also yield lower phone error rates than approaches that do not attempt to enforce a large margin. Finally, experimenting with different schemes for initialization and parameter-tying, we find that <b>acoustic</b> <b>feature</b> adaptation leads to further improvements beyond the already significant gains achieved by large margin training...|$|R
50|$|Whereas {{their early}} albums were almost entirely <b>acoustic,</b> <b>featuring</b> oboes, spinets, {{harmoniums}} and Mellotrons, their 1980s albums tended to {{rely more on}} synthesizer and electric guitar, sounding much more electric.|$|R
40|$|Abstract—A new {{approach}} to represent temporal correlation in an automatic speech recognition system is described. It introduces an <b>acoustic</b> <b>feature</b> set that captures the dynamics of speech signal at the phoneme boundaries {{in combination with the}} traditional <b>acoustic</b> <b>feature</b> set representing the periods that are assumed to be quasi-stationary of speech. This newly introduced feature set represents an observed random vector associated with the state transition in HMM. For the same complexity and number of parameters, this approach improves the phoneme recognition accuracy by 3. 5 % compared to the context-independent HMM models. Stop consonant recognition accuracy is increased by 40 %. I...|$|R
50|$|The most {{remarkable}} {{achievements of the}} Qutb Shahi dynasty is Golkonda fort. It {{is one of the}} most impregnable fort in India. It is also famous for its <b>acoustic</b> <b>features</b> and water management.|$|E
5000|$|Lecture Hall: The {{lecture hall}} has an elegant {{structure}} with aesthetically designed and fully air-conditioned with the seating capacity of 90 {{people at a}} time. It has excellent <b>acoustic</b> <b>features,</b> LCD projector and audio system ...|$|E
50|$|The Everest theatre is an end-stage theatre, seating up to 605, {{depending}} on configuration. It {{was designed for}} musical performances and includes a variety of <b>acoustic</b> <b>features</b> to manipulate and control sound quality, but is also used for theatrical and dance performances.|$|E
3000|$|...] with an <b>acoustic</b> <b>feature</b> {{conveying}} the relative balance between {{two groups of}} partials of the harmonic part of the signal (see Appendix B for more details). The second dimension is correlated [[...]...|$|R
30|$|This paper {{investigates the}} {{important}} role that <b>acoustic</b> <b>feature</b> extraction plays in automatic chord recognition. The chord recognition system under study {{is based on a}} probabilistic approach to statistically model sequences of chroma features.|$|R
3000|$|Here, p(t) is {{the phone}} class feature for frame t. The {{weight of the}} first hidden layer {{consists}} of two different matrices; W^ 1 _a {{is applied to the}} standard <b>acoustic</b> <b>feature</b> x [...]...|$|R
50|$|Robots can {{perceive}} our emotion {{through the}} way we talk. Acoustic and linguistic features are generally used to characterize emotions. The combination of seven <b>acoustic</b> <b>features</b> and four linguistic features improves the recognition performance when compared to using only one set of features.|$|E
50|$|The left {{hemisphere}} is usually dominant in right-handed people, although bilateral activations {{are not uncommon}} {{in the area of}} syntactic processing. It is now accepted that the right hemisphere {{plays an important role in}} the processing of suprasegmental <b>acoustic</b> <b>features</b> like prosody.|$|E
50|$|An accent {{represents}} {{a complex of}} <b>acoustic</b> <b>features</b> such as, sound quality (timbre), quantity, strength (intensity), fundamental frequency (pitch), and degree of pitch separation. These features may be varied {{in order to produce}} dialect and in order to differentiate between two accents when they are present.|$|E
40|$|Abstract—We {{explore the}} use of sequential, mistake-driven updates for online {{learning}} and <b>acoustic</b> <b>feature</b> adaptation in large-margin hidden Markov models (HMMs). The updates are applied to the parameters of acoustic models after the decoding of individual training utterances. For large-margin training, the updates attempt to separate the log-likelihoods of correct and incorrect transcriptions by an amount proportional to their Hamming distance. For <b>acoustic</b> <b>feature</b> adaptation, the updates attempt to improve recognition by linearly transforming the features computed by the front end. We evaluate acoustic models trained in this way on the TIMIT speech database. We find that online updates for large-margin training not only converge faster than analogous batch optimizations, but also yield lower phone error rates than approaches that do not attempt to enforce a large margin. Finally, experimenting with different schemes for initialization and parameter-tying, we find that <b>acoustic</b> <b>feature</b> adaptation leads to further improvements beyond the already significant gains achieved by large-margin training. Index Terms—Acoustic feature adaptation, automatic speech recognition (ASR), discriminative training, hidden Markov models (HMMs), large-margin classification, online learning. I...|$|R
30|$|To {{confirm the}} {{importance}} of delta and acceleration parameters in DNN-based acoustic modeling for reverberant speech recognition, we evaluated the DNN-HMM system trained with only the static part of the <b>acoustic</b> <b>feature</b> of the multi-condition data.|$|R
40|$|International audienceWith {{the purpose}} of {{improving}} Spoken Language Un- derstanding (SLU) performance, a combination of different acoustic speech recognition (ASR) systems is proposed. State a posteriori probabilities obtained with systems using different <b>acoustic</b> <b>feature</b> sets are combined with log-linear inter- polation. In order to perform a coherent combination of these probabilities, acoustic models must have the same topology (i. e. same set of states). For this purpose, a fast and efficient twin model training protocol is proposed. By a wise choice of <b>acoustic</b> <b>feature</b> sets and log-linear interpolation of their like- lihood ratios, a substantial Concept Error Rate (CER) reduction has been observed on the test part of the French MEDIA corpus...|$|R
5000|$|Speaker {{recognition}} has {{a history}} dating back some four decades and uses the <b>acoustic</b> <b>features</b> of speech {{that have been found}} to differ between individuals. These acoustic patterns reflect both anatomy (e.g., size and shape of the throat and mouth) and learned behavioral patterns (e.g., voice pitch, speaking style). Speaker verification has earned speaker recognition its classification as a [...] "behavioral biometric".|$|E
50|$|The Lombard effect or Lombard reflex is the {{involuntary}} {{tendency of}} speakers {{to increase their}} vocal effort when speaking in loud noise to enhance the audibility of their voice. This change includes not only loudness but also other <b>acoustic</b> <b>features</b> such as pitch, rate, and duration of syllables. This compensation effect results {{in an increase in}} the auditory signal-to-noise ratio of the speaker’s spoken words.|$|E
50|$|The {{audio channel}} of the mobile phone is another {{wireless}} interface {{that is used}} to make payments. Several companies have created technology to use the <b>acoustic</b> <b>features</b> of cell phones to support mobile payments and other applications that are not chip-based. The technologies Near sound data transfer (NSDT), Data Over Voice and NFC 2.0 produce audio signatures that the microphone of the cell phone can pick up to enable electronic transactions.|$|E
3000|$|The mood {{associated}} with this numerical representation of words contained in the lyrics is finally learned by the classifier as for any <b>acoustic</b> <b>feature.</b> Note that the word order is neglected in this modelling. One could also consider compounds of words by [...]...|$|R
5000|$|The BE-140 <b>acoustic</b> <b>featured</b> a piezo {{pickup in}} {{the bridge and}} a 21 fret {{cut-away}} body style. They are even more rare than the electrics with only one known example found [...] "in the wild" [...] with others only being built to order.|$|R
40|$|With {{the purpose}} of {{improving}} Spoken Language Un-derstanding (SLU) performance, a combination of different acoustic speech recognition (ASR) systems is proposed. State a posteriori probabilities obtained with systems using differ-ent <b>acoustic</b> <b>feature</b> sets are combined with log-linear inter-polation. In order to perform a coherent combination of these probabilities, acoustic models must have the same topology (i. e. same set of states). For this purpose, a fast and efficient twin model training protocol is proposed. By a wise choice of <b>acoustic</b> <b>feature</b> sets and log-linear interpolation of their like-lihood ratios, a substantial Concept Error Rate (CER) reduc-tion has been observed on the test part of the French MEDIA corpus. Index Terms — speech recognition, posterior probabili-ties combination, speech understanding, frame based combi-nation 1...|$|R
