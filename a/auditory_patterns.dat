44|120|Public
5000|$|Schuell: Shuell views jargon as {{the result}} of severe {{impairment}} in the recall of learned <b>auditory</b> <b>patterns</b> and imperfect auditory feedback processes. The lack of control of verbal output is related to reduced auditory input.|$|E
50|$|Brain-reading {{uses the}} {{responses}} of multiple voxels in the brain evoked by stimulus then detected by fMRI in order to decode the original stimulus. Brain reading studies differ {{in the type of}} decoding (i.e. classification, identification and reconstruction) employed, the target (i.e. decoding visual patterns, <b>auditory</b> <b>patterns,</b> cognitive states), and the decoding algorithms (linear classification, nonlinear classification, direct reconstruction, Bayesian reconstruction, etc.) employed.|$|E
5000|$|Alcohol {{consumption}} has substantial, measurable {{effects on}} working memory, though these effects vary greatly between individual responses. Little {{is known about}} the neural mechanisms that underlie these individual differences. It is also found that alcohol impairs working memory by affecting mnemonic strategies and executive processes rather than by shrinking the basic holding capacity of working memory. Isolated acute-moderate levels of alcohol intoxication do not profoundly physically alter the structures which are critical for working memory function, such as the frontal cortex, parietal cortex, anterior cingulate, and parts of the basal ganglia. [...] One finding regarding the effects of alcohol on working memory points out that alcohol reduces working memory only in individuals with a high baseline working memory capacity, which suggests that alcohol might not uniformly affect working memory in different individuals. Alcohol appears to impair the capacity of working memory to modulate response inhibition. Alcohol disinhibits behaviour, but only in individuals with a low baseline working memory capacity. An interesting finding is that incentive to perform well with working memory measurement tasks while {{under the influence of alcohol}} does in fact have some effect on working memory, in that it boosts scores in rate of mental scanning and reaction time to stimulus, but did not reduce number of errors compared to subjects with no incentive to perform well. Even acute alcohol intoxication (a blood alcohol concentration of 0.08-0.09%) produces a substantial impairment of working memory processes that require mnemonic rehearsal strategies. Alcohol is less likely to impair a working memory task that does not rely on memory rehearsal or associated mnemonic strategies. Because of this, working memory is very susceptible to falter when a person is participating in tasks involving retention concerning auditory and visual sequences. A real world and interesting example of this is failure of guitarists or other musicians performing concerts to cue in on <b>auditory</b> <b>patterns</b> and make it known that their performance is hindered by intoxication, whereas professional basketball (a less sequence-heavy activity for working memory) standout Ron Artest recently admitted in an interview with Sporting News to drinking heavily during half-time in the early days of his career and it having little if any recognizable effect on his working memory. His former coach Fran Fraschilla has gone on record saying: [...] "Its a surprise because every day at practice, he came out in a mood to play. He came out in a basketball rage. He was fully committed; he wanted to let our upperclassmen know that he was the alpha male. It never came up that he had any sort of a problem with alcohol. This is the first Ive heard of it." ...|$|E
5000|$|... 1995 Henning Scheich, <b>Auditory</b> cortex: <b>pattern</b> {{analyzer}} and interpreter ...|$|R
40|$|To {{test the}} {{hypothesis}} that prior <b>patterned</b> or varied <b>auditory</b> experience was necessary {{for the development of}} auditory frequency discrimination and <b>auditory</b> <b>pattern</b> discrimination, groups of sprague-Dawley albino rats were deprived of patterned sound from birth by the novel technique of rearing them in 'white' noise. The sound deprived rats learned a frequency discrimination as easily as controls reared in varied sound conditions, but showed inferior performance on an <b>auditory</b> <b>pattern</b> discrimination task. Supporting experiments showed that the inferiority of varied sound deprived animals on the pattern discrimination task was not likely to have been due to their emotional state {{at the time of the}} testing nor to their inferiority in learning to respond in a discrimination task compared with non-deprived controls. Open-field testing showed that the sound deprived subjects did not differ from non-deprived controls in 'emotionality'. The sound deprived rats were not inferior, either, to controls on a complex visual discrimination task. Experiments were also carried out to explore the effect of various durations of patterned sound deprivation and the effect of the deprivation at various times in the life cycle of the rat on <b>auditory</b> <b>pattern</b> discrimination. The results of these experiments favoured an explanation for the effect of varied sound experience which proposed that <b>patterned</b> <b>auditory</b> discrimination development depended, simply, on prior experience with varied sound rather than an explanation which proposed that the effect depended on varied sound experience during a particular sensitive period in the life of the rat. The research involved a total of seven different experiments, the similarities in the findings of which when compared with those of other investigators working in the area of the effects of deprivation of patterned light on visual discriminations were noted. The present experiments support generalizations about the role of prior experience on later behaviour, based largely on experiments in the visual mode, by supplying evidence from another sensory mode...|$|R
2500|$|Auditory {{processing}} disorder can be developmental or acquired. It {{may result}} from ear infections, head injuries or neurodevelopmental delays that affect processing of auditory information. This can include problems with: [...] "...sound localization and lateralization (see also binaural fusion); <b>auditory</b> discrimination; <b>auditory</b> <b>pattern</b> recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals".|$|R
40|$|It {{is claimed}} here that {{experimental}} evidence about human speech processing and {{the richness of}} memory for linguistic material supports a distributed view of language where every speaker creates an idiosyncratic perspective on the linguistic conventions of the community. In such a system, words are not spelled in memory of speakers from uniform letter-like units (whether phones or phonemes), but rather from the rich <b>auditory</b> <b>patterns</b> of speech plus any coupled visual, somatosensory and motor patterns. The evidence is strong that people actually employ high-dimensional, spectro-temporal, <b>auditory</b> <b>patterns</b> to support speech production, speech perception and linguistic memory in real time. Abstract phonology (with its phonemes, distinctive features, syllable types, etc.) is actually a kind of social institution – a loose inventory of patterns that evolves over historical time in each human community as a structure with many symmetries and regularities in the community corpus. Linguistics studies the phonological (and grammatical) patterns of various communities of speakers. But linguists should not expect to find the descriptions they make to be explicitly represented in any individual speaker’s mind, much less in every mind in the community. The alphabet is actually a technology that has impose...|$|E
40|$|Improvement in {{perception}} {{takes place}} within the training session and from one session to the next. The present study aims at determining {{the time course of}} perceptual learning as revealed by changes in auditory event-related potentials (ERPs) reflecting preattentive processes. Subjects were trained to discriminate two complex <b>auditory</b> <b>patterns</b> in a single session. ERPs were recorded just before and after training, while subjects read a book and ignored stimulation. ERPs showed a negative wave called mismatch negativity (MMN) —which indexes automatic detection of a change in a homogeneous auditory sequence—just after subjects learned to consciously discriminate the two patterns. ERPs were recorded again 12, 24, 36, and 48 h later, just before testing performance on the discrimination task. Additional behavioral and neurophysiological changes were found several hours after the training session: an enhanced P 2 at 24 h followed by shorter reaction times, and an enhanced MMN at 36 h. These results indicate that gains in performance on the discrimination of two complex <b>auditory</b> <b>patterns</b> are accompanied by different learning-dependent neurophysiological events evolving within different time frames, supporting the hypothesis that fast and slow neural changes underlie the acquisition of improved perception. Perceptual learning is defined as gains in performance on perceptual tasks as a result of experience-dependen...|$|E
40|$|Transient {{synchronization}} {{has been}} used as a mechanism of recognizing <b>auditory</b> <b>patterns</b> using integrate-and-fire neural networks. We first extend the mechanism to vision tasks and investigate the role of spike dependent learning. We show that such a temporal Hebbian learning rule significantly improves accuracy of detection. We demonstrate how multiple patterns can be identified by a single pattern selective neuron and how a temporal album can be constructed. This principle may lead to multidimensional memories, where the capacity per neuron is considerably increased with accurate detection of spike synchronization...|$|E
30|$|This paper {{presents}} a new method for {{the selection of}} sinusoidal components for use in compact representations of narrowband audio. The method consists of ranking and selecting the most perceptually relevant sinusoids. The idea behind the method is to maximize the matching between the <b>auditory</b> excitation <b>pattern</b> associated with the original signal and the corresponding <b>auditory</b> excitation <b>pattern</b> associated with the modeled signal that is being represented by a small set of sinusoidal parameters. The proposed component-selection methodology is shown to outperform the maximum signal-to-mask ratio selection strategy in terms of subjective quality.|$|R
40|$|This paper {{provides}} {{an introduction to}} auditory interfaces and suggests some future work will make their development more straightforward. A review of the basic concepts of psychoacoustics is provided as without the knowledge of how humans perceive sounds, interfaces which use them effectively cannot be designed. Pitch and loudness perception, localization and <b>auditory</b> <b>pattern</b> recognition are described. Some psychological reasons for why mixed graphics and sound interfaces can be beneficial are presented and arguments are given for use of non-speech sounds. Four auditory interfaces are examined in detail. The interfac...|$|R
40|$|Temporal pattern {{recognition}} based on instantaneous spike rate coding {{in a simple}} auditory system. J Neurophysiol 90 : 2484 – 2493, 2003; 10. 1152 /jn. 00259. 2003. <b>Auditory</b> <b>pattern</b> recognition by the CNS is a fundamental process in acoustic communication. Because crickets communicate with stereotyped patterns of constant frequency syllables, they are established models to investigate the neuronal mechanisms of <b>auditory</b> <b>pattern</b> recognition. Here we provide evidence that for the neural processing of amplitude-modulated sounds, the instantaneous spike rate rather than the time-averaged neural activity is the appropriate coding principle by comparing both coding parameters in a thoracic interneuron (Omega neuron ON 1) of the cricket (Gryllus bimaculatus) auditory system. When stimulated with different temporal sound patterns, {{the analysis of the}} instantaneous spike rate demonstrates that the neuron acts as a low-pass filter for syllable patterns. The instantaneous spike rate is low at high syllable rates, but prominent peaks in the instantaneous spike rate are generated as the syllable rate resembles that of the species-specific pattern. The occurrence and repetition rate of these peaks in the neuronal discharge are sufficient to explain temporal filtering in the cricket auditory pathway as they closely match the tuning of phonotactic behavior to different sound patterns. Thus temporal filtering or “{{pattern recognition}} ” occurs at an early stage in the auditory pathway...|$|R
40|$|We {{discuss the}} use of dynamic <b>auditory</b> <b>{{patterns}}</b> to detect and control human action. We argue that such patterns can affect the ability of standing persons to control medio-lateral orientation of their body. Subjects (N = 10) stood a mechanical platform that could rotate in the subject's medio-lateral axis. Real-time data about platform roll were used to generate acoustic stimuli, presented via headphones. Subjects were asked to control standing sway so as to achieve specific patterns in the acoustic stimuli. The results suggest that acoustic stimuli {{can be used as}} a source of information for the perception and control of orientation...|$|E
40|$|The {{development}} of expectancies during {{the unfolding of}} <b>auditory</b> <b>patterns</b> in time is a recognized but poorly understood aspect of human cognition. This study investigates {{development of}} pitch-based expectancies in melody prediction. Two artificial neural networks tested the hypothesis that expectancies, such as pitch proximity and pitch reversal as proposed by Narmour [12], {{can be learned from}} exposure to a musical environment. A multi-layered back-propagation network and a perceptron both performed at better than chance level. The way in pitch relations are acquired and represented in networks is discussed together with implications for future experiments and network models of musical pitch development. ...|$|E
40|$|Abstract—Transient {{synchronization}} {{has been}} used as a mechanism of recognizing <b>auditory</b> <b>patterns</b> using integrate-and-fire (IF) neural networks. We first extend the mechanism to vision tasks and investigate the role of spike dependent learning. We show that such a temporal Hebbian learning rule significantly improves accuracy of detection. Second, we demonstrate how multiple patterns can be identified by a single pattern selective neuron and how a temporal album can be constructed. This principle may lead to multidimensional memories, where the capacity per neuron is considerably increased with accurate detection of spike synchronization. Index Terms—Detection of spike synchronization, Hebbian learning rule, integrate-and-fire (IF) model, temporal vision, transient synchrony. I...|$|E
40|$|Book synopsis: The 119 {{contributions}} {{in this book}} cover a range of topics, including parallel computing, parallel processing in biological neural systems, simulators for artificial neural networks, neural networks for visual and <b>auditory</b> <b>pattern</b> recognition {{as well as for}} motor control, AI, and examples of optical and molecular computing. The book may be regarded as a state-of-the-art report {{and at the same time}} as an 'Interdisciplinary Reference Source' for parallel processing. It should catalyze international and interdisciplinary cooperation among computer scientists, neuroscientists, physicists and engineers in the attempt to: 1) decipher parallel information processes in biology, physics and chemistry 2) design conceptually similar technical parallel information processors...|$|R
40|$|A novel {{algorithm}} for convolutive non-negative matrix factorization (NMF) with multiplicative {{rules is}} presented in this paper. In contrast to the standard NMF, the low rank approximation is represented by a convolutive model which has an advantage of revealing the temporal structure possessed by many realistic signals. The convolutive basis decomposition is obtained by the minimization of the conventional squared Euclidean distance, rather than the Kullback-Leibler divergence. The algorithm {{is applied to the}} audio pattern separation problem in the magnitude spectrum domain. Numerical experiments suggest that the proposed algorithm has both less computational loads and better separation performance for <b>auditory</b> <b>pattern</b> extraction, as compared with an existing method developed by Smaragdis. © 2007 IEEE...|$|R
50|$|From the {{cognitive}} perspective, the brain perceives auditory stimuli as music according to gestalt principles, or “principles of grouping.” Gestalt principles include proximity, similarity, closure, and continuation. Each of the gestalt principles illustrates a different element of auditory stimuli that {{cause them to}} be perceived as a group, or as one unit of music. Proximity dictates that auditory stimuli that are near to each other are seen as a group. Similarity dictates that when multiple auditory stimuli are present, the similar stimuli are perceived as a group. Closure is the tendency to perceive an incomplete <b>auditory</b> <b>pattern</b> as a whole—the brain “fills in” the gap. And continuation dictates that auditory stimuli are more likely {{to be perceived as}} a group when they follow a continuous, detectable pattern.|$|R
40|$|An {{analytically}} tractable {{framework is}} presented to describe neural processing {{in the early stages}} of the auditory system. Algorithms are developed to assess the integrity of the acoustic spectrum at all processing stages. The algorithms employ wavelet representations, multiresolution processing, and the method of convex projections to reconstruct close replica of the input stimulus. Reconstructions using natural speech sounds demonstrate minimal loss of information along the auditory pathway. Furthermore, close inspections of the final <b>auditory</b> <b>patterns</b> reveals spectral enhancements and noise suppression that have close perceptual correlates. Finally, the auditory representations are shown to be versatile for many applications, including automatic speech recognition and low bit-rate data compression...|$|E
40|$|Infants (from Latin infans, speechless) {{are human}} beings who cannot speak. It took most of us the whole first year of our lives to {{overcome}} this infancy and to produce our first few meaningful words, but we were not idle as infants. We worked, rather independently, on two basic ingredients of word production. On the one hand, we established our pri-mary notions of agency, interactancy, the temporal and causal structures of events, object permanence and loca-tion. This provided us with a matrix {{for the creation of}} our first lexical concepts, concepts flagged by way of a verbal la-bel. Initially, these word labels were exclusively <b>auditory</b> <b>patterns,</b> picked up from the environment. On the other hand, we created a repertoire of babbles, a set of syllabic ar-ticulatory gestures. These motor patterns normally spring up around the seventh month. The child carefully attend...|$|E
40|$|Deep Boltzmann Machines (DBM) {{have been}} used as a {{computational}} cognitive model in various AI-related research and applications, notably in computational vision and multimodal fusion. Being regarded as a biological plausible model of the human brain, the DBM is also becoming a popular instrument to investigate various cortical processes in neuroscience. In this paper, we describe how a multimodal DBM is implemented as part of a Neural-Symbolic Cognitive Agent (NSCA) for real-time multimodal fusion and inference of streaming audio and video data. We describe how this agent can be used to simulate certain neurological mechanisms related to hallucinations and dreaming and how these mechanisms are beneficial to the integrity of the DBM. Finally, we will explain how the NSCA is used to extract multimodal information from the DBM and provide a compact and practical iconographic temporal logic formula for complex relations between visual and <b>auditory</b> <b>patterns...</b>|$|E
40|$|In {{this paper}} we {{introduce}} {{the use of}} map-seeking circuits for <b>auditory</b> <b>pattern</b> detection and classification. A map-seeking circuit is a signal processing structure used to detect a desired feature in a mixture by iteratively transforming, superposing, and comparing the composite mixture with a pattern template. The result is a mapping between the template and {{the position of the}} matched feature in the mixture. The iterative detection process is inspired by the neural connections in the human visual system. A particularly important feature of map-seeking classification is that the search operates on an additive superposition of allowable transformations of the desired feature vector, giving a linear increase in computation with increasing image complexity, rather than a brute-force feature detection that increases in computation geometrically. 1...|$|R
40|$|Children {{and adults}} who have central {{auditory}} processing disorder (CAPD) are a heterogeneous {{group of people who}} have difficulty using auditory information to communicate and learn. CAPD is not a specific problem or disease; rather it is a set of problems that occur in different kinds of listening tasks. Often children with CAPD are first diagnosed with attention deficit hyperactivity disorder (ADHD) or learning disabilities. Later, an audiologist may render a diagnosis of CAPD. To audiologists, CAPD includes problems with {{one or more of the}} following auditory tasks (ASHA CAPD Task Force, 1996) : • Sound localization and lateralization • Auditory discrimination • <b>Auditory</b> <b>pattern</b> recognition • Temporal aspects of audition (resolution, masking, integration, ordering) • Auditory performance decrements with competing acoustic signals • Auditory performance with degraded acoustic signals Audiologists make the diagnosis using standardized tests of these skills administered i...|$|R
40|$|The role of 2 {{psychological}} processes, differentiation and organization, {{were examined}} {{in the perception of}} musical tonality. Differentiation distinguishes elements from one another and was varied in terms of the distribution of pitch durations within tone sequences. Organization establishes relations between differentiated elements and was varied in terms of either conformity with or deviation from a hierarchical description of tonality. Multiple experiments demonstrated that the perception of tonality depended on a minimal degree of differentiation in the distribution of the duration—but not frequency of occur-rence—of pitches and only when pitch distributions were hierarchically organized. Moreover, the mere differentiation of the tonic from nontonic pitches was not sufficient to induce tonal percepts. These results are discussed in relation to tonal strength, musical expressiveness, and principles of <b>auditory</b> <b>pattern</b> processing. A fundamental aspect of the perception of visual scenes or auditory sequences is the apprehension of their inherent structura...|$|R
40|$|A {{quantitative}} {{model of}} auditory learning {{is presented to}} predict how <b>auditory</b> <b>patterns</b> are stored in the songbird auditory forebrain. This research focuses on the caudomedial nidopallium (NCM) in the songbird telencephalon, a candidate site for song perception {{and the formation of}} song auditory memories. The objective is to introduce simplified features of bird song that could be used by the auditory forebrain to identify and distinguish memorized songs. The results elucidate which biological mechanisms are sufficient for temporal pattern prediction and the storage of higher-order patterns, where by higher-order, we mean the specific arrangement of syllables into song motifs (phrases) to reveal neural mechanisms of syntax. Key words: auditory, learning, avian forebrain, synaptic plasticity, habituation Auditory processing and auditory memories are essential to animals that use vocal communication in social behavoir (see reviews in [1]). In songbirds, vo-cal learning and retention of vocalizations depends on the ability to hear a song model and to perfect vocal performance through auditory feedback [2 – 4]...|$|E
40|$|Recent {{findings}} {{have demonstrated that}} attention to visual events engages the lateral premotor cortex {{even in the absence}} of motor planning. Here, we used functional magnetic resonance imaging to explore acoustically triggered activations within the lateral premotor cortex. Temporal (when), object-related (what), and spatial (where) <b>auditory</b> <b>patterns</b> were to be monitored for violations in a serial prediction task. As a result, we found a modality-dependent modulation for auditory events within the inferior ventrolateral premotor cortex, an area engaged in vocal plans. In addition, however, auditory activations were distributed within the entire premotor cortex depending on which stimulus property was attended to. Attention to where patterns was found to engage fields for gaze and reaching (dorsolateral premotor cortex), what patterns to engage fields for hand movements (superior ventrolateral premotor cortex), and when patterns to engage fields for vocal plans. Together, the findings confirm the idea of a sensory somatotopy in lateral premotor cortex, according to which a perceptual pattern triggers representations within that motor effector which would be most appropriate to generate it as an action effect...|$|E
40|$|All music {{contains}} pitch-time patterns. Gestalt principles, {{which are}} learned from perceptual {{interaction with the}} environment, explain how the elements of <b>auditory</b> <b>patterns</b> are perceptually grouped and how auditory scenes are analyzed. This enables a perceptually grounded understanding {{of the rules of}} harmony and counterpoint. Patterns such as rhythms, chords, melodies, and entire tonal passages tend to be perceived relative to reference points in pitch-time space (downbeat, root, tonic tone or chord) and to have various identities and associations. All these references are, or can be, ambiguous or multiple. Musical emotions are generated either via (ambiguous or multiple) associations from outside the music or (ambiguous or multiple) structures and associated expectations within the music. Ambiguity does not itself generate emotion; instead, both ambiguity and emotion result from pattern recognition processes. Thus, the relationship between ambiguity and emotion is not causal. The art of musical composition and performance involves the manipulation of ambiguous expectations and emotions. Learning, context, ambiguity and multiplicity are central concepts in both psychoacoustics and cultural musicology, suggesting a considerable potential for interaction between musical sciences and humanities...|$|E
40|$|Little {{is known}} about intraspeaker changes in voice across {{changing}} speaking situations in everyday life. In this study, we examined acoustic variations between and within 5 talkers and their effect on the likelihood that voice samples would not be identified as coming from the same talker. Talkers were drawn from a large database recorded to capture everyday variations in vocal characteristics. Nine samples of /a/, recorded on three different days, were examined for each talker. Acoustic characteristics were estimated using VoiceSauce and analysis-by-synthesis, and listeners judged whether pairs of voices {{came from the same}} or two different talkers. Results indicate that interspeaker variability in voice quality exceeds intraspeaker variability, but differences are smaller than expected. As predicted by models that treat voice quality as an <b>auditory</b> <b>pattern,</b> the acoustic attributes associated with incorrect “different speaker ” responses varied from talker to talker, depending on the particular characteristics of the voice in question. Index Terms: voice quality, speaker recognition, intra-speaker variabilit...|$|R
40|$|The human central {{auditory}} {{system has}} a remarkable ability to establish memory traces for invariant {{features in the}} acoustic environment despite continual acoustic variations in the sounds heard. By recording the memory-related mismatch negativity (MMN) component of the auditory electric and magnetic brain responses as well as behavioral performance, we investigated how subjects learn to discriminate changes in a melodic pattern presented at several frequency levels. In addition, we explored whether musical expertise facilitates this learning. Our data show that especially musicians who perform music primarily without a score learn easily to detect contour changes in a melodic pattern presented at variable frequency levels. After learning, their auditory cortex detects these changes even when their attention is directed away from the sounds. The present results thus show that, after perceptual learning during attentive listening has taken place, changes in a highly complex <b>auditory</b> <b>pattern</b> can be detected automatically by the human auditory cortex and, further, that this process is facilitated by musical expertise...|$|R
40|$|A Dutch {{test battery}} {{comprising}} six different tests for auditory processing disorders was evaluated {{in a group}} of 49 adults and children (age 8 - 57 years) with auditory complaints despite normal audiometric thresholds. Percentile scores were derived from normal control groups (n = 132) to determine whether a subject passed or failed a test. A composite score was computed to reflect a general score on all the auditory processing tests. In order to gain insight into underlying auditory processes, factor analysis was performed. Normal scores on all the tests were seen in five subjects. The remaining 44 subjects had at least one test score that was below the cut-off point (10 th percentile). Factor analysis provided evidence for a model comprising four auditory components: auditory sequencing, word recognition in noise, auditory closure, and <b>auditory</b> <b>patterning.</b> This model could be useful in the interpretation of scoring patterns. Although there were some differences in scoring patterns between the children and adults, the test battery proved to be useful in both groups...|$|R
40|$|The {{temporal}} lobe {{in the left}} hemisphere has long been implicated {{in the perception of}} speech sounds. Little is known, however, regarding the specific function of different temporal regions in the analysis of the speech signal. Here we show that an area extending along the left middle and anterior superior temporal sulcus (STS) is more responsive to familiar consonant [...] vowel syllables during an auditory discrimination task than to comparably complex <b>auditory</b> <b>patterns</b> that cannot be associated with learned phonemic categories. In contrast, areas in the dorsal superior temporal gyrus bilaterally, closer to primary auditory cortex, are activated to the same extent by the phonemic and nonphonemic sounds. Thus, the left middle/anterior STS appears {{to play a role in}} phonemic perception. It may represent an intermediate stage of processing in a functional pathway linking areas in the bilateral dorsal superior temporal gyrus, presumably involved in the analysis of physical features of speech and other complex non-speech sounds, to areas in the left anterior STS and middle temporal gyrus that are engaged in higher-level linguistic processes...|$|E
40|$|There are {{algorithms}} for {{the transformation}} of accounting data into music, and there is suggestive evidence {{that it is possible}} to hear different patterns in it than we see when it is transformed into a graph. We cannot say with certainty whether those different patterns are really there, and we cannot even say that if they were, we would be able to perceive them audibly without a disciplining education similar to that which has traditionally taught us to seek and find patterns—knowledge—visually. We can say, however, that there is reason to believe that the mental pathways for the creation of <b>auditory</b> <b>patterns</b> and visual patterns are different. One forms anticipations of events in time; the other forms structures of points in space. One engages the emotions more directly than the other. Each employs different parts of the brain. There are indeed reasons why we might hear something more or at least something else in the music generated by an algorithm than we might see in a picture that was created from the same data...|$|E
40|$|This {{experiment}} analyzed ihe influente of subvocaL {{activity in}} retention of rhythmical <b>auditory</b> <b>patterns.</b> Retention of sixteen percussion sequences was srudied. Each sequence (a 4 -s “door-knocking ” pattern) {{was followed by}} one of the following six retention conditions: silence, unatrended niusic (blocking dic ¡mier car, e,, Gregorian ciianting), unaltended musie (blocking Ihe inner car, ¡e., rock-and-roll), arciculatory suppression (blocking the inner voice), tracing circíes on the table with index finger (spatial task), and tapping (motor control). After silente, unatíended ‘nusie (chanting), or the spatial rask, participanis successfully reproduced most patrerns. Errors increased with unaltended music (rock-and-roll), but significant disruptions only occurred with tapping and articulatory suppression. Whereas dic latrer case supports the roleof an articulatory loop in retention, thc production of successive mps or syltab]es o both interferente conditions probably relies on a general rhythm module, which disrupted retention of the patterns. Key words: auditory imagen, rhythm perception En este experimento se analizó la influencia de la actividad subvocal sobre la retención de patrones rítmicos auditivos. Se investigó la retención de dieciséis secuencias de percusión...|$|E
40|$|A major {{challenge}} for neuroscience is to map accurately the spatiotemporal patterns of {{activity of the}} large neuronal populations that are believed to underlie computing in the human brain. To study a specific example, we selected the mismatch negativity (MMN) brain wave (an event-related potential, ERP) because it gives an electrophysiological index of a bprimitive intelligenceQ capable of detecting changes, even abstract ones, in a regular <b>auditory</b> <b>pattern.</b> ERPs have a temporal resolution of milliseconds but appear to result from mixed neuronal contributions whose spatial location is not fully understood. Thus, {{it is important to}} separate these sources in space and time. To tackle this problem, a two-step approach was designed combining the independent component analysis (ICA) and low-resolution tomography (LORETA) algorithms. Here we implement this approach to analyze the subsecond spatiotemporal dynamics of MMN cerebral sources using trial-by-trial experimental data. We show evidence that a cerebral computation mechanism underlies MMN. This mechanism is mediated by the orchestrated activity of several spatially distributed brain sources located in the temporal, frontal, and parietal areas, which activate at distinct time intervals and are grouped in six main statistically independent components...|$|R
40|$|Auditory graphsdisplays that {{represent}} graphical, quantitative information with soundhave {{the potential to}} make graphical representations of data more accessible to blind students and researchers as well as sighted people. No research to date, however, has systematically addressed the attributes of data {{that contribute to the}} complexity (the ease or difficulty of comprehension) of auditory graphs. A pair of studies examined the role of both data density (i. e., the number of discrete data points presented per second) and the number of trend reversals for both point estimation and trend identification tasks with auditory graphs. For the point estimation task, results showed main effects of both variables, with a larger effect attributable to performance decrements for graphs with more trend reversals. For the trend identification task, a large main effect was again observed for trend reversals, but an interaction suggested that the effect of the number of trend reversals was different across lower data densities (i. e., as density increased from 1 to 2 data points per second). Results are discussed in terms of data sonification applications and rhythmic theories of <b>auditory</b> <b>pattern</b> perception. M. S. Committee Chair: Bruce N. Walker; Committee Member: Gregory Corso; Committee Member: Wendy Roger...|$|R
40|$|The present {{paper is}} {{investigating}} the modelling of the McGurk effect, an audio-visual speech perceptual illusion, with a distributed model of memory. The network is trained with congruent <b>auditory</b> and visual <b>patterns</b> and tested with incongruent sets of patterns considered to produce the McGurk effect...|$|R
