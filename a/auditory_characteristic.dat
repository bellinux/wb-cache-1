2|82|Public
40|$|Abstract: In this research, we analyze how {{the sound}} and music relate to humans from the aspect of Kansei engineering. We analyze what {{features}} of the sound humans pay attention and how humans interpret sound. Therefore, we divide the signal processing of sound that humans do into four levels. At the physiological level, processing {{is done by the}} <b>auditory</b> <b>characteristic.</b> In this level, humans don't interpret the image of the sound yet. There is no subjectivity for the sound. By using <b>auditory</b> <b>characteristic,</b> we investigate the features which help in the case that sound and music is analyzed. We consider that the processing at early stage of auditory nervous system is to extract the change in power, which is obtained from the segmentation of the sound-signals which is divided by band of the frequency and time interval, and its contrast. We also consider the features obtained by that extractation. Moreover, in the cognitive level, we analyze the correlation of that features with the word of interpretation that humans do subjectively. By these modelings, we develop the method of retrieving {{the sound and}} music that having the similarity, or having the image that is expressed by any subjective words...|$|E
40|$|Abstract: When people {{encounter}} unfamiliar words, {{they often}} use {{tools such as}} search engines to obtain background information on these words. However, the semantic content of words can be complex, {{and it is not}} always possible to understand the meaning of words from textual information alone. In this paper we quantify the semantic content of words by means of a simple and convenient text-based method whereby the semantic content is constructed from linguistic, visual and <b>auditory</b> <b>characteristic</b> values. Using characteristic vectors generated in this way, users are able to visually check and search for background information on unfamiliar terms in a web document. ...|$|E
30|$|The {{auditory}} filter modelling {{represents the}} mathematical model {{which tends to}} simulate the basic perceptual and psychophysical aspects of the human <b>auditory</b> <b>characteristics</b> (Lyon et al. 2010). This model consists of the simulation of the outer/middle ear filtering by second-order low-pass filter and the cochlea spectral behaviour by the gammachirp auditory filterbank.|$|R
40|$|The {{physical}} {{world in which}} humans reside is limited to four dimensions, but the mental world in which our knowledge of language resides is not similarly limited. Individuals' knowledge of speech sounds comprises representations of information in multiple sensory domains, including representations of the <b>auditory</b> <b>characteristics</b> of the sounds that they hav...|$|R
30|$|According to the time-domain {{masking effect}} of human auditory, a large signal can make masking {{effect on the}} small signal [1]. So changes in the small signals {{can not be easily}} heard. With this <b>auditory</b> <b>characteristics,</b> we embed the {{watermark}} with LF and HF components parameters into the small signal position to make the watermark hidden better.|$|R
50|$|In humans, its {{earliest}} activations {{in regard to}} visual stimuli occur at 45 ms with activations related to changes in visual stimuli within 45-60 ms (these are comparable with response times in the primary visual cortex).This fast brain pathway also provides auditory input at even shorter times starting at 24 ms and being affected by <b>auditory</b> <b>characteristics</b> at 30-60 ms.|$|R
5000|$|CDS is a {{clear and}} {{simplified}} strategy for communicating to younger children, used not only by adults but also by older children. The vocabulary is limited, speech is slowed with {{a greater number of}} pauses, and the sentences are short and grammatically simplified, often repeated. Although CDS features marked <b>auditory</b> <b>characteristics,</b> other factors aid in development of language. Three types of modifications occur to adult-directed speech in the production of CDS — ...|$|R
40|$|Sensory {{packaging}} design congruent with product and brand characteristics {{may be used}} as an innovative tool to communicate product and brand values to consumers and to enhance taste experience. This study investigated whether consumers associate sensory properties of beer bottles with certain brand values and beer flavours. Participants evaluated five beer products on a list of brand values, flavour characteristics and package characteristics. The results demonstrated that consumers systematically associate tactile and <b>auditory</b> <b>characteristics</b> of a bottle with certain brand values and specific beer flavours. The study creates a conceptual tool for designing brand congruent multisensory beer bottles...|$|R
30|$|Audio watermarks are {{supposed}} to be transparent to human ears, by what means the modification due to watermarking is virtually inaudible. One way to enhance the embedding efficiency is to exploit the <b>auditory</b> <b>characteristics</b> so that the embedding strength is sufficiently high to withstand attacks without introducing audible distortion. The methods presented in [16, 17, 22] demonstrated the benefit of exploiting the signal characteristics, but they relied on heuristic rules to decide the embedding strength. In these methods, even though some attention was paid to adjust relevant parameters to reach optimal performance, the connection between multiple transform domains and human auditory properties has not been thoroughly addressed.|$|R
40|$|Abstract. LPCC and MFCC are {{methods of}} extracting voice characteristic, {{and they are}} based on {{pronunciation}} models and human <b>auditory</b> <b>characteristics.</b> In this paper, both of the two characteristics are used, LPCC and the First Order Differential are used to describe the dynamic changes of speaker channels; MFCC and the First Order Differential are used to describe the audible frequency characteristics of human ears, and the characteristics of input voice are extracted by using Speech Processing Toolbox in MATLAB, and VQ and HMM are combined to applying to speaker recognition, and the experiment result showed that the performance of the speak recognition is obviously improved...|$|R
40|$|In this paper, {{the author}} {{describes}} {{on the results}} of experiments about the reflection and penetration of under-water sound and about the absorption by special liquid layer. He used carbon disulfide CS_ 2 as this absorptive liquid and got the conclusion as follow: - Because CS_ 2 -layer absorbs under-water sound when internal walls of 20 cm (thickness) CS_ 2 -layer are set up, even 1 m cubic aquarium may be used equivalently to 120 m cubic water space. It seems that these studies are very important in purpose to get results exactly on ecological experiments, especially about the <b>auditory</b> <b>characteristics</b> of fish, in small (1 m cubic order) aquarium...|$|R
40|$|The {{extent of}} sensory {{information}} presented to a user within a mediated environment {{has been proposed}} as a determinant of presence. (e. g., Sheridan, 1992). While {{a large proportion of}} presence research has focused on visual manipulations, research on manipulations of <b>auditory</b> <b>characteristics</b> is currently limited. In this paper the effects of several audio manipulations on the sense of presence and ratings of specific audio/visual dimensions were explored. In the first study, a 5. 1 `rally car' audio mix was rated significantly more highly than either a mono or stereo audio mix on some presence measures and specific audio dimensions. A second experiment was designed to investigate potential contributory factors...|$|R
40|$|We {{have been}} {{developing}} a real time speech-displaying system called “KanNon ” which helps deaf person to understand speaker’s speech contents. We designed the KanNon system {{to display a}} sound spectrogram, pitch frequency and loudness of speech as well as characters by speech-recognition system as real-time scrolling image. For the purpose of displaying formant patterns clearly with high accuracy, we applied Burg method combining with the minimum cross-entropy (Burg-MCE) method, and human <b>auditory</b> <b>characteristics</b> such as an equal loudness preemphasis and mel-scale frequency to the sound spectrogram. Finally, we show more effective display for the spectrogram reading in the KanNon system. 2. THE KANNON SYSTEM We show the interface of the KanNon system in Fig...|$|R
40|$|Sexual {{displays}} of the Musk Duck and Blue-billed Duck are described and illustrated. The {{displays of}} male Musk Ducks comprise {{a series of}} three forms exhibiting increasing ritualization, complexity, and time-interval constancy. All of them have conspicuous <b>auditory</b> <b>characteristics</b> as well as variously conspicuous visual features. Displays in the species appear to have evolved under the influence of intense sexual selection resulting from what is probably a more completely promiscuous mating system than occurs in any other species of Anatidae. These selective pressures have also probably promoted the evolution of such features as large size and extreme sexual dimorphism that distinguish the genus Biziura from the typical stiff-tails (Oxyura). Displays in the Blue-billed Duck likewise embody a combination of <b>auditory</b> and visual <b>characteristics,</b> and include a large number of variably ritualized postures, several of which are clearly derived from comfort movements or intention movements. Certain similarities between the major displays of the Blue-billed Duck and the North American Ruddy Duck are thought {{to be the result of}} convergence, and a close relationship between these species is not indicated...|$|R
40|$|Assuming {{that the}} 10 Danish Dogma films can be {{considered}} genre films, the article analyses the use of sound and music in these films. The “Vow of Chastity” proposed by the Dogma directors specifies that sound and image must never be produced separately, and the article presents elements of productions practice as well as aesthetic considerations. Using examples from the finished films, it demonstrates how the artistic intention formulated in the set of rules was handled during production and how it influenced the finished films. The individual Dogma films were quite different from an auditory point of view, but among the <b>auditory</b> <b>characteristics</b> of the films were the auditory jump cut and {{the abolition of the}} traditional dichotomy between diegetic and non-diegetic music...|$|R
40|$|The {{behavioral}} audiograms of slow loris and potto {{were determined}} by the technique of conditioned suppression, and the <b>auditory</b> <b>characteristics</b> of Prosimians then estimated by combining the data with that of bushbaby. It is concluded that high-frequency hearing, low-frequency sensitivity, and total area of the audible field are well correlated with phyletic level, while lowest threshold and best frequency are not. Furthermore, the rela-tion of high-frequency hearing to ecological demands for accurate sound localization is supported. For almost a century it has been known that human hearing differs markedly from that of many other animals (e. g., Galton, 1883). But until recently, {{the nature and extent}} of this difference and the evolutionary transformation that brought it about have been mostly a matter of conjecture (cf...|$|R
40|$|This paper {{reports on}} a study of low bit rate speech coding based on the {{sinusoidal}} model proposed by McAulay and Quatieri. Several techniques are employed to reduce the coder 2 ̆ 7 s bit rate and increase the processing speed. Firstly, the Hilbert transform is used to estimate the system phase response. Secondly, a bilinear transform is used to warp the spectrum to exploit the <b>auditory</b> <b>characteristics</b> of the human ear when coding at very low rates. Thirdly, a method to estimate the initial pitch for the SEEVOC model is introduced. Finally, a simplified birth-and-death algorithm is presented. The simulation {{results show that the}} synthetic speech is of good quality at a bit rate of 4800 b/s, and is still intelligible and speaker recognizable at 1200 b/s...|$|R
30|$|The {{features}} {{proposed in}} the present study are derived from <b>auditory</b> <b>characteristics,</b> which include gammatone filtering, non-linear processing and modulation spectral processing, emulating the cochlear and the middle ear to improve robustness. In earlier studies, several auditory processing-motivated features have improved robustness for small and medium vocabulary tasks. The paper has studied the application of these techniques to large and complex vocabulary task, namely, the Aurora- 4 database. The results have shown that the proposed features considerably improved the robustness in all types of noise conditions. However, the present study is essentially confined to handle noise effects on speech and has not considered reverberant conditions. The selected weights for the non-linearity were heuristic, and automatic selection of optimal weights from the evaluation data is desirable. For the future, we would like to investigate these issues and evaluate the performance of the proposed features for reverberant environments and large vocabulary tasks.|$|R
40|$|Abstract. This paper {{describes}} user specific QoS {{requirements that}} are a critical innovation for improving spectral utilization for wireless systems. An adaptive scheduler is presented that incorporates user specific QoS requirements in the spectral allocation of resources. In this paper, we focus on voice applications, and demonstrate that by dynamically adapting MAC scheduling algorithms to the user specific QoS requirements, user satisfaction, {{as measured by the}} user spe-cific Mean Opinion Score (MOS), is maximized. OPNET LTE system simula-tions have been performed for a set of AMR VoIP users with assigned specific QoS target levels. Simulation results show that significant MOS improvements can be achieved if such user specific QoS requirements are considered in the MAC scheduler. Furthermore, when targeted to maximize spectrum utilization and combined with AMR codecs matched to the <b>auditory</b> <b>characteristics</b> of users, higher system capacity, at comparable MOS levels, may be achieved...|$|R
40|$|This paper proposes {{yet another}} {{representation}} of speech sounds. The proposed speech modeling can remove both multiplicative and linear transformational distortion from speech theoretically. It means that speech sounds are represented without being affected by any static distortion inevitably involved in production, encod-ing, transmission, decoding, and hearing processes, such as differ-ences in vocal tract length, gender, age, microphone, room, line, <b>auditory</b> <b>characteristics,</b> and so on. The method acoustically mod-els not individual phones but their entire system, where only acous-tic interrelation embedded {{in all the}} kinds of phones is focused. Since the method provides us with no absolute acoustic properties of phones, it cannot recognize or synthesize even a single phone. On the contrary, the proposed method is shown {{to be able to}} be ap-plied to pronunciation assessment effectively and reliably, where the proficiency of pronunciation is estimated without using acous-tic models of the individual phones directly in the matching. 1...|$|R
40|$|Speech {{communication}} {{has several}} steps of encoding, transmis-sion, and decoding. In each step, various acoustic distortions are inevitably induced by non-linguistic {{factors such as}} differences of age, gender, microphone, line, room, <b>auditory</b> <b>characteristics</b> of a hearer’s ears, etc. In spite of this large variability, humans can perform very precise speech processing. Recently, the first author proposed a novel representation of speech [1, 2], which is invariant with these factors at all. Only the dynamic motions in speech are focused on and the static features in speech are completely discarded. The high validity of this new representa-tion for speech recognition was already verified experimentally [3, 4, 5]. In this paper, we show that the new representation of the segmental aspect of speech {{can be interpreted as}} a kind of holistic and prosodic feature because the representation cap-tures speech as music, i. e. timbre-based melody. 1...|$|R
40|$|The phonetic {{description}} of a language must {{be related to the}} phonology. A computerized {{description of}} a language can have a very faithful phonetic component, but its phonetic structures are not appropriate for a phonological description. In current systems of linguistic analysis there are three aspects of phonology: (1) the representation of the lexical contrasts in a language; (2) the specification of the constraints on the sounds in lexical items; and (3) the description of phonological patterns of sounds as evident in the relations between the underlying lexical items and the observable phonetic output. There is a conflict between the phonetic component required for the first of these goals and that required for the other two. Characterizing the sounds of languages can be done most efficiently by using a large number of features, all defined in articulatory terms. This will result in having more features than are necessary to characterize phonological patterns efficiently. In addition, some phonological patterns depend on <b>auditory</b> <b>characteristics</b> which will require auditorily define...|$|R
40|$|Abstract — Auditory {{and visual}} cues are {{important}} sensor inputs for biological and artificial systems. They provide crucial information for navigating environments, recognizing categories, animals and people. How to combine effectively these two sensory channels {{is still an}} open issue. As a step towards this goal, this paper {{presents a comparison between}} three different multi-modal integration strategies, for audiovisual object category detection. We consider a high-level and a low-level cue integration approach, both biologically motivated, and we compare them with a mid-level cue integration scheme. All the three integration methods are based on the least square support vector machine algorithm, and state of the art audio and visual feature representations. We conducted experiments on two audio-visual object categories, dogs and guitars, presenting different visual and <b>auditory</b> <b>characteristics.</b> Results show that the high-level integration scheme consistently performs better than single cue methods, and of the other two integration schemes. These findings confirm results from the neuroscience. This suggests that the high-level integration scheme is the most suitable approach for multi-modal cue integration for artificial cognitive systems. I...|$|R
40|$|Area 8 B may {{be treated}} as part of either the prefrontal cortex or the premotor cortex. Previous {{investigations}} showed an involvement of area 8 B in both eye and ear motor control and in auditory perception. In this report, we Studied 139 neurons in three macaque monkeys of these., 32 neurons showed an activity related to environmental auditory stimuli. Fifteen cells with <b>auditory</b> <b>characteristics</b> (15 / 32) presented a firing discharge inhibited during the execution Of Visual fixation. The remaining 107 units presented complex or indefinable behaviour. The presence of auditory environmental cells which activity is related prevalently {{to the voice of}} persons (researchers) suggests that area 8 B may be an area involved in auditory cross-modal association, in natural behaviour. The inhibitory effects during Visual fixation suggest that area 8 B is part of the inhibitory network preventing the gaze shift in relation to ail auditory Stimulus. This may be the consequence of the engagement of attention during fixation that may affect the auditory perception. Both aspects indicate that area 8 B is involved in high cognitive processes in auditory and orienting processes...|$|R
40|$|In {{order to}} foster {{awareness}} of the <b>auditory</b> <b>characteristics</b> of learning disabled students and the essential skills involved in effective listening, this paper suggests teaching strategies to improve listening skills that include both individual and whole class activities specifically geared to learning disabled students. The first section of the paper profiles five students labeled as learning disabled. In the second section, listening {{is defined as the}} ability to perceive an auditory message and gain as much information from that message as possible. Steps toward understanding the listening problem, outlined in the third portion of the paper, include finding an area of interest, being flexible about notetaking, and working at listening. Sample lessons and activities that present listening as an enjoyable challenge are discussed in the fourth section, while five suggestions for developing listening as a study skill are presented in the fifth section. The paper concludes with five guidelines, such as presenting regularly scheduled lessons about listening {{at least twice a week}} and being a good model as a listener. (DF) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|This paper {{considers}} the phonetic {{description of a}} language, and how {{it relates to the}} phonology. A computerized description of a language can have a very faithful phonetic component, but its phonetic structures are not appropriate for a phonological description. In current systems of linguistic analysis there are three aspects of phonology: (1) the representation of the lexical contrasts in a language; (2) the specification of the constraints on the sounds in lexical items; and (3) the description of phonological patterns of sounds as evident in the relations between the underlying lexical items and the observable phonetic output. There is a conflict between the phonetic component required for the first of these goals and that required for the other two. Characterizing the sounds of languages can be done most efficiently by using a large number of features, all defined in articulatory terms. This will result in having more features than are necessary to characterize phonological patterns efficiently. In addition, some phonological patterns depend on <b>auditory</b> <b>characteristics</b> which will require auditorily defined features. Yet other patterns are observable in a language considered as a socia...|$|R
40|$|The {{auditory}} threshold {{levels of}} walleye pollock Theragra chalcogramma (Pallas) were measured {{in order to}} determine the optimum conditions for marine ranching utilizing underwater sound. The <b>auditory</b> <b>characteristics</b> of walleye pollock were determined by heart beat conditioning using 8 frequencies of pure tone stimuli from 60 to 1, 000 Hz coupled with an electric shock. Pure tone conditioning stimuli were presented to the fish for 5 seconds with a 0. 1 second 12 V DC electric shock applied 3 seconds after the start of the sound projection. The positive response to the pure tone stimulus of the conditioned fish consisted of an inhibition of one or more heartbeats. Results show that tested walleye pollock are sensitive to pure tones in the frequency range from 60 to 1 kHz, with greatest sensitivity in the range from 120 to 200 Hz. At the most sensitive frequencies, the mean thresholds were between 97. 7 and 100. 2 dB (re 1 μPa) under a background noise level between 55 and 75 dB (re 1 μPa/√Hz). Hearing ability declined gradually with increasing frequency above 400 Hz. The upper limit of audible frequency is believed to be about 1 kHz...|$|R
40|$|Objectives: The {{objectives}} {{of this study}} were (1) to describe the <b>auditory</b> <b>characteristics</b> of children with autism relative to those of typically developing children and (2) to describe the test-retest reliabil-ity of behavioral auditory test measures with this population of children with autism. Design: Audiometric data were obtained from 22 children diagnosed with autism and 22 of their typically developing peers. The audiologic test bat-tery consisted of behavioral measures (i. e., visual reinforcement audiometry, tangible reinforcement operant conditioning audiometry, and conditioned play audiometry) and physiological measures (audi-tory brain stem response audiometry, distortion product otoacoustic emissions, and acoustic re-flexes). Results: Children with autism had physiologic test results equivalent to their typically developing counterparts. That is, no differences in auditory brain stem response audiometry, distortion product otoacoustic emissions, or acoustic reflex results were noted between the children with autism and typically developing children. However, behavioral measures revealed that about half of the children diagnosed with autism presented pure-tone aver-ages outside of normal limits (i. e.,> 20 dB HL), although their response thresholds to speech were within normal limits. All behavioral test results were within normal limits (i. e., 20 dB HL) despite having normal to near-normal hearing sensitivity as determined by other audiometric measures. (Ear & Hearing 2006; 27; 430 – 441...|$|R
40|$|Speaker {{authentication}} {{has been}} developed rapidly {{in the last few}} decades. This research work attempts to extract the hidden features of human voice that is able to simulate human <b>auditory</b> system <b>characteristics</b> in speaker authentication. The hidden features are then presented as inputs to a Multi-Layer Perceptron Neural Network and Generic Self-organizing Fuzzy Neural Network to verify the speakers with high accuracy. Based on the experimental results, the two networks are able to verify speakers using two method in extracting hidden features from the recorded voice sources. r 2005 Elsevier B. V. All rights reserved...|$|R
40|$|A {{trade-off}} {{between the}} sensory modalities {{of vision and}} hearing {{is likely to have}} occurred in echolocating bats as the sophisticated mechanism of laryngeal echolocation requires considerable neural processing and has reduced the reliance of echolocating bats on vision for perceiving the environment. If such a trade-off exists, it is reasonable to hypothesize that some genes involved in visual function may have undergone relaxed selection or even functional loss in echolocating bats. The Gap junction protein, alpha 10 (Gja 10, encoded by Gja 10 gene) is expressed abundantly in mammal retinal horizontal cells and {{plays an important role in}} horizontal cell coupling. The interphotoreceptor retinoid-binding protein (Irbp, encoded by the Rbp 3 gene) is mainly expressed in interphotoreceptor matrix and is known to be critical for normal functioning of the visual cycle. We sequenced Gja 10 and Rbp 3 genes in a taxonomically wide range of bats with divergent <b>auditory</b> <b>characteristics</b> (35 and 18 species for Gja 10 and Rbp 3, respectively). Both genes have became pseudogenes in species from the families Hipposideridae and Rhinolophidae that emit constant frequency echolocation calls with Doppler shift compensation at high-duty-cycles (the most sophisticated form of biosonar known), and in some bat species that emit echolocation calls at low-duty-cycles. Our study thus provides further evidence for the hypothesis that a trade-off occurs at the genetic level between vision and echolocation in bats...|$|R
40|$|<b>Auditory</b> <b>characteristics</b> of {{metabolic}} or strial presbycusis {{were investigated}} using an animal {{model in which}} young adult Mongolian gerbils (Merionesunguiculates) were implanted with an osmotic pump supplying furosemide continuously to the round window. This model causes chronic lowering of the endocochlear potential (EP) and results in auditory responses {{very similar to those}} seen in quiet-aged gerbils (Schmiedt et al., J. Neurosci. 22 : 9643 – 9650, 2002). Auditory function was examined up to one week post-implant by measurement of auditory brainstem responses (ABRs) and distortion product otoacoustic emissions (DPOAEs). Emission “threshold” was defined as the stimulus level required to reach a criterion emission amplitude. Comparing all responses on a “threshold-shift diagram,” where emission threshold increases were plotted versus ABR threshold increases, the following results were obtained: (1) On average, the increase of the emission threshold was about 55 % of the increase in ABR threshold, with comparatively little scatter. (2) The main dysfunction in metabolic presbycusis appears to be a decrease in the gain of the cochlear amplifier, combined with an additional, smaller increase in neural threshold, both effects caused by a chronically low EP. (3) For ABR threshold increases over 20 dB, the points for the chronic low-EP condition were largely separate from those previously found for permanent acoustic damage. The threshold-shift diagram therefore provides a method for noninvasive differential diagnosis of two common hearing dysfunctions...|$|R
40|$|Sensory {{substitution}} devices convert live {{visual images}} into auditory signals, for example with a web camera (to record the images), a computer (to perform the conversion) and headphones (to {{listen to the}} sounds). In {{a series of three}} experiments, the performance of one such device (‘The vOICe’) was assessed under various conditions on blindfolded sighted participants. The main task that we used involved identifying and locating objects placed on a table by holding a webcam (like a flashlight) or wearing it on the head (like a miner’s light). Identifying objects on a table was easier with a hand-held device, but locating the objects was easier with a head-mounted device. Brightness converted into loudness was less effective than the reverse contrast (dark being loud), suggesting that performance under these conditions (natural indoor lighting, novice users) is related more to the properties of the auditory signal (ie the amount of noise in it) than the cross-modal association between loudness and brightness. Individual differences in musical memory (detecting pitch changes in two sequences of notes) was related to the time taken to identify or recognise objects, but individual differences in self-reported vividness of visual imagery did not reliably predict performance across the experiments. In general, the results suggest that the <b>auditory</b> <b>characteristics</b> of the device may be more important for initial learning than visual associations...|$|R
30|$|In this paper, {{we propose}} a biologically-inspired feature {{extraction}} method for robust recognition of noisy speech signals. The proposed method {{is based on}} the human <b>auditory</b> system <b>characteristics,</b> and relies on both the outer and middle ear filtering and the spectral behaviour of the cochlea. The outer and middle ear filtering is modelled by a second-order low-pass filter (Martens and Van Immerseel 1990; Van Immerseel and Martens 1992). The cochlear filter is modelled by a gammachirp auditory filterbank consisting of 34 filters, where the centre frequencies are equally spaced on the ERB-rate scale from 50  Hz to 8  kHz.|$|R
40|$|PURPOSE: This study {{aimed to}} {{correlate}} probable predisposing factors for {{sensorineural hearing loss}} in elderly by investigating the audiologic characteristics and frequency of mutations in genes considered responsible for non-syndromic hearing loss. METHODS: Sixty elderly patients were separated into two groups: the Case Group, composed of 30 individuals, 21 females and nine males, all 60 years old or older and presenting diagnoses of sensorineural hearing loss, and the Control Group, composed of 30 elderly individuals matched to the experimental group by age and gender, presenting normal hearing. The patients underwent anamnesis and pure tone audiometry in frequencies of 250, 500, 1000, 2000, 3000, 4000 and 6000 Hz. Blood samples were collected from each patient for analysis of mutations in nuclear and mitochondrial genes related to non-syndromic sensorineural hearing loss. RESULTS: It was observed a greater tendency to noise exposure and consumption of alcohol in the Case Group. The statistically significant symptoms between the groups were tinnitus and hearing difficulty in several situations as: silent environment, telephone, television, sound location and in church. All the individuals of Case Group presented sensorineural and bilateral hearing loss. The symmetry and progression of the hearing impairment were also statistically significant between the groups. No genetic mutations were identified. CONCLUSION: The most reported symptoms were communication difficulties and tinnitus. The predominant <b>auditory</b> <b>characteristics</b> included sensorineural, bilateral, progressive and symmetrical hearing loss. It was not evidenced a relationship between sensorineural hearing loss in elderly and genes considered responsible for non-syndromic hearing loss as no genetic mutation was found in this study...|$|R
40|$|The Australian noctuid moth, Speiredonia spectans shares its {{subterranean}} day roosts (caves {{and abandoned}} mines) with insectivorous bats, {{some of which}} prey upon it. The capacity of this moth to survive is assumed to arise from its ability to listen for the bats' echolocation calls and take evasive action; however, the <b>auditory</b> <b>characteristics</b> of this moth or any tropically distributed Australian moth have never been examined. We investigated the ears of S. spectans and determined that {{they are among the}} most sensitive ever described for a noctuid moth. Using playbacks of cave-recorded bats, we determined that S. spectans is able to detect most of the calls of two co-habiting bats, Rhinolophus megaphyllus and Miniopterus australis, whose echolocation calls are dominated by frequencies ranging from 60 to 79 kHz. Video-recorded observations of this roost site show that S. spectans adjusts its flight activity to avoid bats but this defence may delay the normal emergence of the moths and leave some `pinned down' in the roosts for the entire night. At a different day roost, we observed the auditory responses of one moth to the exceptionally high echolocation frequencies (150 - 160 kHz) of the bat Hipposideros ater and determined that S. spectans is unable to detect most of its calls. We suggest that this auditory constraint, in addition to the greater flight manoeuvrability of H. ater, renders S. spectans vulnerable to predation by this bat to the point of excluding the moth from day roosts where the bat occurs. No Full Tex...|$|R
40|$|We {{present an}} Auditory Information Seeking Principle (AISP) (gist, navigate, filter, and details-on-demand) {{modeled after the}} visual {{information}} seeking mantra [1]. We propose that data sonification designs should conform to this principle. We also present some design challenges imposed by human <b>auditory</b> perception <b>characteristics.</b> To improve blind access to georeferenced statistical data, we developed two preliminary sonifications adhering to the above AISP, an enhanced table and a spatial choropleth map. Our pilot study shows people can recognize geographic data distribution patterns on a real map with 51 geographic regions, in both designs. The study also shows evidence that AISP conforms to people's information seeking strategies. Future work is discussed, including {{the improvement of the}} choropleth map design...|$|R
