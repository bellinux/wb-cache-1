0|13|Public
40|$|Image {{orientation}} detection requires high-level scene understanding. Humans use {{object recognition}} and contextual scene information to correctly orient images. In literature, {{the problem of}} image orientation detection is mostly confronted by using low-level vision features, while some approaches incorporate few easily detectable semantic cues to gain minor improvements. The vast amount of semantic content in images makes orientation detection challenging, and therefore {{there is a large}} semantic gap between existing methods and human behavior. Also, existing methods in literature report highly discrepant detection rates, which is mainly due to large differences in datasets and limited variety of test images used for evaluation. In this work, for the first time, we leverage the power of deep learning and <b>adapt</b> <b>pre-trained</b> convolutional neural networks using largest training dataset to-date for the image orientation detection task. An extensive evaluation of our model on different public datasets shows that it remarkably generalizes to correctly orient a large set of unconstrained images; it also significantly outperforms the state-of-the-art and achieves accuracy very close to that of humans...|$|R
40|$|We {{address the}} problem of contour {{detection}} via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in <b>adapting</b> a <b>pre-trained</b> per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS 500. Comment: 9 pages, 3 figure...|$|R
40|$|Camera {{relocalisation}} is {{an important}} problem in computer vision, with applications in simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2 D-to- 3 D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by <b>adapting</b> a <b>pre-trained</b> forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150 ms, making it desirable for real-time systems that require online relocalisation. Comment: To appear in the proceedings of CVPR 201...|$|R
40|$|In this paper, we are {{interested}} in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e. g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can <b>adapt</b> a <b>pre-trained</b> neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods...|$|R
40|$|Speech {{enhancement}} {{deep learning}} systems usually require {{large amounts of}} training data to operate in broad conditions or real applications. This makes the adaptability of those systems into new, low resource environments an important topic. In this work, we present the results of adapting a speech enhancement generative adversarial network by finetuning the generator with small amounts of data. We investigate the minimum requirements to obtain a stable behavior in terms of several objective metrics in two very different languages: Catalan and Korean. We also study the variability of test performance to unseen noise {{as a function of}} the amount of different types of noise available for training. Results show that <b>adapting</b> a <b>pre-trained</b> English model with 10 min of data already achieves a comparable performance to having two orders of magnitude more data. They also demonstrate the relative stability in test performance with respect to the number of training noise types...|$|R
40|$|Abstract. Tracking {{multiple}} {{objects in}} parallel {{is a difficult}} task, espe-cially if instances are interacting and occluding each other. To alleviate the arising problems multiple camera views can be taken into account, which, however, increases the computational effort. Evoking the need for very efficient methods, often rather simple approaches such as back-ground subtraction are applied, which tend to fail for more difficult sce-narios. Thus, in this work, we introduce a powerful multi-instance track-ing approach building on Hough Forests. By adequately refining the time consuming building blocks, we can drastically reduce their computational complexity without a significant loss in accuracy. In fact, we show that the test time can be reduced by one to two orders of magnitude, allow-ing to efficiently process {{the large amount of}} image data coming from multiple cameras. Furthermore, we <b>adapt</b> the <b>pre-trained</b> generic forest model in an online manner to train an instance-specific model, making it well suited for multi-instance tracking. Our experimental evaluations show the effectiveness of the proposed efficient Hough Forests for object detection {{as well as for the}} actual task of multi-camera tracking. ...|$|R
40|$|A common {{approach}} for moving objects segmentation {{in a scene}} is to perform a background subtraction. Several methods have been proposed in this domain. However, they lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose a robust and flexible encoder-decoder type neural network based approach. We <b>adapt</b> a <b>pre-trained</b> convolutional network, i. e. VGG- 16 Net, under a triplet framework in the encoder part to embed an image in multiple scales into the feature space and use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. We train this network end-to-end by using only a few training samples. Our network takes an RGB image in three different scales and produces a foreground segmentation probability mask for the corresponding image. In order to evaluate our model, we entered the Change Detection 2014 Challenge (changedetection. net) and our method outperformed all the existing state-of-the-art methods by an average F-Measure of 0. 9770. Our source code will be made publicly available at [URL] This paper is under consideration at Pattern Recognition Letter...|$|R
40|$|Multi-face {{tracking}} in unconstrained videos is {{a challenging}} problem as faces {{of one person}} often appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we use the contextual constraints to generate {{a large number of}} training samples for a given video, and further <b>adapt</b> the <b>pre-trained</b> face CNN to specific videos using discovered training samples. Using these training samples, we optimize the embedding space so that the Euclidean distances correspond to a measure of semantic face similarity via minimizing a triplet loss function. With the learned discriminative features, we apply the hierarchical clustering algorithm to link tracklets across multiple shots to generate trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques. Comment: Project page: [URL]...|$|R
40|$|Automatically detecting, labeling, and {{tracking}} objects in videos depends {{first and foremost}} on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work {{is the first to}} investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from the pre-trained one, and (ii) we also <b>adapt</b> the <b>pre-trained</b> model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap "mini-Fisher Vectors" and expensive "off-the-shelf" ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT. Comment: To appear at BMVC 201...|$|R
40|$|Deep Neural Networks (DNN) {{trained on}} large {{datasets}} {{have been shown}} to be able to capture high quality features describing image data. Numerous studies have proposed various ways to transfer DNN structures trained on large data sets to perform classification tasks represented by relatively small datasets. Due to the limitations of these proposals, it is not well known how to effectively <b>adapt</b> the <b>pre-trained</b> model into the new task. Typically, the transfer process uses a combination of fine-tuning and training of adaptation layers, however, both tasks are susceptible to problems with data shortage and high computational complexity. This work proposes an improvement to the well-known AlexNet feature extraction technique. The proposed approach applies a Recursive Neural Network (RNN) structure on features extracted by a Deep Convolutional Neural Network (CNN) pre-trained on a large data set. Object recognition experiments conducted on the Washington RGBD image data set have shown that, the proposed method has the advantages of structural simplicity combined with the ability to provide higher recognition accuracy at a low computational cost compared to other relevant methods. The new approach requires no training at the feature extraction phase, and can be performed very efficiently as the output features are compact and highly discriminative, and can be used with a simple classifier in object recognition settings...|$|R
40|$|In this paper, a level-wise mixture model (LMM) is {{developed}} by embedding visual hierarchy with deep networks to support large-scale visual recognition (i. e., recognizing thousands or even {{tens of thousands}} of object classes), and a Bayesian approach is used to <b>adapt</b> a <b>pre-trained</b> visual hierarchy automatically to the improvements of deep features (that are used for image and object class representation) when more representative deep networks are learned along the time. Our LMM model can provide an end-to-end approach for jointly learning: (a) the deep networks to extract more discriminative deep features for image and object class representation; (b) the tree classifier for recognizing large numbers of object classes hierarchically; and (c) the visual hierarchy adaptation for achieving more accurate indexing of large numbers of object classes hierarchically. By supporting joint learning of the tree classifier, the deep networks and the visual hierarchy adaptation, our LMM algorithm can provide an effective approach for controlling inter-level error propagation effectively, thus it can achieve better accuracy rates on large-scale visual recognition. Our experiments are carried on ImageNet 1 K and ImageNet 10 K image sets, and our LMM algorithm can achieve very competitive results on both the accuracy rates and the computation efficiency as compared with the baseline methods...|$|R
40|$|In {{recent years}} {{significant}} {{progress has been}} made learn-ing generic pedestrian detectors from manually labeled large scale training sets. However, when a generic pedes-trian detector is applied to a specific scene where the test-ing data does not match with the training data because of variations of viewpoints, resolutions, illuminations and backgrounds, its accuracy may decrease greatly. In this pa-per, we propose a new framework of <b>adapting</b> a <b>pre-trained</b> generic pedestrian detector to a specific traffic scene by au-tomatically selecting both confident positive and negative examples from the target scene to re-train the detector it-eratively. An important feature of the proposed framework is to utilize unsupervisedly learned models of vehicle and pedestrian paths, together with multiple other cues such as locations, sizes, appearance and motions to select new training samples. The information of scene structures in-creases the reliability of selected samples and is comple-mentary to the appearance-based detector. However, it was not well explored in previous studies. In order to further improve the reliability of selected samples, outliers are re-moved through multiple hierarchical clustering steps. The effectiveness of different cues and clustering steps is evalu-ated through experiments. The proposed approach signifi-cantly improves the accuracy of the generic pedestrian de-tector and also outperforms the scene specific detector re-trained using background subtraction. Its results are com-parable with the detector trained using a large number of manually labeled frames from the target scene. 1...|$|R
40|$|In this paper, {{we present}} a cluster-dependent {{adaptation}} approach for HMM-based acoustic models. The proposed approach employs clustering techniques to group the original training utterances into clusters with predefined number. The clustered speech data are intended to <b>adapt</b> an initially <b>pre-trained</b> acoustic model to the specific cluster by reestimation based on the standard Baum-Welch procedure. The resulting model, adapted to the homogeneous data may markedly improve the baseline recognition rate, whereas the model complexity may be reduced. In the recognition step, the test samples are scored by each adapted model and the most accurate one is chosen. The proposed approach is thoroughly evaluated in Slovak triphone-based large vocabulary continuous speech recognition (LVCSR) system. The results prove that the cluster-sensitive retraining leads to significant improvements over the baseline reference system trained according to the conventional training procedure...|$|R

