258|687|Public
25|$|Construction (<b>arithmetic</b> <b>unit</b> only): transistor-diode {{logic is}} used.|$|E
500|$|By June 1948 the SSEM {{had been}} built and was working. It was [...] in length, [...] tall, and weighed almost [...] The machine {{contained}} 550valves—300diodes and 250pentodes—and had a power consumption of 3500watts. The <b>arithmetic</b> <b>unit</b> was built using EF50 pentode valves, which had been widely used during wartime. The SSEM used one Williams tube to provide 32 by 32-bit words of random-access memory (RAM), a second to hold a 32-bit accumulator in which the intermediate results of a calculation could be stored temporarily, and a third to hold the current program instruction along with its address in memory. A fourth CRT, without the storage electronics of the other three, {{was used as the}} output device, able to display the bit pattern of any selected storage tube.|$|E
50|$|Construction (<b>arithmetic</b> <b>unit</b> only): transistor-diode {{logic is}} used.|$|E
50|$|The CPUs {{included}} two independent <b>arithmetic</b> <b>units</b> with different capabilities.|$|R
5000|$|... 1982 Mike Farmwald, On the Design of High Performance Digital <b>Arithmetic</b> <b>Units</b> ...|$|R
40|$|Processor {{architectures}} with tens {{to hundreds}} of <b>arithmetic</b> <b>units</b> are emerging to handle media processing applications. These applications, such as image coding, image synthesis, and image understanding, require arithmetic rates of up to 10 11 operations per second. As the number of <b>arithmetic</b> <b>units</b> in a processor increases to meet these demands, register storage and communication between the <b>arithmetic</b> <b>units</b> dominate the area, delay, {{and power of the}} <b>arithmetic</b> <b>units.</b> In this paper we show that partitioning the register file along three axes reduces the cost of register storage and communication without significantly impacting performance. We develop a taxonomy of register architectures by partitioning across the data-parallel, instruction-level parallel, and memory hierarchy axes, and by optimizing the hierarchical register organization to operate on streams of data. Compared to a centralized global register file, the most compact of these organizations reduces the register file ar [...] ...|$|R
5000|$|<b>Arithmetic</b> <b>unit</b> {{can make}} division, multiplication, shift and {{normalize}} operations ...|$|E
5000|$|Construction (<b>Arithmetic</b> <b>unit</b> only) Type Quantity Diodes 37,543 All types Transistors 13,819 All types ...|$|E
5000|$|Vladislav Paunović: <b>Arithmetic</b> <b>unit</b> of the CER-12 computer, AUTOMATIKA, No 3, pp. 161-165, Zagreb 1971.|$|E
5000|$|Multiple <b>arithmetic</b> <b>units</b> {{may require}} memory {{architectures}} to support several accesses per instruction cycle ...|$|R
30|$|In particular, we {{are looking}} at <b>arithmetic</b> <b>units</b> (Steffe, 2010 a; Ulrich, 2015), which are interiorized {{counting}} acts.|$|R
40|$|The {{floating}} point <b>arithmetic</b> <b>units</b> are complex in their algorithms and many scientific problems require {{floating point}} units with high accuracy. Hence for increased performance and fault tolerance operations the double precision floating point <b>arithmetic</b> <b>units</b> adder, subtractor, multiplier and divider is designed which {{is enough for}} most System on Chip (SoC) applications and it also improves the accuracy during long chain of computations. The synthesized code results are verified and the complete layout is generated using backend flow...|$|R
5000|$|<b>Arithmetic</b> <b>Unit</b> and Storage was in {{a cabinet}} that was 10 ft high and 11 ft long.|$|E
50|$|The {{instruction}} queue {{is filled}} with the instructions fetched from the memory. The address <b>arithmetic</b> <b>unit</b> serves for address calculations.|$|E
5000|$|<b>Arithmetic</b> <b>unit</b> I {{had three}} 16-bit {{registers}} called A, B, and C, and a 16-bit D register which {{functioned as a}} buffer.|$|E
40|$|Recently, thermal-aware {{digital circuit}} design in {{advanced}} technologies is great challenges to realize high-speed and robust microprocessors. In this paper, we explore temperature gradient alleviating method for <b>arithmetic</b> <b>units.</b> Aiming at alleviating temperature gradients at logical circuit design level, {{we try to}} flatten out a power density by applying delay-balancing technique for equal-delay circuits. Our proposal is evaluated in fine grain thermal simulation. Simulation results show the strong dependency between placement and temperature gradients on <b>arithmetic</b> <b>units...</b>|$|R
40|$|Conference PaperProcessor {{architectures}} with tens {{to hundreds}} of <b>arithmetic</b> <b>units</b> are emerging to handle media processing applications. These applications, such as image coding, image synthesis, and image understanding, require arithmetic rates of up to 10 ^ 11 operations per second. As the number of <b>arithmetic</b> <b>units</b> in a processor increases to meet these demands, register storage and communication between the <b>arithmetic</b> <b>units</b> dominate the area, delay, {{and power of the}} <b>arithmetic</b> <b>units.</b> In this paper we show that partitioning the register file along three axes reduces the cost of register storage and communication without significantly impacting performance. We develop a taxonomy of register architectures by partitioning across the data-parallel, instruction-level parallel, and memory hierarchy axes, and by optimizing the hierarchical register organization to operate on streams of data. Compared to a centralized global register file, the most compact of these organizations reduces the register file area, delay, and power dissipation of a media processor by factors of 195, 20, and 430, respectively. This reduction in cost is achieved with a performance degradation of only 8 % on a representative set of media processing benchmarks...|$|R
40|$|Parity {{prediction}} {{arithmetic operator}} schemes {{have the advantage}} to be compatible with data paths and memory systems checked by parity codes. Nevertheless, the basic drawback of these schemes is {{that they may not}} be fault secure for single faults, since they propagate to multiple output errors that are undetectable by the parity code. In this paper we derive necessary and sufficient conditions for parity prediction arithmetic operators to achieve the fault secure property. From these conditions, various fault secure designs for arithmetic operators are reported. KEYWORDS: Self-Checking Circuits, Fault Secure Circuits, Parity Prediction, Arithmetic Operators,. I. INTRODUCTION Since <b>arithmetic</b> <b>units</b> (i. e. adders, ALUs, multipliers and dividers) are essential elements of computers, designing efficient self-checking <b>arithmetic</b> <b>units</b> is mandatory for designing self-checking and fault tolerant computers. The first self-checking <b>arithmetic</b> <b>units</b> were based on arithmetic r [...] ...|$|R
50|$|The <b>arithmetic</b> <b>unit</b> {{consists}} of three registers: the accumulator (A), lower accumulator (L), and the number register (N). Only the A and L registers are addressable.|$|E
5000|$|The Mark III used nine {{magnetic}} drums (one of {{the first}} computers to do so). One drum could contain 4,000 instructions and has an access time of 4,400 microseconds; thus it was a stored-program computer. The <b>arithmetic</b> <b>unit</b> could access two other drums - one contained 150 words of constants and the other contained 200 words of variables. Both of these drums also had an access time of 4,400 microseconds. This separation of data and instructions {{is known as the}} Harvard architecture. There were six other drums that held a total of 4,000 words of data, but the <b>arithmetic</b> <b>unit</b> couldn't access these drums directly. Data had to be transferred between these drums and the drum the <b>arithmetic</b> <b>unit</b> could access via registers implemented by electromechanical relays. This was a bottleneck in the computer and made the access time to data on these drums long - 80,000 microseconds. This was partially compensated for by the fact that twenty words could be transferred on each access.|$|E
50|$|Control UnitControl unit {{contains}} a program counter and instruction registers. It fetches instructions and facilitates program flow. It supports single-operand instruction set {{and works with}} all 16 index registers of the <b>arithmetic</b> <b>unit.</b>|$|E
40|$|This paper {{presents}} {{principles and}} preferences {{for the implementation}} of computer arithmetic and ideals for the arithmetic facilities in future programming languages. the implementation principles and preferences are for the current approaches to the design of <b>arithmetic</b> <b>units.</b> The ideals are for the long term development of programming languages, with the hope that <b>arithmetic</b> <b>units</b> will be built to support the requirements of program-ming languages. NOTE: This is a draft and has not yet been approved by IFI? WG 2. 5 as stated in the text...|$|R
40|$|Submitted {{on behalf}} of EDA Publishing Association ([URL] audienceRecently, thermal-aware digital circuit design in {{advanced}} technologies is great challenges to realize high-speed and robust microprocessors. In this paper, we explore temperature gradient alleviating method for <b>arithmetic</b> <b>units.</b> Aiming at alleviating temperature gradients at logical circuit design level, we try to flatten out a power density by applying delay-balancing technique for equal-delay circuits. Our proposal is evaluated in fine grain thermal simulation. Simulation results show the strong dependency between placement and temperature gradients on <b>arithmetic</b> <b>units...</b>|$|R
40|$|Media {{processing}} applications, such as three-dimensional graphics, video compression, {{and image}} processing, currently demand 10 - 100 billion {{operations per second}} of sustained computation. Fortunately, hundreds of <b>arithmetic</b> <b>units</b> can easily fit on a modestly sized 1 cm 2 chip in modern VLSI. The challenge is to provide these <b>arithmetic</b> <b>units</b> with enough data {{to enable them to}} meet the computation demands of media applications. Conventional storage hierarchies, which frequently include caches, are unable to bridge the data bandwidth gap between modern DRAM and tens to hundreds of <b>arithmetic</b> <b>units.</b> A data bandwidth hierarchy, however, can bridge this gap by scaling the provided bandwidth across the levels of the storage hierarchy. The stream programming model enables media processing applications to exploit a data bandwidth hierarchy effectively. Media processing applications can naturally be expressed as a sequence of computation kernels that operate on data streams. This programming [...] ...|$|R
50|$|MANIAC II {{was built}} by the University of California and the Los Alamos Scientific Laboratory, {{completed}} in 1957. It used 2,850 Vacuum tubes and 1,040 semiconductor diodes in the <b>arithmetic</b> <b>unit.</b> Overall it used 5,190 vacuum tubes, 3,050 semiconductor diodes, and 1,160 transistors.|$|E
50|$|By 1960 EDVAC {{was running}} over 20 {{hours a day}} with {{error-free}} run time averaging eight hours. EDVAC received a number of upgrades including punch-card I/O in 1953, extra memory in slower magnetic drum form in 1954, and a floating-point <b>arithmetic</b> <b>unit</b> in 1958.|$|E
50|$|All models {{featured}} a CPU {{with at least}} a floating-point <b>arithmetic</b> <b>unit,</b> Memory map with access protection, Memory write protection, Two real-time clocks, a Power fail-safe, an External interface, Ten internal interrupt levels. Also a Multiplexor input/output processor (MIOP) featuring Channel A with eight sub-channels.|$|E
2500|$|The {{techniques}} used for compound <b>unit</b> <b>arithmetic</b> were developed over many centuries and are well-documented in many textbooks {{in many different}} languages. In addition to the basic arithmetic functions encountered in decimal <b>arithmetic,</b> compound <b>unit</b> <b>arithmetic</b> employs three more functions: ...|$|R
40|$|Arithmetic {{operations}} traditionally used fixed-point processing {{because it}} makes them less expensive. In integer and fixed-point arithmetic, multipliers are larger, slower and consume much more power than adders, which are often neglected in performance evaluation of DSP systems. In floating-point arithmetic {{that is not true}} and in this thesis we show that multipliers and adders are equally important. The thesis also emphasizes low power design. For that reason, some of the basic digital filter network structures, built with FP <b>arithmetic</b> <b>units,</b> are revisited to map their performance with different filtering functions. This thesis presents digital filter network structures' performance with different filtering functions. It presents filter network structures transformed from their original form to accommodate pipe-lined <b>arithmetic</b> <b>units.</b> These filter structures can also be implemented with fixed-point <b>arithmetic</b> <b>units</b> because of the speed advantage they provide. Several experiments, through hardware synthesis of the structures, show that FIR filter Direct form structure using an adder tree consumes less power than Direct form structure using a chain of adders and its Transposed form. They also show that for IIR filters, Direct form II using standard floating-point <b>arithmetic</b> <b>units</b> is power optimal. This research work is intended to provide designers with information on the performance of these structures with different applications in an effort to help reduce the "design gap...|$|R
3000|$|Neural {{networks}} have successfully {{been used to}} detect faces in video images. In the paper entitled [...] "Performance Analysis of Bit-Width Reduced Floating-Point <b>Arithmetic</b> <b>Units</b> in FPGAs: Case Study of Neural Network-based Face Detector," [...] the authors describe the implementation of an FPGA-based face detector using a neural network and bit-width reduced floating-point <b>arithmetic</b> <b>units</b> (FPUs). The FPUs and neural network are designed using MATLAB and VHDL, and the two implementations are compared. The authors demonstrate that reductions {{in the number of}} bits used in arithmetic computation can produce significant cost reductions including area, speed, and power with a small sacrifice in accuracy.|$|R
5000|$|<b>Arithmetic</b> <b>unit</b> II {{performed}} both arithmetic {{and addressing}} operations. It had four registers. P was the program counter, R and E were special-purpose, and Q, {{which was used}} for 32-bit operations (with A as the high-order word), or 48-bit operations (with A and B). Q also served as the multiplier-quotient register for multiplication and division.|$|E
50|$|The central {{processor}} was the high-speed <b>arithmetic</b> <b>unit</b> that functioned as the workhorse of the computer. It performed the addition, subtraction, and logical operations {{and all of}} the multiplication, division, incrementing, indexing, and branching instructions for user programs. Note that in the CDC 6000 architecture, the central processing unit performed no input/output (I/O) operations. Input/Output was totally asynchronous, and performed by peripheral processors.|$|E
50|$|A {{very small}} machine with a 6-bit wide mill (<b>arithmetic</b> <b>unit).</b> For {{compatibility}} {{with the other}} machines a 24-bit operation was performed by the processor as four 6-bit operations. Based on the PF183 developed by ICT Stevenage. The 1901 was announced and released after {{the other members of}} the initial range, in response to the IBM System/360 Model 20, and was a great success.|$|E
40|$|Design of <b>arithmetic</b> <b>units</b> {{has been}} an {{important}} issue which can dominate performance of the whole circuits. Recent logic synthesis tools can implement <b>arithmetic</b> <b>units</b> from RTL descriptions by utilizing parameterized design components predefined in arithmetic libraries. This paper reviews the current status in arithmetic synthesis, and some issues, especially on customizability, are pointed out to be tackled for better performance. Further work is still needed in arithmetic synthesis, and it is helpful to have an framework which eases new approaches to be integrated. Requirements for such framework are discussed and a possible synthesis flow within this framework is shown...|$|R
40|$|This paper proposes {{optimizations}} of {{the methods}} and parameters used in both mathematical approximation and hardware design for logarithmic number system (LNS) arithmetic. First, we introduce a general polynomial approximation approach with an adaptive divide-in-halves segmentation method for evaluation of LNS arithmetic functions. Second, we develop a library generator that automatically generates optimized LNS <b>arithmetic</b> <b>units</b> with a wide bit-width range from 21 to 64 bits, to support LNS application development and design exploration. The basic <b>arithmetic</b> <b>units</b> are tested on practical FPGA boards as well as software simulation. When compared with existing LNS designs, our generated units provide in most case...|$|R
40|$|This project {{provides}} {{the experience of}} applying an advanced version of Spurious Power Suppression Technique (SPST) on multipliers for high speed and low power purposes. When a portion of data {{does not affect the}} final computing results, the data controlling circuits of SPST latch this portion to avoid useless data transition occurring inside the <b>arithmetic</b> <b>units,</b> so that the useless spurious signals of <b>arithmetic</b> <b>units</b> are filter out. Modified Booth Algorithm is used in this project for multiplication which reduces the number of partial product to n/ 2. The simulation result shows that the SPST implementation with AND gates lead to a significant speed improvement and power reductio...|$|R
