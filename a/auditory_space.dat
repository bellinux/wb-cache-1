252|57|Public
5000|$|... #Subtitle level 3: Localization {{of sound}} in virtual <b>auditory</b> <b>space</b> ...|$|E
50|$|Typically, sounds {{generated}} from headphones appear to originate {{from within the}} head. In the virtual <b>auditory</b> <b>space,</b> the headphones {{should be able to}} “externalize” the sound. Using the HRTF, sounds can be spatially positioned using the technique described below.|$|E
50|$|A basic {{assumption}} in {{the creation}} of a virtual <b>auditory</b> <b>space</b> is that if the acoustical waveforms present at a listener’s eardrums are the same under headphones as in free field, then the listener’s experience should also be the same.|$|E
40|$|This paper reviews current {{approaches}} to designing virtual environments and investigates aspects {{of influence in}} designing <b>auditory</b> <b>spaces</b> to support novel forms of interaction in virtual places. Initial research on human imagined sounds from places has identified `expectation' as an important psychological construct, which must be considered when designing sounds for virtual places. The research work continues to provide evidence that {{there are differences between}} sounds people expect to hear in places and sounds recorded in real life places. Instead of designing realistic virtual spaces, the paper suggests a user's sense of presence as a measure of the user's experience in virtual environment. The results indicate that using highly expected sounds increases users' sense of presence. As such, it is to propose that designing <b>auditory</b> <b>spaces</b> using expectations as perceived affordance is perhaps a minimal way to design <b>auditory</b> <b>spaces</b> that support sense of place, hence provoke the emergence of virtual communities. Future work of the project is discussed...|$|R
40|$|This paper investigates aspects in {{designing}} <b>auditory</b> <b>spaces</b> to support novel forms of interaction in virtual spaces. Initia l research on human imagined sounds from places has identified `expectation' {{as an important}} psychological construct. Second study shows that {{there are differences between}} sounds people expect to hear in places and sounds recorded from real life places. Instead of designing realistic experience, the paper suggests a user's sense of presence as a measure of the user's experience in virtual spaces. The results indicate that using highly expected sounds increases users' sense of presence. As such, it is to propose that designing <b>auditory</b> <b>spaces</b> using expectations as perceived affordance for presence is perhaps a minimal way to engaging users experience...|$|R
40|$|Abstract—High-quality virtual audio scene {{rendering}} is re-quired for emerging virtual and {{augmented reality}} applications, perceptual user interfaces, and sonification of data. We describe al-gorithms for creation of virtual <b>auditory</b> <b>spaces</b> by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware. Index Terms—Audio user interfaces, head-related transfer func-tion, spatial audio, 3 -D audio processing, user interfaces, virtual <b>auditory</b> <b>spaces,</b> virtual environments, virtual reality...|$|R
50|$|Virtual {{acoustic}} space (VAS), {{also known}} as virtual <b>auditory</b> <b>space,</b> is a technique in which sounds presented over headphones appear to originate from any desired direction in space. The illusion of a virtual sound source outside the listener's head is created.|$|E
50|$|The neurons of {{the primary}} {{auditory}} cortex can be considered to have receptive fields covering a range of auditory frequencies and have selective responses to harmonic pitches. Neurons integrating information from the two ears have receptive fields covering a particular region of <b>auditory</b> <b>space.</b>|$|E
50|$|The central {{auditory}} system converges inputs {{from both}} ears (inputs contain no explicit spatial information) onto single neurons within the brainstem. This system contains many subcortical sites that have integrative functions. The auditory nuclei collect, integrate, and analyze afferent supply, {{the outcome is}} a representation of <b>auditory</b> <b>space.</b> The subcortical auditory nuclei are responsible for extraction and analysis of dimensions of sounds.|$|E
40|$|Psychophysical {{phenomena}} such as categorical {{perception and}} the perceptual magnet effect indicate that our <b>auditory</b> perceptual <b>spaces</b> are warped for some stimuli. This paper investigates {{the effects of}} two different kinds of training on <b>auditory</b> perceptual <b>space.</b> It is first shown that categorization training using non-speech stimuli, in which subjects learn to identify stimuli within a particular frequency range as member...|$|R
40|$|This paper investigates aspects in {{designing}} <b>auditory</b> <b>spaces</b> to support novel forms of interaction in virtual spaces. Initial research on human imagined sounds from places has identified `expectation' {{as an important}} psychological construct. Instead of designing realistic experience, the paper suggests a user's sense of presence {{as a measure of}} the user's experience in virtual spaces. The results indicate that using highly expected sounds increases users' sense of presence...|$|R
40|$|To examine {{relations}} between speech production and perception {{in the human}} brain, we investigated inherent vowel structures in both articulatory <b>space</b> and <b>auditory</b> perceptual <b>space.</b> A nonlinear analysis method (Laplacian eigenmap) was employed to extract inherent vowel structures from an articulatory database of read speech. In articulatory space, the first dimension of the vowel structure represents the degree of tongue-palate approximation, {{and the second is}} related to the ratio of lip opening to oral cavity size, while the vowels scatter on a curved surface in the third dimension. When applying the same technique to corresponding acoustic data from the same corpus, a compatible structure was obtained from the vowels in the <b>auditory</b> perceptual <b>space.</b> The results suggest that consistent topological images of the vowel structures exist in both the articulatory and <b>auditory</b> perceptual <b>spaces.</b> This finding supports the hypothesis that an efficient auditory-articulatory linkage exists in the human brain for speech processing...|$|R
50|$|In {{virtual space}} technology, a {{tracking}} system is generally a system capable of rendering virtual space {{to a human}} observer while tracking the observer's coordinates. For instance, in dynamic virtual <b>auditory</b> <b>space</b> simulations, a real-time head tracker provides feedback to the central processor, allowing for selection of appropriate head-related transfer functions at the estimated current position of the observer relative to the environment.|$|E
50|$|Many {{neuroscience}} {{studies have}} facilitated {{the development and}} refinement of a speech processing model. This model shows cooperation between the two hemispheres of the brain, with asymmetric inter-hemispheric and intrahemispheric connectivity consistent with the left hemisphere specialization for phonological processing. The right hemisphere is more specialized for sound localization, while <b>auditory</b> <b>space</b> representation in the brain requires the integration of information from both hemispheres.|$|E
50|$|An <b>auditory</b> <b>space</b> {{representation}} enables {{attention to}} be given (conscious top-down driven) to a single auditory stream. A gain mechanism can be employed involving the enhancement of the speech stream, and the suppression of any other speech streams and any noise streams. An inhibition mechanism can be employed involving the variable suppression of outputs from the two cochlea. Some of those with spatial hearing loss are unable to suppress unwanted cochlea output.|$|E
40|$|To {{evaluate}} the inner {{anatomy of the}} auditory apparatus by means of virtual endoscopy of spiral computed tomography (CT) data sets. BACKGROUND: Virtual endoscopy permits simulation of the fiberoptic endoscopy perspective by processing CT or magnetic resonance images. METHODS: Seven formalin-fixed specimens of human mastoid were scanned with spiral CT with the following protocol: beam collimation 1 mm, pitch ratio 1, reconstruction spacing 0. 2 to 0. 5 mm, field of view 90 mm. For the generation of endoscopic views of the <b>auditory</b> <b>spaces,</b> the axial images were processed with Navigator software 2. 0 running on UltraSparc I workstation. RESULTS: Virtual endoscopy allowed the demonstration of the external auditory canal, the head and handle of the malleus, the stapes and incudostapedial articulation, the corpus, the long process of the incus with its lenticular process and the short limb, the malleoincudal articulation, the rounded promontory, the round and oval windows, and Prussak's space. From inside the basal turn of the cochlea, virtual endoscopy showed the orifices of the fenestrae cochlea and vestibuli, {{the origin of the}} lateral and the anterior semicircular canals, and the basal turn of cochlea. The optimal perspectives that allowed demonstration of the anatomical details of the middle and inner ear are described. CONCLUSION: Virtual endoscopy allows the generation of inner views of the <b>auditory</b> <b>spaces.</b> This new method of image processing can be proposed as an integrative tool of spiral CT imaging...|$|R
40|$|Much {{theoretical}} and empirical work {{has been focused on}} how language learners/users parse multi-dimensional <b>auditory</b> <b>spaces</b> into the phonetic categories of a native or second language. A more fundamental question is how the listener determines the relevant dimensions for the perceptual space in the first place. Harvey Sussman has offered one of the only principled theoretical accounts for the existence of particular perceptual dimensions – neural columnar encoding that leads to relative dimensions. The framework of Sussman’s theory – interactions of neural processing constraints with statistics of the input – provides insights into potential answers to the more general questions about how listeners can solve the “frame problem” of which dimensions are most relevant to an auditory categorization task...|$|R
40|$|Applications in the {{creation}} of virtual <b>auditory</b> <b>spaces</b> (VAS) and sonification require individualized head related transfer functions (HRTFs) for perceptual fidelity. HRTFs exhibit significant variation from person to person due to differences between their pinnae, and their body sizes. In this paper we propose and preliminarily implement a simple HRTF customization based on use of a recently published database of HRTFs [1] that also contains geometrical measurements of subject pinnae. We measure some of these features via simple image processing, and select the HRTF that has features most closely corresponding to the individual’s features. This selection procedure is implemented along with the virtual auditory system described in [2], and listener tests conducted comparing the “customized...|$|R
5000|$|Let x(t) {{represent}} an electrical signal driving a loudspeaker and y(t) represent the signal received by a microphone inside the listener’s eardrum. Similarly, let x(t) represent the electrical signal driving a headphone and y(t) represent the microphone {{response to the}} signal. The goal of the virtual <b>auditory</b> <b>space</b> is to choose x(t) such that y(t) = y(t). Applying the Fourier transform to these signals, {{we come up with}} the following two equations: ...|$|E
50|$|ShatterFloodMudHouses is an HD video {{animation}} {{displaying a}} portrait exposing, confronting, and forecasting environmental and societal decay. A generic glass house is viewed spinning through myriad cycles {{inherent in the}} causal effects of erratic global warming weather, political divisiveness, and the ever-expanding intolerance of differences. Blurring edges between solid and fictive space questions the real-to-reel while shattering expectations of norms into particles of dust. Viewers are lulled and suddenly tossed between calm and brutal disturbances by interventions that shatter and assault psychological, physical and <b>auditory</b> <b>space.</b>|$|E
5000|$|With {{the passing}} years, the {{accuracy}} of many of Jones's statements on vowels has come increasingly under question, and most linguists now consider that the vowel quadrilateral must {{be viewed as a}} way of representing <b>auditory</b> <b>space</b> in visual form, rather than the tightly defined articulatory scheme envisaged by Jones. Nevertheless, the International Phonetic Association still uses a version of Jones's model, and includes a Jones-type vowel diagram on its influential International Phonetic Alphabet leaflet contained in the [...] "Handbook of the International Association". Many phoneticians (especially those trained in the British school) resort to it constantly as a quick and convenient form of reference.|$|E
40|$|High-quality virtual audio scene {{rendering}} is a {{must for}} emerging {{virtual and augmented reality}} applications, for perceptual user interfaces, and sonification of data. Personalization of HRTF is necessary in applications where perceptual realism and correct elevation perception is critical. We describe algorithms for creation of virtual <b>auditory</b> <b>spaces</b> by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware. 1. INTRODUCTION AND PREVIOU...|$|R
40|$|Presented at the 8 th International Conference on Auditory Display (ICAD), Kyoto, Japan, July 2 - 5, 2002. High-quality virtual audio scene {{rendering}} is a {{must for}} emerging {{virtual and augmented reality}} applications, for perceptual user interfaces,and sonification of data. Personalization of HRTF is necessary in applications where perceptual realism and correct elevation perception is critical. We describe algorithms for creation of virtual <b>auditory</b> <b>spaces</b> by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware...|$|R
40|$|This paper {{investigates the}} notion of {{occupation}} as dynamic physical and multisensory relationships within architectural environments. Our study specifically focuses {{on the construction of}} myriad relationships between physical, visual and <b>auditory</b> articulations of <b>space,</b> and how these shape human activities and interactions. By drawing on literature from the areas of visual and acoustic ecology (Gibson, Truax, Schafer) we seek to frame {{the notion of}} human occupation as temporal interrelations between acoustic arenas, soundmarks, and sonic events, as well as their visual equivalents. Additionally, we use Brian Massumi&# 039;s discussion of synaesthetic fusion, movement and sensation as a philosophical tool for interrogating these interrelations. For the design project we have nominated a site within RMIT University&# 039;s city campus in Melbourne, Australia. The applied research methods include the auditory and visual capture of environmental data via video, still photography and stereo recording techniques. We subsequently produce a series of sound and video compositions, constructing new spatiotemporal and sensory relations from the material captured within Bowen Street. This approach serves as a technique for initiating a qualitative design proposition for the site that shifts modes of occupation through visual and auditory interventions. The paper concludes with speculations about the significance of interrelations between visual and <b>auditory</b> <b>spaces</b> in designing environments for human occupation...|$|R
5000|$|Eric Knudsen is a {{professor}} of neurobiology at Stanford University. He {{is best known for his}} discovery, along with Masakazu Konishi, of a brain map of sound location in two dimensions in the barn owl, tyto alba. His work has contributed to the understanding of information processing in the auditory system of the barn owl, the plasticity of the <b>auditory</b> <b>space</b> map in developing and adult barn owls, the influence of auditory and visual experience on the space map, and more recently, mechanisms of attention and learning. He is a recipient of the Lashley Award, the Gruber Prize in Neuroscience, and the Newcomb Cleveland prize [...] and is a member of the National Academy of Sciences.|$|E
50|$|The <b>auditory</b> <b>space</b> of {{binaural}} hearing is constructed {{based on the}} analysis of differences in two different binaural cues in the horizontal plane: sound level, or ILD, and arrival time at the two ears, or ITD, which allow for the comparison of the sound heard at each eardrum. ITD is processed in the LSO and results from sounds arriving earlier at one ear than the other; this occurs when the sound does not arise from directly in front or directly behind the hearer. ILD is processed in the MSO and results from the shadowing effect that is produced at the ear that is farther from the sound source. Outputs from the SOC are targeted to the dorsal nucleus of the lateral lemniscus {{as well as the}} IC.|$|E
50|$|By {{the time}} sound stream {{representations}} {{reach the end}} of the auditory pathways brainstem inhibition processing ensures that the right pathway is solely responsible for the left ear sounds and the left pathway is solely responsible for the right ear sounds. It is then the responsibility of the auditory cortex (AC) of the right hemisphere (on its own) to map the whole auditory scene. Information about the right auditory hemifield joins with the information about the left hemifield once it has passed through the corpus callosum (CC) - the brain white matter that connects homologous regions of the left and right hemispheres. Some of those with spatial hearing loss are unable to integrate the auditory representations of the left and right hemifields, and consequently are unable to maintain any representation of <b>auditory</b> <b>space.</b>|$|E
40|$|Psychophysical {{phenomena}} such as categorical {{perception and}} the perceptual magnet effect indicate that our <b>auditory</b> perceptual <b>spaces</b> are warped for some stimuli. This paper investigates {{the effects of}} two different kinds of training on <b>auditory</b> perceptual <b>space.</b> It is first shown that categorization training using non-speech stimuli, in which subjects learn to identify stimuli within a particular frequency range {{as members of the}} same category, can lead to a decrease in sensitivity to stimuli in that category. This phenomenon is an example of acquired similarity and apparently has not been previously demonstrated for a category -relevant dimension. Discrimination training with the same set of stimuli was shown to have the opposite effect: subjects became more sensitive to differences in the stimuli presented during training. Further experiments investigated some of the conditions that are necessary to generate the acquired similarity found in the first experiment. The results of these [...] ...|$|R
40|$|HRTF individualization is a {{critical}} issue in high fidelity virtual <b>auditory</b> <b>spaces</b> using binaural reproduction. Given the relative success of recently published models for the head-torso contribution to the HRTF, we are presenting an initial attempt for estimating the pinna contribution, called here as pinna-related transfer function (PRTF). We use a set of PRTF magnitudes presented in the companion study. They were extracted from a HRTF database, and modeled by means of principal component analysis. First, we calculate the correlation between several spectral features and anthropometric dimensions. These features are principal component weights and central frequencies of pinna notches. Then, we derive other anthropometric pinna parameters {{in order to improve}} the correlation. Next, multidimensional linear regression is performed. As a result, linear transformations are calculated by solving the least square problem through QR decomposition. Finally, we are able to collect anthropometric data from new subjects and to estimate approximated individualized PRTFs. 1...|$|R
5000|$|Human {{listeners}} combine {{information from}} two ears to localize and separate sound sources originating in different locations {{in a process}} called binaural hearing. The powerful signal processing methods found in the neural systems and brains of humans and other animals are flexible, environmentally adaptable, and take place rapidly and seemingly without effort. [...] Emulating the mechanisms of binaural hearing can improve recognition accuracy and signal separation in DSP algorithms, especially in noisy environments. [...] Furthermore, by understanding and exploiting biological mechanisms of sound localization, virtual sound scenes may be rendered with more perceptually relevant methods, allowing listeners to accurately perceive the locations of auditory events. One way to obtain the perceptual-based sound localization is from the sparse approximations of the anthropometric features. Perceptual-based sound localization {{may be used to}} enhance and supplement robotic navigation and environment recognition capability.In addition, it is also used to create virtual <b>auditory</b> <b>spaces</b> which is widely implemented in hearing aids.|$|R
50|$|The {{receptive}} field {{of an individual}} sensory neuron is the particular region of the sensory space (e.g., the body surface, or the visual field) in which a stimulus will modify the firing of that neuron. This region can be a hair in the cochlea {{or a piece of}} skin, retina, tongue or other part of an animal's body. Additionally, it can be the space surrounding an animal, such as an area of <b>auditory</b> <b>space</b> that is fixed in a reference system based on the ears but that moves with the animal as it moves (the space inside the ears), or in a fixed location in space that is largely independent of the animal's location (place cells). Receptive fields have been identified for neurons of the auditory system, the somatosensory system, and the visual system.|$|E
50|$|The {{time and}} sound-pressure {{pathways}} converge at the lateral {{shell of the}} central nucleus of the inferior colliculus. The lateral shell projects to the external nucleus, where each space-specific neuron responds to acoustic stimuli only if the sound originates from a restricted area in space, i.e. the receptive field of that neuron. These neurons respond exclusively to binaural signals containing the same ITD and IID that would be created by a sound source located in the neuron’s receptive field. Thus their receptive fields arise from the neurons’ tuning to particular combinations of ITD and IID, simultaneously in a narrow range. These space-specific neurons can thus form a map of <b>auditory</b> <b>space</b> in which the positions of receptive fields in space are isomorphically projected onto the anatomical sites of the neurons.|$|E
5000|$|Rhodes [...] {{sought to}} {{identify}} whether audiospatial attention was represented analogically, that is, if the mental representation of <b>auditory</b> <b>space</b> was arranged {{in the same}} fashion as physical space. If this is the case, then the time to move the focus of auditory attention should be related to the distance to be moved in physical space. Rhodes notes that previous work by Posner, among others, had not found behavioral differences in an auditory attention task that merely requires stimulus detection, possibly due to low-level auditory receptors being mapped tonotopically rather than spatially, as in vision. For this reason, Rhodes utilized an auditory localization task, finding that the time to shift attention increases with greater angular separation between attention and target, although this effect reached asymptote at locations more than of 90° from the forward direction.|$|E
40|$|High-quality virtual audio scene {{rendering}} {{is required}} for emerging {{virtual and augmented reality}} applications, perceptual user interfaces, and sonification of data. We describe algorithms for creation of virtual <b>auditory</b> <b>spaces</b> by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware. EDICS: 3 -VRAR (primary), 2 -PRES, 3 -INTF (secondary). I. INTRODUCTION AND PREVIOUS WORK Many emerging applications require the ability to render audio scenes that are consistent with reality. In multimodal virtual and augmented reality systems using personal visual and auditory displays, the rendered audio and video must be kept consistent {{with each other and with}} the user’s movements to create a virtual scene [1]. A goal of our work is to create rich auditory environment...|$|R
40|$|ABSTRACT—In {{the present}} work, we {{investigated}} whether an <b>auditory</b> peripersonal <b>space</b> exists around {{the hand and}} whether such a space might be extended by a brief tool-use experience or by long-term experience using a tool in ev-eryday life. To this end, we studied audio-tactile integra-tion in the space around the hand and in far space, in blind subjects who regularly used a cane to navigate and in sighted subjects, before and after brief training with the cane. In sighted subjects, <b>auditory</b> peripersonal <b>space</b> was limited to around the hand before tool use, then expanded after tool use, and contracted backward after a resting period. In contrast, in blind subjects, peri-hand space was immediately expanded when they held the cane but was limited to around the hand when they held a short handle. These results suggest that long-term experience with the cane induces a durable extension of the peripersonal space. In a variety of species, stimuli from different sensory modalities are integrated in a limited space surrounding the body. This space has been termed peripersonal space (Rizzolatti, Fadiga, Fogassi, & Gallese, 1997). In monkeys, neurons located in dif-ferent brain areas, especially in the precentral gyrus (Rizzo...|$|R
40|$|This paper {{explores the}} sonic {{characteristics}} of urban spaces, with {{the application of}} apprehending acoustic space and form theory. The theory defines <b>auditory</b> <b>spaces</b> as acoustical arenas, which are spaces defi ned and delineated by sonic events. Historically, cities were built around a soundmark, for example, the resonance of a church bell or propagation of a calling for prayer, or a factory horn. Anyone living beyond the horizon of this soundmark was not considered citizens of that town. Furthermore, the volume of urban sonic arenas depends on natural. Digital simulation is necessary to visualize the ephemeral and temporal nature of sound, within a dynamic immersive environment like urban spaces. This paper digitally analyses the different morphologies of old cities and forms of growth {{in relation to the}} sound propagation and ecological effects. An experiment is conducted with the aid of an ancient North-African city model, exposed to a point cloud agent system. By analysing how the sound propagates from the known soundmark through the urban fabric, with the wind pressure interference; the paper compares the theoretical concept of soundmarks and the known perimeter of the ancient cit...|$|R
