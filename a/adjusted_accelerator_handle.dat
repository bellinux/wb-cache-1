0|33|Public
5000|$|... #Caption: Photo of a {{technician}} <b>adjusting</b> an <b>accelerator</b> build into a gantry ...|$|R
50|$|Typically a {{hardware}} SSL accelerator will offload processing of the SSL handshake while {{leaving it to}} the server software to process the less intense symmetric cryptography of the actual SSL data exchange, but some <b>accelerators</b> <b>handle</b> all SSL operations and terminate the SSL connection, thus leaving the server seeing only unencrypted connections.|$|R
50|$|Sumitomo Heavy Industries, Ltd. (Sumitomo Jūkikai Kōgyō Kabushiki-gaisha) (SHI) is an {{integrated}} manufacturer of industrial machinery, automatic weaponry, ships, bridges and steel structure, equipment for environmental protection, including recycling, power transmission equipment, plastic molding machines, laser processing systems, particle <b>accelerators,</b> material <b>handling</b> systems, cancer diagnostic and treatment equipment and others.|$|R
40|$|Polyester resins {{are being}} {{increasingly}} used in industry. These resins require {{the addition of}} catalysts and <b>accelerators.</b> The <b>handling</b> of polyester resin system materials may give rise to skin irritations, allergic reactions, and burns. The burns are probably due to styrene and organic peroxides. Atmospheric pollution from styrene and explosion and fire risks from organic peroxides must be prevented. Where dimethylaniline is used scrupulous cleanliness and no-touch technique must be enforced. Handling precautions are suggested...|$|R
5000|$|Under the {{guidance}} of the University Athletic audio engineers, the JAR received a major upgrade in the fan atmosphere experience. At construction, the JAR employed a distributed horn powered PA system with very narrow dispersion angles, coupled with near-proximity coaxial speakers for the upper deck. This setup lead to several hot zones and many fans wondering what the announcer said. That changed in 2005 with the installation of a fully zoned system employing EVH model horn loaded boxes coupled with 6 TX Series subwoofers in the South catwalk for that tipoff [...] "Thunderstruck" [...] kick. A separate zone was also included for the floor for the UA Dance Team and Basketball team warmups. Precision series EV combined amplifiers and DSPs tune the system to near perfection in every seat. A Midas Venice series mixing console along with several channels of external dynamic channel processing, as well as enhanced hyperspheric modulation <b>accelerators,</b> <b>handle</b> all audio inputs. The system has reached a maximum decibel level of 115 dB during games, and is rated up to 130 dB. The facility currently seats 5,500 people. The arena has two floors. In addition to the court, the first level features locker rooms, a sports medicine and training facility, a ticket office, a fan team shop and meeting rooms.|$|R
50|$|The P3 family {{processors}} {{share the}} same physical package with, and are also software backwards compatible with, P4 and P5. The P3 processors have 1.3 GHz 64-bit DDR3 memory controllers, 18 SerDes lanes for networking, hardware <b>accelerators</b> for packet <b>handling</b> and scheduling, regular expressions, RAID, security, cryptography and RapidIO.|$|R
40|$|A new,very compact set-up for the {{injection}} of the polar-ized beam at MAMI has been {{realized in the}} last two years. The new injection does not require the integration of a spin-rotator. Longitudinal polarization at the experiments is achieved by <b>adjusting</b> the <b>accelerator</b> energy which re-sults in an additional spin rotation of 3. 9 deg for a rela-tive variation of the MAMI-energy of 104. As a result of the shorter injection we need less beam start-up time and achieve much better long term stability. The emittance of the beam at {{the injection}} point has been reduced by 70 %. ...|$|R
40|$|Abstract — An {{optimized}} rendering algorithm of the OpenVG 2 D {{vector graphics}} for hardware implementation {{is presented in}} this paper. In the rendering algorithm we adopted a hybrid of raster and vector rendering, which uses vector rendering only within each scanline, to reduce both the number of external memory accesses and the computational complexity. We implemented a hardware accelerator with the proposed algorithm. Experimental results show that our hardware <b>accelerator</b> can <b>handle</b> 11. 8 fps of Tiger image for a QVGA panel at the operating clock frequency of 100 MHz. Keywords-OpenVG, 2 D Vector Graphics Hardware Accelerato...|$|R
50|$|The first {{attack on}} this problem was the {{introduction}} of graphics <b>accelerators</b> that <b>handled</b> the texture storage and mapping. These cards, like the original Voodoo Graphics, had the CPU re-calculate the geometry for every frame, and then send the resulting series of co-ordinates to the card. The card then handled {{the rest of the}} operation; applying the textures to the geometry, rendering the frame, applying filtering or anti-aliasing, and outputting the results to a local framebuffer. The bandwidth needs in such a system were dramatically reduced; a scene with 10,000 triangles might need 500 to 1000 kB/s, depending on how many of the geometry points could be shared between triangles.|$|R
2500|$|As {{an example}} of {{negative}} feedback, the diagram might represent a cruise control system in a car, for example, that matches a target speed such as the speed limit. The controlled system is the car; its input includes the combined torque from the engine and from the changing slope of the road (the disturbance). The car's speed (status) is measured by a speedometer. [...] The error signal is {{the departure of the}} speed as measured by the speedometer from the target speed (set point). This measured error is interpreted by the controller to <b>adjust</b> the <b>accelerator,</b> commanding the fuel flow to the engine (the effector). The resulting change in engine torque, the feedback, combines with the torque exerted by the changing road grade to reduce the error in speed, minimizing the road disturbance.|$|R
40|$|In {{this paper}} we propose an {{explicit}} two-level conservative scheme based on a TE/TM like splitting of the field components in time. Its dispersion properties are <b>adjusted</b> to <b>accelerator</b> problems. It is simpler and faster than the implicit version [1]. It does not have dispersion in the longitudinal direction and the dispersion properties in the transversal plane are improved. The explicit character of the new scheme allows a uniformly stable conformal method without iterations and the scheme can be parallelized easily. It assures energy and charge conservation. A version of this explicit scheme for rotationally symmetric structures is free from the progressive time step reducing for higher order azimuthal modes as it takes place for Yee's explicit method used in the most popular electrodynamics codes. Comment: 15 pages, 6 figure...|$|R
50|$|The P5 {{series is}} based on the high {{performance}} 64-bit e5500 core scaling up to 2.5 GHz and allowing numerous auxiliary application processing units as well as multi core operation via the CoreNet fabric. The P5 series processors share the same physical package and are also software backwards compatible with P3 and P4. The P5 processors have 1.3 GHz 64-bit DDR3 memory controllers, 18 SerDes lanes for networking, hardware <b>accelerators</b> for packet <b>handling</b> and scheduling, regular expressions, RAID, security, cryptography and RapidIO.|$|R
40|$|Mobile devices, {{embedded}} sensors, {{and pervasive}} parallelism are changing our computing landscape. I am particularly passionate about {{a vision of}} “liquid computation” that moves effortlessly through wireless and wired networks between sensors, actuators, phones, multicore servers, and <b>accelerators</b> (GPUs, FPGAs), <b>handling</b> this immense heterogeneity and taking advantage of it. Making these systems a reality can require delving into several areas of computer science and engineering. Thus my background and interests are interdisciplinary, spanning sensor networks, parallelizing compilers, embedded operating systems, and domain specific languages...|$|R
40|$|This paper {{presents}} {{an overview of}} the x-ray guided robotic radiosurgery system that has been developed for the ablation of solid tumors. A robot mounted linear accelerator is directed through a sequence of positions and orientations designed to deliver high radiation dosages focussed at a specific location. Patient movement during treatment is identified by stereo x-ray measurements and the robotic system <b>adjusts</b> the linear <b>accelerator</b> prior to the delivery of radiation at each location. The result is accurate delivery without rigid fixation of the tumor relative to the treatment system...|$|R
30|$|As a {{motivator}} for {{the case}} study described in this section the reader is urged to consider the system level challenges of a commercial application such as Netflix [29]. Although an application such as Netflix in its infancy may {{start out with a}} fully customizable computing cluster environment, as its customer base and data requirements expand, a full warehouse data-centre infrastructure is often inevitable [30]. In this scenario, customized acceleration for the encoding required for all standards of input video sources, and the output video stream resolutions produced, will be challenging. Therefore, cloud-based solutions are often sought to handle the scale and possible automation of the encoding workloads [31]. It is precisely this sort of environment that our solution aims to target. In this environment, the flexibility to scale the <b>accelerators</b> that <b>handle</b> the compute bound portions of the application’s code can be leveraged to manage IO constraints if, and only if, the IO interface architecture is appropriately designed and matched.|$|R
40|$|Abstract—We {{propose the}} {{high-level}} synthesis of an FPGA-based hybrid computing system, where the implementations of compute-intensive functions {{are available in}} both software, and as hardware accelerators. The accelerators are optimized to handle common-case inputs, as opposed to worst-case inputs, allowing accelerator area to be reduced by 28 %, on average, while retaining the majority of performance advantages associated with a hardware versus software implementation. When inputs exceed the range that the hardware <b>accelerators</b> can <b>handle,</b> a software fallback is automatically triggered. Optimization of the accelerator area is achieved by reducing datapath widths based on application profiling of variable ranges in software (under typical datasets). The selected widths are passed to a high-level synthesis tool which generates the accelerator for a given function. The optimized accelerators with software fallback capability are generated automatically by our framework, with minimal user intervention. Our study explores the trade-offs of delay and area for benchmarks implemented on an Altera Cyclone II FPGA. I...|$|R
40|$|Acceleration is a {{technique}} of improving performance of general-purpose processor through employing dedicated hardware units (coprocessors, <b>accelerators)</b> to <b>handle</b> some application specific tasks. Even though the acceleration proved its efficiency {{in a number of}} applications (e. g. floating-point coprocessors, graphical accelerators), the issues were encountered that made its benefits doubtful. The main problem was keeping the coprocessor busy most of the time – if the coprocessor is used rarely, investment in its design and maintenance becomes meaningless. This fact and the increasing performance of processors themselves resulted to abandoning the acceleration technique. However, its idea was revived with the increasing importance of configurable devices, especially FPGAs. Although the performance of the configurable devices is generally lower than of ASICs, it has still proven to be superior over the processors in a field of computation intensive applications (e. g. digital signal processing, image processing, cryptography, etc.). Combination of their performance with a possibility of reconfiguration makes the configurable devices an ideal platform for algorithm acceleration. Early works tried to accelerate systems based on conventional personal computer. Th...|$|R
40|$|The Jülich Electric Dipole Moment Investigation (JEDI) Collaboration {{works on}} a {{measurement}} of the electric dipole moment (EDM) of charged hadrons using a storage ring. Such a dipole moment would violate CP symmetry, providing a test for physics beyond the Standard Model. To measure the EDM in a magnetic storage ring, the precession of the spin in the ring {{has to be kept}} in phase with an RF Wien Filter that manipulates the spin. In fall 2015 an active feedback system that meets this requirement was successfully tested at COSY. The system works by <b>adjusting</b> the <b>accelerator</b> frequency, which changes the beam velocity and therefore the rate of spin precession. Data from the polarimeter EDDA are analyzed over a period of about one second to determine the relative phase between the spin precession and the external frequency, which is used to calculate the necessary correction. In absence of a Wien filter an RF solenoid coil was used as a spin manipluator in the tests. The test of the feedback system proofs that the method is suitable for a proof of principle experiment for EDM measurements at COSY...|$|R
40|$|We {{develop a}} Segmented Secondary Emission Monitor (SSEM) as a beam profile monitor for the T 2 K {{experiment}}. The T 2 K experiment is a next generation long baseline neutrino oscillation experiment, {{based on the}} high intensity narrow band neutrino beam produced by the high intensity and high power primary proton beam in the J-PARC proton synchrotron <b>accelerator.</b> To <b>handle</b> the high power proton beam, precise and stable beam control is required. SSEM measures the beam size as well as position of the primary proton beam. The requirements for SSEM are the beam size resolution of better than 3. 5 %, the beam position resolution of ∼ 0. 25 mm, long term gain stability, radiation hardness, to equip with a moving mechanism (∼ 0. 1 mm positioning accuracy) and the proper performance under cryogenic temperature of ∼ 80 K. We design SSEM to satisfy the above requirements. To check the performance, the beam test in the KEK PS line, cryogenic tests, irradiation tests and simulation studies are carried out. We confirm that our design satisfies the requirements. We also design the specification of signal readout devices and confirm the sufficient performance of the prototype...|$|R
5000|$|Both the 300SE and 300SEL {{came with}} the M189 2996 cm³ engine, {{originally}} developed for the Adenauers. It had a modern six-plunger pump that <b>adjusted</b> automatically to <b>accelerator</b> pedal pressure, engine speed, atmospheric pressure, and cooling water temperature, to deliver the proper mixture depending on driving conditions. Producing [...] at 5,400 rpm the cars could accelerate to 200 km/h (195 km/h with automatic transmission) and reach 100 km/h in 12 seconds. The cylinder capacity of the three litre Mercedes engine was unchanged since 1951. From 1965 to 1967, fewer than 3,000 W109s were produced. However, approximately 130,000 of the less powerful 250 S/SE models were built {{during the first two}} years of the W108/109's existence. By 1967 the fuel consumption of the 3 litre unit in this application was becoming increasingly uncompetitive.|$|R
50|$|Most current ADS designs {{propose a}} high-intensity proton {{accelerator}} with an energy of about 1 GeV, directed towards a spallation target or spallation neutron source. The source {{located in the}} heart of the reactor core contains liquid metal which is impacted by the beam, thus releasing neutrons and is cooled by circulating the liquid metal such as lead-bismuth towards a heat exchanger. The nuclear reactor core surrounding the spallation neutron source contains the fuel rods, the fuel being preferably Thorium. Thereby, for each proton intersecting the spallation target, an average of 20 neutrons is released which fission the surrounding fissile part of the fuel and enrich the fertile part. The neutron balance can be regulated or indeed shut off by <b>adjusting</b> the <b>accelerator</b> power so that the reactor would be below criticality. The additional neutrons provided by the spallation neutron source provide the degree of control as do the delayed neutrons in a conventional nuclear reactor, the difference being that spallation neutron source-driven neutrons are easily controlled by the accelerator. The main advantage is inherent safety. A conventional nuclear reactor's nuclear fuel possesses self-regulating properties such as the Doppler effect or void effect, which make these nuclear reactors safe. In addition to these physical properties of conventional reactors, in the subcritical reactor, whenever the neutron source is turned off, the fission reaction ceases and only the decay heat remains.|$|R
40|$|Emerging {{processor}} architectures such as GPUs and Intel MICs {{provide a}} huge performance potential for high performance computing. However developing software using these hardware accelerators introduces additional {{challenges for the}} developer such as exposing additional parallelism, dealing with different hardware designs and using multiple development frameworks {{in order to use}} devices from different vendors. The Dynamic Kernel Scheduler (DKS) is being developed in order to provide a software layer between host application and different hardware <b>accelerators.</b> DKS <b>handles</b> the communication between the host and device, schedules task execution, and provides a library of built-in algorithms. Algorithms available in the DKS library will be written in CUDA, OpenCL and OpenMP. Depending on the available hardware, the DKS can select the appropriate implementation of the algorithm. The first DKS version was created using CUDA for the Nvidia GPUs and OpenMP for Intel MIC. DKS was further integrated in OPAL (Object-oriented Parallel Accelerator Library) to speed up a parallel FFT based Poisson solver and Monte Carlo simulations for particle matter interaction used for proton therapy degrader modeling. DKS was also used together with Minuit 2 for parameter fitting, where χ^ 2 and max-log-likelihood functions were offloaded to the hardwareaccelerator. The concepts of the DKS, first results together with plans for the future will be shown in this paper. Comment: 21 pages, 8 figures, 3 table...|$|R
40|$|This paper {{presents}} an extension to a measurement system to capture bidirectional refEectance distribution function (BRDF) values and texture maps out of images. The extension {{will enable the}} measurement of reflection effects due to surface roughness and additionally extracts a map representing local surface displacements. Different lighting conditions where used to discriminate between surface roughness and bump mapping which both result in variations of gathered reflected intensity. Experimental results will show {{the potential of the}} measurement system to capture complex reflectance characteristics, texture as well as bump maps from a set of few images. Local geometric deformations of the samples are therefore not restricted to a microscopic scale and samples with much coarser variations in surface geometry can be captured. The resulting material description has an important impact on computer graphics applications targeted on photorealism. Using recent graphics <b>accelerators</b> which can <b>handle</b> per-pixel lighting and bump mapping even photorealistic realtime renderings are feasible. ...|$|R
40|$|Jefferson Lab {{contributes}} to the Department of Energy mission to develop and operate major cutting-edge scientific user facilities. Jefferson Lab's CEBAF (Continuous Electron Beam Accelerator Facility) is a unique tool for exploring the transition between the regime where strongly interacting (nuclear) matter {{can be understood as}} bound states of protons and neutrons, and the regime where the underlying fundamental quark-and-gluon structure of matter is evident. The nature of this transition is at the frontier of the authors understanding of matter. Experiments proposed by 834 scientists from 146 institutions in 21 countries await beam time in the three halls. The authors user-customers have been delighted with the quality of the data they are obtaining. Driven by their expressed need for energies higher than the 4 GeV design energy and on the outstanding performance of their novel superconducting accelerator, the laboratory currently delivers beams at 5. 5 GeV and expects to deliver energies approaching 6 GeV for experiments in the near future. Building on the success of Jefferson Lab and continuing to deliver value for the nation's investment is the focus of Jefferson Lab's near-term plans. The highest priority for the facility is to execute its approved experimental program to elucidate the quark structure of matter. The Lab plans to participate in the Strategic Simulation Initiative and benefit from the scientific opportunities that it affords. Initially, the lab will contribute its expertise in simulations for nuclear theory and <b>accelerators,</b> data <b>handling,</b> and distributed systems. As part of its SSI activities, the lab is planning to enhance its expertise in lattice QCD and simulations of photon-driven materials and chemical processes...|$|R
40|$|AbstractTwo {{goals for}} {{security}} scanning of cargo and freight are {{the ability to}} determine the type of material that is being imaged, {{and to do so}} at low radiation dose. One commonly used technique to determine the effective Z of the cargo is dual-energy imaging, i. e. imaging with different x-ray energy spectra. Another technique uses the fact that the transmitted x-ray spectrum itself also depends on the effective Z. Spectroscopy is difficult because the energy of individual x rays needs to be measured in a very high count-rate environment. Typical accelerators for security applications offer large but short bursts of x-rays, suitable for current-mode integrated imaging. In order to perform x-ray spectroscopy, a new accelerator design is desired that has the following features: 1) increased duty factor in order to spread out the arrival of x-rays at the detector array over time; 2) x-ray intensitymodulation from one delivered pulse to the next by <b>adjusting</b> the <b>accelerator</b> electron beam instantaneous current so as to deliveradequate signal without saturating the spectroscopic detector; and 3) the capability to direct the (forward peaked) x-ray intensity towards high-attenuation areas in the cargo (“fan-beam-steering”). Current sources are capable of 0. 1 % duty factor, although usually they are operated at significantly lower duty factors (∼ 0. 04 %), but duty factors in the range 0. 4 - 1. 0 % are desired. The higher duty factor can be accomplished, e. g., by moving from 300 pulses per second (pps) to 1000 pps and/or increasing the pulse duration from a typical 4 μs to 10 μs. This paper describes initial R&D to examine cost effective modifications that could be performed on a typical accelerator for these purposes, as well as R&D for fan-beam steering...|$|R
40|$|The {{stochastic}} {{simulation of}} large-scale biochemical reaction networks {{is of great}} importance for systems biology since it enables the study of inherently stochastic biological mechanisms at the whole cell scale. Stochastic Simulation Algorithms (SSA) allow us to simulate the dynamic behavior of complex kinetic models, but their high computational cost makes them very slow for many realistic size problems. We present a pilot service, named WebStoch, developed {{in the context of}} our StochSoCs research project, allowing life scientists with no high-performance computing expertise to perform over the internet stochastic simulations of large-scale biological network models described in the SBML standard format. Biomodels submitted to the service are parsed automatically and then placed for parallel execution on distributed worker nodes. The workers are implemented using multi-core and many-core processors, or FPGA <b>accelerators</b> that can <b>handle</b> the simulation of thousands of stochastic repetitions of complex biomodels, with possibly thousands of reactions and interacting species. Using benchmark LCSE biomodels, whose workload can be scaled on demand, we demonstrate linear speedup and more than two orders of magnitude higher throughput than existing serial simulators. Comment: The 2017 International Conference on High Performance Computing & Simulation (HPCS 2017), 8 page...|$|R
40|$|International audienceTo {{increase}} software performance, {{it is now}} {{common to}} use hardware accelerators. Currently, GPUs are the most widespread <b>accelerators</b> that can <b>handle</b> general computations. This requires to use GPGPU frameworks such as Cuda or OpenCL. Both are very low-level and make the benefit of GPGPU programming difficult to achieve. In particular, they require to write programs as a combination of two subprograms, and, to manually manage devices and memory transfers. This increases {{the complexity of the}} overall software design. The idea we develop in this paper is to guarantee expressiveness and safety for CPU and GPU computations and memory managements with high-level data-structures and static type-checking. In this paper, we present how statically typed languages, compilers and libraries help harness high level GPGPU programming. In particular, we show how we added high-level user-defined data structures to a GPGPU programming framework based on a statically typed programming language: OCaml. Thus, we describe the introduction of records and tagged unions shared between the host program and GPGPU kernels described via a domain specific language as well as a simple pattern matching control structure to manage them. Examples, practical tests and comparisons with state of the art tools, show that our solutions improve code design, productivity, and safety while providing a high level of performance...|$|R
40|$|In {{recent years}} deep {{learning}} algorithms have shown extremely high performance on machine learning {{tasks such as}} image classification and speech recognition. In support of such applications, various FPGA accelerator architectures have been proposed for convolutional neural networks (CNNs) that enable high performance for classification tasks at lower power than CPU and GPU processors. However, to date, {{there has been little}} research on the use of FPGA implementations of deconvolutional neural networks (DCNNs). DCNNs, also known as generative CNNs, encode high-dimensional probability distributions and have been widely used for computer vision applications such as scene completion, scene segmentation, image creation, image denoising, and super-resolution imaging. We propose an FPGA architecture for deconvolutional networks built around an <b>accelerator</b> which effectively <b>handles</b> the complex memory access patterns needed to perform strided deconvolutions, and that supports convolution as well. We also develop a three-step design optimization method that systematically exploits statistical analysis, design space exploration and VLSI optimization. To verify our FPGA deconvolutional accelerator design methodology we train DCNNs offline on two representative datasets using the generative adversarial network method (GAN) run on Tensorflow, and then map these DCNNs to an FPGA DCNN-plus-accelerator implementation to perform generative inference on a Xilinx Zynq- 7000 FPGA. Our DCNN implementation achieves a peak performance density of 0. 012 GOPs/DSP...|$|R
40|$|SDR {{applications}} are often stream processing applications that are computationally intensive {{which results in}} a low throughput on homogeneous multi-core architectures and thus could benefit significantly {{from the use of}} stream processing accelerators. The integration of stream processing accelerators in an architecture is often facilitated by a NoC. Crossbars or mesh-based NoCs provide guaranteed throughput but tend to have unacceptably high hardware costs. We propose a low-cost heterogeneous multi-processor architecture for real-time stream processing applications together with dataflow models for real-time analysis. This architecture allows compositional temporal dataflow analysis based on independently characterized components. The proposed architecture contains a low-cost ring-shaped interconnect which provides all-to-all guaranteed throughput communication while being work-conserving. Furthermore, cost-effective integration of stream processing accelerators is enabled by combining two low-cost rings and using a small shell in each NI, thereby realizing credit-based hardware flow control for accelerators. To improve the utilization of stream processing accelerators, we propose a sharing approach to multiplex multiple real-time streams of data over accelerators. Data streams between tasks are transferred using our dual-ring interconnect. Software tasks communicate directly using our distributed software FIFO implementation while communication involving stream processing <b>accelerators</b> is <b>handled</b> by our hardware credit-based flow control. In order to reason about the worst-case behavior of our architecture, temporal dataflow models are constructed to obtain bounds on throughput and latency. Three case studies have been carried out to evaluate the hardware costs and performance of the proposed architecture. For these case studies, several instances of the proposed architecture have been implemented on a Xilinx Virtex- 6 FPGA. We show that in our architecture the use of accelerators improves maximum throughput by 366 % and sharing accelerators can reduce hardware costs over 63 %. The results from our case studies show that our ring interconnect has a very small hardware cost and performs within the bounds derived by our dataflow analysis models. We conclude that a considerable reduction of hardware costs can be attained by replacing traditional interconnects by our dual communication ring interconnect. We also conclude that cost-effective shared accelerator integration can improve application performance which demonstrates the merit of our approach...|$|R
40|$|The {{interest}} in {{electric dipole moment}} (EDM) experiments is highly motivated by the problem of matter-antimatter asymmetry in our universe. New sources of CP-violation are needed to explain that phenomenon properly. An EDM of an elementary particle is a perfect candidate to search for these sources because its existence requires CP-violation beyond the Standard Model to be detected. New experiments for the EDM of charged hadrons are proposed. These experiments require {{a new type of}} storage ring to be built. Since an EDM could be as small as 10 - 29 e·cm, a fantastic precision should be achieved. The main cause that limits a potential sensitivity of future experiments are systematic errors. This thesis investigates possible ways to minimize various systematic errors for two versions of a new storage ring and for the precursor experiment, which will be performed by the JEDI (Jülich Electric Dipole moments Investigations) collaboration at the existing Cooler Synchrotron COSY. To study the impact of the systematic errors a large number of spin-orbit tracking simulations were performed in the newly developed program MODE. Two approaches for using a new storage ring were studied: the frozen and the quasi-frozen spin method. In addition, the precursor experiment at COSY was studied. The results of a test run conducted in 2014 made possible to benchmark and <b>adjust</b> the <b>accelerator</b> model and improve the simulation environment. One of the main quantities that defines the sensitivity is the spin decoherence, which takes place at any storage ring. The finite size of the bunch in all three directions, radial, vertical and longitudinal causes the particles’ spins to decohere. Using an RF cavity and a combination of sextupoles allows one to maximize the time during which the spins stay parallel {{to each other in the}} horizontal plane. The main source of systematic error is the misalignment of the elements inside the ring. For a dedicated storage ring, it was proposed to launch two beams in opposite directions (clockwise and counter-clockwise) to average out its impact. For the precursor experiment, the frequency mismatch between an RF Wien filter device that will be used and the frequency of the spin rotation is harmful. All error sources were thoroughly studied and the sensitivity limits were calculated. The EDM limit, which can currently be reached on the future experiments, is of the order of 10 $^{- 25 }$ ─ 10 $^{- 26 }$ e·cm. With the present situation at COSY, the accuracy of the precursor experiment is expected to be of the order of 10 $^{- 19 }$ e·cm...|$|R
40|$|High {{performance}} computing (HPC) platforms are evolving to more heterogeneous configurations {{to support the}} workloads of various applications. The current hardware landscape is composed of traditional multicore CPUs equipped with hardware <b>accelerators</b> that can <b>handle</b> high levels of parallelism. Graphical Processing Units (GPUs) are popular high performance hardware accelerators in modern supercomputers. GPU programming has a different model than that for CPUs, which means that many numerical kernels have to be redesigned and optimized specifically for this architecture. GPUs usually outperform multicore CPUs in some compute intensive and massively parallel applications that have regular processing patterns. However, most scientific applications rely on crucial memory-bound kernels and may witness bottlenecks due to the overhead of the memory bus latency. They can still {{take advantage of the}} GPU compute power capabilities, provided that an efficient architecture-aware design is achieved. This dissertation presents a uniform design strategy for optimizing critical memory-bound kernels on GPUs. Based on hierarchical register blocking, double buffering and latency hiding techniques, this strategy leverages the performance {{of a wide range of}} standard numerical kernels found in dense and sparse linear algebra libraries. The work presented here focuses on matrix-vector multiplication kernels (MVM) as repre- sentative and most important memory-bound operations in this context. Each kernel inherits the benefits of the proposed strategies. By exposing a proper set of tuning parameters, the strategy is flexible enough to suit different types of matrices, ranging from large dense matrices, to sparse matrices with dense block structures, while high performance is maintained. Furthermore, the tuning parameters are used to maintain the relative performance across different GPU architectures. Multi-GPU acceleration is proposed to scale the performance on several devices. The performance experiments show improvements ranging from 10...|$|R
40|$|The Large Hadron Collider (LHC) at CERN (The European Organisation for Nuclear Research) {{is one of}} {{the largest}} and most {{complicated}} machines envisaged to date. The LHC has been conceived and designed over the course of the last 25 years and represents the cutting edge of accelerator technology with a collision energy of 14 TeV, having a stored beam energy over 100 times more powerful than the nearest competitor. Commissioning of the machine is already nderway and operation with beam is intended for Autumn 2007, with 7 TeV operation expected in 2008. The LHC is set to answer some of the fundemental questions in theoretical physics, colliding particles with such high energy that the inner workings of the quantum world can be revealed. Colliding particles together at such high energy makes very high demands on machine operation and protection. The specified beam energy requires strong magnetic fields that are made in superconducting dipole magnets, these magnets are kept only around two degrees above absolute zero and there is a high chance of particle impacts causing a magnet to quench, where the magnet becomes normal conducting and has to be switched off before it destroys itself. Losing as little as 10 eâ 8 of the beam into the superconducting magnets will lead to a quench. A loss of 10 eâ 4 of the beam into any part of the machine will cause damage, such as rupturing the machine vacuum, which in the best case results in costly repairs and weeks of downtime, in a worse case the destruction of one or more dipole magnets would mean many weeks of repairs to return the machine to operation. Due to the unprecedented sensitivity of the machine to beam losses, and the high cost of failure, both financially and in terms of inefficiency, a complex Machine Protection System is envisaged, surveilling and diagnosing the operation of the CERN high energy accelerators, ensuring their safe operation. Machine Protection Systems are employed in the LHC, SPS and beam transfer lines, protecting all parts of the <b>accelerator</b> complex that <b>handle</b> beam above damage thresholds. At the heart of each Machine Protection System lies a Beam Interlock System, connecting the many components of the Machine Protection Systems which are located all around the accelerator complex. The LHC is the ultimate application of these protection systems, here the Beam Interlock System is responsible for relaying a command for controlled removal of the beam (Beam Dump) to the LHC Beam Dumping System. The Beam Dumping System is the only part of the accelerator that is capable of withstanding the impact of the full LHC beam without being damaged in the process. The time response of the LHC Beam nterlock System has to be around 100 $/mu$s, to protect against the fastest events which lead to beam losses. Each Beam Interlock System is made from sixteen Beam Interlock Controllers. These are distributed around the 27 km circumference of the machine, one at the left and one at the right of each Insertion Region, each Controller acts as a local concentrator, monitoring up to 14 User System inputs. Fibre optic links join the sixteen Controllers in so-called beam permit loops, making the high-speed, highly dependable backbone of the Beam Interlock System. This thesis is focussed on the conception, design and realisation of a generic Beam Interlock System used to protect the high energy accelerators at CERN. The Beam Interlock System has been designed to provide the LHC and its injector chain, as well as the SPS and its transfer lines, including the CERN Neutrinos to Gran Sasso project, with an unsurpassed level of protection. In every application the stored beam energy is orders of magnitude above the damage thresholds of the machines...|$|R

