12|19|Public
25|$|The loss of {{his hands}} and serious {{diminution}} of his eyesight apparently resulted in Heim acquiring an eidetic, <b>acoustic</b> <b>memory.</b> He was claimed to rarely forget a formula if he heard it recited, and was said to be able to learn a language in a matter of days. He married a former concert singer from Prague in 1950 named Gerda.|$|E
5000|$|... 64, Oratorio for {{actors in}} 64 {{movements}} through the <b>acoustic</b> <b>memory</b> of Living Theatre from J.Cage to now, with Judith Malina, Hanon Reznikov, Ottavia Fusco, GRM - INA, Paris, 2000 ...|$|E
50|$|In 1947, Gordon {{began his}} {{technical}} career at Philco Corporation and later joined the Eckert-Mauchly Computer Corporation, {{where he was}} responsible {{for the development of the}} standard circuits, <b>acoustic</b> <b>memory,</b> supervisory control, and input/output circuits of the first commercial computer, UNIVAC I.|$|E
40|$|Reviews the {{significance}} of acoustics in environmental design, noting that research on the office environment reveals that acoustics is a primary source of dissatisfaction. Problems of the hearing impaired are discussed, and design suggestions are offered. The significance of <b>acoustic</b> images, <b>memories,</b> and perceptions is also considered. Source type: Electronic(1...|$|R
40|$|This {{presentation}} {{will explore}} how {{the practice of}} soundwalking can be a tool for memory retrieval. I ask: How are memories created and remembered in the mind and fe lt within the body? What happens to our perception of self, home, and knowing as we move through spaces and places of significance? I aim to explore the subject of memory and movement {{within the context of}} soundscape studies; these notions require an understanding of how we “hear ” the past and re-evoke our <b>acoustic</b> <b>memories</b> as we move and act through our environment. Traditional methods for recalling the past involve mainly visual cues and focus on materiality—we look to photographs and hold personal objects, etc. —while remaining visual-centered and localized. I suggest that it is the physical act of moving our body through meaningful environments that unifies the senses, places and knowing and that brings together the local past into the present experience (Casey 1987). My main focus is to understand the ways in which people remember—both as individuals and as groups. This presentation explores how the production of memory and act of remembering are evoked during the process of memory walks (or soundwalks) as a way of understanding and engaging with the world...|$|R
40|$|<b>Acoustic</b> {{short-term}} <b>memory</b> (ASTM) {{refers to}} the temporary retention of acoustic information. In the present study, we investigated the neural correlates of ASTM for pitch using magnetoencephalography (MEG) and functional magnetic resonance imag-ing (fMRI). Both MEG and fMRI analyses revealed brain activations that varied with memory load {{in the vicinity of}} secondary auditory cortex (Brodmann’s area, BA 22) and superior parietal cortex (BA 5 / 7), while analyses specific to MEG data revealed load-related activations in the frontal cortex (BA 9 / 10). Key words: auditory; ASTM; pitch; MEG; fMR...|$|R
50|$|The loss of {{his hands}} and serious {{diminution}} of his eyesight apparently resulted in Heim acquiring an eidetic, <b>acoustic</b> <b>memory.</b> He was claimed to rarely forget a formula if he heard it recited, and was said to be able to learn a language in a matter of days. He married a former concert singer from Prague in 1950 named Gerda.|$|E
50|$|These Mudras {{continue}} {{to be part of}} the classical Indian dance tradition. This interplay of the gesture and sound in Sanskrit recital, state Wilke and Moebus, is similar to the gesture of a conductor and the sound produced by music players in any classical orchestra. In Sanskrit, the posture of the performer is an added dimension to those of pronunciation and gesture, together these empowered muscular memory with <b>acoustic</b> <b>memory</b> in the Hindu tradition of remembering and transmitting Sanskrit texts from one generation to the next, state Wilke and Moebus.|$|E
40|$|Two {{experiments}} {{are reported in}} each of which subjects were required to recall spoken consonant-vowel-consonant (CVC) syllable lists serially. Both experiments contrasted {{the effects of a}} rhyming suffix and an alliterative suffix on recall of the terminal list item. A rhyming suffix reliably attenuated the normally robust suffix effect; an alliterative suffix did not. The finding points to the importance of the location rather than the quantity of phonological repetition in determining the size of the suffix effect. In line with Treiman and Danis (1988), it is argued that the onset (C) and rime (VC) components of CVC syllables may exist as separate entites within short-term <b>acoustic</b> <b>memory.</b> This, coupled with the superior durability of the rime component within <b>acoustic</b> <b>memory,</b> affords the subject a greater probability of recalling correctly the terminal list item in the rhyming suffix condition...|$|E
40|$|This paper {{describes}} the RWTH speech recognition system for Arabic. Several design {{aspects of the}} system, including cross-adaptation, multiple system design and combination, are analyzed. We summarize the semi-automatic lexicon generation for Arabic using a statistical approach to grapheme-tophoneme conversion and pronunciation statistics. Furthermore, a novel ASR-based audio segmentation algorithm is presented. Finally, we discuss practical approaches for parallelized <b>acoustic</b> training and <b>memory</b> efficient lattice rescoring. Systematic results are reported on recent GALE evaluation corpora...|$|R
50|$|The next {{significant}} {{advance in}} computer <b>memory</b> came with <b>acoustic</b> delay line <b>memory,</b> developed by J. Presper Eckert {{in the early}} 1940s. Through {{the construction of a}} glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.|$|R
40|$|Recognizing other {{individuals}} by sound {{is a primary}} function of many vertebrate communication systems. Both speech and birdsong provide an auditory "face," allowing individuals to recognize each other without visual contact. Humans and songbirds have brain mechanisms that have evolved to support these behaviors. In this thesis, I present results from three studies investigating the recognition of complex sounds. Sounds are typically {{described in terms of}} four perceptual features: pitch, timbre, loudness and duration. Here we focus on the role of pitch and timbre in auditory recognition. We trained European starlings, a species of songbird, to recognize excerpts of conspecific song and human melodies. Starlings were more flexibly able to recognize song excerpts that had been shifted than shifted melodies, although they were able to learn to recognize both well. This indicates that the use of auditory cues may be stimulus dependent. We then describe a study investigating the interaction of pitch and timbre cues in recognition. Our hypothesis was that adding timbre cues to tone sequences would enable continued recognition even when pitch was altered. However, {{the results of this study}} suggest that songbirds are maintaining <b>acoustic</b> <b>memories</b> closely tied to specific spectral features and are unable to recognize tone sequences based on pitch cues alone. In contrast, human listeners performed well when asked to recognize pitch- altered tone sequences. We then investigated the role of individual differences in humans learning to recognize unfamiliar voices. Individuals who learned a language early in life were able to recognize talkers in that language better than those who learned the language later. Extensive music training also plays a role [...] listeners with more music training learned to recognize voices in an unfamiliar language more quickly than those with less training. Together, these studies contribute a new comparative perspective on the recognition of complex sounds. A description of natural vocalizations consisting of pitch, timbre, loudness and duration is convenient but limiting. The cognitive processes underlying sound recognition are complex, relying on interactions between spectrotemporal features that depend not only on the features of the signal itself, but also on the listener's auditory experienc...|$|R
40|$|To improve speech {{recognition}} applications, designers must understand <b>acoustic</b> <b>memory</b> and prosody. Human-human relationships are rarely a good model for designing effective user interfaces. Spoken language is effective for human-human interaction but often has severe limitations {{when applied to}} human-computer interaction. Speech is slow for presenting information, is transient and therefore difficult to review or edit, and interferes significantly with other cognitive tasks. However, speech has proved useful for store-and-forward messages, alerts in busy environments, and input-output for blind or motor-impaired users. Continued research and development {{should be able to}} improve certain speech input, output, and dialogue applications. Speech recognition and generation is sometimes helpful for environments that are hands-busy, eyes-busy, mobility-required, or hostile and shows promise for telephone-based services...|$|E
40|$|While {{listening}} to music, a significant {{high degree of}} phase synchrony in the γ frequency range globally distributed over the brain was found in subjects with musical training (musicians) compared with subjects with no such training (non-musicians). No {{significant differences were found}} in other EEG frequency bands. Listening to neutral text did not produce any significant differences in the degree of synchronization between these two groups. For musicians, left-hemispheric dominance was found during {{listening to}} music. The right hemisphere was found to be dominant for non-musicians in text listening. The high degree of synchronization in musicians could be due to their high ability to retrieve musical patterns from their <b>acoustic</b> <b>memory,</b> which is a cogent condition for both listening to and anticipating musical sounds...|$|E
40|$|We {{present a}} pilot study which {{integrates}} articulatory information into the Context Sequence Model (CSM) of speech production [1]. The CSM is an exemplar-theoretic model which builds {{on the concept of}} the speech perception—production loop and incorporates a rich <b>acoustic</b> <b>memory</b> of past speech items which are stored sequentially in their original context. In the present study, we enrich the original <b>acoustic</b> <b>memory</b> of the CSM with articulatory information by using continuous Electromagnetic Midsaggital Articulography (EMA) measurements. To our knowledge, there are no existing speech production models which use the full continuous EMA signals directly and {{in the same way as}} acoustic speech signals. In a first series of experiments, we used data from a Polish corpus [2] designed to investigate the coordination between articulatory gestures within syllables in onset and coda positions (particularly the so-called C-Center effect — a distance of the consonants in a cluster with regards to a vowel [3]). The corpus is composed of a set of repeated target words with simple onsets and codas containing single sonorants, as well as onset and coda clusters containing a voiceless stop and a sonorant, embedded into carrier phrases which guarantee identical contexts of tongue movements for all target consonants and clusters. Their structure is as follows (target words are underlined) : onset: “Ona mówi pranie aktualnie ” (“She is saying laundry currently”); coda: “Ona powiedziała Cypr aktualnie ” (“She is saying Cyprus currently”). The database contained speech recordings and EMA measurements from one male and two female native speakers, recorded with a 2 D Electromagnetic Articulograph, Carstens AG 100. The EMA data was sampled at 400 Hz, postprocessed, and manually annotated with phone segments and articulatory landmarks using the EMU Speech Database Syste...|$|E
40|$|This review {{addresses}} {{recent advances}} in polyurethane-based nanocomposites. It focuses on the enhancement of mechanical, electrical, thermal, <b>acoustic,</b> chemical, shape <b>memory,</b> and viscoelastic properties of the existing polyurethane using nanoparticles or fiber materials {{and it is also}} directed to analyze the potential of incorporating these hybrid polymer composites&# 039; applications. Research on hybrid polymer composites has increased in recent years due to the inherence properties of mixing two or more constituents to reinforce the base material properties. From the discussion in this paper, we can see that polyurethane-based nanocomposite can be modified to suit various applications...|$|R
5000|$|... "Neil Young, having tasted {{fame and}} fortune with After the Goldrush and Harvest, famously said he would rather head for the ditch than stay {{in the middle of}} the road. And that's just what he did with Time Fades Away. Young {{recorded}} the stoned, muddy, hard-rocking album on a stadium tour to confused audiences who had never heard the songs before. No atmosphere, no <b>acoustic</b> balladry, just <b>memories</b> of getting a kicking in the schoolyard and an extended moan about LA. Young's profile duly disappeared." [...] —Bob Stanley of The Guardian, talking about the album's release in 2008.|$|R
40|$|We {{report the}} {{fabrication}} and characterization of nanoscale ferroelectric structures consisting of disk-shaped nanomesas averaging 8. 7 ± 0. 4 nm in height and 95 ± 22 nm in diameter, and nanowells 9. 8 ± 3. 3 nm in depth and 128 ± 37 nm in diameter, formed from Langmuir–Blodgett films of vinylidene fluoride copolymers after annealing in the paraelectric phase. The nanomesas retain the ferroelectric {{properties of the}} bulk material and so may be suitable for use in high-density nonvolatile random-access <b>memories,</b> <b>acoustic</b> transducer arrays, or infrared imaging arrays. The nanomesa and nanowell patterns may provide useful templates for nanoscale molding or contact-printing...|$|R
40|$|Includes bibliographical {{references}} (pages 50 - 54) Twenty persons {{participated in}} paired associate training with nine multidimensional stimulus pairs to a criterion of twenty consecutive errorless trials. Ten participants drew visual responses (stick persons) {{in response to}} their associated acoustic properties (tone patterns), and ten participants produced tone patterns {{in response to the}} stick-person stimuli. A Sternberg reaction time task followed, in which the stick persons served as stimuli. Between group differences in reaction time, predicted by the Response Modality Monitoring (RMM) hypothesis, were not found. According to the RMM hypothesis, stimulus encoding in short term memory will be based on properties of the response rather than the properties of the stimulus. Previous research has suggested that this relationship holds only when responses to stimulus require unique motor responses. In accordance with the RMM hypothesis, longer response latencies were expected from persons trained to respond, and hence encode, acoustically, since their encoded <b>acoustic</b> <b>memory</b> traces would not be optimal for making rapid comparisons to a visual probe stimulus. Since the predicted difference in reaction time was not found, the experiment is seen as failing to support the RMM hypothesis. Possible interpretations of the shape of the obtained function relating reaction time to memory set size and type of response are discussed...|$|E
40|$|Human-human {{relationships}} are rarely a good {{model for the}} design of effective user interfaces. Spoken language is effective for human-human interaction (HHI), but it often has severe limitations when applied to human-computer interaction (HCI). Speech is slow for presenting information, it is difficult to review or edit, and it interferes with other cognitive tasks. However speech has proven to be useful for store-and-forward messages, alerts in busy environments, and input-output for blind or motor-impaired users. Speech recognition for control is helpful for hands-busy, eyes-busy, mobilityrequired, or hostile environments and it shows promise for use in telephone-based services. Dictation input is increasingly accurate, but adoption outside the disabled users community has been slow compared to visual interfaces. Obvious physical problems include fatigue from speaking continuously and the disruption in an office filled with people speaking. By understanding the cognitive processes surrounding human <b>acoustic</b> <b>memory</b> and processing, interface designers may be able to integrate speech more effectively and guide users more successfully. Then by appreciating the differences between HHI and HCI designers may be able to choose appropriate applications for human use of speech with computers. The key distinction may be the rich emotional content conveyed by prosody [...] the pacing, intonation, and amplitude in spoken language. Prosody is potent for HHI, but may be disruptive for HCI...|$|E
40|$|Nectins are cell {{adhesion}} molecules that are widely {{expressed in the}} brain. Nectin expression shows a dynamic spatiotemporal regulation, {{playing a role in}} neural migratory processes during development. Nectin- 1 and nectin- 3 and their heterophilic trans-interactions are important for the proper formation of synapses. In the hippocampus, nectin- 1 and nectin- 3 localize at puncta adherentia junctions and {{may play a role in}} synaptic plasticity, a mechanism essential for memory and learning. We evaluated the potential involvement of nectin- 1 and nectin- 3 in memory consolidation using an emotional learning paradigm. Rats trained for contextual fear conditioning showed transient nectin- 1 -but not nectin- 3 -protein upregulation in synapse-enriched hippocampal fractions at about 2 h posttraining. The upregulation of nectin- 1 was found exclusively in the ventral hippocampus and was apparent in the synaptoneurosomal fraction. This upregulation was induced by contextual fear conditioning but not by exposure to context or shock alone. When an antibody against nectin- 1, R 165, was infused in the ventral-hippocampus immediately after training, contextual fear memory was impaired. However, treatment with the antibody in the dorsal hippocampus had no effect in contextual fear memory formation. Similarly, treatment with the antibody in the ventral hippocampus did not interfere with <b>acoustic</b> <b>memory</b> formation. Further control experiments indicated that the effects of ventral hippocampal infusion of the nectin- 1 antibody in contextual fear memory cannot be ascribed to memory non-specific effects such as changes in anxiety-like behavior or locomotor behavior. Therefore, we conclude that nectin- 1 recruitment to the perisynaptic environment in the ventral hippocampus plays an important role in the formation of contextual fear memories. Our results suggest that these mechanisms could be involved in the connection of emotional and contextual information processed in the amygdala and dorsal hippocampus, respectively, thus opening new venues for the development of treatments to psychopathological alterations linked to impaired contextualization of emotions...|$|E
40|$|Previously {{published}} equations for {{the time}} dependence of the echo and reverberation in a Pekeris waveguide are combined with an expression derived for surface-generated noise. These closed form solutions are applied to the calculation of signal to reverberation ratio and signal to total background ratio for three CW pulses with centre frequencies between 250 Hz and 3. 5 kHz. The scenario considered is Problem A 2. I from the ‘Validation of Sonar Performance Assessment Tools’ meeting of the Institute of <b>Acoustics,</b> held in <b>Memory</b> of David E. Weston, in April 2010. This scenario involves the detection of a spherical target in a Pekeris waveguide, {{against a background of}} rain noise and Lambert-rule reverberation from the seafloo...|$|R
40|$|Music {{perception}} involves <b>acoustic</b> analysis, auditory <b>memory,</b> auditory scene analysis, {{processing of}} interval relations, of musical syntax and semantics, and activation of (pre) motor representations of actions. Moreover, music perception potentially elicits emotions, thus {{giving rise to}} the modulation of emotional effector systems such as the subjective feeling system, the autonomic nervous system, the hormonal, and the immune system. Building on a previous article (Koelsch and Siebel, 2005), this review presents an updated model of music perception and its neural correlates. The article describes processes involved in music perception, and reports EEG and fMRI studies that inform about {{the time course of}} these processes, as well as about where in the brain these processes might be located...|$|R
40|$|Music {{perception}} involves complex brain functions underlying <b>acoustic</b> analysis, auditory <b>memory,</b> auditory scene analysis, {{and processing}} of musical syntax and semantics. Moreover, music perception potentially affects emotion, influences the autonomic nervous system, the hormonal and immune systems, and activates (pre) motor representations. During {{the past few}} years, research activities on different aspects of music processing and their neural correlates have rapidly progressed. This article {{provides an overview of}} recent developments and a framework for the perceptual side of music processing. This framework lays out a model of the cognitive modules involved in music perception, and incorporates information about the time course of activity of some of these modules, as well as research findings about where in the brain these modules might be located...|$|R
50|$|The Williams tube {{works by}} {{displaying}} {{a grid of}} dots on a cathode ray tube (CRT). Due to the way CRTs work, this creates a small charge of static electricity over each dot. The charge {{at the location of}} each of the dots is read by a thin metal sheet {{just in front of the}} display. Since the display faded over time, it was periodically refreshed. In comparison to the contemporary <b>acoustic</b> delay line <b>memory,</b> the Williams-Kilburn tube was much faster as the cycling took place at the speed of the electrons inside the vacuum tube, as opposed to the speed of sound within the delay line. However, the system was negatively impacted by any nearby electrical fields, and required constant alignment to keep operational. Williams-Kilburn tubes were used primarily on high-speed computer designs.|$|R
40|$|Working {{memory is}} a limited resource: brains can only {{maintain}} small amounts of sensory input (memory load) over {{a brief period of}} time (memory decay). The dynamics of slow neural oscillations as recorded using magneto- and electroencephalography (M/EEG) provide a window into the neural mechanics of these limitations. Especially oscillations in the alpha range (8 – 13 Hz) are a sensitive marker for memory load. Moreover, according to current models, the resultant working memory load is determined by the relative noise in the neural representation of maintained information. The auditory domain allows memory researchers to apply and test the concept of noise quite literally: Employing degraded stimulus <b>acoustics</b> increases <b>memory</b> load and, at the same time, allows assessing the cognitive resources required to process speech in noise in an ecologically valid and clinically relevant way. The present review first summarizes recent findings on neural oscillations, especially alpha power, and how they reflect memory load and memory decay in auditory working memory. The focus is specifically on memory load resulting from acoustic degradation. These findings are then contrasted with contextual factors that benefit neural as well as behavioral markers of memory performance, by reducing representational noise. We end on discussing the functional role of alpha power in auditory working memory and suggest extensions of the current methodological toolkit...|$|R
40|$|Beneficial {{effects of}} noise on higher {{cognition}} have recently attracted attention. Hypothesizing an {{involvement of the}} mesolimbic dopamine system and its functional interactions with cortical areas, the current study aimed to demonstrate a facilitation of dopamine-dependent attentional and mnemonic functions by externally applying white noise in five behavioral experiments including a total sample of 167 healthy human subjects. During working <b>memory,</b> <b>acoustic</b> white noise impaired accuracy when presented during the maintenance period (experiment 1 - 3). In a reward based long-term memory task, white noise accelerated perceptual judgments for scene images during encoding but left subsequent recognition memory unaffected (experiment 4). In a modified Posner task (experiment 5), the benefit due to white noise in attentional orienting correlated weakly with reward dependence, a personality trait that {{has been associated with}} the dopaminergic system. These results suggest that white noise has no general effect on cognitive functions. Instead, they indicate differential effects on perception and cognition depending on a variety of factors such as task demands and timing of white noise presentation...|$|R
40|$|This study set out {{to examine}} (a) lexical tone and stress {{perception}} by bilingual and monolingual children, (b) interrelationships between lexical pitches perception, general acoustic mechanism and working memory, and (c) the association between lexical tone awareness and Chinese text reading comprehension. Experiment 1 tested and compared the perception of Cantonese lexical tones, English lexical stress and nonlinguistic pitch between Cantonese-English bilingual and English monolingual children. The relationships between linguistic pitch perception, non-linguistic pitch perception and working memory were also examined among Cantonese-English bilingual children. Experiment 2 explored the relationship between Cantonese tone awareness and Chinese text reading comprehension skills. Results of this study illustrate differential performances in tone perception but similar performances in stress perception between bilinguals and monolinguals. In addition, inter-correlations were found between linguistic pitches perception, general <b>acoustic</b> mechanism, working <b>memory</b> and reading comprehension. These findings provide new insight to native and non-native perception of lexical pitches, and demonstrate an important link that exists between lexical tone awareness and reading comprehension. published_or_final_versionSpeech and Hearing SciencesBachelorBachelor of Science in Speech and Hearing Science...|$|R
40|$|A {{monolithic}} ZnO-on-silicon surface <b>acoustic</b> wave (SAW) <b>memory</b> correlator {{has been}} fabricated which utilizes induced junctions separated by ion implanted regions to shore a reference signal. The performance characteristics of this device {{have been investigated}} including storage time, dynamic range, and degenerate convolution efficiency. Verification {{of the existence of}} charge storage regions is possible prior to completed device fabrication. ^ A theory explaining the charge storage process is developed and applied to the implant-isolated storage correlator. The implant-isolated correlator theory is applied to related structures which employ slightly different storage mechanisms. The ion implanted correlator is used to determine the wave potential associated with a propagating SAW. ^ Characteristics of ZnO-on-Si SAW resonators with sputtered ZnO films limited to the interdigital transducer (IDT) regions are investigated. Upper limits on propagation loss for surface waves on silicon substrates are determined by employing externally coupled limited ZnO SAW resonators. Resonator Q-values are enhanced by restricting the lossy ZnO area and predictions are made as to achievable Q-values for resonators fabricated in the externally coupled configuration. Experimental results for limited ZnO, internally coupled ZnO-on-Si resonators are also given. ^ A complete theory for the mode conversion resonator is presented which predicts the array separation for proper device operation. The theory also gives way to a special condition for spatial independence of resonator output with respect to IDT placement. Mode conversion resonators are fabricated which experimentally verify these predictions. ...|$|R
40|$|Two {{experiments}} critically re-examine {{the finding}} of Campbell and Dodd (1984, Experiment 2), which suggests that irrelevant speech disrupts the encoding of visual material for serial recall. Support is sought for the competing view {{that the effect of}} irrelevant speech is on storage by comparing the effect of a range of <b>acoustic</b> conditions on <b>memory</b> for graphic and lip-read lists. Initially, serial short-term recall of visually presented lists was examined with irrelevant speech that was both asynchronous with the visually presented items and of varied speech content (Experiment 1 a). In this experiment substantial impairments in recall of both graphic and lip-read lists were found. However, with unvarying asynchronous speech (Experiment 1 b) the effect of speech was small and non-significant. Experiment 2 examined the effect of changing state and of synchrony of speech with lip movements. When conditions of synchronous and asynchronous unvarying speech were contrasted, no significant effect of synchrony or irrelevant speech was found (Experiment 2 a and 2 c). In contrast, when the speech was varying in content, a strong effect of irrelevant speech was found; moreover, the effect was roughly the same for synchronous and asynchronous materials (Experiment 2 b). The contrast in outcome with varying and unvarying speech provides strong support for the “changing state” model of the irrelevant speech effect. Coupled with the absence of an effect of synchrony in Experiment 2, these experiments reinforce the view that disruption by irrelevant speech occurs in memory, not at encoding...|$|R
40|$|Using a within {{subjects}} design, we documented morphological, bioacoustical {{and behavioral}} developmental changes in big brown bats. Eptesicus fuscus pups are born naked and blind but assume an adult-like appearance by post-natal day (PND) 45 and flight by PND 30. Adult females use spatial <b>memory,</b> <b>acoustic</b> and olfactory cues to reunite with offspring, {{but it is}} unclear if pups can recognize maternal scents. We tested the olfactory discrimination abilities of young E. fuscus pups and found they exhibited no odor preferences. Pups also emit distinct vocalizations called isolation calls (i-calls) that facilitate mother-offspring reunions, but how pups shift their vocalizations from i-calls to downward frequency modulated (FM) sweeps used in echolocation remains unclear. Between PND 0 – 9, pups emitted mainly long duration, tonal i-calls rich in harmonics, but after they switched to short duration, downward FM sweeps with fewer harmonics. Call maximum frequency and repetition rate showed minor changes across development. Signal duration, bandwidth, and number of harmonics decreased, whereas the maximum, minimum and bandwidth of the fundamental, and peak spectral frequency all increased. We recorded vocalizations during prolonged maternal separation and found that isolated pups called longer and at a faster rate, presumably to signal for maternal assistance. To assess how PND 13 pups alter their signals during interactions with humans we compared spontaneous and provoked vocalizations and found that provoked calls were spectrally and temporally more {{similar to those of}} younger bats suggesting that pups in distress emit signals that sound like younger bats to promote maternal assistance...|$|R

