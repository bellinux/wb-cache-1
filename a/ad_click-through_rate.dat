4|256|Public
40|$|We {{study the}} {{trade-off}} between layout {{elements of the}} search results page and revenue in the real-time sponsored search auction. Using data from a randomized experiment on a major search engine, we find that having images present among the search results tends to simultaneously raise the <b>ad</b> <b>click-through</b> <b>rate</b> and flatten the ad click curve, reducing the premium for occupying the top slot and thus impacting bidding incentives. Theoretical analysis shows {{that this type of}} change creates an ambiguous impact on revenue in equilibrium: a steeper curve with lower total click-through rate is preferable only if the expected revenue distribution is skewed enough towards the top bidder. Empirically, we show that this is a relatively rare phenomenon, and we also find that whole page satisfaction causally raises the click-through rate of the ad block. This means search engines have a short-run incentive to boost search result quality, not just a long-run incentive based on competition between providers...|$|E
40|$|Behavioral Targeting (BT), {{which aims}} to deliver the most {{appropriate}} advertisements to the most appropriate users, is attracting much attention in online advertising market. A key challenge of BT is how to automatically segment users for ads delivery, and good user segmentation may significantly improve the <b>ad</b> <b>click-through</b> <b>rate</b> (CTR). Different from classical user segmentation strategies, which rarely take the semantics of user behaviors into consideration, we propose in this paper a novel user segmentation algorithm named Probabilistic Latent Semantic User Segmentation (PLSUS). PLSUS adopts the probabilistic latent semantic analysis to mine the relationship between users and their behaviors so as to segment users in a semantic manner. We perform experiments on the real world ad click through log of a commercial search engine. Comparing {{with the other two}} classical clustering algorithms, K-Means and CLUTO, PLSUS can further improve the ads CTR up to 100 %. To our best knowledge, this work is an early semantic user segmentation study for BT in academia...|$|E
40|$|Deep {{learning}} gains lots of attentions {{in recent}} years and is more and more important for mining values in big data. However, to make deep learning practical {{for a wide range of}} applications in Tencent Inc., three requirements must be considered: 1) Lots of computational power are required to train a practical model with tens of millions of parameters and billions of samples for products such as automatic speech recognition (ASR), and the number of parameters and training data is still growing. 2) The capability of training larger model is necessary for better model quality. 3) Easy to use frameworks are valuable to do many experiments to perform model selection, such as finding an appropriate optimization algorithm and tuning optimal hyper-parameters. To accelerate training, support large models, and make experiments easier, we built Mariana, the Tencent deep learning platform, which utilizes GPU and CPU cluster to train models parallelly with three frameworks: 1) a multi-GPU data parallelism framework for deep neural networks (DNNs). 2) a multi-GPU model parallelism and data parallelism framework for deep convolutional neural networks (CNNs). 3) a CPU cluster framework for large scale DNNs. Mariana also provides built-in algorithms and features to facilitate experiments. Mariana is in production usage for more than one year, achieves state-of-the-art acceleration performance, and plays a key role in training models and improving quality for automatic speech recognition and image recognition in Tencent WeChat, a mobile social platform, and for <b>Ad</b> <b>click-through</b> <b>rate</b> prediction (pCTR) in Tencent QQ, an instant messaging platform, and Tencent Qzone, a social networking service...|$|E
40|$|Etsy is {{a global}} {{marketplace}} where people across the world connect to make, buy and sell unique goods. Sellers at Etsy can promote their product listings via advertising campaigns similar to traditional sponsored search <b>ads.</b> <b>Click-Through</b> <b>Rate</b> (CTR) prediction {{is an integral part}} of online search advertising systems where it is utilized as an input to auctions which determine the final ranking of promoted listings to a particular user for each query. In this paper, we provide a holistic view of Etsy's promoted listings' CTR prediction system and propose an ensemble learning approach which is based on historical or behavioral signals for older listings as well as content-based features for new listings. We obtain representations from texts and images by utilizing state-of-the-art deep learning techniques and employ multimodal learning to combine these different signals. We compare the system to non-trivial baselines on a large-scale real world dataset from Etsy, demonstrating the effectiveness of the model and strong correlations between offline experiments and online performance. The paper is also the first technical overview to this kind of product in e-commerce context...|$|R
40|$|Predicting <b>ad</b> <b>click–through</b> <b>rates</b> (CTR) is a massive-scale {{learning}} {{problem that}} {{is central to the}} multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal {{of this paper is to}} highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system...|$|R
40|$|Abstract—How can we detect {{suspicious}} {{users in}} large online networks? Online popularity of a user or product (via follows, page-likes, etc.) can be monetized {{on the premise}} of higher <b>ad</b> <b>click-through</b> <b>rates</b> or increased sales. Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking {{to make a quick}} buck. Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent (but sometimes honest) users. However, small-scale, stealthy attacks may go unnoticed {{due to the nature of}} low-rank eigenanalysis used in practice. In this work, we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods and propose FBOX, an algorithm designed to catch small-scale, stealth attacks that slip below the radar. Our algorithm has the following desirable properties: (a) it has theoretical underpinnings, (b) it is shown to be highly effective on real data and (c) it is scalable (linear on the input size). We evaluate FBOX on a large, public 41. 7 million node, 1. 5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day. I...|$|R
40|$|Displaying banner {{advertisements}} (in short, ads) on webpages has {{usually been}} discussed as an Internet economics topic where a publisher uses auction models to sell an online user's page view to advertisers {{and the one}} with the highest bid can have her ad displayed to the user. This is also called real-time bidding (RTB) and the ad displaying process ensures that the publisher's benefit is maximized or there is an equilibrium in ad auctions. However, the benefits of the other two stakeholders [...] the advertiser and the user [...] have been rarely discussed. In this paper, we propose a two-stage computational framework that selects a banner ad based on the optimized trade-offs among all stakeholders. The first stage is still auction based and the second stage re-ranks ads by considering the benefits of all stakeholders. Our metric variables are: the publisher's revenue, the advertiser's utility, the ad memorability, the <b>ad</b> <b>click-through</b> <b>rate</b> (CTR), the contextual relevance, and the visual saliency. To the best of our knowledge, this is the first work that optimizes trade-offs among all stakeholders in RTB by incorporating multimedia metrics. An algorithm is also proposed to determine the optimal weights of the metric variables. We use both ad auction datasets and multimedia datasets to validate the proposed framework. Our experimental results show that the publisher can significantly improve the other stakeholders' benefits by slightly reducing her revenue in the short-term. In the long run, advertisers and users will be more engaged, the increased demand of advertising and the increased supply of page views can then boost the publisher's revenue. Comment: In Proceedings of the 40 th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) 2017, 10 page...|$|E
40|$|This paper {{presents}} {{models for}} predicted <b>click-through</b> <b>rates</b> in position auctions that {{take into account}} two possibilities that are not normally considered [...] -that the identities of ads shown in other positions may affect the probability that an ad in a particular position receives a click (externalities) and that some ads may be less adversely affected by being shown in a lower position than others (brand effects). We present a general axiomatic methodology for how click probabilities {{are affected by the}} qualities of the ads in the other positions, and illustrate that using these axioms will increase revenue as long as higher quality ads tend to be ranked ahead of lower quality ads. We also present appropriate algorithms for selecting the optimal allocation of <b>ads</b> when predicted <b>click-through</b> <b>rates</b> are governed by either the models of externalities or brand effects that we consider. Finally, we analyze the performance of a greedy algorithm of ranking the ads by their expected cost-per- 1000 -impressions bids when the true <b>click-through</b> <b>rates</b> are governed by our model of predicted <b>click-through</b> <b>rates</b> with brand effects and illustrate that such an algorithm will potentially cost as much as half of the total possible social welfare. Comment: 19 pages. A shorter version of this paper will appear in WINE 201...|$|R
40|$|In {{the last}} decade new ways of {{shopping}} online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. The two main reasons are: firstly, a person's buying choices are influenced by psychological factors like impulsiveness, and secondly, some consumers may be more susceptible to making impulse purchases than others. To {{the best of our}} knowledge, the impact of personality factors on advertisements has been largely neglected at the level of recommender systems. This work proposes a highly innovative research which uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. As a matter of fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of state-of-the-art algorithms. We present the ADS Dataset, a publicly available benchmark for computational advertising enriched with Big-Five users' personality factors and 1, 200 personal users' pictures. The proposed benchmark allows two main tasks: rating prediction over 300 real advertisements (i. e., Rich Media Ads, Image Ads, Text <b>Ads)</b> and <b>click-through</b> <b>rate</b> prediction. Moreover, this work carries out experiments, reviews various evaluation criteria used in the literature, and provides a library for each one of them within one integrated toolbox. Comment: This paper is an overview of Personality in Computational Advertising: A Benchmark, G. Roffo, ACM RecSys workshop on Emotions and Personality in Personalized Systems, (EMPIRE 2016...|$|R
40|$|There {{has been}} signicant recent {{interest}} in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing {{measures such as}} average bid, average ad position, total impressions, clicks and cost for each keyword in the advertiser's campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intra-day variation in ad position. We show that estimating random utility models on ag-gregated (daily) data without accounting for this variation will lead to systematically biased estimates { specically, the impact of <b>ad</b> position on <b>click-through</b> <b>rate</b> (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We demonstrate the existence of the bias analytically and show the eect of the bias on the equilibrium of the SSA auction. Using a large dataset from a major search engine, we measure the magnitude of bias and quantify the losses suered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11 % due to aggregation bias. We also present a few dat...|$|R
40|$|There {{has been}} {{significant}} recent interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing {{measures such as}} average bid, average ad position, total impressions, clicks and cost for each keyword in the advertiser 2 ̆ 7 s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intra-day variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates [...] specifically, the impact of <b>ad</b> position on <b>click-through</b> <b>rate</b> (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We demonstrate the existence of the bias analytically and show the effect of the bias on the equilibrium of the SSA auction. Using a large dataset from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11...|$|R
40|$|Recently {{there has}} been {{significant}} interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing measures such as average bid, average ad position, total impressions, clicks, and cost for each keyword in the advertiser’s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intraday variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates. Specifically, the impact of <b>ad</b> position on <b>click-through</b> <b>rate</b> (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We analytically demonstrate the existence of the bias and show the effect of the bias on the equilibrium of the SSA auction. Using a large data set from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11 % due to aggregation bias. We also present a few data summarization techniques that can be used by search engines to reduce or eliminate the bias...|$|R
5000|$|<b>Click-through</b> <b>rates</b> for <b>ad</b> campaigns vary tremendously. The {{very first}} online display ad shown for AT&T {{on the website}} HotWired in 1994, had a 44% <b>click-through</b> <b>rate.</b> [...] Over time the overall rate users click on webpage banner ads has decreased.|$|R
5000|$|<b>Click-through</b> <b>rates</b> for banner <b>ads</b> have {{decreased}} over time. When banner ads first started to appear, {{it was not}} uncommon to have rates above five percent. They have fallen since then, currently averaging closer to 0.2 or 0.3 percent. In most cases, a 2% <b>click-through</b> <b>rate</b> would be considered very successful, though the exact number is hotly debated and would vary depending on the situation. The average <b>click-through</b> <b>rate</b> of 3% in the 1990s declined to 2.4%-0.4% by 2002. [...] Since advertisers typically pay more for a high <b>click-through</b> <b>rate,</b> getting many <b>click-throughs</b> with few purchases is undesirable to advertisers. Similarly, by selecting an appropriate advertising site with high affinity (e.g., a movie magazine for a movie advertisement), the same banner can achieve a substantially higher CTR. Though personalized ads, unusual formats, and more obtrusive ads typically result in higher <b>click-through</b> <b>rates</b> than standard banner ads, overly intrusive ads are often avoided by viewers.|$|R
50|$|An email <b>click-through</b> <b>rate</b> {{is defined}} as the number of {{recipients}} who click one or more links in an email and landed on the sender's website, blog, or other desired destination. More simply, email <b>click-through</b> <b>rates</b> represent the number of clicks that your email generated.|$|R
50|$|Typically a {{large number}} of {{customers}} search for a product/service or register as page view on a referring page which is linked to the e-commerce site by a banner ad, ad network or conventional link. Only a small proportion of those seeing the advertisement or link actually click the link. The metric used to describe this ratio is the <b>click-through</b> <b>rate</b> (CTR) and represents the top level of the funnel. Typical banner and advertising <b>click-through</b> <b>rates</b> are 0.02% in late 2010 and have decreased over the past three years. <b>Click-through</b> <b>rates</b> are highly sensitive to small changes such as link text, link size, link position and many others and these effects interact cumulatively. The process of understanding which creative material brings the highest <b>click-through</b> <b>rate</b> is known as ad optimization.|$|R
40|$|For {{the past}} few years, we used Apache Lucene as {{recommendation}} frame-work in our scholarly-literature recommender system of the reference-management software Docear. In this paper, we share three lessons learned from our work with Lucene. First, recommendations with relevance scores below 0. 025 tend to have significantly lower <b>click-through</b> <b>rates</b> than recommendations with relevance scores above 0. 025. Second, by picking ten recommendations randomly from Lucene's top 50 search results, <b>click-through</b> <b>rate</b> decreased by 15 %, compared to recommending the top 10 results. Third, the number of returned search results tend to predict how high <b>click-through</b> <b>rates</b> will be: when Lucene returns less than 1, 000 search results, <b>click-through</b> <b>rates</b> tend to be around half as high as if 1, 000 + results are returned. Comment: Accepted for publication at the 5 th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2017...|$|R
40|$|We analyse {{the choice}} of {{pay-per-view}} (PPV) and price-per-click (PPC) when a web publisher is a price taker {{in the market for}} advertising banners, and the number of visits is decreasing in advertising. The main result is that the web publisher should always choose either PPV or PPC. If the <b>click-through</b> <b>rate</b> is exogenous, then the optimal amount of advertising is the same for both pricing metholds and {{the choice of}} pricing method is given by the <b>click-through</b> <b>rate.</b> If the <b>click-through</b> <b>rate</b> is endogenous, the amount of advertising will be different under PPV and PPC...|$|R
50|$|Most email {{marketers}} {{use this}} metrics along with open rate, bounce rate and other metrics, {{to understand the}} effectiveness and success of their email campaign. In general there is no ideal <b>click-through</b> <b>rate.</b> This metric can vary based {{on the type of}} email sent, how frequently emails are sent, how the list of recipients is segmented, how relevant the content of the email is to the audience, and many other factors. Even time of day can affect <b>click-through</b> <b>rate.</b> Sunday appears to generate considerably higher <b>click-through</b> <b>rates</b> on average when compared {{to the rest of the}} week.|$|R
40|$|The {{debate about}} which media metric {{efficiently}} measures {{the effectiveness of}} a web-based advertisement, such as banners, is still alive and well. Nonetheless, the most widely used measure of effectiveness for banner advertisements is still the <b>click-through</b> <b>rate.</b> The {{purpose of this article is}} to review the measures currently used to measure effectiveness in web advertising and to empirically determine the factors that might contribute to observed variations in <b>click-through</b> <b>rates</b> based on an actual sample of advertising campaigns. The study examined the complete set of all advertising insertions of 77 customers of a large advertising agency over a one-year period. A resulting sample of 1, 258 placements was used to study the effect of banner formats and exposure levels on <b>click-through</b> <b>rates</b> using analysis of variance. Results suggest that the strongest effect on <b>click-through</b> <b>rates</b> comes from the use of trickbanners ([eta] 2 = 0. 25) and that other factors such as size of the advertisement, motion, use of and type of announcers all have a significant impact of <b>click-through</b> <b>rates.</b> Implications of these findings as well as limitations of the current study are discussed and directions for future research agendas proposed. a...|$|R
5000|$|The <b>click-through</b> <b>rate</b> of an {{advertisement}} {{is defined as}} the number of clicks on an ad divided by the number of times the ad is shown (impressions), expressed as a percentage. [...] For example, if a banner ad is delivered 100 times (100 impressions) and receives one click, then the <b>click-through</b> <b>rate</b> for the advertisement would be 1%.|$|R
40|$|We study {{multiple}} keyword sponsored search auctions with budgets. Each keyword {{has multiple}} ad slots with a <b>click-through</b> <b>rate.</b> The bidders have additive valuations, which are linear in the <b>click-through</b> <b>rates,</b> and budgets, which are restricting their overall payments. Additionally, {{the number of}} slots per keyword assigned to a bidder is bounded. We show the following results: (1) We give the first mechanism for multiple keywords, where <b>click-through</b> <b>rates</b> differ among slots. Our mechanism is incentive compatible in expectation, individually rational in expectation, and Pareto optimal. (2) We study the combinatorial setting, where each bidder is only interested in {{a subset of the}} keywords. We give an incentive compatible, individually rational, Pareto optimal, and deterministic mechanism for identical <b>click-through</b> <b>rates.</b> (3) We give an impossibility result for incentive compatible, individually rational, Pareto optimal, and deterministic mechanisms for bidders with diminishing marginal valuations. © 2012 Springer-Verlag Berlin Heidelberg...|$|R
5000|$|Maintain {{an average}} <b>click-through</b> <b>rate</b> (CTR) of 1% or higher {{over the last}} six-month period ...|$|R
50|$|For a {{comparison}} of two binomial distributions such as a <b>click-through</b> <b>rate</b> one would use Fisher's exact test.|$|R
40|$|A {{growing trend}} in {{commercial}} search engines is {{the display of}} specialized content such as news, products, etc. interleaved with web search results. Ideally, this content should be displayed only when it is highly relevant to the search query, as it competes for space with “regular ” results and advertisements. One measure of the relevance to the search query is the <b>click-through</b> <b>rate</b> the specialized content achieves when displayed; hence, if we can predict this <b>click-through</b> <b>rate</b> accurately, we can use this {{as the basis for}} selecting when to show specialized content. In this paper, we consider the problem of estimating the clickthrough rate for dedicated news search results. For queries for which news results have been displayed repeatedly before, the <b>click-through</b> <b>rate</b> can be tracked online; however, the key challenge for which previously unseen queries to display news results remains. In this paper we propose a supervised model that offers accurate prediction of news <b>click-through</b> <b>rates</b> and satisfies the requirement of adapting quickly to emerging news events...|$|R
50|$|Modern online {{advertising}} has moved beyond just using banner ads. Popular search engines allow advertisers to display ads {{in with the}} search results triggered by a search user. These ads are usually in text format and may include additional links and information like phone numbers, addresses and specific product pages. This additional information {{moves away from the}} poor user experience that can be created from intrusive banner ads and provides useful information to the search user, resulting in higher <b>Click-through</b> <b>rates</b> for this format of Pay Per Click Advertising. Having high <b>click-through</b> <b>rate</b> isn't the only goal for an online advertiser who will occasionally develop campaigns to raise awareness and sacrifice <b>click-through</b> <b>rate</b> for the overall gain of valuable traffic.|$|R
50|$|Most email {{marketing}} software provides tracking features, {{sometimes in}} aggregate (e.g., <b>click-through</b> <b>rate),</b> and sometimes {{on an individual}} basis.|$|R
50|$|Every year {{studies and}} {{various types of}} {{research}} are conducted to track the overall effectiveness of <b>click-through</b> <b>rates</b> in email marketing.|$|R
40|$|<b>Click-through</b> <b>rate</b> is {{considered}} a very important metric and a key performance indicator {{of the success of}} online advertising and is the most frequently used measure to gauge the effectiveness of banner advertising. Marketers also use <b>click-through</b> <b>rates</b> in arriving at performance measurement activities such as the calculation of 2 ̆ 7 customer life time value 2 ̆ 7 and 2 ̆ 7 customer acquisition cost 2 ̆ 7. Click-through is the second most frequently used banner ad pricing method after cost per thousand impressions. Online advertising is facing a new form of challenge â€“ the artificial inflation of <b>click-through</b> <b>rates.</b> We call this practice 2 ̆ 7 cyber-rigging 2 ̆ 7. The objective {{of this paper is to}} explore the ethical dimensions of cyber-rigging through application of ethical principles and theories...|$|R
50|$|Email <b>click-through</b> <b>rate</b> is {{expressed}} as a percentage, and calculated by dividing the number of click throughs {{by the number of}} tracked message deliveries.|$|R
50|$|In {{search engine}} marketing, {{companies}} try {{to place their}} websites in the top results for searches. It was previously considered that a website should {{be seen on the}} first page of the search engine and it was even better if the website was ranked higher on the first page. This was until a trend was identified, showing that it was only the first three results on a search engine that had high <b>click-through</b> <b>rates</b> whilst the remaining results had very low <b>click-through</b> <b>rates.</b>|$|R
40|$|We {{develop a}} Bayes–Nash {{analysis}} of the generalized second-price (GSP) auction, the multi-unit auction used by search engines to sell sponsored advertising positions. Our main result characterizes the efficient Bayes–Nash equilibrium of the GSP and provides a necessary and sufficient condition that guarantees existence of such an equilibrium. With only two positions, this condition requires that the <b>click–through</b> <b>rate</b> of the second position is sufficiently smaller {{than that of the}} first. When an efficient equilibrium exists, we provide a necessary and sufficient condition for the auction revenue to decrease as <b>click–through</b> <b>rates</b> increase. Interestingly, under optimal reserve prices, revenue increases with the <b>click–through</b> <b>rates</b> of all positions. Further, we prove that no inefficient equilibrium of the GSP can be symmetric. Our results are in sharp contrast with the previous literature that studied the GSP under complete information...|$|R
40|$|Predicting the <b>click-through</b> <b>rate</b> of an {{advertisement}} {{is a critical}} component of online advertising platforms. In sponsored search, the <b>click-through</b> <b>rate</b> estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. This is inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the <b>click-through</b> <b>rate</b> of a query-advertisement pair. Specially, the proposed architectures only consider the textual content appearing in a query-advertisement pair as input, and produce as output a <b>click-through</b> <b>rate</b> prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. Through extensive experiments using billions of query-advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-the-art word 2 vec-based approach. Finally, by combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the <b>click-through</b> <b>rate</b> prediction of the production system. Comment: SIGIR 2017, 10 page...|$|R
40|$|In a {{sponsored}} search auction {{the advertisement}} slots on a search result page are generally ordered by <b>click-through</b> <b>rate.</b> Bidders have a valuation, {{which is usually}} assumed to be linear in the <b>click-through</b> <b>rate,</b> a budget constraint, and receive at most one slot per search result page (round). We study multi-round sponsored search auctions, where the different rounds are linked through the budget constraints of the bidders and the valuation of a bidder for all rounds {{is the sum of}} the valuations for the individual rounds. All mechanisms published so far either study one-round sponsored search auctions or the setting where every round has only one slot and all slots have the same <b>click-through</b> <b>rate,</b> which is identical to a multi-item auction. This paper contains the following three results: (1) We give the first mechanism for the multi-round sponsored search problem where different slots have different <b>click-through</b> <b>rates.</b> Our mechanism is incentive compatible in expectation, individually rational in expectation, Pareto optimal in expectation, and also ex-post Pareto optimal for each realized outcome. (2) Additionally we study the combinatorial setting, where each bidder is only interested in a subset of the rounds. We give a deterministic, incentive compatible, individually rational, and Pareto optimal mechanism for the setting where all slots have the same <b>click-through</b> <b>rate.</b> (3) We present an impossibility result for auctions where bidders have diminishing marginal valuations. Specifically, we show that even for the multi-unit (one slot per round) setting there is no incentive compatible, individually rational, and Pareto optimal mechanism for private diminishing marginal valuations and public budgets. Comment: 28 page...|$|R
5000|$|The <b>click-through</b> <b>rate</b> is {{the number}} of times a click is made on the {{advertisement}} divided by the total impressions (the number of times an advertisement was served): ...|$|R
5000|$|The {{move to a}} session-based model {{might even}} give new life to beleaguered metrics such as the click-through. Many {{marketers}} equate a 1% <b>click-through</b> <b>rate</b> with only 1 in 100 visitors clicking on an advertisement. At best, this assumption is slightly imprecise... at worst, it is grossly inaccurate. For example, assume that a particularly [...] "sticky" [...] site averages 11 page views per visitor and a 3% <b>click-through</b> <b>rate.</b> This means that up to 33% of the audience could {{have responded to the}} advertisement, assuming no duplicate clicks.|$|R
