0|10000|Public
40|$|The current {{methodology}} of output casual models and structures of systems of probabilistic dependencies <b>of</b> statistical <b>data</b> <b>of</b> passive observation is analysed. The problems, features, traps {{and limitations of}} the methods of the inductive identification of casual relation in the unit of marcov properties and bayesias nets are highlighted. Several stages of casual models according {{to the level of}} their validity and <b>adequacy</b> <b>of</b> the <b>data</b> <b>source</b> are emphasized. The statistical pattern, which brings the justification of a finding about casual nature of the connections between two variables to the test of a set of statistical facts of (in) dependency is formulated. ???????????????? ??????????? ??????????? ?????? ?????????? ??????? ? ???????? ?????? ????????????? ???????????? ?? ?????????????? ?????? ????????? ??????????. ???????? ???????????, ????????, ???????? ? ??????????? ??????? ??????????? ????????????? ?????????? ????????? ? ???????? ?????????? ??????? ? ????????? ?????. ???????? ????????? ???????? ?????????? ??????? ???????? ?????? ?? ?????????????? ? ???????????? ????????? ??????. ????????????? ?????????????? ???????, ??????? ?????? ??????????? ?????? ? ?????????? ????????? ????? ???? ?????????? ? ???????????? ?????? ?????????????? ?????? (??) ???????????...|$|R
40|$|This project {{describes}} the data resources on arts {{organizations that are}} currently available to inform the efforts of policy makers, arts managers, and researchers working in the arts field. It assesses the <b>adequacy</b> <b>of</b> different <b>data</b> <b>sources</b> for identifying the population of arts and cultural organizations in a community. The report {{is based on a}} review {{of more than a dozen}} sources of information about arts and cultural organizations, interviews with researchers and data specialists, and an empirical study of arts organizations in three metropolitan areas - Philadelphia, Dallas-Fort Worth, and Minneapolis-St. Paul. The report concludes with recommendations for improving data quality and for establishing an ongoing national database on the arts sector. ...|$|R
40|$|This paper uses newly {{available}} {{data from the}} Social Security Administration's Retirement History Survey to examine the <b>adequacy</b> <b>of</b> saving. This <b>data</b> <b>source</b> is particularly rich; survey data for respondents covering the ydars 1969, 197 1 (and 1953 have been matched with Social Security earnings records covering the years dating back to 1951. In addition to information {{on the path of}} lifetime earnhngs, the survey contains extensive data on individual asset holdings. The evidence indicates that surprisingly few couples currantly suffer significant reductions in their standard of living in their old age. This appears due, in large part, to our compulsory savings institutions, the Social Security and private pension systems. These institutions have succeeded in redistributing the lifetime consumption of private individuals from their youth to their old age. ...|$|R
40|$|Testimony {{issued by}} the General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO discussed the <b>adequacy</b> <b>of</b> the <b>data</b> that the Environmental Protection Agency and the states have for making critical water quality decisions required by the Clean Water Act, focusing on: (1) the <b>adequacy</b> <b>of</b> the <b>data</b> for identifying waters for states' 303 (d) lists; (2) the <b>adequacy</b> <b>of</b> <b>data</b> for developing total maximum daily loads (TMDL) for those waters; and (3) key factors that affect the states' abilities to develop TMDLs. ...|$|R
30|$|For toxicity, e.g. {{indications}} of toxicity potential considered when availability or <b>adequacy</b> <b>of</b> <b>data</b> is {{too poor to}} support a definitive or a screening assessment.|$|R
40|$|Testimony {{issued by}} the General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO discussed the data that the Environmental Protection Agency (EPA) and the states have for making critical water quality decisions required by the Clean Water Act, focusing on: (1) the <b>adequacy</b> <b>of</b> the <b>data</b> for identifying waters for states' 303 (d) lists; (2) the <b>adequacy</b> <b>of</b> <b>data</b> for developing total maximum daily loads (TMDL) for those waters; and (3) key factors that affect the states' abilities to develop TMDLs. ...|$|R
5000|$|In 1998, the University of Strathclyde {{published}} [...] "A {{critique of}} GERS: {{government expenditure and revenue}} in Scotland." [...] This criticised primarily the <b>adequacy</b> <b>of</b> the methodology {{used and the}} accuracy <b>of</b> <b>data</b> <b>sources</b> as well as the purpose of a GERS exercise.|$|R
30|$|Two reviewers {{evaluated}} the methodological and reporting {{quality of the}} finally selected reports; discrepancies were resolved by discussion until consensus was reached. The following quality items {{were used to assess}} the methodological quality and risk of bias in the studies [[30],[31]]: eligibility criteria, <b>adequacy</b> <b>of</b> sample size, reporting of randomization, reporting of blinding, avoiding selective reporting, description of intervention details, description of outcome measures, description of adverse effects, and <b>adequacy</b> <b>of</b> <b>data</b> analysis.|$|R
30|$|The number <b>of</b> <b>data</b> <b>sources</b> might increase.|$|R
40|$|Abstract. Most of {{structured}} {{deep web}} <b>data</b> <b>sources</b> are non-cooperation, therefore establish an accurate <b>data</b> <b>source</b> content summary by sampling {{is the core}} technology <b>of</b> <b>data</b> <b>source</b> selection. The content <b>of</b> deep web <b>data</b> <b>source</b> is updated from time to time, however existing efficient methods <b>of</b> non-cooperation structured <b>data</b> <b>source</b> selection does not consider summary update problem. Unrenewed summary <b>of</b> <b>data</b> <b>source</b> can not accurately characterize the content <b>of</b> the <b>data</b> <b>source</b> that produce {{a greater impact on}} <b>data</b> <b>source</b> selection. Base on this, we propose a dynamic <b>data</b> <b>source</b> selection method for non-cooperative structured deep web by combining subject headings sampling technology and subject headings extension technology. The experiment results show that our dynamic structured <b>data</b> <b>source</b> selection method has good recall ratio and precision besides being efficient...|$|R
30|$|It is {{a further}} {{improvement}} of l-diversity group based anonymization {{that is used to}} preserve privacy in data sets by decreasing the granularity <b>of</b> a <b>data</b> representation. This reduction is a trade-off that results in some loss <b>of</b> <b>adequacy</b> <b>of</b> <b>data</b> management or mining algorithms in order to gain some privacy. The t-closeness model(Equal/Hierarchical distance) [29, 33] extends the l-diversity model by treating the values of an attribute distinctly by taking into account the distribution <b>of</b> <b>data</b> values for that attribute.|$|R
30|$|Data were {{analyzed}} with SPSS for Windows Version 20.0. Descriptive statistics {{were used to}} summarize demographic characteristics. The Cronbach coefficient {{was used to assess}} {{the internal consistency of the}} CCES. The test–retest reliability <b>of</b> the baseline <b>data</b> and the 2 -week follow-up were calculated using the interclass correlation coefficient. The construct validity of the CCES was examined using PCA by the varimax rotation method. The Kaiser–Meyer–Olkin (KMO) test was performed to examine the <b>adequacy</b> <b>of</b> <b>data</b> for PCA.|$|R
5000|$|The DERM {{structure}} is data independent {{allowing for the}} general quantization <b>of</b> all georeferenced <b>data</b> <b>sources</b> onto the common grid. Application, algorithms and operations can then be developed on the grid independent <b>of</b> <b>data</b> <b>sources.</b>|$|R
50|$|A variety <b>of</b> <b>data</b> <b>sources</b> are {{available}} for examining shoreline position. However, the availability <b>of</b> historical <b>data</b> is limited at many coastal sites and so the choice <b>of</b> <b>data</b> <b>source</b> is largely limited to what is available for the site at a given time. Shoreline mapping techniques have become more automated. The frequent changes in technology prevented the emergence of one standard mapping approach. Each <b>data</b> <b>source</b> and associated method have capabilities and shortcomings.|$|R
5000|$|Measuring Housing Affordability: A review <b>of</b> <b>data</b> <b>sources</b> - December 2008 (minor update April 2009) ...|$|R
5000|$|Service Data Objects (SDO) for Java, C++ and [...]Net {{clients and}} any type <b>of</b> <b>data</b> <b>source</b> ...|$|R
40|$|The {{expansion}} of the internet has brought with it a huge {{increase in the number}} of instances of personal information sent by businesses and governments from one jurisdiction to another. Concern arising out of Europe, in particular, over the <b>adequacy</b> <b>of</b> <b>data</b> protection measures in many jurisdictions around the world has resulted in increasing international pressure being applied to those countries not meeting adequacy requirements. This paper examines the nature and effect of this pressure particularly on Australian business and government...|$|R
40|$|Data is {{becoming}} a commodity of tremendous value for many do-mains. This is leading to a rapid {{increase in the number}} <b>of</b> <b>data</b> <b>sources</b> and public access data services, such as cloud-based data markets and data portals, that facilitate the collection, publishing and trading <b>of</b> <b>data.</b> <b>Data</b> <b>sources</b> typically exhibit wide variety and heterogeneity in the types or schemas <b>of</b> the <b>data</b> they pro-vide, their quality, and the fees they charge for accessing their data. Users who want to build upon such publicly available data, must (i) discover sources that are relevant to their applications, (ii) identify sources that collectively satisfy the quality and budget requirements of their applications, with few effective clues {{about the quality of the}} sources, and (iii) repeatedly invest many person-hours in assess-ing the eventual usefulness <b>of</b> <b>data</b> <b>sources.</b> All three steps require investigating the content of the sources manually, integrating them and evaluating the actual benefit of the integration result for a de-sired application. Unfortunately, when the number <b>of</b> <b>data</b> <b>sources</b> is large, humans have a limited capability of reasoning about the ac-tual quality of sources and the trade-offs between the benefits and costs of acquiring and integrating sources. In this paper we explore the problems of automatically appraising the quality <b>of</b> <b>data</b> <b>sources</b> and identifying the most valuable sources for diverse applications. We introduce our vision for a new <b>data</b> <b>source</b> management sys-tem that automatically assesses the quality <b>of</b> <b>data</b> <b>sources</b> based on a collection <b>of</b> rigorous <b>data</b> quality metrics and enables the auto-mated and interactive discovery of valuable sources for user appli-cations. We argue that the proposed system can dramatically sim-plify the Discover-Appraise-Evaluate interaction loop that many users follow today to discover sources for their applications. 1...|$|R
50|$|Stimulsoft Reports is an {{application}} used {{to design and}} generate reports {{from a wide range}} <b>of</b> <b>data</b> <b>sources.</b>|$|R
5000|$|Marketers use {{a variety}} <b>of</b> <b>data</b> <b>sources</b> for {{segmentation}} studies and market profiling. Typical sources of information include: ...|$|R
5000|$|OpenLink Software ships {{components}} supporting OLE DB {{access to}} a number <b>of</b> <b>data</b> <b>sources,</b> including several SQL DBMS, as well as Bridges to ODBC- and JDBC-accessible <b>data</b> <b>sources</b> ...|$|R
40|$|In various {{contexts}} (e. g., the Internet), the query-processing capabilities <b>of</b> <b>data</b> <b>sources</b> may be limited. Middleware systems {{based on}} a mediation architecture are employed to provide powerful query processing interfaces to <b>data</b> <b>sources</b> with limited query capabilities. Such systems also provide integrated views to the <b>data</b> across multiple <b>sources.</b> In this paper, we discuss how the query capabilities <b>of</b> <b>data</b> <b>sources</b> can be described and how mediators support extended query sets over the <b>data</b> <b>sources.</b> We also describe how the set of queries supported by a mediator can be computed from the corresponding sets of queries supported by the <b>data</b> <b>sources</b> integrated by the mediator. We present ways in which content description <b>of</b> <b>data</b> <b>sources</b> can help in extending the set of supported queries. Finally, we describe the challenges posed by diverse source capabilities when supporting robust query processing through replicated sources {{in the presence of}} source failures. 1 Introduction In context [...] ...|$|R
5000|$|Data-source marking is {{reflected}} in every sentence of the language. The three major grammatical categories <b>of</b> <b>data</b> <b>source</b> are: ...|$|R
30|$|The {{scalability}} challenge {{arises from}} the vast number <b>of</b> <b>data</b> <b>sources</b> that may input to a data lake [10], and the continuous addition of new and varying <b>data</b> <b>sources</b> [2].|$|R
50|$|GNU Data Access (GDA) {{is a set}} of plugin APIs, {{defined as}} generic as possible, so that any kind <b>of</b> <b>data</b> <b>source</b> can be {{accessed}} through them, to provide uniform access to different kinds <b>of</b> <b>data</b> <b>sources</b> (databases, information servers, mail spools, etc.). Similar to Open Database Connectivity (ODBC) or Java Database Connectivity (JDBC), GNU Data Access is a wrapper but with more features to access several database engines. GNU Data Access has been developed as a complete architecture that provides everything required to access <b>data</b> <b>sources.</b>|$|R
40|$|Abstract—Organizations often hold {{large amounts}} <b>of</b> unused <b>data,</b> trapped in {{fragmented}} databases, locked in legacy data formats. As well, the Web {{offers a variety}} <b>of</b> <b>data</b> <b>sources</b> accessible in diverse ways. There {{is a lack of}} approaches to handle this multiplicity <b>of</b> <b>data</b> <b>sources</b> and combine multi-origin data into coherent smart data sets. We therefore define a meta-model that allows flexible modeling <b>of</b> <b>data</b> <b>source</b> diversity, and we propose a resource-oriented approach to handle data access and processing. We designed and evaluated our approach to offer scalability, responsiveness, as well as dynamic and transparent <b>data</b> <b>source</b> management features. We motivate our solution through a live scenario based on the information system of the Audience Labs French company. This paper describes our models and resource-oriented architecture and shows how they adapt to industrial needs and facilitate smart data production and reuse. Index Terms—resource oriented architecture, data integration, data semantics, smart data F...|$|R
40|$|Economic {{analysis}} {{has become increasingly}} important in healthcare in general, and {{particularly with respect to}} pharmaceuticals. Therefore, it is vital that the methods used in such evaluations are carefully scrutinised and refined. However, guidelines contain {{only a limited number of}} recommendations for the use <b>of</b> secondary <b>data</b> in modelling studies. In this manuscript, the selection <b>of</b> <b>data</b> <b>sources</b> in modelling studies will be addressed. The objectives of this manuscript are as follows: 1. to present a general strategy on how to determine the appropriateness <b>of</b> a <b>data</b> <b>source</b> for a model; and 2. to present recommendations on a transparent reporting format for the selection <b>of</b> <b>data</b> <b>sources.</b> Reviews-on-treatment, Pharmacoeconomics, Clinical-trial-design, Health-economics, Practice-guideline-commentary, Medical-information, Medical-records...|$|R
40|$|AbstractIn {{a variety}} of domains, the amount of Web {{information}} grows rapidly, and the types <b>of</b> <b>data</b> <b>sources</b> are proliferating. Moreover, different <b>data</b> <b>sources</b> often provide heterogeneous or conflicting data, {{so we need to}} resolve data conflicts and find truth by data fusion. Currently, there are several advanced techniques that consider accuracy of sources, freshness of sources and dependencies between sources to solve the conflicts, and these strategies achieved good results. To improve the data fusion, we propose a quality estimation model <b>of</b> Deep Web <b>data</b> <b>sources</b> (DSQ). According to the characteristics <b>of</b> <b>data</b> fusion, our estimation model selects three dimensions of factors-data quality, interface quality and service quality-as estimation criteria, and estimates the quality <b>of</b> <b>data</b> <b>sources.</b> Then, we improve the data fusion using the estimation results. Experiment shows that our model can accurately estimate the quality <b>of</b> Deep Web <b>data</b> <b>sources,</b> and significantly improve the data fusion...|$|R
40|$|To {{represent}} the loads spectra of general aviation aircraft {{operating in the}} Continental United States, VG and VGH data collected since 1963 in eight operational categories were processed and analyzed. <b>Adequacy</b> <b>of</b> <b>data</b> sample and current operational categories, and parameter distributions required for valid data extrapolation were studied along with envelopes of equal probability of exceeding the normal load factor (n sub z) versus airspeed for gust and maneuver loads and the probability of exceeding current design maneuver, gust, and landing impact n sub z limits. The significant findings are included...|$|R
40|$|AbstractThe {{expansion}} <b>of</b> <b>data</b> <b>sources</b> {{existed in}} the web affects {{on the quality of}} research information. The correct answer (answer specific) of a request is all depend terms selected for their construction. Such as these terms sometimes mean more sense, the intended meaning may not be found. Fort this need, the Semantic Web has come to cover the semantic level, it proposed an ontological representation <b>of</b> <b>data</b> <b>sources.</b> This representation is implemented by OWL (Web Ontology Language). The current challenge of the Semantic Web is the transformation <b>of</b> <b>data</b> formats exist (SQL, XML [...] .) to the form of ontology (RDF, OWL [...] .), in order to facilitate the integration <b>of</b> <b>data</b> <b>sources</b> exist in the semantic web. Our target is to provide a series of extension concepts of DTD 2 OWL method, a simple and effective method for transforming XML documents into OWL ontologies...|$|R
40|$|Sumatra TT is a {{universal}} data pre-processing tool allowing to process data stored in various types <b>of</b> <b>data</b> <b>sources</b> (e. g. plain text, SQL, etc). We enriched the set <b>of</b> accessible <b>data</b> <b>sources</b> by Prolog databases. Field conversions to Prolog-speci c syntax such as structured terms and atomized strings are handled on the Sumatra-interpreter level. Moreover...|$|R
40|$|With the {{increasing}} need for accurate weather predictions, we need large samples <b>of</b> <b>data</b> from different <b>data</b> <b>sources</b> for an accurate estimate. There {{are a number}} <b>of</b> <b>data</b> <b>sources</b> that keep publishing data periodically. These <b>data</b> <b>sources</b> have their own server protocols that a user needs to follow while writing client for retrieving data. This project aims at creating a generic semi-automatic client mechanism for retrieving scientific <b>data</b> from such <b>sources.</b> Also, with {{the increasing}} number <b>of</b> <b>data</b> <b>sources</b> {{there is also a}} need for a data model to accommodate data that is published in different formats. We {{have come up with a}} data model that can be used across various applications in the domain <b>of</b> scientific <b>data</b> retrieval. ...|$|R
40|$|Research in nonprofit {{accounting}} {{is steadily}} increasing as more data is available. In {{an effort to}} broaden the awareness <b>of</b> the <b>data</b> <b>sources</b> and ensure the quality of nonprofit research, we discuss archival <b>data</b> <b>sources</b> available to nonprofit researchers, data issues, and potential resolutions to those problems. Overall, our paper should raise awareness <b>of</b> <b>data</b> <b>sources</b> in the nonprofit area, increase production, and {{enhance the quality of}} nonprofit research...|$|R
5000|$|Tests {{accessibility}} <b>of</b> <b>data</b> <b>sources</b> <b>of</b> Open Database Connectivity (ODBC) or native MS SQL. Runs SQL querys {{and checks}} return results as an option.|$|R
40|$|In this paper, we {{point to}} the {{potential}} and implications of digital traces as novel <b>data</b> <b>source</b> {{in the study of}} contemporary activities and behaviors. We do this to raise awareness of IS researchers of such traces in increasingly complex sociomaterial practices. We develop a two-dimensional framework <b>of</b> <b>data</b> <b>sources</b> (subjective/objective and digitalized/non-digitalized) for analyzing a six-year literature survey comprised of five leading IS journals. The analysis positions current <b>data</b> <b>sources</b> employed within the framework, and sheds light on the under utilization <b>of</b> digitalized <b>data</b> <b>sources.</b> This disconcerting result suggests that IS researchers must {{pay more attention to the}} changing landscape <b>of</b> <b>data</b> <b>sources.</b> To motivate and guide fellow colleagues to establish the credibility and reliability of digital traces, we develop a future research agenda that covers both opportunities in theory generation and challenges in data collection...|$|R
40|$|Understanding how to map {{a library}} 2 ̆ 7 s data {{ecosystem}} {{is critical to}} helping manage the variety <b>of</b> <b>data</b> <b>sources</b> that describe the library and its work. Mapping the kinds <b>of</b> <b>data,</b> its <b>source,</b> and the people involved is only the beginning, yet the challenges faced can offer opportunities {{when it comes to}} supporting strategic planning...|$|R
