9|23|Public
5000|$|Melody, an {{application}} for CAE / steel framework analysis and design. It allows <b>automatic</b> <b>dimensioning</b> of steel frames and creation of [...] "turnkey" [...] calculation reports. Melody is available in France and French-speaking countries.|$|E
40|$|A {{general scheme}} for <b>automatic</b> <b>dimensioning</b> of objects defined by solid {{modelling}} techniques is presented. The {{first part of}} the paper describes a theory that relates the geometrical definition of a solid and the dimensions that appeared on the engineering drawings. The second part demonstrates a method that automatically generates adequate dimensions from boundary representations of the solid models. Implementation of the scheme on a constructive solid geometry modeller, PADL 2 of the University of Rochester, is also described. © 1988. link_to_subscribed_fulltex...|$|E
40|$|Abstract. By using AutoLISP {{language}} {{based on}} AutoCAD, {{the development process}} of the box /carton CAD system focuses on software workflow and overall development program. The system uses nodes descript by a relatively polar coordinate, parametric design, the combination of box/carton-type library with box/carton components library, splicing design methods, etc. With the program designed by AutoLISP language, paper box/carton structure parametric drawing can be achieved according to the mathematical model. The system has the input data legality verification, error handling function, output expansion plan, and <b>automatic</b> <b>dimensioning</b> features...|$|E
40|$|Shallit and Wang studied {{deterministic}} automatic {{complexity of}} words. They {{showed that the}} <b>automatic</b> Hausdorff <b>dimension</b> I(t) of the infinite Thue word satisfies 1 / 3 1 / 2. For nondeterministic automatic complexity we show I(t) = 1 / 2. We prove that such complexity A_N of a word x of length n satisfies A_N(x) d for given x, d belongs to NP∩ E...|$|R
40|$|With {{the arrival}} of gene {{expression}} microarrays a new challenge has opened up for identification or classification of cancer tissues. Due to {{the large number of}} genes providing valuable information simultaneously compared to very few available tissue samples the cancer staging or classification becomes very tricky. In this paper we introduce a hierarchical Bayesian probit model for two class cancer classification. Instead of assuming a linear structure for the function that relates the gene expressions with the cancer types we only assume that the relationship is explained by an unknown function which belongs to an abstract functional space like the reproducing kernel Hilbert space. Our formulation automatically reduces the dimension of the problem from the large number of covariates or genes to a small sample size. We incorporate a Bayesian gene selection scheme with the <b>automatic</b> <b>dimension</b> reduction to adaptively select important genes and classify cancer types under an unified model. Our model is highly flexible in terms of explaining the relationship between the cancer types and gene expression measurements and picking up the differentially expressed genes. The proposed model is successfully tested on three simulated data sets and three publicly available leukemia cancer, colon cancer, and prostate cancer real life data sets. ...|$|R
40|$|Abstract: Non-linear {{regression}} {{based on}} reproducing kernel Hilbert space (RKHS) has recently become {{very popular in}} fitting high-dimensional data. The RKHS formulation provides an <b>automatic</b> <b>dimension</b> reduction of the covariates. This is particularly helpful {{when the number of}} covariates (p) far exceed the number of data points. In this paper, we introduce a Bayesian nonlinear multivariate regression model for high-dimensional problems. Our model is suitable when we have multiple correlated observed response corresponding to same set of covariates. We introduce a robust Bayesian support vector regression model based on a multivariate version of Vapnik's ϵ-insensitive loss function. The likelihood corresponding to the multivariate Vapnik's ϵ-insensitive loss function is constructed as a scale mixture of truncated normal and gamma distribution. The regression function is constructed using the finite representation of a function in the reproducing kernel Hilbert space (RKHS). The kernel parameter is estimated adaptively by assigning a prior on it and using the Markov chain Monte Carlo (MCMC) techniques for computation. Practical applications of our model are demonstrated via applications in Near-Infrared (NIR) spectroscopy and simulation studies. Our Bayesian non-linear models are highly accurate in predicting composition of materials based on its near infrared (NIR) spectroscopy signature. We have compare...|$|R
40|$|Abstract: This paper {{describes}} {{a method for}} <b>automatic</b> <b>dimensioning</b> of cylindrical parts represented by features. The identification of the entities in the part {{that need to be}} dimensioned is performed based on the part’s features, which contain information about the part’s geometry and the assembly relations between parts. The dimensioning analysis is based on the concept of functional dimensioning, which presupposes that the position of the part in the assembly is known, and then the dimensions to be controlled are determined, and they are assigned automatically to the part by the system. The dimensions are determined with the aid of an expert system. 1...|$|E
40|$|However, {{nowadays}} {{the tolerance}} analysis and synthesis in CAD/CAM are mostly realized by manual methods or {{the experience of}} the designers. Thus this not only increased the workload of the designers but also wasted a lot of time. As a result, this paper applies dimensional chain technology in AutoCAD 2010 software through ObjectARX development tool to realize the <b>automatic</b> <b>dimensioning</b> and tolerancing. In the process of software secondary development, the deep first search method is adopted to generate dimensional chain automatically, and worst-case method and minimum cost method are separately applied to carry out tolerance analysis and synthesis. Finally an example is presented to validate the given method. ...|$|E
40|$|The {{computerized}} {{design of}} building structures {{at a level}} of an almost total automation give rise to a great number of problems of various kinds that have to be solved. Out of these problems the author has chosen a topic that is of special interest because of its general field of application and because it is different from the previous topics dealing with <b>automatic</b> <b>dimensioning</b> of structures. El diseño computarizado de estructuras de edificación a un nivel de automatismo prácticamente del cien por cien, exige la resolución de múltiples problemas de variada índole. De entre ellos, el autor selecciona un tema de especial interés, por su aplicabilidad genérica y por apartarse de los temas ya clásicos en el dimensionamiento automático de estructuras...|$|E
40|$|Technique insures {{machined}} ceramics {{shrink to}} correct dimensions after baked in kiln. New method automatically compensates during machining for shrinkage later, when part baked. Applicable to numerically controlled machines that include provision {{to adjust for}} variations in cuttingtool size, but do not provide for <b>automatic</b> verification of <b>dimensions</b> of machined parts...|$|R
40|$|Non-linear {{regression}} {{based on}} reproducing kernel Hilbert space (RKHS) has recently become {{very popular in}} fitting high-dimensional data. The RKHS formulation provides an <b>automatic</b> <b>dimension</b> reduction of the covariates. This is particularly helpful {{when the number of}} covariates (p) far exceed the number of data points. In this paper, we introduce a Bayesian nonlinear multivariate regression model for high-dimensional problems. Our model is suitable when we have multiple correlated observed response corresponding to same set of covariates. We introduce a robust Bayesian support vector regression model based on a multivariate version of Vapnik’s ɛ-insensitive loss function. The likelihood corresponding to the multivariate Vapnik’s ɛ-insensitive loss function is constructed as a scale mixture of truncated normal and gamma distribution. The regression function is constructed using the finite representation of a function in the reproducing kernel Hilbert space (RKHS). The kernel parameter is estimated adaptively by assigning a prior on it and using the Markov chain Monte Carlo (MCMC) techniques for computation. Practical applications of our model are demonstrated via applications in Near-Infrared (NIR) spectroscopy and simulation studies. Our Bayesian non-linear models are highly accurate in predicting composition of materials based on its near infrared (NIR) spectroscopy signature. We have compared our method with popularly used methodologies in NIR spectroscopy, like partial least square (PLS), principal component regression (PCA), support vector machine (SVM), and random forest (RF). In all the simulation and real case studies, our multivariate Bayesian RKHS regression model outperforms the standard methods by a substantially large margin. The implementation of our models based on MCMC is fairly fast and straight forward...|$|R
40|$|Semantic web content {{management}} poses much {{manual work}} onto the community. To reduce this labour we have devised Caravela 1, a generic approach to dynamic content integration and automatic categorization. Content and documents {{of different types}} can be integrated from diverse semi-structured sources and categorized along multiple <b>dimensions.</b> <b>Automatic</b> linking provides dynamic categorizations at no user cost. We illustrate our approach by an online bibliography categorizing scientific research publications...|$|R
40|$|The {{software}} was considerably enhanced {{to accommodate a}} more comprehensive examination of data available for field modeling using the equivalent sources method by (1) implementing a dynamic core allocation capability into the software system for the <b>automatic</b> <b>dimensioning</b> of the normal matrix; (2) implementing a time dependent model for the dipoles; (3) incorporating the capability to input specialized data formats in a fashion similar to models in spherical harmonics; and (4) implementing the optional ability to simultaneously estimate observatory anomaly biases where annual means data is utilized. The time dependence capability was demonstrated by estimating a component model of 21 deg resolution using the 14 day MAGSAT data set of Goddard's MGST (12 / 80). The equivalent source model reproduced both the constant and the secular variation found in MGST (12 / 80) ...|$|E
40|$|Wave energy {{conversion}} has an essential difference from other renewable energies since the dependence between the devices {{design and the}} energy resource is stronger. Dimensioning is therefore considered a key stage when a design project of Wave Energy Converters (WEC) is undertaken. Location, WEC concept, Power Take-Off (PTO) type, control strategy and hydrodynamic resonance considerations {{are some of the}} critical aspects to take into account to achieve a good performance. The paper proposes an <b>automatic</b> <b>dimensioning</b> methodology to be accomplished at the initial design project stages and the following elements are described to carry out the study: an optimization design algorithm, its objective functions and restrictions, a PTO model, as well as a procedure to evaluate the WEC energy production. After that, a parametric analysis is included considering different combinations of the key parameters previously introduced. A variety of study cases are analysed {{from the point of view}} of energy production for different design-parameters and all of them are compared with a reference case. Finally, a discussion is presented based on the results obtained, and some recommendations to face the WEC design stage are given...|$|E
40|$|The {{objective}} {{of the work is}} to develop a method for <b>automatic</b> <b>dimensioning</b> of a digital human model (DHM) from a set of calibrated photographs of the subject under study. Fifteen subjects (10 males, 5 females, mean age 27) wearing surface retro-reflective markers at major bony landmarks and standing inside a calibrated space, were photographed by means of low cost main stream digital cameras (face, left and right views). The DHM software used is based on a skeletal structure surrounded by contours defined by cross sections along the skeletal links. Specific points corresponding to bony landmarks are defined in body linkage local reference frames. The photograph calibration method (DLT) allows to superimpose a picture of the DHM on the subject’s photographs. A specific algorithm adjusts the length of the manikin skeletal links in function of the measured distance between markers attached to these links (e. g. knee and ankle for the lower leg). Then the contour sections dimensions were adjusted to fit subject’s silhouette extracted from the photographs. The results obtained within a few seconds of calculation consist of a personalized DHM representing the subject with an error less than 2 % for stature and less than 4 % for weight...|$|E
40|$|Abstract — <b>Automatic</b> affective <b>dimension</b> {{recognition}} from facial expression continuously in naturalistic contexts {{is a very}} challenging research topic but very important in human-computer interaction. In this paper, an automatic recognition system was proposed to predict the affective dimensions such as Arousal, Valence and Dominance continuously in naturalistic facial expression videos. Firstly, visual and vocal features are ex-tracted from image frames and audio segments in facial expres-sion videos. Secondly, a wavelet transform based digital filtering method is applied to remove the irrelevant noise information in the feature space. Thirdly, Partial Least Squares regression is used to predict the affective dimensions from both video and audio modalities. Finally, two modalities are combined to boost overall performance in the decision fusion process. The proposed method is tested in the fourth international Au-dio/Visual Emotion Recognition Challenge (AVEC 2014) dataset and compared to other state-of-the-art methods in the affect recognition sub-challenge with a good performance. I...|$|R
40|$|Hyperspectral imagery, by definition, {{provides}} valuable {{remote sensing}} observations at hundreds of frequency bands. Conventional image classification (interpretation) methods {{may not be}} used without <b>dimension</b> reduction preprocessing. <b>Automatic</b> Wavelet Reduction has been proven to yield better or comparable classification accuracy, while achieving substantial computational savings. However, the large hyperspectral data volumes remain to present a challenge for traditional processing techniques. Reconfigurable Computers (RCs) can leverage the synergism between conventional processors and FPGAs to provide low-level hardware functionality at the same level of programmability as general-purpose computers. In this paper, we investigate the potential of using RCs for on-board, i. e. aboard airborne/spaceborne carriers, preprocessing of hyperspectral imagery by prototyping {{for the first time the}} <b>automatic</b> wavelet <b>dimension</b> reduction algorithm. Our investigation exploits the fine and coarse grain parallelism provided by the RCs and has been experimentally verified on one of the state-of-the art reconfigurable platforms, SRC- 6 E. An order of magnitude speedup over traditional processing techniques has been reported. 1...|$|R
40|$|Efficient network {{provisioning}} mechanisms supporting service differentiation and <b>automatic</b> capacity <b>dimensioning</b> {{are important}} for the realization of a differentiated service Internet. In this paper, we extend our prior work on edge provisioning [71 to interior nodes and core networks including algorithms for: (i) dynamic node provisioning and (ii) dynamic core provisioning. The dynamic node provisioning algorithm prevents transient violations of service level agreements by self-adjusting per-scheduler service weights and packet dropping thresholds at core routers, reporting persistent service level violations to the core provisioning algorithm. The dynamic core provisioning algorithm dimensions traffic aggregates at the network ingress taking into account fairness issues not only across different traffic aggregates, but also within the same aggregate whose packets take different routes in a core IP network. We demonstrate through analysis and simulation that our model is capable of delivering capacity provisioning in an efficient manner providing quantitative delay-bounds with differentiated loss across per-aggregate service classes...|$|R
40|$|In {{this thesis}} {{the use of}} {{state-space}} models for analysis and classification of time series data, gathered from industrial manufacturing processes and the life sciences, is investigated. To overcome hitherto unsolved problems in both application domains the temporal behavior of the data is captured using state-space models. Industrial laser welding processes are monitored with a high speed camera {{and the appearance of}} unusual events in the image sequences correlates with errors on the produced part. Thus, novel classification frameworks are developed to robustly detect these unusual events with a small false positive rate. For classifier learning, class labels are by default only available for the complete image sequence, since scanning the sequences for anomalies is expensive. The first framework combines appearance based features and state-space models for the unusual event detection in image sequences. For the first time, ideas adapted from face recognition are used for the <b>automatic</b> <b>dimension</b> reduction of images recorded from laser welding processes. The state-space model is trained incrementally and can learn from erroneous sequences without the need of manually labeling the position of the error event within sequences. %The limitation to weakly labeled data helps to reduce the labeling effort. In addition, a second framework for the object-based detection of sputter events in laser welding processes is developed. The framework successfully combines for the first time temporal change detection, object tracking and trajectory classification for the detection of weak sputter events. %This {{is the first time that}} object tracking is successfully applied to automatic sputter detection. For the application in the life sciences the improvement and further development of data analysis methods for Single Molecule Fluorescence Spectroscopy (SMFS) is considered. SMFS experiments allow to study biochemical processes on a single molecule basis. The single molecule is excited with a laser and the photons which are emitted thereon by fluorescence contain important information about conformational changes of the molecule. Advanced statistical analysis techniques are necessary to infer state changes of the molecule from changes in the photon emissions. By using state-space models, it is possible to extract information from recorded photon streams which would be lost with traditional analysis techniques...|$|R
40|$|The {{degradation}} of the wall in large cardiovascular vessels, such as the aorta artery, induces weakness in the vessel {{that can lead to}} the formation of aneurysms and the rupture of the vessel. Characterization of the wall integrity is assessed by OCT for future intraoperative assistance in aneurysm graft surgery interventions. Optical Coherence Tomography (OCT) provides cross sectional images of the wall of the aortic media layer. Wall degradations appear as spatial anomalies in the reflectivity profile through the wall thickness. Wall degradation assessment is proposed by <b>automatic</b> identification and <b>dimensioning</b> of these anomalies within the homogeneous surrounding tissue...|$|R
40|$|Rapid {{prototyping}} is {{fast and}} <b>automatic</b> three <b>dimensions</b> physical modeling that uses {{computer aided design}} model as the input. One of the important requirements in various products is the surface quality. Therefore, {{the aim of this}} research is to study and then develop a model that shows the influence of depth of cut, feed rate, and step-over on the vertical and horizontal surface roughness of polycarbonate material in subtractive rapid prototyping. The subtractive rapid prototyping process is performed by using Roland MDX 40 machine assisted by CAM Modela Player 4. 0 software. This research implements response surface methodology to develop the model and then followed by the residual tests. The result shows that the increase of the depth of cut and the interaction between the step-over and the depth of cut will increase the horizontal surface roughness. Meanwhile, the vertical surface roughness will be affected mostly by the step-over. This research provides an insight on how to rapid prototype the polycarbonate material in order to achieve the surface requirement. The result of this research is the basis for achieving the main purpose of subtractive rapid prototyping which are maximum material rate removal and the minimum surface roughness...|$|R
40|$|Efficient network {{provisioning}} {{mechanisms that}} support service differentiation and <b>automatic</b> capacity <b>dimensioning</b> {{are essential to}} the realization of the Differentiated Services (DiffServ) Internet. Building on our prior work on edge provisioning, we propose a set of efficient dynamic node and core provisioning algorithms for interior nodes and core networks, respectively. The node provisioning algorithm prevents transient violations of service level agreements by predicting the onset of service level violations based on a multi-class virtual queue measurement technique, and by automatically adjusting the service weights of weighted fair queueing schedulers at core routers. Persistent service level violations are reported to the core provisioning algorithm, which dimensions traffic aggregates at the network ingress edge. The core provisioning algorithm is designed to address the difficult problem of provisioning DiffServ traffic aggregates (i. e., rate-control can only be exerted at the root of any traffic distribution tree) by taking into account fairness issues not only across different traffic aggregates but also within the same aggregate whose packets take different routes through a core IP network. We demonstrate through analysis and simulation that the proposed dynamic provisioning model is superior to static provisioning for DiffServ in providing quantitative delay bounds with differentiated loss across peraggregate service classes under persistent congestion and device failure conditions when observed in core networks...|$|R
40|$|Subtractive rapid {{prototyping}} is fast and <b>automatic</b> three <b>dimensions</b> physical modelling that uses {{computer aided design}} model as the input. The dimensional accuracy of {{the result of the}} subtractive {{rapid prototyping}} is influenced by its process parameters. The aim of this research is to study and then develop a model that shows the influence of depth of cut, feed rate, and step-over on the vertical length error, horizontal length error, and depth error in subtractive rapid prototyping of polycarbonate material. This research implements response surface methodology to develop the model and then followed by the residual tests to evaluate the developed model. The result shows that the increase of the feed rate and the step-over will increase the horizontal dimension error. The most influenced factor on the horizontal dimension error is the step-over. Meanwhile, the vertical dimension error will be affected mostly by the step-over. Last, the depth error is influenced by the feed rate, the step-over, and the depth of cut. The depth of cut is the most critical factor that increases the depth error. The developed models give an insight on how several process parameters of rapid prototyping will influence the dimensional accuracy of a polycarbonate material. Based on the model, efficient resources utilization can be achieved...|$|R
40|$|Abstract — Efficient network {{provisioning}} {{mechanisms that}} support service differentiation and <b>automatic</b> capacity <b>dimensioning</b> {{are essential to}} the realization of the Differentiated Services (DiffServ) Internet. Building on our prior work on edge provisioning, we propose a set of efficient dynamic node and core provisioning algorithms for interior nodes and core networks, respectively. The node provisioning algorithm prevents transient violations of service level agreements by predicting the onset of service level violations based on a multi-class virtual queue measurement technique, and by automatically adjusting the service weights of weighted fair queueing schedulers at core routers. Persistent service level violations are reported to the core provisioning algorithm, which dimensions traffic aggregates at the network ingress edge. The core provisioning algorithm is designed to address the difficult problem of provisioning DiffServ traffic aggregates (i. e., rate-control can only be exerted at the root of any traffic distribution tree) by taking into account fairness issues not only across different traffic aggregates but also within the same aggregate whose packets take different routes through a core IP network. We demonstrate through analysis and simulation that the proposed dynamic provisioning model is superior to static provisioning for DiffServ in providing quantitative delay bounds with differentiated loss across peraggregate service classes under persistent congestion and device failure conditions when observed in core networks...|$|R
40|$|Biological {{processes}} often produce signals detectable {{by multiple}} means, but with entirely different information content. For both research and clinical applications concurrent multimodality data collection {{plays an important}} role in understanding the signal sources. Electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) are particularly interesting examples, in that each offers largely independent yet complementary information on neuronal activity. While we and others have made great strides in making concurrent EEG-fMRI recordings possible, the EEG data, in particular, still contain signal artifacts of cardiac origin (ballistocardiogram) that make EEG analysis difficult, or even impossible. To date, no satisfying means to separate brain EEG signal and ballistocardiogram (BCG) exist especially for non-event-related-potential experiments and under 3 -T MR scanner. The BCG presents high temporal non-stationarity due to variation in cardiac cycles, and its amplitude scales with magnetic field strength. This explains the considerable variation of success levels among studies, with more successful applications achieved at lower field strength. Previously published methods used one of blind source separation methods to remove the BCG. All such blind source separation approaches are limited to performing component extraction based on the contaminated data alone, agnostic of the structural difference between BCG and EEG. Another kind of approach is to utilize reference signals for the artifact itself. However, this requires purpose-built hardware and exploits no further denoising step besides a simple subtraction. We have developed three algorithms to separate EEG signal and BCG artifacts. Firstly, we have designed a Direct Recording - Prior Encoding (DRPE) method to maximally incorporate prior knowledge of BCG/EEG subspaces described by bases learned from a modified recording configuration, and of the group sparsity characteristics in the signal. To further promote subspace separability, a Direct Recording Joint Incoherent Basis (DRJIB) method is proposed to learn a representative and sparse set of BCG and EEG bases by minimizing a cost function consisting of group sparsity penalties for <b>automatic</b> <b>dimension</b> selection and an energy term for encouraging incoherence. Reconstruction is subsequently obtained by fitting the contaminated data to a generative model using the learned bases subject to regularization. The third algorithm takes advantage of currently available high-density EEG cap, to reliably estimate the full-scalp BCG contribution from a near-optimal small subset (20 out of 256) of channels and a corresponding weight through our modified experimental setup using Orthogonal Matching Pursuit (OMP). We show in carefully constructed simulations that the residual artifacts are reduced by several orders of magnitude to a tiny fraction of the true signal. In human studies we show that the methods work effectively, and validate our quantitative results. Beyond the application to the EEG-fMRI challenge, we expect that our algorithmic methods will have impact in many other domains where signal and contaminant have distinct information structures. Digital signal, imaging, or even financial data are also contaminated with noises so that studying and characterizing the information structures of desired and undesired signals would greatly improve the modeling power...|$|R
40|$|Colouration and {{patterning}} are widespread amongst organisms. Regarding avian eggs, colouration (reflectances) {{has been}} previously measured using spectrometers whereas spottiness has been determined using human-based scoring methods or by applying global thresholding over the luminance channel on photographs. However, the availability of powerful computers and digital image-processing algorithms and software offers new possibilities to develop systematised, automatable, and accurate methods to characterise visual information in eggs. Here, we provide a computing infrastructure (library of functions and a Graphical User Interface) for eggshell colouration and spottiness analysis called SpotEgg, which runs over MATLAB. Compared to previous methods, our method offers four novelties for eggshell visual analysis. First, we have developed a standardised non-human biased method to determine spottiness. Spottiness determination is based on four parameters that allow direct comparisons between studies and may improve results when relating colouration and patterning to pigment extraction. Second, researcher time devoted to routine tasks is remarkably reduced thanks to the incorporation of image-processing techniques that automatically detect the colour reference chart and egg-like shapes in the scene. Third, SpotEgg reduces the errors in colour estimation through the eggshell that are created by the different angles of view subtended {{from different parts of}} the eggshell and the optical centre of the camera. Fourth, SpotEgg runs <b>automatic</b> Fractal <b>Dimension</b> analysis (a measure of how the details in a pattern change with the scale at which this pattern is measured) of the spots pattern in case researchers want to relate other measurements with this special spatial pattern. Finally, although initially conceived for eggshell analysis, SpotEgg can also be applied in images containing objects different from eggs as feathers, frogs, insects, etc., since it allows the user to manually draw any region to be analysed making this tool useful not only for oologist but also for other evolutionary biologists. Peer reviewe...|$|R
40|$|The {{external}} {{platform of}} the International Space Station (ISS) {{will provide a}} unique testbed for exobiological studies of processes under free space conditions. To this end, ESA is developing the EXPOSE facility {{that is to be}} attached to the External Pallet of the truss structure of the ISS for 1. 5 years during the ISS early utilization period. EXPOSE will support long term in situ studies of microbes in artificial meteorites as well as of microbial communities from special ecological niches, such as endolithic and endoevaporitic ecosystems. The Radiation Risks Radiometer-Dosimeter (R 3 D) is a low mass and small <b>dimensions</b> <b>automatic</b> device, which will measure solar radiation in 4 channels and cosmic ionizing radiation. The four-channel: UV-A (315 - 400 nm), UV-B (280 - 315 nm), UV-C (< 280 nm) and Photosynthetic Active Radiation (PAR) (400 - 700 nm) filter dosimeter will measure the solar UV irradiance in W/m 2. Additional measurements of the temperature of the UV detectors are performed for more precise UV irradiance measurements. The deposited energy spectra of the cosmic ionizing radiation will be measured in a 256 -channel spectrometer. The analysis of the spectra will give as well the total dose in µGy/h and the particl...|$|R
40|$|This {{graduation}} thesis {{covers the}} design of medium and large marshalling yards including the dimensioning of their working parts. Firstly, the thesis defines marshalling yard divisioning according to function, design, shape and use of automatic equipment. It furthermore describes the workflow at half automatic and fully automatic marshalling yards, based on {{the data for the}} only Slovenian marshalling yard in Zalog. The next section describes various marshalling yard devices with special emphasis on shorter turnouts specific for hump yards, and different brake systems according to the yard <b>automatic</b> equipment. The <b>dimensioning</b> of different track groups of a marshalling yard and the requirements for upper and lower track superstructure are defined as a part of spatial placement. Different track groups with their longitudinal and transverse profile features, ground plans and equations for different calculation methods are presented. The latter include the number and length of the tracks, track capacity and their utilization. Where multiple methods of calculation can be used, choosing the right one is defined. The goal of the thesis is to check the existing dimensions of Zalog marshalling yard based on an actual capacity common for the last 12 years. For easier understanding, the role of Zalog marshalling yard is defined within the Slovenian and European rail network. Different measures for improvements are presented based on the comparison of the existing and new marshalling yard dimensioning data...|$|R
40|$|An AH (affine hypersurface) {{structure}} {{is a pair}} comprising a projective equivalence class of torsion-free connections and a conformal structure satisfying a compatibility condition which is <b>automatic</b> in two <b>dimensions.</b> They generalize Weyl structures, {{and a pair of}} AH structures is induced on a co-oriented non-degenerate immersed hypersurface in flat affine space. The author has defined for AH structures Einstein equations, which specialize on the one hand to the usual Einstein Weyl equations and, on the other hand, to the equations for affine hyperspheres. Here these equations are solved for Riemannian signature AH structures on compact orientable surfaces, the deformation spaces of solutions are described, and some aspects of the geometry of these structures are related. Every such {{structure is}} either Einstein Weyl (in the sense defined for surfaces by Calderbank) or is determined by a pair comprising a conformal structure and a cubic holomorphic differential, and so by a convex flat real projective structure. In the latter case it can be identified with a solution of the Abelian vortex equations on an appropriate power of the canonical bundle. On the cone over a surface of genus at least two carrying an Einstein AH structure there are Monge-Amp`ere metrics of Lorentzian and Riemannian signature and a Riemannian Einstein K"ahler affine metric. A mean curvature zero spacelike immersed Lagrangian submanifold of a para-K"ahler four-manifold with constant para-holomorphic sectional curvature inherits an Einstein AH structure, and this is used to deduce some restrictions on such immersions...|$|R
40|$|Consider an Ito ̂ process X {{satisfying}} the stochastic differential equation dX = a(X) dt + b(X) dW where a, b are smooth and W is a multidimensional Brownian motion. Suppose that Wn has smooth sample paths and that Wn converges weakly to W. A central ques-tion in stochastic analysis {{is to understand}} the limiting behaviour of solutions Xn to the ordinary differential equation dXn = a(Xn) dt+ b(Xn) dWn. The classical Wong-Zakai theorem gives sufficient conditions under which Xn converges weakly to X provided that the stochastic integral∫ b(X) dW is given the Stratonovich interpretation. The sufficient conditions are <b>automatic</b> in one <b>dimension,</b> but in higher dimensions the correct interpretation of b(X) dW depends sensitively on how the smooth approximation Wn is chosen. In applications, a natural class of smooth approximations arise by setting Wn(t) = n − 1 / 2 ∫ nt 0 v ◦ φs ds where φt is a flow (generated for instance by an ordinary differential equation) and v is a mean zero observable. Under mild conditions on φt we give a definitive answer to the interpretation question for the stochastic integral b(X) dW. Our theory applies to Anosov or Axiom A flows φt, {{as well as to}} a large class of nonuniformly hyperbolic flows (including the one defined by the well-known Lorenz equations) and our main results do not require any mixing assumptions on φt. The methods used in this paper are a combination of rough path theory and smooth ergodic theory...|$|R
40|$|Develops concepts, {{methods and}} a {{prototype}} for an audio framework that describes sounds on a highly abstract semantic level. We describe each sound {{as the result}} of interactions between objects at a certain place and in a certain environment. The interaction attributes influence the generated sound. Simultaneously, the participating objects can consist of different physical conditions, materials and configurations. All relevant attributes have an influence on the generated sound. The hearing of sounds in everyday life is based on the perception of events, not on the perception of sounds as such, so sounds are often described by the events they are based on. A concept for the description of sounds is presented, in which sounds can be represented as auditory signal patterns along several descriptive dimensions of various objects interacting together in a certain environment. On the basis of the differentiation of purely physical and purely semantic descriptive <b>dimensions,</b> <b>automatic</b> sound generation is discussed on the physical and semantic levels. We describe the sound class `solid objects', because this class occurs frequently in everyday life, the interacting objects can be easily described by their material characteristics and the knowledge of solid-state physics can be used. The falling of a spherical elastic object on to a linear elastic beam is modelled, and implemented on a SGI workstation. The main parameters which influence the impact behaviour of such objects are discussed. A better understanding of the capabilities, restrictions and problems of existing instruments for the automatic generation of audio data can be anticipate...|$|R

