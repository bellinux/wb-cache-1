41|3|Public
5000|$|<b>Automatic</b> <b>failover</b> {{to another}} {{controller}} (transparent to computers transmitting data) ...|$|E
5000|$|... 4G LTE/3G Backup for {{fixed-line}} connections, with <b>automatic</b> <b>failover</b> {{for business}} continuity ...|$|E
5000|$|... failureThreshold - {{the number}} of {{consecutive}} getConnection (...) failures that must occur before <b>automatic</b> <b>failover</b> is invoked ...|$|E
5000|$|MC-LAG adds node-level {{redundancy}} to {{the normal}} link-level redundancy that a LAG provides. This allows two or more nodes to share a common LAG endpoint. The multiple nodes present a single logical LAG to the remote end. Note that MC-LAG implementations are vendor-specific, but cooperating chassis remain externally compliant to the IEEE 802.1AX-2008 standard. [...] Nodes in an MC-LAG cluster communicate to synchronize and negotiate <b>automatic</b> switchovers (<b>failover).</b> Some implementations may support administrator-initiated (manual) switchovers.|$|R
5000|$|Global {{distribution}}. Global distribution {{was added}} to DocumentDB's capability in 2016. This feature lets you scale your DocumentDB instance across different regions {{around the world and}} define what type of consistency you expect between the regions, from strong to eventual. It is even possible to configure an <b>automatic</b> and transparent <b>failover</b> for a given region.|$|R
40|$|This paper {{proposes a}} novel {{way to use}} virtual memory mapped {{communication}} (VMMC) to reduce the failover time on clusters. With the VMMC model, applications' virtual address space can be efficiently mirrored on remote memory either automatically or via explicit messages. When a machine fails, its applications can restart from the most recent checkpoints on the failover node with minimal memory copying and disk I/O overhead. This method requires little change to applications' source code. We developed two fast failover protocols: deliberate update failover protocol (DU) and <b>automatic</b> update <b>failover</b> protocol (AU). The rst can run on any system that supports VMMC, whereas the other requires special network interface support. We implemented these two protocols [...] ...|$|R
5000|$|... {{alternate}}ResourceJNDIName - the JNDI name of {{the alternate}} connection factory to use if <b>automatic</b> <b>failover</b> is invoked ...|$|E
50|$|Service Pack 1 (SP1) of SQL Server 2005 {{introduced}} Database Mirroring, a {{high availability}} option that provides redundancy and failover capabilities at the database level. Failover {{can be performed}} manually or can be configured for <b>automatic</b> <b>failover.</b> <b>Automatic</b> <b>failover</b> requires a witness partner and an operating mode of synchronous (also known as high-safety or full safety). Database Mirroring {{was included in the}} first release of SQL Server 2005 for evaluation purposes only. Prior to SP1, it was not enabled by default, and was not supported by Microsoft.|$|E
50|$|All {{models of}} the Hitachi Adaptable Modular Storage 2000 family support host path failover, fully redundant, hot swappable components, online {{microcode}} updates and mirrored cache with battery backup. The active-active controllers include <b>automatic</b> <b>failover.</b>|$|E
5000|$|Automatic {{switchover}} of a {{redundant system}} on an error condition, without human intervention, is called failover. Manual switchover on error {{would be used}} if <b>automatic</b> <b>failover</b> is not available, possibly because the overall system is too complex.|$|E
50|$|With {{integrated}} {{support for}} clustering, GoAnywhere MFT can process high volumes of file transfers for enterprises by load balancing processes across multiple systems. The clustering technology in GoAnywhere MFT also provides active-active <b>automatic</b> <b>failover</b> for disaster recovery.|$|E
5000|$|In July, 2009 {{the company}} {{announced}} the second generation of Truffle (BBNA6401), calling it the Truffle(BBNA5201G). [...] The second generation has higher throughput, enhanced quality-of-service (QoS), and <b>automatic</b> <b>failover</b> {{as well as some}} other enterprise level features.|$|E
50|$|Windows NT Load Balancing Service (WLBS) is {{a feature}} of Windows NT that {{provides}} load balancing and clustering for applications. WLBS dynamically distributes IP traffic across multiple cluster nodes, and provides <b>automatic</b> <b>failover</b> {{in the event of}} node failure. WLBS was replaced by Network Load Balancing Services in Windows 2000.|$|E
5000|$|High {{availability}} implementations {{that rely}} on database replication, with <b>automatic</b> <b>failover</b> to an identical standby database {{in the event of}} primary database failure. To protect against loss of data {{in the case of a}} complete system crash, replication of an IMDB is normally used in addition to one or more of the mechanisms listed above.|$|E
50|$|SpaceWire and IEEE 1355 DS-DE {{allows for}} a wider set of speeds for data transmission, and some new {{features}} for <b>automatic</b> <b>failover.</b> The fail-over features let data find alternate routes, so a spacecraft can have multiple data buses, and be made fault-tolerant. SpaceWire also allows the propagation of time interrupts over SpaceWire links, {{eliminating the need for}} separate time discretes.|$|E
50|$|A cluster-aware {{application}} is a software application designed to call cluster APIs {{in order to}} determine its running state, in case a manual failover is triggered between cluster nodes for planned technical maintenance, or an <b>automatic</b> <b>failover</b> is required, if a computing cluster node encounters hardware or software failure, to maintain business continuity. A cluster-aware application may be capable of failing over LAN or WAN.|$|E
50|$|In general, {{a network}} {{utilizing}} Active Directory {{has more than}} one licensed Windows server computer. Backup and restore of Active Directory is possible for a network with a single domain controller, but Microsoft recommends more than one domain controller to provide <b>automatic</b> <b>failover</b> protection of the directory. Domain controllers are also ideally single-purpose for directory operations only, and should not run any other software or role.|$|E
5000|$|Anycast is {{normally}} highly reliable, {{as it can}} provide <b>automatic</b> <b>failover.</b> Anycast applications typically feature external [...] "heartbeat" [...] monitoring of the server's function, and withdraw the route announcement if the server fails. In some cases this is done by the actual servers announcing the anycast prefix to the router over OSPF or another IGP. If the servers die, the router will automatically withdraw the announcement.|$|E
50|$|Status request failures, {{such as when}} a {{connection}} cannot be established, it times-out, or the document or message cannot be retrieved, usually produce an action from the monitoring system. These actions vary; An alarm may be sent (via SMS, email, etc.) to the resident sysadmin, <b>automatic</b> <b>failover</b> systems may be activated to remove the troubled server from duty until it can be repaired, etc.|$|E
5000|$|Metadata server (MDS) — {{manages the}} {{location}} (layout) of files, file access and namespace hierarchy. The {{current version of}} MooseFS does support multiple metadata servers and <b>automatic</b> <b>failover.</b> Clients only talk to the MDS to retrieve/update a file's layout and attributes; the data itself is transferred directly between clients and chunk servers. The Metadata server is a user-space daemon; the metadata is kept in memory and lazily stored on local disk.|$|E
50|$|Server {{farms are}} {{increasingly}} being used instead of or in addition to mainframe computers by large enterprises, although server farms do not yet reach the same reliability levels as mainframes. Because of {{the sheer number of}} computers in large server farms, the failure of an individual machine is a commonplace event, and the management of large server farms needs to take this into account by providing support for redundancy, <b>automatic</b> <b>failover,</b> and rapid reconfiguration of the server cluster.|$|E
5000|$|High {{availability}} - Since {{a binary}} repository manager maintains all the development dependencies, {{it is vital}} to always maintain access to these artifacts. Any down-time of the binary repository manager can halt development with all the significant consequences to the organization. A high availability instance allows an enterprise to overcome the risk associated with downtime, through <b>automatic</b> <b>failover.</b> This is achieved by having a redundant set of repository managers work against the same database and file storage. Maintaining enterprise wide stability and performance at all times ...|$|E
50|$|Although {{the actual}} {{failover}} mechanism in log shipping is manual, this implementation is often chosen {{due to its}} low cost in human and server resources, and ease of implementation. In comparison, SQL server clusters enable <b>automatic</b> <b>failover,</b> but {{at the expense of}} much higher storage costs. Compared to database replication, log shipping does not provide as much in terms of reporting capabilities, but backs up system tables along with data tables, and locks the standby server from users' modifications. A replicated server can be modified (e.g. views) and is therefore unsuitable for failover purposes.|$|E
5000|$|The eXtremeDB high {{availability}} edition supports both synchronous (2-safe) and asynchronous (1-safe) database replication, with <b>automatic</b> <b>failover.</b> [...] eXtremeDB Cluster edition provides for shared-nothing database clustering. eXtremeDB also supports distributed query processing, {{in which the}} database is partitioned horizontally and the DBMS distributes query processing across multiple servers, CPUs and/or CPU cores. eXtremeDB supports heterogeneous client platforms (e.g. a mix of Windows, Linux and RTOSs) with its clustering and {{high availability}} features. A single partitioned database can include shards running on a mix of hardware and OS platforms ...|$|E
50|$|The Israeli {{university}} {{telecommunications infrastructure}} {{is based on}} a dual-star, eight-node network known as the ILAN-2 network. Interconnecting the network's two central points of presence (POPs), one located at Tel Aviv University and the other at a neutral colocation site called Med-1, is a dark fiber link operating at 10 Gbit/s. Each of the eight Israeli universities in the consortium connects to both POPS via a primary 10Gbit/s and a 1Gbit/s backup link. These links are provided by Cellcom (primary) and Partner Communications Company (failover). <b>Automatic</b> <b>failover</b> of these links is handled by the Open Shortest Path First routing protocol.|$|E
50|$|The {{functionality}} of IBM Spectrum Virtualize {{is provided}} by IBM SAN Volume Controller. IBM Spectrum Virtualize is a block storage virtualization system. Because the IBM Storwize V7000 uses SVC code, {{it can also be}} used to perform storage virtualization in exactly the same way as SVC. Since mid-2012 it offers real time compression with no performance impact, saving up to 80% of disk utilization. SVC can be configured on a Stretched Cluster Mode, with <b>automatic</b> <b>failover</b> between two datacenters and can have SSD (Solid State Drives) that can be used by EasyTier software to perform sub-LUN automatic tiering.|$|E
5000|$|In such systems, {{the spare}} {{processors}} do {{not contribute to}} system throughput between failures, but merely redundantly execute exactly the same data thread as the active processor at the same instant, in [...] "lock step". Faults are detected by seeing when the cloned processors' outputs diverged. To detect failures, the system must have two physical processors for each logical, active processor. To also implement <b>automatic</b> <b>failover</b> recovery, the system must {{have three or four}} physical processors for each logical processor. The triple or quadruple cost of this sparing is practical when the duplicated parts are commodity single-chip microprocessors.|$|E
50|$|This {{architecture}} {{has multiple}} uses, including workload balancing and elastic virtual data marts. Workload balancing {{is achieved by}} the SAP IQ query engine through dynamically increasing/decreasing parallelism in response to changes in server activity. There is <b>automatic</b> <b>failover</b> if a node stops participating in a query, and other nodes will pick up work originally assigned to the failed node so the query can complete. On the client side, compatibility with external load balances ensures that queries are initiated on physical servers in a balanced fashion to eliminate bottlenecks. Physical nodes in the Multiplex can be grouped together into “logical servers” which allow workloads to be isolated from each other (for security or resource balancing purposes); machines {{can be added to}} these as demand changes. The aim of the grid architecture is to enable resiliency even during global transactions.|$|E
5000|$|The HP ConvergedSystem 500 for SAP HANA {{operates}} the SAP HANA in-memory data management platform for data analytics and data warehousing. It {{has been reported}} that the system can process data analytics twice as fast as competing systems for SAP HANA. The HP ConvergedSystem 500 for SAP HANA includes ConvergedSystem 500 hardware and HP ServiceGuard for SAP HANA, a data management tool that protects against unscheduled downtime by providing the capability for <b>automatic</b> <b>failover.</b> The HP ConvergedSystem 500 for SAP HANA enables users to run data analytics and Enterprise resource planning (ERP) on the same workload-based system. It {{has been reported that}} data analytics processes that used to take days or weeks to run can now be accessed in real-time. The HP ConvergedSystem 900 for SAP HANA is designed to manage and analyze very large and varied data sets. [...] It has been reported that the HP ConvergedSystem 900 for SAP HANA is capable of supplying 12 terabytes of data in one memory pool.|$|E
50|$|In {{terrestrial}} {{point to}} point microwave systems ranging from 11 GHz to 80 GHz, a parallel backup link can be installed alongside a rain fade prone higher bandwidth connection. In this arrangement, a primary link such as an 80 GHz 1 Gbit/s full duplex microwave bridge may be calculated to have a 99.9% availability rate over the period of one year. The calculated 99.9% availability rate means that the link may be down for a cumulative total of ten or more hours per year as the peaks of rain storms pass over the area. A secondary lower bandwidth link such as a 5.8 GHz based 100 Mbit/s bridge may be installed parallel to the primary link, with routers on both ends controlling <b>automatic</b> <b>failover</b> to the 100 Mbit/s bridge when the primary 1 Gbit/s link is down due to rain fade. Using this arrangement, high frequency {{point to point}} links (23 GHz+) may be installed to service locations many kilometers farther than could be served with a single link requiring 99.99% uptime {{over the course of}} one year.|$|E
50|$|At {{least two}} {{units of the}} same type will be powered up, {{receiving}} {{the same set of}} inputs, performing identical computations and producing identical outputs in a nearly-synchronous manner. The outputs are typically physical outputs (individual ON/OFF type digital signals, or analog signals), or serial data messages wrapped in suitable protocols depending upon the nature of their intended use. Outputs from only one unit (designated as the master or on-line unit, via application logic) are used to control external devices (such as switches, signals, on-board propulsion/braking control devices, etc.) or simply to provide displays. The other unit is a hot-standby or a hot spare unit, ready to take over if the master unit fails. When the master unit fails, an <b>automatic</b> <b>failover</b> to the hot spare occurs within a very short time and the outputs from the hot spare, now the master unit, are delivered to the controlled devices and displays. The controlled devices and displays may experience a short blip or disturbance during the failover time. However, they can be designed to tolerate/ignore the disturbances so that the overall system operation is not affected.|$|E
40|$|This {{contribution}} {{details the}} deployment of Rucio, the ATLAS Distributed Data Management system. The main complication is that Rucio interacts {{with a wide variety}} of external services, and connects globally distributed data centres under different technological and administrative control, at an unprecedented data volume. It is therefore not possibly to create a duplicate instance of Rucio for testing or integration. Every software upgrade or configuration change is thus potentially disruptive and requires fail-safe software and automatic error recovery. Rucio uses a three-layer scaling and mitigation strategy based on quasi-realtime monitoring. This strategy mainly employs independent stateless services, <b>automatic</b> <b>failover,</b> and service migration. The technologies used for deployment and mitigation include OpenStack, Puppet, Graphite, HAProxy and Apache. In this contribution, the interplay between these component, their deployment, software mitigation, and the monitoring strategy are discussed...|$|E
40|$|SIP {{infrastructure}} on cloud platforms has {{the potential}} to be both scalable and highly available. In our previous project, we focused on the scalability aspect of SIP services on cloud platforms; the focus of this project is on the high availabil-ity aspect. We investigated the effects of component fault on service availability with the goal of understanding how high availability can be guaranteed even in the face of component faults. The experiments were conducted empirically on a real system that runs on Amazon EC 2. Our analysis shows that most compo-nent faults are masked with a simple <b>automatic</b> <b>failover</b> technique. However, we have also identified fundamental problems that cannot be addressed by simple failover techniques; a problem involving DNS cache in resolvers and a problem involving static failover configurations. Recommendations on how to solve thes...|$|E
40|$|The DCE {{standard}} includes {{specifications for}} the Directory Service, a component that performs typical naming services in distributed computing environments. We list some deficiencies in these specifications {{that affect the}} naming service availability and correctness, and suggest possible solutions. We then describe an enhancement of an implementation of the Directory Service that adds support for partial replication of the name space, continuous operation of the service, and <b>automatic</b> <b>failover.</b> Our extensions ensure {{the consistency of the}} name space data, and are transparent to application developers and end users, all without a significant performance penalty. 1. Introduction We are building a toolset [2] to provide high availability for distributed applications that adhere to the Common Object Request Broker Architecture (CORBA) standard [3]. The toolset consists of a number of software-based techniques for high availability that are automatically inserted by an enhanced CORBA compi [...] ...|$|E
40|$|Abstract-This paper shows <b>automatic</b> <b>failover</b> {{and load}} {{balancing}} for Oracle real Application Clusters. rac {{enables you to}} use clustered hardware by running multiple instances against the same database. The database files are stored on disks that are either physically or logically connected to each node, so that every active instance can read from or write to them. Oracle Real Application Clusters manages data access, so that changes are coordinated between the instances and each instance sees a consistent image of the database. The Cluster Interconnect enables instances to pass coordination information and data images between each other. The architecture enables users and application {{to benefit from the}} processing power of multiple machines. Oracle RAC architecture also achieves redundancy in the case of, for example, a system crashing or becoming unavailable; the application can still access the database on any surviving instances. Inexd Terms – Oracle, real application, clusters, rac architecture, clusterware, loadbalancing, server...|$|E
40|$|In {{this paper}} we {{describe}} a software infrastructure that unifies replication and transaction processing in three-tier architectures and, thus, provides high availability and fault tolerance for enterprise applications. The infrastructure {{is based on}} the Fault Tolerant CORBA and CORBA Object Transaction Service standards, and works with commercial-off-the-shelf application servers and database systems. The infrastructure replicates the application servers to protect the business logic processing. In addition, it repli-cates the transaction coordinator, which renders the two-phase commit protocol non-blocking and, thus, avoids po-tentially long service disruptions caused by coordinator failure. The infrastructure handles the interactions between the application servers and the database servers through replicated gateways that prevent duplicate requests from reaching the database servers. The infrastructure imple-ments client-side <b>automatic</b> <b>failover</b> mechanisms, which guarantees that clients know the outcome of the requests that they have made. The infrastructure starts the transac-tions at the application servers, and retries aborted trans-actions, caused by process or communication failures, au-tomatically on the behalf of the clients. ...|$|E
