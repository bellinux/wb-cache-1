271|64|Public
2500|$|The {{autocorrelation}} function, {{more properly}} called the <b>autocovariance</b> <b>function</b> {{unless it is}} normalized in some appropriate fashion, measures {{the strength of the}} correlation between the values of [...] separated by a time lag. This is a way of searching for the correlation of [...] with its own past. It is useful even for other statistical tasks besides the analysis of signals. For example, if [...] represents the temperature at time , one expects a strong correlation with the temperature at a time lag of 24 hours.|$|E
5000|$|... {{is called}} the <b>autocovariance</b> <b>function</b> of the time series. By the mean zero assumption, ...|$|E
5000|$|If {{the process}} is {{continuous}} and purely indeterministic, the <b>autocovariance</b> <b>function</b> can be reconstructed by using the Inverse Fourier transform ...|$|E
50|$|Long-range and {{short-range}} dependent {{processes are}} characterised by their <b>autocovariance</b> <b>functions.</b>|$|R
40|$|The de nition and {{properties}} of Levy-driven CARMA (continuous-time ARMA) processes are reviewed. Gaussian CARMA processes are special {{cases in which}} the driving Levy process is Brownian motion. The use of more general Levy processes permits the speci cation of CARMA processes with a wide variety ofmarginal distributions which may be asymmetric and heavier tailed than Gaussian. Non-negative CARMA processes are of special interest, partly because of the introduction by Barndor-Nielsen and Shephard (2001) of non-negativeLevy-driven Ornstein-Uhlenbeck processes as models for stochastic volatility. Replacing the Ornstein-Uhlenbeck process byaLevy-driven CARMA process with non-negative kernel permits the modelling of non-negative, heavy-tailed processes with a considerably larger range of <b>autocovariance</b> <b>functions</b> than is possible in the Ornstein-Uhlenbeck framework. We also de ne a class of zero-mean fractionally integrated Levy-driven CARMA processes, obtained by convoluting the CARMA kernel with a kernel corresponding to Riemann-Liouville fractional integration, and derive explicit expressions for the kernel and <b>autocovariance</b> <b>functions</b> of these processes. They are long-memory in the sense that their kernel and <b>autocovariance</b> <b>functions</b> decay asymptotically at hyperbolic rates depending on the order of fractional integration. In order to introduce long-memory into non-negative Levy-driven CARMA processes we replace the fractional integration kernel with a closely related absolutely integrable kernel. This gives a class of stationary non-negative continuous-time Levy-driven processes whose <b>autocovariance</b> <b>functions</b> at lag h also converge to zero at asymptotically hyperbolic rates...|$|R
40|$|Abstract: The denition and {{properties}} of Levy-driven CARMA (continuous-time ARMA) processes are reviewed. Gaussian CARMA processes are special {{cases in which}} the driving Levy process is Brownian motion. The use of more general Levy processes permits the specication of CARMA processes {{with a wide variety of}} marginal distributions which may be asymmetric and heavier tailed than Gaussian. Non-negative CARMA processes are of special interest, partly because of the in-troduction by Barndor-Nielsen and Shephard (2001) of non-negative Levy-driven Ornstein-Uhlenbeck processes as models for stochastic volatility. Replacing the Ornstein-Uhlenbeck process by a Levy-driven CARMA process with non-negative kernel permits the modelling of non-negative, heavy-tailed processes with a con-siderably larger range of <b>autocovariance</b> <b>functions</b> than is possible in the Ornstein-Uhlenbeck framework. We also dene a class of zero-mean fractionally integrated Levy-driven CARMA processes, obtained by convoluting the CARMA kernel with a kernel corresponding to Riemann-Liouville fractional integration, and derive ex-plicit expressions for the kernel and <b>autocovariance</b> <b>functions</b> of these processes. They are long-memory in the sense that their kernel and <b>autocovariance</b> <b>functions</b> decay asymptotically at hyperbolic rates depending on the order of fractional inte-gration. In order to introduce long-memory into non-negative Levy-driven CARMA processes we replace the fractional integration kernel with a closely related abso-lutely integrable kernel. This gives a class of stationary non-negative continuous-time Levy-driven processes whose <b>autocovariance</b> <b>functions</b> at lag h also converge to zero at asymptotically hyperbolic rates. Key words and phrases: Continuous-time ARMA process, Levy process, stochastic volatility, long memory, fractional integration...|$|R
5000|$|The {{spectral}} density function is the Fourier transform of the <b>autocovariance</b> <b>function.</b> In discrete terms {{this will be}} the discrete-time Fourier transform: ...|$|E
5000|$|Being {{based on}} the fourier transform, the PSD is a linear {{function}} of the <b>autocovariance</b> <b>function</b> {{in the sense that}} if [...] is decomposed into two functions , then ...|$|E
5000|$|Some authors {{refer to}} [...] as the <b>autocovariance</b> <b>function.</b> They then proceed to {{normalise}} it, by dividing by , to obtain what {{they refer to}} as the autocorrelation function.|$|E
5000|$|Let [...] {{represent}} {{a pair of}} stochastic processes that are jointly wide sense stationary with <b>autocovariance</b> <b>functions</b> [...] and [...] and cross-covariance function [...] Then the cross-spectrum [...] {{is defined as the}} Fourier transform of ...|$|R
40|$|In {{this paper}} we propose a method to derive the {{spectral}} density function of Markov switching ARMA models. We apply the Riesz-Fischer theorem which defines the spectral representation as the Fourier transform of the <b>autocovariance</b> <b>functions.</b> Multivariate ARMA models Regime switching models Markov switching models Frequency domain...|$|R
40|$|A general {{method is}} {{presented}} to explicitly compute <b>autocovariance</b> <b>functions</b> for non-Poisson dichotomous noise based on renewal theory. The method is specialized to a random telegraph signal of Mittag-Leffler type. Analytical predictions are compared to Monte Carlo simulations. Non-Poisson dichotomous noise is non-stationary and standard spectral methods fail to describe it properly as they assume stationarity. Comment: 13 pages, 3 figures, submitted to PR...|$|R
5000|$|... where m &#61; 0, ..., p, {{yielding}} p + 1 equations. Here [...] is the <b>autocovariance</b> <b>function</b> of Xt, [...] is {{the standard}} deviation of the input noise process, and [...] is the Kronecker delta function.|$|E
5000|$|For example, let z be an m-th root {{of unity}} (with the current identification, this is 1/m ∈ 0,1) and f be a random {{variable}} of mean 0 and variance 1. Consider the time series [...] The <b>autocovariance</b> <b>function</b> is ...|$|E
5000|$|It can be {{seen that}} the <b>autocovariance</b> <b>function</b> decays with a decay time (also called time constant) of [...] see this, write [...] where [...] is {{independent}} of [...] Then note that [...] and match this to the exponential decay law [...]|$|E
40|$|In {{this paper}} {{we show that}} the underdetermined ICA problem can be solved using a set of spatial {{covariance}} matrices, in case the sources have sufficiently different temporal <b>autocovariance</b> <b>functions.</b> The result {{is based on a}} link with the decomposition of higher-order tensors in rank-one terms. We discuss two algorithms and present theoretical bounds on the number of sources that can be allowed. status: publishe...|$|R
3000|$|The {{question}} of history-dependent asymptotic behaviour {{is of interest}} not only in demography and population dynamics, but also in financial mathematics and time series, and this also motivates our study here. It {{is well known that}} certain discrete- and continuous-time stochastic processes have <b>autocovariance</b> <b>functions</b> which can be represented as linear difference or delay-differential equations (see, e.g., Küchler and Mensch [3]). In the case of the so-called ARCH ([...] [...]...|$|R
30|$|As argued by Maravall (1987, 1999, 2003), {{the serial}} {{dependence}} {{structure of the}} estimators of the unobserved components can be {{a useful tool for}} model diagnostic. Large discrepancies between theoretical and empirical <b>autocovariance</b> <b>functions</b> of those estimators can be interpreted as indication of model misspecification. On this basis, Maravall (1987) suggested a (Gaussian) parametric bootstrap procedure to obtain confidence intervals for the empirical autocovariances of a single smoothed innovation. Similarly, Maravall (2003) derived expressions for the asymptotic variance of the sampling variances and autocorrelations of the smoothed components using classic results for linear stationary Gaussian processes (see e.g. Lomnicki and Zaremba 1959 or Anderson and Walker 1964). However, in both instances his main objective was to propose useful model diagnostics rather than deriving the null distribution of a formal statistical test. As we shall see in Sect.  3.2, our LM tests carry out the comparison between theoretical and empirical <b>autocovariance</b> <b>functions</b> of the smoothed components in a very precise statistical sense, taking into account both the sampling variability of the estimators of the parameters of the null model and the potential rank failure of the information matrix of the alternative model.|$|R
50|$|In signal processing, {{the above}} {{definition}} {{is often used}} without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is {{sometimes referred to as}} the autocorrelation coefficient or <b>autocovariance</b> <b>function.</b>|$|E
50|$|The same C(x, y) {{is called}} the <b>autocovariance</b> <b>function</b> in two instances: in time series (to denote exactly the same concept except that x and y refer to {{locations}} in time rather than in space), and in multivariate random fields (to refer to the covariance of a variable with itself, {{as opposed to the}} cross covariance between two different variables at different locations, Cov(Z(x1), Y(x2))).|$|E
50|$|One way of characterising {{long-range}} and short-range dependent {{stationary process}} is {{in terms of}} their autocovariance functions. For a short-range dependent process, the coupling between values at different times decreases rapidly as the time difference increases. Either the autocovariance drops to zero after a certain time-lag, or it eventually has an exponential decay. In the case of LRD, there is much stronger coupling. The decay of the <b>autocovariance</b> <b>function</b> is power-like and so is slower than exponential.|$|E
40|$|In situ {{measurements}} in {{the main}} borehole of the German continental deep drilling project (KTB) point to the statistical nature of the continental crystalline crust between 286 and 6000 m depth. P-wave, S-wave and density logs display deviations from Gaussian distribution functions that may be {{partly due to the}} approximately bimodal nature of the lithology, because the crust consists mainly of metabasites and paragneisses. The <b>autocovariance</b> <b>functions</b> and power spectra of the logs indicate that the velocity and density perturbations have a complex statistical nature, but the <b>autocovariance</b> <b>functions</b> can be approximated by a superposition of two exponential functions with correlation lengths of 1 m and 20 m. The associated relative standard deviations for each exponential function are about 3 per cent of the mean for the sonic logs, and nearly 6 per cent of the mean for the density. A vertical seismic profile measured between 3000 and 6000 m depth shows small traveltime fluctua-tions, which can be explained by a generalization of a recently published scattering theory for transverse wavefield fluctuations to include the case of longitudinal fluctuations. The theory is valid for harmonic scalar plane-wave propagation i...|$|R
40|$|For second order {{stationary}} processes, {{the spectral}} distribution function is uniquely deter- mined by the <b>autocovariance</b> <b>functions</b> of the processes. We define the quantiles of the spectral distribution function and propose two estimators for the quantiles. Asymptotic properties of both estimators are elucidated {{and the difference}} from the quantile estimators in time do- main is also indicated. We construct a testing procedure of quantile tests from the asymptotic distribution of the estimators and strong statistical power is shown in our numerical studies. Comment: 20 pages, 4 figure...|$|R
40|$|In {{this paper}} we propose a method to derive the {{spectral}} representation {{in the case of a}} particular class of nonlinear models: Markov Switching ARMA models. The procedure simply relies on the application of the Riesz-Fisher Theorem which describes the spectral density as the Fourier transform of the <b>autocovariance</b> <b>functions.</b> We explicitly show the analytical structure of the spectral density in the simple Markov Switching AR(1). Finally, a monetary policy application of a Markov Switching VAR(4) is presentedMultivariate ARMA models; Regime-switching models; Markov switching models; Frequency Domain...|$|R
50|$|Analysis of {{the loads}} on a wind turbine {{can be carried}} out through use of power spectra. A power {{spectrum}} is defined as the power spectral density function of a signal plotted against frequency. The power spectral density function of a plot is defined as the Fourier transform of the covariance function. Regarding analysis of loads, the analysis involves time series, in which case the covariance function becomes the <b>autocovariance</b> <b>function.</b> In the signal processing sense, the autocovariance can be related to the autocorrelation function.|$|E
5000|$|The {{autocorrelation}} function, {{more properly}} called the <b>autocovariance</b> <b>function</b> {{unless it is}} normalized in some appropriate fashion, measures {{the strength of the}} correlation between the values of [...] separated by a time lag. This is a way of searching for the correlation of [...] with its own past. It is useful even for other statistical tasks besides the analysis of signals. For example, if [...] represents the temperature at time , one expects a strong correlation with the temperature at a time lag of 24 hours.|$|E
5000|$|... where [...] (with k the Boltzmann constant), and V is {{the system}} volume. The {{integral}} is over the equilibrium flux <b>autocovariance</b> <b>function.</b> At zero time the autocovariance is positive {{since it is the}} mean square value of the flux at equilibrium. Note that at equilibrium the mean value of the flux is zero by definition. At long times the flux at time t, J(t), is uncorrelated with its value a long time earlier J(0) and the autocorrelation function decays to zero. This remarkable relation is frequently used in molecular dynamics computer simulation to compute linear transport coefficients; see Evans and Morriss, [...] "Statistical Mechanics of Nonequilibrium Liquids", Academic Press 1990.|$|E
40|$|This paper {{presents}} {{a model for}} the force of interest which is based on the consideration of a real force of interest deviations from its estimated value; the resulting stochastic process for financial evaluation is characterized by its expected value and <b>autocovariance</b> <b>functions.</b> Then applications of the results to actuarial contracts are proposed. In particular, the cases of temporary life annuity and n-year term life insurance are considered and their expected values and variances are illustrated. Copyright © 1999 John Wiley & Sons, Ltd...|$|R
40|$|This paper {{describes}} a new method for generating stationary integer-valued time series from renewal processes. We prove {{that if the}} lifetime distribution of renewal processes is nonlattice and the probability generat-ing function is rational, then the generated time series satisfy causal and invertible ARMA type stochastic difference equations. The result provides an easy method for generating integer-valued time series with ARMA type <b>autocovariance</b> <b>functions.</b> Examples of generating binomial ARMA(p, p− 1) series from lifetime distributions with constant hazard rates after lag p are given as an illustration...|$|R
40|$|We {{develop a}} method to {{validate}} the use of Markov Switching models in modelling time series subject to structural changes. Particularly, we consider multivariate autoregressive models subject to Markov Switching and derive close-form formulae for the spectral density of such models, based on their <b>autocovariance</b> <b>functions</b> and stable representations. Within this framework, we check the capability of the model to capture {{the relative importance of}} high- and low-frequency variability of the series. Applications to U. S. macroeconomic and financial data illustrate the behaviour at different frequencies...|$|R
5000|$|The main {{advantage}} of wide-sense stationarity {{is that it}} places the time-series {{in the context of}} Hilbert spaces. Let H be the Hilbert space generated by {x(t)} (that is, the closure of the set of all linear combinations of these random variables in the Hilbert space of all square-integrable random variables on the given probability space). By the positive definiteness of the <b>autocovariance</b> <b>function,</b> it follows from Bochner's theorem that there exists a positive measure μ on the real line such that H is isomorphic to the Hilbert subspace of L2(μ) generated by {e−2πiξ⋅t}. This then gives the following Fourier-type decomposition for continuous time stationary stochastic process: there exists a stochastic process ωξ with orthogonal increments such that, for all t ...|$|E
40|$|A key {{conceptual}} and methodological tool in time series modeling is the <b>autocovariance</b> <b>function,</b> which, however, presupposes finite variances and excludes heavy tailed distributions and data. To {{allow for the}} latter, which are increasingly of interest in modern statistics, this paper introduces a “Gini <b>autocovariance</b> <b>function</b> ” defined under merely first order moment assumptions. Playing roles {{similar to those of}} the usual <b>autocovariance</b> <b>function,</b> it provides a new fundamental tool for nonparametric description and modeling of time series. It is seen how to fit autoregressive, moving average, and ARMA time series models using Gini autocovariances. Also, the Gini <b>autocovariance</b> <b>function</b> for a nonlinear heavy-tailed (Pareto) autoregressive model is developed and sheds new light on this model. A straightforward sample Gini <b>autocovariance</b> <b>function</b> is formulated and its properties are discussed...|$|E
40|$|AbstractThe {{asymptotic}} {{properties of}} the memory structure of ARCH(∞) equations are investigated. This asymptotic analysis is achieved by expressing the <b>autocovariance</b> <b>function</b> of ARCH(∞) equations as the solution of a linear Volterra summation equation and analysing the properties of an associated resolvent equation via the admissibility theory of linear Volterra operators. It is shown that the <b>autocovariance</b> <b>function</b> decays subexponentially (or geometrically) {{if and only if}} the kernel of the resolvent equation has the same decay property. It is also shown that upper subexponential bounds on the <b>autocovariance</b> <b>function</b> result if and only if similar bounds apply to the kernel...|$|E
40|$|International audienceIn {{the paper}} row-wise {{periodically}} correlated triangular arrays are considered. The period length {{is assumed to}} grow in time. The Fourier decomposition of the mean and <b>autocovariance</b> <b>functions</b> for each row of the matrix is presented. To construct bootstrap estimators of the Fourier coefficients two block bootstrap techniques are used. These are the circular version of the Generalized Seasonal Block Bootstrap and the Circular Block Bootstrap. Consistency results for both methods are presented. Bootstrap-t equal-tailed confidence intervals for parameters of interest are constructed. Results are illustrated by an example based on simulated data...|$|R
40|$|Profiles for the exoplasmic face (EF) of the freeze-fractured plasma {{membrane}} from the root storage tissue of red beets are reconstructed by microdensitometry of micrographs of surface-shadowed-platinum carbon replicas. <b>Autocovariance</b> <b>functions</b> (ACFs) are computed from those profiles. The initial {{portions of the}} ACFs have a Gaussian form whose parameters (root mean square surface roughness and autocovariance length) are estimated. The parameter estimates are used {{to show that the}} pits on the EF faces are in good complementarity with the intramembrane particles seen on the complementary protoplasmic fracture faces...|$|R
40|$|Fixed-point {{smoothing}} with non-independent uncertainty using covariance information S. NAKAMORIy*, R. CABALLERO-A ´ GUILAz, A. HERMOSO-CARAZO and J. LINARES-PE´REZx Recursive filtering and fixed-point smoothing algorithms, using covariance information, {{are designed}} in systems with uncertain observations, when the variables describing the uncertainty {{are not necessarily}} independent. It is assumed that the observations are perturbed by white plus coloured noises, and the <b>autocovariance</b> <b>functions</b> of the signal and coloured noise are given in a semidegenerate kernel form. The estimators are obtained by the orthogonal projection technique and using an invariant imbedding method. The algorithms can be applied for estimating stationary and non-stationary signals. 1...|$|R
