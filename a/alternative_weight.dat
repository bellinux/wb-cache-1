31|392|Public
500|$|In 2001, the physician-scientist Frank Seebacher {{proposed}} a new polynomial method of calculating body-mass estimates for dinosaurs (using body-length, depth, and width), and found Giganotosaurus to have weighed [...] (based {{on the original}} [...] length estimate). In their 2002 description of the braincase of Giganotosaurus, Coria and Currie gave a length estimate of [...] for the holotype skull, and calculated a weight of [...] by extrapolating from the [...] circumference of the femur-shaft. This resulted in an encephalization quotient (a measure of relative brain size) of 1.9. In 2004, Gerardo V. Mazzetta and colleagues pointed out that though the femur of the Giganotosaurus holotype was larger than that of [...] "Sue", the tibia was [...] shorter at [...] They found the holotype specimen to have been equal to Tyrannosaurus in size at [...] (marginally smaller than [...] "Sue"), but that the larger dentary might have represented an animal of , if geometrically similar to the holotype specimen. By using multivariate regression equations, these authors also suggested an <b>alternative</b> <b>weight</b> of [...] for the holotype and [...] for the larger specimen, and that the latter was therefore the largest known terrestrial carnivore.|$|E
5000|$|A paper {{published}} in the World Journal of Gastroenterology in October 2010, on conflicts of interest in <b>alternative</b> <b>weight</b> loss product research, noted that at least three studies supported the safety and effectiveness of CQ for weight loss, but [...] "lack financial disclosures or funding sources, beyond mentioning that the CQ being tested was provided by" [...] General Health Alliances, an herbal products manufacturer. The studies did not disclose {{that one of its}} authors was a chief scientific officer for GHA that holds a patent on a CQ product.|$|E
5000|$|In 1999, Calvo {{referred}} {{an incomplete}} tooth, (MUCPv-52), to Giganotosaurus; this specimen was discovered near Lake Ezequiel Ramos Mexia in 1987 by A. Delgado, {{and is therefore}} the first known fossil of the genus. Calvo further suggested that some theropod trackways and isolated tracks (which he made {{the basis of the}} ichnotaxon Abelichnus astigarrae in 1991) belonged to Giganotosaurus, based on their large size. The largest tracks are 50 cm long with a pace of 130 cm, and the smallest is 36 cm long with a pace of 100 cm. The tracks are tridactyl (three-toed) and have large and coarse digits, with prominent claw impressions. Impressions of the digits occupy most of the track-length, and one track has a thin heel. Though the tracks were found in a higher stratigraphic level than the main fossils of Giganotosaurus, they were from the same strata as the single tooth and some sauropod dinosaurs that are also known from the same strata as Giganotosaurus. In 2001, the physician-scientist Frank Seebacher proposed a new polynomial method of calculating body-mass estimates for dinosaurs (using body-length, depth, and width), and found Giganotosaurus to have weighed 6.6 MT (based on the original 12.5 m length estimate). In their 2002 description of the braincase of Giganotosaurus, Coria and Currie gave a length estimate of 1.60 m for the holotype skull, and calculated a weight of 4.2 MT by extrapolating from the 520 mm circumference of the femur-shaft. This resulted in an encephalization quotient (a measure of relative brain size) of 1.9. In 2004, Gerardo V. Mazzetta and colleagues pointed out that though the femur of the Giganotosaurus holotype was larger than that of [...] "Sue", the tibia was 8 cm shorter at 1.12 m. They found the holotype specimen to have been equal to Tyrannosaurus in size at 8 MT (marginally smaller than [...] "Sue"), but that the larger dentary might have represented an animal of 10 MT, if geometrically similar to the holotype specimen. By using multivariate regression equations, these authors also suggested an <b>alternative</b> <b>weight</b> of 6.5 MT for the holotype and 8.2 MT for the larger specimen, and that the latter was therefore the largest known terrestrial carnivore. In 2005, Christiano Dal Sasso and colleagues described new skull material (a snout) of Spinosaurus (the original fossils of which were also destroyed during World War II), and concluded this dinosaur would have been 16 to 18 m long with a weight 7 to 9 MT, exceeding the maximum size of all other theropods. In 2006, Coria and Currie described the large theropod Mapusaurus from Patagonia; it was closely related to Giganotosaurus and of approximately the same size. In a 2007, François Therrien and Donald M. Henderson found that Giganotosaurus and Carcharodontosaurus would both have approached 13.5 m in length and 13.8 MT in weight (surpassing Tyrannosaurus), and estimated the Giganotosaurus holotype skull to have been 1.56 m long. They cautioned that these measurements depended on whether the incomplete skulls of these animals had been reconstructed correctly, and that more complete specimens were needed for more accurate estimates. They also found that Dal Sasso and colleagues' reconstruction of Spinosaurus was too large, and instead estimated it to have been 14.3 m long, weighing 20.9 MT, and possibly as low as 12.6 m in length and 12 MT in weight. They concluded that these dinosaurs had reached the upper biomechanical size limit attainable by a strictly bipedal animal.|$|E
2500|$|... {{eliminating}} {{the option of}} <b>alternative</b> <b>weights</b> for lending, investment, and service under the large, retail savings association test; ...|$|R
40|$|Composite {{indices are}} widely used and can be highly influential. Yet most remain {{controversial}} owing to inter alia the arbitrary selection of component weights. Several studies have proposed testing the robustness of rankings generated by composite indices with respect to <b>alternative</b> <b>weights</b> but have not provided sufficient guidance on the choice of these alternatives. This paper proposes a holistic yet theoretically novel approach for selecting sets of <b>alternatives</b> <b>weights</b> and assessing comparison robustness that is applicable to linear composite indices with any finite number of dimensions. This approach is applied to robustness testing of inter-temporal country improvements generated by arguably the world’s most influential composite development index, the UNDP Human Development Index (HDI). More than two-thirds of HDI country improvements between 1980 and 2013 {{were found to be}} not robust to the selected set of <b>alternative</b> <b>weights...</b>|$|R
40|$|The aim of {{this thesis}} is to {{evaluate}} how the stock index SIX 30 RX compares against portfolios {{based on the same}} stock selection but with <b>alternative</b> <b>weighting</b> techniques. Eleven <b>alternative</b> <b>weighting</b> techniques are used and divided into three categories; heuristic, optimisation and momentum based ones. These are evaluated from 1990 - 01 - 01 until 2014 - 12 - 31. The results show that heuristic based weighting techniques overperform and show similar risk characteristics as the SIX 30 RX index. Optimisation based weighting techniques show strong overperformance but have different risk characteristics manifested in higher portfolio concentration and tracking error. Momentum based weighting techniques have slightly better performance and risk-adjusted performance while their risk concentration and average annual turnover is higher than all other techniques used. Minimum variance is the overall best performing weighting technique in terms of return and risk-adjusted return. Additionally, the equal weighted portfolio overperforms and has similar characteristics as the SIX 30 RX index despite its simple heuristic approach. In conclusion, all studied <b>alternative</b> <b>weighting</b> techniques except the momentum based ones clearly overperform the SIX 30 RX index...|$|R
3000|$|The rest of {{the paper}} has the {{following}} order. The next section will present the traditional DEA method. “Weight restriction approach” section represents <b>alternative</b> <b>weight</b> restriction approach in the DEA literature, measures units’ efficiency, and then ranks DMUs, including two different weight restriction schemes. “Numerical examples” section gives two numerical examples, and conclusions are offered in section “Conclusions.” [...]...|$|E
40|$|The type {{of growth}} chart now bcoming {{widely used in}} {{developing}} countries is described; its objectives {{differ from those of}} the charts used by paediatricians in industrialized countries. <b>Alternative</b> <b>weight</b> standards to the Harvard have been suggested by a WHO Working Party and adopted for the chart described here. The [...] 3 and [...] 4 standard deviations from these standards are also available printed on plastic overlays...|$|E
30|$|In this paper, we use {{the bases}} from Shen (1997) to develop {{families}} of polynomials that are orthogonal with respect to ω (x)≡ 1 and satisfy the requisite boundary conditions, to facilitate transformation between physical and frequency space without using functions such as the Legendre polynomials that lie outside of the solution space. These families can also be efficiently modified to work with <b>alternative</b> <b>weight</b> functions, thus leading {{to the development of}} new numerical methods. In particular, it is demonstrated that these new families can be used to obtain three-term recurrence relations for the GJPs that satisfy the same boundary conditions.|$|E
40|$|This paper {{presents}} {{the findings of}} an investigation of <b>alternative</b> forms of <b>weighting</b> adjustments to compensate for nonresponse in the Health Care Survey of DoD Beneficiaries. Currently, we compensate for potential nonresponse bias by adjusting for nonresponse within weighting cells defined by the stratification variables: enrollment status, beneficiary group, and geographic area. However, {{a great deal more}} is known about both respondents and nonrespondents from the sample frame. The first stage of the research identified variables from the frame that were most related to response to the survey. Second, we incorporated the chosen auxiliary variables into a weighting adjustment procedure. Three <b>alternative</b> <b>weighting</b> adjustment procedures were used: (1) a response propensity model, (2) forming weighting cells by dividing the predicted response propensity scores distribution into equal, ordered subgroups, and (3) forming weighting cells according to predicted response propensity scores and multiple outcome variables. Lastly, we compared and evaluated the results of using the <b>alternative</b> <b>weighting</b> procedures and the current procedure...|$|R
40|$|Deaton and Lubotsky (2003) {{found that}} the robust {{positive}} relationship across American cities between mortality and income inequality became small, insignificant, and/or non-robust once they controlled for the fraction of each city’s population that is black. Ash and Robinson (Ash, M., & Robinson D. Inequality, race, and mortality in US cities: a political and econometric review. Social Science and Medicine, 2009) consider <b>alternative</b> <b>weighting</b> schemes and show that {{in one of our}} specifications, in one data period, and with one of their <b>alternative</b> <b>weighting</b> schemes, income inequality is estimated to be a risk factor. All of our other specifications, as well as their own preferred specification, replicate our original result, which is supported by the weight of the evidence. Conditional on fraction black, there is no evidence for an effect of income inequality on mortality. Inequality, Race, Health inequality, Mortality, Geography, USA...|$|R
40|$|The {{diagonal}} GARCH(1, 1) {{model is}} shown to support identification of the triangular system and is argued as a higher moment analog to traditional exclusion restrictions. Estimators for this result include QML and GMM. For the GMM estimator, only partial parameterization of the conditional covariance matrix is required. An <b>alternative</b> <b>weighting</b> matrix for the GMM estimator is also proposed. ...|$|R
40|$|The {{problem of}} stratum jumpers in {{stratified}} business surveys is mainly due to inaccurate size {{information at the}} time of sampling, which may result in assigning a large design weight to a large size unit. Such units may be quite influential and substantially inflate the variance of the estimates. They can be viewed as being outliers with respect to the implicit model used at the sampling stage. Therefore, standard robust estimation techniques can be used to handle this problem. Instead, we will consider an <b>alternative</b> <b>weight</b> smoothing method and illustrate its application using data from the Canadian Workplace an...|$|E
30|$|The order {{contiguity}} matrix is {{a binary}} weight matrix with zeroes {{on the main}} diagonal and rows that contain zeroes when spatial units are non-contiguous, while they contain values of one between neighbouring units (Mur [2013]). The inverse distance matrix {{is based on the}} Euclidean distance between centroids of each municipality. Consequently the elements of the matrix measure distances between centroids of the polygons containing municipalities (Mur [2013]). Both weight matrices are row normalised, {{in order to get a}} value of one as a row sum. Consequently, the two <b>alternative</b> <b>weight</b> assumptions treat spatial association by taking into account neighbourhood or closeness among spatial units, respectively.|$|E
30|$|The {{families}} of orthogonal polynomials {{developed in the}} preceding two sections are orthogonal {{with respect to the}} weight function ω (x)≡ 1. In Guo et al. (2009), Shen (2003), {{families of}} generalized Jacobi polynomials/functions (GJP/Fs) are defined {{in such a way as}} to satisfy specified boundary conditions, such as the ones employed in this paper. These functions are orthogonal with respect to a weight function that is determined by the boundary conditions. However, it can be seen from (10) that an <b>alternative</b> <b>weight</b> function may be preferable when solving certain PDEs. In this section, we discuss the modification of sequences of orthonormal polynomials and their three-term recurrence relations as a consequence of changes in the underlying weight function.|$|E
5000|$|NOTE: For some [...] "unrated" [...] risk weights, {{banks are}} {{encouraged}} {{to use their own}} internal-ratings system based on Foundation IRB and Advanced IRB in Internal-Ratings Based approach with a set of formulae provided by the Basel-II accord. There exist several <b>alternative</b> <b>weights</b> for some of the following claim categories published in the original Framework text.|$|R
40|$|Abstract: This {{research}} article presents {{the development process}} of a cost-estimating model for railway renewal projects at the early stage of a project life cycle. The model comprises four main stages: creating a project structure that composes the goal, project criteria, and alternatives; col-lecting the necessary data {{in the form of}} pairwise comparisons made by a domain expert; pro-ducing <b>alternative</b> <b>weights</b> using a geometric mean; finally, employing an algorithmic method using the produced <b>alternative</b> <b>weights</b> and the known cost of one alternative per criteria. The practical implications of the developed model are its ability to estimate renewal project costs of railway assets when {{there is a lack of}} quantitative data and detailed project definition. The process provides a transparent and a structured way of formalizing subjective judgments, which produces a three-point estimate as the resulting output. The model is described and vali-dated using a switch and crossing case study...|$|R
40|$|Diagonal GARCH {{is shown}} to support {{identification}} of the triangular system and is argued as a higher moment analog to traditional exclusion restrictions used for determining suitable instruments. The estimator for this result is ML in the case where a distribution for the GARCH process is known and GMM otherwise. For the GMM estimator, an <b>alternative</b> <b>weighting</b> matrix is proposed. Time-series analysis...|$|R
30|$|This paper {{proposed}} <b>alternative</b> <b>weight</b> restriction {{approach to}} generate {{a common set of}} weights for all DMUs. Besides, this common set of weights looks suitable for a fair ranking of DMUs. Our proposed model not only generates strictly positive weights but also prevents weights dissimilarity. The production of strictly positive weights by the proposed model makes it possible that no input and output variables are ignored. Furthermore, it reduces the number of efficient units and gives less efficiency score. However, {{there is no need for}} initial information on the input and outputs weights in this approach. Admittedly, the proposed method can be extended to other standard DEA models with appropriate modifications. Equally, the proposed weight restriction approach may suffer from existence of alternative optimal solution for common set of weights, which can be investigated in future works.|$|E
40|$|We {{present a}} simple {{formalism}} for {{the evolution of}} timelike jets in which tree-level matrix element corrections can be systematically incorporated, up to arbitrary parton multiplicities and over all of phase space, in a way that exponentiates the matching corrections. The scheme is cast as a shower Markov chain which generates one single unweighted event sample, that can be passed to standard hadronization models. Remaining perturbative uncertainties are estimated by providing several <b>alternative</b> <b>weight</b> sets for the same events, at a relatively modest additional overhead. As an explicit example, we consider Z {yields} q{bar q} evolution with unpolarized, massless quarks and include several formally subleading improvements as well as matching to tree-level matrix elements through {alpha}{sub s}{sup 4 }. The resulting algorithm is implemented in the publicly available VINCIA plugin to the PYTHIA 8 event generator...|$|E
40|$|MULINO, {{an ongoing}} project {{financed by the}} European Commission, has {{released}} the prototype of a Decision Support System software (mDSS) for the sustainable {{management of water resources}} at the catchment scale. The software integrates socio-economic and environmental modelling, with geo-spatial information and multi-criteria analysis. The policy background refers to the EU Water Framework Directive. The challenging multi-disciplinary context was approached by developing an innovative and dynamic implementation of the DPSIR framework, originally proposed by the European Environmental Agency. In mDSS integrated assessment modelling provides the values of quantitative indicators to be used for transparent and participated decisions, through the application of value functions, weights and decision rules chosen by the end user. Simple routines for the sensitivity analysis and comparison of <b>alternative</b> <b>weight</b> vectors also provides effective decision support by exploring and finding compromises between conflicting interests/perspectives in a multi-stakeholder context...|$|E
40|$|Social {{marginal}} welfare weights play {{an important}} role in areas of applied public policy analysis such as tax reform. These weights reflect the values of the social planner, or equivalently the underlying social welfare function. A number of recent papers have questioned the "default" Utilitarian-based approach used to derive these weights, and have suggested potential alternatives. However, there are few examples applying these <b>alternative</b> <b>weighting</b> schemes to traditional, nationally representative, datasets, and in particular, few comparisons of how these <b>alternative</b> <b>weighting</b> schemes would affect the distribution of the welfare effects of a specific tax reform in comparison to the Utilitarian-based approach. This paper aims to fill that gap. Using the nationally representative 2009 / 10 Irish Household Budget Survey, we apply a range of alternatives to Utilitarianism in determining the distribution of social marginal welfare weights, and compare these distributions to that arising from the traditional Utilitarian approach. The <b>alternative</b> <b>weighting</b> schemes we analyse are based upon: the principles of Equal Sacrifice, poverty alleviation, government self-interest and the redistribution of "luck" income. The distribution of welfare weights arising from these approaches are found to differ appreciably from the distribution based upon Utilitarian weights. A simple indirect tax reform model is estimated and applied to the different distributions of welfare weights to investigate the sensitivity of tax reform recommendations to these distributions. Given the importance of social marginal welfare weights in areas of public policy analysis such as optimal labour and commodity tax design, and tax reform evaluation, we believe this detailed examination of the alternatives to Utilitarianism, and their application to a household budget survey dataset, is an important addition to the literature...|$|R
40|$|Preliminary Version This paper {{utilizes}} short–term {{forecasts of}} Euro area real GDP {{to propose a}} unique method for construction of area–wide indicators by pooling {{the information contained in}} country–specific indicators. We generate new area–wide indicators by adopting optimized weights in terms of forecasting instead of economic weights. In an out–of–sample forecast experiment we find that our method improves upon the several <b>alternative</b> <b>weighting</b> and forecasting methods in terms of forecast accuracy...|$|R
40|$|We {{summarise}} {{the main}} features of VINCIA's antenna-based treatment of QCD initial- and final-state showers, which includes iterated tree-level matrix-element corrections and automated evaluations of perturbative shower uncertainties. The latter are computed on the fly and are cast {{as a set of}} <b>alternative</b> <b>weights</b> for each generated event. The resulting algorithm has been made publicly available as a plug-in to the PYTHIA 8 event generator. Comment: 6 pages. Presented at ICHEP 2016, Chicag...|$|R
40|$|Efficiency, {{the basic}} concept of multi-objective {{optimization}} is investigated {{for the class}} of pairwise comparison matrices. A weight vector is called efficient if no <b>alternative</b> <b>weight</b> vector exists such that every pairwise ratio of the latter’s components {{is at least as}} close to the corresponding element of the pairwise comparison matrix as the one of the former’s components is, and the latter’s approximation is strictly better in at least one position. A pairwise comparison matrix is called simple perturbed if it differs from a consistent pairwise comparison matrix in one element and its reciprocal. One of the classical weighting methods, the eigenvector method is analyzed. It is shown in the paper that the principal right eigenvector of a simple perturbed pairwise comparison matrix is efficient. An open problem is exposed: the search for a necessary and sufficient condition of that the principal right eigenvector is efficient...|$|E
40|$|Minimum Classification Error (MCE) {{training}} {{is difficult to}} apply to language modeling due to inherent scarcity of training data (N-best lists). However, a whole-sentence exponential language model is particularly suitable for MCE training, because it can use {{a relatively small number}} of powerful features to capture global sentential phenomena. We review the model, discuss feature induction, find features in both the Broadcast News and Switchboard domains, and build an MCE-trained model for the latter. Our experiments show that even models with relatively few features are prone to overfitting and are sensitive to initial parameter setting, leading us to examine <b>alternative</b> <b>weight</b> optimization criteria and search algorithms. 1. MCE FOR LANGUAGE MODELING Language models are typically used in the context of a Bayesian classifier, usually filling the role of the prior, as in speech recognition: s = arg max s P (sja) = arg max s P (ajs) Δ P (s) where a is the incoming acousti [...] ...|$|E
40|$|An {{attempt is}} made to set rules for a fair and {{fruitful}} competition between alternative inference methods based on their performance in simulation experiments. This leads {{to a list of}} eight methodologic aspirations. Against their background we criticize aspects of many simulation studies that have been used in the past to compare competing estimators for dynamic panel data models. To illustrate particular pitfalls some further Monte Carlo results are produced, obtained from a simulation design inspired by an analysis of the (non-) invariance properties of estimators and occasionally by available higher-order asymptotic results. We focus on the very specific case of alternative implementations of one and two step generalized method of moments (GMM) estimators in homoskedastic stable zero-mean panel AR(1) models with random individual specific effects. We compare a few implementations, including GMM sytem estimators with <b>alternative</b> <b>weight</b> matrices, and illu! strate that an impartial evaluation of the outcome of a Monte Carlo based contest requires evidence - both analytical and empirical - on the completeness, orthogonality and relevance of the simulation design...|$|E
5000|$|Gaussian {{quadrature}} {{as above}} will only produce good results if the function f(x) is well approximated by a polynomial function {{within the range}} [...] The method is not, for example, suitable for functions with singularities. However, if the integrated function can be written as , where [...] is approximately polynomial and [...] is known, then <b>alternative</b> <b>weights</b> [...] and points [...] that depend on the weighting function [...] may give better results, where ...|$|R
40|$|Recent {{research}} has indicated that, while the reliability of ACS estimates of county level characteristics generally agree with theoretical predictions, census tract level reliability is lower than desirable. This paper discusses the results of methodological research to improve the reliability of small-area ACS estimates by reducing tract-level ACS standard errors using <b>alternative</b> <b>weighting</b> methods. These include applying an intermediate set of weighting adjustments at the tract level before the final county level housing unit and population controls are applied...|$|R
40|$|This note {{examines}} {{the accuracy of}} methods that approximate AR(1) processes with discrete Markov chains. Tauchen and Hussey's [Tauchen, G., Hussey, R., 1991. Quadrature-based methods for obtaining approximate solutions to nonlinear asset pricing models. Econometrica 59, 371 - 396] method has problems under high autocorrelation. I suggest an <b>alternative</b> <b>weighting</b> function, and note that Tauchen's [Tauchen, G., 1986. Finite state Markov-chain approximations to univariate and vector autoregressions. Economics Letters 20, 177 - 181] method is relatively robust. ...|$|R
40|$|The Weakness at {{information}} stream and coordination often makes information distortion {{which one of}} them such as happening of ever greater demand amplification in upstream channel compared with the channel downstream and it called phenomenon of bullwhip effect. This was happened in CV. Welirang Tirta Mandiri, Pandaan. In this research it has been done the mathematical analysis to know the level of bullwhip effect and evaluate the cause of happening in bullwhip effect at the company. And {{then we have to}} improve the solution of the alternative from the subyektif opinion to decrease bullwhip effect by applying method of Fuzzy Analytical Hierarchy Process, where later will be obtained weight and ranking priority and also get chosen solution alternative to decrease the happening of bullwhip effect. The Result of the level bullwhip effect that happened at Manufacture Echelon is (1. 56) while at Retailer Echelon is (1. 25). And the chosen alternative is Monitoring to Level Inventory at downstream channel get biggest <b>alternative</b> <b>weight</b> (185. 93) which expected in order to minimize the happening of bullwhip effect...|$|E
40|$|PURPOSE: The {{purpose of}} this study was to {{determine}} the potential utility of <b>alternative</b> <b>weight</b> descriptors in the Cockcroft-Gault equation to more accurately predict carboplatin clearance in underweight, normal weight, overweight and obese patients. METHODS: Clearance values obtained from individual fits using NONMEM were compared to predicted carboplatin clearances calculated using the modified Calvert formula in which creatinine clearance was calculated with the Cockcroft-Gault equation using diverse weight descriptors. RESULTS: This study indicated that lean body mass was the best weight descriptor in underweight and normal weight patients, while adjusted ideal body weight was the best weight descriptor in overweight and obese patients. However, a flat dose based on the population carboplatin clearance performed better in all weight categories than the use of the Cockcroft-Gault equation with diverse weight descriptors. CONCLUSION: These results suggest that in overweight and obese patients, with a normal renal function, a flat carboplatin dose should be administered, based on the population carboplatin clearance (8. 38 l/h = 140 mL/min). Thus, in case an AUC of 5 mg min/mL is desired, the appropriate dose for carboplatin would be 5 x 140 = 700 m...|$|E
40|$|The {{diffusion}} of knowledge generates positive externalities if knowledge flows increase {{the productivity of}} Research and Development (R&D) by the recipients of these flows. This paper investigates {{the extent to which}} this positive spillover effect of knowledge diffusion depends on the similarity of research activities by the originator and recipient of the knowledge. The paper also investigates at what rate these spillover effects diminish as the distance between the originator and recipient increases. We find, using regional patent and R&D expenditure data from the European Union, that similarity between R&D activities is not only statistically significant, but salient: regions with completely dissimilar R&D activities exhibit essentially no spillovers at all. We also find an increase in the distance between the originating and recipient region by 550 kilometres reduces spillovers by 75 % (as low as 55 % in some specifications). Unlike much of the extant literature, the rate of spatial decay of spillovers is estimated jointly with the remaining parameters of the model rather than through specification searches over a set of <b>alternative</b> <b>weight</b> matrices. Our results are robust to the inclusion of unobserved country effects and border barriers. ...|$|E
40|$|The {{extent of}} the dollar's decline {{relative}} to its peak in 2002 depends upon {{the composition of the}} basket of currencies used in calculating the dollars value. Further, the appropriate index depends upon the question being asked, with the type of price deflators used if any dependent upon the question at hand. Various measures of the dollars value are discussed, and compared, including <b>alternative</b> <b>weights</b> based on liabilities and assets, instead of the standard trade flow weights...|$|R
40|$|AHP/ANP {{stability}} {{measurement methods}} are described. In this paper we define the method’s stability as {{the measure of}} its results dependence on the expert’s errors, made during pair comparisons. Ranking Stability (order preservation in alternative ranking under natural expert’s errors, made during expert estimation) and Estimating Stability (maintaining <b>alternative</b> <b>weights</b> within the specified maximal relative inaccuracy range) are considered. Targeted Genetic Algorithm search procedure is used for possible stability violation detection. Then division-in-half (dichotomy) method is applied to calculate stability metric of a given criteria hierarchy...|$|R
40|$|This {{document}} contains supplemental {{results and}} derivations {{for the paper}} “Asset Pricing with Garbage”. These include cross-sectional results for 25 portfolios formed using a double sort of garbage and expenditure betas, alternative test assets for the Fama MacBeth regressions in the paper, <b>alternative</b> <b>weighting</b> schemes for the crosssectional GMM test in the paper, a test using a linear stochastic discount factor, a robustness check using a value-weighted garbage index, and {{a discussion of the}} differences {{in the construction of the}} garbage and expenditure series as they may relate to the results in the paper...|$|R
