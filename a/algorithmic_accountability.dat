13|3|Public
2500|$|Researcher, Andrew Tutt, {{argues that}} {{algorithms}} should be overseen by a specialist regulatory agency, similar to FDA. His academic work {{emphasizes that the}} rise of increasingly complex algorithms calls for the {{need to think about}} the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require [...] "closer forms of federal uniformity, expert judgment, political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market". The issue of <b>algorithmic</b> <b>accountability</b> (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).|$|E
5000|$|... "A.I. {{must have}} <b>algorithmic</b> <b>accountability</b> so that humans can undo {{unintended}} harm".|$|E
50|$|Since DARPA's {{introduction}} of it's program in 2016, {{a number of}} initiatives have started {{to address the issue}} of <b>algorithmic</b> <b>accountability</b> and provide transparency concerning how technologies within this domain function.|$|E
40|$|This {{research}} received {{funding from}} the European Union Horizon 2020 Framework Programme and Seventh Framework Programme through the respective projects DANTE (Detecting and analysing terrorist-related online contents and financing activities), H 2020 - FCT- 2015 - 700367 and VALCRI (Visual Analytics for Sense-making in Criminal Intelligence Analysis), FP 7 -IP- 608142. n the hopes of making law enforcement more effective and efficient, police and intelligence analysts are increasingly relying on algorithms underpinning technology-based and data-driven policing. To achieve these objectives, algorithms must also be accurate, unbiased and just. In this paper, we examine how European data protection law regulates automated profiling and how this regulation impacts police and intelligence algorithms and algorithmic discrimination. In particular, we assess {{to what extent the}} regulatory frameworks address the challenges of <b>algorithmic</b> transparency and <b>accountability.</b> We argue that while the law regulates both algorithms and their discriminatory effects, the framework is insufficient in addressing the complex interactions that must take place between system developers, users, oversight and profiled individuals to fully guarantee <b>algorithmic</b> transparency and <b>accountability.</b> status: publishe...|$|R
40|$|CiTiP Blog Post - Available at: [URL] {{policing}} {{and intelligence}} {{have become increasingly}} data-driven and technology-based, algorithms {{play a crucial role}} in making criminal profiling more effective and efficient through automation. Moreover, criminal profiling must also be accurate, unbiased, and just. This is not an easy task as there is an overarching risk of giving too much deference to machine reasoning that is flexible and evolving, has minimal human supervision, and is poorly understood by human operators. Policymakers, such as within the CoE, the EU, and the United Kingdom, are positing that <b>algorithmic</b> transparency and <b>accountability</b> are viable solutions to this problem. But what does the law say? And even so, is legal compliance enough to solve this problem?status: publishe...|$|R
40|$|So-called “robot ” journalism {{represents}} a shift towards the automation of journalistic tasks related to news reporting, writing, curation, and even data analysis. In this paper, {{we consider the}} extension of robot journalism to the domain of social platforms and study the use of “news bots”—automated accounts that participate in news and information dissemination on social networks. Such bots present an intriguing development opportunity for news organiza-tions and journalists. In particular, we analyze a sample of existing news bot accounts on Twitter to understand how news bots are currently being used and to examine how using automation and algorithms may change the modern media environment. Based on our analy-sis, we propose a typology of news bots {{in the form of}} a design and editorial decision space that can guide designers in defining the intent, utility, and functionality of future bots. The proposed design space highlights the limits of news bots (e. g., automated commentary and opinion, <b>algorithmic</b> transparency and <b>accountability)</b> and areas where news bots may enable innovation, such as niche and local news...|$|R
50|$|Several {{papers have}} been {{published}} on these topics in 2016, the first of which, by Goodman / Flaxman, outlines {{the development of the}} right to explanation. Pasquale does not think the approach goes far enough, as he has stated in a blog entry at the London School of Economics (LSE). In fact at LSE there is a whole series on <b>Algorithmic</b> <b>Accountability</b> of which that was one entry in Feb. of 2016, and other notable ones were by Joshua Kroll and Mireille Hildebrandt.|$|E
40|$|Farida Vis, Research Fellow in the Information School at the University of Sheffield, {{investigates the}} issue of trust in the debate about <b>algorithmic</b> <b>accountability,</b> arguing that we should instead focus on ‘trustworthiness’ and that {{now is the time}} for a {{considered}} debate about algorithmic governance and accountability frameworks...|$|E
40|$|Alison Powell, Assistant Professor at LSE, investigates how {{data and}} {{algorithms}} effect our daily lives, from negotiating public transport and booking restaurants, {{to the more}} serious issues of surveillance and privacy. She argues {{that there is a}} greater need for <b>algorithmic</b> <b>accountability</b> {{in order for us to}} understand its impact, both positive and negative, on not only our day-to-day lives, but also on citizenship and inequality in our societies...|$|E
40|$|Journalists rarely use the web as {{a source}} of data about the state of issues, debates and {{information}} flows in different societies. Liliana Bounegru looks at how media scholars have leveraged digital data and <b>algorithmic</b> <b>accountability.</b> In times of shrinking news budgets and staff cuts journalists can turn to such readily available sources of data as a way to understand public engagement with major issues. Scholars can support this process by making the datasets, tools and protocols developed during their work available to others...|$|E
40|$|In {{the debate}} on <b>algorithmic</b> <b>accountability,</b> and {{platform}} responsibility more specifically, {{the contribution of the}} social researcher is immense. In this set of posts, researchers reflect upon broad themes of control and agency — not only that which is faced by the data subject, but also by the researcher who relies on proprietary platforms to understand how these systems operate and interact with users. This research bears relevance to policy debates, because it provides evidence of ways in which automated systems shape consumer and citizens and look beyond conventional recommendations of transparency or openness...|$|E
40|$|This paper explores how {{accountability}} {{might make}} otherwise obscure and inaccessible algorithms available for governance. The potential import and difficulty of accountability is made {{clear in the}} compelling narrative reproduced across recent popular and academic reports. Through this narrative {{we are told that}} algorithms trap us and control our lives, undermine our privacy, have power and an independent agential impact, {{at the same time as}} being inaccessible, reducing our opportunities for critical engagement. The paper suggests that STS sensibilities can provide a basis for scrutinizing the terms of the compelling narrative, disturbing the notion that algorithms have a single, essential characteristic and a predictable power or agency. In place of taking for granted the terms of the compelling narrative, ethnomethodological work on sense-making accounts is drawn together with more conventional approaches to accountability focused on openness and transparency. The paper uses empirical material from a study of the development of an “ethical,” “smart” algorithmic videosurveillance system. The paper introduces the “ethical” algorithmic surveillance system, the approach to accountability developed, and some of the challenges of attempting <b>algorithmic</b> <b>accountability</b> in action. The paper concludes with reflections on future questions of algorithms and accountability...|$|E
40|$|Black-box medicine—the use of {{big data}} and {{sophisticated}} machine learning techniques for health-care applications—could {{be the future}} of personalized medicine. Black-box medicine promises {{to make it easier}} to diagnose rare diseases and conditions, identify the most promising treatments, and allocate scarce resources among different patients. But to succeed, it must overcome two separate, but related, problems: patient privacy and <b>algorithmic</b> <b>accountability.</b> Privacy is a problem because researchers need access to huge amounts of patient health information to generate useful medical predictions. And accountability is a problem because black-box algorithms must be verified by outsiders to ensure they are accurate and unbiased, but this means giving outsiders access to this health information. This article examines the tension between the twin goals of privacy and accountability and develops a framework for balancing that tension. It proposes three pillars for an effective system of privacy-preserving accountability: substantive limitations on the collection, use, and disclosure of patient information; independent gatekeepers regulating information sharing between those developing and verifying black-box algorithms; and information-security requirements to prevent unintentional disclosures of patient information. The article examines and draws on a similar debate in the field of clinical trials, where disclosing information from past trials can lead to new treatments but also threatens patient privacy...|$|E
40|$|Mark Andrejevic, Professor of Media Studies at the Pomona College in Claremont, California, is a {{distinguished}} critical theorist exploring issues around surveillance from pop culture to {{the logic of}} automated, predictive surveillance practices. In an interview with WPCC issue co-editor Pinelopi Troullinou, Andrejevic responds to pressing questions emanating from the surveillant society looking to shift the conversation to concepts of data holders’ accountability. He insists {{on the need to}} retain awareness of power relations in a data driven society highlighting the emerging challenge, ‘to provide ways of understanding the long and short term consequences of data driven social sorting’. Within the context of Snowden’s revelations and policy responses worldwide he recommends a shift of focus from discourses surrounding ‘pre-emption’ to those of ‘prevention’ also questioning the notion that citizens might only need to be concerned, ‘if we are doing something “wrong”’ as this is dependent on a utopian notion of the state and commercial processes, ‘that have been purged of any forms of discrimination’. He warns of multiple concerns of misuse of data in a context where ‘a total surveillance society looks all but inevitable’. However, the academy may be {{in a unique position to}} provide ways of reframing the terms of discussions over privacy and surveillance via the analysis of ‘the long and short term consequences of data driven social sorting (and its automation) ’ and in particular of <b>algorithmic</b> <b>accountability...</b>|$|E
40|$|This {{article focuses}} upon {{defamation}} law in Australia and its struggles {{to adjust to}} the digital landscape, to illustrate the broader challenges involved in the governance and regulation of data associations. In many instances, online publication will be treated by the courts in a similar fashion to traditional forms of publication. What is more contentious is the question of who, if anyone, should bear the responsibility for digital forms of defamatory publication which result not from an individual author’s activity online but rather from algorithmic associations. This article seeks, in part, to analyse this question, by reference to the Australian case law and associated scholarship regarding search engine liability. Reflecting on the tensions involved here offers us a fresh perspective on defamation law through the conceptual lens of data associations. Here the focus of the article shifts to explore some wider questions posed for defamation law by big data. Defamation law may come to {{play a significant role in}} emerging frameworks for <b>algorithmic</b> <b>accountability,</b> but these developments also call into question many of its traditional concepts and assumptions. It may be time to think differently about defamation and to consider its interrelationship with privacy, speech and data protection more fully. As a result, I conclude that the courts and policymakers need to engage more deeply and explicitly with the rationale(s) for the protection of reputation and that more thought needs to be given to changing conceptions of reputation in the context of data associations...|$|E
40|$|This year’s key {{developments}} will centre on {{fears about}} how changing technology is affecting {{the quality of}} information {{and the state of}} our democracy. The arrival of Donald Trump in the White House and elections in France and Germany will highlight the increasing power of new communication channels as traditional media continues to lose both influence and money. More widely there’ll be heated debate about the role and size of tech platforms {{and the extent to which}} their activities should be regulated. Artificial Intelligence (AI) takes over from mobile as the hottest topic in technology, though the practical and ethical dilemmas around how it will be used become ever more apparent through the year. More specifically: 	A raft of initiatives over so called ‘fake news’ from both publishers and platforms fail to restore public trust. Fact-checking services move centre stage. 	We’ll see further job cuts and losses across the news industry. More papers in the US and Europe go out of business, slim down or become online-only. 	More focus on <b>algorithmic</b> <b>accountability,</b> the use of data for targeting, and the power of technology companies. 	We’ll see a backlash from publishers over Facebook Live as initial investments prove hard to sustain and monetise. 	Publishers force more people to sign-in/register for websites and apps as well as investing heavily in data to help deliver more personalised content and messaging. 	Expect widespread innovation with messaging apps, chat bots and the art of ‘conversational journalism’. 	More of us will be talking to computers via voice driven personal assistants, like Amazon’s Alexa, Apple’s Siri and Google’s Assistant. 	Big year for audio/podcasts as Facebook rolls out social and live audio formats. 	There’ll be an explosion of mobile alerts for news, as the battle for the lockscreen heats up. 	We’ll see more experimentation with Virtual Reality (VR) and Augmented Reality (AR), but results continue to disappoint for news. 	Cyber-wars intensify along with the battles between governments and citizens over the limits of personal surveillance. 	More politicians follow the lead of Donald Trump in using social media to define issues, break new policy and as a substitute for traditional media access. In our survey of 142 leading Editors, CEOs and Digital Leaders for this report: 	 70 % said worries over the distribution of fake/inaccurate news in social networks will strengthen their position, while… 	 46 % say they are more worried about the role of platforms than last year 	 56 % say Facebook Messenger will be important or very important part of their offsite initiatives this year. 53 % say the same for WhatsApp and 49 % for Snapchat 	 33 % of respondents from a newspaper background are more worried about their company’s financial sustainability than last year; just 8 % are less worrie...|$|E

