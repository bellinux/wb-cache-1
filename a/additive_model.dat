1963|4072|Public
25|$|There {{has been}} much {{criticism}} of Kondrashov's theory, since it relies on two key restrictive conditions. The first requires {{that the rate of}} deleterious mutation should exceed one per genome per generation in order to provide a substantial advantage for sex. While there is some empirical evidence for it (for example in Drosophila and E. coli), there is also strong evidence against it. Thus, for instance, for the sexual species Saccharomyces cerevisiae (yeast) and Neurospora crassa (fungus), the mutation rate per genome per replication are 0.0027 and 0.0030 respectively. For the nematode worm Caenorhabditis elegans, the mutation rate per effective genome per sexual generation is 0.036. Secondly, there should be strong interactions among loci (synergistic epistasis), a mutation-fitness relation for which there is only limited evidence. Conversely, there is also the same amount of evidence that mutations show no epistasis (purely <b>additive</b> <b>model)</b> or antagonistic interactions (each additional mutation has a disproportionally small effect).|$|E
2500|$|Genetic studies, {{including}} twin studies, {{have proven}} {{that the overwhelming}} majority of cases of Tourette syndrome are inherited, although the exact mode of inheritance is not yet known. [...] Tic disorders have long been thought to be inherited as an autosomal dominant gene, but recent research challenges the autosomal dominant hypothesis, and suggests an <b>additive</b> <b>model</b> involving multiple genes. [...] According to Roger Freeman, MD, [...] "Genes that cause tics have not yet been identified; it's very unlikely there's just one. Tics are very familial, but not in a Mendelian pattern (dominant, recessive, etc.). You can't 'inherit' the committee decision to define a problem in a specific way, so TS itself can't be inherited. It's a tic disorder that is inherited." ...|$|E
5000|$|Generalized <b>additive</b> <b>model</b> for location, scale, {{and shape}} (GAMLSS) ...|$|E
30|$|A {{number of}} {{quantitative}} methods {{are available for}} assessing the risks from combined exposures. In this paper we have used the Hazard Index (HI) and Hazard Quotient (HQ) approach [3, 4]. Finally, dose <b>additive</b> <b>models</b> are used to assess risks to humans and concentration <b>additive</b> <b>models</b> are used to assess risks to ecological receptors. In this paper both models {{are referred to as}} <b>additive</b> <b>models.</b>|$|R
40|$|This {{chapter is}} about nonparametric <b>additive</b> <b>modeling</b> of a {{conditional}} mean or quantile function. Nonparametric <b>additive</b> <b>modeling</b> relaxes the restrictive functional form assumptions of parametric modeling while avoiding {{many of the}} disadvantages of fully nonparametric estimation. The chapter reviews recently developed methods for estimating nonparametric <b>additive</b> <b>models</b> with and without link functions. The emphasis is on methods that avoid the curse of dimensionality and achieve a desirable property called oracle efficiency. 1...|$|R
40|$|We propose general {{procedures}} for posterior sampling from additive and generalized <b>additive</b> <b>models,</b> with applications to non-parametric, semi-parametric and mixed models. One chooses a linear operator S j for each predictor, and the algorithm requires only {{the application of}} S j and S 1 = 2 j. Both of these can be done applied efficiently (O(n) operations) for many popular operators. The procedure is a stochastic generalization of the backfitting algorithm for fitting <b>additive</b> <b>models.</b> Keywords: <b>additive</b> <b>models,</b> backfitting, Bayes, Gibbs sampling, random effects, Metropolis-Hastings procedure 1 Introduction In this paper we propose {{general procedures}} for posterior sampling from additive and generalized <b>additive</b> <b>models.</b> The main idea evolves from the close relationship between the backfitting algorithm for fitting <b>additive</b> <b>models,</b> and the Gibbs sampler for drawing realizations from a posterior distribution. In a nutshell: ffl Backfitting cycles around and replaces each current funct [...] ...|$|R
5000|$|... #Article: Generalized <b>additive</b> <b>model</b> for location, {{scale and}} shape ...|$|E
5000|$|Hence a {{time series}} using an <b>additive</b> <b>model</b> {{can be thought}} of as ...|$|E
50|$|Generalized <b>additive</b> <b>model</b> is a {{smoothing}} method for multiple predictors {{that allows for}} non-parametric predictions.|$|E
40|$|Wavelet based nonparametric <b>additive</b> <b>models</b> are {{considered}} for nonlinear system identification. Additive functional component representations {{are an important}} class of models for describing nonlinear input-output relationships and eavelets, which have excellent approximation capabilities, can be chosen as the functional components in the <b>additive</b> <b>models.</b> Wavelet based <b>additive</b> <b>models,</b> combined with model order determination and variable selection, are capable of handling problems of high dimensionality. Examples are given to demonstrate the efficiency of this new modelling approach...|$|R
40|$|Sparse <b>additive</b> <b>modeling</b> is a {{class of}} {{effective}} methods for performing high-dimensional nonparametric regression. In this work we show how shape constraints such as convexity/concavity and their extensions, can be integrated into <b>additive</b> <b>models.</b> The proposed sparse difference of convex <b>additive</b> <b>models</b> (SDCAM) can estimate most continuous functions without any a priori smoothness assumption. Motivated by a characterization of difference of convex functions, our method incorporates a natural regularization functional to avoid overfitting and to reduce model complexity. Computationally, we develop an efficient backfitting algorithm with linear per-iteration complexity. Experiments on both synthetic and real data verify that our method is competitive against state-of-the-art sparse <b>additive</b> <b>models,</b> with improved performance in most scenarios. Comment: 17 pages, 2 figure...|$|R
50|$|In statistics, {{projection}} pursuit regression (PPR) is {{a statistical}} model developed by Jerome H. Friedman and Werner Stuetzle {{which is an}} extension of <b>additive</b> <b>models.</b> This <b>model</b> adapts the <b>additive</b> <b>models</b> in that it first projects the data matrix of explanatory variables in the optimal direction before applying smoothing functions to these explanatory variables.|$|R
50|$|An <b>additive</b> <b>model</b> {{appropriate}} if {{the magnitude}} of seasonal fluctuations does not vary with level.|$|E
5000|$|Thus, for a {{response}} Y and two variables x1 and x2 an <b>additive</b> <b>model</b> would be: ...|$|E
5000|$|The {{multiplicative model}} can be {{transformed}} into an <b>additive</b> <b>model</b> by taking the log of the time series; ...|$|E
40|$|Generalized <b>additive</b> <b>models</b> (Generalized <b>Additive</b> <b>Models</b> – GAM) have a {{relatively}} short history, and hence their use is difficult. Although Hastie and Tibshirani [1] and Schimek [3] provide a detailed interpretation of them, yet in professional practice these methods were not used. In the literature for valuer...|$|R
40|$|This article {{describes}} the asymptotic properties of local polynomial regression estimators for univariate and <b>additive</b> <b>models</b> when observation weights are included. The implications of these findings are discussed for local scoring estimators, a widely used class of estimators for generalized <b>additive</b> <b>models</b> described in Hastie and Tibshirani (1990) ...|$|R
40|$|We give an {{overview}} over smooth back tting type estimators in <b>additive</b> <b>models.</b> Moreover we illustrate their wide applicability in models {{closely related to}} <b>additive</b> <b>models</b> such as nonparametric regression with dependent error variables where the errors can be transformed to white noise by a linear transformation, nonparametric regression with repeatedly measured data, nonparametric panels with fixed effects, simultaneous nonparametric equation models, and non- and semiparametric autoregression and GARCH-models. We also discuss extensions to varying coeffcient <b>models,</b> <b>additive</b> <b>models</b> with missing observations, and the case of nonstationary covariates...|$|R
5000|$|LogitBoost {{can be seen}} as a convex optimization. Specifically, {{given that}} we seek an <b>additive</b> <b>model</b> of the form ...|$|E
5000|$|Another {{extension}} in this domain is Functional Generalized <b>Additive</b> <b>Model</b> (FGAM)) {{which is}} a generalization of generalized additive model(GAM) where ...|$|E
50|$|Which is the {{standard}} formulation of a Generalized <b>Additive</b> <b>Model.</b> It was then shown that the backfitting algorithm will always converge for these functions.|$|E
40|$|We {{consider}} {{the problem of}} sparse variable selection in nonparametric <b>additive</b> <b>models,</b> with the prior knowledge of the structure among the covariates to encourage those vari-ables within a group to be selected jointly. Previous works either study the group spar-sity in the parametric setting (e. g., group lasso), or address {{the problem in the}} non-parametric setting without exploiting the structural information (e. g., sparse <b>additive</b> <b>models).</b> In this paper, we present a new method, called group sparse <b>additive</b> <b>models</b> (GroupSpAM), which can handle group spar-sity in <b>additive</b> <b>models.</b> We generalize the ` 1 /` 2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block co-ordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in <b>additive</b> <b>models,</b> and also conduct a comparative ex-periment on a real breast cancer dataset. 1...|$|R
40|$|Aim of this Talk {{analysis}} of highdimensional data by semiparametric (generalized) regression models • compare {{different approaches to}} <b>additive</b> <b>models</b> (AM) and generalized <b>additive</b> <b>models</b> (GAM) • include categorical variables = ⇒ partial linear terms (combination of AM/PLM and GAM/GPLM) • provide software ⇒ R package KernGPLM • focus on kernel-based techniques for high-dimensional dat...|$|R
5000|$|<b>Additive</b> <b>models</b> are a {{class of}} {{non-parametric}} regression models of the form: ...|$|R
5000|$|Given a {{data set}} [...] of n {{statistical}} units, where [...] represent predictors and [...] is the outcome, the <b>additive</b> <b>model</b> {{takes the form}} ...|$|E
5000|$|The <b>additive</b> <b>model</b> can be {{generalized}} to allow for arbitrary interaction effects by setting EYij = μ + αi + βj + γij. However, after fitting the natural estimator of γij, ...|$|E
5000|$|The <b>additive</b> <b>model</b> {{states that}} the {{expected}} response can be expressed EYij = μ + αi + βj, where the αi and βj are unknown constant values. The unknown model parameters are usually estimated as ...|$|E
5000|$|T. Hastie and R. Tibshirani, Generalized <b>Additive</b> <b>Models,</b> Chapman and Hall, 1990.|$|R
50|$|The Huber loss {{function}} {{is used in}} robust statistics, M-estimation and <b>additive</b> <b>modelling.</b>|$|R
40|$|International audienceRecently, Kernel <b>Additive</b> <b>Modelling</b> was {{proposed}} {{as a new}} framework for performing sound source separation. Kernel <b>Additive</b> <b>Modelling</b> assumes that a source at some location can be estimated using its values at nearby locations where nearness is defined through a source-specific proximity kernel. Different proximity kernels {{can be used for}} different sources, which are then separated using an iterative kernel backfitting algorithm. These kernels can efficiently account for features such as continuity, stability in time or frequency and self-similarity. Here, we show that Kernel <b>Additive</b> <b>Modelling</b> can be used to generalise, extend and improve on a widely-used harmonic/percussive separation algorithm which attempts to separate pitched from percussive instruments...|$|R
50|$|An <b>additive</b> <b>model</b> {{would be}} used when the {{variations}} around the trend does not vary {{with the level of}} the time series where as a multiplicative model would be appropriate if the trend is proportional {{to the level of the}} time series.|$|E
5000|$|Sometimes the {{interacting}} {{variables are}} categorical variables rather than real {{numbers and the}} study might then be dealt with as {{an analysis of variance}} problem. For example, members of a population may be classified by religion and by occupation. If one wishes to predict a person's height based only on the person's religion and occupation, a simple <b>additive</b> <b>model,</b> i.e., a model without interaction, would add to an overall average height an adjustment for a particular religion and another for a particular occupation. A model with interaction, unlike an <b>additive</b> <b>model,</b> could add a further adjustment for the [...] "interaction" [...] between that religion and that occupation. This example may cause one to suspect that the word interaction is something of a misnomer.|$|E
50|$|Apolipoprotein A1 and APOE {{interact}} epistatically {{to modulate}} triglyceride levels in {{coronary heart disease}} patients. Individually, neither apo A1 nor apo E {{was found to be}} associated with triglyceride (TG) levels, but pairwise epistasis (additive x <b>additive</b> <b>model)</b> explored their significant synergistic contributions with raised TG levels (P<0.01).|$|E
40|$|Real {{data may}} expose a larger (or smaller) {{variability}} than assumed in an exponential family modeling, {{the basis of}} Generalized linear <b>models</b> and <b>additive</b> <b>models.</b> To analyze such data, smooth estimation of the mean and the dispersion function has been introduced in extended generalized <b>additive</b> <b>models</b> using Psplines techniques. This methodology is further explored here by allowing for the modeling {{of some of the}} covariates parametrically and some nonparametrically. The main contribution in this article is a simulation study investigating the finite-sample performance of the P-spline estimation technique in these extended models, including comparisons with a standard generalized <b>additive</b> <b>modeling</b> approach, as well as with a hierarchical modeling approach...|$|R
30|$|The nonparametric {{predictor}} using <b>additive</b> <b>models</b> {{with the}} estimation of each component independently (NPM).|$|R
5000|$|Relative to {{generalized}} <b>additive</b> <b>models,</b> PPR can estimate a {{much richer}} class of functions ...|$|R
