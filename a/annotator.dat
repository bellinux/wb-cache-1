864|1403|Public
25|$|In {{cases where}} PDFs are {{expected}} to have all of the functionality of paper documents, ink annotation is required. Some programs that accept ink input from the mouse may not be responsive enough for handwriting input on a tablet. Existing solutions on the PC include PDF <b>Annotator</b> and Qiqqa.|$|E
25|$|Officers in {{undergraduate}} chapters mostly have titles derived from Imperial Rome. The top officers of each chapter {{are known as}} the Consul (president), Pro Consul (vice-president), <b>Annotator</b> (minute-taker), Quaestor (treasurer), Magister (pledge trainer), Kustos (sergeant-at-arms), Tribune (communications), and Historian. Those titles are the primary officers common to all chapters. Chapters also have other positions, such as Social Chairman, Sports Chairman, Scholarship Chairman, House Manager, Rush Chairman, etc., plus other positions and titles varying from chapter to chapter.|$|E
500|$|Commentators have {{disagreed}} {{over the}} kelpie's aquatic habitat. Folklorists who define kelpies as spirits living beside rivers, as {{distinguished from the}} Celtic lakeside-dwelling water horse (each-uisge), include 19th-century minister of Tiree John Gregorson Campbell and 20th-century writers Lewis Spence and Katharine Briggs. This distinction is not universally applied however; Sir Walter Scott for instance claims that the kelpie's range may extend to lochs. Mackillop's dictionary reconciles the discrepancy, stating that the kelpie was [...] "initially thought to inhabit... streams, and later any body of water." [...] But the distinction should stand argues one <b>annotator,</b> who suggests that people are led astray when an each uisge in a [...] "common practice of translating" [...] {{are referred to as}} kelpies in English accounts, and thus mistakenly attribute lake-dwelling habits to the latter.|$|E
40|$|We {{present a}} pilot study of word-sense {{annotation}} using multiple <b>annotators,</b> relatively polysemous words, and a heterogenous corpus. <b>Annotators</b> selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that <b>annotators</b> agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and <b>annotators</b> that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across <b>annotators,</b> and present the use of association rules to mine the data for systematic differences across <b>annotators.</b> ...|$|R
30|$|The {{emotion in}} each {{utterance}} was evaluated in a listener test {{by a large}} number of <b>annotators</b> (27 university students) independently of one another. The <b>annotators</b> were asked to listen to the entire speech recordings (randomly permuted) and assign an emotion label (both categorical and dimensional) for each utterance. The <b>annotators</b> only took audio information into consideration.|$|R
40|$|With {{the advent}} of {{crowdsourcing}} services it has become quite cheap and reasonably effective to get a data set labeled by multiple <b>annotators</b> in {{a short amount of}} time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of <b>annotators</b> with different kinds of expertise. Since we do not have control over the quality of the <b>annotators,</b> very often the annotations can be dominated by spammers, defined as <b>annotators</b> who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good <b>annotators.</b> The algorithm is motivated by defining a spammer score {{that can be used to}} rank the <b>annotators.</b> Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of <b>annotators...</b>|$|R
500|$|New Morning for the World, {{composed}} in 1982 on commission from AT, premiered on January 15, 1983 (King's birthday) at the John F. Kennedy Center for the Performing Arts; David Effron conducted the Eastman Philharmonia, and Willie Stargell, then first baseman and team {{captain of the}} Pittsburgh Pirates, served as narrator. Schwantner selected words from public speeches by King that spanned {{more than a decade}} of his life. In the album's liner notes, program <b>annotator</b> and classical music radio host Jim Svejda described the work as having [...] "equal parts" [...] for the orchestra and the speaker, with King's words [...] "supported and illuminated by an orchestra fabric of unusual variety and flexibility". Music critics compared Schwantner's composition to Aaron Copland's Lincoln Portrait because of its prominent narrative passages and its [...] "broad and lyrical scoring that sounds unmistakably American". In describing the work, Melinda Bargreen of The Seattle Times wrote that percussion and [...] "soaring" [...] strings helped to emphasize King's orations. New Morning for the World contains text from the following speeches and writings by King: [...] "Stride Toward Freedom" [...] (1958), [...] "Behind the Selma March" [...] (1965), and [...] "Letter from Birmingham Jail" [...] (1963); the composition ends with King's [...] "I Have a Dream" [...] speech.|$|E
2500|$|The {{belief in}} reincarnation {{may have been}} commonplace among the Norse since the <b>annotator</b> of the Poetic Edda wrote that people {{formerly}} used to believe in it: ...|$|E
2500|$|He {{served as}} musical <b>annotator</b> on several {{subsequent}} East–West collaborations by Shankar, who {{described him as}} [...] "a brilliant young pianist". One such project was Shankar's score for Alice in Wonderland (1966), a BBC TV film directed by Jonathan Miller.|$|E
40|$|We {{describe}} {{results of}} a word sense annotation task using WordNet, involving half a dozen well-trained <b>annotators</b> on ten polysemous words for three parts of speech. One hundred sentences for each word were annotated. <b>Annotators</b> had {{the same level of}} training and experience, but interannotator agreement (IA) varied across words. There was some effect of part of speech, with higher agreement on nouns and adjectives, but within the words for each part of speech there was wide variation. This variation in IA does not correlate with number of senses in the inventory, or the number of senses actually selected by <b>annotators.</b> In fact, IA was sometimes quite high for words with many senses. We claim that the IA variation is due to the word meanings, contexts of use, and individual differences among <b>annotators.</b> We find some correlation of IA with sense confusability as measured by a sense confusion threshhold (CT). Data mining for association rules on a flattened data representation indicating each <b>annotator’s</b> sense choices identifies outliers for some words, and systematic differences among pairs of <b>annotators</b> on others. 1...|$|R
40|$|Semantic {{annotation}} is {{the process}} of identifying expressions in texts and linking them to some semantic structure. In particular, Linked data-based Semantic <b>Annotators</b> are now becoming the new Holy Grail for meaning extraction from unstructured documents. This paper presents an evaluation of the main linked data-based <b>annotators</b> available with a focus on domain topics and named entities. In particular, we compare the ability of each tool to annotate relevant domain expressions in text. The paper also proposes a combination of <b>annotators</b> through voting methods and machine learning. Our results show that some linked-data <b>annotators,</b> especially Alchemy, can be considered as a useful resource for topic extraction. They also show that a substantial increase in recall can be achieved by combining the <b>annotators</b> with a weighted voting scheme. Finally, an interesting result is that by removing Alchemy from the combination, or by combining only the more precise <b>annotators,</b> we get a significant increase in precision, at the cost of a lower recall. Categories and Subject Descriptors [Natural Language Processing]: [Text analysis, Information extraction...|$|R
40|$|Many {{interesting}} {{phenomena in}} conversations require interpretative judgements by the <b>annotators.</b> This leads to data which is annotated {{with lower levels}} of agreement due to the differences in how <b>annotators</b> interpret conversations. Instead of throwing away this data we show how and when we can exploit it. We analyse the (dis) agreements between <b>annotators</b> for two different cases in a multimodal annotated corpus and explicitly relate the results to the way machine-learning algorithms perform on the annotated data...|$|R
2500|$|It {{was said}} that Li Xi was studious and had a large book collection, earning him the {{nickname}} of Shulou (i.e., [...] "tower of books"). [...] He was also a prolific writer and <b>annotator.</b> [...] In the aftermaths of his death, however, the collection was lost.|$|E
2500|$|John Aubrey, in 1693, {{reported}} that Shakespeare {{had been a}} country schoolmaster, a tale augmented in the 20th century with the theory that his employer might have been Alexander Hoghton of Lancashire, a prominent Catholic landowner who left money in his will to a certain [...] "William Shakeshafte", referencing theatrical costumes and paraphernalia. Shakespeare's grandfather Richard had also once used the name Shakeshafte. Peter Ackroyd adds that examinations of the marginal notes in the Hoghton family copy of Edward Hall's Chronicles, an important source for Shakespeare's early histories, [...] "indicate the probability that Shakespeare and the <b>annotator</b> were the same man, but do {{not by any means}} prove it." ...|$|E
2500|$|The {{term used}} for [...] "marine lung" [...] (pleumōn thalattios) appears {{to refer to}} {{jellyfish}} of the type the ancients called sea-lung. The latter are mentioned by Aristotle in On the Parts of Animals as being free-floating and insensate. They are not further identifiable from what Aristotle says but some pulmones appear in Pliny as a class of insensate sea animal; specifically the halipleumon ("salt-water lung"). William Ogle, Aristotle's translator and <b>annotator,</b> attributes the name sea-lung to the lung-like expansion and contraction of the Medusae, a kind of Cnidaria, during locomotion. The ice resembled floating circles in the water. The modern term for this phenomenon is pancake ice.|$|E
40|$|In many {{supervised}} learning tasks {{it can be}} costly or infeasible to obtain objective, reliable labels. We may, however, be able to obtain {{a large number of}} subjective, possibly noisy, labels from multiple <b>annotators.</b> Typically, <b>annotators</b> have di??erent levels of expertise (i. e., novice, expert) and there is considerable diagreement among <b>annotators.</b> We present a Gaussian process (GP) approach to regression with multiple labels but no absolute gold standard. The GP framework provides a principled non-parametric framework that can automatically estimate the reliability of individual <b>annotators</b> from data without the need of prior knowledge. Experimental results show that the proposed GP multi-annotator model outperforms models that either average the training data or weigh individually learned single-annotator models...|$|R
40|$|With {{the rapid}} growth of {{crowdsourcing}} platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple <b>annotators</b> in a short time. However {{due to the lack of}} control over the quality of the <b>annotators,</b> some abnormal <b>annotators</b> may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect <b>annotator's</b> position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased <b>annotators</b> - the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying <b>annotator's</b> abnormal behavior in crowdsourcing data arising from machine learning, sociology, computer vision, multimedia, etc. Comment: ICML 2016 accepte...|$|R
40|$|International audienceThis paper {{presents}} the results of an investigation of inter-annotator agreement for the non-native and native French part of the IFCASL corpus. This large bilingual speech corpus for French and German language learners was manually annotated by several <b>annotators.</b> This manual annotation is the starting point which will be used both to improve the automatic segmentation algorithms and derive diagnosis and feedback. The agreement is evaluated by comparing the manual alignments of seven <b>annotators</b> to the manual alignment of an expert, for 18 sentences. Whereas results for the presence of the devoicing diacritic show a certain degree of disagreement between the <b>annotators</b> and the expert, there is a very good consistency between <b>annotators</b> and the expert for temporal boundaries as well as insertions and deletions. We find a good overall agreement for boundaries between <b>annotators</b> and expert with a mean deviation of 7. 6 ms and 93 % of boundaries within 20 ms...|$|R
2500|$|GenMAPP (Gene Map <b>Annotator</b> and Pathway Profiler) is a free, {{open-source}} bioinformatics {{software tool}} designed to visualize and analyze genomic {{data in the}} context of pathways (metabolic, signaling), connecting gene-level datasets to biological processes and disease. First created in 2000, GenMAPP is developed by an open-source team based in an academic research laboratory. GenMAPP maintains databases of gene identifiers and collections of pathway maps in addition to visualization and analysis tools. [...] Together with other public resources, GenMAPP aims to provide the research community with tools to gain insight into biology through the integration of data types ranging from genes to proteins to pathways to disease.|$|E
2500|$|Reading {{the three}} {{instances}} of the wife-sister motif in (a) [...] (b) [...] and (c) [...] Speiser {{argued that in}} a work by a single author, these three cases would present serious contradictions: [...] Abraham would have learned nothing from his narrow escape in Egypt, and so tried the same ruse in Gerar; and Abimelech {{would have been so}} little sobered by his perilous experience with Abraham and Sarah that he fell into the identical trap with Isaac and Rebekah. [...] Speiser concluded (on independent grounds) that the Jahwist was responsible for incidents (a) and (c), while the Elohist was responsible for incident (b). [...] If the Elohist had been merely an <b>annotator</b> of the Jahwist, however, the Elohist would still have seen the contradictions for Abimelech, a man of whom the Elohist clearly approved. [...] Speiser concluded that the Jahwist and the Elohist therefore must have worked independently.|$|E
2500|$|A {{commonly}} held {{view is that}} Chola is, like Chera and Pandya, {{the name of the}} ruling family or clan of immemorial antiquity. The <b>annotator</b> Parimelazhagar said: [...] "The charity of people with ancient lineage (such as the Cholas, the Pandyas and the Cheras) are forever generous in spite of their reduced means". Other names in common use for the Cholas are Killi (கிள்ளி), Valavan (வளவன்) and Sembiyan (செம்பியன்). Killi perhaps comes from the Tamil kil (கிள்) meaning dig or cleave and conveys the idea of a digger or a worker of the land. This word often forms an integral part of early Chola names like Nedunkilli, Nalankilli and so on, but almost drops out of use in later times. Valavan is most probably connected with [...] "valam" [...] (வளம்)– fertility and means owner or ruler of a fertile country. Sembiyan is generally taken to mean a descendant of Shibi– a legendary hero whose self-sacrifice in saving a dove from the pursuit of a falcon figures among the early Chola legends and forms {{the subject matter of the}} Sibi Jataka among the Jataka stories of Buddhism. In Tamil lexicon Chola means Soazhi or Saei denoting a newly formed kingdom, in the lines of Pandya or the old country.|$|E
40|$|The {{challenge}} of prosodic annotation {{is reflected in}} commonly reported variability among trained <b>annotators</b> in the assignment of prosodic labels. The present study examines individual differences {{in the perception of}} prosody through the lens of prosodic annotation. First, Generalized Additive Mixed Models (GAMMs) reveal the non-linear pattern of some acoustic cues on the perception of prosodic features. Second, these same models reveal that while some of the untrained <b>annotators</b> are using these cues to determine prosodic features, the magnitude of effect differs quite dramatically across the <b>annotators.</b> Finally, the trained <b>annotators</b> follow the same cues as subsets of the untrained <b>annotators,</b> but present a much stronger effect for many of the cues. The findings show that while prosody perception is systemically related to acoustic and contextual cues, there are also individual differences that are limited to the selection and magnitude of the factors that influence prosodic rating, and the relative weighting among those factors...|$|R
40|$|We discuss {{factors that}} affect human {{agreement}} on a semantic labeling task in the art history domain, {{based on the results}} of four experiments where we varied the number of labels <b>annotators</b> could assign, the number of <b>annotators,</b> the type and amount of training they received, and the size of the text span being labeled. Using the labelings from one experiment involving seven <b>annotators,</b> we investigate the relation between interannotator agreement and machine learning performance. We construct binary classifiers and vary the training and test data by swapping the labelings from the seven <b>annotators.</b> First, we find performance is often quite good despite lower than recommended interannotator agreement. Second, we find that on average, learning performance for a given functiona...|$|R
40|$|Abstract. Museums {{are rapidly}} {{digitizing}} their collections, and face a huge challenge to annotate every digitized artifact in store. Therefore they are opening up their archives for receiving annotations from experts world-wide. This paper presents an architecture for choosing the most eligible set of <b>annotators</b> {{for a given}} artifact, based on semantic relatedness measures between {{the subject matter of}} the artifact and topics of expertise of the <b>annotators.</b> We also employ mechanisms for evaluating the quality of provided annotations, and constantly manage and update the trust, reputation and expertise information of registered <b>annotators.</b> 1...|$|R
6000|$|... [43] Terms {{not used}} now, but others quite as bad: Cuticle, Epidermis, Cortical layer, Periderm, Cambium, Phelloderm--six hard words for 'BARK,' says my careful <b>annotator.</b> [...] "Yes; and these new six {{to be changed}} for six newer ones next year, no doubt." ...|$|E
6000|$|Sir,--A copy of verses, to {{the tune}} of 'My boy Tammy,' are {{repeated}} in literary circles, and said to be written by a Noble Lord of the highest poetical fame, upon his quondam friend and <b>annotator.</b> My memory does not enable me to repeat more than the first two verses quite accurately, but the humourous spirit of the Song may be gathered from these:-- ...|$|E
6000|$|The reader may ask: How did Sophia know {{anything}} about the obscure Christian captive? WHY did she leave home exactly in time for his marriage? How came Lord Bateman to be so fickle? The <b>Annotator</b> replies: 'His lordship had doubtless been impelled by despair of ever recovering his lost Sophia, and a natural anxiety not to die without leaving an heir to his estate.' Finally how was the difficulty of Sophia's religion overcome? ...|$|E
40|$|This paper {{presents}} a community-sourcing annotation framework, {{which is designed}} to implement a “marketplace model ” of annotation tasks and <b>annotators,</b> with an emphasis on efficient management of community of potential <b>annotators.</b> As a position paper, it explains the motivation and the design concept of the framework, with a prototype implementation. ...|$|R
40|$|Expertise of <b>annotators</b> has a {{major role}} in {{crowdsourcing}} based opinion aggregation models. In such frameworks, accuracy and biasness of <b>annotators</b> are occasionally taken as important features and based on them priority of the <b>annotators</b> are assigned. But instead of relying on a single feature, multiple features can be considered and separate rankings can be produced to judge the <b>annotators</b> properly. Finally, the aggregation of those rankings with perfect weightage can be done with an aim to produce better ground truth prediction. Here, we propose a novel weighted rank aggregation method and its efficacy with respect to other existing approaches is shown on artificial dataset. The effectiveness of weighted rank aggregation to enhance quality prediction is also shown by applying it on an Amazon Mechanical Turk (AMT) dataset. Comment: Works-in-Progress, Fifth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2017), Quebec City, Canad...|$|R
40|$|Word sense {{disambiguation}} aims {{to identify}} which meaning of a word is present in a given usage. Gathering word sense annotations is a laborious and difficult task. Several methods have been proposed to gather sense annotations using large numbers of untrained <b>annotators,</b> with mixed results. We propose three new annotation methodologies for gathering word senses where untrained <b>annotators</b> are allowed to use multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as <b>annotators</b> in a controlled setting, and in aggregate generate equally as good of a sense labeling. ...|$|R
60|$|For example: One of Byron's poems, written while {{a schoolboy}} at Harrow, is {{addressed}} to 'My Son.' Mr. Moore, and the <b>annotator</b> {{of the standard}} edition of Byron's poems, gravely give the public their speculations on the point, whether Lord Byron first became a father while a schoolboy at Harrow; and go into particulars {{in relation to a}} certain infant, the claim to which lay between Lord Byron and another schoolfellow. It is not the nature of the event itself, so much as the cool, unembarrassed manner in which it is discussed, that gives the impression of the state of public morals. There is no intimation of anything unusual, or discreditable to the school, in the event, and no apparent suspicion that it will be regarded as a serious imputation on Lord Byron's character.|$|E
6000|$|... --Stop! Have {{you turned}} two pages? Still the same. [...] New reign, same date. The scribe {{goes on to}} say [...] How that same year, on such a month and day, [...] "John the Pannonian, groundedly {{believed}} [...] A blacksmith's bastard, whose hard hand reprieved [...] The Empire from its fate the year before, [...] Came, had a mind to take the crown, and wore [...] The same for six years (during which the Huns [...] 40 [...] Kept off their fingers from us), till his sons [...] Put something in his liquor"--and so forth. [...] Then a new reign. Stay--"Take at its just worth" [...] (Subjoins an <b>annotator)</b> [...] "what I give [...] As hearsay. Some think, John let Protus live [...] And slip away. 'Tis said, he reached man's age [...] At some blind northern court; made, first a page, [...] Then tutor to the children; last, of use [...] About the hunting-stables. I deduce [...] He wrote the little tract 'On worming dogs,' [...] 50 [...] Whereof the name in sundry catalogues [...] Is extant yet. A Protus of the race [...] Is rumoured to have died a monk in Thrace, [...] And if the same, he reached senility." ...|$|E
6000|$|Some {{time during}} the month of December, 1817, Byron wrote out a fair copy of the entire canto, {{numbering}} 184 stanzas (MS. D.); and on January 7, 1818, Hobhouse left Venice for England, with the [...] "whole of the MSS.," [...] viz. Beppo (begun October, 1817), and the Fourth Canto of Childe Harold, together with a work of his own, a volume of essays on Italian literature, the antiquities of Rome, etc., which he had put together during his residence in Venice (July--December, 1817), and proposed to publish as an appendix to Childe Harold. In his preface to Historical Illustrations, etc., 1818, Hobhouse explains that on his return to England he considered that this [...] "appendix to the Canto would be swelled to a disproportioned bulk," [...] and that, under this impression, he determined to divide his material into two parts. The result was that [...] "such only of the notes as were more immediately connected with the text" [...] were printed as [...] "Historical Notes to Canto the Fourth," [...] and that his longer dissertations were published in a separate volume, under his own name, as Historical Illustrations to the Fourth Canto of Childe Harold. To these [...] "Historical Notes" [...] an interest attaches apart from any consideration of their own worth and importance; but to understand the relation between the poem and the notes, it is necessary to retrace the movements of the poet and his <b>annotator.</b>|$|E
40|$|Part 2 : Short PapersInternational audienceMuseums {{are rapidly}} {{digitizing}} their collections, and face a huge challenge to annotate every digitized artifact in store. Therefore they are opening up their archives for receiving annotations from experts world-wide. This paper presents an architecture for choosing the most eligible set of <b>annotators</b> {{for a given}} artifact, based on semantic relatedness measures between {{the subject matter of}} the artifact and topics of expertise of the <b>annotators.</b> We also employ mechanisms for evaluating the quality of provided annotations, and constantly manage and update the trust, reputation and expertise information of registered <b>annotators...</b>|$|R
40|$|To enhance {{readability}} and usability {{of speech}} recognition results, automatic punctuation {{is an essential}} process. In this paper, we address automatic comma prediction based on conditional random fields (CRF) using lexical, syntactic and pause information. Since there is large disagreement in comma insertion between humans, we model individual tendencies of punctuation using annotations given by multiple <b>annotators,</b> and combine these models by voting and interpolation frameworks. Experimental evaluations on real lecture speech demonstrated {{that the combination of}} individual punctuation models achieves higher prediction accuracy for commas agreed by all <b>annotators</b> and those given by individual <b>annotators...</b>|$|R
5000|$|Milton: a Sheaf of Gleanings {{after his}} Biographers and <b>Annotators</b> (1850) ...|$|R
