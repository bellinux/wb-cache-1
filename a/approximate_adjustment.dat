3|18|Public
40|$|Treball Final de Grau en Economia. Codi: EC 1049. Curs acadèmic 2013 - 2014 This paper {{explores the}} {{empirical}} distributions and descriptive statistics {{of both the}} economic rentability and economic growth variables (number of employees, total assets and sales) of 182 large German firms during the period 1990 – 2012, along with the theoretical framework of the neoclassical competition, the importance of large firms over the macroeconomic fluctuations and the realocation of capital within the economy based on profit rates. The results show that at the aggregate level, ROA and the growth variables both prove an <b>approximate</b> <b>adjustment</b> to a Laplace distribution. Thus, we can identify similarities in the evolution and properties of these variables with the United States, Spanish and Icelandic models...|$|E
40|$|Mean {{temperatures}} for {{the normal}} period (1961 - 90) from 513 stations in Sweden, {{together with their}} geographical positions, are used to derive temperature sums and average lengths of the growing season for different threshold values as functions of latitude and altitude. Maps are drawn showing the distribution of these temperature indices. Swedish regional average forest yield is well correlated with the temperature sums for the threshold value 5 °C (Pearson correlation coefficient: 0. 89). Solar radiation data for the period 1983 - 91 from 14 stations in Sweden are used to estimate global radiation reaching the earth's surface during clear days and during days irrespective of cloudiness. This data set {{is also used to}} derive global radiation sums expressed as functions of latitude and altitude. Corrections for an <b>approximate</b> <b>adjustment</b> of temperature indices to the soil surface level and with respect to local continentality/maritimity are proposed. The solar radiation on slopes has been calculated for various inclinations and azimuths...|$|E
40|$|The median total mercury {{concentration}} in 898 UK rural topsoils, sampled between 1998 and 2008, was 0. 095 μg g- 1. <b>Approximate</b> <b>adjustment</b> for unreactive metal produced {{an estimate of}} 0. 052 μg g- 1 for reactive Hg. The highest concentrations were {{in the north and}} west, where organic-rich soils with low bulk densities dominate, but the spatial pattern was quite different if soil Hg pools (mg m- 2) were considered, the highest values being near to the industrial north of England and London. Possible toxic effects of Hg were best evaluated by comparison with soil Critical Limits expressed as ratios of Hg to soil organic matter, or soil solution Hg 2 + concentrations, estimated by chemical speciation modelling. Only a few percent of the rural UK soils showed exceedance, and this also applied to rural soils from the whole of Europe. UK urban and industrial soils had higher Hg concentrations and more cases of exceedance...|$|E
40|$|In Australia, {{eutrophication}} {{and reduced}} river ¯ows have {{lead to an}} increase in the frequency and severity of algal blooms. To combat eutrophication, catchment managers need to assess catchment nutrient loads and, for long-term planning, annual average nutrient loads provide a sufficiently good measure. Land use is known to be an integrator of many environmental attributes that in¯uence nutrient export, and is a simple and convenient predictor of nutrient loads. Nutrient export data from Australian studies are limited, and reference to North American data is usually necessary. In this paper we summarise the reliable Australian data on nutrient report from different broad land use types and compare them to the more extensive North American data sets. We also discuss the effects of a number of important environmental attributes on nutrient export. We conclude that while nutrient export rates are well established for North American conditions, these export rates are often signi®cantly higher than Australian export rates and may not always be suitable for supplementing the sparse Australian data set. In addition, while the general effects of several environmental attributes (such as land slope) are fairly well established in both Australia and North America, the effects of others (such as geology) have been studied, but the results are inconclusive. The available data make it impossible to account for any environmental attribute precisely, but we propose that in some cases the data are sufficient to allow <b>approximate</b> <b>adjustments</b> to nutrient export rates. In particular, <b>approximate</b> <b>adjustments</b> can be made in certain circumstances for variations in rainfall intensity and runoff volume...|$|R
40|$|Constant {{maturity}} swaps can {{be valued}} {{by adjusting the}} forward swap rate. The correction {{is known as the}} convexity adjustment. This report examines the framework that allows us to calculate the convexity adjustment. The current approaches to the problem of <b>approximating</b> the convexity <b>adjustment</b> are both derived an...|$|R
40|$|It is {{straight}} forward to analyze {{data from a}} single multinomial table. Speci¯cally, {{for the analysis of}} a two-way categorical table, the common chi-squared test of independence between the two variables and maximum likelihood estimators are readily available. When the counts in the two-way categorical table are formed from familial data (clusters of correlated data), the common chi-squared test no longer applies. We note that there are several <b>approximate</b> <b>adjustments</b> to the common chi-squared test. For example, Choi and McHugh (1989, Biometrics 45, 979 - 996) showed how to adjust the chi-squared statistic for clustered and weighted data. However, our main contribution is the construction and analysis of a Bayesian model which removes all analytical approximations. This is an extension of a standard multinomial-Dirichlet model to include the intra-class correlation associated with the individuals within a cluster. We have used a key formula described by Altham (1976, Biometrika 63, 263 - 269) to incorporate the intra-class correlation. This intra-class correlation varies with the size of the cluster, but we assume that it is the same for all clusters of the same size for the same variable. We use Markov chain Monte Carlo methods to ¯t our model, and to make posterior inference about the intra-class correlations and the cell probabilities. Also, using Monte Carlo integration with a binomial importance function, we obtain the Bayes factor for a test of no association. To demonstrate the performance of the alternative test and estimation procedure, we have used data on activity limitation status and age from the National Health Interview Survey and a simulation study...|$|R
40|$|The paper {{describes}} a new iterative technique for designing FIR (finite duration impulse response) digital filters using a frequency weighted least squares approximation. The technique {{is as easy}} to implement (via FFT) and as effective in two dimensions as in one dimension, and there are virtually no limitations on the class of filter frequency spectra <b>approximated.</b> An adaptive <b>adjustment</b> of the frequency weight to achieve other types of design approximation such as Chebyshev type design is discussed...|$|R
40|$|International audienceThis paper {{presents}} PID tuning method. The proposed {{method is}} based on the performances represented by a permitted domain for time response of the controlled system. To reach these performances, the controller parameters have to satisfy inequality constraints which can be solved by linear matrix inequality (LMI) methods. A brief introduction reviews the principle of parameters adjustment of a controller due to the model method. An <b>approximate</b> method of <b>adjustment</b> with respect to inequality constraints is considered. An optimal version is then give...|$|R
40|$|We {{propose a}} scheme of {{iterative}} {{adjustments to the}} profile score to deal with incidental-parameter bias in models for stratified data with few observations on {{a large number of}} strata. The first-order adjustment is based on a calculation of the profile-score bias and evaluation of this bias at maximum-likelihood estimates of the incidental parameters. If the bias does not depend on the incidental parameters, the first-order adjusted profile score is fully recentered, solving the incidental-parameter problem. Otherwise, it is approximately recentered, alleviating the incidental-parameter problem. In the latter case, the adjustment can be iterated to give higher-order adjustments, possibly until convergence. The adjustments are generally applicable (e. g., not requiring parameter orthogonality) and lead to estimates that generally improve on maximum likelihood. We examine a range of nonlinear models with covariates. In many of them, we obtain an adjusted profile score that is exactly unbiased. In the others, we obtain <b>approximate</b> bias <b>adjustments</b> that yield much improved estimates, relative to maximum likelihood, even when there are only two observations per stratum...|$|R
40|$|This comment {{addresses}} a point raised in Russell Cooper and Jonathan Willis (2003, 2004), which discusses whether the "gap approach" {{is appropriate to}} describe the adjustment of production factors. They show that this approach to labor adjustment as applied in Ricardo J. Caballero, Eduardo Engel, and John C. Haltiwanger (1997) and Caballero and Engel (1993) can falsely generate evidence in favor of nonconvex adjustment costs, even if costs are quadratic. Simulating a dynamic model of firm-level employment decisions with quadratic adjustment costs and estimating a gap model from the simulated data, they identify two factors producing this spurious evidence: <b>approximating</b> dynamic <b>adjustment</b> targets by static ones, and estimating the static targets themselves. This comment reassesses whether the first factor indeed leads to spurious evidence in favor of fixed adjustment costs. We show that the numerical approximation of the productivity process is pivotal for Cooper and Willis's finding. With more precise approximations of the productivity process, it becomes rare to falsely reject the quadratic adjustment cost model due to the approximation of dynamic targets by static ones. (JEL E 24, J 3) ...|$|R
40|$|The paper {{presents}} an econometric model of dynamic agricultural input demand functions that include research based technical change and autoregressive disturbances and fits {{the model to}} annual data {{for a set of}} state aggregates pooled over 1950 – 1982. The methodological approach is one of developing a theoretical foundation for a dynamic input demand system and accepting state aggreage behavior as <b>approximated</b> by nonlinear <b>adjustment</b> costs and long-term profit maximization. Although other studies have largely ignored autocorrelation in dynamic input demand systems, the results show shorter adjustment lags with autocorrelation than without. Dynamic input demand own-price elasticities for the six input groups are inelastic, and the demand functions possess significant cross-price and research effects...|$|R
40|$|The usual {{credibility}} formula holds whenever, (i) claim {{size distribution}} {{is a member}} of the exponential family of distributions, (ii) prior distribution conjugates with claim size distribution, and (iii) square error loss has been considered. As long as, one of these conditions is violent, the usual credibility formula no longer holds. This article, using the mean square error minimization technique, develops a simple and practical approach to the credibility theory. Namely, we approximate the Bayes estimator with respect to a general loss function and general prior distribution by a convex combination of the observation mean and mean of prior, say, <b>approximate</b> credibility formula. <b>Adjustment</b> of the <b>approximate</b> credibility for several situations and its form for several important losses are given. ...|$|R
40|$|This paper {{takes the}} {{permanent}} magnet servo motor-driven hydraulic system as the research object. The hydraulic system power source uses servo motor {{instead of the}} original asynchronous motor, becoming a new energy-saving, fast response, and easy to realize closed-loop control hydraulic power systems. Aiming at variability of the load for the hydraulic system, immune algorithm is introduced to incremental PID controller and derive control law. Using fuzzy strategy <b>approximating</b> antibodies inhibit <b>adjustment</b> function to enhance the stability and robustness of the system, and designing fuzzy immune PID controller, parameters of controller are self-optimized by particle swarm (PSO) algorithm. This controller is applied to a hydraulic system, and achieved the precise control of the system flow {{in a variety of}} typical conditions. Simulation results show that: the controller can immune modulate automatically according to the dynamic changes of load of the hydraulic system, with fast tracking, small overshoot, and strong robustness advantages...|$|R
40|$|This text is {{dedicated}} to my friends. I don’t have any friends. ii Bayesian Model Averaging (BMA) has previously been proposed {{as a solution to}} the vari-able selection problem when there is uncertainty about the true model in regression. Some recent research discusses the drawbacks; specifically, BMA can (and does) give biased pa-rameter estimates in the presence of confounding. This is because BMA is optimized for prediction rather than parameter estimation. Though some newer research attempts to fix the issue of bias under confounding, none of the current algorithms handle either large data sets or survival outcomes. The <b>Approximate</b> Two-phase Bayesian <b>Adjustment</b> for Confounding (ATBAC) algorithm proposed in this paper does both, and we use it on a large medical cohort study called THIN (The Health Improvement Network) to estimate the effect of statins on risk of stroke. We use simulation and some analytical techniques to discuss two main topics in this paper. Firstly, we demonstrate the ability of ATBA...|$|R
40|$|Bayesian Model Averaging (BMA) has {{previously}} been proposed {{as a solution to}} the variable selection problem when there is uncertainty about the true model in regression. Some recent research discusses the drawbacks; specifically, BMA can (and does) give biased parameter estimates in the presence of confounding. This is because BMA is optimized for prediction rather than parameter estimation. Though some newer research attempts to fix the issue of bias under confounding, none of the current algorithms handle either large data sets or survival outcomes. The <b>Approximate</b> Two-phase Bayesian <b>Adjustment</b> for Confounding (ATBAC) algorithm proposed in this paper does both, and we use it on a large medical cohort study called THIN (The Health Improvement Network) to estimate the effect of statins on risk of stroke. We use simulation and some analytical techniques to discuss two main topics in this paper. Firstly, we demonstrate the ability of ATBAC to perform unbiased parameter estimation on survival data while accounting for model uncertainty. Secondly, we discuss when it is, and isn 2 ̆ 7 t, helpful to use variable selection techniques in the first place, and find that in some large data sets variable selection for parameter estimation is unnecessary...|$|R
40|$|In the Bayesian {{framework}} {{a standard}} approach to model criticism {{is to compare}} some function of the observed data to a reference predictive distribution. The result of the comparison can be summarized {{in the form of}} a p-value, and computation of some kinds of Bayesian predictive p-values can be challenging. The use of regression <b>adjustment</b> <b>approximate</b> Bayesian computation (ABC) methods is explored for this task. Two problems are considered. The first is approximation of distributions of prior predictive p-values for the purpose of choosing weakly informative priors in the case where the model checking statistic is expensive to compute. Here the computation is difficult because of the need to repeatedly sample from a prior predictive distribution for different values of a prior hyperparameter. The second problem considered is the calibration of posterior predictive p-values so that they are uniformly distributed under some reference distribution for the data. Computation is difficult because the calibration process requires repeated approximation of the posterior for different data sets under the reference distribution. In both these problems we argue that high accuracy in the computations is not required, which makes fast approximations such as regression adjustment ABC very useful. We illustrate our methods with several examples...|$|R
40|$|The grain density, Nv, in {{the solid}} state after {{solidification}} of AZ 91 /SiC composite {{is a function}} of maximum undercooling, ΔT, of a liquid alloy. This type of function depends on the characteristics of heterogeneous nucleation sites and number of SiC present in the alloy. The aim of this paper was selection of parameters for the model describing the relationship between the grain density of primary phase and undercooling. This model in connection with model of crystallisation, which is based on chemical elements diffusion and grain interface kinetics, can be used to predict casting quality and its microstructure. Nucleation models have parameters, which exact values are usually not known and sometimes even their physical meaning is under discussion. Those parameters can be obtained after mathematical analysis of the experimental data. The composites with 0, 1, 2, 3 and 4 wt. % of SiC particles were prepared. The AZ 91 alloy was a matrix of the composite reinforcement SiC particles. This composite was cast to prepare four different thickness plates. They were taken from the region near to the thermocouple, to analyze the undercooling for different composites and thickness plates and its influence on the grain size. The microstructure and thermal analysis gave set of values that connect mass fraction of SiC particles, and undercooling with grain size. These values were used to <b>approximate</b> nucleation model <b>adjustment</b> parameters. Obtained model can be very useful in modelling composites microstructure...|$|R
40|$|Cerebral {{blood flow}} (CBF) was {{measured}} in 58 patients undergoing elective coronary artery surgery (CABS) prior to, during and following cardiopulmonary bypass. CBF fell significantly during hypothermic bypass and returned to prebypass levels {{following the end of}} bypass. At all times CBF, measured as the initial slope index (ISI), was significantly correlated to arterial carbon dioxide tension (PaCO 2) and cerebral oxygen consumption (CMRO 2). There was no significant correlation between CBF and changes in arterial blood pressure or, when on bypass, perfusion flow. This supports the notion that CBF autoregulation is maintained during bypass under the conditions of this study. The index of cerebral oxygen supply to demand (CERO 2), which reflects the ratio of oxygen consumed by the brain to that supplied by the cerebral circulation, was appropriately matched prior to and following bypass. During bypass the index fell significantly (p < 0. 001) to levels consistent with a mismatch of flow and demand indicating relative cerebral hyperperfusion. Acid base maintenance during bypass was consistent with a ‘pH stat’ protocol with pH on bypass <b>approximating,</b> following temperature <b>adjustment,</b> to 7. 40. The maintenance of pH stat requires a higher PaCO 2 than does the maintenance of alphastat. It may be that the adoption of an alphastat acid base management protocol, as seen in poikilotherms rather than hibernating mammals, and the resultant reduction in absolute PaCO 2 for any given temperature may result in a lowering of CBF and a return to appropriate matching of CBF and demand during bypass. This may be particularly important in relation to the generation of perfusion related, microembolic, cerebral damage during cardiopulmonary bypass. © 1988, Sage Publications. All rights reserved...|$|R
40|$|The present thesis {{discusses}} {{both the}} methodology as developed, and applications in practice. Methods of dealing in {{the statistical analysis}} with the occurrence of missing data are presented in Chapter 2 as a review of recent literature on this topic. Complete issues of Biometrics, Biometrika, Journal of the American Statistical Association, Statistics in Medicine and the American Journal of Epidemiology have been screened from 1986 up to 1991 on this point. Relevant literature from other sources has also been included. Two important statistics used in occupational health epidemiology are relative risk and prevalence. A study of possible effects on the relative risk {{in the event of}} inequality of the fractions of deaths and alives of which vital status is known, is described in Chapter 3. Adjustment procedures are proposed for calculating conservative confidence regions of the 11 trueu relative risk, using only little extra information. With respect to prevalence, adjustment procedures have been proposed by several authors. An attempt to combine the imputation method with postulating a missing-data-generating mechanism is presented in Chapter 4, where missing-data~ corrected estimates for the prevalence are presented. These estimates are compared with respect to their likelihood through a likelihood ratio test. Two general approaches concerning small~sample behaviour of statistics are exemplified in Chapters 5 and 6. Many parameter estimates that are used in the analysis of continuous outcomes, have well-known small-sample behaviour. This is not true for the two statistics that are often used in the analysis of categorical variables. Therefore Chapter 5 discusses an <b>approximate</b> Bartlett <b>adjustment</b> for testing the goodness-of-fit of multinomial regression models. This adjustment is applicable to any multinomial regression model, since it only needs estimated category frequencies and does not need any derivatives to be calculated. To illustrate the second approach, the consequences of calculating exact p~values for the Wilcoxon-Mann-Whitney test statistic in the case of ordinal response variables are presented in Chapter 6 with up to ten observations in each sample. From this it appears that the performance of / conditional statistics, based on observed ranks only, is not -so good as that of unconditional statistics based on an underlying, plausible, distribution. The trial protocol of the back school education programme intervention study included an investigation of differences between subgroups of participants and nonparticipants, in order to gain some understanding of the distinctions between those who want to participate and those who do not. If a determinant of participation is related to the primary outcome variable, adjustments have to be made to allow generalization of conclusions. Results obtained are presented in Chapter 7. Chapter 8 describes a cross-sectional study of person-related determinants of GGT levels. Model-based person-related tolerance regions are developed to facilitate detection of potential determinants. The main fmding is that persons with an observed GGT value under 30 U/ 1 in general do not need further screening, whereas persons with an observed GGT value above 60 U/ 1 do. For persons with intermediate GGT values person-related determinants should be taken into consideration. Thus, each individual is either "labelled 11 to undergo further follow-up or not. Persons who are so labelled are to be compared with persons who are not labelled in the actual liverfunction project. In Chapter 9, a model is postulated for correcting for long-term analytical bias of a specific laboratory compared to a pool of comparable laboratories in an e'-"ternal quality control programme...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 Genetic {{research}} with diverse and underserved communities {{is important for}} expanding the benefits of genetic discoveries and their application to personalized medicine. In partnership with Alaska Native communities, we identified and characterized novel and known variation in CYP 2 C 9, VKORC 1, CYP 4 F 2, CYP 4 F 11, and GGCX, 5 genes known to affect warfarin disposition and response, in 350 Yup’ik people and 365 customer-owners of Southcentral Foundation. Through resequencing and targeted genotyping, we identified two common novel variants M 1 L and N 218 I in CYP 2 C 9 and high frequencies of the VKORC 1 haplotype (- 1639 G>A and 1173 C>T) {{that are expected to}} result in increased warfarin sensitivity. We also observed high frequencies of CYP 4 F 2 * 3, which may increase vitamin K conservation and necessitate a higher warfarin dose to achieve the desired anticoagulation effect. Individual patient needs will depend on a complex mixture of genetic traits. Because of seasonal variation in sunlight exposure, vitamin D deficiency is a concern for people living in circumpolar regions. We characterized 25 (OH) D 3 concentrations in 743 Yup’ik people living in the Yukon Kuskokwim delta of southwestern Alaska and identified sources of inter-individual variability, including season of sample collection, genetic variation in CYP 2 R 1 and DHCR 7, age, gender, body mass index, the degree of consumption of the traditional diet, and inland or coastal geography of the community. Yup’ik participants on average had adequate concentrations of 25 (OH) D 3 (31. 1 +/- 0. 9 ng/mL), but a younger age (33 years), lower levels of a biomarker of traditional food consumption, and greater fluctuation with changes in sunlight exposure. Younger Yup’ik participants may be at increased risk for adverse outcomes associated with vitamin D deficiency, especially during seasons of low sunlight exposure. Finally, in genetic epidemiology research, genome-wide markers or pedigree information is used to adjust for population substructure and prevent confounding of results. Population substructure and the concerns of communities with respect to the methods used to adjust for it are described. A responsive justice framework is suggested as a tool to approach these conflicting demands of research, presenting as an example the stratification of participants by self-identified ancestral language group to <b>approximate</b> the statistical <b>adjustments</b> needed for population stratification...|$|R
40|$|Unmanned aerial {{vehicles}} (UAVs), {{also known}} as unmanned airborne systems (UAS) or remotely piloted airborne systems (RPAS), are an established platform for close range airborne photogrammetry. Compared to manned platforms, the acquisition of local remote sensing data by UAVs is a convenient and very flexible option. For the application in photogrammetry UAVs are typically equipped with an autopilot and a lightweight digital camera. The autopilot includes several navigation sensors, which might allow an automated waypoint flight and offer a systematic data acquisition of the object resp. scene of interest. Assuming a sufficient overlap between the captured images, the position (3 coordinates: x, y, z) and the orientation (3 angles: roll, pitch, yaw) of the images can be estimated within a bundle block adjustment. Subsequently, coordinates of observed points that appear {{in at least two}} images, can be determined by measuring their image coordinates or a dense surface model can be generated from all acquired images by automated image matching. For the bundle block <b>adjustment</b> <b>approximate</b> values of the position and the orientation of the images are needed. To gather this information, several methods exist. We introduce in this contribution one of them: the direct georeferencing of images by using the navigation sensors (mainly GNSS and INS) of a low-cost on-board autopilot. Beside automated flights, the autopilot offers the possibility to record the position and the orientation of the platform during the flight. These values don’t correspond directly to those of the images. To compute the position and the orientation of the images two requirements must be fulfilled. First the misalignment angles and the positional differences between the camera and the autopilot must be determined (mounting calibration). Second the synchronization between the camera and the autopilot has to be established. Due to the limited accuracy of the navigation sensors, a small number of ground control points should be used to improve the estimated values, especially to decrease the amount of systematic errors. For the bundle block adjustment the calibration of the camera and their temporal stability must be determined additionally. This contribution presents next to the theory a practical study on the accuracy analysis of direct georeferenced UAV imagery by low-cost navigation sensors. The analysis was carried out within the research project ARAP (automated (ortho) rectification of archaeological aerial photographs). The utilized UAS consists of the airplane “MAJA”, manufactured by “Bormatec” (length: 1. 2 m, wingspan: 2. 2 m) equipped with the autopilot “ArduPilot Mega 2. 5 ”. For image acquisition the camera “Ricoh GR Digital IV” is utilised. The autopilot includes a GNSS receiver capable of DGPS (EGNOS), an inertial measurement system (INS), a barometer, and a magnetometer. In the study the achieved accuracies for the estimated position and orientation of the images are presented. The paper concludes with a summary of the remaining error sources and their possible corrections by applying further improvements on the utilised equipment and the direct georeferencing process...|$|R

