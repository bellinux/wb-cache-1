0|2392|Public
5000|$|Ambric Am2045, a 336-core Massively <b>Parallel</b> <b>Processor</b> Array (MPPA) ...|$|R
40|$|An optical <b>parallel</b> <b>processor</b> is {{presented}} which optimizes binary phase diffractive optical elements using simulated annealing. A liquid crystal television {{is used to}} simulate the phase element. Experimental results are presented for the optimization of fan-out elements. The performance of the optical <b>parallel</b> <b>processor</b> is analyzed using Fast Fourier Transform methods...|$|R
50|$|Because {{the state}} is just a counter it is trivial to split random number {{generation}} between <b>parallel</b> <b>processors</b> without overlap. Skip-a-head and rewind in the stream of generated random numbers also become trivial O(1) operations. It is easy to organize different numbers of <b>parallel</b> <b>processors</b> to generate the same sequence of pseudorandom numbers.|$|R
5000|$|Massively <b>Parallel</b> <b>Processor</b> (MPP), from NASA/Goddard Space Flight Center, c. 1983-1991 ...|$|R
5000|$|UNC's Pixel-Planes, PixelFlow and WarpEngine {{series of}} <b>parallel</b> <b>processor</b> {{graphics}} workstations ...|$|R
40|$|Abstract — New {{algorithms}} {{are needed}} to solve electromagnetic problems using today’s widely available <b>parallel</b> <b>processors.</b> In this paper, we show that applying the optimized waveform relaxation approach to a partial element equivalent circuit will yield a powerful technique for solving electromagnetic problems {{with the potential for}} a large number of <b>parallel</b> <b>processor</b> nodes. I...|$|R
5000|$|Mitrionics is a Swedish company {{manufacturing}} softcore reconfigurable processors. It {{has been}} mentioned as one of EETimes [...] "60 Emerging startups".The company was founded in 2001 by Stefan Möhl and Pontus Borg to commercialize a massively <b>parallel</b> reconfigurable <b>processor</b> implemented on FPGAs. It {{can be described as}} turning general purpose chips into massive <b>parallel</b> <b>processors</b> {{that can be used for}} high performance computing. Mitrionics massively <b>parallel</b> <b>processor</b> is available on Cray, Nallatech, and Silicon Graphics systems.|$|R
5000|$|Fujitsu VP2000 {{optional}} running UXP/M for Mainframes as Vector and <b>Parallel</b> <b>Processors</b> ...|$|R
40|$|This paper {{presents}} novel {{high speed}} vision chips based on multiple levels of <b>parallel</b> <b>processors.</b> The chip integrates CMOS image sensor, multiple-levels of SIMD <b>parallel</b> <b>processors</b> and an embedded microprocessor unit. The multiple-levels of SIMD <b>parallel</b> <b>processors</b> consist of an array processor of SIMD processing elements(PEs) and {{a column of}} SIMD row processors(RPs). The PE array and RPs have an O(NxN) parallelism and an O(N) parallelism, respectively. The PE array, RPs and MPU can execute low-, mid- and high-level image processing algorithms, respectively. Prototype chips are fabricated using the 0. 18 μm CMOS process. Applications including target tracking, pattern extraction and image recognition are demonstrated. ? 2011 IEEE...|$|R
50|$|Allocating Independent Substaks on <b>Parallel</b> <b>Processors,</b> IEEE Trans. Softw. Eng., 11(10):1001-1016, 1985. With Clyde Kruskal.|$|R
50|$|STARAN {{might be}} the first commercially {{available}} computer designed around an associative memory. The STARAN computer was designed and built by Goodyear Aerospace Corporation. It is a Content Addressable <b>Parallel</b> <b>Processor</b> (CAPP), a type of <b>parallel</b> <b>processor</b> which uses content addressable memory. STARAN is a single instruction, multiple data array processor with a 4x256 1-bit processing element (PE) computer. The STARAN machines became available in 1972.|$|R
50|$|Xetal is {{the name}} of a family of non {{commercial}} massively <b>parallel</b> <b>processors</b> developed within Philips Research..|$|R
50|$|A Content Addressable <b>Parallel</b> <b>Processor</b> (CAPP) {{is a type}} of <b>parallel</b> <b>processor</b> {{which uses}} content-addressing memory (CAM) principles. CAPPs are {{intended}} for bulk computation. The syntactic structure of their computing algorithm are simple, whereas the number of concurrent processes may be very large, only limited by the number of locations in the CAM. The best-known CAPP may be STARAN, completed in 1972; several similar systems were later built in other countries.|$|R
40|$|PARCS is a {{declarative}} parallel constraint {{logic programming}} (CLP) language designed for efficient execution on distributed memory massively <b>parallel</b> <b>processors</b> (MPPs). As a language model, PARCS offers efficient parallel execution control via priority-based goal and constraint execution, powerful pruning mechanisms, and implicit OR-parallelism. Its MPPoriented implementation techniques include efficient load balancing, branching method and static compile-time optimization {{techniques such as}} mode-specific code generation, static ordering of deterministic constraints and determinacy analysis. Actual experiments have shown good speedups {{with respect to the}} number of processors for various kinds of programs, such as N-queens and paraffin problem. Keywords: constraint logic programming, massively <b>parallel</b> <b>processors,</b> load balancing, compilation techniques 1 Introduction Although massively <b>parallel</b> <b>processors</b> (MPPs) are available from various vendors these days, it is often cumbersome t [...] ...|$|R
50|$|In computers, XC is a {{programming}} language for real-time embedded <b>parallel</b> <b>processors,</b> targeted at the XMOS XCore processor architecture.|$|R
5000|$|Geometric-Arithmetic <b>Parallel</b> <b>Processor,</b> from Martin Marietta, {{starting}} in 1981, continued at Lockheed Martin, then at Teranex and Silicon Optix ...|$|R
40|$|In {{this article}} we present {{techniques}} to reduce power consumption in <b>parallel</b> <b>processor</b> arrays. The reduction is substantially reached by avoiding non-essential operations, i. e. operations that hardly contribute to the convergence of the considered algorithm. The presented approach is clarified {{on the basis of}} a parallel implementation of a Jacobi-like real eigenvalue decomposition (EVD) but should be transferable to other algorithms that are suited to be implemented on a <b>parallel</b> <b>processor</b> array...|$|R
50|$|Often N serial {{processors}} {{will take}} less FPGA area {{and have a}} higher total performance than a single N-bit <b>parallel</b> <b>processor.</b>|$|R
40|$|The {{advanced}} {{signal processing}} systems of today require extreme data throughput and low power consumption. The {{only way to}} accomplish this is to use <b>parallel</b> <b>processor</b> architecture. The aim of this thesis was {{to evaluate the use}} of <b>parallel</b> <b>processor</b> architecture in baseband signal processing. This has been done by implementing three demanding algorithms in LTE on Ambric Am 2000 family Massively <b>Parallel</b> <b>Processor</b> Array (MPPA). The Ambric chip is evaluated in terms of computational performance, efficiency of the development tools, algorithm and I/O mapping. Implementations of Matrix Multiplication, FFT and Block Interleaver were performed. The implementation of algorithms shows that high level of parallelism can be achieved in MPPA especially on complex algorithms like FFT and Matrix multiplication. Different mappings of the algorithms are compared to see which best fit the architecture...|$|R
40|$|The {{development}} of numerical methods and software tools for <b>parallel</b> <b>processors</b> can be aided {{through the use}} of a hardware test-bed. The test-bed architecture must be flexible enough to support investigations into architecture-algorithm interactions. One way to implement a test-bed is to use a commercial <b>parallel</b> <b>processor.</b> Unfortunately, most commercial <b>parallel</b> <b>processors</b> are fixed in their interconnection and/or processor architecture. In this paper, we describe a modified n cube architecture, called the hypercluster, which is a superset of many other processor and interconnection architectures. The hypercluster is intended to support research into parallel processing of computational fluid and structural mechanics problems which may require a number of different architectural configurations. An example of how a typical partial differential equation solution algorithm maps on to the hypercluster is given...|$|R
5000|$|It is {{the core}} {{operation}} in the Jacobi eigenvalue algorithm, which is numerically stable and well-suited to implementation on <b>parallel</b> <b>processors</b> [...]|$|R
5000|$|When a [...] constructis executed, each successiveiteration is {{performed}} {{in order and}} one after the other—an impediment to optimizationon a <b>parallel</b> <b>processor.</b>|$|R
40|$|With the {{transition}} from singlecore to multicore processors essentially complete, virtually all commodity CPUs are now <b>parallel</b> <b>processors.</b> Increasing parallelism, rather than increasing clock rate, has become the primary engine of processor performance growth, and this trend is likely to continue. This raises many important questions about how to productively develop efficient parallel programs that will scale well across increasingly <b>parallel</b> <b>processors.</b> Modern graphics processing units (GPUs) {{have been at the}} leading edge of increasing chip-level parallelism for som...|$|R
40|$|Abstract [...] An idea {{of using}} {{analogue}} microprocessors as processing elements in an SIMD array is presented. The resulting system retains some {{of the advantages of}} the analogue signal processing paradigm, while being a fully programmable general-purpose massively <b>parallel</b> <b>processor</b> array. The implementation of an analogue processing element as a switched-current circuit is discussed. Low-level image processing application examples are presented. Index terms [...] SIMD, massively <b>parallel</b> <b>processors,</b> image processing, analogue signal processing, switched-current circuits I...|$|R
5000|$|Loy, Gareth, “On the {{scheduling}} of <b>parallel</b> <b>processors</b> executing synchronously,” Proceedings of the International Computer Music Conference, San Francisco: International Computer Music Association, 1987.|$|R
40|$|For {{simulating}} the strong-strong beam-beam effect, using Particle-In-Cell codes {{has become}} one of the methods of choice. While the two-dimensional problem is readily treatable using PC-class machines, the three-dimensional problem, i. e., a problem encompassing hourglass and phase-averaging effects, requires the use of <b>parallel</b> <b>processors.</b> In this paper, we introduce a strong-strong code NIMZOVICH, which was specifically designed for <b>parallel</b> <b>processors</b> and which is optimally used for many bunches and parasitic crossings. We describe the parallelization scheme and give some benchmarking results...|$|R
40|$|As {{part of the}} Joint CSCS [...] ETH/NEC Collaboration in Parallel Processing, we are {{currently}} developing an integrated tool environment consisting of an extended High Performance Fortran (HPF) compiler, a parallel performance monitor and analyzer, and a parallel debugger for distributed memory <b>parallel</b> <b>processors</b> (DMPPs). The environment is implemented {{on top of a}} subset of the emerging Message Passing Interface standard (MPI), running on several platforms, among others a NEC Cenju- 2 DMPP. We develop a sequence of prototypes, which are continuously evaluated by a team of application developers. This document describes the first prototype currently installed on our systems. Introduction Advances in VLSI technology continue to ease replication and interconnection of many identical processing elements, and as the price/performance ratio of <b>parallel</b> <b>processors</b> decreases, they become increasingly attractive to numerical scientists. In MIMD distributed memory <b>parallel</b> <b>processors</b> (DMPP [...] ...|$|R
40|$|The {{description}} `object-oriented' {{may apply}} to both programming languages and operating systems. However, creating an interface between an object-oriented programming language and an object-oriented operating {{system is not}} necessarily a straightforward task. Chrysalis++ is a C++ interface to the Chrysalis operating system for the BBN Butterfly <b>Parallel</b> <b>Processor.</b> The development of Chrysalis++ highlights strengths and weaknesses of C++ and the problems of adapting a language based on a conventional memory model to a shared memory <b>parallel</b> <b>processor.</b> This work was supported by United States Army Engineering Topographic Laboratories research contract number DACA 76 - 85 -C- 0001 and National Science Foundation Coordinated Experimental Research grant number DCR- 8320136. We thank the Xerox Corporation University Grants Program for providing equipment used in the preparation of this paper. 1 Introduction The Chrysalis operating system for the Butterfly <b>Parallel</b> <b>Processor</b> presents an objectori [...] ...|$|R
40|$|The {{impact of}} a five year space mission {{environment}} on fault-tolerant <b>parallel</b> <b>processor</b> architectures is examined. The target application is a Strategic Defense Initiative (SDI) satellite requiring 256 <b>parallel</b> <b>processors</b> to provide the computation throughput. The reliability requirements are that the system still be operational after five years with. 99 probability and that the probability of system failure during one-half hour of full operation be less than 10 (- 7). The fault tolerance features an architecture must possess to meet these reliability requirements are presented, many potential architectures are briefly evaluated, and one candidate architecture, the Charles Stark Draper Laboratory's Fault-Tolerant <b>Parallel</b> <b>Processor</b> (FTPP) is evaluated in detail. A methodology for designing a preliminary system configuration to meet the reliability and performance requirements of the mission is then presented and demonstrated by designing an FTPP configuration...|$|R
40|$|Abstract-About {{a decade}} ago, a bit-serial {{parallel}} processing system STARAN ® 1 was developed. It used standard integrated circuits {{that were available}} at that time. Now, with the availability of VLSI, a much greater processing capability can be packed in a unit volume. This has Jed to the recent development of two bit-serial parallel processing systems: an airborne associative processor and a ground based massively <b>parallel</b> <b>processor.</b> The airborne associative processor has about the same processing capability as a three cabinet STARA N system in a volume less than 0. 5 cubic feet. The power required and weight 'are also reduced dramatically for the airborne environment. The massively <b>parallel</b> <b>processor</b> has about 100 times tbe processing capability as tb ~ STA RAN system in about the same volume. Floating point speeds are better than 100 MOPS (million operations per second). Integer arithmetic speeds depend on operand lengths-for 16 bit integers the speed is better than 3000 MOPS for addition and 450 MOPS for multiplication. After presenting the basic rationale for bit-serial <b>parallel</b> <b>processors,</b> the organizations of the two recent systems are shown {{and some of their}} applications are outlined [...] Index Terms-Airborne processors, bit-serial processors, custom VI. SI chips, image processing, multidimensional access, <b>parallel</b> <b>processors,</b> radar processing. I...|$|R
40|$|<b>Parallel</b> <b>processors</b> {{reduce the}} {{communication}} overhead {{problem with the}} employment of some form of global communication network. This network however, imposes restrictions on the scaleability and technological evolution of the <b>parallel</b> <b>processor.</b> In this paper a novel architecture called PiSMA is proposed, which consists of a basic substrate, without a network, providing neighborhood connectivity for local communication. The same architecture can be upgraded, {{by the addition of}} a global network. The simulated performance of such an upgradeable parallel computer is measured {{with a wide range of}} parallel applications and fault tolerant issues are discussed. It is found that the substrate connectivity can provide an effective fault tolerant parallel computer for a large class of applications, while the incorporation of a global network broadens the spectrum of applications that can be executed efficiently by the PiSMA architecture. 1. Introduction Tightly coupled <b>parallel</b> <b>processors</b> usua [...] ...|$|R
40|$|JUMP- 1 is {{currently}} under development by seven Japanese universities to establish techniques of an efficient distributed shared memory on a massively <b>parallel</b> <b>processor.</b> It provides a memory coherency control scheme called the hierarchical bit-map directory to achieve cost effective and high performance {{management of the}} cache memory. Messages for maintaining cache coherency are transferred through a fat tree on the RDT(Recursive Diagonal Torus) interconnection network. In this report, we discuss on the scheme and examine its performance. The configuration of the RDT router chip is also discussed. 1 Introduction JUMP- 1 is a massively <b>parallel</b> <b>processor</b> prototype developed by collaboration between 7 Japanese universities[4]. The major goal of this project is to establish techniques for building an efficient distributed shared memory on a massively <b>parallel</b> <b>processor.</b> For this purpose, a sophisticated methodology called Strategic Memory System (SMS) is proposed [11][4]. The cache direct [...] ...|$|R
40|$|The {{first result}} {{of this paper is}} a lower bound on mean {{response}} time, under very general workload assumptions, per class of multiprogrammed <b>parallel</b> <b>processor</b> allocation policies. Each class of parallel allocation policies is defined by the information structure of the policies. Each bound is derived from the mean response time of the optimal uniprocessor scheduling policy that uses (at least) the same workload information as the class of <b>parallel</b> <b>processor</b> allocation policies. The derivation of the bound also suggests how tighter bounds can be obtained for individual <b>parallel</b> <b>processor</b> allocation policies in some cases. The workload assumptions include general distributions of total job processing requirment, desired job parallelism, and inter-arrival times, general job execution rate functions (i. e., general speedup curves), and arbitrary dependencies among these workload variables. The second result is that for linear execution rates (i. e., perfect speedups) and for i. i. d. expone [...] ...|$|R
50|$|A related {{but more}} {{efficient}} sort algorithm is the Batcher odd-even mergesort, using compare-exchange operations and perfect-shuffle operations.Batcher's method is efficient on <b>parallel</b> <b>processors</b> with long-range connections.|$|R
5000|$|In more <b>parallel</b> <b>processor</b> designs, as the {{instruction}} cache latency grows longer and the fetch width grows wider, branch target extraction becomes a bottleneck. The recurrence is: ...|$|R
3000|$|... n− 1 of general-base {{processors}} {{have been}} obtained. Pilot elements addresses and matrix spans to locate their satellites are automatically generated for dispatching and sequencing the <b>parallel</b> <b>processors.</b>|$|R
