3|20|Public
5000|$|The ORSA should {{incorporate}} {{the information provided}} by the <b>actuarial</b> <b>function</b> on the validation of technical provisions; ...|$|E
40|$|The Black-Scholes {{option pricing}} formula from finance theory is {{consistent}} with the assumption that the market price of the underlying asset at any future date is lognormally distributed with time-dependent parameters and can be shown to be a special case of both a more general option model and a familiar <b>actuarial</b> <b>function</b> used in excess of loss applications. This insight leads {{to an understanding of the}} similarity between options and certain insurance concepts. Because insurance and finance have developed separately, different paradigms are used by the practitioners in each field. When these paradigms are shared, a new perspective on risk management, product development, and pricing, especially of insurance and reinsurance, emerges...|$|E
40|$|The {{subject of}} the Master Thesis is risk and risk {{management}} within the new regulatory framework {{in the field of}} insurance Solvency II. The thesis especially deals with the Solvency II directive that has been used as an instrument for the insurance market regulation within the European Union since 1 st January 2016. Besides the risk classification, risk management and an example of its application in an insurance company, the thesis also mentions the global financial crisis (its causes and consequences) on the background of the banking sector. Another important part of the thesis describes the second pillar of the Solvency II directive whose integral parts, among others, are risk management along with its other functions, e. g. <b>actuarial</b> <b>function...</b>|$|E
40|$|<b>Actuarial</b> <b>functions</b> can be {{calculated}} by using mortality table and its approach by mortality law. One famous mortality law is Makeham mortality law. The mortality law approach towards the mortality table is applied because the result is continuous. The mortality law approach can explain any phenomenon which happens in a population. The discrepancies between data which is approached by data from Makeham mortality law and mortality table can affect the accuracy in estimating <b>actuarial</b> <b>functions</b> and <b>actuarial</b> present value of benefit. In this research, writer calculates the actuarial present value of benefit with constant interest rate, Vasicek and Cox-Ingersol-Ross (CIR) interest rate to anticipate the interest rate fluctuation. Writer uses data from United States mortality table period 1979 - 1981, 3 rd Indonesian Mortality Table (2011) for men, 3 rd Indonesian Mortality Table (2011) for women, and the approximation by Makeham mortality law. Writer intends to discuss the relation between age of the insured and stochastic interest rate parameters to the actuarial present value of benefit. Furthermore, writer compares the actuarial present value of benefits obtained from mortality tables and their approximation. It can be inferred that the actuarial present value of benefit is influenced by age of the insured and parameters {{from each of the}} stochastic interest rate...|$|R
40|$|The {{problem of}} {{modelling}} the joint distribution of survival {{times in a}} competing risks model, using copula functions is considered. In order to evaluate this joint distribution and the related overall survival function, a system of non-linear differential equations is solved, which relates the crude and net survival functions of the modelled competing risks, through the copula. A similar approach to modelling dependent multiple decrements was applied by Carriere (1994) who used a Gaussian copula applied to an incomplete double decrement model which {{makes it difficult to}} calculate any <b>actuarial</b> <b>functions</b> and draw relevant conclusions. Here, we extend this methodology by studying the effect of complete and partial elimination of up to four competing risks on the overall survival function, the life expectancy and life annuity values. We further investigate how different choices of the copula function affect the resulting joint distribution of survival times and in particular the <b>actuarial</b> <b>functions</b> which are of importance in pricing life insurance and annuity products. For illustrative purposes, we have used a real data set and used extrapolation to prepare a complete multiple decrement model up to age 120. Extensive numerical results illustrate the sensitivity of the model with respect to the choice of copula and its parameter(s) ...|$|R
40|$|This paper brings {{analysis}} {{of the impact of}} a ban on the use of gender in insurance, with special stress on pension annuity, according to the requirements of the European Court of Justice. The paper brings a state-of-theart overview of known and extended <b>actuarial</b> <b>functions</b> which relate to modeling of a premium of endowment, term life insurance and pension annuity. Moreover, the amounts of the pension annuities payable thly per year in a model of the third pillar pension are modeled and analyzed for different interest rates using life tables for both genders and unisex...|$|R
40|$|In the {{collective}} risk model {{we use the}} objective Bayesian approach to calculate predictive aggregate claims distributions. Very often this {{is equivalent to the}} profile predictive likelihood approach, but the former is more straightforward to apply, and accordingly we use it as a pragmatic device. We compare the predictive distributions with fitted distributions which take no account of parameter uncertainty and show that <b>actuarial</b> <b>functions</b> such as premiums can be substantially understated if parameter uncertainty is ignored. We illustrate the situation when the moments of the predictive individual claim amount distribution do not exist and we discuss ways of applying such distributions to insurance problems...|$|R
40|$|Methods of {{analyzing}} mortality data {{to determine whether}} a particular life table should be used to calculate <b>actuarial</b> <b>functions,</b> and to determine whether two mortality experiences could be merged are outlined. Ways in which hypotheses might be established and tested, in ftccord with specific practical requirements are illustrated. The hypotheses are formulated in terms of sets of annuity values or linear functions of annuity values [...] Methods of approximating these it pJI. functions are used to illustrate that hypotheses about them involving annuity values at many ages may be replaced by hypotheses involving values at only a few ages. The asymptotic distributions of estimates of these functions are derived, and used in constructing test procedures...|$|R
40|$|Our paper {{attempts}} {{to identify the}} types of data nee 3 ed to estimate tradeoffs between wages and fringe benefits (such as pensions); it also explores the usefulness for this estimation of one particular employer- based data set collected by gay Associates. We stress three things: first, that employer-based data sets are required. Second, because pensions and many other fringe benefits are <b>actuarial</b> <b>functions</b> of wages or salaries, these technical relationships must be accounted for in estimation. Third, {{to take account of}} unobservable heterogeneity of employees across employers, one must use econometric methods that control for these unobservable variables. The paper concludes with a discussion of our {{attempts to}} estimate the tradeoff between wages and fringe benefits using a unique database for 200 establishments that contains information on wages and actuarial valuations of employer costs of fringe benefits at three different job levels. ...|$|R
2500|$|The Penrose {{report also}} states that [...] "the DTI {{insurance}} division was ill equipped {{to participate in}} the regulatory process. It had inadequate staff and those involved at line supervisor level in particular were not qualified to make any significant contribution to the process. For all practical purposes, scrutiny of the <b>actuarial</b> <b>functioning</b> of life offices was in the hands of GAD until the reorganisation under FSA was in place". More evidence also strongly suggests that the regulator adopted a conscious and deliberate [...] "hands-off" [...] approach with regard to the Equitable case. If this were proven to be the case, it would constitute a breach of the regulator's obligation to ensure the respect of PRE and therefore a breach of the letter and aim of Article 10 of the 3LD. Both the Baird and Penrose reports contain criticisms of the regulator's lack of a [...] "pro-active approach".|$|R
40|$|When the multi-year {{trend of}} declining auto {{frequency}} finally turns, {{will you be}} prepared {{to be the first}} to recommend that your company raise rates? [...] 3 Kollar Voted President-Elect; Carlson to Become CAS President— Receiving 979 votes, John J. Kollar has been has been voted in as president-elect. Kollar currently serves on the CAS Executive Council as the Vice President-ERM and his governance experience includes a term on the CAS Board (1998 to 2001) and service on numerous CAS committees relating to ERM, education, and long-range planning [...] . 5 Recent Developments in the Treatment of Property and Casualty Insurance Contracts Under Fair Value Accounting—“The times they are a-changing, ” sang Bob Dylan, and they sure are in the financial reporting world for insurance liabilities [...] 14 The CAS ERM Vision—Uncertainty is common to all <b>actuarial</b> <b>functions.</b> Ideally, these functions are well coordinated so that risk is treated consistently and in an integrated fashion across your company. That is the objective of Enterprise Risk Management (ERM) [...] . 22 A Round Table Discussion on the AAA Qualifications an...|$|R
40|$|Abstract. The paper {{takes into}} account the problem of the {{solvency}} of an insurance company referring to the reserve valuation. Considering a portfolio of deferred life annuities the paper presents a model to front this kind of problem based on the consideration of the insurance company portfolio as a block of business, that is as a cash flow of assets and liabilities. In this order of ideas, a stochastic interest environment is assumed; in particular the force of interest is supposed to be modelled by an Ornstein – Uhlenbeck process, and by means of the traditional <b>actuarial</b> <b>functions</b> the reserve is evaluated by calculating the expected value of the portfolio random loss and its variability computing the variance. Then the impact of the longevity risk on the obtained results is studied introducing projected mortality tables. Numerical applications are discussed. Key words: Life annuity portfolio, Ornstein-Uhlenbeck process, cash flow analysis, insolvency risk, investment risk, longevity risk. Main References: [1] Coppola M. (2002) : Stochastic Solvency Valuation for a life Annuity Portfolio...|$|R
40|$|The actuar {{project is}} a package of <b>Actuarial</b> Science <b>functions</b> for the R {{statistical}} system. The project was launched in 2005 and the package is available on CRAN (Comprehensive R Archive Network) since February 2006. The current version of the package contains functions {{for use in the}} fields of risk theory, loss distributions and credibility theory. This paper presents in detail but in non technical terms the most recent version of the package...|$|R
40|$|The {{passage and}} {{implementation}} of the Sarbanes-Oxley Act of 2002 was the most significant landmark legislation in securities regulation and corporate governance in the US since the SEC Act of 1934. In particular, Section 404 of the act requiring management assessment and assertion on the effectiveness of internal controls along with the requirement that the auditor attest to the assertion, has greatly impacted actuarial work processes for many actuaries. This paper discusses the implications of the Act for actuaries based on analysis of <b>actuarial</b> <b>functions</b> within insurance companies. Also discussed are the observed impacts within the industry to date. Based on these observations and experiences, an overview of a typical internal control framework is introduced. Impacts on Actuaries working in financial reporting are far reaching, as well are the risks created by the Act. On the other hand well designed and operated controls may serve to reinforce the professionalism of the Actuarial work product, reducing certain risks for Actuaries. In addition, there is an unrealized potential for the increased focus on controls and documentation to strengthen the integrity of results reported by insurers, leading to increased stability in loss reserve estimates...|$|R
40|$|In {{learning}} mathematical economics, {{the calculation}} of life insurance premiums is a matter concerning {{the application of a}} combination of compound interest, probability, differential and integral.  Life insurance with multilife concept is the one of ap- plied in <b>actuarial</b> mathematics.  A <b>functions,</b> in the <b>actuarial</b> cal- culation, related to death sequence in multilife concept is called as contingent function.    Usage that function in calculation of insurance premium will assist the insurer in giving the benet precisely. Contingent probabilities are resulted by multiplication be- tween the force of mortality of life in the last sequence of death which have been determined and probabilities of life all family member in multilife status. Insurance formulation is obtained by mutiplying this probabilities with vt discount factor and they are integrated by using the assumption of a uniform distribution of death throughout the year of age...|$|R
40|$|Life {{tables are}} {{traditionally}} built with linear assumptions {{for the survival}} curve. Here, considering that survivors can remain {{at the end of}} the observation period, the author shows that non linear modeling is more appropriate. With data on cervix uteri cancer, e 0 ≈ 12. 5 years with standard error ≈ 2. 8 years with infinite time horizon, but e 0 ≈ 6. 0 years with standard error ≈ 0. 1 year in interval with finite time horizon [0, 12 years]. The average hazard function is introduced to estimate the life expectancy, and the actuarial estimate of the hazard function is showed to under-estimate the true hazard values under the exponential distribution. Finally, a sensitivity analysis of the probabilities of death on the estimation of life expectancy completes the study. Life expectancy, Taylor expansion, Hazard <b>function,</b> <b>Actuarial</b> estimate,...|$|R
40|$|The {{development}} {{and management of}} data resources that support property/casualty actuarial work are very challenging undertakings, especially in a high-volume transactional processing environment. In order to equip actuaries with the data resources necessary to excel {{in the performance of}} their <b>functions,</b> an <b>Actuarial</b> Data Management (ADM) support team is needed. It serves as a proactive, added-value conduit of business data and specialized technical support to an actuarial staff. This paper examines the evolution of the <b>actuarial</b> data management <b>function</b> in the context of end user computing, and highlights the key roles and processes that comprise an effective data management operation in a modern property/casualty actuarial department. The paper also includes a case study that describes the development of the data management <b>function</b> in the <b>Actuarial</b> Department of Motors Insurance Corporation, a member of the GMAC Insurance Group, located in Southfield, Michigan...|$|R
40|$|Since the 1960 s simple {{inexpensive}} cold lactated Ringers with additives {{has been}} used for short-term cold preservation of kidneys from living donors. We performed 266 living donor kidney transplantations from January 22, 2003 to October 30, 2006. Donor allografts were recovered laparoscopically and flushed with cold heparin, lactated Ringer's and procaine (HeLP) solution. Warm and cold ischemic times were typically < 45 min and < 90 min, respectively. The mean follow up was 21. 6 ± 12. 2 months. There was no delayed graft <b>function.</b> <b>Actuarial</b> 1 -year patient and graft survival were 98. 6 % and 98. 1 %, respectively. The creatinine at 1 year was 1. 46 ± 0. 51 mg/dL. The cumulative incidences of acute cellular rejection at 6, 12, 18, and 24 months were 3. 0 %, 7. 1 %, 10. 2 %, and 11. 7 %. There were no identifiable side effects attributed to the HeLP solution. This study documents the effectiveness of cold HeLP as a flushing and short-term preservation fluid for living donor kidney transplantation with excellent results and significant cost benefit because of its low cost. © 2007 Lippincott Williams & Wilkins, Inc...|$|R
40|$|Of 304 {{children}} who received primary renal transplants at the University of Minnesota between January 1, 1968, and December 31, 1985, 48 (16 %) {{were under the}} age of 24 months, 60 (20 %) were 2 - 5 years old, and 196 (64 %) were 6 - 17 years old at transplantation. Currently, 254 (84 %) are alive at 2 months to 18 years following their first transplants, 77 % with functioning grafts (188 first, 45 retransplants) and 7 % on dialysis. Overall, patient and graft survival were not significantly different from the primary graft outcome of nondiabetic adults. The <b>actuarial</b> primary graft <b>function</b> rates at 1, 5, and 10 years were 100, 100, and 90 % in 16 HLA-identical sibling kidneys; 84, 64, and 52 % in 210 mismatched related kidneys; and 72, 54, and 47 % in 78 cadaver kidneys (p less than 0. 002). The 1 -year patient survival and primary graft function rates in 44 mismatched related recipients {{under the age of}} 24 months were 92 and 88 %. The use of deliberate, pretransplant random blood transfusion since 1979 has been associated with a decreased rejection rate. Primary graft function of mismatched related kidneys in children receiving standard immunosuppression has significantly improved from 78 % at 1 year in the pretransfusion era to 91 % (p less than 0. 01) in the transfusion era. The overall primary cadaver graft function rate, however, did not improve in the transfusion era. Whether cyclosporine use will improve the cadaver renal allograft function in very young recipients remains to be established. However, with the use of related donors, even very young children can be transplanted safely and with excellent results...|$|R
40|$|BACKGROUND: Drug-orientated, {{pilot study}} was {{conducted}} to estimate the activity of gemcitabine on treatment of head and neck and lung planocellular carcinoma in combination with either radiotherapy or chemotherapy. METHODS There were 22 patients treated with gemcitabine for planocellular carcinoma of head and neck (9 patients) and lung (13 patients). Combined gemcitabine-radiotherapy was applied in 10 patients while gemcitabine-chemotherapy in 12 patients. Eligible and evaluable patients (22) were with either locally advanced (14 patients) or metastatic (8 patients) stage of the disease. In gemcitabine-radiotherapy group, gemcitabine was given IV, 1000 mg/ m 2, on day 1, 8, and 15 during the radiotherapy course as radiopotentiator (65 Gy in 32 fractions for head and neck, and 55 Gy in 20 fractions, split course one month for lung cancer patients). In gemcitabine-chemotherapy group the same dose of gemcitabine was given (4 -week schedule) in combination with platinum based cytotoxic drugs. We analyzed response rate and toxicity. RESULTS: Among patients treated for head and neck planocellular carcinoma, there were 67 % complete responders while there was 15 % complete responders treated for lung cancer. Also, 80 % of patients treated in gemcitabine-radiotherapy group had complete response while 50 % of those treated in gemc- itabine-chemotherapy group. <b>Actuarial</b> survival as <b>function</b> of tumor control was 52 % for lung and 88 % for head and neck cancer 12 months after the initiation of treatment. In gemcitabine-radiotherapy group of patients treated for head and neck carcinoma, the radiation mucositis grade III was observed in 80 % while in gemcitabine - chemotherapy group of patients the most common side effect (60 % of patients) was neutropenia grade II (40 %) /III (20 %). CONCLUSION: There was no statistically significant difference regarding response rate between two groups of patients (head and neck vs. lung cancer, and gemcitabine- radiotherapy vs. gemcitabine - chemotherapy). However, better clinical results were achieved for head and neck cancer patients, particularly in gemcitabine - radiotherapy group but with significant toxicity due to high gemcitabine dose...|$|R
40|$|The {{consideration}} of multivariable analysis derived {{risk factors for}} post transplant events {{in the evaluation of}} potential cardiac transplant recipients and donors may serve to improve mortality and morbidity after cardiac transplantation. However, the application of this information is elusive due to the difficulty of applying all identified recipient and donor related risk factors, expressed as dichotomous and continuous variables, in the assessment of the risk of post transplant events for a single patient. The CTRD has reported multivariable risk factor analyses in the hazard function domain for post cardiac transplant events from all primary transplants from 30 transplant centers with 4. 5 year follow-up. Variables analyzed in theses reports included multiple recipient and donor demographic information, recipient and donor mismatch variables, pre-transplant recipient and donor clinical variables, hemodynamic variables, surgical variables, initial immunosuppression variables, and post transplant event variables. Specific risk factors for post transplant events were identified and multivariable equations were derived and have been published. In order to facilitate the application of this information, a computer program was developed to allow the calculation of the estimated time-related risk of post transplant events including death, time to first infection, time to first rejection, and number of rejections in the first year. One enters the values for specified variables into the program. The program then calculates (utilizing solutions to the multivariable equations) and then displays the time-related <b>actuarial</b> and hazard <b>function</b> curves, with standard errors, for each event in graphical and tabular formats. Patient data may be stored, and data output may be printed. Different patients or clinical situations for the same patient may be compared on screen. The program is written in Microsoft C, version 6, and runs in PC-DOS. Computer requirements include 640 kb RAM, VGA graphics, and a floating point processor. SummaryUtilizing this software package, a clinician may compute and print an individual patient's time related risk profiles for post transplant death, infection, and allograft rejection. This information may be used for pre-transplant recipient education and selection, and donor-recipient matching prior to transplantation...|$|R
40|$|In {{this paper}} we {{concentrate}} on the estimation of loss functions using nonparametric methods. We focus on the parametric transformation approach to kernel smoothing introduced by Wand, Marron and Ruppert (1991) and compare it with the standard kernel estimator and the multiplicative bias correction method (Hjort and Glad, 1995 and Jones, Linton and Nielsen, 1995). We advocate in this paper that the transformation method behaves excellently {{when it comes to}} estimating actuarial and financial loss functions. It is a very suitable approach when estimating functions with a lognormal type of shape. Loss functions have typically one mode for the low loss values and then a long heavy tail (Klugman, Panjer and Willmot, 1998),. We show by a simulation study that the method is able to estimate all three possible kind of tails, as defined in Embrechts, Kl*ppelberg and Mikosch (1999), namely the Fr’chet type, the Weibull type and the Gumbel type. This makes this method extremely powerful specially for actuaries at all levels, i. e. the non-life actuary calculating the risk of his auto claims, when a life actuary consider the risk of a group life insurance, or when these actuaries consider the relevant price for a reinsurance contract, where the reinsurer takes over the risk corresponding to the tail of the distribution. Besides, when the actuary calculates the total risk of a portfolio, perhaps using the famous recursion formula of Panjer (1981) or some more recent generalisation of this formula, then the loss function of the individual claims is needed. We show that our version of the transformation principle of Wand, Marron and Ruppert (1991) is able to estimate the risk of a heavy-tailed distribution beyond the data. A heavy-tailed distribution is here of Pareto shape. When we use our non- parametric estimator to fit the closest possible Pareto distribution in the tail we see, that at least for the estimator we tried out in our simulations, then our method performs much better than the Hill estimator (Hill, 1975), that is widely used in actuarial science and finance, see Embrechts, Kl*ppelberg and Mikosch (1997, p. 330) and Danielson and Vries (1997). On top of this, our estimation technique does not have to bother about where the tail begins such as the Hill estimator has to. This is often a difficult question and fatal errors leading to massive economic losses may happen. The beginning of the tail can be estimated by a methodology that can be compared to the trade-off between bias and variance in kernel density estimation, but the only (to our knowledge) published method on this (see Hall, 1990) involves sub-bootstrapping, that is hard to understand and implement for the practitioner (see, Danielson and Vries, 1997). Recently another method has appeared, namely the semiparametric robust estimator of Feuerverger and Hall (1999). Compared to this estimator, our proposal has the advantage of being directly linked to the non-parametrically estimated loss function. The connection between the loss-function and the tail index is therefore immediate in our estimator of the tail index. We consider this to be a considerable competitive advantage of our method {{when it comes to the}} application in the fields of actuarial and financial loss functions. We believe that our method is a big jump forward for the practitioner, whether it is an actuary or a financial analyst, to get a quick and easily understood estimator of all the things he needs to know about his loss distribution, using a moderate computational effort. The organisation of the paper is as follows: in section one there is an introduction to the estimation of loss functions. Section two presents the theoretical properties when using a fixed transformation in kernel density estimation, and afterwards, section three explains the transformation algorithm that is proposed in this paper. Section four is devoted to the applications, two examples of <b>actuarial</b> loss <b>function</b> estimation are presented. They analyse automobile claim data. Finally, section five describes the results of a large simulation study that shows the performance of the different kernel methods when estimating loss function curves and their tail index, i. e. when describing their behaviour in the tail. ...|$|R

