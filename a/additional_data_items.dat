5|10000|Public
40|$|Abstract Background Radar {{has been}} used for decades to study {{movement}} of insects, birds and bats. In spite of this, there are few readily available software tools for the acquisition, storage and processing of such data. Program radR was developed to solve this problem. Results Program radR is an open source software tool for the acquisition, storage and analysis of data from marine radars operating in surveillance mode. radR takes time series data with a two-dimensional spatial component as input from some source (typically a radar digitizing card) and extracts and retains information of biological relevance (i. e. moving targets). Low-level data processing is implemented in "C" code, but user-defined functions written in the "R" statistical programming language can be called at pre-defined steps in the calculations. Output data formats are designed to allow for future inclusion of <b>additional</b> <b>data</b> <b>items</b> without requiring change to C code. Two brands of radar digitizing card are currently supported as data sources. We also provide an overview of the basic considerations of setting up and running a biological radar study. Conclusions Program radR provides a convenient, open source platform for the acquisition and analysis of radar data of biological targets. </p...|$|E
40|$|Abstract Background Functioning of {{the immune}} system {{requires}} the coordinated expression and action of many genes and proteins. With the emergence of high-throughput technologies, a great amount of molecular data is available for the genes and proteins {{of the immune}} system. However, these data are scattered into several databases and literature and therefore integration is needed. Description The Immunome Knowledge Base (IKB) is a dedicated resource for immunological information. We identified and collected genes that are essential for the immunome. Nucleotide and protein sequences, as well as information about the related pseudogenes are available for 893 human essential immunome genes. To allow the study of the evolution {{of the immune system}}, data for the orthologs of human genes was collected. In addition to the human immunome, ortholog groups of 1811 metazoan immunity genes are available with information about the evidence of their immunity function. IKB combines three previous databases and several <b>additional</b> <b>data</b> <b>items</b> in an integrated system. Conclusion IKB provides in one single service access to several databases and resources and contains plenty of new data about immune system. The most recent addition is variation data on genomic, transcriptomic and proteomic levels for all the immunome genes and proteins. In the future, more data will be added on the function of these genes. The service has a free and public web interface. </p...|$|E
40|$|Aims: To explore {{prevalence}} and related factors for urinary incontinence (UI) among the oldest old institutionalized Chinese men in Taiwan. Methods: All residents living in Banciao Veterans Care Home were invited for study. UI {{was defined as}} urinary leakage at least once weekly. <b>Additional</b> <b>data</b> <b>items</b> from the Minimum Data Set (MDS Nursing Home Chinese Version 2. 1) were used to explore impact associated with physical function, cognitive status {{and quality of life}} (social engagement, SocE). Depressive symptoms were screened by the Short Form Geriatric Depression Scale. Results: Data from 594 male residents (mean age: 80. 9 +/- 5. 3 years) were analyzed. Among all study subjects, 92. 8 % were functionally independent, 20. 4 % had certain cognitive impairment and 8. 2 % had depressive symptoms. The prevalence of UI in the Banciao Veterans Care Home was 10. 1 %. Compared with residents without UI, subjects with UI had poorer physical function, cognitive status, and more depressive symptoms. The mean SocE score was 1. 5 +/- 1. 3, and was similar between UI (+) and UI (-) subjects (1. 4 +/- 1. 2 vs. 1. 6 +/- 1. 3, P = 0. 411). By multivariate logistic regression, poorer physical functional status, cognitive impairment and depressive symptoms were independent risk factors for UI (P < 0. 05). Conclusions: Poorer physical function, poorer cognitive status and depressive symptoms were all statistically significant independent risk factors for UI. However, SocE score (proxy indicator of quality of life) did not differ between subjects with and without UI. Further investigations are needed to evaluate the impact of UI on quality of life among oldest old institutionalized Chinese men in Taiwan. Neurourol. Urodynain. 28 : 335 - 338, 2009. (C) 2008 Wiley-Liss, Inc...|$|E
40|$|This paper gives {{notification}} of changes {{to be included}} in the NHS Data Dictionary Version. 2 and the NHS CDS Manual as appropriate. These will be consolidated into the publications in due course. Summary of Changes: This DSCN introduces <b>data</b> <b>items</b> into the NHS Data Dictionary Version. 2, to support Version 1. 3 _ISB of the National Cancer <b>data</b> set for <b>additional</b> site specific <b>data</b> <b>items.</b> These cover seven types of cancer: Breast, Colorectal, Lung, Head & Neck...|$|R
40|$|This thesis {{discusses}} finite difference approximations, Hermite interpolation and Quasi-Uniform Spectral Schemes. In {{the discussion}} of finite difference approximations, an explicit algebraic condition on the grid points is given that explains when a boosted order of accuracy occurs. Moreover, an efficient method for the computation of the finite difference weights is provided. An elegant derivation of the barycentric Hermite interpolant is shown which leads to an alternative derivation of a known method in the literature for the computation of the barycentric weights. This new approach {{to the construction of}} the barycentric Hermite interpolant leads to a novel and efficient method for updating the barycentric weights when an <b>additional</b> <b>data</b> <b>item</b> is added. Three different Quasi-Uniform Spectral Schemes are discussed in this thesis. The grids used for interpolation are constructed using the Elliptic, Kosloff-Tal-Ezer (KT) and Theta mapping. The interpolant is a mapped cosine interpolant. Practical advice on how to pick the optimal map parameter is provided. A comparison of the approximation errors of the mapped cosine interpolants for a test function illustrates that the three mappings have no significant difference in terms of their approximation error...|$|R
30|$|The second {{approach}} {{is based on}} the signature chaining technique [40, 41, 43, 46]. Mykletun et al. [40] investigated the notion of signature aggregation which allows one to combine multiple signatures into a single one, thereby reducing verification overhead for search results. However, their mechanism ensures correctness for search results and does not provide completeness guarantee. Later, Narasimha et al. [43] addressed completeness by integrating signature aggregation and chaining techniques. Specifically, the client generates a signature for each <b>data</b> <b>item</b> containing all the immediate predecessors in different dimensions. Then, given a range query, their technique requires two boundary <b>data</b> <b>items</b> to be returned along with the target <b>data</b> <b>items.</b> The completeness of the search result can be verified using the chained signature. Pang et al. [46, 48] give two solutions to the completeness problem for static and dynamic outsourced database, respectively. In their solutions, all <b>data</b> <b>items</b> are assumed to be ordered with respect to certain searchable attributes, and the data owner creates a signature for each item that consists of information about the two neighboring items in the ordered sequence. Note that {{there is no need for}} <b>additional</b> boundary <b>data</b> <b>items.</b> Nevertheless, the case when noncontinuous regions are queried is intractable. Recently, Yuan and Yu [59] presented a new verifiable aggregation query scheme for outsourced databases. Specifically, each <b>data</b> <b>item</b> is assigned an authentication tag based on a polynomial, which can be used to check the integrity of query result for certain aggregation queries.|$|R
40|$|The Parliamentary Joint Committee on Intelligence and Security (PJCIS) today {{released}} its {{report on the}} government’s proposed metadata retention laws. The report makes a number of recommendations to revise and clarify the proposed bill, including: 	 	offering monetary compensation for service providers {{to cover the costs}} of implementing the scheme 	 	 	limiting the discretionary powers of the Attorney-General in making changes to the bill once it’s passed 	 	 	preventing the stored metadata from being used as a part of civil litigation and 	 	 	that more consideration is needed regarding how to protect press freedom and anonymous sources. 	 We asked a panel of experts to provide their thoughts on the implications of the report and its recommendations. Angela Daly, Research Fellow in Media and Communications Law at Swinburne University of Technology While the PJCIS’s recommendations are to be welcomed because they clarify the data retention scheme, build in some procdural safeguards and go some way to ensuring the data will be secure, serious problems remain with the scheme. The fact that all Australians 2 ̆ 7 data will be stored means this remains a mass surveillance programme which interferes with the privacy of millions of completely law-abiding people in a very intrusive and totally disproportionate way. The doubts that international experience cast over the effectiveness of a mass data retention scheme in achieving its stated purpose (to keep Australians safe against serious crime and terrorism) have not been addressed at all. We still do not know how much all of this will cost or precisely what data about Australian internet users will be retained, but it seems that a huge amount of money will be invested to set up a scheme which is likely to be ineffective in keeping us safe, yet will be extremely intrusive of our privacy online. It also seems that experience {{from other parts of the}} world, especially the European Union, whose own mass data retention scheme was invalidated last year for being a disproportionate interference with the privacy of millions of law-abiding Europeans, has been completely ignored. Ironically the UK’s data retention scheme has been pointed to in the report as representing best practice when actually it is under challenge for its incompatibility with the right to privacy. A strong, evidence-based case for why mass data retention is necessary in Australia has still not been made. And in its absence, these data retention proposals remain an expensive, intrusive and ineffective folly for the Australian government and opposition to pursue. Adam Henschke, Postdoctoral Research Fellow at Australian National University Many of these recommendations seem to be reasonable. My question is focused on the statement “the Government make a substantial contribution to the upfront capital costs of service providers implementing their data retention obligations. ” I question whether the government plans to outline a similar set of surety for government/security departments. That is, the prime minister has said retention will cost approximately A 400 million per year. And if the government is making a substantial contribution, from where is that money coming? Underpinning this concern is a question of cost-effectiveness. Given the vast cost of this program, it vital that we know how effective it is at reducing and/or investigating the serious crimes it intends to impact. Moreover, I’d suggest that there needs to be some way of measuring this effectiveness against more traditional policing/national security activities. If it comes to light that the metadata retention is diverting a great amount of resources from more effective practices, we ought to seriously reconsider the retention of metadata. Philip Branch, Senior Lecturer in Networking and Telecommunications at Swinburne University of Technology The committee has recommended that the data set to be retained be listed in the primary legislation (with a mechanism for emergency and temporary inclusion for <b>additional</b> <b>data</b> <b>items</b> if needs be). There’s also a recommendation that civil litigants be excluded from accessing it, restricting access for criminal investigations. The risk to journalism is also acknowledged and is to be investigated by another committee. I think these are welcome developments, particularly including the dataset within the primary legislation. All through this whole process it has been immensely frustrating that we really don’t know what exactly is to be retained. The word “metadata” alone tells us very little. I think that the data set needs to be made public as soon as possible and sufficient time be allowed for discussion before it be passed. I have particular concerns about the “messaging” data, but because we haven’t known what the data set is to be, all discussion so far has been built on speculation. It would be wrong to rush the bill through parliament without allowing time for proper analysis when we finally do get to know what is actually being proposed. This article was originally published on The Conversation. Read the original article...|$|E
40|$|We present DIP, a data {{discovery}} and dissemination protocol for wireless networks. Prior approaches, such as Trickle or SPIN, have overheads that scale linearly {{with the number}} of <b>data</b> <b>items.</b> For T items, DIP can identify new items with O(log(T)) packets while maintaining a O(1) detection latency. To achieve this performance in a wide spectrum of network configurations, DIP uses a hybrid approach of randomized scanning and tree-based directed searches. By dynamically selecting which of the two algorithms to use, DIP outperforms both in terms of transmissions and speed. Simulation and testbed experiments show that DIP sends 20 - 60 % fewer packets than existing protocols and can be 200 % faster, while only requiring O(log(log(T))) <b>additional</b> state per <b>data</b> <b>item.</b> ...|$|R
40|$|A {{method of}} {{representing}} {{a group of}} <b>data</b> <b>items</b> comprises, for each of a plurality of <b>data</b> <b>items</b> in the group, determining the similarity between said <b>data</b> <b>item</b> and each of a plurality of other <b>data</b> <b>items</b> in the group, assigning a rank to each pair {{on the basis of}} similarity, wherein the ranked similarity values for each of said plurality of <b>data</b> <b>items</b> are associated to reflect the overall relative similarities of <b>data</b> <b>items</b> in the group...|$|R
50|$|In a list, {{the order}} of <b>data</b> <b>items</b> is significant. Duplicate <b>data</b> <b>items</b> are permitted. Examples of {{operations}} on lists are searching for a <b>data</b> <b>item</b> in the list and determining its location (if it is present), removing a <b>data</b> <b>item</b> from the list, adding a <b>data</b> <b>item</b> to the list at a specific location, etc. If the principal operations on the list are to be the addition of <b>data</b> <b>items</b> {{at one end and}} the removal of <b>data</b> <b>items</b> at the other, it will generally be called a queue or FIFO. If the principal operations are the addition and removal of <b>data</b> <b>items</b> at just one end, it will be called a stack or LIFO. In both cases, <b>data</b> <b>items</b> are maintained within the collection in the same order (unless they are removed and re-inserted somewhere else) and so these are special cases of the list collection. Other specialized operations on lists include sorting, where, again, {{the order of}} <b>data</b> <b>items</b> is of great importance.|$|R
50|$|In COBOL, union <b>data</b> <b>items</b> {{are defined}} in two ways. The first uses the RENAMES (66 level) keyword, which {{effectively}} maps a second alphanumeric <b>data</b> <b>item</b> {{on top of}} the same memory location as a preceding <b>data</b> <b>item.</b> In the example code below, <b>data</b> <b>item</b> PERSON-REC is defined as a group containing another group and a numeric <b>data</b> <b>item.</b> PERSON-DATA is defined as an alphanumeric <b>data</b> <b>item</b> that renames PERSON-REC, treating the data bytes continued within it as character data.|$|R
40|$|To improve data {{accessibility}} in ad hoc networks, {{in our previous}} work we proposed three methods of replicating <b>data</b> <b>items</b> by considering the data access frequencies from mobile nodes to each <b>data</b> <b>item</b> and the network topology. In this paper, we extend our previously proposed methods to consider the correlation among <b>data</b> <b>items.</b> Under these extended methods, the data priority of each <b>data</b> <b>item</b> is defined based on the correlation among <b>data</b> <b>items,</b> and <b>data</b> <b>items</b> are replicated at mobile nodes with the data priority. We employ simulations {{to show that the}} extended methods are more efficient than the original ones. ...|$|R
50|$|In a tree, {{which is}} {{a special kind of}} graph, a root <b>data</b> <b>item</b> has {{associated}} with it some number of <b>data</b> <b>items</b> which in turn have associated with them some number of other <b>data</b> <b>items</b> in what is frequently viewed as a parent-child relationship. Every <b>data</b> <b>item</b> (other than the root) has a single parent (the root has no parent) and some number of children, possibly zero. Examples of operations on trees are the addition of <b>data</b> <b>items</b> so as to maintain a specific property of the tree to perform sorting, etc. and traversals to visit <b>data</b> <b>items</b> in a specific sequence.|$|R
40|$|Abstract. 4 -ary vector {{expression}} {{was defined by}} 3 -ary vector to describe subject and <b>data</b> <b>item</b> in dataspace. Association method between subject and <b>data</b> <b>item</b> was represented. Correlation of <b>data</b> <b>item</b> was defined by 4 -ary vector. Correlation and association way between <b>data</b> <b>items</b> were represented by 4 -ary vector. The validity of these methods was verified in technical document library...|$|R
50|$|This {{system allows}} the player to get Digimon {{which are not}} Partner Digimon. The player must hatch the Mercenary Digi-egg using an {{incubator}} along with the right type of <b>DATA</b> (chips) <b>items.</b> Mercenary Digi-eggs and <b>data</b> <b>items</b> can be acquired through game play (after defeating enemy Digimon) or purchased in the cash shop. The player can hatch the Mercenary Digimon after injecting DATA three times successfully: each time represents one stage of development. The player may inject <b>additional</b> <b>DATA</b> until {{the fourth or fifth}} stage. The higher the stage, the bigger the Digimon. However, the Mercenary Digi-egg may break after one or several attempts past the third stage. The alternate way to hatch a Mercenary Digi-egg is to buy it from the cash shop. The Mercenary Digi-egg bought from the cash shop has a 100% rate of success for at least 3 bars.|$|R
5000|$|The [...] "Point" [...] message defines two {{mandatory}} <b>data</b> <b>items,</b> x and y. The <b>data</b> <b>item</b> {{label is}} optional. Each <b>data</b> <b>item</b> has a tag. The tag is defined after the equal sign. For example, x has the tag 1.|$|R
50|$|In a multiset (or bag), {{like in a}} set, {{the order}} of <b>data</b> <b>items</b> does not matter, {{but in this case}} {{duplicate}} <b>data</b> <b>items</b> are permitted. Examples of operations on multisets are the addition and removal of <b>data</b> <b>items</b> and determining how many duplicates of a particular <b>data</b> <b>item</b> are present in the multiset. Multisets can be transformed into lists by the action of sorting.|$|R
30|$|We {{found that}} the {{proposed}} algorithm is presently in the broadcast structure. The wireless broadcast scheduling has been considered the <b>data</b> <b>item</b> frequency of the fixed and it has an unreasonable supposition. The <b>data</b> <b>item</b> frequency would be {{the request of the}} client for a change under the factual dynamic environments. Each of the <b>data</b> <b>item</b> has a frequency value itself and the each frequency of <b>data</b> <b>item</b> should been computed for its weight value and adjusted for dynamic broadcast adaptive so the frequency of <b>data</b> <b>item</b> has no fixed probability value.|$|R
50|$|Although the {{compiler}} (or interpreter) normally allocates individual <b>data</b> <b>items</b> on aligned boundaries, data structures {{often have}} members with different alignment requirements. To maintain proper alignment the translator normally inserts <b>additional</b> unnamed <b>data</b> members {{so that each}} member is properly aligned. In addition the data structure as a whole may be padded with a final unnamed member. This allows each member of an array of structures to be properly aligned.|$|R
50|$|In a set, {{the order}} of <b>data</b> <b>items</b> does not matter (or is undefined) but {{duplicate}} <b>data</b> <b>items</b> are not permitted. Examples of operations on sets are the addition and removal of <b>data</b> <b>items</b> and searching for a <b>data</b> <b>item</b> in the set. Some languages support sets directly. In others, sets can be implemented by a hash table with dummy values; only the keys are used in representing the set.|$|R
500|$|<b>Data</b> <b>items</b> in COBOL are {{declared}} hierarchically {{through the}} use of level-numbers which indicate if a <b>data</b> <b>item</b> is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level <b>data</b> <b>items,</b> with a level-number of 1, are called [...] Items that have subordinate aggregate data are called those that do not are called [...] Level-numbers used to describe standard <b>data</b> <b>items</b> are between 1 and 49.|$|R
40|$|Delivering patient-specific {{decision-support}} {{based on}} computer-interpretable guidelines (CIGs) requires mapping CIG clinical statements (<b>data</b> <b>items,</b> clinical recommendations) into patients’ data. This is most effectively done via intermediate data schemas, which enable querying the data {{according to the}} semantics of a shared standard intermediate schema. This study aims {{to evaluate the use}} of HL 7 virtual medical record (vMR) and openEHR archetypes as intermediate schemas for capturing clinical statements from CIGs that are mappable to electronic health records (EHRs) containing patient data and patient-specific recommendations. Using qualitative research methods, we analyzed the encoding of ten representative clinical statements taken from two CIGs used in real decision-support systems into two health information models (openEHR archetypes and HL 7 vMR instances) by four experienced informaticians. Discussion among the modelers about each case study example greatly increased our understanding of the capabilities of these standards, which we share in this educational paper. Differing in content and structure, the openEHR archetypes were found to contain a greater level of representational detail and structure while the vMR representations took fewer steps to complete. The use of openEHR in the encoding of CIG clinical statements could potentially facilitate applications other than decision-support, including intelligent data analysis and integration of <b>additional</b> properties of <b>data</b> <b>items</b> from existing EHRs. On the other hand, due to their smaller size and fewer details, the use of vMR potentially supports quicker mapping of EHR data into clinical statements. This study was partially funded by the European Commission 7 th Framework Program, grant # 287811. It has also been supported by the Spanish Ministry of Economy and Competitiveness and the EU FEDER programme through project TIN 2014 - 53749 -C 2 - 1 -R and grant PTQ- 12 - 05620...|$|R
40|$|In {{this paper}} we {{consider}} data freshness and overload handling in embedded systems. The requirements on data management and overload handling {{are derived from}} an engine control software. <b>Data</b> <b>items</b> need to be up-to-date, and to achieve this data dependencies must be considered, i. e., updating a <b>data</b> <b>item</b> requires other <b>data</b> <b>items</b> are upto-date. We also note that a correct result of a calculation can in some cases be calculated using {{a subset of the}} inputs. Hence, data dependencies can be divided into required and not required <b>data</b> <b>items,</b> e. g., only a subset of <b>data</b> <b>items</b> affecting the fuel calculation in an engine control needs to be calculated during a transient overload {{in order to reduce the}} number of calculations. Required <b>data</b> <b>items</b> must always be up-to-date, whereas not required <b>data</b> <b>items</b> can be stale. We describe an algorithm that dynamically determines which <b>data</b> <b>items</b> need to be updated taking workload, data freshness, and data relationships into consideration. Performance results show that the algorithm suppresses transient overloads better than (m, k) - and skipover scheduling combined with established algorithms to update <b>data</b> <b>items.</b> The performance results are collected from an implementation of a real-time database on the realtime operating system µC/OS-II. To investigate whether the system is occasionally overloaded an offline analysis algorithm estimating period times of updates is presented. ...|$|R
50|$|A <b>data</b> <b>item</b> {{describes}} an atomic state {{of a particular}} object concerning a specific property {{at a certain time}} point. A collection of <b>data</b> <b>items</b> for the same object at the same time forms an object instance (or table row). Any type of complex information can be broken down to elementary <b>data</b> <b>items</b> (atomic state). <b>Data</b> <b>items</b> are identified by object (o), property (p) and time (t), while the value (v) is a function of o, p and t: v = F(o,p,t).|$|R
50|$|The {{simplest}} processors are scalar processors. Each instruction {{executed by}} a scalar processor typically manipulates {{one or two}} <b>data</b> <b>items</b> at a time. By contrast, each instruction executed by a vector processor operates simultaneously on many <b>data</b> <b>items.</b> An analogy {{is the difference between}} scalar and vector arithmetic. A superscalar processor is a mixture of the two. Each instruction processes one <b>data</b> <b>item,</b> but there are multiple execution units within each CPU thus multiple instructions can be processing separate <b>data</b> <b>items</b> concurrently.|$|R
3000|$|... {{age of a}} <b>data</b> <b>item,</b> {{calculated}} by taking {{the difference between the}} current time, t_curr, and the measurement time of that <b>data</b> <b>item</b> t(d); [...]...|$|R
50|$|For example, an {{abstract}} stack, {{which is a}} last-in-first-out structure, could be defined by three operations: push, that inserts a <b>data</b> <b>item</b> onto the stack; pop, that removes a <b>data</b> <b>item</b> from it; and peek or top, that accesses a <b>data</b> <b>item</b> {{on top of the}} stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a <b>data</b> <b>item</b> into the queue; dequeue, that removes the first <b>data</b> <b>item</b> from it; and front, that accesses and serves the first <b>data</b> <b>item</b> in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many <b>data</b> <b>items</b> have been pushed into the stack, and that the stack uses a constant amount of storage for each element.|$|R
40|$|Although data {{broadcast}} {{has been}} shown to be an efficient data dissemination technique for mobile computing systems, many issues such as selection of broadcast data and caching strategies at the clients are still active research areas. In this paper, by examining the dynamic properties of the <b>data</b> <b>items</b> in mobile computing systems, we define the validity of a <b>data</b> <b>item</b> by its absolute validity interval (avi). Based on the avi of the <b>data</b> <b>items,</b> we propose different broadcast algorithms in which the selection of <b>data</b> <b>items</b> for broadcast will be based on the avi of the <b>data</b> <b>items</b> and their access frequencies. The purpose of the AVI algorithms is to increase the client cache hit probability so that the access delay for a <b>data</b> <b>item</b> will be much reduced. Simulation experiments have been conducted to compare the AVI algorithms with the algorithm which only considers the popularity of the <b>data</b> <b>items.</b> The results indicate that the AVI algorithms can significantly improve the mean response time and reduce the deadline missing requests. ...|$|R
40|$|In recent years, {{there has}} been much focus on skyline queries that {{incorporate}} and provide more flexible query operators that return <b>data</b> <b>items</b> which are dominating other <b>data</b> <b>items</b> in all attributes (dimensions). Several techniques for skyline have been proposed in the literature. Most of the existing skyline techniques aimed to find the skyline query results by supposing that the values of dimensions are always present for every <b>data</b> <b>item.</b> In this paper we aim to evaluate the skyline preference queries in which some dimension values are missing. We proposed an approach for answering preference queries in a database by utilizing the concept of skyline technique. The skyline set selected for a given query operation is then optimized so that the missing values are replaced with some approximate values that provide a skyline answer with complete data. This will significantly reduce the number of comparisons between <b>data</b> <b>items.</b> Beside that, the number of retrieved skyline <b>data</b> <b>items</b> is reduced and this guides the users to select the most appropriate <b>data</b> <b>items</b> from the several alternative complete skyline <b>data</b> <b>items...</b>|$|R
40|$|Technologies {{such as the}} World Wide Web have {{resulted}} in the access of information over a global network. Data can be text, images, video, or sound. Current network limitations and the large size of <b>data</b> <b>items</b> result in a high response time - the time taken for an image to be transmitted from a remote site to the user. In this paper we focus on reducing this response time for images. Traditionally in an information system exact copies of text <b>data</b> <b>items</b> had to be retrieved for the user but multimedia <b>data</b> <b>items</b> such as video and images can be represented by an equivalent <b>data</b> <b>item</b> which is an approximation of the original <b>data</b> <b>item.</b> Such an equivalent <b>data</b> <b>item</b> might satisfy an application equally well and also have a lower response time. Based on the assumption that such an approximation of a <b>data</b> <b>item</b> is sufficient for several applications, we have developed an architecture where an image equivalent to the original can be retrieved instead of the original image itself. Our architectu [...] ...|$|R
40|$|The {{rapid growth}} of data is inevitable, and {{retrieving}} the best results that meet the user’s preferences is essential. To achieve this, skylines were introduced in which <b>data</b> <b>items</b> that are not dominated by the other <b>data</b> <b>items</b> in the database are retrieved as results (skylines). In most of the existing skyline approaches, the databases {{are assumed to be}} static and complete. However, in real world scenario, databases are not complete especially in multidimensional databases in which some dimensions may have missing values. The databases might also be dynamic in which new <b>data</b> <b>items</b> are inserted while existing <b>data</b> <b>items</b> are deleted or updated. Blindly performing pairwise comparisons on the whole <b>data</b> <b>items</b> after the changes are made is inappropriate as not all <b>data</b> <b>items</b> need to be compared in identifying the skylines. Thus, a novel skyline algorithm, DInSkyline, is proposed in this study which finds the most relevant <b>data</b> <b>items</b> in dynamic and incomplete databases. Several experiments have been conducted and the results show that DInSkyline outperforms the previous works by reducing the number of pairwise comparisons in the range of 52...|$|R
50|$|DataBlitz also {{provides}} higher-layer interfaces for grouping related <b>data</b> <b>items,</b> and performing scans {{as well as}} associative access (via indices) on <b>data</b> <b>items</b> in a group...|$|R
30|$|An {{alternative}} {{data dissemination}} mechanism is the broadcast disks scheme, which permits <b>data</b> <b>items</b> to be broadcast with different frequencies [5]. This algorithm first divides <b>data</b> <b>items</b> {{into a few}} groups (i.e., disks) such that <b>data</b> <b>items</b> with similar popularity are assigned to the same disks. Afterwards, it determines the rotation speed of each disk according to the popularity of <b>data</b> <b>items.</b> In this way, one can construct a broadcast program that adjusts the trade-off between the access time of hot data and that of cold data.|$|R
40|$|Applications {{that make}} use of very large {{scientific}} datasets have become an increasingly important subset of scientific applications. In these applications, datasets are often multi-dimensional, i. e., <b>data</b> <b>items</b> are associated with points in a multi-dimensional attribute space, and access to <b>data</b> <b>items</b> is described by range queries. The basic processing involves mapping input <b>data</b> <b>items</b> to output <b>data</b> <b>items,</b> and some form of aggregation of all the input <b>data</b> <b>items</b> that project to the each output <b>data</b> <b>item.</b> We have developed an infrastructure, called the Active Data Repository (ADR), that integrates storage, retrieval and processing of multi-dimensional datasets on distributed-memory parallel architectures with multiple disks attached to each node. In this paper we address efficient execution of range queries on distributed memory parallel machines within ADR framework. We present three potential strategies, and evaluate them under different application scenarios and machine co [...] ...|$|R
5000|$|With read caches, a <b>data</b> <b>item</b> {{must have}} been fetched from its {{residing}} location {{at least once in}} order for subsequent reads of the <b>data</b> <b>item</b> to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a <b>data</b> <b>item</b> may be realized upon the first write of the <b>data</b> <b>item</b> by virtue of the <b>data</b> <b>item</b> immediately being stored in the cache's intermediate storage, deferring the transfer of the <b>data</b> <b>item</b> to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand, ...|$|R
500|$|An 88 level-number {{declares}} a [...] (a so-called 88-level) {{which is}} true when its parent <b>data</b> <b>item</b> contains one of the values specified in its [...] clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the [...] <b>data</b> <b>item.</b> When the <b>data</b> <b>item</b> contains a value of , the condition-name [...] is true, whereas when it contains a value of [...] or , the condition-name [...] is true. If the <b>data</b> <b>item</b> contains some other value, both of the condition-names are false.|$|R
