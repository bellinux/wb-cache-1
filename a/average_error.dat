3013|5582|Public
5|$|In the past, {{the human}} {{forecaster}} {{was responsible for}} generating the entire weather forecast based upon available observations. Today, human input is generally confined to choosing a model based on various parameters, such as model biases and performance. Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. However, regardless how small the <b>average</b> <b>error</b> becomes with any individual system, large errors within any particular piece of guidance are still possible on any given model run. Humans are required to interpret the model data into weather forecasts that are understandable to the end user. Humans can use knowledge of local effects that may be too small in size to be resolved by the model to add information to the forecast. While increasing accuracy of forecast models implies that humans {{may no longer be}} needed in the forecast process {{at some point in the}} future, there is currently still a need for human intervention.|$|E
500|$|Some of the enzymes {{showing the}} highest {{specificity}} and accuracy {{are involved in}} the copying and expression of the genome. Some of these enzymes have [...] "proof-reading" [...] mechanisms. Here, an enzyme such as DNA polymerase catalyzes a reaction in a first step and then checks that the product is correct in a second step. This two-step process results in <b>average</b> <b>error</b> rates of less than 1 error in 100 million reactions in high-fidelity mammalian polymerases. Similar proofreading mechanisms are also found in RNA polymerase, aminoacyl tRNA synthetases and ribosomes.|$|E
500|$|At 03:00UTC on October22, the NHC {{forecast}} Patricia {{to achieve}} major hurricane status in 36hours; less than 15hours later, the system exceeded their forecast peak. Strengthening into a Category5 hurricane was not forecast at all until Patricia had already reached such intensity, {{although in the}} intermediate advisory immediately before Patricia's upgrade to Category 5, the NHC noted that [...] "Patricia could become a category 5 hurricane overnight". This trend continued throughout the rapid intensification period, resulting {{in some of the}} largest errors on record through 48hours; they were the worst-ever for the Eastern Pacific since the NHC took over operations for the basin in 1988. All forecast models saw enormous errors, most of which performed worse than the official NHC forecasts. No model accurately prognosticated the magnitude nor rate of the intensification. The EMXI—an output from the European Centre for Medium-Range Weather Forecasts—saw the largest <b>average</b> <b>error</b> with [...] at 48hours.|$|E
30|$|Statistical {{parameters}} used in {{the study}} are <b>average</b> percentage <b>error</b> ‘APE’, <b>average</b> absolute percentage <b>error</b> (AAPE), <b>average</b> percentage <b>error</b> (APE), correlation coefficient (R), and standard deviation.|$|R
40|$|Copyright © 2013 Yanjie Jiang, Ziqing Zhang. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. In this paper, we discuss the <b>average</b> <b>errors</b> of function approximation by linear combinations of Bernstein operators. The strongly asymptotic orders for the <b>average</b> <b>errors</b> of the combinations of Bernstein operators sequence are deter-mined on the Wiener space...|$|R
40|$|The Perkin Elmer Model 140 was {{investigated}} for second level alignment. Using a photolithographic evaluation mask, inspection of six wafers yielded overlay <b>errors.</b> The <b>average</b> x-translational <b>error</b> was - 1. 95 urn, the <b>average</b> y-translational <b>error</b> was -. 4 um, and the <b>average</b> rotational <b>error</b> was -. 0005 uradians...|$|R
500|$|High-speed {{computers}} and sophisticated simulation software allow meteorologists to run computer models that forecast tropical cyclone tracks {{based on the}} future position and strength of high- and low-pressure systems. [...] Combining forecast models with increased understanding of the forces that act on tropical cyclones, and a wealth of data from Earth-orbiting satellites and other sensors, scientists have increased the accuracy of track forecasts over recent decades. [...] The addition of dropwindsonde missions around tropical cyclones in {{what are known as}} synoptic flow missions in the Atlantic Basin decreased track error by 15–20 percent. Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. [...] However, regardless how small the <b>average</b> <b>error</b> becomes, large errors within the guidance are still possible. [...] An accurate track forecast is important, because if the track forecast is incorrect, forecasts for intensity, rainfall, storm surge, and tornado threat will also be incorrect.|$|E
500|$|The {{success of}} blind bombing in Flagpole led to Grandy {{deciding}} {{to use the}} blind radar technique again. Hubbard was less sure. In 52 practice drops with blind radar, the <b>average</b> <b>error</b> had been [...] as opposed to [...] with visual bombing. The problem for the aircrew was {{that they would be}} dropping a live hydrogen bomb—generally considered a dangerous thing to do—with no means of verifying that their instruments were correct. Air Chief Marshal Sir Harry Broadhurst, the head of Bomber Command, wished O'Connor luck; his XD827 would make the drop, with Squadron Leader Tony Caillard in XD827, the grandstand aircraft. The aircraft took off at 07:15 on 11 September 1958. Once in the air, though, a fault developed in the ground radar transmitter. Grandy then authorised a visual drop. It was later confirmed that it was [...] from the target. [...] It was detonated at [...] at 08:49 with a yield of about , very close to the predicted yield of [...]|$|E
500|$|In the past, the {{forecaster}} {{was responsible}} for generating the entire weather forecast based upon available observations. Today, meteorologists' input is generally confined to choosing a model based on various parameters, such as model biases and performance. [...] Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. [...] However, regardless how small the <b>average</b> <b>error</b> becomes with any individual system, large errors within any particularly piece of guidance are still possible on any given model run. [...] Professionals are required to interpret the model data into weather forecasts that are understandable to the lay person. [...] Professionals can use knowledge of local effects which may be too small in size to be resolved by the model to add information to the forecast. [...] As an example, terrain is considered in the QPF process by using topography or climatological precipitation patterns from observations with fine detail. [...] Using model guidance and comparing the various forecast fields to climatology, extreme events such as excessive precipitation associated with later flood events lead to better forecasts. [...] While increasing accuracy of forecast models implies that humans {{may no longer be}} needed in the forecast process {{at some point in the}} future, there is currently still a need for human intervention.|$|E
3000|$|... {{which is}} {{reliable}} {{for the original}} discretisation without stabilisation. Undisplayed experiments computed the <b>averaging</b> <b>error</b> estimator[18], which is founded on the same theoretical background as [...]...|$|R
30|$|For all the scenarios, as expected, the users’ {{satisfaction}} decreases {{when the}} <b>average</b> location <b>error</b> increases. The top left figure (max. 4 users/femtocell) shows the good {{performance of the}} HPLM method compared to the MR method when the <b>average</b> location <b>error</b> is less than 5  m. In other case (<b>average</b> location <b>error</b> over 5  m), the MLB algorithm should get the RSS information from the MR method. Next figure, top right (max. 8 users/femtocell), presents similar behavior. Now, the algorithm should avoid HPLM method when the <b>average</b> location <b>error</b> is over 4  m.|$|R
40|$|General {{formulas}} {{are derived}} for determining gage <b>averaging</b> <b>errors</b> of strip-type heat flux meters {{used in the}} measurement of one-dimensional heat flux distributions. The local <b>averaging</b> <b>error</b> e(x) {{is defined as the}} difference between the measured value of the heat flux and the local value which occurs {{at the center of the}} gage. In terms of e(x), a correction procedure is presented which allows a better estimate for the true value of the local heat flux. For many practical problems, it is possible to use relatively large gages to obtain acceptable heat flux measurements...|$|R
2500|$|The Hawk-Eye Innovations website {{states that}} the system {{performs}} with an <b>average</b> <b>error</b> of [...] The standard diameter of a tennis ball is , equating to a 5% error relative to ball diameter. This is roughly equivalent to the fluff on the ball.|$|E
2500|$|A {{test system}} was first {{attempted}} on 10 April 1943 between the LORAN stations at Fenwick and Bonavista, [...] away. This test demonstrated accuracy of ½ mile, {{significantly better than}} normal LORAN. This led to {{a second round of}} tests in late 1943, this time using four stations, Montauk, East Brewster, MA, Gooseberry Falls, MO, and Key West, FL. Extensive evaluation flights revealed an <b>average</b> <b>error</b> of [...]|$|E
2500|$|The pyramid {{remained}} the tallest man-made {{structure in the}} world for over 3,800 years, unsurpassed until the [...] spire of Lincoln Cathedral was completed c.1300. The accuracy of the pyramid's workmanship is such that the four sides of the base have an <b>average</b> <b>error</b> of only 58millimetres in length. The base is horizontal and flat to within ±. The sides of the square base are closely aligned to the four cardinal compass points (within fourminutes of arc) based on true north, not magnetic north, and the finished base was squared to a mean corner error of only 12 seconds of arc.|$|E
50|$|Otherwise, {{these can}} be poor approximations for beta {{distributions}} with other values of α and β, exhibiting <b>average</b> <b>errors</b> of 40% in the mean and 549% in the variance.|$|R
40|$|Abstract: Research for {{advanced}} traveler information systems (ATIS) {{has been focused}} on urban roads. However, research for short-term traffic prediction on all categories of highways is needed, as highway agencies expect to implement intelligent transportation systems across their jurisdictions. In this study, genetic algorithms were used to design time delay neural network (TDNN) models as well as locally weighted regression models to predict short-term traffic for six rural roads from Alberta, Canada. These roads are from various trip-pattern groups and functional classes. Refined TDNN models developed in this study can limit most <b>average</b> <b>errors</b> less than 10 % for all study roads. Refined regression models show even higher accuracy. <b>Average</b> <b>errors</b> for the refined regression models are less than 2 % for roads with stable patterns. Even for roads with unstable patterns, <b>average</b> <b>errors</b> are below 4 %, and the 95 th percentile errors are less than 7 %. It is believed that such accurate predictions would be useful for highway agencies to implement statewide ATIS...|$|R
30|$|However, in the estimations of {{both the}} {{frequency}} and the amplitude, the IECM had the largest errors, especially for the interharmonic components, which were detected with <b>average</b> percentage <b>errors</b> {{in the frequency of}} 0.2 % and 0.6 % for the components near 74.79 and 382.35  Hz, respectively. For the same components, the <b>average</b> <b>errors</b> of amplitude were even worse, at 3 % and 31 %, respectively.|$|R
5000|$|Instead of {{analyzing}} the <b>average</b> <b>error</b> probability, we analyze the expectationof the <b>average</b> <b>error</b> probability, where {{the expectation is}} with respect to therandom choice of code: ...|$|E
50|$|A summary for the 2003 through 2007 seasons {{shows that}} PECOTA's <b>average</b> <b>error</b> between the {{predicted}} and actual team wins declined: 2003 5.91 wins;2004 7.71 wins;2005 5.14 wins;2006 4.94 wins;2007 4.31 wins. Silver conjectures that the improvement {{has come in}} part from taking defense into account in the forecasts beginning in 2005.In 2008 the <b>average</b> <b>error</b> was 8.5 wins.|$|E
50|$|Although RMSE {{is one of}} {{the most}} {{commonly}} reported measures of disagreement, some scientists misinterpret RMSD as <b>average</b> <b>error,</b> which RMSD is not. RMSD is the square root of the average of squared errors, thus RMSD confounds information concerning <b>average</b> <b>error</b> with information concerning variation in the errors. The effect of each error on RMSD is proportional to the size of the squared error thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers.|$|E
3000|$|Focusing on {{the ship}} size estimation, it is {{important}} to note that the system taken as reference, the CA-CFAR, presents poor ship size estimations, with minimum length and width <b>average</b> <b>errors</b> of [...]...|$|R
40|$|Histogram {{techniques}} {{have been used}} in many commercial database management systems to estimate a query result size. Recently, {{it has been shown that}} they are very effective to support approximation of query processing especially aggregates. In this paper, we investigate the problem of minimizing <b>average</b> <b>errors</b> of approximate aggregates using histogram techniques. Firstly, we present a novel linear-spline histogram model that is more accurate than the existing models. Secondly, we propose a novel histogram construction technique for minimizing such <b>average</b> <b>errors,</b> which is shown to generate a near optimal histogram. Our experiment results demonstrate that the new histogram construction techniques lead to a great accuracy improvement on the existing techniques...|$|R
40|$|The error {{introduced}} by the coil size averaging effect when measuring nonuniform low-frequency (LF) magnetic ﬁelds is investigated with particular reference {{to the use of}} ﬁeld meters equipped with nonconcentric coil probes. The analysis is developed by computation and the results are validated by comparison with experimental data. The inﬂuence of different parameters, such as source type, distance of the probe from the source, coil dimension, and probe orientation, is investigated and quantiﬁed. The results are synthesized by introducing limit distances fromthe ﬁeld source, beyond which the <b>averaging</b> <b>error</b> is lower than a stated value. An estimate of the uncertainty component due to <b>averaging</b> <b>error</b> is also given...|$|R
5000|$|After the 2010 midterm elections, Silver {{concluded}} that Rasmussen's polls {{were the least}} accurate of the major pollsters in 2010, having an <b>average</b> <b>error</b> of 5.8 points and a pro-Republican bias of 3.9 points according to Silver's model. Conservative polling analyst Neil Stevens wrote, [...] "after the primaries Silver said Rasmussen was in his crosshairs for ducking out {{on a number of}} races by not polling primaries." [...] FiveThirtyEight currently rates Rasmussen Reports with a C+ grade and notes a simple <b>average</b> <b>error</b> of 5.3 percent across 657 polls analyzed.|$|E
50|$|A 2008 analysis, using Rybka 3, {{showed that}} Capablanca had the {{smallest}} <b>average</b> <b>error</b> factor (i.e. {{the most accurate}} play); but after adjusting for {{factors such as the}} complexity of positions, the best player came out as Fischer, followed by Capablanca, Karpov and Kramnik. The best players had an <b>average</b> <b>error</b> of about 0.07 pawns per move (after the opening). Capablanca was the most positional player, and Anand by far the most tactical. The most complex game tested was Fischer v Spassky (1972 game 6, Fischer won) while the most accurately played game was Tal v Benko (1958, Tal won).|$|E
50|$|The Swordfish {{was also}} capable of {{operating}} as a dive-bomber. During 1939, Swordfish onboard HMS Glorious {{participated in a}} series of dive-bombing trials, during which 439 practice bombs were dropped at dive angles of 60, 67 and 70 degrees, against the target ship HMS Centurion. Tests against a stationary target showed an <b>average</b> <b>error</b> of 49 yd from a release height of 1300 ft and a dive angle of 70 degrees; tests against a manoeuvring target showed an <b>average</b> <b>error</b> of 44 yd from a drop height of 1800 ft and a dive angle of 60 degrees.|$|E
30|$|The <b>average</b> {{position}} <b>error</b> {{values for}} different algorithms and different dimension length {{are shown in}} Fig.  2. It is shown that the <b>average</b> position <b>errors</b> of the three algorithms become smaller as dimension length increases. The worst algorithm is the method based on original VLAD, which has the largest <b>average</b> position <b>error.</b> The best algorithm is still the proposed method based on SSA-VLAD, which has the smallest position estimation error for the same dimension length.|$|R
30|$|In case of [2], the <b>average</b> {{estimation}} <b>error</b> {{rate was}} 3 %, so our assumed estimation error rate {{was higher than}} 3 %. Similarly, for [10, 12], the estimation rate was assumed based on their <b>average</b> estimation <b>error</b> rate.|$|R
30|$|Table  4 {{exhibits}} {{the statistics}} of the Rv correlation (Fattah 2005) {{as compared with}} the new neural network models generated. From this table, one can easily recognize that the new models from the NN and SVM are the best matching models which give the lowest <b>average</b> absolute <b>error</b> 0.1496 and 0.1222 %, respectively. To validate the developed models, super testing was done based on unseen data. According to this test, SVM is the best method in terms of accurate prediction with an <b>average</b> relative <b>error</b> of 0.121 % followed by NN model with an <b>average</b> relative <b>error</b> of 0.313. On the other hand, FN was the worst model with an <b>average</b> relative <b>error</b> of 27.3 %.|$|R
5000|$|A 1996 {{evaluation}} using a PLGR (a 5-channel L2 GPS receiver) {{found no}} clear advantage to using WAGE in its then-current configuration. Its overall <b>average</b> <b>error</b> of 9.1 meters {{was worse than}} when WAGE was not used.|$|E
50|$|However, proper {{coverage}} for the required four satellites to locate a receiver is not achieved with all current designs (2008-11) for indoor operations. Beyond, the <b>average</b> <b>error</b> budget for GNSS systems normally is {{much larger than the}} confinements, in which the locating shall be performed.|$|E
50|$|The Hawk-Eye Innovations website {{states that}} the system {{performs}} with an <b>average</b> <b>error</b> of 3.6 mm. The standard diameter of a tennis ball is 67 mm, equating to a 5% error relative to ball diameter. This is roughly equivalent to the fluff on the ball.|$|E
40|$|We {{investigated}} two optical {{methods for}} characterizing submicron structures. <b>Average</b> <b>errors</b> {{of a few}} nanometers can {{be determined by the}} far-field diffraction metrology utilizing diffractive structures having enhanced sensitivity to fabrication errors. The scanning spot metrology is well suited for analyzing lithographic masks...|$|R
40|$|Characteristics of the {{convergence}} error in a HSCT structural optimization were investigated. A probabilistic model {{is used to}} model the errors in optimal objective function values of poorly converged runs and the Weibull distribution {{was identified as a}} reasonable error model. Once the probabilistic error model is identified, we demonstrate that {{it can be used to}} estimate <b>average</b> <b>errors</b> from a set of pairs of runs. In particular, by performing pairs of optimization runs from two starting points, we can obtain accurate estimates of the mean and standard deviation of {{the convergence}} errors. Positive correlations were identified between the magnitude of the differences of paired optimization runs and the <b>average</b> <b>errors.</b> The results show that finding the error distribution model is...|$|R
40|$|This paper {{proposes a}} new {{generalized}} {{method of moments}} (GMM) estimator for spatial panel models with spatial moving <b>average</b> <b>errors</b> combined with a spatially autoregressive dependent variable. Monte Carlo results are given suggesting that the GMM estimator is consistent. The estimator is applied to English real estate price data...|$|R
