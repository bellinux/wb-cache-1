7|862|Public
5000|$|... #Caption: SPARCserver 1000 and SPARC Storage <b>Array</b> <b>disk</b> array ...|$|E
50|$|In the {{adjacent}} diagram, data are concatenated {{from the end}} of disk 0 (block A63) to the beginning of disk 1 (block A64); end of disk 1 (block A91) to the beginning of disk 2 (block A92). If RAID 0 were used, then disk 0 and disk 2 would be truncated to 28 blocks, the size of the smallest disk in the <b>array</b> (<b>disk</b> 1) for a total size of 84 blocks.|$|E
5000|$|Codex {{products}} use a touchscreen interface and removable [...] "data packs", containing up to 10TB of raid <b>array</b> <b>disk</b> storage. Interfaces {{for digital}} cinematography cameras include single and dual-link HD-SDI and Infiniband. Codex uses {{what they call}} a [...] "Virtual File System" [...] or in technical terms, it acts as a file server. When accessed via a conventional Ethernet network, the captured material can be viewed in a number of resolutions and formats, such as QuickTime, MXF, AVI, WAV and JPEG.|$|E
50|$|Since {{the crucial}} {{function}} of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a <b>disk</b> <b>array.</b> A <b>disk</b> <b>array</b> typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically <b>disk</b> <b>arrays</b> increase level of availability by using redundant components other than RAID, such as power supplies. <b>Disk</b> <b>arrays</b> may be consolidated or virtualized in a SAN.|$|R
50|$|For example, one of {{the checks}} in the risk {{assessment}} report lists volumes whose mirrors are composed of logical units from the same <b>disk</b> <b>array.</b> Logical volumes that are mirrored have higher availability when mirrored across separate <b>disk</b> <b>array</b> controllers. Mirroring logical volumes across separate <b>disk</b> <b>array</b> controllers allows the logical volumes to continue to operate should one <b>disk</b> <b>array</b> fail.|$|R
40|$|Abstract—The {{performance}} {{modeling and}} analysis of <b>disk</b> <b>arrays</b> is challenging due {{to the presence of}} multiple <b>disks,</b> large <b>array</b> caches, and sophisticated array controllers. Moreover, storage manufacturers may not reveal the internal algorithms implemented in their devices, so real <b>disk</b> <b>arrays</b> are effectively black-boxes. We use standard performance techniques to develop an integrated performance model that incorporates some of the complexities of real <b>disk</b> <b>arrays.</b> We show how measurement data and baseline performance models can be used to extract information about the various features implemented in a <b>disk</b> <b>array.</b> In this process, we identify areas for future research in the performance analysis of real <b>disk</b> <b>arrays.</b> Index Terms—RAID, analytical performance model, array cache, parallel I/O, enterprise storage systems, I/O performance evaluation, <b>disk</b> <b>array...</b>|$|R
40|$|A Video-on-Demand (VOD) server {{needs to}} store {{hundreds}} of movie titles {{and to support}} thousands of concurrent accesses. This, technically and economically, imposes a great challenge {{on the design of}} the disk storage subsystem of a VOD server. Due to different demands for different movie titles, the numbers of concurrent accesses to different movie titles can differ a lot. We define access profile as the number of concurrent accesses to each movie title that should be supported by a VOD server. The access profile is derived based on the popularity of each movie title and thus serves as a major design goal for the disk storage subsystem. Since some popular (hot) movie titles may be concurrently accessed by hundreds of users and a current high-end magnetic disk <b>array</b> (<b>disk)</b> can only support tens of concurrent accesses, it is necessary to replicate and/or stripe the hot movie files over multiple disk arrays. The consequence of replication and striping for hot movie titles is the potential [...] ...|$|E
40|$|A Video-on-Demand (VOD) server {{needs to}} store {{hundreds}} of movie titles {{and to support}} thousands of concurrent accesses. We define access profile {{as the number of}} concurrent accesses to each movie title that should be supported by a VOD server. A current highend magnetic disk <b>array</b> (<b>disk)</b> can only support tens of MPEG- 2 concurrent accesses, and it is necessary to replicate and/or stripe the hot movie files over multiple disk arrays. How to replicate, stripe, and place the movie files over a minimum number of magnetic disk arrays such that a given access profile can be supported is an important problem. In this paper, we formulate and solve this problem. The result of this study {{can be applied to the}} design of the storage subsystem of a VOD server to economically minimize the cost or to maximize the utilization of disk arrays. 1. Introduction A central design issue of providing VOD services is how to organize and store hundreds of movie files over multiple disks (disk arrays) such that [...] ...|$|E
40|$|A Video-on-Demand (VOD) server {{needs to}} store {{hundreds}} of movie titles {{and to support}} thousands of concurrent accesses. This, technically and economically, imposes a great challenge {{on the design of}} the disk storage subsystem of a VOD server. Due to different demands for different movie titles, the numbers of concurrent accesses to different movie titles can differ a lot. We define access profile as the number of concurrent accesses to each movie title that should be supported by a VOD server. The access profile is derived based on the popularity of each movie title and thus serves as a major design goal for the disk storage subsystem. Since some popular (hot) movie titles may be concurrently accessed by hundreds of users and a current high-end magnetic disk <b>array</b> (<b>disk)</b> can only support tens of concurrent accesses, it is necessary to replicate and/or stripe the hot movie files over multiple disk arrays. The consequence of replication and striping for hot movie titles is the potential increase on the required number of disk arrays. Therefore, how to replicate, stripe, and place the movie files over a minimum number of magnetic disk arrays such that a given access profile can be supported is an important problem. In this paper, we formulate the problem of the video file allocation over disk arrays, demonstrate that it is a NP-hard problem, and present some heuristic algorithms to find the near-optimal solutions. The result of this study {{can be applied to the}} design of the storage subsystem of a VOD server to economically minimize the cost or to maximize the utilization of disk arrays. Keywords: Multimedia, Video-On-Demand, Concurrent Access, MPEG-II, RAID 3, Replication, Striping, 2 -D Vector Packing 1 Distributed Multimedia Research Center (DMRC) is sponsored by US WEST [...] ...|$|E
25|$|RAID used to mean Redundant <b>Array</b> of Inexpensive <b>Disks,</b> {{but is now}} {{commonly}} {{interpreted as}} Redundant <b>Array</b> of Independent <b>Disks.</b>|$|R
5000|$|The base {{configuration}} of Storage Center includes a <b>disk</b> <b>array</b> controller, <b>disk</b> enclosure, disk drives, connectivity hardware, and software modules. The operating system, also called Storage Center, is regularly updated. Software modules include: ...|$|R
40|$|The {{performance}} {{modeling and}} analysis of <b>disk</b> <b>arrays</b> is challenging due {{to the presence of}} multiple <b>disks,</b> large <b>array</b> caches, and sophisticated array controllers. Moreover, storage manufacturers may not reveal the internal algorithms implemented in their devices, so real <b>disk</b> <b>arrays</b> are effectively black-boxes. We use standard performance techniques to develop an integrated performance model that incorporates some of the complexities of real <b>disk</b> <b>arrays.</b> We show how measurement data and baseline performance models can be used to extract information about the various features implemented in a <b>disk</b> <b>array.</b> In this process, we identify areas for future research in the performance analysis of real <b>disk</b> <b>arrays...</b>|$|R
40|$|In 1989, the RAID {{group at}} U. C. Berkeley built a {{prototype}} <b>disk</b> <b>array</b> called RAID-I. The bandwidth achieved by RAID-I was severely {{limited by the}} memory system bandwidth limitations of the <b>disk</b> <b>array's</b> host workstation. As a result, most of the bandwidth available from the disks could not be delivered to clients of the <b>disk</b> <b>array</b> #le server. We designed our second prototype, RAID-II, to deliver {{as much of the}} <b>disk</b> <b>array</b> bandwidth as possible to #le server clients. A custom-built circuit-board <b>disk</b> <b>array</b> controller, called the XBUS board, connects the disks and the high-speed network directly, allowing data for large requests to bypass the server workstation. A single workstation may control several XBUS boards for increased bandwidth...|$|R
40|$|Declustered data {{organizations}} in <b>disk</b> <b>arrays</b> (RAIDs) achieve less-intrusive {{reconstruction of data}} after a disk failure. We present PDDL, a new data layout for declustered <b>disk</b> <b>arrays.</b> PDDL layouts exist for a large variety of <b>disk</b> <b>array</b> configurations with a distributed spare disk. PDDL declustered <b>disk</b> <b>arrays</b> have excellent run-time performance under light and heavy workloads. PDDL maximizes access parallelism in the most critical circumstances, namely during reconstruction of data on the spare disk. PDDL occurs minimum address translation overhead compared to all other proposed declustering layouts. ...|$|R
5000|$|... #Caption: Two {{cylinders}} in a RAID diagram, symbolizing an <b>array</b> of <b>disks</b> ...|$|R
40|$|Thesis {{focuses on}} <b>disk</b> <b>arrays,</b> where {{the goal is}} to design test {{scenarios}} to measure performance of <b>disk</b> <b>array</b> and use predictive analytics tools to train a model that will predict the selected performance parameter on a measured set of data. The implemented web application demonstrates the functionality of the trained model and shows estimate of the <b>disk</b> <b>array</b> performance...|$|R
40|$|In this paper, {{we propose}} a highly {{reliable}} <b>disk</b> <b>array</b> architecture called Dual-Crosshatch <b>Disk</b> <b>Array</b> (DCDA). We {{have proposed a}} low overhead, triple-erasure correcting parity organization called interleaved 2 d-parity scheme. This parity scheme is {{used as the basis}} for the design of the DCDA architecture. DCDA uses a hybrid approach of RAID- 4 and RAID 5 with one dedicated parity group and another parity group with block-interleaved data and parity. This architecture has very high reliability with low overheads, good degradedmode performance, and acceptable normal-mode performance. The results obtained from simulations indicate that DCDA is about 10 4 times more reliable than the best existing <b>disk</b> <b>array</b> architecture for the parameters considered in this paper. The average response time and throughput of DCDA is better than that of the RAID- 5 organization. Index terms: <b>Disk</b> <b>Arrays,</b> Dual Crosshatch <b>Disk</b> <b>Array,</b> Interleaved 2 d-Parity, RAID, Reliability. 1 A preliminary version of this p [...] ...|$|R
40|$|A <b>disk</b> <b>array</b> is a {{collection}} of physically small magnetic disks that is packaged as a single unit but operates in parallel. <b>Disk</b> <b>arrays</b> capitalize on the availability of small-diameter disks from a price-competitive market to provide the cost, volume, and capacity of current disk systems but many times their performance. Unfortunately, relative to current disk systems, the larger number of components in <b>disk</b> <b>arrays</b> leads to higher rates of failure. To tolerate failures, redundant <b>disk</b> <b>arrays</b> devote a fraction of their capacity to an encoding of their information. This redundant information enables the contents of a failed disk to be recovered from the contents of non-failed disks. The simplest and least expensive encoding for this redundancy, known as N+ 1 parity is highlighted. In addition to compensating for the higher failure rates of <b>disk</b> <b>arrays,</b> redundancy allows highly reliable secondary storage systems to be built much more cost-effectively than is now achieved in conventional duplicated <b>disks.</b> <b>Disk</b> <b>arrays</b> that combine redundancy with the parallelism of many small-diameter disks are often called Redundant <b>Arrays</b> of Inexpensive <b>Disks</b> (RAID). This combination promises improvements to both the performance and the reliability of secondary storage. For example, IBM's premier disk product, the IBM 3390, is compared to a redundant <b>disk</b> <b>array</b> constructed of 84 IBM 0661 3 1 / 2 -inch disks. The redundant <b>disk</b> <b>array</b> has comparable or superior values for each of the metrics given and appears likely to cost less. In the first section of this tutorial, I explain how <b>disk</b> <b>arrays</b> exploit the emergence of high performance, small magnetic disks to provide cost-effective disk parallelism that combats the access and transfer gap problems. The flexibility of disk-array configurations benefits manufacturer and consumer alike. In contrast, I describe in this tutorial's second half how parallelism, achieved through increasing numbers of components, causes overall failure rates to rise. Redundant <b>disk</b> <b>arrays</b> overcome this threat to data reliability by ensuring that data remains available during and after component failures...|$|R
50|$|Of course, {{the overall}} {{performance}} of a system is not only relevant {{to the performance of}} host and network, but also influenced by the performance of the disk constituting file system. So, BWFS file system can be structured by the LUN from multiple <b>disk</b> <b>arrays.</b> It equals to another layer of RAID structured between multiple <b>disk</b> <b>arrays,</b> which maximizes the performance of <b>disk</b> <b>arrays.</b>|$|R
40|$|In 1989, the RAID (Redundant <b>Arrays</b> of Inexpensive <b>Disks)</b> {{group at}} U. C. Berkeley built a {{prototype}} <b>disk</b> <b>array</b> called RAID-I. The bandwidth delivered to clients by RAID-I was severely {{limited by the}} memory system bandwidth of the <b>disk</b> <b>array's</b> host workstation. We designed our second prototype, RAID-II, to deliver more of the <b>disk</b> <b>array</b> bandwidth to file server clients. A custom-built crossbar memory system called the XBUS board connects the disks directly to the high-speed network, allowing data for large requests to bypass the server workstation. RAID-II runs Log-Structured File System (LFS) software to optimize performance for bandwidth-intensive applications. Th...|$|R
40|$|During {{the past}} decade, {{advances}} in processor and memory technology have {{given rise to}} increases in computational performance that far outstrip increases in the performance of secondary storage technology. Coupled with emerging small-disk technology, <b>disk</b> <b>arrays</b> provide the cost, volume, and capacity of current disk subsystems, by leveraging parallelism, many times their performance. Unfortunately, <b>arrays</b> of small <b>disks</b> may have much higher failure rates than the single large disks they replace. Redundant <b>arrays</b> of inexpensive <b>disks</b> (RAID) use simple redundancy schemes to provide high data reliability. The data encoding, performance, and reliability of redundant <b>disk</b> <b>arrays</b> are investigated. Organizing redundant data into a <b>disk</b> <b>array</b> is treated as a coding problem. Among alternatives examined, codes as simple as parity are shown to effectively correct single, self-identifying disk failures...|$|R
40|$|As <b>disk</b> <b>arrays</b> {{become widely}} used, tools for {{understanding}} and analyzing their performance become increasingly important. In particular, performance models can be invaluable in both configuring and designing <b>disk</b> <b>arrays.</b> Accurate analytic performance models are desirable over {{other types of}} models {{because they can be}} quickly evaluated, are applicable under a wide range of system and workload parameters, and can be manipulated by a range of mathematical techniques. Unfortunately, analytical performance models of <b>disk</b> <b>arrays</b> are difficult to formulate due to the presence of queuing and fork-join synchronization; a <b>disk</b> <b>array</b> request is broken up into independent disk requests which must all complete to satisfy the original request. We develop, validate, and apply an analytic performance model for <b>disk</b> <b>arrays.</b> We derive simple equations for approximating their utilization, response time, and throughput. We then validate the analytic model via simulation and investigate the accuracy of each approximation used in deriving the analytical model. Finally, we apply the analytical model to derive an equation for the optimal unit of data striping in <b>disk</b> <b>arrays...</b>|$|R
40|$|The {{excellent}} reliability {{provided by}} RAID Level 5 data organization {{has been seen}} to be insufficient for future mass storage systems. We analyze the multi-dimensional <b>disk</b> <b>array</b> {{in search of the}} necessary improved reliability. The paper begins by introducing multi-dimensional <b>disk</b> <b>array</b> data organization schemes based on maximum distance separable error correcting codes and incorporating both strings and spares. Several figures of merit are calculated using a standard Markov failure and repair model for these organizations. Based on our results, the multi-dimensional <b>disk</b> <b>array</b> organization is an excellent approach to providing improved reliability. 1 Introduction <b>Disk</b> <b>array</b> storage systems, especially those with redundant <b>arrays</b> of independent <b>disks</b> (RAID) Level 5 data organization [6], provide excellent cost, run-time performance as well as reliability and will meet the needs of computing systems for the immediate future. Computing systems, especially those with massive storage requ [...] ...|$|R
50|$|In {{a modern}} {{enterprise}} architecture <b>disk</b> <b>array</b> controllers (sometimes also called storage processors, or SPs) {{are parts of}} physically independent enclosures, such as <b>disk</b> <b>arrays</b> placed in a storage area network (SAN) or network-attached storage (NAS) servers.|$|R
40|$|All server storage environments {{depend on}} <b>disk</b> <b>arrays</b> {{to satisfy their}} capacity, reliability, and {{availability}} requirements. In order to manage these storage systems efficiently, {{it is necessary to}} understand the behavior of <b>disk</b> <b>arrays</b> and predict their performance. We develop an analytical model that estimates mean performance measures of <b>disk</b> <b>arrays</b> under a synchronous I/O workload. Synchronous I/O requests are generated by jobs that each block while their request is serviced. Upon I/O service completion, a job may use other computer resources before issuing another I/O request. Our <b>disk</b> <b>array</b> model considers the effect of workload sequentiality, read-ahead caching, write-back caching, and other complex optimizations incorporated into most <b>disk</b> <b>arrays.</b> The model is validated against a mid-range disk-array for a variety of synthetic I/O workloads. The model is computationally simple and scales easily as the number of jobs issuing requests increases, making it potentially useful to performance engineers...|$|R
40|$|Abstract—Disk {{scrubbing}} periodically {{scans the}} contents of a <b>disk</b> <b>array</b> to detect the presence of irrecoverable read errors and reconstitute {{the contents of}} the lost blocks using the builtin redundancy of the <b>disk</b> <b>array.</b> We address the issue of scheduling scrubbing runs in <b>disk</b> <b>arrays</b> that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole <b>array</b> whenever a <b>disk</b> failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of <b>disk</b> <b>arrays</b> over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twentyfour hour interval. Keywords-irrecoverable read errors; RAID arrays; disk scrubbing. I...|$|R
40|$|In this paper, {{we examine}} how <b>disk</b> <b>arrays</b> and shared memory multiprocessors {{lead to an}} {{effective}} method for constructing database machines for general-purpose complex query processing. We show that <b>disk</b> <b>arrays</b> can lead to cost-effective storage systems if they are configured from suitably small formfactor disk drives. We introduce the storage system metric data temperature {{as a way to}} evaluate how well a disk configuration can sustain its workload, and we show that <b>disk</b> <b>arrays</b> can sustain the same data temperature as a more expensive mirrored-disk configuration. We use the metric to evaluate the performance of <b>disk</b> <b>arrays</b> in XPRS, an operational shared-memory multiprocessor database system being developed at the University of California, Berkeley...|$|R
50|$|Typically a <b>disk</b> <b>array</b> {{provides}} increased availability, resiliency, and maintainability {{by using}} existing components (controllers, power supplies, fans, etc.), often {{up to the}} point where all single points of failure (SPOFs) are eliminated from the design. Additionally, <b>disk</b> <b>array</b> components are often hot-swappable.|$|R
5000|$|Clariion (styled CLARiiON) is a {{discontinued}} [...] SAN <b>disk</b> <b>array</b> manufactured {{and sold}} by EMC Corporation, it occupied the entry-level and mid-range of EMC's SAN <b>disk</b> <b>array</b> products. In 2011, EMC introduced the EMC VNX Series, designed to replace both the Clariion and Celerra products.|$|R
40|$|Large <b>arrays</b> {{of small}} <b>disks</b> are {{providing}} an attractive approach for high performance I/O systems. In {{order to make}} effective use of <b>disk</b> <b>arrays</b> and other multi-disk architectures, {{it is necessary to}} develop intelligent software tools that allow automatic tuning of the <b>disk</b> <b>arrays</b> to varying workloads. In this paper we describe an adaptive method for data allocation and load balancing in <b>disk</b> <b>arrays.</b> Our method deals with dynamically changing access frequencies of files by reallocating file extents, thus "cooling down" hot disks. In addition, the method takes into account the fact that some files may exhibit periodical access patterns, and considers explicitly the cost of performing the "cooling" operations. Preliminary performance studies based on real-life I/O traces demonstrate the effectivity of this approach...|$|R
25|$|For {{relational}} databases {{or other}} systems that require ACID transactions, even a modest amount of flash storage can offer vast speedups over <b>arrays</b> of <b>disk</b> drives.|$|R
50|$|Protocol analyzers {{can also}} be hardware-based, either in probe format or, as is {{increasingly}} common, combined with a <b>disk</b> <b>array.</b> These devices record packets (or {{a slice of the}} packet) to a <b>disk</b> <b>array.</b> This allows historical forensic analysis of packets without users having to recreate any fault.|$|R
50|$|While AoE {{is mostly}} {{supported}} on Linux, Z-SAN is supported on Microsoft Windows platforms. A Z-SAN can <b>array</b> many more <b>disks</b> than a standard RAID. The Zetera website claims that MIT has a Z-SAN array totaling 1.4 Petabytes of storage. The <b>disk</b> <b>arrays</b> {{can be both}} striped and mirrored.|$|R
40|$|Provisioning {{storage in}} <b>disk</b> <b>arrays</b> is a {{difficult}} problem because many applications with different workload characteristics and priorities share resources provided by the array. Currently, storage in <b>disk</b> <b>arrays</b> is statically partitioned, leading to difficult choices between over-provisioning to meet peak demands and resource sharing to meet efficiency targets. In this paper, we present Maestro, a feedback controller that can manage resources on large <b>disk</b> <b>arrays</b> to provide performance differentiation among multiple applications. Maestro monitors the performance of each application and dynamically allocates the array resources so that diverse performance requirements can be met without static partitioning. It supports multiple performance metrics (e. g., latency and throughput) and application priorities so that important applications receive better performance in case of resource contention. By ensuring that high-priority applications sharing storage with other applications obtain the performance levels they require, Maestro {{makes it possible to}} use storage resources efficiently. We evaluate Maestro using both synthetic and real-world workloads on a large, commercial <b>disk</b> <b>array.</b> Our experiments indicate that Maestro can reliably adjust the allocation of <b>disk</b> <b>array</b> resources to achieve application performance targets. 1...|$|R
40|$|Abstract: <b>Disk</b> <b>arrays</b> were {{proposed}} in the 1980 s {{as a way to}} use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of most major computer manufacturers. This paper gives a comprehensive overview of <b>disk</b> <b>arrays</b> and provides a framework in which to organize current and future work. The paper first introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It then discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the paper describes seven <b>disk</b> <b>array</b> architectures, called RAID (Redundant <b>Arrays</b> of Inexpensive <b>Disks)</b> levels 0 - 6 and compares their performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to improve performance and designing algorithms to maintain data consistency. Last, the paper describes five <b>disk</b> <b>array</b> prototypes or products and discusses future opportunities for research. The pape...|$|R
40|$|<b>Disk</b> <b>arrays</b> were {{proposed}} in the 1980 s {{as a way to}} use parallelism between multiple disks to improve aggregate 1 / 0 performance. Today they appear in the product lines of most major computer manufacturers. This article gives a comprehensive overview of <b>disk</b> <b>arrays</b> and provides a framework in which to organize current and future work. First, the article introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the article describes seven <b>disk</b> <b>array</b> architectures, called RAID (Redundant <b>Arrays</b> of Inexpensive <b>Disks)</b> levels O– 6 and compares their performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to improve performance and designing algorithms to maintain data consistency. Last, the article describes six <b>disk</b> <b>array</b> prototypes or products and discusses future opportunities for research, with an annotated bibliography of disk array-related literature...|$|R
