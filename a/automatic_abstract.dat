13|372|Public
50|$|In the 1980s, {{after graduating}} from the Academy of Fine Arts in Bratislava, she moved to Moscow, working on <b>automatic</b> <b>abstract</b> {{paintings}} questioning the notion of authorship. From this period comes also the series of black-and-white photographs documenting certain aspects of late Soviet era daily life. On her return to Bratislava in 1991 she helped found Aspekt, Slovakia’s first feminist journal. In 1994 she abandoned painting to dedicate her practice to installation, performance and video art. Throughout all the 1990s her research was focused on alternative modes of sensuality depiction in the medium of video, often combining video screenings with live performances. Since 2000 her work is mainly focused on notion of political body in the geopolitical context of Central-East Europe.|$|E
40|$|In {{this paper}} we will {{illustrate}} {{the approach to}} multilingual <b>automatic</b> <b>abstract</b> production adopted by the EU-sponsored project MLIS-MUSI. Although a small scale research project, MUSI has tried to tackle the challenges set by multilingual summarization by adopting an original approach based on {{the definition of a}} shared ontology and representation language, and on the reuse of existing linguistic resources. MUSI combines a linguistic-based module for relevant sentence extraction and a concept-based component to generate multilingual summaries. 1...|$|E
40|$|We have {{developed}} an <b>automatic</b> <b>abstract</b> generation system for Japanese expository writings based on rhetorical structure extraction. The system first extracts the rhetorical structure, the compound of the rhetorical relations between sentences, and then cuts out less important parts in the extracted structure to generate an abstract of the desired length. Evaluation of the generated abstract showed that it contains at maximum 74 % {{of the most important}} sentences of the original text. The system is now utilized as a text browser for a prototypical interactive document retrieval system. ...|$|E
40|$|The present paper {{discusses}} a new knowledge-based approach {{proposed for}} <b>automatic</b> <b>abstracting</b> (AA) {{in a limited}} domain. Unlike most of the <b>automatic</b> <b>abstracting</b> models which do not make use of domain knowledge (an therefore are hardly capable of differentiating the &quot;important &quot; from the other concepts in the area) and linguistic knowledge (the summaries obtained usually do not undergo any additional modifications and the uncontrolled linkage of the text picked up for summary may sometimes be unacceptable), our approach is based on empirical, domain and linguistic knowledge, the latter enabling an additional treatment of the extracted text {{in order to obtain}} more coherent and natural text. The principal ideas presented in the paper are incorporated in the development of an <b>automatic</b> <b>abstracting</b> program...|$|R
40|$|In what follows, {{we want to}} {{describe}} a system for the <b>automatic</b> <b>abstracting</b> of textual material. In designing the system, major theoretical questions have arisen not unlike those that arise in dealing with any natural language system. In addition, we want {{to describe}} some proposed revisions which have important theoretical implications and which should lead to significant improvements in {{the capabilities of the}} present system. Our initial efforts at <b>automatic</b> <b>abstracting</b> began in 1969 as part of a more general questionanswering system (Tharp, 1969; Tharp and Krulee, 1969) which made use of short stories about famous discoveries taken from a children's encyclopedia...|$|R
40|$|Knowledge {{about the}} {{argumentative}} structure of scientific articles can, amongst other things, {{be used to}} improve <b>automatic</b> <b>abstracts.</b> We argue that the argu- mentative structure of scientific discourse can be auto- matically detected because reasoning about problems, research tasks and solutions follows predictable pat- terns...|$|R
40|$|Adapted {{to the web}} at this URL [[URL] {{we propose}} a system that,online, {{automatically}} and directly transforming a source text into a reduced target text. It {{is based on the}} identification of specific expressions allowing an evaluation of the relevance of the sentence concerned, which can then be selected for the elaboration of the summary. To construct the PERTINENCE system, we resorted to the linguistic means of discourse analysis and the computing capacity of data processing instruments. To produce more reliable summary we imagined a method that takes in account human summaries to produce a relevant <b>automatic</b> <b>abstract.</b> This system is developed with XML in the Java language; it can be installed on any server like Intranet, Extranet or Internet. PERTINENCE is an advanced version of a system called RAFI...|$|E
40|$|Abstract: In this paper, some {{applications}} of {{natural language processing}} techniques for information retrieval have been introduced, but the results are known not to be satisfied. In order to find the roles of some classical natural language processing techniques in information retrieval and to find {{which one is better}} we compared the effects with the various natural language techniques for information retrieval precision, and the experiment results show that basic natural language processing techniques with small calculated consumption and simple implementation help a small for information retrieval. Senior high complexity of natural language processing techniques with high calculated consumption and low precision can not help the information retrieval precision even harmful to it, so the role of natural language understanding may be larger in the question answering system, <b>automatic</b> <b>abstract</b> and information extraction...|$|E
40|$|A {{critical}} component of an abstract interpretation algorithm is the abstract domain, and traditionally, it is manually defined. Recently, <b>automatic</b> <b>abstract</b> interpretation techniques have emerged. A prominent technique, called Counter Example Guided Abstraction Refinement (CEGAR), {{and is based on}} an iterative process given a target property. Beginning with an abstract domain with a low level of detail, failure to prove the property gives rise a process of abstraction refinement so that the next iteration can use a new abstract domain with a higher level of detail. The idea is to build the final abstract domain with {{just the right amount of}} detail. In this paper, we present a complementary approach: we abstract on demand, as opposed to refine on demand. Starting with no abstraction at all, our algorithm iteratively builds, if needed, abstract domains with lesser detail. We demonstrate advantages of our algorithm over CEGAR. ...|$|E
40|$|In {{order to}} build robust <b>automatic</b> <b>abstracting</b> systems, {{there is a need}} for better {{training}} resources than are currently available. In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Ou...|$|R
40|$|We have {{developed}} a new methodology for <b>automatic</b> <b>abstracting</b> of scientific and technical articles called Selective Analysis. This methodology allows the generation of indicativeinformative abstracts integrating different types of information extracted from the source text. The indicative part of the abstract identifies the topics of the document while the informative one elaborates some topics according to the reader's interest. The first evaluation of our methodology demonstrates that Selective Analysis performs well in the task of signaling {{the topic of the}} document demonstrating the viability of such a technique. The sentences the system produces from instantiated templates are considered to be as acceptable as human produced sentences. 1 Introduction <b>Automatic</b> <b>abstracting</b> of textual data {{can be seen as a}} process composed of four main steps: (i) the source text is interpreted in order to obtain a "meaning" representation of the source; (ii) the representation is then used [...] ...|$|R
40|$|Sentence {{similarity}} computing {{plays an}} important role in machine question-answering systems, machine-translation systems, information retrieval and <b>automatic</b> <b>abstracting</b> systems. This article firstly sums up several methods for calculating similarity between sentences, and brings out a new method which takes all factors into consideration including critical words, semantic information, sentential form and sen-tence length. And on this basis, a <b>automatic</b> <b>abstracting</b> system based on LexRank algorithm is implemented. We made several improvements in both sentence weight computing and redundancy resolution. The system described in this article could deal with single or multi-document summarization both in English and Chinese. With evaluations on two corpuses, our system could produce better summaries to a certain degree. We also show that our system is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents. And in the end, existing problem and the developing trend of automatic summariza-tion technology are discussed...|$|R
40|$|We {{present a}} {{framework}} for automatic abstraction planning, called Spatula. The framework provides a structure within which different parameterized methods for automatic abstraction can be instantiated to generate abstraction planning behavior. Abstraction is performed at problem-solving time with respect to impasses in the current problem context, and thus the planner can generate abstractions in response to specific situations. We describe the framework, {{and the way in}} which it provides an integrated environment for <b>automatic</b> <b>abstract</b> problem-solving and learning. We then present instances of the family of abstraction methods that may be used with Spatula. We first describe an implementation of two abstraction methods within the framework, and present results which show the utility of the method both in terms of tractability and solution quality. Then, we present a third abstraction method for use in a real-time planning environment. 1 Introduction In this paper, we p [...] ...|$|E
40|$|Abstract. Among precise {{abstract}} interpretation methods {{developed during the}} last decade, policy iterations {{is one of the}} most promising. Despite its efficiency, it has not yet seen a broad usage in static analyzers. We believe the main explanation to this restrictive use, beside the novelty domain framework. This prevents an easy integration in existing static domains through reduced product. This paper aims at providing a classic abstract domain interface to policy iterations. Usage of semidefinite programming to infer quadratic invariants on linear systems {{is one of the most}} appealing use of policy iteration. Combination with a template generation heuristic, inspired from existing methods from control theory, gives a fully <b>automatic</b> <b>abstract</b> domain to infer quadratic invariants on linear systems with guards. Those systems often constitute the core of embedded control systems and are hard, when not impossible, to analyze with linear abstract domains. The method has been implemented and applied to some benchmark systems, giving good results...|$|E
40|$|Abstract: We {{present a}} design of {{cognitive}} system architecture with an internal world model. The internal world model is realized {{with the help}} of artificial mirror neurons. We consider generalized artificial mirror neurons acting both as a mechanism for assembling and learning multimodal sensorimotor information and as associative memory for invoking multimodal information given only some of its components. We show that within an artificial cognitive system a network of generalized mirror neurons can simultaneously serve as an internal world model recognized by the agent and as that of the agent’s position within this world. We also specify a self-organizing control mechanism, which is based on the basic operations over concepts that were essentially identified by the British 18 th century philosopher David Hume. This control mechanism makes use of the internal world model constructed in agent’s interaction with real world and straightforwardly supports imitation learning. Building heavily on the properties of the generalized mirror net and on <b>automatic</b> <b>abstract</b> concept creation, we offer an algorithmic explanation of computational language acquisition, thinking and consciousness in our model. Rather than describing an implementation of the respective mechanisms, the aim of the paper is to present a plausible hypothesis concerning the architecture and functionality of artificial systems exhibiting higher cognitive functions. 1...|$|E
40|$|Abst rac t We present {{work on the}} {{automatic}} generation of short indicative-informative abstracts of scien-tific and technical articles. The indicative part of the abstract identifies the topics of the docu-elaborate some topics according to the reader's interest by motivating the topics, describing en-tities and defining concepts. We have defined our method of <b>automatic</b> <b>abstracting</b> by study-ing a corpus professional bstracts. The method also considers the reader's interest as essential {{in the process of}} abstracting. ...|$|R
40|$|We present {{work on the}} {{automatic}} generation of short indicative-informative abstracts of scien- tific and technical articles. The indicative part of the abstract identifies the topics of the document while the informative part of the abstract elaborate some topics according to the reader's interest by motivating the topics, describing entities and defining concepts. We have defined our method of <b>automatic</b> <b>abstracting</b> by study~ ing a corpus professional abstracts. The method also considers the reader's interest as essential {{in the process of}} abstracting...|$|R
5000|$|Luhn spent {{greater and}} greater amounts {{of time on the}} {{problems}} of information retrieval and storage faced by libraries and documentation centers, and pioneered the use of data processing equipment in resolving these problems. [...] "Luhn was the first, or among the first, to work out many of the basic techniques now commonplace in information science." [...] These techniques included full-text processing; hash codes; Key Word in Context indexing (see also Herbert Marvin Ohlman); auto-indexing; <b>automatic</b> <b>abstracting</b> and the concept of selective dissemination of information (SDI).|$|R
40|$|In {{the time}} of {{overloaded}} online information, automatic text summarization is especially demanded for salient information retrieval from huge amount electronic text. For the blessing of World Wide Web, the mass of data is now enormous in its volume. Researchers realized this fact from various aspects and tried to generate an <b>automatic</b> <b>abstract</b> of the gigantic body of data from the commencement of the last half century. Numerous ways are there for characterizing different approaches to passage recapitulation: extractive and abstractive from single or compound document, objective of content abridgement, characteristic of text summarization, level of processing from superficial to profound and sort of article's content. A significant précis is very much helpful in our day to day life which can save valuable time. The investigation was at first commenced naively on single document abstraction. In this paper, automatic single document text summarization task is addressed and different methodologies of various researchers are discussed {{from the very beginning}} of this research to this modern age. This literature review intends to observe the trends of abstraction procedure using natural language processing. Also some promising approaches are indicated and particular concentration is dedicated for the categorization of diversified methods from raw level to similar like human professionals, so that in future one can get precious direction for further analysis...|$|E
40|$|Copyright © 2013 ISSR Journals. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: In the time of overloaded online information, automatic text summarization is especially demanded for salient information retrieval from huge amount electronic text. For the blessing of World Wide Web, the mass of data is now enormous in its volume. Researchers realized this fact from various aspects and tried to generate an <b>automatic</b> <b>abstract</b> of the gigantic body of data from the commencement of the last half century. Numerous ways are there for characterizing different approaches to passage recapitulation: extractive and abstractive from single or compound document, objective of content abridgement, characteristic of text summarization, level of processing from superficial to profound and sort of article’s content. A significant précis is very much helpful in our day to day life which can save valuable time. The investigation was at first commenced naively on single document abstraction. In this paper, automatic single document text summarization task is addressed and different methodologies of various researchers are discussed {{from the very beginning}} of this research to this modern age. This literature review intends to observe the trends of abstraction procedure using natural language processing. Also some promising approaches are indicated and particular concentration is dedicated for the categorization of diversified methods from raw level to similar like human professionals, so that in future one can ge...|$|E
40|$|Problem statement: This study {{proposed}} a {{system based on}} a heuristic lemmatization for Arabic text indexation and classification. This research is needed {{for a lot of}} NLP applications such as the research of information and <b>automatic</b> <b>abstract.</b> This system was not related to any linguistic rule. The proposed method was limited to five different domains: Sports, medicine, politics, economics and agriculture. The main idea is collecting different texts related to the chosen domains and studying them by extracting the pertinent terms. Approach: Every entered text had the formatting stage in which we can remove some words and letters that do not have any importance for the meaning. After that, the frequencies average is calculated to classify the text and its related domain. Results: The main finality of the System of Indexation and Classification of Arabic Texts (SICAT) is to classify finally an unknown text in its suitable domain. So, its to detect the text theme. To do this task, we applied a method by pertinent terms correspondence. It is about testing the correspondence of all pertinent terms of the text to classify with the keywords of every domain of the corpus. The domain, that constitutes the majority of terms having a correspondence with terms of the text, represents the theme that we look for to classify our unknown text. Conclusion: It holds two main parts: the indexation and the classification. The indexation stage is composed of three main parts: the pre-learning, the lemmatization and the frequencies calculation. The classification stage is composed of two main components: the extraction of keywords and classification of new text. We have made many tests of verification to test the validation of the system. The system performance was evaluated on the different chosen domains, achieves 90 % precision and 85 % recall...|$|E
5000|$|<b>Automatic</b> {{generation}} of <b>abstract</b> for village, tehsil and district.|$|R
40|$|Natural Language Processing (NLP) is {{that field}} of {{computer}} science {{which consists of}} interfacing computer representations of information with natural languages used by humans. It examines {{the use of computers}} in understanding and manipulating the natural language text and speech. The main aim of the researchers in this field is to collect the necessary details about how natural languages are being used and understood by humans. They use these details to develop the tools for making the computers understand and manipulate the natural languages to perform the desired tasks. In this paper we describe some of the theoretical developments that have influenced research in NLP. We also discuss <b>automatic</b> <b>abstracting</b> and information retrieval in natural language processing applications. We conclude with a discussion o...|$|R
40|$|Information Extraction (IE) {{approaches}} currently {{assume that}} a template exists which sufficiently defines {{the requirements of the}} task. Substantial human effort is required to generate these basic templates and to provide a development corpus. In the two principal IE competitions, the Message Understanding Conference (MUC) and Tipster, the templates were constructed directly from the experience of analysts. This manual approach cannot always be assumed. This proposal concerns the automatic construction of MUC-style templates, substantially reducing the human effort required. The approach will carry out a corpus-based analysis of task-relevant documents, identifying and analysing the interaction between the fundamental elements. A resource which defines semantic relationships will be necessary to identify and categorise these fundamental elements. This application is of particular interest to researchers in the field of IE and <b>automatic</b> <b>abstracting.</b> 1 1...|$|R
40|$|Abstract: Problem statement: This study {{proposed}} a {{system based on}} a heuristic lemmatization for Arabic text indexation and classification. This research is needed {{for a lot of}} NLP applications such as the research of information and <b>automatic</b> <b>abstract.</b> This system was not related to any linguistic rule. The proposed method was limited to five different domains: Sports, medicine, politics, economics and agriculture. The main idea is collecting different texts related to the chosen domains and studying them by extracting the pertinent terms. Approach: Every entered text had the formatting stage in which we can remove some words and letters that do not have any importance for the meaning. After that, the frequencies ’ average is calculated to classify the text and its related domain. Results: The main finality of the System of Indexation and Classification of Arabic Texts (SICAT) is to classify finally an unknown text in its suitable domain. So, it’s to detect the text theme. To do this task, we applied a method by pertinent terms correspondence. It is about testing the correspondence of all pertinent terms of the text to classify with the keywords of every domain of the corpus. The domain, that constitutes the majority of terms having a correspondence with terms of the text, represents the theme that we look for to classify our unknown text. Conclusion: It holds two main parts: the indexation and the classification. The indexation stage is composed of three main parts: the pre-learning, the lemmatization and the frequencies ’ calculation. The classification stage is composed of two main components: the extraction of keywords and classification of new text. We have made many tests of verification to test the validation of the system. The system performance was evaluated on the different chosen domains, achieves 90 % precision and 85 % recall. Key words: Natural language processing, indexation and classificatio...|$|E
40|$|Introduction Many {{words have}} two or more very {{distinct}} meanings. For example, the word pen can refer to a writing implement or to an enclosure. Many natural language applications, including information retrieval, content analysis and <b>automatic</b> <b>abstracting,</b> and machine translation, require the resolution of lexical ambiguity for words in an input text, or are significantly improved when this can be accomplished. 1 That is, the preferred input to these applications is a text in which each word is "tagged" {{with the sense of}} that word intended in the particular context. However, at present there is no reliable way to automatically identify the correct sense of a word in running text. This task, called word sense disambiguation, is especially difficult for texts from unconstrained domains because the number of ambiguous words is potentially very large. The magnitude of the problem can be reduced by considering only very gro...|$|R
40|$|Key words: Free {{boundary}} problems, shape sensitivity analysis, <b>automatic</b> differentia-tion. <b>Abstract.</b> In {{this paper}} we shall study different solution strategies for abstract form of free boundary problems. Using {{the tools of}} shape calculus we analyse different solution strategies. As {{a result of our}} analysis we provide guidelines for a generic approach for deriving efficient numerical algorithms for stationary FBP:s and discuss their implemen-tation. ...|$|R
40|$|An {{experiment}} in the computer generation of coherent discourse was successfully conducted to test a hypothesis about the transitive nature of syntactic dependency relations among elements of the English language. The two primary components of the experimental computer program consisted of a phrase structure generation grammar capable of generating grammatical nonsense, and a monitoring system which would abort the generation process whenever {{it was apparent that}} the dependency structure of a sentence being generated was not in harmony with the dependency relations existing in an input source text. The final outputs of the system were coherent paraphrases of the source text. An implication of the hypothesis is that certain types of dependency relations are invariant under a variety of linguistic transformations. Potential applications include automatic kernelizing, question answering, automatic essay writing, and <b>automatic</b> <b>abstracting</b> systems. The question of the validity of transitive dependency models for languages other than English should be explored...|$|R
40|$|In {{order to}} build robust <b>automatic</b> <b>abstracting</b> systems, {{there is a need}} for better {{training}} resources than are currently available. In this paper, we introduce an annotation scheme for scientic articles which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use. 1 Introduction Current approaches to automatic summarization cannot create coherent, exible automatic summaries. Sentence selection techniques (e. g. Brandow et al., 1995; Kupiec et al. 1995) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e. g. Rau et al., 1989, Young and Hayes, 1985) are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricte [...] ...|$|R
40|$|An {{important}} {{number of}} methods for scene break detection and <b>automatic</b> video <b>abstracting</b> {{have been investigated}} in recent years. These methods have to detect two kinds of scene changes known as abrupt and gradual transition. The present study we present two old and reliable algorithms on which we brought modifications {{in order to reduce}} complexity while preserving the same accuracy and satisfy the real-time constraints of new multimedia applications...|$|R
40|$|International audienceEfficiently {{programming}} shared-memory machines is {{a difficult}} challenge because mapping application threads onto the memory hierarchy has a strong impact on the performance. However, optimizing such thread placement is difficult: architectures become increasingly complex and application behavior changes with implementations and input parameters, e. g problem size and number of threads. In this work, we propose a fully <b>automatic,</b> <b>abstracted</b> and portable affinity module. It produces and implements an optimized affinity strategy that combines knowledge about application characteristics and the platform topology. Implemented in the back-end of our runtime system (ORWL), our approach was used to enhance the performance and the scalability of several unmodified ORWL-coded applications: matrix multiplication, a 2 D stencil (Livermore Kernel 23), and a video tracking real world application. On two SMP machines with quite different hardware characteristics, our tests show spectacular performance improvements for these unmodified application codes due to a dramatic decrease of cache misses and pipeline stalls. A comparison to reference implementations using OpenMP confirms this performance gain of almost one order of magnitude...|$|R
40|$|ABSTRACT. This paper {{describes}} {{new methods}} of automatically extracting documents for screening purposes, i. e. the computer selection of sentences having the greatest potential for conveying to the reader {{the substance of the}} document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: prag-matic words (cue words); title and heading words; and structural indicators (sentence loca-tion). The research has resulted in an operating system and a research methodology. The extract-ing system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed com-ponents dominate the frequency component in the production of better extracts. KEY WORDS AND PHRASES: <b>automatic</b> extracting, <b>automatic</b> <b>abstracting,</b> sentence selection, document screening, sentence significance, relevance, content words, key words, pragmati...|$|R
50|$|The use of {{metamorphosis}} through Picasso influenced Surrealism in the 1920s, and {{it appeared}} both as subject matter and as procedure in the figurative paintings of Leonora Carrington and in the more <b>abstract,</b> <b>automatic</b> works of André Masson.|$|R
40|$|This {{paper is}} a {{translation}} of a french paper wich has been submitted to "Journes Scientifiques et Techniques de la Francophonie AUPELF-UREF" How to Appreciate the Quality of Automatic Text Summarization? Examples of FAN and MLUCE Protocols and their Results on SERAPHIN Summary For the SERAPHIN project, we set up two assessment protocols {{in order to be}} able to more accurately assess the quality of abstracts - the FAN protocol and the MLUCE protocol, for which we provide the results. The FAN protocol assesses the legibility of an abstract, independently from the source text. The MLUCE protocol is designed to allow users of <b>automatic</b> <b>abstracts</b> to assess their quality. These protocols were applied to a corpus of 27 texts which varied in length from between three and twelve pages. These texts were randomly chosen from EDF archives. They include both scientific and general press articles, extracts from books, and internal EDF notes. The results of the FAN protocol demonstrate the difficulty of using surface linguistic indicators to assess the quality of an abstract; the results of the MLUCE protocol illustrate the importance of user expectation...|$|R
40|$|A {{new concept}} of {{architectural}} and algorithmic organization of software that supports information learning environment was worked out. The concept describes {{a number of}} modules such as monitoring and processing of the heterogeneous educational information resources module, module for indexing {{for all types of}} search (attributive, fulltext, quasisemantic, associative etc), modules which provides intellecualized possibilities such as referencing, clasterization, classification. A new method of organizing data to support the entire range of search possibilities was created. Algorithms for indexing and retrieval of structured and semi-structured textual data were developed. A new method of structural-algorithmic organization of the quasisemantic search means were proposed. The method identifies user?s search interest by modifying the search query using ontologies and knowledge bases of the relevant subject areas, which allows to significantly increase pertinence of the search results. New methods of <b>automatic</b> <b>abstracting</b> scientific and educational textual data based on hierarchical fuzzy neural networks and genetic algorithm-based were developed. The new algorithms for identification and comparison of textual information objects were proposed. The algorithms allow implementation of the classification and clustering software, which have high efficiency when working with educational data. The methods and algorithms were practically embodied in experimental software modules prototypes. Experiments proved high efficiency of the proposed methods. ??????????? ????? ????????? ????????????? ? ??????????????? ??????????? ??????????? ??????? ????????? ?????????????? ????? ????????, ??????? ???????? ?????? ??????????? ??????????? ? ????????? ?????? ???????????? ??????????????? ?????????????? ????????, ?????? ?????????????? ????????? ??????????, ??????????? ??????????? ???? ????? ?????? (?????????????, ???????????????, ???????????????????, ?????????????? ? ?. ?), ??????????? ??????????????????????? ????????????, ???????????? ?? ????????????? ?????????, ? ?????? ?????????????, ?????????????, ?????????????. ????????? ????? ????? ??????????? ?????? ??? ????????? ????? ???????? ????????? ????????????. ?????????? ???????? ?????????????? ? ?????? ????????????????? ? ???????? ????????????????? ???????????????? ??????. ?????? ????? ????? ??????????-??????????????? ??????????? ??????? ??????????????????? ?????? ????????? ??????????, ??????????????? ?? ??????????? ?????????? ???????? ???????????? ?? ???? ??????????? ?????????? ??????? ? ?????????????? ????????? ? ??? ?????? ??????????????? ?????????? ????????, ??? ????????? ??????? ????????? ?????????????? ??????????? ??????. ??????????? ????? ?????? ??????????????? ????????????? ???????????????? ?????? ??????-??????????????? ?????????????? ?? ?????? ???????? ????????????? ????????? ????? ? ?? ?????? ????????????? ?????????. ?????????? ????? ????????? ????????????? ? ????????? ???????????????? ?????????????? ????????, ??? ????????? ??????????? ???????? ????????????? ? ?????????????, ??????? ??????????????? ?????????? ?????????????? ??? ?????? ? ?????????? ???????. ???????????? ?????? ? ????????? ???? ??????????? ??????????? ? ????????????????? ?????????? ??????????????? ??????????? ???????. ??????????? ???????????? ???????????? ???????? ???????? ?? ?????????????...|$|R
40|$|<b>Abstract.</b> <b>Automatic</b> {{supervision}} of chemical dosing system {{was developed in}} fossil power plant by use of ADAM 6050 module and Advantech WebAccess. Communication configuration must be accorded with Bus protocol and device type; the system is confirmed to achieve stability and real-time performance after practical application...|$|R
