1017|222|Public
5|$|In the 2013, the perturbational {{complexity}} index (PCI) was proposed, {{a measure}} of the <b>algorithmic</b> <b>complexity</b> of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.|$|E
25|$|The Baby-step giant-step {{algorithm}} {{is often used}} to solve for the shared key in the Diffie Hellman key exchange, when the modulus is a prime number. If the modulus is not prime, the Pohlig–Hellman algorithm has a smaller <b>algorithmic</b> <b>complexity,</b> and solves the same problem.|$|E
25|$|Unlike {{groups of}} finite order, which exhibit {{complexity}} and diversity and whose first-order theory is decidable only in special cases, all finite Boolean algebras {{share the same}} theorems and have a decidable first-order theory. Instead the intricacies of Boolean algebra are divided between the structure of infinite algebras and the <b>algorithmic</b> <b>complexity</b> of their syntactic structure.|$|E
50|$|The {{decision}} {{procedures are}} not necessarily practical. The <b>algorithmic</b> <b>complexities</b> of all known decision procedures for real closed fields are very high, so that practical execution times can be prohibitively high except for very simple problems.|$|R
30|$|In this context, many {{algorithms}} {{have been}} proposed to generate association rules from large data such as the algorithms presented in “Knowledge Discovery in Database Process” section. The difference between these algorithms exists in terms of response time and memory space, these methods are distinguishable so naturally by their <b>algorithmic</b> <b>complexities</b> [7 – 10].|$|R
5000|$|... 7) Results {{addressing}} graph theoretic, <b>algorithmic,</b> and <b>complexity</b> {{issues with}} regard to tree spanners.|$|R
25|$|The {{field is}} at the {{intersection}} of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include source coding, channel coding, <b>algorithmic</b> <b>complexity</b> theory, algorithmic information theory, information-theoretic security, and measures of information.|$|E
25|$|Later, Kolmogorov {{focused his}} {{research}} on turbulence, where his publications (beginning in 1941) significantly influenced the field. In classical mechanics, he {{is best known for}} the Kolmogorov–Arnold–Moser theorem, first presented in 1954 at the International Congress of Mathematicians. In 1957, working jointly with his student V. I. Arnold, he solved a particular interpretation of Hilbert's thirteenth problem. Around this time he also began to develop, and was considered a founder of, <b>algorithmic</b> <b>complexity</b> theory – often referred to as Kolmogorov complexity theory.|$|E
2500|$|For {{dynamical}} systems, entropy {{rate and}} <b>algorithmic</b> <b>complexity</b> of the trajectories are related by a theorem of Brudno, that the equality K(x;T) = [...] h(T) holds {{for almost all}} x.|$|E
40|$|Abstract—An {{event with}} low {{probability}} {{is unlikely to}} happen, but events with low probability happen all of the time. This is because many distinct low probability events can have a large combined probability. However, some low probability events {{can be seen to}} follow an independent pattern. <b>Algorithmic</b> specified <b>complexity</b> (ASC) measures the degree to which an event is improbable and follows a pattern. We show a bound on the probability of obtaining a particular value of <b>algorithmic</b> specified <b>complexity.</b> Consequently we can say that high ASC objects are improbable. Index Terms—Keywords: specified <b>complexity,</b> <b>algorithmic</b> in-formation theory, Kolmogorov complexity I...|$|R
50|$|During 1981-1993 he was {{the head}} of Laboratory of Theory of Algorithms at the Leningrad Institute for Informatics and Automation of the USSR Academy of Sciences. From 1993 until 2009 he was a full {{professor}} of the University Paris-Est Créteil(UPEC), France, and since 2009 he remains professor emeritus of this university. At UPEC {{he was the}} head (and in a way a founder) of Laboratory for <b>Algorithmics</b> <b>Complexity</b> and Logic (LACL) that he headed from 1997 until 2007.|$|R
40|$|An ordered {{labeled tree}} is {{a tree in}} which the nodes are labeled and the left-to-right order among {{siblings}} is relevant. The edit distance between two ordered labeled trees is the minimum cost of changing one tree into the other through a sequence of edit steps. In the literature, there are a class of algorithms based on different yet closely related path-decomposition schemes. This article reviews the principles of these algorithms, and studies the concepts related to the <b>algorithmic</b> <b>complexities</b> {{as a consequence of}} the decomposition schemes. Comment: 23 pages. 13 figures. Revisions: minor change...|$|R
2500|$|Applications of {{fundamental}} topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)). [...] The field {{is at the}} intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. Its impact has been crucial {{to the success of the}} Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. Important sub-fields of information theory are source coding, channel coding, <b>algorithmic</b> <b>complexity</b> theory, algorithmic information theory, information-theoretic security, and measures of information.|$|E
2500|$|In 2009, the OEIS {{database}} {{was used}} by an amateur mathematician to measure the [...] "importance" [...] of each integer number. The result shown in the plot on the right shows a clear [...] "gap" [...] between two distinct points clouds the [...] "uninteresting numbers" [...] (blue dots) and the [...] "interesting" [...] numbers that occur comparatively more often in series from the OEIS. It contains essentially prime numbers (red), numbers of the form a^n (green) and highly composite numbers (yellow). This phenomenon was studied by Nicolas Gauvrit, Jean-Paul Delahaye and Hector Zenil who explained {{the speed of the}} 2 clouds in terms of <b>algorithmic</b> <b>complexity</b> and the gap by social factors based on an artificial preference for sequences of primes, even numbers, geometric and Fibonacci-type series and so on. Sloane's gap was featured on a Numberphile video.|$|E
2500|$|In the {{mathematical}} subject of geometric group theory, a Dehn function, named after Max Dehn, is an optimal function associated to a finite group presentation which bounds {{the area of}} a relation in that group (that is a freely reduced word in the generators representing the identity element of the group) {{in terms of the}} length of that relation (see pp.7980 in [...] ). The growth type of the Dehn function is a [...] quasi-isometry invariant of a finitely presented group. The Dehn function of a finitely presented group is also closely connected with non-deterministic <b>algorithmic</b> <b>complexity</b> of the word problem in groups. In particular, a finitely presented group has solvable word problem if and only if the Dehn function for a finite presentation of this group is recursive (see Theorem 2.1 in [...] ). The notion of a Dehn function is motivated by isoperimetric problems in geometry, such as the classic isoperimetric inequality for the Euclidean plane and, more generally, the notion of a filling area function that estimates the area of a minimal surface in a Riemannian manifold in terms of the length of the boundary curve of that surface.|$|E
5000|$|<b>Algorithmic</b> Randomness and <b>Complexity</b> (with D. Hirschfeldt, Springer, 2010) ...|$|R
50|$|Juraj Hromkovič (born 1958) is a Slovak Computer Scientist and Professor at ETH Zürich. He is {{the author}} of {{numerous}} monographs and scientific publications in the field of <b>algorithmics,</b> computational <b>complexity</b> theory, and randomization.|$|R
40|$|Finding vacua for {{the four}} {{dimensional}} effective theories for supergravity which descend from flux compactifications and analyzing them according to their stability {{is one of the}} central problems in string phenomenology. Except for some simple toy models, it is, however, difficult to find all the vacua analytically. Recently developed algorithmic methods based on symbolic computer algebra can be of great help in the more realistic models. However, they suffer from serious <b>algorithmic</b> <b>complexities</b> and are limited to small system sizes. In this article, we review a numerical method called the numerical polynomial homotopy continuation (NPHC) method, first used in the areas of lattice field theories, which by construction finds all of the vacua of a given potential that is known to have only isolated solutions. The NPHC method is known to suffer from no major <b>algorithmic</b> <b>complexities</b> and is embarrassingly parallelizable, and hence its applicability goes way beyond the existing symbolic methods. We first solve a simple toy model as a warm up example to demonstrate the NPHC method at work and compare the results with the available results from the symbolic methods. We then show that all the vacua of a more complicated model of M theory compactified on the coset SU(3) × U(1) /U(1) × U(1), which has an SU(3) structure, can be obtained by the NPHC method using a desktop machine in just about one hour, a feat which was reported to be prohibitively difficult by the existing symbolic methods. Finally, we compare the various technicalities between the two methods. Comment: 19 pages. Published in the Special Issue on Computational Algebraic Geometry in String and Gauge Theory: Advances in High Energy Physics Volume 2011 (2011...|$|R
50|$|In {{computer}} science, hardness of approximation is a {{field that}} studies the <b>algorithmic</b> <b>complexity</b> of finding near-optimal solutions to optimization problems.|$|E
50|$|The data {{compression}} ratio {{can serve as}} a measure of the complexity of a data set or signal, in particular it is used to approximate the <b>algorithmic</b> <b>complexity.</b>|$|E
5000|$|Bisection is in LSPACE {{having an}} <b>algorithmic</b> <b>complexity</b> of [...] with [...] {{denoting}} {{the number of}} revisions in the search space, and {{is similar to a}} binary search.|$|E
40|$|The Core-based {{approach}} is inevitable in multicast routing protocols as it provides efficient management of multicast path in changing group memberships, and scalability and performance. In this paper, {{we present a}} comprehensive analysis of this approach {{with the emphasis on}} core selection {{for the first time in}} literature. We first examine the evolution of multicast routing protocols into the core-based architecture and the motivation for the approach. Then we review the core-selection algorithms in the literature for their algorithmic structure and performance. Our study involves an extensive computational and message complexity analysis of each algorithm, and a classification for their deployment characteristics and <b>algorithmic</b> <b>complexities.</b> To the best of our knowledge ours is the first paper providing such extensive comparative analysis of core-based multicast routing protocols. 1...|$|R
40|$|In {{this paper}} we present <b>algorithmic</b> and <b>complexity</b> results for {{polynomial}} sign evaluation over two real algebraic numbers, and for real solving of bivariate polynomial systems. Our main tool is signed polynomial remainder sequences; we exploit {{recent advances in}} univariate root isolation as well as multipoint evaluation techniques...|$|R
40|$|The {{study of}} {{computational}} processes {{based on the}} laws of quantum mechanics {{has led to the}} discovery of new algorithms, cryptographic techniques, and communication primitives. This book explores quantum computation {{from the perspective of the}} branch of theoretical computer science known as semantics, as an alternative to the more well-known studies of <b>algorithmics,</b> <b>complexity</b> theory, and information theory. It collects chapters from leading researchers in the field, discussing the theory of quantum programming languages, logics and tools for reasoning about quantum systems, and novel approaches to the foundations of quantum mechanics. This book is suitable for graduate students and researchers in quantum information and computation, as well as those in semantics, who want to learn about a new field arising from the application of semantic techniques to quantum information and computation...|$|R
50|$|For {{dynamical}} systems, entropy {{rate and}} <b>algorithmic</b> <b>complexity</b> of the trajectories are related by a theorem of Brudno, that the equality K(x;T) = h(T) holds {{for almost all}} x.|$|E
50|$|Techniques for {{single-stepping}} {{a victim}} program include file system mazes and <b>algorithmic</b> <b>complexity</b> attacks. In both cases, the attacker manipulates the OS state to control scheduling of the victim.|$|E
5000|$|Compute Intensity, {{the number}} of {{arithmetic}} operations per I/O or global memory reference. In many signal processing applications today it is well over 50:1 and increasing with <b>algorithmic</b> <b>complexity.</b>|$|E
40|$|Classical {{probability}} theory considers probability distributions that assign probabilities to all events (at {{least in the}} finite case). However, there are natural situations where {{only part of the}} process is controlled by some probability distribution while for the other part we know only the set of possibilities without any probabilities assigned. We adapt the notions of <b>algorithmic</b> information theory (<b>complexity,</b> <b>algorithmic</b> randomness, martingales, a priori probability) to this frame work and show that many classical results are still valid...|$|R
50|$|These <b>algorithmic</b> {{measures}} of <b>complexity</b> tend to assign high values to random noise. However, those studying complex systems would not consider randomness as complexity.|$|R
40|$|This {{document}} provides {{supplementary material}} describing <b>algorithmic</b> and <b>complexity</b> details of e-CCC-Biclustering. For clarity we repeat here {{the details of}} the main steps of e-CCC-Biclustering already presented in the main manuscript. We believe that a complete description of the algorithmic details makes it easier to understand the detailed complexity analysis presented afterwards. 1 e-CCC-Biclusterin...|$|R
50|$|He is {{well known}} for his work in {{randomness}} in computing, <b>algorithmic</b> <b>complexity</b> and intractability, average-case complexity, foundations of mathematics and computer science, algorithmic probability, theory of computation, and information theory.|$|E
50|$|In particular, {{files of}} random data cannot be {{consistently}} compressed by any conceivable lossless data compression algorithm: indeed, {{this result is}} used to define the concept of randomness in <b>algorithmic</b> <b>complexity</b> theory.|$|E
50|$|More formally, the <b>Algorithmic</b> <b>Complexity</b> (AC) of {{a string}} x {{is defined as the}} length of the {{shortest}} program that computes or outputs x, where the program is run on some fixed reference universal computer.|$|E
40|$|NEST is a {{simulator}} {{for large}} networks of spiking point neurons for neuroscience research. A typical NEST simulation {{consists of two}} stages: first the network is wired up, and second {{the dynamics of the}} network is simulated. Our work aims at developing a performance model for the second stage, the simulation stage, by a semi-empirical approach. We collected measurements of the runtime performance of NEST under varying parameter settings on the JUQUEEN supercomputer at Forschungszentrum Jülich, and subsequently fitted a theoretical model to this data. This performance model defines the simulation time as weighted sum of <b>algorithmic</b> <b>complexities</b> which have been identified in the NEST source code. After parameter fitting, the coefficient of determination on the training data is close to 1. 0, and the model can be used to successfully extrapolate NEST simulation times. Furthermore, recommendations for algorithmic improvements of the NEST code can be derived from the modeling results...|$|R
40|$|In {{this paper}} we study {{combinatorial}} and <b>algorithmic</b> resp. <b>complexity</b> questions of upper domination, i. e., the maximum cardinality of a minimal dominating {{set in a}} graph. We give a full classification of the related maximisation and minimisation problems, {{as well as the}} related parameterised problems, on general graphs and on graphs of bounded degree, and we also study planar graphs...|$|R
40|$|We {{discuss some}} new <b>algorithmic</b> and <b>complexity</b> issues in coalitional and dynamic/evolutionary games, {{related to the}} understand- ing of modern selflsh and Complex networks. In particular: (a) We examine the {{achievement}} of equilibria via natural distributed and greedy approaches in networks. (b) We present {{a model of a}} coalitional game in order to capture the anarchy cost and complexity o...|$|R
