40|136|Public
2500|$|Formerly {{distributed}} nationally by NPR, WNYC began {{distributing the}} show in 2015. The change was noticeably marked by the omission of NPR's name in the show's opening <b>audio</b> <b>sequence</b> after the tagline, [...] "You're listening to Radiolab...from WNYC." ...|$|E
50|$|Parts of {{the movie}} {{were meant to be}} used for {{fundraising}} purposes in order to complete the whole project. As soon as October 1969 the film story was outpaced, though, by the Tate-Labianca murders which were carried out by members of the Manson Family in Los Angeles and shattered the American public. Morrison showed HWY during his second stay in Paris in early 1971. The film was publicly shown only once in Vancouver in 1970 and again in Paris in 1993. An <b>audio</b> <b>sequence</b> from the film was published on The Doors' spoken word album An American Prayer in 1978.|$|E
40|$|The device has a {{determining}} device (102) {{for determining}} multiple test vectors {{based on an}} <b>audio</b> <b>sequence</b> (110). A provision unit provides a code book with multiple support vectors. An approximating device approximates the test vectors based on the support vectors to approximated test vectors, where error vectors describe a deviation of the test vectors from the approximated test vectors. A detection device detects an acoustic event based on the error vectors and determines a point of time of detection of the acoustic event. Independent claims are also included for the following: (1) a method for recognizing an acoustic event in an <b>audio</b> <b>sequence</b> (2) a computer program comprising a set of instructions to execute a method for recognizing an acoustic event in an <b>audio</b> <b>sequence...</b>|$|E
50|$|Sections {{of track}} trigger various <b>audio</b> <b>{{sequence}}s</b> stored onboard 16-gig EEPROM storage, which also stores motion sequence programming.|$|R
40|$|The Problem: In this work, we {{tackle the}} problem of {{morphing}} between different <b>audio</b> <b>sequences.</b> The system should take as input 2 <b>audio</b> <b>sequences,</b> and produce as output intermediate <b>audio</b> <b>sequences</b> that represent natural exemplars lying between the 2 input <b>sequences.</b> Motivation: <b>Audio</b> morphing might have important applications in speech recognition, speech synthesis, music synthesis, and other applications where large corpora are recorded {{and there is a}} strong need to interpolate between the exemplars in the corpora to produce new exemplars. Previous Work: There has been a spate of recent work on voice conversion [1, 4, 5], where a reference speaker speech sample is warped to match the statistical properties of a target speaker. Most authors resort to mixed time- and frequency- domain methods to alter pitch, duration, and spectral features. Audio morphing [3] is closest in spirit to the goal of this work. The author used dynamic timewarping to time-align two speech samples, cross-faded the respective smoothed spectrograms, and warped a pitch residual to morph between two sounds. Approach: In this work, we aim to explore and develop a purely time-domain method of audio morphing, motivated by the success of methods such as TD-PSOLA [2] for warping <b>audio</b> <b>sequences...</b>|$|R
40|$|MPEG- 7 {{provides}} description tools for describing one multimedia document. Multimedia documents supported by MPEG- 7 {{are not only}} typical audiovisual documents such as image, videos, <b>audio</b> <b>sequences</b> and audiovisual sequences but also special types of multimedia documents such as ink content, MPEG- 4 presentations and Web pages wit...|$|R
40|$|Acoustic Event Classification (AEC) {{has become}} a {{significant}} task for machines to perceive the surrounding auditory scene. However, extracting effective representations that capture the underlying characteristics of the acoustic events is still challenging. Previous methods mainly focused on designing the audio features in a 'hand-crafted' manner. Interestingly, data-learnt features have been recently reported to show better performance. Up to now, these were only considered on the frame-level. In this paper, we propose an unsupervised learning framework to learn a vector representation of an <b>audio</b> <b>sequence</b> for AEC. This framework consists of a Recurrent Neural Network (RNN) encoder and a RNN decoder, which respectively transforms the variable-length <b>audio</b> <b>sequence</b> into a fixed-length vector and reconstructs the input sequence on the generated vector. After training the encoder-decoder, we feed the audio sequences to the encoder and then take the learnt vectors as the <b>audio</b> <b>sequence</b> representations. Compared with previous methods, the proposed method can not only {{deal with the problem}} of arbitrary-lengths of audio streams, but also learn the salient information of the sequence. Extensive evaluation on a large-size acoustic event database is performed, and the empirical results demonstrate that the learnt <b>audio</b> <b>sequence</b> representation yields a significant performance improvement by a large margin compared with other state-of-the-art hand-crafted sequence features for AEC...|$|E
30|$|We have {{replaced}} some {{words in the}} speech at different positions, for a total tampering length of 3.75 seconds (about 11.7 % of the total length of the <b>audio</b> <b>sequence).</b>|$|E
40|$|Abstract—In this paper, {{we present}} an {{optimization}} framework for transmitting high quality audio sequences over error-prone wireless links. Our framework introduces apparatus and technique to optimally protect a stored <b>audio</b> <b>sequence</b> transmitted over a wireless link while considering the packetization overhead of audio frames. Utilizing rate compatible punctured RS codes and dynamic program-ming, it identifies the optimal assignment of parity to audio frames {{according to their}} perceptual importance such that the Segmented SNR of the received <b>audio</b> <b>sequence</b> is maximized. Our framework covers two cases. In the first case, a frame grouping technique is proposed to packetize audio frames and protect them against temporarily corre-lated bit errors introduced by a fading wireless channel. In this case, each packet is treated as a channel coding codeword. In the second case, a one-dimensional RS coder is applied vertically to a sequence of horizontally formed packets associated with an <b>audio</b> <b>sequence</b> {{in order to protect}} the sequence against both bit errors introduced by fading wireless channels and packet erasures introduced by network buffering. Our numerical results capture the performance advantage of our framework compared to existing techniques proposed in the literature of audio transmission. We also note that our framework can be generically applied to a variety of audio coders making it attractive in terms of implementation...|$|E
40|$|Cambience is a sonic media space. Using {{a visual}} {{programming}} environment, people can map properties {{of a video}} stream into <b>audio</b> <b>sequences,</b> thus delivering sonic awareness cues of presence and activities to distance collaborators without the visual distraction. This demonstration illustrates how a Cambience sonic ecology is created...|$|R
50|$|Nuendo is {{an audio}} {{software}} product developed by Steinberg for music recording, arranging, editing and post-production {{as part of}} a Digital Audio Workstation. The package is aimed at audio and video post-production market segments, but it also contains optional modules {{that can be used for}} multiple multimedia creation and <b>audio</b> <b>sequencing.</b>|$|R
2500|$|... 8-bit Atari {{computers}} {{used the}} opening {{bars of the}} Promenade in their <b>audio</b> self-test <b>sequence.</b>|$|R
40|$|An audio {{segmentation}} {{method and}} system which automatically segments an <b>audio</b> <b>sequence</b> into audio scenes of similar semantic content is described. The method and system initially splits the <b>audio</b> <b>sequence</b> into segments of arbitrary length (step 101). Next, each segment {{is subject to}} short term spectral analysis (step 102) to generate feature vectors characterising the audio. A vector quantisation (VQ) technique is used to generate a signature codeboook using the feature vectors of the audio segments (step 103). An Earth Mover's Distance (EMD) measure is then used to calculate distances between consecutive audio segments (step 104). By statistically analysing the respective (EMD) measures to identify peaks therein, changes in the dominant audio content can be detected indicative of audio scene changes (step 105). In this way, {{it is possible to}} automate the time­consuming and laborious process of organising and indexing increasingly large audio databases such that they can be easily browsed and searched using natural query structures...|$|E
40|$|Cover song {{detection}} {{is becoming}} a very hot research topic when plentiful personal music recordings or performance are released on the Internet. A nice cover song recognizer helps us group and detect cover songs to improve the searching experience. The traditional detection is to match two musical audio sequences by exhaustive pairwise comparisons. Different from the existing work, our aim is to generate a group of concatenated feature sets based on regression modeling and arrange them by indexing-based approximate techniques to avoid complicated <b>audio</b> <b>sequence</b> comparisons. We mainly focus on using Exact Locality Sensitive Mapping (ELSM) to join the concatenated feature sets and soft hash values. Similarity-invariance among <b>audio</b> <b>sequence</b> comparison is applied to define an optimal combination of several audio features. Soft hash values are pre-calculated to help locate searching range more accurately. Furthermore, we implement our algorithms in analyzing the real audio cover songs and grouping and detecting a batch of relevant cover songs embedded in large audio datasets. © 2008 IEEE...|$|E
40|$|Sample-based music {{composition}} {{often involves}} {{the task of}} manually searching appropriate samples from existing audio. Audio mosaicing {{can be regarded as}} a way to automatize this process by specifying the desired audio attributes, so that sound snippets that match these attributes are concatenated in a synthesis engine. These attributes are typically derived from a target <b>audio</b> <b>sequence,</b> which might limit the musical control of the user. In our approach, we replace the target <b>audio</b> <b>sequence</b> by a symbolic sequence constructed with pre-defined sound object categories. These sound objects are extracted by means of automatic classification techniques. Three steps are involved in the sound object extraction process: supervised training, automatic classification and user-assisted selection. Two sound object categories are considered: percussive and noisy. We present an analysis/synthesis framework, where the user explores first a song collection using symbolic concepts to create a set of sound objects. Then, the selected sound objects are used in a performance environment based on a loop-sequencer paradigm. ...|$|E
40|$|Wepresent a {{framework}} for the analysis and synthesis of acoustical instruments based on data-driven probabilistic inference modeling. Audio time series and boundary conditions of a played instrument are recorded and the non-linear mapping from the control data into the audio space is inferred using the general inference framework of Cluster-Weighted Modeling. The resulting model is used for real-time synthesis of <b>audio</b> <b>sequences</b> from new input data...|$|R
50|$|The {{structure}} {{of the show is}} a pioneering format on India's relatively young Metro network culture. Both in terms of content and form, U-Special is unlike previous televised youth forums in India. A segmented packaged aesthetic of capsulated realism in the everyday life of university in Delhi, is intercut with underlaid <b>audio</b> <b>sequencing</b> and a montaged para-narrative that projects a metamorphosed half-hour of televised campus trends.|$|R
40|$|Abstract: This paper {{describes}} {{the implementation of}} software tools that provide a complete virtual editing multimedia streaming broadcast information from geographically dispersed servers and presented in various formats. This software is able to operate in environments of all major browsers and automatically identify {{the format of the}} next incoming flow and supporting the playback of the resulting video and <b>audio</b> <b>sequences</b> without delay between the fragments. Note: Publication language:russia...|$|R
40|$|The vector {{representations}} of fixed dimensionality for words (in text) offered by Word 2 Vec {{have been shown}} to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word 2 Vec. It offers the vector {{representations of}} fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word 2 Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input <b>audio</b> <b>sequence</b> into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input <b>audio</b> <b>sequence.</b> The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning...|$|E
30|$|Studies {{show that}} {{presenting}} visual and audio information together enhances perception {{in comparison to}} presenting only visual information [4, 5]. Adding audio ambiance for the specific location, even having different ambiances depending {{on the time of}} the day, would enhance the user experience in the virtual location-exploration services. A few works that combine visual and audio representations for a location suggest using abstract or natural sounds for representing additional information [6, 7] or using characteristic <b>audio</b> <b>sequence</b> for mapping environmental noise into city models [8]. However, none of these prior works use specific audio ambiances recorded at the location of interest.|$|E
40|$|Here we {{describe}} {{an approach to}} the expressive synthesis of jazz saxophone melodies that reuses audio recordings and carefully concatenates note samples. The aim is to generate an expressive <b>audio</b> <b>sequence</b> from the analysis of an arbitrary input score using a previously induced performance model and an annotated saxophone note database extracted from real performances. We push {{the idea of using}} the same corpus for both inducing an expressive performance model and synthesizing sound by concatenating samples in the corpus. Therefore, a connection between the performers’ instrument sound and performance characteristics is kept during the synthesis process...|$|E
50|$|In this section, the {{candidate}} is presented with several recordings {{to test the}} listening comprehension and lasts for 50 min (40 min working time + 10 min transfer time). The three individual parts of the listening section include a dialogue of ca. 700 words, a monologue of ca. 700 words, and four <b>audio</b> <b>sequences</b> of ca. 100 words. The candidate is asked to fill out comprehension questions regarding the selections played (multiple choice, mapping).|$|R
40|$|International audienceMODIS is a {{free speech}} and audio motif {{discovery}} software developed at IRISA Rennes. Motif discovery is the task of discovering and collecting occurrences of repeating patterns {{in the absence of}} prior knowledge, or training material. MODIS is based on a generic approach to mine repeating <b>audio</b> <b>sequences,</b> with tolerance to motif variability. The algorithm implementation allows to process large audio streams at a reasonable speed where motif discovery often requires huge amount of time...|$|R
40|$|This paper reports {{two studies}} that {{measured}} {{the effects of}} different “video skim ” techniques on comprehension, navigation, and user satisfaction. Video skims are compact, content-rich abstractions of longer videos, condensations that preserve frame rate while greatly reducing viewing time. Their characteristics depend on the image- and audio-processing techniques used to create them. Results from the initial study helped refine video skims, which were then reassessed in the second experiment. Significant benefits were found for skims built from <b>audio</b> <b>sequences</b> meeting certain criteria...|$|R
40|$|In this paper, {{we propose}} a {{statistical}} optimization framework for transmitting audio sequences over wireless links. Our proposed framework protects audio frames against both temporally cor-related random bit errors introduced by a fading channel and packet erasures caused by network buffering. Forming a two-dimensional grid of symbols, our framework forms horizontal packets that are compensated only vertically against {{both types of}} errors. The utilized one-dimensional error correction coding scheme of our framework assigns parity bits according to the perceptual importance of frames such that the Segmented SNR of a received <b>audio</b> <b>sequence</b> is maximized. In addition, the proposed framework suggests an effective way of reducing the packetization over-head of small audio frames. I...|$|E
40|$|In {{this paper}} we {{formulate}} {{the problem of}} synthesizing facial animation from an input <b>audio</b> <b>sequence</b> (a. k. a. video rewrite, voice puppetry) as dynamic audio/visual mapping. We propose that audio/visual mapping should be modeled with an input-output hidden Markov model, or IOHMM. An IOHMM is an HMM for which the emission and transition probabilities are conditional on the input sequence. We train IOHMMs using the expectation-maximization (EM) algorithm with a novel architecture to explicitly model the relationship between each transition probability and the input using a neural network. Given an input sequence, an output sequence is synthesized by a maximum likelihood estimation. Experimental results demonstrate that IOHMMs can generate good-quality and natural facial animation sequences from input audio. 1...|$|E
40|$|Multimedia content still {{presented}} {{on the web}} sites. The visualization of multimedia content by the users with disabilities, those that usually use screen readers, is extremely diffi-cult. With {{the onset of the}} <b>audio</b> <b>sequence</b> of multimedia presentation it is difficult for users with visual impairs to listen the audio component of presentation and the audio version of the screen readers too, because the two audio streams cannot be controlled using only one vo-lume control. Therefore, because of the difficulties to control the available audio streams and because of the difficulties to access the control buttons by people with disabilities, the multi-media content is often inaccessible for users with visual problems. More than this, the use of dynamic users ’ interfaces is a critical problem because the screen-readers cannot detect the dynamics in content changes. The current paper presents some solutions for multimedia content production and distribution in distributed multimedia web presentations...|$|E
50|$|Oldfield used Emagic Logic <b>Audio</b> for <b>sequencing</b> and Pro Tools {{hardware}} for {{the recording}} of the album {{using a combination of}} tape and hard drive recording.|$|R
40|$|Vehicles may be {{recognized}} {{from the sound}} they emit when driving along a road. Characteristic acoustic finger prints and audio features {{can be used to}} increase the ro-bustness of existing video based vehicle tracking and clas-sification algorithms. Using this information in a multi-sensor surveillance system helps to improve various param-eters such as recognition rates, detection times and robust-ness. We propose a two-fold approach, where vehicle detec-tion and classification are handled separately. We demon-strate the feasibility of the proposed method using outdoor <b>audio</b> <b>sequences</b> of traffic situations. 1...|$|R
40|$|Despite the {{multiplicity}} of data types and rich linking and nesting available in general multimedia systems, most digital video systems have represented video only as linear sequences of frames and shots. We extend previous work that proposed representing digital video as hierarchically structured documents composed of modular building blocks including outlines, scripts, <b>audio</b> <b>sequences,</b> still images, titles, and motion sequences. We review how such a representation can aid video authoring. We then show how such structure can aid video editing, localizing, browsing, updating, publishing, navigating, and searching. Applications are illustrated with examples from real projects...|$|R
40|$|Automatic text {{analysis}} widened {{the perspective of}} work on document contents by opening up the studies on the linguistic productions. In this case, we are using annotation as a case study. In our approach, annotation is defined as textual, graphic or sound information, attached to document source (text, photo, <b>audio</b> <b>sequence</b> or video sequence : multimedia). The source of our corpus is from INA databases (ie. Institut National de l'Audiovisuel, Paris). Our research task consisted of identifying what are the appropriate characteristics of a multimedia document, its context and information retrieval {{in the context of}} natural language processing (NLP), automatic indexing and knowledge representation. We discuss the crucial role of annotation process in the Knowledge Extraction tools and Management {{as well as in the}} design of Information Retrieval Systems. Our focus is more specifically on the new approach in information system design dedicated to “economic intelligence”...|$|E
40|$|Abstract Until fairly recently, medical {{publications}} {{have been}} handicapped by being restricted to non-electronic formats, effectively preventing {{the dissemination of}} complex audiovisual and three-dimensional data. However, authors and readers could significantly profit from advances in electronic publishing that permit the inclusion of multimedia content directly into an article. For the first time, the de facto gold standard for scientific publishing, the portable document format (PDF), is used here as a platform to embed a video and an <b>audio</b> <b>sequence</b> of patient data into a publication. Fully interactive three-dimensional models of a face and a schematic representation of a human brain {{are also part of}} this publication. We discuss the potential of this approach {{and its impact on the}} communication of scientific medical data, particularly with regard to electronic and open access publications. Finally, we emphasise how medical teaching can benefit from this new tool and comment on the future of medical publishing. </p...|$|E
40|$|In this {{invention}} we {{introduce a}} method and system for segmenting both a video sequence and an accompanying <b>audio</b> <b>sequence</b> into respective {{video and audio}} scenes. To allow for ready integration of the audio and video scene segmentation information {{the concept of an}} audio shot is introduced corresponding to readily identifiable video shots. By synchronising the audio stream with the video stream on the basis of corresponding audio shots and video shots the integration of respective determined audio and video semantic scene information (a scene comprising one or more shots) becomes relatively straightforward, thus giving richer semantic understanding of the content. Moreover the fusion of audio and visual analysis results through heuristic rules also provides further advantages. The industrial applicability of the invention is in the field of automating the time­ consuming and laborious process of organising and indexing increasingly large video databases such that they can be easily browsed and searched using natural query structures that are close to human concepts...|$|E
40|$|We {{formulate}} {{alignment of}} multiple and partially overlapping <b>audio</b> <b>sequences</b> in a probabilistic framework. We define and compare four generative models for time varying features extracted from audio clips that are recorded independently and asynchronously. We {{are able to}} handle missing data and multiple clips where no clip is covering the entire material. We define proper scoring functions for each model and the matching is achieved with a sequential alignment algorithm. The simulation results on real {{data suggest that the}} approach is able to handle difficult ambiguous scenarios or partial matchings...|$|R
40|$|Abstract—The {{discovery}} of repeated structure, i. e. motifs/nearduplicates, {{is often the}} first step in exploratory data mining. As such, the last decade has seen extensive research efforts in motif discovery algorithms for text, DNA, time series, protein sequences, graphs, images, and video. Surprisingly, there has been less attention devoted to finding repeated patterns in <b>audio</b> <b>sequences,</b> in spite of their ubiquity in science and entertainment. While there is significant work for the special case of motifs in music, virtually all this work makes many assumptions about data (often {{to the point of being}} genre specific) and thus these algorithms do not generalize to <b>audio</b> <b>sequences</b> containing animal vocalizations, industrial processes, or a host of other domains that we may wish to explore. In this work we introduce a novel technique for finding audio motifs. Our method does not require any domainspecific tuning and is essentially parameter-free. We demonstrate our algorithm on very diverse domains, finding audio motifs in laboratory mice vocalizations, wild animal sounds, music, and human speech. Our experiments demonstrate that our ideas are effective in discovering objectively correct or subjectively plausible motifs. Moreover, we show our novel probabilistic early abandoning approach is efficient, being two to three orders of magnitude faster than brute- force search, and thus faster than real-time for most problems. Keywords-audio motif; spectrogram; anytime algorithm I...|$|R
40|$|An {{efficient}} audio indexing and retrieval {{algorithm is}} proposed to locate similar audio segments in the database. A new boundary detection technique based on audio shot is proposed for audio segmentation. Subsequently, a new method is employed to convert the <b>audio</b> shot <b>sequence</b> to <b>audio</b> word <b>sequence,</b> which utilizes a self-learning audio shot dictionary. We also borrow the idea of inverted file from text retrieval to locate candidates efficiently. Furthermore, a similarity measure combining content and temporal order matching is proposed. Experiment results show a retrieval precision of 94. 70 % within an average response time of 6. 344 seconds...|$|R
