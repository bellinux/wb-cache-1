323|0|Public
50|$|The {{specific}} use cases and characteristics {{is an active}} research field in bibliometrics, providing the much needed data to measure the impact of <b>altmetrics</b> itself. Public Library of Science has an <b>Altmetrics</b> Collection and both the Information Standards Quarterly and the Aslib Journal of Information Management recently published special issues on <b>altmetrics.</b> A series of articles that extensively reviews <b>altmetrics</b> was published in late 2015.|$|E
5000|$|<b>Altmetrics</b> are a {{very broad}} group of metrics, {{capturing}} various parts of impact a paper or work can have. A classification of <b>altmetrics</b> was proposed by ImpactStory in September 2012, and a very similar classification {{is used by the}} Public Library of Science: ...|$|E
50|$|While {{the concept}} of <b>altmetrics</b> is questioned, the {{interpretation}} of <b>altmetrics</b> in particular is discussed. Proponents of <b>altmetrics</b> make clear {{that many of the}} metrics show influence or engagement, rather than impact on the progress of science. It should be noted that even citation-based metrics do not indicate if a high score implies a positive impact on science; that is, papers are also cited in papers that disagree with the cited paper, an issue for example addressed by the Citation Typing Ontology project.|$|E
50|$|<b>Altmetrics</b> can be gamed: for example, {{likes and}} {{mentions}} can be bought. <b>Altmetrics</b> {{can be more}} difficult to standardize than citations. One example is the number of tweets linking to a paper where the number can vary widely depending on how the tweets are collected.|$|E
5000|$|... #Caption: The {{original}} logotype {{from the}} <b>Altmetrics</b> Manifesto.|$|E
50|$|In {{scholarly}} and scientific publishing, <b>altmetrics</b> are non-traditional metrics proposed {{as an alternative}} to more traditional citation impact metrics, such as impact factor and h-index. The term <b>altmetrics</b> was proposed in 2010, as a generalization of article level metrics, and {{has its roots in the}} #altmetrics hashtag. Although <b>altmetrics</b> are often thought of as metrics about articles, they can be applied to people, journals, books, data sets, presentations, videos, source code repositories, web pages, etc. They are related to Webometrics, which had similar goals but evolved before the social web. <b>Altmetrics</b> did not originally cover citation counts. It also covers other aspects of the impact of a work, such as how many data and knowledge bases refer to it, article views, downloads, or mentions in social media and news media.|$|E
5000|$|<b>Altmetrics</b> - Rate {{journals}} {{based on}} scholarly references added to academic social media sites.|$|E
50|$|Hence, {{this school}} argues {{that there are}} faster impact {{measurement}} technologies that can account {{for a range of}} publication types as well as social media web coverage of a scientific contribution to arrive at a complete evaluation of how impactful the science contribution was. The gist of the argument for this school is that hidden uses like reading, bookmarking, sharing, discussing and rating are traceable activities, and these traces can and should be used to develop a newer measure of scientific impact. The umbrella jargon for this new type of impact measurements is called <b>altmetrics,</b> coined in a 2011 article by Priem et al., (2011). Markedly, the authors discuss evidence that <b>altmetrics</b> differ from traditional webometrics which are slow and unstructured. <b>Altmetrics</b> are proposed to rely upon a greater set of measures that account for tweets, blogs, discussions, and bookmarks. The authors claim that the existing literature has often proposed that <b>altmetrics</b> should also encapsulate the scientific process, and measure the process of research and collaboration to create an overall metric. However, the authors are explicit in their assessment that few papers offer methodological details as to how to accomplish this. The authors use this and the general dearth of evidence to conclude that {{research in the area of}} <b>altmetrics</b> is still in its infancy.|$|E
5000|$|Article impact metrics e.g. <b>altmetrics</b> are {{included}} when available to allow readers to view article data ...|$|E
50|$|In 2008, the Journal of Medical Internet Research {{started to}} {{systematically}} collect tweets about its articles. Starting in March 2009, the Public Library of Science also introduced article-level metrics for all articles. Funders have started showing interest in alternative metrics, including the UK Medical Research Council. <b>Altmetrics</b> {{have been used}} in applications for promotion review by researchers. Furthermore, several universities, including the University of Pittsburgh are experimenting with <b>altmetrics</b> at an institute level.|$|E
5000|$|Neylon {{advocates for}} the use of <b>altmetrics</b> in {{determining}} the impact of scholarly publications. Neylon is part of Flooved Advisory Board.|$|E
5000|$|Zhdanov Renad I., <b>altmetrics</b> of {{scientists}} at Kazan University, Russian FederationKazan and Volga Region Scientists: Ranking according Google Scholar Citations http://www.webometrics.info/en/node/60 ...|$|E
50|$|The {{article has}} been {{downloaded}} over 111,000 times and ranks in the top 5% of all research output, as scored using <b>Altmetrics.</b>|$|E
50|$|Another {{source of}} {{objections}} against <b>altmetrics,</b> or any metrics, is how universities are using metrics to rank their employees, and the aim should limited to measure engagement.|$|E
5000|$|Important in {{determining}} the relative impact of a paper, a service that calculates <b>altmetrics</b> statistics needs a considerably sized knowledge base. The following table shows the number of papers covered by services: ...|$|E
50|$|A term first coined in the <b>altmetrics</b> {{manifesto}} in 2010, <b>altmetrics</b> (also {{known as}} ‘alternative metrics’) {{were developed to}} provide authors and other stakeholders a more comprehensive record of engagement with scholarly work, particularly that which takes place beyond the academy amongst a broader audience.In order to do this, Altmetric tracks a range of online sites and sources looking for ‘mentions’ (links or written references) to scholarly outputs (which include journal articles, blogs, data sets and more). Sources of the attention include the mainstream media, public policy documents, social and academic networks, post-publication peer-review forums and, more recently, Wikipedia and the Open Syllabus Project.|$|E
50|$|Projects such as ImpactStory, {{and various}} companies, {{including}} Altmetric, and Plum Analytics are calculating <b>altmetrics.</b> Several publishers have started providing such information to readers, including BioMed Central, Public Library of Science (PLOS), Frontiers, Nature Publishing Group, and Elsevier.|$|E
5000|$|Every {{research}} article on ScienceOpen has a traceable genealogy through citations, a public peer review process, and social interaction tracked by <b>altmetrics,</b> which they call research [...] "context". The technology behind the ScienceOpen platform {{is provided by}} Ovitas.|$|E
50|$|However, {{it is also}} {{observed}} that an article needs little attention to jump to the upper quartile of ranked papers, suggesting that not enough sources of <b>altmetrics</b> are currently available to give a balanced picture of impact {{for the majority of}} papers.|$|E
5000|$|PLOS Biology uses [...] "Article Level Metrics" [...] (a {{suite of}} <b>altmetrics)</b> {{to provide a}} measure of the impact of their {{published}} articles. PLOS Biology articles display numbers of page views, downloads, citations, social bookmarking and dissemination activity, media and blog coverage.|$|E
50|$|Besides the {{traditional}} metrics based on citations in scientific literature, for example as obtained from Google Scholar, CrossRef, PubMed Central, and Scopus, <b>altmetrics</b> also adopts citations in secondary and other knowledge sources. For example, ImpactStory counts {{the number of}} times a paper has been referenced by Wikipedia.|$|E
50|$|The {{score on}} each field does not {{directly}} tell you anything about the quality or impact of the paper. For example, a much discussed paper may merely be very controversial: papers discussed on Retraction Watch will typically get high <b>altmetrics</b> score, despite being retracted from the literature.|$|E
50|$|Semantometrics is a {{tool for}} {{evaluating}} research. It is functionally an extension of tools such as Bibliometrics, Webometrics, and <b>Altmetrics,</b> but instead of just evaluating citations - which entails relying on outside evidence - it uses a semantic evaluation of {{the full text of}} the research paper being evaluated.|$|E
50|$|Traditionally, {{bibliometrics}} {{have been}} used to evaluate the usage and impact of research, but have usually been focused on journal-level metrics such as the impact factor or researcher-level metrics such as the h-index. Article-level metrics, on the other hand, may demonstrate the impact of an individual article. This is related to, but distinct from, <b>altmetrics.</b>|$|E
5000|$|The Winnower is a {{publishing}} platform and journal that offers traditional scholarly publishing tools (Digital Object Identifiers (DOIs), permanent archival, <b>Altmetrics,</b> PDF creation, etc.) to enable rigorous scholastic discussion of topics across {{all areas of}} intellectual inquiry, whether in the sciences, humanities, public policy, or otherwise. [...] The Winnower currently publishes and archives the following: ...|$|E
50|$|Altmetric {{was founded}} by Euan Adie in 2011. Previously a researcher, Adie had already worked on Postgenomic.com, an open source {{scientific}} blog aggregator founded in 2006. In 2011 Adie entered an <b>altmetrics</b> app into Elsevier’s Apps for Science competition and won. The prize money enabled Altmetric to develop a full version of the Altmetric Explorer, released in February 2012.|$|E
50|$|Altmetric, or altmetric.com, is a {{data science}} company that tracks where {{published}} research is mentioned online, and provides tools {{and services to}} institutions, publishers, researchers, funders and other organisations to monitor this activity, {{commonly referred to as}} <b>altmetrics.</b> Altmetric was recognized by European Commissioner Máire Geoghegan-Quinn in 2014 as a company challenging the traditional reputation systems.|$|E
50|$|Starting in March 2009, the Public Library of Science {{introduced}} article-level metrics for all articles.The {{open access}} publisher PLOS provides article level metrics {{for all of}} its journals including downloads, citations, and <b>altmetrics.</b> In March 2014 it was announced that COUNTER statistics, which measure usage of online scholarly resources, are now available at the article level.|$|E
50|$|<b>Altmetrics</b> {{for more}} recent {{articles}} may be higher {{because of the}} increasing uptake of the social web and because articles may be mentioned mainly when they are published. As a result, it is not fair to compare the altmetric scores of articles unless they {{have been published in}} the same year and, especially for fast increasing social web sites, at similar times in the same year.|$|E
50|$|As {{with its}} sister forums, all content on MSDF is {{provided}} {{free of charge}} to the research community, and editorial independence from sponsors and donors is strictly maintained. MSDF articles have unique digital object identifiers (DOI) to provide stable linking over time and to facilitate discussion and <b>altmetrics</b> tracking of scientific articles in social media forums, s. MSDF articles are indexed by Google News.|$|E
50|$|Kudos {{partnered with}} Thomson Reuters to add {{citation}} data to author dashboards in October 2014 utilizing citation data from Web of Science; in May 2016, {{the company announced}} that it would also integrate with Thomson Reuters' ScholarOne manuscript submission and peer review system. In January 2017, manuscript submission integration was extended to include Aries' Editorial Manager. Kudos also integrates data from <b>altmetrics</b> provider Altmetric.com.|$|E
5000|$|ImpactStory {{is an open}} source, web-based {{tool that}} {{provides}} <b>altmetrics</b> to help researchers measure and share the impacts of all their research outputs—from traditional ones such as journal articles, to alternative research outputs such as blog posts, datasets, and software. It aims to change {{the focus of the}} scholarly reward system to value and encourage web-native scholarship. ImpactStory is a nonprofit organisation funded by the Alfred P. Sloan Foundation [...] and the National Science Foundation.|$|E
50|$|The {{usefulness}} of metrics for estimating impact is controversial, but the community shows a clear need: funders demand measurables {{on the impact}} of their spending. Limitations that affect the usefulness due to heterogeneity, data quality and particular dependencies have been studied. Like other metrics, <b>altmetrics</b> are prone to self-citation, gaming, and other mechanisms to boost one's apparent impact. Additionally, {{it has been argued that}} the currently adopted metrics are suggestive of positive impact, while negative metrics are equally important.|$|E
50|$|Located on {{the third}} floor of the Main Library, the Scholarly Commons is a hub for digital and data-based research. The space {{includes}} software packages and hardware for data visualization, manipulation, and analysis, including ArcGIS, R, and Atlas.ti. In addition to tools, many experts staff the Scholarly Commons. These experts have experience and knowledge in digitization, metadata creation and schemas, copyright law, statistical analysis, database design, and digital humanities. All experts are available to provide consultation to Illinois affiliates needing assistance in their area. Additionally, Scholarly Commons staff manage IDEALS, the Library's university repository and host events on a variety of topics, including <b>altmetrics</b> and the non-academic job search.|$|E
50|$|Pacifica’s PlumX {{installation}} {{provides a}} story of Pacifica’s faculty scholarship and publication impact, by means of <b>altmetrics.</b> Profiles for core, distinguished, and visiting faculty provide cumulative insights into how faculty research is interacted with, shared, commented on, purchased, and disseminated in both print and electronic format. PlumX gathers and brings together metrics {{for all types of}} scholarly research output, including published books, journal articles, conference papers, videos, data sets, interviews, and other online web resources. Pacifica’s PlumX installation categorizes Pacifica metrics into 5 separate types: usage (e.g. downloads and library holdings), captures (e.g. bookmarks, favorites, readers, watchers), mentions (e.g. blog posts, comments, reviews), social media (e.g. likes, shares, tweets), and citations (how many times research has been cited).|$|E
5000|$|Figshare was {{launched}} in January 2011 by Mark Hahnel and has been supported by Digital Science since a January 2012 relaunch. Hahnel first developed the platform as a personal custom solution for the organization and publication of diverse research products generated {{in support of his}} PhD in stem cell biology. [...] In January 2013 Figshare announced a partnership with PLOS to integrate Figshare data hosting, access, and visualization with their associated PLOS articles. In September 2013, the service launched an institutional repository service, which offers organizations a pre-developed infrastructure for hosting academic materials generated by their member communities. [...] In December 2013, they announced integration with ImpactStory to support the collection of <b>altmetrics.</b> Figshare made 200,000 files publicly available in its first year, which grew to approximately one million objects by September 2013.|$|E
