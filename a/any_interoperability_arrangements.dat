0|42|Public
40|$|The aim of {{this study}} is to {{investigate}} the requirements and challenges for the development of INSPIRE Invoke services specifications. Invoke services provide a mean for invoking INSPIRE spatial data services, thus the dependencies between the Invoke and spatial data services are reflected in this study. In particular, the spatial data service <b>interoperability</b> <b>arrangements</b> study has recently started and here we anticipate some assumptions on spatial data service characteristics that will be used to define the context within which the Invoke development issues will be analyzed. Consequently this study will be validated, and eventually updated, against the spatial data service <b>interoperability</b> <b>arrangements</b> once it will be released. JRC. H. 6 -Spatial data infrastructure...|$|R
40|$|This Call for Participation (CFP) seeks {{participants}} in a coordinated Geospatial Architecture Implementation Pilot. A Pilot is a collaborative effort that applies open standards for interoperability to achieve user objectives in an environment representative of operational use. Outcomes include best-practices and <b>interoperability</b> <b>arrangements</b> suitable for an operational capability. An aim of the Pilot is to reach consensus on architectural elements that initiatives supporting geospatial information systems can carry forward into operations, thereby increasing the overall level of interoperability. This CFP seeks proposals from organizations involved with Earth Observation systems to: Identify components with services, e. g., portals, catalogs and other services; Participate in confirming the interoperability of those identified services using standards and <b>interoperability</b> <b>arrangements</b> as identified in the preliminary architecture of this CFP; and, Participate in the collaborative development of societal benefit scenarios to guide testing and demonstrations of the identified interoperable services...|$|R
50|$|Because OAuth 2.0 {{is more of}} a {{framework}} than a defined protocol, one OAuth 2.0 implementation is less likely to be naturally interoperable with another OAuth 2.0 implementation. Further deployment profiling and specification is required for <b>any</b> <b>interoperability.</b>|$|R
50|$|However, 10 Gigabit Ethernet {{does not}} {{explicitly}} provide <b>any</b> <b>interoperability</b> at the bitstream level with other SDH/SONET systems. This differs from WDM system transponders, including both coarse and dense wavelength-division multiplexing systems (CWDM and DWDM) that currently support OC-192 SONET signals, which can normally support thin-SONET-framed 10 Gigabit Ethernet.|$|R
50|$|Joint defence {{arrangements}} involving {{both countries}} include the Five Power Defence Arrangements, ANZUS, and the UK-USA Security Agreement for intelligence sharing. Since 1964, Australia, and since 2006, New Zealand have been {{parties to the}} ABCA <b>interoperability</b> <b>arrangement</b> of national defence forces. ANZUK was a tripartite force formed by Australia, New Zealand and the United Kingdom to defend the Asian Pacific region after the United Kingdom withdrew forces from the east of Suez in the early seventies. The ANZUK force was formed in 1971 and disbanded in 1974. The SEATO anti-communist defence organisation also extended membership to both countries {{for the duration of}} its existence from 1955 to 1977.|$|R
40|$|Many {{securities}} and derivatives markets, including most that are traded on an exchange, {{are served by}} a central counterparty (CCP). After trades are executed, the CCP inserts itself between both trading counterparties, {{to protect them from}} the risk that one defaults before the obligations are settled. CCP <b>interoperability</b> is an <b>arrangement</b> that links different CCPs, allowing participants of one CCP to seamlessly deal with participants of another CCP. This can make it cheaper for traders to participate in a wider range of financial markets, and can facilitate competition between CCPs by opening up participant networks. However, interoperability also introduces financial stability risks, primarily by creating dependencies between the linked CCPs, and so it may be unsuitable for some markets. <b>Interoperability</b> <b>arrangements</b> are currently in place between some CCPs serving European equity markets, and another type of arrangement is in place linking several US CCPs. There are currently no links involving Australian CCPs, although the evolving CCP landscape may encourage links of some form in the future...|$|R
50|$|New Zealand and Britain {{are both}} {{members of a}} number of {{international}} bodies, including the United Nations, the Commonwealth of Nations and the OECD. Defence arrangements involving both Britain and New Zealand include the Five Power Defence Arrangements, and the UK-USA Security Agreement for intelligence sharing. Since 2006, New Zealand has been a party to the ABCA <b>interoperability</b> <b>arrangement</b> of national defence forces, which has always included Britain. ANZUK was a tripartite force formed by Australia, New Zealand and Britain to defend the Asian Pacific region after Britain withdrew forces from the east of Suez in the early seventies. The ANZUK force was formed in 1971 and disbanded in 1974. The SEATO anti-communist defence organisation also extended membership to both countries for the duration of its existence from 1955 to 1977.|$|R
50|$|The two {{countries}} and their colonial precursors have enjoyed unbroken friendly diplomatic relations {{over the entire}} period of their coexistence from {{the early nineteenth century}} up to the present. They are founding and continuing United Nations member states and they were formerly founding members of the League of Nations carrying through for the entire period until its dissolution. There is otherwise a high degree of commonality between Australia's international organisation memberships and those of New Zealand, although New Zealand may have cause for envy of Australia's acceptance to membership of the G20. There is a high degree of commonality in their co-membership of international organisations and their coparticipation as signatories of multilateral treaties of significance. They are conjoint members of a number of influential trade blocs, forums, military alliances, sharing and <b>interoperability</b> <b>arrangements,</b> and regional associations.|$|R
40|$|International audienceWithin the Architecture Implementation Pilot (AIP- 3) of GEOSS, we have {{developed}} a scenario called "environmental impact assessment of the production, transportation and use of energy for the photovoltaic (PV) sector through Life Cycle Assessment (LCA) ". It aims at providing decision-makers and policy-planners with reliable and geo-localized knowledge of several impacts induced by various technologies of the PV sector. The scenario is implemented in the GEOSS Common Infrastructure (GCI) and benefits from the GEOSS <b>interoperability</b> <b>arrangements.</b> The FP 7 -co-funded EnerGEO project provides a GEOSS compliant Catalogue Service for the Web (CSW) that permits to discover the Web Processing Service (WPS) allowing computation of the environmental impact. A WebGIS client provided by the FP 7 -co-funded GENESIS platform allows users to interact with geospatial data and computation processes. This scenario {{has proven to be}} an efficient tool to disseminate knowledge on environmental impacts related to PV because of the GEOSS capabilities in interoperability...|$|R
40|$|Standardization {{organizations}} {{are working for}} syntactic and schematic level of interoperability. At the same time, semantic interoperability must {{be considered as a}} heterogeneous condition and also very diversified with a large-volume data. The ontology registry has been developed and ontological information such as technical vocabularies for earth observation has been collected for data <b>interoperability</b> <b>arrangement.</b> This is a very challenging method for earth observation data interoperability because collaboration or cooperation with scientists of different disciplines is essential for common understanding. Multiple semantic MediaWikis are applied to register and update technical vocabularies {{as a part of the}} ontology registry, which promises to be a useful tool for users. In order to invite contributions from the user community, it is necessary to provide sophisticated and easy-to-use tools and systems, such as table-like editor, reverse dictionary, and graph representation for sustainable development and usage of ontological information. Registered ontologies supply the reference information required for earth observation data retrieval. We proposed data/metadata search with ontology such as technical vocabularies and visualization of relations among dataset to very large scale and various earth observation data...|$|R
40|$|This {{document}} updates RFC 4960 {{by defining}} {{a method for}} the sender of a DATA chunk {{to indicate that the}} corresponding Selective Acknowledgment (SACK) chunk should be sent back immediately and should not be delayed. It is done by specifying a bit in the DATA chunk header, called the (I) mmediate bit, which can get set by either the Stream Control Transmission Protocol (SCTP) implementation or the application using an SCTP stack. Since unknown flags in chunk headers are ignored by SCTP implementations, this extension does not introduce <b>any</b> <b>interoperability</b> problems. Status of This Memo This is an Internet Standards Track document. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|R
5000|$|Big River Telephone Company {{operates}} {{a network of}} class 4 and class 5 switches. To ensure reliability and security, Big River has implemented Class 5 VoIP switching technologies by MetaSwitch. With the deployment of MetaSwitch equipment on their network, Big River is able to deliver the advantages of VoIP technology directly to the end user. The implementation of the MetaSwitch allows Big River the opportunity to streamline its existing operations and to expand its innovative services including VoIP from a single platform. [...] This technology {{is the core of}} delivering dial tone, connecting calls, providing call features, and routing calls for Big River Telephone Company. The Big River telephone and MetaSwitch relationship is an active public partnership program that is engineering focused to ensure that both companies can co-operatively work together on <b>any</b> <b>interoperability</b> matters.|$|R
40|$|The {{complexity}} of installations {{in the oceans}} to carry out observations on specific processes and for detecting long-term trends have grown significantly in the past years. This applies also to the type and number of sensors that are in use in observing systems. In these days, sensors shall be compatible to different platforms that are in use like floats, gliders or moorings, and accordingly also different data acquisition systems. Facilitating the integration process in existing or newly established observing systems comes with a real benefit for the operators and {{is important for the}} broader application of different sensors. However, how to achieve the goals is under debate. The most serious obstacle for all initiatives is the willingness of stakeholders to adopt a strategy and, even more so, to adopt a specific architecture to enable interoperability across platforms and observing systems. Therefore, the situation {{at this point in time}} is characterized by the fact that parallel approaches have been developed (IEEE 1451, the OGC set of standards, etc.) that are ready to be evaluated but still lacking the support by the community. Therefore it seems to be a good time to consider and to agree on the implementation of <b>interoperability</b> <b>arrangements.</b> Peer ReviewedPostprint (published version...|$|R
40|$|Drought {{is one of}} {{the major}} weather-related disasters. Persisting over months or years, {{droughts}} can affect large areas and may have serious environmental, social and economic impacts. Over the last decades many European countries have repeatedly been affected by droughts, resulting in considerable ecological and economic damage. Climate change studies further indicate a trend towards increasing climate variability in many parts of the world, most likely resulting in more frequent drought occurrences also in Europe. The need for an adequate monitoring and management is, therefore, evident and has recently triggered action at different political and management levels. An adequate management of droughts requires up-to-date information on the occurrence and severity of drought episodes as well as information on possible impacts and the probability of their duration. JRC, therefore, develops the prototype of a European Drought Observatory (EDO) to monitor, assess and forecast drought events across the entire European continent. As droughts can affect the entire water cycle (e. g., precipitation, soil moisture, stream flow and groundwater) and have direct impacts on the vegetation cover, all these components need careful monitoring. To do so, a suite of indicators is calculated from different data sources in order to capture various aspects of a drought event and to forecast its probable evolution. In order to provide information at different operational scales, close collaboration with national and regional observatories needs to be ensured through adequate <b>interoperability</b> <b>arrangements,</b> thus providing comparable information at different spatial and temporal resolutions. At the core of the European Drought Observatory (EDO) are a web portal and map server presenting up-to-date drought relevant information for entire Europe to the public and to decision makers in policy and water resources management. The current version of the EDO map server publishes continental information based on data processed and analysed at JRC. Available drought products include a monthly updated Standardized Precipitation Index (SPI), daily updated modelled soil moisture anomalies, and remote sensing observations on the state of the vegetation cover (i. e. anomaly of the fraction of Absorbed Photosynthetically Active Radiation (fAPAR), Normalized DifferenceWater Index (NDWI)). A one-week soil moisture anomaly forecast complements the picture. First <b>interoperability</b> <b>arrangements</b> allow accessing more detailed information at national and river basin scale. In addition, time series of drought indices can be retrieved for all administrative regions in Europe, visualizing the temporal evolution over several years. Current research work is focusing on validating the available products, extending the interoperability with national and regional drought information systems and testing medium-range probabilistic drought forecasting products. Probabilistic forecasts are attractive in that they provide an estimate of the range of uncertainty in a particular forecast. Longer-term goals include the development of long-range drought forecasting products, the development of methodologies for monitoring drought impact and the integration of EDO in a global drought monitoring system. JRC. DDG. H. 7 -Land management and natural hazard...|$|R
40|$|Presently, {{solutions}} for geo-information sharing are mainly based on Web technologies, implementing service-oriented frameworks, and applying open (international or community) standards and <b>interoperability</b> <b>arrangements.</b> Such frameworks take {{the name of}} Spatial Data Infrastructures (SDIs). The recent evolution of the World Wide Web (WWW), {{with the introduction of}} the Semantic Web and the Web 2. 0 concepts and solutions, made available new applications, architectures, and technologies for sharing and linking resources published on the Web. Such new technologies can be conveniently applied to enhance capabilities of present SDIs—in particular, discovery functionality. Different strategies can be adopted in order to enable new ways of searching geospatial resources, leveraging the Semantic Web and Web 2. 0 technologies. The authors propose a Discovery Augmentation Methodology which is essentially driven by the idea of enriching the searchable information that is associated with geospatial resources. They describe and discuss three different high-level approaches for discovery augmentation: Provider-based, User-based, and Third-party based. From the analysis of these approaches, the authors suggest that, due to their flexibility and extensibility, the user-based and the third-party based approaches result more appropriate for heterogeneous and changing environments such as the SDI one. For the user-based approach, they describe a conceptual architecture and the main components centered on the integration of user-generated content in SDIs. For the third-party approach, the authors describe an architecture enabling semantics-based searches in SDIs. JRC. H. 6 -Digital Earth and Reference Dat...|$|R
40|$|International audienceGlobal {{computing}} platforms {{have become}} popular tools {{for the resolution}} of large scale problems. They are often independent without <b>any</b> <b>interoperability</b> between each other. Therefore, clients are now asking for a better availability and scalability. In previous work, we presented the YML framework which was a first attempt to enable the development and deployment of applications on several global computing middleware. However, it suffered from scalability issues and a static approach. In this paper, we highlight recent extensions of YML. We achieve a dynamic federation of computing middleware because YML is now able to manage at the run-time several middleware back-ends. Other improvements such as an OmniRPC back-end, a component binary cache mechanism, a new data management module, and a new data type supported by the YML front-end, yield to a much better scalability. We present the first evaluations of these extensions on a network of workstations and with a typical distributed sort application. Although the results show a significant overhead, they stress the benefits of binary caching and dynamic back-ends federation...|$|R
40|$|Remotely sensed ground {{reflectance}} {{is the foundation}} of <b>any</b> <b>interoperability</b> or change detection technique. Satellite intercomparisons and accurate vegetation indices, such as the Normalized Difference Vegetation Index (NDVI), require the generation of accurate reflectance maps (NDVI is used to describe or infer a wide variety of biophysical parameters and is defined in terms of near-infrared (NIR) and red band reflectances). Accurate reflectance-map generation from satellite imagery relies on the removal of solar and satellite geometry and of atmospheric effects and is generally referred to as atmospheric correction. Atmospheric correction of remotely sensed imagery to {{ground reflectance}} has been widely applied to a few systems only. The ability to obtain atmospherically corrected imagery and products from various satellites is essential to enable widescale use of remotely sensed, multitemporal imagery for a variety of applications. An atmospheric correction approach derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) that can be applied to high-spatial-resolution satellite imagery under many conditions was evaluated to demonstrate a reliable, effective reflectance map generation method. Additional information is included in the original extended abstract...|$|R
40|$|Linguistic Databases {{that are}} {{currently}} available {{for research and development}} can be currently classified as a heterogeneous collection of different proprietary databases with minimal means, if <b>any,</b> of <b>interoperability</b> with other linguistic databases, making it hard to extend the database usefulness beyond the life of their originating projects (Cunningham, 1999). This paper discusses an interoperable extensible linguistic database system developed for the Maltilex Project at the University of Malta (Rosner et. al., 1999) ...|$|R
30|$|Rezaei et al. (2014) {{performed}} a Systematic Literature Review (Kitchenham et al. 2009) on interoperability focusing on interoperability evaluation models, {{but did not}} focus on cloud interoperability. They searched for evaluation models for interoperability in general and included cloud interoperability in their results. The authors also offer a comparative analysis between their findings to determine {{similarities and differences between}} the interoperability evaluation models found. As we mentioned before, they did not find <b>any</b> cloud <b>interoperability</b> model.|$|R
40|$|The {{need for}} {{connecting}} information systems of collaborating organizations {{has become increasingly}} common. Significant advantages, such as increased speed, efficiency, and reliability, {{can be obtained by}} automating inter-organizational business processes. To achieve this, business-to-business integration, i. e. facilitating interoperation of disparate information systems of different organizations, must be performed. E-business frameworks are generic solutions for performing such integration. RosettaNet is an industry consortium that maintains an e-business framework that specifies inter-organizational business processes for multiple industries. Process specifications include messages that are exchanged between organizations, and related messaging choreography. RosettaNet Implementation Framework (RNIF) is the messaging portion of the RosettaNet framework, specifying how messages are exchanged. The purpose of this work is to gather experience on implementing RNIF with currently available tools. Research questions include how can RNIF be implemented in practice, how suitable are current tools and what could be improved in them, how much effort is required, what level of performance can be obtained, and are there <b>any</b> <b>interoperability</b> problems with different RNIF implementations. To answer these questions, a prototype system is developed using J 2 EE (Java 2 platform Enterprise Edition) and web service technologies. The prototype is a middleware system that provides RNIF functionality...|$|R
40|$|The goal of {{this work}} is to {{contribute}} to the field of interoperability of Workflow models. To achieve this interoperability, we have built a generic architecture that addresses three levels of abstraction: the common meta-model that the Workflow models must share, the common model that they enact collectively, and the common data model whose management is shared. So, the approach we have adopted is based on a strategy of uniformity to solve the problems related to the semantic, syntactic and execution platform heterogeneity. The common meta-model gathers the common concepts that are shared between all these Workflow models and it defines their semantics. These concepts (activity, event, etc.) are extracted from different formalisms used in the field of business process (or Workflow). This latter allows us to instantiate a canonical model that describes only common parts (activities, artifacts). As for the common data model, it is instantiated by the common model. For the control interoperability, we have adopted an approach that deals with a more flexible connection mechanism based on events through a connection server. This approach can be implemented above <b>any</b> <b>interoperability</b> platform (CORBA, EJB, etc.). Also, the shared canonical model that we have proposed is generic, simple and re-usable...|$|R
40|$|This paper applies Stamper’s {{semiotic}} {{ladder to}} assess {{the extent to which}} Conceptual Graph (CG) software tools can interoperate with each other and others. The CharGer CG editor, the Amine multifaceted CG environment, and the non-CG multi-agent simulation environment SeSAm were selected as a representative set of tools for this purpose. Using these exemplars, the paper explores through the semiotic ladder the interoperation that CG tools presently reach. It identifies that an external XML converter is required for <b>any</b> meaningful <b>interoperability,</b> and suggests this is where future research on obtaining CG tools interoperability is directed...|$|R
40|$|Abstract. iStarML is an XML-based {{format for}} {{enabling}} i * interoperability. A relevant difference with <b>any</b> other <b>interoperability</b> proposal is that iStarML is founded {{under the assumption}} {{that there is not a}} common ontology guiding this communication proposal. The different i * variants and even particular applications proposing new language constructors forced to confront a theoretical approach for supporting an interoperability approach in an evolving and variable semantic scenario. In this paper we focused on the theories behind the iStarML proposal, which include sociological, cybernetics and linguistics approaches. Finally, we apply what these theories predict to the case of the i* framework and its research community. ...|$|R
40|$|In {{the next}} years, smart cards {{are going to}} become the main {{personal}} identification document in many nations. In particular, both Europe and United States are Currently working to this aim. Therefore, {{tens of millions of}} smart cards. based oil hardware devices provided by many different manufacturers. will be distributed all over the world, and used in particular to accomplish the security tasks of electronic authentication and electronic signature. In this context, the so called Common Criteria define the security requirements for digital signature devices. Unfortunately, these criteria do not address <b>any</b> <b>interoperability</b> issue between smart cards of different manufacturers, which usually implement digital signature process in still correct but Slightly different ways. To face the interoperability problem. we realized a complete testing environment whose core is the Crypto Probing System (c) Nestor Lab, an abstract interface to a generic cryptographic smart card, embedding a standard model of the correct card behavior. which can be used to test the digital signature process behavior, also in the presence of alternate or disturbed command sequences, in conjunction with automatic verification techniques such as model checking. The framework allows to verify abstract behavior models against real smart cards, so {{it can be used to}} automatically verify the Common Criteria as well as the extended interoperability criteria above and many other low-level constraints. In particular, in this paper we show how we can verify that the card, in the presence of a sequence of (partially) modified commands, rejects them Without any side effect, remaining usable, or accepts them, generating a correct final result...|$|R
50|$|The {{origins of}} the AUSCANNZUKUS {{organization}} arose from dialogue between Admiral Arleigh Burke, USN, and Admiral Lord Louis Mountbatten, RN, in 1960. Their intention was to align naval communications policies and prevent, or at least limit, <b>any</b> barriers to <b>interoperability,</b> with the imminent introduction of sophisticated new communications equipment. AUSCANNZUKUS matured to the current five-nation organization in 1980 when New Zealand became a full member.|$|R
40|$|This report {{addresses}} {{the question of}} how geographic and environmental information created and maintained by different organisations in Europe can be embedded in Spatial Data Infrastructures (SDIs) and reused in various applications by different people. The main challenge related to this task is to deal with the heterogeneity of data managed by others. The core concept of SDIs is interoperability, which “means the possibility for spatial data sets to be combined and for services to interact, without repetitive manual intervention, {{in such a way that}} the result is coherent and the added value of the data sets and services is enhanced”. INSPIRE, which is used as the main SDI initiative from which this report draws its examples and best practices, is built on the existing standards, information systems and infrastructures, professional and cultural practices of 27 Member States of the European Union in more than 23 languages. The main part of this report describes the conceptual framework for the development of interoperability specifications that define the targets to which existing data should be transformed. The conceptual framework is composed of two fundamental parts: the Generic Conceptual Model (GCM) and the methodology for data specification development. The GCM defines 26 aspects or elements for achieving data interoperability in an SDI. These include registers and registries, coordinate reference systems, identifier management, metadata, maintenance, to name just a few. The description of the methodology for developing data specifications for interoperability includes a detailed discussion of the relevant actors, steps and the overall workflow – from capturing user requirements to documenting and testing the specifications that emerge from this process. The GCM and the methodology together help to understand the organisational and technical aspects how the data component of an SDI can be established, how <b>interoperability</b> <b>arrangements,</b> data standardisation and harmonisation contribute to this process. Since 2005 INSPIRE has been pioneering the introduction, development, and application of a conceptual framework for establishing the data component in an SDI. This experience shows that the conceptual framework described in this report is robust enough to reinforce interoperability across the 34 data specifications developed for the infrastructure. Moreover, because the framework is platform and theme independent, able to deal with the cultural diversity, and based on best practice examples from Europe and beyond, it may provide solutions for SDI challenges in other environments too. JRC. H. 6 -Digital Earth and Reference Dat...|$|R
5000|$|Since {{introduction}} of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In {{order to achieve}} the highest possible degree of compatibility and <b>interoperability</b> <b>any</b> implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And {{if the goal is}} communication and not quibbling, each implementation should overcome minor nonconformities with standard: ...|$|R
40|$|This {{selected}} bibliography {{focuses on the}} special challenges of Multinational Operations such as command <b>arrangements,</b> <b>interoperability,</b> intelligence sharing, multilateralism, and cultural diversity. It does not include general descriptions of multinational operations and exercises. With the exception of some important older titles, most of the books, documents, articles and online resources cited are dated 2001 to the present. All items in this bibliography {{are available in the}} USAWC Library. For your convenience, {{at the end of the}} entries, we have added library call numbers, Internet addresses, or database links. Call numbers indicate the item’s shelf location in our library; please note that call numbers can vary from library to library. Web sites were accessed durin...|$|R
5000|$|Lack {{of an open}} {{standard}} {{can also}} become problematic for the customers, as in case of the original vendor's inability to fix a certain problem that is an artifact of technical limitations in the original product. The customer wants that fault fixed, but the vendor has to maintain that faulty state, even across newer revisions of the same product, because that behaviour is a de facto standard and many more customers {{would have to pay}} the price of <b>any</b> break in <b>interoperability</b> caused by fixing the original problem and introducing new behaviour.|$|R
40|$|We {{introduce}} {{the first known}} mechanism providing realtime server location verification. Its uses include enhancing server authentication (e. g., augmenting TLS) by enabling browsers to automatically interpret server location information. We describe the design of this new measurement-based technique, Server Location Verification (SLV), and evaluate it using PlanetLab. We explain how SLV {{is compatible with the}} increasing trends of geographically distributed content dissemination over the Internet, without causing <b>any</b> new <b>interoperability</b> conflicts. Additionally, we {{introduce the}} notion of (verifiable) "server location pinning" within TLS (conceptually similar to certificate pinning) to support SLV, and evaluate their combined impact using a server-authentication evaluation framework. The results affirm the addition of new security benefits to the existing SSL/TLS-based authentication mechanisms. We implement SLV through a location verification service, the simplest version of which requires no server-side changes. We also implement a simple browser extension that interacts seamlessly with the verification infrastructure to obtain realtime server location-verification results. Comment: 14 pages, 4 figure...|$|R
40|$|Archives, {{libraries}} and museums preserve collections containing diverse material. In managing these collections different standards are used, each developed inside their own professional community. While searching for material preserved in different institutions users shouldn’t stumble over inability {{to cope with}} different descriptive standards and access points defined by these standards. As this research suggests, the path to <b>any</b> kind of <b>interoperability</b> starts with people who implement standards. This exploratory study investigates curators’ understandings of archival and documentary materials held in their museums (i. e. rather than in archives) by analyzing and describing their attitude towards records that surround them in their daily professional practice...|$|R
40|$|The report {{summarizes}} the presentations, discussions, and {{conclusions of the}} Citizen Science and Smart Cities Summit organised by the European Commission Joint Research Centre on 5 - 7 th February 2014. In {{the context of the}} Summit, the label Citizen Science was used to include both citizen science projects, and others that are about user-generated content, not necessarily addressing a scientific process or issues. The evidence presented by 27 different projects shows the vitality and diversity of the field but also a number of critical points: •	Citizen science project are more than collecting data: they are about raising awareness, building capacity, and strengthening communities. •	Likewise, smart cities are not only about ICT, energy and transport infrastructures: Smart cities are about smart citizens, who participate in their city’s daily governance, are concerned about increasing {{the quality of life of}} their fellow-citizens, and about protecting their environment. Technology may facilitate, but is no solution per se. •	Unfortunately to date there seems to be little synergy between citizen science and smart cities initiatives, and there is little interoperability and reusability of the data, apps, and services developed in each project. •	It is difficult to compare the results among citizen science, and smart cities projects or translate from one context to another. •	The ephemeral nature of much of the data, which disappear short after the end of the projects, means lack of reproducibility of results and longitudinal analysis of time series challenging, if not impossible. •	There are also new challenges with respect to the analytical methods needed to integrate quantitative and qualitative data from heterogeneous sources that need further research. •	Building and maintaining trust are key points of any citizen science or smart city project. There is a need to work with the community and not just for, or on, the community. It is critical not just to take (data, information, knowledge) but to give back something that is valued by the community itself. The development of citizen science associations in Europe and the US are important developments that may address some of the points above. There are also actions through which the European Commission Joint Research Centre can make an important contribution: •	Map citizen science and smart cities projects, and generate a semantic network of concepts between the projects to facilitate search of related activities, and community building. •	Provide a repository for citizen science and smart cities data (anonymised and aggregated), software, services, and applications so that they are maintained beyond the life of the projects they originate from, and made shareable and reusable. •	Develop regional test beds for the analysis and integration of social and environmental data from heterogeneous sources, with a focus on quality of life and well-being. •	Undertake comparative studies, and analyse issues related to scaling up to the European dimension. •	Support citizen science and smart cities projects with the JRC knowledge on semantic interoperability, data models, and <b>interoperability</b> <b>arrangements.</b> •	Partner with the European Citizen Science Association, and contribute to its interoperability activities. •	Work towards making the JRC, and the European Commission, a champion of citizen participation in European science. JRC. H. 6 -Digital Earth and Reference Dat...|$|R
40|$|The current thesis {{focuses on}} {{interoperability}} issues within healthcare organizations. It attempts {{to formulate a}} set of best practices and guidelines for the successful design and implementation of interoperability frameworks in healthcare. Thus, it provides a discussion on several healthcare-oriented objectives and pre-requisites along {{with a set of}} approaches for achieving interoperability. The emphasis of the current work is on the key role of healthcare processes within healthcare organizations and their tight connection with <b>any</b> <b>interoperability</b> solution introduced, in terms of modeling, utilizing and coordinating all the available resources (data, human and material assets). The approach which is proposed in the current thesis is related with the adoption of a Service-Oriented Architectural framework (SOA). In order to demonstrate the capabilities of such a framework in the healthcare context, a prototype implementation was developed. An important feature of the proposed framework is the implementation of a virtual electronic health record paradigm which is exclusively operating over the SOA framework and it can dynamically synthesize distributed patient data into a unified environment for cross-platform data delivery to a variety of clients. In addition, the thesis attempts to investigate the current status, in terms of interoperability, within the Greek Public healthcare prefectures all over Greece. In order to fulfill this objective, publicly available data were collected from the Greek Information Society organization regarding specific IT attributes of the Greek healthcare organizations for the period 2000 until 2006. The study included fifteen out of the total seventeen regional healthcare prefectures in Greece. The data gathered was utilized in order to perform an initial statistical analysis but was also used in order to perform a Data Envelopment Analysis (DEA) research study for the level of Information Technology (IT) efficiency in terms of infrastructure and software applications within the Greek National Health System. The results produced clearly highlighted that despite the limited sampling, there is also limited utilization of the physical and software infrastructure for achieving interoperability. Furthermore, the current work is investigating the evaluation strategies and criteria regarding service oriented interoperability frameworks in the healthcare domain. A thorough literature review is performed based on the dilemma of technical versus non-technical evaluation methodologies. Based on such a discussion, the thesis is proposing an organizational impact evaluation. The proposed methodology is based on the initial IS Success model, as proposed by DeLone and McLean (1992). The IS Success model was adapted in the context of specific service-oriented characteristics which are used in order to assess specific dimensions like System Quality and Information Quality of the DeLone and McLean model. The proposed theoretical framework was utilized in order to perform an empirical study, amongst sixty two (62) participants, for the organizational impact of the service-oriented prototype implementation. The data gathered was analyzed using a structural equation modeling technique, specifically partial least squares (PLS). ...|$|R
40|$|Valuable library {{collections}} {{often not}} accessible in their original, print form are being digitized. Numerous digitalization projects aim to offer access to digitized items/collections via Internet {{and make them}} part of educational and scholar programs Role of a library, as a mediator, is selection, digitalization, storage, long-term preservation, access, marketing and organization of all those processes. Cooperation of similar institutions like libraries, museums and archives appears as very important factor in users' needs and usage of digitized material. Appropriate technical and network facilities allows fast and reliable transfer large data in <b>any</b> digital format. <b>Interoperability</b> and communication among different and distributed collections is easily done by following certain standards. Larger bandwidth allows usage {{of any kind of}} digital material weather it is digital item or more sophisticated multimedia packages made for educational purposes and distance learning, all of them available through on line library catalogues...|$|R
25|$|Late in {{the year}} 1999 through early 2000, with funding from the Japan Science and Technology Corporation (JST), Hiroaki Kitano and John C. Doyle {{assembled}} a small team of researchers to work on developing better software infrastructure for computational modeling in systems biology. Hamid Bolouri {{was the leader of}} the development team, which consisted of Andrew Finney, Herbert Sauro, and Michael Hucka. Bolouri identified the need for a framework to enable interoperability and sharing between the different simulation software systems for biology in existence during the late 1990s, and he organized an informal workshop in December 1999 at the California Institute of Technology to discuss the matter. In attendance at that workshop were the groups responsible for the development of DBSolve, E-Cell, Gepasi, Jarnac, StochSim and The Virtual Cell. Separately, earlier in 1999, some members of these groups also had discussed the creation of a portable file format for metabolic network models in the BioThermoKinetics (BTK) group. The same groups who attended the first Caltech workshop met again on April 28–29, 2000, at the first of a newly created meeting series called Workshop on Software Platforms for Systems Biology. It became clear during the second workshop that a common model representation format was needed to enable the exchange of models between software tools as part of <b>any</b> functioning <b>interoperability</b> framework, and the workshop attendees decided the format should be encoded in XML.|$|R
40|$|The JISC TechDis service aims to be {{the leading}} {{educational}} advisory service, working across the UK, {{in the fields of}} technology, disability and inclusion. TechDis aims to enhance prov ision for disabled students and staff in higher, further and specialist education and adult and community learning, through the use of technology. TechDis has been undertaking research for the JISC on the issues of guidelines and advice that is available to services involved in its Exchange for Learning (X 4 L) and Digital Libraries in the Classroom (DLiC) projects. This report discusses some of the outc omes and areas that will need further clarification, research and development. This report contains the following sections • Legislative Compliance • Available Guidelines • Supporting the sectors It should be stated at the outset that this work bears relevance only to e- learning in UK education institutions, it does not, nor does it seek to influence any existing guidelines pertaining to the Internet (Such as the W 3 C WCAG 1. 0 guidelines) nor <b>any</b> issues of <b>interoperability</b> and metadat...|$|R
