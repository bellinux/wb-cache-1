0|10000|Public
40|$|<b>File</b> <b>Systems</b> {{are hard}} to develop. File-system {{development}} is a long process and involves writing huge amount of kernel code by experienced and expert programmers. However, these do not allow customization. Their implementation is not tunable as per application and hence lacks flexibility. Various ways of incremental development have been proposed {{but they do not}} rule out kernel consideration completely. In order to aid for flexibility, efficient <b>automatic</b> <b>file</b> <b>system</b> compilers can play the trick. The idea is to develop a mechanism that will generate a <b>file</b> <b>system</b> as per user specifications. This paper presents the concept of automatic file-systems compilers and, implementation details of an installable <b>file</b> <b>system</b> –a step towards <b>automatic</b> <b>file</b> <b>systems.</b> ...|$|R
5000|$|ZFS — a <b>file</b> <b>system</b> which {{performs}} <b>automatic</b> <b>file</b> {{integrity checking}} using checksums ...|$|R
50|$|A {{limitation}} of shred when invoked on ordinary files {{is that it}} only overwrites the data in place without overwriting other copies of the file. Copies can manifest themselves {{in a variety of}} ways, such as through manual and <b>automatic</b> backups, <b>file</b> <b>system</b> snapshots, copy-on-write filesystems, wear leveling on flash drives, caching such as NFS caching, and journaling. All limitations imposed by the <b>file</b> <b>system</b> can be overcome by shredding every device on which the data resides instead of specific files. However, since wear leveled devices do not guarantee a fixed relationship between logical blocks addressable through the interface and the physical locations in which the data is stored, shredding may not provide adequate security. If available, the SATA secure erase command, issued through hdparm or a similar utility, may be helpful in this situation. Even for magnetic devices, SATA secure erase will be faster and more reliable than shredding. Physical destruction may be necessary to securely erase devices such as memory cards and unusable hard disks.|$|R
40|$|In the Open University {{environment}} where {{students are not}} centrally located and are not under any direct supervision the potential for plagiarism definitely exists. The very technology that facilitates open learning also allows easy exchange of papers among peers. Students can also easily access “paper-mills ” where essays can be quickly customised to suit requirements, for a fee. There is also the vast information residing on the Internet ready for creative reuse. In the light of all these temptations the Open University of Malaysia (OUM) is exploring {{the use of technology}} to educate students and deter plagiarism. One approach that appears promising is to use a good commercial plagiarism detection system. The system, by being able to detect cases of plagiarism would serve as a deterrent and hopefully contribute towards inculcating the culture of honesty. This paper presents the findings from a small study using two detection systems, a commercial Plagiarism Detection System, MyDropBox and a simple and free <b>automatic</b> <b>file</b> comparison <b>system...</b>|$|R
40|$|Conventional <b>file</b> <b>systems</b> do {{not support}} <b>automatic</b> <b>file</b> versioning. To keep a record of file changes, users would need to {{manually}} save copies of a file at different point in time. This consumes {{a large amount of}} disk space, and brings users unnecessary operational overhead. In this report, we outline the design of SEV [...] a storage-efficient versioning <b>file</b> <b>system...</b>|$|R
40|$|The steady {{increase}} in the power and complexity of modern computer systems has encouraged the implementation of <b>automatic</b> <b>file</b> migration <b>systems</b> which move <b>files</b> dynamically between mass storage devices and disk in response to user reference patterns. Using information describing thirteen months of text editor data set file references, (analyzed in detail {{in the first part}} of this paper), they develop and evaluation algorithms for the selection of files to be moved from disk to mass storage. They find that algorithms based on both the file size and the time since the file was last used work well. The best realizable algorithms tested condition on the empirical distribution of the times between file references. Acceptable results are also obtained by selecting for replacement that file whose size times time to last reference is maximal. Comparisons are made with a number of standard algorithms developed for paging, such as Working Set. Sufficient information (parameter values, fitted equations) is provided that our algorithms may be easily implemented on other systems...|$|R
40|$|In {{this paper}} we {{describe}} the HPUFS, a system designed for high performance in Network <b>File</b> <b>System</b> by incorporating <b>File</b> caching and File locking. This paper also describes the implementation of <b>Automatic</b> <b>file</b> synchronization tool which is used for backing up the data in the local systems to the remote Storage Server. HPUFS is also designed to supports Low Bandwidth Networks. It is also intended to provide locking. The paper describes the initial design and expected use. A number of programs have implemented portable network <b>file</b> <b>systems.</b> However, they have been plagued by low performance. HPUFS is implemented using asynchronous APIs (libasync) which can improve the performance...|$|R
40|$|MPISS is an <b>automatic</b> <b>file</b> {{transfer}} <b>system</b> that implements {{a combination}} of standard and mission-unique transfer protocols required by the Global Precipitation Measurement Mission (GPM) Precipitation Processing System (PPS) to control the flow of data between the MOC and the PPS. The primary features of MPISS are file transfers (both with and without PPS specific protocols), logging of <b>file</b> transfer and <b>system</b> events to local files and a standard messaging bus, short term storage of data files to facilitate retransmissions, and generation of file transfer accounting reports. The system includes a graphical user interface (GUI) to control the system, allow manual operations, and to display events in real time. The PPS specific protocols are an enhanced version of those that were developed for the Tropical Rainfall Measuring Mission (TRMM). All file transfers between the MOC and the PPS use the SSH File Transfer Protocol (SFTP). For reports and data files generated within the MOC, no additional protocols are used when transferring files to the PPS. For observatory data files, an additional handshaking protocol of data notices and data receipts is used. MPISS generates and sends to the PPS data notices containing data start and stop times along with a checksum for the file for each observatory data file transmitted. MPISS retrieves the PPS generated data receipts that indicate {{the success or failure}} of the PPS to ingest the data file and/or notice. MPISS retransmits the appropriate files as indicated in the receipt when required. MPISS also automatically retrieves files from the PPS. The unique feature of this software is the use of both standard and PPS specific protocols in parallel. The advantage of this capability is that it supports users that require the PPS protocol as well as those that do not require it. The system is highly configurable to accommodate the needs of future users...|$|R
40|$|A {{semantic}} <b>file</b> <b>system</b> is {{an information}} storage system that provides flexible associative {{access to the}} system's contents by automatically extracting attributes from files with file type specific transducers. Associative access is provided by a conservative extension to existing tree-structured <b>file</b> <b>system</b> protocols, and by protocols that are designed specifically for content based access. Compatibility with existing <b>file</b> <b>system</b> protocols is provided by introducing {{the concept of a}} virtual directory. Virtual directory names are interpreted as queries, and thus provide flexible associative access to files and directories in a manner compatible with existing software. Rapid attribute-based access to <b>file</b> <b>system</b> contents is implemented by automatic extraction and indexing of key properties of <b>file</b> <b>system</b> objects. The <b>automatic</b> indexing of <b>files</b> and directories is called "semantic" because user programmable transducers use information about the semantics of updated <b>file</b> <b>system</b> objects to extract the properties for indexing. Experimental results from a semantic <b>file</b> <b>system</b> implementation support the thesis that semantic <b>file</b> <b>systems</b> present a more effective storage abstraction than do traditional tree structured <b>file</b> <b>systems</b> for information sharing and command level programming. ...|$|R
5000|$|... {{real-time}} remote file access, replication, collaboration & {{sharing with}} <b>automatic</b> <b>file</b> synchronization ...|$|R
5000|$|<b>Automatic</b> <b>file</b> {{handling}} (listeners {{interact with}} multiple MP3 files {{as a single}} book).|$|R
5000|$|Strives {{to deliver}} netbook {{functionality}} like the Social Desktop and <b>automatic</b> <b>file</b> synchronization ...|$|R
50|$|From {{the first}} version Shareaza has {{supported}} swarming, metadata, library management, and <b>automatic</b> <b>file</b> hashing.|$|R
5000|$|... <b>automatic</b> <b>filing</b> chest: a small button open {{directly}} {{the filing}} chest, {{no need to}} open drawers ...|$|R
40|$|Log-structured <b>file</b> <b>systems</b> (LFSs) were {{developed}} to eliminate latencies involved in accessing disk devices, but their sequential write patterns also match well with tertiary storage characteristics. Unfortunately, existing versions only manage memory caches and disks, and do not support a broader storage hierarchy. Robotic storage devices offer huge storage capacity at a low cost per byte, but with large access times. Integrating these devices into the storage hierarchy presents a challenge to <b>file</b> <b>system</b> designers. HighLight extends 4. 4 BSD LFS to incorporate both secondary storage devices (disks) and tertiary storage devices (such as robotic tape jukeboxes), providing a hierarchy within the <b>file</b> <b>system</b> that does not require any application support. This report presents the design of HighLight, proposes various policies for <b>automatic</b> migration of <b>file</b> data between the hierarchy levels, and presents initial migration mechanism performance figures. Log-structured <b>file</b> <b>systems,</b> with thei [...] ...|$|R
5000|$|Worker Node 5 - <b>automatic</b> <b>file</b> {{processing}} {{based on}} changes occurring in specified “watch” folders. For Windows and Mac OS X.|$|R
40|$|Today {{managing}} {{files in}} a server system {{has the same}} magnitude as managing the World Wide Web due to the dynamic nature of the <b>file</b> <b>system.</b> Even searching for files over the <b>file</b> <b>system</b> is time consuming because finding a file on hard disk is a long-running task. Every file on the disk has to be read with dangling pointers to files which no longer exist {{because they have been}} changed, moved or deleted. This makes the user frustrated. The <b>Automatic</b> <b>file</b> indexing framework facilitates users to resolve file names and locate documents stored in file repositories. The main design objective of the framework is to maintain sub-indexes at the folder level that have the full knowledge of the revisions that are made at the folder level automatically. This research proposes a framework that manages the creation and maintenance of the file index, with the use of Resources Description Framework (RDF) and retrieval using semantic query languages i. e. SPARQL. The sub-indexes are maintained hierarchically starting from the leaf node to the root node recursively. The proposed framework will monitor the <b>file</b> <b>system</b> continuously and update individual folder descriptors (sub-indexes) stored on each node as the <b>file</b> <b>system</b> changes making the cached indexes resilient to any file changes. The framework is resilient of file or folder name changes. Further, the study explores avenues to build an offline semantic index that can be used by the clients to perform distribute file search without performing the search on the server itself. This is viable since the framework uses semantic languages to describe and build file descriptors that can easily integrate semantic indexing and hence this makes the index readily available for the Web...|$|R
40|$|HighLight is a <b>file</b> <b>system</b> {{combining}} secondary {{disk storage}} and tertiary robotic storage {{that is being}} developed {{as part of the}} Sequoia 2000 Project. [1]. HighLight is an extension of the 4. 4 BSD log-structured <b>file</b> <b>system</b> (LFS) [2], that provides hierarchical storage management without requiring any special support from applications. This paper presents HighLight's design and various policies for <b>automatic</b> migration of <b>file</b> data between the hierarchy levels. INTRODUCTION 4. 4 BSD LFS derives directly from the Sprite log-structured <b>file</b> <b>system</b> (LFS) [3], developed by Mendel Rosenblum and John Ousterhout as part of the Sprite operating system. Primarily, LFS is optimized for writing data, whereas most <b>file</b> <b>systems</b> are optimized for reading data. LFS divides the disk into 512 KB or 1 MB segments, and writes data sequentially within each segment. The segments are threaded together to form a log, so recovery is simple and quick, entailing a roll-forward of the log from the last checkpoint. Disk [...] ...|$|R
5000|$|TextEdit gains a new {{graphical}} toolbar with font {{selection and}} text highlighting. The new TextEdit also supports Apple's new <b>automatic</b> <b>file</b> saving and versions technologies.|$|R
40|$|This paper {{describes}} the design, implementation, {{and evaluation of}} an <b>automatic</b> application-specific <b>file</b> prefetching mechanism {{that is designed to}} improve the I/O performance of multimedia programs with complicated access patterns. The key idea of the proposed approach is to convert an application into two threads: a computation thread, which is the original program containing both computation and disk I/O, and a prefetch thread, which contains all the instructions in the original program that are related to disk accesses. At run time, the prefetch thread is scheduled to run far ahead of the computation thread, so that disk blocks can be prefetched and put in the <b>file</b> <b>system</b> buffer cache before the computation thread needs them. A source-to-source translator is developed to automatically generate the prefetch and computation thread from a given application program without any user intervention. We have successfully implemented a prototype of this <b>automatic</b> application-specific <b>file</b> pr [...] ...|$|R
40|$|Robotic {{storage devices}} offer huge storage {{capacity}} {{at a low}} cost per byte, but with large access times. Integrating these devices into the storage hierarchy presents a challenge to <b>file</b> <b>system</b> designers. Log-structured <b>file</b> <b>systems</b> (LFSs) were developed to reduce latencies involved in accessing disk devices, but their sequential write patterns match well with tertiary storage characteristics. Unfortunately, existing versions only manage memory caches and disks, and do not support a broader storage hierarchy. HighLight extends 4. 4 BSD LFS to incorporate both secondary storage devices (disks) and tertiary storage devices (such as robotic tape jukeboxes), providing a hierarchy within the <b>file</b> <b>system</b> that does not require any application support. This paper presents the design of HighLight, proposes various policies for <b>automatic</b> migration of <b>file</b> data between the hierarchy levels, and presents initial migration mechanism performance figures. 1...|$|R
50|$|<b>File</b> <b>system</b> types can be {{classified}} into disk/tape <b>file</b> <b>systems,</b> network <b>file</b> <b>systems</b> and special-purpose <b>file</b> <b>systems.</b>|$|R
40|$|From its inception, UNIX {{has been}} built around two {{fundamental}} entities: processes and files. In this chapter, {{we look at the}} implementation of files in Solaris and discuss the framework for <b>file</b> <b>systems.</b> 14. 1 <b>File</b> <b>System</b> Framework Solaris OS includes a framework, the virtual <b>file</b> <b>system</b> framework, under which multiple <b>file</b> <b>system</b> types are implemented. Earlier implementations of UNIX used a single <b>file</b> <b>system</b> type for all of the mounted <b>file</b> <b>systems,</b> typically, the UFS <b>file</b> <b>system</b> from BSD UNIX. The virtual <b>file</b> <b>system</b> framework was developed to allow Sun’s distributed computing <b>file</b> <b>system</b> (NFS) to coexist with the UFS <b>file</b> <b>system</b> in SunOS 2. 0; it became a standard part of System V in SVR 4 and Solaris OS. We can categorize Solaris <b>file</b> <b>systems</b> into the following types: Storage-based. Regular <b>file</b> <b>systems</b> that provide facilities for persistent storage and management of data. The Solaris UFS and PC/DOS <b>file</b> <b>systems</b> are examples. Network <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that provide <b>files</b> that are accessible in a local directory structure but are stored on a remote network server; for example, NFS. Pseudo <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that present various abstractions as files in a <b>file</b> <b>system.</b> The /proc pseudo <b>file</b> <b>system</b> represents the address space of a process as a series of files. 657 658 Chapter 14 <b>File</b> <b>System</b> Framework The framework provides a single set of well-defined interfaces that are <b>file</b> <b>system</b> independent; the implementation details of each <b>file</b> <b>system</b> are hidden behind these interfaces. Two key objects represent these interfaces: the virtual file, or vnode, and the virtual <b>file</b> <b>system,</b> or vfs objects. The vnode interfaces implement file-related functions, and the vfs interfaces implement <b>file</b> <b>system</b> management functions. The vnode and vfs interfaces direct functions to specific <b>file</b> <b>systems,</b> {{depending on the type of}} <b>file</b> <b>system</b> being operated on. Figure 14. 1 shows the <b>file</b> <b>system</b> layers. File-related functions are initiated through a system call or from another kernel subsystem and are directed to the appropriate <b>file</b> <b>system</b> by the vnode/vfs layer...|$|R
50|$|Distributed <b>file</b> <b>systems</b> can be {{optimized}} for different purposes. Some, {{such as those}} designed for internet services, including GFS, are {{optimized for}} scalability. Other designs for distributed <b>file</b> <b>systems</b> support performance-0intensive applications usually executed in parallel. Some examples include: MapR <b>File</b> <b>System</b> (MapR-FS), Ceph-FS, Fraunhofer <b>File</b> <b>System</b> (BeeGFS), Lustre <b>File</b> <b>System,</b> IBM General Parallel <b>File</b> <b>System</b> (GPFS), and Parallel Virtual <b>File</b> <b>System.</b>|$|R
50|$|For example, {{to migrate}} a FAT32 <b>file</b> <b>system</b> to an ext2 <b>file</b> <b>system.</b> First {{create a new}} ext2 <b>file</b> <b>system,</b> then copy the data to the <b>file</b> <b>system,</b> then delete the FAT32 <b>file</b> <b>system.</b>|$|R
5000|$|Blue Whale Clustered <b>file</b> <b>system</b> (BWFS) is {{a shared}} disk <b>file</b> <b>system</b> (also called {{clustered}} <b>file</b> <b>system,</b> shared storage <b>file</b> <b>systems</b> or SAN <b>file</b> <b>system)</b> made by Tianjin Zhongke Blue Whale Information Technologies Company in China.|$|R
5000|$|Virtual <b>file</b> <b>system</b> (VFS): A VFS is a <b>file</b> <b>system</b> used to {{help the}} user to hide the {{different}} <b>file</b> <b>systems</b> complexities. A user can use the same standard <b>file</b> <b>system</b> related calls to access different <b>file</b> <b>systems.</b>|$|R
5000|$|Use default {{settings}}. Default {{settings are}} defined per <b>file</b> <b>system</b> at the <b>file</b> <b>system</b> level. For ext3 <b>file</b> <b>systems</b> {{these can be}} set with the tune2fs command. The normal default for Ext3 <b>file</b> <b>systems</b> is equivalent to (no acl support). Modern Red Hat based systems set acl support as default on the root <b>file</b> <b>system</b> but not on user created Ext3 <b>file</b> <b>systems.</b> Some <b>file</b> <b>systems</b> such as XFS enable acls by default. Default <b>file</b> <b>system</b> mount attributes can be overridden in /etc/fstab.|$|R
40|$|Abstract. HFS+ <b>file</b> <b>system</b> is a <b>file</b> <b>system</b> of the Mac OS. In {{order to}} achieve data {{manipulation}} of the <b>file</b> <b>system</b> based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ <b>file</b> <b>system,</b> but also propose a efficient method to analyze the <b>file</b> <b>system.</b> Research contains the exploration of the <b>file</b> <b>system</b> and program implementation to analyze the <b>file</b> <b>system...</b>|$|R
40|$|With the {{emergence}} of Storage Networking, distributed <b>file</b> <b>systems</b> that allow data sharing through shared disks will become vital. We refer to Cluster <b>File</b> <b>Systems</b> as a distributed <b>file</b> <b>systems</b> optimized for environments of clustered servers. The requirements such <b>file</b> <b>systems</b> is that they guarantee <b>file</b> <b>systems</b> consistency while allowing shared access from multiple nodes in a shared-disk environment. In this paper we evaluate three approaches for designing a cluster <b>file</b> <b>system</b> - conventional client/server distributed <b>file</b> <b>systems,</b> symmetric shared <b>file</b> <b>systems</b> and asymmetric shared <b>file</b> <b>systems.</b> These alternatives are considered by using our prototype cluster <b>file</b> <b>system,</b> HAMFS (Highly Available Multi-server <b>File</b> <b>System).</b> HAMFS is classified as an asymmetric shared <b>file</b> <b>system.</b> Its technologies are incorporated into our commercial cluster <b>file</b> <b>system</b> product named SafeFILE. SafeFILE offers a disk pooling facility that supports off-the-shelf disks, and balances file load across these disks automatically and dynamically. From our measurements, we identify the required disk capabilities, such as multi-node tag queuing. We also identify the advantages of an asymmetric shared <b>file</b> <b>system</b> over other alternatives...|$|R
50|$|There {{are various}} User Mode <b>File</b> <b>System</b> (FUSE)-based <b>file</b> <b>systems</b> for Unix-like {{operating}} systems (Linux, etc.) {{that can be}} used to mount an S3 bucket as a <b>file</b> <b>system.</b> Note that as the semantics of the S3 <b>file</b> <b>system</b> are not that of a Posix <b>file</b> <b>system,</b> the <b>file</b> <b>system</b> may not behave entirely as expected.|$|R
50|$|Other Unix virtual <b>file</b> <b>systems</b> {{include the}} <b>File</b> <b>System</b> Switch in System V Release 3, the Generic <b>File</b> <b>System</b> in Ultrix, and the VFS in Linux. In OS/2 and Microsoft Windows, the virtual <b>file</b> <b>system</b> {{mechanism}} {{is called the}} Installable <b>File</b> <b>System.</b>|$|R
40|$|Abstract—Researches on {{technologies}} about testing {{aggregate bandwidth}} of <b>file</b> <b>systems</b> in cloud storage systems. Through the memory <b>file</b> <b>system,</b> network <b>file</b> <b>system,</b> parallel <b>file</b> <b>system</b> theory analysis, {{according to the}} cloud storage system polymerization bandwidth and concept, developed to cloud storage environment <b>file</b> <b>system</b> polymerization bandwidth test software called FSPoly. In this paper, use FSpoly to luster <b>file</b> <b>system</b> testing, find reasonable test methods, and then evaluations latest development in cloud storage <b>system</b> <b>file</b> <b>system</b> performance by using FSPoly. Keywords-cloud storage, aggregate bandwidth, <b>file</b> <b>system,</b> performance evaluation I...|$|R
40|$|We propose and {{evaluate}} an approach for decoupling persistent-cache management from general <b>file</b> <b>system</b> design. Several distributed <b>file</b> <b>systems</b> maintain a persistent cache {{of data to}} speed up accesses. Most of these <b>file</b> <b>systems</b> retain complete control over various aspects of cache management, such as granularity of caching, and policies for cache placement and eviction. Hardcoding cache management into the <b>file</b> <b>system</b> often results in sub-optimal performance as the clients of the <b>file</b> <b>system</b> are prevented from exploiting information about their workload in order to tune cache management. We introduce xCachefs, a framework that allows clients to transparently augment the cache management of the <b>file</b> <b>system</b> and customize the caching policy based on their resources and workload. xCachefs {{can be used to}} cache data persistently from any slow <b>file</b> <b>system</b> to a faster <b>file</b> <b>system.</b> It mounts over two underlying <b>file</b> <b>systems,</b> which can be local disk <b>file</b> <b>systems</b> like Ext 2 or remote <b>file</b> <b>systems</b> like NFS. xCachefs maintains the same directory structure as in the source <b>file</b> <b>system,</b> so that disconnected reads are possible when the source <b>file</b> <b>system</b> is down. ...|$|R
40|$|In this note, we {{introduce}} a simple <b>file</b> <b>system</b> implementation, known as vsfs (the Very Simple <b>File</b> <b>System).</b> This <b>file</b> <b>system</b> is a simplified {{version of a}} typical UNIX <b>file</b> <b>system</b> and thus serves to introduce {{some of the basic}} on-disk structures, access methods, and policies that you will find in many <b>file</b> <b>systems</b> today. The <b>file</b> <b>system</b> is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the <b>file</b> <b>system</b> work better (though we will want to pay attention to device characteristics to make sure the <b>file</b> <b>system</b> works well). Because of the great flexibility we have in building a <b>file</b> <b>system,</b> many different ones have been built, literally from AFS (the Andrew <b>File</b> <b>System)</b> to ZFS (Sun’s Zettabyte <b>File</b> <b>System).</b> All of these <b>file</b> <b>systems</b> have different data structures and and do some things better or worse than their peers. Thus, the way we will be learning about <b>file</b> <b>systems</b> is through case studies: first, a simple <b>file</b> <b>system</b> (vsfs) in this chapter to introduce most concepts, and then a series of studies of real <b>file</b> <b>systems</b> to understand how they can differ in practice...|$|R
5000|$|FFS2, Unix <b>File</b> <b>System,</b> Berkeley Fast <b>File</b> <b>System,</b> the BSD Fast <b>File</b> <b>System</b> or FFS ...|$|R
