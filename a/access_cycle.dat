21|150|Public
50|$|The Ashton Avenue Bridge is {{a former}} road-rail bridge located in Bristol, England. Grade II listed, it was {{constructed}} {{as part of the}} Bristol Harbour Railway, and is now part of a local <b>access</b> <b>cycle</b> path.|$|E
50|$|The CPU used an 8-bit {{microarchitecture}} {{with only}} a few hardware registers; everything that the programmer saw was emulated by the microprogram. Handling a 4-byte word took (at least) 6 microseconds, based on a 1.5 microsecond storage <b>access</b> <b>cycle</b> time.|$|E
50|$|Another {{strategy}} to handle suspend/resume is to reuse the exception logic. The machine takes an exception on the offending instruction, and all further instructions are invalidated. When the cache has {{been filled with}} the necessary data, the instruction that caused the cache miss restarts. To expedite data cache miss handling, the instruction can be restarted so that its <b>access</b> <b>cycle</b> happens one cycle after the data cache is filled.|$|E
50|$|When every CPU {{connected}} to the memory arbiter has synchronized memory <b>access</b> <b>cycles,</b> the memory arbiter can be designed as a synchronous arbiter.Otherwise the memory arbiter must be designed as an asynchronous arbiter.|$|R
50|$|The {{commands}} {{correspond to}} standard DRAM <b>access</b> <b>cycles,</b> such as row select, precharge, and refresh commands. Read and write commands include only column addresses. All commands include a 3-bit FB-DIMM address, allowing up to 8 FB-DIMM modules on a channel.|$|R
40|$|Abstract. This paper {{focuses on}} the Cyclops 64 {{computer}} architecture and presents an analytical model and performance simulation results for the preloading and loop unrolling approaches to optimize the performance of SVD (Singular Value Decomposition) benchmark. A performance model for dissecting the total execution cycles is presented. The data preloading using “memcpy ” or hand optimized “inline ” assembly code, and the loop unrolling approach are implemented and compared {{with each other in}} terms {{of the total number of}} memory <b>access</b> <b>cycles.</b> The key idea is to preload data from offchip to onchip memory and store the data back after the computation. These approaches can reduce the total memory <b>access</b> <b>cycles</b> and can thus improve the benchmark performance significantly. ...|$|R
50|$|EDO DRAM, {{sometimes}} referred to as Hyper Page Mode enabled DRAM, is similar to Fast Page Mode DRAM with the additional feature that a new <b>access</b> <b>cycle</b> can be started while keeping the data output of the previous cycle active. This allows a certain amount of overlap in operation (pipelining), allowing somewhat improved performance. It was 5% faster than FPM DRAM, which it began to replace in 1995, when Intel introduced the 430FX chipset that supported EDO DRAM.|$|E
50|$|In the {{original}} implementation, each main memory module contained 0.5 Mbytes of storage with parity protection constructed using 64K dynamic MOS RAMs. Random <b>access</b> <b>cycle</b> time was 500 ns per 32-bit word but multi-word transfers, for example {{to and from}} the cache, yielded an effective cycle time of 250 ns per 32-bit word (16 Mbytes per second). The memory modules decoded 26-bit physical word addresses and within this limit total memory capacity was restricted only by the number of available system bus slots; depending on the I/O configuration of the system, up to 10 Mbytes of physical memory could be installed. A later implementation of the memory module increased the size to 2 MB using 256K RAMs.|$|E
50|$|Despite being a 32-bit bus, Zorro III {{used the}} same 100 way slot and edge {{connector}} as Zorro II. The extra address and data lines were provided by multiplexing some of the existing connections {{with the nature of}} the lines changing at different stages of the bus <b>access</b> <b>cycle</b> (e.g. address becoming data). However, the bus was not fully multiplexed; the lower 8-bits of address were available during data cycles, which allowed Zorro III to support a fast burst cycle in page-mode. Of course, properly designed Zorro II expansion cards could coexist with Zorro III cards; it was not a requirement of a Zorro III bus master to support DMA access to Zorro II bus targets. Cards could detect a Zorro III vs. Zorro II backplane, allowing certain Zorro III cards to function when connected to the older Zorro II bus, though at Zorro II's reduced data rates.|$|E
40|$|The Draper {{fault-tolerant}} {{processor with}} fault-tolerant shared memory (FTP/FTSM), {{which is designed}} to allow application tasks to continue execution during the memory alignment process, is described. Processor performance is not affected by memory alignment. In addition, the FTP/FTSM incorporates a hardware scrubber device to perform the memory alignment quickly during unused memory <b>access</b> <b>cycles.</b> The FTP/FTSM architecture is described, followed by an estimate of the time required for channel reintegration...|$|R
40|$|This paper {{proposes a}} simple {{protocol}} for low-power sensor networks with battery-operated sensing devices. The sensors {{are expected to}} become active only when certain events in the environment occur. Therefore, a scheme for the application and medium <b>access</b> <b>cycles</b> is developed which avoids common problems of energy loss due to idle actions. Subsequently, the protocol is evaluated according to sensor lifetime. In contrast to common performance analysis, the approach considers the application behavior as a main impact on the sensor lifetime. Capabilities to save energy are derived. ...|$|R
5000|$|... #Caption: <b>Cycle</b> <b>access</b> ramp to the Kurushima-kaikyo Ohashi Bridge ...|$|R
50|$|Although partly shadowed {{by other}} design choices in this {{particular}} chip, the multiplexed address and data buses limit performance slightly; transfers of 16-bit or 8-bit quantities are done in a four-clock memory <b>access</b> <b>cycle,</b> which is faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions vary from one to six bytes, fetch and execution are made concurrent and decoupled into separate units (as it remains in today's x86 processors): The bus interface unit feeds the instruction stream to the execution unit through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations unfortunately became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, {{as well as other}} enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).|$|E
40|$|The paper investigates a multiserver {{queueing}} {{system with}} feedback {{and a computer}} used as an information processing unit. A simple model is proposed and fundamental equations describing the probabilistic behavior of the system in statistical equilibrium state are derived. Generating function for the stationary state probabilities is also given. Furthermore, by using the generating function, a necessary and sufficient condition {{for the existence of}} statistical equilibrium is obtained. In the limiting case when the <b>access</b> <b>cycle</b> becomes zero the queueing system considered reduce to an Erlang system. It is hoped that some of the results may serve as a guideline in the design of time-sharing systems for choice of a proper <b>access</b> <b>cycle</b> and for simulation...|$|E
40|$|Abstract- We {{designed}} a compact, high-speed, and low-power bank-type 12 -port register file test chip for highly-parallel pro-cessors in 0. 35 µm CMOS technology. In this full-custom test chip design, 72 % smaller area, 25 % shorter <b>access</b> <b>cycle</b> time, and 62 % lower power consumption are achieved {{in comparison to}} the conventional 12 -port-cell-based register file. I...|$|E
50|$|Pedestrian and <b>cycle</b> <b>access</b> {{from the}} Lee Valley Walk. Vehicular access at Wormley.|$|R
5000|$|Multiple {{arithmetic}} units {{may require}} memory architectures to support several <b>accesses</b> per instruction <b>cycle</b> ...|$|R
50|$|Pedestrian and <b>cycle</b> <b>access</b> by the towpath {{which is}} part of the Lea Valley Walk.|$|R
40|$|A {{traditional}} {{implementation of}} the set-associative cache has the disadvantage of longer <b>access</b> <b>cycle</b> times {{than that of a}} direct-mapped cache. Several methods have been proposed for implementing associativity in non-traditional ways. However, most of them have only achieved an associativity of two. The others with higher associativity still present longer <b>access</b> <b>cycle</b> times or suffer from larger average access times. In this paper, we first systematically address implementation issues of associativity and evaluate existing implementation methods. We then propose two schemes for implementing higher associativity: the Sequential Multi-Column Cache, which is an extension of the Column Associative Cache, and the Parallel Multi-Column Cache. In order to achieve the same <b>access</b> <b>cycle</b> time as that of a direct-mapped cache, data memory in the cache is organized into one bank in both schemes. We use the multiple MRU block technique to increase the first hit ratio, thus reducing the average access time. While the Parallel Multi-Column Cache performs the tag checking in parallel, the Sequential Multi-Column Cache sequentially searches through places in a set, and uses index information to filter out unnecessary probes. In the case of an associativity of 4, they both achieve the low miss rate of a 4 -way set-associative cache. Our simulation results using ATUM traces show that both schemes can effectively reduce the average access time. They have average improvements of 9. 8 % and 10. 8 % in average access time over a direct-mapped cache, respectively, for a cache size of 4 K bytes and a miss penalty of 20 cycles, in which case the average improvement of the Column Associative Cache is only 4. 3 %. The improvement of the Sequential Multi-Column Cache in average access time reaches 22. 4 % when the associativity is 8 and the miss penalty increases to 100 cycles. The two schemes are effective for both small and large caches (1 K bytes to 128 K bytes) ...|$|E
40|$|Memory {{subsystems}} {{have been}} considered {{as one of the}} most critical components in embedded systems and further-more, displaying increasing complexity as application re-quirements diversify. Modern embedded systems are gener-ally equipped with multiple heterogeneous memory devices to satisfy diverse requirements and constraints. NAND flash memory has been widely adopted for data storage because of its outstanding benefits on cost, power, capacity and non-volatility. However, in NAND flash memory, the intrinsic costs for the read and write accesses are highly dispropor-tionate in performance and access granularity. The conse-quent data management complexity and performance deteri-oration have precluded the adoption of NAND flash memory. In this paper, we introduce a highly effective non-volatile primary memory architecture which incorporates applica-tion specific information to develop a NAND flash based pri-mary memory. The proposed architecture provides a unified non-volatile primary memory solution which relieves design complications caused by the growing complexity in mem-ory subsystems. Our architecture aggressively minimizes the overhead and redundancy of the NAND based systems by exploiting efficient address space management and dy-namic data migration based on accurate application behav-ioral analysis. We also propose a highly parallelized memory architecture through an active and dynamic data redistri-bution over the multiple flash memories based on run-time workload analysis. The experimental results show that our proposed architecture significantly enhances average mem-ory <b>access</b> <b>cycle</b> time which is comparable to the standard DRAM <b>access</b> <b>cycle</b> time and also considerably prolongs the device life-cycle by autonomous wear-leveling and minimiz-ing the program/erase operations...|$|E
30|$|The {{black and}} white {{reference}} images are {{of the same size}} as the incoming video frames. Due to the significant amount of data, these images are stored in an external DRAM connected to the FPGA. Because both video streams are synchronized at the pixel level, external memory accesses can be structured to reduce the memory bandwidth requirements. To this end, the reference images for both cameras are stored in interleaved order, such that random accesses are avoided: one value for one pixel of each left and right black image and one value for each left and right white image can be read together in a single <b>access</b> <b>cycle.</b>|$|E
50|$|Pedestrian and <b>cycle</b> <b>access</b> via the towpath {{that forms}} {{part of the}} Lea Valley Walk.|$|R
50|$|Walking and <b>cycle</b> <b>access</b> {{along the}} towpath that forms {{part of the}} Lea Valley Walk.|$|R
5000|$|Pedestrian and <b>cycle</b> <b>access</b> via the towpath {{which forms}} {{part of the}} Lea Valley Walk ...|$|R
40|$|The {{importance}} of the dynamic analysis for SRAM operation increases {{as a result of}} shrinking <b>access</b> <b>cycle</b> time, voltage scaling and increased process variations. In this paper, quantitative study of the dynamic read noise margin (DNM) is introduced showing the evolution from the static read noise margin (SNM) to DNM through cumulative dynamic effects in 28 nm FDSOI. The impact of parasitic capacitances on the DNM is further analyzed. Finally, we show that by sizing for a 150 -mV DNM instead of a 150 -mV SNM and by inserting two 0. 5 fF extra caps in the bitcell allows reducing the pull-down NMOS width by a factor 3. 5 ×...|$|E
40|$|In this paper, {{we propose}} a {{processor}} architecture with programmable on-chip memory for a high-performance SMP (symmetric multi-processor) node named SCIMA-SMP (Software Controlled Integrated Memory Architecture for SMP) {{with the intent}} of solving the performance gap problem between a processor and o#-chip memory. With special instructions which enable the explicit data transfer between onchip memory and o#-chip memory, this architecture is able to control the data transfer timing and its granularity by the application program, and the SMP bus is utilized e#ciently compared with traditional cache-only architecture. Through the performance evaluation based on clock-level simulation for various HPC applications, we confirmed that this architecture largely reduces the bus <b>access</b> <b>cycle</b> by avoiding redundant data transfer and controlling the granularity of the data movement between on-chip and o#-chip memory...|$|E
40|$|Conventionally, {{the test}} of {{multiport}} memories is considered difficult because of the complex behavior of the faulty memories and {{the large number of}} inter-port faults. This paper presents an efficient approach for testing and diagnosing multiport RAMs. Our approach takes advantage of the higher access bandwidth due to the increased number of read/write ports, which also provides higher observability and controllability that effectively reduces the test time. A sequence of March operations can be folded and executed within a single <b>access</b> <b>cycle.</b> Based on the idea, multiple operations applying on a certain memory cell are able to be performed in a single cycle. We have also developed an efficient test algorithm for the port-specific faults as well as the traditional cell faults. The port-specific faults include the stuck-open, address decoder, and inter-port faults, for both bit-oriented and word-oriented RAMs. Experimental results for our folding scheme show that the test time reduction is about 28 % for a commercial 8 KB embedded SRAM. An efficient diagnostic algorithm is also proposed for the port-specific faults and traditional cell faults. ...|$|E
5000|$|The data TLB has {{two copies}} which keep {{identical}} entries. The two copies allow two data <b>accesses</b> per <b>cycle</b> to translate virtual addresses to physical addresses. Like the instruction TLB, this TLB is {{split into two}} kinds of entries.|$|R
50|$|There is {{pedestrian}} and <b>cycle</b> <b>access</b> via the towpath {{which also}} forms {{part of the}} Lea Valley Walk.|$|R
50|$|Close to {{the village}} is the local area {{headquarters}} of the Forestry Commission. The commission {{has been instrumental in}} developing a mountain biking centre, which provides <b>access</b> to <b>cycle</b> trails of various difficulty. To cater for visitors, there is a small café and a bike shop.|$|R
40|$|This paper {{describes}} a low-power object recognition processor VLSI for HDTV resolution video at 60 {{frames per second}} (fps) using an object recognition algorithm with Sparse FIND features. The VLSI processor features two-stage feature extraction processing by HOG and Sparse FIND, a highly parallel classification in the support vector machine (SVM), and a block-parallel processing for RAM <b>access</b> <b>cycle</b> reduction. Compared to the accuracy by the original Sparse FIND algorithm, the two-stage object detection demonstrates insignificant accuracy degradation. Using this architectural design, a 60 fps performance for object recognition of HDTV resolution video was attained at an operating frequency of 130 MHz. This 3. 35 x 3. 35 mm(2) chip, designed with 40 nm CMOS technology, contains 8. 22 M gates and 5 Mb SRAM in the chip of 3. 35 x 3. 35 mm(2). The simulated power consumption at 133 MHz were 528 mW and 702 mW at the slow process condition (SS, 0. 81 V, - 40 °C) and typical process condition (TT, 0. 9 V, 25 °C), respectively...|$|E
40|$|In this paper, {{we propose}} an {{adaptive}} Medium Access Control (MAC) protocol for full-duplex (FD) cognitive radio networks in which FD secondary users (SUs) perform channel contention followed by concurrent spectrum sensing and transmission, and transmission only with maximum power {{in two different}} stages (called the FD sensing and transmission stages, respectively) in each contention and <b>access</b> <b>cycle.</b> The proposed FD cognitive MAC (FDC-MAC) protocol does not require synchronization among SUs and it efficiently utilizes the spectrum and mitigates the self-interference in the FD transceiver. We then develop a mathematical model to analyze the throughput performance of the FDC-MAC protocol where both half-duplex (HD) transmission (HDTx) and FD transmission (FDTx) modes are considered in the transmission stage. Then, we study the FDC-MAC configuration optimization through adaptively controlling the spectrum sensing duration and transmit power level in the FD sensing stage where we prove that there exists optimal sensing time and transmit power to achieve the maximum throughput and we develop an algorithm to configure the proposed FDC-MAC protocol. Extensive numerical results are presented to illustrate the characteristic of the optimal FDC-MAC configuration and the impacts of protocol parameters and the self-interference cancellation quality on the throughput performance. Moreover, we demonstrate the significant throughput gains of the FDC-MAC protocol with respect to existing half-duplex MAC (HD MAC) and single-stage FD MAC protocols. Comment: To Appear, IEEE Access, 201...|$|E
40|$|This thesis {{presents}} digital {{design and}} implementation of a controller module for serial flash memories. Firstly, the platform including the serial flash memory controller, flash memories and SPI (Serial Peripheral Interface) protocol have been investigated to solve the current problems related with controlling of serial flash memories. Then, in the implementation part of the thesis, the Serial Flash Memory Controller module has been designed by using VHDL (VHSIC Hardware Description Language-VHDL) and synthesized in CMOS 0. 35 Mm technology. Functional and gate-level simulations have been done with Cadence simulator. Lastly the final gate level netlist has been placed and routed with Cadence Silicon Ensemble. A {{great deal of attention}} has been given to design a generic controller that needs simple software and minimum processor <b>access</b> <b>cycle.</b> It is programmed from the processor for different operations of serial flash memories. The structure of the frame, control data and timings are controlled by hardware according to the programmed operation. In addition to this, our Serial Flash Memory Controller module can be used with different flash memories. This is very important property for reusability of the module. The Serial Flash Memory Controller module is capable to work up to 20 MHz serial communication speed and it can be integrated to processor platforms that have AMBA (Advanced Microcontroller Bus Architecture) APB (Advanced Peripheral Bus) interface...|$|E
40|$|Abstract: Consumer {{electronics}} vendors increasingly deploy shared-memory multiprocessor SoCs, such as Philips Nexperia, {{to balance}} flexibility (late changes, software download, reuse) and cost (silicon area, power consumption) requirements. With {{the convergence of}} storage, digital television, and connectivity, these media-processing systems must support numerous operational modes. Within a mode, the system concurrently processes many streams, each imposing a potentially dynamic workload on the scarce system resources. The dynamic sharing of scarce resources is known to jeopardize robustness and predictability. Resource reservation is an accepted approach to tackle this problem. This chapter applies the resource reservation paradigm to interrelated SoC resources: processor cycles, cache space, and memory <b>access</b> <b>cycles.</b> The presented virtual platform approach aims to integrate the reservation mechanisms of each shared SoC resource {{as the first step}} towards robust, yet flexible and cost-effective consumer products...|$|R
40|$|Due to {{the infamous}} “memory wall ” problem and a drastic {{increase}} in the number of data intensive applications, memory rather than processor has become the leading performance bottleneck of modern computing systems. Evaluating and understanding memory system performance is increasingly becoming the core of high-end computing. Conventional memory metrics, such as miss ratio, average miss latency, average memory access time, etc., are designed to measure a given memory performance parameter, and do not reflect the overall performance of a memory system. On the other hand, widely used system measurement metrics, such as IPC and Flops are designed to measure CPU performance, and do not directly reflect memory performance. In this paper, we proposed a novel memory metric, <b>Access</b> Per <b>Cycle</b> (APC), to measure overall memory performance with consideration of the complexity of modern memory systems. A unique contribution of APC is its separation of memory evaluation from CPU evaluation; therefore, it provides a quantitative measurement of the “data-intensiveness ” of an application. The concept of APC is introduced; a constructive investigation counting the number of data <b>accesses</b> and <b>access</b> <b>cycles</b> at differing levels of the memory hierarchy is conducted; finally some important usages of APC are presented. Simulation results show that APC is significantly more appropriate than existing memory metrics in evaluating modern memory systems...|$|R
50|$|There {{is a basic}} {{waiting room}} and {{sheltered}} area on each platform with some bench seating. The station has <b>cycle</b> <b>access</b> but no wheelchair access.|$|R
