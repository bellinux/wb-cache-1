3|456|Public
40|$|In 1975 Parsons {{developed}} his dictionary of musical themes {{based on a}} simple contour representation. The motivation was that people with little training in music {{would be able to}} identify pieces of music. We decided to test whether people of various levels of musical skill could indeed make use of a text representation to describe a simple melody query. The results indicate that the task is beyond those who are unmusical, and that a scale numeric representation is easier than a contour one for those of moderate musical skill. Further, a common error when using the scale representation still yields a more accurate contour representation than if a user is asked to enter a contour query. We observed an <b>average</b> <b>query</b> <b>length</b> of about seven symbols for the retrieval task. ...|$|E
40|$|One of {{the main}} bottle-necks in {{providing}} more effective information access is the poverty of the query end. With an <b>average</b> <b>query</b> <b>length</b> of about two terms, users provide only a highly ambiguous statement of the, often complex, underlying information need. Implicit and explicit feedback can provide us with additional information that can help disambiguate the query and provide more focused search results. We investigate the effects of using different types of feedback. Retrieval results of pseudo-relevance, explicit relevance and topical feedback are compared. Although on average explicit relevance feedback in combination with pseudo relevance feedback works best, for individual queries results are unpredictable. There is a large potential for improvement if we can predict which type of feedback will perform best for a query. Since {{we are dealing with}} feedback potentially provided by users standard evaluation measures are not sufficient to evaluate feedback techniques, and the quality of user interaction should also be taken into account...|$|E
40|$|As {{a plethora}} of various {{distributed}} applications emerge, new computing platforms are necessary to support their extra and sometimes evolving requirements. This research derives its motive from deficiencies of real networked applications deployed on platforms unable to fully support their characteristics and proposes a network architecture to address that issue. Hoverlay {{is a system that}} enables logical movement of nodes from one network to another aiming to relieve requesting nodes, which experience high workload. Node migration and dynamic server overlay differentiate Hoverlay from Condor-based architectures, which exhibit more static links between managers and nodes. In this paper, we present a number of important extensions to the basic Hoverlay architecture, which collectively enhance the degree of control owners have over their nodes and the overall level of cooperation among servers. Furthermore, we carried out extensive simulations, which proved that Hoverlay outperforms Condor and Flock of Condors in both success rate and <b>average</b> successful <b>query</b> path <b>length</b> at a negligible increase in messages...|$|R
40|$|We analyse <b>query</b> <b>length,</b> and fit power-law and Poisson {{distributions}} to {{four different}} query sets. We provide a practical model for <b>query</b> <b>length,</b> {{based on the}} truncation of a Poisson distribution for short queries and a power-law distribution for longer queries, that better fits real <b>query</b> <b>length</b> distributions than earlier proposals...|$|R
40|$|<b>Query</b> <b>length</b> in best-match {{information}} retrieval (IR) systems {{is well known}} to be positively related to effectiveness in the IR task, when measured in experimental, non-interactive environments. However, in operational, interactive IR systems, <b>query</b> <b>length</b> is quite typically very short, {{on the order of}} two to three words. We report on a study which tested the effectiveness of a particular query elicitation technique in increasing initial searcher <b>query</b> <b>length,</b> and which tested the effectiveness of queries elicited using this technique, and the relationship in general between <b>query</b> <b>length</b> and search effectiveness in interactive IR. Results show that the specific technique results in longer queries than a standard query elicitation technique, that this technique is indeed usable, that the technique results in increased user satisfaction with the search, and that <b>query</b> <b>length</b> is positively correlated with user satisfaction with the search...|$|R
3000|$|... b {{shows the}} <b>average</b> <b>query</b> {{response}} time {{based on the}} last QR message received. RTC has the <b>average</b> <b>query</b> response time slightly increasing {{with the number of}} query sessions, which is consistent with the number of transmitted messages in Fig. 8 [...]...|$|R
50|$|A newer GPU CUDA {{implementation}} of SW {{is now available}} that is faster than previous versions and also removes limitations on <b>query</b> <b>lengths.</b> See CUDASW++.|$|R
5000|$|<b>Query</b> <b>length</b> {{should be}} non-fixed, i.e., a query {{can be as}} long as deemed necessary. A sentence, a paragraph, or even an entire {{document}} can be submitted as a query.|$|R
40|$|Recently, Sanders and Schultes {{presented}} a shortest path algorithm, named Highway Hierarchies, for fast point-to-point shortest path queries. They report extremely quick <b>average</b> <b>queries</b> {{on the road}} network of USA. We consider the I/O efficiency of the algorithm and investigate how a good graph layout affects the <b>average</b> <b>query</b> time. We experiment with a few layout heuristics and obtain a speed-up factor of around 1. 3 {{as compared to the}} default layout and around 1. 7 as compared to a random layout. ...|$|R
40|$|Understanding the {{specificity}} of Web search queries can help search systems better address the underlying needs of searchers and provide them relevant content. The goal of this work is to automatically determine {{the specificity}} of web search queries. Although many factors may impact {{the specificity of}} Web search queries, we investigate two factors of specificity in this research, (1) part of speech and (2) <b>query</b> <b>length.</b> We use content analysis and prior research to develop a list of nine attributes to identify query specificity. The attributes are whether a query contains a URL, a location or place name along with additional terms, compares multiple things, contains multiple distinct ideas or topics, a question that has a clear answer, request for directions, instructions or tips, a specific date and additional terms or a name and additional terms. We then apply these attributes to classify 5, 115 unique queries as narrow or general. We then analyze the differences between narrow and general queries based on part of speech and <b>query</b> <b>length.</b> Our results indicate that <b>query</b> <b>length</b> and parts-of-speech usage, by themselves, can distinguish narrow and general queries. We discuss {{the implications of this}} work for search engines, marketers and users...|$|R
40|$|This paper {{addresses}} {{the problem of}} minimizing the <b>average</b> <b>query</b> complexity of inferring a pair of nested monotone Boolean functions defined on { 0, 1 } n using a pair of oracles. Here, nested refers to the case {{when one of the}} functions is always {{greater than or equal to}} the other function. It is shown that the nested case is equivalent to inferring the single function case defined on { 0, 1 } n+ 1 when access to the two oracles is unrestricted. Two common examples of restricted oracles, namely sequential oracles and a single three-valued oracle, are also analyzed. The most efficient known approach to minimizing the <b>average</b> <b>query</b> complexity in inferring a single monotone Boolean function is based on a query selection criterion. It is shown that the selection criterion approach is easily modified for use with restricted oracles. Several real world examples illustrate the necessity and sufficiency of the nested monotone Boolean function model. Extensive computational results indicate that the nestedness assumption reduces the <b>average</b> <b>query</b> complexity by a few percent. This is a dramatic improvement considering the fact that this complexity is exponential in n...|$|R
40|$|In {{the reverse}} {{complement}} (RC) equivalence model, {{it is not}} possible to distinguish between a string and its reverse complement. We show that one can still reconstruct a binary string of length n, up to reverse complement, using a linear number of subsequence <b>queries</b> of bounded <b>length.</b> A simple information theoretic lower bound proves the number of queries to be tight. Our result is also optimal w. r. t. the bound on the <b>query</b> <b>length</b> given in [Erdos et al., Ann. of Comb. 2006]...|$|R
2500|$|BLAT {{can handle}} long {{database}} sequences, however, {{it is more}} effective with short query sequences than long query sequences. Kent recommends a maximum <b>query</b> <b>length</b> of 200,000 bases. The UCSC browser limits query sequences to less than 25,000 letters (i.e. nucleotides) for DNA searches and less than 10,000 letters (i.e. amino acids) for protein and translated sequence searches.|$|R
40|$|In this paper, we {{demonstrate}} that verbose and grammatically complex <b>queries</b> retrieve, on <b>average,</b> more diverse results across different search engines than the short keyword queries. Our evaluation using both commercial and open source search engines {{shows that the}} overlap between search engine results decreases by up to 50 % {{as a function of}} <b>query</b> <b>length.</b> 1...|$|R
5000|$|BLAT {{can handle}} long {{database}} sequences, however, {{it is more}} effective with short query sequences than long query sequences. Kent recommends a maximum <b>query</b> <b>length</b> of 200,000 bases. The UCSC browser limits query sequences to less than 25,000 letters (i.e. nucleotides) for DNA searches and less than 10,000 letters (i.e. amino acids) for protein and translated sequence searches.|$|R
40|$|When searching, people’s {{information}} needs flow through to expressing an information retrieval request posed to a search engine. We hypothesise {{that the degree}} of specificity of an IR request might correspond to the length of a search query. Our results show a strong correlation between decreasing <b>query</b> <b>length</b> and increasing broadness or generality of the IR request. We found an average cross-over point of specificity from broad to narrow of 3 words in the query. These results have implications for search engines in responding to <b>queries</b> of differing <b>lengths...</b>|$|R
40|$|A music {{retrieval}} system that matches a short <b>length</b> music <b>query</b> with its variations in a database is proposed. In {{order to avoid}} the negative effects of different orchestration and performance style and tempo on transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using n-grams of sizes 2 through 6. We observe improvements in retrieval performance with increasing <b>query</b> <b>length</b> and n-gram order. The improvement converges to a little over one for all <b>query</b> <b>lengths</b> tested when n reaches 6. 1...|$|R
40|$|The {{retrieval}} {{effectiveness of}} Query Expansion (QE) {{is very much}} dependent {{on the ability to}} accurately identify and expand core concepts which are truly representative of the intended search goal. Two characteristics of natural language queries which hinder the performance of query expansion for information retrieval are <b>query</b> <b>length</b> and structure. The varying <b>lengths</b> of a <b>query</b> translate to the number of core concepts that may exist and the possibility of there being multiple query intents embedded within a single query. On the other hand, the structure of queries reveals the linguistic properties which allows for the determination of whether they take the form of well-formed sentences or are simply bags-of-words which in the strictest sense are a series of words with no obvious relations amongst them. Whilst <b>query</b> <b>lengths</b> are easily assessed, we propose a two-level automated classification technique consisting of linguistics based and statistical processing for query structure classification. The proposed method has revealed high levels of classification accuracy on TREC ad hoc test queries...|$|R
3000|$|... c. Because {{the query}} disseminations in DRIVE were {{terminated}} before covering {{all of the}} selected road segments, its <b>average</b> <b>query</b> response time is much lower. Also, because DRIVE uses a single broadcast message in each hop, {{the average number of}} transmitted messages is much smaller {{at the expense of the}} a less percentage of road segment coverage.|$|R
40|$|In general, ranking {{entities}} (resources) on the Semantic Web (SW) {{is subject}} to importance, relevance, and <b>query</b> <b>length.</b> Few existing SW search systems cover all of these aspects. Moreover, many existing efforts simply reuse the technologies from conventional Information Retrieval (IR), which are not designed for SW data. This paper proposes a ranking mechanism, which includes all three categories of rankings and are tailored to SW data. ...|$|R
40|$|AbstractIn {{the reverse}} {{complement}} equivalence model, {{it is not}} possible to distinguish a string from its reverse complement. We show that one can still reconstruct a string of length n, up to reverse complement, using a linear number of subsequence <b>queries</b> of bounded <b>length.</b> We first give the proof for strings over a binary alphabet, and then extend it to arbitrary finite alphabets. A simple information theoretic lower bound proves the number of queries to be asymptotically tight. Furthermore, our result is optimal w. r. t. the bound on the <b>query</b> <b>length</b> given in Erdős et al. (2006) [6]...|$|R
40|$|Abstract. We analyse the {{statistical}} behavior of query-associated quantities in query-logs, namely, the sum and mean of IDF of query terms, {{otherwise known as}} query specificity and query mean specificity. We narrow down the possibilities for modeling their distributions to gamma, log-normal, or log-logistic, depending on <b>query</b> <b>length</b> and on whether the sum or the mean is considered. The results have applications in query performance prediction and artificial query generation. ...|$|R
40|$|Abstract. In general, ranking {{entities}} (resources) on the Semantic Web (SW) {{is subject}} to importance, relevance, and <b>query</b> <b>length.</b> Few existing SW search systems cover all of these aspects. Moreover, many existing efforts simply reuse the technologies from conventional Information Retrieval (IR), which are not designed for SW data. This paper proposes a ranking mechanism, which includes all three categories of rankings and are tailored to SW data...|$|R
40|$|Our focus {{this year}} was to {{investigate}} methods for increasing <b>query</b> <b>length</b> in interactive information searching in the Web context, and to see if these methods led to changes in task performance and/or interaction. Thirty-four subjects each searched on four of the Interactive Track topics, {{in one of two}} conditions: a 'qox" query input mode; and a "line" query input mode. One-half of the subjects were inst'ucted to enter their queries as complete sentences or questions; the other half as lists of words or phrases. Results are that: queries entered as questions or statements were significantly longer than those entered as words or phrases (twice as long); that {{there was no difference in}} <b>query</b> <b>length</b> between the box and line modes (except for medical topics, where keyword mode led to significantly more unique terms per search); and, that longer queries led to better performance. Other results of note are that satisfaction with the search was negatively correlated with length of time searching and other measures of interaction effort, and that the 'quying" topics were significantly more difficult than the other three types...|$|R
30|$|The <b>average</b> <b>query</b> {{execution}} time uses the average location server to query an anonymous location time. It {{is used to}} measure the position of the server query execution cost and query time spent. If the server query execution cost is lower, the performance of algorithm will be improved. The location server simulation experiment, the hidden subgraph for a Kmm query, tests the performance of the proposed algorithm.|$|R
40|$|This paper reports {{selected}} {{findings from}} an ongoing {{series of studies}} analyzing large-scale data sets containing queries posed by Excite users, a major Internet search service. The findings presented report on: (1) <b>queries</b> <b>length</b> and frequency, (2) Boolean queries, (3) query reformulation, (4) phrase searching, (5) search term distribution, (6) relevance feedback, (7) viewing pages of results, (8) successive searching, (9) sexually-related searching, (10) image queries and (11) multi-lingual aspects. Further research is discussed...|$|R
40|$|The recent {{file storage}} {{applications}} built {{on top of}} peer-to-peer distributed hash tables lack search capabilities. We believe that search {{is an important part}} of any document publication system. To that end, we have designed and analyzed a distributed search engine based on a distributed hash table. Our simulation results predict that our search engine can answer an <b>average</b> <b>query</b> in under one second, using under one kilobyte of bandwidth...|$|R
40|$|The Mobile nodes are {{communicating}} {{with each other}} without centralized administration and data is accessed from the data source through multi-hop environment. The accessed data is stored in the mobile node’s cache for its own and neighbor’s future use. Caching is a significant process to store the frequently accessed data item in the MANET. Data availability and accessibility is a challenging task due to mobility of nodes, limited battery power and insufficient bandwidth. Cooperative caching addresses these challenges to improve the data availability and efficiency of data access by sharing and coordination among the mobile nodes. These challenges have received {{a tremendous amount of}} concentration from researchers and led to development of many different cooperative caching strategies. This paper attempts to provide the review and hypothetical analysis of various cooperative caching strategies in the mobile ad hoc networks based on their performance metrics such as cache hit ratio and <b>average</b> <b>query</b> delay with respect to cache size and number of mobile nodes. The Global Cluster Cooperative caching provides better performance than others in terms of cache hit and <b>average</b> <b>query</b> delay...|$|R
40|$|Abstract. We {{discuss the}} {{construction}} of probabilistic models centering on temporal patterns of query refinement. Our analyses are derived from a large corpus of Web search queries extracted from server logs recorded by a popular Internet search service. We frame the modeling task in terms of pursuing an understanding of probabilistic relationships among temporal patterns of activity, informational goals, and classes of query refinement. We construct Bayesian networks that predict search behavior, {{with a focus on}} the progression of queries over time. We review a methodology for abstracting and tagging user queries. After presenting key statistics on <b>query</b> <b>length,</b> <b>query</b> frequency, and informational goals, we describe user models that capture the dynamics of query refinement. ...|$|R
40|$|We explore {{retrieval}} {{effectiveness of}} vectorial, link analyses, probabilistic and classification retrieval system parameters {{and compare the}} different performance in a controlled environment (using WT 10 g collection from TREC). Main retrieval parameters are captured from related to: (1) retrieval methods (e. g., vectorial, link analyses, probabilistic and classification); (2) main internal system parameters (e. g., <b>query</b> <b>length,</b> URL length, phrase, feedback, index); (3) and also internal system parameters combination (e. g., combination of different <b>query</b> and URL <b>lengths,</b> phrase, feedback and index) and we concluded about most important parameters, retrieval method performance and that combination can improve results. We analyses many cases from around 500 retrieval systems using our own modular platform, WebSearchTester...|$|R
30|$|Figure  7 {{illustrates}} the <b>average</b> <b>query</b> execution times on DG-SPARQL and Giraph for the 20 instances {{of each of}} the identified four query types on the four experimental datasets of the LUMB benchmark. The results of the experiments show that DG-SPARQL is able to outperform Giraph with orders of magnitudes on the various query types. It also shows that DG-SPARQL is able to scale well to handle the increasing datasets of the LUMB benchmark.|$|R
40|$|This {{paper is}} an {{experimental}} {{study on the}} performance of the two major methods for macro-level similarity measurement: Hnear weighted merging and logical retrieval. Performance is measured as the <b>average</b> <b>query</b> execution time for a significant number of tests. The two models were implemented in the standard version (as they are applied in a number of prototypes) and in an optimized version. The results show, that optimized logical retrieval clearly outperforms optimized linear weighted merging...|$|R
40|$|With fast {{changing}} {{information needs}} in today’s world, {{it is imperative}} that search engines precisely understand and exploit temporal changes in Web queries. In this work, we look at shifts in preferred positions of segments in queries over an interval of four years. We find that such shifts can predict key changes in usage patterns, and explain the observed increase in <b>query</b> <b>lengths.</b> Our findings indicate that recording positional statistics can be vital for understanding user intent in Web search queries...|$|R
40|$|In {{this paper}} we {{investigate}} a Persian search engine log and present a comprehensive analysis of question queries in three levels: structure, click and topic. By analyzing question queries characteristics, we explore behavior of Persian language users. Our experimental {{results show that}} question <b>queries</b> <b>length</b> are larger than normal queries. Most of these queries contained question words 2 ̆ 7 How 2 ̆ 7 and 2 ̆ 7 What 2 ̆ 7 and their topics were mainly about health, policy, religion and society...|$|R
40|$|We {{examine the}} issues of {{combining}} multiple query representations in a single IR engine. Differing query representations are used to retrieve different documents. Thus, when combining their results, improvements are observed in effectiveness. We use multiple TREC query representations (title, description and narrative) {{as a basis for}} experimentation. We examine several combination approaches presented in the literature (vector addition, CombSUM and CombMNZ) and present a new combination approach using <b>query</b> vector <b>length</b> normalization. We examine two query representation combination approaches (title + description and title + narrative) for 150 queries from TREC 6, 7 and 8 topics. Our QLN (<b>Query</b> <b>Length</b> Normalization) technique outperformed vector addition and data fusion approaches by as much as 32 % and was on average 24 % better. Additionally, QLN always outperformed the single best query representation in terms of effectiveness...|$|R
40|$|Increasing {{need for}} {{large-scale}} data analytics {{in a number}} of ap-plication domains has led to a dramatic {{rise in the number of}} dis-tributed data management systems, both parallel relational databases, and systems that support alternative frameworks like MapReduce. There is thus an increasing contention on scarce data center re-sources like network bandwidth (especially cross-rack bandwidth); further, the energy requirements for powering the computing equip-ment are also growing dramatically. As we show empirically, in-creasing the execution parallelism by spreading out data across a large number of machines may achieve the intended goal of de-creasing query latencies, but in most cases, may increase the total resource and energy consumption significantly. For many analyt-ical workloads, however, minimizing query latencies is often not critical; in such scenarios, we argue that we should instead focus on minimizing the <b>average</b> <b>query</b> span, i. e., the average number of machines that are involved in processing of a query, through co-location of data items that are frequently accessed together. In this work, we exploit the fact that most distributed environments need to use replication for fault tolerance, and we devise workload-driven replica selection and placement algorithms that attempt to mini-mize the <b>average</b> <b>query</b> span. We model a historical query work-load trace as a hypergraph over a set of data items (which could be relation partitions, or file chunks), and formulate and analyze the problem of replica placement by drawing connections to several well-studied graph theoretic concepts. We use these connections to develop a series of algorithms to decide which data items to repli-cate, and where to place the replicas. We show effectiveness of our proposed approach by building a trace-driven simulation frame-work and by presenting results on a collection of synthetic and real workloads. Our experiments show that careful data placement and replication can dramatically reduce the <b>average</b> <b>query</b> spans result-ing in significant reductions in the resource consumption. 1...|$|R
