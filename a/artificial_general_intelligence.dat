243|2257|Public
25|$|In December 2015 it was {{announced}} that Thiel is one of the financial backers of OpenAI, a non-profit company aimed at the safe development of <b>artificial</b> <b>general</b> <b>intelligence.</b>|$|E
25|$|At the 2012 Singularity Summit, Stuart Armstrong did a {{study of}} <b>artificial</b> <b>general</b> <b>intelligence</b> (AGI) {{predictions}} by experts and found {{a wide range of}} predicted dates, with a median value of 2040.|$|E
25|$|Researchers in the 1960s and the 1970s were {{convinced}} that symbolic approaches would eventually succeed in creating a machine with <b>artificial</b> <b>general</b> <b>intelligence</b> and considered this the goal of their field.|$|E
5000|$|... #Subtitle level 2: Research and {{development}} of <b>General</b> <b>Artificial</b> <b>Intelligence</b> ...|$|R
5000|$|In January 2014, Rosa started {{actively}} {{pursuing the}} field of <b>general</b> <b>artificial</b> <b>intelligence.</b> In his interview to Singularity Weblog, he explains that his curiosity about how things work and dream of working on AI was there since his childhood, but he chose {{to start with the}} game development to prepare for it [...] Rosa perceives general AI as a leverage would allow humankind to automate and optimize all the processes: science, medicine, technology, exploration - ultimately, create better future and understand the universe.On July 7, 2015 Marek Rosa announced GoodAI, his company in <b>general</b> <b>artificial</b> <b>intelligence</b> research and development. He invested his personal $10 million into the <b>general</b> <b>artificial</b> <b>intelligence</b> company, and personally takes active part in the research and development.|$|R
40|$|We outline {{a general}} {{conceptual}} definition of real-world <b>general</b> <b>intelligence</b> that avoids the twin pitfalls of excessive mathematical generality, and excessive anthropomorphism [...] Drawing on prior literature, {{a definition of}} <b>general</b> <b>intelligence</b> is given, which defines the latter by reference to an assumed measure of the simplicity of goals and environments. The novel contribution presented is to gauge the simplicity of an entity {{in terms of the}} ease of communicating it within a community of embodied agents (the so-called Embodied Communication Prior or ECP). Augmented by some further assumptions about the statistical structure of communicated knowledge, this choice is seen to lead to a model of intelligence in terms of distinct but interacting memory and cognitive subsystems dealing with procedural, declarative, sensory/episodic, attentional and intentional knowledge. A sister paper then extends these ideas to yield a “Cognitive Synergy Theory ” that suggests specific conclusions for the architecture of <b>artificial</b> <b>general</b> <b>intelligences,</b> based on the ECP...|$|R
25|$|Many {{researchers}} {{think that}} their work will eventually be incorporated into a machine with <b>artificial</b> <b>general</b> <b>intelligence,</b> combining all the skills mentioned above and even exceeding human ability in most or all these areas. A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.|$|E
25|$|Kurzweil {{has also}} {{directed}} his own adaptation, called The Singularity is Near, which mixes documentary with a science-fiction story involving his robotic avatar Ramona's transformation into an <b>artificial</b> <b>general</b> <b>intelligence.</b> It was screened at the World Film Festival, the Woodstock Film Festival, the Warsaw International FilmFest, the San Antonio Film Festival in 2010 and the San Francisco Indie Film Festival in 2011. The movie was released generally on July 20, 2012. It {{is available on}} DVD or digital download and a trailer is available.|$|E
25|$|I. J. Good speculated in 1965 that <b>artificial</b> <b>general</b> <b>{{intelligence}}</b> {{might bring}} about an intelligence explosion. Good's scenario runs as follows: as computers increase in power, it becomes possible {{for people to}} build a machine that is more intelligent than humanity; this superhuman intelligence possesses greater problem-solving and inventive skills than current humans are capable of. This superintelligent machine then designs an even more capable machine, or re-writes its own software to become even more intelligent; this (ever more capable) machine {{then goes on to}} design a machine of yet greater capability, and so on. These iterations of recursive self-improvement accelerate, allowing enormous qualitative change before any upper limits imposed by the laws of physics or theoretical computation set in.|$|E
40|$|Human {{speech is}} the most {{important}} part of <b>General</b> <b>Artificial</b> <b>Intelligence</b> and subject of much research. The hypothesis proposed in this article provides explanation of difficulties that modern science tackles in the field of human brain simulation. The hypothesis is based on the author's conviction that the brain of any given person has different ability to process and store information. Therefore, the approaches that are currently used to create <b>General</b> <b>Artificial</b> <b>Intelligence</b> have to be altered...|$|R
40|$|When {{the first}} <b>artificial</b> <b>general</b> <b>intelligences</b> are built, they may improve {{themselves}} to far-above-human levels. Speculations about such future entities are already affected by anthropomorphic bias, {{which leads to}} erroneous analogies with human minds. In this chapter, we apply a goal-oriented understanding of intelligence to show that humanity occupies only a tiny portion of the design space of possible minds. This space is much larger than what we are familiar with from the human example; and the mental architectures and goals of future superintelligences need not have most of the properties of human minds. A new approach to cognitive science and philosophy of mind, one not centered on the human example, is needed {{to help us understand}} the challenges which we will face when a power greater than us emerges...|$|R
50|$|Lenat's quest, in the Cyc project, is {{to build}} {{the basis of a}} <b>general</b> <b>artificial</b> <b>intelligence</b> by {{manually}} representing knowledge in the formal language, CycL, based on extensions to first-order predicate calculus.|$|R
500|$|<b>Artificial</b> <b>general</b> <b>intelligence</b> (AGI) {{describes}} {{research that}} aims to create machines capable of general intelligent action.|$|E
2500|$|Is <b>artificial</b> <b>general</b> <b>intelligence</b> possible? Can {{a machine}} solve any problem {{that a human}} being can solve using intelligence? Or are there hard limits to what a machine can accomplish? ...|$|E
2500|$|Singularitarians {{believe in}} some sort of [...] "accelerating change"; that the rate of {{technological}} progress accelerates as we obtain more technology, and that this will culminate in a [...] "Singularity" [...] after <b>artificial</b> <b>general</b> <b>intelligence</b> is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary, but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.|$|E
40|$|Abstract. Perceptual {{time is a}} {{critical}} aspect of how humans (and probably animals too) perceive the world. It underlies <b>general</b> <b>intelligence,</b> particularly where that <b>general</b> <b>intelligence</b> is about interacting with the world on an everyday basis. We discuss {{what is meant by}} the perceptual instant, and how this may be important for (<b>artificial</b> and real) <b>general</b> <b>intelligence.</b> Lastly, we briefly discuss how perceptual time might be included in an artificial system which might display <b>general</b> <b>intelligence.</b> ...|$|R
40|$|Artificial intelligence, attention, {{resource}} management Much of present AI research {{is based on}} the assumption of computational systems with infinite resources, an assumption that is either explicitly stated or implicit in the work as researchers ignore the fact that most real-world tasks must be finished within certain time limits, and it is the role of intelligence to effectively deal with such limitations. Expecting AI systems to give equal treatment to every piece of data they encounter is not appropriate in most real-world cases; available resources are likely to be insufficient for keeping up with available data in even moderately complex environments. Even if sufficient resources are available, they might possibly be put to better use than blindly applying them to every possible piece of data. Finding inspiration for more intelligent {{resource management}} schemes is not hard, we need to look no further than ourselves. This paper explores what human attention has to offer in terms of ideas and concepts for implementing intelligent resource management and how the resulting principles can be extended to levels beyond human attention. We also discuss some ideas for the principles behind attention mechanisms for <b>artificial</b> (<b>general)</b> <b>intelligences.</b> ...|$|R
40|$|The {{thesis is}} {{organized}} in the following approach. First, we will {{address some of the}} main issues for <b>general</b> <b>artificial</b> <b>intelligence.</b> Second, to enhance our understanding of the problems, this article will provide a general overview of two axiological theories presented from philosophy: absolutism and relativism. We will discuss some examples of how these concepts relate to machine learning algorithms. Third, we will argue the thesis statement that classification requires relativism to be useful. The goal {{of this paper is to}} show how relativism can be used as a strategy to help solve some of the issues for <b>general</b> <b>artificial</b> <b>intelligence.</b> The concept of relativity can be useful for: (1) axiology, (2) defining things, and (3) memory. Then, the paper will conclude with future and final thoughts on the subject matter...|$|R
2500|$|The term [...] "technological singularity" [...] {{reflects}} {{the idea that}} such change may happen suddenly, {{and that it is}} difficult to predict how the resulting new world would operate. It is unclear whether an intelligence explosion of this kind would be beneficial or harmful, or even an existential threat, as the issue has not been dealt with by most <b>artificial</b> <b>general</b> <b>intelligence</b> researchers, although the topic of friendly artificial intelligence is investigated by the Future of Humanity Institute and the Machine Intelligence Research Institute.|$|E
2500|$|Newell and Simon had conjectured that a {{physical}} symbol system (such as a digital computer) had all the necessary machinery for [...] "general intelligent action", or, {{as it is known}} today, <b>artificial</b> <b>general</b> <b>intelligence.</b> They framed this as a philosophical position, the {{physical symbol system}} hypothesis: [...] "A physical symbol system has the necessary and sufficient means for general intelligent action." [...] The Chinese room argument does not refute this, because it is framed in terms of [...] "intelligent action", i.e. the external behavior of the machine, rather than {{the presence or absence of}} understanding, consciousness and mind.|$|E
2500|$|The {{technological}} singularity (also, simply, the singularity) is {{the hypothesis}} that the invention of artificial superintelligence will abruptly trigger runaway technological growth, resulting in unfathomable changes to human civilization. According to this hypothesis, an upgradable intelligent agent (such as a computer running software-based <b>artificial</b> <b>general</b> <b>intelligence)</b> would enter a [...] "runaway reaction" [...] of self-improvement cycles, with each new and more intelligent generation appearing more and more rapidly, causing an intelligence explosion and resulting in a powerful superintelligence that would, qualitatively, far surpass all human intelligence. John von Neumann first used the term [...] "singularity" [...] (c. 1950s), in the context of technological progress causing accelerating change: [...] "The accelerating progress of technology and changes in the mode of human life, give the appearance of approaching some essential singularity {{in the history of the}} race beyond which human affairs, as we know them, could not continue". Subsequent authors have echoed this viewpoint. I. J. Good's [...] "intelligence explosion" [...] model predicts that a future superintelligence will trigger a singularity. Emeritus professor of computer science at San Diego State University and science fiction author Vernor Vinge said in his 1993 essay The Coming Technological Singularity that this would signal the end of the human era, as the new superintelligence would continue to upgrade itself and would advance technologically at an incomprehensible rate.|$|E
5000|$|The 2011 Federal Virtual World Challenge advertised by The White House and {{sponsored}} by the US Army held a competition offering a total of $52,000 USD in cash prize awards for <b>general</b> <b>artificial</b> <b>intelligence</b> applications, including [...] "adaptive learning systems, intelligent conversational bots, adaptive behavior (objects or processes)" [...] and more.|$|R
5000|$|Marek Rosa is a Slovak {{video game}} programmer, designer, {{producer}} and entrepreneur. He {{is known as}} the CEO and founder of Keen Software House, an independent game development studio that produces the games Space Engineers [...] and Medieval Engineers. [...] He is the founder, CEO and CTO of GoodAI, a research and development company building <b>general</b> <b>artificial</b> <b>intelligence.</b>|$|R
50|$|A few {{researchers}} {{grasped the}} potential of probabilistic methods and predicted that they would come to dominate computer game-playing, but many others considered a strong Go-playing program {{something that could be}} achieved only in the far future, as a result of fundamental advances in <b>general</b> <b>artificial</b> <b>intelligence</b> technology. Even writing a program capable of automatically determining the winner of a finished game was seen as no trivial matter.|$|R
5000|$|Artificial {{intelligence}} (but not <b>Artificial</b> <b>general</b> <b>intelligence)</b> ...|$|E
5000|$|... #Article: Existential {{risk from}} <b>artificial</b> <b>general</b> <b>intelligence</b> ...|$|E
5000|$|... #Subtitle level 3: The {{limits of}} <b>artificial</b> <b>general</b> <b>intelligence</b> ...|$|E
40|$|Abstract. The paper {{contributes}} {{to the development of}} the theory of AI-Completeness by formalizing the notion of AI-Complete and AI-Hard problems. The intended goal is to provide a classification of problems in the field of <b>General</b> <b>Artificial</b> <b>Intelligence.</b> We prove Turing Test to be an instance of an AI-Complete problem and further show certain AI problems to be AI-Complete or AI-Hard via polynomial time reductions. Finally, the paper suggests some directions for future work on the theory of AI-Completeness...|$|R
40|$|Verification and {{validation}} of agentic behavior {{have been suggested}} as important research priorities in efforts to reduce risks associated {{with the creation of}} <b>general</b> <b>artificial</b> <b>intelligence</b> (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of <b>general</b> <b>artificial</b> <b>intelligence.</b> Comment: 13 pages, 0 figure...|$|R
40|$|There is a {{significant}} lack of unified approaches to building generally intelligent machines. The majority of current artificial intelligence research operates within a very narrow field of focus, frequently without {{considering the importance of}} the 'big picture'. In this document, we seek to describe and unify principles that guide the basis of our development of <b>general</b> <b>artificial</b> <b>intelligence.</b> These principles revolve around the idea that intelligence is a tool for searching for general solutions to problems. We define intelligence as the ability to acquire skills that narrow this search, diversify it and help steer it to more promising areas. We also provide suggestions for studying, measuring, and testing the various skills and abilities that a human-level intelligent machine needs to acquire. The document aims to be both implementation agnostic, and to provide an analytic, systematic, and scalable way to generate hypotheses that we believe are needed to meet the necessary conditions in the search for <b>general</b> <b>artificial</b> <b>intelligence.</b> We believe that such a framework is an important stepping stone for bringing together definitions, highlighting open problems, connecting researchers willing to collaborate, and for unifying the arguably most significant search of this century...|$|R
5000|$|Founded in 2001, the <b>Artificial</b> <b>General</b> <b>Intelligence</b> Research Institute's (AGIRI) {{mission is}} to [...] "foster the {{creation}} of powerful and ethically positive" [...] <b>artificial</b> <b>general</b> <b>intelligence.</b> AGIRI hosts an online forum, publishes material on the development, application and implications for AGI, and hosts AGI workshops in the Washington, D.C. area.|$|E
5000|$|<b>Artificial</b> <b>general</b> <b>intelligence</b> (AGI) [...] - [...] some {{consider}} AC a subfield of AGI research ...|$|E
5000|$|... (Also see Cognitive {{architecture}} & <b>Artificial</b> <b>General</b> <b>Intelligence</b> {{for a list}} of SDM related projects) ...|$|E
40|$|Abstract — The paper {{contributes}} {{to the development of}} the theory of AI-Completeness by formalizing the notion of AI-Complete and AI-Hard problems. The intended goal is to provide a classification of problems in the field of <b>General</b> <b>Artificial</b> <b>Intelligence.</b> We prove Turing Test to be an instance of an AI-Complete problem and further show numerous AI problems to be AI-Complete or AI-Hard via polynomial time reductions. Finally, the paper suggests some directions for future work on the theory of AI-Completeness...|$|R
40|$|Aims {{and scope}} of the series This series publishes books {{resulting}} from theoretical research on and reproductions of <b>general</b> <b>Artificial</b> <b>Intelligence</b> (AI). The book series focuses on the establishment of new theories and paradigms in AI. At the same time, the series aims at exploring multiple scientific angles and methodologies, including results from research in cognitive science, neuroscience, theoretical and experimental AI, biology and from innovative interdisciplinary methodologies. For more information on this series and our other book series, please visit our website at: www. atlantis-press. com/publications/books AMSTERDAM –PARIS –BEIJING...|$|R
40|$|AbstractThis paper {{describes}} the DATA-CHASER Automated Planner/Scheduler (DCAPS) system for automated generation and repair of command sequences for the DATA-CHASER shuttle payload. DCAPS uses <b>general</b> <b>Artificial</b> <b>Intelligence</b> (AI) heuristic search techniques, including an iterative repair {{framework in which}} the system iteratively resolves conflicts with the state, resource, and temporal constraints of the payload activities. DCAPS {{was used in the}} operations of the shuttle payload for the STS- 85 shuttle flight in August 1997 and enabled a 80 % reduction in mission operations effort and a 40 % increase in science return...|$|R
