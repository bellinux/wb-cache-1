0|633|Public
40|$|A {{metasearch}} engine supports unified access to multiple component search engines. To build a very large-scale {{metasearch engine}} that can access up {{to hundreds of}} thousands of component search engines, one major challenge is to incorporate large numbers of autonomous search engines in a highly effective manner. To solve this problem, we propose <b>automatic</b> <b>search</b> engine discovery, <b>automatic</b> <b>search</b> engine connection, and <b>automatic</b> <b>search</b> engine result extraction techniques. Experiments indicate that these techniques are highly effective and efficient...|$|R
40|$|The Carnegie Mellon University Informedia {{group has}} enjoyed {{consistent}} success with TRECVID interactive search using traditional storyboard interfaces for shot-based retrieval. For TRECVID 2006 {{the output of}} <b>automatic</b> <b>search</b> was included {{for the first time}} with storyboards, both as an option for an interactive user and in a different run as the sole means of access. The <b>automatic</b> <b>search</b> makes use of relevance-based probabilistic retrieval models to determine weights for combining retrieval sources when addressing a given topic. Storyboard-based access using <b>automatic</b> <b>search</b> output outperformed extreme video retrieval interfaces of manual browsing with resizable pages and rapid serial visualization of keyframes that used the same output. Further, the full Informedia interface with <b>automatic</b> <b>search</b> results as an option along with other query mechanisms scored significantly better than all other TRECVID 2006 interactive search systems. Attributes of the <b>automatic</b> <b>search</b> and interactive search systems are discussed to further optimize shot-based retrieval from news corpora...|$|R
5000|$|<b>Automatic</b> <b>searches</b> for the minima on conical {{intersection}} seams.|$|R
30|$|We {{constructed}} the search string using several iterations and pilot tests {{to ensure that}} we used a comprehensive set of synonyms to allow for high coverage while keeping the number of retrieved articles under control. Considering the high number of results from our <b>automatic</b> <b>search</b> process (over 8740 articles), we believe that we achieved a reasonable coverage level with our search string in the <b>automatic</b> <b>search.</b>|$|R
5000|$|... {{compromised}} data tracking and data extraction from botnets; <b>automatic</b> <b>search</b> and monitoring of [...] "underground" [...] forums; ...|$|R
5000|$|Capturing {{user input}} (such as a user {{personal}} identification number (PIN), search query, or email) and <b>automatic</b> <b>search</b> suggestions.|$|R
40|$|The astounding rate {{at which}} digital video is {{becoming}} available has stimulated research into video retrieval systems that incorporate visual, auditory, and spatio-temporal analysis. In the beginning, these multimodal systems required intensive user interaction, but {{during the past few}} years <b>automatic</b> <b>search</b> systems that need no interaction at all have emerged, requiring only a string of natural language text and a number of multimodal examples as input. We apply ourselves to this task of <b>automatic</b> <b>search,</b> and investigate the feasibility of <b>automatic</b> <b>search</b> without multimodal examples. The result is AutoSeek, an <b>automatic</b> multimodal <b>search</b> system that requires only text as input. In our search strategy we first extract semantic concepts from text and match them to semantic concept indices using a large lexical database. Queries are then created for the semantic concept indices as well as for indices that incorporate ASR text. Finally, the result sets from the different indices are fused with a combination strategy that was created using a set of development search statements. We subject our system to an external assessment in the form of the TRECVID 2005 <b>automatic</b> <b>search</b> task, and find that our system performs competitively when compared to systems that also use multimodal examples, ranking in the top three systems for 25 % of the search tasks and scoring the fourth highest in overall mean average precision. We conclude that <b>automatic</b> <b>search</b> without using multimodal examples is a realistic task, and predict that performance will improve further as semantic concept detectors increase in quantity and quality. ii...|$|R
30|$|Six {{researchers}} {{performed the}} manual and <b>automatic</b> <b>searches</b> working in partnership {{on a given}} engine or set of manual sources. As automatic sources have features for filtering papers according a date interval, we applied the third exclusion criterion, thereby excluding studies not published between 2005 and 2016. Note that in <b>automatic</b> <b>search,</b> we preferred to opt for a wider search string, a task we gave to the researchers, {{with a view to}} reducing the chance of losing relevant articles. Many of these articles were eliminated in the primary selection, where the researchers evaluated the results from the <b>automatic</b> <b>search</b> (n= 8740) and from the manual search (n= 194) by looking at the title and abstract and excluding the papers that were either clearly not relevant or duplicated ones. This resulted in 193 relevant studies.|$|R
40|$|In {{this paper}} we {{describe}} our system and experiments performed {{for both the}} <b>automatic</b> <b>search</b> task and the event detection task in TRECVid 2008. For the <b>automatic</b> <b>search</b> task for 2008 we submitted 3 runs utilizing only visual retrieval experts, continuing our previous work in examining techniques for query-time weight generation for data-fusion and determining what we can get from global visual only experts. For the event detection task we submitted results for 5 required events (ElevatorNoEntry, OpposingFlow, PeopleMeet, Embrace and PersonRuns) and 1 optional event (DoorOpenClose) ...|$|R
40|$|This paper {{describes}} the MSRA experiments for TRECVID 2008. We performed the experiments in high-level feature extraction and <b>automatic</b> <b>search</b> tasks. For high-level feature extraction, we representatively investigated {{the benefit of}} global and local low-level features {{by a variety of}} learning-based methods, including supervised and semi-supervised learning algorithms. For <b>automatic</b> <b>search,</b> we focused on text and visual baseline, query-independent learning, and various reranking methods. Index Terms — support vector machines, bag of words, semi-supervised learning, optimal multi-graph learning, transductive multi-label learning, video annotation, video search, query-independent learning, search reranking. 1...|$|R
50|$|Noam Cohen of The New York Times {{described}} {{the situation as}} a hijacking of online identity. He questioned whether <b>automatic</b> <b>search</b> algorithms should be entirely devoid of human discretion.|$|R
30|$|The <b>automatic</b> <b>search</b> and {{optimization}} of {{the form}} and associated parameters of the linear/nonlinear data-fitting function, performed by the GPSR obviates the trial and error approach associated with the traditional linear/nonlinear regression analysis.|$|R
30|$|Selected {{conferences}} and journals: {{in addition to}} the studies retrieved from the <b>automatic</b> <b>search</b> engines and related work analysis, we performed a manual search in well-known {{conferences and}} journals in cloud computing. This increased the chances of finding additional relevant studies.|$|R
40|$|In {{this paper}} we present our runs {{submitted}} to the <b>automatic</b> <b>search</b> tasks of TRECVID 2009. This year, we submitted six runs for the <b>automatic</b> <b>search</b> task. These search runs primarily focused on (1) adaptively ranking the relevance of low level visual features to a topic and fusing those results (2) Estimating topic distribution using the Latent Dirichlet Allocation (LDA) technique during the match in LDA topic space and (3) formation of bag of words for low level features. The automatic runs submitted include, two runs based on MPEG 7 visual features, two runs based on visual and high level features and two runs based on bag-of-words derived from SIFT features. 1...|$|R
40|$|This paper {{describes}} the MSRA-USTC-SJTU experiments for TRECVID 2007. We performed the experiments in high-level feature extraction and <b>automatic</b> <b>search</b> tasks. For high-level feature extraction, we investigated {{the benefit of}} unlabeled data by semi-supervised learning, and the multi-layer (ML) multi-instance (MI) relation embedded in video by MLMI kernel, {{as well as the}} correlations between concepts by correlative multi-label learning. For <b>automatic</b> <b>search,</b> we fuse text, visual example, and concept-based models while using temporal consistency and face information for re-ranking and result refinement. Index Terms — support vector machines, semi-supervised learning, manifold ranking, multi-layer multi-instance kernel, linear neighborhood propagation, temporally consistent Gaussian random field, optimal multi-graph learning, correlative multi-label annotation, video annotation, video search. 1...|$|R
40|$|This {{article was}} {{published}} in the journal Robotica [© Cambridge University Press] and the definitive version is available at: [URL] paper investigates worst-case analysis of a moving obstacle avoidance algorithm for unmanned vehicles in a dynamic environment in the presence of uncertainties and variations. <b>Automatic</b> worst-case <b>search</b> algorithms are developed based on optimization techniques, and illustrated by a Pioneer robot with a moving obstacle avoidance algorithm developed using the potential field method. The uncertainties in physical parameters, sensor measurements, and even the model structure of the robot are taken into account in the worst-case analysis. The minimum distance to a moving obstacle is considered as an objective function in <b>automatic</b> <b>search</b> process. It is demonstrated that a local nonlinear optimization method may not be adequate, and global optimization techniques are necessary to provide reliable worst-case analysis. The Monte Carlo simulation is carried out to demonstrate that the proposed <b>automatic</b> <b>search</b> methods provide a significant advantage over random sampling approaches...|$|R
40|$|Term {{bindings}} in archetypes are at {{a boundary}} between health information models and health terminology for dual model-based electronic health-care record (EHR) systems. The development of archetypes and the population of archetypes with bound terms is in its infancy. Terminological binding is currently performed “manually” by the teams who create archetypes. This process could be made more efficient, if it was supported by automatic tools. This paper presents a method for evaluating the performance of <b>automatic</b> code <b>search</b> approaches. In order to assess {{the quality of the}} <b>automatic</b> <b>search,</b> the authors extracted all the unique bound codes from 1133 archetypes from an archetype repository. These “manually bound ”SNOMED-CT codes were compared against the codes suggested by the authors 2 ̆ 7 <b>automatic</b> <b>search</b> and used for assessing the algorithm 2 ̆ 7 s performance in terms of accuracy and category matching. The result of this study shows a sensitivity analysis of a set of parameters relevant to the matching process...|$|R
30|$|SATINE (Dogac et al. 2004) is a {{the famous}} {{framework}} which extends Global Distribution System (GDS) connecting Online Travel Agent (OTA) between semantic webs for distributed web service platforms. Each tourism service needs the registry {{to be included}} for the <b>automatic</b> <b>search.</b>|$|R
5000|$|Gates, K. M., Molenaar, P. C., Hillary, F. G., Ram, N., & Rovine, M. J. (2010). <b>Automatic</b> <b>search</b> for fMRI {{connectivity}} mapping: {{an alternative}} to Granger causality testing using formal equivalences among SEM path modeling, VAR, and unified SEM. NeuroImage, 50(3), 1118-1125.|$|R
40|$|We have {{developed}} a database named BRITE, which contains knowledge of interacting molecules and/or genes concering cell cycle and early development. Here, we report {{an overview of the}} database and the method of <b>automatic</b> <b>search</b> for functionally common sub-pathways between two biological pathways in BRITE...|$|R
40|$|Abstrac- The subject remote {{diagnostics}} technique development of ultrasonic process equipment is {{considered in the}} article. The analysis of the most vulnerable functional blocks and ways of <b>automatic</b> <b>search</b> of their malfunctions, and also the transfer of this information, with telecommunication systems usage by experts is provided. I...|$|R
30|$|The {{keywords}} {{used for}} the <b>automatic</b> <b>search</b> were (consumer OR personal OR digital) AND (image(s) OR photo(s) or photograph(s) OR photographic archive) AND (value OR quality OR aesthetics OR visual quality) AND (evaluation OR assessment OR analysis OR estimation). A publication period between 2006 and 2012 was defined.|$|R
40|$|Keywords-RFID;robot; singlechip;automatic;intelligent Abstract—In this paper, a novel {{mobile robot}} system for searching books {{automatically}} in libraries is designed {{based on the}} radio frequency identification (RFID) and singlechip programming techniques. Recently, the barcode identification is frequently used in libraries. However, because of its unique advantages of RFID technology and its application in the intelligent robots {{will be able to}} achieve high-efficient <b>automatic</b> <b>search</b> of books. RFID has the advantages of large information storage and long identification distance etc., thus application in library management information system has great development potential. By using RFID long-distance identification ability combined with the close accurate positioning technologies, a mobile robot for <b>automatic</b> <b>search</b> of books in libraries is designed, which can effectively improve the book search efficiency, saves the massive cost and improve the library informatization level...|$|R
40|$|The LHC Collider Ring is {{proposed}} {{to be turned}} into an ultimate <b>automatic</b> <b>search</b> engine for new physics in four consecutive phases: (1) Searches for heavy particles produced in Central Exclusive Process (CEP) : pp -> p + X + p based on the existing Beam Loss Monitoring (BLM) system of the LHC; (2) Feasibility study of using the LHC Ring as a gravitation wave antenna; (3) Extensions to the current BLM system to facilitate precise registration of the selected CEP proton exit points from the LHC beam vacuum chamber; (4) Integration of the BLM based event tagging system together with the trigger/data acquisition systems of the LHC experiments to facilitate an on-line <b>automatic</b> <b>search</b> machine for the physics of tomorrow. Comment: 5 pages, 3 figures, Diffraction 2016 CNUM: C 16 - 09 - 0...|$|R
5000|$|Scan Disk: <b>Automatic</b> rapid <b>search</b> for {{identifier}} or condition.|$|R
40|$|The {{origin and}} {{application}} of molecular topology — a new method in drug design is described. Its scope is the topological characterization of molecules by means of numerical invariants, called topological indices. Their numerical format facilitates the <b>automatic</b> <b>search</b> of other molecules with similar structural, physicochemical, biological and pharmacological properties...|$|R
50|$|UTEXAS {{employs a}} fast <b>automatic</b> <b>search</b> {{algorithm}} {{to find the}} failure surface with the lowest factor of safety with respect to shear strength. This is the critical failure surface. Alternatively an arbitrary surface can be entered by the user and UTEXAS can determine the factor of safety associated with it.|$|R
40|$|Highly {{optimised}} provers like MSPASS [HS 00] and FaCT [HPS 98] {{can test}} formulae {{with hundreds of}} symbols within a few seconds. Generic logical frameworks like Isabelle [Pau 93] allow us to implement almost any logical calculus as a “shallow embedding ” with an <b>automatic</b> <b>search</b> tactic. But researchers often find thes...|$|R
40|$|AbstractAn {{approach}} to the synthesis of technological processes is proposed. Technological knowledge {{is presented in the}} form of a technological model. An algorithm for <b>automatic</b> <b>search</b> for the solution of technological tasks is proposed, which is implemented in the framework of propositional logic programming. The approach has been implemented and tested in various applications...|$|R
40|$|This paper {{provides}} an overview of a technique for extracting information from the Web search interfaces of e-commerce search engines that is useful for supporting <b>automatic</b> <b>search</b> interface integration. In particular, we discuss how to group elements and labels on a search interface into attributes and how to derive certain meta-information for each attribute...|$|R
40|$|Semantic concept {{detection}} is {{an active}} research topic {{as it can provide}} semantic filters and aid in <b>automatic</b> <b>search</b> of image and video databases. The annual NIST TRECVID video retrieval benchmarking event [1] has greatly contributed to this area by providing benchmark datasets and performing system evaluation. As acquiring ground truths of semantic concepts i...|$|R
40|$|Because the {{law says}} we can do it” was the {{response}} Officer Griffith offered when asked why officers searched Rodney Gant’s car {{when he was arrested}} for driving with a suspended license. Officer Griffith’s honest answer exemplifies the effect of prior Supreme Court decisions on search incident to arrest power in the vehicle context: that a vehicle search incident to arrest is a police entitlement divorced from any rationale whatsoever. Concerns for officer safety and preservation of evidence [...] legal justifications that generally permit warrantless searches incident to arrest generally [...] had been utterly abandoned by the Court in the automobile context. This police entitlement led to invasions of privacy against persons guilty of no more than mere traffic violations, as searches were conducted simply because they were legally permissible. However, the Supreme Court in Arizona v. Gant shifted course and strengthened Fourth Amendment protections by terminating the entitlement that permitted vehicle searches incident to arrest as a matter of right. The tumultuous jurisprudence of the search incident to arrest doctrine under the Fourth Amendment has often produced inconsistent and varied results. In keeping with this tradition, the Supreme Court in Gant revised nearly thirty years of search incident to arrest law in the automobile context. Unlike Gant’s predecessors, Gant generally enhanced Fourth Amendment protections against unreasonable searches by holding that <b>automatic</b> vehicle <b>searches</b> incident to arrest are unconstitutional. On the other hand, Gant’s second holding created a new warrant exception to govern searches of automobiles incident to arrest by allowing officers to search a vehicle, even when the justifications of officer safety and preservation of evidence are nonexistent. The author argues that Gant not only enhances Fourth Amendment protections overall by limiting authority to search an automobile upon arrest, but that its first holding also undermines other cases permitting <b>automatic</b> <b>searches</b> incident to arrest in non-vehicular situations. Gant’s affirmation of two specific rationales that permit a search incident to arrest, officer safety and the preservation of evidence, directly conflicts with non-vehicular cases that allow <b>automatic</b> <b>searches</b> irrespective of these rationales. Since Gant undermines such cases by reconnecting the search incident to arrest exception with its justifications, applying Gant to cases that permit <b>automatic</b> <b>searches</b> of containers on the person, and certain <b>automatic</b> home <b>searches</b> incident to arrest, serves to enhance privacy protections against these non-vehicular searches that have become police entitlements. Part I outlines the judicial origin of search incident to arrest law and its schizophrenic history, exposes the fundamental conflict between the cases, and discuss the legal rules and reasoning of Gant. Part II argues that the standard governing Gant’s second holding is vague, and is concerned with whether the crime of arrest involves tangible evidence rather than a quantum of proof analysis prevalent in standards such as probable cause and reasonable suspicion. Part III analyzes the effect of applying Gant’s first holding to an <b>automatic</b> <b>search</b> of containers on the person incident to arrest, while Part IV applies Gant to certain <b>automatic</b> home <b>searches</b> incident to arrest. Part IV also addresses some Counterarguments and potential pitfalls. This Comment concludes that Gant’s retraction of the search incident to arrest power may serve to end, or at the least severely undermine, <b>automatic</b> <b>searches</b> of containers on the person and homes incident to arrest...|$|R
40|$|This paper {{describes}} BUPT-MCPRL {{systems for}} TRECVID 2009. We performed experiments in <b>automatic</b> <b>search,</b> HLF extraction, copy detection and event detection tasks. A. <b>Automatic</b> <b>search</b> A semantic-based video search system was proposed and {{brief description of}} submitted 10 runs is shown in Table. 1. Table 1 The performance of 10 runs for <b>automatic</b> <b>search</b> Run ID infMAP Description F_A_N_BUPT-MCPR 1 0. 104 HLF-based retrieval and positive WDSS method F_A_N_BUPT-MCPR 2 0. 070 Concept-based retrieval and positive WDSS method F_A_N_BUPT-MCPR 3 0. 059 Concept-based retrieval and positive and negative WDSS method F_A_N_BUPT-MCPR 4 0. 131 Combining concept lexicons of MCPR 1 High-Level-Features and MCPR 2 search topics and using positive WDSS method F_A_N_BUPT-MCPR 5 0. 032 Concept-based retrieval with example bagging method F_A_N_BUPT-MCPR 6 0. 024 Visual example-based retrieval F_A_N_BUPT-MCPR 7 0. 024 Concept-based retrieval with example weighting method F_A_N_BUPT-MCPR 8 0. 016 Re-rank MCPR 7 with face score F_A_N_BUPT-MCPR 9 0. 009 Fusion MCPR 6 and MCPR 7 and re-rank with face score F_A_N_BUPT-MCPR 10 0. 048 Fusion with MCPR 5, MCPR 6 and MCPR 7 B. High-level feature extraction In this year, focus of our HLF system was on boosting and fusion of low-level features, the difference of classifiers with cross-validation, and re-ranking of results according to face detection. HLF Run infMAP Description Table 2 HLF results and description of BUPT-MCPRL system BUPT-MCPRL_Sys 1 0. 0313 BUPT-MCPRL_Sys 3 is modified by face results...|$|R
40|$|<b>Automatic</b> <b>searching,</b> {{knowledge}} acquisition and question answering are crying needs among the contemporary World Wide Web users. However conventional web {{is the major}} barriers for realizing above applications. This is because almost all important information in it is in natural languages and natural languages are {{very hard to be}} manipulated and understood by a computer. As a solution, more than a decade ago, semantic web was introduced, {{there was a lot of}} hope on machine understandability of the web. However the semantic web is still very far from realization due to the effort required for semantic tagging of available information. In this project we try to build a solution for <b>automatic</b> <b>searching</b> in conventional web and similar information sources by mimicking the human natural language learning and knowledge representation process. Our approach is based on the hypothesis, which inspired by popular philosophy of science, that learning is matching known facts with new facts. In the context of conventional web, we employ statistical natural language processing technique co-word analysis for matching already available facts with new facts collected during <b>automatic</b> <b>searching</b> process. As a proof of above hypothesis we have built a personalized automatic knowledge extraction application. That extracts knowledge from conventional web or similar information source regarding user queries and present synthesized documents related to the knowledge area of the query. Evaluation done by manual comparison of documents produced by the application and a document produced by a human user by web <b>searching.</b> Results showed <b>automatic</b> {{knowledge acquisition}} performs acceptable manner in most of the situations. <b>Automatic</b> <b>searching,</b> knowledge acquisition and question answering are crying needs among the contemporary World Wide Web users. However conventional web is the major barriers for realizing above applications. This is because almost all important information in it is in natural languages and natural languages are very hard to be manipulated and understood by a computer. As a solution, more than a decade ago, semantic web was introduced, there was a lot of hope on machine understandability of the web. However the semantic web is still very far from realization due to the effort required for semantic tagging of available information. In this project we try to build a solution for <b>automatic</b> <b>searching</b> in conventional web and similar information sources by mimicking the human natural language learning and knowledge representation process. Our approach is based on the hypothesis, which inspired by popular philosophy of science, that learning is matching known facts with new facts. In the context of conventional web, we employ statistical natural language processing technique co-word analysis for matching already available facts with new facts collected during <b>automatic</b> <b>searching</b> process. As a proof of above hypothesis we have built a personalized automatic knowledge extraction application. That extracts knowledge from conventional web or similar information source regarding user queries and present synthesized documents related to the knowledge area of the query. Evaluation done by manual comparison of documents produced by the application and a document produced by a human user by web <b>searching.</b> Results showed <b>automatic</b> knowledge acquisition performs acceptable manner in most of the situations...|$|R
30|$|The {{combination}} of <b>automatic</b> <b>search</b> {{in the most}} popular search engines, and manual search in relevant publication venues, improved mapping coverage. However, coverage is an inherent threat to validity in any systematic review or mapping. In general, {{it is not possible}} to reach 100 % of coverage, and it is very difficult to estimate reliably the coverage within a literature review.|$|R
50|$|For the {{relaunch}} in October 2010, the metasearch {{was replaced}} by a search engine index (Lucene/SOLR). In addition, the STW Thesaurus for Economics was integrated in order to enable <b>automatic</b> <b>searches</b> for synonyms and translations. By integrating other web services, such as the Electronic Journals Library or the Journals Database, it is possible to check if and where an item is available.|$|R
