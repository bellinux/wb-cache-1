181|78|Public
25|$|In non-tonal languages, {{more of the}} <b>articulatory</b> <b>features</b> {{of speech}} are retained, and the {{normally}} timbral variations imparted by {{the movements of the}} tongue and soft palate are transformed into pitch variations. Certain consonants can be pronounced while whistling, so as to modify the whistled sound, much as consonants in spoken language modify the vowel sounds adjacent to them.|$|E
25|$|The grave/acute {{distinction}} {{has lost}} its relevance in modern phonetics (though it may still be relevant to other disciplines). It dates from relatively early {{in the days of}} acoustic phonetics, when some phonologists believed that one could categorize all speech sounds by means of a finite set of acoustically-defined distinctive features. These were supposed to correspond to auditory impressions of sounds. The pioneering publication for this was Jakobson, Fant and Halle (1951) Preliminaries to Speech Analysis (MIT). The feature(s) grave/acute were defined primarily in acoustic terms (with some reference to auditory qualities), but were given a secondary description (or gloss) in terms of their articulation. Features like grave/acute could be used to divide speech sounds into broad classes. For most phoneticians, the JF features had been superseded by 1968 by the <b>articulatory</b> <b>features</b> set out in Chomsky and Halle’s Sound Pattern of English and by competing <b>articulatory</b> <b>features</b> devised by Ladefoged in such publications as Preliminaries to Linguistic Phonetics (1971).|$|E
2500|$|Another {{tendency}} {{arose in}} the Common Slavic period wherein successive segmental phonemes in a syllable assimilated <b>articulatory</b> <b>features</b> (primarily place of articulation). This is called syllable synharmony or intrasyllabic harmony. Thus syllables (rather than just the consonant or the vowel) were distinguished as either [...] "soft" [...] (palatal) or [...] "hard" [...] (non-palatal). This led to consonants developing palatalized allophones in syllables containing front vowels, resulting in the first regressive palatalization. It also led to the fronting of back vowels after /j/.|$|E
40|$|We present {{experiments}} on mono-lingual and cross-lingual <b>articulatory</b> <b>feature</b> recognition for English and German speech data. Our {{goal is to}} investigate to what extent {{it is possible to}} derive and reuse <b>articulatory</b> <b>feature</b> recognizers, whether particular features are better suited to this task. Finally whether this goal is practically achievable with the chosen machine learning technique and the selected set of speech signal descriptors...|$|R
40|$|This paper {{describes}} {{our research}} on adaptation methods applied to <b>articulatory</b> <b>feature</b> detection on soft whispery speech recorded with a throat microphone. Since {{the amount of}} adaptation data is small and the testing data {{is very different from}} the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, featurespace adaptation, and re-training with downsampling, sigmoidal low-pass filter, and linear multivariate regression. Adapted <b>articulatory</b> <b>feature</b> detectors are used in parallel to standard senonebased HMM models in a stream architecture for decoding. With these adaptation methods, <b>articulatory</b> <b>feature</b> detection accuracy improves from 87. 82 % to 90. 52 % with corresponding F-measure from 0. 504 to 0. 617, while the final word error rate improves from 33. 8 % to 31. 2 %. 1...|$|R
5000|$|In {{the early}} Balto-Slavic period, an {{additional}} <b>articulatory</b> <b>feature,</b> the acute register, had developed on certain syllables, {{particularly those that}} ended in a PIE laryngeal consonant (detailed further below). It was a suprasegmental feature whose exact phonetic nature is not quite clear. It likely involved glottalization at some stage, as a similar <b>articulatory</b> <b>feature</b> {{is found in the}} Latvian [...] "broken tone", which is a reflex of it. It is denoted variously with a superscript glottalization symbol ˀ, a glottal stop symbol ʔ, or simply as the laryngeal cover symbol H.|$|R
50|$|In Tuvan, phonemic vowels are {{specified}} {{with the}} <b>articulatory</b> <b>features</b> of tongue height, backness, and lip rounding. The archiphoneme |U| is an underspecified high vowel where only the tongue height is specified.|$|E
50|$|In non-tonal languages, {{more of the}} <b>articulatory</b> <b>features</b> {{of speech}} are retained, and the {{normally}} timbral variations imparted by {{the movements of the}} tongue and soft palate are transformed into pitch variations. Certain consonants can be pronounced while whistling, so as to modify the whistled sound, much as consonants in spoken language modify the vowel sounds adjacent to them.|$|E
50|$|The grave/acute {{distinction}} {{has lost}} its relevance in modern phonetics (though it may still be relevant to other disciplines). It dates from relatively early {{in the days of}} acoustic phonetics, when some phonologists believed that one could categorize all speech sounds by means of a finite set of acoustically-defined distinctive features. These were supposed to correspond to auditory impressions of sounds. The pioneering publication for this was Jakobson, Fant and Halle (1951) Preliminaries to Speech Analysis (MIT). The feature(s) grave/acute were defined primarily in acoustic terms (with some reference to auditory qualities), but were given a secondary description (or gloss) in terms of their articulation. Features like grave/acute could be used to divide speech sounds into broad classes. For most phoneticians, the JF&H features had been superseded by 1968 by the <b>articulatory</b> <b>features</b> set out in Chomsky and Halle’s Sound Pattern of English and by competing <b>articulatory</b> <b>features</b> devised by Ladefoged in such publications as Preliminaries to Linguistic Phonetics (1971).|$|E
40|$|<b>Articulatory</b> <b>feature</b> {{modeling}} in Automatic Speech Recognition (ASR), {{while not}} (yet) mainstream, {{has received a}} significant amount of attention in recent research ([1, 2, 3, 4] inter alia). One study in particular [1] has provided evidence that hierarchical <b>articulatory</b> <b>feature</b> models can potentially significantly outperform their non-hierarchical counterparts. In such a system, the probability of an <b>articulatory</b> <b>feature</b> is conditional upon some other feature – for example, the classifier for place of articulation may depend on the manner of articulation. In this work, we seek to further the studies in [1] by changing the assumption of perfect recognition of the conditioning class made in that study. The gains shown over non-hierarchical classification are minimized; our analysis shows that this is in part because the errors in different acoustic feature streams are in fact correlated. We conclude the study by observing that joint acoustic feature modeling, rather than conditional modeling, may provide better gains. 1...|$|R
50|$|In Proto-Slavic, {{the acute}} was lost as an <b>articulatory</b> <b>feature</b> and {{retained}} {{only as a}} tonal distinction on accented syllables. The acute produced a rising tone and the circumflex a falling tone, as in Latvian and Old Prussian.|$|R
40|$|Abstract This paper {{builds on}} {{previous}} work where dynamic Bayesian networks (DBN) were proposed {{as a model}} for <b>articulatory</b> <b>feature</b> recognition. Using DBNs makes it possible to model the dependencies between features, an addition to previous approaches which was found to improve feature recognition performance. The DBN results were promising, giving close to the accuracy of artificial neural nets (ANNs). However, the system was trained on canonical labels, leading to an overly strong set of constraints on feature co-occurrence. In this study, we describe an embedded training scheme which learns a set of data-driven asynchronous feature changes where supported in the data. Using a subset of the OGI Numbers corpus, we describe <b>articulatory</b> <b>feature</b> recognition experiments using both canonically-trained and asynchronous-feature DBNs. Performance using DBNs is found to exceed that of ANNs trained on an identical task, giving a higher recognition accuracy. Furthermore, inter-feature dependencies result in a more structured model, giving rise to fewer feature combinations in the recognition output. In addition to an empirical evaluation of this modeling approach, we give a qualitative analysis, investigating the asynchrony found through our data-driven method and interpreting it using linguistic knowledge. Key words <b>Articulatory</b> <b>feature</b> recognition, dynamic Bayesian networks 1...|$|R
50|$|Phonetotopy is {{the concept}} that <b>articulatory</b> <b>{{features}}</b> as well as perceptual features of speech sounds are ordered in the brain {{in a similar way}} as tone (tonotopy), articulation and its somatosensory feedback (somatotopy), or visual location of an object (retinotopy). It is assumed that a phonetotopic ordering of speech sounds as well as of syllables can be found at a supramodal speech processing level (i.e. at a phonetic speech processing level) within the brain.|$|E
5000|$|Another {{tendency}} {{arose in}} the Common Slavic period wherein successive segmental phonemes in a syllable assimilated <b>articulatory</b> <b>features</b> (primarily place of articulation). This is called syllable synharmony or intrasyllabic harmony. Thus syllables (rather than just the consonant or the vowel) were distinguished as either [...] "soft" [...] (palatal) or [...] "hard" [...] (non-palatal). This led to consonants developing palatalized allophones in syllables containing front vowels, resulting in the first regressive palatalization. It also led to the fronting of back vowels after /j/.|$|E
5000|$|Phonology is {{a branch}} of {{linguistics}} concerned with the systematic organization of sounds in languages. It has traditionally focused largely {{on the study of}} the systems of phonemes in particular languages (and therefore used to be also called phonemics, or phonematics), but it may also cover any linguistic analysis either at a level beneath the word (including syllable, onset and rime, articulatory gestures, <b>articulatory</b> <b>features,</b> mora, etc.) or at all levels of language where sound is considered to be structured for conveying linguistic meaning.|$|E
50|$|Labialization is a {{secondary}} <b>articulatory</b> <b>feature</b> of sounds in some languages. Labialized sounds involve the lips while {{the remainder of}} the oral cavity produces another sound. The term is normally restricted to consonants. When vowels involve the lips, they are called rounded.|$|R
40|$|The {{quality of}} unit {{selection}} speech synthesisers depends significantly {{on the content}} of the speech database being used. In this paper a technique is introduced that can highlight mispronunciations and abnormal units in the speech synthesis voice database through the use of <b>articulatory</b> acoustic <b>feature</b> extraction to obtain an additional layer of annotation. A set of <b>articulatory</b> acoustic <b>feature</b> classifiers help minimise the selection of inappropriate units in the speech database and are shown to significantly improve the word error rate of a diphone synthesiser. Index Terms: speech synthesis, unit selection, <b>articulatory</b> acoustic <b>feature</b> extractio...|$|R
40|$|Automatic {{language}} {{identification is}} one of the important topics in multilingual speech technology. Ideal language identification systems should be able to classify the language of speech utterances within a specific time before further processing by language-dependent speech recognition systems or monolingual listeners begins. Currently the best language identification systems are based on HMM-based speech recognition systems. However, with the cost of this low percentage error, comes an increase in computational complexity. This paper proposes an alternative way of using HMM-based speech recognition systems. Instead of using phoneme level acoustic models and n-gram language models, <b>articulatory</b> <b>feature</b> level acoustic models and n-gram language models are introduced. With this approach, the computational complexities of language identification systems are considerably reduced {{due to the fact that}} the size of the <b>articulatory</b> <b>feature</b> inventory is naturally smaller than that of the of phoneme inventory. 1...|$|R
5000|$|... ==Articulation== The {{traditional}} view of vowel production, reflected {{for example}} in the terminology and presentation of the International Phonetic Alphabet, is one of <b>articulatory</b> <b>features</b> that determine a vowel's quality as distinguishing it from other vowels. Daniel Jones developed the cardinal vowel system to describe vowels {{in terms of the}} features of tongue height (vertical dimension), tongue backness (horizontal dimension) and roundedness (lip articulation). These three parameters are indicated in the schematic quadrilateral IPA vowel diagram on the right. There are additional features of vowel quality, such as the velum position (nasality), type of vocal fold vibration (phonation), and tongue root position.|$|E
5000|$|Originally, {{the terms}} {{were used to}} refer to an impressionistic sense of {{strength}} differences, though more sophisticated instruments eventually gave the opportunity to search for the acoustic and articulatory signs. For example, [...] tested whether articulatory strength could be detected by measuring the force of the contact between the articulators or of the peak pressure in the mouth. Because such studies initially found little to substantiate the terminology, phoneticians have largely ceased using them, though they are still commonly used as [...] "phonological labels for specifying a dichotomy when used language-specifically." [...] This can be useful when the actual <b>articulatory</b> <b>features</b> underlying the distinction are unknown or unimportant.|$|E
40|$|Speech sounds can be {{characterized}} by <b>articulatory</b> <b>features.</b> <b>Articulatory</b> <b>features</b> are typically estimated using a set of multilayer perceptrons (MLPs), i. e., a separate MLP is trained for each articulatory feature. In this report, we investigate multitask learning (MTL) approach for joint estimation of <b>articulatory</b> <b>features</b> with and without phoneme classification as subtask. The effect of number of subtasks in MTL is studied by selecting two different articulatory feature representations. Our studies show that MTL MLP can estimate <b>articulatory</b> <b>features</b> compactly and efficiently by learning the inter-feature dependencies through a common hidden layer representation, irrespective of number of subtasks. Furthermore, adding phoneme as subtask while estimating <b>articulatory</b> <b>features</b> improves both articulatory feature estimation and phoneme recognition. On TIMIT phoneme recognition task, articulatory feature posterior probabilities obtained by MTL MLP achieve a phoneme recognition accuracy of 73. 8 %, while the phoneme posterior probabilities achieve an accuracy of 74. 2 %. Index Terms — multitask learning, <b>articulatory</b> <b>features,</b> posterior probabilities, multilayer perceptrons...|$|E
40|$|We {{study the}} problem of {{automatic}} visual speech recognition (VSR) using dynamic Bayesian network (DBN) -based models consisting of multiple sequences of hidden states, each corresponding to an <b>articulatory</b> <b>feature</b> (AF) such as lip opening (LO) or lip rounding (LR). A bank of discriminative <b>articulatory</b> <b>feature</b> classifiers provides input to the DBN, {{in the form of}} either virtual evidence (VE) (scaled likelihoods) or raw classifier margin outputs. We present experiments on two tasks, a medium-vocabulary word-ranking task and a small-vocabulary phrase recognition task. We show that articulatory feature-based models outperform baseline models, and we study several aspects of the models, such as the effects of allowing articulatory asynchrony, of using dictionary-based versus whole-word models, and of incorporating classifier outputs via virtual evidence versus alternative observation models. United States. Defense Advanced Research Projects AgencyIndustrial Technology Research Institut...|$|R
50|$|Early Balto-Slavic {{retained}} a simple accent {{in which only}} {{the placement of the}} accent was distinctive, but there were no pitch distinctions. The acute register was initially no more than an <b>articulatory</b> <b>feature</b> on certain syllables and could occur independently of accent placement. However, the acute was the trigger for several sound changes that affected the placement of the accent. For example, under Hirt's law, the accent tended to shift leftwards onto a syllable that bore the acute.|$|R
40|$|This paper {{builds on}} {{previous}} work where dynamic Bayesian networks (DBN) were proposed {{as a model}} for <b>articulatory</b> <b>feature</b> recognition. Using DBNs makes it possible to model the dependencies between features, an addition to previous approaches which was found to improve feature recognition performance. The DBN results were promising, giving close to the accuracy of artificial neural nets (ANNs). However, the system was trained on canonical labels, leading to an overly strong set of constraints on feature co-occurrence. In this study, we describe an embedded training scheme which learns a set of data-driven asynchronous feature changes where supported in the data. Using a subset of the OGI Numbers corpus, we describe <b>articulatory</b> <b>feature</b> recognition experiments using both canonically-trained and asynchronous-feature DBNs. Performance using DBNs is found to exceed that of ANNs trained on an identical task, giving a higher recognition accuracy. Furthermore, inter-feature dependencies result in a more structured model, giving rise to fewer feature combinations in the recognition output. In addition to an empirical evaluation of this modeling approach, we give a qualitative analysis, investigating the asynchrony found through our data-driven method and interpreting it using linguistic knowledge...|$|R
40|$|This paper {{describes}} some of {{the results}} from the project entitled “New Parameterization for Emotional Speech Synthesis ” held at the Summer 2011 JHU CLSP workshop. We describe experiments on how to use <b>articulatory</b> <b>features</b> as a meaningful intermediate rep-resentation for speech synthesis. This parameterization not only al-lows us to reproduce natural sounding speech but also allows us to generate stylistically varying speech. We showmethods for deriving <b>articulatory</b> <b>features</b> from speech, predicting <b>articulatory</b> <b>features</b> from text and reconstructing natural sounding speech from the predicted <b>articulatory</b> <b>features.</b> The meth-ods were tested on clean speech databases in English and German, as well as databases of emotionally and personality varying speech. The resulting speech was evaluated both objectively, using tech-niques normally used for emotion identification, and subjectively, using crowd-sourcing. Index Terms — speech synthesis, <b>articulatory</b> <b>features,</b> emo-tional speech, meta-data extraction, evaluation 1...|$|E
40|$|Abstract � This paper {{presents}} {{a method to}} control the characteristics of synthetic speech flexibly by integrating <b>articulatory</b> <b>features</b> into a Hidden Markov Model (HMM) -based parametric speech synthesis system. In contrast to model adaptation and interpolation approaches for speaking style control, this method is driven by phonetic knowledge, and target speech samples are not required. The joint distribution of parallel acoustic and <b>articulatory</b> <b>features</b> considering cross-stream feature dependency is estimated. At synthesis time, acoustic and <b>articulatory</b> <b>features</b> are generated simultaneously based on the maximum-likelihood criterion. The synthetic speech can be controlled flexibly by modifying the generated <b>articulatory</b> <b>features</b> according to arbitrary phonetic rules in the parameter generation process. Our experiments show that the proposed method is effective in both changing the overall character of synthesized speech and in controlling {{the quality of a}} specific vowel. Index Terms: speech synthesis, hidden Markov model, <b>articulatory</b> <b>features,</b> phonetic knowledg...|$|E
40|$|We propose an {{information}} theoretic region selection algorithm {{from the real}} time magnetic resonance imaging (rtMRI) video frames for a broad phonetic class recognition task. Representations derived from these optimal regions are used as the <b>articulatory</b> <b>features</b> for recognition. A set of connected and arbitrary shaped regions are selected such that the <b>articulatory</b> <b>features</b> computed from such regions provide maximal information about the broad phonetic classes. We also propose a tree-structured greedy region splitting algorithm to further segment these regions so that <b>articulatory</b> <b>features</b> from these split regions enhance {{the information about the}} phonetic classes. We find that some of the proposed <b>articulatory</b> <b>features</b> correlate well with the articulatory gestures from the Articulatory Phonology theory of speech production. Broad phonetic class recognition experiment using four rtMRI subjects reveals that the recognition accuracy with optimal split regions is, on average, higher than that using only acoustic features. Combining acoustic and <b>articulatory</b> <b>features</b> further reduces the error-rate by 8. 25 % (relative). (c) 2016 Elsevier Ltd. All rights reserved...|$|E
40|$|This {{paper is}} {{intended}} to advertise the public availability of the <b>articulatory</b> <b>feature</b> (AF) classification multi-layer perceptrons (MLPs) which {{were used in the}} Johns Hopkins 2006 summer workshop. We describe the design choices, data preparation, AF label generation, and the training of MLPs for feature classification on close to 2000 hours of telephone speech. In addition, we present some analysis of the MLPs in terms of classification accuracy and confusions along with a brief summary of the results obtained during the workshop using the MLPs. We invite interested parties to make use of these MLPs. 1...|$|R
40|$|We {{describe}} {{our recent}} work on improving an overlapping <b>articulatory</b> <b>feature</b> (sub-phonemic) based speech recognizer with robustness to {{the requirement of}} training data. A new decision-tree algorithm is developed and applied to the recognizer design which results in hierarchical partitioning of the articulatory state space. The articulatory states associated with common acoustic correlates, a phenomenon caused by the many-to-one articulation-to-acoustics mapping well known in speech production, are automatically clustered by the decision-tree algorithm. This enables effective prediction of the unseen articulatory states in the training, thereby increasing the recognizer's robustness. Some preliminary experimental results are provided...|$|R
2500|$|On {{the above}} interpretation, {{the split between}} the centum and satem groups {{would not have been}} a {{straightforward}} loss of an <b>articulatory</b> <b>feature</b> (palatalization or labialization). Instead, the uvulars [...] (the [...] "plain velars" [...] of the traditional reconstruction) would have been fronted to velars across all branches. In the satem languages, it caused a chain shift, and the existing velars (traditionally [...] "palatovelars") were shifted further forward to avoid a merger, becoming palatal: [...] > [...] > [...] In the centum languages, no chain shift occurred, and the uvulars merged into the velars. The delabialisation in the satem languages would have occurred later, in a separate stage.|$|R
40|$|This paper {{presents}} {{a method to}} produce a new vowel by articulatory control in hidden Markov model (HMM) based parametric speech synthesis. A multiple regression HMM (MRHMM) is adopted to model the distribution of acoustic features, with <b>articulatory</b> <b>features</b> used as external auxiliary variables. The dependency between acoustic and <b>articulatory</b> <b>features</b> is modelled {{by a group of}} linear transforms that are either estimated context-dependently or determined by the distribution of <b>articulatory</b> <b>features.</b> Vowel identity is removed from the set of context features used to ensure compatibility between the contextdependent model parameters and the <b>articulatory</b> <b>features</b> of a new vowel. At synthesis time, acoustic features are predicted according to the input <b>articulatory</b> <b>features</b> as well as context information. With an appropriate articulatory feature sequence, a new vowel can be generated even when it does not exist in the training set. Experimental results show this method is effective in creating the English vowel / 2 / by articulatory control without using any acoustic samples of this vowel...|$|E
40|$|This paper {{presents}} {{an investigation of}} ways to integrate <b>articulatory</b> <b>features</b> into Hidden Markov Model (HMM) -based parametric speech synthesis, primarily {{with the aim of}} improving the performance of acoustic parameter generation. The joint distribution of acoustic and <b>articulatory</b> <b>features</b> is estimated during training and is then used for parameter generation at synthesis time in conjunction with a maximum-likelihood criterion. Different model structures are explored to allow the <b>articulatory</b> <b>features</b> to influence acoustic modeling: model clustering, state synchrony and cross-stream feature dependency. The results of objective evaluation show that the accuracy of acoustic parameter prediction can be improved when shared clustering and asynchronous-state model structures are adopted for combined acoustic and <b>articulatory</b> <b>features.</b> More significantly, our experiments demonstrate that modeling the dependency between these two feature streams can make speech synthesis more flexible. The characteristics of synthetic speech can be easily controlled by modifying generated <b>articulatory</b> <b>features</b> as {{part of the process of}} acoustic parameter generation...|$|E
40|$|This paper {{presents}} {{a method to}} control the characteristics of synthetic speech flexibly by integrating <b>articulatory</b> <b>features</b> into a Hidden Markov Model (HMM) -based parametric speech synthesis system. In contrast to model adaptation and interpolation approaches for speaking style control, this method is driven by phonetic knowledge, and target speech samples are not required. The joint distribution of parallel acoustic and <b>articulatory</b> <b>features</b> considering cross-stream feature dependency is estimated. At synthesis time, acoustic and <b>articulatory</b> <b>features</b> are generated simultaneously based on the maximum-likelihood criterion. The synthetic speech can be controlled flexibly by modifying the generated <b>articulatory</b> <b>features</b> according to arbitrary phonetic rules in the parameter generation process. Our experiments show that the proposed method is effective in both changing the overall character of synthesized speech and in controlling {{the quality of a}} specific vowel...|$|E
40|$|<b>Articulatory</b> <b>feature</b> {{models have}} been {{proposed}} in the automatic speech recognition community {{as an alternative to}} phone-based models of speech. In this paper, we extend this approach to the visual modality. Specifically, we adapt a recently proposed feature-based model of pronunciation variation to visual speech recognition (VSR) using a set of visually-salient features. The model uses a dynamic Bayesian network to represent the evolution of the feature streams. A bank of SVM feature classifiers, with outputs converted to likelihoods, provides input to the DBN. We present preliminary experiments on an isolated-word VSR task, comparing feature-based and viseme-based units and studying the effects of modeling inter-feature asynchrony. 1...|$|R
40|$|Annotation {{of large}} {{multilingual}} corpora remains {{a challenge to}} the data-driven approach to speech research, especially for under-resourced languages. This paper presents crosslanguage automatic phonetic segmentation using Hidden Markov Models (HMMs). The underlying notion is segmentation based on articulation (manner and place) so as to provide extensive models that will be applicable across languages. A test on the Appen Spanish speech corpus gives phone recognition accuracy of 61. 15 % when bootstrapped with acoustic models trained on the TIMIT as compared with a baseline result of 54. 63 % for flat start initialization of the monophone models. Index Terms — Automatic phonetic segmentation, hidden Markov models, <b>articulatory</b> <b>feature...</b>|$|R
40|$|In the {{presence}} of pronunciation variation and the masking ef-fects of additive noise, we investigate the role of phonetic in-formation reduction and lexical confusability on ASR perfor-mance. Contrary to previous work [1], we show that place of ar-ticulation as a representation for unstressed segments performs at least as well as manner of articulation in {{the presence}} of ad-ditive noise. Methods of phonetic reduction introduce lexical confusibility which negatively impact performance. By limiting this confusability, recognizers that employ high levels of pho-netic reduction (40. 1 %) can perform as well a baseline system in {{the presence of}} nonstationary noise. Index Terms: spoken language analysis, speech recognition, <b>articulatory</b> <b>feature...</b>|$|R
