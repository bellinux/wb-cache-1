29|25|Public
5|$|A {{separate}} 2008 {{study from}} Western Michigan University found that hybrids and conventional vehicles are equally safe when travelling {{more than about}} , because tire and wind noise generate most of the <b>audible</b> <b>cues</b> at those speeds. Hybrid cars were also tested safe when leaving a stoplight and {{it was found that}} under this condition they do not pose a risk to pedestrians. All Prius models used in the study engaged their internal combustion engines when accelerating from a standstill and produced enough noise to be detected.|$|E
25|$|The ball used is {{significantly}} {{larger than the}} regular cricket ball and is filled with ball bearings that provide <b>audible</b> <b>cues.</b>|$|E
5000|$|Additionally, The Way Out and The Way In have {{numerous}} connective {{visual and}} <b>audible</b> <b>cues</b> throughout {{to establish the}} narrative and physical connection between stories that create the film's never-ending loop. These include: ...|$|E
50|$|The show's {{complete}} schedule can {{be found}} on its website. Because the show is so frequently repeated, <b>audible</b> <b>cue</b> signals ("stingers") are inserted {{at the beginning and end}} of commercial breaks, to facilitate substitution of commercials by local stations.|$|R
50|$|The key to {{defeating}} each {{opponent is}} to learn their fighting patterns, avoid their attacks and respond with a counterattack. Opponents will often give a visual or <b>audible</b> <b>cue</b> to signal their attacks. If the player successfully dodges an attack, the opponent will be left vulnerable for a while, allowing the player to strike back. The player can defeat enemy boxers by knocking them down for a count of 10, downing them three times in one round for a TKO, or by a referee's decision.|$|R
2500|$|There {{have also}} been several cases of cashiers at lottery {{retailers}} who have attempted to scam customers out of their winnings. Some locations require the patron to hand the lottery ticket to the cashier {{to determine how much}} they have won, or if they have won at all, the cashier then scans the ticket to determine one or both. In cases where there is no visible or <b>audible</b> <b>cue</b> to the patron of the outcome of the scan some cashiers have taken the opportunity to claim that the ticket is a loser or that it is worth far less than it is and offer to [...] "throw it away" [...] or surreptitiously substitute it for another ticket. The cashier then pockets the ticket and eventually claims it as their own.|$|R
5000|$|Pedestrian {{infrastructure}} such as sidewalks; {{traditional and}} raised crosswalks; median crossing islands; Americans with Disabilities Act of 1990 compliant facilities including <b>audible</b> <b>cues</b> {{for people with}} low vision, pushbuttons reachable by people in wheelchairs, and curb cuts; and curb extensions.|$|E
5000|$|Since 2002, the Evangelical School for the Deaf bell choir {{has been}} {{performing}} in venues around Puerto Rico. Initially conceived {{and directed by}} Marisol Matos, the bell choir has grown in popularity and skill. The students are directed using sign language instead of <b>audible</b> <b>cues</b> and rely on sheet music {{to know when to}} play their parts.|$|E
5000|$|Accessibility - [...] "the {{matching}} of people's {{information and}} service needs with {{their needs and}} preferences in terms of intellectual and sensory engagement with that information or service, and control of it". [...] A service need could include a driver who cannot take their eyes off the road to study a map display; or a visitor {{in a foreign country}} who cannot understand the language of the <b>audible</b> <b>cues</b> of a LBS provider.|$|E
40|$|Steriade (1993, 1995, 1996) has {{proposed}} that the distribution of phonological contrasts is explained, not by a principle of Prosodic Licensing (Itô 1986, Goldsmith 1990) (licensing of features only in certain prosodic positions, e. g. onset position), but rather {{by the presence of}} sufficiently <b>audible</b> phonetic <b>cues</b> in the relevant contexts tha...|$|R
40|$|Popping of starchy pellets in a {{domestic}} microwave oven has proven difficult compared to pellets expanded in frying oil, {{and even to}} microwave expanded popcorn. These pellets encounter problems like uneven popping, burning {{and the absence of}} an <b>audible</b> <b>cue</b> for the end of popping. The lack of a moisture barrier, like the pericarp of popcorn, leads to the development of an inverse temperature gradient in the pellets and a significant moisture loss before and after expansion. In this paper we review the hypotheses and solutions for the criticality of microwave popping of starchy pellets, as discussed in scientific and patent literature. We have found a large collection of critical factors and their associated hypotheses, which have been structured via linking them to specific steps in the physical expansion process. We conclude with a list of hypotheses that we view valuable for further investigation. </p...|$|R
500|$|The team {{developed}} the game's puzzles by first assuming the player was their [...] "worst enemy", and made puzzles as devious as possible, but then scaled back their difficulty or added visual and audible aids {{as if the}} player was a friend. One example given by Carlsen is a puzzle involving a spider early in the game; the solution requires pushing a bear trap to snare the spider's legs in it. Early designs of this puzzle had the bear trap on the same screen as the spider, and Playdead found playtesters focused {{too much on the}} trap. The developers altered the puzzle to put the trap in a tree in an earlier off-screen section when facing the spider; the spider's actions would eventually cause this trap to drop to the ground and become a weapon against the spider. Carlsen stated that this arrangement created a situation where the player felt helpless when initially presented with the deadly spider, but then assisted the player through an <b>audible</b> <b>cue</b> when the trap had dropped, enabling the player to discover the solution.|$|R
50|$|Bobino is a Quebec French {{language}} children's {{television show}} made in Quebec and broadcast on Radio Canada, the French language television {{service of the}} Canadian Broadcasting Corporation, between 1957 and 1985. Its stories revolved around Bobino (a kind gentleman played by Guy Sanche) and his sister Bobinette (a puppet voiced by Paule Bayard and later by Christine Lamer). The cast is complemented {{by a number of}} other characters which never appear on screen but who interact with the cast by visual or <b>audible</b> <b>cues.</b>|$|E
50|$|A {{separate}} 2008 {{study from}} Western Michigan University found that hybrids and conventional vehicles are equally safe when travelling {{more than about}} 20 mph, because tire and wind noise generate most of the <b>audible</b> <b>cues</b> at those speeds. Hybrid cars were also tested safe when leaving a stoplight and {{it was found that}} under this condition they do not pose a risk to pedestrians. All Prius models used in the study engaged their internal combustion engines when accelerating from a standstill and produced enough noise to be detected.|$|E
50|$|There are {{concerns}} about the access Echo has to private conversations in the home, or other non-verbal indications that can identify who {{is present in the}} home and who is not—based on <b>audible</b> <b>cues</b> such as footstep-cadence or radio/television programming. Amazon responds to these concerns by stating that Echo only streams recordings from the user's home when the 'wake word' activates the device, though the device is technically capable of streaming voice recordings at all times, and in fact will always be listening to detect if a user has uttered the word.|$|E
40|$|The {{question}} of whether power-frequency magnetic fields of strengths relevant to industrial exposure can affect heart rhythm remains controversial. Because the reported effects on heart rate (HR) are so small, procedures which can provoke changes in the sympathovagal balance {{in a controlled manner}} may have a greater capacity for identifying subtle field-related changes, if they do exist. We have investigated HR and heart rate variability (HRV) spectral indices in 20 volunteers subjected to a tilt from the supine position to 60 °, head up. The tilting procedure was carried out under two conditions, field (28 µT resultant, circularly polarized) and sham, in a balanced double-blind design. Subjects were instructed to breathe in time with an <b>audible</b> <b>cue</b> at 2. 5 s intervals. Although the anticipated significant changes in HR and the high frequency (HF), low frequency (LF) and LF/HF ratio (log transformed) occur with tilting, {{there were no significant differences}} between corresponding measures with and without exposure to magnetic fields (tilt ln LF/HF ratio 0. 94 ± 0. 19 and 0. 95 ± 0. 20 for sham and field, respectively). There was also no evidence of a field-related trend in spectral alterations when the time following tilting was divided into three 256 s epochs...|$|R
5000|$|Mazda: Driver Attention Alert [...] Activates {{at speeds}} above 65 km/h. Learns driving {{behavior}} through steering input and position of road during {{the beginning of}} the ride and compares the learned data during later stages of the ride. A difference above a certain threshold triggers an <b>audible</b> and visual <b>cue.</b>|$|R
40|$|Gestures {{and visible}} speech cues are often {{available}} to listeners to aid their {{comprehension of the}} speaker’s meaning. However, visual communication cues are not always beneficial over-and-above the <b>audible</b> speech <b>cues.</b> My goal is to outline several types of constraints which operate in the human cognitive processing system that bear on this question: When do visual language cues (visible speech and gestures) provide an aid to comprehension, and when do they not? Research on visual-spoken language comprehension carried out in my lab over recent years is described and recommendations will be made concerning the design of multi-modal interfaces...|$|R
50|$|In some titles, {{the world}} is {{generated}} randomly so that players must actively search for food and weapons, often provided with visual and <b>audible</b> <b>cues</b> {{of the types of}} resources that may be found nearby. The player-character will typically have a health bar and will take damage from falling, starving, drowning, contact with fire or deadly liquids, or being attacked by monsters that inhabit the world. Other metrics may also come into play; the survival title Don't Starve features a separate hunger gauge and a sanity meter, which (if allowed to fully deplete) will cause the death of the character. Character death may not {{be the end of the}} game; the player may be able to respawn and return to the game location at which his character died in order to retrieve lost equipment. Other survival games use permadeath: the character has one life, and dying requires that the game be restarted from the beginning. While many survival games are aimed at constantly putting the player at risk from hostile creatures or the environment, others may downplay the amount of danger the player faces and instead encourage more open-world gameplay, where player-character death can still occur if the player is not careful or properly equipped.|$|E
40|$|Sound {{localisation}} is {{a complex}} perceptual process that involves the integration of information derived from multiple <b>audible</b> <b>cues.</b> Human sound localisation is mediated by these <b>audible</b> <b>cues.</b> The primary <b>audible</b> <b>cues</b> are the interaural time difference (ITD), phase difference (IPD) and intensity difference (IID) that result from diffraction of sound waves around the head and pinnae. The ITD and the IID have received much attention in the literature as they {{are considered to be}} the most important binaural cues. These cues result from a neural comparison between the signals at both ears. Although these cues are signal-dependent, they have not been investigated using day-to-day life signals due to the experimentation complexity. Many research studies employed either single frequency or unnatural signals to draw conclusions. Further, the IPD could not be distinguished from the ITD for multiple frequency signals. This thesis presents new experimental techniques which are developed to investigate the role of primary <b>audible</b> <b>cues</b> in sound localisation. These techniques independently control and investigate the cues with multiple frequency signals. Listening experiments are initially completed with single cues which are followed by experiments with cues in conflict. The stimuli are chosen to include representative day-to-day life signals. Two <b>audible</b> <b>cues</b> at a time are used simultaneously in conflict so that their relative importance can be determined. Experimental techniques included a mathematical approach to manipulate the phase of all the component frequencies in a multiple frequency signal, whilst leaving the amplitude structure unchanged. This approach is adopted to distinguish the IPD from an equivalent ITD for multiple frequency signals. It is therefore possible to investigate the IPD independently with multiple frequency signals. The experimental results indicate that the effect of the IPD can be compensated for by an appropriate opposing ITD. In general, localisations become centrally diffuse when either the ITD or the IPD is placed in conflict with the IID. A localisation model is developed to provide predictions for the effect of one cue against an alternative cue, similar to the experiments. The results from the model are in agreement with the experiments. It is not evident whether the ITD or the IPD is more effective. The new techniques and the localisation model provide the opportunity to investigate the primary <b>audible</b> <b>cues</b> with representative day-to-day life signals. It has become possible to independently assess these cues and to draw conclusions based on their role in sound localisation...|$|E
40|$|This paper {{describes}} {{the development of}} a wearabIe, stereo-camera based prosthetic device, allowing visually impaired persons to perceive a 'depth-image ' of the scene ahead, conveyed using vibrating actuator cues. Preliminary tests indicate that such a device may be a viable and preferable alternative to existing aids including those based on ultra-sonic sensors and <b>audible</b> <b>cues,</b> while other alternatives such as retinal and cortical implants, are still only {{in the early stages of}} development. ...|$|E
50|$|Rhythmic Auditory Stimulation or RAS is a Neurologic {{music therapy}} tool in which <b>audible,</b> rhythmic <b>cues</b> are {{provided}} to clients {{in order to}} strengthen and improve intrinsic rhythmic functions, such as gait. Through entrainment, the process in which a person's internal rhythm syncs with an external source, RAS can provide a safe, effective treatment in rehabilitation of rhythmic biological movements. The rhythmic cues can be provided by a guitar, a metronome, or any other medium that can deliver accented beats {{in a manner that}} follows a prescribed tempo. These cues are most often in meters of 2/4 and 4/4.|$|R
5000|$|The NBC chimes are a {{sequence}} of three tones played on National Broadcasting Company (NBC) broadcasts. Originally developed in 1927 as seven notes, they were standardized to the current three note version by the early 1930s, and possibly as early as 1929. The chimes were originally employed as an <b>audible</b> programming <b>cue,</b> used to alert network control engineers and the announcers at NBC's radio network affiliates. They soon became associated with NBC programming in general, and are an early example of an [...] "interval signal" [...] used to help establish a broadcaster's identity with its audience.|$|R
40|$|We {{describe}} the initial {{development of an}} artificial phonological loop (APL), a new technology to assist individuals with impairment of the working memory system. The phonological loop, along with the visuospatial sketchpad, {{is one of the}} two slave short-term memory subsystems that comprise working memory, a cognitive function closely associated with the control of attention. In the phonological loop, phonological (speech) information lasting for 1 – 2 second is maintained active by repetitive, subvocal (silent speech) rehearsal. Deficits in working memory, specifically in the phonological loop, occur in many disorders, including attention-deficit disorder and Alzheimer’s disease. In these disorders, it appears that the ability for phonological rehearsal is intact, but the regulation or triggering of the rehearsal process is inadequate, thus causing the contents of working memory to be lost. The purpose, then, of the APL is to facilitate the phonological loop by artificially extending the duration of phonological rehearsals. The APL mimics the natural phonological loop by providing audible vocal echoes {{to take the place of}} subvocal rehearsals. In this system, the user talks to him/herself in short (1 – 2 second) phrases; the device records these phrases, stores them in electronic memory, and then repeats— i. e., echoes—the phrases multiple times over an extended period. Two versions of this device have been developed: the Echo-APL and the Rearticulation-APL. In the Echo-APL, only echoing is involved. In the Rearticulation-APL, however, the user re-vocalizes (rearticulates) the phrase in response to an <b>audible</b> <b>cue.</b> The device repeats the cue until it detects (hears) the re-vocalization. Future research and development of the APL will require extensive testing and careful evaluation of possible echo-schedules: the predefined program controlling inter-echo time intervals and echo-amplitude (echo loudness). The APL essentially exteriorizes the silent phonological loop and makes it audible. It is a system that helps the user to “talk to him/herself” to keep ideas in mind...|$|R
40|$|Conversation {{partners}} on {{mobile phones}} can align their walking gait without physical proximity or visual feedback. We investigate gait synchronization, measured by accelerometers while users converse via mobile phones. Hilbert transforms {{are used to}} infer gait phase angle, and techniques from synchronization theory are used to infer level of alignment. Experimental conditions {{include the use of}} vibrotactile feedback to make one conversation partner aware of the other’s footsteps. Three modes of interaction are tested: reading a script, discussing a shared image and spontaneous conversation. The vibrotactile feedback loop on its own is sufficient to create synchronization, but there are complex interference effects when users converse spontaneously. Even without vibration crosstalk, synchronisation appeared for long periods in the spontaneous speech condition, indicating that users were aligning their walking behaviour from <b>audible</b> <b>cues</b> alone. Author Keywords Accelerometer, alignment, synchronization, mobile devices...|$|E
40|$|This {{research}} {{was designed to}} investigate the fundamental nature in which blind people utilize <b>audible</b> <b>cues</b> to attend to their surroundings. Knowledge on how blind people respond to external spatial stimuli is expected to assist in development of better tools for helping people with visual disabilities navigate their environment. There was also interest in determining how blind people compare to sighted people in auditory localization tasks. The ability of sighted individuals, blindfolded individuals, and blind individuals in localizing spatial auditory targets was assessed. An acoustic display board allowed the researcher to provide multiple sound presentations to the subjects. The subjects’ responses in localization tasks were measured {{using a combination of}} kinematic head tracking and eye tracking hardware. Data was collected and analyzed to determine the ability of the groups in localizing spatial auditory targets. Significant differences were found among the three groups in spatial localization error and temporal patterns...|$|E
40|$|In a {{baseline}} study for training purposes, two indicators of acute respiratory infections (the respiratory rate (RR) and chest indrawing) were assessed by Ministry of Health physicians in Egypt using a WHO test videotape. Chest indrawing, {{as defined by}} the WHO Acute Respiratory Infections (ARI) programme, was not widely recognized by current health personnel. Viewing a WHO training videotape led to significantly more correct assessments of chest indrawing compared with a group that had not viewed this videotape. The accuracy of using a timer versus a watch, and a 30 -second versus 60 -second counting interval was also evaluated. Rates counted over 60 seconds were more accurate than 30 -second counts although the difference between them was not clinically significant. Counting of rates using timers with <b>audible</b> <b>cues</b> was comparable to using watches with second hands. Careful training of primary health workers in the assessment of RR and chest indrawing is essential if these clinical findings are to be used as reliable indicators in pneumonia treatment algorithms...|$|E
40|$|Closed access. This paper proposes an {{immersive}} audio rendering scheme for networked 3 D multimedia systems. The spatial audio rendering method based on wave field synthesis is particularly useful for applications where multiple listeners experience a true spatial soundscape while being {{free to move}} without losing spatial sound properties. The proposed approach {{can be considered as}} a general solution to the static listening restriction imposed by conventional methods, which rely on an accurate sound reproduction within a sweet spot only. The paper reports on the results of numerical analysis and experimental validation using various sound sources. It is demonstrated and confirmed that while covering the majority of the listening area, the developed approach can create a variety of virtual audio objects at target positions with very high accuracy. Subjective evaluation results show that an accurate spatial impression can be achieved with multiple simultaneous <b>audible</b> depth <b>cues</b> improving localization accuracy over single object rendering...|$|R
40|$|It is {{important}} to understand the occupant behavioural response in fire emergency in high-rise buildings so that proper decisions for a fire safety management plan could be made. In this study, behavioural responses of building occupants to an <b>audible</b> fire alarm <b>cue</b> in high-rise buildings in Hong Kong were investigated by a survey of 327 building occupants. They were divided into two groups, where Group 1 had experienced a building fire before while Group 2 had not. The results showed that occupant experience of fire incidents might influence their perception of a fire alarm cue. Group 1 respondents would likely recognize an <b>audible</b> fire alarm <b>cue</b> as a true fire alarm and give immediate response, while the Group 2 respondents would treat the cue as a false alarm or a fire alarm test. Group 2 respondents would investigate the situation if only the fire alarm sustained for a few minutes. Regarding the evacuation routes, respondents tended to select a staircase for emergency evacuation. The choice of route did not depend on the floor on which the evacuation of an occupant started. Apart from the nearest staircase, some occupants would choose a familiar staircase. The results of this study would be a useful source of reference for evacuation plan development of high-rise buildings by taking human behaviour into account. Department of Building Services Engineerin...|$|R
40|$|Face {{recognition}} {{has been}} a huge area of research over the past 25 years [Gao and Leung, 2002]. However the field is still highly unsolved, largely due to variations in pose, illumination and expression. In this paper we propose using pose recognition to solve the first of the face recognition variation problems and simultaneously use both recognised pose and face information to control a robot in a surveillance application. The main advantage of using face and pose recognition to control a robot is a more natural man/machine interaction with the recognised controller (with sufficient clearance) being able to control the robot from their point of view. Also, less manual (hands-on) controls will be required, if the control system is mainly visual. Finally in crowded environments visual control cues from a recognised person are more robust and secure than <b>audible</b> control <b>cues</b> (which are the alternative to either visual or hands-on cues). The recognition method used for both the face and pose recognition is based on geometric 3 -Dimensional feature point matching. ...|$|R
40|$|This is {{a conference}} paper. It is {{available}} from; [URL] impact that wind turbines {{have on the}} environment, {{particularly with respect to}} wildlife such as bat species, has generated increasing concern over the last decade. Although the harnessing of wind power is becoming much more widespread as a clean, renewable energy resource, the increasing global turbine mortality rates for bats are thought to be significantly detrimental to susceptible species. Much research is still needed to fully understand the ways in which turbines affect bats, since they rely on echolocation and <b>audible</b> <b>cues</b> to hunt and navigate, therefore having a unique acoustic perspective of objects in their vicinity. Here we present an overview of what is currently known regarding ultrasonic emissions from operational wind turbine structures, including noise generated from the gearing mechanism, rotor, or through blade defects, and how such noise may be perceptible to some bat species in the local turbine habitat...|$|E
40|$|The {{older adults}} will {{eventually}} experience unavoidable {{changes in their}} cognitive, physical and perceptual ability. Several assistive technologies {{have been used to}} support the physical, emotional and social needs of the elderly to enable them to perform their daily activities but lack the support for the activities related to their spiritual needs. We present our work where we explore the intersections of technological development and spiritual practices. As we argue that supporting the praying needs will support the spiritual needs of the older adults, we proposed a technological aid for supporting the praying activity among elderly Muslims as a form of mediated practice. We present the description and design of our Smart Prayer Mat, a smart praying mat embedded with a textile-based sensor that prompts <b>audible</b> <b>cues</b> to alert elderly Muslims with cognitive impairment of the completion of ritual cycles (raka’ahs) while performing their prayer practices. This novel aid will shed new light upon the convergence of technology and spirituality, techno spiritual space...|$|E
40|$|A. M. Gadomski, ' N. Khallaf, 2 S. El Ansary, 3 & R. E. Black 4 In a {{baseline}} study for training purposes, two indicators of acute respiratory infections (the respiratory rate (RR) and chest indrawing) were assessed by Ministry of Health physicians in Egypt using a WHO test videotape. Chest indrawing, {{as defined by}} the WHO Acute Respiratory Infections (ARI) programme, was not widely recognized by current health personnel. Viewing a WHO training videotape led to signifi-cantly more correct assessments of chest indrawing compared with a group that had not viewed this videotape. The accuracy of using a timer versus a watch, and a 30 -second versus 60 -second counting interval was also evaluated. Rates counted over 60 seconds were more accurate than 30 -second counts although the difference between them was not clinically significant. Counting of rates using timers with <b>audible</b> <b>cues</b> was comparable to using watches with second hands. Careful training of primary health workers in the assessment of RR and chest indrawing is essential if these clinical findings are to be used as reliable indicators in pneumonia treatment algorithms...|$|E
40|$|Background: The {{amount of}} {{tinnitus}} related distress is {{correlated with the}} amount of attention being dedicated to the tinnitus. Unconsciously, much of attention is being attended towards the tinnitus signal. Attending to a particular frequency decreases detection performance for an auditory target of a different frequency. This is called an attention filter. When subjects are exposed to a clearly <b>audible</b> <b>cue</b> that tells what frequency to expect, the subject responds better to this particular frequency. Objective: The aim {{of this study was to}} investigate the best research setting to measure an attention filter. In addition, the aim was to determine if tinnitus patients show a performance peak at their tinnitus frequency when performing a tone-detection task and whether this performance decreases when tinnitus patients attend to a different frequency than their tinnitus frequency (cue tone). Method: First case-control study with healthy subjects was performed to determine the optimal research setting. Subsequently the optimal research setting was used for tinnitus patients and control subjects. The participants performed four listening tasks. Continuous broadband noise was present during the experiments. Target tones were presented in one of two intervals. In two experiments a cue tone was added before the first interval. Statistical analysis was done by using Z-test for proportions. Results: The attention filter was observed for a cue of 1000 Hz, but not for a cue of 820 Hz. A change in level of background noise and a baseline performance of 85 % instead of 75 % did not make any difference for this attention filter. A baseline performance of 85 % provided a better motivation. In contrast to control subjects, tinnitus patients did show an attention filter for a cue of 1000 Hz. Tinnitus patients did not show an attention filter for their tinnitus frequency, nor did they show enhanced mean detection for a cue tone of 0. 68 *Ft (tinnitus frequency). The performance at tinnitus frequency did not decrease when patients attended to a different frequency (cue tone). Conclusion: In healthy subjects it is possible to perform an increase in attention to a cue tone of 1000 Hz. In contrast to a cue tone of 820 Hz, an attention filter can be witnessed for a cue tone of 1000 Hz. This research does not demonstrate an increased attention at the tinnitus frequency. There is an increase in performance for an octave below tinnitus frequency. The detection performance does not decrease when tinnitus patients attend to a different frequency (cue tone). ...|$|R
40|$|BACKGROUND: Deficits in emotion {{perception}} are {{a well-established}} phenomenon in schizophrenic patients and studies have typically used unimodal emotion tasks, presenting either emotional faces or emotional vocalizations. We introduced bimodal emotion conditions in two previous studies {{in order to}} study the process of multisensory integration of visible and <b>audible</b> emotion <b>cues.</b> We now build on our earlier work and address the regulatory effects of selective attention mechanisms {{on the ability to}} integrate emotion cues stemming from multisensory channels. METHODS: We added a neutral secondary distractor condition to the original bimodal paradigm in order to investigate modality-specific selective attention mechanisms. We compared schizophrenic patients (n= 50) to non-schizophrenic psychotic patients (n= 46), as well as to healthy controls (n= 50). A trained psychiatrist used the Schedules of Clinical Assessment in Neuropsychiatry (SCAN 2. 1) to diagnose the patients. RESULTS: As expected, in healthy controls, {{and to a lesser extent}} in non-schizophrenic psychotic patients, modality-specific attention attenuated multisensory integration of emotional faces and vocalizations. Conversely, in schizophrenic patients, auditory and visual distractor conditions yielded unaffected and even exaggerated multisensory integration. CONCLUSIONS: These results suggest that schizophrenics, as compared to healthy controls and non-schizophrenic psychotic patients, have modality-specific attention deficits when attempting to integrate information regarding emotions that stem from multichannel sources. Various explanations for our findings, as well as their possible consequences, are discussed...|$|R
40|$|Emotions can be {{recognized}} by <b>audible</b> paralinguistic <b>cues</b> in speech. By detecting these paralinguistic cues that can consist of laughter, a trembling voice, coughs, changes in the intonation contour etc., information about the speaker’s state and emotion can be revealed. This paper describes {{the development of a}} gender-independent laugh detector with the aim to enable automatic emotion recognition. Different types of features (spectral, prosodic) for laughter detection were investigated using different classification techniques (Gaussian Mixture Models, Support Vector Machines, Multi Layer Perceptron) often used in language and speaker recognition. Classification experiments were carried out with short pre-segmented speech and laughter segments extracted from the ICSI Meeting Recorder Corpus (with a mean duration of approximately 2 s). Equal error rates of around 3 % were obtained when tested on speaker-independent speech data. We found that a fusion between classifiers based on Gaussian Mixture Models and classifiers based on Support Vector Machines increases discriminative power. We also found that a fusion between classifiers that use spectral features and classifiers that use prosodic information usually increases the performance for discrimination between laughter and speech. Our acoustic measurements showed differences between laughter and speech in mean pitch and in the ratio of the durations of unvoiced to voiced portions, which indicate that these prosodic features are indeed useful for discrimination between laughter and speech...|$|R
