0|22|Public
30|$|The {{strengths}} of the grading methodology are {{that it is an}} objective method that is reproducible and allows assessment while masked to treatment <b>assignment.</b> <b>Limitations</b> of the fundus photographic grading include the relatively small field of view captured with this protocol, compared with the overall retina. Macular edema cannot be easily graded with fundus photographs in a uveitis cohort as epiretinal membrane presence precludes the assessment of underlying retinal thickness. The solution for this in MUST was the reliance on OCT grading for determining retinal thickness and morphology associated with edema, such as cystoid spaces [37]. Patients in the MUST Trial had substantial media opacities (vitreous haze, cataract, keratitic precipitates, pupillary adhesions) that limited the number of eyes that could be adequately photographed. Despite these limitations, the MUST study provides the largest repository of morphological data obtained from a prospective uveitis clinical trial to date, allowing improved correlation of baseline and follow-up findings with other outcome measures such as visual acuity and uveitis control.|$|R
50|$|Spectrum <b>assignments</b> and {{operational}} <b>limitations</b> are not consistent worldwide: Australia and Europe allow {{for an additional}} two channels (12, 13) beyond the 11 permitted in the United States for the 2.4 GHz band, while Japan has three more (12-14). In the US and other countries, 802.11a and 802.11g devices may be operated without a license, as allowed in Part 15 of the FCC Rules and Regulations.|$|R
40|$|Petroleum {{asphaltenes}} are {{a complex}} mixture of organic molecules containing mainly fused polyaromatic and naphthenic systems and pendant chains, polar moieties with heteroatoms (S, N, and O), and transition metals. A variety of spectroscopic techniques has been employed to characterize asphaltenes, but their structures remain largely elusive {{because of the}} complexity, variety of samples, and <b>assignment</b> <b>limitations.</b> Carbon- 13 nuclear magnetic resonance (13 C NMR) spectroscopy has contributed extensively to asphaltene characterization. However, proper assignment of 13 C NMR spectra is very challenging because spectra of natural asphaltenes feature {{a large number of}} peaks in unusual environments, which may be hard to assign and interpret. We employ the dispersion-corrected 3 ̆c 9 B 97 X-D density functional with 6 - 31 G(d,p) basis set to rationalize common trends in the 13 C NMR chemical shifts of asphaltene model compounds. The calculated 13 C NMR chemical shifts for a calibration series of 14 aromatic and heterocyclic reference compounds containing C atoms of types similar to those in the asphaltene model compounds are found to correlate linearly with the respective experimental values. The linear fitting yields a correlation coefficient of R 2 = 0. 99 and absolute errors of less than 10 ppm. Moreover, we calculate and calibrate the 13 C chemical shifts of asphaltenes extracted from Brazilian vacuum residues to analyze and correlate the C atom types with those of the reference compounds. It is found that the presence of heteroatoms as well as environments with a high aromatic condensation index can significantly affect the chemical shifts. The effect of heteroatoms on the chemical shift, a situation that has scarcely been addressed in the literature, is evaluated here in detail. The results are intended to help interpret 13 C NMR spectra and allow for a more complete characterization of asphaltene molecules. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Although a {{large number}} of {{economic}} analyses of allergic rhinitis have been published, there are relatively few empirically based studies, particularly outside the US. The majority of these analyses can be classified as burden-of-illness studies. Most estimates of the annual cost of allergic rhinitis range from $US 2 - 5 billion (2003 values). The wide range of estimates can be attributed to differences in identifying patients with allergic rhinitis, differences in cost <b>assignment,</b> <b>limitations</b> associated with available data and difficulties in assigning indirect costs (associated with reduced productivity) of allergic rhinitis. Approximately one-third of burden-of-illness studies include direct and indirect costs of allergic rhinitis, about one-third focus on direct costs only, and the remaining one-third focus exclusively on indirect costs due to reduced productivity. Indirect costs attributable to allergic rhinitis were higher in studies only estimating indirect costs ($US 5. 5 - 9. 7 billion) than in those estimating both direct and indirect costs ($US 1. 7 - 4. 3 billion). Although there are many economic evaluations of allergic rhinitis treatments in the published medical literature, very few represent formal cost-effectiveness evaluations that compare the incremental costs and benefits of alternative treatment strategies. Those that are incremental cost-effectiveness analyses have several limitations, including small samples, short study periods and the lack of a standardized measure of effectiveness. To date, the medical literature is lacking a comprehensive economic evaluation of general treatment strategies for allergic rhinitis. In undertaking such an analysis, serious consideration must be given to the study population of interest, the choice of appropriate comparators, the perspective from which the analysis is conducted, the target audience, the changing healthcare marketplace and the selection of a measure of effectiveness that incorporates both positive and negative aspects of treatments for allergic rhinitis. Future work would benefit from the development of a consensus on a summary measure of effectiveness {{that could be used in}} cost-effectiveness analyses of therapies for allergic rhinitis as well as additional empirical work to measure the association between severity of disease and its impact on worker productivity. Allergic-rhinitis, Allergic-rhinitis, Antihistamines, Corticosteroids, Cost-of-illness...|$|R
30|$|The first module is trip generation, {{which makes}} use of land use and {{socioeconomic}} data, such as demographic and population data, {{to determine the number}} of trips produced by and attracted to traffic zones. The second module is trip distribution which determines the OD of trips that have been estimated in the first module. The third module, model split, organises the trip into different modes of transport (i.e. private mode, train, tram, bus, cycling and walking). The fourth module is traffic assignment which allocates trips to different modes in the transportation network [12]. This study did not involve traffic <b>assignment</b> due to <b>limitations</b> in resources and data. Base year travel time and cost estimates were assumed in this study.|$|R
40|$|A {{heuristic}} {{procedure is}} developed to assign buses to transit centers (garages) {{in such a}} way that all the buses on a particular route are assigned to a single transit center. This research builds on an optimal mixed integer programming location/allocation model that splits the bus <b>assignments</b> when capacity <b>limitations</b> were reached at a transit center. The heuristic procedure adopts a two-step process: namely, assignment of all buses of a route to a unique transit center, then switching of routes to alternative transit centers to enforce capacity limitations. The procedure is shown to still provide cost savings over current locations and allocations for the Vancouver Regional Transit System (VRTS), Canada’s largest urban transit net...|$|R
40|$|Human {{resource}} managers {{struggle with}} attracting potential expatriates {{as well as}} enhancing the success rate {{of those who do}} accept expatriate assignments. The trend toward increased numbers of expatriates is expected to continue. More recently, companies are requiring more junior-level employees to accept international moves. This present study examines the experiential and psychological factors related to new hires' propensity to accept international assignments. Psychological variables included international orientation, career insight, attachment to family and friends, outcome expectancies, extraversión, and selfefTicacy. The experiential variables included prior international experiences and prior number of domestic moves. The study employed samples from both an Australia (n = 89) and a United States (n = 86) university. The hypotheses were tested by hierarchical multiple regression to test if the situational variable of attachment to family and friends remained related to receptivity for international work after the individual variables were taken into account. The model explained 69 % of the variance with international orientation being the most highly related variable. The country of origin was the next most important and then outcome expectancies and self-efficacy. International experience was also relevant but less so and willingness to relocate domestically was very weakly related. The results have implications for human resource policies related to selection, training and reward systems related to expatriate <b>assignments.</b> The <b>limitations</b> of the present study are examined and future research needs are discussed...|$|R
40|$|Grid {{computing}} infrastructures need {{to provide}} traceability and accounting of their users" activity and protection against misuse and privilege escalation. A central aspect of multi-user Grid job environments is the necessary delegation of privileges {{in the course of}} a job submission. With respect to these generic requirements this document describes an improved handling of multi-user Grid jobs in the ALICE ("A Large Ion Collider Experiment") Grid Services. A security analysis of the ALICE Grid job model is presented with derived security objectives, followed by a discussion of existing approaches of unrestricted delegation based on X. 509 proxy certificates and the Grid middleware gLExec. Unrestricted delegation has severe security consequences and limitations, most importantly allowing for identity theft and forgery of delegated <b>assignments.</b> These <b>limitations</b> are discussed and formulated, both in general and with respect to an adoption in line with multi-user Grid jobs. Based on the architecture of the ALICE Grid Services, a new general model of mediated definite delegation is developed and formulated, allowing a broker to assign context-sensitive user privileges to agents. The model provides strong accountability and long- term traceability. A prototype implementation allowing for certified Grid jobs is presented including a potential interaction with gLExec. The achieved improvements regarding system security, malicious job exploitation, identity protection, and accountability are emphasized, followed by a discussion of non- repudiation in the face of malicious Grid jobs...|$|R
30|$|The Center for Employment and Training (CET) is {{a nonprofit}} {{community}} based program in San Jose California that serves groups with labor market problems. During the 1980 s, CET participated in two randomized controlled trials (RCTs) and was the only site in either demonstration to achieve labor market success. Based on these promising findings in two separate RCTs, the US Department of Labor replicated the CET program in 12 sites. The findings in the replication sites were disappointing, with no statistically significant findings 54 months after random <b>assignment.</b> An important <b>limitation</b> of the replication efforts was that only four of the 12 sites implemented the CET model with high fidelity. The evaluators offered several hypotheses on why the replications failed to reproduce the strong findings in the original studies, {{but there was no}} way to test the hypotheses.|$|R
40|$|When {{designing}} adaptive tutoring systems, {{a myriad}} of psychological theories {{must be taken into}} account. Popular notion follows cognitive theory in supporting multi-channel processing, while working under assumptions that pedagogical agents and affect detection are of the utmost significance. However, motivation and affect are complex human characteristics that can muddle human-computer interactions. The following study considers the promotion of the growth mindset, as defined by Carol Dweck, within middle school students using an intelligent tutoring system. A randomized controlled trial comprised of six conditions is used to assess various delivery mediums of growth mindset oriented motivational messages. Student persistence and mastery speed are examined across multiple math domains, and self-response items are used to gauge student mindset, enjoyment, and perception of system helpfulness upon completion of the <b>assignment.</b> Findings, design <b>limitation,</b> and suggestions for future analysis are discussed...|$|R
50|$|However, {{deployment}} of these ships revealed the inherent problem {{resulted from the}} original design: the cargo ship design meant that {{the stability of the}} ship in harsh sea state in poor, and in severe bad weather, the ships would move violently and some medical operations such as surgeries could not be performed. Consequently, Nan-Yun 832 was converted back into cargo ship role and only one ship Nan-Kang (南康, meaning South Health in Chinese) is left in the service {{to play the role of}} medical provider. This is why the official Chinese classification of this version is ambulance transport, instead of hospital ship, because it cannot carry out some of the medical <b>assignment</b> due to <b>limitations</b> of weather. The experience gained from these ambulance transport, however, provided valuable lessons in the development of true hospital ship, Type 920 hospital ship.|$|R
40|$|Abstract — Though {{enormous}} improvement {{has occurred}} in the field of transmission media layer for which gigantic amount of data can be transferred but there is bottleneck problem at the upper layer protocol stack so that eventually the data transmission process becomes slower. Data may be transferred by optical fiber as a physical media which have large bandwidth but at the intermediary system the O-E-O process is accomplished for routing purpose which makes the system slower. At the intermediate nodes there are also buffering problem for this conversion. So beneath the IP layer a process called MPLS is proposed for removing the extra processing at the intermediary system. At the optical layer the point-to-point WDM technology is enhanced with light path known as wavelength routing resolves O-E-O processing and buffering problem. It also enhanced the node processing capability and protocol transparency. There is also a challenge of wavelength <b>assignment</b> for the <b>limitation</b> of wavelength. So, a wavelength routing algorithm is used for the purpose of wavelength assignment and route selection various important concerns still need to be addressed regarding IP/WDM integration. This include light path routing coupled with tighter inter networking with IP routing and resource management protocols, survivability provisioning, framing / monitoring solutions and others...|$|R
40|$|The current {{pilot study}} was an {{investigation}} into the effectiveness of the Magic Penny Early Literacy Program reading curriculum among kindergarten students. Magic Penny was introduced as a supplement to the existing curriculum in the intervention classroom for approximately three months. Reading achievement was assessed using a selection of tests from the Woodcock-Johnson III Tests of Cognitive Abilities and the Woodcock-Johnson III Tests of Achievement that comprise the Basic Reading Skills Cluster and the Phonemic Awareness III Clinical Cluster. Utilizing a between-group comparison and longitudinal study design, group differences between intervention (n = 19) and comparison (n = 19) classrooms on the early literacy outcome measures were examined using linear regression models. Results provided mixed support for the Magic Penny Early Literacy Program. Analyses indicated that intervention group membership was associated with greater improvement in children’s Basic Reading scores, when controlling for pre-test scores. In contrast, intervention group membership was not associated with greater improvement in children’s Phonemic Awareness scores. This study represents the first formal evaluation of the Magic Penny Early Literacy Program. However, this pilot study was limited in scope and lacked random <b>assignment.</b> Given its <b>limitations,</b> additional, larger-scale research is warranted to further examine the impact of this new program...|$|R
40|$|The present paper {{describes}} the preliminary {{stages of the}} development of a new, comprehensive model conceived to simulate the evacuation of transport airplanes in certification studies. Two previous steps were devoted to implementing an efficient procedure to define the whole geometry of the cabin, and setting up an algorithm for assigning seats to available exits. Now, to clarify the role of the cabin arrangement in the evacuation process, the paper addresses the influence of several restrictions on the seat-to-exit assignment algorithm, maintaining a purely geometrical approach for consistency. Four situations are considered: first, an <b>assignment</b> method without <b>limitations</b> to search the minimum for the total distance run by all passengers along their escaping paths; second, a protocol that restricts the number of evacuees through each exit according to updated FAR 25 capacity; third, a procedure which tends to the best proportional sharing among exits but obliges to each passenger to egress through the nearest fore or rear exits; and fourth, a scenario which includes both restrictions. The four assignment strategies are applied to turboprops, and narrow body and wide body jets. Seat to exit distance and number of evacuees per exit are the main output variables. The results show the influence of airplane size and the impact of non-symmetries and inappropriate matching between size and longitudinal location of exits...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. The applications for Unmanned Aerial Vehicles are numerous and cover a range of areas from military applications, scientific projects to commercial activities, {{but many of these}} applications require substantial human involvement. This work focuses on the problems and limitations in cooperative Unmanned Aircraft Systems to provide increasing realism for cooperative algorithms. The Consensus Based Bundle Algorithm is extended to remove single agent limits on the task allocation and consensus algorithm. Without this limitation the Consensus Based Grouping Algorithm is proposed that allows the allocation and consensus of multiple agents onto a single task. Solving these problems further increases the usability of cooperative Unmanned Aerial Vehicles groups and reduces the need for human involvement. Additional requirements are taken into consideration including equipment requirements of tasks and creating a specific order for task completion. The Consensus Based Grouping Algorithm provides a conflict free feasible solution to the multi-agent task assignment problem that provides a reasonable <b>assignment</b> without the <b>limitations</b> of previous algorithms. Further to this the new algorithm reduces the amount of communication required for consensus and provides a robust and dynamic data structure for a realistic application. Finally this thesis provides a biologically inspired improvement to the Consensus Based Grouping Algorithm that improves the algorithms performance and solves some of the difficulties it encountered with larger cooperative requirements...|$|R
40|$|To {{examine the}} {{possible}} health {{risks associated with}} occupational exposure to formaldehyde a proportional mortality analysis was conducted on deaths occurring between 1950 and 1976 among 136 {{men who had been}} employed a month or more in one of five formaldehyde-related areas of a large chemical producing plant located in Springfield, Massachusetts, USA. Overall, no statistically significant excesses or deficits in proportional mortality were observed among the formaldehyde-exposed group based on comparisons with both United States men and men from the local county area. In addition, no important differences in mortality were observed among this group when comparisons were made with 456 male decedents from the same plant who had not had a month or more of formaldehyde exposure. Within the calendar period examined, no deaths from sinonasal cancer were observed among the chemical workers studied nor was mention made on any death certificate of sinonasal cancer as a contributory cause of death. No important excesses, trends, or patterns in cancer mortality were observed among white male formadelhyde-exposed workers when consideration was given to age and time period of death, type and duration of formaldehyde exposure, and the lapse period from the onset of the first formaldehyde-related job <b>assignment.</b> Although certain <b>limitations</b> of this study do not allow definite conclusions to be drawn, the results indicate no trends or patterns in proportional mortality that could be directly linked to exposures to formaldehyde...|$|R
40|$|Metro 2 ̆ 7 s Research and Modeling Services Program is {{responsible}} for the development, maintenance, and application of travel demand models for application in long-range planning efforts in the Portland metropolitan region. Representation of traffic—both vehicular and transit—plays an integral role in the travel demand modeling process. Complex software is required to assign vehicles and transit users to transportation networks to determine viable options available to travelers, costs associated with those options, and sets of routes by which travelers might navigate their trips. Metro 2 ̆ 7 s current static assignment model has traditionally sufficed for use with Metro 2 ̆ 7 s four-step travel demand model. However, static <b>assignments</b> have well-documented <b>limitations</b> that preclude the ability of the analyst to answer complex policy questions, especially those related to greenhouse gas emissions, congestion, and transportation network reliability. In addition, static assignments cannot fulfill a need for small-duration travel time increments required by the next generation activity-based models. The shortcomings of the static assignment necessitates Metro’s development and application of regional dynamic traffic assignment (DTA) models. The resolution of these models allows for continuous modeling of traffic over an analysis period, which allows the analyst to capture temporally-based traffic events such as the building and dissipation of queues, measurement of the duration of congestion, and high fidelity speed profiles for use in emissions analysis. This presentation will focus on why Metro has developed a DTA, how DTA compares with other models—specifically macro-scale static assignments and micro-simulations—and how DTA has been applied in Metro 2 ̆ 7 s modeling process. [URL]...|$|R
40|$|The {{scope of}} this paper is eminently practical. It {{addresses}} the problem of estimating and updating observed O-D matrices based upon available link flow information by means of the non-linear programming approach corresponding to the Augmented Lagrangian Function (ALM). In this way, one can estimate O-D matrices in a way which uses an efficient algorithm that minimizes the amount of stored information required for solving large-sized problems. A solution algorithm is provided, which has been completely developed and designed to be used within commercial <b>assignment</b> codes. The <b>limitations</b> and the features of these programmes have, in a natural way, conditioned the solution adopted for the model proposed. The case of updating a trip matrix, from a home-based survey, of a real problem corresponding to an existing city is analysed. For this application case two methodologies are reviewed; the first one does not incorporate constraints related to the total number of trips, the zonal productions and attractions, or individual O-D pairs; the second one does take them into account. The results obtained from the application of the proposed method to a prior O-D trip matrix, show huge differences with respect to the results yielded when the adjustment process does not limit the variations of the single terms (O-D pairs) with upper or lower boundaries. This comparison is carried out and based upon the wide spread Spiess's Gradient Algorithm. The graphical results depict the drastic contrasts observed, related to the distortion experienced by the information contained in the O-D trip matrix of a home-based survey, when using this methodology and the one proposed here. ...|$|R
40|$|Despite the {{ubiquity of}} {{transportation}} data, methods to infer the state parameters of a network either ignore sensitivity of route decisions, require route enumeration for parameterizing descriptive models of route selection, or require complex bilevel models of route <b>assignment</b> behavior. These <b>limitations</b> prevent modelers from fully exploiting ubiquitous data in monitoring transportation networks. Inverse optimization methods that capture network route choice behavior can address this gap, but {{they are designed to}} take observations of the same model to learn the parameters of that model, which is statistically inefficient (e. g. requires estimating population route and link flows). New inverse optimization models and supporting algorithms are proposed to learn the parameters of heterogeneous travelers' route behavior to infer shared network state parameters (e. g. link capacity dual prices). The inferred values are consistent with observations of each agent's optimization behavior. We prove that the method can obtain unique dual prices for a network shared by these agents in polynomial time. Four experiments are conducted. The first one, conducted on a 4 -node network, verifies the methodology to obtain heterogeneous link cost parameters even when multinomial or mixed logit models would not be meaningfully estimated. The second is a parameter recovery test on the Nguyen-Dupuis network that shows that unique latent link capacity dual prices can be inferred using the proposed method. The third test on the same network demonstrates how a monitoring system in an online learning environment can be designed using this method. The last test demonstrates this learning on real data obtained from a freeway network in Queens, New York, using only real-time Google Maps queries...|$|R
40|$|There is {{a growing}} {{interest}} {{in the use of}} speculative multithreading to speed up the execution of a program. In speculative multithreading model, threads are extracted from a sequential program and are speculatively executed in parallel, without violating sequential program semantics. In order to get the best performance from this model, a highly accurate thread selection scheme is needed in order to accurately assign threads to processing elements (PEs) for parallel execution. This is done using a thread predictor that assigns threads to PEs sequentially. However, this in-order thread <b>assignment</b> has severe <b>limitations.</b> One of the limitations is when the thread predictor is unable to predict the successor of a particular thread. This may cause successor PEs to remain idle for many cycles. Another limitation has to do with control independence. When a misprediction occurs, all threads, starting from the misprediction point, get squashed, although many of them may be control independent of the misprediction. In this paper we present a hierarchical technique for building threads, as well as a non-sequential scheme of assigning them to PEs, and a selective approach to squash threads in case of misprediction, in order to take advantage of control independences. This technique uses dynamic resizing, and builds threads in two steps, statically using the compiler as well as dynamically at run-time. Based on the dynamic behavior of the program, a thread can dynamically expand or shrink in size, and can span several PEs. Detailed simulation results show that our dynamic resizing based approach results in a 11. 6 % average increase in speedup relative to a conventional speculative multithreaded processor. 1...|$|R
40|$|Activity of the Na+/glucose cotransporter endogenously {{expressed}} in LLC-PK 1 cells {{was measured using}} whole cell recording techniques under three different sodium concentration conditions: 1) externally saturating, zero trans; 2) 40 mM external, zero trans; and 3) externally saturating, 50 mM trans. Activity of the transporter with increasing concentrations of sugar was measured for each set of conditions, from which the maximal current for saturating sugar, Im, was determined. The Im measured shows substantial potential dependence for each set of conditions. The absolute Im and the relative potential dependence of Im compared among the various solute conditions were used to identify which loci in the transport cycle are responsible for potential-dependent changes in function. The experimental data were compared with the predicted Im values calculated from an eight-state, sequential, reversible model of a transport reaction kinetic scheme. Predictions derived from <b>assignment</b> of rate <b>limitation</b> and/or potential dependence {{to each of the}} 16 transitions in the transport pathway were derived and compared with the measured data. Most putative models were dismissed because of lack of agreement with the measured data, indicating that several steps along the transport pathway are not rate limiting and/or not potential dependent. Only two models were found that can completely account for the measured data. In one case, translocation of the free carrier must be rate limiting, and both extracellular sodium-binding events as well as translocation of both free and fully loaded carrier forms must be potential-dependent transitions. In the second case, translocation of the free carrier and dissociation of the first sodium to be released intracellularly must be equivalently rate limiting. In this case only the two translocation events are required to be potential dependent. The two external sodium-binding events might still be potential dependent, but this is not required to fit the data. Previous reports suggest that the first model is correct; however, no direct experimental data compel us to dismiss the second option as a feasible model...|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 133 p. The study applied an Information Integration Theory model to the study of moral development. The model aided distinction between valuation and reasoning processes in judgments of morality, and developmental trends were explored along the latter dimension. College students of high or low developmental levels of principled moral reasoning, as determined by Rest's Defining Issues Test (1974) : (a) judged morality of stimulus persons described by brief moral scenarios, or (b) decided whether or not they would behave as did the person described. All subjects read scenarios containing morally positive or negative descriptions of acts, intentions, and consequences. The moral value of each separate component description had been previously determined. In addition to making morality ratings or indicating the likelihood they would behave as the person described, subjects assigned weights according to perceived importance of the informational components, and indicated whether they wanted additional information regarding each component. Predictions considered the relative influence of intentions and of consequences on judgments as compared to decisions, and the form of the integration strategy used in combining the act, intention, and consequence information into a unitary response. Congruent with previous findings and prevalent developmental theory, it was predicted that high moral level subjects (HM) would be more influenced by intentions in forming their judgments, while low moral level subjects (LM) would give more weight to consequence information. Further, it was expected that greater maturity would result in more complex strategies in integrating information. Thus, HM's were expected to demonstrate complex weighting strategies while LM's were expected to use a simpler method [...] constant weighting. Finally, consideration of intention and consequence information was expected to be influenced by task constraints. In accord with the actor/observer distinction in Attribution Theory, it was expected that subjects would be more influenced by consequence information in a decision task, since as actors they might appeal more to situational cues to explain their behavior. In a judgment task, however, intentions were expected to be more important since observers seem to rely more on internal cues to interpret behavior. Data analysis revealed that HM subjects were only slightly more affected by intentions than LM subjects in the judgment task condition. This conclusion was regarded as tentative though, since results only approached significance. No interaction occurred between consequence information and subject reasoning level. Both HM and LM subjects used complex weighting strategies; observed component interactions implied differential weighting by subjects at both levels. Separation of groups according to age, and other cognitive skills in addition to moral reasoning level may be necessary to demonstrate developmental differences in integration strategies. Though, as expected, the importance of intentions was greater for a judgment than a decision task, consequences were not more important in the latter situation. Finally, there were apparent contradictions between morality ratings and subjective reports of weight <b>assignment,</b> implying serious <b>limitations</b> in the response measures often used in traditional moral judgment research. It is questionable whether subjective accounts of input importance reflected actual use of such inputs in the judgment itself. In addition to clearly separating valuation and reasoning processes, the IIT model allowed more complete investigation of the latter by revealing integration strategies, and permitted more thorough assessment of component influence [...] both major concerns in developmental theory. Moreover, by covarying the act itself, previously unnoted mediating effects of actions upon effects of intentions and consequences were found...|$|R

