20|20|Public
50|$|An {{alphanumeric}} grid (also {{known as}} <b>atlas</b> <b>grid)</b> {{is a simple}} coordinate system on a grid in which each cell is identified {{by a combination of}} a letter and a number.|$|E
40|$|The ATLAS Experiment {{benefits}} from computing resources distributed worldwide {{at more than}} 100 WLCG sites. The <b>ATLAS</b> <b>Grid</b> sites provide over 100 k CPU job slots, over 100 PB of storage space on disk or tape. Monitoring of status of such a complex infrastructure is essential. The <b>ATLAS</b> <b>Grid</b> infrastructure is monitored 24 / 7 by two teams of shifters distributed world-wide, by the ATLAS Distributed Computing experts, and by site administrators. In this paper we summarize automation efforts performed within the ATLAS Distributed Computing team {{in order to reduce}} manpower costs and improve the reliability of the system. Different aspects of the automation process are described: from the <b>ATLAS</b> <b>Grid</b> site topology provided by the <b>ATLAS</b> <b>Grid</b> Information System, via automatic site testing by the HammerCloud, to automatic exclusion from production or analysis activities...|$|E
40|$|The {{variety of}} the ATLAS Computing Infrastructure {{requires}} a central information system to define the topology of computing resources and to store the different parameters and configuration data which are needed by the various ATLAS software components. The <b>ATLAS</b> <b>Grid</b> Information System is the system designed to integrate configuration and status information about resources, services and topology of the computing infrastructure used by ATLAS Distributed Computing applications and services...|$|E
40|$|Effective {{distributed}} user analysis {{requires a}} system which meets the demands of running arbitrary user applications on sites with varied configurations and availabilities. The challenge of tracking such a system requires a tool to monitor not only the functional statuses of each grid site, but also to perform large-scale analysis challenges on the <b>ATLAS</b> <b>grids.</b> This work presents one such tool, the ATLAS GangaRobot, {{and the results of}} its use in tests and challenges. For functional testing, the GangaRobot performs daily tests of all sites; specifically, a set of exemplary applications are submitted to all sites and then monitored for success and failure conditions. These results are fed back into Ganga to improve job placements by avoiding currently problematic sites. For analysis challenges, a cloud is first prepared by replicating a number of desired DQ 2 datasets across all the sites. Next, the GangaRobot is used to submit and manage a large number of jobs targeting these datasets. The high-loads resulting from multiple parallel instances of the GangaRobot exposes shortcomings in storage and network configurations. The results from a series of cloud-by-cloud analysis challenges starting in fall 2008 are presente...|$|R
40|$|The {{huge amount}} of {{resources}} available in the Grids, and the necessity {{to have the most}} updated experiment software deployed in all the sites within a few hours, have spotted the need for automatic installation systems for the LHC experiments. In this paper we describe the ATLAS system for the experiment software installation in LCG/EGEE, based on the Lightweight Job Submission Framework for Installation (LJSFi). This system is able to automatically discover, check, install, test and tag the full set of resources made available in LCG/EGEE to the ATLAS Virtual Organization in a few hours, depending on the site availability. The installations or removals may be centrally triggered as well as requested by the end-users for each site. A fallback solution to the manual operations is also available, in case of problems. The installation data, status and job history are centrally kept in the installation DB and browseable via a web interface. The installation tasks are performed by one or more automatic agents. The ATLAS installation team is automatically notified in case of problems, in order to proceed with the manual operations. Each user may browse or request an installation activity in a site, directly by accessing the web pages, being identified by his personal certificate. This system has been successfully used by ATLAS since 2003 to deploy about 60 different software releases and has performed more than 75000 installation jobs so far. The LJSFi framework is currently being extended to the other <b>ATLAS</b> <b>Grids</b> (NorduGrid and OSG) ...|$|R
40|$|With the {{exponential}} growth of LHC (Large Hadron Collider) {{data in the}} years 2010 - 2012, distributed computing has become the established way to analyze collider data. The <b>ATLAS</b> experiment <b>Grid</b> infrastructure includes more than 130 sites worldwide, ranging from large national computing centres to smaller university clusters. So far the storage technologies and access protocols to the clusters that host this tremendous amount of data vary from site to site. HTTP/WebDAV offers the possibility to use a unified industry standard to access the storage. We present the deployment and testing of HTTP/WebDAV for local and remote data access in the ATLAS experiment for the new data management system Rucio and the PanDA workload management system. Deployment and large scale tests have been performed using the Grid testing system HammerCloud and the ROOT HTTP plugin Davix...|$|R
40|$|The PanDA Production and Distributed Analysis System is the ATLAS {{workload}} {{management system}} for processing user analysis, group analysis and production jobs. In 2011 more than 1400 users have submitted jobs through PanDA to the <b>ATLAS</b> <b>grid</b> infrastructure. The system processes more than 2 million analysis jobs per week. Analysis jobs are routed to sites {{based on the}} availability of relevant data and processing resources, taking account of the nonuniform distribution of CPU and storage resources in the <b>ATLAS</b> <b>grid.</b> The data distribution has to be optimized to fit the resource distribution, and also has to be dynamically changed to meet rapidly evolving requirements for analysis use cases. The PanDA Dynamic Data Placement (PD 2 P) system has been developed to cope with difficulties of data placement for ATLAS. PD 2 P is an intelligent subsystem of PanDA to distribute data by taking the following factors into account: popularity, locality, the usage pattern of the data, the distribution of CPU and storage resources, network topology between sites, site operation downtime and reliability, and so on. We will describe the design of the new system, its performance during the past year of data taking, dramatic improvements it has brought about in the efficient use of storage and processing resources, associated reductions in average wait time for user analysis jobs, and plans for the future...|$|E
40|$|In {{the past}} two years the ATLAS Collaboration at the LHC has {{collected}} a large volume of data and published a number of ground breaking papers. The Grid-based ATLAS distributed computing infrastructure {{played a crucial role in}} enabling timely analysis of the data. We will present a study of the performance and usage of the <b>ATLAS</b> <b>Grid</b> as platform for physics analysis in 2011. This includes studies of general properties as well as timing properties of user jobs (wait time, run time, etc). These studies are based on mining of data archived by the PanDA workload management system...|$|E
40|$|This paper {{presents}} initial {{results of}} investigating and developing grid-enabled tools for collaborative image processing, visualisation, and database access {{within the context}} of the mouse atlas project at the MRC Human Genetics Unit. OGSI compliant grid services for compute intensive task in the mouse atlas have been investigated and a grid service has been developed to implement a grid-enabled deconvolution tool. We have developed EMAP portal to deliver Mouse <b>Atlas</b> <b>grid</b> functionality and thereby minimise technical details for the end users. JSR- 168 compliant portlets have been implemented to access our grid-enabled tools, viewing and comparing experimental results using a visualisation grid service, and also collaboration with mouse atlas users. 1...|$|E
40|$|Grids include {{heterogeneous}} resources, {{which are}} based on different hardware and software architectures or components. In correspondence with this diversity of the infrastructure, the execution time of any single job, as well as the total grid performance can both be affected substantially, which can be demonstrated by measurements. Running a simple benchmarking suite can show this heterogeneity and give us results about the differences over the grid sites. Comment: 29 pages, 35 figures, including charts and results of benchmarking over the <b>grid,</b> <b>ATLAS</b> V. ...|$|R
30|$|After {{loading the}} finder grid from the cryo shuttle into a cryo {{transmission}} electron microscope, for example, a Cs-corrected Titan Krios in this work, {{by using the}} Cryo Transfer Workstation, a montaged EM <b>atlas</b> of a <b>grid</b> is acquired at a nominal magnification in low magnification range (40 × for Gatan Orius SC 200 CCD camera, 100 × for Falcon camera, 175 × for Gatan GIF K 2 camera) as exampled in Fig.  5 B. The acquisition usually starts from the stage center and stage movement is used.|$|R
40|$|AbstractDuring {{the current}} LHC {{shutdown}} period the ATLAS experiment will upgrade the Trigger and Data Acquisition system {{to include a}} hardware tracker coprocessor: the Fast TracKer (FTK). The FTK receives data from the 80 million of channels of the ATLAS silicon detector, identifying charged tracks and reconstructing their parameters {{at a rate of}} up to 100 KHz and within 100 microseconds. To achieve this performance, the FTK system identifies candidate tracks utilizing the computing power of a custom ASIC chip with associative memory (AM) designed to perform “pattern matching” at very high speed; track parameters are then calculated using modern FPGAs. A detailed simulation of this massive system has been developed with the goal of supporting the hardware design and studying its impact in the ATLAS online event selection at high LHC luminosities. We present the issues related to emulating FTK on a general-purpose CPU platform, using <b>ATLAS</b> computing <b>Grid</b> resources, and the solutions developed in order to mitigate these problems and allow the studies required to support the system design, construction and installation...|$|R
40|$|After {{the early}} success in {{discovering}} a new particle {{consistent with the}} long awaited Higgs boson, Large Hadron Collider experiments {{are ready for the}} precision measurements and further discoveries that will be made possible by much higher LHC collision rates from spring 2015. A proper understanding of the detectors performance at high occupancy conditions is important for many on-going physics analyses. The ATLAS Transition Radiation Tracker (TRT) is one of these detectors. TRT is a large straw tube tracking system that is the outermost of the three subsystems of the ATLAS Inner Detector (ID). TRT contributes significantly to the resolution for high-pT tracks in the ID providing excellent particle identification capabilities and electron-pion separation. ATLAS experiment is using Worldwide LHC Computing Grid. WLCG is a global collaboration of computer centers and provides seamless access to computing resources which include data storage capacity, processing power, sensors, visualisation tools and more. WLCG resources are fully utilised and it is important to integrate opportunistic computing resources such as supercomputers, commercial and academic clouds no to curtail the range and precision of physics studies. One of the most important study dedicated to be solved on a supercomputer is reconstruction of proton-proton events with large number of interactions in Transition Radiation Tracker. This studies are made for ATLAS TRT SW group. It becomes clear that high-performance computing contributions become important and valuable. An example of very successful approach is Kurchatov Institute’s Data Processing Center including Tier- 1 grid site and supercomputer. TRT jobs have been submitted using the same PanDA portal and it was transparent for physicists. Results have been transferred to the <b>ATLAS</b> <b>Grid</b> site. The presented talk includes TRT performance results obtained with the usage of the <b>ATLAS</b> <b>GRID</b> and "Kurchatov" supercomputer as well as analysis of CPU efficiency during these studies...|$|E
40|$|With the {{exponential}} growth of LHC (Large Hadron Collider) data in 2012, distributed computing {{has become the}} established way to analyze collider data. The <b>ATLAS</b> <b>grid</b> infrastructure includes more than 130 sites worldwide, ranging from large national computing centers to smaller university clusters. HammerCloud was previously introduced {{with the goals of}} enabling VO- and site-administrators to run validation tests of the site and software infrastructure in an automated or on-demand manner. The HammerCloud infrastructure has been constantly improved to support the addition of new test work-flows. These new work-flows comprise e. g. tests of the ATLAS nightly build system, ATLAS MC production system, XRootD federation FAX and new site stress test work-flows. We report on the development, optimization and results of the various components in the HammerCloud framework...|$|E
40|$|Automated {{distributed}} analysis {{tests are}} necessary to ensure smooth operations of the <b>ATLAS</b> <b>grid</b> resources. In this work we present the recent developments of the ATLAS GangaRobot, a tool designed to perform regular tests of all grid sites by running arbitrary user applications with varied configurations at predifined time intervals. Specifically the GangaRobot creates and submits several real ATLAS user applications to the various grid sites using the distributed analysis framework GANGA, a front end for easy grid job definition and management. Success or failure rates of these test jobs are individually monitored. Test definitions and results are stored in a database and made available to users and site administrators through a web interface, the ATLAS Site Status Board (SSB) and the Service Availability Monitor (SAM). The test results provide {{on the one hand}} a fast way to to identify systematic or temporary site problems, {{and on the other hand}} allow for an effective distribution of the workload on the available resources...|$|E
40|$|Every day {{hundreds}} of tests are {{run on the}} Worldwide LHC Computing <b>Grid</b> for the <b>ATLAS,</b> and CMS experiments in order to evaluate the performance and reliability of the different computing sites. All this activity is steered, controlled, and monitored by the HammerCloud testing infrastructure. Sites with failing functionality tests are auto-excluded from the <b>ATLAS</b> computing <b>grid,</b> therefore {{it is essential to}} provide a detailed and well organized web interface for the local site administrators such that they can easily spot and promptly solve site issues. Additional functionality has been developed to extract and visualize the most relevant information. The site administrators can now be pointed easily to major site issues which lead to site blacklisting as well as possible minor issues that are usually not conspicuous enough to warrant the blacklisting of a specific site, but can still cause undesired effects such as a non-negligible job failure rate. This paper summarizes the different developments and optimizations of the HammerCloud web interface and gives an overview of typical use cases...|$|R
40|$|We {{present a}} new grid of model photospheres for the SDSS-III/APOGEE survey of stellar {{populations of the}} Galaxy, {{calculated}} using the ATLAS 9 and MARCS codes. New opacity distribution functions were generated to calculate ATLAS 9 model photospheres. MARCS models were calculated based on opacity sampling techniques. The metallicity ([M/H]) spans from - 5 to 1. 5 for ATLAS and - 2. 5 to 0. 5 for MARCS models. There are three main differences with respect to previous ATLAS 9 model grids: a new corrected H 2 O linelist, {{a wide range of}} carbon ([C/M]) and alpha element [alpha/M] variations, and solar reference abundances from Asplund et al. 2005. The added range of varying carbon and alpha element abundances also extends the previously calculated MARCS model grids. Altogether 1980 chemical compositions were used for the <b>ATLAS</b> 9 <b>grid,</b> and 175 for the MARCS grid. Over 808 thousand ATLAS 9 models were computed spanning temperatures from 3500 K to 30000 K and log g from 0 to 5, where larger temperatures only have high gravities. The MARCS models span from 3500 K to 5500 K, and log g from 0 to 5. All model atmospheres are publically available online. Comment: 8 pages, 6 figures, 5 tables, accepted for publication in The Astronomical Journa...|$|R
40|$|ATLAS is {{the largest}} {{particle}} detector under construction at CERN Geneva. Frequency scanning interferometry (FSI), also known as absolute distance interferometry, {{will be used to}} monitor shape changes of the SCT (semiconductor tracker), a particle tracker in the inaccessible, high radiation environment at the centre of <b>ATLAS.</b> Geodetic <b>grids</b> with several hundred fibre-coupled interferometers (30 mm to 1. 5 m long) will be measured simultaneously. These lengths will be measured by tuning two lasers and comparing the resulting phase shifts in grid line interferometers, (GLIs) with phase shifts in a reference interferometer. The novel inexpensive GLI design uses diverging beams to reduce sensitivity to misalignment, albeit with weaker signals. One micrometre precision length measurements of grid lines will allow 10 μm precision tracker shape corrections to be fed into ATLAS particle tracking analysis. The technique was demonstrated by measuring a 400 mm interferometer to better than 400 nm and a 1195 mm interferometer to better than 250 nm. Precise measurements were possible, even with poor quality signals, using numerical analysis of thousands of intensity samples. Errors due to drifts in interferometer length were substantially reduced using two lasers tuned in opposite directions and the precision was further improved by linking measurements made at widely separated laser frequencies. © 2004 IOP Publishing Ltd...|$|R
40|$|The ATLAS {{experiment}} is a High Energy Physics experiment that utilizes {{the services of}} Grid 3 now migrating to the Open Science Grid (OSG). This thesis provides monitoring and analysis of performance and statistical data from individual distributed clusters that combine to form the <b>ATLAS</b> <b>Grid</b> and will ultimately {{be used to make}} scheduling decisions on this Grid. The system developed in this thesis uses a layered architecture such that predicted future developments or changes brought to the existing Grid infrastructure can easily utilize this work with minimum or no changes. The starting point of the system is based on the existing scheduling that is being done manually for ATLAS job flow. We have provided additional functionality based on the requirements of the High Energy Physics ATLAS team of physicists at UTA. The system developed in this thesis has successfully monitored and analyzed distributed cluster performance at three sites and is waiting for access to monitor data from three more sites. (Abstract shortened by UMI. ...|$|E
40|$|Fifteen Chinese High-Performance Computing sites, {{many of them}} on the TOP 500 list of most {{powerful}} supercomputers, are integrated into a common infrastructure providing coherent access to a user through an interface based on a RESTful interface called SCEAPI. These resources have been integrated into the <b>ATLAS</b> <b>Grid</b> production system using a bridge between ATLAS and SCEAPI which translates the authorization and job submission protocols between the two environments. The ARC Computing Element (ARC-CE) forms the bridge using an extended batch system interface to allow job submission to SCEAPI. The ARC-CE was setup at the Institute for High Energy Physics, Beijing, in order to be {{as close as possible to}} the SCEAPI front-end interface at the Computing Network Information Center, also in Beijing. This paper describes the technical details of the integration between ARC-CE and SCEAPI and presents results so far with two supercomputer centers, Tianhe-IA and ERA. These two centers have been the pilots for ATLAS Monte Carlo Simulation in SCEAPI and have been providing CPU power since fall 2015...|$|E
40|$|Automated {{distributed}} analysis {{tests are}} necessary to ensure smooth operations of the <b>ATLAS</b> <b>grid</b> resources. The HammerCloud framework allows for easy definition, submission and monitoring of grid test applications. Both functional and stress test applications can be defined in HammerCloud. Stress tests are large-scale tests meant to verify the behaviour of sites under heavy load. Functional tests are light user applications running at each site with high frequency, {{to ensure that the}} site functionalities are available at all times. Success or failure rates of these tests jobs are individually monitored. Test definitions and results are stored in a database and made available to users and site administrators through a web interface. In this work we present the recent developments of the GangaRobot framework. GangaRobot monitors the outcome of functional tests, creates a blacklist of sites failing the tests, and exports the results tothe ATLAS Site Status Board (SSB) and to the Service Availability Monitor (SAM), providing on the one hand a fast way to identify systematic or temporary site failures, {{and on the other hand}} allowing for an effective distribution of the work load on the available resources...|$|E
40|$|The {{computing}} {{model of}} the ATLAS experiment was designed around the concept of grid computing and, {{since the start of}} data taking, this model has proven very successful. However, new cloud computing technologies bring attractive features to improve the operations and elasticity of scientific distributed computing. <b>ATLAS</b> sees <b>grid</b> and cloud computing as complementary technologies that will coexist at different levels of resource abstraction, and two years ago created an R&D working group to investigate the different integration scenarios. The ATLAS Cloud Computing R&D has been able to demonstrate the feasibility of offloading work from grid to cloud sites and, as of today, is able to integrate transparently various cloud resources into the PanDA workload management system. The ATLAS Cloud Computing R&D is operating various PanDA queues on private and public resources and has provided several hundred thousand CPU days to the experiment. As a result, the ATLAS Cloud Computing R&D group has gained a significant insight into the cloud computing landscape and has identified points that still need to be addressed in order to fully utilize this technology. contribution will explain the cloud integration models that are being evaluated and will discuss ATLAS’ learning during the collaboration with leading commercial and academic cloud providers...|$|R
40|$|Programmes of {{monitoring}} of bird occurrence and abundance {{as a source}} of ecological data Jindra Mourková Abstact In the thesis, regional data on bird communities were studied in terms of species richness and occurrence of bird species in dependence on habitat and, in case of wintering birds, also on climatic conditions. The regional <b>grid</b> <b>atlas</b> of Prague (Fuchs et al. 2002), records from Wintering waterbird census in Central Bohemia, own data on the breeding population of the Mute Swan in the Trebon region and data from individual marking of Mute Swans provided by the Bird Ringing Centre of the National Museum were used as the sources of data on the occurrence and abundance of bird species. The thesis also includes a part of the text of the prepared Atlas of breeding distribution of birds in the Trebon region...|$|R
40|$|We {{present a}} summary of biomass data for 11 Plankton Functional Types (PFTs) plus {{phytoplankton}} pigment data, compiled {{as part of the}} MARine Ecosystem biomass DATa (MAREDAT) initiative. The goal of the MAREDAT initiative is to provide global gridded data products with coverage of all biological components of the global ocean ecosystem. This special issue is the first step towards achieving this. The PFTs presented here include picophytoplankton, diazotrophs, coccolithophores, Phaeocystis, diatoms, picoheterotrophs, microzooplankton, foraminifers, mesozooplankton, pteropods and macrozooplankton. All variables have been gridded onto a World Ocean <b>Atlas</b> (WOA) <b>grid</b> (1 ° × 1 ° × 33 vertical levels × monthly climatologies). The data show that (1) the global total heterotrophic biomass (2. 0 – 6. 4 Pg C) is at least as high as the total autotrophic biomass (0. 5 – 2. 6 Pg C excluding nanophytoplankton and autotrophic dinoflagellates), (2) the biomass of zooplankton calcifiers (0. 9 – 2. 3 Pg C) is substantially higher than that of coccolithophores (0. 01 – 0. 14 Pg C), (3) patchiness of biomass distribution increases with organism size, and (4) although zooplankton biomass measurements below 200 m are rare, the limited measurements available suggest that Bacteria and Archaea are not the only heterotrophs in the deep sea. More data will be needed to characterize ocean ecosystem functioning and associated biogeochemistry in the Southern Hemisphere and below 200 m. Microzooplankton database: <a href="[URL]...|$|R
40|$|The ATLAS {{computing}} {{model was}} originally designed as static clouds (usually national or geographical groupings of sites) around the Tier 1 centres, which confined tasks {{and most of}} the data traffic. Since those early days, the sites' network bandwidth has increased at O(1000) and the difference in functionalities between Tier 1 s and Tier 2 s has reduced. After years of manual, intermediate solutions, we have now ramped up to full usage of World-cloud, the latest step in the PanDA Workload Management System to increase resource utilization on the <b>ATLAS</b> <b>Grid,</b> for all workflows (MC production, data (re) processing, etc.). We have based the development on two new site concepts. Nuclei sites are the Tier 1 s and large Tier 2 s, where tasks will be assigned and the output aggregated, and satellites are the sites that will execute the jobs and send the output to their nucleus. PanDA dynamically pairs nuclei and satellite sites for each task based on the input data availability, capability matching, site load and network connectivity. This contribution will introduce the conceptual changes for World-cloud, the development necessary in PanDA, an insight into the network model and the first half-year of operational experience...|$|E
40|$|AbstractWe {{aimed to}} develop a new method to convert T 1 -weighted brain MRIs to feature vectors, which could be used for {{content-based}} image retrieval (CBIR). To overcome the wide range of anatomical variability in clinical cases and the inconsistency of imaging protocols, we introduced the Gross feature recognition of Anatomical Images based on <b>Atlas</b> <b>grid</b> (GAIA), in which the local intensity alteration, caused by pathological (e. g., ischemia) or physiological (development and aging) intensity changes, as well as by atlas–image misregistration, is used to capture the anatomical features of target images. As a proof-of-concept, the GAIA was applied for pattern recognition of the neuroanatomical features of multiple stages of Alzheimer's disease, Huntington's disease, spinocerebellar ataxia type 6, and four subtypes of primary progressive aphasia. For each of these diseases, feature vectors based on a training dataset were applied to a test dataset to evaluate the accuracy of pattern recognition. The feature vectors extracted from the training dataset agreed well with the known pathological hallmarks of the selected neurodegenerative diseases. Overall, discriminant scores of the test images accurately categorized these test images to the correct disease categories. Images without typical disease-related anatomical features were misclassified. The proposed method is a promising method for image feature extraction based on disease-related anatomical features, which should enable users to submit a patient image and search past clinical cases with similar anatomical phenotypes...|$|E
40|$|Capacity {{factors in}} Finland for four wind turbine models, Vestas V 90 - 3. 0 MW, V 90 - 2. 0 MW, V 110 - 2. 0 MW and V 136 - 3. 45 MW at four turbine hub heights 100, 125, 150 and 200 m. Wind speed data are from Finnish Wind Atlas [1, 2], {{from which the}} Weibull {{distribution}} shape and scale parameters (labeled ‘Weibull all data k’ and ‘Weibull all data A’, respectively) and the frequencies of the wind sectors (‘Frequency all data’) were used. The calculation of capacity factor cf at wind <b>atlas</b> <b>grid</b> point k is described by the formula c_f, k = E_i, s g(w_i) ≈∑_s= 1 ^ 12 f_k,s∑_i= 1 ^N p_k,s(w_i) g(w_i) Δ w, where g(w) is the power curve function for current wind turbine model, wi the mean wind speed of bin i, fk,s the frequency of occurrence of wind direction s at point k, N the number of wind speed bins, pk,s(w) the Weibull probability density function for sector s at point k at the hub height and Δw {{the width of the}} wind speed bin. References 	Finnish Meteorological Institute, “Finnish Wind Atlas,” 2008. [Online]. Available: [URL] [Accessed: 28 -Jun- 2016] 	B. Tammelin, T. Vihma, E. Atlaskin, J. Badger, C. Fortelius, H. Gregow, M. Horttanainen, R. Hyvönen, J. Kilpinen, J. Latikka, K. Ljungberg, N. G. Mortensen, S. Niemelä, K. Ruosteenoja, K. Salonen, I. Suomi, and A. Venäläinen, “Production of the Finnish Wind Atlas,” Wind Energy, vol. 16, no. 1, pp. 19 – 35, Jan. 2013...|$|E
40|$|Climate {{archives}} are time series. They {{are used}} to assess temporal trends of a climate-dependent target variable, and to make climate <b>atlases.</b> A high-resolution <b>gridded</b> dataset with 1728 layers of monthly mean maximum, mean and mean minimum temperatures and precipitation for the NW Maghreb (28 °N– 37. 3 °N, 12 °W– 12 °E, 1 -km resolution) from 1973 through 2008 is presented. The surfaces were spatially interpolated by ANUSPLIN, a thin-plate smoothing spline technique approved by the World Meteorological Organization (WMO), from georeferenced climate records drawn from the Global Surface Summary of the Day (GSOD) and the Global Historical Climatology Network-Monthly (GHCN-Monthly version 3) products. Absolute errors for surface temperatures are approximately 0. 5 °C for mean and mean minimum temperatures, and peak up to 1. 76 °C for mean maximum temperatures in summer months. For precipitation, the mean absolute error ranged from 1. 2 to 2. 5 mm, but very low summer precipitation caused relative errors of up to 40...|$|R
40|$|We review {{here the}} {{progress}} of the Variability of the American MOnsoon Systems (VAMOS) extremes working group since it was formed in February of 2010. The goals of the working group are to 1) develop an atlas of warm-season extremes over the Americas, 2) evaluate existing and planned simulations, and 3) suggest new model runs to address mechanisms and predictability of extremes. Substantial {{progress has been made in}} the development of an extremes <b>atlas</b> based on <b>gridded</b> observations and several reanalysis products including Modern Era Retrospective-Analysis for Research and Applications (MERRA) and Climate Forecast System Reanalysis (CFSR). The status of the atlas, remaining issues and plans for its expansion to include model data will be discussed. This includes the possibility of adding a companion atlas based on station observations based on the software developed under the World Climate Research Programme (WCRP) Expert Team on Climate Change. Detection and Indices (ETCCDI) activity. We will also review progress on relevant research and plans for the use and validation of the atlas results...|$|R
40|$|International audienceWe {{present a}} summary of biomass data for 11 {{plankton}} functional types (PFTs) plus phytoplankton pigment data, compiled {{as part of the}} MARine Ecosystem biomass DATa (MAREDAT) initiative. The goal of the MAREDAT initiative is to provide, in due course, global gridded data products with coverage of all planktic components of the global ocean ecosystem. This special issue is the first step towards achieving this. The PFTs presented here include picophytoplankton, diazotrophs, coccolithophores, Phaeocystis, diatoms, picoheterotrophs, microzooplankton, foraminifers, mesozooplankton, pteropods and macrozooplankton. All variables have been gridded onto a World Ocean <b>Atlas</b> (WOA) <b>grid</b> (1 × 1 × 33 vertical levels × monthly climatologies). The results show that abundance is much better constrained than their carbon content/elemental composition, and coastal seas and other high productivity regions have much better coverage than the much larger volumes where biomass is relatively low. The data show that (1) the global total heterotrophic biomass (2. 0 - 4. 6 Pg C) is at least as high as the total autotrophic biomass (0. 5 - 2. 4 Pg C excluding nanophytoplankton and autotrophic dinoflagellates); (2) the biomass of zooplankton calcifiers (0. 03 - 0. 67 Pg C) is substantially higher than that of coccolithophores (0. 001 - 0. 03 Pg C); (3) patchiness of biomass distribution increases with organism size; and (4) although zooplankton biomass measurements below 200 m are rare, the limited measurements available suggest that Bacteria and Archaea are not the only important heterotrophs in the deep sea. More data will be needed to characterise ocean ecosystem functioning and associated biogeochemistry in the Southern Hemisphere and below 200 m. Future efforts to understand marine ecosystem composition and functioning will be helped both by further archiving of historical data and future sampling at new locations...|$|R
40|$|The ATLAS Distributed Computing project (ADC) was {{established}} in 2007 to develop and operate a framework, following the ATLAS computing model, to enable data storage, processing and bookkeeping {{on top of the}} WLCG distributed infrastructure. ADC development has always been driven by operations and this contributed to its success. The system has fulfilled the demanding requirements of ATLAS, daily consolidating worldwide up to 1 PB of data and running more than 1. 5 million payloads distributed globally, supporting almost one thousand concurrent distributed analysis users. Comprehensive automation and monitoring minimized the operational manpower required. The flexibility of the system to adjust to operational needs has been important {{to the success of the}} ATLAS physics program. The LHC shutdown in 2013 - 2015 affords an opportunity to improve the system in light of operational experience and scale it to cope with the demanding requirements of 2015 and beyond, most notably a much higher trigger rate and event pileup. We will describe the evolution of the ADC software foreseen during this period. This includes consolidating the existing Production and Distributed Analysis framework (PanDA) and <b>ATLAS</b> <b>Grid</b> Information System (AGIS), together with the development and commissioning of next generation systems for distributed data management (DDM/Rucio) and production (PRODSYS 2). We will explain how new technologies such as Cloud Computing and NoSQL databases, which ATLAS investigated as R&D projects in past years, will be integrated in production. Finally, we will describe more fundamental developments such as breaking job-to-data locality by exploiting storage federations and caches, and event level (rather than file or dataset level) workload engines...|$|E
40|$|Summary : In the LHC {{operations}} era {{analyzing the}} large data by the distributed physicists becomes a challenging task. The Computing Model of the ATLAS experiment at the LHC at CERN was designed around {{the concepts of}} grid computing. Large data volumes from the detectors and simulations require {{a large number of}} CPUs and storage space for data processing. To cope with these challenges a global network known as the Worlwide LHC Computing Grid (WLCG) was built. This is the most sophisticated data taking and analysis system ever built. Since the start of data-taking, the ATLAS Distributed Analysis (ADA) service has been running stably with the huge amount of data. The reliability of the ADA service is high but steadily improving; grid sites are continually validated against a set of standard tests, and a dedicated team of expert shifters provides user support and communicates user problems to the sites. The <b>ATLAS</b> <b>Grid</b> Computing Model is reviewed in this talk. Emphasis is given to ADA system. Description : The central goal here is to analyze the results of the collisions of high energy particles as a way of probing the fundamental forces of nature. The challenge in a LHC data analysis is the need to explore large data volumes from the detectors and simulation, which require a large number of CPUs for processing. In addition to the complex experiment software structure and a high connectivity to the data that needs to be provided by the large-scale physics analysis activities. Analysis jobs are routed to sites based on the availability of relevant data and processing resources. The data distribution is optimized to fit the resource distribution, and it is dynamically changed to meet rapidly the evolving requirements for analysis use cases. Distributed analysis tools used to analyze the data are reliable and fast to work with. Both the user support techniques and the direct feedback of users are the goal of improving the success rate and user experience when utilizing the distributed computing environment. The service is actively used: more than 1600 users have submitted jobs in the year 2012 and more than 2 million analysis jobs per week. PanDA is the ATLAS workload management system for processing user analysis, group analysis and production jobs. The reliability of the ADA service is high but steadily improving; grid sites are continually validated against a set of standard tests, and a dedicated team of expert shifters provides user support and communicates user problems to the sites. The <b>ATLAS</b> <b>Grid</b> Computing Model is reviewed in this talk as to how the ATLAS data is stored, distributed and processed. An emphasis is given on the distributed data analysis services, summarize this year of distributed analysis activity, and present the perspectives for future improvements to the system. Impact : The WLCG approach has the benefit of distributing responsibility {{in such a way that}} CERN’s role is to generate the raw data along with the additional calibration needed to interpret it while the broad international community accesses and analyzes this data through its own hierarchical network. The main point is that the data analysis, storage and deployment are driven by the requirements of the experimenters and theoretical analysts. Big achievements make the collection, handling and basic analysis of large data feasible. The LHC data intensive science activities have proved that the data volumes are large but they are not unmanageable. This is a very valuable experience for the scientific community that faces the challenge of storing, analysing and accessing large data sets, as well as the need to archive data in a robust and enduring way...|$|E
40|$|Efficient {{distribution}} of physics data over <b>ATLAS</b> <b>grid</b> sites {{is one of}} the most important tasks for user data processing. ATLAS' initial static data distribution model over-replicated some unpopular data and under-replicated popular data, creating heavy disk space loads while underutilizing some processing resources due to low data availability. Thus, a new data distribution mechanism was implemented, PD 2 P (PanDA Dynamic Data Placement) within the production and distributed analysis system PanDA that dynamically reacts to user data needs, basing dataset distribution principally on user demand. Data deletion is also demand driven, reducing replica counts for unpopular data. This dynamic model has led to substantial improvements in efficient utilization of storage and processing resources. Based on this experience, in this work we seek to further improve data placement policy by investigating in detail how data popularity is calculated. For this it is necessary to precisely define what data popularity means, what types of data popularity exist, how it can be measured, and most importantly, how the history of the data can help to predict the popularity of derived data. We introduce locality of the popularity: a dataset may be only of local interest to a subset of clouds/sites or may have a wide (global) interest. We also extend the idea of the “data temperature scale” model and a popularity measure. Using the ATLAS data replication history, we devise data distribution algorithms based on popularity measures and past history. Based on this work we will describe how to explicitly identify why and how datasets become popular and how such information can be used to predict future popularity...|$|E
40|$|Defining a {{bird species}} as a ‘forest’ one is often troublesome, {{owing to the}} lack of overall {{ecological}} knowledge and to differences among regions. In spite of this, in a lot of studies the set of ‘forest’ species is empirically chosen. The authors present an objective measure of the relationship existing, at the Italian national scale, between bird species and woods. Using data of the MITO 2000 project (> 17000 point-counts), 138 species were scored according to the alpha coefficient of the logistic function linking species-presence and wood-cover. A community index (WBCI), obtained simply as the arithmetic mean of the scores of all present species, has proven (trying it with independent samples) to be strictly linked with the actual degree of wood cover, either at small (e. g. point-count) or large (i. e. 20 km <b>grid</b> <b>Atlas)</b> scale. The WBCI was tested also for its response to various sampling-effort levels, showing stability also with very incomplete data-sets (2 / 3 the actual species richness). Owing to its stability, specificity, ease, and sensitiveness (allowing to detect also fine temporal habitat change trends), WBCI seems an useful and concise indicator of the complex relationships existing between woodland and birds...|$|R
40|$|To {{overcome}} scalability {{limitations in}} database access on the <b>Grid,</b> <b>ATLAS</b> introduced the Database Release technology replicating databases in files. For years Database Release technology assured scalable database access for Monte Carlo production on the Grid. Since previous CHEP, Database Release technology was used successfully in ATLAS data reprocessing on the Grid. Frozen Conditions DB snapshot guarantees reproducibility and transactional consistency isolating Grid data processing tasks from continuous conditions updates at the “live” Oracle server. Database Release technology fully satisfies {{the requirements of}} ATLAS data reprocessing and Monte Carlo production. It is fast (on-demand access to ~ 100 MB of data takes less than 10 s), robust (failure rate less than 10 **- 6 per job that makes 10 K queries), and scalable (served 1 B queries {{in one of the}} reprocessing campaigns). We parallelized the Database Release build workflow to avoid linear dependency of the build time on the length of LHC data-taking period. In recent data reprocessing campaigns the build time was reduced by an order of magnitude thanks to a proven master-worker architecture used in the Google MapReduce. We describe further Database Release optimizations scaling up the technology for the LHC long run...|$|R
40|$|Climate {{archives}} are time series. They {{are used}} to assess temporal trends of a climate-dependent target variable, and to make climate <b>atlases.</b> A high-resolution <b>gridded</b> dataset with 1728 layers of monthly mean maximum, mean and mean minimum temperatures and precipitation for the NW Maghreb (28 ºN- 37. 3 ºN, 12 ºW- 12 ºE,  1 -km resolution) from 1973 through 2008 is presented. The surfaces were spatially interpolated by ANUSPLIN, a thin-plate smoothing spline technique approved by the World Meteorological Organization (WMO), from georeferenced climate records drawn from the Global Surface Summary of the Day (GSOD) and the Global Historical Climatology Network-Monthly (GHCN-Monthly version 3) products. Absolute errors for surface temperatures are approximately 0. 5 ºC for mean and mean minimum temperatures, and peak up to 1. 76 ºC for mean maximum temperatures in summer months. For precipitation, the mean absolute error ranged from 1. 2 to 2. 5 mm, but very low summer precipitation caused relative errors of up to 40 % in July. The archive successfully captures climate variations associated with large to medium geographic gradients. This includes the main aridity gradient which increases in the S and SE, {{as well as its}} breaking points, marked by the Atlas mountain range. It also conveys topographic effects linked to kilometric relief mesoforms. Focus on Morocco, Algeria and Tunisia North of 28 ºNDeSurvey IP (European Commission FP 6 contract No. 003950), MesoTopos (Junta de Andalucia PE P 08 -RNM- 04023) and MELODIES (European Commission FP 7, contract No. 603525) Ye...|$|R
