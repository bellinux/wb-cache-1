8|9|Public
40|$|We {{study the}} {{trade-off}} between layout {{elements of the}} search results page and revenue in the real-time sponsored search auction. Using data from a randomized experiment on a major search engine, we find that having images present among the search results tends to simultaneously raise the <b>ad</b> <b>click-through</b> rate and flatten the ad click curve, reducing the premium for occupying the top slot and thus impacting bidding incentives. Theoretical analysis shows {{that this type of}} change creates an ambiguous impact on revenue in equilibrium: a steeper curve with lower total click-through rate is preferable only if the expected revenue distribution is skewed enough towards the top bidder. Empirically, we show that this is a relatively rare phenomenon, and we also find that whole page satisfaction causally raises the click-through rate of the ad block. This means search engines have a short-run incentive to boost search result quality, not just a long-run incentive based on competition between providers...|$|E
40|$|Behavioral Targeting (BT), {{which aims}} to deliver the most {{appropriate}} advertisements to the most appropriate users, is attracting much attention in online advertising market. A key challenge of BT is how to automatically segment users for ads delivery, and good user segmentation may significantly improve the <b>ad</b> <b>click-through</b> rate (CTR). Different from classical user segmentation strategies, which rarely take the semantics of user behaviors into consideration, we propose in this paper a novel user segmentation algorithm named Probabilistic Latent Semantic User Segmentation (PLSUS). PLSUS adopts the probabilistic latent semantic analysis to mine the relationship between users and their behaviors so as to segment users in a semantic manner. We perform experiments on the real world ad click through log of a commercial search engine. Comparing {{with the other two}} classical clustering algorithms, K-Means and CLUTO, PLSUS can further improve the ads CTR up to 100 %. To our best knowledge, this work is an early semantic user segmentation study for BT in academia...|$|E
40|$|The Internet {{plays an}} {{important}} role in many companies ’ advertising strategies. According to Forrester Research, total interactive marketing spending will triple from $ 18. 4 billion in 2007 to $ 61. 3 billion in 2012 (VanBoskirk, 2008). Among the various forms of online advertising, emerging media such as social media and mobile are expected to grow the fastest at 59 % per year. Although these numbers show phenomenal growth of online advertising, consumers ’ reaction toward online advertising did not improve over the years. To the contrary, online <b>ad</b> <b>click-through</b> rates have steadily declined, and some intrusive online advertising formats have stimulated intense negative reactions from consumers (Edwards, Li, & Lee, 2002; Elkin, 2004). The much-anticipated partnership between online advertising giant Google and a top social network Myspace has also produced disappointing results so far (Blodget, 2009). Despite its start as an interactive channel that should draw consumers, online advertising now faces the danger of becoming another “push” media and being rejected by consumers as with traditional advertising. These developments run against the trend of integrated marketin...|$|E
40|$|Etsy is {{a global}} {{marketplace}} where people across the world connect to make, buy and sell unique goods. Sellers at Etsy can promote their product listings via advertising campaigns similar to traditional sponsored search <b>ads.</b> <b>Click-Through</b> Rate (CTR) prediction {{is an integral part}} of online search advertising systems where it is utilized as an input to auctions which determine the final ranking of promoted listings to a particular user for each query. In this paper, we provide a holistic view of Etsy's promoted listings' CTR prediction system and propose an ensemble learning approach which is based on historical or behavioral signals for older listings as well as content-based features for new listings. We obtain representations from texts and images by utilizing state-of-the-art deep learning techniques and employ multimodal learning to combine these different signals. We compare the system to non-trivial baselines on a large-scale real world dataset from Etsy, demonstrating the effectiveness of the model and strong correlations between offline experiments and online performance. The paper is also the first technical overview to this kind of product in e-commerce context...|$|R
5000|$|Malvertising often {{involves}} {{the exploitation of}} trustworthy companies. Those attempting to spread malware place [...] "clean" [...] advertisements on trustworthy sites first {{in order to gain}} a good reputation, then they later [...] "insert a virus or spyware in the code behind the ad, and after a mass virus infection is produced, they remove the virus", thus infecting all visitors of the site during that time period. The identities of those responsible are often hard to trace, making it hard to prevent the attacks or stop them altogether, because the [...] "ad network infrastructure is very complex with many linked connections between <b>ads</b> and <b>click-through</b> destinations." ...|$|R
40|$|In {{the last}} decade new ways of {{shopping}} online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. The two main reasons are: firstly, a person's buying choices are influenced by psychological factors like impulsiveness, and secondly, some consumers may be more susceptible to making impulse purchases than others. To {{the best of our}} knowledge, the impact of personality factors on advertisements has been largely neglected at the level of recommender systems. This work proposes a highly innovative research which uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. As a matter of fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of state-of-the-art algorithms. We present the ADS Dataset, a publicly available benchmark for computational advertising enriched with Big-Five users' personality factors and 1, 200 personal users' pictures. The proposed benchmark allows two main tasks: rating prediction over 300 real advertisements (i. e., Rich Media Ads, Image Ads, Text <b>Ads)</b> and <b>click-through</b> rate prediction. Moreover, this work carries out experiments, reviews various evaluation criteria used in the literature, and provides a library for each one of them within one integrated toolbox. Comment: This paper is an overview of Personality in Computational Advertising: A Benchmark, G. Roffo, ACM RecSys workshop on Emotions and Personality in Personalized Systems, (EMPIRE 2016...|$|R
40|$|Predicting <b>ad</b> <b>click–through</b> rates (CTR) is a massive-scale {{learning}} {{problem that}} {{is central to the}} multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal {{of this paper is to}} highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system...|$|E
40|$|Abstract—How can we detect {{suspicious}} {{users in}} large online networks? Online popularity of a user or product (via follows, page-likes, etc.) can be monetized {{on the premise}} of higher <b>ad</b> <b>click-through</b> rates or increased sales. Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking {{to make a quick}} buck. Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent (but sometimes honest) users. However, small-scale, stealthy attacks may go unnoticed {{due to the nature of}} low-rank eigenanalysis used in practice. In this work, we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods and propose FBOX, an algorithm designed to catch small-scale, stealth attacks that slip below the radar. Our algorithm has the following desirable properties: (a) it has theoretical underpinnings, (b) it is shown to be highly effective on real data and (c) it is scalable (linear on the input size). We evaluate FBOX on a large, public 41. 7 million node, 1. 5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day. I...|$|E
40|$|Deep {{learning}} gains lots of attentions {{in recent}} years and is more and more important for mining values in big data. However, to make deep learning practical {{for a wide range of}} applications in Tencent Inc., three requirements must be considered: 1) Lots of computational power are required to train a practical model with tens of millions of parameters and billions of samples for products such as automatic speech recognition (ASR), and the number of parameters and training data is still growing. 2) The capability of training larger model is necessary for better model quality. 3) Easy to use frameworks are valuable to do many experiments to perform model selection, such as finding an appropriate optimization algorithm and tuning optimal hyper-parameters. To accelerate training, support large models, and make experiments easier, we built Mariana, the Tencent deep learning platform, which utilizes GPU and CPU cluster to train models parallelly with three frameworks: 1) a multi-GPU data parallelism framework for deep neural networks (DNNs). 2) a multi-GPU model parallelism and data parallelism framework for deep convolutional neural networks (CNNs). 3) a CPU cluster framework for large scale DNNs. Mariana also provides built-in algorithms and features to facilitate experiments. Mariana is in production usage for more than one year, achieves state-of-the-art acceleration performance, and plays a key role in training models and improving quality for automatic speech recognition and image recognition in Tencent WeChat, a mobile social platform, and for <b>Ad</b> <b>click-through</b> rate prediction (pCTR) in Tencent QQ, an instant messaging platform, and Tencent Qzone, a social networking service...|$|E
40|$|Web {{advertising}} is {{growing faster than}} any other media. Web advertising {{has been known for}} its terrific communication power, and for its clickable and linkable abilities. The banner {{is one of the most}} well known forms of web advertising. However, no research has been undertaken to research the design elements in high click-through advertising banners. The purpose of this study is to find out the most effective elements that can make up the high <b>click-through</b> <b>ad</b> banners on the Web. Existing data of high click-through banners are used to find out what kind of design elements and principles are most common in high click through Web advertising banners. The research found that certain principles of design, interface and verbal elements can increase the click through rate, and semiotic approaches such as symbols and icons, visual metaphors and irony, are also found to make a high click-through banner. This research can be a guide for designing high click-through Web banners. In the future, with new technologies, advertising on the Web will become more sophisticated and complex. However, proper use of the design principles to convey the messages and organize information will remain...|$|R
40|$|This paper {{presents}} {{models for}} predicted click-through rates in position auctions that {{take into account}} two possibilities that are not normally considered [...] -that the identities of ads shown in other positions may affect the probability that an ad in a particular position receives a click (externalities) and that some ads may be less adversely affected by being shown in a lower position than others (brand effects). We present a general axiomatic methodology for how click probabilities {{are affected by the}} qualities of the ads in the other positions, and illustrate that using these axioms will increase revenue as long as higher quality ads tend to be ranked ahead of lower quality ads. We also present appropriate algorithms for selecting the optimal allocation of <b>ads</b> when predicted <b>click-through</b> rates are governed by either the models of externalities or brand effects that we consider. Finally, we analyze the performance of a greedy algorithm of ranking the ads by their expected cost-per- 1000 -impressions bids when the true click-through rates are governed by our model of predicted click-through rates with brand effects and illustrate that such an algorithm will potentially cost as much as half of the total possible social welfare. Comment: 19 pages. A shorter version of this paper will appear in WINE 201...|$|R
40|$|There {{has been}} signicant recent {{interest}} in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing {{measures such as}} average bid, average ad position, total impressions, clicks and cost for each keyword in the advertiser's campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intra-day variation in ad position. We show that estimating random utility models on ag-gregated (daily) data without accounting for this variation will lead to systematically biased estimates { specically, the impact of <b>ad</b> position on <b>click-through</b> rate (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We demonstrate the existence of the bias analytically and show the eect of the bias on the equilibrium of the SSA auction. Using a large dataset from a major search engine, we measure the magnitude of bias and quantify the losses suered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11 % due to aggregation bias. We also present a few dat...|$|R
40|$|Displaying banner {{advertisements}} (in short, ads) on webpages has {{usually been}} discussed as an Internet economics topic where a publisher uses auction models to sell an online user's page view to advertisers {{and the one}} with the highest bid can have her ad displayed to the user. This is also called real-time bidding (RTB) and the ad displaying process ensures that the publisher's benefit is maximized or there is an equilibrium in ad auctions. However, the benefits of the other two stakeholders [...] the advertiser and the user [...] have been rarely discussed. In this paper, we propose a two-stage computational framework that selects a banner ad based on the optimized trade-offs among all stakeholders. The first stage is still auction based and the second stage re-ranks ads by considering the benefits of all stakeholders. Our metric variables are: the publisher's revenue, the advertiser's utility, the ad memorability, the <b>ad</b> <b>click-through</b> rate (CTR), the contextual relevance, and the visual saliency. To the best of our knowledge, this is the first work that optimizes trade-offs among all stakeholders in RTB by incorporating multimedia metrics. An algorithm is also proposed to determine the optimal weights of the metric variables. We use both ad auction datasets and multimedia datasets to validate the proposed framework. Our experimental results show that the publisher can significantly improve the other stakeholders' benefits by slightly reducing her revenue in the short-term. In the long run, advertisers and users will be more engaged, the increased demand of advertising and the increased supply of page views can then boost the publisher's revenue. Comment: In Proceedings of the 40 th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) 2017, 10 page...|$|E
40|$|There {{has been}} {{significant}} recent interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing {{measures such as}} average bid, average ad position, total impressions, clicks and cost for each keyword in the advertiser 2 ̆ 7 s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intra-day variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates [...] specifically, the impact of <b>ad</b> position on <b>click-through</b> rate (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We demonstrate the existence of the bias analytically and show the effect of the bias on the equilibrium of the SSA auction. Using a large dataset from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11...|$|R
40|$|Recently {{there has}} been {{significant}} interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing measures such as average bid, average ad position, total impressions, clicks, and cost for each keyword in the advertiser’s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intraday variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates. Specifically, the impact of <b>ad</b> position on <b>click-through</b> rate (CTR) is attenuated and the predicted CTR {{is higher than the}} actual CTR. We analytically demonstrate the existence of the bias and show the effect of the bias on the equilibrium of the SSA auction. Using a large data set from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11 % due to aggregation bias. We also present a few data summarization techniques that can be used by search engines to reduce or eliminate the bias...|$|R
40|$|Social {{phenomena}} {{have been}} studied extensively in small scales by social scientists. With the increasing popularity of Web 2. 0 and online social networks/media, {{a large amount of}} data on social phenomena have become available. In this dissertation we study online social phenomena such as social influence in social networks in various contexts. This dissertation has two major components: 1. Identifying and characterizing online social phenomena 2. Leveraging online social phenomena for economic and commercial purposes. We begin the dissertation by developing multi-level revenue sharing schemes for viral marketing on social networks. Viral marketing leverages social influence among users of the social network. For our proposed models, we develop results on the computational complexity, individual rationality, and potential reach of employing the Shapley value as a revenue sharing scheme. Our results indicate that under the multi-level tree-based propagation model, the Shapley value is a promising scheme for revenue sharing, whereas under other models there are computational or incentive compatibility issues that remain open. We continue with another application of social influence: social advertising. Social advertising is a new paradigm that is utilized by online social networks. Social advertising is based in the premise that social influence can be leveraged to place ads more efficiently. The goal of our work is to understand how social <b>ads</b> can affect <b>click-through</b> rates in social networks. We propose a formal model for social ads in the context of display advertising. In our model, ads are shown to users one after the other. The probability of a user clicking an ad depends on the users who have clicked this ad so far. This information is presented to users as a social cue, thus the click probability is a function of this cue. We introduce the social display optimization problem: suppose an advertiser has a contract with a publisher for showing some number (say B) impressions of an ad. What strategy should the publisher use to show these ads so as to maximize the expected number of clicks? We show hardness results for this problem and in light of the general hardness results, we develop heuristic algorithms and compare them to natural baseline ones. We then study distributed content curation on the Web. In recent years readers have turned to the social web to consume content. In other words, they rely on their social network to curate content for them as opposed to the more traditional way of relying on news editors for this purpose [...] this is an implicit consequence of social influence as well. We study how efficient this is for users with limited budgets of attention. We model distributed content curation as a reader-publisher game and show various results. Our results imply that in the complete information setting, when publishers maximize their utility selfishly, distributed content curation reaches an equilibrium which is efficient, that is, the social welfare is a constant factor of that under an optimal centralized curation. Next, we initiate the study of an exchange market problem without money that is a natural generalization of the well-studied kidney exchange problem. From the practical point of view, the problem is motivated by barter websites on the Internet, e. g., swap. com, and u-exchange. com. In this problem, the users of the social network wish to exchange items with each other. A mechanism specifies for each user a set of items that she gives away, and a set of items that she receives. Consider a set of agents where each agent has some items to offer, and wishes to receive some items from other agents. Each agent would like to receive as many items as possible from the items that she wishes, that is, her utility is equal to the number of items that she receives and wishes. However, she will have a large dis-utility if she gives away more items than what she receives, because she considers such a trade to be unfair. To ensure voluntary participation (also known as individual rationality), we require the mechanism to avoid this. We consider different variants of this problem: with and without a constraint on the length of the exchange cycles and show different results including their truthfulness and individual rationality. In the other main component of this thesis, we study and characterize two other social phenomena: 1. friends vs. the crowd and 2. altruism vs. reciprocity in social networks. More specifically, we study how a social network user's actions are influenced by her friends vs. the crowd's opinion. For example, in social rating websites where both ratings from friends and average ratings from everyone is available, we study how similar one's ratings are to each other. In the next part, we aim to analyze the motivations behind users' actions on online social media {{over an extended period of}} time. We look specifically at users' likes, comments and favorite markings on their friends' posts and photos. Most theories of why people exhibit prosocial behavior isolate two distinct motivations: Altruism and reciprocity. In our work, we focus on identifying the underlying motivations behind users' prosocial giving on social media. In particular, our goal is to identify if the motivation is altruism or reciprocity. For that purpose, we study two datasets of sequence of users' actions on social media: a dataset of wall posts by users of Facebook. com, and another dataset of favorite markings by users of Flickr. com. We study the sequence of users' actions in these datasets and provide several observations on patterns related to their prosocial giving behavior...|$|R

