110|4924|Public
2500|$|The primary {{characteristics}} of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of : a 1-terabyte (TB) drive has {{a capacity of}} [...] gigabytes (GB; where 1 gigabyte = [...] bytes). Typically, some of an HDD's capacity is unavailable to the user because it {{is used by the}} file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (<b>average</b> <b>access</b> <b>time)</b> plus the time it takes for the desired sector to move under the head (average latency, which {{is a function of the}} physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).|$|E
5000|$|DSS270 {{disk storage}} {{subsystem}} provided up to 20 modules of head-per-track disk. Capacity per module was 15.3 million characters. <b>Average</b> <b>access</b> <b>time</b> was 26 ms, and maximum transfer rate was 333,000 cps.|$|E
5000|$|DSS167 {{disk storage}} {{subsystem}} allowed {{up to eight}} online disk drives plus an offline spare. Per disk capacity was 15 million characters; <b>average</b> <b>access</b> <b>time</b> was 87.5 ms and data transfer rate was 208,000 cps.|$|E
50|$|This {{improves}} <b>average</b> <b>access</b> <b>times,</b> with {{a worst-case}} seek request needing spool through only {{a maximum of}} half the tape's length. In comparison, traditional formats may need to spool through the full tape length, doubling the time-to-seek of IBM's product. This family of drives, alongside the StorageTek 9840, {{may be the only}} modern tape storage products taking this mid-tape load-point approach.|$|R
50|$|Instruction {{execution}} <b>times</b> (including <b>average</b> memory <b>access</b> <b>times)</b> were 5.25-5.75 milliseconds for addition and subtraction, and 21.25 ms for multiplication and division.|$|R
50|$|<b>Average</b> data <b>access</b> <b>time</b> was 100 ms (max. 165 ms). Maximum {{seek time}} was 25 ms. Raw {{transfer}} sector write speed was 208,333 characters/s.|$|R
50|$|A drum could record 12,000 {{words of}} data. It also had 400 words of 'reserved' storage where the computer's {{bootstrap}} program (then called Initial Orders) was stored. Up to 8 drums could be attached. <b>Average</b> <b>access</b> <b>time</b> was 5.7 ms.|$|E
50|$|The IBM 350 drive had fifty 24 in platters, {{with a total}} {{capacity}} of five million 6-bit characters (3.75 megabytes). A single head assembly having two heads was used for {{access to all the}} platters, yielding an <b>average</b> <b>access</b> <b>time</b> of just under 1 second.|$|E
5000|$|Mass storage was {{available}} {{in the form of}} both magnetic drum and magnetic disc with an interchangeable disc-pack capacity of 7.25 MB at a data interchange rate of 156 kbit/s. The high-speed drum had a capacity of 1 MB with an <b>average</b> <b>access</b> <b>time</b> of 8.6 milliseconds.|$|E
40|$|Vertical-Bloch-line {{memory is}} {{developmental}} very-large-scale integrated-circuit block-access magnetic memory. Stores data {{in form of}} localized pairs of twists (VBL pairs) in magnetic field at edge of ferromagnetic domain in each stripe. Presence or absence of VBL pair at bit position denotes one or zero, respectively. Offers advantages of resistance to ionizing radiation, potential areal storage density approximately less than 1 Gb/cm squared, data rates approximately less than 1 Gb/s, and <b>average</b> <b>access</b> <b>times</b> of order of milliseconds. Furthermore, mass, volume, and demand for power less than other magnetic and electronic memories...|$|R
40|$|To {{handle the}} demand for very large main memory, {{we are likely to}} use {{nonvolatile}} memory (NVM) as main memory. NVM main memory will have higher latency than DRAM. To cope with this, we advocate a less-deep cache hierar-chy based on a large last-level, NVM cache. We develop a model that estimates <b>average</b> memory <b>access</b> <b>time</b> and power of a cache hierarchy. The model is based on captured application behavior, an analytical power and performance model, and circuit-level memory models such as CACTI and NVSim. We use the model to explore the cache hierarchy design space and present latency-power tradeoffs for mem-ory intensive SPEC benchmarks and scientific applications. The results indicate that a flattened hierarchy lowers power and improves <b>average</b> memory <b>access</b> <b>time...</b>|$|R
40|$|This {{dissertation}} analyzes a way {{to improve}} cache performance via active management of a target cache space. As microprocessor speeds continue to grow faster than memory subsystem speeds, minimizing the <b>average</b> data <b>access</b> <b>time</b> grows in importance. As current data caches are often poorly and inefficiently managed, a good management technique can improve the <b>average</b> data <b>access</b> <b>time.</b> Cache management involves two main processes: block allocation decisions and block replacement decisions. Active block allocation can be performed most efficiently in multilateral caches (several distinct data stores with disjoint contents placed in parallel within L 1), where blocks exhibiting particular characteristics can {{be placed in the}} appropriate store. To aid in our evaluation of different active block management schemes, we have developed a multilateral cache simulator, mlcach [...] ...|$|R
50|$|Employing {{approximately}} 1,700 vacuum tubes, the computer's {{word length}} was 33 bits. It had an ultrasonic mercury {{delay line memory}} of 255 words, with an <b>average</b> <b>access</b> <b>time</b> of 500 microseconds. An addition or subtraction was clocked at 100 microseconds, multiplication at 1,600 microseconds, and division at 2,100 microseconds.|$|E
50|$|An HDD's <b>Average</b> <b>Access</b> <b>Time</b> is {{its average}} seek time which {{technically}} {{is the time to}} do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.|$|E
5000|$|DSS190 {{removable}} disk storage subsystem provided up to 16 drives using disks compatible with IBM 3336-11 drives {{used in the}} 3330. The disks were formatted with variable-length sectors in multiples of 384 characters. One pack could hold up to 133,320,000 characters. The <b>average</b> <b>access</b> <b>time</b> was 30 ms and data transfer rate was 1,074,000 cps.|$|E
40|$|Concurrency is {{a common}} {{technique}} used in modern memory systems. However, the effectiveness of memory concurrency is application dependent. It varies largely from application to application and from implementation to implementation. Understanding and utilizing memory concurrency is a vital and timely task for data intensive applications. Traditional memory performance metrics, such as <b>Average</b> Memory <b>Access</b> <b>Time</b> (AMAT), are designed for sequential data accesses, and have inherent limitations in characterizing concurrency. In this study, we propose Concurrent <b>Average</b> Memory <b>Access</b> <b>Time</b> (C-AMAT) as an accurate metric for modern memory systems. C-AMAT {{has the ability to}} examine the impact of concurrent memory behavior at both the component and system level of modern memory systems. It is a good guide for design choices, while other conventional memory metrics often mislead in measurement when concurrency is present. Keywords Memory concurrency; memory metric; memory performance measurement 1...|$|R
40|$|As {{microprocessor}} speeds {{continue to}} outgrow memory subsystem speeds, minimizing the <b>average</b> data <b>access</b> <b>time</b> grows in importance. As current data caches are often poorly and inefficiently managed, a good management technique {{can improve the}} <b>average</b> data <b>access</b> <b>time.</b> This paper presents a comparative evaluation of two approaches that utilize reuse information for more efficiently managing the firstlevel cache. While one approach {{is based on the}} effective address of the data being referenced, the other uses the program counter of the memory instruction generating the reference. Our evaluations show that using effective address reuse information performs better than using program counter reuse information. In addition, we show that the Victim cache performs best for multi-lateral caches with a direct-mapped main cache and high L 2 cache latency, while the NTS (effective-addressbased) approach performs better as the L 2 latency decreases or the associativity of the main cache increases...|$|R
5000|$|The G-15 has 180 {{vacuum tube}} packs and 300 {{germanium}} diodes. [...] It has {{a total of}} about 450 tubes (mostly dual triodes). [...] Its magnetic drum memory held 2,160 words of twenty-nine bits. <b>Average</b> memory <b>access</b> <b>time</b> is 14.5 milliseconds, but its instruction addressing architecture can reduce this dramatically for well-written programs. Its addition time is 270 microseconds (not counting memory <b>access</b> <b>time).</b> Single-precision multiplication took 2,439 microseconds and double-precision multiplication take 16,700 microseconds.|$|R
5000|$|ProxmapSort {{allows for}} the use of ProxmapSearch. Despite the O(n) build time, ProxMapSearch makes up for it with its [...] <b>average</b> <b>access</b> <b>time,</b> making it very {{appealing}} for large databases. If the data doesn't need to be updated often, the access time may make this function more favorable than other non-comparison sorting based sorts.|$|E
5000|$|A {{voice coil}} {{actuator}} moved a bar containing multiple single track recording heads, so these drums operated much like moving head disk drives with multiple disks. The heads [...] "flew" [...] on self-acting hydrodynamic air bearings. The drums had a plated magnetic recording surface. An optional feature called Fastband included 24 additional tracks with fixed read/write heads. This feature provided rapid access (35 ms. <b>average</b> <b>access</b> <b>time),</b> and a write lockout feature.|$|E
50|$|The IBM 3380 Direct Access Storage Device was {{introduced}} in June 1980. It uses film head technology and has a unit capacity of 2.52 gigabytes (two hard disk assemblies each with two independent actuators each accessing 630 MB within one chassis) with a data transfer rate of 3 megabytes per second. <b>Average</b> <b>access</b> <b>time</b> was 16 ms. Purchase price at time of introduction ranged from $81,000 to $142,200. Due to tribology problems encountered between heads and media, the first units did not ship until October, 1981.|$|E
40|$|Continued {{increases}} in clock rates of VLSI processors demand {{a reduction in}} the frequency of expensive off-chip memory references. Without such a reduction, the chip crossing time and the constraints of external logic will severely impact the clock cycle. By absorbing a large fraction of instruction references, on-chip caches substantially reduce off-chip com-munication. Minimizing the <b>average</b> instruction <b>access</b> <b>time</b> with a limited silicon budget requires careful analysis of both cache architecture and implementation. This paper exam-ines some important design issues and tradeoffs that maximize the performance of on-chip instruction caches, while retaining implementation ease. Our discussion focuses on the in-struction cache design for MIPS-X, a pipelined, 32 -bit, reduced instruction set, 20 MIPS peak, CMOS processor designed at Stanford. The on-chip instruction cache is 2 K bytes and allows single-cycle instruction accesses. Trace driven simulations show that the cache has an average miss rate of 12 % resulting in an <b>average</b> instruction <b>access</b> <b>time</b> of 1. 24 cycles. 4...|$|R
40|$|Abstract—This paper {{addresses}} feedback-directed restructuring techniques {{tuned to}} Non Uniform Cache Architectures (NUCA) in CMPs running multi-threaded applications. <b>Access</b> <b>time</b> to NUCA caches {{depends on the}} location of the referred block, so the locality and cache mapping of the application influence the overall performance. We show techniques for altering the distribution of applications into the cache space as to achieve improved <b>average</b> memory <b>access</b> <b>time.</b> In CMPs running multi-threaded applications, the aggregated accesses (and locality) of the processors form the actual cache load and pose specific issues. We consider a number of Splash- 2 and Parsec benchmarks on an 8 processor system and we show that a relatively simple remapping algorithm is able to improve the average Static-NUCA (SNUCA) cache <b>access</b> <b>time</b> by 5. 5 % and allows an SNUCA cache to surpass the performance of a more complex dynamic-NUCA (DNUCA) for most benchmarks. Then, we present a more sophisticated remapping algorithm, relying on cache geometry information and on the access distribution statistics from individual processors, that reduces the <b>average</b> cache <b>access</b> <b>time</b> by 10. 2 % and is very stable across all benchmarks. Keywords-Feedback-directed optimizations, compiler optimizations, NUCA caches, CMPs, multi-threaded applications. I...|$|R
40|$|This paper {{addresses}} feedback-directed restructuring techniques {{tuned to}} Non Uniform Cache Architectures (NUCA) in CMPs running multi-threaded applications. <b>Access</b> <b>time</b> to NUCA caches {{depends on the}} location of the referred block, so the locality and cache mapping of the application inﬂuence the overall performance. We show techniques for altering the distribution of applications into the cache space as to achieve improved <b>average</b> memory <b>access</b> <b>time.</b> In CMPs running multi-threaded applications, the aggregated accesses (and locality) of the processors form the actual cache load and pose speciﬁc issues. We consider a number of Splash- 2 and Parsec benchmarks on an 8 processor system and we show that a relatively simple remapping algorithm is able to improve the average Static-NUCA (SNUCA) cache <b>access</b> <b>time</b> by 5. 5 % and allows an SNUCA cache to surpass the performance of a more complex dynamicNUCA (DNUCA) for most benchmarks. Then, we present a more sophisticated remapping algorithm, relying on cache geometry information and on the access distribution statistics from individual processors, that reduces the <b>average</b> cache <b>access</b> <b>time</b> by 10. 2 % and is very stable across all benchmark...|$|R
50|$|While {{specifications}} vary somewhat {{between different}} drives, a typical LTO-3 drive {{will have a}} maximum rewind time of about 80 seconds and an <b>average</b> <b>access</b> <b>time</b> (from beginning of tape) of about 50 seconds. Because of the serpentine writing, rewinding often takes less time than the maximum. If a tape is written to full capacity, there is no rewind time, since the last pass is a reverse pass leaving the head {{at the beginning of}} the tape (number of tracks ÷ tracks written per pass is always an even number).|$|E
5000|$|Bloom filters {{also have}} the unusual {{property}} that the time needed either to add items or to check whether an item is in the set is a fixed constant, O (...) , completely independent {{of the number of}} items already in the set. No other constant-space set data structure has this property, but the <b>average</b> <b>access</b> <b>time</b> of sparse hash tables can make them faster in practice than some Bloom filters. In a hardware implementation, however, the Bloom filter shines because its [...] lookups are independent and can be parallelized.|$|E
50|$|The Monrobot XI's rewritable, {{persistent}} memory {{consisted of}} a rotating magnetic drum storing 1,024 words of 32-bits, which could record either a single integer or a pair of zero/single-address instructions. The <b>average</b> <b>access</b> <b>time</b> of 6,000 microseconds implies the drum made a rotation every 12 milliseconds, i.e. rotated at 5,000 RPM. Even the high-speed registers of the central processing unit (CPU) physically resided on the drum, being replicated 16 times (with 16 times as many re-write heads distributed around the drum periphery), {{so that they might}} be read or written 16 times as fast as the bulk of persistent memory.|$|E
50|$|IBM FlashSystem A9000 is a 8U rackmount {{unit with}} up to 300TB of usable storage {{capacity}} provided by FlashSystem 900 modules, managed by IBM Spectrum Accelerate software. It's scalable sibling, the FlashSystem A9000R, consists of {{a minimum of two}} units, scaling to 6 units or 1.8 PB usable in a 42U rack. A9000R units share CPU, cache and access paths with their neighbours, leveraging a zero-tuning data distribution design. The FlashSystem A9000 family supports IBM Real-time Compression, real-time global deduplication and real-time pattern removal, while maintaining <b>average</b> <b>access</b> <b>times</b> of 250 µs under database workloads. Up to 144 instances of FlashSystem A9000 and XIV Storage Systems can be combined into one HyperScale cluster with client multitenancy. Since A9000, A9000R and XIV Storage Systems share the Spectrum Accelerate management software, the FlashSystem A9000R is occasionally referred to as XIV Gen4.|$|R
40|$|MEMS-based {{storage devices}} are {{seen by many}} as {{promising}} alternatives to disk drives. Fabricated using conventional CMOS processes, MEMS-based storage consists of thousands of small, mechanical probe tips that access gigabytes of high-density, nonvolatile magnetic storage. This paper takes a first step towards understanding the performance characteristics of these devices by mapping them onto a disk-like metaphor. Using simulation models based on the mechanics equations governing the devices 2 ̆ 7 operation, this work explores how different physical characteristics (e. g., actuator forces and per-tip data rates) impact the design trade-offs and performance of MEMS-based storage. Overall results indicate that <b>average</b> <b>access</b> <b>times</b> for MEMS-based storage are 6. 5 times faster than for a modern disk (1. 5 ms vs. 9. 7 ms). Results from filesystem and database benchmarks show that this improvement reduces application I/O stall times up to 70...|$|R
40|$|The rapid {{advancement}} of mobile devices and wireless network technology has accelerated {{the development of}} applications in the mobile computing field. In particular, LBS (Location-Based Services) {{are one of the}} fastest growing areas in mobile applications. In a wireless data broadcasting environment, a server can effectively provide LBS to a large population of mobile clients. The two most typical types of LBS queries are window and kNN(k-Nearest Neighbors) queries. Previously proposed window and kNN query processing schemes in a single wireless broadcast channel environment tend to access much unnecessary index information and data objects due to their low filtering powers. In this paper, we propose a new spatial index called HMI (Hilbert curve-based MBR filtering Index) for the efficient processing of window and kNN queries in a single wireless broadcast channel. HMI is a tree-structured index whose construction is based on the Hilbert curve and MBR (Minimum Bounding Rectangle). By combining an MBR structure with the Hilbert curve order, HMI obtains the advantages of both structures. We present an in-depth experimental analysis of our method by comparing it with current existing schemes. Our performance analysis shows that HMI significantly decreases the <b>average</b> <b>access</b> and tuning <b>times</b> of kNN queries over current existing schemes. HMI also gives better average tuning <b>times</b> and comparable <b>average</b> <b>access</b> <b>times</b> of window queries over current existing schemes...|$|R
5000|$|EDSAC, {{the second}} full scale stored-program digital computer, began {{operation}} with 512 35-bit words of memory, stored in 32 delay lines holding 576 bits each (a 36th bit {{was added to}} every word as a start/stop indicator). In the UNIVAC I this was reduced somewhat, each column stored 120 bits (although the term [...] "bit" [...] was not in popular use at the time), requiring seven large memory units with 18 columns each {{to make up a}} 1000-word store. Combined with their support circuitry and amplifiers, the memory subsystem formed its own walk-in room. The <b>average</b> <b>access</b> <b>time</b> was about 222 microseconds, which was considerably faster than the mechanical systems used on earlier computers.|$|E
50|$|The IBM 1302 Disk Storage Unit was {{introduced}} in September 1963. Improved recording quadrupled its capacity over that of the 1301, to 117 million 6-bit characters per module. <b>Average</b> <b>access</b> <b>time</b> is 165 ms and data can be transferred at 180 K characters/second, {{more than double the}} speed of the 1301. A second arm accesses a separate group of 250 tracks. As with the 1301, there is a Model 2 with twice the capacity. The IBM 1302 Model 1 leased for $5,600 per month or could be purchased for $252,000. Prices for the Model 2 were $7,900 per month or $355,500 to purchase. The IBM 7631 controller cost an additional $1,185 per month or $56,000 to purchase. The 1302 was withdrawn in February 1965.|$|E
50|$|Access to {{the main}} memory for each {{instruction}} execution may result in the very slow processing with the clock speed dependent on the time taken to find the data in main memory and fetch it. In order to hide this memory latency from the processor, data caching is used. Whenever the data is required by the processor, it is fetched from the memory and stored in the small structure called Cache. For any further references to that data, the cache is searched first before going to main memory. This structure resides closer to the processor {{in terms of time}} taken to search and fetch data with respect to Main Memory. The advantages of using cache can be proven by calculating the <b>Average</b> <b>Access</b> <b>Time</b> (AAT) for the memory hierarchy without cache and with the cache.|$|E
40|$|On-chip L 1 and L 2 caches {{represent}} a sizeable {{fraction of the}} total power consumption of microprocessors. In deep sub-micron technology, the subthreshold leakage power is becoming the dominant {{fraction of the total}} power consumption of those caches. In this paper, we present optimization techniques to reduce the leakage power of on-chip caches assuming that there are multiple threshold voltages, VTH’s, available. First, we show a cache leakage optimization technique that examines the trade-off between <b>access</b> <b>time</b> and leakage power by assigning distinct VTH's to each of the four main cache components — address bus drivers, data bus drivers, decoders, and SRAM cell arrays with sense-amps. Second, we show optimization techniques to reduce the leakage power of L 1 and L 2 on-chip caches without affecting the <b>average</b> memory <b>access</b> <b>time.</b> The key results are: 1) 2 VTH's are enough to minimize leakage in a single cache; 2) if L 1 size is fixed, increasing the L 2 size can result in much lower leakage without reducing <b>average</b> memory <b>access</b> time; 3) if L 2 size is fixed, reducing L 1 size can result in lower leakage without loss of the <b>average</b> memory <b>access</b> time; and 4) smaller L 1 and larger L 2 caches than are typical in today’s processors result in significant leakage and dynamic power reduction without affecting the <b>average</b> memory <b>access</b> <b>time.</b> 1...|$|R
5000|$|These cache misses {{directly}} correlate to {{the increase}} in cycles per instruction (CPI). However the amount of effect the cache misses have on the CPI also depends on how much of the cache miss can be overlapped with computations due to the ILP ( [...] Instruction-level parallelism) and how much of it can be overlapped with other cache misses due to Memory-level parallelism. If we ignore both these effects, then the <b>Average</b> memory <b>access</b> <b>time</b> is a metric which becomes important. It is used to measure the performance of the memory systems and hierarchies. It refers to the time necessary to perform a memory <b>access</b> on an <b>average</b> rate. It is the addition of the execution time for the memory instructions and the memory stall cycles. The execution time is the time taken for a cache access and the memory stall cycles include the time taken to service a cache miss and access the lower levels of memory. If the access latency, miss rate and miss penalty are known, the <b>average</b> memory <b>access</b> <b>time</b> can be calculated with the following relation.|$|R
40|$|For decades, {{the memory}} {{hierarchy}} access gap has plagued computer architects with the RAM/disk gap widening to about 6 {{orders of magnitude}} in 1999. However, an exciting new storage technology based on MicroElectroMechanical Systems (MEMS) is poised to fill {{a large portion of}} this performance gap, delivering significant performance improvements and enabling many new types of applications. This research explores the impact MEMS-based storage will have on computer systems. Working closely with researchers building MEMS-based storage devices, we examine the performance impact of several design points. Results from five different applications show that MEMS-based storage can reduce application I/O stall times by 80 - 99 %, with overall performance improvements ranging from 1. 1 &times; to 20 &times; for these applications. Most of these improvements result from the fact that <b>average</b> <b>access</b> <b>times</b> for MEMS-based storage are 5 times faster than disks (e. g., 1 - 3 ms). Others result from fundamental dierences in the p [...] ...|$|R
