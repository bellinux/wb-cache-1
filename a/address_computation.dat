55|271|Public
25|$|The {{performance}} {{increase of}} the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was {{partly due to the}} non-multiplexed address and data buses, but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286, while the older 8086 had to do effective <b>address</b> <b>computation</b> using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.|$|E
50|$|Address {{generation}} unit (AGU), sometimes also called <b>address</b> <b>computation</b> unit (ACU), is an execution unit inside {{central processing units}} (CPUs) that calculates addresses used by the CPU to access main memory. By having address calculations handled by separate circuitry that operates in parallel {{with the rest of}} the CPU, the number of CPU cycles required for executing various machine instructions can be reduced, bringing performance improvements.|$|E
50|$|The {{performance}} {{increase of}} the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was {{partly due to the}} non-multiplexed address and data buses but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286 while the older 8086 had to do effective <b>address</b> <b>computation</b> using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.|$|E
40|$|This paper {{describes}} a new code optimization technique for digital signal processors #DSPs#. One important characteristic of DSP algorithms are iterative accesses to data array elements within loops. DSPs support e#cient <b>address</b> <b>computations</b> for such array accesses {{by means of}} dedicated address generation units #AGUs#. We present a heuristic technique which, given an AGU with a #xed number of address registers, minimizes the number of instructions needed for array <b>address</b> <b>computations</b> in a program loop...|$|R
40|$|International audienceMultimedia {{applications}} such as video and image processing are often characterized by {{a huge number of}} data accesses. In many digital signal processing applications, array access patterns are regular and periodic. In these cases, optimized architectures using pipelined memory access controllers can be generated. In this paper, we focus on implementing memory interfacing modules that can be automatically generated from a high-level synthesis tool and which can efficiently handle predictable address patterns as well as random ones (i. e., dynamic <b>address</b> <b>computations).</b> The benefits of balancing dynamic <b>address</b> <b>computations</b> from datapath to dedicated computation units in the memory controller is also analyzed as well as operator bitwidth optimization and data locality to save power consumption and reduce latency...|$|R
40|$|Abstract â€” Effective address {{calculation}} for {{load and}} store instructions needs {{to compete for}} ALU with other instructions and hence extra latencies might be incurred to data cache accesses. Fast address generation is an approach proposed to reduce cache access latencies. This paper presents a fast address generator that can eliminate most of the effective <b>address</b> <b>computations.</b> Experimental results show that this fast address generator can reduce effective <b>address</b> <b>computations</b> of load and store instructions by about 74 % on average for SPECint 2000 benchmarks and cut the execution times by 8. 5 %. In addition, further improvement can be made if data of previous load operations are buffered in the unused data field of LSQ entries as well. Runtime impact will expand to 10. 5 % on average when the default LSQ is modified to the cached LSQ design. I...|$|R
50|$|There {{are various}} forms of the buddy system; those in which each block is {{subdivided}} into two smaller blocks are the simplest and most common variety. Every memory block in this system has an order, where the order is an integer ranging from 0 to a specified upper limit. The size of a block of order n is proportional to 2n, so that the blocks are exactly {{twice the size of}} blocks that are one order lower. Power-of-two block sizes make <b>address</b> <b>computation</b> simple, because all buddies are aligned on memory address boundaries that are powers of two. When a larger block is split, it is divided into two smaller blocks, and each smaller block becomes a unique buddy to the other. A split block can only be merged with its unique buddy block, which then reforms the larger block they were split from.|$|E
50|$|Group {{address and}} unknown {{destination}} individual frames are optimally transmitted to only {{members of the}} same Ethernet service. IEEE 802.1aq supports the creation of thousands of logical Ethernet services in the form of E-LINE, E-LAN or E-TREE constructs which are formed between non participating logical ports of the IEEE 802.1aq network. These group address packets are encapsulated with a PBB header which indicates the source participating address in the SA while the DA indicates the locally significant group address this frame should be forwarded on and which source bridge originated the frame. The IEEE 802.1aq multicast forwarding tables are created based on computations such that every bridge which is on the shortest path between a pair of bridges which are {{members of the same}} service group will create proper forwarding database (FDB) state to forward or replicate frames it receives to that members of that service group. Since the group <b>address</b> <b>computation</b> produce shortest path trees, there is only ever one copy of a multicast packet on any given link. Since only bridges on a shortest path between participating logical ports create forwarding database (FDB) state the multicast makes the efficient use of network resources.|$|E
40|$|An {{important}} {{portion of}} end [...] to [...] end latency in data transfer {{is spent in}} <b>address</b> <b>computation,</b> determining a relation between sender and receiver addresses. In deposit model communication, this computation happensonlyon the sender {{and some of its}} results are embeddedin the message. Conventionally, <b>address</b> <b>computation</b> takes place on [...] line, as the message is assembled. If the amount of <b>address</b> <b>computation</b> is significant, and the communication is repeated, it may make sense to remove <b>address</b> <b>computation</b> from the critical path by caching its results. However, assembling a message using the cache uses additional memory bandwidth. We present a fine grain analytic model for simple address relation caching in deposit model communication. The model predicts how many times a communication must be repeated in order for the average end [...] to [...] end latency of an implementation which caches to break even with that of an implementation which doesn't cache. The model also predicts speedup and those regim [...] ...|$|E
40|$|This paper {{describes}} a new code optimization technique for digital signal processors (DSPs). One important characteristic of DSP algorithms are iterative accesses to data array elements within loops. DSPs support efficient <b>address</b> <b>computations</b> for such array accesses {{by means of}} dedicated address generation units (AGUs). We present a heuristic technique which, given an AGU with a fixed number of address registers, minimizes the number of instructions needed for array <b>address</b> <b>computations</b> in a program loop. 1 1 Introduction DSPs are a special class of embedded processors, which show highly specialized instruction sets and pose challenges both to compilers and assembly programmers. Many of today's C compilers for DSPs {{have been shown to}} produce code of poor quality. In order to overcome this problem, new DSP-specific code optimization techniques are required. In DSP algorithms frequent references to elements of data arrays are very common. Mostly, such array elements are iteratively a [...] ...|$|R
50|$|Some languages, such as C, {{provide only}} zero-based array types, {{for which the}} minimum valid value for any index is 0. This choice is {{convenient}} for array implementation and <b>address</b> <b>computations.</b> With a language such as C, a pointer to the interior of any array can be defined that will symbolically act as a pseudo-array that accommodates negative indices. This works only because C does not check an index against bounds when used.|$|R
40|$|Multimedia {{applications}} are often {{characterized by a}} large number of data accesses with regular and periodic ac-cess patterns. In these cases, optimized pipelined memory access controllers can be generated improving the pipeline access mode to RAM. We focus on the design and the im-plementation of memory sequencers that can be automati-cally generated from a behavioral synthesis tool and which can efficiently handle predictable address patterns as well as unpredictable ones (dynamic <b>address</b> <b>computations)</b> in a pipeline way. ...|$|R
40|$|Abstract:- Implementation of an <b>address</b> <b>computation</b> {{unit for}} a {{configurable}} parallel memory architecture is presented. The benefit of configurability {{is that a}} multitude of access formats and module assignment functions {{can be used with}} a single hardware implementation, which has not been possible in prior embedded parallel memory system. In addition to design and implementation, a simulation example is given for an image processing application. Results show significant reduction in the overall memory accesses, which is the key to solve the processor-memory bottleneck. Key-Words: configurable parallel memory, <b>address</b> <b>computation,</b> conflict free access, module assignment function, page tabl...|$|E
40|$|The B-tree is {{probably}} the most popular method in use today for indexes and inverted files in database management systems. The traditional implementation for a B-tree uses many pointers (more than one per key), and which can directly affect the performance of the B-tree. A general method of file organization and access (called Dynamic <b>Address</b> <b>Computation)</b> has been described by Cook {{that can be used to}} implement B-trees using no pointers. A minimal amount of storage (in addition to the keys) is required. This paper gives a detailed description of Direct <b>Address</b> <b>Computation</b> and the resulting system is analyzed, leading to the conclusion that, while the approach results in a simple implementation of B-trees, more work is required to achieve performance for large B-trees...|$|E
40|$|Existent {{algorithms}} {{to perform}} geometric transformations on octrees {{can be classified}} in two families: inverse transformation and <b>address</b> <b>computation</b> ones. Those in the inverse transformation family essentially resample the target octree from the source one, {{and are able to}} cope with all the affine transformations. Those in the <b>address</b> <b>computation</b> family only deal with translations, but are commonly accepted as faster than the former ones for they do no intersection tests, but directly calculate the transformed address of each black node in the source tree. This work introduces a new translation algorithm that shows to perform better than previous one when very small displacements are involved. This property is particularly useful in applications such as simulation, robotics or computer animation. 1 Introduction Octrees {{are one of the most}} common hierarchical data structures in three dimensional space. They have been successfully used in many different, areas, such as solid modeling, [...] ...|$|E
40|$|The VLSI binary adder is {{the basic}} {{building}} block in any computation unit. It is widely used in the arithmetic logic unit, memory <b>addressing</b> <b>computation</b> and in many other places. In this paper the binary adder is presented with keeping in mind speed, power and finally area. In this paper the BCD adder is designed using the mixed approach such as hierarchical, muxing and variable grouping techniques. The design and result are presented in this paper...|$|R
40|$|Abstract: This paper <b>addresses</b> <b>computations</b> of a robustly safe {{region on}} the state space for {{uncertain}} constrained systems subject to disturbances based on a probabilistic approach. We first define a probabilistic output admissible (POA) set. This set is {{a subset of the}} state space which excludes with high probability initial states violating the constraint. Then, an algorithm for computing the POA set is developed based on a randomized technique. The utility of the POA set is demonstrated through a numerical simulation. 1...|$|R
40|$|Uncertainty is a {{key issue}} in {{decision}} analysis {{and other kinds of}} applications. Researchers have developed a number of approaches to <b>address</b> <b>computations</b> on uncertain quantities. When doing arithmetic operations on random variables, an important question has to be considered: the dependency relationships among the variables. In practice, we often have partial information about the dependency relationship between two random variables. This information may result from experience or system requirements. We can use this information to improve bounds on the cumulative distributions of random variables derived from the marginals whose dependency is partially known...|$|R
40|$|Using a {{prime number}} p of memory banks on a vector {{processor}} allows a conflict-free access for any slice of p consecutive {{elements of a}} vector stored with a stride not multiple of p. To reject {{the use of a}} prime number of memory banks, it is generally advanced that <b>address</b> <b>computation</b> for such a memory system would require systematic Euclidean division by the number p. The Chinese Remainder Theorem allows a simple mapping of data onto the memory banks for which <b>address</b> <b>computation</b> does not require any Euclidean division. However, this requires that the number of words in each memory module m and p be relatively prime. We propose a method based on the Chinese Remainder Theorem for moduli with common factors that does not have such a restriction. The proposed method does not require Euclidean division and also results in an efficient error detection/correction mechanism for address translation...|$|E
40|$|An {{important}} {{class of}} problems used widely {{in both the}} embedded systems and scientific domains perform memory intensive computations on large data sets. These data sets get to be typically stored in main memory, {{which means that the}} compiler needs to generate the address of a memory location in order to store these data elements and generate the same address again when they are subsequently retrieved. This memory <b>address</b> <b>computation</b> is quite expensive, and if it is not performed efficiently, the performance degrades significantly. In this paper, we have developed a new compiler approach for optimizing the memory performance of subscripted or array variables and their address generation in stencil problems that are common in embedded image processing and other applications. Our approach makes use of the observation that in all these stencils, most of the elements accessed are stored close to one other in memory. We try to optimize the stencil codes with a view of reducing both the arithmetic and the <b>address</b> <b>computation</b> overhead. The regularity of the access pattern and the reuse of data elements between successive iterations of the loop body means that there is a common sub-expression between any two successive iterations; these common sub-expressions are difficult to detect using state-ofthe-art compiler technology. If we were to store the value of the common sub-expression in a scalar, then for the next iteration, the value in this scalar could be used instead of performing the computation all over again. This greatly reduces the arithmetic overhead. Since we store only one scalar in a register, there is almost no register pressure. Also all array accesses are now replaced by pointer dereferences, where the pointers are incremented after each iteration. This reduces the <b>address</b> <b>computation</b> overhead. Our solution is the only one so far to exploit both scalar conversion and common sub-expressions. Extensive experimental results on several codes show that our approach performs better than the other approaches. ...|$|E
40|$|We {{present an}} {{explicit}} memory organization scheme for ditributing M data items among N memory modules where M=Theta(N^(1. 5 -O(1 /log N))). Each datum is replicated into a constant {{number of copies}} assigned to distinct modules. Assuming that N processors {{are connected to the}} memories through a complete graph, we provide an access protocol so that the processors can read/write any set of N' <= N distinct data in O((N') ^(1 / 3) log*N'+logN) worst-case time. The <b>address</b> <b>computation</b> can be carried out efficiently without resort to a complete memory map and using O(1) internal registers per processor...|$|E
40|$|Integer division, modulo, and {{remainder}} {{operations are}} expressive and useful operations. They are logical candidates to express complex data accesses {{such as the}} wrap-around behavior in queues using ring buers. In addition, they appear frequently in <b>address</b> <b>computations</b> {{as a result of}} compiler optimizations that improve data locality, perform data distribution, or enable parallelization. Experienced application programmers, however, avoid them because they are slow. Furthermore, while advances in both hardware and software have improved the performance of many parts of a program, few are applicable to division and modulo operations. This trend makes these operations increasingly detrimental to program performance...|$|R
40|$|Functional memory (FM) uses {{memory mapped}} {{reprogrammable}} field programmable gate arrays (FPGAs) for fine-grained parallel processing. Multioperand expressions are computed in combinational logic eliminating processor computation steps. FPGAs capture operands as memory is written, eliminating separate processor load-stores to pass operands. This paper describes how program expressions {{can be implemented}} in FM, including branch <b>address</b> <b>computations.</b> It concludes with a load store analysis comparing a conventional yon Neumann processor with and without FM for a shortest path program. The load store count stays about the same but eliminating the computation steps results in a onethird step reduction overall with FM...|$|R
40|$|We {{propose a}} class of interleavers for a novel deep neural network (DNN) {{architecture}} that uses algorithmically pre-determined, structured sparsity to significantly lower memory and computational requirements, and speed up training. The interleavers guarantee clash-free memory accesses to eliminate idle operational cycles, optimize spread and dispersion to improve network performance, and are designed to ease the complexity of memory <b>address</b> <b>computations</b> in hardware. We present a design algorithm with mathematical proofs for these properties. We also explore interleaver variations and analyze the behavior of neural networks {{as a function of}} interleaver metrics. Comment: Presented at the 2017 Asilomar Conference on Signals, Systems, and Computer...|$|R
40|$|Using a {{prime number}} N of memory banks on a vector {{processor}} allows a conflict-free access for any slice of N consecutive {{elements of a}} vector stored with a stride not multiple of N. To reject {{the use of such}} a prime number of memory banks, it is generally advanced that <b>address</b> <b>computation</b> for such a memory system would require systematic Euclidean Division by the prime number N. In this short note, we show that there exists a very simple mapping of data in the memory banks for which address compulations does not require any Euclidean Division...|$|E
40|$|This paper {{describes}} an ACL 2 interpreter for "netlists" describing quantum circuits. Several quantum gates are implemented, including the Hadamard gate H, which rotates vectors by 45 degrees, necessitating {{the use of}} irrational numbers, {{at least at the}} logical level. Quantum measurement presents an especially difficult challenge, because it requires precise comparisons of irrational numbers and the use of random numbers. This paper does not <b>address</b> <b>computation</b> with irrational numbers or the generation of random numbers, although future work includes the development of pseudo-random generators for ACL 2. Comment: In Proceedings ACL 2 2013, arXiv: 1304. 712...|$|E
40|$|Abstract | An {{increasing}} number of components in embedded systems are implemented by software running on embedded processors. This trend creates a need for compilers for embedded processors capable of generating high quality machine code. Particularly for DSPs, such compilers are hardly available, and novel DSP-speci c code optimization techniques are required. In this paper we focus on e cient <b>address</b> <b>computation</b> for array accesses in loops. Based on previous work, we present a new and optimal algorithm for address register allocation and provide an experimental evaluation of di erent algorithms. Furthermore, an e cient and close-to-optimum heuristic is proposed for large problems. 1 I...|$|E
5000|$|The problem Monte Carlo {{integration}} <b>addresses</b> is the <b>computation</b> of {{a multidimensional}} definite integral ...|$|R
40|$|International audienceMultimedia {{applications}} {{are characterized by}} {{a large number of}} data accesses (i. e. RAM accesses). In many digital signal-processing applications, the array access patterns are regular and periodic. Optimized Pipelined Memory Access Controllers can be generated. The computational complexity reduction of algorithms, for instance in multidimensional signal processing applications, can be achieved by using execution hazards (conditional statements). In this paper we propose a behavioral synthesis design flow based for data-dominated systems under timing constraints handling both predictable and unpredictable memory access patterns in a memory sequencer. We also analyze the benefits of balancing dynamic <b>address</b> <b>computations</b> from datapath to specialized computation units placed in the memory sequencer thus optimizing area, latency, data locality i. e. reducing the power consumption...|$|R
40|$|Memory {{intensive}} applications require considerable arithmetic for the computation {{and selection}} of the different memory access pointers. These memory address calculations often involve complex (non) linear arithmetic expressions which have to be calculated during program execution under tight timing constraints, thus becoming a crucial bottleneck in the overall system performance. This paper explores applicability and effectiveness of sourcelevel optimisations (as opposed to instruction-level) for <b>address</b> <b>computations</b> {{in the context of}} multimedia. We propose and evaluate two processor-target independent source-level optimisation techniques, namely, global scope operation cost minimisation complemented with loop-invariant code hoisting, and non-linear operator strength reduction. The transformations attempt to achieve minimal code execution within loops and reduced operator strengths. The effectiveness of the transformations is demonstrated with two real-life multimedia applica [...] ...|$|R
40|$|Part 5 : I/O, File Systems, and Data ManagementInternational audienceIn {{this paper}} we put forward an {{annotation}} system for specifying {{a sequence of}} data layout transformations for loop vectorization. We propose four basic primitives for data layout transformations that programmers can compose to achieve complex data layout transformations. Our system automatically modifies all loops and other code operating on the transformed arrays. In addition, we propose data layout aware loop transformations to reduce the overhead of <b>address</b> <b>computation</b> and help vectorization. Taking the Scalar Penta-diagonal (SP) solver, from the NAS Parallel Benchmarks as a case study, we show that the programmer can achieve significant speedups using our annotations...|$|E
40|$|This paper {{presents}} a new DSP-oriented code optimization method to enhance performance by exploiting the specific architectural features of digital signal processors. In the proposed method, a source code is translated into the static single assignment form while preserving the high-level {{information related to}} loops and the <b>address</b> <b>computation</b> of array accesses. The information is used in generating hardware loop instructions and parallel instructions provided by most digital signal processors. In addition to the conventional control-data flow graph, a new graph is employed {{to make it easy}} to find auto-modification addressing modes efficiently. Experimental results on benchmark programs show that the proposed method is effective in improving performance. 1...|$|E
40|$|Abstract â€” Transport {{triggered}} architecture (TTA) {{offers a}} cost-effective trade-off between the energy-efficiency and perfor-mance of an ASIC implementation and the flexibility {{provided by a}} software implementation on a programmable processor. In this paper, we describe a programmable TTA processor, which is tailored for computing mixed-radix fast Fourier transform (FFT). Several approaches has been exploited to reduce the power consumption of the processor; e. g., special function units for complex-valued arithmetics and <b>address</b> <b>computation,</b> clock gating, instruction compression are utilized. The paper shows FFT implementation supporting power-of-two FFTs {{with the aid of}} mixed radix algorithm consisting of radix- 4 and radix- 2 computations. The developed processor is programmable but shows energy-efficiency comparable to fixed-function ASIC im-plementations. I...|$|E
40|$|The paper {{presents}} {{practical and}} theoretical results acquired during a scientific project addressing multi-robot control and management challenges with possible application in greenhouse automation solutions. The {{main focus of}} the paper is path planning of mobile robots <b>addressing</b> <b>computation</b> time and memory constraints in embedded robotic systems. The paper presents an analysis {{that is based on}} practically experienced cases and proposes modifications of the discussed RRT-based methods in order to ensure better quality of the planning result as well as saved time in specific cases. A key factor of the compareson analysis is time and memory usage that usually are limited in embedded devices like small scale mobile robots. The paper also presents experiment results collected using a prototype robotic system...|$|R
40|$|This paper <b>addresses</b> the <b>computation</b> of the {{three-dimensional}} transient heat transfer through a layered solid and/or fluid formation containing irregular inclusions. The use of appropriate Green's functions for a flat layer formation in a boundary element method formulation avoids the discretization of the layer interface boundaries. [URL]...|$|R
40|$|This paper {{introduces}} the MoM- 3 (Map oriented Machine 3) architecture as an accelerator for applications {{with a large}} amount of 32 -bit arithmetic. The data manipulations in the inner loop of the application can be configured as a complex hardware operator into special fieldprogrammable devices. Several complex address generators are programmed to implement the control structures of the algorithm, so that data is accessed completely under hardware control, without requiring memory cycles for <b>address</b> <b>computations.</b> The MoM- 3 supports pipelining on statement and expression level. The compiler takes C as input language and produces the configuration code for the field-programmable devices and the address generators without further user interaction. Acceleration factors in the range of 12 to 62 have been obtained, compared to a high-end Sun SPARC workstation. 1...|$|R
