20|30|Public
5000|$|MacKay, Donald MacCrimmon, and Michael Ellis Fisher. <b>Analogue</b> <b>computing</b> at ultra-high speed: an {{experimental}} and theoretical study. London: Chapman & Hall, 1962.|$|E
50|$|The Luton <b>Analogue</b> <b>Computing</b> Engine (LACE) was a {{code name}} for a {{military}} general purpose analogue computer, predominantly used for missile simulation.It was developed in 1946 by English Electric's Guided Missile Division in Luton, UK. Upon {{the closure of the}} Luton factory in 1962, LACE was transferred to the British Aircraft Corporation (BAC) Guided Weapons Division in Stevenage.|$|E
50|$|Throughout the 20th century {{air defence}} {{was one of}} the fastest-evolving areas of {{military}} technology, responding to the evolution of aircraft and exploiting various enabling technologies, particularly radar, guided missiles and computing (initially electromechanical <b>analogue</b> <b>computing</b> from the 1930s on, as with equipment described below). Air defence evolution covered the areas of sensors and technical fire control, weapons, and command and control. At the start of the 20th century these were either very primitive or non-existent.|$|E
40|$|We {{present a}} set of 144 galactic {{chemical}} evolution models applied to a Milky Way <b>analogue,</b> <b>computed</b> using four sets of low and intermediate star nucleosynthetic yields, six massive star yield compilations, and six functional forms for the initial mass function. The integrated or true yields for each combination are derived. A comparison is made between a grid of multiphase chemical evolution models computed with these yield combinations and empirical data drawn from the Milky Way's disc, including the solar neighbourhood. By means of a chi 2 methodology, applied {{to the results of}} these multiphase models, the best combination of stellar yields and initial mass function capable of reproducing these observations is identified. Comment: 16 pages, 13 figures, accepted for publication in Monthly Notices of the Royal Astronomical Societ...|$|R
40|$|Two {{different}} {{tools to}} evaluate quantile regression forecasts are proposed: MAD, to summarize forecast errors, and a fluctuation test to evaluate in-sample predictions. The {{scores of the}} PISA test to evaluate student proficiency is considered. Growth analysis relates school attainment to economic growth. The analysis is complemented by investigating the estimated regression and predictions {{not only at the}} centre but also in the tails. For out-of-sample forecasts the estimates in one wave are employed to forecast the following waves. The reliability of in-sample forecasts is controlled by excluding part of the sample selected by a specific rule: boys to predict girls, public schools to forecast private ones, vocational schools to predict non-vocational, etc. The gradient computed in the subset is compared to its <b>analogue</b> <b>computed</b> in the full sample in order to verify the validity of the estimated equation and thus of the in-sample predictions...|$|R
40|$|The {{molecular}} {{and electronic}} structure of [M(η 3 -C 3 H 5) 2] (M = Ni, Pd, Pt) {{has been investigated}} by means of quasi-relativistic gradient-corrected density functional calculations. Geometries have been fully optimized by considering both trans and cis arrangements of the bis(η 3 -allyl) moiety. Binding energy differences between isomers are always smaller than 0. 2 kcal/mol; in particular, cis-[Ni(η 3 -C 3 H 5) 2] is computed to be more stable than trans-[Ni(η 3 -C 3 H 5) 2], while a reversed order is obtained for Pd and Pt <b>analogues.</b> <b>Computed</b> geometrical parameters of trans-[Ni(η 3 -C 3 H 5) 2] compare very well with available structural data. Moreover, a new assignment of variable energy photoelectron spectroscopy measurements [Li, X.; Bancroft, G. M.; Puddephatt, R. J.; Liu, Z. F.; Hu, Y. F.; Tan, K. H. J. Am. Chem. Soc. 1994, 116, 9543 − 9554] is proposed by assuming that the trans:cis ratio in the gas phase is close to one...|$|R
50|$|The rectangulus {{was a form}} of {{skeleton}} torquetum. This was {{a series}} of nested angular scales, so that measurements in azimuth and elevation could be made directly in polar coordinates, relative to the ecliptic. Conversion from these coordinates though was difficult, involving what was the leading mathematics of the day. The rectangulus was an <b>analogue</b> <b>computing</b> device to simplify this: instead of measuring in angular measurements it could resolve the angles to Cartesian components directly. This then simplified the further calculations.|$|E
50|$|Structural {{equation}} modeling, as {{the term}} is currently used in sociology, psychology, and other social sciences evolved from the earlier methods in genetic path modeling of Sewall Wright. Their modern forms came about with computer intensive implementations in the 1960s and 1970s. SEM evolved in three different streams: (1) systems of equation regression methods developed mainly at the Cowles Commission; (2) iterative maximum likelihood algorithms for path analysis developed mainly by Karl Gustav Jöreskog at the Educational Testing Service and subsequently at the University of Uppsala; and (3) iterative canonical correlation fit algorithms for path analysis also developed at the University of Uppsala by Hermann Wold. Much of this development occurred {{at a time that}} automated computing was offering substantial upgrades over the existing calculator and <b>analogue</b> <b>computing</b> methods available, themselves products of the proliferation of office equipment innovations in the late 19th century. The 2015 text Structural Equation Modeling: From Paths to Networks provides a history of the methods.|$|E
40|$|It {{has become}} commonplace {{to apply the}} terms {{analogue}} and digital to various technologies. During the twentieth century, this classification was applied to computing technology and created two very separate technical cultures. From the perspective of twenty-first century technology, <b>analogue</b> <b>computing</b> is often {{assumed to be a}} technology with a continuous representation of state. However, analogue computers were originally so-called because they supported the construction of analogies. This paper reviews the history of <b>analogue</b> <b>computing</b> and presents the history as a three-stranded chronology, separating the ideas of analogue computer as a 'continuous' machine {{from the perspective of the}} analogue computer as an 'analogy' or 'modelling' machine. Around 1940, these two conceptual identities became entwined, creating the analogue/digital discourse familiar today...|$|E
40|$|Electrooptic {{modulation}} {{performs the}} conversion between the electrical and optical domain with applications in data communication for optical interconnects, {{but also for}} novel optical compute algorithms such as providing nonlinearity at the output stage of optical perceptrons in neuromorphic <b>analogue</b> optical <b>computing.</b> Since the clock frequency for photonics on chip has a power overhead sweet slot around 10 s of GHz, ultrafast modulation may only be required in long distance communication, but not for short onchip links. Here we show a roadmap towards atto Joule per bit efficient modulators on chip {{as well as some}} experimental demonstrations of novel plasmon modulators with sub 1 fJ per bit efficiencies. We then discuss the first experimental demonstration of a photon plasmon-hybrid Graphene-based electroabsorption modulator on silicon. The device shows a sub 1 V steep switching enabled by near ideal electrostatics delivering a high 0. 05 dB per V um performance requiring only 110 aJ per bit. Improving on this design, we discuss a plasmonic slot based Graphene modulator design, where the polarization of the plasmonic mode matches with Graphenes inplane dimension. Here a push pull dual gating scheme enables 2 dB per V um efficient modulation allowing the device to be just 770 nm short for 3 dB small signal modulation. This in turn allows for a device-enabled two orders of magnitude improvement of electrical optical co integrated network on chips over electronic only architectures. The latter opens technological opportunities in in cognitive computing, dynamic data-driven applications system, and optical <b>analogue</b> <b>compute</b> engines to include neuromorphic photonic computing...|$|R
40|$|We first {{consider}} {{the estimation of}} the finite rate of population increase or population growth rate, u i, using capture-recapture data from open populations. We review estimation and modelling of u i under three main approaches to modelling openpopulation data: the classic approach of Jolly (1965) and Seber (1965), the superpopulation approach of Crosbie & Manly (1985) and Schwarz & Arnason (1996), and the temporal symmetry approach of Pradel (1996). Next, we {{consider the}} contributions of different demographic components to u i using a probabilistic approach based on {{the composition of the}} population at time i + 1 (Nichols et al., 2000 b). The parameters of interest are identical to the seniority parameters, n i, of Pradel (1996). We review estimation of n i under the classic, superpopulation, and temporal symmetry approaches. We then compare these direct estimation approaches for u i and n i with <b>analogues</b> <b>computed</b> using projection matrix asymptotics. We also discuss various extensions of the estimation approaches to multistate applications and to joint likelihoods involving multiple data types. ...|$|R
40|$|This {{article has}} been {{accepted}} for publication in Monthly Notices of the Royal Astronomical Society ©: 2015 The Authors. Published by Oxford University Press {{on behalf of the}} Royal Astronomical Society. All rights reservedWe present a set of 144 galactic chemical evolution models applied to a Milky Way <b>analogue,</b> <b>computed</b> using four sets of low and intermediate star nucleosynthetic yields, six massive star yield compilations, and six functional forms for the initial mass function. The integrated or true yields for each combination are derived. A comparison is made between a grid of multiphase chemical evolution models computed with these yield combinations and empirical data drawn from the Milky Way's disc, including the solar neighbourhood. By means of a chi 2 methodology, applied to the results of these multiphase models, the best combination of stellar yields and initial mass function capable of reproducing these observations is identifiedThis work has been supported by DGICYT grant AYA 2010 - 21887 -C 04 - 02. Also, partial support from the Comunidad de Madrid under grant CAM S 2009 /ESP- 1496 (AstroMadrid) is grateful. This work has been financially supported by the grant numbers 2010 / 18835 - 3, 2012 / 22236 - 3 and 2012 / 01017 - 1, from the Säo Paulo Research Foundation (FAPESP...|$|R
40|$|The {{users of}} <b>analogue</b> <b>computing</b> {{employed}} techniques that have important similarities {{to the ways}} scientific instruments have been used historically. <b>Analogue</b> <b>computing</b> was for many years an alternative to digital computing, and historians often frame the emergence of <b>analogue</b> <b>computing</b> as a development from various mathematical instruments. These instruments employed analogies to create artefacts that embodied some aspect of theory. Ever since the phrase 'analogue computing ' was first used in the 1940 s, a central example of analogue technology has been the planimeter, a nineteenth century scientific instrument for area calculation. The planimeter mechanism developed {{from that of the}} single instrument to become a component of much larger and more complex instruments designed by Lord Kelvin in the 1870 s, and Vannevar Bush in the 1920 s. Later definitions of computing would refer to algorithms and numerical calculation, but for Bush emphasis was placed on the cognitive support provided by the machine. He understood his “differential analyser ” to be an instrument that provided a “suggestive auxiliary to precise reasoning ” and under the label “instrumental analysis”, classified all apparatus that “aid[ed] the mind ” of the mathematician. Rather than placing emphasis on automation, an analogue computer provided an environment where th...|$|E
40|$|Classical {{computation}} {{is essentially}} local in time, yet some formulations of physics are global in time. Here I examine these differences, {{and suggest that}} certain forms of unconventional computation are needed to model physical processes and complex systems. These include certain forms of <b>analogue</b> <b>computing,</b> massively parallel field computing, and self-modifying computations...|$|E
40|$|Despite {{remarkable}} {{achievements in}} its practical tractability, the notorious class of NP-complete problems has been escaping {{all attempts to}} find a worst-case polynomial time-bound solution algorithms for any of them. The vast majority of work relies on Turing machines or equivalent models, all of which relate to digital computing. This {{raises the question of}} whether a computer that is (partly) non-digital could offer a new door towards an efficient solution. And indeed, the partition problem, which is another NP-complete sibling of the famous Boolean satisfiability problem SAT, might be open to efficient solutions using <b>analogue</b> <b>computing.</b> We investigate this hypothesis here, providing experimental evidence that Partition, and in turn also SAT, may become tractable on a combined digital and <b>analogue</b> <b>computing</b> machine. This work provides mostly theoretical and based on simulations, and as such does not exhibit a polynomial time algorithm to solve NP-complete problems. Instead, it is intended as a pointer to new directions of research on special-purpose computing architectures that may help handling the class NP efficiently...|$|E
40|$|The {{purpose of}} this paper is to {{describe}} a general procedure for <b>computing</b> <b>analogues</b> of Young’s seminormal representations of the symmetric groups. The method is to generalize the Jucys – Murphy elements in the group algebras of the symmetric groups to arbitrary Weyl groups and Iwahori – Hecke algebras. Th...|$|R
50|$|In the {{mathematics}} of signal processing, the harmonic wavelet transform, introduced by David Edward Newland in 1993, is a wavelet-based linear transformation {{of a given}} function into a time-frequency representation. It combines advantages of the short-time Fourier transform and the continuous wavelet transform. It can be {{expressed in terms of}} repeated Fourier transforms, and its discrete <b>analogue</b> can be <b>computed</b> efficiently using a fast Fourier transform algorithm.|$|R
40|$|Using a Madelung-Buckingham model, {{we study}} LixMn 2 O 4 and its fluorine-substituted <b>analogue</b> to <b>compute</b> their voltages, lattice volume changes, and {{ordering}} phenomena during charge/discharge. The interactions included are the long-range Coulombic, short-range electron-electron repulsion, and the van der Waals. The voltage of the fluorine-substituted spinel {{is found to}} be slightly less than that of the unsubstituted. However, the former undergoes a greater crystal volume change than the latter during intercalation and de-intercalation. Investigations of lithium sublattice ordering in this system indicates that during intercalation lithium starts filling exclusively into one sublattice until x) 0. 5, and only from x) 0. 5 the other sublattice is filled up to x) 1. The models are compared with quantum ab initio and experimental results...|$|R
40|$|This article {{describes}} {{the evolution of the}} design of Vannevar Bush's Memex, tracing its roots in Bush's earlier work with analog computing machines, and his understanding of the technique of associative memory. It argues that Memex was the product of a particular engineering culture, and that the machines that preceded Memex [...] -the Differential Analyzer and the Selector in particular [...] -helped engender this culture, and the discourse of <b>analogue</b> <b>computing</b> itself...|$|E
40|$|Cellular neural {{networks}} are a remarkable {{artificial neural network}} class well suited for real time image processing tasks. In fact, the parallel <b>analogue</b> <b>computing</b> feature makes them really effective in such problems which require a real time response. Moreover, the limited amount of interconnections relative to cell's neighbourhood only, lend themselves to easy VLSI implementation. In previous papers, the authors presented some CNN hardware. Therefore, in this paper, an algorithm for character recognition developed on the 9 × 9 DPCNN board is presented...|$|E
40|$|<b>Analogue</b> <b>Computing</b> Methods {{presents}} {{the field of}} analogue computation and simulation in a compact and convenient form, providing an outline of models and analogues that have been produced to solve physical problems for the engineer {{and how to use}} and program the electronic analogue computer. This book consists of six chapters. The first chapter provides an introduction to analogue computation and discusses certain mathematical techniques. The electronic equipment of an analogue computer is covered in Chapter 2, while its use to solve simple problems, including the method of scaling is elabora...|$|E
40|$|The Cellular Neural Networks (CNN) {{model is}} now a {{paradigm}} of cellular analog programmable multidimensional processor array with distributed local logic and memory. CNNs consist of many parallel <b>analogue</b> processors <b>computing</b> in real time. One desirable feature is that these processors arranged in a two dimensional grid only have local connections, which lend themselves easily to VLSI implementations. In this paper, we present a new algorithm for image segmentation using CNN. We start from a mathematical viewpoint (i. e., statistical regularization based on Markov Random Field, (MRF) and proceed by mapping the algorithm onto a cellular neural network. Because of the temporal dynamics inherent in {{the cells of the}} CNN it is well suited to processing time-varying images. A robust motion estimation algorithm is achieved by using a spatiotemporal neighborhood for modeling pixel interactions...|$|R
40|$|Semiconductor information-processing {{devices are}} among the most sophisticated, complex {{high-performance}} structures. As the construction cost of a new fabrication line approaches $ 3. 5 billion, and 25 % of its tools are obsolete and need replacement every three years, it is reasonable to question the appeal of alternate methods of information processing. Is it even realistic to imagine that an entirely distinct method of performing logical operations might be competitive? Metallic spintronic devices, such as hard disk read heads and magnetic random access memory (MRAM) {{are one of the most}} successful technologies of the past decade, with scaling trends outdoing even CMOS — can semiconductor analogues provide enough new functionality to warrant interest? Electro-optic modulators are also a successful room-temperature technology. What is the advantage of replacing these devices with magnetic <b>analogues?</b> Quantum <b>computing</b> seems to be progressing rapidly with atom...|$|R
40|$|This paper {{examines}} {{the use of}} evolutionary programming in agent-based modelling to implement the theory of bounded rationality. Evolutionary programming, which draws on Darwinian <b>analogues</b> of <b>computing</b> to create software programs, is a readily accepted means for solving complex computational problems. Evolutionary programming is also increasingly used to develop problem-solving strategies in accordance with bounded rationality, which addresses features of human decision-making such as cognitive limits, learning, and innovation. There remain many unanswered methodological and conceptual questions about the linkages between bounded rationality and evolutionary programming. This paper reports on how changing parameters in one variant of evolutionary programming, genetic programming, affects the representation of bounded rationality in software agents. Of particular interest are: the ability of agents to solve problems; limits {{to the complexity of}} agent strategies; the computational resources with which agents create, maintain, or expand strategies; {{and the extent to which}} agents balance exploration of new strategies and exploitation of old strategies. Keywords: Agent-based model; Bounded rationality; Evolutionary programs; Land chang...|$|R
40|$|What {{dynamics}} do simple recurrent networks (SRNs) {{develop to}} represent stack-like and queue-like memories? SRNs {{have been widely}} used as models in cognitive science. However, they are interesting in their own right as non-symbolic computing devices from the viewpoints of <b>analogue</b> <b>computing</b> and dynamical systems theory. In this paper, SRNs are trained oil two prototypical formal languages with recursive structures that need stack-like or queue-like memories for processing, respectively. The evolved dynamics are analysed, then interpreted in terms of simple dynamical systems, and the different ease with which SRNs aquire them is related to the properties of these simple dynamical Within the dynamical systems framework, it is concluded that the stack-like language is simpler than the queue-like language, without making use of arguments from symbolic computation theory. ...|$|E
40|$|During {{the last}} ten years, superconducting {{circuits}} and systems have passed from interesting physical devices to contenders for useful information processing in the near future. There are now advanced simulation experiments with nine qubits, and commitments to demonstrate quantum supremacy with fifty qubits within just a few years. The time is therefore ripe for providing an overview of superconducting devices and systems: to discuss {{the state of the}} art of applications to quantum information processing (QIP), and to describe recent and upcoming applications of superconducting systems to digital and <b>analogue</b> <b>computing</b> and simulation in Physics and Chemistry. On top of that, the review will try to address general questions like "What can a quantum computer do that a classical computer can't?". Comment: 94 pages, 34 figures, review articl...|$|E
40|$|Wiechert {{seismograph}} {{was designed}} by Professor Wiechert in the early this century. Vertical and two horizontal components were installed in 1930 at Abuyama Seismological Observatory, Kyoto University. We observed many earthquakes by this seismograph for the period over the half century. The instruments are, however, old and decrepit nowadays. The observation system equivalent to the wiechert seismograph was developed. New system is constructed from servo-type seismoneter, analogue electric circuits and recording systems. The servo-type seismometer responses to the ground velocity for wide frequency range. The ground displacement, resonant frequency, damping ratio equal {{to those of the}} wiechert seismograph are obtained by the <b>analogue</b> <b>computing</b> circuits which solve a second linear differential equation. Two kind recording systems are adopted, namely digital and pen-writing system. The former is trigger-compile system and the latter is drum-type one which can record continuously four days per one sheet...|$|E
40|$|We {{discuss the}} issues that arise when we add {{aggregation}} to a constraint database query language. One example {{of the use of}} aggregation in such a context is to compute the area of a region in a geographic database. We show how aggregation could be added to the query language, tuple calculus, and discuss the problems that arise from the interaction of aggregate operators and constraints. 1 Introduction [KKR 90] proposes the use of constraint query languages as a natural way of combining relational databases with constraint formalisms (see also [Kuper 90] [Revesz 90]). One important aspect of relational databases that is not discussed in that paper is the use of aggregation. Typical aggregate operators in relational databases are: Sum, Average, Count. In a spatial database, the simplest <b>analogue</b> is <b>computing</b> the area of an object. More complicated aggregate operations, such as averaging an attribute over a given region, could be useful. In this paper, we outline a method to add aggregatio [...] ...|$|R
40|$|Thesis is {{submitted}} in fulfilment of {{the requirements}} for the degree Master of Technology: Electrical Engineering in the Faculty of Engineering at the Cape Peninsula University of Technology 2013 The CubeSat standard has various engineering challenges due to its small size and surface area. The challenge is to incorporate {{a large amount of}} technology into a form factor no bigger than 10 cm 3. This research project investigates the space environment, solar cells, secondary sources of power, and Field-Programmable-Gate-Array (FPGA) technology in order to address the size, weight and power challenges presented by the CubeSat standard. As FPGAs have not yet been utilised in this particular sub-system as the main controller, this research investigates whether or not the implementation of an FPGA-based electronic power supply sub-system will optimise its functionality by overcoming these size weight and power challenges. The SmartFusion FPGA was chosen due to its analogue front end which can reduce the number of peripheral components required by such complex systems. Various maximum power point tracking algorithms were studied and it was determined that the perturb-and-observe maximum power point tracking algorithm best suits the design constraints, as it only requires the measurement of either solar cell voltage or solar cell current, thus further decreasing the component count. The SmartFusion FPGA <b>analogue</b> <b>compute</b> engine allows for increased performance of the perturb-and-observe algorithm implemented on the microcontroller sub-system as it allows for the offloading of many repetitive calculations. A VHDL implementation of the pulse-width-modulator was developed in order to produce the various changes in duty cycle produced by the perturb-and-observe algorithm. The aim of this research project was achieved through the development and testing of a nano-satellite power system prototype using the SmartFusion FPGA from Microsemi with a decreased number of peripheral circuits. Maximum power point was achieved in 347 ms at worst case with a 55 % decrease in power consumption from the estimated 330 mW as indicated in the power budget. The SmartFusion FPGA consumes only a worst case of 148. 93 mW. It was found that the unique features of the SmartFusion FPGA do in fact address the size weight and power constraints of the CubeSat standard within this sub-system...|$|R
40|$|Although several {{treatments}} {{have been}} suggested for nasal polyposis, from medical to surgical, there is no standard guideline {{for the management of}} this disease. During recent years increasing attention has been directed toward the effects of macrolide antibiotics on chronic sinusitis and nasal polyposis. In this study, the efficacy of clarithromycin on severe nasal polyposis were examined. In a Prospective, before - after study, forty patients with severe nasal polyposis received clarithromycin 500 mg twice a day for 8 weeks. At {{the beginning and end of}} treatment, the severity of patients' symptoms (using subjective <b>analogue</b> scale), <b>computed</b> tomography (CT) scan and endoscopic findings were recorded. After treatment, the severity of nasal obstruction, smelling problems, Post Nasal Discharge and rhinorrhea decreased significantly (P< 0. 05). Furthermore, the degree of sinus opacification in CT scan and endoscopic findings showed significant improvement. Most patients completed their treatment course without significant side effects. Although a course of clarithromycin improved nasal symptoms, polyp size and CT findings, further studies with more patients are required to recommend this drug as a general treatment in nasal polyposis...|$|R
40|$|Neural {{networks}} are currently implemented on digital Von Neumann machines, {{which do not}} fully leverage their intrinsic parallelism. We demonstrate {{how to use a}} novel class of reconfigurable dynamical systems for analogue information processing, mitigating this problem. Our generic hardware platform for dynamic, <b>analogue</b> <b>computing</b> consists of a reciprocal linear dynamical system with nonlinear feedback. Thanks to reciprocity, a ubiquitous property of many physical phenomena like the propagation of light and sound, the error backpropagation-a crucial step for tuning such systems towards a specific task-can happen in hardware. This can potentially speed up the optimization process significantly, offering important benefits for the scalability of neuro-inspired hardware. In this paper, we show, using one experimentally validated and one conceptual example, that such systems may provide a straightforward mechanism for constructing highly scalable, fully dynamical analogue computers. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|Microorganisms {{are able}} to respond {{effectively}} to diverse signals from their environment and internal metabolism owing to their inherent sophisticated information processing capacity. A central aim of synthetic biology is to control and reprogramme the signal processing pathways within living cells so as to realise repurposed, beneficial applications ranging from disease diagnosis and environmental sensing to chemical bioproduction. To date most examples of synthetic biological signal processing have been built based on digital information flow, though <b>analogue</b> <b>computing</b> is being developed to cope with more complex operations and larger sets of variables. Great {{progress has been made}} in expanding the categories of characterised biological components that can be used for cellular signal manipulation, thereby allowing synthetic biologists to more rationally programme increasingly complex behaviours into living cells. Here we present a current overview of the components and strategies that exist for designer cell signal processing and decision making, discuss how these have been implemented in prototype systems for therapeutic, environmental, and industrial biotechnological applications, and examine emerging challenges in this promising field...|$|E
40|$|This article {{discusses}} moment planimeters, {{which are}} mechanical devices with {{which is it}} possible to locate the centre of mass of an irregular plane shape by mechanical and graphical methods. They are a type of <b>analogue</b> <b>computing</b> device. In addition to this they may be used to find the static moment (first moment) and moment of inertia (second moment) of a shape about a fixed line. Moment planimeters, sometimes called integrometers or integrators, are direct developments of the planimeter which is a mechanical device used to directly measure the area of a plane shape. While planimeters are reasonably well known, linear planimeters are less common than the polar planimeters of Amsler. Hence in this article we explain how planimeters work through the example of a linear planimeter, and then consider how these may be adapted to find the centre of mass. More detailed comparisons between other types of area measuring planimeters may be found in the comprehensive survey article of [2]. 1 Area and centre of mass Consider the region enclosed by the closed curve in Figure 1, through which we have drawn the x-axis. We consider the area to be split into two regions by this axis, and these regions are described by the the two functions, f 1 (x) and f 2 (x). Since a general plane shape cannot be described in thi...|$|E
40|$|Reservoir {{computing}} is a bio-inspired computing {{paradigm for}} processing time dependent signals. The performance of its analogue implementations matches other digital algorithms {{on a series}} of benchmark tasks. Their potential can be further increased by feeding the output signal back into the reservoir, which would allow to apply the algorithm to time series generation. This requires, in principle, implementing a sufficiently fast readout layer for real-time output computation. Here we achieve this with a digital output layer driven by a FPGA chip. We demonstrate the first opto-electronic reservoir computer with output feedback and test it on two examples of time series generation tasks: frequency and random pattern generation. We obtain very good results on the first task, similar to idealised numerical simulations. The performance on the second one, however, suffers from the experimental noise. We illustrate this point with a detailed investigation of the consequences of noise on the performance of a physical reservoir computer with output feedback. Our work thus opens new possible applications for <b>analogue</b> reservoir <b>computing</b> and brings new insights on the impact of noise on the output feedback. info:eu-repo/semantics/publishe...|$|R
50|$|From 1943 on, Booth {{started working}} on the {{determination}} of crystal structures using X-ray diffraction data. The computations involved were extremely tedious and there was ample incentive for automating the process and he developed an <b>analogue</b> computer to <b>compute</b> the reciprocal spacings of the diffraction pattern. In 1947, along with his collaborator and future spouse Kathleen Britten, he spent a few months with von Neumann's team, which was the leading edge in computer research at the time. Booth designed an electromechanical computer, the ARC, in the late 1940s. Later on, they built an experimental electronic computer named SEC (Simple Electronic Computer), and finally the APE(X)C (All-Purpose Electronic Computer) series.|$|R
40|$|The {{purpose of}} this paper is to {{describe}} a general procedure for <b>computing</b> <b>analogues</b> of Young's seminormal representations of the symmetric groups. The method is to generalize the Jucys-Murphy elements in the group algebras of the symmetric groups to arbitrary Weyl groups and Iwahori-Hecke algebras. The combinatorics of these elements allows one to compute irreducible representations explicitly and often very easily. In this paper we do these computations for Weyl groups and Iwahori-Hecke algebras of types An, Bn, Dn, G 2. Although these computations are in reach for types F 4, E 6, and E 7, we shall, in view of the length of the current paper, postpone this to another work...|$|R
