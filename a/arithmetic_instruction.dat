30|148|Public
25|$|MIPS I has {{instructions}} for signed and unsigned integer multiplication and division. These instructions source their operands from two GPRs and write their results {{to a pair}} of 32-bit registers called HI and LO, since they may execute separately from (and concurrently with) the other CPU instructions. For multiplication, the high- and low-order halves of the 64-bit product is written to HI and LO (respectively). For division, the quotient is written to LO and the remainder to HI. To access the results, a pair of instructions (Move from HI and Move from LO) is provided to copy the contents of HI or LO to a GPR. These instructions are interlocked: reads of HI and LO do not proceed past an unfinished <b>arithmetic</b> <b>instruction</b> that will write to HI and LO. Another pair of instructions (Move to HI or Move to LO) copies the contents of a GPR to HI and LO. These instructions are used to restore HI and LO to their original state after exception handling. Instructions that read HI or LO must be separated by two instructions that do not write to HI or LO.|$|E
500|$|In practice, {{computational}} addition may {{be achieved}} via XOR and AND bitwise logical operations {{in conjunction with}} bitshift operations {{as shown in the}} pseudocode below. Both XOR and AND gates are straightforward to realize in digital logic allowing the realization of full adder circuits which in turn may be combined into more complex logical operations. In modern digital computers, integer addition is typically the fastest <b>arithmetic</b> <b>instruction,</b> yet it has the largest impact on performance, since it underlies all floating-point operations as well as such basic tasks as address generation during memory access and fetching instructions during branching. To increase speed, modern designs calculate digits in parallel; these schemes go by such names as carry select, carry lookahead, and the Ling pseudocarry. Many implementations are, in fact, hybrids of these last three designs. Unlike addition on paper, addition on a computer often changes the addends. On the ancient abacus and adding board, both addends are destroyed, leaving only the sum. The influence of the abacus on mathematical thinking was strong enough that early Latin texts often claimed that in the process of adding [...] "a number to a number", both numbers vanish. In modern times, the ADD instruction of a microprocessor often replaces the augend with the sum but preserves the addend. In a high-level programming language, evaluating [...] does not change either a or b; if the goal is to replace a with the sum this must be explicitly requested, typically with the statement [...] Some languages such as C or C++ allow this to be abbreviated as [...]|$|E
5000|$|A decimal {{overflow}} exceptionPoOps {{is recognized}} when significant digits are {{lost in a}} decimal <b>arithmetic</b> <b>instruction,</b> other than divide.|$|E
25|$|<b>Arithmetic</b> <b>instructions</b> alter {{condition}} codes only when desired.|$|R
25|$|The integer <b>arithmetic</b> <b>{{instruction}}s</b> use the integer operate instruction formats.|$|R
5000|$|RISC â€” <b>arithmetic</b> <b>instructions</b> use {{registers}} only, so explicit 2-operand load/store {{instructions are}} needed: ...|$|R
5000|$|SRAM, the [...] "{{successor}} RAM" [...] {{with only}} one <b>arithmetic</b> <b>instruction,</b> the successor (INCREMENT h). The others include [...] "CLEAR h", and an IF equality-between-register THEN jump-to xxx.|$|E
5000|$|Standard {{features}} of the Model 115 were the System/370 standard and commercial (including decimal <b>arithmetic)</b> <b>instruction</b> sets, and a [...] ""direct disk attachment" [...] for IBM 3340 disk drives. Optional features included System/370 floating point instructions, System/360 Model 20 compatibility, IBM 1401, 1440 and 1460 compatibility, and various input/output attachment options (see below).|$|E
50|$|In September 2006, NCTM {{released}} Curriculum Focal Points for Prekindergarten through Grade 8 Mathematics: A Quest for Coherence. In the Focal Points, NCTM identifies what {{it believes}} {{to be the}} most important mathematical topics for each grade level, including the related ideas, concepts, skills, and procedures that form the foundation for understanding and lasting learning. In the Focal Points, NCTM made it clear that the standard algorithms were to be included in <b>arithmetic</b> <b>instruction.</b>|$|E
5000|$|Nord-100/CE, Commercial Extended, with decimal <b>arithmetic</b> <b>{{instruction}}s</b> (The decimal {{instruction set}} was later renamed CX) ...|$|R
2500|$|... 32-bit barrel shifter {{can be used}} without {{performance}} penalty {{with most}} <b>arithmetic</b> <b>instructions</b> and address calculations.|$|R
5000|$|The {{instruction}} set is strongly orthogonal: all logic and <b>arithmetic</b> <b>instructions</b> can use all nine addressing modes: ...|$|R
50|$|A {{half-carry}} flag (also {{known as}} an auxiliary flag or decimal adjust flag) is a condition flag bit in the status register of many CPU families, such as the Intel 8080, Zilog Z80, the x86, and the Motorola 68000 series, among others. It indicates when a carry or borrow has been generated out of the least significant four bits of the accumulator register following the execution of an <b>arithmetic</b> <b>instruction.</b> It is primarily used in decimal (BCD) arithmetic instructions.|$|E
50|$|For each tick it {{is common}} to find that only some {{portions}} of the CPU are used, with the remaining groups of bits in the microinstruction being no-ops. With careful design of hardware and microcode, this property can be exploited to parallelise operations that use different areas of the CPU; for example, in the case above, the ALU is not required during the first tick, so it could potentially be used to complete an earlier <b>arithmetic</b> <b>instruction.</b>|$|E
5000|$|Assembly {{language}} programmers must {{be aware}} of hidden side effects [...] - [...] instructions that modify parts of the processor state which are not mentioned in the instruction's mnemonic. A classic example of a hidden side effect is an <b>arithmetic</b> <b>instruction</b> that implicitly modifies condition codes (a hidden side effect) while it explicitly modifies a register (the overt effect). One potential drawback of an instruction set with hidden side effects is that, if many instructions have side effects on a single piece of state, like condition codes, then the logic required to update that state sequentially may become a performance bottleneck. The problem is particularly acute on some processors designed with pipelining (since 1990) or with out-of-order execution. Such a processor may require additional control circuitry to detect hidden side effects and stall the pipeline if the next instruction depends on the results of those effects.|$|E
50|$|The {{instructions}} {{may have}} up to three operands. A major drawback {{is that there are}} no floating-point or decimal <b>arithmetic</b> <b>instructions.</b>|$|R
50|$|There is a {{specific}} set of <b>arithmetic</b> <b>instructions</b> that use a different interpretation of the bits in word as a floating-point number.|$|R
2500|$|Arithmetic: <b>arithmetic</b> <b>instructions</b> may {{operate on}} all {{registers}} or {{on just a}} special register (e.g. accumulator). They are usually chosen from the following sets (but exceptions abound): ...|$|R
50|$|The i960CA, first {{announced}} in July 1989, {{was the first}} pure RISC implementation of the i960 architecture. It featured a newly designed superscalar RISC core and added an unusual addressable on-chip cache, but lacked an FPU and MMU, as it was intended for high-performance embedded applications. The i960CA is widely considered {{to have been the}} first single-chip superscalar RISC implementation. The C-series only included one ALU, but could dispatch and execute an <b>arithmetic</b> <b>instruction,</b> a memory reference, and a branch instruction at the same time, and sustain two instructions per cycle under certain circumstances. The first versions released ran at 33 MHz, and Intel promoted the chip as capable of 66 MIPS. The i960CA microarchitecture was designed in 1987-1988 and formally announced on September 12, 1989. Later, in May 1992, the i960CF included larger instruction cache (4 KB instead of 1 KB) and added 1 KB of data cache, but continued to omit any MMU or FPU.|$|E
5000|$|In practice, comutational {{addition}} may achieved via XOR and AND bitwise logical {{operations in}} conjunction with bitshift operations {{as shown in the}} pseudocode below. Both XOR and AND gates are straightforward to realize in digital logic allowing the realization of full adder circuits which in turn may be combined into more complex logical operations. In modern digital computers, integer addition is typically the fastest <b>arithmetic</b> <b>instruction,</b> yet it has the largest impact on performance, since it underlies all floating-point operations as well as such basic tasks as address generation during memory access and fetching instructions during branching. To increase speed, modern designs calculate digits in parallel; these schemes go by such names as carry select, carry lookahead, and the Ling pseudocarry. Many implementations are, in fact, hybrids of these last three designs. Unlike addition on paper, addition on a computer often changes the addends. On the ancient abacus and adding board, both addends are destroyed, leaving only the sum. The influence of the abacus on mathematical thinking was strong enough that early Latin texts often claimed that in the process of adding [...] "a number to a number", both numbers vanish. In modern times, the ADD instruction of a microprocessor often replaces the augend with the sum but preserves the addend. In a high-level programming language, evaluating a + b does not change either a or b; if the goal is to replace a with the sum this must be explicitly requested, typically with the statement a = a + b. Some languages such as C or C++ allow this to be abbreviated as a += b.|$|E
40|$|The {{aim of this}} {{exploratory}} {{study was to investigate}} the quality of arithmetic education for children with cerebral palsy. The use of individual educational plans, amount of <b>arithmetic</b> <b>instruction</b> time, arithmetic instructional grouping, and type of arithmetic teaching method were explored in three groups: children with cerebral palsy (CP) in special (CP-special; n = 41) and mainstream schools (CP-mainstream; n = 16) and a control group in mainstream schools (n = 16). The majority of individual educational plans did not include well-formulated arithmetic goals and many were not based on optimal assessment. Special schools scheduled much less <b>arithmetic</b> <b>instruction</b> time. Many CP-mainstream children received individualized instruction, which may help to explain why their arithmetic performance did not differ from controls. Remedial arithmetic teaching methods used in special schools {{did not seem to be}} optimal, but more research is required. Suggestions to improve arithmetic education to children with CP were discussed...|$|E
5000|$|Floating point - <b>arithmetic</b> <b>instructions</b> {{supported}} an eight-digit mantissa and two-digit characteristic (offset exponent) - MMMMMMMMCC, {{providing a}} range of Â±0.00000001E-50 to Â±0.99999999E+49. (7 extra operation codes) ...|$|R
50|$|The TurboSPARC was {{a simple}} scalar in-order design. During the fetch stage, two {{instructions}} were fetched from a 16 KB direct-mapped instruction cache. During the decode stage, one instruction was decoded, and its operands read from its register file. Execution began in stage three. The TurboSPARC had an integer unit and a floating-point unit. Most integer <b>arithmetic</b> <b>instructions</b> except for multiply and divide have a single-cycle latency. Multiply and divide was executed by the FPU. Multiply had a seven cycle latency while divide had an 8- to 33-cycle latency. Most floating-point <b>arithmetic</b> <b>instructions</b> except for divide and square-root had a four-cycle latency.|$|R
25|$|Count Extensions (CIX) was an {{extension}} to the architecture which introduced three instructions for counting bits. These instructions {{were categorized as}} integer <b>arithmetic</b> <b>instructions.</b> They were first implemented on the Alpha 21264A (EV67).|$|R
40|$|The aim of {{this study}} was to {{establish}} whether children with a physical disability resulting from central nervous system disorders (CNSd) show a level of arithmetic achievement lower than that of non-CNSd children and whether this is related to poor automaticity of number facts or reduced <b>arithmetic</b> <b>instruction</b> time. Twenty-two children with CNSd (M age = 10. 7 years old) were compared with two groups of typically developing children: a same-aged group (n = 21) and a younger group (by 1 year) matched on arithmetic achievement (n = 23). Although their intelligence was in the average range, the arithmetic achievement level of the CNSd group lagged nearly 1. 5 years behind their typically developing peers. There was no strong evidence that this was due to a specific automaticity deficit. However, the difference on an arithmetic achievement test between the CNSd group and same-aged control group was fully accounted for by the difference in hours of <b>arithmetic</b> <b>instruction.</b> Â© 2009 Hammill Institute on Disabilities...|$|E
40|$|The {{development}} of addition and subtraction accuracy was assessed in first graders {{with cerebral palsy}} (CP) in both mainstream (16) and special education (41) and a control group of first graders in mainstream education (16). The control group out-performed the CP groups in addition and subtraction accuracy and this difference could not be fully explained by differences in intelligence. Both CP groups showed evidence of working memory deficits. The three groups exhibited different developmental patterns {{in the area of}} early numeracy skills. Children with CP in special education were found to receive less <b>arithmetic</b> <b>instruction</b> and instruction time was positively related to arithmetic accuracy. Structural equation modeling revealed that the effect of CP on arithmetic accuracy is mediated by intelligence, working memory, early numeracy, and instruction time...|$|E
40|$|This paper {{considers}} encrypted computation {{where the}} user specifies encrypted inputs to an untrusted program, and the server computes on those encrypted inputs. To this end we propose a secure processor architecture, called Ascend, that guarantees privacy of data when arbitrary programs use the data {{running in a}} cloud-like environment (e. g., an untrusted server running an untrusted software stack). The key idea to guarantee privacy is obfuscated instruction execution; Ascend does not disclose what instruction is being run at any given time, be it an <b>arithmetic</b> <b>instruction</b> or a memory instruction. Periodic accesses to external instruction and data memory are performed through an Oblivious RAM (ORAM) interface to prevent leakage through memory access patterns. We evaluate the processor architecture on SPEC benchmarks running on encrypted data and quantify overheads...|$|E
5000|$|... x86 {{assembly}} has {{the standard}} mathematical operations, , , , with the logical operators , , , bitshift arithmetic and logical, /, /; rotate {{with and without}} carry, /, /, a complement of BCD <b>arithmetic</b> <b>instructions,</b> , , [...] and others.|$|R
50|$|The decimal <b>arithmetic</b> feature {{provides}} <b>instructions</b> {{that operate}} on packed decimal data. A packed decimal number has 1-31 decimal digits {{followed by a}} 4-bit sign. All of the decimal <b>arithmetic</b> <b>instructions</b> except PACK and UNPACK generate a Data exception if a digit {{is not in the}} range 0-9 or a sign is not in the range A-F.|$|R
5000|$|... needs four {{instructions}}. For stack machines, {{the terms}} [...] "0-operand" [...] and [...] "zero-address" [...] apply to <b>arithmetic</b> <b>instructions,</b> {{but not to}} all instructions, as 1-operand push and pop instructions are used to access memory.|$|R
40|$|A dedicated, non-symbolic, system {{yielding}} imprecise {{representations of}} large quantities (Approximate Number System, or ANS) {{has been shown}} to support arithmetic calculations of addition and subtraction. In the present study, 5 â€“ 7 -year-old children without formal schooling in multiplication and division were given a task requiring a scalar transformation of large approximate numerosities, presented as arrays of objects. In different conditions, the required calculation was doubling, quadrupling, or increasing by a fractional factor (2. 5). In all conditions, participants were able to represent the outcome of the transformation at above-chance levels, even on the earliest training trials. Their performance could not be explained by processes of repeated addition, and it showed the critical ratio signature of the ANS. These findings provide evidence for an untrained, intuitive process of calculating multiplicative numerical relationships, providing a further foundation for formal <b>arithmetic</b> <b>instruction...</b>|$|E
40|$|Multiplicative Binary Moment Diagram (*BMD) {{is a new}} {{representation}} that inherits from BDD. *BMDs map Boolean {{variables to}} integer values {{and so they are}} well-suited to handle a hierarchical verification methodology. In this paper we experiment this representation to verify sequential arithmetic circuits. We extend *BMDs and provide an algebra that includes main operators commonly used at the bitvector level. We illustrate the paper with the proof of a sequential multiplication instruction. 1 Introduction 1. 1 Motivations Binary Moment Diagram (*BMD) is a new symbolic representation that has been used to verify combinational arithmetic circuits [7, 8]. We propose in this paper a first [...] to our knowledge [...] experiment in using *BMDs to formally verify sequential circuits. More precisely, we use *BMDs in a microprocessor verification framework [1, 2] when other methods like symbolic simplification or BDDs are not efficient. This is the case when verifying <b>arithmetic</b> <b>instruction</b> [...] ...|$|E
40|$|Multiple memory banks {{design is}} {{employed}} in many high performance DSP processors. This architectural feature supports higher memory bandwidth by allowing multiple data memory access {{to be executed}} in parallel. Dedicated address generation units (AGUs) are commonly presented in DSPs to perform address arithmetic in parallel to the main datapath. Address assignment, optimization of memory layout of program variables to reduce address <b>arithmetic</b> <b>instruction,</b> has been studied extensively on single memory architecture. Make effective use of AGUs on multiple memory banks is a great challenge to compiler design {{and has not been}} studied previously. In this paper, we exploit address assignment with variable partitioning for scheduling on DSP architectures with multiple memory banks and AGUs. Our approach is built on novel graph models which capture both parallelism and serialism demands. An efficient scheduling algorithm, Address Assignment Sensitive Variable Partitioning (AASVP), is proposed to best leverage both multiple memory banks and AGUs. Experimental results show significant improvement compare to existing methods...|$|E
50|$|<b>Arithmetic</b> <b>instructions</b> {{except for}} compares have a four-cycle latency. Single and double {{precision}} divides have latencies of 14 and 20 cycles, respectively; and single and double precision square-roots have latencies of 14 and 23 cycles, respectively.|$|R
50|$|Arithmetic was ones' complement, {{so there}} were two forms of zero: {{positive}} zero and negative zero. The A and Q register could function as a combined 48-bit register for certain <b>arithmetic</b> <b>instructions.</b> The E register had 48 bits.|$|R
40|$|DSP {{processors}} have address generation {{units that}} can perform address computation {{in parallel with}} other operations. This feature reduces explicit address <b>arithmetic</b> <b>instructions,</b> often required to access locations in the stack frame, through auto-increment and decrement addressing modes, thereby decreasing the code size. Decreasing code size in embedded applications is extremely important as it directly impacts the size of on-chip program memory and hence {{the cost of the}} system. Effective utilization of auto-increment and decrement modes requires an intelligent placement of variables in the stack frame which is termed as "offset assignment". Although a number of algorithms for efficient offset assignment have been proposed in the literature, they do not consider possible instruction reordering {{to reduce the number of}} address <b>arithmetic</b> <b>instructions.</b> In this paper, we propose an integrated approach that combines instruction reordering and algebraic transformations to reduce the number of address <b>arithmetic</b> <b>instructions.</b> The proposed approach has been implemented in the SUIF compiler framework. We conducted our experiments on a set of real programs. and compared its performance with that of Liao's heuristic for Simple Offset Assignment (SOA), Tie-break SOA, Naive offset assignment, and Rao and Pande's algebraic transformation approach...|$|R
