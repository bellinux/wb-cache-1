24|253|Public
5000|$|Foveated imaging {{has been}} used to {{simulate}} visual fields with arbitrary spatial resolution. For example, one may present video containing a blurred region representing a scotoma. By utilizing <b>an</b> <b>eye-tracker</b> <b>and</b> holding the blurred region fixed relative to the viewer's gaze, the viewer will have a visual experience similar to that of a person with an actual scotoma. The figure on the right shows a frame from a simulation of a glaucoma patient with the eye fixated on the word [...] "similar." ...|$|E
40|$|By {{means of}} <b>an</b> <b>eye-tracker</b> <b>and</b> a home-made {{software}} of analysis, we {{have investigated the}} relationship among {{the ability of the}} humans to memorize and recognize images (and their modifications), the characteristics of the testers (strategy of exploration, cultural background, sex, etc [...] .) and the construction of the image. Author Keywords Visual memory, human factors, eye-tracking, huma...|$|E
40|$|International audienceAn {{investigation}} of the eye and hand movement during the task of signature simulation was conducted. Three subjects' eye movements and hand movements were recorded using <b>an</b> <b>eye-tracker</b> <b>and</b> a digitizing tablet while they simulated signatures. The study revealed that eye gaze most frequently shifted within less than 17 msec of a pen velocity minimum. It is thought that the cognitive processes overseeing this movement control and {{the limitations of the}} visuomotor buffer could {{play an important role in}} the behaviour of simulating signatures and signature simulation quality...|$|E
50|$|The {{technical}} {{principle of}} the paradigm involves a computer interfaced with both an eye-movement tracking system (<b>Eye-tracker)</b> <b>and</b> <b>a</b> display of the visual stimulus. <b>A</b> fast computer, <b>eye-tracker</b> <b>and</b> display allow reliable results (Veneri 2010, Pomplun 2001). In gaze-contingent paradigms the stimulus display is continuously updated {{as a function of}} the observers' current gaze position; for instance, Shimojo & Simion 2003 applied a central hole to see thescene only through the fovea, giving to subjects the sensation of seeing through a telescope.|$|R
40|$|First prototypes of {{bi-directional}} OLED microdisplay {{devices that}} combine both display and camera functionality {{on a single}} CMOS chip (OLED-on-CMOS) have been designed. The major goal of this integration is to provide capabilities for eye-tracking in see-through HMDs to achieve gaze-based human-display interaction, e. g., in augmented-reality applications. The development of the prototype was accompanied by user studies with a simulated bi-directional microdisplay consisting of <b>a</b> commercially available <b>eye-tracker</b> <b>and</b> <b>a</b> see- through HMD. These tests were aimed at providing basic minimum requirements in terms of temporal and spatial resolution of <b>an</b> <b>eye-tracker</b> to be implemented within the prototype, {{as well as to}} evaluate ergonomics of an appropriate user-interface design. A description of {{the current state of the}} hardware architecture and design aspects for bi-directional OLED microdisplays are also presented...|$|R
40|$|Analysis of eye {{movements}} {{recorded with}} <b>a</b> mobile <b>eye-tracker</b> is difficult since the eye-tracking data are severely affected by simultaneous head and body movements. Automatic analysis methods developed for remote-, <b>and</b> tower-mounted <b>eye-trackers</b> {{do not take}} this into account and are therefore not suitable to use for data where also head- and body movements are present. As a result, data recorded with <b>a</b> mobile <b>eye-tracker</b> are often analyzed manually. In this work, we investigate how simultaneous recordings of eye- and head movements can be employed to isolate {{the motion of the}} eye in the eye-tracking data. We recorded eye-in-head movements with <b>a</b> mobile <b>eye-tracker</b> <b>and</b> head movements with an Inertial Measurement Unit (IMU). Preliminary results show that by compensating the eye-tracking data with the estimated head orientation, the standard deviation of the data during vestibular-ocular reflex (VOR) eye movements, was reduced from 8. 0 to 0. 9 in the vertical direction and from 12. 9 to 0. 6 in the horizontal direction. This suggests that a head compensation algorithm based on IMU data can be used to isolate the movements of the eye and therefore simplify the analysis of data recorded using <b>a</b> mobile <b>eye-tracker...</b>|$|R
40|$|AbstractThis paper {{presents}} a software environment for a human-aware ambient agent providing {{support for a}} human performing an attention-demanding task. The agent obtains human attention-awareness by use of a dynamical model of human attention, gaze sensoring by <b>an</b> <b>eye-tracker,</b> <b>and</b> information about features of the objects in the environment. It has been implemented in a component-based, event-driven manner within the Adobe®Flex®environment, thereby also integrating the Tobii®eye-tracker. It has been applied in a setup for a task where the human has to identify enemies and allies, and eliminate the enemies...|$|E
40|$|This paper {{presents}} a software environment for a human-aware ambient agent providing {{support for a}} human performing a demanding task that requires attention. The agent obtains human attention-awareness by use of a dynamical model of human attention, gaze sensoring by <b>an</b> <b>eye-tracker,</b> <b>and</b> information about features of the objects in the environment. It has been implemented in a component-based, event-driven manner within the Adobe ® Flex ® environment, thereby also integrating the Tobii ® eye-tracker. It has been applied in a setup for a task where the human has to identify enemies and allies, and eliminate the enemies...|$|E
40|$|This paper {{presents}} a software environment providing human-aware ambient {{support for a}} human performing a task that demands substantial amounts of attention. The agent obtains human attention-awareness in an adaptive manner by use of a dynamical model of human attention, gaze sensoring by <b>an</b> <b>eye-tracker,</b> <b>and</b> information about features of the objects in the environment which is parameterised for characteristics of the human specified above. The agent uses a built-in adaptation model to adapt on the fly, the values of these parameters to the personal characteristics of the human. The software agent has been implemented in a component-based manner within the Adob...|$|E
40|$|Video-based {{eye-tracking}} {{systems are}} especially suited to studying eye movements during naturally occurring {{activities such as}} locomotion, but eye velocity records suffer from broad band noise that is not amenable to conventional filtering methods. We evaluated the effectiveness of combined median and moving-average filters by comparing prefiltered and postfiltered records made synchronously with <b>a</b> video <b>eye-tracker</b> <b>and</b> the magnetic search coil technique, which is relatively noise free. Root-mean-square noise was reduced by half, without distorting the eye velocity signal. To illustrate the practical use of this technique, we studied normal subjects and patients with deficient labyrinthine function and compared their ability to hold gaze on a visual target that moved with their heads (cancellation of the vestibulo-ocular reflex). Patients and normal subjects performed similarly during active head rotation but, during locomotion, patients held their eyes more steadily on the visual target than did subjects...|$|R
40|$|Intensive care nurses {{care for}} {{critically}} ill {{patients in the}} cognitively demanding intensive care environment. While doing so, nurses make discretionary use of memory aids to ensure the safe completion of future tasks (Grundgeiger et al., 2009). In a full-scale patient simulator using a representative scenario, we manipulated {{the presence or absence}} of reminders for different prospective memory (PM) tasks, expecting increased PM performance with reminders. In addition, we observed when and how nurses used artifacts and investigated how these artifacts effected remembering of intentions. Twenty-four nurses participated in a 40 minute scenario of a morning shift start which involved eight PM tasks. The use of a simulator and a scripted scenario enabled us to control the initial exposure to the PM tasks but allowed nurses to make discretionary use of artifacts. Scenarios were AV-recorded with room cameras <b>and</b> <b>a</b> mobile <b>eye-tracker</b> <b>and</b> followed up by a semi-structured interview. Results show that: (1) only reminders that had the specific function of reminding increased performance; (2) nurses changed the cognitive demands of PM tasks by using artifacts such as written notes; and (3) reminders can change interruption management behavior and retrieval processes of PM tasks. The results give detailed insights in the use and effect of reminders...|$|R
40|$|Purpose: To {{investigate}} {{the effect of}} increasing navigation speed on (a) readers’ visual search and (b) decision-making during polyp identification for CT colonography (CTC). 2 Methods: Ethical permission was granted for this prospective study. Following informed consent, twelve CTC fly-through examinations (depicting 8 polyps) were presented at four different fixed navigation speeds to 23 radiologists. Speeds ranged from 1 cm/s to 4. 5 cm/s. Gaze position was tracked using <b>an</b> infra-red <b>eye-tracker,</b> <b>and</b> readers indicated seeing a polyp by clicking a mouse. Patterns of search and decision-making by speed were investigated graphically and by multi-level modelling. Results: Readers identified polyps correctly in 73 % of viewings at the slowest speed but only 61 % of viewings at the fastest (p= 0. 004). They also identified fewer false positive features at faster speeds (37 % of videos at slowest speed, 26 % at fastest, p= 0. 02). Gaze location was highly concentrated towards the central quarter of the screen area at faster speeds (mean 86 % of gaze points at slowest speed, 97 % at fastest speed). Conclusions: Faster navigation speed at endoluminal CTC leads to progressive restriction of visual search patterns. Greater speed also reduces both true-positive and false-positive colorectal polyp identification...|$|R
40|$|This paper {{describes}} experiments {{performed with}} 40 subjects wearing <b>an</b> <b>eye-tracker</b> <b>and</b> watching and imitating videos of finger, hand, and arm movements. For {{all types of}} stimuli, the subjects tended to fixate on the hand, {{regardless of whether they}} were imitating or just watching. The results lend insight into the connection between visual perception and motor control, suggesting that: Ž. 1 people analyze human arm movements largely by tracking the hand or the end-point, even if the movement is performed with the entire arm, and Ž. 2 when imitating, people use internal innate and learned models of movement, possibly in the form of motor primitives, to recreate th...|$|E
40|$|This master thesis report {{describes}} {{the work of}} evaluating the approach of using <b>an</b> <b>eye-tracker</b> <b>and</b> machine learning to generate an interaction model for clicks. In the study, recordings were done from 10 participants using a quiz application, and machine learning was then applied. Models were created with varying quality from a machine learning view, although most models did not work well for interaction. One model was created that enable correct interaction 80 % of the time, although the specific circumstances for success were not identified. The conclusion of the thesis is that the approach works in some cases, but that more {{research needs to be}} done to evaluate general suitability, and approaches to make it work reliably...|$|E
40|$|The shared {{translation}} {{task of the}} Workshop of Statistical Machine Translation (WMT) {{is one of the}} key annual {{events of}} the field. Participating machine translation systems in WMT translation task are manually evaluated by relatively ranking five candidate translations of a given sentence. This style of evaluation has been used since 2007 with some discussion on interpreting the collected judgements but virtually no insight into what the annotators are actually doing. The scoring task is relatively cognitively demanding and many scoring strategies are possible, influencing the reliability of the final judgements. In this paper, we describe our first steps towards explaining the scoring task: we run the scoring under <b>an</b> <b>eye-tracker</b> <b>and</b> monitor what the annotators do. At the current stage, our results are more of a proof-of-concept, testing the feasibility of eye tracking for the analysis of such a complex MT evaluation setup...|$|E
40|$|De Beugher S., Ichiche Y., Brône G., Goedemé T., ''Automatic {{analysis}} of eye-tracking data using object detection algorithms'', Proceedings 2 nd international workshop on pervasive eye tracking and mobile eye-based interaction - PETMEI 2012, {{in conjunction with}} UbiComp 2012, pp. 677 - 680, September 8, 2012, Pittsburgh, Pennsylvania, USA. In this paper we investigate the integration of object detection algorithms with eye-tracking data. The emerging technology of lightweight mobile eye-trackers enables realistic in-the-wild user experience experiments. Unfortunately, mobile <b>eye-trackers</b> generate <b>a</b> large amount of video data, which up to now requires manual analysis. This time-consuming and repetitive task renders processing large datasets economically infeasible. Our main contribution {{is the use of}} object detection algorithms to perform this analysis task automatically. We compare several object detection algorithms with regard to both speed and accuracy. To prove their functionality, we have recorded <b>an</b> <b>eye-tracker</b> shopping experiment <b>and</b> processed the data using object detection techniques. status: publishe...|$|R
40|$|Multiscreening, the {{simultaneous}} usage of multiple screens, {{is a relatively}} understudied phenomenon that may have a large impact on media effects. First, we explored people's viewing behavior while multiscreening by means of <b>an</b> <b>eye-tracker.</b> Second, we examined people's reporting of attention, by comparing <b>eye-tracker</b> <b>and</b> self-reported attention measures. Third, we assessed the effects of multiscreening on people's memory, by comparing people's memory for editorial and advertising content when multiscreening (television–tablet) versus single screening. The results of the experiment (N = 177) show that (a) people switched between screens 2. 5 times per minute, (b) people were capable of reporting their own attention, and (c) multiscreeners remembered content {{just as well as}} single screeners, when they devoted sufficient attention to the content...|$|R
40|$|The {{precision}} of <b>an</b> <b>eye-tracker</b> {{is critical to}} the correct identification of eye movements and their properties. To measure a system’s precision, artificial eyes (AEs) are often used, to exclude eye movements influencing the measurements. A possible issue, however, is that {{it is virtually impossible to}} construct AEs with sufficient complexity to fully represent the human eye. To examine the consequences of this limitation, we tested currently used AEs from three manufacturers of <b>eye-trackers</b> <b>and</b> compared them to a more complex model, using 12 commercial eye-trackers. Because precision can be measured in various ways, we compared different metrics in the spatial domain and analyzed the power-spectral densities in the frequency domain. To assess how precision measurements compare in artificial and human eyes, we also measured precision using human recordings on the same eye-trackers. Our results show that the modified eye model presented can cope with all <b>eye-trackers</b> tested <b>and</b> acts as a promising candidate for further development of a set of AEs with varying pupil size and pupil–iris contrast. The spectral analysis of both the AE and human data revealed that human eye data have different frequencies that likely reflect the physiological characteristics of human eye movements. We also report the effects of sample selection methods for precision calculations. This study is part of the EMRA/COGAIN Eye Data Quality Standardization Project...|$|R
40|$|We {{present a}} {{methodology}} for characterizing the gaze behavior of viewers of animated displays. We introduce a transition graph called Viewing Behavior Model Graph (VBMG) {{to characterize the}} behavior of users with similar viewing patterns into separate groups. We apply this methodology to the viewing of program visualizations. In this method the user’s eye-fixation sequences are obtained using <b>an</b> <b>Eye-Tracker</b> <b>and</b> per-user viewing behavior models are created. We then cluster these per-user models to build VBMGs for each cluster. The VBMGs are useful because they help us classify users into separate groups, each user within a group having viewing behavior similar {{to others in the}} group. One useful application of VBMGs would be to dynamically capture viewing behavior and predict the cluster to which a user belongs, thus permitting on-the-fly adaptation of displays and other teaching materials. INDEX WORDS: vbmg, viewing behavior model graph, clustering, program visualizationVIEWING BEHAVIOR MODEL GRAPHS (VBMG) FOR CHARACTERIZING USE...|$|E
40|$|International audienceThis paper {{presents}} the results of two psychophysical experiments and an associated computational analysis designed to quantify the relationship between visual salience and visual importance. In the first experiment, importance maps were collected by asking human subjects to rate the relative visual importance of each object within a database of hand-segmented images. In the second experiment, experimental saliency maps were computed from visual gaze patterns measured for these same images by using <b>an</b> <b>eye-tracker</b> <b>and</b> task-free viewing. By comparing the importance maps with the saliency maps, we found that the maps are related, but perhaps less than one might expect. When coupled with the segmentation information, the saliency maps were shown to be effective at predicting the main subjects. However, the saliency maps were less effective at predicting the objects of secondary importance and the unimportant objects. We also found {{that the vast majority of}} early gaze position samples (0 - 2000 ms) were made on the main subjects, suggesting that a possible strategy of early visual coding might be to quickly locate the main subject(s) in the scene...|$|E
40|$|This study {{concerns}} {{the processes of}} eye movements during simple sight-reading tasks, concentrating on the processing of melodic groups separated by larger intervals {{and the effects of}} metrical placement of such groups. Thirty-two participants with varying musical background sight-read eight short melodies in two separate sessions. Eye movements during playing were recorded using <b>an</b> <b>eye-tracker,</b> <b>and</b> the performances on an electric piano with sequencer software. The melodies in question were of two types: melodic groupings were either aligned with the metrical divisions or made to overlap with them. The results indicate that reinspections to previously fixated notes during sight-reading increased with previous musical experience. During the performance of a single melody, the alignment of melodic groups with metrical units led to a gradual decrease of reinspections. Within the melodic groups themselves, the second notes of the groups received the largest total processing times. In addition, processing differences for notes involved in larger melodic intervals emerged depending on the visual placement of the notes within, or below, the staff system. Possible explanations of these findings as well as implications for future sight-reading studies will be discussed...|$|E
40|$|The {{emerging}} eye-tracking {{technique has}} opened a window of opportunities not only in medical research but also in Translation and Interpreting Studies. In recent years this research method {{has been used to}} trace the processes of reading, translation and interpreting. Eye-tracking has recently become a popular technique to examine cognitive effort involved in written translation, audiovisual translation and conference interpreting. Thanks to the use of <b>an</b> <b>eye-tracker</b> one is able to investigate the whole process and not limit oneself to analysing the quality of the output. To be more precise, by means of eye-tracking experimenters may investigate moment-by-moment changes in the cognitive effort necessary to perform a given translation/interpreting task. Useful as the eye-tracking technique may be, researchers must often face methodological and apparatus-related challenges. The present paper is intended to discuss the eye-tracking methodology and then to address the potential problems of applying this method to investigate the processes of translation and interpreting. Among the notions to be discussed are: the types of <b>eye-trackers</b> <b>and</b> their usability, accuracy vs. ecological validity, accommodation (O'Brien 2010), sampling, the use of inferential statistics for small experimental groups as well as ethics. I will also refer to my own research on the notion of language-pair specificity in sight translation (Korpal 2012) as well as a collaborative work on numerical data processing in simultaneous interpreting (Korpal and Stachowiak, manuscript in preparation) ...|$|R
40|$|To {{efficiently}} deploy eye-tracking within 3 D graphics applications, {{we present}} a new probabilistic method that predicts the patterns of user’s eye fixations in animated 3 D scenes from noisy eye-tracker data. The proposed method utilises both the <b>eye-tracker</b> data <b>and</b> the known information about the 3 D scene to improve the accuracy, robustness and stability. Eye-tracking can thus be used, for example, to induce focal cues via gaze-contingent depth-of-field rendering, add intuitive controls to a video game, and create a highly reliable scene-aware saliency model. The computed probabilities rely on {{the consistency of the}} gaze scan-paths to the position and velocity of a moving or stationary target. The temporal characteristic of eye fixations is imposed by a Hidden Markov model, which steers the solution towards the most probable fixation patterns. The derivation of the algorithm is driven by the data from two eye-tracking experiments: the first experiment provides actual <b>eye-tracker</b> readings <b>and</b> the position of the target to be tracked. The second experiment is used to derive a JND-scaled (Just Noticeable Difference) quality metric that quantifies the perceived loss of quality due to the errors of the tracking algorithm. Data from both experiments are used to justify design choices, and to calibrate and validate the tracking algorithms. This novel method outperforms commonly used fixation algorithms and is able to track objects smaller then the nominal error of <b>an</b> <b>eye-tracker...</b>|$|R
40|$|International audienceIn this paper, we firstly present what is Interactive Evolutionary Computation (IEC) {{and rapidly}} {{how we have}} {{combined}} this artificial intelligence technique with <b>an</b> <b>eye-tracker</b> for visual optimization. Next, in order to correctly parameterize our application, we present results from applying data mining techniques on gaze information coming from experiments conducted on about 80 human individuals...|$|R
40|$|We {{present a}} model that predicts saccadic eye-movements and can be tuned to a {{particular}} human observer who is viewing a dynamic sequence of images. Our work is motivated by applications that involve gaze-contingent interactive displays on which information is displayed {{as a function of}} gaze direction. The approach therefore differs from standard approaches in two ways: (i) we deal with dynamic scenes, and (ii) we provide means of adapting the model to a particular observer. As an indicator for the degree of saliency we evaluate the intrinsic dimension of the image sequence within a geometric approach implemented by using the structure tensor. Out of these candidate saliencybased locations, the currently attended location is selected according to a strategy found by supervised learning. The data are obtained with <b>an</b> <b>eye-tracker</b> <b>and</b> subjects who view video sequences. The selection algorithm receives candidate locations of current and past frames and a limited history of locations attended in the past. We use a linear mapping that is obtained by minimizing the quadratic difference between the predicted and the actually attended location by gradient descent. Being linear, the learned mapping can be quickly adapted to the individual observer. Keywords: Eye-movements, saccades, saliency map, intrinsic dimension, machine learning, gaze-contingent displa...|$|E
40|$|The current {{research}} investigated whether video-based training could improve the hazard perception of learners and non-drivers. Participants were randomly allocated to a training or control condition. Within each group, {{half of the}} participants engaged in a distracter task to simulate the cognitive load associated with being a novice driver. Gaze direction was monitored with <b>an</b> <b>eye-tracker</b> <b>and</b> participants 2 ̆ 7 first fixation point and response reaction times were measured in relation to the appearance of each traffic hazard on the computer screen, both pre- and post-training. In line with hypotheses, hazard perception training significantly improved fixation and reaction time responses, and significantly reduced the processing interval between fixation and response production. However, against predictions, engaging in a distracter task did not significantly impede fixation or response reaction times, and did not {{have a significant effect on}} the interval between first fixation and response production. Furthermore, no support was found for the prediction that the distracter task would substantially attenuate the training effect, but not eliminate it entirely, as no group differences reached significance. These findings are discussed in relation to novice drivers 2 ̆ 7 situation awareness, with particular reference to the perception, comprehension, and projection components of working memory. Potential implications for graduated licencing systems are also discussed...|$|E
40|$|Attentional biases {{arising from}} {{classical}} conditioning processes {{may contribute to}} the maintenance of drug addictions and anxiety disorders. This thesis examined whether attentional mechanisms for conditioned stimuli (CS) would be dominated by affective properties (Lang, Greenwald, Bradley, & Hamm, 1993), or the uncertainty of the stimulus in predicting the outcome (Pearce & Hall, 1980). In chapter one affective and uncertainty-driven mechanisms of attention are discussed in relation to rewarding and aversive outcomes. In experimental chapter 2 methodological issues are addressed. In experimental chapters three and four attentional mechanisms are tested using a discriminative conditioning procedure with visual stimuli of varying predictive certainty (CS+,CS+/-,CS-) for a monetary or noise outcome (US). Attention was measured using <b>an</b> <b>eye-tracker,</b> <b>and</b> emotional conditioning and learning were measured using Likert scales. It was found that attention was mediated by uncertainty (chapter 3), but increasing the intensity of the outcome switched attention to affective-driven mechanisms for the noise outcome (chapter 4). In a further experiment this effect on attention remained for the noise outcome even under conditions promoting uncertainty-driven mechanisms (chapter 6). When cigarettes were the unconditioned stimuli instead of money in the appetitive conditioning, attention was also mediated by stimulus affect (chapter 5). In chapter 7 the data are discussed and it is concluded that when the outcome is highly emotionally salient, affective-driven mechanisms of attention dominate over uncertainty. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|In this paper, we {{describe}} a new algorithm that consists in combining <b>an</b> <b>eye-tracker</b> for minimizing the fatigue of a user during {{the evaluation process}} of Interactive Evolutionary Computation. The approach is then applied to the Interactive One-Max optimization problem. Categories and Subject Descriptors D. 3. 3 [Programming Languages]: Language Contructs and Features – abstract data types, polymorphism, control structures...|$|R
40|$|In natural course, {{human beings}} usually {{make use of}} multi-sensory {{modalities}} for effective communication or efficiently executing day-to-day tasks. For instance, during verbal conversations we make use of voice, eyes, and various body gestures. Also effective human-computer interaction involves hands, eyes, and voice, if available. Therefore by combining multi-sensory modalities, {{we can make the}} whole process more natural and ensure enhanced performance even for the disabled users. Towards this end, we have developed a multi-modal human-computer interface (HCI) by combining <b>an</b> <b>eye-tracker</b> with <b>a</b> soft-switch which may be considered as typically representing another modality. This multi-modal HCI is applied for text entry using a virtual keyboard appropriately designed in-house, facilitating enhanced performance. Our experimental results demonstrate that using multi-modalities for text entry through the virtual keyboard is more efficient and less strenuous than single modality system and also solves the Midas-touch problem, which is inherent in <b>an</b> <b>eye-tracker</b> based HCI system where only dwell time is used for selecting a character...|$|R
25|$|A study {{published}} in the conference PPIG evaluated the effects of syntax highlighting on the comprehension of short programs, finding that the presence of syntax highlighting significantly reduces the time taken for a programmer to internalise the semantics of a program. Additionally, data gathered from <b>an</b> <b>eye-tracker</b> during the study suggested that syntax highlighting enables programmers to pay less attention to standard syntactic components such as keywords.|$|R
40|$|Most {{research}} on language in autism spectrum disorders (ASD) has focussed {{on social and}} communicative impairments, whereas grammatical impairments are less explored. This study delves into the syntactical knowledge and processing of passives in children with high-functioning autism (HFA) and aims to answer the research question “Do children and adults with HFA comprehend and process passive sentences differently from typically developing children and adults?” Four groups, comprising children with HFA (N= 2; age 8. 52 - 8. 75), TD children (N= 13; age 6. 78 - 11. 07), adults with HFA (N= 3, age 23. 23 - 25. 92), and TD adults (N= 6; age 22. 63 - 29. 65) were tested on their processing of passives through reading of a sentence (recorded by <b>an</b> <b>eye-tracker)</b> <b>and</b> comprehension (picture-selection) of long actional passives. Results suggest that children and adults with HFA have a good comprehension of long actional passive sentences, as they perform at ceiling level. The TD adults answered 100 % correctly too, and the TD children selected 90. 4 % of the correct pictures. The eye-tracking results are insufficient {{to comment on the}} processing of passive sentences, because the collection of eye-tracking data was unsuccessful. The comprehension results are discussed in comparison to three recent studies on passives in Farsi-, Greek-, and English-speaking children (Heshmati, 2013; Terzi et al. 2014; Perovic, unpublished manuscript). Options for the analysis of complete eye-tracking data and suggestions for further research are discussed...|$|E
40|$|International audienceThe {{aim of this}} {{research}} is to understand the difference in visual attention to 2 D and 3 D content depending on texture and amount of depth. Two experiments were conducted using <b>an</b> <b>eye-tracker</b> <b>and</b> a 3 DTV display. Collected fixation data were used to build saliency maps and to analyze the differences between 2 D and 3 D conditions. In the first experiment 51 observers participated in the test. Using scenes that contained objects with crossed disparity, it was discovered that such objects are the most salient, even if observers experience discomfort due to the high level of disparity. The goal of the second experiment is to decide whether depth is a determinative factor for visual attention. During the experiment, 28 observers watched the scenes that contained objects with crossed and uncrossed disparities. We evaluated features influencing the saliency of the objects in stereoscopic conditions by using contents with low-level visual features. With univariate tests of significance (MANOVA), it was detected that texture is more important than depth for selection of objects. Objects with crossed disparity are significantly more important for selection processes when compared to 2 D. However, objects with uncrossed disparity have the same influence on visual attention as 2 D objects. Analysis of eye movements indicated that there is no difference in saccade length. Fixation durations were significantly higher in stereoscopic conditions for low-level stimuli than in 2 D. We believe that these experiments can help to refine existing models of visual attention for 3 D content...|$|E
40|$|International audienceThe usual event-related {{potential}} (ERP) estimation is {{the average}} across epochs time-locked on stimuli of interest. These stimuli are repeated several times to improve the signal-to-noise ratio (SNR) and only one evoked potential is estimated inside the temporal window of interest. Consequently, the average estimation {{does not take into}} account other neural responses within the same epoch that are due to short inter stimuli intervals. These adjacent neural responses may overlap and distort the evoked potential of interest. This overlapping process is a significant issue for the eye fixation-related potential (EFRP) technique in which the epochs are time-locked on the ocular fixations. The inter fixation intervals are not experimentally controlled and can be shorter than the neural response’s latency. To begin, the Tikhonov regularization, applied to the classical average estimation, was introduced to improve the SNR for a given number of trials. The generalized cross validation was chosen to obtain the optimal value of the ridge parameter. Then, to deal with the issue of overlapping, the general linear model (GLM), was used to extract all neural responses inside an epoch. Finally, the regularization was also applied to it. The models (the classical average and the GLM with and without regularization) were compared on both simulated data and real datasets from a visual scene exploration in co-registration with <b>an</b> <b>eye-tracker,</b> <b>and</b> from a P 300 Speller experiment. The regularization was found to improve the estimation by average for a given number of trials. The GLM was more robust and efficient, its efficiency actually reinforced by the regularization...|$|E
40|$|The {{knowledge}} of the visual strategies adopted while walking in cognitively engaging environments is extremely valuable. Analyzing gaze when a treadmill and a virtual reality environment are used as motor rehabilitation tools is therefore critical. Being completely unobtrusive, remote eye-trackers are the most appropriate way to measure the point of gaze. Still, the point of gaze measurements are affected by experimental conditions such as head range of motion and visual stimuli. This study assesses the usability limits and measurement reliability of <b>a</b> remote <b>eye-tracker</b> during treadmill walking while visual stimuli are projected. During treadmill walking, the head remained within the remote eye-tracker workspace. Generally, {{the quality of the}} point of gaze measurements declined as the distance from the remote <b>eye-tracker</b> increased <b>and</b> data loss occurred for large gaze angles. The stimulus location (a dot-target) did not influence the point of gaze accuracy, precision, and trackability during both standing and walking. Similar results were obtained when the dot-target was replaced by a static or moving 2 D target and “region of interest” analysis was applied. These findings foster the feasibility of the use of <b>a</b> remote <b>eye-tracker</b> for the analysis of gaze during treadmill walking in virtual reality environments...|$|R
40|$|This master {{thesis is}} done at CrossControl, {{a company in}} the {{automation}} industry. A literature review was done covering {{state of the art}} technology of eye-tracking and gaze control and how it can be used in the automation industry. One purpose with the study was to indentify features important for <b>an</b> <b>eye-tracker</b> in automation applications. Findings suggests that there are no standard methods for measuring the performance of trackers and further that performance in regard to gaze control is limited by physiology of the eye rather than limitations of the devices. The differences between trackers are found in robustness when it comes to differences in people or conditions in the environment. A prototype was made for demonstration. The prototype was built on equipment used in the industry combined with <b>an</b> <b>eye-tracker</b> from Tobii. <b>A</b> library, where coordinates from the eye-tracker were used to indicate what object in the interface was gazed upon, was implemented and used to demonstrate concepts such as buttons outside the screen activated by gaze...|$|R
40|$|Introduction. In this paper, {{we propose}} a new {{approach}} for developing a naïve ontology {{as the basis for}} optimal information access interfaces for multimedia digital documents intended for novice users. Method. We try to elicit the knowledge structure of domain novices and patterns of its modification in their searching and learning processes by <b>eye-tracker</b> <b>and</b> showing eye-movements in the post-search interviews. Analysis. Recorded interview data were fully transcribed and coded using Atlas. ti and analysed following a bottom-up strategy of the constant-comparative technique. Results. We developed a taxonomy of knowledge modification which includes (1) adding, (2) correcting, (3) limiting, (4) relating, (5) specifying and (6) transforming. Conclusion. The taxonomy may be expanded and elaborated as the project progress and findings are expected to be incorporated into the design of the naïve ontology. The study results provided theoretical implications on knowledge building, methodological implications on data collection using <b>eye-tracker</b> <b>and</b> showing eye-movements in the post-search interviews and useful information on the design of information access interface for novices users...|$|R
