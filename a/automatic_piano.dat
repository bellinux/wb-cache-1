13|28|Public
2500|$|Photoplayer, an <b>automatic</b> <b>piano</b> and {{orchestra}} used by smaller theatres to accompany silent films ...|$|E
60|$|They dined extravagantly {{at their}} hotel at night, and next morning sneaked {{round the corner}} to economize at a Childs' Restaurant. They were tired by three in the afternoon, and dozed at the motion-pictures and said they wished {{they were back in}} Gopher Prairie--and by eleven in the evening they were again so lively that they went to a Chinese {{restaurant}} that was frequented by clerks and their sweethearts on pay-days. They sat at a teak and marble table eating Eggs Fooyung, and listened to a brassy <b>automatic</b> <b>piano,</b> and were altogether cosmopolitan.|$|E
5000|$|Photoplayer, an <b>automatic</b> <b>piano</b> and {{orchestra}} used by smaller theatres to accompany silent films ...|$|E
5000|$|Keith Prowse & Co Ltd, Manufacturer of <b>Automatic</b> Machines, <b>Automatic</b> <b>Pianos</b> and Musical Instruments, Effects Machines, Bioscope Entertainment Provider, (38, Berners Street) 1909 ...|$|R
50|$|The Nisco Museum of Mechanical Music in Ein Hod is {{the first}} museum in Israel {{dedicated}} to antique musical instruments. The collection, accumulated over 40 years by Nisan Cohen, contains music boxes, hurdy-gurdies, an automatic organ, a reproducing player piano, a collection of 100-year-old manivelles, gramophones, hand-operated <b>automatic</b> <b>pianos</b> and other instruments.|$|R
50|$|From 1880 through 1892, {{he focused}} on <b>automatic</b> player <b>piano</b> mechanisms, the free reed organette.|$|R
50|$|Ed Link {{used the}} bellows {{technology}} from the <b>automatic</b> <b>piano</b> in his Link Trainer flight simulators.|$|E
50|$|A piano roll is a {{specific}} type of music roll, and is designed to operate an <b>automatic</b> <b>piano</b> like the player piano or the reproducing piano.|$|E
5000|$|The Mechanical Organ Museum is {{situated}} {{at the north end}} of the village, on the road to Lea and Ross-on-Wye. It has been called [...] "a unique collection of mechanical music spanning the last 150 years, hidden away on the edge of the Forest of Dean. Mechanical organs, polyphons, pianola, <b>automatic</b> <b>piano,</b> electronic organs & musical boxes".|$|E
25|$|In 1804, Joseph-Marie Jacquard {{developed}} a loom {{in which the}} pattern being woven was controlled by a paper tape constructed from punched cards. The paper tape could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punched cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for <b>automatic</b> <b>pianos</b> and more recently numerical control machine tools.|$|R
6000|$|As {{they drove}} {{back through the}} {{outskirts}} of Monarch, down streets of small brown wooden cottages of workmen, characterless as cells, as they rattled across warehouse-districts which by drunken night seemed vast and perilous, as they were borne toward the red lights and violent <b>automatic</b> <b>pianos</b> and the stocky women who simpered, Babbitt was frightened. He wanted to leap from the taxicab, but all his body was a murky fire, and he groaned, [...] "Too late to quit now," [...] and knew {{that he did not}} want to quit.|$|R
5000|$|... #Caption: Mills Novelty Company's <b>automatic</b> {{violin and}} <b>piano</b> player ...|$|R
50|$|The Star Theater, {{which was}} the oldest movie house in Binghamton, {{was the first to}} use the Link <b>automatic</b> <b>piano</b> to provide music for its silent films. As larger and more {{elaborate}} movie theatres were built, and more versatile organs were demanded, the company began manufacturing theatre pipe organs. At this time, the company's name was expanded to the Link Piano and Organ Company.|$|E
5000|$|As a young man, Edwin Link used {{apparatus}} {{from his}} father's <b>automatic</b> <b>piano</b> and organ factory (of the Link Piano and Organ Company) {{to produce an}} advertising airplane. A punched roll and pneumatic system from a player piano controlled sequential lights on the lower surfaces of the wings to spell out messages like [...] "ENDICOTT-JOHNSON SHOES". To attract more attention, he added a set of small but loud organ pipes, also controlled by the roll.|$|E
40|$|Rules for legato, staccato, and {{repeated}} notes articulation in <b>automatic</b> <b>piano</b> performance are presented. These rules were obtained applying the analysis-by-measurement method to performances of Mozart piano sonatas on MIDIfied acoustic grand pianos. The effects {{produced by the}} rules vary with tempo indications, with expressive intentions and with musical context. The articulation rules {{are part of the}} Director Musices system for automatic music performance. It has been demonstrated that proper setting of the articulation rules enables more realistic automatic performances that can also differ in emotional expression. These rules are also being applied in control models for sound synthesis algorithms. ...|$|E
5000|$|The Aeolian Company {{was founded}} by New York City piano maker William B. Tremaine as the Æolian Organ & Music Co. (1887) to make {{automatic}} organs and, after 1895, as the Æolian Co. <b>automatic</b> <b>pianos</b> as well. He had previously founded the Mechanical Orguinette Co. in 1878 to manufacture automated reed organs. The manufacture of residence or [...] "chamber" [...] organs to provide entertainment in the mansions of millionaires was an extremely profitable undertaking, and Aeolian virtually cornered the market in this trade, freeing them from the tight competition of church-organ building with its narrow profit margins. Elaborate cases and consoles were often featured in residence organs. In other installations, the pipes were hidden behind tapestries, under or above staircases, or spoke from the basement through grilles or tone chutes. They also made Organettes and Player Pump Organs for the [...] "Working Man" [...] to buy.|$|R
25|$|During the 1920s, Hickman {{then worked}} for the American Piano Company (also known as Ampico), {{improving}} the company's player piano products. The inventor of the Ampico piano, Charles Fuller Stoddard, needed a physicist and mathematician to develop improvements to reproducing instruments as well as manufacturing aspects of <b>automatic</b> <b>pianos.</b> He employed Hickman in this role at a research laboratory that Ampico established in 1924 at the Chickering Hall in New York. Hickman's work enabled the development of Ampico's dynamic recording machine and the Model 'B' player piano. Author Larry Givens wrote: Dr. Hickman's employment with American Piano Company, from 1924 {{through the end of}} 1929, may accurately be said to represent the only period {{in the history of the}} player piano industry in which real scientific methodology was applied to the development of the player piano. Most development work in the industry had theretofore consisted of scratch-paper sketches and empirical constructing of models with hopes that they would function! For his achievements at Ampico, Hickman was inducted into the Automatic Musical Instruments Collector's Association Hall of Fame in 1976.|$|R
6000|$|Here--she meditated--is {{the newest}} empire of the world; the Northern Middlewest; {{a land of}} dairy herds and {{exquisite}} lakes, of new automobiles and tar-paper shanties and silos like red towers, of clumsy speech and a hope that is boundless. An empire which feeds {{a quarter of the}} world--yet its work is merely begun. They are pioneers, these sweaty wayfarers, for all their telephones and bank-accounts and <b>automatic</b> <b>pianos</b> and co-operative leagues. And for all its fat richness, theirs is a pioneer land. What is its future? she wondered. A future of cities and factory smut where now are loping empty fields? Homes universal and secure? Or placid chateaux ringed with sullen huts? Youth free to find knowledge and laughter? Willingness to sift the sanctified lies? Or creamy-skinned fat women, smeared with grease and chalk, gorgeous in the skins of beasts and the bloody feathers of slain birds, playing bridge with puffy pink-nailed jeweled fingers, women who after much expenditure of labor and bad temper still grotesquely resemble their own flatulent lap-dogs? The ancient stale inequalities, or something different in history, unlike the tedious maturity of other empires? What future and what hope? ...|$|R
40|$|In {{order to}} achieve human-like {{computer}} performances, the difference features between real performances and musical scores must be investigated. It {{is also important to}} construct a performance system by taking account of performer's individuality. This paper addresses a method to derive a normative performance data from multiple performances and a strategy to extract performance rules by using each performer's data and normative data. To confirm normative performance rules extracted from multiple performances, <b>automatic</b> <b>piano</b> performances synthesized by the proposed method are compared with performers' data. The possibility of extracting individual traits of performers is also discussed by comparing synthesized performance data with each performance data. 1...|$|E
40|$|In this paper, a score-informed {{transcription}} {{method for}} <b>automatic</b> <b>piano</b> tutoring is proposed. The method takes as input a recording {{made by a}} student which may contain mistakes, along with a reference score. The recording and the aligned synthesized score are automatically transcribed using the non-negative matrix factorization algorithm for multi-pitch estimation and hidden Markov models for note tracking. By comparing the two transcribed recordings, common errors occurring in transcription algorithms such as extra octave notes can be suppressed. The result is a piano-roll description which shows the mistakes made by the student along with the correctly played notes. Evaluation was performed on six pieces recorded using a Disklavier piano, using both manually-aligned and automatically-aligned scores as an input. Results comparing the system output with ground-truth annotation of the original recording reach a weighted F-measure of 93 %, indicating that the proposed method can successfully analyze the student's performance...|$|E
40|$|PhDAutomatic music {{transcription}} is {{the process}} of converting an audio recording into a symbolic representation using musical notation. It has numerous applications in music information retrieval, computational musicology, and the creation of interactive systems. Even for expert musicians, transcribing polyphonic pieces of music is not a trivial task, and while the problem of automatic pitch estimation for monophonic signals is considered to be solved, the creation of an automated system able to transcribe polyphonic music without setting restrictions on the degree of polyphony and the instrument type still remains open. In this thesis, research on automatic transcription is performed by explicitly incorporating information on the temporal evolution of sounds. First efforts address the problem by focusing on signal processing techniques and by proposing audio features utilising temporal characteristics. Techniques for note onset and offset detection are also utilised for improving transcription performance. Subsequent approaches propose transcription models based on shift-invariant probabilistic latent component analysis (SI-PLCA), modeling the temporal evolution of notes in a multiple-instrument case and supporting frequency modulations in produced notes. Datasets and annotations for transcription research have also been created during this work. Proposed systems have been privately as well as publicly evaluated within the Music Information Retrieval Evaluation eXchange (MIREX) framework. Proposed systems have been shown to outperform several state-of-the-art transcription approaches. Developed techniques have also been employed for other tasks related to music technology, such as for key modulation detection, temperament estimation, and <b>automatic</b> <b>piano</b> tutoring. Finally, proposed music transcription models have also been utilized in a wider context, namely for modeling acoustic scenes...|$|E
40|$|Abstract. In {{this paper}} we {{investigate}} {{on the use}} locally recurrent neural networks (LRNN), trained by a discriminative learning approach, for <b>automatic</b> polyphonic <b>piano</b> music transcription. Due to polyphonic characteristic of the input signal standard discriminative learning (DL) is not adequate and a suitable modification, called multi-classification discriminative learning (MCDL), is introduced. The automatic music transcription architecture presented in the paper is composed by a preprocessing unit which performs a constant Q Fourier transform such that the signal is represented in both time and frequency domain, followed by a peak-peaking and decision blocks: the last built with a LRNN. In order to demonstrate {{the effectiveness of the}} proposed MCDL for LRNN several experiments have been carried out. ...|$|R
40|$|In this paper, {{a number}} {{theoretical}} method is {{developed for the}} purpose of analyzing the spectre of a mixture of harmonic sounds. The method is based on the properties of prime numbers and on non-linear filtering. It is shown that a number theoretical approach is of vital importance in order to detect and observe harmonic sounds in musical polyphonies. The method is verified by applying it to the <b>automatic</b> transcription of <b>piano</b> music. ...|$|R
2500|$|Under the {{ownership}} of Kimball, Bösendorfer built and sold {{a small number of}} 290SE <b>automatic</b> reproducing <b>pianos.</b> [...] The 'SE' designation was for Stahnke Engineering, whose founder, Wayne Stahnke, invented the mechanism. The 290 was fitted with electronics and mechanics to record on magnetic tape and playback through electro-mechanical actuation of the piano. After the release of the Microsoft Windows v3.1 operating system, the 290SE could be attached to a PC computer for recording, editing, and playback. The 290SE system was the first commercially available computer-controlled [...] "player piano" [...] capable of accurately reproducing both the notes and intensity of a performer's playing. [...] Unfortunately this system was not further developed or patented due to its high cost. [...] Competitors soon introduced patented reproducing piano technologies such as the Yamaha Disklavier in 1982.|$|R
40|$|Editors: Alexander Refsum Jensenius, Anders Tveit, Rolf Inge Godøy, Dan Overholt Table of Contents -Tellef Kvifte: Keynote Lecture 1 : Musical Instrument User Interfaces: the Digital Background of the Analog Revolution - page 1 -David Rokeby: Keynote Lecture 2 : Adventures in Phy-gital Space - page 2 -Sergi Jordà: Keynote Lecture 3 : Digital Lutherie and Multithreaded Musical Performance: Artistic, Scientific and Commercial Perspectives - page 3 Paper session A — Monday 30 May 11 : 00 – 12 : 30 -Dan Overholt: The Overtone Fiddle: an Actuated Acoustic Instrument - page 4 -Colby Leider, Matthew Montag, Stefan Sullivan and Scott Dickey: A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications - page 8 -Greg Shear and Matthew Wright: The Electromagnetically Sustained Rhodes Piano - page 14 -Laurel Pardue, Christine Southworth, Andrew Boch, Matt Boch and Alex Rigopulos: Gamelan Elektrika: An Electronic Balinese Gamelan - page 18 -Jeong-Seob Lee and Woon Seung Yeo: Sonicstrument: A Musical Interface with Stereotypical Acoustic Transducers - page 24 Poster session B— Monday 30 May 13 : 30 – 14 : 30 -Scott Smallwood: Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies - page 28 -Niklas Klügel, Marc René Frieß and Georg Groh: An Approach to Collaborative Music Composition - page 32 -Nicolas Gold and Roger Dannenberg: A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems - page 36 -Mark Bokowiec: V’OCT (Ritual) : An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization - page 40 -Florent Berthaut, Haruhiro Katayose, Hironori Wakama, Naoyuki Totani and Yuichi Sato: First Person Shooters as Collaborative Multiprocess Instruments - page 44 -Tilo Hähnel and Axel Berndt: Studying Interdependencies in Music Performance: An Interactive Tool - page 48 -Sinan Bokesoy and Patrick Adler: 1 city 1001 vibrations: {{development}} of a interactive sound installation with robotic instrument performance - page 52 -Tim Murray-Browne, Di Mainstone, Nick Bryan-Kinns and Mark D. Plumbley:The medium is the message: Composing instruments and performing mappings - page 56 -Seunghun Kim, Luke Keunhyung Kim, Songhee Jeong and Woon Seung Yeo: Clothesline as a Metaphor for a Musical Interface - page 60 -Pietro Polotti and Maurizio Goina: EGGS in action - page 64 -Berit Janssen: A Reverberation Instrument Based on Perceptual Mapping - page 68 -Lauren Hayes: Vibrotactile Feedback-Assisted Performance - page 72 -Daichi Ando: Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure - page 76 -Ryan McGee, Yuan-Yi Fan and Reza Ali: BioRhythm: a Biologically-inspired Audio-Visual Installation - page 80 -Jon Pigott: Vibration, Volts and Sonic Art: A practice and theory of electromechanical sound - page 84 -George Sioros and Carlos Guedes: Automatic Rhythmic Performance in Max/MSP: the kin. rhythmicator - page 88 -Andre Goncalves: Towards a Voltage-Controlled Computer — Control and Interaction Beyond an Embedded System - page 92 -Tae Hun Kim, Satoru Fukayama, Takuya Nishimoto and Shigeki Sagayama: Polyhymnia: An <b>automatic</b> <b>piano</b> performance system with statistical modeling of polyphonic expression and musical symbol interpretation - page 96 -Juan Pablo Carrascal and Sergi Jorda: Multitouch Interface for Audio Mixing - page 100 -Nate Derbinsky and Georg Essl: Cognitive Architecture in Mobile Music Interactions - page 104 -Benjamin D. Smith and Guy E. Garnett: The Self-Supervising Machine - page 108 -Aaron Albin, Sertan Senturk, Akito Van Troyer, Brian Blosser, Oliver Jan and Gil Weinberg: Beatscape, a mixed virtual-physical environment for musical ensembles - page 112 -Marco Fabiani, Gaël Dubus and Roberto Bresin: MoodifierLive: Interactive and collaborative expressive music performance on mobile devices - page 116 -Benjamin Schroeder, Marc Ainger and Richard Parent: A Physically Based Sound Space for Procedural Agents - page 120 -Francisco Garcia, Leny Vinceslas, Esteban Maestre and Josep Tubau Acquisition and study of blowing pressure profiles in recorder playing - page 124 -Anders Friberg and Anna Källblad:Experiences from video-controlled sound installations - page 128 -Nicolas d’Alessandro, Roberto Calderon and Stefanie Müller: ROOM# 81 —Agent-Based Instrument for Experiencing Architectural and Vocal Cues - page 132 Demo session C — Monday 30 May 13 : 30 – 14 : 30 -Yasuo Kuhara and Daiki Kobayashi: Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices - page 136 -Christopher Carlson, Eli Marschner and Hunter Mccurry: The Sound Flinger: A Haptic Spatializer - page 138 -Ravi Kondapalli and Benzhen Sung: Daft Datum – an Interface for Producing Music Through Foot-Based Interaction - page 140 -Charles Martin and Chi-Hsia Lai: Strike on Stage: a percussion and media performance - page 142 Paper session D — Monday 30 May 14 : 30 – 15 : 30 -Baptiste Caramiaux, Patrick Susini, Tommaso Bianco, Frédéric Bevilacqua, Olivier Houix, Norbert Schnell and Nicolas Misdariis: Gestural Embodiment of Environmental Sounds: an Experimental Study - page 144 -Sebastian Mealla, Aleksander Valjamae, Mathieu Bosi and Sergi Jorda: Listening to Your Brain: Implicit Interaction in Collaborative Music Performances - page 149 -Dan Newton and Mark Marshall: Examining How Musicians Create Augmented Musical Instruments - page 155 Paper session E — Monday 30 May 16 : 00 – 17 : 00 -Zachary Seldess and Toshiro Yamada: Tahakum: A Multi-Purpose Audio Control Framework - page 161 -Dawen Liang, Guangyu Xia and Roger Dannenberg: A Framework for Coordination and Synchronization of Media - page 167 -Edgar Berdahl and Wendy Ju: Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform - page 173 Paper session F — Tuesday 31 May 09 : 00 – 10 : 50 -Nicholas J. Bryan and Ge Wang: Two Turntables and a Mobile Phone - page 179 -Nick Kruge and Ge Wang: MadPad: A Crowdsourcing System for Audiovisual Sampling - page 185 -Patrick O’Keefe and Georg Essl: The Visual in Mobile Music Performance - page 191 -Ge Wang, Jieun Oh and Tom Lieber: Designing for the iPad: Magic Fiddle - page 197 -Benjamin Knapp and Brennon Bortz: MobileMuse: Integral Music Control Goes Mobile - page 203 -Stephen Beck, Chris Branton, Sharath Maddineni, Brygg Ullmer and Shantenu Jha: Tangible Performance Management of Grid-based Laptop Orchestras - page 207 Poster session G— Tuesday 31 May 13 : 30 – 14 : 30 -Smilen Dimitrov and Stefania Serafin: Audio Arduino—an ALSA (Advanced Linux Sound Architecture) audio driver for FTDI-based Arduinos - page 211 -Seunghun Kim and Woon Seung Yeo: Musical control of a pipe based on acoustic resonance - page 217 -Anne-Marie Hansen, Hans Jørgen Andersen and Pirkko Raudaskoski: Play Fluency in Music Improvisation Games for Novices - page 220 -Izzi Ramkissoon: The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance - page 224 -Ajay Kapur, Michael Darling, James Murphy, Jordan Hochenbaum, Dimitri Diakopoulos and Trimpin: The KarmetiK NotomotoN: A New Breed of Musical Robot for Teaching and Performance - page 228 -Adrian Barenca Aliaga and Giuseppe Torre: The Manipuller: Strings Manipulation and Multi-Dimensional Force Sensing - page 232 -Alain Crevoisier and Cécile Picard-Limpens: Mapping Objects with the Surface Editor - page 236 -Jordan Hochenbaum and Ajay Kapur: Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces - page 240 -Andrew Milne, Anna Xambó, Robin Laney, David B. Sharp, Anthony Prechtl and Simon Holland: Hex Player—A Virtual Musical Controller - page 244 -Carl Haakon Waadeland: Rhythm Performance from a Spectral Point of View - page 248 -Josep M Comajuncosas, Enric Guaus, Alex Barrachina and John O’Connell: Nuvolet : 3 D gesture-driven collaborative audio mosaicing - page 252 -Erwin Schoonderwaldt and Alexander Refsum Jensenius: Effective and expressive movements in a French-Canadian fiddler’s performance - page 256 -Daniel Bisig, Jan Schacher and Martin Neukom: Flowspace – A Hybrid Ecosystem - page 260 -Marc Sosnick and William Hsu: Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs - page 264 -Axel Tidemann: An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation - page 268 -Luke Dahl, Jorge Herrera and Carr Wilkerson: TweetDreams: Making music with the audience and the world using real-time Twitter data - page 272 -Lawrence Fyfe, Adam Tindale and Sheelagh Carpendale: JunctionBox: A Toolkit for Creating Multi-touch Sound Control Interfaces - page 276 -Andrew Johnston: Beyond Evaluation: Linking Practice and Theory in New Musical Interface Design - page 280 -Phillip Popp and Matthew Wright: Intuitive Real-Time Control of Spectral Model Synthesis - page 284 -Pablo Molina, Martin Haro and Sergi Jordà: BeatJockey: A new tool for enhancing DJ skills - page 288 -Jan Schacher and Angela Stoecklin: Traces – Body, Motion and Sound - page 292 -Grace Leslie and Tim Mullen: MoodMixer: EEG-based Collaborative Sonification - page 296 -Ståle A. Skogstad, Kristian Nymoen, Yago de Quay and Alexander Refsum Jensenius: OSC Implementation and Evaluation of the Xsens MVN suit - page 300 -Lonce Wyse, Norikazu Mitani and Suranga Nanayakkara: The effect of visualizing audio targets in a musical listening and performance task - page 304 -Freed Adrian, John Maccallum and Andrew Schmeder: Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP - page 308 -Kristian Nymoen, Ståle A. Skogstad and Alexander Refsum Jensenius: SoundSaber —A Motion Capture Instrument - page 312 -Øyvind Brandtsegg, Sigurd Saue and Thom Johansen: A modulation matrix for complex parameter sets - page 316 Demo session H— Tuesday 31 May 13 : 30 – 14 : 30 -Yu-Chung Tseng, Che-Wei Liu, Tzu-Heng Chi and Hui-Yu Wang: Sound Low Fun- page 320 -Edgar Berdahl and Chris Chafe: Autonomous New Media Artefacts (AutoNMA) - page 322 -Min-Joon Yoo, Jin-Wook Beak and In-Kwon Lee: Creating Musical Expression using Kinect - page 324 -Staas de Jong: Making grains tangible: microtouch for microsound - page 326 Baptiste Caramiaux, Frederic Bevilacqua and Norbert Schnell: Sound Selection by Gestures - page 329 Paper session I — Tuesday 31 May 14 : 30 – 15 : 30 -Hernán KerlleÃevich, Manuel Eguia and Pablo Riera: An Open Source Interface based on Biological Neural Networks for Interactive Music Performance - page 331 -Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping - page 337 -Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: A Machine Learning Toolbox For Musician Computer Interaction - page 343 Paper session J — Tuesday 31 May 16 : 00 – 17 : 00 -Elena Jessop, Peter Torpey and Benjamin Bloomberg: Music and Technology in Death and the Powers - page 349 -Victor Zappi, Dario Mazzanti, Andrea Brogni and Darwin Caldwell: Design and Evaluation of a Hybrid Reality Performance - page 355 -Jérémie Garcia, Theophanis Tsandilas, Carlos Agon and Wendy Mackay: InkSplorer : Exploring Musical Ideas on Paper and Computer - page 361 Paper session K — Wednesday 1 June 09 : 00 – 10 : 30 -Pedro Lopes, Alfredo Ferreira and Joao Madeiras Pereira: Battle of the DJs: an HCI perspective of Traditional, Virtual, Hybrid and Multitouch DJing - page 367 -Adnan Marquez-Borbon, Michael Gurevich, A. Cavan Fyans and Paul Stapleton: Designing Digital Musical Interactions in Experimental Contexts - page 373 -Jonathan Reus: Crackle: A mobile multitouch topology for exploratory sound interaction - page 377 -Samuel Aaron, Alan F. Blackwell, Richard Hoadley and Tim Regan: A principled approach to developing new languages for live coding - page 381 -Jamie Bullock, Daniel Beattie and Jerome Turner: Integra Live: a new graphical user interface for live electronic music - page 387 Paper session L — Wednesday 1 June 11 : 00 – 12 : 30 -Jung-Sim Roh, Yotam Mann, Adrian Freed and David Wessel: Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers - page 393 -Mark Marshall and Marcelo Wanderley: Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument - page 399 -Dimitri Diakopoulos and Ajay Kapur: HIDUINO: A firmware for building driverless USB-MIDI devices using the Arduino microcontroller - page 405 -Emmanuel Flety and Côme Maestracci: Latency improvement in sensor wireless transmission using IEEE 802. 15. 4 - page 409 -Jeff Snyder: The Snyderphonics Manta, a Novel USB Touch Controller - page 413 Poster session M — Wednesday 1 June 13 : 30 – 14 : 30 -William Hsu: On Movement, Structure and Abstraction in Generative Audiovisual Improvisation - page 417 -Claudia Robles Angel: Creating Interactive Multimedia Works with Bio-data - page 421 -Paula Ustarroz: TresnaNet: musical generation based on network protocols - page 425 -Matti Luhtala, Tiina Kymäläinen and Johan Plomp: Designing a Music Performance Space for Persons with Intellectual Learning Disabilities - page 429 -Tom Ahola, Teemu Ahmaniemi, Koray Tahiroglu, Fabio Belloni and Ville Ranki: Raja —A Multidisciplinary Artistic Performance - page 433 -Emmanuelle Gallin and Marc Sirguy: Eobody 3 : A ready-to-use pre-mapped & multi-protocol sensor interface- page 437 -Rasmus Bååth, Thomas Strandberg and Christian Balkenius: Eye Tapping: How to Beat Out an Accurate Rhythm using Eye Movements - page 441 -Eric Rosenbaum: MelodyMorph: A Reconfigurable Musical Instrument - page 445 -Karmen Franinovic: Flo) (ps: Between Habitual and Explorative Action-Sound Relationships - page 448 -Margaret Schedel, Rebecca Fiebrink and Phoenix Perry: Wekinating 000000 Swan: Using Machine Learning to Create and Control Complex Artistic Systems - page 453 -Carles F. Julià, Daniel Gallardo and Sergi Jordà: MTCF: A framework for designing and coding musical tabletop applications directly in Pure Data - page 457 -David Pirrò and Gerhard Eckel: Physical modelling enabling enaction: an example - page 461 -Thomas Mitchell and Imogen Heap: SoundGrasp: A Gestural Interface for the Performance of Live Music - page 465 -Tim Mullen, Richard Warp and Adam Jansch: Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface - page 469 -Stefano Papetti, Marco Civolani and Federico Fontana: Rhythm’n’Shoes: a wearable foot tapping interface with audio-tactile feedback - page 473 -Cumhur Erkut, Antti Jylhä and Reha Di¸sçio˘glu: A structured design and evaluation model with application to rhythmic interaction displays - page 477 -Marco Marchini, Panos Papiotis, Alfonso Perez and Esteban Maestre: A Hair Ribbon Deflection Model for Low-Intrusiveness Measurement of Bow Force in Violin Performance - page 481 -Jonathan Forsyth, Aron Glennon and Juan Bello: Random Access Remixing on the iPad - page 487 -Erika Donald, Ben Duinker and Eliot Britton: Designing the EP trio: Instrument identities, control and performance practice in an electronic chamber music ensemble - page 491 -Cavan Fyans and Michael Gurevich: Perceptions of Skill in Performances with Acoustic and Electronic Instruments - page 495 -Hiroki Nishino: Cognitive Issues in Computer Music Programming - page 499 -Roland Lamb and Andrew Robertson: Seaboard: a new piano keyboard-related interface combining discrete and continuous control - page 503 -Gilbert Beyer and Max Meier: Music Interfaces for Novice Users: Composing Music on a Public Display with Hand Gestures - page 507 -Birgitta Cappelen and Anders-Petter Andersson: Expanding {{the role of the}} instrument - page 511 -Todor Todoroff: Wireless Digital/Analog Sensors for Music and Dance Performances - page 515 -Trond Engum: Real-time control and creative convolution— exchanging techniques between distinct genres - page 519 -Andreas Bergsland: The Six Fantasies Machine – an instrument modelling phrases from Paul Lansky’s Six Fantasies - page 523 Demo session N — Wednesday 1 June 13 : 30 – 14 : 30 -Jan Trützschler von Falkenstein: Gliss: An Intuitive Sequencer for the iPhone and iPad - page 527 -Jiffer Harriman, Locky Casey, Linden Melvin and Mike Repper: Quadrofeelia — A New Instrument for Sliding into Notes - page 529 -Johnty Wang, Nicolas D’Alessandro, Sidney Fels and Bob Pritchard: SQUEEZY: Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis - page 531 -Souhwan Choe and Kyogu Lee: SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape - page 533 -Norbert Schnell, Frederic Bevilacqua, Nicolas Rasamimana, Julien Blois, Fabrice Guedy and Emmanuel Flety: Playing the "MO" —Gestural Control and Re-Embodiment of Recorded Sound and Music - page 535 -Bruno Zamborlin, Marco Liuni and Giorgio Partesana: (LAND) MOVES - page 537 -Bill Verplank and Francesco Georg: Can Haptics make New Music? —Fader and Plank Demos - page 53...|$|E
40|$|We propose an {{algorithm}} {{for multiple}} F 0 estimation and tracking. It has been developped for <b>automatic</b> transcription of <b>piano</b> music, {{based on a}} maximum likelihood approach for joint estimation of multiple F 0 [1]. The whole algorithm performs a segmentation of the signal by detecting onsets. In each segment, a set of candidates is then setected, followed by the multiple F 0 estimation. The robustness of the multipitch detection is further enhanced by embedding the likelihood estimation into a Hidde...|$|R
40|$|International audienceThis work {{deals with}} the <b>automatic</b> {{transcription}} of <b>piano</b> recordings into a MIDI symbolic file. The system consists of subsequent stages of onset detection and multipitch estimation and tracking. The latter {{is based on a}} Hidden Markov Model framework, embedding a spectral maximum likelihood method for joint pitch estimation. The complexity issue of joint estimation techniques is solved by selecting subsets of simultaneously played notes within a pre-estimated set of candidates. Tests on a large database and comparisons to state-of-the-art methods show promising results...|$|R
40|$|The {{automatic}} transcription {{of music}} is a key task {{in the field of}} information retrieval in audio signals. This paper summarizes recent works on <b>automatic</b> transcription of <b>piano</b> music. The first one is a full transcription system that analyzes an input recording and provides a MIDI file including the estimation of notes. The multipitch estimation stage of the system is based on a method which is detailed separately. Finally, we report some advances in the evaluation of the resulting transcriptions in order to obtain relevant metrics from a perceptually-based point of view. 1...|$|R
40|$|We propose an {{algorithm}} for {{the multiple}} F 0 estimation and tracking {{task of the}} Third Music Information Retrieval Evaluation eXchange (MIREX 2007). It has been developped for <b>automatic</b> transcription of <b>piano</b> music, based on a maximum likelihood approach for joint estimation of multiple F 0 ’s. The whole algorithm performs a segmentation of the signal by detecting onsets. In each segment, a set of candidates is then setected, followed by the multiple F 0 estimation. The robustness of the multipitch detection is further enhanced by embedding the maximum likelihood estimation into a Hidden Markov Model framework. waveform...|$|R
40|$|This paper {{investigates the}} perceptual {{importance}} of typical errors occurring when transcribing polyphonic music excerpts into a symbolic form. The {{case of the}} <b>automatic</b> transcription of <b>piano</b> music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcription errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and {{the results show that}} pitch errors are more clearly perceived than incorrect loudness estimations or temporal deviations from the original recording. A second test presents a first attempt to include this information in more perceptually motivated measures for evaluating transcription systems...|$|R
40|$|In this paper, {{we present}} a system for <b>automatic</b> {{recognition}} of <b>piano</b> music compositional styles. It {{is based on a}} classifier that takes as input a set of MIDI files containing music pieces composed by Bach, Mozart, Beethoven and Debussy. Several features are extracted from MIDI data: the number of times each degree of the tempered scale is repeated in the whole piece, the interval between a note and the preceding one, and finally a feature based on the Forti's representation of chords. Each piece of a training set is associated with the corresponding author. After a learning phase, the classifier is able to recognize the author of an unknown piece, by analyzing and classifying the feature set extracted from the unknown piece...|$|R
40|$|This paper {{proposes a}} Hidden Markov Model (HMM) -based {{algorithm}} for <b>automatic</b> decision of <b>piano</b> fingering. We represent the positions {{and forms of}} hands and fingers as HMM states and model the resulted sequence of performed notes as emissions associated with HMM transitions. Optimal fingering decision is thus formulated as Viterbi search {{to find the most}} likely sequence of state transitions. The proposed algorithm models the required efforts in pressing a key with a finger followed by another key with another finger, and in two-dimensional positioning of fingers on the piano keyboard with diatonic and chromatic keys. Fundamental functionality of the algorithm was verified through experiments with real piano pieces. This framework can be extended to polyphonic music containing chords. ...|$|R
40|$|One {{important}} {{problem in}} Musical Information Retrieval is Automatic Music Transcription, {{which is an}} automated conversion process from played music to a symbolic nota-tion such as sheet music. Since the accuracy of previous audio-based transcription systems is not satisfactory, we propose an innovative visual-based automatic music tran-scription system named claVision to perform piano music transcription. Instead of processing the music audio, the system performs the transcription only from the video per-formance captured by a camera mounted over the piano keyboard. claVision {{can be used as}} a transcription tool, but it also has other applications such as music education. The claVision software has a very high accuracy (over 95 %) and a very low latency in real-time music transcription, even under different illumination conditions. Author Keywords claVision, <b>automatic</b> music transcription, <b>piano,</b> deaf musi-cian, computer vision ACM Classificatio...|$|R
40|$|We present work {{towards a}} {{computer}} system for the <b>automatic</b> transcription of <b>piano</b> performances. The system takes audio files containing polyphonic piano music as input, and produces MIDI output, representing the pitch, timing and volume of the musical notes. The aim of this work is not to reduce the performance data to common music notation, but to extract the performance parameters for a quantitative study of musical expression in piano performance. Standard signal processing techniques based on the short time Fourier transform are {{used to create a}} time-frequency representation of the signal, and adaptive peak-picking and pattern matching algorithms are employed to find the musical notes. In order to perform large scale testing, the test process is automated by synthesizing audio data from MIDI files using high quality sofware synthesis, and comparing results with the original MIDI data. The test data used is Mozart piano sonatas performed by a concert pianist. ...|$|R
40|$|This paper {{introduces}} "vertical line notation'' (VLN) {{of music}} for piano beginners, a conversion method from standard MIDI files to VLN scores, and an algorithm of <b>automatic</b> decision of <b>piano</b> fingering for it. Currently, staff notation {{is widely used}} for various instruments including piano. However, this notation often appears hard to beginners. On the other hand, VLN is intuitive and easy to understand for piano beginners since it graphically indicates the time order of notes as well as fingering. With the VLN score, piano beginners can make smooth progress with correct fingering. VLN scores are expected to help piano beginners make smooth progress with correct fingering. An issue with VLN {{is that it is}} currently created by hand with a spreadsheet software. It would be desirable to automatically produce VLN scores from existing digital scores. In this paper, we propose a method of converting standard MIDI files into VLN scores and an algorithm of automatic fingering decision for piano beginners. Some examples of practical and successful use of VLN scores are shown...|$|R
40|$|The {{purpose of}} my {{doctorate}} work has consisted in {{the exploration of}} the potentialities and of the effectiveness of different neural classifiers, by experimenting their application in the solution of classification problems occurring in the fields of interest typical of the research group of the “Laboratorio Circuiti” at the Department of Electronic Engineering in Tor Vergata. Moreover, though inspired by works already developed by other scholars, the adopted neural classifiers have been partially modified, in order to add to them interesting peculiarities not present in the original versions, as well as to adapt them to the applications of interest. These applications can be grouped in two great families. As regards the first application, the objects to be classified are identified by features of static nature, while as regards the second family, the objects to be classified are identified by features evolving in time. In relation to the research fields taken as reference, the ones that belong to the first family are the following: • classification, by means of fuzzy algorithms, of acoustic signals, with the aim of attributing them to the source that generated them (recognition of musical instruments) • exclusive classification of simple human motor acts for the purpose of a precocious diagnosis of nervous system diseases The second family of application has been represented by that research field that aims to the development of neural tools for the <b>Automatic</b> Tanscription of <b>piano</b> pieces. The first part of this thesis has been devoted to the detailed description of the adopted neural classification techniques, {{as well as of the}} modifications introduced in order to improve their behavior in relation to the particular applications. In the second part, the experiments by means of which I have estimated the before-mentioned neural classification techniques have been introduced. It exactly deals with experiments carried out in the chosen research fields. For every application, the II results achieved have been reported; in some cases, the further steps to perform have also been proposed. After a brief introduction to the biological neural model, a description follows about the model of the artificial neuron that has afterwards inspired all the other models: the one proposed by McCulloch and Pitts in 1943. Subsequently, the different typologies of architectures that characterize neural networks are shortly introduced, as regards the feed-forward networks as well as the recursive networks. Then, a description of some learning strategies (supervised and unsupervised), adopted in order to train neural networks, is also given; some criteria by means of which one can estimate the goodness of an opportunely trained neural network are also given (errors made vs. generalization capability). A great part of the adopted networks is based on adaptations of the Backpropagation algorithm; the other networks have been instead trained by means of algorithms based on statistical or geometric criteria. The Backpropagation algorithm has been improved by augmenting the degrees of freedom to the learning ability of a feed-forward neural network with the introduction of a spline adaptive activation function. A wide description has been given of the recurrent neural networks and particularly of the locally recurrent neural networks, networks for dynamic classification exploited in the <b>automatic</b> transcription of <b>piano</b> music. After a more or less rigorous definition of the concepts of classification and clustering, some paragraphs have been devoted to some statistical and geometric neural architectures, exploited in the implementation of static classifiers of common use and in particular in the application fields that have regarded my doctorate work. A separate paragraph has been devoted to the Simpson’s classifier and to the variants originated from my research work. They have revealed themselves to be static classifiers very simple to implement and at the same time very ductile and efficient, in many situations as well as regards the problem of musical source recognition. Two have been the choices in this case. In the first one, III these classifiers have been trained, by means of a pure supervised learning approach, while in the second the training algorithm, though keeping a substantially supervised nature, is prepared by a clustering phase, with the aim of improving, in terms of errors and generalization, the covering of the input space. Subsequently, the locally recurrent neural networks seen as dynamic classifiers are retrieved. However, their training has been rethought according to the effective reduction of the classification error instead of the classic mean-square error. The last three paragraphs have been devoted to a detailed description, in terms of specifications, implementative choices and final results, of the aforesaid fields of applications. The results obtained in all the three fields of application can be considered encouraging. Particularly, the recognition of musical instruments by means of the adopted neural networks has shown results tha can be considered out comparable if not better than those obtained by means of other techniques, but with considerably less complex structures. In case of the <b>Automatic</b> Transcription of <b>piano</b> pieces, the dynamic networks I adopted have given good results. Unfortunately, the required computational resources required by such networks cannot be considered negligible. As far as the medical applications, we are still in an incipent phase of the research. However, opinions expressed by those people who work in this field can be considered substantially eulogistic. The research activities my doctorate work is part of have been carried out in collaboration with the Department “INFOCOM” of the first University of Rome “La Sapienza”, as far as the recognition of musical instruments and the <b>Automatic</b> Transcription of <b>piano</b> pieces. The necessity to study the potentialities of neural classifiers in medical application has instead come from a profitable existing collaboration with the Istituto Superiore di Sanità in Rome...|$|R
