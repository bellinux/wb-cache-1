3|10000|Public
5000|$|March 1981 - June 1982, F-16 <b>acceptance</b> <b>test</b> <b>pilot,</b> Air Force Plant Representative Office, General Dynamics, Fort Worth, Texas ...|$|E
50|$|After one tour of duty, the U.S. Navy {{sent him}} to Massachusetts Institute of Technology to study the brand new {{discipline}} of aeronautical engineering. After {{the completion of the}} course, Grumman's first posting, along with a promotion to lieutenant, was at the League Island Naval Yard as an <b>acceptance</b> <b>test</b> <b>pilot</b> for Curtiss- and Navy-built flying boats.|$|E
50|$|Nye {{graduated}} from Barton Academy (Vermont) (1936) and the University of Vermont (BSc, 1941) and, in February 1942, completed flying training and was commissioned a second lieutenant. Through April 1943, he flew 36 missions with the 98th Bombardment Group from bases in Palestine, Egypt, and North Africa. Nye {{returned to the}} United States in April 1943 and, in September, entered the B-29 aircraft program first as an <b>acceptance</b> <b>test</b> <b>pilot</b> at the Boeing plant in Wichita, Kansas.|$|E
500|$|Flight testing {{began with}} RL-201 on 25 March 1958, {{and the design}} quickly {{demonstrated}} excellent handling and overall performance, reaching Mach 1.9 in level flight. Powered by the Pratt & Whitney J75, another three Mk. 1s were completed, RL-202 through -204. [...] The lighter and more powerful Orenda Iroquois engine was soon ready for testing, and the first Mk.II with the Iroquois, RL-206, was ready for taxi testing in preparation for flight and <b>acceptance</b> <b>tests</b> by RCAF <b>pilots</b> by early 1959.|$|R
50|$|Aircrew {{training}} include Class A (Experimental and <b>Acceptance</b> <b>Testing)</b> or {{the shorter}} Class B (<b>Acceptance</b> <b>Testing)</b> courses on either fixed or rotary-wing aircraft. A light aircraft <b>test</b> <b>pilot</b> course is also offered.|$|R
40|$|Contemporary helmet mounted {{displays}} integrate high-resolution {{display units}} together with precise headtracking solutions. This combination o�ers {{the opportunity to}} show symbols in a conformal way. Conformality here means that a hazard symbol {{is linked to the}} outside scenery. Thus, a pilot intuitively understands the connection between the symbol and its corresponding terrain feature, even if the feature is not fully visible due to degraded visual conditions. To accomplish this purpose the symbol has to be su�ciently noticeable in terms of size and brightness. However, this gives rise to the danger that parts of the outside scenery are occluded by the symbol. Furthermore, symbols should not clutter the display, in order not to distract the pilot. We present a solution framework of highlighting obstacles by symbols that balance low occlusion against noticeability. Our concept allows including di�erent representations for individual classes of obstacles in a uni�ed way. We detail the implementation of the display symbols. Finally, we present results of a �rst <b>acceptance</b> <b>test</b> with <b>pilots...</b>|$|R
50|$|The Twin Landplane was {{completed}} in July 1915, and {{was found to have}} adequate flying characteristics during testing at Eastchurch during August. In September 1915, however, a French <b>test</b> <b>pilot</b> crashed the Twin Landplane during <b>acceptance</b> <b>testing,</b> and the contract for the aircraft was cancelled.|$|R
25|$|Following {{the high}} speed flights, Waterton joined the Gloster Aircraft Company in September 1946 as a {{development}} <b>test</b> <b>pilot.</b> He became the company's Chief Test Pilot, primarily {{in charge of}} experimental flight testing their new designs, the Meteor family, the experimental Gloster E.1/44, and the delta-wing Gloster GA.5 which entered service as the Gloster Javelin. He also was involved in <b>acceptance</b> <b>test</b> flying on production aircraft.|$|R
50|$|Melnick spent 20 {{years in}} the United States Coast Guard, rising {{to the rank of}} Commander. His {{assignments}} included serving as operations officer and chief <b>test</b> <b>pilot</b> at the Coast Guard Aircraft Program Office in Grand Prairie, Texas. In that capacity, he conducted most of the developmental and all of the <b>acceptance</b> <b>test</b> flights for the HH-65 Dolphin helicopter, including sea trials, and wrote the HH-65 flight manual.|$|R
40|$|Software <b>acceptance</b> <b>testing</b> is an {{industry}} best practice. Most development teams do it poorly. This is because teams often misunderstand what <b>acceptance</b> <b>testing</b> is and why it is important. Furthermore, these teams often {{do not have an}} extensible framework for automated <b>acceptance</b> <b>testing.</b> In this paper, we define <b>acceptance</b> <b>testing</b> and discuss why it is important, and we describe our custom automated <b>acceptance</b> <b>testing</b> framework...|$|R
40|$|Abstract. In Executable Acceptance Test Driven Development, <b>acceptance</b> <b>tests</b> {{represent}} {{the requirements of}} a software system. As requirements change over time, the <b>acceptance</b> <b>tests</b> have to be updated and maintained. This process can be time-consuming and risky as <b>acceptance</b> <b>tests</b> lack the regression safety net that production code has. Refactoring of <b>acceptance</b> <b>tests</b> is used to keep the fixtures and the <b>acceptance</b> <b>test</b> definitions consistent...|$|R
50|$|The {{customer}} specifies scenarios to test when a user {{story has}} been correctly implemented. A story can have one or many <b>acceptance</b> <b>tests,</b> {{whatever it takes to}} ensure the functionality works. <b>Acceptance</b> <b>tests</b> are black-box system <b>tests.</b> Each <b>acceptance</b> <b>test</b> represents some expected result from the system. Customers are responsible for verifying the correctness of the <b>acceptance</b> <b>tests</b> and reviewing test scores to decide which failed tests are of highest priority. <b>Acceptance</b> <b>tests</b> are also used as regression tests prior to a production release. A user story is not considered complete until it has passed its <b>acceptance</b> <b>tests.</b> This means that new <b>acceptance</b> <b>tests</b> must be created for each iteration or the development team will report zero progress.|$|R
500|$|State <b>acceptance</b> <b>tests</b> {{began on}} 17 October, using the second prototype, and {{finished}} on 30 January 1948. The group of military <b>test</b> <b>pilots</b> {{concluded that the}} afterburner was unreliable and the aircraft was difficult to control in roll. Other complaints were focused on the cockpit; it was deemed was too small and lacked sufficient armor and heating or ventilation. They decided that the Yak-19 could not be recommended for service. Rather than modify the aircraft to address these problems, Yakovlev chose to cancel it entirely in favor of designs using the more-powerful (...) Rolls-Royce Derwent-derived Klimov RD-500, like his Yak-23 and Yak-25 fighters then under development.|$|R
50|$|In {{software}} testing the ISTQB defines <b>acceptance</b> as: formal <b>testing</b> {{with respect to}} user needs, requirements, and business processes conducted {{to determine whether a}} system satisfies the acceptance criteria and to enable the user, customers or other authorized entity {{to determine whether or not}} to accept the system. <b>Acceptance</b> <b>testing</b> is also known as user <b>acceptance</b> <b>testing</b> (UAT), end-user <b>testing,</b> operational <b>acceptance</b> <b>testing</b> (OAT) or field (<b>acceptance)</b> <b>testing.</b>|$|R
5000|$|<b>Acceptance</b> <b>testing</b> {{performed}} by the customer, often in their lab environment on their own hardware, is known as user <b>acceptance</b> <b>testing</b> (UAT). <b>Acceptance</b> <b>testing</b> may be performed {{as part of the}} hand-off process between any two phases of development.|$|R
40|$|Functionally {{generated}} <b>acceptance</b> <b>tests</b> {{are examined}} using structural coverage metrics. A method of comparing <b>acceptance</b> <b>tests</b> and operational usage was generated. <b>Acceptance</b> <b>tests</b> are prepresentative of operational usage {{except for the}} mix of statement types. Structural coverage metrics may provide insight into software faults...|$|R
40|$|Abstract. User <b>acceptance</b> <b>testing</b> {{is finally}} getting the {{attention}} and tool support it deserves. It is imperative that <b>acceptance</b> <b>tests</b> follow the best practices and embody the critical success factors that have been established over the years for automated unit testing. However, it is often challenging for <b>acceptance</b> <b>tests</b> to be repeatable, readable, and maintainable {{due to the nature}} of the tests and the tools currently available for automation. The key contributions this paper makes to the agile community are: first, it provides concrete examples of applying test automation patterns to user <b>acceptance</b> <b>testing,</b> and secondly it provides a description of various extensions to the WebTest <b>acceptance</b> <b>testing</b> framework that facilitate developing automated <b>acceptance</b> <b>tests</b> according to these established best practices...|$|R
40|$|The overall {{objective}} of the <b>acceptance</b> <b>test</b> is to demonstrate a combined system. This includes associated tools and equipment necessary to perform cleaning in the 105 K East Basin (KE) for achieving optimum reduction {{in the level of}} contamination/dose rate on canisters prior to removal from the KE Basin and subsequent packaging for disposal. <b>Acceptance</b> <b>tests</b> shall include necessary hardware to achieve acceptance of the cleaning phase of canisters. This <b>acceptance</b> <b>test</b> procedure will define the <b>acceptance</b> <b>testing</b> criteria of the high pressure water jet cleaning fixture. The focus of this procedure will be to provide guidelines and instructions to control, evaluate and document the <b>acceptance</b> <b>testing</b> for cleaning effectiveness and method(s) of removing the contaminated surface layer from the canister presently identified in KE Basin. Additionally, the desired result of the <b>acceptance</b> <b>test</b> will be to deliver to K Basins a thoroughly tested and proven system for underwater decontamination and dose reduction. This report discusses the <b>acceptance</b> <b>test</b> procedure for the High Pressure Water Jet...|$|R
40|$|The <b>acceptance</b> <b>test</b> {{procedure}} is described for the Lockheed Electronics Elevon Servoactuator Simulator {{to be used}} in the Shuttle Avionics Integration Laboratory (SAIL). The intent of this <b>acceptance</b> <b>test</b> {{procedure is}} to comply with the technical Shuttle Actuators Simulator Requirements. <b>Acceptance</b> <b>tests</b> will be performed on each Elevon Servoactuator Simulator...|$|R
50|$|Test pilots can be {{experimental}} and engineering <b>test</b> <b>pilots</b> (investigating {{the characteristics of}} new types of aircraft during development) or production <b>test</b> <b>pilots</b> (the more mundane role of confirming the characteristics of new aircraft as they come off the production line); many <b>test</b> <b>pilots</b> would perform both roles during their careers. Modern <b>test</b> <b>pilots</b> often receive formal training from highly-selective military <b>test</b> <b>pilot</b> schools, although other <b>test</b> <b>pilots</b> receive training and experience from civilian institutions and/or manufacturers' <b>test</b> <b>pilot</b> development programs.|$|R
5000|$|... system {{development}} lifecycle - including {{the topics of}} internal kickoff, requirements, design, development, unit/module & integration <b>testing,</b> factory <b>acceptance</b> <b>testing,</b> system shipping, installation, commissioning and site <b>acceptance</b> <b>testing</b> ...|$|R
30|$|Adjudicators {{generally}} {{come in two}} flavours, {{voters and}} <b>Acceptance</b> <b>Tests</b> (ATs). A brief description of voter characteristics and differences are presented in Section 5, {{in the context of}} the proposed taxonomy. We refer to Pullum ([2001]) for a more detailed description about the various types of adjudicators and their operations (pages 269 - 324). For example, specific adjudicators covered by Pullum ([2001]) are exact majority, consensus, formal consensus, formal majority, median, mean, weighted, and dynamic voters; <b>acceptance</b> <b>tests</b> can be based on satisfaction of requirements, accounting <b>tests,</b> computer run-time <b>acceptance</b> <b>tests</b> and reasonableness <b>acceptance</b> <b>tests.</b>|$|R
40|$|In this paper, we {{show that}} two {{recently}} published on-line <b>acceptance</b> <b>tests</b> for guaranteeing hard deadline aperiodic tasks scheduled under the Slack Stealing algorithm are insufficient: they may guarantee tasks which will then miss their deadlines. Further, we derive a sufficient <b>acceptance</b> <b>test</b> which is both efficient and effective. An evaluation {{of the performance of}} this <b>acceptance</b> <b>test</b> is given in the paper...|$|R
5000|$|ATDD {{is closely}} related to test-driven {{development}} (TDD). [...] It differs by the emphasis on developer-tester-business customer collaboration. ATDD encompasses <b>acceptance</b> <b>testing,</b> but highlights writing <b>acceptance</b> <b>tests</b> before developers begin coding.|$|R
40|$|Abstract. We {{present results}} of a case study looking at how domain {{knowledge}} is communicated to developers using executable <b>acceptance</b> <b>test</b> driven development at a large software development company. We collected and analyzed qualitative data on a large software development team's testing practices and their use of a custom-built executable <b>acceptance</b> <b>testing</b> tool. Our findings suggest that executable <b>acceptance</b> <b>tests</b> (1) helps communicate domain knowledge required to write software and (2) can help software developers to communicate {{the status of the}} software implementation better. In addition to presenting these findings, we discuss several human aspects involved in facilitating executable <b>acceptance</b> <b>test</b> driven development...|$|R
40|$|In this paper, {{we argue}} that {{executable}} <b>acceptance</b> <b>test</b> driven development (EATDD) allows tighter integration between the software requirements and the implementation. We argue that EATDD improves communication between all project stakeholders. We give an overview of why previous approaches to requirements specifications are less than impressive and how executable <b>acceptance</b> <b>tests</b> help fix problems. In addition, we argue for multi-modal executable <b>acceptance</b> <b>tests</b> {{and how it can}} help improve the requirements specification. We provide some of the immediate research questions {{that need to be addressed}} in order to push forward more wide-spread use of executable <b>acceptance</b> <b>test</b> driven development...|$|R
5000|$|If {{the test}} is successful, the product is copied to an <b>Acceptance</b> <b>test</b> environment. During the <b>Acceptance</b> <b>test,</b> the {{customer}} will test the product in this environment to verify whether it meets their expectations.|$|R
50|$|<b>Acceptance</b> <b>tests</b> {{are a part}} of {{an overall}} testing strategy. They are the {{customer}} tests that demonstrate the business intent of a system. Component <b>tests</b> are technical <b>acceptance</b> <b>tests</b> developed by an architect that specify the behavior of large modules. Unit tests are created by the developer to drive easy-to-maintain code. They are often derived from <b>acceptance</b> <b>tests</b> and unit tests. Cross-functional testing includes usability testing, exploratory testing, and property testing (scaling and security).|$|R
40|$|The {{development}} and <b>acceptance</b> <b>testing</b> of the 4 -band Multispectral Scanners to be flown on LANDSAT D and LANDSAT D Earth resources satellites are summarized. Emphasis {{is placed on}} the <b>acceptance</b> <b>test</b> phase of the program. Test history and <b>acceptance</b> <b>test</b> algorithms are discussed. Trend data of all the key performance parameters are included and discussed separately for each of the two multispectral scanner instruments. Anomalies encountered and their resolutions are included...|$|R
5000|$|In {{contract}} <b>acceptance</b> <b>testing,</b> {{a system}} is <b>tested</b> against <b>acceptance</b> criteria as documented in a contract, before the system is accepted. In regulation <b>acceptance</b> <b>testing,</b> {{a system is}} tested to ensure it meets governmental, legal and safety standards.|$|R
50|$|In 2014, AUTOSAR <b>Acceptance</b> <b>Tests</b> were {{introduced}} to minimize test effort and <b>test</b> costs. <b>Acceptance</b> Test Specifications are system test specifications with interfaces to the application and the bus. The specification of standard <b>acceptance</b> <b>tests</b> contribute to these objectives.|$|R
5000|$|This {{may include}} factory <b>acceptance</b> <b>testing</b> (FAT), i.e. the testing {{done by a}} vendor before the product or system is moved to its {{destination}} site, after which site <b>acceptance</b> <b>testing</b> (SAT) may be performed by the users at the site.|$|R
40|$|<b>Acceptance</b> <b>Testing</b> will be {{performed}} by a medical physicist before the scanner is used to scan patients. The goal of the <b>acceptance</b> <b>testing</b> {{is to ensure that}} the scanner meets or exceeds the manufacturer’s specifications as is required by the CAR Standards for Magnetic Resonance Imaging. The references {{at the end of this}} section provide guidelines for the <b>acceptance</b> <b>tests.</b> However, actual tests may deviate from the guidelines due to the capabilities and limitations of the scanner and phantoms...|$|R
5000|$|In {{addition}} to <b>acceptance</b> <b>tests</b> for requirements, <b>acceptance</b> <b>tests</b> {{can be used}} on a project as a whole. For example, if this requirement was part of a library book checkout project, there could be <b>acceptance</b> <b>tests</b> for the whole project. These are often termed SMART objectives. An example test is [...] "When the new library system is in production, the users will be able to check books in and out three times as fast as they do today".|$|R
40|$|Abstract. We {{conducted}} a survey on Executable Acceptance Test Driven Development (or: Story Test Driven Development). The results {{show that there is}} often a substantial delay between defining an <b>acceptance</b> <b>test</b> and its first successful pass. Therefore, it becomes important for teams to easily be able to distinguish between tasks that were never tackled before and tasks that were already completed but whose tests are now failing again. We then describe our FitClipse tool that extends Fit by maintaining a history of <b>acceptance</b> <b>test</b> results. Based on the history, FitClipse is able to generate reports that show when an <b>acceptance</b> <b>test</b> is suddenly failing again. Keywords: Executable Acceptance Test-Driven Development (EATDD), executable <b>acceptance</b> <b>test,</b> Fit...|$|R
40|$|Abstract—During <b>acceptance</b> <b>testing</b> {{customers}} {{assess whether}} a system meets their expectations and often identify issues {{that should be}} improved. These findings have to be communicated to the developers – a task we observed to be error prone, especially in distributed teams. Here, it is normally not possible to have developer representatives from every site attend the test. Developers who were not present might misunderstand insufficiently documented findings. This hinders fixing the issues and endangers customer satisfaction. Integrated feedback systems promise to mitigate this problem. They allow to easily capture findings and their context. Correctly applied, this technique could improve feedback, while reducing customer effort. This paper collects our experiences from comparing <b>acceptance</b> <b>testing</b> with and without feedback systems in a distributed project. Our results indicate that this technique can improve <b>acceptance</b> <b>testing</b> – if certain requirements are met. We identify key requirements feedback systems should meet to support <b>acceptance</b> <b>testing.</b> Keywords-distributed software development; requirements engineering; <b>acceptance</b> <b>testing</b> I...|$|R
