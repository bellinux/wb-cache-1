58|3|Public
5000|$|In [...] "basic mode", the JBIG encoder must {{split the}} image into {{horizontal}} stripes of 128 lines (parameter L0=128), and restart the <b>arithmetic</b> <b>encoder</b> for each stripe.|$|E
50|$|Snappy {{encoding}} is not bit-oriented, but byte-oriented (only whole bytes are emitted or consumed from a stream). The format uses no entropy encoder, like Huffman tree or <b>arithmetic</b> <b>encoder.</b>|$|E
3000|$|... [...]). Each packet is, then, {{compressed}} {{using the}} <b>arithmetic</b> <b>encoder</b> {{and the resulting}} binary stream b = (b 1,..., b [...]...|$|E
40|$|In this paper, a novel {{technique}} is presented for using state observers {{in conjunction with}} an entropy source encoder to enable highly compressed telemetry of autonomous underwater vehicle (AUV) position vectors. In this work, both the sending vehicle and receiving vehicle or human operator are equipped with a shared real-time simulation of the sender's state based on the prior transmitted positions. Thus, only the innovation between the sender's actual state and the shared state need be sent over the link, such as a very low throughput acoustic modem. The distribution of this innovation can be modeled a priori or assembled adaptively. This distribution is then paired with an <b>arithmetic</b> entropy <b>encoder,</b> producing a very low cost representation of the vehicle's position vector. This system was analyzed on experimental data from the GLINT 10 and AGAVE 07 expeditions involving two different classes of AUVs performing a diverse number of maneuvers, and implemented on a fielded vehicle in the MBAT 12 experiment. Using an adaptive probability distribution in combination with either of two state observer models, greater than 90 % compression, relative to a 32 -b integer baseline, was achieved. United States. Office of Naval Research (Grant N 00014 - 08 - 1 - 0011) United States. Office of Naval Research (Grant N 00014 - 11 - 1 - 0097...|$|R
40|$|Operational complexity-distortion curves for H. 264 /JVT {{decoding}} {{are generated}} and analyzed for low-complexity mobile devices under {{a variety of}} bitrate constraints. The focus of our study is on achieving optimum complexity-distortion operational points by evaluating different combinations of Group of Picture (GoP) types and varying the Quantization Parameter (QP) and entropy <b>encoder</b> (<b>arithmetic</b> or universal) to meet the desired rate constraints. Using a 400 Mhz Intel PXA 255 platform (found in popular iPaq devices), complexity-distortion curves are developed for common GoP structures and a wide range of QP values. The curves, based on extensive operational experimentation, indicate that under typical conditions for mobile platforms (low to mid computational complexity), I or I & P-frame combinations outperform the more compressed streams that include B-frames. In the 25 - 45 dB range, the optimum complexity-distortion I or I & P structures outperformed B-frame structures by up to 21 % in complexity for the equivalent distortion level and under the same bitrate constraints. Further, under the same complexity and bitrate constraints, selecting the optimum GoP structure achieves as much as a 10 dB PSNR improvement...|$|R
40|$|The Context-Adaptive Binary Arithmetic Coding(CABAC) used in High Efficiency Video Coding(HEVC/H. 265) is a near optimal {{entropy coding}} method. As a {{consequence}} of this coding efficiency, CABAC implementation is a complicated and highly serialized algorithm. With the CABAC becoming a bottleneck in encoder and decoder performance, a major innovation {{has taken place in}} the binarization scheme of the transform-coefficient level values. HEVC introduces an adaptive binarization scheme that allows more data to be encoded using a high throughput bypass mode. This adaptive binarization scheme utilizes three different coding methods, Truncated Unary(TrU), k-th order Truncated Rice(TRk) and k-th order Exp-Golomb(EGk). By exploiting the properties of the video-coding data structure, as well as the properties each of these coding methods hold, this binarization scheme is able to achieve a near optimal code. Thorough analysis of the binarization scheme has been performed, with a main focus on finding an efficient hardware implementation. A major challenge was finding an efficient way of coding the remaining absolute transform-coefficient level(ALRem). ALRem is coded using an truncation of TRk and EGk, with an adaptive level(k). A finite state machine approach was found, that proved to be a very efficient at coding the absolute remaining level. This approach was implemented in hardware. The Context Index Calculator, that form {{an integral part of the}} HEVC CABAC system was not implemented. When this module is designed, it is proposed to combine the Binarizer and Context Index Calculator. This is due to the large amount of shared data dependencies. A simplified version of an actual Context-Adaptive Binary <b>Arithmetic</b> Coding <b>encoder</b> architecture is implemented. It performs CABAC encoding as specified in by the HEVC standard, but is limited to the encoding of a subset of the transform-coefficient level data. Verifying the correctness of this hardware encoder required the development of a software model. This software encoder was expanded to also include a decoder, which allowed for additional functional verification. Because of the inconsistent throughput of the encoder modules, an asynchronous fifo was developed to simplify data flow, and improve performance. Due to the unfinished state of both the binarizer and context index calculator, the completed system was not implemented...|$|R
30|$|A {{significant}} coefficient is encoded {{by means}} of a symbol indicating the number of bits required to represent that coefficient. An <b>arithmetic</b> <b>encoder</b> with two contexts is used to efficiently store that symbol. As coefficients in the same subband have similar magnitude, an adaptive <b>arithmetic</b> <b>encoder</b> is able to represent this information in a very efficient way. After that, the significant bits and sign of the wavelet coefficient are raw-encoded to speed up the execution time.|$|E
3000|$|The {{considered}} system {{consists of}} a finite alphabet source, a classical <b>arithmetic</b> <b>encoder,</b> an Additive White Gaussian Noise (AWGN) channel and an arithmetic decoder. The source generates packets of L symbols s = (s 1,..., s [...]...|$|E
3000|$|... subband to the first-level wavelet subbands {{and symbols}} {{computed}} {{in the first}} stage are entropy coded by means of an <b>arithmetic</b> <b>encoder.</b> Recall that no LOWER_COMPONENT is encoded. The value of significant coefficients and their sign are raw encoded.|$|E
3000|$|Sayir [10] {{showed that}} an {{arithmetic}} coder {{can be an}} entropy source encoder when the model is matched with the source and can be a channel encoder when the probability space is properly reserved for error protection and {{can act as a}} convolutional code. After inserting the forbidden symbol to a source with M alphabet, we will have an arithmetic coding with M + 1 alphabet in which one of the symbols never appears. Therefore, adding parity is performed while compression without adding more additional operations to the conventional arithmetic coding. If the source has M alphabet, so this method just adds M multiplication and 1 additional operation to the complexity of conventional <b>arithmetic</b> <b>encoder.</b> But if we want to place a convolutional encoder after <b>arithmetic</b> <b>encoder,</b> according to the amount of redundancy, it needs some shift and XOR operations and increasing memory usage. For example, if the bit rate is 1 / 2 and the code generator polynomial is [...]...|$|E
40|$|J’his paper {{presents}} {{a study of}} seismic data compression techniques and a compression algorithm based on subband coding. ‘ 1 ’hcalgorithrn includes threcstagcs: adccorrelation stage, a quantization stage that introduces a controlled amount of distortion to allow for high compression ratios, and a IOSSICSS entropy coding stage based on a simple but cfficicnt arithmetic coding method. Subband coding methods arc particularly suited to the decorrelation of non-station ary processes such as seismic events. Adaptivity to the non-stationary behavior of the waveform is achieved by dividing the data into separate blocks which arc cncodcd separately with an adaptive <b>arithmetic</b> <b>encoder.</b> ‘J’his is done with high efficiency duc to the low overhead introduced by the <b>arithmetic</b> <b>encoder</b> in specifying its parameters, “J’hc tcchniquc {{could be used as}} a progressive transmission system, where successive refinements of the data can bc requested by the user. ‘J’his allows seismologists to first examine a coarse version of waveforms with minimal usage of the channel and then decide where refinements arc required. Mm-distort ion performance results arc presented and comparisons arc made with two block transform methods. ...|$|E
30|$|Bitplane coding is {{implemented}} by the JPEG 2000 encoding codeblocks with three passes per plane, so the most important information, from a rate-distortion (R/D) point of view, is first encoded. It also uses an optional and low-complexity post-compression optimization algorithm, based on the Lagrange multiplier method. Besides, it uses {{a large number of}} contexts for the <b>arithmetic</b> <b>encoder.</b> This post-compression rate-distortion optimization algorithm selects the most important coefficients by weighting them, based on the mean square error (MSE) distortion measurement.|$|E
40|$|We {{consider}} nonparametric sequential {{hypothesis testing}} when the distribution under null hypothesis is fully known and the alternate hypothesis corresponds {{to some other}} unknown distribution. We use easily implementable universal lossless source codes to propose simple algorithms for such a setup. These algorithms are motivated from spectrum sensing application in Cognitive Radios. Universal sequential hypothesis testing using Lempel Ziv codes and Krichevsky-Trofimov estimator with <b>Arithmetic</b> <b>Encoder</b> are considered and compared for different distributions. Cooperative spectrum sensing with multiple Cognitive Radios using universal codes is also considered...|$|E
40|$|We present our new low-complexity {{compression}} algorithm for lossless coding of video sequences. This new coder produces better compression ratios than lossless compression of individual images by exploiting temporal {{as well as}} spatial and spectral redundancy. Key features of the coder are a pixel-neighborhood backward-adaptive temporal predictor, an intra-frame spatial predictor and a differential coding scheme of the spectral components. The residual error is entropy coded by a context-based <b>arithmetic</b> <b>encoder.</b> This new lossless video encoder outperforms state [...] of [...] the [...] art lossless image compression techniques, enabling more efficient video storage and communications...|$|E
40|$|We {{present a}} new {{lossless}} image compression algorithm based on Arithmetic Coding. Our algorithm selects appropriately, for each pixel position, {{one of a}} large number of possible, dynamic, probability distributions, and encodes the current pixel prediction error by using this distribution as the model for the <b>arithmetic</b> <b>encoder.</b> We have experimentally compared our algorithm with Lossless JPEG, that is currently the lossless image compression standard, and also with FELICS and other lossless compression algorithms. Our tests show that the new algorithm outperforms Lossless JPEG and FELICS leading to a compression improvement of about 12 % over Lossless JPEG and 10 % over FELICS...|$|E
40|$|Abstract—An {{efficient}} {{technique for}} encoding arbitrary contours is presented. It {{is based on}} the concept of transition point of a contour map and relies on a new four-symbol adaptive context-based <b>arithmetic</b> <b>encoder</b> (CAE) that calculates contexts in the (binary) domain of the contour map. The results obtained with this new technique applied to the compression of image partitions are substantially better than those previously attained, showing a clear superiority over adaptive CAE-based differential chain coding and also over the MPEG- 4 shape coder. Index Terms—Chain coding, contour coding, partition coding, region-based image coding, shape coding, transition points. I...|$|E
40|$|In this paper, a {{high speed}} {{architecture}} with a well designed pipeline is presented for the <b>arithmetic</b> <b>encoder</b> in JPEG 2000 algorithm. The coding algorithm {{has also been}} improved by changing the renormalization and byteout extraction steps. The proposed algorithm has been implemented with a four stage pipelined architecture in VHDL. The new design has reduced, {{and in some cases}} removed data dependencies. The proposed architecture has been synthesized on a Virtex 2 FPGA and its power consumption, working frequency and encoding rate were analyzed. Simulation results indicate that the proposed design is able to encode a CIF video sequence at 47 frames/s. ...|$|E
40|$|Neural {{networks}} {{have the potential}} to extend data compression algorithms beyond the character level n-gram models now in use, but have usually been avoided because they are too slow to be practical. We introduce a model that produces better compression than popular Limpel-Ziv compressors (zip, gzip, compress), and is competitive in time, space, and compression ratio with PPM and Burrows-Wheeler algorithms, currently the best known. The compressor, a bit-level predictive <b>arithmetic</b> <b>encoder</b> using a 2 layer, 4 × 10 6 by 1 network, is fast (about 10 4 characters/second) because only 4 - 5 connections are simultaneously active and because it uses a variable learning rate optimized for one-pass training...|$|E
40|$|This paper {{presents}} a fast and low cost context adaptive binary <b>arithmetic</b> <b>encoder</b> for H. 264 /MPEG- 4 AVC video coding standard through both algorithm level and architecture level optimizations. First in the algorithm level, we process the binarization and context generation in parallel {{to reduce the}} encoding iteration cycles {{to three or four}} cycles from five cycles in the previous design. Second, in the architecture level, we reduce the cycles of renormalization loops by employing one-skipping and bit-parallelism, and save hardware cost of arithmetic coder by merging three different modes. The implemented design shows that it can achieve the 333 MHz frequency with only 13. 3 K gate count...|$|E
30|$|This work {{presents}} a parallel context-modeling coding architecture and a matching arithmetic coder (MQ-coder) for the embedded block coding (EBCOT) {{unit of the}} JPEG 2000 encoder. Tier- 1 of the EBCOT consumes most of the computation time in a JPEG 2000 encoding system. The proposed parallel architecture can increase the throughput rate of the context modeling. To match the high throughput rate of the parallel context-modeling architecture, an efficient pipelined architecture for context-based adaptive <b>arithmetic</b> <b>encoder</b> is proposed. This encoder of JPEG 2000 can work at 180 MHz to encode one symbol each cycle. Compared with the previous context-modeling architectures, our parallel architectures can improve the throughput rate up to 25 %.|$|E
40|$|We {{proposed}} a hardware accelerator IP for the Tier- 1 portion of Embedded Block Coding with Optimal Truncation (EBCOT) {{used in the}} JPEG 2000 next generation image compression standard. EBCOT Tier- 1 {{accounts for more than}} 70 % of encoding time due to extensive bit-level processing. Our architecture consists of a 16 -bit parallel context formation module and a 3 -stage pipelined <b>arithmetic</b> <b>encoder.</b> Compared with the known best design, we reduce 17 % of the cycle count and have achieved within 5 % of the theoretical lower bound. We have integrated our synthesizable RTL with an AMBA-AHB interface for SOC design. FPGA prototyping has been successfully demonstrated and substantial speedup achieved...|$|E
40|$|This work {{presents}} a parallel context-modeling coding architecture and a matching arithmetic coder (MQ-coder) for the embedded block coding (EBCOT) {{unit of the}} JPEG 2000 encoder. Tier- 1 of the EBCOT consumes most of the computation time in a JPEG 2000 encoding system. The proposed parallel architecture can increase the throughput rate of the context modeling. To match the high throughput rate of the parallel context-modeling architecture, an efficient pipelined architecture for context-based adaptive <b>arithmetic</b> <b>encoder</b> is proposed. This encoder of JPEG 2000 can work at 180 MHz to encode one symbol each cycle. Compared with the previous context-modeling architectures, our parallel architectures can improve the throughput rate up to 25 %. </p...|$|E
40|$|A {{lossless}} wavelet-based {{image compression}} method with adaptive prediction is proposed. Firstly, we analyze {{the correlations between}} wavelet coefficients to identify a proper wavelet basis function, then predictor variables are statistically test to determine which relative wavelet coefficients {{should be included in}} the prediction model. At last, prediction differences are encoded by an adaptive <b>arithmetic</b> <b>encoder.</b> Instead of relying on a fixed number of predictors on fixed locations, we proposed the adaptive prediction approach to overcome the multicollinearity problem. The proposed innovative approach integrating correlation analysis for selecting wavelet basis function with predictor variable selection is fully achieving high accuracy of prediction. Experimental results show that the proposed approach indeed achieves a higher compression rate on CT, MRI and ultrasound images comparing with several state-of-the-art methods...|$|E
30|$|For the context, it is firstly {{introduced}} in image and video coding. Here, context-based adaptive binary arithmetic coding (CABAC) in H. 264 /AVC [10] {{is taken as}} an example. CABAC {{is one of the}} two entropy coding methods of the new ITU-T/ISO/IEC standard for video coding, i.e., H. 264 /AVC, and plays a very important role in the efficiency improvement of the video coding. Through combining an adaptive binary arithmetic coding technique with context modeling of the neighboring symbols in binary bit stream and macro block, a high degree of adaptation and redundancy reduction is achieved. The encoding process of CABAC consists of three elementary steps: binarization, context model selecting, and adaptive binary arithmetic encoding. The last step consists of probability estimation and binary <b>arithmetic</b> <b>encoder.</b>|$|E
40|$|We propose an {{architecture}} for Tier- 1 of Embedded Block Coding With Optimized Truncation (EBCOT) in the JPEG 2000 standard. The architecture {{is composed of}} a 16 -bit parallel context generator and a 3 -stage pipelined binary <b>arithmetic</b> <b>encoder.</b> The former is designed for low power consumption. The later is used to achieve an high throughput. The design is verified on an AMBA-based system as an accelerator. Compared with the best-known column-based method, we reduce the cycle count by 17 %. Let the number of context-decision (CX, D) pairs be the lower bound on the cycle count, we have achieved 6 % within the optimal. architecture. Section IV shows the experimental results and Section V makes a concise conclusion Pre-proces...|$|E
40|$|Abstract—This paper {{presents}} a parallel architecture for the Embedded Block Coding (EBC) in JPEG 2000. The architecture {{is based on}} the proposed word-level EBC algorithm. By processing all the bit planes in parallel, the state variable memories for the context formation (CF) can be completely eliminated. The length of the FIFO (first-in first-out) between the CF and the <b>arithmetic</b> <b>encoder</b> (AE) is optimized by a reconfigurable FIFO architecture. To reduce the hardware cost of the parallel architecture, we proposed a folded AE architecture. The parallel EBC architecture can losslessly process 54 MSamples/s at 81 MHz, which can support HDTV 720 p resolution at 30 frames/s. Index Terms—Discrete wavelet transform (DWT), embedded block coding (EBC), EBC with optimized truncation (EBCOT), image processing, JPEG 2000, parallel processing. I...|$|E
40|$|Here we {{describe}} a baseline-based binary shape cod-ing method in which arbitrarily shaped object is repre-sented by the traced 1 -D data from baseline and turn-ing point (TP). The shape coding is performed based on contour-based method in each separated shape. There are two coding modes in shape coding, i. e. intra and inter mode as in texture coding. In inter cod-ing mode, object identification, global shape matching, and local contour matching are used. In intra mode and residue coding, the DPCM values of TP and the 1 -D dzstance {{values of the}} shape are encoded by fixed <b>arithmetic</b> <b>encoder.</b> Simulation {{results show that the}} proposed method has better codzng eficiency and sub-jective quality compared with the block-based method i. e. context arithmetic encoding (CAE). ...|$|E
30|$|We {{recall that}} the {{arithmetic}} decoding machine {{is based on a}} recursive procedure which terminates when all l bits are processed or when L symbols are obtained in the decoded sequence. However, the proposed decoder has to use information about L to detect erroneous sequences and improve decoding performance. To address this requirement, a proper AC termination strategy is implemented. In fact, the <b>arithmetic</b> <b>encoder</b> terminated each input sequence with an End-of-Block (EoB) symbol. The same rule is enforced at the decoder and only sequences that decode exactly L symbols and whose EoB symbol is determined by the last two bits is considered to be correct. This supplementary error detection tool can ameliorate the arithmetic decoder performance since it reduces the size of the search space and consequently, increases the Hamming distance between candidates.|$|E
30|$|In USAC [34], an {{up-to-date}} MPEG standardization, MDCT {{plays an}} important role [35]. In the USAC encoder, the MDCT coefficients are firstly companded with a power low function before scalar quantization, achieving in effect a non-uniform scalar quantization. And then, the residuals are further entropy coded. To improve the performance of MDCT coefficients quantization and coding, a novel scheme [29], which combined a scalar quantization with a context-based entropy coding, was developed in the USAC. In this new scheme, the input tuples (blocks) were first quantized by a scalar quantizer (SQ), and then the generated tuple indices were further encoded through a context-based <b>arithmetic</b> <b>encoder.</b> In the USAC final version (FINAL), the tuple length of this scheme was selected to be 2, in order to decrease the total memory requirements.|$|E
40|$|Abstract—We {{consider}} nonparametric or universal sequential {{hypothesis testing}} when the distribution under {{the null hypothesis}} is fully known but the alternate hypothesis corresponds to some other unknown distribution. These algorithms are primarily motivated from spectrum sensing in Cognitive Radios and intruder detection in wireless sensor networks. We use easily implementable universal lossless source codes to propose simple algorithms for such a setup. The algorithms are first proposed for discrete alphabet. Their performance and asymptotic properties are studied theoretically. Later these are extended to continuous alphabets. Their performance with two well known universal source codes, Lempel-Ziv code and KT-estimator with <b>Arithmetic</b> <b>Encoder</b> are compared. These algorithms are also compared with the tests using various other nonparametric estimators. Finally a decentralized version utilizing spatial diversity is also proposed and analysed...|$|E
40|$|Abstract — JBIG 2 is {{the latest}} {{international}} standard for bi-level image compression. It partitions a bi-level image into three types of region—text, halftone, and generic region. This paper addresses the problem of large memory requirements for the contexts. The arithmetic encoders of text region in JBIG 2 require a large memory for the contexts. In this work, we proposed several algorithms to reduce the memory requirements. A large portion of total memory requirements are reduced by the proposed algorithms. Moreover, the proposed algorithms are implemented as a memory efficient hardware architecture, the unified <b>arithmetic</b> <b>encoder.</b> The experimental {{results show that the}} proposed algorithms are efficient. The proposed algorithms can reduce the memory requirements for the contexts by 98. 7 %. Moreover, the drop of coding gain due to memory reduction for the contexts is only 4. 83 % in average. I...|$|E
40|$|This paper {{proposed}} a conflict-free multi-symbol <b>arithmetic</b> <b>encoder</b> (AE) for H. 264 /AVC. The throughput of a multi-symbol AE is usually limited by concurrent access of context memory which degrades {{the performance of}} the whole H. 264 /AVC encoder. With proposed Hybrid Context Memory architectures (HCM), about 1 / 10 contexts under critical conflict are implemented by Critical Register Array to solve the insufficient ports bottleneck. The number of encoding symbol per cycle is improved by 15 % and 55. 5 % in 2 -symbol and 4 -symbol version, respectively. The implementation results also show that the hardware design can achieve a high encoding rate and throughput comparing to the previous works. APSIPA ASC 2009 : Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. 4 - 7 October 2009. Sapporo, Japan. Poster session: Advanced Circuits and Systems/VLSI (5 October 2009) ...|$|E
40|$|Redundancy is dened as {{the excess}} of the code length over the optimal (ideal) code length. We study the average {{redundancy}} of an idealized arithmetic coding (for memoryless sources with unknown distributions) in which the Krichevsky and Tromov estimator {{is followed by the}} Shannon{Fano code. We shall ignore here important prac-tical implementation issues such as nite precisions and nite buer sizes. In fact, our idealized arithmetic code can be viewed as an adaptive innite precision implementation of <b>arithmetic</b> <b>encoder</b> that resembles Elias coding. However, we provide very precise results for the average redundancy that takes into account integer{length constraints. These ndings are obtained by analytic methods of analysis of algorithms such as theory of distribution of sequences modulo 1 and Fourier series. These estimates can be used to study the average redundancy of codes for tree sources, and ultimately the context-tree weighting algorithms. ...|$|E
30|$|Instead {{of using}} run-length count symbols, {{we could have}} used a single symbol to encode each {{insignificant}} coefficient. However, {{we would need to}} encode a larger amount of symbols, and therefore, the complexity of the algorithm would increase (most of all, {{in the case of a}} large number of insignificant contiguous symbols, which usually occurs in moderate-to-high compression ratios). However, the compression performance is increased if a specific symbol is used for every insignificant coefficient since an <b>arithmetic</b> <b>encoder</b> processes more efficiently many likely symbols than a lower amount of less likely symbols. So, for short run-lengths, we encode a LOWER symbol for each insignificant coefficient instead of coding a run-length count symbol for all the sequence. The threshold to enter the run-length mode and start using run-length count symbols is defined by the enter_run_mode parameter. The formal description of the depicted algorithm can be found in Algorithm 1.|$|E
40|$|We {{present a}} wavelet image coder {{based on an}} {{explicit}} model of the conditional statistical relationships between coefficients in different subbands. In particular, we construct a parameterized model for the conditional probability of a coefficient given coefficients at a coarser scale. Subband coefficients are encoded one bitplane at a time using a non-adaptive <b>arithmetic</b> <b>encoder.</b> The overall ordering of bitplanes {{is determined by the}} ratio of their encoded variance to compressed size. We show rate-distortion comparisons of the coder to first and second-order theoretical entropy bounds and the EZW coder [1]. The coder is inherently embedded, and should prove useful in applications requiring progressive transmission. Orthonormal wavelet decompositions have proven to be extremely effective for image compression [2, 3, 4, 5, 1]. We believe there are several statistical reasons for this success. Similar to the Fourier transform, wavelets are quite good at decorrelating the second-order sta [...] ...|$|E
40|$|Abstract—We {{study the}} problem of {{compressing}} a block of symbols (a block quantum state) emitted by a memoryless quantum Bernoulli source. We present a simple-to-implement quantum algorithm for projecting, with high probability, the block quantum state onto the typical subspace spanned by the leading eigenstates of its density matrix. We propose a fixed-rate quantum Shannon–Fano code to compress the projected block quantum state using a per-symbol code rate that is slightly higher than the von Neumann entropy limit. Finally, we propose quantum arithmetic codes to efficiently implement quantum Shannon–Fano codes. Our <b>arithmetic</b> <b>encoder</b> and decoder have a cubic circuit and a cubic computational complexity in the block size. Both the encoder and decoder are quantum-mechanical inverses of each other, and constitute an elegant example of reversible quantum computation. Index Terms—Arithmetic coding, noiseless coding, quantum communication, quantum computation, quantum information theory, quantum measurement, reversible computation, Schumacher coding. I...|$|E
