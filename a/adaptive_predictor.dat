93|126|Public
50|$|The {{general rule}} for a {{two-level}} <b>adaptive</b> <b>predictor</b> with an n-bit history {{is that it}} can predict any repetitive sequence with any period if all n-bit sub-sequences are different.|$|E
5000|$|If an [...] {{statement}} is executed three times, the decision {{made on the}} third execution might depend upon whether the previous two were taken or not. In such scenarios, two-level <b>adaptive</b> <b>predictor</b> works more efficiently than a saturation counter. Conditional jumps that are taken every second time or have some other regularly recurring pattern are not predicted well by the saturating counter. A two-level <b>adaptive</b> <b>predictor</b> remembers {{the history of the}} last n occurrences of the branch and uses one saturating counter for each of the possible 2n history patterns. This method is illustrated in figure 3.|$|E
50|$|A local branch {{predictor}} has {{a separate}} history buffer for each conditional jump instruction. It may use a two-level <b>adaptive</b> <b>predictor.</b> The history buffer is separate for each conditional jump instruction, while the pattern history table may be separate {{as well or}} it may be shared between all conditional jumps.|$|E
40|$|The {{purpose of}} the present paper is to analyze how the {{predictive}} distribution estimated using a set of context dependent nonlinear <b>adaptive</b> <b>predictors</b> can be used to localize edges in graylevel images. Since the <b>adaptive</b> <b>predictors</b> have the potential of learning repetitive structure, as those characteristic to certain textures, our predictive edge detection scheme may be a practical way to conceal the relative high contrast of certain texture regions. 1...|$|R
50|$|APC {{is related}} to linear {{predictive}} coding (LPC) in that both use <b>adaptive</b> <b>predictors.</b> However, APC uses fewer prediction coefficients, thus requiring a higher sampling rate than LPC.|$|R
40|$|We propose <b>adaptive</b> channel <b>predictors</b> for {{orthogonal}} {{frequency division}} multiplexing (OFDM) communications over time-varying channels. Successful {{application of the}} normalized least-meansquare (NLMS) and recursive least-squares (RLS) algorithms is demonstrated. We also consider the use of <b>adaptive</b> channel <b>predictors</b> for delay-free equalization, thereby avoiding the need for regular transmission of pilot symbols. Simulation results demonstrate the good performance of the proposed techniques...|$|R
50|$|The {{advantage}} of the two-level <b>adaptive</b> <b>predictor</b> {{is that it can}} quickly learn to predict an arbitrary repetitive pattern. This method was invented by T.-Y. Yeh and Yale Patt at the University of Michigan. Since the initial publication in 1991, this method has become very popular. Variants of this prediction method are used in most modern microprocessors.|$|E
5000|$|A {{two-level}} <b>adaptive</b> <b>predictor</b> with globally shared history buffer {{and pattern}} history table {{is called a}} [...] "gshare" [...] predictor if it xors the global history and branch PC, and [...] "gselect" [...] if it concatenates them. Global branch prediction is used in AMD processors, and in Intel Pentium M, Core, Core 2, and Silvermont-based Atom processors.|$|E
50|$|An agree {{predictor}} is a two-level <b>adaptive</b> <b>predictor</b> with globally shared history buffer {{and pattern}} history table, {{and an additional}} local saturating counter. The outputs of the local and the global predictors are XORed {{with each other to}} give the final prediction. The purpose is to reduce contentions in the pattern history table where two branches with opposite prediction happen to share the same entry in the pattern history table.|$|E
40|$|In this paper, {{we examine}} the {{application}} of simple neural processing elements {{to the problem of}} dynamic branch prediction in high-performance processors. A single neural network model is considered: the Perceptron. We demonstrate that a predictor based on the Perceptron can achieve a prediction accuracy in excess of that given by conventional Two-level <b>Adaptive</b> <b>Predictors</b> and suggest that neural predictors merit further investigation...|$|R
40|$|Neural-based branch {{predictors}} {{have been}} among the most accurate in the literature. The recently proposed scaled neural analog predictor, or SNAP, builds on piecewise-linear branch prediction and relies on a mixed analog/digital implementation to mitigate latency as well as power requirements over previous neural predictors. I present an optimized version of the SNAP predictor, hybridized with two simple two-level <b>adaptive</b> <b>predictors.</b> The resulting optimized predictor, OH-SNAP, delivers high accuracy...|$|R
40|$|The {{estimation}} of Hidden Markov Models {{has attracted a}} lot of attention recently, see results of Legland and Mevel (2000) and Leroux (1992). The {{purpose of this paper is}} to give a view for the analysis of the maximum-likelihood {{estimation of}} HMM-s. General consistency results are compared to the new approach. The new approach is potentially useful for deriving strong approximation results, which are in turn applicable to analyze <b>adaptive</b> <b>predictors.</b> ...|$|R
50|$|A global branch {{predictor}} {{does not}} keep a separate history record for each conditional jump. Instead it keeps a shared history of all conditional jumps. The {{advantage of a}} shared history is that any correlation between different conditional jumps is part of making the predictions. The disadvantage is that the history is diluted by irrelevant information if the different conditional jumps are uncorrelated, and that the history buffer may not include any bits from the same branch if {{there are many other}} branches in between. It may use a two-level <b>adaptive</b> <b>predictor.</b>|$|E
5000|$|An {{indirect}} {{jump instruction}} can choose among {{more than two}} branches. Some processors have specialized indirect branch predictors. Newer processors from Intel and AMD can predict indirect branches by using a two-level <b>adaptive</b> <b>predictor.</b> This kind of instruction contributes more than one bit to the history buffer. The zBC12 and later z/Architecture processors from IBM support a [...] instruction that can preload the branch predictor entry for a given instruction with a branch target address constructed by adding {{the contents of a}} general-purpose register to an immediate displacement value.|$|E
30|$|This paper {{investigates the}} {{nonlinear}} {{effects of the}} Least Mean Square (LMS) <b>adaptive</b> <b>predictor.</b> Traditional analysis of the adaptive filter ignores the statistical dependence among successive tap-input vectors and bounds {{the performance of the}} adaptive filter by that of the finite-length Wiener filter. It is shown that the nonlinear effects make it possible for an adaptive transversal prediction filter to significantly outperform the finite-length Wiener predictor. An approach is derived to approximate the total steady-state Mean Square Error (MSE) for LMS adaptive predictors with stationary or chirped input signals. This approach shows that, while the nonlinear effect is small for the one-step LMS <b>adaptive</b> <b>predictor,</b> it increases in magnitude as the prediction distance is increased. We also show that the nonlinear effect of the LMS <b>adaptive</b> <b>predictor</b> is more significant than that of the Recursive Least Square <b>adaptive</b> <b>predictor.</b>|$|E
40|$|An {{analysis}} of predictability of a nonlinear and nonstationary ozone time series is provided. For rigour, the deterministic versus stochastic (DVS) analysis is first undertaken {{to detect and}} measure inherent nonlinearity of the data. Based upon this, neural and linear <b>adaptive</b> <b>predictors</b> are compared on this time series for various filter orders, hence indicating the embedding dimension. Simulation results confirm the analysis and show that for this class of air pollution data, neural, especially recurrent neural predictors, perform bes...|$|R
40|$|This {{paper is}} a {{preliminary}} report on {{a study of the}} application of two-dimensional linear prediction in image quantization. The study has focused on three major concerns: implementation of an <b>adaptive</b> linear <b>predictor,</b> <b>adaptive</b> quantization of the prediction error signal, and the adaptive predictive coding of density (logarithm of intensity) images. The results of the study indicate that through th...|$|R
30|$|<b>Adaptive</b> <b>predictors</b> using {{least square}} {{approach}} are also introduced in many papers [17, 18] and applied in reversible data hiding [19, 20]. Edge-directed prediction (EDP) is a least square predictor which optimizes the prediction coefficients locally inside a training set. Kau and Lin [17] proposed edge-look-ahead (ELA) scheme using least square prediction with efficient edge detector {{to maximize the}} edge-directed characteristics. Wu et al. improved the least square predictor by determining the order of predictor and support pixels adaptively [18].|$|R
30|$|The {{coefficients}} for support pixels are computed adaptively by the least square (LS) methods in linear prediction. It {{is one of}} the most advanced types of <b>adaptive</b> <b>predictor,</b> and it normally can provide better performance than fixed predictors [6, 9]. The fixed predictor uses the fixed coefficients. However, <b>adaptive</b> <b>predictor</b> computes the coefficients dynamically according to the context.|$|E
40|$|A new {{approach}} for cancelling carrier wave interference signals in Loran-C receivers is presented. The system, {{which we have}} called an <b>Adaptive</b> <b>Predictor</b> Interference Canceler, {{is based on a}} linear <b>adaptive</b> <b>predictor,</b> which has been implemented using the Normalized LMS algorithm. A big advantage of the proposed method is that no a priori information of the frequency distribution of the interference signals is needed. The result is an adaptive system that automatically implements notches at the frequency of each sinusoid which is present in the input signal. When used in Loran-C receivers, the <b>Adaptive</b> <b>Predictor</b> Interference Canceler provides an efficient and simple method for cancelling Continuous Wave Interference...|$|E
40|$|The {{application}} of Exponential Power Estimation (E P E) technique to realize computationally simple predictor adaptation algorithm has been investigated. An ADPCM system which employs the EPE-based <b>adaptive</b> <b>predictor</b> provides a computational advantage of 2 N multiplications (N being the predictor order) and one division per iteration {{compared to the}} ADPCM which employs a block power estimator based <b>adaptive</b> <b>predictor.</b> Performance comparison has been done with two conventional ADPCM systems at different bit rates and channel conditions, using different quantizer characteristics. The {{results indicate that the}} proposed ADPCM performs identical to the conventional ones at 32 kbps (kilo bits per second) and better than them at 16 kbps...|$|E
5000|$|... #Caption: Figure 3: Two-level <b>adaptive</b> branch <b>predictor.</b> Every {{entry in}} the pattern history table {{represents}} a 2-bit saturating counter of the type shown in figure 2.|$|R
40|$|We {{describe}} {{the results of}} analytic calculations and computer simulations of <b>adaptive</b> <b>predictors</b> (predictive agents) responding to an evolving chaotic environment and to one another. Our simulations are designed to quantify adaptation and to explore co-adaptation for a simple calculable model of a complex adaptive system. We first consider {{the ability of a}} single agent, exposed to a chaotic environment, to model, control, and predict the future states of that environment. We then introduce a second agent which, in attempting to model and control both the chaotic environment and the first agent, modifies the extent to which that agent can identify patterns and exercise control. We find that (i) optimal <b>adaptive</b> <b>predictors</b> have an optimal memory and an optimal complexity, which are small for a a rapidly changing map dynamics and (ii) that the predictive power can be increased by imposing chaos or random noise onto the map dynamics. The competition between the two predictive agents can lead either to chaos, or to metastable emergent behavior, best described as a leader-follower relationship. Our results suggest a correlation between optimal adaptation, optimal complexity, and emergent behavior, and provide preliminary support for the concept of optimal co-adaptation near the edge of chaos. 2...|$|R
40|$|In {{adaptive}} signal processing, joint process estimation {{plays an}} important role in various estimation problems. It is well known that a joint process estimator consists of two structures, namely the orthogonalizer and the regression filter. In literature, orthogonalization step is performed either by orthogonal transformations or by linear predictors. While the orthogonal transformations do not preserve entropy; the predictors, such as the lattice, do preserve it. However, the steady-state performance of such linear predictors is not as good as those of the orthogonal transformations. Lattice filters do not perform perfect orthogonalization when they operate as gradient-based <b>adaptive</b> <b>predictors.</b> In this work, <b>adaptive</b> escalator <b>predictor</b> is proposed to be used as the orthogonalizer of the joint process estimator. The proposed method preserves the entropy and achieves perfect orthogonalization at all times. Moreover it has good steady-state performance compared to those structures utilizing gradient adaptive lattice filters. 1...|$|R
40|$|AbstractA {{reliable}} multi-step predictor is {{very useful}} in industries to forecast the future {{behavior of a}} dynamic system. In this paper, an <b>adaptive</b> <b>predictor</b> is developed based on a novel weighted recurrent neuro-fuzzy paradigm to forecast properties of dynamic systems. An online training technique is adopted to improve forecasting convergence and accommodate different operating conditions. The effectiveness of the developed predictor is evaluated based on some benchmark data set; it is also implemented for machinery system monitoring. The monitoring index is derived from measurement based on a beta kurtosis reference function. The investigation {{results show that the}} developed <b>adaptive</b> <b>predictor</b> is a reliable forecasting tool. It can capture the system's dynamic behavior quickly and track the system's characteristics accurately...|$|E
40|$|Building {{on results}} from data compression, we prove nearly tight bounds {{on how well}} {{sequences}} of length n can be predicted {{in terms of the}} size σ of the alphabet and the length k of the context considered when making predictions. We compare the performance achievable by an <b>adaptive</b> <b>predictor</b> with no advance knowledge of the sequence, to the performance achievable by the optimal static predictor using a table listing the frequency of each (k + 1) -tuple in the sequence. We show that, if the elements of the sequence are chosen uniformly at random, then an <b>adaptive</b> <b>predictor</b> can compete in the expected case if k ≤ logσ n – 3 – ε, for a constant ε > 0, but not if k ≥ logσ n...|$|E
30|$|Least square {{approach}} {{is one of}} the most advanced types of <b>adaptive</b> <b>predictor</b> in reversible data hiding. However, in statistics, it is well known that penalized regression approach which accompanies efficient variable selection can lead to finding smaller and more necessary supports for the purpose of good prediction accuracy.|$|E
40|$|Among the {{hardware}} techniques, two-level <b>adaptive</b> branch <b>predictors</b> with two-bit saturating counters are acknowledged as best branch predictors. They accomplish very competitive performance at low hardware cost. However, {{with the rapid}} of evolution of superscalar processors, the more accurate predictors are desired for more correct branch prediction as one of speculation method. They will lead to higher performance of processors with no doubt. This article presents alternative new and potential neural net methods for branch prediction. The advanced applications of the neural networks more than perceptron or backpropagation are examined as alternative methods. They are radial basis networks, Elman networks and Learning vector quantization (LVQ) networks. I demonstrate that these neural methods can achieve misprediction rate comparable to the conventional two-level <b>adaptive</b> <b>predictors,</b> representatively a Gshare method, without consideration of hardware and prediction latency. I also present {{the effects of the}} history length of the global history shift register (HR) {{and the size of the}} pattern history table (PHT) on the misprediction rate for each neural method. 1...|$|R
40|$|During the 1990 s Two-level <b>Adaptive</b> Branch <b>Predictors</b> were {{developed}} to meet the requirement for accurate branch prediction in high-performance superscalar processors. However, while two-level <b>adaptive</b> <b>predictors</b> achieve very high prediction rates, {{they tend to be}} very costly. In particular, the size of the second level Pattern History Table (PHT) increases exponentially as a function of history register length. Furthermore, many of the prediction counters in a PHT are never used; predictions are frequently generated from non-initialised counters and several branches may access the same counter, resulting in branch interference or aliasing. In this paper, we propose a Cached Correlated Branch Predictor in which the PHT is replaced by a Prediction Cache. Unlike the PHT, the Prediction Cache saves only relevant branch prediction information. Furthermore, predictions are never based on uninitialised entries, and branch interference is eliminated. We simulate two versions of our cached correlated branch predictors, the first uses global branch history information and the second uses local branch history information. We demonstrate that our predictors deliver higher prediction accuracy than conventional predictors at a significantly lower cost...|$|R
40|$|In the {{predictability}} minimization approach (Schmidhuber 1992), input {{patterns are}} fed {{into a system}} consisting of adaptive, initially unstructured feature detectors. There are also <b>adaptive</b> <b>predictors</b> constantly trying to predict current feature detector outputs from other feature detector outputs. Simultaneously, however, the feature detectors try to become as unpredictable as possible, resulting in a co-evolution of predictors and feature detectors. This paper describes {{the implementation of a}} visual processing system trained by semi-linear predictability minimization, and presents many experiments that examine its response to artificial and real-world images. In particular, we observe that under a wide variety of conditions, predictability minimization results in the development of well-known visual feature detectors...|$|R
40|$|A new {{lossless}} predictive image coder {{is introduced}} and tested. The predictions {{are made with}} a nonlinear, vector quantizer based, <b>adaptive</b> <b>predictor.</b> The prediction errors are losslessly compressed with an arithmetic coder that presumes they are Laplacian distributed with variances that are estimated during the prediction process, as in the approach of Howard and Vitter...|$|E
40|$|In this paper, {{we propose}} a context-based <b>adaptive</b> <b>predictor</b> {{for use in}} {{lossless}} image coding. Most often, lossless image coders utilize non-adaptive linear predictors {{for the sake of}} simplicity and to reduce the complexity of the coder. In DPCM-based lossless image coders, adaptivity can result in significant improvements in the performance. However, adaptive prediction is faced with a number of problems chiefly its extensive computational demands. The predictor proposed in this paper allows for a lower computational cost while guaranteeing the stability of the predictor. The Context-Based <b>Adaptive</b> <b>Predictor</b> (CBAP) was found to outperform or at least perform equally as well as the optimum linear predictor for a variety of test images. We should also note that designing an optimum linear predictor requires some knowledge of the image prior to coding while the CBAP requires no such knowledge and operates “on- the-fly’ ’. 1...|$|E
40|$|This paper {{summarizes}} a {{study on}} two-dimensional linear prediction of images and its application to adaptive predictive coding of monochrome images. The study was focused on three major areas: two-dimensional linear prediction of images and its performance, implementation of an <b>adaptive</b> <b>predictor</b> and adaptive quantizer for use in image coding, and linear prediction and adaptive predictive coding of density (logarith...|$|E
40|$|This paper {{describes}} a new coding structure {{based on the}} combination of Vector Quantizati. on, Linear Prediction l) nd Subband Splitting that achieves high guality speech at rates below 10 Kbit/sec. In this scheme, a vector is formed with one sanple of the normalized prediction error of each band and then a vector quanti. zer is applied to it. This guantization of the prediction error a. llows to use scalar <b>adaptive</b> <b>predictors</b> while conserving {{the advantages of the}} vector guantization. The necessary noise shap~ for achjev:ing high subjective quality is obtained by the use of a Freguency-Weighted distance in thc vecto r guantizerPeer ReviewedPostprint (published version...|$|R
40|$|Second-order {{statistics}} of the received signal {{can be used}} to equalize a communication channel without knowledge of the transmitted sequence. Blind zero-forcing (ZF) and mini- mum mean-square error (MMSE) equalization can be achieved with linear prediction-error filtering. The equivalence with the equalizers derived by Giannakis and Halford [1] is shown and <b>adaptive</b> <b>predictors</b> that result in a lattice filtering structure are applied. The required channel coefficient vector is obtained with adaptive eigen-pair tracking. Either forward or backward prediction errors can be used. The performance of the blind equalizer is examined by simulations. The MMSE of the optimum FSE is approached and the algorithm exhibits robustness to channels with common subchannel zeros...|$|R
30|$|Reversible {{watermarking}} {{is a kind}} {{of digital}} watermarking which is able to recover the original image exactly as well as extracting hidden message. Many algorithms have aimed at lower image distortion in higher embedding capacity. In the reversible data hiding, the role of efficient predictors is crucial. Recently, <b>adaptive</b> <b>predictors</b> using least square approach have been proposed to overcome the limitation of the fixed predictors. This paper proposes a novel reversible data hiding algorithm using least square predictor via least absolute shrinkage and selection operator (LASSO). This predictor is dynamic in nature rather than fixed. Experimental results show that the proposed method outperforms the previous methods including some algorithms which are based on the least square predictors.|$|R
