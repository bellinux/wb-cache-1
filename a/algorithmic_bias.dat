11|10|Public
50|$|Applications of {{retrievability}} include detecting {{search engine}} bias, measuring <b>algorithmic</b> <b>bias,</b> evaluating {{the influence of}} search technology, tuning information retrieval systems and {{evaluating the quality of}} documents in a collection.|$|E
40|$|In {{the context}} of the IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems, and with support from its {{executive}} director, the author have proposed {{the development of a new}} IEEE Standard on <b>Algorithmic</b> <b>Bias</b> Considerations. The aim is for this to become part of a set of ethical design standards, such as the IEEE P 7001 ™ Standards Project called Transparency of Autonomous Systems with a Working Group. Whereas the Transparency of Autonomous Systems Standard will be focused on the important issue of “breaking open the black box” for users and/or regulators, the <b>Algorithmic</b> <b>Bias</b> Standard is focused on “surfacing” and evaluating societal implications of the outcomes of algorithmic systems, with the aim of countering non-operationally-justified results...|$|E
40|$|Dr. Safiya Umoja Noble is an {{assistant}} professor in the Department of Information Studies in the Graduate School of Education and Information Studies at UCLA. She also holds appointments in the Departments of African American Studies, Gender Studies, and Education. Her research on the design and use of applications on the Internet is at the intersection of race, gender, culture, and technology. She is currently working on a monograph on racist and sexist <b>algorithmic</b> <b>bias</b> in search engines like Google (forthcoming, NYU Press). She currently serves as an Associate Editor for the Journal of Critical Library and Information Studies, and is the co-editor of two books: The Intersectional Internet: Race, Sex, Culture and Class Online (Peter Lang, Digital Formations, 2016), and Emotions, Technology 2 ̆ 6 Design (Elsevier, 2015) ...|$|E
30|$|Popularity {{prediction}} as {{a scientific}} problem is, to some extent, {{a response to the}} age-old problem of information saturation [15]. Since its development, the World Wide Web has only served to intensify Herbert Simon’s dictum that an overload of information leads to a poverty of attention [16]. Deciding which sources to attend to is a daily struggle for users [17, 18] and is open to numerous biases including subconscious human, as well as <b>algorithmic</b> <b>biases</b> [19].|$|R
40|$|Abstract Background Classifying the fungal and viral {{content of}} a sample is an {{important}} component of analyzing microbial communities in environmental media. Therefore, a method to classify any fragment from these organisms' DNA should be implemented. Results We update the näive Bayes classification (NBC) tool to classify reads originating from viral and fungal organisms. NBC classifies a fungal dataset similarly to Basic Local Alignment Search Tool (BLAST) and the Ribosomal Database Project (RDP) classifier. We also show NBC's similarities and differences to RDP on a fungal large subunit (LSU) ribosomal DNA dataset. For viruses in the training database, strain classification accuracy is 98 %, while for those reads originating from sequences not in the database, the order-level accuracy is 78 %, where order indicates the taxonomic level in the tree of life. Conclusions In addition to being competitive to other classifiers available, NBC has the potential to handle reads originating from any location in the genome. We recommend using the Bacteria/Archaea, Fungal, and Virus databases separately due to <b>algorithmic</b> <b>biases</b> towards long genomes. The tool is publicly available at: [URL]. </p...|$|R
5000|$|The intuition {{for this}} family of {{algorithms}} {{can come from}} an extension of Von-Neumann's solution for the problem of obtaining fair results from a biased coin. In this approach to <b>algorithmic</b> cooling, the <b>bias</b> of the qubits is merely a probability bias, or the [...] "unfairness" [...] of a coin.|$|R
30|$|Indeed, {{the smart}} city is {{bound up in}} {{fine-grained}} monitoring and the predictive modelling of the actions and movements of its “citizens”. It {{is not the case}} that it is an “individual”, as in a single person, that is “targeted” through various techniques and technologies in the Foucauldian sense (Foucault 1977, 2009). Rather, that what a citizen “is” is the product of algorithmic neural network modelling and aggregation from which new indices of normality, deviance, and risk arise. These are then reapplied or “folded back” on to, and into, the everyday lives of people. The “normal” way to talk and walk, for example, is being built through digital modelling (Amoore and Raley 2017). These algorithmic calculations and statistical representations feed into policing techniques on the ground (Sanders and Sheptycki 2017). Following the positivist logic of evidence led policy, data is assumed to have value neutrality. Yet, techniques such as predictive policing target the usual suspects in poverty-stricken areas creating a self-confirming <b>algorithmic</b> <b>bias</b> (Hannah-Moffat 2018). Nonetheless, the drive towards the uptake of these systems is relentless, see Barbuta (2017) for recommendations.|$|E
40|$|Due to {{the recent}} cases of <b>algorithmic</b> <b>bias</b> in {{data-driven}} decision-making, machine learning methods are being put under the microscope {{in order to understand}} the root cause of these biases and how to correct them. Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set. Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model). Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness. However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set - an important and classic requirement in machine learning. Can diversity and fairness be simultaneously ensured? We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required. Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples. Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds...|$|E
40|$|Predictive {{modeling}} {{is increasingly}} being employed to assist human decision-makers. One purported advantage of replacing human judgment with computer models in high stakes settings [...] such as sentencing, hiring, policing, college admissions, and parole decisions [...] is the perceived "neutrality" of computers. It {{is argued that}} because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it, since training data were inevitably generated by {{a process that is}} itself biased. In this paper, we provide a probabilistic definition of <b>algorithmic</b> <b>bias.</b> We propose a method to remove bias from predictive models by removing all information regarding protected variables from the permitted training data. Unlike previous work in this area, our framework is general enough to accommodate arbitrary data types, e. g. binary, continuous, etc. Motivated by models currently in use {{in the criminal justice system}} that inform decisions on pre-trial release and paroling, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce "race-neutral" predictions of re-arrest. In the process, we demonstrate that the most common approach to creating "race-neutral" models [...] omitting race as a covariate [...] still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy...|$|E
40|$|A {{new class}} of high-contrast image {{analysis}} algorithms that empirically fit and subtract systematic noise has lead to recent discoveries of faint exoplanet /substellar companions and scattered light images of circumstellar disks. These methods are extremely efficient at enhancing the detectability of faint astrophysical signal, but they do generally create systematic biases in their observed properties. This paper provides a general solution for this outstanding problem. We present the analytical derivation of a linear expansion that captures the impact of astrophysical over-subtraction and/or self-subtraction these image analysis techniques. We examine the general case for which the reference images of the astrophysical scene move azimuthally and/or radially across {{the field of view}} {{as a result of the}} observation strategy. Our new method is based on perturbing the covariance matrix underlying any least-squares speckles problem, and propagating this perturbation through the data analysis algorithm. We then demonstrate practical applications of this new algorithm. We first consider the case of the spectral extraction of faint point sources in IFS data and illustrate, using public Gemini Planet Imager commissioning data, that our novel perturbation-based Karhunen-Loève Image Processing Forward Modeling (KLIP-FM) can indeed alleviate <b>algorithmic</b> <b>biases.</b> We then apply KLIP-FM to the problem associated with the detection of point sources. We show how it decreases the rate of false negatives (e. g missed planets) while keeping the rate of false positives unchanged when compared to classical least-squares fitting methods. This can potentially have important consequences on the design of follow-up strategies of ongoing direct imaging surveys...|$|R
40|$|Abstract. In this paper, we {{describe}} brkgaAPI, an efficient and easy-to-use object oriented {{application programming interface}} for the <b>algorithmic</b> framework of <b>biased</b> random-key genetic algorithms. Our cross-platform library automatically handles the large portion of problem-independent modules {{that are part of}} the framework, including population management and evolutionary dynamics, leaving to the user the task of implementing a problem-dependent procedure to convert a vector of random keys into a solution to the underlying optimization problem. Our implementation is written in the C++ programming language and may benefit from shared-memory parallelism when available. 1...|$|R
40|$|Accurately {{predicting}} {{the outcome of}} sporting events has been a goal for many groups who seek to maximize profit. What makes this challenging is that the outcome of an event can be influenced by many factors that dynamically change across time. Oddsmakers attempt to estimate these factors by using both algorithmic and subjective methods to set the spread. However, it is well-known that both human and algorithmic decision-making can be biased, so this paper explores if oddsmaker biases {{can be used in}} an exploitative manner, in order to improve the prediction of NFL game outcomes. Real-world gambling data was used to train and test different predictive models under varying assumptions. The results show that methods that leverage oddsmaker biases in an exploitative manner perform best under the conditions tested in this paper. These findings suggest that leveraging human and <b>algorithmic</b> decision <b>biases</b> in an exploitative manner may be useful for {{predicting the}} outcomes of competitive events, and could lead to increased profit for those who have financial interest in the outcomes...|$|R
40|$|<b>Algorithmic</b> <b>bias</b> {{presents}} a difficult challenge within Information Retrieval. Long {{has it been}} known that certain algorithms favour particular documents due to attributes of these documents that are {{not directly related to}} relevance. The evaluation of bias has recently been made possible through the use of retrievability, a quantifiable measure of bias. While evaluating bias is relatively novel, the evaluation of performance has been common since the dawn of the Cranfield approach and TREC. To evaluate performance, a pool of documents to be judged by human assessors is created from the collection. This pooling approach has faced accusations of bias {{due to the fact that}} the state of the art algorithms were used to create it, thus the inclusion of biases associated with these algorithms may be included in the pool. The introduction of retrievability has provided a mechanism to evaluate the bias of these pools. This work evaluates the varying degrees of bias present in the groups of relevant and non-relevant documents for topics. The differentiating power of a system is also evaluated by examining the documents from the pool that are retrieved for each topic. The analysis finds that the systems that perform better, tend to have a higher chance of retrieving a relevant document rather than a non-relevant document for a topic prior to retrieval, indicating that retrieval systems which perform better at TREC are already predisposed to agree with the judgements regardless of the query posed...|$|E
40|$|Situated at the {{intersection}} of information technology, advertising and creativity theory, this thesis presents a detailed picture of the influence of autonomous software applications on the creative process of advertising art directors and copywriters. These applications, which are known in the field of information technology as ‘intelligent agents,’ commonly possess the ability to learn from the user and autonomously pursue their own goals. The search engine Google, which employs intelligent agency to pre-empt and personalise search results based on the collective and individual behaviour of users, is the investigation’s focal point due to its widespread use in the production of creative advertising. To understand how intelligent agents are deployed and received in industry practice, the thesis is organised around a qualitative study comprised of semi-structured interviews with eighteen art directors and copywriters in three Australian capital cities. The results from this study are analysed in terms of both theories of creative practice (stage-based and systems models) and the network society. The thesis finds that Google search provides participants with the ability to conveniently and quickly access converged media content that can evoke ideas for advertisements either immediately or at some stage in the future. However, an <b>algorithmic</b> <b>bias</b> towards the presentation of the most popular and familiar search results, and thus online content, emerges as a less obvious consequence of using a search engine with intelligent agent capabilities. The algorithms responsible for Google’s intelligent agency result in two tendencies. The first is the flattening of ideation possibilities evoked by Google’s pre-emption of users’ search intentions and personalisation of their search results. And secondly, the deployment of increasingly autonomous software applications that valorise efficiency, speed and new forms of flexibility are subtly shaping the institutional context in which advertising is produced. These outcomes are unfolding with limited practitioner awareness of the affordances and influences of software with intelligent agency on the creative process...|$|E
30|$|Algorithmic {{data mining}} also poses {{considerable}} conceptual challenges. Many papers claimed that automatic {{decision making and}} profiling are reshaping the concept of discrimination, beyond legally accepted definitions. In the United States (US), for example, Barocas and Selbst [8] claimed that <b>algorithmic</b> <b>bias</b> and automatization are blurring notions of motive, intention and knowledge, {{making it difficult for}} the US doctrine on disparate impact and disparate treatment to be used to evaluate and persecute causes of algorithmic discrimination. One article [48], discussing European Union (EU) regulation, argued {{that it is necessary to}} rethink discrimination in the context of data driven profiling, since the production of arbitrary categories in data mining technologies and the automatic correlation of the individual’s attributes by the algorithm differ from traditional profiling, which is based on the establishment of a causal chain developed by human logic. Some articles have also pointed out that concepts like “identity” and “group” are being transformed by data mining technologies. de Vries argued that individual identity is increasingly shaped by profiling algorithms and ambient intelligence in terms of increased grouping created in accordance with algorithms’ arbitrary correlations, which sort individuals into a virtual, probabilistic “community “or “crowd” [27]. This typology of “group” or “crowd” differs from the traditional understanding of groups, since the people involved in the “group” might not be aware of (1) their membership to that group, (2) the reasons behind their association with that group and, most importantly, (3) the consequences of being part of that group [54]. Two other concepts are being reshaped by data technologies. The first is the concept of border [1], which is no longer a physical and static divider between countries but has become a pervasive and invisible entity embedded in bureaucratic processes and the administration of the state due to Big Data surveillance tools such as electronic passports and airport security measures. The second is the concept of disability, which needs to be broadened to include all diseases and health conditions, such as obesity, high blood pressure and minor cardiac conditions, which might result in discriminatory outcomes from automatic classifiers through algorithmic correlation with more serious diseases [37, 38].|$|E
40|$|As one {{component}} of a ground validation system to meet requirements for the upcoming Global Precipitation Measurement (GPM) mission, a quasi-operational prototype a system to compare satellite- and ground-based radar measurements has been developed. This prototype, the GPM Validation Network (VN), acquires data from the Precipitation Radar (PR) on the Tropical Rainfall Measuring Mission (TRMM) satellite and from ground radar (GR) networks in the continental U. S. and participating international sites. PR data serve as a surrogate for similar observations from the Dual-frequency Precipitation Radar (DPR) to be present on GPM. Primary goals of the VN prototype are to understand and characterize the variability and bias of precipitation retrievals between the PR and GR in various precipitation regimes at large scales, and to improve precipitation retrieval algorithms for the GPM instruments. The current VN capabilities concentrate on comparisons of the base reflectivity observations between the PR and GR, and include support for rain rate comparisons. The VN algorithm resamples PR and GR reflectivity and other 2 -D and 3 -D data fields to irregular common volumes defined by the geometric intersection of the instrument observations, and performs statistical comparisons of PR and GR reflectivity and estimated rain rates. <b>Algorithmic</b> <b>biases</b> and uncertainties introduced by traditional data analysis techniques are minimized by not performing interpolation or extrapolation of data to a fixed grid. The core VN dataset consists of WSR- 88 D GR data and matching PR orbit subset data covering 21 sites in the southeastern U. S., from August, 2006 to the present. On average, about 3. 5 overpass events per month for these WSR- 88 D sites meet VN criteria for significant precipitation, and have matching PR and GR data available. This large statistical sample has allowed the relative calibration accuracy and stability of the individual ground radars, {{and the quality of}} the PR reflectivity attenuation correction in convective and stratiform precipitation to be evaluated. We will present results of PR-GR reflectivity and rain rate bias comparisons for each OR site, and for different rain types, for the full data set and as time series. The capabilities of the statistical analysis and vertical cross section tools for display and analysis of individual site overpass event data will be described, and examples of the tools' outputs will be shown...|$|R
40|$|To {{understand}} how the components of a complex system like the biological cell interact and regulate each other, we need to collect data for how the components respond to system perturbations. Such data can then be used to solve the inverse problem of inferring a network that describes how the pieces influence each other. The work in this thesis deals with modelling the cell regulatory system, often represented as a network, with tools and concepts derived from systems biology. The first investigation focuses on network sparsity and <b>algorithmic</b> <b>biases</b> introduced by penalised network inference procedures. Many contemporary network inference methods rely on a sparsity parameter such as the L 1 penalty term used in the LASSO. However, a poor choice of the sparsity parameter can give highly incorrect network estimates. In order to avoid such poor choices, we devised a method to optimise the sparsity parameter, which maximises {{the accuracy of the}} inferred network. We showed that it is effective on in silico data sets with a reasonable level of informativeness and demonstrated that accurate prediction of network sparsity is key to elucidate the correct network parameters. The second investigation focuses on how knowledge from association networks can be transferred to regulatory network inference procedures. It is common that the quality of expression data is inadequate for reliable gene regulatory network inference. Therefore, we constructed an algorithm to incorporate prior knowledge and demonstrated that it increases the accuracy of network inference when the quality of the data is low. The third investigation aimed to understand the influence of system and data properties on network inference accuracy. L 1 regularisation methods commonly produce poor network estimates when the data used for inference is ill-conditioned, even when the signal to noise ratio is so high that all links in the network can be proven to exist for the given significance. In this study we elucidated some general principles for under what conditions we expect strongly degraded accuracy. Moreover, it allowed us to estimate expected accuracy from conditions of simulated data, which was used to predict the performance of inference algorithms on biological data. Finally, we built a software package GeneSPIDER for solving problems encountered during previous investigations. The software package supports highly controllable network and data generation as well as data analysis and exploration in the context of network inference. At the time of the doctoral defense, the following paper was unpublished and had a status as follows: Paper 4 : Manuscript.  </p...|$|R
40|$|Algorithms {{that favor}} popular items {{are used to}} help us select among many choices, from {{engaging}} articles on a social media news feed to songs and books that others have purchased, and from top-raked search engine results to highly-cited scientific papers. The goal of these algorithms is to identify high-quality items such as reliable news, beautiful movies, prestigious information sources, and important discoveries [...] - in short, high-quality content should rank at the top. Prior work has shown that choosing what is popular may amplify random fluctuations and ultimately lead to sub-optimal rankings. Nonetheless, it is often assumed that recommending what is popular will help high-quality content "bubble up" in practice. Here we identify {{the conditions in which}} popularity may be a viable proxy for quality content by studying a simple model of cultural market endowed with an intrinsic notion of quality. A parameter representing the cognitive cost of exploration controls the critical trade-off between quality and popularity. We find a regime of intermediate exploration cost where an optimal balance exists, such that choosing what is popular actually promotes high-quality items to the top. Outside of these limits, however, popularity bias is more likely to hinder quality. These findings clarify the effects of <b>algorithmic</b> popularity <b>bias</b> on quality outcomes, and may inform the design of more principled mechanisms for techno-social cultural markets...|$|R
40|$|Algorithms, {{particularly}} {{of the machine}} learning (ML) variety, are increasingly important to individuals' lives, but have caused a range of concerns evolving mainly around unfairness, discrimination and opacity. Transparency {{in the form of}} a "right to an explanation" has emerged as a compellingly attractive remedy since it intuitively presents as a means to "open the black box", hence allowing individual challenge and redress, as well as potential to instil accountability to the public in ML systems. In the general furore over <b>algorithmic</b> <b>bias</b> and other issues laid out in section 2, any remedy in a storm has looked attractive. However, we argue that a right to an explanation in the GDPR is unlikely to be a complete remedy to algorithmic harms, particularly in some of the core "algorithmic war stories" that have shaped recent attitudes in this domain. We present several reasons for this conclusion. First (section 3), the law is restrictive on when any explanation-related right can be triggered, and in many places is unclear, or even seems paradoxical. Second (section 4), even were some of these restrictions to be navigated, the way that explanations are conceived of legally — as "meaningful information about the logic of processing" — is unlikely to be provided by the kind of ML "explanations" computer scientists have been developing. ML explanations are restricted both by the type of explanation sought, the multi-dimensionality of the domain and the type of user seeking an explanation. However “subject-centric" explanations (SCEs), which restrict explanations to particular regions of a model around a query, show promise for interactive exploration, as do pedagogical rather than decompositional explanations in dodging developers' worries of IP or trade secrets disclosure. As an interim conclusion then, while convinced that recent research in ML explanations shows promise, we fear that the search for a "right to an explanation" in the GDPR may be at best distracting, and at worst nurture a new kind of "transparency fallacy". However, in our final sections, we argue that other parts of the GDPR related (i) to other individual rights including the right to erasure ("right to be forgotten") and the right to data portability and (ii) to privacy by design, Data Protection Impact Assessments and certification and privacy seals, may have the seeds we can use to build a more responsible, explicable and user-friendly algorithmic society...|$|E
40|$|Goals Over {{the past}} decade {{there has been a}} growing public fascination with the complex "{{connectedness}}" of modern society. This connectedness is found in many contexts: in the rapid growth of the Internet and the Web, in the ease with which global communication now takes place, and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity. These are phenomena that involve networks and the aggregate behavior of groups of people; they are based on the links that connect us {{and the ways in which}} each of our decisions can have subtle consequences for the outcomes of everyone else. This crash course is an introduction to the analysis of complex networks, made possible by the availability of big data, with a special focus on the social network and its structure and function. Drawing on ideas from computing and information science, complex systems, mathematic and statistical modelling, economics and sociology, this lecture sketchily describes the emerging field of study that is growing at the interface of all these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected. Syllabus •Big graph data and social, information, biological and technological networks •The architecture of complexity and how real networks differ from random networks: node degree and long tails, social distance and small worlds, clustering and triadic closure. Comparing real networks and random graphs. The main models of network science: small world and preferential attachment. •Strong and weak ties, community structure and long-range bridges. Robustness of networks to failures and attacks. Cascades and spreading. Network models for diffusion and epidemics. The strength of weak ties for the diffusion of information. The strength of strong ties for the diffusion of innovation. •Practical network analytics with Cytoscape and Gephi. Simulation of network processes with NetLogo. Reference Textbooks David Easley, Jon Kleinberg: Networks, Crowds, and Markets (2010) [URL] Albert-Laszlo Barabasi. Network Science (2016) [URL] Network Analytics Software Visual Analytics: Cytoscape [URL] Gephi [URL] Network Simulation: NetLogo [URL] Data science ethics & privacy-preserving analytics Data science created unprecedented opportunities but also new risks. Data science techniques might expose sensitive traits of individuals and invade their privacy, this information could be used to discriminate people based on their presumed characteristics, or profiles. Sophisticated data driven machine learning algorithms yield classification and prediction models of behavioral traits of individuals, such as credit score, insurance risk, health status, personal preferences and orientations, on the basis of personal data disseminated in the digital environment by users, with or sometimes without their awareness. Such automated decision-making systems are often "black boxes", mapping user's features into a class label or a ranking value without exposing the reasons. This is worrying not only for the lack of transparency, which undermines the trust of stakeholders, but also for possible social biases and prejudices hidden in the training data and learned by the algorithms, which may bring to discriminatory decisions or unfair actions. Gartner says that, within 2018, half of business ethics violations will occur through improper use of Big Data analytics. Often, the achievements of data science are the result of re-interpreting available data for analysis goals that differ from the original reasons motivating data collection. Examples include mobile phone call records, originally collected by telecom operators for billing and operations, used for accurate and timely demography and human mobility analysis at country orregional scale. This re-purposing of data clearly shows the importance of legal compliance and data ethics technologies and safeguards to protect privacy and anonymity, secure data, engage users, avoid discrimination and misuse, account for transparency and fair use - to the purpose of seizing the opportunities of data science while controlling the associated risks. This is the focus of my lecture. Syllabus • Fairness, Accountability, Confidentiality, Accuracy: the ethical challenges of data science • Privacy-preserving data mining • Privacy-by-design and data-driven risk assessment • Democratizing data science: centralised vs. user-centric analytics • Personal data analytics, collective awareness • <b>Algorithmic</b> <b>bias</b> and ethical challenges of machine learning • Discrimination-aware data minin...|$|E
40|$|Databases {{of audio}} can {{form the basis}} for new {{algorithmic}} critic systems, applying techniques from the growing field of music information retrieval to meta-creation in algorithmic composition and interactive music systems. In this article, case studies are described where critics are derived from larger audio corpora. In the first scenario, the target music is electronic art music, and two corpuses are used to train model parameters and then compared with each other and against further controls in assessing novel electronic music composed by a separate program. In the second scenario, a “real-world” application is described, where a “jury” of three deliberately and individually <b>biased</b> <b>algorithmic</b> music critics judged the winner of a dubstep remix competition. The third scenario is a live tool for automated in-concert criticism, based on the limited situation of comparing an improvising pianists' playing to that of Keith Jarrett; the technology overlaps that described in the other systems, though now deployed in real time. Alongside description and analysis of these systems, the wider possibilities and implications are discussed...|$|R

