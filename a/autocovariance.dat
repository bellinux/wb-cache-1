696|159|Public
2500|$|The {{autocorrelation}} function, {{more properly}} called the <b>autocovariance</b> function {{unless it is}} normalized in some appropriate fashion, measures {{the strength of the}} correlation between the values of [...] separated by a time lag. This is a way of searching for the correlation of [...] with its own past. It is useful even for other statistical tasks besides the analysis of signals. For example, if [...] represents the temperature at time , one expects a strong correlation with the temperature at a time lag of 24 hours.|$|E
2500|$|Practical {{implementation}} of the Kalman Filter is often difficult due {{to the difficulty of}} getting a good estimate of the noise covariance matrices Qk and Rk. [...] Extensive research has been done in this field to estimate these covariances from data. [...] One practical approach to do this is the <b>autocovariance</b> least-squares (ALS) technique that uses the time-lagged autocovariances of routine operating data to estimate the covariances. [...] The GNU Octave and Matlab code used to calculate the noise covariance matrices using the ALS technique is available online under the GNU General Public License.|$|E
5000|$|In {{the case}} of a multivariate random vector , the <b>autocovariance</b> becomes a square n × n matrix with entries given by [...] and {{commonly}} referred to as the <b>autocovariance</b> matrix associated with vectors [...] and [...]|$|E
40|$|We {{show that}} if a process can be {{obtained}} by filtering an autoregressive process, then the asymptotic distribution of sample <b>autocovariances</b> of the former {{is the same as the}} asymptotic distribution of linear combinations of sample <b>autocovariances</b> of the latter. This result is used to show that for small lags the sample <b>autocovariances</b> of the filtered process have the same asymptotic distribution as estimators utilizing more information (observations on the associated autoregression process and knowledge of the parameters of the filter). In particular, for a Gaussian ARMA process the first few sample <b>autocovariances</b> are jointly asymptotically efficient. Asymptotic efficiency Multivariate ARMA Serial covariances...|$|R
40|$|AbstractWe {{show that}} if a process can be {{obtained}} by filtering an autoregressive process, then the asymptotic distribution of sample <b>autocovariances</b> of the former {{is the same as the}} asymptotic distribution of linear combinations of sample <b>autocovariances</b> of the latter. This result is used to show that for small lags the sample <b>autocovariances</b> of the filtered process have the same asymptotic distribution as estimators utilizing more information (observations on the associated autoregression process and knowledge of the parameters of the filter). In particular, for a Gaussian ARMA process the first few sample <b>autocovariances</b> are jointly asymptotically efficient...|$|R
40|$|In this paper, we {{consider}} a method (splitting) for calculating the <b>autocovariances</b> of fractional integrated processes (ARFIMA) and generalized integrated processes (GARMA). The splitting method {{does not require}} any restriction on the autoregressive roots, and allows fast calculation of the <b>autocovariances</b> of these processes...|$|R
50|$|One way of characterising {{long-range}} and short-range dependent {{stationary process}} is {{in terms of}} their <b>autocovariance</b> functions. For a short-range dependent process, the coupling between values at different times decreases rapidly as the time difference increases. Either the <b>autocovariance</b> drops to zero after a certain time-lag, or it eventually has an exponential decay. In the case of LRD, there is much stronger coupling. The decay of the <b>autocovariance</b> function is power-like and so is slower than exponential.|$|E
5000|$|In {{probability}} theory and statistics, given a stochastic process , the <b>autocovariance</b> {{is a function}} that gives the covariance of the process with itself at pairs of time points. With the usual notation E for the expectation operator, if the process has the mean function , then the <b>autocovariance</b> is given by ...|$|E
50|$|Long-range and {{short-range}} dependent {{processes are}} characterised by their <b>autocovariance</b> functions.|$|E
40|$|AbstractThis paper quantifies {{the form}} of the {{asymptotic}} covariance matrix of the sample <b>autocovariances</b> in a multivariate stationary time series—the classic Bartlett formula. Such quantification is useful in many statistical inferences involving <b>autocovariances.</b> While joint asymptotic normality of the sample <b>autocovariances</b> is well-known in univariate settings, explicit forms of the asymptotic covariances have not been investigated in the general multivariate non-Gaussian case. We fill this gap by providing such an analysis, bookkeeping all skewness terms. Additionally, following a recent univariate paper by Francq and Zakoian, we consider linear processes driven by non-independent errors, a feature that permits consideration of multivariate GARCH processes...|$|R
40|$|This study aims {{to develop}} the limit theorems on the sample <b>autocovariances</b> and sample autocorrelations for certain {{stationary}} infinitely divisible processes. We consider the case where the infinitely divisible process has heavy tail marginals and is generated by a conservative flow. Interestingly, {{the growth rate of}} the sample <b>autocovariances</b> is determined by not only heavy tailedness of the marginals but also memory length of the process. Although this feature was first observed by resnick:samorodnitsky:xue: 2000 for some very specific processes, we will propose a more general framework from the viewpoint of infinite ergodic theory. Consequently, the asymptotics of the sample <b>autocovariances</b> can be more comprehensively discussed. Comment: 26 page...|$|R
40|$|We derive the {{asymptotic}} {{moments of}} the <b>autocovariances</b> of a seasonal time series with the first difference of order s (s [greater-or-equal, slanted] 1) which is stationary. We study these statistics for centered and for uncentered data. Further, we show that the corresponding autocorrelations, at lags that are a multiple of the period s, converge in probability to one. seasonal time series nonstationarity <b>autocovariances</b> autocorrelations asymptotic moments integrated processes...|$|R
5000|$|The <b>autocovariance</b> matrix {{is related}} to the {{autocorrelation}} matrix as follows: ...|$|E
5000|$|<b>Auto{{covariance}},</b> the covariance of {{a signal}} with a time-shifted version of itself ...|$|E
5000|$|... {{is called}} the <b>autocovariance</b> {{function}} of the time series. By the mean zero assumption, ...|$|E
40|$|We {{propose a}} nonparametric method for {{automatically}} selecting {{the number of}} <b>autocovariances</b> to use in computing a heteroskedasticity and autocorrelation consistent covariance matrix. For a given kernel for weighting the <b>autocovariances,</b> we prove that our procedure is asymptotically equivalent to one that is optimal under a mean squared error loss function. Monte Carlo simulations suggest that our procedure performs tolerably well, although it does result in size distortions. ...|$|R
40|$|The {{analysis}} of time series with slowly decaying <b>autocovariances,</b> usually called long-memory processes, {{has been extensively}} studied over the past decade. Time series with slowly decaying periodic <b>autocovariances</b> have caught only limited attention recently. We investigate the ability of wavelet transforms, specifically the discrete wavelet packet transform, to analyze and adequately estimate parameters {{of interest in the}} case of seasonal long memory. We apply our methodology to atmospheric CO 2 measurements collected at the Mauna Loa observatory...|$|R
40|$|The authors {{propose a}} nonparametric method for {{automatically}} selecting {{the number of}} <b>autocovariances</b> to use in computing a heteroskedasticity and autocorrelation consistent covariance matrix. For a given kernel for weighting the <b>autocovariances,</b> they prove that their procedure is asymptotically equivalent to one that is optimal under a mean-squared error loss function. Monte Carlo simulations suggest that the authors' procedure performs tolerably well, although it does result in size distortions. Copyright 1994 by The Review of Economic Studies Limited. ...|$|R
50|$|<b>Autocovariance</b> {{is closely}} related to the more {{commonly}} used autocorrelation of the process in question.|$|E
5000|$|If Y is {{the same}} {{variable}} as X, the above expressions are called the <b>autocovariance</b> and autocorrelation: ...|$|E
5000|$|When {{normalizing}} the <b>autocovariance</b> C of a weakly {{stationary process}} with its variance , one obtains the autocorrelation coefficient ...|$|E
40|$|An {{important}} reason for analyzing panel data is to observe the dynamic nature of an economic variable separately from its time-invariant unobserved heterogeneity. This paper examines how to estimate the <b>autocovariances</b> of a variable separately from its time-invariant unobserved heterogeneity. When both cross-sectional and time series sample sizes tend to infinity, we show that the within-group <b>autocovariances</b> are consistent, although they are severely biased when the time series length is short. The biases have the leading term that converges to the long-run variance of the individual dynamics. This paper develops methods to estimate the long-run variance in panel data settings and to alleviate the biases of the within-group <b>autocovariances</b> based on the proposed long-run variance estimators. Monte Carlo simulations reveal that the procedures developed in this paper effectively reduce the biases of the estimators for small samples. ...|$|R
40|$|We examine {{some aspects}} of {{estimating}} sample <b>autocovariances</b> for spatial processes. Especially, we note that for such processes, {{it is not possible}} to approximate the expectation by the sample mean, like in the case of time series data. Then, we propose a consistent nonparametric estimation of sample <b>autocovariances</b> for an irregularly scattered spatial process, derived from a transformation of the initial process. We also suggest an L_ 2 -consistent weighting matrix. Monte Carlo simulations are used to evaluate the performance of the proposed estimators in finite samples. ...|$|R
40|$|Correlated random {{fields are}} a common way to model {{dependence}} struc- tures in high-dimensional data, especially for data collected in imaging. One important parameter characterizing the degree of dependence is the asymp- totic variance which adds up all <b>autocovariances</b> in the temporal and spatial domain. Especially, it arises in the standardization of test statistics based on partial sums of random fields and thus the construction of tests requires its estimation. In this paper we propose consistent estimators for this parameter for strictly stationary ϕ-mixing random fields with arbitrary dimension of the domain and taking values in a Euclidean space of arbitrary dimension, thus allowing for multivariate random fields. We establish consistency, provide cen- tral limit theorems and show that distributional approximations of related test statistics based on sample <b>autocovariances</b> of random fields {{can be obtained by}} the subsampling approach. As in applications the spatial-temporal correlations are often quite local, such {{that a large number of}} <b>autocovariances</b> vanish or are negligible, we also investigate a thresholding approach where sample <b>autocovariances</b> of small magnitude are omitted. Extensive simulation studies show that the proposed estimators work well in practice and, when used to standardize image test statistics, can provide highly accurate image testing procedures...|$|R
5000|$|The <b>autocovariance</b> matrix is {{much larger}} in 2D than in 1D, {{therefore}} it is limited by memory available.|$|E
5000|$|The {{autocorrelation}} coefficient at lag h is given bywhere ch is the <b>autocovariance</b> functionand c0 is the variance function ...|$|E
5000|$|If {{the process}} is {{continuous}} and purely indeterministic, the <b>autocovariance</b> function can be reconstructed by using the Inverse Fourier transform ...|$|E
40|$|This paper proposes the {{analysis}} of panel data whose dynamic structure is heterogeneous across individuals. Our aim is to estimate the cross-sectional distributions and/or some distributional features of the heterogeneous mean and <b>autocovariances.</b> We do not assume any specific model for the dynamics. Our proposed method is easy to implement. We first compute the sample mean and <b>autocovariances</b> for each individual and then estimate the parameter of interest based on the empirical distributions of the estimated mean and <b>autocovariances.</b> The asymptotic properties of the proposed estimators are investigated using double asymptotics under which both the cross-sectional sample size (N) {{and the length of}} the time series (T) tend to infinity. We prove the functional central limit theorem for the empirical process of the proposed distribution estimator. By using the functional delta method, we also derive the asymptotic distributions of the estimators for various parameters of interest. We show that the distribution estimator exhibits a bias whose order is proportional to 1 /√T. Conversely, when the parameter of interest can be written as the expectation of a smooth function of the heterogeneous mean and/or <b>autocovariances,</b> the bias is of order 1 /T and can be corrected by the jackknife method. The results of Monte Carlo simulations show that our asymptotic results are informative regarding the finitesample properties of the estimators. They also demonstrate that the proposed jackknife bias correction is successful...|$|R
40|$|Autoregressive {{moving-average}} (ARMA) difference equations are ubiquitous {{models for}} short memory time series and have parsimoniously described many stationary series. Variants of ARMA {{models have been}} proposed to describe more exotic series features such as long memory <b>autocovariances,</b> periodic <b>autocovariances,</b> and count support set structures. This review paper enumerates, compares, and contrasts the common variants of ARMA models in today’s literature. After the basic properties of ARMA models are reviewed, we tour ARMA variants that describe seasonal features, long memory behavior, multivariate series, changing variances (stochastic volatility) and integer counts. A list of ARMA variant acronyms is provided...|$|R
40|$|In this {{simulation}} study, I {{compare the}} efficiency and finite sample bias of parameter estimators for popular income dynamic models using various forms of <b>autocovariances.</b> The dynamic models have a random walk or a heterogeneous growth permanent component, a persistent autoregressive component and a white noise transitory component. I compare the estimators using <b>autocovariances</b> in level, first differences (FD), and <b>autocovariances</b> between level and future first differences (LD), where the last one is new {{in the literature of}} income dynamics. To maintain the same information used as in using level covariances, I also augment the FD and LD covariances with level variances in the estimation. The results show that using level covariances can give rise to larger finite sample biases and larger standard errors than using covariances in FD and LD augmented by level variance. Without augmenting the level variances, LD provides more efficient estimators than FD in estimating the non-permanent components. I also show that LD provides a convenient test between random walk and heterogeneous growth models with good power. ...|$|R
5000|$|The {{spectral}} density function is the Fourier transform of the <b>autocovariance</b> function. In discrete terms {{this will be}} the discrete-time Fourier transform: ...|$|E
50|$|Different {{fields of}} study define {{autocorrelation}} differently, {{and not all of}} these definitions are equivalent. In some fields, the term is used interchangeably with <b>autocovariance.</b>|$|E
50|$|Analysis of {{the loads}} on a wind turbine {{can be carried}} out through use of power spectra. A power {{spectrum}} is defined as the power spectral density function of a signal plotted against frequency. The power spectral density function of a plot is defined as the Fourier transform of the covariance function. Regarding analysis of loads, the analysis involves time series, in which case the covariance function becomes the <b>autocovariance</b> function. In the signal processing sense, the <b>autocovariance</b> can be related to the autocorrelation function.|$|E
40|$|We {{consider}} processes {{with second}} order long range dependence resulting from heavy tailed durations. We {{refer to this}} phenomenon as duration- driven long range dependence (DDLRD), {{as opposed to the}} more widely studied linear long range dependence based on fractional differencing of an $iid$ process. We consider in detail two specific processes having DDLRD, originally presented in Taqqu and Levy (1986), and Parke (1999). For these processes, we obtain the limiting distribution of suitably standardized discrete Fourier transforms (DFTs) and sample <b>autocovariances.</b> At low frequencies, the standardized DFTs converge to a stable law, as do the standardized <b>autocovariances</b> at fixed lags. Finite collections of standardized <b>autocovariances</b> at a fixed set of lags converge to a degenerate distribution. The standardized DFTs at high frequencies converge to a Gaussian law. Our asymptotic results are strikingly similar for the two DDLRD processes studied. We calibrate our asymptotic results with a simulation study which also investigates the properties of the semiparametric log periodogram regression estimator of the memory parameter. Long Memory; Structural Change...|$|R
40|$|Analysis of {{economic}} time series often involves correlograms and partial correlograms as graphical descriptions of temporal dependence. Two methods {{are available for}} computing these statistics: one based on autocorrelations {{and the other on}} scaled <b>autocovariances.</b> For stationary time series the resulting plots are nearly identical. When it comes to economic time series that usually exhibit non-stationary features these methods can lead to very different results. This has two consequences: (i) incorrect inferences can be drawn when confusing these concepts; (ii) a better discrimination between stationary and non-stationarity appears when using autocorrelations rather than <b>autocovariances</b> which are commonly used in econometric software. correlogram, covariogram, non-stationary...|$|R
30|$|In this regard, an {{important}} advantage of our frequency domain {{approach is that}} we implicitly compute the required <b>autocovariances</b> without explicitly obtaining the time processes for the unobserved components. Nevertheless, for pedagogical purposes it is of interest to understand those processes.|$|R
