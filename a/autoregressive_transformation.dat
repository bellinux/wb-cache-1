6|1|Public
40|$|The {{errors in}} the linear models which are used so widely by economists may be {{generated}} by mixed moving-average autoregressive processes. If errors on an aggregative equation are generated by a mixed moving-average autoregressive process and the weights of the moving-average component of this process are known, then the least-squares procedure can yield consistent estimators of both signal and autoregressive parameters if two adjustments are made to the equation. The <b>autoregressive</b> <b>transformation</b> is combined with pre-multiplication by a Moore-Penrose inverse based on the known weights of the moving-average component. ...|$|E
40|$|Graduation date: 1972 Edible {{tree nut}} and peanut {{industries}} are important agricultural {{industries in the}} United States. Major tree nuts produced domestically are almonds, filberts, pecans,and walnuts. The United States imports and exports some quantities of these tree nuts and peanuts, {{but at the same}} time the U. S. imports cashew, brazil, pignolia, and pistachio nuts which are not domestically produced. Tariffs on imported nuts are imposed, and there are marketing orders or other programs which are in operation in the marketing of several of the domestically produced nuts. Demand interrelationships between nuts are of importance for pragmatic policy formulation for tariffs for imported nuts and marketing control of domestically produced nuts. The overall objective of the study was to formulate and test economic hypotheses of related demands for eight tree nuts and peanuts, thereby classifying them into categories of substitutes, complements, and independents in demand. The U. S. consumption data on tree nuts and peanuts for the years 1947 - 68 were used for analysis purposes. All the nuts were converted on a shelled basis by using appropriate shelling ratios. Adjustments were made for missing inventories. Results of other studies relevant to this study were investigated and were modified. In order to achieve the overall objective, different econometric models employing the single stage least squares (OLS) approach were used for estimation purposes. A double precision and an extended precision; stepwise, time series, correlogram, and <b>autoregressive</b> <b>transformation</b> algorithms were used for analysis purposes. Stability conditions for estimated demand curves using the Chow-test were tested, and the demand curves were found to be stable over the time period of this study. According to the Kamien-condition, interrelated demands were identified. Tests of the severity of multicollinearity were performed. Second-order autoregressive scheme in the time series data employed was detected by the use of correlograms algorithm and a second-order <b>autoregressive</b> <b>transformation</b> was made to arrive at efficient estimates of the parameters. The study revealed that theoretical hypotheses of the ordinal theory of related demands such as the "Hotelling-condition," and the "Slutsky-Friedman condition," were, in general, inconclusive as a method of classifying tree nuts and peanuts as substitutes, complements and independents in demand. However, pecans and walnuts, pecans and brazil nuts, brazil nuts,and cashews were found to be substitutes; whereas almonds and filberts, pecans and pistachios were complements in demand. Except for some nuts with conflicts in signs, all other nuts were found to be statistically nonsignificant, and therefore, may be classified as independents in demand...|$|E
40|$|Cointegrated bivariate nonstationary {{time series}} are {{considered}} in a fractional context, without allowance for deterministic trends. Both the observable series and the cointegrating error can be fractional processes. The familiar {{situation in which}} the respective integration orders are 1 and 0 is nested, but these values have typically been assumed known. We allow one or more of them to be unknown real values, in which case Robinson and Marinucci (1997, 2001) have justified least squares estimates of the cointegrating vector, as well as narrow-band frequency-domain estimates, which may be less biased. While consistent, these estimates do not always have optimal convergence rates, and they have non-standard limit distributional behaviour. We consider estimates formulated in the frequency domain, that consequently allow {{for a wide variety of}} (parametric) autocorrelation in the short memory input series, as well as time-domain estimates based on <b>autoregressive</b> <b>transformation.</b> Both can be interpreted as approximating generalized least squares and Gaussian maximum likelihood estimates. The estimates share the same limiting distribution, having mixed normal asymptotics (yielding Wald test statistics with χ 2 null limit distributions), irrespective of whether the integration orders are known or unknown, subject in the latter case to their estimation with adequate rates of convergence. The parameters describing the short memory stationary input series are √ n-consistently estimable, but the assumptions imposed on these series are much more general than ones of autoregressive moving average type. A Monte Carlo study of finite-sample performance is included. JEL Classification: C 2...|$|E
40|$|The {{need for}} careful {{specification}} in econometric analyses is well-known. If some {{component of a}} particular specification is incorrect, then conventional inferential procedures may be invalid. In particular, the presence of autocorrelation in linear economic models {{may lead to the}} use of inappropriate formulae for estimates of standard errors of estimated coefficients and for F-statistics which provide the bases for tests of significance. Concern over this problem has generally been represented in recent years by two checks of sensitivity, namely, the calculation of Durbin-Watson d-statistics and the use of either <b>autoregressive</b> <b>transformations</b> of the type introduced by Cochrane and Orcutt (1949) or similar approaches such as the Hildreth-Lu (1960) scan procedure, which can be associated with maximum likelihood methods. At the same time, econometric studies of wage determination provide an excellent illustration of certain specific deficiencies which may be introduced by conventional checks of sensitivity. Clarification of the quantitative significance of these deficiencies depends critically upon adequate investigation of institutional features of the labour market, and we cannot claim to have achieved this final goal. However, incompleteness of knowledge does not preclude a demonstration that current practices in empirical studies of wage-determination leave much to be desired. Two well-known studies of Phillips curves, those of Perry and Bodkin et al., provide the bases for discussion. ...|$|R
40|$|This {{dissertation}} {{consists of}} three essays on time series econometrics, examining some implications of non-stationary data for standard estimation and testing procedures. The first chapter studies the asymptotic distributions of the ordinary least squares (OLS), fixed effects (FE), and first-difference (FD) estimators in the panel cointegration model. We show that all the estimators are asymptotically normal and only OLS estimator is consistent. We use Monte Carlo experiments to examine and compare finite sample properties of OLS, FE, and FD estimators when endogeneity of the regressor and serial correlation in the error terms are included, and {{evaluate the effectiveness of}} the autoregressive transformations in panel cointegraion regression. We find that OLS is preferred in terms of bias and size and <b>autoregressive</b> <b>transformation</b> is not a reliable procedure in terms of reducing bias. The second chapter expands on Entorf 2 ̆ 7 s results [Journal of Econometrics, 80 (1997) 287 - 296] on panel spurious regressions. We replicate Entorf 2 ̆ 7 s experiments and emphasize the role of drifts and the relative number of time series (T) and cross sectional observations (n) in the estimations and tests. Generally, the spurious regressions phenomenon arises even for large n and small T. The third chapter investigates the properties of the pre-test estimation and hypothesis test after pre-testing for a unit root in an ARMA(1, 1) model. We examine the recursive bootstrap and moving-block methods in estimation after pre-test to approximate the distribution of pre-test estimators and compare bootstrap inference with conventional asymptotic inference...|$|E
40|$|Data {{are often}} used to test hypotheses, the outcome of which {{influences}} the final model, and the same data are used in estimating {{the parameters of the}} final model. This paper reviews the properties of such pretest estimators and surveys recent work on optimal pretest critical values. Implications are drawn for applied econometric research. Key words: pretest estimation, sequential estimators. Pretest or sequential estimation occurs whenever the same data are used to select a model by prior testing as well as to estimate the parameters in the final specification, The leading example is the use of stepwise regres-sion programs to add or drop regressors. Other examples include use of an F-test to decide whether to pool data, use of a Durbin-Watson test to decide whether to use an <b>autoregressive</b> <b>transformation</b> on a regres-sion, and F-testing for the degree of the polynomial in an Almon-type distributed lag. Given the nature of economic data, pretest-ing has been and probably will continue to be widely used in spite of sharp criticism. ' Critics often condemn the use of stepwise estimation as "data dredging " or "data grubbing, " and so long as such procedures exist, spuriously "significant " results can be obtained, even starting with statistically independent vectors of data. Upon reflection, everyone would agree that results based on stepwise procedures are not as convincing as they might otherwise be. But many may not be familiar with a substantial body of literature that deals with a formal in-vestigation of the statistical properties of pre-Editor's note: This article was invited by the editor with the intent of improving the statistical practices of authors using empir-ical econometrics. T. Dudley Wallace is {{a professor of economics at}} Duke Univer-sity. The editor of this Journal and his referees were very helpful. The author's failure to accept all their suggestions exonerates them from blame. The author would also like to thank those students in statistics at North Carolina State University who helped him learn what he knows about this subject: Vijay Ashar, Dick Brook. Burt Holland, and Carlos Toro. Thanks to Elma McCorkle for her patience in typing the several drafts. I Good attributed a remark to R, H. Coase, "If you torture the data long enough, it will confess " (p. 14). Coase, in a letter to th...|$|E

