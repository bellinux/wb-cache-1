13|257|Public
5000|$|A {{standard}} {{scenario in}} many computer applications {{is a collection}} of points (measurements, dark pixels in a bit map, etc.) in which one wishes to find a topological feature. Homology can serve as a qualitative tool to search for such a feature, since it is readily computable from combinatorial data such as a simplicial complex. However, the data points have to first be triangulated, meaning one replaces the data with a simplicial complex <b>approximation.</b> <b>Computation</b> of persistent homology involves analysis of homology at different resolutions, registering homology classes (holes) that persist as the resolution is changed. Such features can be used to detect structures of molecules, tumors in X-rays, and cluster structures in complex data.|$|E
40|$|Don’t-Care Computation using k-clause <b>Approximation</b> <b>Computation</b> of the satisfiability and {{observability}} care {{sets for}} a sub-circuit in a Boolean network {{is essentially a}} problem of quantifier elimination in propositional logic. In this paper, we introduce a method of approximate quantifier elimination that computes the strongest over-approximation expressible using clauses of a given length. The method uses a Boolean satisfiability solver in a machine-learning framework. Experiments using the SIS system show that the method can produce useful care set information in cases where earlier approaches are prohibitively costly. ...|$|E
40|$|AbstractWith {{increased}} emergency {{cases in}} the world, the government has realized {{that it is necessary}} to manage and control the information spreading in social network. Recently with the rapid increase of information in society activities, distributed information system engineering such as MapReduce based cloud computing distributed system is becoming more and more important in large scale social network data computing in emergency case management. In this article, a novel MapReduce based SNA (Social Network Analysis) system was proposed on layered distributed system. According to independent identically distributed probability statistics of node Betweenness Centrality, a more efficient betweenness <b>approximation</b> <b>computation</b> method was proposed in this article in very large scale SNA graph from emergency case management with better accuracy...|$|E
5000|$|Prediction and the Inverse of Toeplitz Matrices, Israel Gohberg and H. J. Landau, <b>Approximation</b> and <b>Computation,</b> Int. Series of Numerical Mathematics, R. Zahar (editor), Birkhauser, Boston, 119 (1995), pp. 219 - 230.|$|R
2500|$|In a {{stationary}} Gaussian time series model, the likelihood function is (as usual in Gaussian models) {{a function of}} the associated mean and covariance parameters. With a large number (...) of observations, the (...) covariance matrix may become very large, making computations very costly in practice. However, due to stationarity, the covariance matrix has a rather simple structure, and by using an <b>approximation,</b> <b>computations</b> may be simplified considerably (from [...] to [...] ). The idea effectively boils down to assuming a heteroscedastic zero-mean Gaussian model in Fourier domain; the model formulation is based on the time series' discrete Fourier transform and its power spectral density.|$|R
40|$|We propose an {{extrapolation}} {{technique that}} allows accuracy {{improvement of the}} discrete dipole <b>approximation</b> <b>computations.</b> The performance of this technique was studied empirically based on extensive simulations for 5 test cases using many different discretizations. The quality of the extrapolation improves with refining discretization reaching extraordinary performance especially for cubically shaped particles. A two order of magnitude decrease of error was demonstrated. We also propose estimates of the extrapolation error, which were proven to be reliable. Finally we propose a simple method to directly separate shape and discretization errors and illustrated this for one test case. Comment: 17 pages, 8 figure...|$|R
40|$|Schwarz {{demonstrates}} the reuse of a multiplier partial product array (PPA) to approximate higher-order {{functions such as}} the reciprocal, division, and square root. This work presents techniques to decrease the worst case error for the reciprocal approximation computed on a fixed height PPA. In addition, a compensation table is proposed that when combined with the reciprocal approximation produces a fixed precision result. The design space for a 12 -bit reciprocal is then studied and the area-time tradeoff for three design points is presented. Increasing the reciprocal <b>approximation</b> <b>computation</b> decreases the area needed to implement the function while increasing the overall latency. Finally, {{the applicability of the}} proposed technique to the bipartite ROM reciprocal table is discussed. The proposed technique allows hardware reconfigurability. Programmable inputs for the PPA allow the hardware unit to be reconfigured to compute various higher-order function approximations...|$|E
40|$|Abstract This paper {{presents}} {{three new}} systematic approaches for computing coefficient matrices of the Heffron-Phillips multi-machine model (K 1, …, K 6). The amount of computations needed for conventional and three new approaches are compared by counting number of multiplications and divisions. The advantages of new approaches are: (1) their computation burdens {{are less than}} 73 percent of that of conventional approach, for a reduced network, (2) {{they are able to}} model infinite bus directly, whereas the conventional approach cannot, (3) The second and third approaches are able to account for voltage dependent loads and (4) The third approach preserves network structure and doesn’t deal with q and d components and Blondel-Park transformation. The coefficients of the Heffron-Phillips model for a five-bus network and the New England system are computed by four approaches. The results agree within the bounds of admissible <b>approximation.</b> <b>Computation</b> time...|$|E
40|$|To {{study the}} data {{dependencies}} over heterogeneous data in dataspaces, we define a general dependency form, namely comparable dependencies (cds), which specifies constraints on comparable attributes. It covers the semantics {{of a broad}} class of dependencies in databases, including functional dependencies (fds), metric functional dependencies (mfds), and matching dependencies (mds). As we illustrated, comparable dependencies are useful in real practice of dataspaces, such as semantic query optimization. Due to heterogeneous data in dataspaces, the first question, known as the validation problem, is to tell whether a dependency (almost) holds in a data instance. Unfortunately, as we proved, the validation problem with certain error or confidence guarantee is generally hard. In fact, the confidence validation problem is also np-hard to approximate to within any constant factor. Nevertheless, we develop several approaches for efficient <b>approximation</b> <b>computation,</b> such as greedy and randomized approaches with an approximation bound on th...|$|E
5000|$|In a {{stationary}} Gaussian time series model, the likelihood function is (as usual in Gaussian models) {{a function of}} the associated mean and covariance parameters. With a large number (...) of observations, the (...) covariance matrix may become very large, making computations very costly in practice. However, due to stationarity, the covariance matrix has a rather simple structure, and by using an <b>approximation,</b> <b>computations</b> may be simplified considerably (from [...] to [...] ). The idea effectively boils down to assuming a heteroscedastic zero-mean Gaussian model in Fourier domain; the model formulation is based on the time series' discrete Fourier transform and its power spectral density.|$|R
40|$|We review {{recent works}} on the {{modelling}} of Generalised Parton Distributions within the Dyson-Schwinger formalism. We highlight how covariant computations, using the impulse approximation, allows one to fulfil most of the theoretical constraints of the GPDs. Specific attention is brought to chiral properties and especially the so-called soft pion theorem, and its link with the Axial-Vector Ward-Takahashi identity. The limitation of the impulse approximation are also explained. Beyond impulse <b>approximation</b> <b>computations</b> are reviewed in the forward case. Finally, we stress {{the advantages of the}} overlap of lightcone wave functions, and possible ways to construct covariant GPD models within this framework, in a two-body approximation. Comment: Review paper, 52 pages, 20 figure...|$|R
40|$|We {{propose a}} new exact Euclidean {{distance}} transformation (DT) by propagation, using bucket sorting. A fast but approximate DT is first computed using a coarse neighborhood. A sequence of larger neighborhoods is {{then used to}} gradually improve this <b>approximation.</b> <b>Computations</b> are kept short by restricting {{the use of these}} large neighborhoods to the tile borders in the Voronoi diagram of the image. We assess the computational cost of this new algorithm and show that it is both smaller and less image-dependent than all other DTs recently proposed. Contrary to all other propagation DTs, it appears to remain o(n 2) even in the worst-case scenario. c ○ 1999 Academic Press 1...|$|R
40|$|Penalized {{likelihood}} method can be {{used for}} hazard estimation with lifetime data that are right-censored, left-truncated, and possibly with covariates. In this article, we are concerned with more scalable computation of the method and with the derivation and assessment of certain interval estimates. The asymptotic convergence rates are preserved when the estimation is restricted to certain q-dimensional spaces with q increasing at a much slower rate than the sample size n, and simulation studies are performed to determine default values of q for practical use; the computation cost is of the order O(nq 2). Through a quadratic approximation of the log likelihood, approximate Bayesian confidence intervals can be derived for log hazard, and empirical studies are conducted to assess their properties. The techniques are implemented in open-source R code and real-data example is presented to illustrate the applications of the techniques through the use of the software. Bayesian confidence interval Efficient <b>approximation</b> <b>Computation</b> Hazard Penalized likelihood...|$|E
40|$|Abstract—To study data {{dependencies}} over heterogeneous data in dataspaces, {{we define}} a general dependency form, namely comparable dependencies (CDs), which specifies constraints on comparable attributes. It covers the semantics {{of a broad}} class of dependencies in databases, including functional dependencies (FDs), metric functional dependencies (MFDs), and matching dependencies (MDs). As we illustrated, comparable dependencies are useful in real practice of dataspaces, e. g., semantic query optimization. Due to the heterogeneous data in dataspaces, the first question, known as the validation problem, is {{to determine whether a}} dependency (almost) holds in a data instance. Unfortunately, as we proved, the validation problem with certain error or confidence guarantee is generally hard. In fact, the confidence validation problem is also NP-hard to approximate to within any constant factor. Nevertheless, we develop several approaches for efficient <b>approximation</b> <b>computation,</b> including greedy and randomized approaches with an approximation bound on the maximum number of violations that an object may introduce. Finally, through an extensive experimental evaluation on real data, we verify the superiority of our methods. I...|$|E
40|$|Kernelization algorithms, {{usually a}} {{preprocessing}} step before other more traditional algorithms, are very special {{in the sense}} that they return (reduced) instances, instead of final results. This characteristic excludes the freedom of applying a kernelization algorithm for the weighted version of a problem to its unweighted instances. Thus with only very few special cases, kernelization algorithms have to be studied separately for weigthed and unweighted versions of a single problem. feedback arc set on tournament is currently a very popular problem in recent research of parameterized, as well as <b>approximation</b> <b>computation,</b> and its wide applications in many areas make it appear in all top conferences. The theory of graph modular decompositions is a general approach in the study of graph structures, which only had its surfaces touched in previous work on kernelization algorithms of feedback arc set on tournament. In this paper, we study further properties of graph modular decompositions and apply them to obtain the first linear kernel for the unweighted feedback arc set on tournament problem, which only admits linear kernel in its weighted version, while quadratic kernel for the unweighted. Comment: further improvement under progres...|$|E
40|$|International audienceWe review {{recent works}} on the {{modelling}} of generalised parton distributions within the Dyson-Schwinger formalism. We highlight how covariant computations, using the impulse approximation, allows one to fulfil most of the theoretical constraints of the GPDs. Specific attention is brought to chiral properties and especially the so-called soft pion theorem, and its link with the Axial-Vector Ward-Takahashi identity. The limitation of the impulse approximation are also explained. Beyond impulse <b>approximation</b> <b>computations</b> are reviewed in the forward case. Finally, we stress {{the advantages of the}} overlap of lightcone wave functions, and possible ways to construct covariant GPD models within this framework, in a two-body approximation...|$|R
40|$|AbstractIn {{this work}} we apply the {{high-order}} Discontinuous Galerkin (DG) {{finite element method}} to internal low-Mach number turbulent flows. The method here presented is designed to improve {{the performance of the}} solution in the incompressible limit using an implicit scheme for the temporal integration of the compressible Reynolds Averaged Navier Stokes (RANS) equations. The per- formance of the scheme is demonstrated by solving a well-known test-case consisting of an abrupt axisymmetric expansion using various degrees of polynomial <b>approximation.</b> <b>Computations</b> with k–ω model are performed to assess the modelling capabilities, with high-order accurate DG discretizations of the RANS equations, in presence of non-equilibrium flow conditions...|$|R
40|$|Abstract. The {{standard}} inverse scaling and squaring algorithm for computing {{the matrix}} logarithm begins by transforming the matrix to Schur triangular form {{in order to}} facilitate subsequent matrix square root and Padé <b>approximation</b> <b>computations.</b> A transformation-free form of this method that exploits incomplete Denman–Beavers square root iterations and aims for a specified accuracy (ignoring roundoff) is presented. The error introduced by using approximate square roots is accounted for by a novel splitting lemma for logarithms of matrix products. The number of square root stages and the degree of the finalPadé approximation are chosen to minimize the computationalwork. This new method is attractive for high-performance computation since it uses only the basic building blocks of matrix multiplication, LU factorization and matrix inversion...|$|R
40|$|Real-world {{classification}} {{tasks are}} {{a rich source}} of problems for which traditional approaches {{to the development of a}} computational solution do not easily apply; the main difficulties are: 1. The task is defined by a set of data (input-output pairs) rather than by an abstract specification [...] - they are data-defined problems. 2. It is usually not possible, nor expected, nor required that an implementation be `correct ’ in the traditional sense, i. e. that every valid input will be correctly classified. Indeed, it may be that certain valid inputs do not have an unequivocally correct classification. Such problems vary from classification of persons in a database as likely to purchase a given product, in which case very small success rates can be commercially valuable, and misclassifications amount to no more than a reduction in profit. At the other extreme, an air-traffic control collision avoidance problem may demand 100 % success (to within a demanding probabilistic bound) for potential collision inputs and involve major loss of life when dangerous situations are misclassified. We propose one manifestation of accurate <b>approximation</b> <b>computation</b> (AAC) that might offer an optimal computational solution to a wide variety of such data-defined classification tasks. AAC fundamentals...|$|E
40|$|Abstract. Angular {{orientation}} {{refers to}} the position of a rigid body intrinsic coordinate system relative to a reference coordinate system with the same origin. It is determined with a sequence of rotations needed to move the rigid-body coordinate-system axes initially aligned with the reference coordinate-system axes to their new position. In this paper we present a novel way for representing angular orientation. We define the Simultaneous Orthogonal Rotations Angle (SORA) vector with components equal to the angles of three simultaneous rotations around the coordinate-system axes. The problem of non-commutativity is here avoided. We numerically verify that SORA is equal to the rotation vector – the three simultaneous rotations it comprises are equivalent to a single rotation. The axis of this rotation coincides with the SORA vector while the rotation angle is equal to its magnitude. We further verify that if the coordinate systems are initially aligned, simultaneous rotations around the reference and rigid-body intrinsic axes represent the same angular orientation. Considering the SORA vector, angular orientation of a rigid body can be calculated in a single step thus avoiding the iterative infinitesimal rotation <b>approximation</b> <b>computation.</b> SORA can thus be a very convenient way for angular-orientation representation...|$|E
40|$|In {{this paper}} a new {{approach}} will be discussed for describing land subsidence due to the extraction of hydrocarbons. In order to exploit the smooth and gradual subsidence behavior above deep gas reservoirs, a rather simple parametric spatial-temporal trend model is employed as an approximation of the subsidence bowl at the centimeter level. An optimal fit of the trend model is obtained through an estimation procedure using extensive hypothesis testing. The model is determined from the original multi-epoch levelling data, avoiding the cumbersome connection to stable reference points. Special {{attention is paid to}} the stochastic model, distinguishing levelling measurement noise, stochastic benchmark instability and model imperfections. The least-squares residuals to the levelling data include, amongst measurement noise and individual benchmark instability, the remaining subsidence due to gas extraction, which is not described by the trend <b>approximation.</b> <b>Computation</b> and visualization of this signal could reveal spatial and/or temporal coherent deviations from the trend, that require further analysis in order to facilitate a detailed match with production history. The approach is demonstrated on two cases: a subsidence area above an isolated gas and oil field, and a more complex area with overlapping subsidence bowls above several gas fields. 1...|$|E
40|$|In {{this paper}} {{we present a}} Discontinuous Galerkin (DG) method {{designed}} to improve the accuracy and efficiency of laminar flow simulations at low Mach numbers using a fully implicit scheme. The algorithm {{is based on the}} flux preconditioning approach, which modifies only the dissipative terms of the numerical flux. This formulation is quite simple to implement in existing implicit DG codes, it overcomes the time-stepping restrictions of explicit multistage algorithms, is consistent in time and thus applicable to unsteady flows. The performance of the method is demonstrated by solving a laminar flow past a NACA 0012 airfoil at different low Mach numbers using various degrees of polynomial <b>approximations.</b> <b>Computations</b> with and without flux preconditioning are performed on different grid topologies to analyze the influence of the spatial discretization on the accuracy of the DG solutions at low Mach numbers...|$|R
40|$|We {{consider}} {{the problem of}} ordering perishable inventory when there is uncertainty in both the demand and the lifetime of the product. Under the assumption that units outdate {{in the same order}} in which they enter inventory, it is shown that the structure of the optimal policy is essentially the same as in the case where the lifetime is deterministic. An explicit expression for the expected outdating of any order is derived. Two different bounds on the expected outdating are then used to construct two critical number <b>approximations.</b> <b>Computations</b> for a discrete version of the problem are performed to compare the expected costs of both approximations with the optimal. One approximation appeared to give slightly better results and produced an expected cost generally within a fraction of a percent of the optimal for the cases tested. ...|$|R
40|$|Formulas {{are derived}} for the {{computation}} of the random input-describing functions for MIMO nonlinearities; these straightforward and rigorous derivations {{are based on}} the optimal mean square linear <b>approximation.</b> The <b>computations</b> involve evaluations of multiple integrals. It is shown that, for certain classes of nonlinearities, multiple-integral evaluations are obviated and the computations are significantly simplified...|$|R
40|$|The {{standard}} inverse scaling and squaring algorithm for computing {{the matrix}} logarithm begins by transforming the matrix to Schur triangular form {{in order to}} facilitate subsequent matrix square root and Pad'e <b>approximation</b> <b>computations.</b> A transformation-free form of this method is presented that exploits incomplete Denman [...] Beavers square root iterations and aims for a specified accuracy. The error introduced by using approximate square roots is accounted for by a novel splitting lemma for logarithms of matrix products. The number of square root stages and the degree of the final Pad'e approximation are chosen to minimize the computational work. This new method is attractive for high-performance computation since it uses only the basic building blocks of matrix multiplication, LU factorization and matrix inversion. Key words. matrix logarithm, Pad'e approximation, inverse scaling and squaring method, matrix square root, Denman [...] Beavers iteration AMS subject classifications. 65 F 30 1 [...] ...|$|R
40|$|The Wiener-Hopf {{factorization}} {{is obtained}} in closed form for a phase type approximation to the CGMY Lévy process. This allows, for the <b>approximation,</b> exact <b>computation</b> of first passage times to barrier levels via Laplace transform inversion. Calibration of the CGMY model to market option prices defines the risk neutral process {{for which we}} infer the first passage times of stock prices to 30...|$|R
40|$|Effect of heat {{generation}} on free convection boundary layer flow of a viscoelastic fluid past a horizontal circular cylinder with constant surface heat flux has been investigated. The boundary layer equations are an order {{higher than those}} for the Newtonian (viscous) fluid and the adherence boundary conditions are insufficient to determine the solution of these equations completely. The governing equations are transformed into dimensionless non-similar equations by using a set of suitable transformations and solved numerically by the finite difference method along with Newton's linearization <b>approximation.</b> <b>Computations</b> are performed numerically by using Keller-box method by augmenting an extra boundary condition at infinity. We have focused our attention on the evaluation of velocity profiles, temperature profiles, shear stress in terms of local skin friction {{and the rate of}} heat transfer in terms of local Nusselt number for different values of {{heat generation}} parameter, viscoelastic parameter and the Prandlt number and the numerical results have been shown graphically...|$|R
40|$|Abstract—A new loss {{model for}} {{cognitive}} radio spectrum access with finite user population are presented, and exact {{solution for the}} model and its <b>approximation</b> for <b>computation</b> scalability are given. Our model provides {{the investigation of the}} delay performance of a cognitive radio system. We study the delay performance of a cognitive radio system under various primary traffic loads and spectrum band allocations. Index Terms—Cognitive radio networks, spectrum access, performance evaluation. I...|$|R
40|$|The {{transport}} of infrared radiation {{in a single}} cuboidal cloud is modeled using a variable azimuth two-stream <b>approximation.</b> <b>Computations</b> are made at 10 microns for a Deirmendjian (1969) C- 1 water cloud where the single scattering albedo is equal to 0. 638 and the asymmetry parameter is 0. 865. The {{results indicate that the}} emittance of the top face of the model cloud is always less than that for a plane parallel cloud of the same optical depth. The hemispheric flux escaping from the cloud top possesses a gradient from the center to the edges which are warmer when the cloud is over warmer ground. Cooling rate calculations in the 8 - 13. 6 micron region demonstrate that there is cooling out of the sides of the cloud at all levels even when there is heating of the core from the ground below. The radiances exiting from model cuboidal clouds are computed by path integration over the source function obtained with the two-stream approximation. Results indicate that the brightness temperature measured from finite clouds will overestimate the cloud-top temperature...|$|R
40|$|Well-known, {{respected}} introduction, updated {{to integrate}} concepts and procedures associated with computers. <b>Computation,</b> <b>approximation,</b> interpolation, numerical differentiation and integration, smoothing of data, other topics in lucid presentation. Includes 150 additional {{problems in this}} edition. Bibliography...|$|R
40|$|Alternating-Direction Explicit (A. D. E.) finite-difference methods {{make use}} of two approximations that are {{implemented}} for computations proceeding in alternating directions, e. g., {{from left to right}} and from right to left, with each approximation being explicit in its respective direction of computation. Stable A. D. E. schemes for solving the linear parabolic partial differential equations that model heat diffusion are well-known, as are stable A. D. E. schemes for solving the first-order equations of fluid advection. Several of these are combined here to derive A. D. E. schemes for solving time-dependent advection-diffusion equations, and their stability characteristics are discussed. In each case, it is found that it is the advection term that limits the stability of the scheme. The most stable of the combinations presented comprises an unconditionally stable <b>approximation</b> for <b>computations</b> carried out in the direction of advection of the system, from left to right in this case, and a conditionally stable <b>approximation</b> for <b>computations</b> proceeding in the opposite direction. To illustrate the application of the methods and verify the stability conditions, they are applied to some quasi-linear one-dimensional advection-diffusion problems. © 2007 Wiley Periodicals, Inc. Nume...|$|R
40|$|In {{this work}} we extend the {{high-order}} Discontinuous Galerkin (DG) Finite element method to inviscid low Mach number flows. The method here presented {{is designed to}} improve the accuracy and efficiency of the solution at low Mach numbers using both explicit and implicit schemes for the temporal discretization of the compressible Euler equations. The algorithm {{is based on a}} classical preconditioning technique that in general entails modifying both the instationary term of the governing equations and the dissipative term of the numerical flux function (full preconditioning approach). In the paper we show that full preconditioning is beneficial for explicit time integration while the implicit scheme turns out to be efficient and accurate using just the modified numerical flux function. Thus the implicit scheme could also be used for time accurate computations. The performance of the method is demonstrated by solving an inviscid flow past a NACA 0012 airfoil at different low Mach numbers using various degrees of polynomial <b>approximations.</b> <b>Computations</b> with and without preconditioning are performed on different grid topologies to analyze the influence of the spatial discretization on the accuracy of the DG solutions at low Mach numbers...|$|R
40|$|Abstract [...] We {{present a}} {{generalization}} f the recently developed first-order perturbation <b>approximation</b> for the <b>computation</b> ofradiative ffects uch as layer heating rates and surface fluxes. The {{basis for this}} generalization is Dyson's equation for Green's function or the inverse transport operation...|$|R
40|$|The {{calculation}} of the dipole-quadrupole dispersion coefficient is discussed through a perturbation and a variation method. Accurate combination rules are obtained from both methods, one new and one already known. Further <b>approximations</b> permit <b>computations</b> in terms of accessible parameters. Values are calculated for the interactions of atomic pairs formed from hydrogen, alkali, and rare-gas atoms. A new relation giving the dipole-quadrupole coefficient {{in terms of the}} dipole-dipole coefficient and the dipole and quadrupole polarizabilities seems accurate, but needs further testing...|$|R
40|$|Combinatorial state {{equations}} of generalized two- or three-dimensional compound-lattice models are derived. Analytical expressions are constructed for the pressure-density functions. The <b>approximation</b> allows the <b>computation</b> of phase diagrams {{of a wide}} class of liquid and solid one-component and multicomponent systems consisting of spherical nonpolar molecules...|$|R
40|$|Random {{matching}} {{models with}} di erent states {{are an important}} class of dynamic games; for example, money search models, job search models, and some games in biology are special cases. In this paper, we investigate {{the basic structure of}} the models: the existence of equilibria, the global structure of the set of equilibria, and the <b>approximation</b> and <b>computation</b> of equilibria. Under conditions which are typically satisfied in monetary models, the equilibrium condition can be considered as a non-linear complementarity problem with some new feature. ...|$|R
