7|10000|Public
50|$|For in this {{instance}} the quartic is actually factorable {{as the product of}} two quadratics (x2 + 1)(x2 + 2). These, at last, are irreducible over the rationals (and, indeed, the reals as well in this example); so now we are done; P(x) = (x2 + 1)(x2 + 2)(x &minus; 3). In {{this instance}} it is in fact easy to factor our quartic by treating it as a biquadratic equation; but finding such factorings of <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> can be very difficult.|$|E
50|$|In {{response}} surface methodology, {{the objective is}} to find the relationship between the input variables and the response variables. The process starts from trying to fit a linear regression model. If the P-value turns out to be low, then <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> regression, which is usually quadratic, will be implemented. The process of finding a good relationship between input and response variables will be done for each simulation test. In simulation optimization, {{response surface}} method can be used to find the best input variables that produce desired outcomes in terms of response variables.|$|E
40|$|A {{photographic}} stereopair of the Metric Camera (MC) {{and another}} one of the Large Format Camera (LFC) covering parts of the Red Sea Hills in Sudan were evaluated for height accuracy using the parallax bar heighting technique. X-parallaxes of a number of points on the two stereo models were measured and used to compute the crude heights of the points. Corrections to the crude heights were then computed using different mathematical algorithms and a set of well-defined control points. Discrepancies between corrected (computed) heights and known heights of another set of points were used to compute the root-mean-square errors of height measurement. The results show that Shepard's interpolation or <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> can produce height errors of the order of :t 53 m. This error value may serve a useful purpose in some basic thematic applications where high height accuracies are not of paramount importance...|$|E
40|$|Abstract: In this paper, we derived a new formula using <b>a</b> <b>high</b> <b>degree</b> <b>polynomial</b> (fifth <b>degree</b> curve) in {{representation}} {{ground surface}} profile. Also, we show {{the result of}} calculation of volume by this new formula give a better accuracy because of using <b>high</b> <b>degree</b> curve in representing the ground profile which provides more smoothness in the representation...|$|R
5000|$|Maximum {{likelihood}} {{can also}} be used to estimate the parameters [...] and [...] However, this tends to be complicated by the fact that this requires finding the roots of <b>a</b> <b>high</b> <b>degree</b> <b>polynomial,</b> and there can be multiple roots that represent local maxima. Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples. [...] The log-likelihood function for the Cauchy distribution for sample size [...] is: ...|$|R
30|$|In simple {{mathematical}} formulation, quadratic map exhibits {{very complicated}} dynamical properties [16] ([URL] and concerns the asymptotic behavior of iterates, when n → +∞. Moreover, such features may {{change in a}} dramatic way under variation of the parameter a. This {{is related to the}} fact that for large n, being <b>a</b> <b>high</b> <b>degree</b> <b>polynomial,</b> depends in <b>a</b> complicated way on x and a. The quadratic mapping {{can be used as a}} model for the description of such dynamics with wider scope.|$|R
40|$|Abstract. The {{parameter}} identification for problems where losses arising from overestimation and underestimation are different {{and can be}} described by an asymmetrical and polynomial function is investigated in this paper. The Bayes decision rule allowing to minimize potential losses is used. Calculation algorithms {{are based on the}} nonparametric methodology of statistical kernel estimators, which releases the method from dependence on distribution type. Three basic cases are considered in detail: a linear, a quadratic, and finally a general concept for <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> – here the cube-case is described in detail as an example. For each of them, the final result constitutes a numerical proce-dure enabling to effectively calculate the optimal value of a parameter in question, presented in its complete form which demands neither detailed knowledge of the theoretical aspects nor laborious research of the user. Although the above method was investigated {{from the point of view}} of automatic control problems, it is universal in character and can be applied to a wide range of tasks, also outside the realm of engineering...|$|E
40|$|The Polynomial Texture Map {{framework}} (PTM) {{extends the}} simple model of image formation from the Lambertian variant of Photometric Stereo (PST) to more general reflectances and to more complex-shaped surfaces. It forms an alternative method for apprehending object colour, albedo, and surface normals. Here we consider solving {{such a model}} in a robust version, not to date attempted for PTM, with the upshot that both shadows and specularities are identified automatically {{without the need for}} any thresholds. Instead of the linear model used in few-source PST for Lambertian surfaces, PTM adopts <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> model. PTM has two aims: interpolation of images for new lighting directions, and recovery of surface properties. Here we show that a robust approach is a good deal more accurate in recovering surface properties. For new-lighting interpolation, we demonstrate that a simple radial basis function interpolation can accurately interpolate specularities as well as attached and cast shadows even with a medium-sized image set, with no need for reflectance sharing across pixels or extremely large numbers of interpolation coefficients...|$|E
40|$|The Schwarzian {{derivative}} of a function f(x) which {{is defined in}} the interval (a, b) having higher order derivatives is given by Sf(x) =(f''(x) /f'(x)) '- 1 / 2 (f''(x) /f'(x)) ^ 2. A sufficient condition for a function to behave chaotically is that its Schwarzian derivative is negative. In this paper, we {{try to find a}} sufficient condition for a non-linear dynamical system to behave chaotically. The solution function of this system is <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial.</b> We define n-th Schwarzian derivative to examine its general properties. Our analysis shows that the sufficient condition for chaotic behavior of higher order polynomial is provided if its highest order three terms satisfy an inequality which is invariant under the degree of the polynomial and the condition is represented by Hankel determinant of order 2. Also the n-th order polynomial can be considered to be the partial sum of real variable analytic function. Let this analytic function be the solution of non-linear differential equation, then the sufficient condition for the chaotical behavior of this function is the Hankel determinant of order 2 negative, where the elements of this determinant are the coefficient of the terms of n, n- 1, n- 2 in Taylor expansion. Comment: 8 page...|$|E
40|$|AbstractA {{systematic}} {{description of}} the Carathéodory-Fejér method (CF method) is given for near-best uniform rational approximation of <b>a</b> (<b>high</b> <b>degree)</b> <b>polynomial</b> on <b>a</b> disk. On the basis of Takagi's extension of the Carathéodory-Fejér theorem degeneracies are characterized and it is proven that they appear in the CF table, which is introduced, in square blocks. Related method for complex and real trigonometric rational approximation, and for ordinary real rational approximation on an interval, are then derived. For each problem several types of CF approximation are defined depending on truncation. Certain weight functions are also allowed...|$|R
40|$|Abstract. Evolutions in the {{communication}} technology and Internet have enhanced efficient distributed and scalable systems. Now, with the distribution processing through the Internet, {{it is possible}} to solve problems which time of processing demand was recently prohibitive by centralized solution unviable. Using centralizes processing solution, problems such as, how to find <b>a</b> <b>high</b> <b>degree</b> two-dimensional <b>polynomial</b> that represents a relief of an area, are impracticable. The objective of this project is to develop a methodology for distribution of processing generated by a mathematical formulation that estimate the coefficients of <b>a</b> <b>high</b> <b>degree</b> <b>polynomial</b> to represent the relief of the state of Minas Gerais in Brazil. In order to distribute such processing, a mathematical formulation was developed based in the nonlinear regression model. The next step was the modeling of a system to distribute the processing among interlinked computers through the Internet. The system will be constituted by a coordinating application (responsible for the processing distribution) and several agents applications (to process data received from the coordinating application) ...|$|R
40|$|Recursive maps of {{high order}} of {{convergence}} m (say m= 2 ^ 10 or m= 2 ^ 20) induce certain monotone step functions from {{which one can}} filter relevant information needed to globally separate and compute the real roots of a function on a given interval [a,b]. The process is here called a root distiller. A suitable root distiller has a powerful preconditioning effect enabling the computation, on the whole interval, of accurate roots of <b>an</b> <b>high</b> <b>degree</b> <b>polynomial.</b> Taking as model high-degree inexact Chebyshev polynomials and using the Mathematica system, worked numerical examples are given detailing our distiller algorithm...|$|R
40|$|We study ultrametric germs in one {{variable}} {{having an}} irrationally indifferent fixed {{point at the}} origin with a prescribed multiplier. We show that for many values of the multiplier, the cycles in the unit disk of the corresponding monic quadratic polynomial are "optimal" in the following sense: They minimize {{the distance to the}} origin among cycles of the same minimal period of normalized germs having an irrationally indifferent fixed point at the origin with the same multiplier. We also give examples of multipliers for which the corresponding quadratic polynomial does not have optimal cycles. In those cases we exhibit <b>a</b> <b>higher</b> <b>degree</b> <b>polynomial</b> such that all of its cycles are optimal. The proof of these results reveals a connection between the geometric location of periodic points of ultrametric power series and the lower ramification numbers of wildly ramified field automorphisms. We also give an extension of Sen's theorem on wildly ramified field automorphisms, and a characterization of minimally ramified power series in terms of the iterative residue. Comment: Accepted for publication in Compositio Mathematica; This version is just a minor revision of the 2 nd version (where we improved Theorem E in which we now give a characterization of minimally ramified power series in terms of Ecalle's iterative residue); Key Words: Non-Archimedean dynamical systems, periodic points, rotation domains, ramification theory; Comments welcome; 37 page...|$|E
40|$|Abstract. This is the {{continuation}} of the paper ”central discontinuous Galerkin methods on overlapping cells with a non-oscillatory hierarchical reconstruction ” by the same authors. The hierarchical reconstruction introduced therein is applied to central schemes on overlapping cells and to finite volume schemes on non-staggered grids. This takes a new finite volume approach for approximating non-smooth solutions. A critical step for high order finite volume schemes is to reconstruct <b>a</b> nonoscillatory <b>high</b> <b>degree</b> <b>polynomial</b> approximation in each cell out of nearby cell averages. In the paper this procedure is accomplished in two steps: first to reconstruct <b>a</b> <b>high</b> <b>degree</b> <b>polynomial</b> in each cell by using e. g., a central reconstruction, which is easy to do {{despite the fact that the}} reconstructed polynomial could be oscillatory; then to apply the hierarchical reconstruction to remove the spurious oscillations while maintaining the high resolution. All numerical computations for systems of conservation laws are performed without characteristic decomposition. In particular, we demonstrate that this new approach can generate essentially non-oscillatory solutions even for 5 th order schemes withou...|$|R
40|$|Research resume Numerical {{analysis}} and scientific computing. I {{have developed a}} novel spectral method for linear ordinary and partial differential equations, fast Gauss quadrature routines, a new continuous linear algebra for functions, <b>a</b> <b>high</b> <b>degree</b> bivariate <b>polynomial</b> rootfinder, and a fast Chebyshev–Legendre transform. I also developed the two-dimensional functionality of the software package Chebfun...|$|R
40|$|Positive and {{negative}} quadratic forms {{are well known}} and widely used. They are multivariate homogeneous <b>polynomials</b> of <b>degree</b> two taking positive or negative values respectively for any values of their arguments not all zero. In the present paper <b>a</b> certain <b>higher</b> <b>degree</b> <b>polynomial</b> is associated with each quadratic form such that the form is definite {{if and only if}} this polynomial is sign-definite. Comment: AmSTeX, 5 pages, amsppt styl...|$|R
40|$|In 1955 B. Segre {{showed that}} any oval in a {{projective}} plane over a finite field of odd order is a conic. His proof constructs a conic which matches the oval in some points, and then {{shows that it}} actually coincides with the oval. Here we give another proof. We describe the oval by <b>a</b> possibly <b>high</b> <b>degree</b> <b>polynomial,</b> and then show that the degree is actually 2. Comment: 2 page...|$|R
40|$|This is the {{continuation}} of the paper "central discontinuous Galerkin methods on overlapping cells with a non-oscillatory hierarchical reconstruction" by the same authors. The hierarchical reconstruction introduced therein is applied to central schemes on overlapping cells and to nite volume schemes on non-staggered grids. This takes a new nite volume approach for approximating non-smooth solutions. A critical step for high order nite volume schemes is to reconstruct <b>a</b> nonoscillatory <b>high</b> <b>degree</b> <b>polynomial</b> approximation in each cell out of nearby cell averages. In the paper this procedure is accomplished in two steps: first to reconstruct <b>a</b> <b>high</b> <b>degree</b> <b>polynomial</b> in each cell by using e. g., a central reconstruction, which is easy to do {{despite the fact that the}} reconstructed polynomial could be oscillatory; then to apply the hierarchical reconstruction to remove the spurious oscillations while maintaining the high resolution. All numerical computations for systems of conservation laws are performed without characteristic decomposition. In particular, we demonstrate that this new approach can generate essentially non-oscillatory solutions even for 5 th order schemes without characteristic decomposition. The research of Y. Liu was supported in part by NSF grant DMS- 0511815. The research of C. -W. Shu was supported in part by the Chinese Academy of Sciences while this author was visiting the University of Science and Technology of China (grant 2004 - 1 - 8) and the Institute of Computational Mathematics and Scienti c/Engineering Computing. Additional support was provided by ARO grant W 911 NF- 04 - 1 - 0291 and NSF grant DMS- 0510345. The research of E. Tadmor was supported in part by NSF grant 04 - 07704 and ONR grant N 00014 - 91 -J- 1076. The research of M. Zhang was supported in part by the Chinese Academy of Sciences grant 2004 - 1 - 8...|$|R
40|$|Loss {{tomography}} {{has been}} studied for more than 10 years {{and a number of}} estimators have been proposed. The estimators can be divided into two classes: maximum likelihood and non-maximum likelihood. The maximum likelihood estimators rely on the maximum likelihood principle to ensure the accuracy of the estimates obtained by the estimators. Unfortunately, all of the maximum likelihood estimators need to use an iterative procedure to search the solution space for the maximum or to solve <b>a</b> <b>high</b> <b>degree</b> <b>polynomial.</b> An iterative procedure can be computationally expensive and may even converge to a local maximum. On the other hand, the non-maximum likelihood estimators pursue closed form solutions by scarifying the accuracy of estimates. To overcome the pitfalls, we, in this paper, propose a closed form and maximum likelihood estimator to estimate the loss rate of a link in a network. The closed form solution is built on the discovery of a connection between the number of probes passing a link and the number of probes passing its parent. The proposed estimator is applicable to both the tree topology and the general one. Comment: this paper has been withdrawn by the author becausse it has been published in a conferenc...|$|R
40|$|We {{construct}} some infinite {{series of}} free and nearly free curves using pencils of conics with a base locus of cardinality at most two. These curves {{have an interesting}} topology, e. g. <b>a</b> <b>high</b> <b>degree</b> Alexander <b>polynomial</b> that can be explicitly determined, a Milnor fiber homotopy equivalent to a bouquet of circles, or an irreducible translated component in the characteristic variety of their complement. Monodromy eigenspaces in the first cohomology group of the corresponding Milnor fibers are also {{described in terms of}} explicit differential forms. Comment: v 3. Additional references adde...|$|R
40|$|A {{finite element}} method for solving {{nonlinear}} differential equations on a grid, with po-tential applicability to computational fluid dynamics (CFD), is developed and tested. The current method facilitates the computation of solutions of <b>a</b> <b>high</b> <b>polynomial</b> <b>degree</b> on <b>a</b> grid. <b>A</b> <b>high</b> <b>polynomial</b> <b>degree</b> is achieved by interpolating both the value, {{and the value of}} the derivatives up to a given order, of continuously distributed unknown variables. The two-dimensional lid-driven cavity, a common benchmark problem for CFD methods, is used as a test case. It is shown that increasing the <b>polynomial</b> <b>degree</b> has some advan-tages, compared to increasing the number of grid-points, when solving the given benchmark problem using the current method. The current method yields results which agree well with previously published results for this test case. ...|$|R
40|$|In this paper, we {{consider}} {{the existence of a}} factorization of a monic, bounded motion polynomial. We prove existence of factorizations, possibly after multiplication with a real polynomial and provide algorithms for computing polynomial factor and factorizations. The first algorithm is conceptually simpler but may require <b>a</b> <b>high</b> <b>degree</b> of the <b>polynomial</b> factor. The second algorithm gives an optimal degree...|$|R
40|$|In this paper, I {{describe}} the implementation details of some functional, triangular data fitting schemes. The schemes in question use derivative information to find initial settings of control points, giving a C 0, piecewise <b>polynomial</b> surface with <b>a</b> <b>high</b> <b>degree</b> of <b>polynomial</b> precision. The interior control points are then modified {{to increase the}} continuity between patches without decreasing the polynomial precision. In implementing these schemes, I had to address several issues, including basis conversion for bivariate functions, finding the weights of control points used to compute derivatives, {{and the construction of}} data sets for testing. In addition, I developed and tested a new scheme that uses fewer derivatives than the other schemes discussed in this paper. ...|$|R
40|$|Lanczos-type {{algorithms}} are {{efficient and}} easy to implement. Unfortunately they breakdown frequently and well before convergence has been achieved. These algorithms are typically based on recurrence relations which involve formal orthogonal <b>polynomials</b> of low <b>degree.</b> In this paper, we consider a recurrence relation {{that has not been}} studied before and which involves <b>a</b> relatively <b>higher</b> <b>degree</b> <b>polynomial.</b> Interestingly, it leads to an algorithm that shows superior stability when compared to existing Lanczos-type algorithms. This new algorithm is derived and described. It is then compared to the best known algorithms of this type, namely A 5 /B 10, A 8 /B 10, as well as Arnoldi's algorithm, on a set of standard test problems. Numerical results are included. Comment: arXiv admin note: text overlap with arXiv: 1403. 032...|$|R
40|$|The {{notion of}} non-classical {{correlations}} {{is a powerful}} contrivance for explaining phenomena exhibited in quantum systems. It is well known, however, that quantum systems are not free to explore arbitrary correlations [...] -the church of the smaller Hilbert space only accepts monogamous congregants. We demonstrate how to characterize the limits of what is quantum mechanically possible with a computable measure, entanglement negativity. We show that negativity only saturates the standard linear monogamy inequality in trivial cases implied by its monotonicity under LOCC, and derive a necessary and sufficient inequality which, for the first time, is <b>a</b> non-linear <b>higher</b> <b>degree</b> <b>polynomial.</b> For very large quantum systems, we prove that the negativity can be distributed at least linearly for the tightest constraint and conjecture that it is at most linear. Comment: 5 pages, 3 figure...|$|R
40|$|In {{this paper}} we develop an {{objective}} functionbased clustering algorithm to build fuzzy {{models of the}} Takagi-Sugeno (TS) type automatically from data. In contrast {{to most of the}} TS models that {{can be found in the}} literature, we decided to use very simple input-space partitions and <b>a</b> <b>higher</b> <b>degree</b> of consequence <b>polynomials</b> (quadratic). Only in this way transparency and interpretability can be guaranteed. We also show how to derive linguistic labels for the polynomials found by the algorithm...|$|R
40|$|Abstract. In {{this work}} a new {{approach}} to time dependent problems in combination with the Least-Squares Spectral Element Method (LSQSEM) will be discussed. Various time-stepping formulations will be presented. These time-stepping formulations will be compared to the full space-time formulation. It will be shown that time-stepping formulations give accurate results for comparable CPU times. Furthermore is will be shown that a smaller timestep or <b>a</b> <b>higher</b> <b>polynomial</b> <b>degree</b> not always decreases the error norm. ...|$|R
40|$|Thermocouples are {{the most}} {{frequently}} used sensors for temperature measurement because of their wide applicability, long-term stability and high reliability. However, one of the major utilization problems is the linearization of the transfer relation between temperature and output voltage of thermocouples. The linear calibration equation and its modules could be improved by using regression analysis to help solve this problem. In this study, two types of thermocouple and five temperature ranges were selected to evaluate the fitting agreement of different-order polynomial equations. Two quantitative criteria, the average of the absolute error values |e|ave and the standard deviation of calibration equation estd, were used to evaluate the accuracy and precision of these calibrations equations. The optimal order of polynomial equations differed with the temperature range. The accuracy and precision of the calibration equation could be improved significantly with <b>an</b> adequate <b>higher</b> <b>degree</b> <b>polynomial</b> equation. The technique could be applied with hardware modules to serve as an intelligent sensor for temperature measurement...|$|R
40|$|This report {{presents}} {{the application of}} polynomial regression for estimating free energy differences using thermodynamic integration. We employ linear regression to construct a polynomial that optimally fits the thermodynamic integration data, and thus reduces the bias and uncertainty of the resulting free energy estimate. Two test systems with analytical solutions were used to verify the accuracy and precision of the approach. Our results suggest that regression with <b>a</b> <b>high</b> <b>degree</b> of <b>polynomials</b> give the most accurate free energy difference estimates, but often with a slightly larger variance, compared to commonly used quadrature techniques. <b>High</b> <b>degrees</b> of <b>polynomials</b> possess the flexibility to closely fit the thermodynamic integration data but are often sensitive to small changes in data points. To further improve overall accuracy and reduce uncertainty, we also examine the use of Chebyshev nodes to guide the selection of non-equidistant lambda values for the thermodynamic integration scheme. We conclude that polynomial regression with non-equidistant lambda values delivers the most accurate and precise free energy estimates for thermodynamic integration data. Software and documentation is available at [URL] 18 pages, 4 figures and 4 table...|$|R
40|$|Lanczos-Lovelock {{models of}} gravity {{represent}} {{a natural and}} elegant generalization of Einstein's theory of gravity to higher dimensions. They are characterized {{by the fact that}} the field equations only contain up to second derivatives of the metric even though the action functional can be <b>a</b> quadratic or <b>higher</b> <b>degree</b> <b>polynomial</b> in the curvature tensor. Because these models share several key properties of Einstein's theory they serve as a useful set of candidate models for testing the emergent paradigm for gravity. This review highlights several geometrical and thermodynamical aspects of Lanczos-Lovelock models which have attracted recent attention. Comment: 95 pages; final version published in Physics Report...|$|R
40|$|Parametric {{surfaces}} and implicit surfaces are generally used for representing curved surfaces in CAD/CG Systems. This paper discusses a curved tubular object (or generalized cylinder) {{which is a}} surface swept by a sphere/circle moving along a curve. For the trajectory curve, a 3 D Bezier curve is employed, and its radius can be varied along the curve. In general, its surface cannot be dened by a closed form, while <b>a</b> <b>high</b> <b>degree</b> of <b>polynomial</b> must be solved for ray/surface intersection. This paper proposes an eective rendering method which uses a scan line algorithm for detecting curved tubular objects on the projection plane. The calculation of the distance from a point to a curve {{plays an important role}} in our algorithm; Bezier Clipping Method[11] is employed for this calculation. Keywords: Curved tubular objects, Swept volume, Bezier curve, Projection for curve, Closest point, Bezier Clipping If the paper is accepted, one of the authors will present the paper at the [...] ...|$|R
40|$|The {{nonlinear}} classifier {{is effective}} for many practical problems. We have already proposed {{a method for}} constructing a nonlinear classifier using Legendre polynomials and have obtained good results on many actual data. In this approach, a set of original features is first extended to {{a large number of}} new features in a nonlinear fashion and then some substantial features are chosen for the nonlinear classifier. In this study, we have improved the selection process in the second stage by using some conventional feature selection algorithms. In addition, important features were selected from the original features in the preprocessing stage. The {{reduction in the number of}} the original features permits the nonlinear classifier to use <b>a</b> <b>higher</b> <b>degree</b> of <b>polynomials.</b> 1 Introduction In our previous study [1], we proposed a nonlinear classifier and confirmed its effectiveness on many data. In this method, a large number of Legendre polynomials of the original features are constructed as ex [...] ...|$|R
30|$|According to {{the results}} in Figure 6 a,c, it emerges that <b>a</b> <b>higher</b> <b>polynomial</b> <b>degree</b> leads to <b>a</b> better performance, {{regardless}} {{of the value of}} ρsz, in terms of both D and TE. Conversely, since the PAF is highly selective for large values of g (as shown in Figure 4), this leads to poor performance in terms of RE, as shown in Figure 6 b. By considering small values of g (e.g., g = 0 corresponds to flooding), one observes the opposite phenomenon: a drastic improvement in terms of RE, at the price of <b>a</b> slightly <b>higher</b> D and <b>a</b> smaller TE.|$|R
40|$|This {{thesis is}} {{concerned}} with the characteristics and behaviours of random polynomi- als of <b>a</b> <b>high</b> <b>degree.</b> Random <b>polynomials</b> are polynomials with random coefficients and take the form '£']= 0 ajxj, where the coefficients aj, (j = 0, [...] ., n) are random variables of a probability distribution, such as the normal or uniform distribution. The polynomials featured in this thesis are of the algebraic, hyperbolic and trigonometric type, all of which have coefficients that are independent random variables of the nor- mal distribution. In Chapter 1, we discuss the characteristics featured throughout this thesis, namely, the expected number of level crossings, the expected number of maxima(minima), the expected number of maxima below a fixed level u, the expected number of points of inflection as well as the covariance of the number of zeros. In this chapter we also present the formulae used in this thesis to prove the results obtained. In Chapter 2 we discuss results previously obtained for polynomials with similar characteristics to those featured in this thesis. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Spatial {{temperature}} {{profiles of}} metal-halide lamps have been acquired using x-ray absorption of the Hg density distribution. The temperature profiles {{were determined by}} combining the measured absorption of the spatially resolved Hg density with the wall temperature. The data analysis of x-ray absorption is extensive. After the necessary image reconstruction, the line integrated density profile needs to be Abel inverted. The solution from the Abel inversion is stabilized with the Tikhonov regularization parameter. The previous method for x-ray absorption developed in our group has been modified and improved by utilizing the following: (1) a larger outer bulb, (2) the corrections for x-ray scattering on the lamp materials, (3) an optimum value of Tikhonov regularization parameter µ, and (4) employing <b>a</b> <b>higher</b> <b>degree</b> of <b>polynomials</b> used for the Abel inversion. For a similar lamp as reported previously by Zhu, we found an axis temperature of 6200 K instead of 5200 K. The higher temperature is similar to what is found using optical spectroscopic methods. Typical results from a metal-halide lamp with and without salts are shown...|$|R
40|$|Our {{approach}} is to use fully three-dimensional models for both the fluid and the structure. For thin-walled structures, which are typically sensitive to loads resulting from the surrounding fluid, it will be shown {{that the use of}} high-order hexahedral ele-ments with high aspect ratios is feasible. Furthermore, it will be demonstrated that three-dimensional elements of high order can be used very efficiently by choosing <b>a</b> <b>high</b> <b>polynomial</b> <b>degree</b> in in-plane direction and a low <b>polynomial</b> <b>degree</b> in thickness direction. By varying the <b>polynomial</b> <b>degrees</b> in the local di-rections of the elements, the choice of an appropriate structural model can be achieved in an adaptive way. This will be demon-strated by means of a numerical example. ...|$|R
