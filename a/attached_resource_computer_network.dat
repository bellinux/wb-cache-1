3|10000|Public
50|$|<b>Attached</b> <b>Resource</b> <b>Computer</b> <b>NETwork</b> (ARCNET or ARCnet) is a {{communications}} protocol for local area networks. ARCNET {{was the first}} widely available networking system for microcomputers; it became popular in the 1980s for office automation tasks. It was later applied to embedded systems where certain features of the protocol are especially useful.|$|E
40|$|BACnet is a {{standard}} data communication protocol for building automation and control systems. BACnet defines an object-based model {{of the information that}} is exchanged between components of the building automation system and an application layer protocol that is used to access and manipulate this information. It also provides a way to convey the information across a variety of local and wide-area networks that may be interconnected to form an internetwork. In this study, the performance of three BACnet local area networking options is investigated using simulation models developed using ARENA, a tool for simulating discrete event dynamic systems. This study evaluates the delay characteristics of Master-Slave/Token-Passing (MS/TP), <b>Attached</b> <b>Resource</b> <b>Computer</b> <b>Network</b> (ARCNET), and ISO- 8802 - 3 (Ethernet) networks being used to deliver BACnet application services. Analysis of the simulation results was used to identify the network parameters that influence the performance of BACnet application services and to develop recommendations that should be considered when designing and operating BACnet systems. Key words: ANSI/ASHRAE Standard 135; BACnet; building automation and control...|$|E
40|$|KEKB is a double-ring, asymmetric-energy, electron-positron collider, {{which is}} {{pursuing}} luminosity frontier. In {{order to improve}} its luminosity, commissioning has been continued incessantly. Because various machine tunings are frequently necessary during the commissioning, the control system is required to be highly responsive and flexible. Especially for the magnets and magnet power supplies, which define optics of the accelerators, the efficiency and flexibility of the control system are essentially important. To achieve these requirements the design {{and the construction of}} the control system of the magnet power supplies have been carried out with the following view points. The most important point is the unified treatment of the magnet power supplies. KEKB has wide variety of the magnet power supplies, from the large power supplies for the main bending magnets to the small power supplies for the steering magnets. The bending magnets, quadrupole magnets, sextupole magnets and steering magnets require different type of power supplies and are operated in the different manner. The magnet can be iron core or air core, superconducting or normal conducting, with or without auxiliary windings, and so on. There are many different types of the magnet power supplies, in the scale, in the regulation scheme, bipolar or unipolar, with or without pole changer, and so on. Although such differences exist, focusing on the common properties of the magnet power supplies, the hardware of the interface and the software of the control system have been designed not separately for each type but designed as universal for the all types as possible. Another important point is reliability. Although resources of the budget and the man power to construct KEKB have been limited, it has been required to keep the control system reliable. Not only in the construction phase but also in the maintenance phase, reliability should be considered. In order to maintain reliability in such conditions, the design strategy adapted is simplification. Especially it is important how to simplify the interface to the magnet power supply with the control computer. It greatly influences the reliability of the control system. In the previous, TRISTAN, control system CAMAC was adopted as the standard interface. Between the CAMAC module and the magnet power supply there were wired signal lines for each signal one by one. If KEKB adopted the same scheme, the amount of wires could be huge because the number of magnet power supplies in KEKB is large, more than double. In KEKB, instead of the parallel wiring, one serial line has been introduced for a magnet power supply. All signals are exchanged through the single serial line. Thus, considering unification and simplification as the important principles, the magnet power supply control system has been developed. There are two major tasks in the development. One is the development of the interface between the control computers and the magnet power supplies. The other is the development of the control software. KEKB control system has adopted EPICS (Experimental Physics and Industrial Control System) as the core software framework. And two types of computers have been installed. One is IOC (Input/Output Controller), which directly controls the equipments through its own hardware connection to the equipments. 　The other is OPI (Operator Interface), which runs various high level applications like operator interface. The synchronous operation of multiple magnet power supplies is required in order to change optics without loosing stored beams. For this purpose, the synchronous setting has been designed that the tracking data are calculated in the IOC and sent to the magnet power supply beforehand and then the synchronous start signal triggers the synchronous tacking. In this method the magnet power supply is required to store the array data for the tracking and to set them in sequence with the interval clock. Although it requires some intelligence in the magnet power supply, total system can be simplified and flexible operation is possible. ARCNET (<b>Attached</b> <b>Resource</b> <b>Computer</b> <b>NETwork)</b> has been adopted as the serial interface. It supports relatively long packet and communication speed is enough for our purpose. The twisted-pair cable with RS 485 type differential driver has been chosen as the media of ARCNET. This configuration allows multi-drop wiring. The synchronous start signal is also delivered by the cable combined with ARCNET. Thus, the simplification of the wirings has been achieved. For the implementation of the ARCNET interface, the PSICM (Power Supply Interface Controller Module) has been developed. It is the plug-in module in the magnet power supply and has an ARCNET interface and a microcomputer with the control software (firmware). PSICM has been designed to be universal for any type of the power supplies. All of the magnet power supplies can be controlled in single manner using PSICM. In the development of the control software, the magnet power supplies have been treated in unified manner. Among the different types of the magnets and the power supplies, the common features are abstracted. Major functions incorporated in the IOC are followings. (1) parameter conversion from abstracted magnetic field strength to the current(2) regular setting sequences to reduce the magnetic hysteresis problem(3) synchronous and asynchronous operations for the current settingAlthough the magnet power supply control system has been originally designed for the KEKB, because of its universal design, it has been also well applied to the PF-AR magnet control system. PF-AR has been upgraded in 2001. At that time its control system has been renewed using the same way as KEKB. PSICM can be used for PF-AR without any modifications. Most of the control software for KEKB can be also applied for PF-AR. In addition for PF-AR, the pattern operation of the acceleration has been developed based on the same mechanism of the synchronous operation. The tracking pattern of the acceleration can be flexibly configured. As the unification has been implemented in the IOC layer, the magnet power supply can be treated as the abstracted object independent of the hardware in the OPI layer. Such abstraction has reduced the load of the development of the user application programs. Thus, during a decade of the commissioning of the KEKB accelerators, many application programs have been developed and have contributed to the tuning up and improvement of the accelerators continuously...|$|E
50|$|A <b>computer</b> <b>network</b> or {{data network}} is a digital {{telecommunications}} network which allows nodes to share <b>resources.</b> In <b>computer</b> <b>networks,</b> networked computing devices exchange data {{with each other}} using a data link. The connections between nodes are established using either cable media or wireless media.|$|R
40|$|<b>Computer</b> <b>network</b> is a {{combination}} of various communication equipment and computers that are connected to one another via a communications medium so that all network users can communicate with each other electronically. Internet gives an opportunity for users worldwide to communicate and will share information <b>resources.</b> <b>Computer</b> <b>network</b> is just a medium that carries information, the efficiency of the Internet lies in the information itself is oriented to human. As a user of the Internet we can communicate with other users simply by sending and receiving e-mail or held talks in realtime with friends around the world. Therefore, the authors try to give some idea that the Internet network requires sophisticated tools to make the Internet <b>network</b> as a <b>computer</b> <b>network</b> implementation. And the author also provides information on the actual LAN is connected to the Internet at all because it includes a LAN equipment required by the Internet in addition to other equipment...|$|R
40|$|Today an {{efficient}} (cost-effective) design and usage of networks {{is of particular}} importance. As more and more computer systems become context-aware {{the question of how}} context information can be used to improve <b>computer</b> <b>networks</b> arises. In this poster we describe how context information can be used to optimize the usage of <b>resources</b> in a <b>computer</b> <b>network.</b> By means of a mobile payment system we show how these optimization method can be applied...|$|R
40|$|International audienceIn this article, we {{illustrate}} {{practical issues}} arising {{in the development}} of efficient implementation of distributed algorithms that solve a general (concave) constrained maximization problem. Such optimizations arise in many situations. One typical example is those of <b>resource</b> allocation in <b>computer</b> <b>networks,</b> where the system aims at maximizing some global function of the users individual throughput subject to link capacity constraints...|$|R
40|$|Digital library {{research}} has and {{is focused on}} addressing {{the problems associated with}} describing and sharing digital <b>resources</b> across <b>computer</b> <b>networks</b> of various scales. Digital librarians are therefore well placed to make important contributions to big data research and infrastructure. However, the emerging complexity of the big data research paradigm can make it difficult to identify important points for strategic collaboration between digital librarians and big data researchers. This panel brings together presenters from a variety of backgrounds, who will discuss various aspects of one focus for collaboration, the management and sharing of large-scale data and metadata...|$|R
40|$|In today’s {{business}} environment {{it is difficult}} to obtain senior management approval for the expenditure of valuable resources to “guarantee “that a potentially disastrous event will not occur that could affect the organisation performance. Analysing potential risk and the allocation of <b>resources</b> for <b>computer</b> <b>network</b> security and business continuity require strategic, long-term planning. Most companies tend to be reactive and respond with quick infrastructure solutions. A strategic approach to <b>computer</b> <b>network</b> security leads to a more efficient plan and a less expensive risk-management strategy. Financial modelling is a fundamental component of all business investment cases. IT security investment proposals have unique qualities that can pose expenditure justification challenges. This paper aims to explore various financial models and to develop one that IT managers can effectively use to support their business cases...|$|R
40|$|Analysing {{potential}} risk and {{the allocation of}} <b>resources</b> for <b>computer</b> <b>network</b> security and business continuity require strategic, long-term planning. Most companies tend to be reactive and respond with quick infrastructure solutions. The purpose of risk analysis should be to assist managers in making informed decisions about investment and developing risk management policies. High countermeasures expenditure on every aspect of an information system is out of question in a commercial organisation. Therefore, this expenditure must be directed to reduce corporate exposure to information system risks {{in the context of}} overall business risks. The aim {{of this paper is to}} report the on going research to justify funding for network security expenditure through risk assessment practice...|$|R
40|$|The Cave Project is a {{research}} initiative aiming to make possible a user-transparent distribution of CAD <b>resources</b> over <b>computer</b> <b>networks.</b> It can be divided in three parts: * a Framework of reusable software, composed by CAD tool modules and design data representation primitives * a web based design environment, implemented over the Framework foundations, together with a Service Space, which provides the necessary control {{on the distribution of}} design resources and the data sharing among designers * a Communication Channel, which allows synchronous and asynchronous interaction among the designers The modules can be distributed over nodes of a Internet Protocol based network. The designers interact with all of the modules using a Java-enabled client software, e. g. a web browser...|$|R
40|$|A {{number of}} {{terminals}} supported by the NOVA computer-based DATERCOM systems are given access to ACL-NOVA, the IBM 3031 central <b>computer</b> and other <b>resources</b> of the AAEC <b>computer</b> <b>network</b> at Lucas Heights. Some of the other Dataway computers provide their own terminals with the same ACL and non-ACL mode support by communicating with DATERCOM, using a restricted set of Dataway sequences to communicate with DATERCOM for ACL mode support. The extension to DATERCOM provides an even better use of Dataway resources by giving ACL mode support to terminals of Dataway computers that only provide non-ACL mode access to the <b>resources</b> of the <b>computer</b> <b>network...</b>|$|R
40|$|In current, Online social {{that are}} many {{information}} <b>resources</b> on <b>computer</b> <b>network</b> which many format, resource and contents are variety. Online news is one type that present facts including person, events,location, time and context {{to let people}} know the current situation early that have effect to many people in society. Nowadays, a number of Thai news online is increasing because the internet infrastructure is powerful and support requirement to access are appropriately, conveniently and quickly. This article apply to text mining technique for analysis the relationship between keywords and terms that define to data refer to person, events, location, time and discovery knowledge from amounts Thai news online. Including text mining process for Thai news online analysis and the limitations of analysis for article or text which are Thai language by using text mining...|$|R
40|$|Grid {{computing}} is {{the infrastructure}} that involves {{a large number}} of <b>resources</b> like <b>computers,</b> <b>networks</b> and databases which are owned by many organizations. Job scheduling problem {{is one of the key}} issues because of high heterogeneous and dynamic nature of resources and applications in the grid computing environment. Bee colony approach has been used to solve this problem because it can be easily adapted to the grid scheduling environment. The bee algorithms have shown encouraging results in terms of time and co st. In this paper a framework for multi objective bee colony optimization is proposed to schedule batch jobs to available resources where the number of jobs is greater than the number of resources. Pareto analysis and k-means analysis are integrated in the bee colony optimization algorithm to facilitate the scheduling of jobs to resources...|$|R
50|$|Domain names {{serve to}} {{identify}} Internet <b>resources,</b> such as <b>computers,</b> <b>networks,</b> and services, with a text-based label that {{is easier to}} memorize than the numerical addresses used in the Internet protocols. A domain name may represent entire collections of such resources or individual instances. Individual Internet host computers use domain names as host identifiers, also called host names. The term host name is also used for the leaf labels in the domain name system, usually without further subordinate domain name space. Host names appear as a component in Uniform Resource Locators (URLs) for Internet resources such as web sites (e.g., en.wikipedia.org).|$|R
40|$|Market-based {{control is}} {{attractive}} for networked computing utilities in which consumers compete for shared <b>resources</b> (<b>computers,</b> storage, <b>network</b> bandwidth). This paper proposes a new self-recharging virtual currency model {{as a common}} medium of exchange in a computational market. The key idea is to recycle currency through the economy automatically while bounding the rate of spending by consumers. Currency budgets may be distributed among consumers according to any global policy; consumers spend their budgets to schedule their resource usage through time, but cannot hoard their currency or starve. We outline the design and rationale for self-recharging currency in Cereus, a system for market-based community resource sharing, in which participants are authenticated and sanctions are sufficient to discourage fraudulent behavior. Currency transactions in Cereus are accountable: offline third-party audits can detect and prove cheating, so participants may transfer and recharge currency autonomously without involvement of the trusted banking service...|$|R
40|$|This paper {{outlines}} some of {{the reasons}} for the use of computer networking for teacher education and professional development, and discusses why networking can be used to support teacher education and development. A review of the literature suggests that among other reasons, networking is particularly conducive to teacher development because: (a) it helps break down teacher isolation and build a supportive learning community, (b) it serves as an agent of change, and (c) it helps disseminate educational materials and <b>resources.</b> The <b>computer</b> <b>network</b> recently developed specifically for secondary English teachers in Hong Kong, the TeleNex, is reported in this paper to highlight {{some of the}} factors that should be considered in the design and implementation of networks for teacher development. Preliminary observations suggest that these teachers are on their way to using this new technology to build and own their own electronic learning community. © 1996 Chapman & Hall. link_to_subscribed_fulltex...|$|R
40|$|Now a days cloud {{computing}} is an emerging area and it replace grid computing. Cloud computingelaborate that how to utilize <b>computer</b> <b>resources</b> on <b>network.</b> In {{fact it is}} the market model that describesservices provided over the internet. In Future, not all {{but most of the}} resources are controlled by cloudcomputing. This paper present an overview of {{cloud computing}} and how it is going take place position ofERP and SME like management tool...|$|R
40|$|Feedback-based {{adjustment}} of load {{is a common}} mechanism for <b>resource</b> allocation in <b>computer</b> <b>networks.</b> This paper disputes the popular beliefs that the additive-increase multiplicative-decrease adjustment policy is optimal or even necessary for convergence to fair resource sharing. We demonstrate that, in the classic synchronous model, additive increase does not guarantee the quickest convergence of fairness. Moreover, not only fairness but also efficiency converges very slowly under additive increase. For an asynchronous model, we show that the additive-increase multiplicative-decrease algorithm fails to converge to optimal fairness. We observe that the TCP congestion control algorithm suffers from the problems detected by our analysis and is unfair. ...|$|R
40|$|Grid {{computing}} is {{the infrastructure}} that involves {{a large number}} of <b>resources</b> like <b>computers,</b> <b>networks</b> and databases which are owned by many organizations. These resources are collected together to make a huge computing power. Job scheduling problem {{is one of the key}} issues in grid computing and failing to look into grid scheduling results in uncompleted view of the grid computing. Achieving optimized performance of grid system, and matching application requirements with available computing resources, are the objectives of grid job scheduling. Bee colony approaches are more adaptive to grid scheduling due to high heterogeneous and dynamic nature of resources and applications in grid. These algorithms have shown encouraging results in terms of time and cost. This paper presents some resent research activities inspired by bee foraging behavior for grid job scheduling especially ABC and BCO approaches. Different original studies related to this area are briefly described along with their comparisons against them and results. The review summary of their derived algorithms and research efforts is done...|$|R
40|$|I {{describe}} four electronic educational <b>resources</b> available via <b>computer</b> <b>networks</b> {{and electronic}} mail: (1) a curriculum module on user interface development; (2) a bibliography on human-computer interaction; (3) {{a description of}} courses and curricula in human-computer interaction, and (4) a survey of educational opportunities in human-computer interaction. For each of these, I discuss: (1) why the resource was created, (2) how the information was gathered, (3) what information has been gathered, (4) how to access the information, (5) how {{it has been used}} in HCI education, and (6) how the information (or information like it) can be applied to Human Factors and Ergonomics (HFE) education. I conclude with recommendations for the modernization of HFE information infrastructure...|$|R
40|$|The {{aim of the}} CatNet {{project is}} to combine {{economic}} and computer science research to provide new coordi-nation mechanisms for large-scale application-layer net-works. The ability of a free-market economy to balance and satisfy the con¤icting needs of millions of human agents recommends it as a decentralized organizational princi-ple. CatNet will evaluate a decentralized mechanism for <b>resource</b> allocation in <b>computer</b> <b>networks,</b> {{which is based on}} the economic paradigm of the Catallaxy. The technical realization of the paradigm builds on software agents which buy and sell network services and resources. This concept is applied both to initial service deployment and service ac-cess and to provisioning during the network’s lifecycle. 1. Service Control and Resource Allocation i...|$|R
5000|$|Cloud {{computing}} is a computing {{infrastructure and}} software model for enabling ubiquitous access to shared pools of configurable <b>resources</b> (e.g., <b>computer</b> <b>networks,</b> servers, storage, applications and services), {{which can be}} rapidly provisioned with minimal management effort, often over the Internet. Cloud computing allows users, and enterprises, with various computing capabilities to store and process data in either a privately owned cloud, or on a third-party server located in a data center {{in order to make}} data accessing mechanisms more efficient and reliable. [...] Cloud computing relies on sharing of resources to achieve coherence and economy of scale, similar to a utility. Advocates note that cloud computing allows companies to avoid, or minimize, up-front infrastructure costs. As well, third party clouds enable organizations to focus on their core businesses instead of expending <b>resources</b> on <b>computer</b> infrastructure and maintenance. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables information technology (IT) teams to more rapidly adjust resources to meet fluctuating and unpredictable business demand. Cloud providers typically use a [...] "pay as you go" [...] model. This could lead to unexpectedly high charges if administrators are not familiarized with cloud pricing models.|$|R
40|$|Acknowledgments 03 3. NetSolve 04 i. Definition ii. Components 4. NetSolve Architecture 06 5. Hardware Software Server 07 i. What {{is it all}} about? ii. Motivation iii. Functionality iv. Software caching v. Implementation vi. Benefits 6. HW-SW Architecture 11 7. Future work 12 8. Appendix-A 13 9. Appendix-B 14 10. References 15 2 Abstract NetSolve is {{a project}} that makes use of {{distributed}} computational <b>resources</b> connected by <b>computer</b> <b>networks</b> to efficiently solve complex scientific problems. It is a remote procedure call (RPC) based client/agent/server system that allows users to discover, access, and utilize remote software modules and the hardware needed to run these modules. NetSolve facilitates heterogeneous computing, {{or the ability to}} combine different machine architectures and/or operating systems to solve a problem...|$|R
30|$|Fog Orchestration {{refers to}} the process of {{automating}} application workflows in the sense of providing dynamic policy-based lifecycle management of Fog infrastructure and services. The Orchestration includes the provisioning, management, and monitoring on a large number of Fog nodes (i.e. Cloudlets) with a broad range of capabilities that include computing (<b>computer</b> <b>resources),</b> routing (<b>network)</b> and distributed databases (storage). The Fog Orchestration system must manage heterogeneous, and distributed systems spread across a wide geographical area. This requires a hierarchical organization with effective policies integrated with the Cloud orchestration system via intelligent interfaces.|$|R
40|$|This paper proposes {{highlevel}} applicationindepend ent {{framework for}} the construction of distributed systems within <b>resource</b> sharing <b>computer</b> <b>network</b> The framework generalizes design techniques in use within the ARPA <b>Computer</b> <b>Network</b> It eliminates the need for applicationspecific communication pro tocols and support software thus easing the task of the applications programmer and so encouraging the sharing of resources The framework consists of net workwide protocol for invoking arbitrary named func tions in remote process and machinedependent sys tem software that interfaces one applications program to another via the protocol The protocol provides mechanisms for supplying arguments to remote func tions and for retrieving their results it also defines small number of standard data types from which all ments and results must be modeled The paper further proposes that remote functions be thought of as remotely callable subroutines or procedures This model would enable the framework to more gracefully extend the local programming environment to embrace modules on other machines THE GOAL RESOURCE SHARING The principal goal of all resourcesharing <b>computer</b> <b>networks</b> including the now international ARPA Net work the ARPANET is to usefully interconnect geographically distributed hardware software and human 1 h this goal requires the design and implementation of various levels of support software within each constituent computer and the specification of networkwide protocols that is con ventions regarding the format and the relative timing of network messages governing their interaction This paper outlines an alternative to the approach that ARPANET system builders have been taking since work in this area began in 1970 and suggests strategy for modeling distributed systems within any large com puter networ...|$|R
40|$|Cloud {{computing}} is {{a hybrid}} model that provides both {{hardware and software}} <b>resources</b> through <b>computer</b> <b>networks.</b> Data services (hardware) together with their functionalities (software) are hosted on web servers rather than on single <b>computers</b> connected by <b>networks.</b> Through a device (e. g., either a computer or a smartphone), a browser and an Internet connection, each user accesses a cloud platform and asks for specific services. For example, a user can ask for executing some applications (jobs) on the machines (hosts) of a cloud infrastructure. Therefore, it becomes significant to provide optimized job scheduling approaches suitable to balance the workload distribution among hosts of the platform. In this paper, a multi-objective mathematical formulation of the job scheduling problem in a homogeneous cloud computing platform is proposed in order to optimize the total average waiting time of the jobs, the average waiting time of the jobs in the longest working schedule (such as the makespan) and the required number of hosts. The proposed approach {{is based on an}} approximate ϵ-constraint method, tested on a set of instances and compared with the weighted sum (WS) method. The computational results highlight that our approach outperforms the WS method in terms of a number of non-dominated solution...|$|R
40|$|Abstract—The task of allocating {{preventative}} <b>resources</b> to a <b>computer</b> <b>network</b> {{in order}} to protect against the spread of viruses is addressed. Virus spreading dynamics are described by a linearized SIS model and protection is framed by an optimization problem which maximizes the rate at which a virus in the network is contained given finite resources. One approach to problems of this type involve greedy heuristics which allocate all resources to the nodes with large centrality measures. We address the worst case performance of such greedy algorithms be constructing networks for which these greedy allocations are arbitrarily inefficient. An example application is presented in which such a worst case network might arise naturally and our results are verified numerically by leveraging recent results which allow the exact optimal solution to be computed via geometric programming. I...|$|R
40|$|Grid {{technology}} {{emerged from}} the need for huge computational power and data storage capabilities {{for a range of}} applications such as high energy physics. The main objective of the Grid project is to combine the computation power of distributed <b>resources</b> such as <b>computers,</b> <b>networks,</b> databases and scientific instruments, across research centers over the country or even the globe. In such an arrangement, individual users can reach and share the desired resources from any point in the system. This system allows the researchers to share their experience and to process their data in a very short time. In this paper along with the basics of the Grid technology, the present infrastructure and possible applications in Turkey, especially in high energy physics, as in Atlas experiment, are investigated. Also future possibilities are discussed in this context...|$|R
40|$|The task of allocating {{preventative}} <b>resources</b> to a <b>computer</b> <b>network</b> {{in order}} to protect against the spread of viruses is addressed. Virus spreading dynamics are described by a linearized SIS model and protection is framed by an optimization problem which maximizes the rate at which a virus in the network is contained given finite resources. One approach to problems of this type involve greedy heuristics which allocate all resources to the nodes with large centrality measures. We address the worst case performance of such greedy algorithms be constructing networks for which these greedy allocations are arbitrarily inefficient. An example application is presented in which such a worst case network might arise naturally and our results are verified numerically by leveraging recent results which allow the exact optimal solution to be computed via geometric programming...|$|R
5000|$|... <b>computer</b> <b>networks</b> and {{distributed}} systems assign names to <b>resources,</b> such as <b>computers,</b> printers, websites, (remote) files, etc.|$|R
40|$|Obtaining {{efficient}} {{execution of}} parallel programs in workstation networks {{is a difficult}} problem for the user. Unlike dedicated parallel <b>computer</b> <b>resources,</b> <b>network</b> resources are shared, heterogeneous, vary in availability, and offer communication performance that is still {{an order of magnitude}} slower than parallel <b>computer</b> interconnection <b>networks.</b> Prophet, a system that automatically schedules data parallel SPMD programs in workstation networks for the user has been developed. Prophet uses application and resource information to select the appropriate type and number of workstations, divide the application into component tasks and data across these workstations, and assign tasks to workstations. This system has been integrated into the Mentat parallel processing system developed at the University of Virginia. A suite of scientific Mentat applications have been scheduled using Prophet on a heterogeneous workstation network. The results are promising and demonstrate that scheduling SP [...] ...|$|R
40|$|This {{paper is}} an {{investigation}} into the development of a novel control technique termed market-based control (MBC) for application to structural control systems. In market-based control, the complex dynamic system is modelled as a market whose operation is akin to #nancial markets. A scarce system resource is identi#ed and is optimally distributed in a decentralized manner. Researchers have investigated the use of MBC techniques to microelectro -mechanical systems (MEMS) where hundreds of actuators and sensors are employed in system plants of high uncertainty with high likelihood of actuation failure [9]. In the area of computer architecture, market-based control has been applied to problems of optimal <b>resource</b> allocation on <b>computer</b> <b>networks</b> {{as well as in the}} time-sharing of microprocessor power for software processes [10]. Market-based control has even been applied to systems that regulate the #ow of #uid in tanks [11...|$|R
40|$|Abstract- Probabilistic Key pre-distribution schemes (P-KPSs) are {{candidates}} for securing interactions between <b>resource</b> limited <b>computer</b> <b>networks.</b> Collusion susceptible P-KPSs are trade-offs between security and complexity and security include resistance to passive eavesdropping attacks, and active message injection attacks. The existing work presented the P-KPS, the subset keys and identity tickets (SKIT) scheme, SI Scheme, MBK Scheme whose performance {{were compared with}} deterministic KPS model to facilitate facets {{of the complexity of}} key pre-distribution schemes. The security model described the resistance of P-KPSs to active message-injection attacks. Most of the existing schemes are based on probabilistic approach and shows poor resiliency against coalition attack and connectivity. The storage costs of deterministic schemes are all relatively high and easy to support large size networks. The proposed work presented a resilient deterministic key pre-distribution scheme which show better detection against coalition attack. For the neighboring nodes in the same group, the polynomial-based key pre-distribution scheme is used to generate pair wise keys for them. And for the neighboring nodes in different groups, the binding secrets generated by a ECC are used to establish the pair wise key. Index Terms- Probabilistic Key pre-distribution schemes, subset keys and identity tickets (SKIT) scheme, SI Scheme, MBK Schem...|$|R
50|$|The {{defense of}} {{computers}} against intrusion and unauthorized use of <b>resources</b> is called <b>computer</b> security. Similarly, {{the defense of}} <b>computer</b> <b>networks</b> is called network security.|$|R
40|$|Security {{of network}} assets {{is a high}} {{priority}} with little traditional return on investment. Increasingly, cyber attacks are being used by both terrorist and unfriendly government organizations. The HACKING Game, a Cross Impact Analysis planning tool, {{can be used to}} plan security <b>resource</b> allocation in <b>computer</b> <b>networks.</b> Cross Impact Analysis provides a mathematical basis to determine the interrelationships of one event with a set of other events. Output from the HACKING Game’s Cross Impact Analysis model can be used to help justify security expenditures, with an added benefit of being a training tool for employees learning to protect networks. This paper presents details of the Hacking Game’s design and its capabilities. Cross impact modeling can be used to develop games for any situation characterized by a set of offense and defense events to produce an individual or collaborative model for such things as natural and man-made disasters...|$|R
