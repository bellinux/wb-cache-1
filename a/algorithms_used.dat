5221|10000|Public
5|$|Following a {{decompression}} schedule {{does not}} completely protect against DCS. The <b>algorithms</b> <b>used</b> {{are designed to}} reduce the probability of DCS to a very low level, but do not reduce it to zero.|$|E
5|$|Significant {{modifications}} to the <b>algorithms</b> <b>used</b> meant that a chipset intended for a four-function calculator was able to process scientific functions, but {{at the cost of}} reduced speed and accuracy. Compared to contemporary scientific calculators, some functions were slow to execute, and others had limited accuracy or gave the wrong answer, but the cost of the Sinclair was {{a fraction of the cost}} of competing calculators.|$|E
25|$|The {{cryptographic}} security of PGP encryption {{depends on the}} assumption that the <b>algorithms</b> <b>used</b> are unbreakable by direct cryptanalysis with current equipment and techniques.|$|E
50|$|The Lehmer-Schur <b>algorithm</b> <b>uses</b> the Schur-Cohn {{test for}} circles, Wilf's global bisection <b>algorithm</b> <b>uses</b> a winding number {{computation}} for rectangular {{regions in the}} complex plane.|$|R
30|$|The <b>algorithm</b> <b>used</b> for {{training}} with the Cross-Entropy error {{is the one}} described in[43], while the <b>algorithm</b> <b>used</b> {{for training}} with the Sum-of-Squares error and the Minkowski error (R[*]=[*] 1), is the Conjugate Gradient method[44].|$|R
30|$|Below, for short, the {{associative}} spatial crowdsourcing <b>algorithm</b> <b>using</b> {{the immediate}} neighbourhood heuristic {{given in the}} previous section is termed ASC-IN, and the associative spatial crowdsourcing <b>algorithm</b> <b>using</b> the spatial point process modelling is termed ASC-SPP.|$|R
25|$|When {{working with}} digital audio, digital signal {{processing}} techniques are commonly used to implement compression via digital audio editors, or dedicated workstations. Often the <b>algorithms</b> <b>used</b> emulate the above analog technologies.|$|E
25|$|Many {{of these}} models bear {{similarity}} to the <b>algorithms</b> <b>used</b> in search engines (for example, see Griffiths, et al., 2007 and Anderson, 1990), {{though it is not}} yet clear whether they really use the same computational mechanisms.|$|E
25|$|Until recently, {{software}} tools {{for carrying out}} this form of analysis have been heavily underdeveloped, and {{were based on the}} same <b>algorithms</b> <b>used</b> to detect germline variations. Such procedures are not optimized for this task, because they do not adequately model the statistical correlation between the genotypes present in multiple tissue samples from the same individual.|$|E
40|$|Designed {{and helped}} {{implement}} {{three generations of}} the routing engine used by Bing Maps to compute driv-ing directions. Developed the <b>algorithm</b> <b>used</b> by Bing Maps for finding journeys in public transportation systems. Developed the <b>algorithm</b> <b>used</b> by Bing Local to quickly rank points of interest based on driving times from one or multiple locations. Contributed to the <b>algorithm</b> <b>used</b> by Microsoft’s Business Intelligence Unit to transform natural-language expressions into formal queries into a database...|$|R
5000|$|There {{are several}} {{algorithms}} available {{to identify the}} [...] "best" [...] cladogram. Most <b>algorithms</b> <b>use</b> a metric to measure how consistent a candidate cladogram is with the data. Most cladogram <b>algorithms</b> <b>use</b> the mathematical techniques of optimization and minimization.|$|R
50|$|The main <b>algorithm</b> just <b>used</b> {{the most}} recent poll(s) in every state. If two polls {{came out on the}} same day, they were averaged. This <b>algorithm</b> <b>used</b> all {{published}} polls, including those by partisan pollsters such as Strategic Vision (R) and Hart Research (D). A second <b>algorithm</b> <b>used</b> only nonpartisan polls and averaged all polls during the past three days. A third <b>algorithm</b> <b>used</b> historical data to predict how undecided voters would break. Maps for each of the algorithms were given every day, but the first one got most of the publicity since it was on the main page.|$|R
25|$|As a {{demonstration}} of the principles involved in raytracing, let us consider how one would find the intersection between a ray and a sphere. This is merely the math behind the line–sphere intersection and the subsequent determination of the colour of the pixel being calculated. There is, of course, far more to the general process of raytracing, but this demonstrates an example of the <b>algorithms</b> <b>used.</b>|$|E
25|$|The {{special case}} of linear support vector {{machines}} {{can be solved}} more efficiently by {{the same kind of}} <b>algorithms</b> <b>used</b> to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training time properties. Each convergence iteration takes time linear in the time taken to read the train data and the iterations also have a Q-Linear Convergence property, making the algorithm extremely fast.|$|E
25|$|To {{illustrate}} the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT <b>algorithms</b> (<b>used</b> {{heavily in the}} field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.|$|E
30|$|Nearest-three-node {{selection}} <b>algorithm</b> <b>used</b> in [1].|$|R
50|$|The {{pathfinder}} <b>algorithm</b> <b>uses</b> two parameters.|$|R
50|$|Some <b>algorithms</b> <b>use</b> {{experimental}} evidence on structural complexes, the atomic details of binding interfaces and produce detailed atomic models of protein-protein complexes {{as well as}} other protein-molecule interactions. Other <b>algorithms</b> <b>use</b> only sequence information, thereby creating unbiased complete networks of interaction with many mistakes.|$|R
25|$|More {{sophisticated}} algorithms {{depend on}} the material type covered. Organic compounds might be searched for {{on the basis of}} certain molecular fragments. Inorganic compounds, on the other hand, might be of interest with regard to a certain type of coordination geometry. More advanced algorithms deal with conformation analysis (organics), supramolecular chemistry (organics), interpolyhedral connectivity (‘non-organics’) and higher-order molecular structures (biological macromolecules). Search <b>algorithms</b> <b>used</b> for a more complex analysis of physical properties, e.g. phase transitions or structure-property relationships, might apply group-theoretical concepts.|$|E
25|$|One of {{the first}} <b>algorithms</b> <b>used</b> a Cα-Cα {{distance}} map together with a hierarchical clustering routine that considered proteins as several small segments, 10 residues in length. The initial segments were clustered one after another based on inter-segment distances; segments with the shortest distances were clustered and considered as single segments thereafter. The stepwise clustering finally included the full protein. Go also exploited the fact that inter-domain distances are normally larger than intra-domain distances; all possible Cα-Cα distances were represented as diagonal plots in which there were distinct patterns for helices, extended strands and combinations of secondary structures.|$|E
25|$|Many of the {{trajectory}} and guidance <b>algorithms</b> <b>used</b> {{were based on}} earlier work by Richard Battin. The first command module flight was controlled by a software package called CORONA whose development was led by Alex Kosmala. Software for lunar missions consisted of COLOSSUS for the command module, whose development was led by Frederic Martin, and LUMINARY on the lunar module led by George Cherry. Details of these programs were implemented by a team {{under the direction of}} Margaret Hamilton. In total, software development on the project comprised 1400 person-years of effort, with a peak workforce of 350 people. In 2016, Hamilton received the Presidential Medal of Freedom for her role in creating the flight software.|$|E
5000|$|As {{cryptographic}} primitives, the Double Ratchet <b>Algorithm</b> <b>uses</b> ...|$|R
40|$|We {{present an}} {{approximation}} algorithm for the NP-hard problem {{of finding the}} largest linearly separable subset of examples among a training set. The <b>algorithm</b> <b>uses,</b> incrementally, a Linear Programming procedure. We present numerical evidence of its superiority over the Pocket <b>algorithm</b> <b>used</b> by many neural net constructive algorithms...|$|R
40|$|Graduation date: 1969 This paper makes {{available}} practical algorithms {{and their}} associated FORTRAN IV computer programs for finding {{the roots of}} polynomial equations. The {{purpose of this paper}} is to examine effective algorithms for solving polynomial algebraic equations in one unknown on a digital computer. The advent of high - speed digital computing systems makes it practical to examine numerical methods which otherwise would be too time consuming if not impossible. Algorithms requiring only the polynomial coefficients are examined since they can be used as subprograms to solve polynomial equations which arise in other computer programs. The above considerations have lead to the examination of the following algorithms: Lehmeris <b>algorithm,</b> (<b>used</b> to find rough approximations to the roots). a) The Newton-Raphson <b>algorithm,</b> (<b>used</b> to refine the root approximations). (ii). Muller s algorithm. (iii). Rutishauser's Quotient-Difference (QD) <b>algorithm,</b> (<b>used</b> to find rough approximations to the roots). a) Newton-Raphson's <b>algorithm,</b> (<b>used</b> to refine approximations to simple roots). b) Bairstow's <b>algorithm,</b> (<b>used</b> to refine approximations to two roots i. e. complex conjugates). (iv). The Steepest Descent algorithm...|$|R
25|$|For {{the most}} part, {{computer}} vision <b>algorithms</b> <b>used</b> on color images are straightforward extensions to algorithms designed for grayscale images, for instance k-means or fuzzy clustering of pixel colors, or canny edge detection. At the simplest, each color component is separately {{passed through the}} same algorithm. It is important, therefore, that the features of interest can be distinguished in the color dimensions used. Because the R, G, and B components of an object’s color in a digital image are all correlated {{with the amount of}} light hitting the object, and therefore with each other, image descriptions in terms of those components make object discrimination difficult. Descriptions in terms of hue/lightness/chroma or hue/lightness/saturation are often more relevant.|$|E
500|$|The {{navigation}} system proved difficult to develop because computers {{of the time}} lacked power. Artificial intelligence was considered crucial, as the navigation and other aspects relied on it. According to co-designer Jonty Barnes, the team wanted the creatures complex without high computational costs. Healey {{came up with the}} idea of slapping creatures to make them work faster, and Barnes considered it a [...] "great game decision". A great deal of time was spent working on the user interface, and at one point the idea of having no interface was considered. Carter stated that the team tried to make the sounds atmospheric and industrial so players got a sense of power. Using others' 3D sound routines proved troublesome, so he and his colleague Tony Cox wrote their own. Dungeon Keeper uses lighting <b>algorithms</b> <b>used</b> in Magic Carpet, which provided effects such as fireballs lighting corridors.|$|E
2500|$|... {{contains}} extensive documentation {{about the}} <b>algorithms</b> <b>used</b> in TeX.|$|E
25|$|There is a pseudo-polynomial time <b>algorithm</b> <b>using</b> dynamic programming.|$|R
50|$|Google Hummingbird is {{a search}} <b>algorithm</b> <b>used</b> by Google.|$|R
50|$|See {{also the}} Adam7 <b>algorithm</b> <b>used</b> in PNG interlacing.|$|R
2500|$|The Needleman–Wunsch {{algorithm}} {{and other}} <b>algorithms</b> <b>used</b> in bioinformatics, including sequence alignment, structural alignment, RNA structure prediction ...|$|E
2500|$|Such {{hardware}} captures [...] "images" [...] {{that are}} then processed often {{using the same}} computer vision <b>algorithms</b> <b>used</b> to process visible-light images.|$|E
2500|$|Digital image {{processing}} to determine PSD {{has now become}} the quickest and easiest method for analyzing rock masses. [...] The <b>algorithms</b> <b>used</b> to analyze digital images and produce PSD has been researched and developed at the University of Arizona since 1997, and now in software offered by multiple companies around the world.|$|E
5000|$|PageRank, {{the ranking}} <b>algorithm</b> <b>used</b> by Google's search engine ...|$|R
5000|$|There is a pseudo-polynomial time <b>algorithm</b> <b>using</b> dynamic programming.|$|R
50|$|In {{all of the}} {{algorithms}} {{the messages}} emanating from check nodes are the same; however, since the verification rules are different for different algorithms the messages produced by variable nodes will be different in each algorithm. The algorithm given above works {{for all of the}} VB-MPA's, and different <b>algorithms</b> <b>use</b> different rules in half round 2 of round 1 and 2. For instance, Genie <b>algorithm</b> <b>uses</b> D1CN rule in Half round 2 of round 1, and in fact the half round 2 of round 2 which uses ZCN rule is useless in Genie <b>algorithm.</b> LM <b>algorithm</b> <b>uses</b> D1CN in Half round 2 of round 1 and XH <b>algorithm</b> <b>uses</b> ECN rule in this stage instead of D1CN. SBB <b>algorithm</b> also <b>uses</b> both D1CN and ECN rule in the second half round of round 1. All of these rules can be efficiently implemented in update_rule function in the second half round of round 1.|$|R
