7|16|Public
2500|$|In {{probability}} and statistics, {{a random}} variable, random quantity, <b>aleatory</b> <b>variable,</b> or stochastic variable is a variable [...] whose {{possible values are}} numerical [...] outcomes of a random phenomenon. As a function, a random variable is required to be measurable, which rules out certain pathological cases where the quantity which the random variable returns is infinitely sensitive to small changes in the outcome.|$|E
40|$|Among {{the factors}} which are {{peculiar}} to ancient masonry structures, creep and the creep-fatigue interaction {{have been shown}} to influence strongly their mechanical behaviour and continuous damage appearing to be caused by longterm heavy loads. The problem of achieving a reliable lifetime estimate of historic masonry due to the effects of persistent loading has been dealt with by a probabilistic approach. The results of pseudo-creep tests carried out on ancient masonry of different ages is presented and their interpretation by means of a probabilistic model proposed. This is aimed at the individuation of an <b>aleatory</b> <b>variable</b> as a significant index of vulnerability, and to the solution of the classic problem of reliability in stochastic conditions...|$|E
40|$|The {{integral}} of {{the standard}} normal distribution function is an integral without solution and represents the probability that an <b>aleatory</b> <b>variable</b> normally distributed has values between zero and. The normal distribution integral is used in several areas of science. Thus, this work provides an approximate solution to the Gaussian distribution integral by using the homotopy perturbation method (HPM). After solving the Gaussian integral by HPM, the result served as base to solve other integrals like error function and the cumulative distribution function. The error function is compared against other reported approximations showing advantages like less relative error or less mathematical complexity. Besides, some integrals related to the normal (Gaussian) distribution integral were solved showing a relative error quite small. Also, the utility for the proposed approximations is verified applying them {{to a couple of}} heat flow examples. Last, a brief discussion is presented about the way an electronic circuit could be created to implement the approximate error function...|$|E
3000|$|... [*]Statistics-of-interval poses an {{optimization}} problem for {{each set of}} <b>aleatory</b> <b>variables,</b> and repeated optimization evaluations over the epistemic design space {{can be used to}} determine the relevant statistics of the interval [12].|$|R
3000|$|... ext as a {{function}} of the <b>aleatory</b> <b>variables,</b> which enables the extraction of a large number of samples in order to obtain accurate statistics for very low computational cost. Because the number of <b>aleatory</b> <b>variables</b> used here is relatively small, the required number of training points for an accurate surrogate is small, necessitating only a small amount of optimizations. Because the optimization results are viewed as general random variables, any surrogate can be used to represent the aleatory dependence of the variables. A Kriging surrogate model is employed in this work. The details of the construction of this particular Kriging model, which can utilize gradient and Hessian information and employ a dynamic training point selection, is described in previously published papers [34 – 37]. The center of the Kriging domain is prescribed by the mean value of α, D [...]...|$|R
40|$|The use of {{optimization}} for {{the propagation}} of mixed epistemic/aleatory uncertainties is demonstrated {{within the context of}} hypersonic flows. Specifically, this work focuses on strategies applicable for models where input parameters can be divided into a set of <b>variables</b> containing only <b>aleatory</b> uncertainties and a set with epistemic uncertainties. With the input parameters divided in this way, uncertainty due to the epistemic vari-ables is propagated via a constrained optimization approach, while the uncertainty due to <b>aleatory</b> <b>variables</b> is propagated via sampling. A statistics-of-intervals approach is pro-posed in which the constrained optimization results are treated as a random variable and multiple optimizations are performed to quantify the aleatory uncertainty. In order to reduce the total number of optimizations required, a surrogate is employed to model the variation of the optimization results with respect to the <b>aleatory</b> <b>variables,</b> and exhaustive sampling is performed on this surrogate to determine the desired statistics. The proper-ties of the statistics-of-intervals approach are demonstrated by using the Fay-Riddell stag-nation heating correlations and compared with a competing method based on uncertain optimization. Additionally, the statistics-of-intervals approach is demonstrated for mixed epistemic/aleatory uncertainty quantification on a real gas computational fluid dynamic simulation. I...|$|R
30|$|Some further {{indications}} on the estimations {{of critical}} headways {{were derived from}} Romano [48] at three multi-lane roundabouts. For one site, the empirical distribution of critical headways showed two peaks, that characterized two classes of users and a double normal <b>aleatory</b> <b>variable</b> was chosen to fit the empirical distribution. For the other roundabouts the situation resulted more homogeneous and the gamma function was selected to interpret the empirical distributions. Nicolosi et al. [49] investigated three roundabouts: one single-lane roundabouts and two multi-lane roundabouts and measured the values of critical and follow-up headways; the method proposed by Dawson [50] and the regression method proposed by Siegloch [51] were applied. The analysis of experimental data showed that the hypothesis of Gamma distribution was always verified for the follow-up headway. De Luca et al. [52] investigated four existing rural roundabouts and used real data to calibrate a simulation model. The analysis of the sample allowed to identify the Gumbel distribution as the function that best approximated the observed distribution of the data. Table 2 reports {{the values of the}} estimated critical headway and the standard deviation.|$|E
40|$|Researchers {{interested}} in the biological effects of electromagnetic (EM) fields are focusing their attention {{on the behavior of}} transmembrane ionicchannels and on their kinetic properties. Theoretical studies of the biochemical dynamic properties of the channels have suggested the development of a modelistic approach considering the membrane channel as a non-deterministic state machine. Its behavior is fully described by a set of states, a matrix of transition rates, and a vector for the probability of the machine to be in each single state at a certain instant. In this work astochasticmodel is developed, generating random processes where the probability for each state is an <b>aleatory</b> <b>variable.</b> The model can be applied to both voltage- and ligand-dependent channels, both unexposed and exposed to EM fields. The response of the model, for voltage-dependent channels such as K+, Na+ and Ca 2 + in a voltage-clamp situation, is analyzed for sinusoidal EM fields in the ELF range. The results obtained appear more satisfactory than those presented in earlier papers using similar approaches, as this model shows the sensitivity of the channel response to both the frequency and amplitude of the EM stimulation...|$|E
40|$|We {{present the}} {{probabilistic}} {{version of the}} analysis performed in Azzaro et al. (2006 a) on the attenuation of the seismic intensity in Italian volcanic districts. The main results are the estimate of the probability distribution of the intensity at site IS, conditioned on the site-epicenter distance d and on I 0, and then, assuming the mode of this distribution as estimator of IS, the forecasting of future macroseismic fields given I 0. To this end we have modified the method presented in Rotondi and Zonno (2004) by inserting the following innovative elements: identification of possible different trends and exploitation of knowledge from prior experience or data. Data set. The intensity dataset considered in the present analysis is the same {{used in the study}} by Azzaro et al. (2006 a), based on a deterministic approach. We consider a total of 38 earthquakes located in the Italian volcanic areas, so distributed: Etna region (24 events), Aeolian Islands (6 events), Vesuvius-Ischia (3 events) and Albani Hills (5 events). The CMTE local earthquake catalogue (Azzaro et al., 2000, 2002, 2006 b) has been used for the Etna region while for the other Italian volcanic districts (Aeolian Islands, Ischia, Vesuvius and Albani Hills) the CPTI 04 Italian seismic catalogue (Gruppo di lavoro CPTI, 2004) and the DBMI 04 associated database (Stucchi et al., 2007) have been considered (Tab. 1). For the analysis, subsets of earthquakes with epicentral intensity I 0 ≥ VII MCS and I 0 ≥ VI MCS were used for the Etna region and for the other Italian volcanic districts, respectively. Probability model. We cite here the key-elements of the probabilistic method, referring to Rotondi and Zonno (2004) for a detailed description. Instead of adding a gaussian error to deterministic relationships which express the intensity decay as a function of some factors (epicentral intensity, site-epicenter distance, depth, site types, and styles of faulting), we treat the decay as an <b>aleatory</b> <b>variable</b> defined on the domain { 0, I 0 }. Consequently, we assume that the intensity IS is a discrete binomial distributed variable Bin(I 0, p) where pI 0 means the probability of null decay, and p belongs to [0, 1]. According to the Bayesian approach, p is considered as a random variable following the beta distribution Beta(α, β). Since mean and variance of p are functions of the α, β hyperparameters, we can express our initial knowledge on the decay process through these parameters. To do this, we have divided each macroseismic field in bins of fixed width and the intensity data points in subsets according to this spatial subdivision. For each bin we have repeated the following procedure: a) assessing the prior values to α, β, that is a prior distribution for p; b) updating, through Bayes’ theorem, the hyperparameters {{on the basis of the}} current observations; c) estimating the p parameter through the mean of its posterior distribution. By substituting this estimate in the distribution Bin(I 0, p), we obtain an updated binomial distribution indicated as plug-in distribution. Its mode has been assumed as the expected value of the intensity at the sites within the corresponding bin. To predict the intensity at any distance we have smoothed the p’s estimated in the different bins through a monotonically decreasing function; the lowest mean squared error was given by the inverse power function. Hence, the mode of the plug-in distribution obtained by setting p=g(d) provides an expected value for IS at any distance. If, on the contrary, we assume that, from the attenuation viewpoint, the sites inside any bin behave in the same way, we can average over the domain [0, 1] of p by integrating the product of the likelihood with respect to the posterior Beta distribution of p. In this way we have obtained the so-called predictive distribution for every bin and its mode is taken as expected value for IS at any site inside that bin. Trends in the intensity decay. We have analysed the macroseismic field of the 38 earthquakes constituting our dataset (Tab. 1) by drawing the decay versus the site-epicenter distance of each data point. A quick look at these graphical representations suggests that these earthquakes do not show an homogeneous decay. To identify different trends in the decay, we have synthetized the information contained in each field by collecting, in a matrix, median, mean, and quartile of each set of distances from the epicenter of the points with the same ΔI. Then we have applied to this matrix a clustering algorithm based on the evaluation of the distance between each pair of rows of the matrix. The dataset has been thus partitioned into two groups of events according to their attenuation trend: the first set mainly formed by the earthquakes of Mt. Etna and Vesuvius-Ischia areas, the second one including the events of the Aeolian Islands and Albani Hills. The set 1 shows an higher decay than the set 2, so two different spatial scales are required: bins of width 1 km for the set 1 and of width 25 km for the set 2. A similar classification analysis was performed in Zonno et al. (2008) on 55 earthquakes representative of the Italian territory; in that case three classes were identified. The probabilistic analysis above described has been separately applied to the two sets, discriminating the events of from those of, and using as a priori distributions for the parameters p’s those indicated in Zonno et al. (2008) for the class of earthquakes with the highest attenuation. The hyperparameters α’s and β’s have been then updated through the observed intensity data points according to the expressions α=α 0 + ΣNn= 1 IS (n) and β= β 0 + ΣNn= 1 (I 0 - IS (n)). Some results. For each bin the values of the predictive probability function of for the Etna area and Aeolian Islands, are shown in Fig. 1; the squares indicate the values of the intensity decay computed through the logarithmic regressions (Tab. 2) obtained by Azzaro et al. (2006) with the same dataset. These values can be compared with the mode of the predictive function in each bin. The fit between the two methods is good but much more information is provided by the probabilistic approach. In addition to the estimate of the intensity at any site, the probability distribution of IS provides a measure of the uncertainty and its values can be directly used in the software “SASHA” (D’Amico and Albarello, 2007) to calculate the probabilistic seismic hazard at the site. Conclusions. The identification of different decay trends produced by the clustering algorithm matches well with that already presented in the literature (Azzaro et al. 2006), and this suggests that the method could be successfully applied to other cases. Only two earthquakes in Albani Hills - 1876 / 10 / 26, I 0 VI-VII, 1927 / 12 / 26, I 0 VII-VIII - are unexpectedly included in the set 1 together with the events of Mt. Etna and Vesuvio-Ischia areas; further, detailed analyses are required to explain such an anomaly. Some problems are still open: a) most of the earthquakes here considered have epicentral intensity I 0 VII or VIII, so that we have evaluated the probability functions of IS conditioned on these two values of I 0. Also other values of I 0 must be used in the analysis; b) the method should be also validated on other earthquakes not included in the dataset of Tab. 1, on the basis of probabilistic measures of the degree to which the model predicts the decay in the data points of a macroseismic field (Rotondi and Zonno, 2004) ...|$|E
40|$|There will be {{simplifying}} {{assumptions and}} idealizations in the availability models of complex processes and phenomena. These simplifications and idealizations generate uncertainties {{which can be}} classified as aleatory (arising due to randomness) and/or epistemic (due to lack of knowledge). The problem of acknowledging and treating uncertainty is vital for practical usability of reliability analysis results. The distinction of uncertainties is useful for taking the reliability/risk informed decisions with confidence and also for effective management of uncertainty. In level- 1 probabilistic safety assessment (PSA) of nuclear power plants (NPP), the current practice is carrying out epistemic uncertainty analysis {{on the basis of}} a simple Monte-Carlo simulation by sampling the epistemic variables in the model. However, the aleatory uncertainty is neglected and point estimates of <b>aleatory</b> <b>variables,</b> viz., time to failure and time to repair are considered. Treatment of both types of uncertainties would require a two-phase Monte-Carlo simulation, outer loop samples epistemic variables and inner loop samples <b>aleatory</b> <b>variables.</b> A methodology based on two-phase Monte-Carlo simulation is presented for distinguishing both the kinds of uncertainty in the context of availability/reliability evaluation in level- 1 PSA studies of NPP. © Elsevie...|$|R
30|$|This article {{describes}} the use of gradient-based optimizations and Kriging surrogate models for the propagation of mixed aleatory/epistemic uncertainties for a robust lift-constrained drag minimization problem. Uncertainty due to epistemic variables is propagated via a box-constrained optimization approach, while the uncertainty due to <b>aleatory</b> <b>variables</b> is propagated via sampling of a Kriging surrogate model built with the optimization results. This statistics-of-intervals approach makes robust design under mixed aleatory/epistemic uncertainty possible {{while at the same}} time keeping the computational cost for these types of problems manageable.|$|R
30|$|In the statistics-of-interval approach, {{gradient-based}} optimization {{methods can}} be employed, {{assuming that the}} global extrema in the epistemic design space can be found this way, reducing the cost of each optimization and ensuring very good scaling {{as the number of}} epistemic variables increases if adjoint capabilities [14, 15] are used. To reduce the number of required optimizations for low statistical errors, a surrogate model of the optimization results can be constructed with respect to the <b>aleatory</b> <b>variables</b> which can then be sampled exhaustively, ensuring that fewer optimizations are required to characterize the statistics of the interval accurately.|$|R
3000|$|... at {{the mean}} {{values of the}} <b>aleatory</b> {{uncertainty}} <b>variables</b> α and midpoints of the intervals for the epistemic variables β. This derivative is, in general, non-zero since for the epistemic optimizations, the extreme value is typically encountered on the interval bound. The variances for the problems studied in this paper are {{much smaller than the}} mean values which allow the neglection of [...]...|$|R
30|$|Even {{though one}} flow solve takes only about 10 s on 12 Intel Xeon {{processors}} with 3.33 GHz each {{it is still}} prohibitively expensive to obtain the mixed aleatory/epistemic optimization under uncertainty results through either nested sampling or exhaustive sampling of optimization results. In order to provide validation for the OUU framework with mixed aleatory/epistemic uncertainty the uncertainty propagations of <b>aleatory</b> and epistemic <b>variables</b> are validated only for the initial and optimized points and also only using 3, 000 sample points. But before these combined results are shown, the uncertainty propagations of <b>aleatory</b> and epistemic <b>variables</b> are validated separately.|$|R
30|$|In this paper, mixed aleatory/epistemic {{uncertainties}} in {{a robust}} design problem are propagated via {{the use of}} box-constrained optimizations and surrogate models. The assumption is that the uncertain input parameters {{can be divided into}} a set only containing aleatory uncertainties and a set with only epistemic uncertainties. Uncertainties due to the epistemic inputs can then be propagated via a box-constrained optimization approach, while the uncertainties due to aleatory inputs can be propagated via sampling. A statistics-of-intervals approach is used in which the box-constrained optimization results are treated as a random variable and multiple optimizations need to be performed to quantify the aleatory uncertainties via sampling. A Kriging surrogate is employed to model the variation of the optimization results with respect to the <b>aleatory</b> <b>variables</b> enabling exhaustive Monte-Carlo sampling to determine the desired statistics for each robust design iteration. This approach is applied to the robust design of a transonic NACA 0012 airfoil where shape design variables are assumed to have epistemic uncertainties and the angle of attack and Mach number are considered to have aleatory uncertainties. The very good scalability of the framework in the number of epistemic variables is demonstrated as well.|$|R
40|$|A {{probabilistic}} tornado wind hazard {{model for}} the continental United States (CONUS) is described. The model incorporates both aleatory (random) and epistemic uncertainties associated with quantifying the tornado wind hazard parameters. The temporal occurrences of tornadoes within the continental United States (CONUS) {{is assumed to be}} a Poisson process. A spatial distribution of tornado touchdown locations is developed empirically based on the observed historical events within the CONUS. The hazard model is an aerial probability model that takes into consideration the size and orientation of the facility, the length and width of the tornado damage area (idealized as a rectangle and dependent on the tornado intensity scale), wind speed variation within the damage area, tornado intensity classification errors (i. e.,errors in assigning a Fujita intensity scale based on surveyed damage), and the tornado path direction. Epistemic uncertainties in describing the distributions of the <b>aleatory</b> <b>variables</b> are accounted for by using more than one distribution model to describe aleatory variations. The epistemic uncertainties are based on inputs from a panel of experts. A computer program, TORNADO, has been developed incorporating this model; features of this program are also presented...|$|R
40|$|Nonlinear Energy Sinks (NESs) are a {{promising}} technique for passively reducing the amplitude of vibrations. Through nonlinear stiffness properties, a NES {{is able to}} passively absorb energy. Unlike a traditional Tuned Mass Damper (TMD), NESs do not require a specific tuning and absorb energy {{from a wide range}} of frequencies. However, each NES is only efficient over a limited range of excitations. In addition, NES efficiency is extremely sensitive to perturbations in design parameters or loading, demonstrating a nearly discontinuous efficiency. Therefore, in order to optimally design a NES, uncertainties must be accounted for. This thesis focuses on optimally selecting parameters to design an effective NES system through optimization under uncertainty. For this purpose, a specific algorithm is introduced that makes use of clustering techniques to segregate efficient and inefficient NES behavior. SVM and Kriging approximations as well as new adaptive sampling techniques are used for the optimization under uncertainty. The variables of the problems are either random design <b>variables</b> or <b>aleatory</b> <b>variables.</b> For example, the excitation applied to the main vibrating system is treated as aleatory. In an effort to increase the range of excitations for which NESs are effective, a combination of NESs configured in parallel is considered. Optimization under uncertainty is performed on several examples with varying design parameters as well as different numbers of NESs (from 1 to 10). Results show that combining NESs in parallel is an effective method to increase the excitation range over which a NES is effective...|$|R
40|$|Sampling {{methodology}} in adoption-and-impact assessment {{studies of}} agricultural technology is unique and, {{in a certain}} way, complex. Firstly, assessment studies are conducted over time, and, in most cases, a lack of baseline studies hinders {{the analysis of the}} adoption process. A methodology that can compare the former and current situations has to be designed. Secondly, rural areas must be surveyed, but these are characterized by a lack of the basic information needed to determine the optimum sample size, such as number of sample units (number of farms), farm size, and statistics of several important <b>variables.</b> <b>Aleatory</b> sampling is also difficult because of logistic problems such as limited access to certain municipalities and/or rural areas and insecurity. This paper describes a sampling methodology developed for studying the adoption patterns of cassava production technology on Colombia`s Atlantic Coast, an area which presents the abovementioned constraints. This methodology successfully fulfills all statistical requirements...|$|R
40|$|Traditional {{engineering}} {{analyses and}} designs {{are based on}} deterministic input variables, and variability seen {{in the real world}} are often ignored to simplify the work. Formal reliability analyses are generally avoided by engineers due to large computational costs associated with the traditional methods, such as simulations. Analysis done by engineers in this age of advanced technology are done using finite element analysis which further increase the computational cost of analyzing a reliability problem. Using reliability methods such as Monte Carlo Simulation (MCS) with a finite element analysis requires thousands of trials to be done. This ultimately is not feasible for a complex problem which takes long computational time. Multiplicative Dimensional Reduction Method (MDRM) is a tool which can be used to calculate the statistical parameters of the response of a function with a large reduction in computational efforts. This method has not been applied to uncertainty analysis, geomechanics and fire resistant design problems to determine if this method is indeed worth using over traditional reliability methods (MCS). The Cubature method is another tool which can be used to calculate the statistical moments of a response. This method will be compared to MCS and MDRM to determine its effectiveness. The research objectives in this thesis are therefore 1) to determine if the code developed to use MDRM provides accurate results, 2) to compare the results of MDRM and Cubature to MCS to see how accurate the results of MDRM and Cubature are based on equation based problems, 3) to determine the feasibility of using MDRM with uncertainty analysis problems (where epistemic and <b>aleatory</b> <b>variables</b> are defined), 4) to determine the feasibility of solving a MDRM reliability analysis for fire resistant design problems and 5) to determine the feasibility and computational efficiency of using MDRM for geomechanics problems which are both equation based and finite element analysis. To perform the first objective a problem from Zhang & Pandey (2013) was redone using the code that was developed to make sure the results matched. The second objective was performed by solving steam generator tube failure problem and a time to leak of a pipe problem. The third objective was performed by solving the time to leak of a pipe problem again but this time designating one variable as epistemic and another as aleatory and comparing results between MDRM and MCS. To perform the fourth objective a performance based approach is outlined on how to calculate fire resistant design of a protected and unprotected beam. The results from MDRM and MCS are compared. The fifth and final objective is performed by first showing a step by step method on how to apply MDRM while solving a uni-dimensional consolidation example (settlement of foundation). Lastly two finite element analysis problems are solved to show the application of MDRM with the combination of a finite element analysis. The first problem is of vertical drains and the second problem is of a concrete infinite beam on an elastic foundation. These problems are done using MDRM and MCS and the results and computational effort are compared...|$|R
40|$|In {{this paper}} a unified {{probabilistic}} framework for solving inverse {{problems in the}} presence of epistemic and aleatory uncertainty is presented. The aim is to establish a flexible theory that facilitates Bayesian data analysis in experimental scenarios as they are commonly met in engineering practice. Problems are addressed where learning about unobservable inputs of a forward model, e. g. reducing the epistemic uncertainty of fixed yet unknown parameters and/or quantifying the <b>aleatory</b> uncertainty of <b>variable</b> inputs, is based on processing response measurements. Approaches to Bayesian inversion, hierarchical modeling and uncertainty quantification are combined into a generic framework that eventually allows to interpret and accomplish this task as multilevel model calibration. A joint problem formulation, where quantities that are not of particular interest are marginalized out from a joint posterior distribution, or an intrinsically marginal formulation, which is based on an integrated likelihood function, can be chosen according to the inferential objective and computational convenience. Fully Bayesian probabilistic inversion, i. e. the inference the variability of unobservable model inputs across a number of experiments, is derived as a special case of multilevel inversion. Borrowing strength, i. e. the optimal estimation of experiment-specific unknown forward model inputs, is introduced as a means for combining information in inverse problems. Two related statistica...|$|R
40|$|In {{decision-making}} processes on emission reduction, {{not only are}} emission data needed but also information on the uncertainty of these data. Here, structured expert elicitation was used an uncertainty analysis on NOx emissions from Dutch passenger cars in 1998. Experts from several Dutch research institutes were elicited on individual car performance (emission factors) and volumetric (kilometres driven) variables could be obtained with the expert elicitation method. Total population uncertainty was calculated by propagation and aggregation of individual car uncertainty in a Monte Carlo simulation. The calculation process was explicitly geared to variables showing inherent variability (<b>aleatory</b> uncertainty) and <b>variables</b> that are uncertain {{because of a lack}} of knowledge (epistemic uncertainty). The smallest 95 % uncertainty interval for total population NOx emission was obtained for the TNO-CBS (Statistics Netherlands) expert (- 12 % to + 15 %), while the largest interval was obtained for the RIVM expert (- 35 % to + 51 %). The combination of experts (called decision-makers [DM]) showed intervals of - 30 % to + 41 % (DM before propagation) and - 46 % to + 81 % (DM after aggregation). The use of structured expert elicitation was very time consuming, and there is still a lot of discussion on combining expert data. Therefore, the need for structured expert elicitation should be firmly substantiated and focused on sensitive and controversial variables...|$|R
40|$|International audienceIn {{this paper}} a unified {{probabilistic}} framework for solving inverse {{problems in the}} presence of epistemic and aleatory uncertainty is presented. The aim is to establish a flexible theory that facilitates Bayesian data analysis in experimental scenarios as they are commonly met in engineering practice. Problems are addressed where learning about unobservable inputs of a forward model, e. g. reducing the epistemic uncertainty of fixed yet unknown parameters and/or quantifying the <b>aleatory</b> uncertainty of <b>variable</b> inputs, is based on processing response measurements. Approaches to Bayesian inversion, hierarchical modeling and uncertainty quantification are combined into a generic framework that eventually allows to interpret and accomplish this task as multilevel model calibration. A joint problem formulation, where quantities that are not of particular interest are marginalized out from a joint posterior distribution, or an intrinsically marginal formulation, which is based on an integrated likelihood function, can be chosen according to the inferential objective and computational convenience. Fully Bayesian probabilistic inversion, i. e. the inference the variability of unobservable model inputs across a number of experiments, is derived as a special case of multilevel inversion. Borrowing strength, i. e. the optimal estimation of experiment-specific unknown forward model inputs, is introduced as a means for combining information in inverse problems. Two related statistical models for situations involving finite or zero model/measurement error are devised. Multilevel-specific obstacles to Bayesian posterior computation via Markov chain Monte Carlo are discussed. The inferential machinery of Bayesian multilevel model calibration and its underlying flow of information are studied {{on the basis of a}} system from the domain of civil engineering. A population of identically manufactured structural elements serves as an exemplary system for examining different experimental settings from the standpoint of uncertainty quantification and reduction. In a series of tests the material variability throughout the ensemble of specimens, the entirety of specimen-specific material properties and the measurement error level are inferred under various uncertainties in the problem setup...|$|R

