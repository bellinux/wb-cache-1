11|10000|Public
50|$|Atomicity {{does not}} behave {{completely}} orthogonally {{with regard to}} the other ACID properties of the transactions. For example, isolation relies on atomicity to roll back changes in the event of isolation failures such as deadlock; consistency also relies on rollback {{in the event of a}} consistency-violation by an illegal transaction. Finally, atomicity itself relies on durability to ensure the <b>atomicity</b> <b>of</b> <b>transactions</b> even in the face of external failures.|$|E
40|$|Many {{researchers}} {{have investigated the}} process of decomposing transactions into smaller pieces to increase concurrency. The focus of the research is typically on implementing a decomposition supplied by the database application developer, with relatively little attention to what constitutes a desirable decomposition and how the developer should obtain such a decomposition. In this paper, we argue that the decomposition process itself is worthy of attention. A decomposition generates a set of proof obligations that must be satisfied {{to show that a}} particular decomposition correctly models the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties. Since the decomposition impacts not only the <b>atomicity</b> <b>of</b> <b>transactions,</b> but isolation and consistency properties as well, we present a technique based on formal methods that allows these properties to be surrendered in a carefully controlled manner...|$|E
40|$|Existing {{concurrency}} control systems cannot execute transactions with overlapping updates concurrently. This is especially problematic for bulk updates, which usually overlap with all concurrent transactions. To solve this, {{we have developed}} a {{concurrency control}} mechanism based on lazy evaluation, which moves evaluation of operations from the writer to the reader. This allows readers to prioritize evaluation of those operations {{in which they are}} interested, without loss of <b>atomicity</b> <b>of</b> <b>transactions.</b> To handle bulk operations, we dynamically split large transactions into transactions on smaller parts of the data. In this paper we present an abstract lazy index structure for lazy transactions, and show how transactions can be encoded to effectively use this data structure. Moreover, we discuss evaluation strategies for lazy transactions, where trade-offs can be made between latency and throughput. To evaluate our approach, we have implemented a concurrent lazy trie, on which we performed a number of micro benchmarks...|$|E
3000|$|Proof: Manipulating {{transactions}} is read-only, {{and they}} do not violate the consistency of data objects. The <b>atomicity</b> <b>of</b> manipulating <b>transactions</b> is that, whether all actions of T, AS(T)[*]=[*]{A [...]...|$|R
40|$|Object-oriented {{programming}} languages provide transaction {{behavior by}} means of library classes. In enterprise level programming, transactions are provided as an external feature, not available in standalone programs. This paper introduces transactions as a first-class programming concept, using the <b>atomicity</b> property <b>of</b> <b>transactions</b> to keep the program in a consistent state. Program execution can go wrong, leading to exceptional results and leaving objects possibly in an inconsistent state and as such compromising {{the consistency of the}} entire application. In this paper we primarily focus on the <b>atomicity</b> property <b>of</b> <b>transactions</b> and show that the expressiveness of the programming language is enhanced. This results in programs that are easier to read, and moreover more robust and easier to maintain. The need for boilerplate code, now necessary to handle exceptional cases, is reduced. status: publishe...|$|R
50|$|Other {{features}} of the language are intended to help MUMPS applications {{interact with each other}} in a multi-user environment. Database locks, process identifiers, and <b>atomicity</b> <b>of</b> database update <b>transactions</b> are all required of standard MUMPS implementations.|$|R
40|$|The {{purpose of}} this paper is to report on the first step in our quest for an {{efficient}} atomic commit protocol in realtime databases. This includes the development of realtime implicit yes-vote (RT-IYV), a new real-time atomic commit protocol. In contrast to other real-time commit protocols that provide for semantic atomicity, RT-IYV is designed to ensure the traditional notion of transaction atomicity. To illustrate its performance advantages, we compare RT-IYV with the recently proposed optimistic commit protocol which is also designed to support the standard transaction atomicity in real-time databases. 1 Introduction An atomic commit protocol provides the only mean to ensure the traditional notion of <b>atomicity</b> <b>of</b> <b>transactions</b> in any distributed database system. The two-phase commit protocol [3] is the simplest and most widely used atomic commit protocol. The two-phase commit protocol (2 PC) consumes a substantial amount of a transaction's execution time due to the cost associated w [...] ...|$|E
40|$|A multidatabase system (MDBMS) is a {{facility}} that allows users to access data located in multiple autonomous database management systems (DBMSs) at different sites. To ensure global atomicity for multidatabase transactions, a reliable global atomic commitment protocol is a possible solution. In this protocol a centralized transaction manager (TM) receives global transactions, submits subtransactions to the appropriate sites via AGENTS. An AGENT is a component of MDBS that runs on each site; AGENTS after receiving subtransactions from the transaction manager perform the transaction and send the results back to TM. We have presented a unique proof-of-concept, a JAVA application for an Agent Based Transaction Manager that preserves global atomicity. It provides a user friendly interface through which reliable atomic commitment protocol for global transaction execution in multidatabase environment can be visualized. We demonstrated with three different test case scenarios how the protocol works. This is useful in further {{research in this area}} where <b>atomicity</b> <b>of</b> <b>transactions</b> can be verified for protocol correctness...|$|E
40|$|Directory-based {{coherency}} (DBC) {{has proven}} to be a popular and effective means for making a collection of local memories appear as one large global memory. Database systems composed of multiple nodes, such as client-server and shared-disk systems, often use DBC to achieve coherency. In this paper, we present a method for achieving failure <b>atomicity</b> <b>of</b> <b>transactions</b> in DBC systems, which support fine-granularity locking, and that avoid unnecessary transaction aborts. We use the term Isolated Failure Atomicity (IFA), to denote this property because it ensures failure atomicity while isolating node failures. IFA is achieved through schemes that allow surviving nodes to recover from the effects of transactions that were executing on failed nodes and hence were aborted [...] without waiting for the crashed node(s) to recover first. To this end, we have developed techniques (a) to reconstruct directory information and (b) to recover the data touched by transactions that were running on crashed n [...] ...|$|E
40|$|The Atomic Web Browser {{achieves}} atomicity for distributed transactions {{across multiple}} RESTful APIs. Assuming that theparticipantAPIsfeaturesupportfortheTry-Confirm/Cancel pattern, theusermaynavigatewiththeAtomicWebBrowser among multiple Web sites to perform local resource state transitions (e. g., reservations or bookings). Once the user {{indicates that the}} navigation has successfully completed, the Atomic Web browser takes care of confirming the local transitions to achieve the <b>atomicity</b> <b>of</b> the global <b>transaction...</b>|$|R
40|$|Global {{transaction}} management requires {{cooperation from}} local sites {{to ensure the}} consistent and reliable execution <b>of</b> global <b>transactions</b> in a distributed database system. In a heterogeneous distributed database (or multidatabase) environment, various local sites make conflicting assertions of autonomy over the execution <b>of</b> global <b>transactions.</b> A flexible transaction model for the specification <b>of</b> global <b>transactions</b> {{makes it possible to}} deal robustly with these conflicting requirements. This paper presents an approach that preserves the semi-atomicity (a weaker form <b>of</b> <b>atomicity)</b> <b>of</b> flexible <b>transactions,</b> allowing local sites to autonomously maintain serializability and recoverability. We offer a fundamental characterization <b>of</b> the flexible <b>transaction</b> model and precisely define the semi-atomicity. We investigate the commit dependencies among the subtransactions <b>of</b> a flexible <b>transaction.</b> These dependencies are used to control the commitment order of the subtransactions. We next iden [...] ...|$|R
40|$|AbstractThe {{classical}} theory <b>of</b> <b>transaction</b> management {{contains two}} different aspects, namely concurrency control and recovery, which ensure serializability and <b>atomicity</b> <b>of</b> <b>transaction</b> executions, respectively. Although concurrency control and recovery are not independent of each other, {{the criteria for}} these two aspects were developed orthogonally and as a result, in most cases these criteria are incompatible with each other. Recently a unified theory of concurrency control and recovery for databases with read and write operations has been introduced in [19, 1] that allows reasoning about serializability and atomicity within the same framework. In [19, 1] a class of schedules (called prefix reducible), which guarantees both serializability and atomicity in a failure prone environment with read/write operations was introduced. Several protocols were developed to generate such schedules by a database concurrency control mechanism. We present here a unified transaction model for databases with an arbitrary set of semantically rich operations. We investigate constructive characterization {{of the class of}} prefix reducible schedules with semantically rich operations. It turns out that unlike databases with only read/write operations, the exact characterization of prefix reducible schedules in databases with arbitrary operations is rather infeasible. Thus, we propose here several sufficiently rich subclasses of prefix reducible schedules, and design concurrency control protocols that guarantee both serializability and atomicity for schedules from these classes...|$|R
40|$|Abstract- P 2 P {{technology}} {{provides an}} excellent vehicle for the distribution of digital content at low cost to the content producer and distributor obviating the need for publishing middlemen. However, P 2 P content sharing networks are flawed with freeriding and copyright infringement. Providing financial incentives is one solution to overcome these problems; however the introduction of payments as a method for recompense into the P 2 P environment brings with it many challenges, in particular ensuring <b>atomicity</b> <b>of</b> <b>transactions,</b> non-repudiation and fair exchange of goods for payment. These issues are even more challenging due to the dynamic nature of pure P 2 P systems. In this paper we describe our overlay network of bank peers and illustrate how these challenges are addressed using our Bank service protocols. We have developed a prototype to demonstrate how digital learning material is exchanged for payments using these protocols. Finally, this paper presents an analysis of our network of bank peers {{based on the results}} gathered from our case study. I...|$|E
40|$|AbstractWe {{present a}} new model for {{describing}} and reasoning about transaction-processing algorithms. The model provides a comprehensive, uniform framework for rigorous correctness proofs. The model generalizes previous work on concurrency control to encompass nested transactions and type-specific concurrency control algorithms. Using our model, we describe general conditions for a concurrency control algorithm to be correct-i. e., to ensure that transactions appear to be atomic. We also present a new concurrency control algorithm for abstract data types in a nested transaction system. The algorithm uses commutativity properties of operations to allow high levels of concurrency. The results of operations, {{in addition to their}} names and arguments, can be used in checking for conflicts, further increasing concurrency. We show, using our general model, that the new algorithm is correct. We also present a read-update locking algorithm due to Moss and prove it correct. The correctness proofs for the algorithms are modular, {{in the sense that we}} consider a system structure consisting of many objects, with concurrency control and recovery performed independently at each object. We define a condition on individual objects, called dynamic atomicity, which has the property that as long as all objects in the system are dynamic atomic, transactions will appear atomic. We then show that each algorithm, considered at a single object, ensures dynamic atomicity. This means that different algorithms can be used at different objects; as long as each ensures dynamic atomicity, global <b>atomicity</b> <b>of</b> <b>transactions</b> is guaranteed...|$|E
40|$|In {{the future}} it is {{expected}} that some form of public key infrastructure will be used to support critical applications such as financial transactions which require a high level of reliability, timeliness, and robustness. Current certificate management protocols are implemented on an ad hoc basis with no real guarantee as to their scalability or capability of meeting the required levels of performance. This paper examines the issue of certificate management from a transaction processing perspective, starting with the principles of transaction processing and applying them to fundamentals such as issuing certificates and CRLs, and then moving on to more advanced transaction-based operations such as the management of certificate status information across distributed and often unreliable systems. By building on the extensive body of knowledge which has been established through operating other transaction processing systems such as those used in the financial and airline industries, it is possible to create certificate management infrastructures which can provide certain guarantees which aren’t present in current ad hoc approaches. 1. Certificate Management as Transaction Processing It is envisaged that {{at some point in the}} future some form of certificate-based infrastructure will be used to provide authorisation and authentication facilities for high-value or critical transactions which are currently being carried out without the use of a PKI. These transactions are typically built on top of standard transaction processing (TP) principles which provide certain guarantees of reliability, timeliness, <b>atomicity</b> <b>of</b> <b>transactions,</b> and so on...|$|E
40|$|Timely and {{accurate}} order commitment decisions {{are important to}} supply networks. Many firms face problems such as order over-commitment and inability to fulfill committed orders. Addressing shortcomings in the state-of-the-practice, we developed a protocol based upon the distributed database two-phase commit protocol that guarantees the <b>atomicity</b> <b>of</b> global <b>transactions.</b> We developed extensions of the basic protocol based on requirements of experienced supply chain professionals. Supply chain managers experienced with order promising evaluated the protocol. They opined that adoption of this protocol would reduce the decision-making time by 80 %, enhance the accuracy by 30 % and also improve customer service levels. Supply chain management Inventory Decision support systems Information systems Protocol...|$|R
50|$|If, after a start, the {{database}} {{is found in}} an inconsistent state or not been shut down properly, {{the database}} management system reviews the database logs for uncommitted transactions and rolls back the changes made by these transactions. Additionally, all transactions that are already committed but whose changes were not yet materialized in the database are re-applied. Both are done to ensure <b>atomicity</b> and durability <b>of</b> <b>transactions.</b>|$|R
40|$|This paper studies obstruction-free {{software}} transactional memory systems (OFTMs). These {{systems are}} appealing, for they combine the <b>atomicity</b> property <b>of</b> <b>transactions</b> with a liveness property that ensures the commitment <b>of</b> every <b>transaction</b> that eventually encounters no contention. We precisely define OFTMs and establish {{two of their}} fundamental properties. First, we prove that the consensus number of such systems is 2. This indicates that OFTMs cannot be implemented with plain read/write shared memory, on the one hand, but, on the other hand, do not require powerful universal objects, such as compare-and-swap. Second, we prove that OFTMs cannot ensure disjoint-access-parallelism (in a strict sense). This may result in artificial “hot spots ” and thus limit the performance of OFTMs. ...|$|R
40|$|AbstractA multidatabase system (MDBS) is a {{software}} system for integration of preexisting and independent local database management systems (DBMSs). The transaction management problem in MDBSs consists of designing appropriate software, {{on top of}} local DBMSs, such that users can execute transactions that span multiple local DBMSs without jeopardizing database consistency. The difficulty in transaction management in MDBSs arises due to the heterogeneity of the transaction management algorithms used by the local DBMSs, {{and the desire to}} preserve their local autonomy. In this paper, we develop a framework for designing fault-tolerant transaction management algorithms for MDBS environments that effectively overcomes the heterogeneity- and autonomy-induced problems. The developed framework builds on our previous work. It uses the approach described in S. Mehrotra et al. (1992, in “Proceedings of ACM–SIGMOD 1992 International Conference on Management of Data, San Diego, CA”) to overcome the problems in ensuring serializability that arise due to heterogeneity of the local concurrency control protocols. Furthermore, it uses a redo approach to recovery for ensuring transaction atomicity (Y. Breitbart et al., 1990, in “Proceedings of ACM–SIGMOD 1990 International Conference on Management of Data, Atlantic City, NJ;” Mehrotra et al., 1992, in “Proceedings of the Eleventh ACM SIGACT–SIGMOD–SIGART Symposium on Principles of Database Systems, San Diego, CA;” and A. Wolski and J. Veijalainen, 1990, in “Proceedings of the International Conference on Databases, Parallel Architectures and Their Applications”, pp. 321 – 330), that strives to ensure <b>atomicity</b> <b>of</b> <b>transactions</b> without the usage of the 2 PC protocol. We reduce the task of ensuring serializability in MDBSs in the presence of failures to solving three independent subproblems, solutions to which together constitute a complete strategy for failure-resilient transaction management in MDBS environments. We develop mechanisms with which each of the three subproblems can be solved without requiring any changes be made to the preexisting software of the local DBMSs and without compromising their autonomy...|$|E
40|$|The {{rapid growth}} of the global {{information}} infrastructure opens new opportunities to perform joint work {{in areas such as}} cooperative authoring, distributed software development, and business workflows. Cooperative work on shared data requires computer support to coordinate the work of multiple users and to ensure the consistency of data processed within a cooperative application. Maintaining data consistency in multi-user systems is the classical problem of transaction management. The traditional transaction model addresses environments in which a large number of relatively short-lived transactions access a shared database. It is based on the fundamental properties of atomicity, consistency, isolation, and durability of transactions. However, this transaction model does not meet the requirements of cooperative applications. Isolation of transactions, as guaranteed by the ACID properties, contradicts the need to cooperate. <b>Atomicity</b> <b>of</b> <b>transactions</b> is too strict for interactive environments, where activities are of long duration. A cooperative transaction model needs to support the interactive execution of longrunning activities in which competition for resources, on which the locking protocols are based, is replaced by the need to cooperate. The emphasis, therefore, is not on preventing access to resources, but rather on the semantically correct exchange of information among concurrent activities of cooperating users. In this dissertation, we present a semantics-based cooperative transaction model, called COACT, that addresses the key features of cooperative applications, i. e., interactive user control, long-duration activities, and multiuser cooperation on shared persistent data. The concepts presented in this thesis are preceded by a thorough analysis of requirements derived from a broad spectrum of cooperative applications, ranging from cooperative hypermedia authoring to workflow applications. The COACT cooperative transaction model builds on the observation that cooperative work is characterized by alternating periods of individual and joint work. Therefore, COACT combines a workspace concept with a new synchronization mechanism, called history merging. To allow for individual work, we assign a private workspace to every user participating in a cooperative activity. Additionally, there is a common workspace accessible by all participants. To enable joint work, COACT provides a set of information exchange primitives that are all based on history merging. These primitives allow group members to exchange information among workspaces in order to combine their work into a coherent whole. To that end, we take an operation-oriented view in COACT. State transitions on workspaces are modeled as operations and are recorded in workspace histories. Information exchange among workspaces is realized by the exchange of operations among workspaces, instead of data objects. The consistency of common work results is determined {{on the basis of the}} semantics of operations performed in the workspaces. COACT utilizes both backward and forward commutativity relations for this. The flexibility of our approach is mainly achieved by its consideration of operations semantics for dynamically determining consistent units of work, for detecting and resolving conflicts during a merge, and for selective compensation of operations in workspaces. Merging histories with pre-specified commutativity relations provides transactional guarantees to a cooperative activity. In particular, we guarantee in COACT that all workspace histories remain legal. Using legality as correctness criterion, history merging relaxes the isolation and atomicity properties of the traditional transaction model, while ensuring consistency of the workspaces. History merging also provides a basis for management of consistency between disconnected or mobile users who operate independently and, yet, must occasionally reconcile their work with each other or with the stationary system. We show in this thesis how to adapt the COACT cooperative transaction model to the special characteristics of mobile environments in order to support disconnected operation mode. Besides the conceptual development of a formal transaction model, a proof-of-concept demonstrator has been implemented. We describe in this dissertation how the COACT cooperative transaction model has been implemented using an object-oriented database management system and how the new cooperation facilities can be integrated into a hypermedia authoring system. We believe that the integration of a cooperative transaction model like COACT with advanced database technology can ease the development and deployment of cooperative applications. Furthermore, history merging provides directions for solving open research problems, like semi-automatic merging of versions and re-integration support in mobile environments and asynchronous replication...|$|E
40|$|Transactional memory (TM) systems {{implement}} {{the concept of}} an atomic execution unit called a transaction in order to discharge programmers from explicit synchronization management. But when shared data is atomically accessed by both transaction and non-transactional code, a TM system must provide strong isolation in order to overcome consistency problems. Strong isolation enforces ordering between non-transactional operations and transactions and preserves the <b>atomicity</b> <b>of</b> a <b>transaction</b> even with respect to non-transactional code. This paper presents a TM algorithm that implements strong isolation with the following features: (a) concurrency control of non-transactional operations is not based on locks and is particularly efficient, and (b) any non-transactional read or write operation always terminates (there is no notion of commit associated with them) ...|$|R
40|$|Atomicity is an {{important}} correctness condition for concurrent systems. Informally, atomicity is the property that every concurrent execution of a set <b>of</b> <b>transactions</b> is equivalent to some serial execution <b>of</b> the same <b>transactions.</b> In multi-threaded programs, executions of procedures (or methods) {{can be regarded as}} transactions. Correctness in the presence of concurrency often requires <b>atomicity</b> <b>of</b> these <b>transactions.</b> Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. This paper presents new algorithms for runtime (dynamic) detection of violations of conflict-atomicity and view-atomicity, which are analogous to conflict-serializability and view-serializability in database systems. In these algorithms, the recorded events are formed into a graph with edges representing the synchronization within each transaction and possible interactions between transactions. We give conditions on the graph that imply conflict-atomicity and view-atomicity. Experiments show that these new algorithms are more efficient in most experiments and are more accurate than previous algorithms with comparable asymptotic complexity...|$|R
40|$|This {{dissertation}} describes runtime, static, and hybrid {{analyses to}} detect synchronization errors in multi-threaded programs. Three kinds of synchronization errors are considered: deadlocks, data races, and atomicity violations. Deadlocks and data races are well-known {{and have been}} studied for a long time. Atomicity violation is not as well-known and deeply studied. This dissertation focuses on detecting atomicity violations. Atomicity is a correctness condition for concurrent systems. Informally, atomicity is the property that every concurrent execution of a set <b>of</b> <b>transactions</b> is equivalent to some serial execution <b>of</b> the same <b>transactions.</b> In multi-threaded programs, executions of procedures (or methods) {{can be regarded as}} transactions. Correctness in the presence of concurrency typically requires <b>atomicity</b> <b>of</b> these <b>transactions.</b> Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. Furthermore, an atomic code block can be treated as a single transition during subsequent analysis of the program; this can greatly improve the efficiency of the subsequent analysis. This dissertation describes three algorithms for runtime detection <b>of</b> <b>atomicity</b> viola...|$|R
40|$|A {{transaction}} {{is traditionally}} defined {{so as to}} provide the properties <b>of</b> <b>atomicity,</b> consistency, integrity, and durability (ACID) for any operation it performs. In order to ensure the <b>atomicity</b> <b>of</b> distributed <b>transactions,</b> an atomic commit protocol needs {{to be followed by}} all sites participating in a transaction execution to agree on the final outcome, that is, commit or abort. A variety of commit protocols have been proposed that either enhance the performance of the classical two-phase commit protocol during normal processing or reduce the cost of recovery processing after a failure. In this chapter, we survey a number of two-phase commit variants and optimizations, including some recent ones, providing an insight in the performance trade-off between normal and recovery processing. We also analyze the performance of a representative set of commit protocols both analytically as well as empirically using simulation. 13. 1 INTRODUCTION Transactions are powerful abstractions that facil [...] ...|$|R
40|$|Atomicity is a {{correctness}} {{condition for}} concurrent systems. Informally, atomicity is the property that every concurrent {{execution of a}} set <b>of</b> <b>transactions</b> is equivalent to some serial execution <b>of</b> the same <b>transactions.</b> In multi-threaded programs, executions of procedures (or methods) {{can be regarded as}} transactions. Correctness in the presence of concurrency typically requires <b>atomicity</b> <b>of</b> these <b>transactions.</b> Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. This paper describes two algorithms for runtime detection <b>of</b> <b>atomicity</b> violations and compares their cost and effectiveness. The reduction-based algorithm checks atomicity based on commutativity properties of events in a trace; the block-based algorithm efficiently represents the relevant information about a trace as a set of blocks (i. e., pairs of events plus associated synchronizations), and checks atomicity by comparing each block with other blocks. To improve the efficiency and accuracy of both algorithms, we incorporate a multi-lockset algorithm for checking data races, dynamic escape analysis, and happenbefore analysis. Experiments show that both algorithms are effective in finding atomicity violations. The block-based algorithm is more accurate but more expensive than the reduction-based algorithm...|$|R
40|$|This paper {{addresses}} the problem <b>of</b> a <b>transaction</b> {{reading and writing}} data at multiple classification levels in a Multilevel Secure (MLS) database. We refer to such transactions as multilevel update transactions. We show that no scheduler can ensure <b>atomicity</b> <b>of</b> multilevel update <b>transactions</b> in the presence <b>of</b> <b>transaction</b> aborts {{and at the same}} time be secure. There are essentially two ways of scheduling multilevel update transactions. The first method, which ensures strong atomicity, involves delaying low-level subtransactions until the fates of the sibling high-level subtransactions are known. The second scheme, which ensures only semantic atomicity, involves compensating the effects of any committed subtransactions. Analysis of these schemes indicates that the compensation approach leads to lower covert channel bandwidths. A concurrency control and recovery protocol based on compensation is proposed for multilevel update transactions. The security and correctness of the protocol is [...] ...|$|R
40|$|We {{identify}} {{one of the}} incompatibility {{problems associated}} with atomic commit protocols that prevents them from being used together and we derive a correctness criterion that captures the correctness of their integration. We also present a new atomic commit protocol, called Presumed Any, that integrates the three commonly known two-phase commit protocols and prove its correctness. Keywords Two-Phase Commit, Atomic Commit Protocols, Distributed Transaction Processing, Multi-database Systems, Distributed Systems. 1. INTRODUCTION An atomic commit protocol (ACP) is the only mean to ensure the traditional <b>atomicity</b> property <b>of</b> <b>transactions</b> in any distributed database system. This is to guarantee, in spite of communication and site failures, that all sites participating in a transaction's execution agree on the final outcome <b>of</b> the <b>transaction,</b> i. e., to either commit or abort the transaction. Since commit processing consumes {{a substantial amount of}} a transaction's execution time and ACPs [...] ...|$|R
40|$|Federated {{transaction}} management (also {{known as}} multidatabase transaction {{management in the}} literature) is needed to ensure the consistency of data that is distributed across multiple, largely autonomous, and possibly heterogeneous component databases and accessed by both global and local transactions. While the global <b>atomicity</b> <b>of</b> such <b>transactions</b> can be enforced by using a standardized commit protocol like XA or its CORBA counterpart OTS, global serializability is not self-guaranteed as the underlying component systems may {{use a variety of}} potentially incompatible local concurrency control protocols. The problem of how to achieve global serializability, by either constraining the component systems or implementing additional global protocols at the federation level, has been intensively studied in the literature, but did not have much impact on the practical side. A major deficiency of the prior work has been that it focused on the idealized correctness criterion of serializability a [...] ...|$|R
40|$|Abstract—Atomicity is a {{correctness}} {{condition for}} concurrent systems. Informally, atomicity is the property that every concurrent {{execution of a}} set <b>of</b> <b>transactions</b> is equivalent to some serial execution <b>of</b> the same <b>transactions.</b> In multithreaded programs, executions of procedures (or methods) {{can be regarded as}} transactions. Correctness in the presence of concurrency typically requires <b>atomicity</b> <b>of</b> these <b>transactions.</b> Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. This paper describes two algorithms for runtime detection <b>of</b> <b>atomicity</b> violations and compares their cost and effectiveness. The reduction-based algorithm checks atomicity based on commutativity properties of events in a trace; the block-based algorithm efficiently represents the relevant information about a trace as a set of blocks (i. e., pairs of events plus associated synchronizations) and checks atomicity by comparing each block with other blocks. To improve the efficiency and accuracy of both algorithms, we incorporate a multilockset algorithm for checking data races, dynamic escape analysis, and happenbefore analysis. Experiments show that both algorithms are effective in finding atomicity violations. The block-based algorithm is more accurate but more expensive than the reduction-based algorithm. Index Terms—Concurrent programming, testing and debugging, Java, data race, atomicity. ...|$|R
40|$|We {{present the}} TIC (Transactions with Isolation and Cooperation) model for {{concurrent}} programming. TIC adds to standard transactional memory {{the ability for}} a transaction to observe the effects of other threads at selected points. This allows transactions to cooperate, {{as well as to}} invoke nonrepeatable or irreversible operations, such as I/O. Cooperating transactions run the danger of exposing intermediate state and of having other threads change the transaction’s state. The TIC model protects against unanticipated interference by having the type system keep track of all operations that may (transitively) violate the <b>atomicity</b> <b>of</b> a <b>transaction</b> and require the programmer to establish consistency at appropriate points. The result is a programming model that is both general and simple. We have used the TIC model to re-engineer existing lock-based applications including a substantial multi-threaded web mail server and a memory allocator with coarse-grained locking. Our experience confirms the features of the TIC model: It is convenient for the programmer, while maintaining the benefits of transactional memory...|$|R
40|$|RDF {{is one of}} the {{technologies}} proposed to realise the vision of the Semantic Web and it is being increasingly used in distributed web-based applications. The use of RDF in applications that require timely noti cation of metadata changes raises the need for mechanisms for monitoring and processing such changes. Event-Condition-Action (ECA) rules are a natural candidate to ful ll this need. In this paper, we study ECA rules on RDF metadata in P 2 P environments. We describe a language for de ning ECA rules on RDF metadata, including its syntax and execution semantics. We develop conservative tests for determining the termination and con uence of sets of such ECA rules. We describe an architecture supporting such rules in P 2 P environments, and our current implementation of this architecture. We also discuss techniques for relaxing the isolation and <b>atomicity</b> requirements <b>of</b> <b>transactions.</b> ...|$|R
40|$|Abstract. RDF {{is one of}} the {{technologies}} proposed to realise the vision of the Semantic Web and it is being increasingly used in distributed webbased applications. The use of RDF in applications that require timely notification of metadata changes raises the need for mechanisms for monitoring and processing such changes. Event-Condition-Action (ECA) rules are a natural candidate to fulfill this need. In this paper, we study ECA rules on RDF metadata in P 2 P environments. We describe a language for defining ECA rules on RDF metadata, including its syntax and execution semantics. We also develop conservative tests for determining when the order of execution of pairs of rules, or instances of the same rule, is immaterial to the final state of the RDF metadata after rule execution terminates. We describe an architecture for supporting such rules in P 2 P environments, and discuss techniques for relaxing the isolation and <b>atomicity</b> requirements <b>of</b> <b>transactions.</b> ...|$|R
40|$|Abstract — Business {{processes}} involve dynamic compositions of interleaved tasks. Therefore ensuring reliable transactional {{processing of}} Web services {{is crucial for}} the success of Web service-based B 2 B and B 2 C applications. But the inherent autonomy and heterogeneity of Web services render the applicability <b>of</b> conventional ACID <b>transaction</b> models for Web services far from being straightforward. Current Web service transaction models relax the isolation property and rely on compensation mechanisms to ensure <b>atomicity</b> <b>of</b> business <b>transactions</b> in the presence of service failures. However, ensuring consistency in the open and dynamic environment of Web services, where interleaving business transactions enter and exit the system independently, remains an open issue. In this paper we address this problem and propose an architecture that supports concurrency control on the Web services level. An extension to the standard framework for Web service transactions is proposed to enable detecting and handling transactional dependencies between concurrent business transactions. We also present an optimistic protocol for concurrency control that can be deployed in a fully distributed fashion within the proposed architecture. We also empirically evaluate the performance of the proposed solutions in terms of throughput and response time...|$|R
40|$|In the {{presence}} of semantic information, serializability is too strong a correctness criterion and unnecessarily restricts concurrency. We use the semantic information <b>of</b> a <b>transaction</b> to provide different <b>atomicity</b> views <b>of</b> the <b>transaction</b> to other transactions. The proposed approach improves concurrency and allows interleavings among transactions which are non-serializable, but which nonetheless preserve {{the consistency of the}} database and are acceptable to the users. We develop a graph-based tool whose acyclicity is both a necessary and sufficient condition for the correctness of an execution. Our theory encompasses earlier proposals that incorporate semantic information <b>of</b> <b>transactions.</b> Furthermore it is the first approach that provides an efficient graph based tool for recognizing correct schedules without imposing any restrictions on the application domain. Our approach is widely applicable to many advanced database applications such as systems with long-lived transactions and co [...] ...|$|R
40|$|The authors {{consider}} the failure <b>atomicity</b> problem <b>of</b> distributed <b>transactions</b> {{in conjunction with}} the maximization of database availability. They propose a new information-based model for the distributed transaction execution, which explicitly expresses the information at each stage during a protocol. In addition to rederiving certain existing results. They prove a fundamental relation among the site failures and the network partitioning. The authors also propose a realistic model for site failures under which it is shown that the costs of commit and termination protocols can be greatly reduced. Finally, they explore the possible recovery strategies for a failed site and show how these are improved under their site failure model. link_to_subscribed_fulltex...|$|R
40|$|In {{distributed}} transactional systems, an Atomic Commitment Protocol (ACP) is used {{to ensure}} the <b>atomicity</b> <b>of</b> distributed <b>transactions</b> even {{in the presence of}} failures. An ACP is said to be non-blocking if it allows correct participants to decide on the transaction despite the failure of others. Several non-blocking protocols have been proposed in the literature. However, none of these protocols is able to combine high efficiency during normal processing with fault-tolerance (i. e. non-blocking). In this paper, we present a non-blocking atomic commitment protocol, noted ANB-CLL (Asynchronous Non-Blocking Coordinator Logical Log), that drastically reduces the cost <b>of</b> distributed <b>transaction</b> commitment in terms of time delay and message complexity. Performance analysis shows that the resulting protocol is more efficient than all other non-blocking protocols proposed in the literature. An important characteristic of ANB-CLL is that it can be applied to commercial transactional systems that are not 2 PC compliant. To achieve non-blocking, ANB-CLL uses a uniform consensus protocol as a termination protocol in an asynchronous system augmented with an unreliable failure detector, and in which processes may crash and recover. By supporting recovery, we study, for the first time, the problem of non-blocking atomic commitment in asynchronous systems based on a crash-recovery model of computation. Key Words: atomic commitment protocols, fault tolerance, asynchronous systems...|$|R
