28|10000|Public
50|$|Etirinotecan pegol was in {{the phase}} III BEACON trial, {{and is in the}} I-SPY2 <b>adaptive</b> <b>clinical</b> <b>trial</b> for breast cancer.|$|E
50|$|Activity {{has been}} seen in a phase II trial for metastatic triple-negative breast cancer. It was {{included}} in the I-SPY2 <b>adaptive</b> <b>clinical</b> <b>trial</b> for breast cancer and has 'graduated' to the later stage trials.|$|E
5000|$|... 2011: A phase 1 {{clinical}} trial of MK-2206 alone has reported {{it was well}} tolerated.2014: A phase 1 {{clinical trial}} of MK-2206 {{with a variety of}} other agents in 72 patients with advanced cancer reported acceptable side-effects.2016: MK-2206 is one of the treatments in the I-SPY2 <b>Adaptive</b> <b>clinical</b> <b>trial</b> for breast cancer that had been selected for later stage trials.|$|E
40|$|Much {{attention}} has been given in recent years to <b>adaptive</b> designs of <b>clinical</b> <b>trials</b> as ethical alternatives when the traditional randomization becomes ethically infeasible. But such designs create dependency among the collected data, and hence statistical methods for <b>adaptive</b> <b>clinical</b> <b>trials</b> are more complex than those for traditional randomized <b>clinical</b> <b>trials.</b> In this paper, we examine some extensions of common statistical methods for independent data. Under regularity conditions, the logarithm of likelihood ratio statistic 2 ln[lambda] for dependent data is shown to be asymptotically chi-square distributed, providing a foundation for asymptotic analysis of <b>adaptive</b> <b>clinical</b> <b>trials</b> with k treatments. We also discuss both the consistency and the asymptotic normality of the maximum likelihood estimators for a wide class of <b>adaptive</b> designs. <b>Clinical</b> <b>trials</b> Response <b>adaptive</b> designs Consistency Asymptotic normality Chi-square distribution...|$|R
50|$|CompassCompass {{is used by}} biostatisticians and clinicians to {{plan and}} design earlier stage <b>adaptive</b> <b>clinical</b> <b>trials</b> (traditionally known as phase 1 human {{tolerance}} and phase 2 dose-selection studies).|$|R
50|$|ClearTrial’s <b>clinical</b> <b>trial</b> {{subject matter}} experts (SMEs) have {{published}} peer-reviewed articles on functional service provider outsourcing in <b>clinical</b> <b>trials,</b> <b>adaptive</b> <b>clinical</b> <b>trials,</b> and <b>clinical</b> <b>trials</b> for medical devices, {{as well as}} articles on <b>clinical</b> <b>trial</b> trends. In April 2011, ClearTrial was named one of five “Cool Vendors in Life Sciences” by the research firm Gartner.|$|R
5000|$|An <b>adaptive</b> <b>{{clinical}}</b> <b>trial</b> is {{a clinical}} trial that evaluates a medical device or treatment by observing participant outcomes (and possibly other measures, such as side-effects) on a prescribed schedule, and modifying parameters of the trial protocol in accord with those observations. The adaptation process generally continues throughout the trial, as prescribed in the trial protocol. Modifications may include dosage, sample size, drug undergoing trial, patient selection criteria and [...] "cocktail" [...] mix. In some cases, trials have become an ongoing process that regularly adds and drops therapies and patient groups as more information is gained. Importantly, the trial protocol is set before the trial begins; the protocol pre-specifies the adaptation schedule and processes.|$|E
5000|$|I-SPY 2 is an <b>adaptive</b> <b>clinical</b> <b>trial</b> of {{multiple}} Phase 2 treatment regimens combined with standard chemotherapy. I-SPY 2 linked 19 academic cancer centers, two community centers, the FDA, the NCI, pharmaceutical and biotech companies, patient advocates and philanthropic partners. The trial {{is sponsored by}} the Biomarker Consortium of the Foundation for the NIH (FNIH), and is co-managed by the FNIH and QuantumLeap Healthcare Collaborative. I-SPY 2 was designed to explore the hypothesis that different combinations of cancer therapies have varying degrees of success for different patients. Conventional clinical trials that evaluate post-surgical tumor response require a separate trial with long intervals and large populations to test each combination. Instead, I-SPY 2 is organized as a continuous process. It efficiently evaluates multiple therapy regimes by relying on the predictors developed in I-SPY 1 that help quickly determine whether patients with a particular genetic signature will respond to a given treatment regime. The trial is adaptive in that the investigators learn as they go, and do not continue treatments that appear to be ineffective. All patients are categorized based on tissue and imaging markers collected early and iteratively (a patient's markers may change over time) throughout the trial, so that early insights can guide treatments for later patients. Treatments that show positive effects for a patient group can be ushered to confirmatory clinical trials, while those that do not can be rapidly sidelined. Importantly, confirmatory trials can serve as a pathway for FDA Accelerated Approval. I-SPY 2 can simultaneously evaluate candidates developed by multiple companies, escalating or eliminating drugs based on immediate results. Using a single standard arm for comparison for all candidates in the trial saves significant costs over individual Phase 3 trials. All data are shared across the industry. [...] I-SPY 2 is comparing 11 new treatments against 'standard therapy', and is estimated to complete in Sept 2017. By mid 2016 several treatments had been selected for later stage trials.|$|E
40|$|Administration (FDA) {{to explore}} how <b>adaptive</b> <b>clinical</b> <b>trial</b> design might improve the {{evaluation}} of drugs and medical devices. ADAPT-IT uses the National Institute of Neurologic Disorders & Stroke-supported Neurological Emergencies Treatment Trials (NETT) network as a ‘laboratory ’ in which to study the development of <b>adaptive</b> <b>clinical</b> <b>trial</b> designs in the confirmatory setting. The Stroke Hyperglycemia Insulin Network Effort (SHINE) trial was selected for funding by the NIH-NINDS {{at the start of}} ADAPT-IT and is currently an ongoing phase III trial of tight glucose control in hyperglycemic acute ischemic stroke patients. Within ADAPT-IT, a Bayesian adaptive Goldilocks trial design alternative was developed. Methods: The SHINE design includes response adaptive randomization, a sample size re-estimation, and monitoring for early efficacy and futility according to a group sequential design. The Goldilocks design includes more frequent monitoring for predicted success or futility and a longitudinal model of the primary endpoint. Both trial designs were simulated and compared in terms of their mean sample size and power across a range of treatment effects and success rates for the control group. Results: As simulated, the SHINE design tends to have slightly higher power and the Goldilocks design has a lower mean sample size. Both designs were tuned to have approximately 80 % power to detect a difference of 25 % versus 32 % between control and treatment, respectively. In this scenario, mean sample sizes are 1, 114 and 979 for th...|$|E
5000|$|According to FDA guidelines, an <b>adaptive</b> Bayesian <b>clinical</b> <b>trial</b> can involve: ...|$|R
40|$|ABSTRACT: The {{randomized}} play-the-winner rule is {{an adaptive}} randomized design, {{based on an}} urn model, that is used occasionally in <b>clinical</b> <b>trials.</b> This paper discusses practical and theoretical issues arising from its use, including stratification, delayed response, operating characteristics, selection of urn parameters, and inference. The paper also discusses recent experience with <b>adaptive</b> <b>clinical</b> <b>trials</b> within the pharmaceutical indus-try. The author concludes that the randomized play-the-winner rule is appropriate for some <b>clinical</b> <b>trials,</b> but intense and thoughtful planning must {{take place in the}} design phase. Such planning should incorporate considerations of variability, power, and ap...|$|R
50|$|In 2004, a Strategic Path Initiative was {{introduced}} by the United States’ Food and Drug Administration (FDA) to modify the way drugs travel from lab to market. This initiative aimed at dealing with the high attrition levels observed in the clinical phase. It also attempted to offer flexibility to investigators to find the optimal clinical benefit without affecting the study's validity. <b>Adaptive</b> <b>clinical</b> <b>trials</b> initially came under this regime.|$|R
40|$|Developing {{a vaccine}} against the human {{immunodeficiency}} virus (HIV) poses an exceptional challenge. There are no documented cases of immune-mediated clearance of HIV from an infected individual, and no known correlates of immune protection. Although nonhuman primate models of lentivirus infection have provided valuable data about HIV pathogenesis, such models do not predict HIV vaccine efficacy in humans. The combined lack of a predictive animal model and undefined biomarkers of immune protection against HIV necessitate that vaccines to this pathogen be tested directly in clinical trials. <b>Adaptive</b> <b>clinical</b> <b>trial</b> designs can accelerate vaccine development by rapidly screening out poor vaccines while extending the evaluation of efficacious ones, improving the characterization of promising vaccine candidates and the identification of correlates of immune protection...|$|E
40|$|Prior to {{the start}} of an <b>adaptive</b> <b>clinical</b> <b>trial,</b> demand for an investigational drug can be highly uncertain. Both {{recommended}} dosages and patient recruitment can fluctuate in re-sponse to early trial results. While initial demand forecasts can be very wrong, the factors influencing future demand can be learned during the trial. To take advantage of this learn-ing, intra-trial batches can be produced, but at the expense of scale economies. Using various learning curves, we study this balance between learning and economies of scale in a finite horizon inventory model with fixed production costs and two production options: The pre-trial batch and the intra-trial batch. We characterize the optimal policy for both produc-tion batches in regards to optimally scheduling and sizing production. Through analytical and numerical studies, we develop insights on the impact of fixed costs, learning rates, and penalty costs on the value of the intra-trial batch, the timing of the intra-trial batch, {{and the size of the}} pre-trial batch. 1...|$|E
40|$|Bayesian {{methods are}} used {{increasingly}} across {{all phases of}} the pharmaceutical research and development cycle. Examples include early-phase <b>adaptive</b> <b>clinical</b> <b>trial</b> design, population-pk modeling and analysis of safety data in all phases. These applications are driving the need for robust, flexible and novel Bayesian analytics, graphics and reporting software. This article and presentation reviews Insightful Corporation’s approach to providing Bayes analysis software tools and solutions to support the needs of pharmaceutical development. In particular, the Insightful S-PLUS ® flexBayes library (S+flexBayes), {{in conjunction with the}} collaborative S-PLUS framework, provide statisticians and statistical programmers with a flexible and productive solution for Bayes modeling. Results from these analyses are readily deployed as interactive clinical review graphics or publication reports as part of the Insightful Clinical Review and Reporting Solutions. We illustrate the S-PLUS Bayes modeling and collaborative Clinical Review Solutions with an application of the Berry and Berry (2004) model for analysis of adverse events...|$|E
40|$|In {{this review}} article, we address the {{radiation}} oncology process improvements in <b>clinical</b> <b>trials</b> and review how these changes {{improve the quality}} {{for the next generation}} of trials. In recent years, we have progressed from a time of limited data acquisition to the present in which we have real-time influence of <b>clinical</b> <b>trials</b> quality. This enables immediate availability of the important elements, including staging, eligibility, response, and outcome for all trial investigators. Modern informatics platforms are well designed for future <b>adaptive</b> <b>clinical</b> <b>trials.</b> We review what will be needed in the informatics architecture of current and future <b>clinical</b> <b>trials...</b>|$|R
40|$|Growing {{interest}} in stratified medicine {{is leading to}} increasing importance of subgroup analyses in confirmatory <b>clinical</b> <b>trials.</b> Conventionally, confirmatory <b>clinical</b> <b>trials</b> either focus on a subgroup identified in advance or assess subgroup effects once the trial is completed. The {{focus of this article}} is methodology for <b>adaptive</b> <b>clinical</b> <b>trials</b> that both identify whether a treatment is particularly effective in a predefined subgroup, potentially enabling alteration of recruitment, and assess the effectiveness in the subgroup and/or whole population. Methods for such adaptive trials are described and compared, and the logistical and regulatory issues associated with such approaches are discussed...|$|R
40|$|Abstract. One {{element of}} the {{analysis}} of <b>adaptive</b> <b>clinical</b> <b>trials</b> is combining the evidence from several (often two) stages. When the endpoint is binary, standard single stage tests statistics do not control size well. Yet the combined test might not be valid if the single stage tests are not. The {{purpose of this paper}} is to numerically and theoretically examine the extent to which combining basic tests statistics mitigates or magnifies the size violation of the final test. ...|$|R
40|$|From {{scientific}} experiments to online A/B testing, the previously observed data often affects how future experiments are performed, {{which in turn}} affects which data will be collected. Such adaptivity introduces complex correlations between the data and the collection procedure. In this paper, we prove that when the data collection procedure satisfies natural conditions, then sample means of the data have systematic negative biases. As an example, consider an <b>adaptive</b> <b>clinical</b> <b>trial</b> where additional data points {{are more likely to}} be tested for treatments that show initial promise. Our surprising result implies that the average observed treatment effects would underestimate the true effects of each treatment. We quantitatively analyze the magnitude and behavior of this negative bias in a variety of settings. We also propose a novel debiasing algorithm based on selective inference techniques. In experiments, our method can effectively reduce bias and estimation error. Comment: Accepted to the 21 st International Conference on Artificial Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spai...|$|E
40|$|We have {{constructed}} a response <b>adaptive</b> <b>clinical</b> <b>trial</b> to treat patients sequentially {{in order to}} maximize the total survival time of all patients. Typically the response adaptive design is based on the urn models or on sequential estimation procedures, but we used a bandit process in this dissertation. The objective of a bandit process is to optimize a measure of sequential selections from several treatments. Each treatment consist of a sequence of conditionally independent and identically distributed random variables, and some of these treatment have unknown distribution functions. For the purpose of this clinical trial, we are focusing on the bandit process with delayed response. These responses are lifetime variables which may be censored upon their observations. Following the Bayesian approach and dynamic programming technique, we formulated a controlled stochastic dynamic model. In addition, we used an example to illustrate the possible application of the main results as well as "R" to implement a model simulation...|$|E
40|$|Drug {{development}} for tuberculosis (TB) faces numerous practical obstacles, including {{the need for}} combination treatment with at least three drugs, reliance on possibly unrepresentative animal models which may not reproduce key features of human disease {{and the lack of}} a well-validated surrogate endpoint for stable cure. Pivotal Phase III trials are large, lengthy and expensive, and the funding and capacity to conduct them are limited worldwide. More rational methods for the selection of priority regimens for Phase III are urgently needed to avoid costly late-stage failures. We examine the suitability of <b>adaptive</b> <b>clinical</b> <b>trial</b> designs for drug development in TB, focusing on designs for Phase IIB and III trials, where we believe the biggest gains in efficiency can be made. Key areas that may be addressed by such designs are improvements in the selection of doses and combinations of drugs in early clinical development and in maximising the power of confirmatory trials in multidrug-resistant TB, where patient numbers and complexity pose practical limitations. We encourage trialists and regulators in this area to consider the advantages that may be offered by these designs and their potential to more effectively and rapidly identify better treatment regimens for TB patients worldwide...|$|E
50|$|Donald Arthur Berry (born 1940) is a {{statistician}} {{and one of}} {{the proponents}} of Bayesian statistics. He was the chairman of the Department of Biostatistics and Applied Mathematics at the University of Texas M. D. Anderson Cancer Center from 1999-2010, where he {{played a role in the}} use of Bayesian methods to develop innovative, <b>adaptive</b> <b>clinical</b> <b>trials.</b> He is a fellow of the American Statistical Association and the Institute of Mathematical Sciences. He founded Berry Consultants, a statistical consulting group, with Scott Berry in 2000.|$|R
40|$|Background Publicly funded trials {{regularly}} fail {{to recruit}} their target sample size {{or find a}} significant positive result. <b>Adaptive</b> <b>clinical</b> <b>trials</b> which may partly mediate against the problems are not often applied. In this paper we investigate the potential of a form of adaption in a <b>clinical</b> <b>trial</b> - a futility analysis - {{to see if it}} has potential to improve publicly funded trials. Methods Outcome data from trials funded by two UK bodies, the Health Technology Assessment (HTA) programme and the UK Medical Research Council (MRC), were collected. These data were then used to simulate each trial with a single futility analysis using conditional power, undertaken after 50...|$|R
40|$|Full list {{of author}} {{information}} {{is available at the}} end of the articlegreat majority of proposals were accepted (15, 25 %) or conditionally accepted (32, 54 %). In the more recent 41 procedures, the most frequent concerns raised by CHMP/SAWP were insufficient justifications of the adaptation strategy, type I error rate control and bias. Conclusions: For the majority of proposed <b>adaptive</b> <b>clinical</b> <b>trials,</b> an overall positive opinion was given albeit with critical comments. Type I error rate control, bias and the justification of the design are common issues raised by the CHMP/SAWP...|$|R
40|$|In an <b>adaptive</b> <b>clinical</b> <b>trial</b> research, it {{is common}} to use certain data {{dependent}} design weights to assign individuals to treatments so that more study subjects are assigned to the better treatment. These design weights must also be used for consistent estimation of the treatment effects as well as the effects of other prognostic factors. In practice, there are however situations where {{it may be necessary to}} collect binary responses repeatedly from an individual over a period of time and to obtain consistent and efficient estimates for the treatment effects as well as the effects of the other covariates. In this thesis, we introduce a binary response based longitudinal adaptive design for the allocation of individuals to a better treatment, and propose a weighted generalized quasi-likelihood (WGQL) approach for the consistent and efficient estimation of the regression parameters, including the treatment effects. We also introduce a binary longitudinal adaptive mixed model assuming that given the treatment effects and the unobservable individual random effect, repeated responses of an individual are longitudinally correlated. An extended WGQL approach is also used to obtain consistent and efficient estimators for the regression parameters and the variance component of individual random effects...|$|E
40|$|The National Cancer Institute {{clinical}} cooperative {{groups have}} been instrumental over the past 50 years in developing clinical trials and evidence based process improvements for clinical oncology patient care. The cooperative groups are undergoing a transformation process as we further integrate molecular biology into personalized patient care and move to incorporate international partners in clinical trials. To support this vision, data acquisition and data management informatics tools must become both nimble and robust to support transformational research at an enterprise level. Information, including imaging, pathology, molecular biology, radiation oncology, surgery, systemic therapy and patient outcome data needs to {{be integrated into the}} clinical trial charter using <b>adaptive</b> <b>clinical</b> <b>trial</b> mechanisms for design of the trial. This information needs to be made available to investigators using digital processes for real time data analysis. Future clinical trials will need to be designed and completed in a timely manner facilitated by nimble informatics processes for data management. This paper discusses both past experience and future vision for clinical trials as we move to develop data management and quality assurance processes {{to meet the needs of}} the modern trial...|$|E
40|$|Since the {{introduction}} of combination antiretroviral therapy (ART), {{there has been a}} dramatic improvement in the prognosis of people living with HIV. Indeed, data from numerous cohorts now show that for people commenced on ART at high CD 4 counts and who are retained in care, life expectancy is similar to matched HIV-negative people. Consequently, the number of years an individual diagnosed with HIV can expect to be treated with ART is set to increase markedly. With the emergence of an older population, polypharmacy and drug-drug interactions are now more prominent clinical problems in the management of HIV. Furthermore, as the number of years an individual spends on ART increases, treatment fatigue leading to suboptimal adherence and antiretroviral resistance are frequently observed. As such, the development of long-acting parenteral antiretroviral drugs with low propensity for drug-drug interactions, low toxicity and high genetic barriers to resistance is highly desirable. Fusion inhibitors are a class of antiretroviral drugs which display limited systemic toxicities and few drug-drug interactions. They are rarely used, however, owing to injection site reactions associated with their delivery. C 34 -PEG 4 -Chol is a novel fusion inhibitor derived from the lead molecule, C 34, but modified with the addition of polyethylene glycol (PEG) and cholesterol (Chol). With addition of cholesterol, potency of the drug is enhanced by concentrating it in cell membrane domains where viral fusion occurs. Moreover, the addition of cholesterol has been shown to markedly enhance its circulatory half-life in rodents such that it may potentially act as a long-acting antiretroviral drug in humans with infrequent subcutaneous injections. In this thesis, I assess the antiretroviral resistance pattern of C 34 -PEG 4 -Chol in vitro. An <b>adaptive</b> <b>clinical</b> <b>trial</b> design was simulated to examine options of optimising the output of useful pharmacokinetic data after administration of single doses of C 34 -PEG 4 -Chol to HIV-positive men. Finally, I undertook a ‘first-in-man’ study to examine the pharmacokinetic profile of C 34 -PEG 4 -Chol and potential of this agent as a long-acting antiretroviral drug. My work demonstrates the emergence of mutations to C 34 -PEG 4 -Chol via novel pathways within the heptad repeat (HR) - 1 domains of gp 41. Compensatory mutations were also observed within the HR- 2 domain. The phenotypic effect of a HR- 1 mutation was demonstrated following generation of mutant viral clones and assessing viral entry into cells in the presence of drug. An <b>adaptive</b> <b>clinical</b> <b>trial</b> design has shown, through simulations, to lead to more efficient allocation of trial participants across potentially therapeutic doses of the drug. Following a ‘first-in-man’ study, a promising pharmacokinetic profile was characterised which showed an extended drug half-life several fold above the 90...|$|E
5000|$|<b>Adaptive</b> <b>clinical</b> <b>trials</b> use {{existing}} data {{to design the}} trial, and then use interim results to modify the trial as it proceeds. Modifications include dosage, sample size, drug undergoing trial, patient selection criteria and [...] "cocktail" [...] mix. Adaptive trials often employ a Bayesian experimental design to assess the trial's progress. In some cases, trials have become an ongoing process that regularly adds and drops therapies and patient groups as more information is gained. The aim is to more quickly identify drugs that have a therapeutic effect and {{to zero in on}} patient populations for whom the drug is appropriate.|$|R
40|$|We present {{differentially}} private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is {{a problem}} for applications such as <b>adaptive</b> <b>clinical</b> <b>trials,</b> experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist (ϵ, δ) differentially private variants of Upper Confidence Bound algorithms which have optimal regret, O(ϵ^- 1 + T). This is a significant improvement over previous results, which only achieve poly-log regret O(ϵ^- 2 ^ 2 T), because of our use of a novel interval-based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds...|$|R
5000|$|... caBIG {{sought to}} provide {{foundational}} technology for {{an approach to}} biomedicine it called a “learning healthcare system.” This relies on the rapid exchange of information between all sectors of research and care, so that researchers and clinicians are able to collaboratively review and accurately incorporate the latest findings into their work. The ultimate goal was to speed the biomedical research process. It was also promoted for what is often called Personalized Medicine.caBIG technology was used in <b>adaptive</b> <b>clinical</b> <b>trials</b> such as the Investigation of Serial studies to Predict Your Therapeutic Response with Imaging and molecular AnaLysis 2 (I-SPY2), {{which was designed to}} use biomarkers to determine the appropriate therapy for women with advanced breast cancer.|$|R
40|$|Many {{papers have}} {{introduced}} <b>adaptive</b> <b>clinical</b> <b>trial</b> methods that allow {{modifications to the}} sample size based on interim estimates of treatment effect. There has been extensive commentary on type I error control and efficiency considerations, but little research on estimation after an adaptive hypothesis test. We evaluate the reliability and precision of different inferential procedures {{in the presence of}} an adaptive design with pre-specified rules for modifying the sampling plan. We extend group sequential orderings of the outcome space based on the stage at stopping, likelihood ratio test statistic, and sample mean to the adaptive setting in order to compute median-unbiased point estimates, exact confidence intervals, and P-values uniformly distributed under the null hypothesis. The likelihood ratio ordering is found to average shorter confidence intervals and produce higher probabilities of P-values below important thresholds than alternative approaches. The bias adjusted mean demonstrates the lowest mean squared error among candidate point estimates. A conditional error-based approach in the literature has the benefit of being the only method that accommodates unplanned adaptations. We compare the performance of this and other methods in order to quantify the cost of failing to plan ahead in settings where adaptations could realistically be pre-specified at the design stage. We find the cost to be meaningful for all designs and treatment effects considered, and to be substantial for designs frequently proposed in the literature...|$|E
40|$|Background: <b>adaptive</b> <b>clinical</b> <b>trial</b> {{design has}} been {{proposed}} as a promising new approach to improve the drug discovery process. Among the many options available, adaptive sample size re-estimation is of great interest mainly because {{of its ability to}} avoid a large ‘up-front’ commitment of resources. In this simulation study, we investigate the statistical properties of two-stage sample size re-estimation designs in terms of type I error control, study power and sample size, in comparison with the fixed-sample study. Methods: we simulated a balanced two-arm trial aimed at comparing two means of normally distributed data, using the inverse normal method to combine the results of each stage, and considering scenarios jointly defined by the following factors: the sample size re-estimation method, the information fraction, the type of group sequential boundaries and the use of futility stopping. Calculations were performed using the statistical software SAS™ (version 9. 2). Results: under the null hypothesis, any type of adaptive design considered maintained the prefixed type I error rate, but futility stopping was required to avoid the unwanted increase in sample size. When deviating from the null hypothesis, the gain in power usually achieved with the adaptive design and its performance in terms of sample size were influenced by the specific design options considered. Conclusions : we show that adaptive designs incorporating futility stopping, a sufficiently high information fraction (50 - 70 %) and the conditional power method for sample size re-estimation have good statistical properties, which include a gain in power when trial results are less favourable than anticipated.  </p...|$|E
40|$|Growing {{interest}} in personalised medicine and targeted therapies {{is leading to}} an increase in the importance of subgroup analyses. If it is planned to view treatment comparisons in both a predefined subgroup and the full population as co-primary analyses, {{it is important that the}} statistical analysis controls the familywise type I error rate. Spiessens and Debois (Cont. Clin. Trials, 2010, 31, 647 – 656) recently proposed an approach specific for this setting, which incorporates an assumption about the correlation based on the known sizes of the different groups, and showed that this is more powerful than generic multiple comparisons procedures such as the Bonferroni correction. If recruitment is slow relative to the length of time taken to observe the outcome, it may be efficient to conduct an interim analysis. In this paper, we propose a new method for an <b>adaptive</b> <b>clinical</b> <b>trial</b> with co-primary analyses in a predefined subgroup and the full population based on the conditional error function principle. The methodology is generic in that we assume test statistics can be taken to be normally distributed rather than making any specific distributional assumptions about individual patient data. In a simulation study, we demonstrate that the new method is more powerful than previously suggested analysis strategies. Furthermore, we show how the method can be extended to situations when the selection is not based on the final but on an early outcome. We use a case study in a targeted therapy in oncology to illustrate the use of the proposed methodology with non-normal outcomes. Copyright © 2012 John Wiley & Sons, Ltd...|$|E
40|$|The ethical {{tension in}} {{research}} design is often characterized as that between indi-vidual and collective ethics. While <b>adaptive</b> <b>clinical</b> <b>trials</b> (ACTs) are generally consid-ered {{to be more}} sensitive to individual ethics, the concomitant loss of statistical power associated with them is often used to justify randomized <b>clinical</b> <b>trials</b> (RCTs). This pa-per challenges this characterization of the central ethical problem in research design. It argues that the key consideration in clinical research hinges on the process of informed consent. When the research context is such that the subject is able to provide informed consent, RCTs can be justified and may be required. However, in desperate medical sit-uations the process of informed consent is often undermined. It is argued that in such situations ACTs are ethically required. We introduce “the principle of interchangeabil-ity ” and argue that it must be satisfied if research in desperate medical situations is to be justified...|$|R
40|$|New drug {{development}} is a time-consuming and expensive process. Recently, there has been stagnation {{in the development of}} novel compounds. Moreover, the attrition rate in clinical research is also on the rise. Fearing more stagnation, the Food and Drug Administration released the critical path initiative in 2004 and critical path opportunity list in 2006 thus highlighting the need of advancing innovative trial designs. One of the innovations suggested was the <b>adaptive</b> designed <b>clinical</b> <b>trials,</b> a method promoting introduction of pre-specified modifications in the design or statistical procedures of an on-going trial depending on the data generated from the concerned trial thus making a trial more flexible. The adaptive design trials are proposed to boost clinical research by cutting on the cost and time factor. Although the concept of <b>adaptive</b> designed <b>clinical</b> <b>trials</b> is round-the-corner for the last 40 years, there is still lack of uniformity and understanding on this issue. This review highlights important adaptive designed methodologies besides covering the regulatory positions on this issue...|$|R
40|$|Les rapports de {{recherche}} du LIG - ISSN : 2105 - 0422 Data-driven research, or {{the science}} of letting data {{tell us what we}} are looking for, is in many areas, the only viable approach to research. In some domains like <b>adaptive</b> <b>clinical</b> <b>trials</b> and emerging research areas such as social computing, useful results are highly dependent on the ability to observe and interactively explore large volumes of real datasets. Database management is {{the science of}} efficiently storing and retrieving data. Data mining is the science of discovering hidden correlations in data. Interactive data-driven research is a natural meeting point that presents a new research opportunity. The ability to conduct effective data-driven research requires to combine efficient indexing and querying from databases and pattern mining and classification from data mining to help analysts understand what lies behind large data volumes. In this paper, we explore key challenges and new opportunities in building robust systems for interactive data-driven research...|$|R
