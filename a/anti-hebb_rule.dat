3|1|Public
40|$|Abstract. It is {{well-known}} that Hebbian synapses, with appropriate weight normalization, extract the first principal {{component of the}} input patterns (Oja 1982). Anti-Hebb rules {{have been used in}} combination with Hebb rules to extract additional principal components or generate sparse codes (e. g., Rubner and Schulten 1990; FoldiAk 1990). Here we show that the simple anti-Hebbian synapses alone can support an important computational function: solving simultaneous linear equations. During repetitive learning with a simple <b>anti-Hebb</b> <b>rule,</b> the weights onto an output unit always converge to the exact solution of the linear equations whose coefficients correspond to the input patterns and whose constant terms correspond to the biases, provided that the solution exists. If there are more equations than unknowns and no solution exists, the weights approach the values obtained by using the Moore-Penrose generalized inverse (pseudoinverse). No explicit matrix inversion is involved and {{there is no need to}} normalize weights. Mathematically, the <b>anti-Hebb</b> <b>rule</b> may be regarded as an iterative algorithm for learning a special case of the linear associative mapping (Koho-‘These authors have made equal contributions to this work. KZ’s present address: Computationa...|$|E
40|$|One of {{the most}} {{interesting}} domains of feedforward networks is the processing of sensor signals. There do exist some networks which extract most of the information by implementing the maximum entropy principle for Gaussian sources. This is done by transforming input patterns to the base of eigenvectors of the input autocorrelation matrix with the biggest eigenvalues. The basic building block of these networks is the linear neuron, learning with the Oja learning rule. Nevertheless, some researchers in pattern recognition theory claim that for pattern recognition and classification clustering transformations are needed which reduce the intra-class entropy. This leads to stable, reliable features and is implemented for Gaussian sources by a linear transformation using the eigenvectors with the smallest eigenvalues. In another paper (Brause 1992) it is shown that the basic building block for such a transformation can be implemented by a linear neuron using an <b>Anti-Hebb</b> <b>rule</b> and restricted weights. This paper shows the analog VLSI design for such a building block, using standard modules of multiplication and addition. The most tedious problem in this VLSI-application is the design of an analog vector normalization circuitry. It can be shown that the standard approaches of weight summation will not give the convergence to the eigenvectors for a proper feature transformation. To avoid this problem, our design differs significantly from the standard approaches by computing the real Euclidean norm. Keywords: minimum entropy, principal component analysis, VLSI, neural networks, surface approximation, cluster transformation, weight normalization circuit...|$|E
40|$|A {{substantial}} number of works aimed at modeling the receptive field properties of the primary visual cortex (V 1). Their evaluation criterion is usually the similarity of the model response properties to the recorded responses from biological organisms. However, as several algorithms were able to demonstrate some degree of similarity to biological data based on the existing criteria, {{we focus on the}} robustness against loss of information in the form of occlusions as an additional constraint for better understanding the algorithmic level of early vision in the brain. We try to investigate the influence of competition mechanisms on the robustness. Therefore, we compared four methods employing different competition mechanisms, namely, independent component analysis, non-negative matrix factorization with sparseness constraint, predictive coding/biased competition, and a Hebbian neural network with lateral inhibitory connections. Each of those methods is known to be capable of developing receptive fields comparable to those of V 1 simple-cells. Since measuring the robustness of methods having simple-cell like receptive fields against occlusion is difficult, we measure the robustness using the classification accuracy on the MNIST hand written digit dataset. For this we trained all methods on the training set of the MNIST hand written digits dataset and tested them on a MNIST test set with different levels of occlusions. We observe that methods which employ competitive mechanisms have higher robustness against loss of information. Also the kind of the competition mechanisms {{plays an important role in}} robustness. Global feedback inhibition as employed in predictive coding/biased competition has an advantage compared to local lateral inhibition learned by an <b>anti-Hebb</b> <b>rule...</b>|$|E
40|$|In a {{previous}} paper, a model was presented showing how {{the group of}} Ca 2 +/calmodulin-dependent protein kinase II molecules contained within a postsynaptic density could stably store a graded synaptic weight. This paper completes the model by showing how bidirectional control of synaptic weight could be achieved. It is proposed that the quantitative level of the activity-dependent rise in postsynaptic Ca 2 + determines whether the synaptic weight will increase or decrease. It is further proposed that reduction of synaptic weight is governed by protein phosphatase 1, an enzyme indirectly controlled by Ca 2 + through reactions involving phosphatase inhibitor 1, cAMP-dependent protein kinase, calcineurin, and adenylate cyclase. Modeling of this biochemical system shows that it can function as an analog computer that can store a synaptic weight and modify it {{in accord with the}} Hebb and <b>anti-Hebb</b> learning <b>rules...</b>|$|R

