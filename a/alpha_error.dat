113|51|Public
50|$|Charles Dunnett (1955, 1966) {{described}} an alternative <b>alpha</b> <b>error</b> adjustment when k groups are {{compared to the}} same control group. Now known as Dunnett's test, this method is less conservative than the Bonferroni adjustment.|$|E
50|$|The figure below {{illustrates}} {{the relationship between}} the blank, the limit of detection (LOD), and the limit of quantification (LOQ) by showing the probability density function for normally distributed measurements at the blank, at the LOD defined as 3 * standard deviation of the blank, and at the LOQ defined as 10 * standard deviation of the blank. For a signal at the LOD, the <b>alpha</b> <b>error</b> (probability of false positive) is small (1%). However, the beta error (probability of a false negative) is 50% for a sample that has a concentration at the LOD (red line). This means a sample could contain an impurity at the LOD, but there is a 50% chance that a measurement would give a result less than the LOD. At the LOQ (blue line), there is minimal chance of a false negative.|$|E
30|$|Qualitative {{variables}} were expressed as percentages and quantitative variables as {{mean and standard}} deviation. They have been used t-student and Chi 2 for the univariate analysis as required and binary logistic regression for multivariate analysis. Has employed a maximum of 5 % <b>alpha</b> <b>error.</b>|$|E
40|$|This paper {{proposes to}} revisit both the CAPM and the three-factor model of Fama and French (1993) in {{presence}} of {{errors in the}} variables. To reduce the bias induced by measurement and specification errors, we transpose {{to the cost of}} equity an estimator based on cumulants of order three and four initially developed by Dagenais and Dagenais (1997) and lated generalized to financial models by Racicot (2003). Our results show that our technique has great and significant consequences on the measure of the cost of equity. We obtain ipso facto a new estimator of the Jensen <b>alpha.</b> <b>Errors</b> in the variables, cumulants, higher moments, instrumental variables, cost of equity, Jensen alpha. ...|$|R
40|$|We {{revisit the}} problem of calculating the exact {{distribution}} of optimal investments in a mean variance world under multivariate normality. The context we consider is where problems in optimisation are addressed {{through the use of}} Monte-Carlo simulation. Our findings give clear insight as to when Monte-Carlo simulation will, and will not work. Whilst a number of authors have considered aspects of this exact problem before, we extend the problem by considering {{the problem of}} an investor who wishes to maximise quadratic utility defined in terms of <b>alpha</b> and tracking <b>errors.</b> The results derived allow some exact and numerical analysis. Furthermore, they allow us to also derive results for the more traditional nonbenchmarked portfolio problem. <b>alpha,</b> tracking <b>error,</b> mean-variance, Monte-Carlo...|$|R
40|$|Count Plus {{with the}} N 95 -Companion) were {{evaluated}} {{for their ability}} to identify wearers of respirators that do not provide adequate protection during a simulated workplace test. Thirty models of NIOSH-certified N 95 half-facepiece respirators (15 filtering-facepiece models and 15 elastomeric models) were tested by a panel of 25 subjects using each of the three fit testing methods. Fit testing results were compared to 5 th percentiles of simulated workplace protection factors. <b>Alpha</b> <b>errors</b> (the chance of failing a fit test in error) for all 30 respirators were 71 % for the Bitrex method, 68 % for the saccharin method, and 40 % for the Companion method. Beta errors (the chance of passing a fit test in error) for all 30 respirator models combined were 8 % for the Bitrex method, 8 % for the saccharin method, and 9 % for the Companion method. The three fit test methods had different error rate...|$|R
30|$|Statistical {{significance}} {{of the differences between}} the groups was determined using Student’s t test. The statistical power was calculated using the Biomath online software ([URL] We analyzed 10 samples for each group (<b>alpha</b> <b>error</b> 0.05), which resulted in a statistical power of 92 %.|$|E
30|$|Descriptive {{statistical}} {{analysis on the}} sample was performed using {{means and standard deviations}} (SD). The total FFI score was correlated with the subscales of the SF- 36 and FAOS questionnaires, by means of Pearson’s linear correlation coefficient. The <b>alpha</b> <b>error</b> was set at p <  0.05.|$|E
30|$|With an {{expected}} extubation failure rate of 28 % {{in the control}} group and an absolute expected improvement with HFNC of 7 % (25 % relative reduction) [25], the planned sample was 592 patients in each arm, for an <b>alpha</b> <b>error</b> of 5 % and a power of 80 %.|$|E
40|$|In this research, {{we present}} the {{inferential}} statistics for Cronbach’s Coefficient Alpha {{based on the}} standard statistical assumption of multivariate normality. The estimation of <b>alpha’s</b> standard <b>error</b> (ASE) and confidence interval for alpha is described, and we conduct an analytical demonstration to illustrate the effects on these estimates {{of the components of}} the equations, including the number of items, the item intercorrelations, and sample size. We then empirically examine the consistency and efficiency of alpha and its standard error in a Monte Carlo simulation modeling the effects of alpha’s components, sample size, and compound symmetry. We then demonstrate the superiority of this estimate compared to previous derivations of <b>alpha’s</b> standard <b>error</b> in a separate Monte Carlo simulation. For the researcher interested in assessing the difference between alphas obtained in two independent samples, we present a sampling error and test statistic for such a comparison. We conclude with a prescription that includes the recommendation that all alpha coefficients be reported in conjunction with a standard error or confidence interval estimate. We offer SAS and SPSS programming codes for easy implementation...|$|R
2500|$|Foundations of the {{statistical}} hypothesis testing theory and {{the statistical}} decision theory. In the seminal [...] "On the criterion..." [...] paper, Pearson proposed testing the validity of hypothesized values by evaluating the chi distance between the hypothesized and the empirically observed values via the p-value, which was proposed in the same paper. The use of preset evidence criteria, so called <b>alpha</b> type-I <b>error</b> probabilities, was later proposed by Jerzy Neyman and Egon Pearson.|$|R
40|$|Keywords: Covert {{channels}} in IP networks are investigated. The possibilities adversary needs to construct covert channels are given. Current methods of covert channels elimination, detection and capacity limitation are examined. Detection methods are compared using such criteria: <b>alpha</b> and beta <b>errors,</b> an ability of implementation...|$|R
30|$|We {{estimated}} that {{a sample size}} of 149 patients (AS, n =  73 and IV, n =  76) would provide a power of 80  % to demonstrate the non-inferiority of AS regimen compared with parenteral colistin with a lower toxicity in the intervention arm at a two-sided <b>alpha</b> <b>error</b> of 5  %. All statistical analyses were based on intention-to-treat principle.|$|E
30|$|The {{sample size}} was {{calculated}} with G-Power 3.1. 3 (Faul, University of Kiel, Kiel, Germany). Assuming an effect size of 0.7 from previous reports, a total sample size of 40 animals was calculated to determine {{a significant difference between}} SEP amplitudes with an <b>alpha</b> <b>error</b> of 0.05 and a power of 0.95 between the four groups.|$|E
30|$|Mean values, {{standard}} deviations, medians, 95 % {{confidence intervals}} (CI), and frequency distributions {{were calculated for}} all outcomes assessed. The data rows were examined with the Shapiro-Wilk test for normal distribution. Between-group comparisons were accomplished using the unpaired t test. Linear regression analyses were used to depict the relationship between BI 26 and CWb as well as SA 26 values in both groups. The <b>alpha</b> <b>error</b> was set at 0.05.|$|E
3000|$|... {{which was}} {{examined}} in this retrospective study. The groups were comparable regarding their skeletal pattern, age, sex, and treatment time. The review of the confidence interval show, that {{a sufficient number of}} patients were evaluated. The significant differences are thereby supported by <b>alpha</b> and beta <b>errors.</b>|$|R
40|$|The {{current study}} investigates the {{reliability}} {{estimates of the}} newly adopted writing placement test at ELI. To examine whether three raters’ judgment on learner level were consistent enough to justify the placement, statistical reliability measures such as Cronbach’s <b>alpha,</b> Standard <b>Error</b> of Measurement, and Correlation coefficients were adopted. The analysis of the consistencies among raters revealed that although generally the placement results were consistent among raters, several limitations were found. For example, frequent rater inconsistencies were found and oral report seemed to reveal high variability in their first decision and their final report. Further suggestions to improve inter-rater reliability are presented...|$|R
50|$|In desperation, Flashback {{contacted}} Sasquatch and his Alpha Flight {{team and}} convinced {{them to help}} him. The Alpha Flight member Nemesis {{came to the conclusion}} that she would have to kill him with her enchanted blade in order to stop him from dying in the future. After much trial and <b>error</b> <b>Alpha</b> Flight managed to keep Flashback from being sent to the past.|$|R
30|$|Changes in {{perceived}} and physiological stress over the general periods (before, during, {{and after the}} task; see Fig.  1) were analyzed via repeated-measures analysis of variance (ANOVA), with time of measurement as the repeated within-subjects factor (aggregated measures for the physiological measures: before, during, and after the task). Greenhouse-Geisser-adjusted p values {{for the analysis of}} multiple measures and Bonferroni-corrected p values for the multiple comparisons to control for <b>alpha</b> <b>error</b> inflation are reported.|$|E
30|$|The {{significance}} of the association between AU and HLA-class I and class II allotypes was calculated by Fisher's exact test, corrected for the <b>alpha</b> <b>error,</b> using a database on a control population of 212 HLA-I- and HLA-II-typed healthy unrelated blood donors. Both the p values and the odds ratio (OR) with the 95 % confidence interval were calculated using the Statcalc program. Significance was defined as p[*]<[*] 0.05, with a relative risk > 1.|$|E
30|$|Power {{analysis}} {{is the most}} important discussion for statistical test results [6]. The results of the paired t test between N’ and N showed that the mean and the SD were 0.174 and 7.326, respectively, from 100 samples. Based on these difference parameters, when the probability of <b>alpha</b> <b>error</b> was 0.05, the effect size (d) and the power of (1 [*]−[*]ß) error probability were derived as 0.0238 and 0.056, respectively, by using G*Power software (Ver. 3.1. 9.2) [7, 8].|$|E
40|$|Covert timing {{channels}} became widespread {{with the}} increasing popularity of packet switching networks. Detection of these channels is the only approach to counter that {{does not lead to}} a decrease of channel’s capacity. Known methods of the IP covert timing channels detection were systematized. Detection methods based on the analysis of patterns in the distribution of inter-packet delays according to criteria such as <b>alpha</b> and beta <b>errors,</b> an ability of implementation were compared...|$|R
5000|$|Alpha Pictoris (α Pic, α Pictoris) is the {{brightest}} star in the southern constellation of Pictor. It has an apparent visual magnitude of 3.27, which is bright enough to be viewed from urban areas in the southern hemisphere. This star is close enough for its distance to be measured using parallax shifts, which yields a value of roughly [...] from the Sun, with a 5% margin of <b>error.</b> <b>Alpha</b> Pictoris has the distinction of being the south pole star of the planet Mercury.|$|R
40|$|Alpha is {{a measure}} of {{risk-adjusted}} performance. Positive alpha means an investment manager has generated returns in excess of what should have been earned given the level of risk taken. However, what if two funds have the same alpha [...] are they equally good? Well, tracking error helps to distinguish which fund might be better, as a lower tracking error might mean less risk and a more significant alpha. (See article here, Wall Street Journal.) The Information Ratio divides <b>alpha</b> by tracking <b>error</b> to provide a more comparable performance metric...|$|R
30|$|The needed {{sample size}} was {{assessed}} based on the minimum requirement of a 5 % <b>alpha</b> <b>error</b> and 80 % statistical power [11]. The difference to detect (effect size) was set equal to the variance of the main outcome variable (i.e., number of MMA events per sleep hour). Based on those calculations, a study sample of 19 healthy volunteers (14 females; mean age 28.3 [*]±[*] 2.4, range 25 – 35  years) was recruited among {{the staff of the}} Post-Graduate School in Orthodontics, University of Ferrara, Ferrara, Italy.|$|E
30|$|Sample size {{calculation}} Epinfo version 6 {{guided by}} the following data: power of the test 80 %, confidence level 95 %, <b>alpha</b> <b>error</b> 5 %, risk ratio 2, odds ratio 3.5, and total sample 100 on 2 equal groups. Statistical analysis was performed using a standard SPSS software package version 17 (Chicago, IL). Data were expressed as mean values[*]±[*]SD, numbers (%). Student’s t {{test was used to}} analyze the parametric data, and discrete (categorical) variables were analyzed using the χ 2 test, with p values <[*] 0.05 considered statistically significant.|$|E
30|$|A paired Student’s t-test {{was used}} to assess {{significant}} differences in area burned between the published map perimeters and the postprocessing perimeters from the imagery, with a confidence level of 95 % (p ≤ 0.05 <b>alpha</b> <b>error).</b> To test whether the error in mapping was a function of TR, we calculated a Pearson correlation coefficient to correlate percent agreement, percent omission, and percent commission in mapping to each of the four values of TR. We also correlated the three error percentages to area burned to determine whether mapping accuracy is associated with fire size.|$|E
40|$|Recent {{developments}} in statistics and psychology supported {{an increase in}} quality of measurement in the educational area. The present study empirically examined the characteristics of item and person statistics of mathematics category of School Olympiad Examination (SOE) within the Classical Test Theory (CTT) measurement frameworks. When constructing tests, es-pecially in examining reliability of a test, CTT approach gives opportunity to obtain required targets easily. In this study, various statistics are used to judge {{the quality of the}} items. Mainly, item discrimination, item difficulty, item-total inter correlations are found to clarify real aspect of data. Before determining internal consistency, unidimensionality of mathematics examination is analyzed. In this process, Principal Component Analysis is implemented and using item component correlation, behaviors of items are detected from different point of view. Internal consistency is examined by Cronbach’s <b>Alpha.</b> Standard <b>error</b> of meas-urement is calculated to identify confidence interval around an observed score. As a conclusion, reliability of the mathematics part of the SOE is proved by CTT assumptions...|$|R
40|$|Active {{portfolio}} management often involves {{the objective of}} selecting a portfolio with minimum tracking error variance (TEV) for some expected gain in return over a benchmark. However, Roll (1992) shows that such portfolios are generally suboptimal {{because they do not}} belong to the mean-variance frontier and are thus overly risky. Our paper proposes an appealing method to lessen this suboptimality that involves the objective of selecting a portfolio from the set of portfolios that have minimum TEV for various levels of ex-ante alpha, which we refer to as the alpha-TEV frontier. Since practitioners commonly use ex-post alpha to assess the performance of managers, the use of this frontier aligns the objectives of managers with how their performance is evaluated. Furthermore, sensible choices of ex-ante alpha lead to the selection of portfolios that are less risky (in variance terms) than the portfolios that active managers would otherwise select. Active {{portfolio management}} Benchmarking <b>Alpha</b> Tracking <b>error</b> Risk management...|$|R
40|$|Abstract. We {{present an}} {{algorithm}} for extracting high quality temporally coherent alpha mattes of objects from a video. Our approach extends the conventional image matting approach, i. e. closed-form matting, to video by using multi-frame nonlocal matting Laplacian. Our multi-frame nonlocal matting Laplacian is defined over a nonlocal neighborhood in spatial temporal domain, and it solves the alpha mattes of several video frames all together simultaneously. To speed up computation {{and to reduce}} memory requirement for solving the multi-frame nonlocal matting Laplacian, we use the approximate nearest neighbor(ANN) to find the nonlocal neighborhood and the k-d tree implementation to divide the nonlocal matting Laplacian into several smaller linear systems. Finally, we adopt the nonlocal mean regularization to enhance temporal coherence of the estimated alpha mattes and to correct <b>alpha</b> matte <b>errors</b> at low contrast regions. We demonstrate the effectiveness of our approach on various examples with qualitative comparisons to the results from previous matting algorithms. ...|$|R
40|$|In {{most areas}} in life, it is {{difficult}} to work with populations and hence researchers work with samples. The calculation of the sample size needed depends on the data type and distribution. Elements include consideration of the <b>alpha</b> <b>error,</b> beta error, clinically meaningful difference, and the variability or standard deviation. The final number arrived at should be increased to include a safety margin and the dropout rate. Over and above this, sample size calculations must take into account all available data, funding, support facilities, and ethics of subjecting patients to research...|$|E
30|$|Statistical {{analysis}} was performed using WinSTAT plug-in for Excel 2007 (Microsoft office 2007). As equivalence is assumed, median age and body mass index (BMI) were compared between the breathing and breath-hold group using the parameter-free U test with <b>alpha</b> <b>error</b> set to 0.3. The Wilcoxon signed rank sum test with continuity correction was applied to compare the median 3 D vector lengths of both groups. Fisher’s exact test—ignoring the potential cluster error in the data—was applied to compare the frequency of volumes in both groups which did not require manual registration. Corrected P values less than 0.05 were assumed to indicate significant differences.|$|E
30|$|No power {{calculation}} {{was conducted}} for this experiment {{since we had}} no previous data on bacterial growth in our models. The preceding experiments [5, 6, 10] utilized a power calculus based on a systemic TNF-alpha difference of 15 % at 6  h, an <b>alpha</b> <b>error</b> of 0, 05, a power of 0, 8, and an SD of 10 %, which has yielded six evaluable animals per group. Based on the preceding calculation and {{with the aim of}} reducing the number of animals and still allowing for a slightly larger variability in the bacterial outcomes we chose 8 animals per group in the current experiment.|$|E
40|$|Abstract: Although {{mutual fund}} {{performance}} has been dissected from almost every angle, very {{little attention has}} been paid to the connection between the actual active decisions made by management and the subsequent performance outcomes. In this paper we use information on institutional mutual funds to examine the implications of active positions and style tilts taken by management for the fund’s realised <b>alpha,</b> tracking <b>error</b> and information ratio. We identify some areas where the funds across the entire sample have success (active positions, and growth and winning stock tilts) and others where they fall short (value and loser stock tilts). We identify that there is significant variation in these findings when we extend our analysis to examine the impact of these active decisions on performance for different styles of funds during periods of weak and strong markets. Finally, we repeat the analysis by incorporating the initial choice of investment style with the active decisions in order to judge their dual impact on investment performance...|$|R
40|$|In this research, we {{investigate}} {{the behavior of}} Cronbach’s coefficient alpha and its new standard error. We systematically analyze the effects of sample size, scale length, strength of item intercorrelations, and scale dimensionality. We demonstrate the beneficial effects of sample size on <b>alpha’s</b> standard <b>error</b> and of scale length and the strengths of item intercorrelations (effects that are substitutes in their benefits) on both alpha and its standard error. Our findings also speak to this adage: Heterogeneity within the item covariance matrix (e. g., through multidimensionality or poor items) negatively impacts reliability by decreasing the precision of the estimation. We also examined the question of “equilibrium” scale length, showing the conditions {{for which it is}} optimal to add no items, or one, or multiple items to a scale. In terms of “best practices,” we recommend that researchers report a confidence interval or standard error along with the coefficient alpha point estimate. measurement, survey research, reliability, coefficient alpha...|$|R
40|$|Reports of {{negative}} trials arc increasing in number as standard therapy for many gastrointestinal diseases is refined. The validity {{of a negative}} report depends {{on the number of}} patients in the trial, the <b>alpha</b> and bern <b>error</b> and the difference in efficacy which the trial is able to detect. The relationship between these parameters is discussed and a formula given for the calculation of trial size. All reports {{of negative}} trials should include not only the number of patients involved and the level of significance of the results but also the beta error and the detectable difference in efficacy of the treatments...|$|R
