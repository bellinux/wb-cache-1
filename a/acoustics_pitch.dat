0|88|Public
40|$|Are there {{consistent}} {{markers of}} atypical prosody in speakers with high functioning autism (HFA) compared to typically-developing speakers? We examined: (1) <b>acoustic</b> measurements of <b>pitch</b> range, mean pitch and speech rate in conversation, (2) perceptual ratings of conversation for these features and overall prosody, and (3) acoustic measurements of speech from a structured task. Increased pitch range {{was found in}} speakers with HFA during both conversation and structured communication. In global ratings listeners rated speakers with HFA as having atypical prosody. Although the HFA group demonstrated increased <b>acoustic</b> <b>pitch</b> range, listeners did not rate speakers with HFA as having increased pitch variation. We suggest {{that the quality of}} pitch variation used by speakers with HFA was non-conventional and thus not registered as such by listeners...|$|R
2500|$|Music is {{composed}} of aural phenomena; [...] "music theory" [...] considers how those phenomena apply in music. Music theory considers melody, rhythm, counterpoint, harmony, form, tonal systems, scales, tuning, intervals, consonance, dissonance, durational proportions, the <b>acoustics</b> of <b>pitch</b> systems, composition, performance, orchestration, ornamentation, improvisation, electronic sound production, etc.|$|R
40|$|International audienceIn {{the context}} of {{content-based}} multimedia indexing gender identification based on speech signal is an important task. In this paper a set of <b>acoustic</b> and <b>pitch</b> features along with different classifiers are compared for the problem of gender identification. We show that the fusion of features and classifiers performs better than any individual classifier. Based on such conclusions we built a system for gender identification in multimedia applications. The system uses a set of Neural Networks with <b>acoustic</b> and <b>Pitch</b> related features. 90 % of classification accuracy is obtained for 1 second segments and with independence to the language and the channel of the speech. Practical considerations, such as the continuity of speech {{and the use of}} mixture of experts instead of one single expert are shown to improve the classification accuracy to 93 %. When used on a subset of the Switchboard database, the classification accuracy attains 98. 5 % for 5 seconds segments...|$|R
40|$|In {{the context}} of {{content-based}} multimedia indexing gender identification based on speech signal is an important task. In this paper a set of <b>acoustic</b> and <b>pitch</b> features along with different classifiers are compared for the problem of gender identification. We show that the fusion of features and classifiers performs better than any individual classifier. Based on such conclusions we built a system for gender identification in multimedia applications. The system uses a set of Neural Networks with <b>acoustic</b> and <b>Pitch</b> related features. 90 % of classification accuracy is obtained for 1 second segments and with independence to the language and the channel of the speech. Practical considerations, such as the continuity of speech {{and the use of}} mixture of experts instead of one single expert are shown to improve the classification accuracy to 93 %. When used on a subset of the Switchboard database, the classification accuracy attains 98. 5 % for 5 seconds segments. Keywords: Content-based audio indexing, Piecewise Gaussian Modeling, Mixture of Neural Networks...|$|R
50|$|The Auditorium of The Tabernacle has {{excellent}} <b>acoustics</b> and <b>Pitch</b> Pine pews to seat 350 people. Chamber and choral music, drama, {{lectures and}} conferences regularly take place here. A Steinway grand piano has been purchased; translation booths, recording facilities and a cinema screen have been installed; the oak-beamed Foyer has a bar; and extensive access {{for the disabled}} has been {{made possible by a}} lift.|$|R
40|$|Our study {{investigated}} the pitch and duration properties of word stress in a large speech corpus. We concluded that pitch and duration play different roles in distinguishing word-level stress classes. In the case of pitch, primary-stress vowels were different from secondary-stress and reduced vowels; {{in the case of}} duration, reduced vowels were different from the other two types of vowels. Index Terms: stress, <b>acoustic</b> correlates, <b>pitch,</b> duration 1...|$|R
40|$|We {{present a}} {{straightforward}} and robust algorithm for periodicity detection, {{working in the}} lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much {{greater than that of}} any of the usual frequency-domain methods. By definition, the best candidate for the <b>acoustic</b> <b>pitch</b> period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for pitch detection, and to the exclusive use of frequency-domain methods for the determination of the harmonics-to-noise ratio...|$|R
50|$|In 2016, Distributel {{acquired}} Yak Communications, re-branded its Home Phone product set as Yak Digital Home Phone, and retained many service advantages {{developed by}} Yak in its integrated offering. Clear Voice technology is one service advantage Distributel integrated, which serves to reduce transmitted noise, <b>acoustic</b> echo and <b>pitch</b> imbalances.|$|R
40|$|In Mandarin, {{there is}} a special <b>acoustic</b> feature—tonal <b>pitch</b> range, which is {{relative}} to stress. In this paper, we present a novel concept—tonal range ratio (TRR), which is based on tonal pitch range, and make a study on the correlation between TRR and stress in Mandarin. And we developed a system to automatically detect stresses in words and sentences based on TRR in Mandarin. We obtained high success rate (92. 67 % in words and 82. 0 % in sentences). The results show that TRR has strong correlation with stress and is powerful in detecting stresses in Mandarin. 1...|$|R
50|$|The {{instrument}} {{consists of}} a wooden box that {{is attached to a}} taut string. The string/chord is thrummed and the box creates a heavy <b>acoustic</b> sound. The <b>pitch</b> of the sound is changed by altering the tension of the string, which changes the pitch of the sound. The instrument is used to keep tempo of the Parang Band.|$|R
40|$|Marking {{stress is}} {{important}} in conveying meaning and drawing listener’s attention to specific parts of a message. Extensive {{research has shown that}} healthy speakers mark stress using three main <b>acoustic</b> cues; <b>pitch,</b> intensity, and duration. The relationship between acoustic and perception cues is vital {{in the development of a}} computer-based tool that aids the therapists in providing effective treatment to people with Dysarthria. It is, therefore, important to investigate the acoustic cues deficiency in dysarthric speech and the potential compensatory techniques needed for effective treatment. In this study, we investigate the relationship between acoustic and perceptive cues in dysarthric speech. This is achieved by modifying stress marked sentences from 10 speakers with Ataxic dysarthria. Each speaker produced 30 sentences using the 10 Subject-Verb-Object-Adjective (SVOA) structured sentences across three stress conditions. These stress conditions are stress on the initial (S), medial (O) and final (A) target words respectively. To effectively measure the deficiencies in Dysarthria speech, the <b>acoustic</b> features (<b>pitch,</b> intensity, and duration) are modified incrementally. This paper presents the techniques involved in the modification of these acoustic features. The effects of these modifications are analysed based on steps of 25 % increments in pitch, intensity and duration. For robustness and validation, 50 untrained listeners participated in the listening experiment. In this paper, we present the results and the relationship between acoustic modifications (what is measured) and perception (what is heard) in Dysarthric speech...|$|R
40|$|This article {{describes}} a {{neural network model}} capable of generating a spatial representation of the <b>pitch</b> of an <b>acoustic</b> source. <b>Pitch</b> {{is one of several}} auditory percepts used by humans to separate multiple sound sources in the environment from each other. The model provides a neural instantiation of a type of "harmonic sieve". It is capable of quantitatively simulating a large body of psychoacoustical data, including new data on octave shift perception. Air Force Office of Scientific Research (90 - 0128, 90 - 0175); Defense Advanced Research Projects Agency (90 - 0083); National Science Foundation (IRI 90 - 24877); American Society for Engineering Educatio...|$|R
5000|$|A pitch {{generator}} {{is a type}} {{of signal}} generator optimized for use in audio and <b>acoustics</b> applications. <b>Pitch</b> generators typically include sine waves over the audio frequency range (20 Hz - 20 kHz). Sophisticated pitch generators will also include sweep generators (a function which varies the output frequency over a range, in order to make frequency-domain measurements), multipitch generators (which output several pitches simultaneously, and are used to check for intermodulation distortion and other non-linear effects), and tone bursts (used to measure response to transients). Pitch generators are typically used in conjunction with sound level meters, when measuring the acoustics of a room or a sound reproduction system, and/or with oscilloscopes or specialized audio analyzers.|$|R
50|$|Further choral-orchestral works came in {{the shape}} of Alleluia for chorus and orchestra, {{composed}} for the reopening of the Royal Festival Hall ("The London Philharmonic Choir, with nowhere to hide in such a revealing <b>acoustic,</b> maintained <b>pitch</b> admirably and delivered a virtuoso cadenza of animated susurration"), and the shorter Harmony, commissioned as the opening work for the 2013 season of the BBC Proms. In between these came Fantasias, a 25-minute orchestral work premiered by the Cleveland Orchestra in November 2009 displaying a new interest in multi-movement structures, and The Discovery of Heaven, commissioned and premiered as part of Anderson's composer residency with the London Philharmonic Orchestra; the latter two works feature on a recent portrait disc of the composer by the same orchestra.|$|R
40|$|In this paper, a {{bottom-up}} integration {{structure to}} model tone influence {{at various levels}} is proposed. At <b>acoustic</b> level, <b>pitch</b> is extracted as a continuous acoustic variable. At phonetic level, we treat the main vowel with different tones as different phonemes. In triphone building phase, we evaluated a set of questions about tone for each decision tree node. At word level, a set of tone change rules was used to build transcription for training data and word lattice for decoding. At sentence level, some sentence ending words with light tone are added to system vocabulary. Integration at these five levels experimentally drops the word error rate from 9. 9 to 7. 8 on a Chinese continuous speech dictation task. 1...|$|R
50|$|Acoustic {{qualification}} of tinnitus {{will include}} measurement of several <b>acoustic</b> parameters like <b>pitch,</b> or frequency {{in cases of}} monotone tinnitus, or frequency range and bandwidth in cases of narrow band noise tinnitus, loudness in dB above hearing threshold at the indicated frequency, mixing-point, and minimum masking level. In most cases, tinnitus pitch or frequency range is between 5000 Hz and 8000 Hz, and loudness less than 10 dB above the hearing threshold.|$|R
40|$|This study {{investigated}} whether {{individual differences in}} cognitive functions, attentional abilities in particular, were associated with individual differences {{in the quality of}} phonological representations, resulting in variability in speech perception and production. To do so, we took advantage of a tone merging phenomenon in Cantonese, and identified three groups of typically-developed speakers who could differentiate the two rising tones (high and low rising) in both perception and production [+Per+Pro], only in perception [+Per-Pro], or in neither modalities [-Per-Pro]. Perception and production were reflected respectively by discrimination sensitivity d prime and <b>acoustic</b> measures of <b>pitch</b> offset and rise time differences. Components of event-related potential (ERP) - the mismatch negativity (MMN) and the ERPs to amplitude rise time were taken to reflect the representations of the acoustic cues of tones. Components of attention and working memory in the auditory and visual modalities were assessed with published test batteries. The results show that individual differences in both perception and production are linked to how listeners encode and represent the <b>acoustic</b> cues (<b>pitch</b> contour and rise time) as reflected by ERPs. The present study has advanced our knowledge from previous work by integrating measures of perception, production, attention, and those reflecting quality of representation, to offer a comprehensive account for the underlying cognitive factors of individual differences in speech processing. Particularly, it is proposed that domain-general attentional switching affects the quality of perceptual representations of the acoustic cues, giving rise to individual differences in perception and production...|$|R
40|$|In this paper, {{a method}} of pitch contour {{modelling}} based on the hidden Markov model (HMM) states of an acoustic unit is presented. A pair of vectors is computed from the alignment of the speech data with the acoustic unit’s HMM states. The pitch contour feature of the acoustic unit {{is represented by the}} vector pair so that the variants of the <b>acoustic</b> unit’s <b>pitch</b> contour can be measured and compared. Using this model, pitch contour decision trees are constructed for phones in Mandarin from a single speaker’s continuous reading speech database. The trees are used in the Mandarin speech synthesis system, which is trained over the same database, to predict the pitch contour of a certain phone according to its phone context. The naturalness of the synthesized Mandarin speech is highly improved. 1...|$|R
30|$|Ma et al. [87] {{proposed}} a combined approach for detection re-scoring from linear interpolation of a rule-based detection re-scoring system, a logistic regression-based detection re-scoring system, and a rank learning-based detection re-scoring system. The detection re-scoring {{system based on}} word-burst features (e.g., number, strength, and proximity of neighbor hypothesis, etc.), consensus network features (e.g., posterior probability, number of hit arcs, number of average arcs per bin, etc.), and <b>acoustic</b> features (e.g., <b>pitch,</b> number of unvoiced frames, jitter, etc.).|$|R
40|$|This paper {{presents}} a mathematical description of style in speech and singing. These styles are {{represented as a}} set of portable prosodic features along with a set of rules to choose where the features are to be applied. Speakers and singers make creative choices to express their personal style, which may involve specific accent shapes, phrase curves, or (similarly) musical embellishment. Therefore a quantitative model of style needs to support unconstrained accent and phrase curve description, and to solve potential conflicts that arise from this freedom. Our current implementation modifies two <b>acoustic</b> parameters: <b>pitch</b> and loudness. We use an articulator-based model, Stem-ML, to resolve conflicts between intended accents or embellishments and their environment. We present several examples to illustrate the modeling of accents and phrase curves, as well as the usefulness of style/content separation, and the similarity between speech and music. ...|$|R
40|$|Intonation can be {{perceived}} in whispered speech despite {{the absence of}} the fundamental frequency. In the past, <b>acoustic</b> correlates of <b>pitch</b> in whisper have been sought in vowel content, but, recently, studies of normal speech demonstrated correlates of intonation in consonants as well. This study examined how consonants may contribute to the coding of intonation in whispered relative to normal speech. The acoustic characteristics of whispered, voiceless fricatives /s/ and /f/, produced at different pitch targets (low, mid, high), were investigated and compared to corresponding normal speech productions to assess if whisper contained secondary or compensatory pitch correlates. Furthermore, listener sensitivity to fricative cues to pitch in whisper was established, also relative to normal speech. Consistent with recent studies, acoustic correlates of whispered and normal speech fricatives systematically varied with pitch target. Comparable findings across speech modes showed that acoustic correlates were secondary. Discrimination of vowel-fricative-vowel stimuli was less accurate and slower in whispered than normal speech, which is attributed to differences in acoustic cues available. Perception of fricatives presented without their vowel contexts, however, revealed comparable processing speeds and response accuracies between speech modes, supporting the finding that within fricatives, <b>acoustic</b> correlates of <b>pitch</b> are similar across speech modes...|$|R
40|$|Lingual organ pipes produce sound {{by means}} of the {{interaction}} of a vibrating tongue and an <b>acoustic</b> resonator. The <b>pitch</b> of the pipe is determined by the frequency of tongue vibration, whereas the timbre is mostly inuenced by the resonator. The two components form a coupled vibroacoustic - fluid dynamic system. In this contribution a modeling approach for the simulation of the sound generation of lingual organ pipes is presented. The proposed techique incorporates a tongue and a resonator model, which are coupled {{by means of}} the ow equations...|$|R
50|$|There is {{evidence}} that the acquisition of language-specific prosodic qualities start even before an infant is born. This is seen in neonate crying patterns, which have qualities that are similar to the prosody of the language that they are acquiring. The only way that an infant could be born with this ability is if the prosodic patterns of the target language are learned in utero. Further evidence of young infants using prosodic cues is their ability to discriminate the <b>acoustic</b> property of <b>pitch</b> change by 1-2 months old.|$|R
40|$|This paper {{focuses on}} {{experimental}} evaluations {{designed to determine}} the relative quality {{of the components of}} the Whistler TTS engine. Eight different systems were compared pairwise to determine a rank ordering as well as a measure of the quality difference between the systems. The most interesting aspect of the results is that the simple unit duration scheme used in Whistler was found to be very good, both when it was used in combination with natural <b>acoustics</b> and <b>pitch</b> as well as when it was taken in combination with synthetic pitch. The synthetic pitch was found to be the aspect of the system that results in greatest quality degradation. 1. INTRODUCTION We have presented Whistler, Microsoft's Trainable Text-ToSpeech (TTS) system in [1][2]. We will primarily look at three aspects of the system, the pitch (fundamental frequency), phoneme duration, and acoustics. Whistler has a concatenative synthesizer, using context-dependent phoneme units that are automatically selected from a tr [...] ...|$|R
40|$|We {{developed}} a pain analyzer (ABC analyzer) to perform automatic acoustic analysis of neonatal crying {{and to provide}} an objective estimate of neonatal pain. The ABC analyzer uses a validated pain scale (ABC scale) based on three <b>acoustic</b> parameters: <b>pitch</b> frequency, normalized RMS amplitude, and presence of a characteristic frequency- and amplitude-modulated crying feature, defined as "siren cry". Here we assessed {{the reliability of the}} analyzer. We enrolled 57 healthy neonates. Each baby was recorded with a video camera during heel prick. Pain intensity was evaluated using a validated scale [Douleur Aigue du Nouveau-Né (DAN) scale] and the analyzer and the two scores were compared. We found a statistically significant concordance between the DAN score and ABC analyzer score (p < 0. 0001). The ABC analyser is a novel approach to cry analysis that should now have its properties carefully evaluated in a series of studies, just as is necessary in the development of any other pain measurement tool...|$|R
40|$|Speakers employ <b>acoustic</b> cues (<b>pitch</b> accents) to {{indicate}} that a word is important, but may also use visual cues (such as manual beat gestures, head nods, and eyebrow movements) for this purpose. Even though these acoustic and visual cues are related, {{the exact nature of}} this relationship is far from well understood. We investigate whether producing a visual beat leads to changes in how acoustic prominence is realized in speech. For this, we use an original experimental paradigm in which speakers are instructed to realize a target sentence with different distributions of acoustic and visual cues for prominence. Acoustic analyses reveal that the production of a visual beat indeed has an effect on the acoustic realization of the co-occuring speech, in particular on duration and the higher formants (F 2 and F 3), independent of the kind of visual beat and of the presence and position of pitch accents. Index Terms: audiovisual speech production, gestures, manual beats, facial expressions...|$|R
40|$|Abstract — {{an emotion}} is a mental and {{physiological}} state {{associated with a}} wide variety of feelings, thoughts, and behavior. Emotions are subjective experiences, or experienced from an individual point of view. Emotion is often associated with mood, temperament, personality, and disposition. Hence, in this paper method for detection of human emotions is discussed based on the <b>acoustic</b> features like <b>pitch,</b> energy etc. The proposed system is using the traditional MFCC approach [2] and then using nearest neighbor algorithm for the classification. Emotions has been classified separately for male and female based on the fact male and female voice has altogether different range [1][4] so MFCC varies considerably for the two...|$|R
40|$|Although many Karaoke systems {{come with}} a scoring feature that {{provides}} singers with a simple performance rating, the automated scoring, however, is either random or incomparable to that of human evaluation. This study exploits various <b>acoustic</b> features, including <b>pitch,</b> volume, and rhythm to assess a singing performance. We invited a number of singers having different levels of singing capabilities to record for Karaoke solo vocal samples. The performances were rated independently by four musicians, and then {{used in conjunction with}} additional Karaoke VCD music for the training of our proposed system. Our experiment shows that the results of automated singing evaluation are close to the human rating. Index Terms—Accompaniment, Karaoke, Singing Evaluation 1...|$|R
40|$|To {{determine}} the neural mechanisms involved in vocal emotion processing, {{the current study}} employed {{functional magnetic resonance imaging}} (fMRI) to investigate the neural structures engaged in processing acoustic cues to infer emotional meaning. Two critical <b>acoustic</b> cues – <b>pitch</b> and speech rate – were systematically manipulated and presented in a discrimination task. Results confirmed that a bilateral network constituting frontal and temporal regions is engaged when discriminating vocal emotion expressions; however, we observed greater sensitivity to pitch cues in the right mid superior temporal gyrus/sulcus (STG/STS), whereas activation in both left and right mid STG/STS was observed for speech rate processing. Index terms: prosody, fMRI, pitch processing, speech rate processing, emotion comprehensio...|$|R
40|$|Presented at the 10 th International Conference on Auditory Display (ICAD 2004) Auditory icons – or {{environmental}} sounds – {{have the potential}} to convey information by non-verbal means quickly and accurately. In addition, human listeners are quick to determine many qualities of an auditory object, such as location, distance, size, and motion, from acoustics of the signal. An experiment tests these two coupled assumptions in a controlled laboratory context. Stimuli consisted of auditory icons ``loaded'' with information achieved through systematic manipulation of the <b>acoustic</b> parameters <b>pitch,</b> volume ramping, and reverberation. Sixty adult listeners were asked to recognize and describe four auditory icons wherein object size, distance and direction of motion were captured in the parameters of each 1 -second sound. Participants were accurate at recognizing and interpreting the icons 70 - 80 % of the time. Recognition rate was consistently high when participants responded to one, two or three parameters. However, recognition was significantly poorer when in response to all four parameters. There was a significant effect of icon type and parameter manipulation: dog bark was the most easily recognized icon, and the direction parameter interpreted most accurately. Implications of the findings for applied contexts are discussed...|$|R
40|$|One {{reason for}} the poor pitch {{performance}} in current cochlear-implant users may be the highly synchronized neural firing in electric hearing that lacks stochastic properties of neural firing in normal acoustic hearing. This study used three different electric stimulation patterns, jittered, probabilistic, and auditory-model-generated pulses, to mimic {{some aspects of the}} normal neural firing pattern in <b>acoustic</b> hearing. <b>Pitch</b> discrimination was measured at standard frequencies of 100, 250, 500, and 1000 Hz on three Nucleus- 24 cochlear-implant users. To test the utility of the autocorrelation pitch perception model in electric hearing, one, two, and four electrodes were stimulated independently with the same patterned electric stimulation. Results showed no improvement in performance with any experimental pattern compared to the fixed-rate control. Pitch discrimination was actually worsened with the jittered pattern at low frequencies (125 and 250 Hz) than that of the control, suggesting that externally introduced stochastic properties do not improve pitch perception in electric stimulation. The multiple-electrode stimulation did not improve performance but did not degrade performance either. The present results suggest that both "the right time and the right place" may be needed to restore normal pitch perception in cochlear-implant users. (c) 2005 Acoustical Society of America...|$|R
40|$|Auditory icons – or {{environmental}} sounds – {{have the potential}} to convey information by non-verbal means quickly and accurately. In addition, human listeners are quick to determine many qualities of an auditory object, such as location, distance, size, and motion, from acoustics of the signal. An experiment tests these two coupled assumptions in a controlled laboratory context. Stimuli consisted of auditory icons “loaded ” with information achieved through systematic manipulation of the <b>acoustic</b> parameters <b>pitch,</b> volume ramping, and reverberation. Sixty adult listeners were asked to recognize and describe four auditory icons wherein object size, distance and direction of motion were captured in the parameters of each 1 -second sound. Participants were accurate at recognizing and interpreting the icons 70 - 80 % of the time. Recognition rate was consistently high when participants responded to one, two or three parameters. However, recognition was significantly poorer when in response to all four parameters. There was a significant effect of icon type and parameter manipulation: dog bark was the most easily recognized icon, and the direction parameter interpreted most accurately. Implications of the findings for applied contexts are discussed. 1...|$|R
40|$|I {{would like}} to express my {{greatest}} gratitude {{to the people who}} have helped and supported me. First, I {{would like to}} thank Professor Wendi Heinzelman for her continuous guidance and invaluable advice on my thesis. Many thanks to my parents for their undivided support and encouragement. I {{would also like to thank}} Na Yang and He Ba from the Wireless Communication and Networking Group for providing advice on my thesis. This research was part of the Bridge project, which was supported by funding from National Institute of Health NICHD (Grant R 01 HD 060789). iv Acoustic feature extraction algorithms play a central role in many speech and music processing applications. However, noise usually prevents acoustic feature extraction algorithms from obtaining the correct information from speech and music signals. Thus, the robustness of acoustic feature extraction algorithms is an area worth studying. In this thesis, we consider two important <b>acoustic</b> features: <b>pitch</b> and speaking rate. For each acoustic feature, we introduce several classic and state-of-the-art feature extraction algorithms and evaluate the performance of each of them in noisy environments. We analyze the results and provide possible explanations why some feature extraction algorithms outperform the others in noisy environments. ...|$|R
40|$|The <b>acoustic</b> {{correlates}} of <b>pitch</b> variation {{were examined in}} 40 participants who received analogy instructions or explicit instructions that required them to modulate their intonation during speech production. First, using focus group methodology, professional speech-language pathologists {{were asked to identify}} analogies that best described minimum pitch variation (monotone), moderate pitch variation (normal intonation), and maximum pitch variation (exaggerated intonation) in speech. The focus group established that an appropriate pitch variation metaphor may be related to imagery of "waves at sea", with minimum pitch variation represented by a flat calm sea, moderate pitch variation represented by a moderate sea, and maximum pitch variation represented by a choppy sea. Forty adult participants without speech impairments were asked to read aloud a standard paragraph using their habitual pitch variation (control condition). They were then allocated randomly to an analogy or an explicit instruction group and were asked to read aloud different paragraphs with minimum, moderate, or maximum pitch variations. Results revealed that <b>acoustic</b> {{correlates of}} <b>pitch</b> variation (standard deviation of fundamental frequency, SDF 0) were not different for the control condition, or moderate and maximum pitch variation conditions in the two groups. However, the analogy instruction was significantly more effective than the explicit instruction for inducing minimum pitch variation. Analysis of participants in each group who showed higher than normal pitch variation in the control condition (>. 5 SD above the group SDF 0) revealed that the analogy instruction was more effective than the explicit instruction in the minimum variation condition. It was concluded that analogy instructions may be a useful tool in speech rehabilitation. © 2012 The Speech Pathology Association of Australia Limited. link_to_subscribed_fulltex...|$|R
40|$|A magnetoencephalographic {{marker for}} pitch {{analysis}} (the pitch onset response) {{has been reported}} for different types of pitch-evoking stimuli, irrespective of whether the <b>acoustic</b> cues for <b>pitch</b> are monaurally or binaurally produced. It is claimed that the pitch onset response reflects a common cortical representation for pitch, putatively in lateral Heschl's gyrus. The result of this functional MRI study sheds doubt on this assertion. We report a direct comparison between iterated ripple noise and Huggins pitch in which we reveal a different pattern of auditory cortical activation associated with each pitch stimulus, even when individual variability in structure-function relations is accounted for. Our results suggest it may be premature to assume that lateral Heschl's gyrus is a universal pitch center...|$|R
40|$|International audienceBirdsong usually {{serves to}} attract mates and to repel {{territorial}} rivals {{and is often}} produced by males only. During territorial conflicts song may provide information on the signallers physical strength, and on its motivation to fight. Males may vary aspects of their singing behaviour when engaged in territorial interactions, and such variation may be an honest signal of certain traits of the signaller such as body size, condition or motivation. This information may be used by receivers in territorial decisions. We studied contextual variation in the song of skylarks, Alauda arvensis, a songbird with a large vocal repertoire and a continuous and versatile singing style. We challenged subjects with simulated territorial intrusions by broadcasting conspecific song and recorded their vocal responses. When challenged, males increased their duty cycle. Duty cycle was calculated by subtracting all silent pauses within a song (inter-syllable pauses) and dividing the remaining on-song-time by the total song duration. We found no contextual variation in other <b>acoustic</b> parameters (<b>pitch,</b> syllable and inter-syllable duration, song rate and song versatility). Duty cycle might be an honest signal for the competitive ability and might be perceived as such by skylarks - this hypothesis will be tested in the future...|$|R
