8|13|Public
50|$|Instance {{features}} are numerical representations of instances. For example, {{we can count}} the number of variables, clauses, <b>average</b> <b>clause</b> length for Boolean formulas, or number of samples, features, class balance for ML data sets to get an impression about their characteristics.|$|E
5000|$|Typically, in discourse, {{sentences}} are short, {{and verbal}} arguments (actants) are often left to context (leading to <b>average</b> <b>clause</b> length {{of less than}} 2 signs). In addition, NSL (like other sign languages), tends to be topicalizing; that is, the topic is fronted (moved {{to the front of}} the clause). Given these two facts, it is hard to a [...] "Basic Word Order" [...] for NSL; nevertheless, in those instances where both agent and patient are lexicalized and where there is no topicalization (e.g. in artificially elicited sentences in isolation), the word order tends to be SOV (Subject-Object-Verb) --- just like Nepali and most members of the South Asian Sprachbund.|$|E
40|$|A {{general rule}} of thumb is to tackle the hardest part of a search problem first. Many {{heuristics}} thereforetrytobranchonthemostconstrained variable. Totesttheir effectiveness at this, we measure theconstrainednessofaproblemduringsearch. Werunexperimentsinseveral different domains, using both random and non-random problems. In each case, weobserveaconstrainedness "knife-edge" in which critically constrained problemstendtoremaincriticallyconstrained. Weshowthatthisknife-edgeis predicted by a theoretical lower-bound calculation. Wealsoobserveaverysimplescalingwithproblemsize forvariouspropertiesmeasuredduring search including the ratio of clause sto variables, and the <b>average</b> <b>clause</b> size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability...|$|E
50|$|To {{remove the}} risk of homeowners unwittingly falling into an {{underinsurance}} trap, New Zealand has legislation to ban the use of <b>average</b> <b>clauses</b> for dwelling houses.|$|R
50|$|Where the {{contracts}} differ substantially, {{there may be}} complications. One contract may, for example, insure the common object amongst other objects, without apportioning to it a specific amount. <b>Average</b> <b>clauses</b> may also be present.|$|R
50|$|<b>Average</b> <b>clauses</b> {{can cause}} {{problems}} with claims made {{during periods of}} volatility in commodities markets. For example, if crude was being shipped {{from one part of}} the world to another, and a partial loss occurred, if its current value had risen, the amount paid out by the insurance company may not cover the value of the contract.|$|R
40|$|A {{general rule}} of thumb istotackle {{the hardest part of}} a search problem first. Many {{heuristics}} therefore try to branch on the most constrained variable. To test their effectiveness at this, we measure the constrainedness of a problem during search. We run experiments in several different domains, using both random and non-random problems. In each case, we observe a constrainedness "knife-edge" in which critically constrained problems tend to remain critically constrained. We show that this knife-edge is predicted by a theoretical lower-bound calculation. We also observe a very simple scaling with problem size for various properties measured during search including the ratio of clauses to variables, and the <b>average</b> <b>clause</b> size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability...|$|E
40|$|There {{are many}} {{problems}} related to holding commercial property as an Investment. One of the greatest risks {{is the possibility of}} total or partial loss by fire or other misfortune. The value of property destroyed each year by some form of damage is immense. The proper insurance of property is therefore of critical importance. This has been tragically illustrated by a number of recent local disasters concerning the loss of property. Insurance policies must therefore be comprehensive and ensure that any loss will be received in some form or other. There are two bases on which property may be insured, namely, Indemnity and Reinstatement. An Indemnity policy ensures that the property owner does not lose from a financial viewpoint following any damage to his property. A reinstatement policy makes provision for the repair or replacement of any damage so that the property is reconstructed in exactly the same manner after the damage has occurred as it existed beforehand. The onus for fixing a suitable value for insurance purposes falls directly on the Insured. The Insurer will calculate the insurance premium on this sum. However, when a claim is made, the insurer will calculate the 'full insurable value' of the property and this is taken as a ceiling value for insurance cover. Also, if the 'full insurable value is higher than the 'Insured value', the Pro Rata <b>Average</b> <b>Clause</b> is generally operated and only a proportion of the claim will be met. This paper explains the bases of both forms of insurance and the methods of assessing 'full insurable value'. It also explains the working of the Pro Rata <b>Average</b> <b>Clause</b> and the special provisions which ought to be incorporated in an insurance policy for property assets. Finally, the paper concludes by emphasising the responsibilities which face the professional property manager/valuer in advising the property owner in the subject area of property insurance, which is growing increasingly complicated and expensive...|$|E
40|$|A {{general rule}} of thumb is to tackle the hardest part of a search problem first. Many {{heuristics}} therefore try to branch on the most constrained variable. To test their effectiveness at this, we measure the constrainedness of a problem during search. We run experiments in several different domains, using both random and non-random problems. In each case, we observe a constrainedness "knife-edge" in which critically constrained problems tend to remain critically constrained. We show that this knife-edge is predicted by a theoretical lower-bound calculation. We also observe a very simple scaling with problem size for various properties measured during search including the ratio of clauses to variables, and the <b>average</b> <b>clause</b> size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability. Introduction Empirical studies of search procedures usually focus on statistics like the run-time or the total number of nodes visited. It can also be [...] ...|$|E
50|$|The {{history of}} <b>average</b> <b>clauses</b> began with cargo insurance. Here, if a {{proportion}} of a cargo had to be thrown overboard in storm to save the ship, {{the owners of the}} remaining cargo would jointly make good the loss to the owner of the cargo thrown overboard. The share each of the owners would pay would be based on their proportion of the total value of cargo. This is commonly termed the law of general average.|$|R
30|$|Complexity was {{measured}} by vocabulary complexity and grammatical complexity. Vocabulary complexity {{was based on a}} sophisticated type-token ratio (Larsen-Freeman 2006), which is calculated by Juku Grading system automatically. Grammatical complexity {{was measured}} in terms of the <b>average</b> number of <b>clauses</b> per t-unit. These measures have been regarded as reliable and effective in evaluating second language development in writing (Larsen-Freeman 2006).|$|R
30|$|It {{can also}} be seen in Table  2 that essays are longer than question-answers since their <b>average</b> number of <b>clauses</b> per text is 128. However, in essays, only 11.3 % of all {{processes}} are verbal. In this case, the standard deviation is ± 5, and by adding and deducting this number to the average, we get one text above the representation threshold (2) and one below (4).|$|R
40|$|Abstract. In {{earlier work}} on a limited form of {{extended}} resolution for CDCL based SAT solving, new literals were introduced to factor out parts of learned clauses. The main goal was to shorten clauses, reduce proof size and memory usage and thus speed up propagation and conflict analysis. Even though some reduction was achieved, {{the effectiveness of this}} technique was rather modest for generic SAT solving. In this paper we show that factoring out literals is particularly useful for incremental SAT solving, based on assumptions. This is the most common approach for incremental SAT solving and was pioneered by the authors of MINISAT. Our first contribution is to focus on factoring out only assumptions, and actually all eagerly. This enables the use of compact dedicated data structures, and naturally suggests a new form of clause minimization, our second contribution. As last main contribution, we propose to use these data structures to maintain a partial proof trace for learned clauses with assumptions, which gives us a cheap way to flush useless learned clauses. In order {{to evaluate the effectiveness of}} our techniques we implemented them within the version of MINISAT used in the publically available state-of-the-art MUS extractor MUSer. An extensive experimental evaluation shows that factoring out assumptions in combination with our novel clause minimization procedure and eager clause removal is particularly effective in reducing <b>average</b> <b>clause</b> size, improves running time and in general the state-of-the-art in MUS extraction. ...|$|E
40|$|This PhD thesis investigates lexicosyntactic {{features}} in writings in Norwegian L 1 by 60 16 - year-old pupils written using two different writing tools: hand-writing and typing. Its methodological approach is corpus-based and statistical, and its theoretical foundations are mainly those of theories of complexity and register variation. The {{main focus of}} the thesis is on the analysis of 5 lexical and 8 syntactic variables. The lexical variables are average word-length, average word-length in lexical words, lexical density, global TTR and local TTR, both adjusted or neutralised for text-length. The syntactic variables are t-unit length, clause length, frequency of short subclauses, number of prepositional phrases per clause, number of adverbial subclauses per clause, number of subclauses per t-unit, ratio of t-units with a short frontal constituent and frequency of attributive adjectives. All these are analysed using anova on four pupil features, which are gender, general writing skills, total text length of the two texts and ratio of text lengths in the two texts. Ten of the variables are included in an overall principal component analysis. The main findings come in four general categories: Some variables seem unaffected by the writing tool. This applies to <b>average</b> <b>clause</b> length and frequency of attributive adjectives. Some variables display {{a shift in the}} direction of more spontaneous, "oral" {{features in}} the typed texts. This applies to subclause frequency and perhaps average t-unit length. Some variables have different properties in boys&# 39; and girls&# 39; writings; they display more spontaneous features in the typed texts written by girls and more planned or edited features in the typed texts written by boys. This applies to average word-length, average word-length in lexical words, global TTR and ratio of short frontal constituents. Some variables have different properties in the writings of pupils who write considerably longer texts when typing compared to the writings of pupils who write texts of approximately equal lengths with both writing tools. The former pupils have more planned or edited features in their typed texts, whereas the latter pupils have more spontaneous features in their typed texts. This applies to local TTR, global TTR and ratio of adverbial clauses. The principal component analysis confirms the two effects splitting the sample of pupils in terms of gender and production length. In addition, some variables display types of patterns which do not fit neatly into any of the four categories above, namely average t-unit length (longer in typed texts), ratio of short subclauses (complex interactions of several pupil-related factors), lexical density (interactions of two pupil-related factors), frequency of prepositional phrases (higher in typed texts by productive writers, lower in typed texts by terse writers), and ratio of short frontal constituents (interactions between difference in text length and gender). An important goal of the project was the further development of methods for this kind of study based on statistical stylistics. As part of the study, several lexical variables are constructed, compared and evaluated with respect to reliability and validity, among these entropy-based lexical distribution measures and frequency-based measures termed logarithmic frequency index. Also, emphasis has been put on investigating what relevant and valid textual or linguistic features are possible to extract from a corpus using a combination of automatic procedures and a limited extent of manual procedures, namely the manual segmentation of the texts into t-units and clauses, and correction of orthography and punctuation. </p...|$|E
40|$|Commonwealth {{legislation}} covering {{insurance contracts}} contains numerous provisions designed {{to control the}} operation and effect of terms in life and general insurance contracts. For example, the Life Insurance Act 1995 (Cth) contains provisions regulating the consequences attendant upon incorrect statements in proposals [1] and non-payment of premiums, [2] provides that an insurer may only exclude liability {{in the case of}} suicide if it has made express provision for such contingency in its policy, [3] and severely restricts the efficacy of conditions as to war risks. [4] The Insurance Contracts Act 1984 (Cth) is even more intrusive and has a major impact upon contractual provisions in the general insurance field. It {{is beyond the scope of}} this note to explore all of these provisions in any detail but examples of controls and constraints imposed upon the operation and effect of contractual provisions include the following. A party is precluded from relying upon a provision in a contract of insurance if such reliance would amount to a failure to act with the utmost good faith. [5] Similarly, a policy provision which requires differences or disputes arising out of the insurance to be submitted to arbitration is void, [6] unless the insurance is a genuine cover for excess of loss over and above another specified insurance. [7] Similarly clause such as conciliation <b>clauses,</b> [8] <b>average</b> <b>clauses,</b> [9] and unusual terms [10] are given qualified operation. [11] However the provision in the Insurance Contracts Act that has the greatest impact upon, and application to, a wide range of insurance clauses and claims is s 54. This section has already generated a significant volume of case law and is the focus of this note. In particular this note examines two recent cases. The first, Johnson v Triple C Furniture and Electrical Pty Ltd [2012] 2 Qd R 337, (hereafter the Triple C case), is a decision of the Queensland Court of Appeal; and the second, Matthew Maxwell v Highway Hauliers Pty Ltd [2013] WASCA 115, (hereafter the Highway Hauliers case), is a decision of the Court of Appeal in Western Australia. This latter decision is on appeal to the High Court of Australia. The note considers too the decision of the New South Wales Court of Appeal in Prepaid Services Pty Ltd v Atradius Credit Insurance NV [2013] NSWCA 252 (hereafter the Prepaid Services case). These cases serve to highlight the complex nature of s 54 and its application, as well as the difficulty in achieving a balance between an insurer and an insured's reasonable expectations...|$|R
40|$|In this paper, {{the authors}} present an {{empirical}} {{research on the}} content of cohabitation contracts in the Netherlands, conducted in 2013. The legal professionals who mostly deal with cohabitation contracts - the notaries - have been asked to fill in a digital questionnaire. The format of this research is exploratory, painting a first picture of legal practice on making cohabitation contracts. The content of the average cohabitation contract differs very much compared to the content of the <b>average</b> marriage contract. <b>Clauses</b> that express solidarity between cohabitants (sharing income or property values or maintenance) are rare in cohabitation contracts, whereas they are rather popular in matrimonial property contracts. Further research is necessary to gain more insight into the legal practice of making cohabitation contracts...|$|R
40|$|In {{this paper}} {{we show that}} the {{performance}} of the quantum adiabatic algorithm is determined by phase transitions in underlying problem in the presence of transverse magnetic field Γ. We show that the quantum version of random Satisfiability problem with 3 bits in a clause (3 -SAT) has a first-order quantum phase transition. We analyze the phase diagram γ=γ(Γ) where γ is an <b>average</b> number of <b>clauses</b> per binary variable in 3 -SAT. The results are obtained in a closed form assuming replica symmetry and neglecting time correlations at small values of the transverse field Γ. In the limit of Γ= 0 the value of γ(0) ≈ 5. 18 corresponds to that given by the replica symmetric treatment of a classical random 3 -SAT problem. We demonstrate the qualitative similarity between classical and quantum versions of this problem. Comment: 30 pages, 7 figure...|$|R
30|$|Table  2 {{shows that}} question-answers are {{relatively}} short texts since the <b>average</b> number of <b>clauses</b> per text is 28.4. In this genre, 25.3 % of all processes in finite clauses are verbal. The table {{also includes the}} standard deviation, which is {{the measure of the}} dispersion of the sample scores around the average score. In this case, the standard deviation is ± 13, which means that if we add or deduct this number to the total average (25.3), we get the maximum or minimum number of verbal processes that a text can have in order to be represented by the total average. In question-answers, one text is above that threshold (5) and three are below (8, 14 and 7). This means text 5 uses an atypical high number of verbal processes, and texts 8, 14 and 7 use an atypical low number of them.|$|R
40|$|This study {{investigated}} the link between working memory capacity and narrative task performance. The participants {{of the study were}} 44 secondary school students in their second academic year of an English-Hungarian bilingual educational program in Hungary. The backward digit span test was used to measure participants’ working memory capacity. The students performed two narrative tasks of different degrees of cognitive complexity: one with a given story line and another where the content of the narrative had to be invented. Four global aspects performance were measured: fluency, lexical complexity, accuracy, and grammatical complexity. Task specific measures included the ratio of correctly used relative clauses, verbs, and past-tense verbs, as well as the ratio of relative clauses compared to the total number of clauses. The findings suggest that the linguistic variables that differentiate students with different working memory spans are the <b>average</b> length of <b>clauses</b> and the subordination ratio. We hypothesized that high working memory capacity might allow students to produce narratives with high clausal complexity, but it might not be conducive to directing learners’ attention to specific dimensions of the task such as subordinatio...|$|R
40|$|In {{this work}} we propose and analyze a simple {{randomized}} algorithm {{to find a}} satisfiable assignment for a Boolean formula in conjunctive normal form (CNF) having at most 3 literals in every clause. Given a k-CNF formula phi on n variables, and alpha in{ 0, 1 }^n that satisfies phi, a clause of phi is critical if exactly one literal of that clause is satisfied under assignment alpha. Paturi et. al. (Chicago Journal of Theoretical Computer Science 1999) proposed a simple randomized algorithm (PPZ) for k-SAT for which success probability increases {{with the number of}} critical clauses (with respect to a fixed satisfiable solution of the input formula). Here, we first describe another simple randomized algorithm DEL which performs better if the number of critical clauses are less (with respect to a fixed satisfiable solution of the input formula). Subsequently, we combine these two simple algorithms such that the success probability of the combined algorithm is maximum of the success probabilities of PPZ and DEL on every input instance. We show that when the <b>average</b> number of <b>clauses</b> per variable that appear as unique true literal in one or more critical clauses in phi is between 1 and 1. 9317, combined algorithm performs better than the PPZ algorithm...|$|R
40|$|Given a MAX- 2 -SAT instance, {{we define}} a local maximum {{to be an}} {{assignment}} such that changing any single variable reduces the number of satisfied clauses. We consider {{the question of the}} number of local maxima that an instance of MAX- 2 -SAT can have. We give upper bounds in both the sparse and nonsparse case, where the sparse case means that there is a bound $d$ on the <b>average</b> number of <b>clauses</b> involving any given variable. The bounds in the nonsparse case are tight up to polylogarithmic factors, while in the sparse case the bounds are tight up to a multiplicative factor in $d$ for large $d$. Additionally, we generalize to the question of assignments which are maxima up to changing $k> 1 $ variables simultaneously; in this case, we give explicit constructions with large (in a sense explained below) numbers of such maxima in the sparse case. The basic idea of the upper bound proof is to consider a random assignment to some subset of the variables and determine the probability that some fraction of the remaining variables can be fixed without considering interactions between them. The bounded results hold in the case of weighted MAX- 2 -SAT as well. Using this technique and combining with ideas from Ref. 6, we find an algorithm for weighted MAX- 2 -SAT which is faster for large $d$ than previous algorithms which use polynomial space; this algorithm does require an additional bounds on maximum weights and degree. Comment: 17 pages, no figure...|$|R

