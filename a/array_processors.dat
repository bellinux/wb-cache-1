144|1135|Public
50|$|The Warp {{machines}} were {{a series of}} increasingly general-purpose systolic <b>array</b> <b>processors,</b> created by Carnegie Mellon University (CMU), in conjunction with industrial partners G.E., Honeywell and Intel, and funded by the U.S. Defense Advanced Research Projects Agency (DARPA).|$|E
5000|$|Floating Point Systems Inc. (FPS) was a Beaverton, Oregon vendor of {{attached}} <b>array</b> <b>processors</b> and minisupercomputers. The {{company was}} founded in 1970 by former Tektronix engineer Norm Winningstad, with partners Tom Prince, Frank Bouton and Robert Carter. Carter was a salesman for Data General Corp. who persuaded Bouton and Prince to leave Tektronix to start the new company. Winningstad was the fourth partner.|$|E
50|$|These {{processors}} {{were widely}} used as attached processors for scientific applications in reflection seismology, physical chemistry, NSA cryptology and other disciplines requiring {{large numbers of}} computations. Attached <b>array</b> <b>processors</b> were usually used in facilities where larger supercomputers were either not needed or not affordable. Hundreds if not thousands of FPS boxes were delivered and highly regarded. FPS's primary competition up to this time was IBM (3838) and CSPI.|$|E
40|$|The {{purpose of}} this paper is to {{describe}} a method of searching and sorting data by using some of the unique capabilities of an associative <b>array</b> <b>processor.</b> To understand the application, the associative <b>array</b> <b>processor</b> is described in detail. In particular, the content addressable memory and flip network are discussed because these two unique elements give the associative <b>array</b> <b>processor</b> the power to rapidly sort and search. A simple alphanumeric sorting example is explained in hardware and software terms. The hardware used to explain the application is the STARAN (Goodyear Aerospace Corporation) associative <b>array</b> <b>processor.</b> The software used is the APPLE (<b>Array</b> <b>Processor</b> Programming Language) programming language. Some applications of the <b>array</b> <b>processor</b> are discussed. This summary tries to differentiate between the techniques of the sequential machine and the associative <b>array</b> <b>processor...</b>|$|R
40|$|The {{application}} of an <b>array</b> <b>processor</b> to laser velocimeter data processing is presented. The hardware is described {{along with the}} method of parallel programming required by the <b>array</b> <b>processor.</b> A portion of the data processing program is described in detail. The increase in computational speed of a microcomputer equipped with an <b>array</b> <b>processor</b> is illustrated by comparative testing with a minicomputer...|$|R
50|$|SIMD engines: vector <b>processor,</b> <b>array</b> <b>processor,</b> {{digital signal}} processor, stream processor.|$|R
50|$|Other {{examples}} followed. Control Data Corporation {{tried to}} re-enter the high-end market again with its ETA-10 machine, but it sold poorly {{and they took}} that {{as an opportunity to}} leave the supercomputing field entirely. In the early and mid-1980s Japanese companies (Fujitsu, Hitachi and Nippon Electric Corporation (NEC) introduced register-based vector machines similar to the Cray-1, typically being slightly faster and much smaller. Oregon-based Floating Point Systems (FPS) built add-on <b>array</b> <b>processors</b> for minicomputers, later building their own minisupercomputers.|$|E
5000|$|The term [...] "software radio" [...] {{was coined}} in 1984 {{by a team}} at the Garland, Texas Division of E-Systems Inc. (now Raytheon) {{to refer to a}} digital {{baseband}} receiver and published in their E-Team company newsletter. A 'Software Radio Proof-of-Concept' laboratory was developed there that popularized Software Radio within various government agencies. This 1984 Software Radio was a digital baseband receiver that provided programmable interference cancellation and demodulation for broadband signals, typically with thousands of adaptive filter taps, using multiple <b>array</b> <b>processors</b> accessing shared memory.|$|E
5000|$|As an {{outgrowth}} of his work in neurophysiology, while still working as a post-doctoral fellow and {{an assistant professor of}} neurology, Sgro founded Alacron, Inc. (formerly Corteks, Inc.until 1990) in 1985 to manufacture technologies relevant to his neurological research. In 1989 he commercialized this technology and began developing <b>array</b> <b>processors,</b> frame grabbers, vision processors, and most recently supported advances in BSI sensor technology. Extending his work in machine vision technology, in 2002, Sgro founded FastVision, LLC, a maker of smart cameras, as a subsidiary of Alacron, Inc [...]|$|E
30|$|Theocharides et al. [12] have {{proposed}} a parallel architecture {{taking advantage of a}} grid <b>array</b> <b>processor.</b> This <b>array</b> <b>processor</b> is used as memory to store the computation data and as data transfer unit, to aid in accessing the integral image in parallel. This implementation can achieve 52 frames per second at a 500 [*]MHz frequency. However, details about the image resolution were not mentioned.|$|R
40|$|A formal {{approach}} for {{the transformation of}} computation intensive digital signal processing algorithms into suitable <b>array</b> <b>processor</b> architectures is presented. It covers the complete design flow from algorithmic specifications in a high-level programming language to architecture descriptions in a hardware description language. The transformation itself is divided into manageable design steps and implemented in the CAD-tool DECOMP which allows the exploration of different architectures in a short time. With the presented approach data independent algorithms can be mapped onto <b>array</b> <b>processor</b> architectures. To allow this, a known mapping methodology for <b>array</b> <b>processor</b> design is extended to handle inhomogeneous dependence graphs with nonregular data dependences. The implementation of the formal approach in the DECOMP {{is an important step}} towards design automation for massively parallel systems...|$|R
40|$|Since the {{successful}} hunch of the Japan Earth Resources Satellite 1 (JERSI) in February of 1892, it has hen sending back {{images of the}} earth for various studies, including the investigation of earth resources, the preservation of environments, and the nhservation of coastal lines. Because images sent from the. JERS- 1 have very high resolution, their analyses require {{the power of a}} highspeed image processor. For this purpse, (. he ERSDIS, a celluEnr <b>array</b> <b>processor</b> based system, llas been adopted. The cellular <b>array</b> <b>processor</b> con~ists nT 4096 processing ~lements in a twwdirnensional array ant 1 ii 's suital~le to process largescde image data at R high-speed. This paper describes the structure of the ERSDIS, and also the details of the cellular <b>array</b> <b>processor</b> unit used...|$|R
50|$|Exploitation of {{the concept}} of Data Parallelism started in 1960s with the {{development}} of Solomon machine. Solomon machine, also called a vector processor wanted to expedite the math performance by working on a large data array(operating on multiple data in consecutive time steps). Concurrency of data was also exploited by operating on multiple data points at the same time using a single instruction. These generation of processors were called <b>Array</b> <b>Processors.</b> Today, data parallelism is best exemplified in graphics processing units(GPUs) which use both the techniques of operating on multiple data points in space and time using a single instruction.|$|E
50|$|Pipelining {{began in}} earnest in the late 1970s in {{supercomputers}} such as vector processors and <b>array</b> <b>processors.</b> One of the early supercomputers was the Cyber series built by Control Data Corporation. Its main architect, Seymour Cray, later headed Cray Research. Cray developed the XMP line of supercomputers, using pipelining for both multiply and add/subtract functions. Later, Star Technologies added parallelism (several pipelined functions working in parallel), developed by Roger Chen. In 1984, Star Technologies added the pipelined divide circuit developed by James Bradley. By the mid 1980s, supercomputing was used by many different companies around the world.|$|E
5000|$|Most modern CPUs are microprocessors, {{meaning they}} are {{contained}} {{on a single}} integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called [...] "cores"; in that context, one can speak of such single chips as [...] "sockets". <b>Array</b> <b>processors</b> or vector processors have multiple processors that operate in parallel, with no unit considered central. There also exists the concept of [...] virtual CPUs which are an abstraction of dynamical aggregated computational resources.|$|E
5000|$|... the world's first commercially {{available}} massively parallel computer, the Distributed <b>Array</b> <b>Processor</b> (DAP), that first ran as an attached processor to the ICL 2980.|$|R
40|$|The {{processing}} of raw {{data obtained by}} the solar vector magnetograph at NASA-Marshall requires extensive arithmetic operations on large arrays of real numbers. The objectives of this summer faculty fellowship study are to: (1) learn the programming language of the MicroMSP <b>Array</b> <b>Processor</b> and adapt some existing data reduction routines to exploit its capabilities; and (2) identify other applications and/or existing programs which lend themselves to <b>array</b> <b>processor</b> utilization which can be developed by undergraduate student programmers under the provisions of project JOVE...|$|R
40|$|Atmospheric {{scientists}} need {{to observe}} {{fluctuations in the}} ionosphere, both to probe the underlying atmospheric physics, and to remove {{the effects of these}} fluctuations from other measurements. We have built an FPGA-based, pipelined <b>array</b> <b>processor</b> that allows us to make these observations in real-time, using passive radar techniques. Our <b>array</b> <b>processor</b> time-multiplexes 16 multiply [...] accumulators across 1536 radar ranges, performing a pipelined correlation and integration of the radar signal for each range. A DSP-based postprocessor generates realtime range-Doppler profiles of the ionospheric targets...|$|R
40|$|Abstract:- <b>Array</b> <b>processors</b> are {{widespread}} in real–time systems. In {{the last ten}} years phase–locked loops (PLL) have widely been used in <b>array</b> <b>processors</b> as control devices correcting a clock skew. In this paper new type of floating phase–locked loops for <b>array</b> <b>processors</b> is designed. For the floating phase locked loops new stability conditions are obtained...|$|E
40|$|New {{direct and}} {{implicit}} algorithms for optical matrix-vector and systolic <b>array</b> <b>processors</b> are considered. Direct rather than indirect algorithms to solve linear systems and implicit rather than explicit solutions to solve second-order partial differential equations are discussed. In many cases, such approaches more properly utilize the advantageous features of optical systolic <b>array</b> <b>processors.</b> The matrix-decomposition operation (rather than {{solution of the}} simplified matrix-vector equation that results) is recognized as the computationally burdensome aspect of such problems that should be computed on an optical system. The Householder QR matrix-decomposition algorithm is considered as a specific example of a direct solution. Extensions to eigenvalue computation and formation of matrices of special structure are also noted...|$|E
40|$|DDNAM and MNAM, are {{effective}} for small-scale neuronal assembly modeling. Realistic modeling of massive neuronal assemblies demands enormous computation resources beyond the number crunching capability of either a MNAM or a DDNAM processor. This has {{led us to}} develop two <b>array</b> <b>processors,</b> one based on mixed signal and other using a digital approach, for brain function modeling. Extensive Simulation of these array architectures for modeling assemblies and a comparative analysis with published experimental results were carried out. The comparison shows the fulfillment of {{the objective of the}} array processor with regards to realistic modeling of massive neuronal assemblies. For neuronal modeling, <b>Array</b> <b>processors</b> with specialized processing elements, like NAMs [2], have ever been proposed. ...|$|E
40|$|The {{application}} of an <b>array</b> <b>processor</b> to laser velocimeter data processing is presented. The hardware is described {{along with the}} method of parallel programming required by the <b>array</b> <b>processor.</b> A portion of the data processing program is described in detail. The increase in computational speed of a microcomputer equipped with an <b>array</b> <b>processor</b> is illustrated by comparative testing with a minicomputer. INTRODUCTION In the beginning of laser velocimeter development, signal processing consisted of viewing the Doppler signal on a spectrum analyzer and writing down the measured frequency. In time more sophisticated signal processors were developed such as frequency trackers, highspeed burst counters and photon correlators which required computer data acquisition. In those days data was acquired with minicomputers then transferred to main frame computers for processing. By the mid 1970 s the minicomputers had sufficient computing power to perform both the data acquisition and data processing [...] ...|$|R
3000|$|In the frequency-domain {{detection}} algorithm, {{the output}} of <b>array</b> <b>processor</b> is influenced by user random data. The detection threshold not only depends on the false alarm probability P [...]...|$|R
5000|$|A {{computer}} which exploits multiple data streams {{against a}} single stream to perform operations {{which may be}} naturally parallelized. For example, an <b>array</b> <b>processor</b> or graphics processing unit (GPU) ...|$|R
40|$|Techniques for {{integrating}} microprocessors, <b>array</b> <b>processors,</b> and other intelligent devices in control systems are reviewed, {{with an emphasis}} on the (re) arrangement of components to form distributed or parallel processing systems. Consideration is given to the selection of the host microprocessor, increasing the power and/or memory capacity of the host, multitasking software for the host, <b>array</b> <b>processors</b> to reduce computation time, the allocation of real-time and non-real-time events to different computer subsystems, intelligent devices to share the computational burden for real-time events, and intelligent interfaces to increase communication speeds. The case of a helicopter vibration-suppression and stabilization controller is analyzed as an example, and significant improvements in computation and throughput rates are demonstrated...|$|E
40|$|Digital {{computers}} {{are becoming increasingly}} popular {{for a variety of}} purposes in nuclear medicine. They are particuiarly useful in the areas of nuclear imaging and gamma camera image processing, radionuclide inventory and patient record keeping. By far the most important use of the digital computer is in <b>array</b> <b>processors</b> which are commonly available with emission computed systems for fast reconstruction of images in transverse, coronal and sagittal views, particularly when the data to be handled is enormous and involves filtration and correction processes. The addition of <b>array</b> <b>processors</b> to computer systems has h ~ k d the clinicians in improving diagnostic nuclear medicine imaging capability. This paper reviews briefly the role of computers in the field of nuclear medicine imaging. 1...|$|E
40|$|The {{rapid growth}} in device density {{achieved}} by VLSI technology {{over the past decade}} had the attention of researchers centered around the design of <b>array</b> <b>processors.</b> The systolic arrays had been designed {{for a wide variety of}} applications, and consequently formal strategies for mapping algorithms onto processor arrays wer...|$|E
40|$|Abstract high NRE cost. Therefore, {{general purpose}} {{programmable}} processors using software to perform In this paper, high performance <b>array</b> <b>processor</b> for signal processing algorithms with high computational complexities is implemented using 0. 16 µ m CMOS standard cell library. The proposed <b>array</b> <b>processor</b> consists of simple processing elements. The architectural benefits of highly regular, parallel, and pipelined processing elements simplify {{the design of}} complex signal processing systems and enable high throughput rate by massive parallel computations. We show {{the utility of the}} proposed architecture as a configurable core by mapping inverse discrete cosine transform (IDCT), motion compensation (MC), and inverse quantization (IQ) onto the proposed fabric. In addition, we propose a novel scheme that integrates the inverse quantization part of video decoding into the 2 -D IDCT process simplifying computational logics. The results show that a high throughput rate to meet the real-time requirement is effectively achieved by exploiting the properties of both compressed video data statistics and the <b>array</b> <b>processor</b> architecture. 1...|$|R
40|$|In this paper, {{the index}} {{space of the}} (n×n) -matrix multiply-add problem C = C +A·B is {{represented}} as a 3 D n×n×n torus. All possible time-scheduling functions to ac-tivate the computation and data rolling inside the 3 D torus index space are determined. To maximize efficiency when solving a single problem, we mapped the computations into the 2 D n×n toroidal <b>array</b> <b>processor.</b> All optimal 2 D data allocations that solve the problem in n multiply-add-roll steps are obtained. The well known Cannon’s algorithm {{is one of the}} resulting allocations. We used the optimal data allocations to describe all variants of the GEMM operation on the 2 D toroidal <b>array</b> <b>processor.</b> By controling the data movement, the transposition operation is avoided in 75 % of the GEMM variants. However, only one matrix transpose is needed for the remaining 25 %. Ultimately, we described four versions of the GEMM operation covering the possible layouts of the initially loaded data into the <b>array</b> <b>processor.</b> 1...|$|R
40|$|Herein, a {{processing}} net {{consisting of}} a set of interconnected high performance <b>array</b> <b>processor</b> nodes, which composes object binary relations along all the possible net directions, is investigated. Object binary relation compositions are required for the query processing of an object oriented database, whose functionality is based upon the binary relations algebra. An object binary relation is a set of ordered object pairs consisting of ids of objects, which are holding references one of the other. The net architecture is the mapping of the object oriented system, the derived objects of which constitute the object oriented database. For every object oriented system and its corresponding object database, the required <b>array</b> <b>processor</b> net architecture can be obtained, by applying the same mapping methodology. The basic elements of the net processing nodes are multiple copies of a high performance <b>array</b> <b>processor,</b> which can synthesize a new object binary relation from two existing ones, by applying the binary relations algebra composition operator. © 2009 American Institute of Physics...|$|R
40|$|ABSTRACT: Ultra high {{frame rate}} image {{processing}} {{was achieved by}} applying CNN-UM chips as focal plane <b>array</b> <b>processors.</b> By applying parallel optical input, and reading out binary decision from the chip only the computational overhead is negligible. This makes possible even 50, 000 fps image capturing and complex processing. Experiments were done and are described in the paper...|$|E
40|$|Decoding {{techniques}} and equipment used by MST radars are described and some recommendations for new systems are presented. Decoding {{can be done}} either by software in special-purpose (<b>array</b> <b>processors,</b> etc.) or general-purpose computers or in specially designed digital decoders. Both software and hardware decoders are discussed and the special case of decoding for bistatic radars is examined...|$|E
40|$|SEP {{has always}} been {{interested}} in problems that stretch our computational resources. Over time the high-end computer at SEP has gone from <b>array</b> <b>processors</b> (Newkirk, 1977) to the Convex (Claerbout, 1985) to the CM 5 (Biondi, 1991) to the Power Challenge to multi-processor Linux machines (Biondi et al., 1999), and in the last year, its first Linux Beowulf cluster...|$|E
40|$|Systems and Processes Engineering Corporation (SPEC) has {{developed}} an innovative <b>array</b> <b>processor</b> architecture for computing Fourier transforms and other commonly used signal processing algorithms. This architecture is designed to extract the highest possible array performance from state-of-the-art GaAs technology. SPEC's architectural design includes a high performance RISC processor implemented in GaAs, along with a Floating Point Coprocessor and a unique Array Communications Coprocessor, also implemented in GaAs technology. Together, these data processors represent the latest in technology, both from an architectural and implementation viewpoint. SPEC has examined numerous algorithms and parallel processing architectures to determine the optimum <b>array</b> <b>processor</b> architecture. SPEC {{has developed}} an <b>array</b> <b>processor</b> architecture with integral communications ability to provide maximum node connectivity. The Array Communications Coprocessor embeds communications operations directly {{in the core of}} the processor architecture. A Floating Point Coprocessor architecture has been defined that utilizes Bit-Serial arithmetic units, operating at very high frequency, to perform floating point operations. These Bit-Serial devices reduce the device integration level and complexity to a level compatible with state-of-the-art GaAs device technology...|$|R
5000|$|... 1999: Eurotech {{started to}} co-operate with INFN (Italian Institute of Nuclear Physics) {{for the study}} and {{implementation}} of its third generation of <b>Array</b> <b>Processor</b> Experiment supercomputers, called APEmille. Eurotech's HPC business unit began with this cooperation.|$|R
50|$|DAP FORTRAN was an {{extension}} of the non IO parts of FORTRAN with constructs that supported parallel computing for theICL Distributed <b>Array</b> <b>Processor</b> (DAP). The DAP had a Single Instruction Multiple Data (SIMD) architecture with 64x64 single bit processors.|$|R
