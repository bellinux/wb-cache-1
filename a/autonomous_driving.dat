849|104|Public
25|$|According to the Sydney Morning Herald, Apple {{wants to}} start {{producing}} an electric car with <b>autonomous</b> <b>driving</b> as soon as 2020. Apple has made efforts to recruit battery development engineers and other electric automobile engineers from A123 Systems, LG Chem, Samsung Electronics, Panasonic, Toshiba, Johnson Controls and Tesla Motors.|$|E
25|$|In August 2013, Nissan {{announced}} {{its plans to}} launch several driverless cars by 2020. The company is building in Japan a dedicated <b>autonomous</b> <b>driving</b> proving ground, {{to be completed in}} 2014. Nissan installed its autonomous car technology in a Nissan Leaf for demonstration purposes. The car was demonstrated at Nissan 360 test drive event held in California in August 2013. In September 2013, the Leaf fitted the prototype Advanced Driver Assistance System was granted a license plate that allows to drive it on Japanese public roads. The testing car will be used by Nissan engineers to evaluate how its in-house <b>autonomous</b> <b>driving</b> software performs in the real-world. Time spent on public roads will help refine the car’s software for fully automated driving. The autonomous Leaf was demonstrated on public roads {{for the first time at}} a media event held in Japan in November 2013. The Leaf drove on the Sagami Expressway in Kanagawa prefecture, near Tokyo. Nissan vice chairman Toshiyuki Shiga and the prefecture’s Governor, Yuji Kuroiwa, rode in the car during the test.|$|E
25|$|NASA {{reports that}} the Curiosity rover has {{successfully}} upgraded, {{for the third time}} since landing, its software programs and is now operating with version 11. The new software is expected to provide the rover with better robotic arm and <b>autonomous</b> <b>driving</b> abilities. Due to wheel wear, a concern to drive more carefully, over the rough terrain the rover is currently traveling on its way to Mount Sharp, was also reported.|$|E
5000|$|In the figure, {{an example}} of the RCS {{methodology}} for designing a control system for <b>autonomous</b> onroad <b>driving</b> under everyday traffic conditions is summarized in six steps.|$|R
50|$|On November 3, 2007, Victorville {{hosted the}} DARPA Urban Challenge, a six-hour <b>autonomous</b> robot <b>driving</b> contest {{through the streets}} of the Southern California Logistics Airport. The $2 million first prize went to the Carnegie Mellon University team.|$|R
5000|$|The film I, Robot (2004), set in Chicago in 2035, {{features}} <b>autonomous</b> vehicles <b>driving</b> on highways, {{allowing the}} car to travel safer at higher speeds than if manually controlled. The option to manually operate the vehicles is available.|$|R
25|$|One of {{the newer}} {{application}} areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It {{can also be used}} for detecting certain task specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for <b>autonomous</b> <b>driving</b> of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Mars Exploration Rover and ESA's ExoMars Rover.|$|E
50|$|In 2010, the Institute of Control Engineering of the Technische Universität Braunschweig {{demonstrated}} the first <b>autonomous</b> <b>driving</b> on public streets in Germany {{with the research}} vehicle Leonie. It was the first car licensed for <b>autonomous</b> <b>driving</b> {{on the streets and}} highways in Germany.|$|E
5000|$|... the {{standard}} supports advanced navigation technologies like eHorizon, <b>autonomous</b> <b>driving,</b> and cloud solutions ...|$|E
50|$|In July 2015 Ambarella {{acquired}} VisLab, {{a pioneer}} in perception systems and autonomous vehicle research founded by Professor Alberto Broggi. VisLab has developed computer vision and intelligent control systems for automotive and commercial applications, including Advanced driver-assistance systems and several generations of <b>autonomous</b> vehicle <b>driving</b> systems.|$|R
40|$|In this paper, {{the traffic}} {{efficiency}} of an advisory cooperative driving system, Advisory Acceleration Control is examined and {{compared to the}} efficiency of an <b>autonomous</b> cooperative <b>driving</b> system, Cooperative Adaptive Cruise Control. The algorithms and implementation thereof are explained. The results of both systems are presented and discussed. © 2011 IEEE...|$|R
50|$|In 2002, the DARPA Grand Challenge {{competitions}} were announced. The 2004 and 2005 DARPA competitions allowed international {{teams to}} compete in fully autonomous vehicle races over rough unpaved terrain and in a non-populated suburban setting. The 2007 DARPA challenge, the DARPA urban challenge, involved <b>autonomous</b> cars <b>driving</b> in an urban setting.|$|R
5000|$|... 2015, Apple {{electric}} car (iCar) project with <b>autonomous</b> <b>driving</b> is called Project Titan.|$|E
5000|$|... #Caption: The Volvo S60 Drive Meautonomous {{test vehicle}} is {{considered}} Level 3 <b>autonomous</b> <b>driving.</b>|$|E
5000|$|... #Caption: 2005 DARPA Grand Challenge winner STANLEY {{performed}} SLAM {{as part of}} its <b>autonomous</b> <b>driving</b> system ...|$|E
40|$|The paper {{describes}} {{the high level}} software issues in the task of <b>autonomous</b> robot <b>driving</b> on the park pavement. The task in question {{is to use the}} sensor data in order to autonomously navigate on the path given in predefined map. The paper gives detail information on the development of software tools for the task and our experience with different ways of sensors data acquisition...|$|R
40|$|Our team {{created an}} <b>autonomous</b> {{omnidirectional}} <b>driving</b> robot that {{was capable of}} small micro adjustments to accurately follow the black guide line using four motors. This omnidirectional robot used a linear potentiometer to raise and lower the grabbing mechanism. The grabbing mechanism was a claw with 3 D printed extensions which increased the amount of rings that could be picked up in a single motion. We expect this methodology of quick driving and streamlined movements would be a deciding factor in scoring points for our team...|$|R
40|$|ABSTRACT — The {{development}} of performance metrics {{is critical in}} the evaluation and advancement of intelligent systems. Obtaining the pinnacle of intelligence in autonomous vehicles requires evolutionary standards and community support. In order to analyze and compare competing implementations of intelligent systems, the critical components of the system must be decoupled to facilitate repeatable trials that target specific aspects of the system’s overall task. This paper presents a framework for a real virtual simulation environment that provides the facilities and tools to formally test the limitations and capabilities of <b>autonomous</b> road <b>driving</b> vehicles. Keywords...|$|R
5000|$|NDS Working Groups (WG): {{deals with}} {{dedicated}} technical topics, such as map format development, <b>autonomous</b> <b>driving,</b> and cloud navigation ...|$|E
5000|$|AImotive - an ADAS and <b>autonomous</b> <b>driving</b> {{software}} for self-driving cars, based on artificial intelligence, computer vision and sensor fusion ...|$|E
5000|$|Plenary {{sessions}} addressed robotics, technology {{needs for}} future consumer devices, commercialization of the Internet of Things (IoT), and <b>autonomous</b> <b>driving.</b>|$|E
40|$|In this paper, synergies {{between the}} {{recognition}} and tracking processes for <b>autonomous</b> vehicle <b>driving</b> are studied. Object recognition is periodically performed {{to focus attention}} on specific parts of the visual signal and to assign them symbolic meanings. Tracking is used to maintain correspondences between objects identified at successive recognition instants as well as to provide further features (e,g,, spatio-temporal trajectories) on which to base object-pose estimation. Results obtained by using complex road scenes are reported, which demonstrate the validity of the approach in terms of robustness, accuracy, and time responses...|$|R
40|$|Traffic sign {{detection}} and recognition (TSDR) has drawn considerable attention on developing intelligent transportation systems (ITS) and <b>autonomous</b> vehicle <b>driving</b> systems (AVDS) since 1980 ’s. Unlikely {{to the general}} TSDR systems that deal with real-time images captured by the in-vehicle cameras, this research aims on developing techniques for detecting, extracting, and positioning of traffic signs from Google Street View (GSV) images along user-selected routes for low-cost, volumetric and quick establishment of the traffic sign infrastructural database that {{may be associated with}} Google Maps. The framework and techniques employed in the proposed system are described...|$|R
40|$|Abstract — The {{intelligence}} of artificial systems is well quantified {{by the amount}} of specified complexity inherent in the representation, provided we have tools to measure it. Some may generally agree with this claim, but argue that it is simply intractable to successfully and accurately measure the specified complexity of any system, no matter how it was represented. We respond to this important and substantive criticism by performing a computation required by the NIM on an example problem. We have chosen <b>autonomous</b> on-road <b>driving,</b> a problem that has already been solved by &quot;systems &quot; that are known to be both complex and specified, namely, humans. We will begin with a concise statement of the scope of the problem and a summary description of an appropriate system representation approach. We then apply a previously published Native Intelligence Metric (NIM) to measure the specification inherent in that representation and perform some preliminary intelligence measurements for a particular <b>autonomous</b> on-road <b>driving</b> subsystem. We claim that with an appropriate intelligence metric and an appropriate system representation, we can establish an equivalency between 1) the &quot;state of the world &quot; conditions, forming the input to the system, that the system can respond to successfully, 2) the system representation, and 3) the system performance. This equivalency is a potentially powerful result and is a key benefit and uniqueness of the theory proposed in this paper. I...|$|R
5000|$|E! 45 {{helped to}} fund the Prometheus project for safer road vehicles, such as through <b>autonomous</b> <b>driving</b> with 745 million euros.|$|E
5000|$|The DARPA Grand Challenge {{was held}} in 2004, 2005 and 2007 as an <b>autonomous</b> <b>driving</b> {{competition}} with {{millions of dollars in}} prize money.|$|E
50|$|TomTom's main <b>autonomous</b> <b>driving</b> HD maps {{competitor}} is Here {{which is}} owned by a consortium of German automotive companies (namely Volkswagen, BMW, and Daimler).|$|E
40|$|Building knowledge-intensive {{real-time}} {{intelligent control}} systems {{is one of}} the most difficult tasks that humans attempt. It is motivated by the desire to create an artificial reasoning system that displays intelligent behavior (i. e. that can act on the world and successfully accomplish activities that are only possible with the levels of knowledge processing exhibited by human beings). Measuring and evaluating the success of such systems is difficult-a system’s observable behavior is not always indicative of its correctness or quality. This is especially true in complex real-time control systems such as <b>autonomous</b> on-road <b>driving,</b> which is th...|$|R
40|$|Abstract — We {{present a}} motion planner for <b>autonomous</b> highway <b>driving</b> that adapts the state lattice {{framework}} pioneered for planetary rover navigation to the structured environment of public roadways. The main contribution {{of this paper}} is a search space representation that allows the search algorithm to systematically and efficiently explore both spatial and temporal dimensions in real time. This allows the low-level trajectory planner to assume greater responsibility in planning to follow a leading vehicle, perform lane changes, and swerve around obstacles in the presence of other vehicles. We show that our algorithm can readily be accelerated on a GPU, and demonstrate it on an autonomous passenger vehicle. I...|$|R
40|$|Urban driving {{has become}} the focus of {{autonomous}} robotics in recent years. Many groups seek to benefit from the research in this field including the military, who hopes to deploy autonomous rescue forces to battle-torn cities, and consumers, who will benefit from the safety and convenience resulting from new technologies finding purpose in consumer automobiles. One key aspect of <b>autonomous</b> urban <b>driving</b> is localization, or the ability of the robot to determine its position on a road network. Any information that can be obtained for the surrounding area including stop signs, road lines, and intersecting roads can aid this localization. The work here attempts to combine some previously established computer vision methods to identify roads and develop a new method that can identify both the road and any possible intersecting roads present in front of a vehicle using a single color camera. Computer vision systems rely on a few basic methods to understand and identify what they are looking at. Two valuable methods are the detection of edges that are present in the image and analysis of the colors that compose the image. The method described here attempts to utilize edge information to find road lines and color information to find the road area and any similarly colored intersecting roads. This work demonstrates that combining edge detection and color analysis methods utilizes their strengths and accommodates for their weaknesses and allows for a method that can successfully detect road lanes and intersecting roads at speeds fast enough for use with <b>autonomous</b> urban <b>driving...</b>|$|R
50|$|According to The Wall Street Journal, {{it will be}} {{a battery}} {{electric}} vehicle, initially lacking full <b>autonomous</b> <b>driving</b> capability, with a possible unveiling around 2019.|$|E
50|$|In 2016, Kagermann was {{appointed}} by Federal Minister of Transport and Digital Infrastructure Alexander Dobrindt {{to serve on the}} German government’s Ethics Commission on <b>Autonomous</b> <b>Driving.</b>|$|E
50|$|It will be {{the first}} car in the world to have Level 3 <b>autonomous</b> <b>driving</b> - capable of {{self-driving}} at speeds up to 60 km/h (37 mph) and self-parking.|$|E
30|$|Applications of {{adhesive}} bonding {{are described in}} 2 papers. Ayyildiz et al. evaluate the shear bond strength (SBS) of three different cements to zirconia and lithium disilicate ceramic surface after thermal cycling. It was concluded that the self adhesive resin cement had the highest shear bond strength values when bonded to lithium disilicate and zirconia ceramic surface. However zinc-phosphate cement demonstrated significantly lower shear bond strength values for both ceramic groups. Wisner et al. used {{adhesive bonding}} for a fully <b>autonomous</b> pile <b>driving</b> monitoring of an impact-driven large-scale foundation structure for an offshore wind farm. A boosted moisture-curable polyurethane adhesive applied on appropriate primer coatings was successfully applied with all hardware of the measurement system.|$|R
40|$|Abstract — We {{present a}} motion {{planning}} framework for <b>autonomous</b> on-road <b>driving</b> considering both the uncertainty {{caused by an}} autonomous vehicle and other traffic participants. The future motion of traffic participants is predicted using a local planner, and the uncertainty along the predicted trajectory is computed based on Gaussian propagation. For the autonomous vehicle, the uncertainty from localization and control is estimated based on a Linear-Quadratic Gaussian (LQG) framework. Compared with other safety assessment methods, our framework allows the planner to avoid unsafe situations more efficiently, thanks to the direct uncertainty information feedback to the planner. We also demonstrate our planner’s ability to generate safer trajectories compared to planning only with a LQG framework. I...|$|R
40|$|International audienceThis {{paper is}} {{dedicated}} to the sensor fault tolerantcontrol scheme for <b>autonomous</b> vehicle <b>driving.</b> The nonlinearlateral vehicle model is described by the fuzzy Takagi-Sugeno(TS) model. The contributions aspects of this work consist of thedevelopment of a descriptor observer to estimate the state systemand faults by ensuring robustness against external disturbances. The gains of this observer are obtained by solving the LMIconstraints, which are developed using a L 2 gain technique andHinfinity criterion. Indeed, the proposed fault tolerant control strategyis justified by its ability to maintain an acceptable performance inthe presence of the sensor failure. Simulation results areaddressed to demonstrate the capability of this fault tolerantcontrol to counteract the effect of the sensor fault...|$|R
