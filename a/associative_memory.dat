2209|716|Public
5|$|MU5 was {{designed}} to be about 20 times faster than Atlas, and was optimised for running compiled programs rather than hand-written machine code, something that contemporary computers were unable to do efficiently. A major factor in the MU5's much-improved performance over its predecessors was its incorporation of <b>associative</b> <b>memory,</b> which greatly speeded up access to its main store.|$|E
25|$|L.L. Thurstone {{argued for}} a model of {{intelligence}} that included seven unrelated factors (verbal comprehension, word fluency, number facility, spatial visualization, <b>associative</b> <b>memory,</b> perceptual speed, reasoning, and induction). While not widely used, Thurstone's model influenced later theories.|$|E
25|$|Serotonin {{is known}} to {{regulate}} aging, learning and memory. The first evidence comes {{from the study of}} longevity in C. elegans. During early phase of aging, the level of serotonin increases, which alters locomotory behaviors and <b>associative</b> <b>memory.</b> The effect is restored by mutations and drugs (including mianserin and methiothepin) that inhibit serotonin receptors. The observation does not contradict with the notion that the serotonin level goes down in mammals and humans, which is typically seen in late but not early phase of aging.|$|E
40|$|Memory plays a {{major role}} in Artificial Neural Networks. Without memory, Neural Network can not be learned itself. One of the primary {{concepts}} of memory in neural networks is <b>Associative</b> neural <b>memories.</b> A survey has been made on <b>associative</b> neural <b>memories</b> such as Simple <b>associative</b> <b>memories</b> (SAM), Dynamic <b>associative</b> <b>memories</b> (DAM), Bidirectional <b>Associative</b> <b>memories</b> (BAM), Hopfield memories, Context Sensitive Auto-associative memories (CSAM) and so on. These memories can be applied in various fields to get the effective outcomes. We present a study on these <b>associative</b> <b>memories</b> in artificial neural network...|$|R
40|$|Quantum <b>associative</b> <b>memories</b> are connectionist {{structures}} that demonstrate the particle-wave {{nature of information}} and are compatible with quantum mechanics postulates. Following the solution of Schrödinger's diffusion equation, and using the Hopfield <b>memory</b> model, quantum <b>associative</b> <b>memories</b> are developed. It is proved that the weight matrix of quantum <b>associative</b> <b>memories</b> can be decomposed in a superposition of matrices, thus resulting in an exponential increase {{of the number of}} attractors (memory patterns). The storage and recall of patterns in quantum <b>associative</b> <b>memories</b> is studied through a numerical example and simulation tests. © 2007 - IOS Press and the author(s). All rights reserved...|$|R
40|$|The Quadratic Hebbian-type <b>associative</b> <b>memories</b> have {{superior}} performance, {{but they}} {{are more difficult to}} implement because of their large interconnection values in chips than are the first order Hebbian-type <b>associative</b> <b>memories.</b> In order to reduce the inter-connection value for a neural network with M patterns stored, the interconnection value [− M, M] is mapped to [− H, H] linearly, where H is the quantization level. The probability of direct convergence equation of quantized Quadratic Hebbian-type <b>associative</b> <b>memories</b> is derived and the performances are explored. The experiments demonstrate that the quan-tized network approaches the original recall capacity at a small quantization level. Quad-ratic Hebbian-type <b>associative</b> <b>memories</b> usually store more patterns; therefore, the strat-egy of linear quantization reduces interconnection value more efficiently...|$|R
2500|$|In 2010, Alibart, Gamrat, Vuillaume et al. {{introduced}} a new hybrid organic/nanoparticle device (the NOMFET : Nanoparticle Organic Memory Field Effect Transistor), which behaves as a memristor [...] and which exhibits the main behavior of a biological spiking synapse.This device, also called synapstor (synapse transistor), was used to demonstrate a neuro-inspired circuit (<b>associative</b> <b>memory</b> showing a pavlovian learning) ...|$|E
2500|$|Feingold (1992b) and Hedges and Nowell (1995) have {{reported}} that, despite average sex differences being small and relatively stable over time, test score variances of males were generally {{larger than those}} of females. Feingold found that males were more variable than females on tests of quantitative reasoning, spatial visualisation, spelling, and general knowledge. [...] Hedges and Nowell go one step further and demonstrate that, {{with the exception of}} performance on tests of reading comprehension, perceptual speed, and <b>associative</b> <b>memory,</b> more males than females were observed among high-scoring individuals.|$|E
2500|$|A {{growing body}} of {{research}} has found key points of the biological argument to be groundless. [...] For example, it was asserted for over a century that women were not as intellectually competent as men because they have slightly smaller brains on average. However, no substantiated significant difference in average intelligence has been found between the sexes. However men have a greater variability in intelligence and except in tests of reading comprehension, perceptual speed, and <b>associative</b> <b>memory,</b> males typically outnumber females substantially among high-scoring individuals. Furthermore, no discrepancy in intelligence is assumed between men of different heights, even though on average taller men have been found to have slightly larger brains. Feminists assert that although women may excel in certain areas and men in others, women are just as competent as men.|$|E
5000|$|Neural Networks (Perceptron, <b>associative</b> <b>memories,</b> spiking nets) ...|$|R
40|$|Abstract—A {{new family}} of <b>associative</b> <b>memories</b> based on sparse neural {{networks}} has been recently introduced. These memories achieve excellent performance {{thanks to the}} use of error-correcting coding principles. In this work, we introduce a new family of codes termed clique codes. These codes are based on the cliques in balanced n-partite graphs describing <b>associative</b> <b>memories.</b> In particular, we study an ensemble of random clique codes, and prove that such ensemble contains asymptotically good codes. Furthermore, these codes can be efficiently decoded using the neural networks based <b>associative</b> <b>memories</b> with limited complexity and memory consumption. I...|$|R
40|$|Bidirectional <b>Associative</b> <b>Memories</b> (BAM) {{based on}} Kosko’s model are {{implemented}} through iterative algorithms and present stability problems. Also, these models {{along with other}} models based on different methods, {{have not been able}} to perfectly recall all trained patterns. In this paper we present an English-Spanish / Spanish-English translator based on a new BAM model denominated Alpha-Beta BAM, whose process is non iterative and does not require to find stable states. The translator recalls the whole set of learned patterns, even when the presented word is incomplete. Key words: Bidirectional <b>Associative</b> <b>Memories,</b> Alpha-Beta <b>Associative</b> <b>Memories,</b> perfect recall, transaltor...|$|R
2500|$|The {{experiment}} of Kemenes et al. {{demonstrated that}} in an extrinsic modulatory neuron, nonsynaptic plasticity influences the expression of long-term <b>associative</b> <b>memory.</b> The relationship between nonsynaptic plasticity and memory was assessed using cerebral giant cells (CGCs). Depolarization from conditioned stimuli increased the neuronal network response. This depolarization lasted {{as long as the}} long-term memory. Persistent depolarization and behavioral memory expression occurred more than 24 hours after training, indicating long-term effects. In this experiment, the electrophysiological expression of the long-term memory trace was a conditioned stimulus induced feeding response. CGCs were significantly more depolarized in the trained organisms than the control group, indicating association with learning and excitability changes. When CGCs were depolarized, they showed an increased response to the conditional stimuli and a stronger fictive feeding response. This demonstrated that the depolarization is enough to produce a significant feeding response to the conditioned stimuli. [...] Additionally, no significant difference was observed in the feeding rates between conditioned organisms and ones that were artificially depolarized, reaffirming that depolarization is sufficient to generate the behavior associated with long-term memory.|$|E
2500|$|A {{standard}} model of memory that employs association {{in this manner}} is the Search of <b>Associative</b> <b>Memory</b> (SAM) model. Though SAM was originally designed to model episodic memory, its mechanisms are sufficient to support some semantic memory representations, as well. The SAM model contains a short- term store (STS) and long term store (LTS), where STS is a briefly activated subset of {{the information in the}} LTS. [...] The STS has limited capacity and affects the retrieval process by limiting the amount of information that can be sampled and limiting the time the sampled subset is in an active mode. [...] The retrieval process in LTS is cue dependent and probabilistic, meaning that a cue initiates the retrieval process and the selected information from memory is random. [...] The probability of being sampled is dependent on the strength of association between the cue and the item being retrieved, with stronger associations being sampled and finally one is chosen. [...] The buffer size is defined as r, and not a fixed number, and as items are rehearsed in the buffer the associative strengths grow linearly {{as a function of the}} total time inside the buffer. In SAM, when any two items simultaneously occupy a working memory buffer, the strength of their association is incremented. Thus, items that co-occur more often are more strongly associated. Items in SAM are also associated with a specific context, where the strength of that association determined by how long each item is present in a given context. In SAM, then, memories consist of a set of associations between items in memory and between items and contexts. The presence of a set of items and/or a context is more likely to evoke, then, some subset of the items in memory. The degree to which items evoke one another—either by virtue of their shared context or their co-occurrence—is an indication of the items' semantic relatedness.|$|E
50|$|<b>Associative</b> <b>memory</b> becomes poorer {{in humans}} as they age. Additionally, {{it has been}} shown to be non-correlational with single item (non-associative) memory function. Transcranial direct-current {{stimulation}} has improved performance on <b>associative</b> <b>memory</b> tasks. Patients with Alzheimer's disease have been shown to be poorer in multiple forms of <b>associative</b> <b>memory.</b>|$|E
40|$|Nonlinear {{spectral}} <b>associative</b> <b>memories</b> {{are proposed}} as quantized frequency domain formulations of nonlinear, recurrent <b>associative</b> <b>memories</b> in which volatile network attractors are instantiated by attractor waves. In contrast to conventional <b>associative</b> <b>memories,</b> attractors encoded {{in the frequency}} domain by convolution {{may be viewed as}} volatile on-line inputs, rather than nonvolatile, off-line parameters. Spectral memories hold several advantages over conventional <b>associative</b> <b>memories,</b> including decoder/attractor separability and linear scalability, which make them especially well suited for digital communications. Bit patterns may be transmitted over a noisy channel in a spectral attractor and recovered at the receiver by recurrent, spectral decoding. Massive nonlocal connectivity is realized virtually, maintaining high symbol-to-bit ratios while scaling linearly with pattern dimension. For-bit patterns, autoassociative memories achieve the highest noise immunity, whereas heteroassociative memories offer the added flexibility of achieving various code rates, or degrees of extrinsic redundancy. Due to linear scalability, high noise immunity and use of conventional building blocks, spectral <b>associative</b> <b>memories</b> hold much promise for achieving robust communication systems. Simulations are provided showing bit error rates (BERs) for various degrees of decoding time, computational oversampling, and signal-to-noise ratio (SNR) ...|$|R
40|$|Recurrent {{networks}} {{can be used}} as <b>associative</b> <b>memories</b> {{where the}} stored memories represent fixed points to which the dynamics of the network converges. These networks, however, also can present continuous attractors, as limit cycles and chaotic attractors. The use of these attractors in recurrent networks for the construction of <b>associative</b> <b>memories</b> is argued. Here, we provide a training algorithm for continuous attractors and present some numerical results of the learning method which involves genetic algorithms. Continuous attractors. A simple recurrent neural network can exhibit a diversity of dynamic behaviors. This diversity, which includes unstable states and continuous attractors, are particularly undesirable in the case <b>associative</b> <b>memories</b> as fixed attractors...|$|R
30|$|Next we give {{a second}} {{numerical}} example on application of <b>associative</b> <b>memories</b> based on CVNNs.|$|R
50|$|Bidirectional <b>associative</b> <b>memory</b> (BAM) {{is a type}} of {{recurrent}} neural network. BAM was introduced by Bart Kosko in 1988. There are two types of <b>associative</b> <b>memory,</b> auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of <b>associative</b> <b>memory.</b> However, Hopfield nets return patterns of the same size.|$|E
50|$|In psychology, <b>associative</b> <b>memory</b> {{is defined}} as the ability to learn and {{remember}} the relationship between unrelated items. This would include, for example, remembering the name of someone or the aroma of a particular perfume. This type of memory deals specifically with the relationship between these different objects or concepts. A normal <b>associative</b> <b>memory</b> task involves testing participants on their recall of pairs of unrelated items, such as face-name pairs. <b>Associative</b> <b>memory</b> is a declarative memory structure and episodically based.|$|E
5000|$|... #Subtitle level 2: <b>Associative</b> <b>memory</b> and {{similarity}} analytics ...|$|E
40|$|Abstract – <b>Associative</b> <b>memories</b> are data {{structures}} ad-dressed using part of {{the content}} rather than an index. They offer good fault reliability and biological plausibility. Among different families of <b>associative</b> <b>memories,</b> sparse ones are known to offer the best efficiency (ratio {{of the amount of}} bits stored to that of bits used by the network itself). Their re-trieval process performance has been shown to benefit from the use of iterations. In this paper, we introduce several rules to enhance the performance of the retrieval process in recent-ly proposed sparse <b>associative</b> <b>memories</b> based on binary neural networks. We show that these rules provide better performance than existing techniques. We also analyze the required number of iterations and derive corresponding curves...|$|R
40|$|International audienceA {{new family}} of <b>associative</b> <b>memories</b> based on sparse neural {{networks}} has been recently introduced. These memories achieve excellent performance {{thanks to the}} use of error-correcting coding principles. Based on these devices, we introduce a new family of codes termed clique codes. These codes are based on the cliques in balanced c-partite graphs describing <b>associative</b> <b>memories.</b> In particular, we study an ensemble of random clique codes, and prove that such ensemble contains asymptotically good codes. Furthermore, these codes can be efficiently decoded using the neural network-based <b>associative</b> <b>memories</b> with limited complexity and memory consumption. They offer a new interesting alternative to existing codes, in particular when the underlying channel is assumed to be a memoryless erasure channel...|$|R
40|$|Abstract—A word {{recognition}} architecture {{based on}} a network of neural <b>associative</b> <b>memories</b> and hidden Markov models has been developed. The input stream, composed of subword-units like wordinternal triphones consisting of diphones and triphones, is provided to the network of neural <b>associative</b> <b>memories</b> by hidden Markov models. The word recognition network derives words from this input stream. The architecture {{has the ability to}} handle ambiguities on subword-unit level and is also able to add new words to the vocabulary during performance. The architecture is implemented to perform the word recognition task in a language processing system for understanding simple command sentences like “bot show apple”. Keywords—Hebbian learning, hidden Markov models, neural <b>associative</b> <b>memories,</b> word recognition...|$|R
5000|$|The {{first version}} of the HNeT Application Development System was {{released}} in 1990 and published in 1991 which contained a number of example applications, based on the complex valued phase coherence/decoherence process. Among these applications were the complex valued Hopfield network or complex <b>associative</b> <b>memory,</b> which was discovered by S. Jankowski [...] in 1996 according to A. Hirose et al. The concepts initially developed and applied within the HNeT technology {{form the basis for}} several related academic fields; these referred to by the acronyms Quantum neural network (QNN), Holographic <b>associative</b> <b>memory</b> (HAM), Complex <b>Associative</b> <b>Memory</b> (CAM) and Complex Valued Neural Networks (CVNN).|$|E
5000|$|Ramtha on Cell Biology, <b>Associative</b> <b>Memory,</b> and the Personality (CD-9424.1) (2007) ...|$|E
5000|$|Trinary <b>Associative</b> <b>Memory,</b> US Patent No. 5,257, 387, October 26, 1993 ...|$|E
50|$|Bidirectional <b>associative</b> <b>memories</b> (BAM) are {{artificial}} {{neural networks}} {{that have long}} been used for performing heteroassociative recall.|$|R
40|$|<b>Associative</b> <b>memories</b> are data {{structures}} that allow retrieval of stored messages from {{part of their}} content. They thus behave similarly to human brain that is capable for instance of retrieving {{the end of a}} song given its beginning. Among different families of <b>associative</b> <b>memories,</b> sparse ones are known to provide the best efficiency (ratio of the number of bits stored to that of bits used). Nevertheless, {{it is well known that}} non-uniformity of the stored messages can lead to dramatic decrease in performance. We introduce several strategies to allow efficient storage of non-uniform messages in recently introduced sparse <b>associative</b> <b>memories.</b> We analyse and discuss the methods introduced. We also present a practical application example. Comment: 21 pages, 8 figures, submitted to Neurocomputin...|$|R
40|$|Quantum {{information}} processing in neural structures {{results in an}} exponential increase of patterns storage capacity and can explain the extensive memorization and inferencing capabilities of humans. An example {{can be found in}} neural <b>associative</b> <b>memories</b> if the synaptic weights are taken to be fuzzy variables. In that case, the weights' update is carried out {{with the use of a}} fuzzy learning algorithm which satisfies basic postulates of quantum mechanics. The resulting weight matrix can be decomposed into a superposition of <b>associative</b> <b>memories.</b> Thus, the fundamental memory patterns (attractors) can be mapped into different vector spaces which are related to each other via unitary rotations. Quantum learning increases the storage capacity of <b>associative</b> <b>memories</b> by a factor of 2 (N), where N is the number of neurons. (C) 2006 Elsevier B. V. All rights reserved...|$|R
5000|$|Holographic <b>{{associative}}</b> <b>memory</b> {{is part of}} {{the family}} of correlation-based associative memories, where information is mapped onto the phase orientation of complex numbers on a Riemann plane. It was inspired by holonomic brain model by Karl H. Pribram. Holographs {{have been shown to be}} effective for <b>associative</b> <b>memory</b> tasks, generalization, and pattern recognition with changeable attention.|$|E
5000|$|Otto Hahn Research Group on <b>Associative</b> <b>Memory</b> in Old Age (Head: Yvonne Brehmer) ...|$|E
5000|$|<b>Associative</b> <b>memory</b> {{system for}} {{sequential}} retrieval of data, filed November 1963, issued August 1967 ...|$|E
40|$|Bidirectional <b>associative</b> <b>memories</b> (BAMs) {{are being}} used {{extensively}} for solving a variety of problems related to pattern recognition. The simulation of BAMs comprising of large number of neurons involves intensive computation and communication. In this paper we discuss implementation of bidirectional <b>associative</b> <b>memories</b> on various multiprocessor topologies. Our studies reveal that BAMs can be implemented efficiently on the extended hypercube toplogy since {{the performance of the}} extended hypercube is better than that of the binary hypercube topology...|$|R
40|$|The {{relation}} existing between support vector machines (SVMs) and recurrent <b>associative</b> <b>memories</b> is investigated. The {{design of}} <b>associative</b> <b>memories</b> {{based on the}} generalized brain-state-in-a-box (GBSB) neural model formulated {{as a set of}} independent classification tasks, which can be efficiently solved by standard software packages for SVM learning. Some properties of the networks designed in this way are evidenced, like a surprising generalized Hebb's law. The performance of the SVM approach is compared to existing methods with non-symmetric connections, by some design examples...|$|R
40|$|<b>Associative</b> <b>memories</b> are data {{structures}} addressed using part of {{the content}} rather than an index. They offer good fault reliability and biological plausibility. Among different families of <b>associative</b> <b>memories,</b> sparse ones are known to offer the best efficiency (ratio {{of the amount of}} bits stored to that of bits used by the network itself). Their retrieval process performance has been shown to benefit from the use of iterations. However classical algorithms require having prior knowledge about the data to retrieve such as the number of nonzero symbols. We introduce several families of algorithms to enhance the retrieval process performance in recently proposed sparse <b>associative</b> <b>memories</b> based on binary neural networks. We show that these algorithms provide better performance, along with better plausibility than existing techniques. We also analyze the required number of iterations and derive corresponding curves...|$|R
