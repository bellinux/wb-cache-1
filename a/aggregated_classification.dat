11|86|Public
25|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example. However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. Nonetheless, the coherence of AUC as a measure of <b>aggregated</b> <b>classification</b> performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|E
40|$|This paper aims {{to analyze}} {{occupational}} and industrial segregation in the Spanish labor market {{by using the}} alternative tools proposed by Alonso-Villar and Del Río (2007), along with some new extensions put forward here. In particular, two decompositions of their segregation curves are proposed. The approach followed in this article allows measuring segregation {{of women and men}} separately, since the distribution of each group of workers across occupations and industries is compared with the distribution of total employment. To analyze industrial segregation, an <b>aggregated</b> <b>classification</b> of industries in four large groups (agriculture-fishing, industry, construction and services) and another by branches of activity are considered while to study occupational segregation, several partitions of individuals and of occupations are included. Occupational and industrial segregation; Segregation curves; Gender...|$|E
40|$|Objectives: Demonstration of the {{applicability}} of a framework called indirect classification to the example of glaucoma classification. Indirect classification combines medical a priori knowledge and statistical classification methods. The method is compared to direct classification approaches {{with respect to the}} estimated misclassification error. Methods: Indirect classification is applied using classification trees and the diagnosis of glaucoma. Misclassification errors are reduced by bootstrap aggregation. As direct classification methods linear discriminant analysis, classification trees and bootstrap <b>aggregated</b> <b>classification</b> trees are utilized in the problem of glaucoma diagnosis. Misclassification rates are estimated via 10 -fold cross-validation. Results: Indirect classification techniques reduce the misclassification error in the context of glaucoma classification compared to direct classification methods. Conclusions: Embedding a priori knowledge into statistical classification techniques can improve misclassification results. Indirect classification offers a framework to realize this combination...|$|E
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Coarse aggregates are the major constituents of concrete or asphalt mixtures and are widely used in various construction purposes. A classification system for these aggregates would provide a systematic means of aggregate identification which could be used in the selection of aggregates for different construction activities. The objectives of this research were: (1) to characterize the aggregates based on their properties (2) to develop a framework for an <b>Aggregate</b> <b>Classification</b> System (3) to provide basis for implementation of the classification system and (4) to recommend and list test procedures and equipment needed to carry out the tests on aggregates. Extensive field and laboratory investigations have been carried to study the performance of pavements made with different types of coarse aggregates and the properties which affected the performance have been incorporated in the <b>aggregate</b> <b>classification</b> system. The framework for the classification system is developed based on various physical, chemical, mechanical and thermal properties of <b>aggregates.</b> The <b>classification</b> system is developed in two stages: (1) A comprehensive system of <b>aggregate</b> <b>classification</b> incorporating all significant aggregate properties affecting performance and (2) A simplified version is arrived at from the first stage classification system. The classification system is recommended for implementation at three levels. The first level of implementation consists of aggregate identification and the second level provides detailed aggregate evaluation. The third level is aimed at supplementing the findings of the first two levels by providing a detailed evaluation of aggregates as needed. The basic tests recommended for aggregate evaluation are discussed and the required equipment for these tests are also listed. Aggregate properties and respective performance indicators are tabulated. Various areas of further research are identified and recommendations are made for implementation of the proposed classification system...|$|R
30|$|A RF {{algorithm}} is basically an ensemble nonlinear classification and regression machine-learning algorithm, which was first proposed by Breiman (2001). The algorithm improves model robustness and prediction accuracy by <b>aggregating</b> <b>classification</b> or regression trees. The algorithm also increases diversity and predictive power by modifying the tree construction method. Each node {{of the tree}} is split by the best variable, instead {{of all of the}} input variables, from several randomly selected variables.|$|R
50|$|Only {{the best}} four results count towards the championship. The race {{classification}} {{is given by}} the <b>aggregate</b> points <b>classification</b> of the round.|$|R
30|$|A {{lesson from}} the {{examples}} in Fig. 1 a and b is that {{the heart of the}} complexity of service level agreements is far beyond the simple configuration of arrivals at the port. Both figures show the overlap of supply chains beyond the port. Furthermore, not only do multiple supply chains use these resources, but individual chains overlap, and even in the same chain have different requirements for different cargoes. While integration with the port terminal itself is a start, there are potentially many uncoordinated factors. The risk of delay or error goes up with the number of partners and the overlap of individual customer supply chains implies that factors along the way may have an agency conflict that might create preferred treatment for one or the other chain on quixotic grounds. A standard Terms of Service which applies to the whole chain would assist these intermediate partners downstream to determine how to coordinate with each other through an <b>aggregated</b> <b>classification</b> of cargos.|$|E
40|$|To enable {{efficient}} browsing {{and interactive}} querying of very large collections, {{such as those}} found in digital libraries, {{it is essential to}} provide users with summaries of query result sets. Smart indexes can be used to generate summary statistics, <b>aggregated</b> <b>classification</b> information, and/or aggregated contentbased information for the result sets of arbitrary queries. We present the basic model of a smart index, as well as variations of smart indexes that are suitable when the size of summaries is large. An algorithm for generating summaries of the results of arbitrary queries is given, and algorithms for updating various summaries are discussed. Experimental results show that smart indexes generate summaries much more efficiently than traditional trees for all query areas greater than 1 %- 2 % of the data space, with a relatively small additional storage overhead. Contrary to traditional trees, smart indexes in general perform better as the query area grows larger. 1. Introduction D [...] ...|$|E
40|$|The talk {{introduces}} a new <b>aggregated</b> <b>classification</b> scheme aimed {{to support the}} implementation of text analysis methods in contexts characterised {{by the presence of}} rare text categories. This approach starts from the aggregate supervised text classifier developed by Hopkins and King and moves forward relying on rare event sampling methods. In details, it enables the analyst to enlarge the number of text categories whose proportions can be estimated preserving the estimation accuracy of standard aggregate supervised algorithms and reducing the working time w. r. t. to unconditionally increase the size of the random training set. The approach is applied to study the daily evolution of the web reputation of Expo Milano 2015, before, during and after the event. The data set is constituted by about 900, 000 tweets in Italian and 260, 000 tweets in English, posted about the event between March 2015 and December 2015. The analysis provides an interesting portray of the evolution of Expo stakeholders’ opinions over time and allow to identify the main drivers of Expo reputation. The algorithm will be implemented as a running option of the next release of R package ReadM...|$|E
40|$|Judgment (or logical) {{aggregation}} {{theory is}} logically {{more powerful than}} social choice theory and has been put to use to recover some classic results of this field. Whether it could also enrich it with genuinely new results is still controversial. To support a positive answer, we prove a social choice theorem by using the advanced nonbinary form of judgment aggregation theory developed by Dokow and Holzman (2010 c). This application involves <b>aggregating</b> <b>classifications</b> (specifically assignments) instead of preferences, and this focus justifies shifting away from the binary framework of standard judgement aggregation theory to a more general one...|$|R
40|$|Brain {{computer}} interfaces (BCIs) offer individuals {{suffering from}} major disabilities an alternative method {{to interact with}} their environment. Sensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks; however, the traditional SMR paradigms intuitively disconnect the control and real task, making them non-ideal for complex control scenarios. In this study, we design a new, intuitively connected motor imagery (MI) paradigm using hierarchical common spatial patterns (HCSP) and context information to effectively predict intended hand grasps from electroencephalogram (EEG) data. Experiments with 5 participants yielded an <b>aggregate</b> <b>classification</b> accuracy [...] intended grasp prediction probability [...] of 64. 5 % for 8 different hand gestures, more than 5 times the chance level. Comment: This work has been submitted to EMBC 201...|$|R
40|$|Constructing {{accurate}} classifier {{based on}} association rule {{is an important}} and challenging task in data mining. In this paper, a novel combination strategy based on rough sets (RST) and evidence theory (DST) for associative classification (RSETAC) is proposed. In RSETAC, rules are regarded as classification experts, after the calculation of the basic probability assignments (bpa) according to rule confidences and evidence weights employing RST, Yang's rule of combination is employed to combine the distinct evidences to realize an <b>aggregate</b> <b>classification.</b> A numerical example is shown to highlight the procedure of the proposed method. The comparison with popular methods like CBA, C 4. 5, RIPPER and MCAR indicates that RSETAC is a competitive method for classification based on association rule...|$|R
40|$|In {{this chapter}} we {{document}} the disaggregation of the input-output (I-O) tables. This {{is the major}} step undertaken in our in-house processing of the tables (for {{an overview of the}} processing, see chapter 11. A). It takes place after some initial clean-ups and adjustments, and before the synthesis of the composite region tables (chapter 14), further minor adjustments, and the fitting of the I-O tables to external data (chapter 19). While we use a 57 -sector classification for the GTAP 6 Data Base, many countries ’ input-output source statistics do not support such detail. In particular, few I-O sources match the relatively ambitious requirements for agriculture and food processing, where GTAP distinguishes 20 sectors, while some sources distinguish as few as two (agriculture and food processing separately). Accordingly, we accept contributed I-O tables using any reasonable aggregation of this classification, (Huff, McDougall and Walmsley, 2000). Where the contributed table uses an <b>aggregated</b> <b>classification,</b> we must disaggregate it to the standard 57 sectors in-house. Altogether, in GTAP 6, 17 regions require sectoral disaggregation. Of these five require disaggregation only in non-agricultural sectors, while four require disaggregation in agriculture. For non-agricultural sectors we use a disaggregation procedure that is relatively undemanding in data...|$|E
40|$|Lepidium latifolium (perennial pepperweed) is a noxious Eurasian weed invading {{riparian}} and wetland {{areas of}} the western US. Effective management of Lepidium requires detailed, accurate maps of its distribution, as may be provided by remote sensing, to contain existing infestations and eradicate incipient populations. We mapped Lepidium with 3 m spatial resolution, 128 -band HyMap image data in three sites of California's San Francisco Bay/Sacramento-San Joaquin Delta Estuary (Rush Ranch in Suisun Marsh and the Greater Jepson Prairie Ecosystem and the Cosumnes River Preserve in the Delta). These sites are markedly different in terms of hydrology, salinity, species composition, and structural and landscape diversity. <b>Aggregated</b> <b>classification</b> and regression tree models (CART), incorporating the results of mixture tuned matched filter (MTMF) analyses and spectral physiological indexes, were used to map Lepidium at the three sites. This approach was sufficiently flexible and robust to detect Lepidium with similar accuracies (~ 90 %) at both Rush Ranch and Jepson Prairie, but was unsuccessful at Cosumnes River Preserve. Comparisons {{of the behavior of}} the MTMFs and the CARTs between sites reveal the importance of environmental context in species mapping. Rush Ranch presents the simplest conditions for mapping Lepidium: it is the wettest and least diverse site and Lepidium is spectrally distinct from co-occurring species. At Jepson Prairie, several co-occurring species closely resemble Lepidium spectrally. Nevertheless, hyperspectral data provide sufficient spectral detail to resolve Lepidium even at this challenging site, which is facilitated by phenological separation from the matrix of annual grasses. At Cosumnes River Preserve, however, Lepidium is neither spectrally nor phenologically distinct, and consequently could not be mapped successfully. Evidence suggests that the success of a remote sensing analysis declines as site complexity increases (species, structural, and landscape diversity; spectral variability; etc.), although this relationship is complex, indirect, and may be phenology-dependent...|$|E
40|$|Heavy-duty {{machines}} are equipment constructed for working under rough conditions and their design {{is meant to}} withstand heavy workloads. However, the last decades technical development in cheap electronically components have {{lead to an increase}} of electrical systems in traditionally mainly mechanical systems of heavy-duty machines. As the complexity of these machines increases, so does the complexity of detecting and diagnosing machine faults. However, the addition of new electrical systems, such as on-board computational power and telematics, makes it possible to add new sensors that measure signals relevant for fault detection and diagnosis, and to process signals on-board or off-board the machines. In this thesis, we address the diagnostic problem by investigating data-driven methods for remote diagnosis of heavy-duty machines, where a part of the analysis is performed on-board the machine (fault detection), while another part is performed off-board the machine (fault classification). We propose a diagnostic framework where we use a novel combination of methods for each step in the diagnosis. On-board the machine, we have used logistic regression as an anomaly detector to detect faults that will lead to a stream of individual cases classified as anomalous or not. Then, either on-board or off-board, we can use a probabilistic anomaly detector to identify whether the stream of cases is truly anomalous {{when we look at the}} stream of cases as a group. The anomalous group of cases is called a composite case. Thereafter, off-board the machine, each anomalous individual case is classified into a fault type using a case-based reasoning approach to fault diagnosis. In the final step, we fuse the individual classifications into a single <b>aggregated</b> <b>classification</b> for the composite case. In order to be able to assess the reliability of a diagnosis, we also propose a novel case-based approach to estimating the reliability of probabilistic predictions. It can, for instance, be used for assessing the confidence of the classification of a composite case given historical data of the predictive reliability...|$|E
40|$|This paper updates {{earlier work}} by the authors (Elias and Purcell 2004) to create a {{statistical}} classification for analysis {{of the relationship between}} higher education (HE) and employment. Based on the Standard Classification of Occupations (SOC 2010), occupation unit groups are allocated to four categories via analysis of the tasks associated with typical jobs in each unit group. For three of the four categories (Experts, Orchestrators and Communicators) we postulate a link between the constituent tasks and the skills/knowledge provided via higher education. Validation of the new <b>aggregate</b> <b>classification</b> [SOC(HE) 2010] is then undertaken, using information from the 2011 and 2012 UK Labour Force Surveys and Futuretrack – a longitudinal study of applicants to HE in 2006...|$|R
40|$|Electronic Tongue {{is a kind}} of {{intelligent}} equipment which is used to distinguish tastes. An electronic tongue made by a sensor array of ion-selective electrodes (ISE) has been developed and used for the qualitative analysis of five different kinds of mineral water. The acquired original data has been optimized by the principle component analysis (PCA) and independent component analysis (ICA). Then a wavelet neural network (WNN) model was designed based on the local optimalizing searching characteristic of BP neural network and an appropriate set of the parameters. The application results show that the performance of the proposed method surpasses the traditional BP algorithm. It can improve convergence and the learning capability of the network, and gives the Electronic Tongue a higher <b>aggregate</b> <b>classification</b> rate...|$|R
40|$|The main {{challenge}} of designing classification algorithms for sensor networks {{is the lack}} of labeled sensory data, due to the high cost of manual labeling in the harsh locales where a sensor network is normally deployed. Moreover, delivering all the sensory data to the sink would cost enormous energy. Therefore, although some classification techniques can deal with limited label information, they cannot be directly applied to sensor networks since they are designed for centralized databases. To address these challenges, we propose a hierarchical <b>aggregate</b> <b>classification</b> (HAC) protocol which can reduce the amount of data sent by each node while achieving accurate classification in the face of insufficient label information. In this protocol, each sensor node locally makes cluster analysis and forwards only its decision to th...|$|R
40|$|Although the MODIS Collection 5. 1 Land Cover Type (MODIS v 5. 1 LCT) {{product is}} one of the most recent global land cover {{datasets}} and has the shortest updating cycle, evaluations regarding this collection have not been reported. Given the importance of evaluating global land cover data for producers and potential users, the 2010 MODIS v 5. 1 LCT product IGBP (International Geosphere-Biosphere Programme) layer was evaluated based on two grid maps at scales of 100 -m and 500 -m,which were derived by rasterizing the 2010 data from the national land use/cover database of China (NLUD-C). This comparison was conducted based on a new legend consisting of nine classes constructed based on the definitions of classes in the IGBP and NLUD-C legends. The overall accuracies of the <b>aggregated</b> <b>classification</b> data were 64. 62 % and 66. 42 % at the sub-pixel and pixel scales, respectively. These accuracies differed significantly in different regions. Specifically, high-quality data were obtained more easily for regions with a single land cover type, such as Xinjiang province and the northeast plain of China. The lowest accuracies were obtained for the middle of China, including Ningxia, Shaanxi, Chongqing, Yunnan and Guizhou. At the sub-pixel scale, relatively high producer and user accuracies were obtained for cropland, grass and barren regions; the highest producer accuracy was obtained for forests, and the highest user accuracy was obtained for water bodies. Shrublands and wetlands were associated with low producer and user accuracies at the sub-pixel and pixel scales, of less than 10 %. Based on dominant-type reference data, the errors were classified as mixed-pixel errors and labeling errors. Labeling errors primarily originated from misclassification between grassland and barren lands. Mixed pixel errors increased as the pixel diversity increased and as the percentage of dominant-type sub-pixels decreased. Overall, mixed pixels were sources of error for most land cover types other than grassland and barren lands; whereas labeling errors were more prevalent than mixed pixel errors when considering all of the land cover data over China, due to the large amount of misclassification between the pure pixels of grassland and barren lands. Next, the accuracy of cropland/natural vegetation mosaics was assessed based on the qualitative (a mosaic of croplands, forests, shrublands, and grasslands) and quantitative (no single component composes more than 60 % of the landscape) parts in the definition, which resulted in accuracies of 91. 43 % and less than 19. 26 %, respectively. These results are summarized with their implications for the development of the next generation of MCD 12 Q 1 data and with suggestions for potential users of MCD 12 Q 1 v 5. 1...|$|E
40|$|Ensemble {{learning}} algorithms {{combine the}} results of several classifiers to yield an <b>aggregate</b> <b>classification.</b> We present a normative evaluation of combination methods, applying and extending existing axiomatizations from social choice theory and statistics. For the case of multiple classes, we show that several seemingly innocuous and desirable properties are mutually satisfied only by a dictatorship. A weaker set of properties admit only the weighted average combination rule. For the case of binary classification, we give axiomatic justifications for majority vote and for weighted majority. We also show that, even when all component algorithms report that an attribute is probabilistically independent of the classification, common ensemble algorithms often destroy this independence information. We exemplify these theoretical results with experiments on stock market data, demonstrating how ensembles of classifiers can exhibit canonical voting paradoxes. 1. Introduct [...] ...|$|R
40|$|Abstract—Electronic Tongue {{is a kind}} of {{intelligent}} equipment which is used to distinguish tastes. An electronic tongue made by a sensor array of ion-selective electrodes (ISE) has been developed and used for the qualitative analysis of five different kinds of mineral water. The acquired original data has been optimized by the principle component analysis (PCA) and independent component analysis (ICA). Then a wavelet neural network (WNN) model was designed based on the local optimalizing searching characteristic of BP neural network and an appropriate set of the parameters. The application results show that the performance of the proposed method surpasses the traditional BP algorithm. It can improve convergence and the learning capability of the network, and gives the Electronic Tongue a higher <b>aggregate</b> <b>classification</b> rate. Index Terms—electronic tongue, mineral water, principal component analysis, independent component analysis, wavelet neural network I...|$|R
40|$|The {{performance}} of hot mix asphalt, Portland cement concrete, unbound base, and subbase layers in a pavement are significantly affected by <b>aggregate</b> shape characteristics. <b>Classification</b> of coarse and fine aggregate shape properties such as shape (form), angularity, and texture, {{are important in}} predicting the {{performance of}} pavements. Consequently, {{there is a need}} to implement a system that can characterize aggregates without the limitations of the current <b>aggregate</b> <b>classification</b> standards. The <b>Aggregate</b> Image Measurement System (AIMS) was developed as a comprehensive and capable means of measuring aggregate shape properties. A new design of AIMS will be introduced with several modifications to improve the operational and physical components. The sensitivity, repeatability, and reproducibility are analyzed to evaluate the quality of AIMS measurements. The sensitivity of AIMS is evaluated and found to be good for several operational and aggregate parameters. Important operational and environmental factors that could affect the AIMS results are identified and appropriate limits are recommended. AIMS is able to control normal variations in the system without affecting the results. A comprehensive analysis is conducted to determine the repeatability and reproducibility of AIMS for multiple users and laboratories. Single-operator and multi-laboratory precision statements are developed for the test method in order to be implemented into test standards...|$|R
40|$|Abstract: Alkali-silica {{reaction}} {{is one of}} the most recognized deleterious phenomenon in concrete that results in excessive expansion, cracks, loss in mechanical properties and serviceability problems. This paper reports overview of alkali-silica reactivity (ASR) in concrete including background, chemistry behind ASR, factors affecting ASR, and symptoms of ASR. The aggregates susceptible to ASR were evaluated using field performance, petrographic analysis, aggregate mineralogy and the standard and modified methods of ASTM C 1260 and C 1293, and their modifications. <b>Aggregate</b> <b>classifications</b> into innocuous and reactive based on the stated mineralogy tests, and the expansion limits of the standard methods and their modifications were compared. The study demonstrated that none of the single method is an ideal approach to evaluate the alkali-silica reactivity of an aggregate, and a suitable combination of various methods can be utilized to better predict the potential ASR reactivity of an aggregate...|$|R
40|$|Credit {{is one of}} the {{facilities}} provided by banks to lend money to someone or a business entity within the prescribed period. The smooth repayment of credit is essential for the bank because it influences the performance as well as its presence in daily life. Acceptance of prospective credit customers should be considered to minimize the occurrence of bad credit. Classification and Regression Trees (CART) is a statistical method {{that can be used to}} identify potency of credit customer status such as current credit and bad credit. The predictor variables used in this study are gender, age, marital status, number of children, occupation, income, tenor / period, and home ownership. To improve the stability and accuracy of the prediction were used the Bootstrap <b>Aggregating</b> <b>Classification</b> and Regression Trees (Bagging CART) method. The classification of credit customers using Bagging CART gives the classification accuracy 81, 44 %...|$|R
40|$|The Iowa D. O. T. has a {{classification}} {{system designed to}} rate coarse aggregates as to their skid resistant characteristics. Aggregates have been classified into five functional types, with a Type 1 being the most skid resistant. A complete description of the classification system {{can be found in}} the Office of Materials Instructional Memorandum T- 203. Due to the variability of ledges within any given quarry the classification of individual ledges becomes necessary. The type of aggregate is then specified for each asphaltic concrete surface course. As various aggregates become used in a. c. paving, there is a continuing process of evaluating the frictional properties of the pavement surface. It is primarily through an effort of this sort that information on aggregate sources and individual ledges becomes more refined. This study is being conducted to provide that needed up-to-date information that can be used to monitor the <b>aggregate</b> <b>classification</b> system...|$|R
40|$|PHBs {{defined in}} the IETF, the Expedited Forwarding (EF) PHD [2, 3] and the Assured Forwarding (AF) PHB [4]. The basic {{features}} of a DiffServ architecture are: (i) multiple flows are mapped to aggregate service levels, (ii) qualitative QoS assurances can be provided to applications using various service levels, and (iii) state information about every flow need not be maintained along the path. DiffServ performs <b>aggregate</b> <b>classification</b> 1 Table 1 : Differentiated Services Code Point Space Pool Code point Space Assignment Policy 1 xxxxx 0 Standard Action 2 xxxx 11 Experimental or local use 3 xxxx 01 Experimental or local use of packets in contrast to IntServ, which provides a per-flow classification. In principal, Differentiated Services will support QoS based on flows and aggregated flows by differentiation based on a certain code point. The code points are divided into three code point pools (Table 1. One is for standards {{and the other two}} are for experimental or local use. One o...|$|R
40|$|ABSTRACT: This paper {{discusses}} {{a quality}} control method, based on artificial neural networks, that enables a plant operator to quickly detect property variations during {{the production of}} stone aggregates. The group texture concept in digital image analyses, two-dimensional wavelet transforms, and artificial neural networks are reviewed first. An artificial intelligence based <b>aggregate</b> <b>classification</b> system is then described. This system relies on three-dimensional aggregate particle surface data, acquired with a laser profiler, and conversion of this data into digital images. Two-dimensional wavelet transforms are applied to the images and used to extract important features that can help to differentiate between in-spec and out-of-spec aggregates. These wavelet-based features are used as inputs to an artificial neural network, {{which is used to}} assign a predefined class to the aggregate sample. Verification tests show that this approach can potentially help a plant operator determine, in a fast and accurate manner, if the aggregates currently being produced are in-spec or out-of-spec...|$|R
30|$|When <b>aggregating</b> the <b>classification</b> {{results for}} both {{observers}} {{in order to}} compare inter-method agreement between quantitative and visual assessment of planar scintigraphy, k was 0.42 (p[*]<[*] 0.01). When comparing the same quantitative results to visual assessment of the SPECT/CT, k was 0.62 (p[*]<[*] 0.01). The results of the aggregated comparison of visual and quantitative assessment and the uptake ratios {{can be found in}} Table  8.|$|R
40|$|The paper {{deals with}} {{measurement}} error, and its potentially distorting role, in information on industry and professional status collected by labour force surveys. The {{focus of our}} analyses is on inconsistent information on these employment characteristics resulting from yearly transition matrices for workers who were continuously employed over the year and who did not change job. As a case-study we use yearly panel data for the period from April 1993 to April 2003 collected by the Italian Quarterly Labour Force Survey. The analysis goes through four steps: (i) descriptive indicators of (dis) agreement; (ii) testing whether the consistency of repeated information significantly increases {{when the number of}} categories is collapsed; (iii) examination of the pattern of inconsistencies among response categories by means of Goodman’s quasi-independence model; (iv) comparisons of alternative classifications jointly by professional status and occupation. Results document sizable measurement error, which is only moderately reduced by more <b>aggregated</b> <b>classifications.</b> They suggest that even cross-section estimates of employment by industry and/or professional status are affected by non-random measurement erro...|$|R
30|$|F 2 topic-interesting-me {{reflects}} {{the extent to}} which the retweeter is interested in the article’s topic. To compute it, we create, for each user, his interest-vector by considering each article the user posts, classifying the article’s categories, and <b>aggregating</b> the <b>classifications</b> of all the user’s articles into a unique interest-vector. The classification consists of 12 categories and is performed by the Alchemy Application Programming Interface ([URL] which is a popular text-mining web service that classifies news articles in a number of topic.|$|R
30|$|Companies adopt {{a variety}} of {{financial}} performance measures in executive compensation plans. Prior literature shows that performance measure choice affects managerial decisions (e.g., Marquardt and Wiedman, 2005; Young and Yang, 2011; Huang et al., 2014). Performance measures adopted in executive compensation contracts are important in communicating corporate objectives to managers and evaluating managerial performance. Due to data limitations, prior studies in executive compensation have generally investigated <b>aggregate</b> performance measure <b>classifications,</b> such as net income and stock returns, rather than specific performance measures (Ittner and Larcker, 2002). However, Ittner and Larcker (2002) assert that the <b>aggregate</b> performance measure <b>classifications,</b> such as accounting vs. market performance measures, commonly used in compensation research provide somewhat misleading inferences regarding performance measure choices since factors influencing the use of specific measures vary.|$|R
40|$|Poster Session: UltrasoundMitral valve repair {{is one of}} {{the most}} {{prevalent}} operations for various mitral valve conditions. Echocardiography, being famous for its low-cost, non-invasiveness and speediness, is the dominant imaging modality used for carrying out mitral valve condition analysis in both pre-operative and intra-operative examinations. In order to perform analysis on different phases of a cardiac cycle, it is necessary to first classify the echocardiograhic data into volumes corresponding to the systole and diastole phases. This often requires tedious manual work. This paper presents a fully-automatic method for systole-diastole classification of real-time three-dimensional transesophageal echocardiography (RT- 3 D-TEE) data. The proposed method first resamples the data with radial cutting planes, segments the mitral valve by thresholding, and removes noise by median filtering. Classification is then carried out based on the number of identified mitral valve regions. A multiresolution processing scheme is proposed to further improve the <b>classification</b> accuracy by <b>aggregating</b> <b>classification</b> results obtained from different image resolution scales. The proposed method was evaluated against the classification results produced by a cardiologist. Experimental results show that the proposed method, without the use of computationally intensive algorithms or the use of any training database, can achieve a classification accuracy of 91. 04 %. published_or_final_versio...|$|R
40|$|Numerous {{methods are}} {{currently}} available for motion detection using background modeling and subtraction. However, {{there are still}} many challenges to take into account such as moving shadows, illumination changes, moving background, relocation of background objects, and initialization with moving objects. This paper provides a new background subtraction algorithm that <b>aggregates</b> the <b>classification</b> results of several foreground extraction techniques based on UV color deviations, probabilistic gradient information and vector deviations, in order to produce a single decision that is more robust to those challenges. Categories and Subject Descriptors I. 4 [Image processing and computer vision]: Segmentation- pixe...|$|R
40|$|In this paper, we {{continue}} our investigations of “web spam”: the injection of artificially-created pages into the web in order to influence the results from search engines, to drive traffic to certain pages for fun or profit. This paper considers some previously-undescribed techniques for automatically detecting spam pages, examines {{the effectiveness of these}} techniques in isolation and when <b>aggregated</b> using <b>classification</b> algorithms. When combined, our heuristics correctly identify 2, 037 (86. 2 %) of the 2, 364 spam pages (13. 8 %) in our judged collection of 17, 168 pages, while misidentifying 526 spam and non-spam pages (3. 1 %) ...|$|R
40|$|Status of this Memo An Architecture for Differentiated Services This memo {{provides}} {{information for the}} Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (1998). All Rights Reserved. This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by <b>aggregating</b> traffic <b>classification</b> state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need onl...|$|R
40|$|Seven asphaltic {{concrete}} resurfacing projects were tested for their frictional properties {{to determine the}} age-friction relationship of new paving. Projects studied included Type A {{asphaltic concrete}} which is generally used for higher traffic volume roads and Type B asphaltic concrete, a lower type material. Also {{included in the study}} were asphaltic concretes containing Type 3 and Type 4 coarse <b>aggregate</b> texture <b>classifications.</b> The classifications are based upon material type and grain size composition. Surfaces both with and without sprinkle treatment aggregates were also included. The data gathered suggests that properly designed and placed dense graded asphaltic concrete mixes are adequate to serve the traveling public at all ages tested...|$|R
