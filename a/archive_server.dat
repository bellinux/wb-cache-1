36|53|Public
5000|$|... #Caption: The <b>archive</b> <b>server</b> in the Music Library, Belfast Central Library ...|$|E
5000|$|Koschitzki, Thomas. GIS-basierte, automatische Erfassung natürlicher Fließgewässerhierarchien und ihre Abbildung in Datenbanken, beispielhaft dargestellt am Einzugsgebiet der Salza. Dissertation, Martin-Luther-Universität Halle-Wittenberg, Halle (Saale), 2004, URN (NBN) urn:nbn:de:gbv:3-000007179 (Weblink, <b>archive</b> <b>server</b> DNB) ...|$|E
3000|$|The available, cost-intensive PACS-infrastructure (long-term <b>archive,</b> <b>server,</b> network) {{is used by}} all ward systems [...]...|$|E
40|$|Nowadays, the Video {{conferencing}} {{systems are}} widely used in many areas. The multimedia conference system (MCS) {{is one of the}} Video conferencing systems which increasingly gaining acceptance because of its unique features. However, the MCS is lacking of the archiving system which used to store the session data for later retrieve. This paper proposed to add <b>archiving</b> <b>server</b> to the MCS, in order to store the session data. The proposed archiving system store four types of media data, which they are video, audio, files, and chat. The four types of media data stored in the <b>archiving</b> <b>server</b> through FTP session between the <b>archiving</b> <b>server</b> and the client...|$|R
50|$|Lync Server {{also has}} the {{capability}} to log and archive all instant message traffic passing through the server and to create Call Detail Records for conferences and voice. These features can help provide compliance with legal requirements for many organizations. The <b>Archiving</b> <b>server</b> is not an overall end-to-end compliance solution, as archiving requires you to install the <b>Archiving</b> <b>Server</b> and to configure front end servers accordingly.|$|R
5000|$|The CSC servers {{create a}} common user {{environment}}. The service environment {{is made up}} of several supercomputers, database servers and information servers that are all linked together with a fast data transfer connection into one metacomputer. Extensive data and <b>archive</b> <b>servers</b> are available for saving results.|$|R
40|$|In this paper, a {{teleservice}} for archiving and retrieving multimedia documents using public networks is described. This teleservice {{encourages a}} broad range of commercially applicable multimedia archiving applications suitable for an asynchronous access mechanism. It is based on an integrated architecture comprising stand alone archive clients and a multimedia <b>archive</b> <b>server</b> which is realized using a database management system. Archive clients access the <b>archive</b> <b>server</b> via an extended X. 400 Multimedia Mail Teleservice. This teleservice reflects the specific requirements of dealing with multimedia documents in a networked environment by supporting a global reference mechanism. The <b>archive</b> <b>server</b> can dynamically compose new versions from an original archived multimedia document including extractions of subsequences of continuous data streams, coding and quality transformations. Thus, users can retrieve multimedia documents that explicitly reflect his individual workstation environment, i [...] ...|$|E
40|$|The present {{application}} {{addresses the}} issue of portability {{in the context of}} linguistic fieldwork, both in the sense of platform interoper-ability and in the sense of ultra-mobility. A three-level networked architecture, the UbiCorpus model, for information gathering in the field is described: (1) a Resource <b>Archive</b> <b>server</b> layer, (2) a Data Processing application layer, and (3) a new Corpus Pilot layer designed to support specific fieldwork sessions under adverse conditions, for on-site questionnaire presentation and metadata editing. 1. Goals In linguistic fieldwork, 1 conceptually the initial stage in any language documentation procedure, {{the issue of}} porta-bility is important in two senses: first, the sense of platform interoperability and second, in the sense of ultra-mobility. This issue is addressed by the present application. A three-level networked architecture, the UbiCorpus model, for in-formation gathering in the field is described: (1) a Re-source <b>Archive</b> <b>server</b> layer, typically non-mobile, and dis-tributed; (2) a Data Processing application layer, typicall...|$|E
40|$|Navigation (Nav) /Prop {{software}} {{is used to}} support shuttle mission analysis, production, and some operations tasks. The Nav/Prop suite containing configuration items (CIs) resides on IPS/Linux workstations. It features lifecycle documents, and data files used for shuttle navigation and propellant analysis for all flight segments. This suite also includes trajectory server, <b>archive</b> <b>server,</b> and RAT software residing on MCC/Linux workstations. Navigation/Prop represents tool versions established during or after IPS Equipment Rehost- 3 or after the MCC Rehost...|$|E
50|$|Syndie {{operates}} {{in a manner}} similar to blogs, newsgroups, forums, and other content tools; it allows one or more authors to privately or publicly post messages. Messages are pushed and pulled to and from <b>archive</b> <b>servers</b> (other peers that choose to be), which are hosted in a variety of anonymous and non-anonymous locations.|$|R
50|$|On 27 September 2016 OCLC {{announced}} a {{cooperation with the}} Internet Archive resulting in {{the transfer of the}} resolver service and its administration interface to Internet Archive. This service is supported on newly created software, separate from all previous implementations. This transfer reenabled the ability manage PURL definitions that had been disabled in the OCLC hosted service for several months. This service hosted on Internet <b>Archive</b> <b>Servers</b> supports access via purl.org, purl.net, purl.info, and purl.com. OCLC are now redirecting DNS requests for purl.oclc.org to purl.org.|$|R
40|$|Within the European HARP project, a Java-based Open Platform {{has been}} {{specified}} and implemented to support trustworthy distributed applications for health. Emphasis {{was put on}} security services for enabling both communication and application security. The Open Platform is Web-based and comprises the Client environment, Web/Application server, as well as Database and <b>Archive</b> <b>servers.</b> Servlets composed and executed according to the user's authorisation create signed XML messages. From those messages, user-role-related applets are generated. The technical details of the realisation are presented. Possible future enhancements for user-centric, adaptable services based on next-generation mobile service environments are outlined...|$|R
40|$|This Supplement to EBU Technical {{document}} 3285 {{contains the}} specification {{for the use}} of the BWF to carry information on the audio material gathered and computed by a cap-turing workstation (DAW). The BWF file is used as a platform-independent container for the sound signal and all the relevant metadata. The receiving <b>archive</b> <b>server</b> is able to extract the needed information from the file and use it as required; for example, enter it into a database, etc. This supplement specifies a new chunk to carry the information not already present in...|$|E
40|$|Electronic patient {{records for}} {{radiation}} therapy (RT) consists of text, images and graphics. To enable {{the exchange of}} RT patient information between systems and institutions, a common standard is called for and the DICOM standard extension to radiation therapy is an appropriate means for standardization. This paper describes a model of DICOM-based electronic patient record system for information exchange and sharing. The system used a DICOM-based RT <b>archive</b> <b>server</b> as a common platform for archival of all RT related information including images and the web technology for distribution and viewing of the patient electronic record. School of Optometr...|$|E
40|$|In this paper, a {{teleservice}} for archiving and retrieving multimediadocumentsusingpublicnetworks is described. Thisteleserviceencouragesabroadrange {{of multimedia}} archiving applications. The described implementation {{is based on}} standardized multimediamailand suitable for an asynchronous access mechanism. Theoriginalityofthiswork is theintegrationofmultimediamailwithsimplestandalone archive clients for heterogeneous platforms and an <b>archive</b> <b>server</b> {{which is based on}} an object-oriented DBMS. The modeling of the server is described indepthshowingthecontributionsofobject -orienteddatabasetechnologyfortherealization of generic electronic archives that deal with structured multimedia documents. Helpfulhintsandimportantdesignsuggestionsfortheimplementationofadvanced archive functionality are provided. Furthermore, functionality for efficient access on the client side is outlined. A sample application showing a Multimedia Calendar of Events is sketched as well. 1 Introduction Advances in hardw [...] ...|$|E
50|$|Most <b>archive</b> <b>servers</b> are HTTP <b>{{archives}}</b> hosted {{inside the}} I2P network, {{but there are}} syndie archives in Freenet {{as well as the}} normal internet. Each archive does not control the content stored on it; by default all messages are pushed and pulled by all participants. In this way, every message is backed up by every user, so should one archive go down, the content can be pushed to a different archive then pulled by other users of that archive. This means that even if all of the users and archives delete a message, as long as one person has it and there is one pushable archive, the message will be redistributed to every user.|$|R
50|$|Reports {{defined by}} RDL can be {{downloaded}} {{to a variety of}} formats including Excel, PDF, CSV, XML, TIFF (and other image formats), and HTML Web <b>Archive.</b> SQL <b>Server</b> 2008 and 2012 SSRS can also prepare reports in Microsoft Word (DOC) format, while third-party report generators offer additional output formats.|$|R
50|$|X-No-Archive, {{also known}} colloquially as xna, is a {{newsgroup}} message header field {{used to prevent}} a Usenet message from being <b>archived</b> in various <b>servers.</b>|$|R
40|$|Abstract: In 2004, a city-wide weather {{wireless}} sensor network, Taipei Weather Inquiry-Based Learning Network, {{composed of}} sixty school-based weather sensor nodes and a centralized weather <b>archive</b> <b>server</b> {{was established to}} facilitate students having weather science inquiry-based learning. The network covers the whole Taipei City, collects the city’s weather status, and opens the weather data to the general public. A series of annual weather inquiry-based learning tournaments was held since 2006 to engage the students to use the network’s resource. Until now, there have been 171 registered teams which include 447 grade 4 - 9 students and 220 teachers involved in it. The study of the tournaments data indicated that the usability of the network was satisfied...|$|E
40|$|International Telemetering Conference Proceedings / October 22 - 25, 2001 / Riviera Hotel and Convention Center, Las Vegas, NevadaA {{traditional}} Front-end Processor (FEP) {{with local}} RAID storage can limit the operational throughput of a high-rate telemetry ground station. The Front-end processor must perform pass processing (frame synchronization, decoding, routing, and storage), post-pass processing (level-zero processing), and tape archiving. A typical fifteen minute high-rate satellite pass can produce data files of 10 to 20 GB. The FEP may require up to 2 hours {{to perform the}} post-pass processing and tape archiving functions for these size files. During this time, it is not available to support real-time pass operations. Honeywell faced this problem {{in the design of}} the data management system for the DataLynx ä* ground stations. Avtec Systems, Inc. and Honeywell worked together to develop a data management system that utilizes a Storage Area Network (SAN) in conjunction with multiple High-speed Front-end Processors (HSFEP) for Pass Processing (PFEP), multiple HSFEPs for Post-pass Processing (PPFEP), and a dedicated Tape <b>Archive</b> <b>server.</b> A SAN consists of a high-capacity, high-bandwidth shared RAID that is connected to multiple nodes using 1 Gbps Fibre Channel interfaces. All of the HSFEPs as well as the Tape <b>Archive</b> <b>server</b> have direct access to the shared RAID via a Fibre Channel network. The SAN supports simultaneous read/write transfers between the nodes at aggregate rates up to 120 Mbytes/sec. With the Storage Area Network approach, the High-Speed Front-end Processors can quickly transfer the data captured during a pass to the shared RAID for post-processing and tape archiving so that they are available to support another satellite pass. This paper will discuss the architecture of the Storage Area Network and how it optimizes ground station data management in a high-rate environment...|$|E
40|$|Within the European HARP project, the HARP Cross Security Platform (HCSP) {{has been}} {{specified}} {{to design and}} to implement trustworthy distributed applications for health over the open Internet enabling both communication and application security services. Certified servlets composed and attributed according to the user's authorisation create certified and signed XML messages. From those messages, user-role-related applets are generated. The HCSP consists of a client environment, web server, an application server, {{as well as a}} database server and an <b>archive</b> <b>server.</b> The needed Privilege Management Infrastructure (PMI) has been established by an Attribute Authority and a policy server. The HCSP components are distributed installed over all countries involved. The role-based authorization has been defined according to the policy deploying the user's attribute certificates. The HARP solution has been practically implemented for a Clinical Study demonstrator. © 2003 Elsevier B. V. All rights reserved...|$|E
5000|$|Voicing {{a strong}} {{reaction}} {{to the idea of}} books simply being thrown away, and inspired by the Svalbard Global Seed Vault, Kahle now envisions collecting one copy of every book ever published. [...] "We're not going to get there, but that's our goal", he said. Alongside the books, Kahle plans to store the Internet <b>Archive's</b> old <b>servers,</b> which were replaced in 2010.|$|R
30|$|One {{important}} information retrieval task is {{the search for}} patterns in the huge collection of time series data. This procedure is very I/O intensive, {{because we need to}} read the full respective data files for computing occurrences or similarities to the given pattern. Finding the most similar time series, matching a search pattern in millions of large files could take unacceptable duration to complete. Processing huge time series data with a standard PC architecture has many limitations. One is the decrease of processing speed whenever data to be processed does not fit in local RAM, which has a typical size of about 16 GB in current personal computers. Then, the next slower storage layer must be used, which is the hard disk. Again, as soon as the hard drive’s storage limit is reached, data must be fetched from <b>archive</b> <b>servers</b> over the Ethernet, which is again slower. Data I/O becomes the main bottleneck and not - like in other fields - computing effort.|$|R
40|$|The {{concept of}} the {{institutional}} repository is currently discussed in library science and in practice around the globe. Many universities hope for significant advantages caused by publishing servers collecting the outflow of their scientific institutions as electronic documents and providing them via internet. If nothing else they count on a huge advertising impact for their own institution. For this reason, in recent years, many universities and scientific institutions in Germany as {{in the rest of}} the world decided to build their own document server. From the beginning, there has been a close link between this approach and the international open-access movement. Not a few represent the view, both concepts could create a decisive influence on the changing system of scientific publishing. But contrary to the initial optimistic estimates, many operators of this type of <b>archive</b> <b>servers</b> today find themselves confronted with significant difficulties. In this regard primarily the lack of acceptance by the scientific authors proves to be a fundamental problem that may threaten the success of the concept...|$|R
40|$|Over {{the past}} few years, {{we have seen the}} {{proliferation}} of Internet-based ser-vices ranging from search engines and map services to video-on-demand servers. All of these kinds of services {{need to be able to}} provide certain guarantees of availability and scalability to their users. With millions of users on the Internet today, these services must have the capacity to handle a large number of clients and remain available even in the face of extremely high load. In this work, we present a generic architecture for supporting such Internet applications. We provide a substrate for Scalable Network Services (SNS), on top of which application developers can design their services without worrying about the details of service management. We back our design with three real-world services: a web distillation proxy, a proxy-based web-browser for PDAs, and an MBone <b>archive</b> <b>server.</b> 1...|$|E
40|$|The Sloan Digital Sky Survey (SDSS) Data <b>Archive</b> <b>Server</b> (DAS) {{provides}} {{public access}} to over 12 Tb of data in 17 million files produced by the SDSS data reduction pipeline. Many tasks which seem trivial when serving smaller, less complex data sets present challenges when serving data of this volume and technical complexity. The included output files should be chosen to support as much science as possible from publicly released data, and only publicly released data. Users must have the resources needed to read and interpret the data correctly. Server administrators must generate new data releases at regular intervals, monitor usage, quickly recover from hardware failures, and monitor the data served by the DAS both for contents and corruption. We discuss these challenges, describe tools we use to administer and support the DAS, and discuss future development plans...|$|E
40|$|Advances in robotic {{devices and}} storage media now make it {{possible}} to design near-line automated storage systems. These systems aim to provide responsive performance to users of tertiary storage devices. The Jaquith system is a prototype <b>archive</b> <b>server</b> that lets network users archive their own files using automated storage. It provides semi-interactive file access to its clients by combining a high-density robotic tape system with disk-based indexing. Jaquith presents an FTP interface whereby whole files are moved between the client and its storage archive. Each client's archive is separately governed to provide independent namespaces, added security, and parallel operation. A wildcard query mechanism lets users manipulate arbitrary subsets of their files. Two important aspects of the query system are abstracts, text tags that can be associated with files, and versions, date-stamps that are applied to archived files. Jaquith throughput is about 135 KB/second when archiving small (10 K [...] ...|$|E
5000|$|Users {{also have}} the ability to {{download}} the complete <b>archive</b> of a <b>server's</b> available files, commonly called a [...] "list" [...] due to the [...]txt format that the script's output code creates. To request a server's list, there is a separate 'list trigger' used.|$|R
40|$|The {{survey results}} of the MÉTA program are managed with {{centralised}} relational database management system (MS SQL 2000) developed and {{set up in a}} local area network. Besides the MÉTA database server, a publishing <b>server,</b> an <b>archiving</b> <b>server</b> and a GIS workstation were applied. The core information entities of the MÉTA database are: information sub-project, MÉTA quadrate, MÉTA hexagon, (semi-) natural habitat, potential vegetation with numerous habitats, landscape ecology and land use attributes, and surveyor. This informa-tion is coded in the nine main tables of the normalised database. In the recent state there are almost 1, 500, 000 records in the main tables that are managed in 241 independent fields. The published version of the MÉTA database supports the query service, and handles this infor-mation in 7 denormalised main tables. This much more redundant version is 11 GB in size. The 20. 6 % (179 man-month) of the human resources in the MÉTA program were devoted to the information tasks (set up and preparation, MÉTA database and information system de-velopment, replenishment and quality assessment, MÉTA query, GIS and printing ser-vices) between 2002 and 2007. The basic structure of the MÉTA database version 1. 2 is final-ised and the main functions regarding data processing have been developed. Th...|$|R
50|$|IBM DeepFlash-ESS is an {{integrated}} device combining {{one or two}} DeepFlash 150 drawers with IBM Spectrum Scale software for Exascale storage repositories with analytics capabilities (Hadoop, CCTV, analytics <b>archive,</b> media <b>server</b> etc.). The DeepFlash-ESS can be clustered non-disruptively with existing IBM Elastic Storage Servers, up to a theoretical limit of 8000 clustered devices. It features file (NFS, SMB), object (Swift, S3) and Hadoop transparent access. Spectrum Scale offers automated data placement and lifecycle management from Memory to Flash to Disk to Tape, besides geographically distributed caching and replication.|$|R
40|$|This project {{analyzed}} {{the effectiveness of}} a city-wide wireless weather sensor network, the Taipei Weather Science Learning Network (TWIN), in facilitating elementary and junior high students ’ study of weather science. The network, composed of sixty school-based weather sensor nodes and a centralized weather data <b>archive</b> <b>server,</b> provides students with current weather data at specific locations in the city. In 2006 - 2008, annual weather science tournaments were held to encourage students to use this resource, and up to now 171 registered teams, including 447 grade 4 - 9 students and 220 teachers, have participated in competitions. This study of the tournament data makes clear the over-all efficacy and usability of the network. An analysis of the students’ weather science ability demonstrated that they could perform well in the questioning phase, the planning phase and the analyzing phase but not as well in the interpreting phase of their specific weather-science inquires...|$|E
40|$|OF THE THESIS <b>ARCHIVE</b> <b>SERVER</b> FOR REAL-TIME COLLABORATION IN DISCIPLE by Dawei Wu Thesis Director: Professor James L. Flanagan Our goal is {{to create}} a server which {{coordinates}} the work of clients in the DISCIPLE synchronous groupware system, documents the session history, and maintains a database. This thesis work develops method for interactive delayed-sharing of a session in the DISCIPLE groupware to allow reuse of such session at a later time. The actions of each client are saved to streams which represent temporally-ordered input action sequences in the shared workspace. The playback of these actions is realized by re-execution of their relevant streams. The contributions of this thesis are as follows: 1. The mechanism for object persistence is implemented by using CORBA Externalization Service. Thus, we can save all clients' actions into data streams in the permanent storage and replay them at a later time. 2. An object database is created and integrated with the data model in the [...] ...|$|E
40|$|We {{describe}} the design model {{and implementation of}} the data archiving system used by the Berkeley [...] Illinois [...] Maryland Association (BIMA) Millimeter Array Telescope. This system transmits data in real-time from the BIMA telescope at the Hat Creek Observatory in northern California via the Internet to an <b>archive</b> <b>server</b> at the National Center for Supercomputing Applications (NCSA) in Urbana, Illinois. Once at NCSA, the data undergo minor processing to make it available to astronomers via an HTML interface: (1) the data are checked for successful transmission, (2) metadata are extracted from the data and inserted into a searchable database, and (3) the data are sent to the NCSA Mass Storage System. When necessary, the system can carry out rollback operations which allow it to easily recover from errors, particularly those associated with the often unstable Internet. We also comment on some ways in which the system can be improved and expanded to adapt to changing observing strategies. Ke [...] ...|$|E
40|$|Mark Turin {{has agreed}} with the {{copyright}} holder the right to host {{a copy of this}} piece (whether audio, text or video) on University of Cambridge <b>archives</b> and <b>servers.</b> Of the world's 6, 500 living languages, half will cease to be spoken {{by the end of this}} century. Dr Mark Turin, director of the World Oral Literature Project, has spent much of his life travelling to remote corners of the Himalayas to study languages and cultures that are at risk and document them before they disappear without record...|$|R
50|$|Alternatively, a {{supplementary}} mechanism may {{transfer the}} archived redo logs. On the standby database a Fetch Archive Log (FAL) client monitors for {{gaps in the}} sequence of received logs. If it finds a gap, it may invoke one or more Fetch <b>Archive</b> Log (FAL) <b>servers</b> to run on the primary database to forward the missing item(s).|$|R
40|$|Here {{we present}} the New York University Value-Added Galaxy Catalog (NYU-VAGC), {{a catalog of}} local galaxies (mostly below a {{redshift}} of about 0. 3) based {{on a set of}} publicly-released surveys (including the 2 dFGRS, 2 MASS, PSCz, FIRST, and RC 3) matched to the Sloan Digital Sky Survey (SDSS) Data Release 2. Excluding areas masked by bright stars, the photometric sample covers 3514 square degrees and the spectroscopic sample covers 2627 square degrees (with about 85 % completeness). Earlier, proprietary versions of this catalog have formed the basis of many SDSS investigations of the power spectrum, correlation function, and luminosity function of galaxies. We calculate and compile derived quantities (for example, K-corrections and structural parameters for galaxies). The SDSS catalog presented here is photometrically recalibrated, reducing systematic calibration errors across the sky from about 2 % to about 1 %. We include an explicit description of the geometry of the catalog, including all imaging and targeting information as a function of sky position. Finally, we have performed eyeball quality checks on a large number of objects in the catalog in order to flag deblending and other errors. This catalog is complementary to the SDSS <b>Archive</b> <b>Servers,</b> in that NYU-VAGC's calibration, geometrical description, and conveniently small size are specifically designed for studying galaxy properties and large-scale structure statistics using the SDSS spectroscopic catalog. Comment: accepted by AJ; full resolution version available at [URL] data files available at [URL]...|$|R
