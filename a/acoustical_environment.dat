129|75|Public
500|$|Oxman {{has also}} premiered new {{printing}} tools and processes. In 2015, she designed Gemini, a large chaise lounge combining a milled wood shell with a 3D-printed surface. [...] Both {{the outer shell}} and {{the texture of the}} inner surface were designed to produce a soothing <b>acoustical</b> <b>environment</b> for someone reclining in it. Gemini was later acquired by SF MoMA.|$|E
50|$|The {{most common}} driver type is an {{electro-mechanical}} transducer using a voice coil rigidly {{connected to a}} diaphragm (generally a cone). Other types have similar connections, though differing in detail, between their <b>acoustical</b> <b>environment</b> and their electrical properties.|$|E
50|$|On {{location}} recording is {{more like}} film recording than traditional, studio-based radio drama. Actors, director, and sound recordist go to an <b>acoustical</b> <b>environment</b> similar to that referenced in the play, and record in stereo to a compact flash recording device.|$|E
40|$|This paper {{reports on}} a project for {{collection}} of the sound scene data. The sound scene data is necessary for studies such as sound source localization, sound retrieval, sound recognition and hands-free speech recognition in real <b>acoustical</b> <b>environments.</b> There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources and room acoustics. The number of combination of the sound sources, source positions and rooms is huge in real <b>acoustical</b> <b>environments.</b> However, the sound in the environments can be simulated by convolution of the isolated sound sources and impulse responses. As an isolated sound source, a hundred kinds of non-speech sounds and speech sounds are collected. The impulse responses are collected in various <b>acoustical</b> <b>environments.</b> In this paper, progress of our sound scene database project and application to environment sound recognition are described. 1...|$|R
40|$|EUROSPEECH 1999 : the 6 th European Conference on Speech Communication and Techinology, September 5 - 9, 1999, Budapest, Hungary. This paper {{describes}} a sound scene database necessary for studies such as sound source localization, sound re trieval, sound recognition and hands-free speech recognition in real <b>acoustical</b> <b>environments.</b> This paper {{reports on a}} project for collection of the sound scene data supported by Real World Computing Partnership(RWCP). There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources and room acoustics. The number of combination of the sound sources, source positions and rooms is huge in real <b>acoustical</b> <b>environments.</b> Two approaches are taken to build the sound scene database in the early stage of the project. The rst approach is to collect isolated sound sources of many kinds of non-speech sounds and speech sounds. The second approach is to collect impulse responses in various <b>acoustical</b> <b>environments.</b> The sound in the environments can be simulated by convolution of the isolated sound sources and impulse responses. In a later stage, the sound scene data in real <b>acoustical</b> <b>environments</b> is planned to be collected using a three dimensional microphone array. In this paper, the plan and progress of our sound scene database project are described...|$|R
40|$|LREC 2000 : the 2 nd International Conference on Language Resources and Evaluation, May 31 - June 2, 2000, Athens, Greece. This paper {{reports on}} a project for {{collection}} of the sound scene data. The sound scene data is necessary for studies such as sound source localization, sound retrieval, sound recognition and hands-free speech recognition in real <b>acoustical</b> <b>environments.</b> There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources and room acoustics. The number of combination of the sound sources, source positions and rooms is huge in real <b>acoustical</b> <b>environments.</b> However, the sound in the environments can be simulated by convolution of the isolated sound sources and impulse responses. As an isolated sound source, a hundred kinds of non-speech sounds and speech sounds are collected. The impulse responses are collected in various <b>acoustical</b> <b>environments.</b> In this paper, progress of our sound scene database project and application to environment sound recognition are described...|$|R
50|$|Oxman {{has also}} premiered new {{printing}} tools and processes. In 2015, she designed Gemini, a large chaise lounge combining a milled wood shell with a 3D-printed surface. Both {{the outer shell}} and {{the texture of the}} inner surface were designed to produce a soothing <b>acoustical</b> <b>environment</b> for someone reclining in it. Gemini was later acquired by SF MoMA.|$|E
50|$|Omaha has a {{thriving}} performing arts community {{that includes the}} Magic Theatre, a 40-year-old experimental theatre, along with Astro Theatre, which hosts the Emmy Gifford Children's Theater. The Holland Performing Arts Center was built in 2005 with the overwhelming support and generosity of the Omaha community. The Holland Center specializes in events requiring a more <b>acoustical</b> <b>environment,</b> including performances by the Omaha Symphony.|$|E
50|$|Sound masking must {{satisfy the}} persons that listen to it. People ask {{themselves}} {{a number of}} questions about the <b>acoustical</b> <b>environment.</b> The following questions were deduced from employee comments about their office environment. These are questions the listeners implicitly ask themselves to determine their response to their environment. The design of a sound masking system must take these opinions into account.|$|E
5000|$|The latest DOD LPC-10 {{algorithm}} (V58) {{which has}} been enhanced to provide high-quality secure narrowband voice for military handsets and to maintain that quality and intelligibility in noisy <b>acoustical</b> <b>environments.</b>|$|R
40|$|This paper {{describes}} a sound scene database necessary for studies such as sound source localization, sound retrieval, sound recognition and speech recognition in real <b>acoustical</b> <b>environments.</b> Many speech databases {{have been collected}} for speech recognition so far. The statistical modeling of speech based on the collected speech databases realizes a drastic improvement of speech recognition performance. However, {{there are only a}} few databases available for sound scene data including non-speech sound in real environments. A sound scene database is obviously necessary for studies of acoustical signal processing and sound recognition. This paper reports on d project for collection of the sound scene database supported by Real World Computing Partnership (RWCP). There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources and room acoustics. The number of combination of the sound sources, source positions and rooms is huge in real <b>acoustical</b> <b>environments.</b> Two approaches are taken to build the sound scene database in the early stage of the project. The first approach is to collect isolated sound sources of many kinds of non-speech sounds and speech sounds. The second approach is to collect impulse responses in various <b>acoustical</b> <b>environments.</b> The sound in the collected environments can be simulated by convolution of the isolated sound sources and impulse responses. In a later stage, the sound scene data in real <b>acoustical</b> <b>environments</b> is planned to be collected using a three dimensional microphone array. In this paper, the plan and progress of our sound scene database project are described...|$|R
40|$|Intelligibility {{of speech}} {{evaluated}} for specified <b>acoustical</b> <b>environments.</b> Program makes systematic prediction of {{noise and vibration}} environment of craft defined by user and evaluates relative acceptability of predicted environment for effective communication by speech. Written in MicroSoft FORTRAN Version 3. 3...|$|R
50|$|Over the years, Artec {{became one}} of the most sought after {{acoustical}} consulting firms because of their unique movable and automatic technology that helped revolutionize the field. Their design enable through movable overhead canopies, retractable draperies and associated features, the adjustment of the <b>acoustical</b> <b>environment</b> to meet the needs of different kinds of music that could suit different performances or groups. The reflectors combined with a traditional shoebox shape design are consider trademarks of Artec.|$|E
50|$|Bufano {{gave the}} Trompeta Majestatis, built by Möller and voiced by Adolph Zajic, {{in memory of}} his mother in 1978. The Grand Chorus {{division}} was added two years later (1980), and a complete new principal chorus was installed in the chancel Pedal. In 1994, a Solid State Logic multi-level combination action was installed and the console was completely rewired. In summer 1995, the dry acoustics were improved when 10 coats of sealant were applied to the ceiling. During 1995-96, organ curator Robert Pearson supervised the complete cleaning, tuning, and revoicing of the organ to suit the new <b>acoustical</b> <b>environment.</b> The organ is the 14th largest in the world.|$|E
5000|$|Nothing {{is known}} about his life prior to his arrival at Venice, but he was {{probably}} born at Udine sometime before {{the middle of the}} 16th century. He was first hired by the musical establishment of St Mark's Basilica on 29 January 1568, along with his two brothers, Giovanni and Nicolò, where they formed the first permanent instrumental ensemble [...] The sonorous <b>acoustical</b> <b>environment</b> of this basilica was the center of activity of the Venetians. Giovanni Gabrieli clearly had Dalla Casa's group in mind for much of his music, and the Dalla Casas are presumed to have played in many the elaborate polychoral compositions of the time.|$|E
40|$|New {{emerging}} multimedia standards, {{like the}} MPEG- 4 standard allow {{the creation of}} virtual or synthetic <b>acoustical</b> <b>environments.</b> To auralize an environment from its geometric description, simulation algorithms for acoustic wave propagation are required. The different methods used for computational modeling of room acoustics {{can be divided into}} three groups [1]: statistical models...|$|R
25|$|Some of his masses are polychoral, a {{technique}} which involved multiple, spatially separated groups of singers. While {{this was also}} {{a technique}} which developed in Venice, it was widespread {{by the end of}} the 16th century: almost all composers of sacred polyphony used polychoral techniques at some time, especially those working in large <b>acoustical</b> <b>environments</b> (such as most cathedrals in Europe).|$|R
40|$|Numerous {{studies are}} {{currently}} {{focusing on the}} modelling of humans and their behavior in 3 D environments. The CINEMA project has similar goals but differentiates itself by aiming at enhanced interactions, the creation of mixed reality, {{and the creation of}} interactive and reactive <b>acoustical</b> <b>environments.</b> Part of the project consists in gesture recognition of a user, who is given the real-time control of auralization and audio spatialization processes...|$|R
50|$|The first {{element in}} the signal flow is the vocalist, which {{produces}} the signal. This signal propagates acoustically to the microphone according to the Inverse-square law, where it is converted by a transducer into an electrical signal. Other objects may also produce sound in the <b>acoustical</b> <b>environment,</b> such as HVAC systems, computer fans, traffic noise, elevators, plumbing, etc. These noise sources can also {{be picked up by}} the microphone. It is therefore important to optimize the acoustical signal/noise ratio at the microphone. This can be accomplished by reducing the amplitude of unwanted noise (for example, turning off the HVAC system while recording), or by taking advantage of the inverse-square law; by moving the microphone closer to the signal source and farther away from any noise sources, the signal/noise ratio is increased.|$|E
5000|$|Named after Jack Singer (thanks to a $1.5M {{contribution}} from his sons, Alan Singer {{and the late}} Stephen Singer), the concert hall opened its doors in 1985. The Jack Singer Concert Hall {{is located at the}} Arts Commons, a performing arts complex occupying 10 acres (4.5 hectares) in downtown Calgary. [...] The Jack Singer Concert Hall is described {{as one of the most}} beautiful and acoustically acclaimed venues in North America. Artec consultants designed the adjustable acoustics system that includes a canopy, clad in laminated spruce wood. Suspended high above the stage and weighing 185,000-pounds (90-tons), this massive acoustical reflector can be raised or lowered to adjust the <b>acoustical</b> <b>environment</b> to meet the needs of the broad spectrum of music performed in the hall. [...] Another prominent feature is the Carthy Organ. [...] As one of Canada’s largest pipe organs, the Carthy Organ is hand-carved from solid oak and contains more than 1600 polished alloy and wood pipes. The acquisition of the organ was made possible through the Carthy Foundation, thanks to a $750,000 donation from the Mannix Family. [...] With 2021 seats, the Jack Singer Concert Hall is home to the Calgary Philharmonic Orchestra, BD&P World Music, and TD Jazz. [...] The hall also welcomes a spectrum of events each season from TED (conference) talks and National Geographic speakers to wedding dinners on the stage and rock stars on tour.|$|E
3000|$|... setting {{according}} to the <b>acoustical</b> <b>environment,</b> but in principle {{it should be possible}} that this adjustment is made in the hearing aid {{according to}} the <b>acoustical</b> <b>environment</b> with the sound classifiers installed in modern hearing aids.|$|E
30|$|Obviously, {{an optimal}} STFT frame size may exist for a {{specific}} reverberation. However, due to complex <b>acoustical</b> <b>environments</b> and varieties of source signals, {{it is difficult to}} determine this value precisely. How to choose an appropriate frame length may be a topic of our future research. Generally, 1024 or 2048 can be used as a common frame length. Here we use an analysis frame length of 2048 for all reverberant conditions in the remaining experiments.|$|R
40|$|Voice {{activity}} detection (VAD) is {{an important}} enabling technology {{for a variety of}} speech-based applications including speech recognition, speech encoding, and hands-free telephony. The primary function of a voice activity detector is to pro-vide an indication of speech presence in order to facilitate speech processing as well as possibly provide delimiters for {{the beginning and end of}} a speech segment. While VAD is often quite effective in benign <b>acoustical</b> <b>environments,</b> e. g. a con...|$|R
40|$|This {{dissertation}} {{describes a}} number of algorithms developed to increase the robustness of automatic speech recognition systems with respect {{to changes in the}} environment. These algorithms attempt to improve the recognition accuracy of speech recognition systems when they are trained and tested in different <b>acoustical</b> <b>environments,</b> and when a desk-top microphone (rather than a close-talking microphone) is used for speech input. Without such processing, mismatches between training and testing conditions produce an unacceptable degradation in recognition accuracy. Two kinds o...|$|R
40|$|International audienceIn {{previous}} studies, the localisation {{accuracy and}} the spatial impression of 3 - 2 stereo microphone arrays were discussed. These showed that 3 - 2 stereo cannot produce stable images {{to the side}} and {{to the rear of the}} listener. An octagon loudspeaker array was therefore proposed. Microphone array design for this loudspeaker configuration was studied in terms of localisation accuracy, locatedness and sound image width. This paper describes an experiment conducted to evaluate the spaciousness of 10 different microphone arrays used in different acoustical environments. Spaciousness was analyzed as a function of sound signal, <b>acoustical</b> <b>environment</b> and microphone array's characteristics. It showed that the height of the microphone array and the original <b>acoustical</b> <b>environment</b> are the two variables that have the most influence on the perceived spaciousness, but that microphone directivity and the position of sound sources is also important...|$|E
40|$|We {{listen to}} speech (as {{well as to}} other sounds) with two ears, and it is quite {{remarkable}} how well we can separate and selectively attend to individual sound sources in a cluttered <b>acoustical</b> <b>environment.</b> In fact, the familiar term ‘cocktail party processing ’ was coined in an early study of how the binaural system enables us to selectively attend to individual conversation...|$|E
40|$|Asserting {{that without}} an {{adequate}} <b>acoustical</b> <b>environment,</b> learning {{activities can be}} hindered, this paper reviews the literature on classroom acoustics, particularly noise, reverberation, signal-to-noise ratio, task performance, and recommendations for improvement. Through this review, the paper seeks to determine whether portable classrooms provide acoustically adequate environments for learning. (Contains 63 references.) (EV) Reproductions supplied by EDRS are the best {{that can be made}} from the original document...|$|E
40|$|Article {{published}} pre-print in "Newly published" {{section of}} JSLHR: [URL] We describe the natural auditory environment {{of people with}} cochlear implants (CIs), how it changes across the life span, and how it varies between individuals. Method. We performed a retrospective cross-sectional analysis of Cochlear Nucleus 6 CI sound-processor data logs. The logs were obtained from 1, 501 people with CIs (ages 0 – 96 years). They covered over 2. 4 million hr of implant use and indicated how much time the CI users had spent in various <b>acoustical</b> <b>environments.</b> We investigated exposure to spoken language, noise, music, and quiet, and analyzed variation between age groups, users, and countries. Results. CI users spent {{a substantial part of}} their daily life in noisy environments. As a consequence, most speech was presented in background noise. We found significant differences between age groups for all auditory scenes. Yet even within the same age group and country, variability between individuals was substantial. Conclusions. Regardless of their age, people with CIs face challenging <b>acoustical</b> <b>environments</b> in their daily life. Our results underline the importance of supporting them with assistive listening technology. Moreover, we found large differences between individuals' auditory diets that might contribute to differences in rehabilitation outcomes. Their causes and effects should be investigated further. status: publishe...|$|R
40|$|Thermal, <b>acoustical,</b> and {{acceleration}} <b>environments</b> of XB- 70 airplane {{crew compartment}} in airworthiness tests. Prepared at Flight Research Center. "October 1969. "Cover title. Includes bibliographical references (p. 22). Thermal, <b>acoustical,</b> and acceleration <b>environments</b> of XB- 70 airplane crew compartment in airworthiness tests. Mode of access: Internet...|$|R
40|$|The {{location}} of an acoustical source {{can be found}} robustly using the Steered Response Pattern- Phase Transform (SRP-PHAT) al-gorithm. However SRP-PHAT can be computationally expensive, requiring a search {{of a large number}} of candidate locations. The required spacing between these locations is dependent on sam-pling rate, microphone array geometry, and source location. In this work, a novel method will be presented that calculates a smaller number of test points using an efficient closed-form localization algorithm. This method significantly reduces the number of calcu-lations, while still remaining robust in <b>acoustical</b> <b>environments.</b> 1...|$|R
40|$|Includes bibliographical {{references}} (pages 123 - 126) The Acoustical {{characteristics of}} a rehearsal room are of primary importance to the music teacher. From the sound environment that surrounds the teacher and his students as based the judgments and evaluations of the music being rehearsed. If this <b>acoustical</b> <b>environment</b> is faulty, {{the analysis of the}} music being rehearsed can also be faulty and misleading. See more in text...|$|E
40|$|This Digest {{examines}} the <b>acoustical</b> <b>environment</b> {{of an open}} office plan, and discusses its problems and the elements that can be utilised to obtain acceptable results. A design procedure follows that allows the condition at any point within an open office to be determined at the planning stage. Aussi disponible en fran 7 ais : Insonorisation des bureaux sans cloisonsPeer reviewed: NoNRC publication: Ye...|$|E
40|$|Two {{objective}} {{measurement techniques}} {{have been proposed}} that relate the fluctuations in interaural time difference {{to one or more}} attributes of subjective spatial perception. This paper reviews these measurements, discusses how these fluctuations may be created in a real <b>acoustical</b> <b>environment,</b> summarises the experiments carried out to elicit the subjective effect of the fluctuations, and suggests ways in which this research can be applied to sound reproduction. </p...|$|E
40|$|In {{this paper}} we {{describe}} {{and compare the}} performance of a series of cepstrum-based procedures that enable the CMU SPHINX-II speech recognition system to maintain a high level of recognition accuracy over a wide variety of <b>acoustical</b> <b>environments.</b> We describe the MFCDCN algorithm, an environment-independent extension of the efficient SDCN and FCDCN algorithms developed previously. We compare the performance of these algorithms with the very simple RASTA and cepstral mean normalization procedures, describing the performance of these algorithms {{in the context of the}} 1992 DARPA CSR evaluation using secondary microphones, and in the DARPA stress-test evaluation. 1...|$|R
40|$|Abstract. In {{recent years}} many {{approaches}} {{have been developed}} {{to address the problem of}} robust speaker recognition in adverse <b>acoustical</b> <b>environments.</b> In this paper we propose a robust auditory-based feature extraction method for speaker recognition according to the characteristics of the auditory periphery and cochlear nucleus. First, speech signals are represented based on frequency selectivity at basilar membrane and inner hair cells. Then, features are mapped into different linear subspaces using independent subspace analysis (ISA) method, which can represent some high order, invariant statistical features by maximizing the inde-pendence between norms of projections. Experiment results demonstrate that our method can considerably increase the speaker recognition accuracy specifically in noisy environments. ...|$|R
40|$|Abstract: Since the {{dimensions}} {{of many of the}} objects/surfaces encountered in our daily lives are within an order of magnitude of the wavelength of audible sounds, diffraction is an elementary means of sound propagation. Despite its importance in the real-world, diffraction effects are often overlooked by acoustical modeling methods leading to a degredation in immersion or presence. This paper describes an acoustical diffraction method based on the Huygens-Fresnel principle. The method is simple and efficient allowing it to be incorporated in interactive <b>acoustical</b> <b>environments</b> including virtual environments. Experimental results are presented that illustrate the performance and effectiveness of the method and its conformance to theoretical diffraction models. ...|$|R
