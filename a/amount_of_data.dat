10000|10000|Public
5|$|Some {{studies have}} {{reported}} reductions in numbers of cells or volume of tissue, but the <b>amount</b> <b>of</b> <b>data</b> relating to this question is not very large.|$|E
5|$|Data {{is stored}} in a 160Gb (20GB) flash memory module {{consisting}} of over 700 memory chips, each with a 256Mbit capacity. This memory capacity is not actually that large considering the <b>amount</b> <b>of</b> <b>data</b> to be acquired; for example, a single image from the HiRISE camera can be as large as 28Gb.|$|E
5|$|One major {{advantage}} of genome-scale RNAi screening {{is its ability}} to simultaneously interrogate thousands of genes. With the ability to generate a large <b>amount</b> <b>of</b> <b>data</b> per experiment, genome-scale RNAi screening has led to an explosion data generation rates. Exploiting such large data sets is a fundamental challenge, requiring suitable statistics/bioinformatics methods. The basic process of cell-based RNAi screening includes the choice of an RNAi library, robust and stable cell types, transfection with RNAi agents, treatment/incubation, signal detection, analysis and identification of important genes or therapeutical targets.|$|E
40|$|Soft {{computing}} {{methods such as}} fuzzy control, neural networks, etc., {{often require}} lots of computations even for small <b>amounts</b> <b>of</b> <b>data.</b> It is, therefore, sometimes believed that for larger <b>amounts</b> <b>of</b> <b>data,</b> the required <b>amount</b> <b>of</b> computations will be so large that we will reach the frontiers of soft computing. In this paper, we show, on the example of hyper-spectral satellite imaging, that this belief is often too pessimistic. We should not be afraid to use (or at least to try to use) soft computing methods even for large <b>amounts</b> <b>of</b> <b>data...</b>|$|R
30|$|High scalability: We {{are able}} to {{retrieve}} the behavior pattern for different <b>amounts</b> <b>of</b> <b>data</b> since the Morisita index provides the scalability adapted to different <b>amounts</b> <b>of</b> tracking <b>data.</b>|$|R
5000|$|One well-constructed {{experiment}} produces enormous <b>amounts</b> <b>of</b> <b>data.</b>|$|R
5|$|Although {{the link}} between {{performance}} in chess and general intelligence is often assumed, researchers have largely failed to confirm its existence. For example, a 2006 study found no differences in fluid intelligence, as measured by Raven's Progressive Matrices, between strong adult chess players and regular people. There is some evidence towards a correlation between performance in chess and intelligence among beginning players. However, performance in chess also relies substantially on one's experience playing the game, {{and the role of}} experience may overwhelm the role of intelligence. Chess experts are estimated to have in excess of 10,000 and possibly as many as 300,000 position patterns stored in their memory; long training is necessary to acquire that <b>amount</b> <b>of</b> <b>data.</b>|$|E
5|$|Ancestors were worshiped in Qatna; {{the royal}} hypogeum {{provided}} a large <b>amount</b> <b>of</b> <b>data</b> concerning {{the cult of}} ancestor worshiping and the practices associated with it. Two kinds of burials are distinguished; a primary burial intended to transport the dead into the netherworld, and a secondary burial that was intended to transform the deceased into their ultimate form: an ancestor. The royal hypogeum provides hints at the different rituals taking place during a secondary burial; a noticeable character is that skeletons were not complete, and no skulls are found {{for the majority of}} secondary burial remains. There is no evidence that skulls decayed as they would have left behind teeth, of which very few were found, indicating that the skulls were removed to be venerated in another location.|$|E
5|$|Despite {{the lack}} of {{close-up}} imaging and mechanical problems that greatly restricted the <b>amount</b> <b>of</b> <b>data</b> returned, several significant discoveries were made during Galileos primary mission. Galileo observed {{the effects of a}} major eruption at Pillan Patera and confirmed that volcanic eruptions are composed of silicate magmas with magnesium-rich mafic and ultramafic compositions. Distant imaging of Io was acquired for almost every orbit during the primary mission, revealing large numbers of active volcanoes (both thermal emission from cooling magma on the surface and volcanic plumes), numerous mountains with widely varying morphologies, and several surface changes that had taken place both between the Voyager and Galileo eras and between Galileo orbits.|$|E
40|$|Soft {{computing}} {{methods such as}} fuzzy control, neural networks, etc., {{often require}} lots of com-putations even for small <b>amounts</b> <b>of</b> <b>data.</b> It is, therefore, sometimes believed that for larger <b>amounts</b> <b>of</b> <b>data,</b> the required <b>amount</b> <b>of</b> com-putations will be so large that we will reach the frontiers of soft computing. In this paper, we show, on the example of hyper-spectral satellite imaging, that this belief is often too pessimistic. We should not be afraid to use (or at least to try to use) soft computing methods even for large <b>amounts</b> <b>of</b> <b>data.</b> The problem: it looks like soft computing is approaching its frontier...|$|R
40|$|Many space {{applications}} such as sensor networks, on-board satellite-based platforms, on-board vehicle monitoring systems, etc. handle large <b>amounts</b> <b>of</b> <b>data</b> and analysis <b>of</b> such <b>data</b> is often critical for the scientific mission. Transmitting such large <b>amounts</b> <b>of</b> <b>data</b> to the remote control station for analysis is usually too expensive for time-critical applications. Instead, modern spac...|$|R
5000|$|Web storage (simpler {{standard}} for smaller <b>amounts</b> <b>of</b> <b>data</b> without indices) ...|$|R
25|$|With {{a greater}} <b>amount</b> <b>of</b> <b>data</b> being collected, how systems must will be {{established}} for the categorizing and storing of information.|$|E
25|$|Time {{to access}} {{data can be}} {{improved}} by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the <b>amount</b> <b>of</b> <b>data</b> under a set of heads, thereby potentially reducing seek activity for a given <b>amount</b> <b>of</b> <b>data.</b> The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.|$|E
25|$|The {{need for}} {{reproducibility}} and efficient {{management of the}} large <b>amount</b> <b>of</b> <b>data</b> associated with genome projects mean that computational pipelines have important applications in genomics.|$|E
5000|$|Scalability: Does the {{classifier}} function efficiently {{with large}} <b>amounts</b> <b>of</b> <b>data?</b> ...|$|R
40|$|The Information and Communication Technologies {{revolution}} {{brought a}} digital world with huge <b>amounts</b> <b>of</b> <b>data</b> available. Enterprises use mining technologies to search vast <b>amounts</b> <b>of</b> <b>data</b> for vital insight and knowledge. Mining {{tools such as}} data mining, text mining, and web mining are used to find hidden knowledge in large databases or the Internet. Comment: 21 page, journal pape...|$|R
5000|$|In fact, vector {{processors}} work best {{only when there}} are large <b>amounts</b> <b>of</b> <b>data</b> to be worked on. For this reason, these sorts of CPUs were found primarily in supercomputers, as the supercomputers themselves were, in general, found {{in places such as}} weather prediction centres and physics labs, where huge <b>amounts</b> <b>of</b> <b>data</b> are [...] "crunched".|$|R
25|$|Opera Software {{claims that}} when the Opera Turbo mode is enabled, the {{compression}} servers compresses requested web pages (excepts HTTPS pages) by up to 50%, depending on the content, before sending them to the users. This process reduces the <b>amount</b> <b>of</b> <b>data</b> transferred and is particularly useful for crowded or slow network connections, making web pages load faster or when there are costs dependent for the total <b>amount</b> <b>of</b> <b>data</b> usage. This technique is also used in Opera Mini for mobile devices and smartwatches.|$|E
25|$|An {{example of}} an amplified DDoS attack through NTP is through a command called monlist, which sends {{the details of the}} last 600 people who have {{requested}} the time from that computer back to the requester. A small request to this time server can be sent using a spoofed source IP address of some victim, which results in 556.9 times the <b>amount</b> <b>of</b> <b>data</b> that was requested back to the victim. This becomes amplified when using botnets that all send requests with the same spoofed IP source, which will send a massive <b>amount</b> <b>of</b> <b>data</b> back to the victim.|$|E
25|$|Since Bluetooth devices {{become more}} {{prevalent}} on board vehicles {{and with more}} portable electronics broadcasting, the <b>amount</b> <b>of</b> <b>data</b> collected over time becomes more accurate and valuable for travel time and estimation purposes.|$|E
2500|$|Streams, usually {{containing}} large <b>amounts</b> <b>of</b> <b>data,</b> {{which can}} be compressed and binary ...|$|R
5000|$|Structured append (linking {{of up to}} 16 symbols {{to encode}} larger <b>amounts</b> <b>of</b> <b>data)</b> ...|$|R
5000|$|Seismological {{instruments}} {{can generate}} large <b>amounts</b> <b>of</b> <b>data.</b> Systems for processing such data include: ...|$|R
25|$|As it is a {{research}} {{as well as an}} operational radar, the large <b>amount</b> <b>of</b> <b>data</b> accumulated is studied for further development in radar hardware and software capabilities. The data are correlated with the other instruments in related research.|$|E
25|$|Since the 1980s a {{long-term}} project directed by Miguel Rivera Dorado (of Madrid, Spain) {{has produced a}} vast <b>amount</b> <b>of</b> <b>data</b> on Oxkintok. Most recently, Mexico's INAH has invested in excavations and reconstructions at the site (under the direction of Ricardo Velazquez Valadez).|$|E
25|$|Opera Mini can run in Turbo and Uncompressed modes, in {{addition}} to Mini mode. In Turbo mode, the <b>amount</b> <b>of</b> <b>data</b> transferred is still much reduced by compression, but, unlike Mini mode, JavaScript is not intercepted by the server and works properly.|$|E
40|$|An {{important}} goal of visualization technology {{is to support}} the exploration and analysis <b>of</b> very large <b>amounts</b> <b>of</b> <b>data</b> which are usually stored in databases. Since number {{and size of the}} databases is growing rapidly, {{there is a need for}} novel visualization techniques which allow a visualization <b>of</b> larger <b>amounts</b> <b>of</b> <b>data.</b> Most <b>of</b> today’s databases store typical transaction-generated multi...|$|R
40|$|Virtual environments present {{opportunities}} for novel interaction with and visualisation <b>of</b> abstract <b>data.</b> However, the inherent difficulties of rendering 3 D environments, {{as well as}} bottlenecks in traditional virtual reality systems, make real-time manipulation <b>of</b> realistic <b>amounts</b> <b>of</b> <b>data</b> difficult. We present QSPACE, a data visualisation tool with which we have begun to explore techniques and architectures for making real-time interaction with largest <b>amounts</b> <b>of</b> <b>data</b> possible...|$|R
5000|$|... iDisk - A {{version of}} InternetDISK {{designed}} for ISPs to host larger <b>amounts</b> <b>of</b> <b>data</b> ...|$|R
25|$|Inadequacies in the {{physical}} construction and layout of the Emergency Services Bureau centre in Curtin were a hindrance. The centre was unable to handle efficiently the large <b>amount</b> <b>of</b> <b>data</b> and communications traffic {{into and out of}} the centre at the height of the crisis.|$|E
25|$|Previous studies, {{conducted}} by research consortia mentioned above and during scouting field trips by Eni-Agip experts, {{had shown that}} a large <b>amount</b> <b>of</b> <b>data</b> regarding fracture and fault systems and their characteristics can easily be acquired in the area due to the excellent degree of outcropping.|$|E
25|$|The JPEG {{compression}} algorithm {{is at its}} best on photographs and paintings of realistic scenes with smooth variations of tone and color. For web usage, where the <b>amount</b> <b>of</b> <b>data</b> used for an image is important, JPEG is very popular. JPEG/Exif {{is also the most}} common format saved by digital cameras.|$|E
50|$|For programmers {{who need}} to store large <b>amounts</b> <b>of</b> <b>data,</b> the 64-bit address space usually suffices.|$|R
5000|$|... #Caption: Due {{to massive}} <b>amounts</b> <b>of</b> <b>data</b> processing, NSA {{is the largest}} {{electricity}} consumer in Maryland.|$|R
40|$|Abstract — Almost all {{business}} organizations these days generate large <b>amounts</b> <b>of</b> <b>data</b> regarding their work. Simply stated, data mining refers to extracting or “mining ” knowledge from large <b>amounts</b> <b>of</b> <b>data.</b> The information thus extracted {{can be used}} by organizations in decision making process. In this paper, we study the data mining techniques used for credit risk analysis, in particular the decision tree technique...|$|R
