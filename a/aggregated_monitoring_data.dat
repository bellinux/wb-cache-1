0|10000|Public
40|$|International audienceThe easily-accessible {{computation}} power {{offered by}} cloud infrastructures {{coupled with the}} revolution of Big Data are expanding the scale and speed at which data analysis is per-formed. In their quest for finding the Value in the 3 Vs of Big Data, applications process larger data sets, within and across clouds. Enabling fast data transfers across geograph-ically distributed sites becomes particularly important for applications which manage continuous streams of events in real time. Scientific applications (e. g. the Ocean Observa-tory Initiative or the ATLAS experiment) as well as com-mercial ones (e. g. Microsoft's Bing and Office 365 large-scale services) operate on tens of data-centers around the globe and follow similar patterns: they <b>aggregate</b> <b>monitoring</b> <b>data,</b> assess the QoS or run global data mining queries based on in-ter site event stream processing. In this paper, we propose a set of strategies for efficient transfers of events between cloud data-centers and we introduce JetStream: a prototype implementing these strategies as a high performance batch-based streaming middleware. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites. The prototype was validated on tens of nodes from US and Europe data-centers of the Windows Azure cloud using synthetic benchmarks and with application code from {{the context of the}} Alice experiment at CERN. The results show an increase in transfer rate of 250 times over individual event streaming. Besides, introducing an adaptive transfer strategy brings an additional 25 % gain. Finally, the transfer rate can further be tripled thanks to the use of multi-route streaming...|$|R
40|$|<b>Aggregate</b> <b>monitoring</b> over <b>data</b> streams is {{attracting}} {{more and more}} attention in research community due to its broad potential applications. Existing methods suffer two problems, 1) The aggregate functions which could be monitored are restricted to be first-order statistic or monotonic {{with respect to the}} window size. 2) Only a limited number of granularity and time scales could be monitored over a stream, thus some interesting patterns might be neglected, and users might be misled by the incomplete changing profile about current data streams. These two impede the development of online mining techniques over data streams, and some kind of breakthrough is urged. In this paper, we employed the powerful tool of fractal analysis to enable the monitoring of both monotonic and non-monotonic aggregates on time-changing data streams. The monotony property of <b>aggregate</b> <b>monitoring</b> is revealed and monotonic search space is built to decrease the time overhead for accessing the synopsis from O(m) to O(log m), where m is the number of windows to be monitored. With the help of a novel inverted histogram, the statistical summary is compressed to be fit in limited main memory, so that high aggregates on windows of any length can be detected accurately and efficiently on-line. Theoretical analysis show the space and time complexity bound of this method are relatively low, while experimental results prove the applicability and efficiency of the proposed algorithm in different application settings. 1...|$|R
5000|$|A <b>data</b> <b>monitoring</b> switch is a {{networking}} {{hardware appliance}} {{that provides a}} pool of monitoring tools with access to traffic from {{a large number of}} network links. It provides a combination of functionality that may include <b>aggregating</b> <b>monitoring</b> traffic from multiple links, regenerating traffic to multiple tools, pre-filtering traffic to offload tools, and directing traffic according to one-to-one and many-to-many port mappings.|$|R
40|$|Online <b>monitoring</b> of <b>data</b> streams poses a {{challenge}} in many data-centric applications, such as telecommunications networks, traffic management, trend-related analysis, webclick streams, intrusion detection, and sensor networks. Mining techniques employed in these applications have to be efficient in terms of space usage and per-item processing time while providing a high quality of answers to (1) <b>aggregate</b> <b>monitoring</b> queries, such as finding surprising levels, i. e., "volatility" of a data stream, and detecting bursts, and to (2) similarity queries, such as detecting correlations and finding similar patterns. We propos...|$|R
40|$|Developers {{and users}} of {{high-performance}} distributed systems often observe performance {{problems such as}} unexpectedly low throughput or high latency. To determine the source of these performance problems, detailed end-to-end <b>monitoring</b> <b>data</b> from applications, networks, operating systems, and hardware must be correlated across time and space. Researchers {{need to be able}} to view and compare this very detailed <b>monitoring</b> <b>data</b> from a variety of angles. To address this problem, we propose a relational <b>monitoring</b> <b>data</b> archive that is designed to efficiently handle high-volume streams of <b>monitoring</b> <b>data.</b> In this paper we present an instrumentation and monitoring event archive service that can be used to collect and <b>aggregate</b> detailed end-to-end <b>monitoring</b> information from distributed applications. This archive service is designed to be scalable and fault tolerant. We also show how the archive is based on the "Grid Monitoring Architecture" defined by the Global Grid Forum...|$|R
40|$|Introduction: There is {{increasing}} {{evidence of an}} association between individual long-term PM 2. 5 exposure and human health. Mortality and morbidity data collected at the area-level are valuable resources for investigating corresponding population-level health effects. However, PM 2. 5 <b>monitoring</b> <b>data</b> are available for limited periods of time and locations, and are not adequate for estimating area-level concentrations. We developed a general approach to estimate county-average concentrations representative of population exposures for 1980 - 2010 in the continental U. S. Methods: We predicted annual average PM 2. 5 concentrations at about 70, 000 census tract centroids, using a point prediction model previously developed for estimating annual average PM 2. 5 concentrations in the continental U. S. for 1980 - 2010. We then averaged these predicted PM 2. 5 concentrations in all counties weighted by census tract population. In sensitivity analyses, we compared the resulting estimates to four alternative county average estimates using MSE-based R 2 in order to capture both systematic and random differences in estimates. These estimates included crude <b>aggregates</b> of regulatory <b>monitoring</b> <b>data,</b> averages of predictions at residential addresses in Southern California, and two sets of averages of census tract centroid predictions unweighted by population and interpolated from predictions at 25 -km national grid coordinates. Results: The county-average mean PM 2. 5 was 14. 40 (standard deviation= 3. 94) µg/m 3 in 1980 and decreased to 12. 24 (3. 24), 10. 42 (3. 30), and 8. 06 (2. 06) µg/m 3 in 1990, 2000, and 2010, respectively. These estimates were moderately related with crude averages in 2000 and 2010 when <b>monitoring</b> <b>data</b> were available (R 2 = 0. 70 - 0. 82) and almost identical to the unweighted averages in all four decennial years. County averages were also consistent with the county averages derived from residential estimates in Southern California (0. 95 - 0. 96). We found grid-based estimates of county-average PM 2. 5 were more consistent with our estimates when we also included <b>monitoring</b> <b>data</b> (0. 95 - 0. 98) than grid-only estimates (0. 91 - 0. 96); both had slightly lower concentrations than census tract-based estimates. Conclusions: Our approach to estimating population representative area-level PM 2. 5 concentrations is consistent with averages across residences. These exposure estimates {{will allow us to}} assess health impacts of ambient PM 2. 5 concentration in datasets with area-level health data...|$|R
40|$|Abstract. <b>Monitoring</b> <b>data</b> is the {{important}} signal for safety situation decision of shield method construction. This paper analyzes the abnormal types appearing in safety <b>monitoring</b> <b>data</b> of shield method construction, and points out its caused by the environmental quantity anomaly, structure form change, misreading mistake, instrument error, and human negligence. Based on the test {{and analysis of the}} abnormal <b>monitoring</b> <b>data,</b> it considers the <b>monitoring</b> <b>data</b> as abnormal value if the <b>monitoring</b> <b>data</b> beyond the confidence interval. And then uses the theoretical model to eliminate the observation error and systematic error. Furthermore, this paper analyzes the reasons of abnormal of the shield method construction settlement <b>monitoring</b> <b>data,</b> displacement <b>monitoring</b> <b>data,</b> shaft force <b>monitoring</b> <b>data,</b> deep horizontal displacement <b>monitoring</b> <b>data,</b> the water level <b>monitoring</b> <b>data</b> according to theory above. <b>Monitoring</b> <b>data</b> is an important signal of determine the shield method construction safety situation, its accuracy directly affect the evaluation result [1]. The abnormal of the subsidence, horizontal displacement monitoring, deep horizontal displacement monitoring, axial force monitoring and water <b>monitoring</b> <b>data</b> are all attribute to narrow and complicated of the shield subway construction site. It should analyzed according to the data types and anomaly genesis a...|$|R
40|$|Coastal {{environmental}} monitoring {{has become an}} important technique for scientific research and integrated management of coastal zone, which could produce {{a large number of}} long- term, fixed-point, continuous, large-scale, multi-factor <b>monitoring</b> <b>data</b> from space, air, sea surface, underwater, to seabed and etc. In this paper, the framework of the coastal environmental <b>monitoring</b> <b>data</b> integration was proposed, which aimed to solve three main problems of coastal {{environmental monitoring}}: to acquire and express <b>monitoring</b> <b>data,</b> then to achieve the integration of <b>monitoring</b> <b>data</b> and services, and finally to apply <b>monitoring</b> <b>data</b> to visualization and professional applications. The framework could effectively implement collection, transmission, storage, handling and application of these <b>monitoring</b> <b>data,</b> to provide more <b>monitoring</b> <b>data</b> and applications to end users. Coastal environmental monitoring system can enhance a variety of coastal phenomena monitoring capabilities for coastal zone. ...|$|R
40|$|International audienceScientific and {{commercial}} applications operate nowadays on tens of cloud datacenters around the globe, following similar patterns: they <b>aggregate</b> <b>monitoring</b> or sensor <b>data,</b> assess the QoS or run global data mining queries based on inter-site event stream processing. Enabling fast data transfers across geographically distributed sites allows such applications {{to manage the}} continuous streams of events in real time and quickly react to changes. However, traditional event processing engines often consider data resources as second-class citizens and support access to data only as a side-effect of computation (i. e. they are not concerned by the transfer of events from their source to the processing site). This is an efficient approach {{as long as the}} processing is executed in a single cluster where nodes are interconnected by low latency networks. In a distributed environment, consisting of multiple datacenters, with orders of magnitude differences in capabilities and connected by a WAN, this will undoubtedly lead to significant latency and performance variations. This is namely the challenge we address in this paper, by proposing JetStream, a high performance batch-based streaming middleware for efficient transfers of events between cloud datacenters. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites, {{while at the same time}} optimizing resource utilization and increasing cost efficiency. The prototype was validated on tens of nodes from US and Europe datacenters of the Windows Azure cloud with synthetic benchmarks and a real-life application monitoring the ALICE experiment at CERN. The results show a 3 x increase of the transfer rate using the adaptive multi-route streaming, compared to state of the art solutions...|$|R
40|$|Environmental <b>monitoring</b> <b>data,</b> {{subsidence}} <b>monitoring</b> <b>data,</b> and meteorology <b>monitoring</b> <b>data</b> {{were collected}} at {{and around the}} Area 3 and Area 5 Radioactive Waste Management Sites (RWMSs) at the Nevada Test Site (NTS) (refer to Figure 1). These <b>monitoring</b> <b>data</b> include radiation exposure, air, groundwater, meteorology, vadose zone, subsidence, and biota data. Although some of these media (radiation exposure, air, and groundwater) are reported in detail in other Bechtel Nevada (BN) reports (Annual Site Environmental Report [ASER], the National Emissions Standard for Hazardous Air Pollutants [NESHAP] report, and the Annual Groundwater Monitoring Report), they are also summarized in this report to provide an overall evaluation of RWMS performance and environmental compliance. Direct radiation <b>monitoring</b> <b>data</b> indicate that exposure at and around the RWMSs is not above background levels. Air <b>monitoring</b> <b>data</b> indicate that tritium concentrations are slightly above background levels. Groundwater <b>monitoring</b> <b>data</b> indicate that the groundwater in the uppermost aquifer beneath the Area 5 RWMS has not {{been affected by the}} facility. Meteorology data indicate that 2001 was an average rainfall year: rainfall totaled 150 mm (5. 9 in) at the Area 3 RWMS and 120 mm (4. 7 in) at the Area 5 RWMS. Vadose zone <b>monitoring</b> <b>data</b> indicate that 2001 rainfall infiltrated less than one meter (3 ft) before being returned to the atmosphere by evaporation. Soil-gas tritium <b>monitoring</b> <b>data</b> indicate slow subsurface migration, and tritium concentrations in biota were lower than in previous years. All 2001 <b>monitoring</b> <b>data</b> indicate that the Area 3 and Area 5 RWMSs are performing within expectations of the model and parameter assumptions for the facility performance assessments...|$|R
30|$|In ICPS scenario, sensors are {{deployed}} and various <b>monitoring</b> <b>data</b> {{are to be}} collected according to the different timeliness requirements. For example, industrial process <b>monitoring</b> <b>data</b> transmission for online control can only tolerate the latency no more than tens of milliseconds [5]. Thus, they require higher transmission priority than other <b>monitoring</b> <b>data</b> which may allow long latency, such as the machine health <b>monitoring</b> <b>data.</b> Priority-aware schemes are often used to tackle the differentiated data transmission. However, it is still challenging to coordinate the multichannel access while meeting the priority-aware transmission requirement due {{to the complexity of}} collision avoidance and transmission prioritization.|$|R
40|$|DSN <b>Monitor</b> <b>Data</b> Reader is a {{computer}} program that, as its name suggests, reads file of <b>monitor</b> <b>data</b> from the Deep Space Network (DSN). The <b>monitor</b> <b>data</b> constitute information on the status and performance of tracking, telemetry, command, and pointing equipment at the DSN antennas. The DSN has recently introduced a new, more advanced <b>monitor</b> <b>data</b> format, denoted 0158 -Mon, {{that is based on}} the standard formatted data unit (SFDU) and compressed header data objects (CHDO) of the Consultative Committee for Space Data Systems (CCSDS). The 0158 -Mon data format is a very flexible generic format that provides for specific variable-length formats and for self-identifying parameters that obviate the proprietary NASA Communications (NASCOM) bit-packed formats of the past. The <b>monitor</b> <b>data</b> SFDUs are also encapsulated in Standard DSN Blocks and routed to DSN customers for processing at their local mission control centers. This program helps a DSN customer to read and parse the <b>monitor</b> <b>data</b> to assess the statuses of the DSN stations in support of spacecraft flight operations...|$|R
40|$|Abstract. We {{present an}} event-based {{monitoring}} approach for service composition infrastructures. While existing approaches mostly monitor these infrastructures in isolation, we provide a holistic monitoring approach by leveraging Complex Event Processing (CEP) techniques. The {{goal is to}} avoid fragmentation of <b>monitoring</b> <b>data</b> across different subsystems in large enterprise environments by connecting various event producers. They provide <b>monitoring</b> <b>data</b> that might be relevant for composite service monitoring. Event queries over <b>monitoring</b> <b>data</b> allow to correlate different <b>monitoring</b> <b>data</b> to achieve more expressiveness. The proposed system has been implemented for a WS-BPEL composition infrastructure and the evaluation demonstrates the low overhead and feasibility of the system...|$|R
40|$|In September 1994, a {{team was}} formed to develop, document, and {{implement}} technical specifications for transmitting ambient air environmental compliance and <b>monitoring</b> <b>data</b> to the Oak Ridge Environmental Information System (OREIS). The approach used to transmit this data is documented in the {open_quotes}Plan for Integrating Environmental Compliance and <b>Monitoring</b> <b>Data</b> into OREIS. {close_quotes} This plan addresses the consolidated data requirements defined by the Federal Facility Agreement (FFA) and the Tennessee Oversight Agreement (TOA) as they pertain to environmental compliance and <b>monitoring</b> <b>data</b> maintained by Energy Systems` Oak Ridge Environmental Management organizations. Ibis document describes. the requirements, responsibilities, criteria, and format for transmitting ambient air compliance and <b>monitoring</b> <b>data</b> to OREIS...|$|R
40|$|Today, <b>monitoring</b> <b>data</b> {{analysis}} is an underrated but important process in tech industries. Almost, every industry gathers and analyzes <b>monitoring</b> <b>data</b> to improve offered services or to predict critical issues in advance. However, the <b>monitoring</b> <b>data</b> constitutes V's of big data (i. e. Volume, Variety, Velocity, Value, and Veracity). Exploration of big <b>monitoring</b> <b>data</b> possess� several issues and challenges. Firstly, {{a wide range}} of <b>monitoring</b> <b>data</b> analysis tools are available and these tools offer a variety of features (i. e. functional and non-functional) that affect the analysis process. However, these features come with their own setbacks. Therefore, selection of a suitable <b>monitoring</b> <b>data</b> tools is challenging and difficult to decide. Secondly, the big <b>monitoring</b> <b>data</b> analysis process contains two main operations of querying and processing a large amount of data. Since the volume of <b>monitoring</b> <b>data</b> is big, these operations require a scalable and reliable architecture to extract, aggregate and analyze data in an arbitrary range of granularity. Ultimately, the results of analysis form the knowledge of the system and should be shared and communicated. The contribution of this research study is two-fold. Firstly, we propose a generic performance evaluation methodology. The method uses the Design of Experiment (DoE) evaluation method for the assessment of tools, workflows and techniques. The evaluation results generated from this methodology provide a base for selection. Secondly, we designed and implemented a big <b>monitoring</b> <b>data</b> analysis architecture to provide advanced analytics such as workload forecasting and pattern matching. The architecture offers these services in an available and scalable environment. We implement our design using distributed tools such as Apache Solr, Apache Hadoop and Apache Spark. We also assessed the performance aspects (i. e. Latency and Fault-tolerance) of the architecture design using the proposed evaluation method...|$|R
50|$|Real time <b>monitoring</b> <b>data</b> and {{forecasts}} of air quality that are color-coded {{in terms of}} the air quality index are available from EPA's AirNow web site. Historical air <b>monitoring</b> <b>data</b> including AQI charts and maps are available at EPA's AirData website.|$|R
40|$|The {{management}} of large-scale distributed information systems {{relies on the}} effective use and modeling of <b>monitoring</b> <b>data</b> collected at various points in the distributed information systems. A traditional approach to model <b>monitoring</b> <b>data</b> is to discover invariant relationships among the <b>monitoring</b> <b>data.</b> Indeed, we can discover all invariant relationships among all pairs of <b>monitoring</b> <b>data</b> and generate invariant networks, where a node is a <b>monitoring</b> <b>data</b> source (metric) and a link indicates an invariant relationship between two <b>monitoring</b> <b>data.</b> Such an invariant network representation can help system experts to localize and diagnose the system faults by examining those broken invariant relationships and their related metrics, since system faults usually propagate among the <b>monitoring</b> <b>data</b> and eventually lead to some broken invariant relationships. However, at one time, there are usually a lot of broken links (invariant relationships) within an invariant network. Without proper guidance, {{it is difficult for}} system experts to manually inspect this large number of broken links. To this end, in this article, we propose the problem of ranking metrics according to the anomaly levels for a given invariant network, while this is a nontrivial task due to the uncertainties and the complex nature of invariant networks. Specifically, we propose two types of algorithms for ranking metric anomaly by link analysis in invariant networks. Along this line, we first define two measurements to quantify the anomaly level of each metric, and introduce the mRank algorithm. Also, we provide a weighte...|$|R
40|$|This package {{contains}} the <b>monitoring</b> <b>data,</b> the ideal behavior models, computed clustering results for behavior models {{based on the}} <b>monitoring</b> <b>data,</b> and {{the interpretation of the}} analysis results. We processed these data with the following two tooling sources: Experiment setup: [URL] Analysis software: [URL]...|$|R
30|$|Monitoring {{provides}} tangible {{information on}} a regular basis {{over an extended period of}} time about past and present conditions of the environment and thus, generates large amounts of data. Until now, many construction project teams have collected and analyzed <b>monitoring</b> <b>data</b> for each instrument manually, consequently wasting a lot of time. Moreover, these data will be stored in a file format. This approach is difficult for handling <b>monitoring</b> <b>data</b> used for querying and reviewing. In this project, all <b>monitoring</b> <b>data</b> will be collected and stored into a database and visualized based on a BIM system. A construction project team can manage <b>monitoring</b> <b>data</b> via a database or a visualization environment. These systems provide many functions for assisting a user to manage data efficiently and effectively.|$|R
40|$|Environmental, subsidence, and {{meteorological}} <b>monitoring</b> <b>data</b> {{were collected}} at {{and around the}} Area 3 and Area 5 Radioactive Waste Management Sites (RWMSs) at the Nevada Test Site (NTS) (refer to Figure 1). These <b>monitoring</b> <b>data</b> include radiation exposure, air, groundwater,meteorology, vadose zone, subsidence, and biota data. Although some of these media (radiation exposure, air, and groundwater) are reported in detail in other Bechtel Nevada (BN) reports (Annual Site Environmental Report [ASER], the National Emissions Standard for Hazardous Air Pollutants [NESHAP] report, and the Annual Groundwater Monitoring Report), they are also summarized in this report to provide an overall evaluation of RWMS performance and environmental compliance. Direct radiation <b>monitoring</b> <b>data</b> indicate that exposure at and around the RWMSs is not above background levels. Air <b>monitoring</b> <b>data</b> indicate that tritium concentrations are slightly above background levels. Groundwater <b>monitoring</b> <b>data</b> indicate that the groundwater in the uppermost aquifer beneath the Area 5 RWMS has not {{been affected by the}} facility. Meteorological data indicate that 2002 was a dry year: rainfall totaled 26 mm (1. 0 in) at the Area 3 RWMS and 38 mm (1. 5 in) at the Area 5 RWMS. Vadose zone <b>monitoring</b> <b>data</b> indicate that 2002 rainfall infiltrated less than 30 cm (1 ft) before being returned to the atmosphere by evaporation. Soil-gas tritium <b>monitoring</b> <b>data</b> indicate slow subsurface migration, and tritium concentrations in biota were lower than in previous years. Special investigations conducted in 2002 included: a comparison between waste cover water contents measured by neutron probe and coring; and a comparison of four methods for measuring radon concentrations in air. All 2002 <b>monitoring</b> <b>data</b> indicate that the Area 3 and Area 5 RWMSs are performing within expectations of the model and parameter assumptions for the facility Performance Assessments (PAs) ...|$|R
30|$|Software {{systems with}} produce and/or consume <b>monitoring</b> <b>data.</b>|$|R
5000|$|... #Subtitle level 3: Environmental <b>monitoring</b> <b>data</b> {{management}} systems ...|$|R
40|$|Recent {{work has}} {{identified}} that circumstances of equipment operation can radically change condition <b>monitoring</b> <b>data.</b> This contribution investigates {{the significance of}} considering circumstance monitoring on the diagnostic interpretation of such condition <b>monitoring</b> <b>data.</b> Electrical treeing partial discharge data {{have been subjected to}} a data mining investigation, providing a platform for classification of harmonic influenced partial discharge patterns. The Total Harmonic Distortion (THD) index was varied to a maximum of 40 %. The results show progressive development for interpretation of condition <b>monitoring</b> <b>data,</b> improving the asset manager's holistic view of an asset's health...|$|R
40|$|This report {{presents}} {{an evaluation of}} the groundwater <b>monitoring</b> <b>data</b> obtained in the Bear Creek Hydrogeologic Regime (Bear Creek Regime) during calendar year (CY) 1996. The <b>monitoring</b> <b>data</b> were collected for the multiple programmatic purposes of the Y- 12 Plant Groundwater Protection Program (GWPP) and have been reported in Calendar Year 1996 Annual Groundwater Monitoring Report for the Bear Creek Hydrogeologic Regime at the US Department of Energy Y- 12 Plant, Oak Ridge, Tennessee. The Annual Monitoring report presents only the results of the <b>monitoring</b> <b>data</b> evaluations required for waste management sites addressed under the Resource Conservation and Recovery Act (RCRA) post-closure permit for the Bear Creek Regime. The Annual Monitoring Report also serves as a consolidated reference for the groundwater and surface water <b>monitoring</b> <b>data</b> obtained throughout the Bear Creek Regime {{under the auspices of the}} Y- 12 GWPP. This report provides {{an evaluation of the}} CY 1996 <b>monitoring</b> <b>data</b> with an emphasis on regime-wide groundwater and surface water quality and long-term concentration trends of regulated and non-regulated monitoring parameters...|$|R
40|$|Environmental <b>monitoring</b> <b>data</b> were {{collected}} at {{and around the}} Area 3 and Area 5 Radioactive Waste Management Sites (RWMSs) at the Nevada Test Site (NTS). These <b>monitoring</b> <b>data</b> include radiation exposure, air, groundwater, meteorology, vadose zone, and biota data. Although some of these media (radiation exposure, air, and groundwater) are reported in detail in other Bechtel Nevada reports (Annual Site Environmental Report [ASER], the National Emissions Standard for Hazardous Air Pollutants [NESHAP] report, and the Annual Groundwater Monitoring Report), they are also summarized in this report to provide an overall evaluation of RWMS performance and environmental compliance. Direct radiation <b>monitoring</b> <b>data</b> indicate that exposure at and around the RWMSs is not above background levels. Air <b>monitoring</b> <b>data</b> indicate that tritium concentrations are slightly above background levels, whereas radon concentrations are not above background levels. Groundwater <b>monitoring</b> <b>data</b> indicate that the groundwater in the alluvial aquifer beneath the Area 5 RWMS has not {{been affected by the}} facility. Meteorology data indicate that 1999 was a dry year: rainfall totaled 3. 9 inches at the Area 3 RWMS (61 percent of average) and 3. 8 inches at the Area 5 RWMS (75 percent of average). Vadose zone <b>monitoring</b> <b>data</b> indicate that 1999 rainfall infiltrated less than one foot before being returned to the atmosphere by evaporation. Soil-gas tritium data indicate very slow migration, and tritium concentrations in biota were insignificant. All 1999 <b>monitoring</b> <b>data</b> indicate that the Area 3 and Area 5 RWMSs are performing as expected at isolating buried waste...|$|R
40|$|In {{this paper}} we propose the {{development}} of a cloud moni-toring suite to provide scalable and robust lookup, data col-lection and analysis services for large-scale cloud systems. We propose a multi-tier architecture using a layered gossip protocol to <b>aggregate</b> <b>monitoring</b> information and facilitate lookup, information collection and the identification of re-dundant capacity. This enables monitoring to be done in-situ without the need for significant additional infrastructure to facilitate monitoring services. 1...|$|R
5000|$|Lay Monitoring Program - Water {{chemistry}} <b>monitoring</b> <b>data</b> summary: ...|$|R
40|$| expression, {{to enable}} easier {{interpretation}} of the <b>monitoring</b> <b>data.</b> Main|$|R
40|$|Automated {{techniques}} to diagnose {{the cause of}} system failures based on <b>monitoring</b> <b>data</b> is an active area {{of research at the}} intersection of systems and machine learning. In this paper, we identify three tasks that form key building blocks in automated diagnosis: 1. Identifying distinct states of the system using <b>monitoring</b> <b>data.</b> 2. Retrieving <b>monitoring</b> <b>data</b> from past system states that are similar to the current state. 3. Pinpointing attributes in the <b>monitoring</b> <b>data</b> that indicate the likely cause of a system failure. We provide (to our knowledge) the first apples-to-apples comparison of both classical and state-of-the-art techniques for these three tasks. Such studies are vital to the consolidation and growth of the field. Our study is based on a variety of failures injected in a multitier Web service. We present empirical insights and research opportunities. ...|$|R
40|$|This {{document}} {{is the first}} part of the monitoring report from the Kortrijk ECO-Life community. This report is based on the available <b>monitoring</b> <b>data</b> and design information one year after occupation and monitoring started in the Kortrijk community, that is in the Venning Phase 1 buildings. The deliverable starts with some general considerations on the gathering, processing and presentation of the <b>monitoring</b> <b>data</b> and climate data. In chapter 3, 4 and 5 the monitoring status and results (if available) for the different project phases are summarised. The <b>monitoring</b> <b>data,</b> results and background information of all demonstration buildings and energy systems are structured in the ECO-Life <b>Monitoring</b> <b>data</b> sheets, that are added in Annex A, B and C to this report. Finally in chapter 6 the operational performance of the Venning Phase 1 multi-family buildings is reported more in detail...|$|R
40|$|This thesis {{discusses}} {{design and}} implementation of a cloud computer system which processes end-user <b>monitoring</b> <b>data.</b> Using a defined web service interface, the system is receiving raw <b>monitoring</b> <b>data</b> and then it processes them. Finally, it presents the data to end-users. Entire application is written in C# and deployed to the Windows Azure Platform...|$|R
40|$|Summary: Complex {{computer}} models {{play a crucial}} role in air quality research. These models are used to evaluate potential regulatory impacts of emission control strategies and to estimate air quality in areas without <b>monitoring</b> <b>data.</b> For both of these purposes, it is important to calibrate model output with <b>monitoring</b> <b>data</b> to adjust for model biases and improve spatial prediction. In this paper, we propose a new spectral method to study and exploit complex relationships between model output and <b>monitoring</b> <b>data.</b> Spectral methods allow us to estimate the relationship between model output and <b>monitoring</b> <b>data</b> separately at different spatial scales, and to use model output for prediction only at the appropriate scales. The proposed method is computationally efficient and can be implemented using standard software. We apply the method to compare Community Multiscale Air Quality (CMAQ) model output with ozone measurements in the United States in July, 2005. We find that CMAQ captures large-scale spatial trends, but has low correlation with the <b>monitoring</b> <b>data</b> at small spatial scales. Key words: Computer model output; Data fusion; Kriging; Multiscale analysis. This paper has been submitted for consideration for publication in Biometrics A spectral method for spatial downscaling 1 1...|$|R
40|$|LHC {{experiments}} are {{depending on the}} distributed EGEE infrastructure for their core activities. The Experiment Dashboard is a monitoring framework aiming {{to provide for the}} LHC experiments the overview of their activities on the EGEE infrastructure with a special emphasis in support for the user community. Existing monitoring tools are usually focusing on a specific usage like specific Grid middleware/infrastructures, specific submission tool, etc. The Experiment Dashboard has been built to <b>aggregate</b> existing <b>monitoring</b> infrastructures (from experiment specific software, infrastructure itself, monitoring tools) and provide unified views and information correlation. Experiment Dashboard is covering different areas of the LHC activities - job processing, data transfer, and data publishing. It is deployed for four LHC experiments (CMS, ATLAS, LHCb, ALICE). Some of the core functionality of the Experiment Dashboard like job monitoring can be used for other virtual organizations. Experiment Dashboard is currently in production and is used by LHC users with different roles for their everyday work. The whole EGEE monitoring infrastructure can be considerably improved. Very often the error messages indicating various failures are not clear and do not point to the real problem. The variety of the local fabric monitoring systems used by sites complicates the task of creation of the common framework for aggregation of the <b>monitoring</b> <b>data</b> in the central repository. Transparent navigation of the <b>monitoring</b> <b>data</b> provided by different monitoring systems is often not possible...|$|R
30|$|Other {{staff who}} use <b>monitoring</b> <b>data</b> {{to varying degrees}} within their roles.|$|R
30|$|Customers or {{clients who}} may {{generate}} and {{make use of}} <b>monitoring</b> <b>data.</b>|$|R
30|$|As no {{reliable}} <b>monitoring</b> <b>data</b> {{were available for}} the individual buildings {{in the town of}} Essen, a simulation tool (see below the simulation tool description) comparison was used here to check the results. Validation with <b>monitoring</b> <b>data</b> was then done on a city quarter aggregation level. Care was taken on using the same input data for all models.|$|R
