23|10000|Public
40|$|The {{recombinant}} {{estimation technique}} of Mullin and Reiley (2006) {{can be a}} useful tool for analyzing data from normal-form games. The recombinant estimator falls within a general category of statistics known as U-statistics. This classification has both theoretical and practical implications: (1) the recombinant estimator is optimal (minimum variance) among unbiased estimators, (2) there is a computationally simple method for computing its <b>asymptotic</b> <b>standard</b> <b>error,</b> and (3) the estimation technique can be extended to multiple outcomes and to other types of inferential procedures commonly used for experimental data, such as the sign test. Simulation evidence suggests that researchers should use the <b>asymptotic</b> <b>standard</b> <b>error</b> rather than the standard error of Mullin and Reiley (2006) since the latter exhibits a downward bias. Copyright Economic Science Association 2008 Recombinant estimation, U-statistics,...|$|E
40|$|Root-mean-squared {{errors and}} {{asymptotic}} standard errors of ML coefficient estimates are compared for equation systems of different size, using normal and discrete bootstrap. It appears that exploiting normality does not yield any gain. Suggestions {{are made for}} correcting the downward bias of bootstrap RMSEs. <b>asymptotic</b> <b>standard</b> <b>error</b> bootstrap equation system maximum likelihood...|$|E
40|$|A {{formula is}} derived for the <b>asymptotic</b> <b>standard</b> <b>error</b> of a true-score equating by item {{response}} theory. The equating method is applicable {{when the two}} tests to be equated are administered to different groups along with an anchor test. Numerical standard errors are shown for an actual equating (1) comparing the standard errors of IRT, linear, e and equipercentile methods and (2) illustrating {{the effect of the}} length of the anchor test on the standard error of the equating...|$|E
40|$|This paper {{shows how}} to compute <b>asymptotic</b> <b>standard</b> <b>errors</b> of the {{characteristic}} roots of a nonlinear econometric model. The system of simultaneous equations is linearized {{in the neighborhood}} of a given point, then characteristic roots and related <b>standard</b> <b>errors</b> are computed. Nonlinear econometric models; characteristic roots; eigenvalues; <b>asymptotic</b> <b>standard</b> <b>errors...</b>|$|R
40|$|<b>Asymptotic</b> <b>standard</b> <b>errors</b> of the {{estimates}} of the obliquely rotated parameters by the Harris-Kaiser Case II orthoblique method are derived under the assumption of the multivariate normal distribution for observed variables. A covariance structure model for observed variables is constructed such that both unrotated and orthogonally rotated parameters {{are involved in the}} model. The <b>asymptotic</b> <b>standard</b> <b>errors</b> for the final oblique solution (orthoblique solution) are derived by a stepwise method. First, the asymptotic variance-covariance matrix for {{the estimates}} of the unrotated and orthogonally rotated parameters is derived. Second, the delta method is used to obtain the asymptotic variances of {{the estimates of}} the obliquely rotated parameters. Results by simulation indicate that the theoretical values of the <b>asymptotic</b> <b>standard</b> <b>errors</b> are close to simulated ones...|$|R
3000|$|The {{initial and}} {{equilibrium}} contact angles of adhesive on the treated wood surface, K values, <b>asymptotic</b> <b>standard</b> <b>errors</b> (SE), and the coefficients of determination (R [...]...|$|R
40|$|A {{new measure}} for {{evaluating}} {{the strength of the}} association between a nominal variable and an ordered categorical response variable is introduced. The introduction of a new measure is justified by analysing the characteristics of a measure of the nominal-ordinal association proposed by Agresti (1981), especially with respect to the problem of the 'choice' of a predictive variable. The sample-based version of the index is studied, and its <b>asymptotic</b> <b>standard</b> <b>error</b> and asymptotic distribution are derived. Simulations are considered to evaluate the adequacy of the asymptotic approximation determined, following Goodman & Kruskal (1963). ...|$|E
40|$|A general Computer {{program is}} {{described}} that will compute asymptotic standard errors {{and carry out}} significance tests for an endless variety of (standard and) nonstandard large-sample statistical problems, without requiring the statistician to derive <b>asymptotic</b> <b>standard</b> <b>error</b> formulas. The program assumes that the observations have a multinormal distribution and that the null hypothesis to be tested has the form xi = 0 where xi is some function (to be specified by the user) of means, variances, and covariances. Only minor programming is required to replace either or both of these assumptions. The package performs the automated hypothesis testing and consists of amain program and six subroutines. The package is written in Fortran IV [...] (Author/ON...|$|E
40|$|A general {{formula of}} the higher-order <b>asymptotic</b> <b>standard</b> <b>error</b> is derived for the estimators of the {{parameters}} in structural equation modeling. The formula {{can be used}} for nonnormally distributed data as well as normally distributed ones. For this derivation, the third- and fourth-order asymptotic central moments of sample variances and covariances are provided for nonnormally and normally distributed cases. The formula requires the partial derivatives of an estimator up to the third order with respect to sample variances and covariances, which are shown for the case of the Wishart maximum likelihood estimator. To see the accuracy of the formula, simulations are performed using the factor/component analysis models. It is numerically shown that some of the added contributions of the higher-order asymptotic standard errors are substantial with small to modest sample sizes. Key words: Mean square errors, asymptotic standard errors, asymptoti...|$|E
40|$|The <b>asymptotic</b> <b>standard</b> <b>errors</b> of the {{estimates}} of rotated factor loadings and factor correlations are derived for the cases with weights for observed variables such as those for Kaiser 2 ̆ 7 s normalization. The factor analysis models employed in this paper are the exploratory ones which have orthogonal or oblique common factors and unstandardized or standardized observed variables. The <b>asymptotic</b> <b>standard</b> <b>errors</b> are given from an augmented information matrix. As an application, the result for the direct oblique rotation by general quartic criteria with Kaiser 2 ̆ 7 s normalization is derived. The results of simulation show that the theoretical <b>standard</b> <b>errors</b> are close to simulated ones...|$|R
40|$|The <b>asymptotic</b> <b>standard</b> <b>errors</b> of the IRT equating {{coefficients}} {{given by}} the mean/sigma, mean/mean and mean/geometric mean methods are derived when the two-parameter logistic model holds and item parameters are obtained by the marginal maximum likelihood estimation. The case of two nonequivalent examinee-groups and the case of single group are considered. The numerical examples show that the mean/mean and mean/geometric mean methods are superior to the mean/sigma method. The results also show {{that the number of}} quadrature points in the numerical approximation to the integration of ability parameters is crucial to the estimation of the <b>asymptotic</b> <b>standard</b> <b>errors...</b>|$|R
40|$|For some {{structural}} econometric models, {{the contribution}} of the off-diagonal blocks of the coefficients covariance matrix to the <b>asymptotic</b> <b>standard</b> <b>errors</b> of multipliers and forecasts is empirically evaluated. Econometric models; impact multipliers; forecast errors; <b>asymptotic</b> <b>standard</b> errors; structural form; reduced form; coefficients covariance matrix...|$|R
40|$|This article proposes semiparametric {{generalized}} least-squares {{estimation of}} parametric restrictions between the conditional mean and the conditional variance of excess returns given {{a set of}} parametric factors. A distinctive feature of our estimator {{is that it does}} not require a fully parametric model for the conditional mean and variance. We establish consistency and asymptotic normality of the estimates. The theory is nonstandard due to the presence of estimated factors. We provide sufficient conditions for the estimated factors not to have an impact in the <b>asymptotic</b> <b>standard</b> <b>error</b> of estimators. A simulation study investigates the finite sample performance of the estimates. Finally, an application to the CRSP value-weighted excess returns highlights the merits of our approach. In contrast to most previous studies using nonparametric estimates, we find a positive and significant price of risk in our semiparametric setting. status: publishe...|$|E
40|$|In item {{response}} theory (IRT), ability estimation can {{be seriously}} affected by abnormal responses occurring from e. g., cheating, inattention, lack of time, guessing, tiredness, stress… Those phenomena {{may influence the}} ability estimation process tremendously. One the one hand, person fit indices were developed as post-hoc approaches to identify abnormal responses patterns as a whole (e. g., Meijer & Sijtsma, 2001). On the other hand, getting uncontaminated ability estimates {{would also be a}} challenging issue. Robust estimators were proposed in the IRT framework to lessen the impact of abnormal responses onto the estimation process (Mislevy & Bock, 1982; Schuster & Yuan, 2011; Wainer & Wright, 1980). Yet, these estimators are still rarely used in practice, mostly because very little is known about their statistical properties. The purpose of this talk is to briefly present these robust ability estimators, and to derive their asymptotic distribution under mild regularity conditions. In particular, a simple formula for the <b>asymptotic</b> <b>standard</b> <b>error</b> (ASE) of these estimators is obtained (Magis, in press). Results of a simulation study that involves both presence and absence of cheating in the data generation process will be outlined. References: Magis, D. (in press). On the <b>asymptotic</b> <b>standard</b> <b>error</b> of a class of robust estimators of ability in dichotomous item response models. British Journal of Mathematical and Statistical Psychology. Meijer, R., & Sijtsma, K. (2001). Methodology review: Evaluating person fit. Applied Psychological Measurement, 25, 107 - 135. doi: 10. 1177 / 01466210122031957 Mislevy, R. J., & Bock, R. D. (1982). Biweight estimates of latent ability. Educational and Psychological Measurement, 42, 725 - 737. doi: 10. 1177 / 001316448204200302 Schuster, C., & Yuan, K. -H. (2011). Robust estimation of latent ability in item response models. Journal of Educational and Behavioral Statistics, 36, 720 - 735. doi: 10. 3102 / 1076998610396890 Wainer, H., & Wright, B. D. (1980). Robust estimation of ability in the Rasch model. Psychometrika, 45, 373 - 391. doi: 10. 1007 /BF 0229391...|$|E
40|$|This study {{evaluates the}} {{industry}} effects of monetary transmission mechanism {{in line with}} the literature on disaggregated approach to policy transmission mechanism. The study uses vector auto regression (VAR) model and monthly data from April 1993 to October 2011 pertaining to output growth of five use-based industries, call money rate and WPI inflation rate for evaluating the transmission mechanism. The generalised accumulated impulse response analysis from the VAR model showed that following a tight monetary policy shock, the output growth could be affected more for capital goods and consumer durables than basic, intermediate and consumer non-durable goods. Intermediate and consumer non-durable goods could show a relatively moderate transient response and transmission lag could be evident for the consumer non-durable goods. However, relatively wide <b>asymptotic</b> <b>standard</b> <b>error</b> bands associated with the impulse responses could be reflecting uncertainty in the impact of transmission mechanism. JEL classification: E 52, L 6...|$|E
40|$|The paper {{provides}} a technical discussion {{on how the}} multivariate persistence measures described in VandeGucht, Kwok and Dekimpe (1993) can be estimated both when cointegration is present and absent. Procedures to derive the associated <b>asymptotic</b> <b>standard</b> <b>errors</b> are discussed. Foreign exchange; Foreign exchange rates;...|$|R
40|$|A maximum {{likelihood}} solution is {{obtained for the}} simple linear structural relation model where the underlying incidental distribution and one error variance are assumed known. Expressions for the <b>asymptotic</b> <b>standard</b> <b>errors</b> of the {{maximum likelihood}} estimates are obtained and these are verified using a simulation study...|$|R
40|$|Some {{analytic}} simulation {{techniques for}} the analysis of the reduced form and of the dynamic properties of econometric models are described. Comparisons are made with analytical methods available for linear models. Econometric models; structural form; reduced form; analytic simulation; stochastic simulation; impact multipliers; dynamic multipliers; forecast errors; <b>asymptotic</b> <b>standard</b> <b>errors...</b>|$|R
40|$|This article derives {{analytic}} finite sample approximations to {{the bias}} and standard error {{of a class}} of statistics which test the hypothesis of no serial correlation in market returns. They offer an alternative to both the widely used Monte Carlo approach for calculating the bias, as well as <b>asymptotic</b> <b>standard</b> <b>error</b> calculations. These approximations are calculated {{under the assumption that}} returns are spherically symmetrically distributed (such as Gaussian) and also under the weaker assumption that returns follow any arbitrary continuous distribution. The class of statistics examined here includes many of those employed in the finance and macroeconomics literature to test for the existence of random walk, including the variance ratio and the multi-period return regression on past returns. The accuracy of the approximations is benchmarked using simulated data, where arbitrarily tight estimates of the bias and standard error can be calculated. The approximations are then applied to adjust the statistics calculated using returns on the NYSE firom 1926 - 1991...|$|E
40|$|In this paper, we derive valid Edgeworth {{expansions}} for studentized {{versions of}} a large class of statistics when the data are generated by a strongly mixing process. Under dependence, the asymptotic variance of such a statistic is given by an infinite series of lag-covariances, and therefore, studentizing factors (i. e., estimators of the <b>asymptotic</b> <b>standard</b> <b>error)</b> typically involve an increasing number, say, ℓ of lag-covariance estimators, which are themselves quadratic functions of the observations. The unboundedness of the dimension ℓ of these quadratic functions makes the derivation and {{the form of the}} expansions nonstandard. It is shown that in contrast to the case of the studentized means under independence, the derived Edgeworth expansion is a superposition of three distinct series, respectively, given by one in powers of n^- 1 / 2, one in powers of [n/ℓ]^- 1 / 2 (resulting from the standard error of the studentizing factor) and one in powers of the bias of the studentizing factor, where n denotes the sample size. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|We {{present a}} nonparametric {{methodology}} {{for evaluation of}} agreement between multiple methods of measurement of a continuous variable. Our approach is unified in that it can deal with any scalar measure of agreement currently available in the literature, and can incorporate repeated and unreplicated measurements, and balanced as well as unbalanced designs. Our key idea is to treat an agreement measure as a functional of the joint cumulative distribution function of the measurements from multiple methods. This measure is estimated nonparametrically by plugging-in a weighted empirical counterpart of the joint distribution function. The resulting estimator is shown to be asymptotically normal under some specified assumptions. A closed-form expression is provided for the <b>asymptotic</b> <b>standard</b> <b>error</b> of the estimator. This asymptotic normality is used to derive a large-sample distribution-free methodology for simultaneously comparing the multiple measurement methods. The small-sample performance of this methodology is investigated via simulation. The asymptotic efficiency of the proposed nonparametric estimator relative to the normality-based maximum likelihood estimator is also examined. The methodology is illustrated by applying it to a blood pressure data set involving repeated measurements from three measurement methods. ...|$|E
40|$|We examine {{estimation}} {{of a model}} of producer behavior {{in the presence of}} correlated measurement errors in the regressors. Scale economies and price-cost margins are estimated from a set of panel data for manufacturing plants. The paper presents a somewhat new model for {{estimation of}} these parameters which is highly flexible but with a simple regression structure. Perhaps the most important contribution of the paper is some new results on deriving parameter bounds for a regression model with errors in variables. In particular, we consider the case where the measurement errors might be correlated. We derive <b>asymptotic</b> <b>standard</b> <b>errors</b> for the parameter bounds. These <b>asymptotic</b> <b>standard</b> <b>errors</b> are compared to bootstrap estimates. Our new results on parameter bounds are applied to the estimation of the model of producer behavior. Estimation; errors-in-variables; parameter bounds; imperfect competition; scale economies. ...|$|R
40|$|This paper {{presents}} a new polychoric instrumental variable (PIV) estimator {{to use in}} structural equa-tion models (SEMs) with categorical observed variables. The PIV estimator is a generalization of Bollen’s (Psychometrika 61 : 109 – 121, 1996) 2 SLS/IV estimator for continuous variables to categorical endogenous variables. We derive the PIV estimator and its <b>asymptotic</b> <b>standard</b> <b>errors</b> for the regression coefficients in the latent variable and measurement models. We also provide an estimator of the variance and covariance parameters of the model, <b>asymptotic</b> <b>standard</b> <b>errors</b> for these, and test statistics of overall model fit. We examine this estimator via an empirical study and also via a small simulation study. Our results illustrate the greater robustness of the PIV estimator to structural misspecifications than the system-wide estimators that are commonly applied in SEMs. Key words: latent variables, ordinal variables, dichotomous variables, instrumental variables, two-stage least squares (2 SLS), factor analysi...|$|R
40|$|We {{show that}} the quantile-based skew {{logistic}} distribution possesses kurtosis measures based on L-moments and on quantiles which are skewness invariant. We furthermore derive closed-form expressions for method of L-moments estimators for the distribution’s parameters together with <b>asymptotic</b> <b>standard</b> <b>errors</b> for these estimators. Vice-Chancellor’s Academic Development Grant at the University of Pretoria and the University of Newcastle Special Studies Program. [URL]...|$|R
40|$|Quantifying {{scientific}} uncertainty when setting {{total allowable catch}} {{limits for}} fish stocks is a major challenge, {{but it is a}} requirement in the United States since changes to national fisheries legislation. Multiple sources of error are readily identifiable, including estimation error, model specification error, forecast error, and errors associated with the definition and estimation of reference points. Our focus here, however, is to quantify the influence of estimation error and model specification error on assessment outcomes. These are fundamental sources of uncertainty in developing scientific advice concerning appropriate catch levels and although a study of these two factors may not be inclusive, it is feasible with available information. For data-rich stock assessments conducted on the U. S. west coast we report approximate coefficients of variation in terminal biomass estimates from assessments based on inversion of the assessment of the model’s Hessian matrix (i. e., the <b>asymptotic</b> <b>standard</b> <b>error).</b> To summarize variation “among” stock assessments, as a proxy for model specification error, we characterize variation among multiple historical assessments of the same stock. Results indicate that for 17 groundfish and coastal pelagic species, the mean coefficient of variation of terminal biomass is 18...|$|E
40|$|The maximum {{likelihood}} (ML) and the weighted likelihood (WL) estimators {{are commonly used}} to obtain proficiency level estimates with pre-calibrated item parameters. Both estimators have the same <b>asymptotic</b> <b>standard</b> <b>error</b> (ASE) {{that can be easily}} derived from the expected information function of the test. However, the accuracy of this asymptotic formula is uncertain with short tests when only a few items are administered. The {{purpose of this paper is}} to compare the ASE of these estimators to their exact values, evaluated at the proficiency level estimates. The exact SE is computed by generating the full exact sample distribution of the estimators, so its practical feasibility is limited to small tests (except under the Rasch model). A simulation study was conducted to compare the ASE and the exact SE of the ML and WL estimators, to the “true” SE (i. e., computed as the exact SE with the true proficiency levels). It is concluded that with small tests, the exact SEs are less biased and return smaller root mean squared error values than the asymptotic SEs, while as expected the two estimators return similar results with longer tests. Peer reviewe...|$|E
40|$|A {{number of}} authors have {{proposed}} clinical trial designs involving {{the comparison of}} several experimental treatments with a control treatment in two or more stages. At {{the end of the}} first stage, the most promising experimental treatment is selected, and all other experimental treatments are dropped from the trial. Provided it is good enough, the selected experimental treatment is then compared with the control treatment in one or more subsequent stages. The analysis of data from such a trial is problematic because of the treatment selection and the possibility of stopping at interim analyses. These aspects lead to bias in the maximum-likelihood estimate of the advantage of the selected experimental treatment over the control and to inaccurate coverage for the associated confidence interval. In this paper, we evaluate the bias of the maximum-likelihood estimate and propose a bias-adjusted estimate. We also propose an approach to the construction of a confidence region for the vector of advantages of the experimental treatments over the control based on an ordering of the sample space. These regions are shown to have accurate coverage, although they are also shown to be necessarily unbounded. Confidence intervals for the advantage of the selected treatment are obtained from the confidence regions and are shown to have more accurate coverage than the standard confidence interval based upon the maximum-likelihood estimate and its <b>asymptotic</b> <b>standard</b> <b>error...</b>|$|E
40|$|It {{is shown}} that for finite {{mixtures}} the missing information tends to zero {{as the number}} of observations on each subject increases. Then, the classes become perfectly separated (i. e. the posterior membership probabilities are close to 0 or 1), the observed information tends to the complete information and the class-specific parameters in the mixture model become information orthogonal across classes. Then the <b>asymptotic</b> <b>standard</b> <b>errors</b> of parameter estimates can be obtained directly from the EM algorithm. The degree of class-separation is derived for which the amount of missing observation is approximately negligible and the <b>asymptotic</b> <b>standard</b> <b>errors</b> based on the complete information matrix are sufficiently accurate. Empirical illustrations are provided. A Monte Carlo study is performed to examine {{the extent to which the}} approximation is adequate. A comparison is made with other methods to approximate the observed information matrix. It is concluded that if the entropy of the posterior probabilities is larger than 0. 95 the proposed approximation is reasonably accurate...|$|R
40|$|Formulas for the {{asymptotic}} biases of the estimators of {{the normal}} theory <b>standard</b> <b>errors</b> in factor analysis are given with and without the assumption of multivariate normality for observed variables. The biases are derived from the <b>asymptotic</b> variances of <b>standard</b> <b>error</b> estimators and the asymptotic biases of the estimated variances of parameter estimators. The latter biases are derived from the asymptotic variances/covariances and asymptotic biases of the parameter estimators. The formulas cover the cases for unstandardized and standardized variables. Numerical examples using factor analysis models show {{the accuracy of the}} formulas. The biases of <b>standard</b> <b>error</b> estimators are theoretically and empirically shown to be of the same order as that {{of the differences between the}} <b>asymptotic</b> <b>standard</b> <b>errors</b> neglecting higher-order terms and those considering them...|$|R
50|$|The idea of profile {{likelihood}} {{can also}} be used to compute confidence intervals that often have better small-sample properties than those based on <b>asymptotic</b> <b>standard</b> <b>errors</b> calculated from the full likelihood. In the case of parameter estimation in partially observed systems, the profile likelihood can be also used for identifiability analysis. Results from profile likelihood analysis can be incorporated in uncertainty analysis of model predictions.|$|R
40|$|The author {{investigates the}} {{determinants}} and dynamics of poverty during the five-year growth period {{that followed the}} 1994 CFA franc devaluation in Burkina Faso. Results show that the nature and dynamics of poverty determinants {{are influenced by the}} spatial location of households and that the post-devaluation growth period did not significantly alter the pattern of poverty determinants. The most significant determinants of poverty over the growth period include the burden of age dependency, human and physical assets, household amenities, and spatial location. Though consistently significant at the national level, the direction of association between these determinants and welfare depends on their nature. While the burden of age dependency is consistently negatively associated with welfare, asset ownership is positively associated. The probability of being poor declines with increasing share of household assets and increases with the burden of age dependency. There are some variations at the regional level, however, shown by the difference in the scope of significance of these determinants. While the ratio of age dependency remains the most significant determinant of rural poverty, its explanatory power decreases considerably inurban areas where its marginal effect on the probability of being poor is relatively low over the two reference periods, despite the significance of the probit coefficient and the relatively low <b>asymptotic</b> <b>standard</b> <b>error.</b> Health Economics&Finance,Environmental Economics&Policies,Services&Transfers to Poor,Public Health Promotion,Health Systems Development&Reform,Poverty Assessment,Achieving Shared Growth,Environmental Economics&Policies,Health Economics&Finance,Poverty Reduction Strategies...|$|E
40|$|Maximum {{likelihood}} and {{censored sample}} theory are applied for flood frequency analysis purposes to the Two Parameter Gamma, log Two Parameter Gamma, Pearson Type III, log Pearson Type III (LP 3), and Generalized Gamma distributions. The logarithmic likelihood functions {{are given in}} terms of the fully specified floods, the historical information, and the parameters to be estimated. Solution of the appropriate transcendental equations yields maximum likelihood estimators of the parameters. T-year floods are expressed as a function of these parameters and the standard normal variate. The <b>asymptotic</b> <b>standard</b> <b>error</b> of estimate of the T-year flood is derived using the general equation for the variance of estimate of a function. The variances and covariances of the parameters are obtained through inversion of Fisher's information matrix. The method is illustrated by application of the LP 3 distribution to two sites having historical information. Monte Carlo studies were conducted for the LP 3 distribution to analytically verify the accuracy of the derived asymptotic expression for the 10 -, 50 -, 100 -, and 500 -year floods. Results indicated that the asymptotic expressions were accurate for both Type I and Type II censored samples, while the bias was less than 2. 5 %. Subsequently, the Type II censored data were subjected to a random, multiplicative error. Results indicated that historical information contributes greatly to the accuracy of the estimate of the 100 -year flood even when the error of its measurement becomes excessive. It is demonstrated that historical information can significantly reduce the standard error of estimate of flood quantiles...|$|E
40|$|AbstractThe {{theory of}} photon count {{histogram}} (PCH) analysis describes {{the distribution of}} fluorescence fluctuation amplitudes due to populations of fluorophores diffusing through a focused laser beam and provides a rigorous framework through which the brightnesses and concentrations of the fluorophores can be determined. In practice, however, the brightnesses and concentrations {{of only a few}} components can be identified. Brightnesses and concentrations are determined by a nonlinear least-squares fit of a theoretical model to the experimental PCH derived from a record of fluorescence intensity fluctuations. The χ 2 hypersurface in the neighborhood of the optimum parameter set can have varying degrees of curvature, due to the intrinsic curvature of the model, the specific parameter values of the system under study, and the relative noise in the data. Because of this varying curvature, parameters estimated from the least-squares analysis have varying degrees of uncertainty associated with them. There are several methods for assigning confidence intervals to the parameters, but these methods have different efficacies for PCH data. Here, we evaluate several approaches to confidence interval estimation for PCH data, including <b>asymptotic</b> <b>standard</b> <b>error,</b> likelihood joint-confidence region, likelihood confidence intervals, skew-corrected and accelerated bootstrap (BCa), and Monte Carlo residual resampling methods. We study these with a model two-dimensional membrane system for simplicity, but the principles are applicable as well to fluorophores diffusing in three-dimensional solution. Using simulated fluorescence fluctuation data, we find the BCa method to be particularly well-suited for estimating confidence intervals in PCH analysis, and several other methods to be less so. Using the BCa method and additional simulated fluctuation data, we find that confidence intervals can be reduced dramatically for a specific non-Gaussian beam profile...|$|E
40|$|In {{this paper}} the {{asymptotic}} {{distribution of the}} absolute residual autocorrelations from generalized autoregressive conditional heteroscedastic (GARCH) models is derived. The correct <b>asymptotic</b> <b>standard</b> <b>errors</b> for the absolute residual autocorrelations are also obtained and based on these results, a diagnostic test for checking the adequacy of GARCH-type models are developed. Our results do {{not depend on the}} existence of higher moments and is therefore robust under heavy-tailed distributions. </p...|$|R
40|$|With a view towards {{lessening}} {{the analytic}} and computational burden faced by researchers in empirical health economics who seek {{an alternative to}} bootstrapping for the <b>standard</b> <b>errors</b> of two-stage estimators, we offer heretofore unexploited simplifications of the typical, but somewhat daunting, textbook approach. For the most commonly encountered cases in empirical health economics – two-stage estimators that, in either stage, involve maximum likelihood estimation or the nonlinear least squares method – we show that: 1) the usual textbook formulation of the relevant asymptotic covariance can be substantially reduced in complexity; and 2) nearly all components of our simplified formulation can be retrieved as outputs from packaged regression routines (e. g., in Stata). With the applied researcher in mind, we illustrate these points with two examples in empirical health economics that involve the estimation of causal effects {{in the presence of}} endogeneity – a sampling problem that can often be solved via two-stage estimation. As a by-product of this illustrative discussion, we detail four very useful two-stage estimators (and their <b>asymptotic</b> <b>standard</b> <b>errors)</b> that are consistent for the model parameters in such settings, along with their corresponding multi-stage causal effect estimators (and their <b>asymptotic</b> <b>standard</b> <b>errors)</b> ...|$|R
40|$|We derive <b>asymptotic</b> <b>standard</b> <b>errors</b> of risk premia {{estimates}} {{based on}} the popular two-pass cross-sectional regression methodology developed by Black, Jensen, and Scholes (1972) and Fama and MacBeth (1973) when univariate betas are used as regressors. Our <b>standard</b> <b>errors</b> are robust to model misspecification and allow for general distributional assumptions. In testing whether the beta risk of a given factor is priced, our misspecification robust <b>standard</b> <b>error</b> can lead to economically different conclusions from those {{based on the}} Jagannathan and Wang (1998) <b>standard</b> <b>error</b> which is derived under the correctly specified model. Asset pricing models Risk premia Univariate betas Model misspecification...|$|R
