4|10000|Public
40|$|The {{statistical}} technique {{known as}} {{analysis of variance}} (ANOVA) requires several <b>assumptions</b> <b>to</b> <b>be</b> <b>met.</b> My thesis examines the effect of different violations of assumptions {{on the size of}} the test. I found that the size of the test is only influenced when different treatment groups have different sample sizes and are drawn from distributions with different standard deviations. Other violations of the ANOVA assumptions, such as the assumption that treatment groups must be drawn from the same family of distribution, do not significantly impact the size of the test...|$|E
40|$|Most process {{research}} {{relies heavily}} {{on the use of}} terms and concepts whose validity depends on a variety of <b>assumptions</b> <b>to</b> <b>be</b> <b>met.</b> As it is difficult to guarantee that they are met, such work continually runs the risk of being invalid. We propose a different and complementary approach to understanding process: Perform all description bottom-up and based on hard data alone. We call the approach actual process and the data actual events. Actual events can be measured automatically. This paper describes what has been done in this area already and what are the core problems to be solved in the future...|$|E
40|$|In many recent {{analyses}} of energy consumption, the following functional form is assumed: Qt = BXt + [lambda]Qt- 1 + et where Qt is a vector of consumption data, Xt is {{a matrix of}} explanatory economic variables, Qt- 1 is a vector of consumption lagged one observation period, B and [lambda] are regression parameters to be estimated and et is a vector of appropriately distributed random disturbances. Although this specification is attractive {{in the sense that}} both short- and long-term elasticities can be obtained from it, it does require certain restrictive <b>assumptions</b> <b>to</b> <b>be</b> <b>met</b> in order for it to be theoretically valid. This paper tests those assumptions and finds them not met for certain petroleum products. An alternative specification is suggested and policy differences between the popular but incorrect specification and the suggested one are presented. These differences are shown to be substantial. ...|$|E
5000|$|Bootstrapping is {{becoming}} the most popular method of testing mediation {{because it does not}} require the normality <b>assumption</b> <b>to</b> <b>be</b> <b>met,</b> and because it can be effectively utilized with smaller sample sizes (N < 25). However, mediation continues <b>to</b> <b>be</b> most frequently determined using the logic of Baron and Kenny [...] or the Sobel test. It {{is becoming}} increasingly more difficult to publish tests of mediation based purely on the Baron and Kenny method or tests that make distributional assumptions such as the Sobel test. Thus, it <b>is</b> important <b>to</b> consider your options when choosing which test to conduct.|$|R
30|$|It {{is known}} that SEA is {{computationally}} very efficient but relies on quite a few assumptions, such as that each subsystem is of diffuse field and/or that waves are uncorrelated. Since these major <b>assumptions</b> tend <b>to</b> <b>be</b> <b>met</b> only above a certain frequency limit, the method <b>is</b> restricted <b>to</b> the high-frequency range. Enhanced statistical methods therefore try to extend the statistical approaches towards lower frequencies [14].|$|R
30|$|We {{have shown}} that polices devoting higher {{resources}} to disadvantaged students typically lead to small individual benefits. However, it <b>is</b> difficult <b>to</b> compare these low returns with those found for programs involving the whole population of students because individuals with different characteristics <b>are</b> likely <b>to</b> differ {{in terms of their}} policy responsiveness. In addition, as the underlying <b>assumptions</b> that have <b>to</b> <b>be</b> <b>met</b> in order <b>to</b> obtain estimates of causal estimates differ across estimation methods, particular care must be paid when comparing evaluations using different approaches.|$|R
40|$|International audienceThe GPS and GALILEO {{systems will}} take {{advantage}} of new signal modulations such as Binary Offset Carrier (BOC) that uses a square wave sub-carrier to create separate spectra {{on each side of the}} transmitted carrier. That signal could share the existing frequency bands with each other while reserving the spectrum, it provides spectral isolation and leads to significant improvements in terms of tracking, interference and multipath mitigation. As this BOC modulation, along with a modified version called alternate BOC (ALTBOC), is a serious candidate for GALILEO and GPS, it is important to understand all characteristics of this signal, in order to conduct several studies like payload design, receiver implementation, performance and robustness evaluation. One of the aspects of this signal is the power spectrum density that impacts all characteristics presented above. The aim of this paper is, in a first part, to review the currently admitted BOC power spectrum density theoretical expressions, and to discuss a deviation of the assumptions necessary for the derivation of these classical theoretical expressions w. r. t to the reality of these signals. Then, a second objective is to present theoretical expressions of ALTBOC and constant envelope ALTBOC power spectrum densities. First the paper recalls the formal expressions of the different possible offset carrier modulations: BOC, ALTBOC and constant envelop ALTBOC signals in the time domain. Then, the assumptions for the development of the classical power spectrum density calculation are clearly laid down. Next, it is shown that in the case of odd ratio (2 *fs/fc) BOC signals, these assumptions, and particularly the fact that the BOC square sub-carrier can be incorporated in the chip waveform, are not met. Then, we show that, for these calculation <b>assumptions</b> <b>to</b> <b>be</b> <b>met</b> in that case of odd ratio BOC, it is equivalent to modify the code sequence. Finally, with this view, the obtained power spectrum density is the same as the one obtained with the classical theory, due to the negligible effect of the code sequence correlation values. Finally, we present the theoretical expressions of the ALTBOC and constant envelope ALTBOC power spectrum densities...|$|E
40|$|In {{the setting}} of {{competing}} risks, the marginal survival functions of the latent failure times are nonidentifiable without making further assumptions about the joint distribution, the majority of which are untestable. One exception is the random signs censoring assumption which assumes the main event time is independent of the indicator that the main event preceded the competing event. Few methods exist to formally test this assumption, and none consider a stratified test, which detects whether random signs censoring <b>is</b> <b>met</b> within subgroups of a categorical covariate. We develop a nonparametric stratified test for random signs censoring that <b>is</b> easy <b>to</b> implement. In addition, it is often of interest to model the effects of several covariates {{in relation to the}} cause of interest. Thus, {{as an extension of the}} stratified test, we also propose a test for conditional random signs censoring, which allows for the random signs censoring <b>assumption</b> <b>to</b> <b>be</b> <b>met</b> after adjusting for categorical and/or continuous covariates. Through Monte Carlo simulations, we show our proposed test statistics have empirical levels close to the nominal level and maintain adequate power even with relatively small sample sizes and random right censoring. Compared to the standard test, both of our proposed tests have nearly equivalent power under random signs censoring and are superior in situations of stratified or conditional random signs censoring, where the standard test fails to detect random signs censoring within subgroups or after adjusting for covariates, respectively. Their ease of implementation and utility are illustrated through an application to liver transplant data from the United Network for Organ Sharing. Public Health Significance: Clinicians must make decisions affecting patients’ lives using the information available to them. Relying on research results based on models that use unverifiable <b>assumptions</b> can lead <b>to</b> inaccurate conclusions. The methods proposed here offer a solution to allow for more accurate modeling of marginal survival functions with competing risk data. Through use of these new methods, patient outcomes can be improved over time...|$|R
40|$|The clean {{technology}} industry (including recycling, renewable energy, information technology, green transportation, electric motors, green chemistry, lighting, etc) {{has been one}} of the fastest growing sectors, and billions of dollars have been poured into the industry from investors and governments worldwide. Having been through two boom-bust cycles in the 21 st century alone, many investors are worried whether the industry is worth the investment. The concern mainly comes from overheated investment into the industry and investors’ overly high expectations. This study will compare the {{clean technology}} industry with the dot-com industry in 2000, and perform an industry valuation analysis and then compare the result with the current investment level. By building a discounted cash flow model of selected companies in the solar photovoltaic (PV) sector, the equity value is calculated and compared with the current market cap of the company set. In addition, this thesis will see what <b>assumptions</b> have <b>to</b> <b>be</b> <b>met</b> for the current investment level, and whether those assumptions are realizable...|$|R
40|$|The {{evidence}} for amphibian population declines {{is based on}} count data that were not adjusted for detection probabilities. Such data are not reliable even when collected using standard methods. The formula C = Np (where C is a count, N the true parameter value, and p is a detection probability) relates count data to demography, population size, or distributions. With unadjusted count data, one assumes a linear relationship between C and N and that p is constant. These <b>assumptions</b> <b>are</b> unlikely <b>to</b> <b>be</b> <b>met</b> in studies of amphibian populations. Amphibian population data {{should be based on}} methods that account for detection probabilities...|$|R
30|$|There is {{an ongoing}} debate about using Likert-type data and scales for {{standard}} multiple regression analysis. This contention focuses on whether Likert-scales can be treated as interval data, which is a key <b>assumption</b> that has <b>to</b> <b>be</b> <b>met</b> for multiple regression analysis (Field 2009). In the field of social sciences (in which this study is situated), Likert-type data are consistently treated as interval data and used in the regression analysis (Johnson and Slovic 1995; Peters et al. 1997; Sjoberg 1998; Leiserowitz 2006). Because of this, additional steps were taken in this research to further decrease the likelihood of information loss and erroneous results {{as well as to}} acknowledge the arguments by skeptics, who caution against the use of Likert-type scales as interval data.|$|R
40|$|Record {{matching}} is {{a fundamental}} and ubiquitous part of today?s society. Anything from typing in a password in order to access your email to connecting existing health records in California with new health records in New York requires matching records together. In general, {{there are two types}} of record matching algorithms: deterministic, a more rules-based approach, and probabilistic, a model-based approach. Both types have their advantages and disadvantages. If the amount of data is relatively small, deterministic algorithms yield very high success rates. However, the number of common mistakes, and subsequent rules, becomes astronomically large as the sizes of the datasets increase. This leads to a highly labor-intensive process updating and maintaining the matching algorithm. On the other hand, probabilistic record matching implements a mathematical model that can take into account keying mistakes, does not require as much maintenance and over- head, and provides a probability that two particular entities should be linked. At the same time, as a model, <b>assumptions</b> need <b>to</b> <b>be</b> <b>met,</b> fitness has <b>to</b> <b>be</b> assessed, and predictions can be incorrect. Regardless of the type of algorithm, nearly all utilize a 0 / 1 field-matching structure, including the Fellegi-Sunter algorithm from 1969. That <b>is</b> <b>to</b> say that either the fields match entirely, or they do not match at all. As a result, typographical errors can get lost and false negatives can result. My research has yielded that using Jaro-Winkler string comparator scores as predictors to a Bayesian logistic regression model in lieu of a restrictive binary structure yields marginal improvement over current methodologies...|$|R
40|$|When {{estimating}} {{future or}} pro forma financial statements and free cash flows {{we need to}} estimate future prices. In doing this we must estimate nominal increases in prices of many items, for instance selling prices, inputs prices (raw material, labor, overhead, etc.), cost of future debt, and others. If we set nominal price increases without a proper link to inflation we might end up with price increases independent of the inflation rate. The purpose of this teaching note <b>is</b> <b>to</b> present an approach to estimate nominal price increases examining historical nominal prices and inflation rates. This way we can "discover" which policy, if any, the decision maker used to fix prices assuming {{that she has a}} fair estimate of immediate future inflation rate. This approach can <b>be</b> used <b>to</b> asses the risk premium a debt holder (in case it is a bank loan) <b>is</b> applying <b>to</b> the cost of debt. This way we could estimate the cost of future debt, given an estimation of future inflation rate. We present an appendix where the formal <b>assumptions</b> that has <b>to</b> <b>be</b> <b>met</b> for robust econometric analysis. ...|$|R
40|$|Goldhammer (this issue) proposes an {{interesting}} approach {{to dealing with}} the speededness of item responses. Rather than modeling speed as a latent variable that varies from person to person, he proposes to use experimental conditions that <b>are</b> expected <b>to</b> fix the speed, thereby eliminating individual differences on this dimension in order to make unconfounded comparisons of a person’s ability possible. We applaud his efforts for considering the gains that can be obtained by changing the test conditions to better match the measurement aims of ability tests, rather than just considering altering the measurement model. We agree that the model provides {{an interesting}} theoretical exploration into possible conditions under which the measurement of speed and ability would not be confounded by the speed ability compromise. However, the model <b>is</b> only able <b>to</b> achieve this unconfounded measurement of ability by imposing a number of restrictive assumptions. We believe that the merit of the approach will depend on the extent <b>to</b> which these <b>assumptions</b> <b>are</b> likely <b>to</b> <b>be</b> <b>met</b> in practice. We will discuss two main concerns: issues with the practical realizability of fixing effective speed and consequences of fixing speed for the measurement of ability...|$|R
40|$|Abstract- Measurement of {{software}} reliability by life testing involves executing the software on {{large numbers of}} test cases and recording the results. The number of failures observed <b>is</b> used <b>to</b> bound the failure probability even {{if the number of}} failures observed is zero. Typical analyses assume that all failures that occur are observed, but, in practice, failures occur without being observed. In this paper, we examine the effect of imperfect error detection, i. e., the situation in which a failure of the software may not be observed. If a conventional analysis associated with life testing is used, the confidence in the bound on the failure probability is optimistic. Our results show that imperfect error detection does not necessarily limit the ability of life testing to bound the probability of failure to the very low values required in critical systems. However, we show that the confidence level associated with a bound on failure probability cannot necessarily be made as high as desired, unless very strong assumptions are made about the error detection mechanism. Such <b>assumptions</b> <b>are</b> unlikely <b>to</b> <b>be</b> <b>met</b> in practice, and so life testing <b>is</b> likely <b>to</b> <b>be</b> useful only for situations in which very high confidence levels are not required. Index Terms-Error detection, software reliability assessment, software testing, test oracles...|$|R
40|$|Spatial capture–recapture (SCR) {{models are}} arelatively recent {{development}} in quantitative ecology, andthey are becoming widely used to model density in studiesof animal populations using camera traps, DNA samplingand other methods which produce spatially explicit individualencounter information. One {{of the core}} assumptionsof SCR models is that individuals possess home ranges thatare spatially stationary during the sampling period. Formany species, this <b>assumption</b> <b>is</b> unlikely <b>to</b> <b>be</b> <b>met</b> and,even for species that are typically territorial, individualsmay disperse or exhibit transience at some life stages. Inthis paper we first conduct a simulation study to evaluatethe robustness of estimators of density under ordinary SCRmodels when dispersal or transience is present in thepopulation. Then, using both simulated and real data, wedemonstrate that such models can easily be described in theBUGS language providing a practical framework for theiranalysis, which allows us to evaluate movement dynamicsof species using capture–recapture data. We find that whileestimators of density are extremely robust, even to pathologicallevels of movement (e. g., complete transience), theestimator of the spatial scale parameter of the encounterprobability model is confounded with the dispersal/transiencescale parameter. Thus, use of ordinary SCR modelsto make inferences about density is feasible, but interpretationof SCR model parameters in relation <b>to</b> movementshould <b>be</b> avoided. Instead, when movement dynamics areof interest, such dynamics should be parameterizedexplicitly in the model...|$|R
40|$|Multi-year tag-recovery {{models can}} <b>be</b> used <b>to</b> derive {{estimates}} of age- and year-specific annual survival rates and year-specific instantaneous fishing and natural mortality rates. The latter, {{which are often}} of interest to fisheries managers, usually can only be estimated when the tag-reporting rate () and the short-term tag-induced mortality and tag-shedding rate () are known a priori. We present a new multi-year tagging model that permits estimation of instantaneous mortality rates independently of, provided tagged animals from two adjacent size groups are released simultaneously. If the two size groups comprise animals just {{above and below the}} minimum harvestable size limit, then it <b>is</b> possible <b>to</b> estimate year-specific instantaneous fishing and natural mortality rates after 2 yr of tagging and tag-recovery. In addition <b>to</b> the standard <b>assumptions</b> of multi-year tag-recovery models, it <b>is</b> necessary <b>to</b> assume that recruited animals have equal selectivity, pre-recruited animals become fully recruited in 1 or 2 yr, and the size groups experience the same natural mortality rate. Applicability of the model to the Tasmania southern rock lobster (Jasus edwardsii) fishery is evaluated using a simulation model and parameters based on data from the lobster fishery; <b>assumptions</b> <b>are</b> likely <b>to</b> <b>be</b> <b>met</b> and precision should be adequate if at least 1000 animals are tagged per year in each size group...|$|R
40|$|Processes of {{evaluation}} of crop varieties {{for inclusion in}} a recommended list are {{based on data from}} series of field experiments conducted at a number of locations (sites) and possibly over several years. Such series of variety trials are called multi-environment variety trials (METs). There exist an immense literature on transforming MET data into information useful for breeders and crop evaluators. It reflects different approaches to the statistical analysis of such data. Going back to the origins, it may <b>be</b> interesting <b>to</b> recall the approach of Yates and Cochran (1938, Section 2) who indicated that although “it <b>is</b> usually impossible <b>to</b> secure a set of sites selected entirely at random”, it <b>is</b> advisable <b>to</b> proceed {{in such a way that}} the experimental fields actually used in the trials could be considered as “representative ” for “all fields which <b>are</b> <b>to</b> <b>be</b> covered by the subsequent recommendations”. This approach has been adopted by organizers of variety trials in many countries. Hence, when modelling the analysis of MET data, a mixed model is usually adopted (see the overview by Denis, Piepho, and van Eeuwijk, 1997; Smith, Cullis, and Thompson, 2005). In deriving a mixed model for such analysis various <b>assumptions</b> related <b>to</b> <b>METs</b> <b>are</b> usually made. They have to take into account that in these trials (a) “the varieties are individual...|$|R
40|$|The {{phenomenon}} of skewed sex ratios at birth {{has been reported}} in many ungulate species. So far, no consistent trend has emerged for roe deer (Capreolus capreolus), because male-biased, female-biased and equal sex ratios at birth have all been found. Nevertheless, both the Trivers-Willard hypothesis and the theory of local resource competition have gained support. Despite the great number of studies carried out regarding the ecology of roe deer, too many aspects remain unclear, and contradictory results have been produced with respect to several crucial elements. Without further research, the discussion on which theory applies will therefore remain inconclusive. We put forward the argument that eventually the theories of Trivers-Willard and local resource competition can be considered as being not essentially different. After all, both theories explain the observed skewed sex ratios as being due to the effect of the progeny's sex on the mother's body condition and hence her reproductive success in subsequent years. Furthermore, neither theory <b>is</b> likely <b>to</b> prove <b>to</b> <b>be</b> suitable for roe deer, as several <b>assumptions</b> <b>are</b> unlikely <b>to</b> <b>be</b> <b>met.</b> In roe deer, skewed ratios probably only have a temporal character. As a matter of fact, several observations of skewed sex ratios in birds and mammals did not withstand the accumulation of further data, as sex ratios that <b>were</b> initially believed <b>to</b> <b>be</b> biased turned out <b>to</b> <b>be</b> equal in the long term. This <b>is</b> likely <b>to</b> <b>be</b> the case in roe deer as well. We hypothesize that roe deer, as r-strategists, will produce as many offspring as possible, regardless of sex...|$|R
40|$|Official Statistics bureaus <b>are</b> {{periodically}} asked <b>to</b> give {{an estimate}} of their country's population, which can be defined {{by the number of}} usual residents. A person is considered a usual resident when they have lived in the Netherlands for longer than a year, or if they have the intention to reside for longer than a year. For the Dutch Census, Statistics Netherlands makes use of the Population Register (PR). However, for numerous reasons, immigrants that have taken residence in the Netherlands may not register and become undocumented immigrants. Thus, the PR alone <b>is</b> not sufficient <b>to</b> estimate the number of usual residents, and has an undercoverage considering the number of Dutch usual residents. One commonly used method to estimate population sizes is the capture-recapture methodology. First the PR <b>is</b> linked <b>to</b> two other registers. Then capture-recapture methodology using a covariate that denotes residence duration can <b>be</b> used <b>to</b> estimate the number of usual residents missed by all three registers. However, for the valid use of capture-recapture methodology, a set of <b>assumptions</b> has <b>to</b> <b>be</b> <b>met.</b> Additionally, practical issues such as missing data may occur. Such practical issue have <b>to</b> <b>be</b> resolved before one can estimate the number of Dutch usual residents via capture-recapture methodology. For that purpose there are two central questions answered in this thesis: 1) what is the effect of violated assumptions and missing data on the robustness of population size estimation via capture-recapture methodology, and 2) how can the information gained in 1) <b>be</b> used <b>to</b> achieve a trustworthy estimate of the under coverage of usual residents in the Population Register in the Netherlands? To answer the first question in this thesis, research has been conducted into the robustness of population size estimation via capture-recapture methodology when the following assumptions are violated: 1) independence of the inclusion probabilities of the registers, 2) no erroneous captures in the registers, and 3) perfect linkage of the units in the used registers. For the independence assumption, this research also investigated the robustness for independence conditional on fully and partially observed covariates. Additionally research has been conducted into the effect missing data have on the population size estimation, and most notably how different methods of handling missing data differ in their effect on the resulting population size estimate. It has been found that implied coverage of one register, given the other register is important tot he extent that violated assumptions will bias the population size estimation. Implied coverage {{plays an important role in}} this thesis given that it cannot be ascertained from the data whether assumptions are violated, but implied coverage can. The results obtained in answering the first question have <b>been</b> used <b>to</b> conduct research into the undercoverage of the PR of the Netherlands. It is concluded that for reference date september 2010, the PR has an undercoverage of 0, 5 to 1, 1 % usual residents...|$|R
40|$|The {{economics}} {{discipline is}} broadly {{concerned with the}} allocation of scarce societal resources {{in the context of}} unlimited societal wants. Intrinsic <b>to</b> economics <b>is</b> the concept of choice – that is, how can we best use scarce societal resources when our wants are greater than the resources available to us. If we <b>were</b> able <b>to</b> satisfy all our wants and needs with our available resources, there would be no need for the discipline of economics! In most economies, markets <b>are</b> used <b>to</b> make these decisions. Markets are basically a mechanism whereby consumers and producers interact {{in such a way that}} the “best” allocation of resources <b>is</b> thought <b>to</b> occur. This “best” allocation of resources in economics <b>is</b> said <b>to</b> <b>be</b> an efficient allocation. Efficiency basically assumes that the correct types of services are being produced (allocative efficiency) in the least resource-intensive way (technical efficiency). Inherent within all these concepts is not just cost but also the benefit derived from the consumption of different goods and services. A central tenant of economics is the concept of opportunity cost whereby the true cost of any given action (or service) is the benefit which would have been attained if the resources used in providing that action or service were used in an alternative way. Therefore, both costs and benefits <b>are</b> central <b>to</b> the economic way of thinking. Contrary to much public perception, economics is not necessarily about cutting costs; rather, it is about using resources in the “best” possible way. Inherent within this idea of “best” is “value,” “benefit,” or “utility” (utility is the term most often seen in economics textbooks to refer to the value of using resources). Unfortunately, there are many <b>assumptions</b> which need <b>to</b> <b>be</b> <b>met</b> for markets <b>to</b> operate in an ideal way. One important assumption is that consumers of goods and services need <b>to</b> <b>be</b> aware of the full impact and consequences of all consumption choices. When market failures occur, governments can sometimes intervene in the operation of markets either because the markets are not working properly (largely because the assumptions underpinning the market mechanism <b>are</b> not <b>met)</b> or for social-justice or equity considerations (Rice and Unruh, 2009) ...|$|R
40|$|This is a {{case for}} teaching. This case shows through several {{examples}} that the Net Present Value for project evaluation should be calculated based on estimates at current prices. It has been a widespread practice to evaluate projects at constant prices {{with a great deal}} of -today- unnecessary oversimplifications. An example is presented were it is shown that the constant price methodology (zero increase in prices of year 0 and real discount rate) is biased upwards and there is a risk that bad projects in the reality be accepted as good projects. Example Setting: Hypothetical firm in an inflationary environment. In this example it is shown how the usual procedure for evaluating a project (i. e., assuming constant prices or constant dollars and a deflated or real rate of discount) could give an inappropriate investment recommendation. Situation: This is a technical note, useful for supporting a lecture, class discussion, or case analysis. The example of a hypothetical firm illustrates how to construct a free cash flow based on given parameters (inflation rate, real interest rate, risk premiums, prices, price increases, elasticity function, accounts receivable and accounts payable policies, etc.) and then value the firm. This technical note has six objectives: * illustrate how to construct a pro-forma financial statement, such as Balance Sheet, Profit and Loss Statement, and cash flow forecast for the new firm. * show how the financial evaluation of the firm as a project made with constant prices and/or constant dollar and real or deflated interest rates might differ from the evaluation of the same project with current or nominal prices. * suggest the conditions or <b>assumptions</b> that have <b>to</b> <b>be</b> <b>met</b> in order for the two approaches (the constant price approach and the current or nominal price approach) to give equal results (this is, identical NPV). * show which problems in the follow up and monitoring of a project might be present when working with the constant price approach. * illustrate why NPV calculated at constant prices and real or deflated rate of interest is, in general, different to the NPV calculated at current or nominal prices and discounted at nominal rates of interest, contrary <b>to</b> what <b>is</b> written in many financial textbooks. * describe how a spreadsheet might <b>be</b> utilized <b>to</b> make a more sophisticated analysis and avoid unnecessary, but widely-used oversimplifications. An Excel spreadsheet accompanies the note and allows students to conduct sensitivity analysis on the project's NPV and other results. ...|$|R
30|$|Engineers, physicists, and mathematicians {{have always}} shown their incessant {{interest}} in studying nonlinear {{problems related to}} numerous scientific applications, such as fluid dynamics, high-energy physics, plasma physics, elastic media, optical fibers, biomathematics, chemical kinematics, chemical physics and geochemistry. Many young scientists have also shown their increased interest in last two decades because of plausible development of nonlinear science {{during this period of}} time. In order to understand the behavior of a nonlinear phenomenon we need to solve the nonlinear equation/set of equations describing that phenomenon, is often very much challenging. There are so many approaches developed over years to analyze/solve such system of nonlinear equations, most of them are based on some assumptions, and hence approximations. Though perturbation methods, like other nonlinear analysis techniques, have their own limitations, are most useful methods among all these approaches so far. Using perturbation method, to achieve the ideal results an appropriate choice of small parameter has <b>to</b> <b>be</b> made efficiently, otherwise, a fatal error in results may occur. The perturbation methods are not even applicable to many nonlinear equations because of not having small parameter, which is the principal <b>assumption</b> that has <b>to</b> <b>be</b> <b>met</b> by an equation in order to apply perturbation method. Furthermore, the approximate solutions obtained using perturbation methods are valid only for the small values of the parameters (Ghorbani and Saberi-Nadjafi [2007]; Mohiud-Din [2007]; Mohyud-Din and Noor, [2009]). The investigation of exact traveling wave solutions to these nonlinear equations (NPDEs) have also been observed as a field of great interest to many mathematicians and physicists because of its significant role in understanding the behavior of nonlinear physical phenomena. As a result, numerous techniques of obtaining traveling wave solutions have been developed over last three decades, such as, the Hirota’s bilinear transformation method (Hirota [1973, 1981]), the modified simple equation method (Jawad et al. [2010]; Khan and Akbar [2013 a]; Ahmed et al. [2013]; Zayed and Hoda [2013]; Zayed and Arnous [2012]), the tanh-function method (Wazwaz [2005]; Parkes and Duffy [1996]), the Exp-function method (He and Wu [2006]; Akbar and Ali [2011 b]; Bekir and Boz [2008]; Xu et al. [2009]), the Jacobi elliptic function method (Ali [2011]), the (G '/G) -expansion method (Akbar et al. [2012 a, 2012 b]; Akbar and Ali [2011 a]; Wang et al. [2008]; Shehata [2010]; Koll and Tabi [2011]; Naher et al. [2013]; Zayed [2009, 2010]; Aslan [2010]; Bekir and Aksoy [2012]), the homotopy perturbation method (Mohiud-Din [2007]; Mohyud-Din and Noor [2009]), transformed rational function method (Ma and Jyh [2009]; Ma and Fuchssteiner [1996]), multiple exp-function method (Ma et al. [2010]; Ma and Zhu [2012]), generalize Hirota bilinear method (Ma[*][2013]), enhanced (G[*]'/G) -expansion method (Khan and Akbar [2013 b]), The Sine-Cosine method (Bibi and Mohyud-Din [2013]), the first integral method (Tascan and Bekir [2010]; Feng [2002]), the ansatz method (Hu [2001 a, 2001 b]) and many others.|$|R
40|$|We {{developed}} a data-mining method, Model-Based Multifactor Dimensionality Reduction (MB-MDR) to detect epistatic interactions for {{different types of}} traits. MB-MDR enables the fast identification of gene-gene interactions among 1000 nds of SNPs, without the need <b>to</b> make restrictive <b>assumptions</b> about the genetic modes of inheritance. This thesis primarily focused on applying Model-Based Multifactor Dimensionality Reduction for quantitative traits, its performance and application {{to a variety of}} data problems. We carried out several simulation studies to evaluate quantitative MB-MDR in terms of power and type I error, when data are noisy, non-normal or skewed and when important main effects are present. Firstly, we assessed the performance of MB-MDR in the presence of noisy data. The error sources considered were missing genotypes, genotyping error, phenotypic mixtures and genetic heterogeneity. Results from this study showed that MB-MDR is least affected by presence of small percentages of missing data and genotyping errors but much affected in the presence of phenotypic mixtures and genetic heterogeneity. This is in line with a similar study performed for binary traits. Although both Multifactor Dimensionality Reduction (MDR) and MB-MDR are data reduction techniques with a common basis, their ways of deriving significant interactions are substantially different. Nevertheless, effects on power of introducing error sources were quite similar. Irrespective of the trait under consideration, epistasis screening methodologies such as MB-MDR and MDR mainly suffer from the presence of phenotypic mixtures and genetic heterogeneity. Secondly, we extensively addressed the issue of adjusting for lower-order genetic effects during epistasis screening, using different adjustment strategies for SNPs in the functional SNP-SNP interaction pair, and/or for additional important SNPs. Since, in this thesis, we restrict attention to 2 -locus interactions only, adjustment for lower-order effects always (and only) implies adjustment for main genetic effects. Unfortunately most data dimensionality reduction techniques based on MDR do not explicitly require that lower-order effects are included in the ‘model’ when investigating higher-order effects (a prerequisite for most traditional, especially regression-based, methods). However, epistasis results may be hampered by the presence of significant lower-order effects. Results from this study showed hugely increased type I errors when main effects were not taken into account or were not properly accounted for. We observed that additive coding (the most commonly used coding in practice) in main effects adjustment does not remove all of the potential main effects that deviate from additive genetic variance. In addition, also adjusting for main effects prior to MB-MDR (via a regression framework), whatever coding is adopted, does not control type I error in all scenarios. From this study, we concluded that correction for lower-order effects should preferentially be done via codominant coding, to reduce the chance of false positive epistasis findings. The recommended way of performing an MB-MDR epistasis screening <b>is</b> <b>to</b> always adjust the analysis for lower-order effects of the SNPs under investigation, “on-the-fly”. This correction avoids overcorrection for other SNPs, which {{are not part of the}} interacting SNP pair under study. Thirdly, we assessed the cumulative effect of trait deviations from normality and homoscedasticity on the overall performance of quantitative MB-MDR to detect 2 -locus epistasis signals in the absence of main effects. Although MB-MDR itself is a non-parametric method, in the sense that no assumptions are made regarding genetic modes of inheritance, the data reduction part in MB-MDR relies on association tests. In particular, for quantitative traits, the default MB-MDR way <b>is</b> <b>to</b> use the Student’s t-test (steps 1 and 2 of MB-MDR). Also when correcting for lower-order effects during quantitative MB-MDR analysis, we intrinsically maneuver within a regression framework. Since the Student’s t-statistic is the square root of the ANOVA F-statistic. Hence, along these lines, for MB-MDR to give valid results, ANOVA <b>assumptions</b> have <b>to</b> <b>be</b> <b>met.</b> Therefore, we simulated data from normal and non-normal distributions, with constant and non-constant variances, and performed association tests via the student’s t-test as well as the unequal variance t-test, commonly known as the Welch’s t-test. At first somewhat surprising, the results of this study showed that MB-MDR maintains adequate type I errors, irrespective of data distribution or association test used. On the other hand, MB-MDR give rise to lower power results for non-normal data compared to normal data. With respect to the association tests used within MB-MDR, in most cases, Welch’s t-test led to lower power compared to student’s t-test. To maintain the balance between power and type I error, we concluded that when performing MB-MDR analysis with quantitative traits, one ideally first rank-transforms traits to normality and then applies MB-MDR modeling with Student’s t-test as choice of association test. Clearly, before embarking on using a method in practice, there <b>is</b> a need <b>to</b> extensively check the applicability of the method to the data at hand. This is a common practice in biostatistics, but often a forgotten standard operating procedure in genetic epidemiology, in particular in GWAI studies. In addition to the presentation of extensive simulation studies, we also presented some MB-MDR applications to real-life data problems. These analyses involved MB-MDR analyses on quantitative as well as binary complex disease traits, primarily in the context of asthma/allergy and Crohn’s disease. In two of the presented analyses, MB-MDR confirmed logistic regression and transmission disequilibrium test (TDT) results. Part of the aforementioned methodological developments was initiated on the basis of observations of MB-MDR behavior on real-life data. Both the practical and theoretical components of this thesis confirm our belief in the potential of MB-MDR as a promising and versatile tool for the identification of epistatic effects, irrespective of the design (family-based or unrelated individuals) and irrespective of the targeted disease trait (binary, continuous, censored, categorical, multivariate). A thorough characterization of the different faces of MB-MDR this versatility gives rise <b>to</b> <b>is</b> work in progress...|$|R
30|$|We list here more <b>assumptions</b> <b>to</b> <b>be</b> used later.|$|R
30|$|We {{list the}} {{following}} <b>assumptions</b> <b>to</b> <b>be</b> {{used in this}} paper.|$|R
50|$|Determine {{the minimum}} {{conditions}} <b>to</b> <b>be</b> <b>met</b> by those.|$|R
30|$|For {{the sake}} of convenience, we list the <b>assumptions</b> <b>to</b> <b>be</b> used in this section.|$|R
2500|$|Requirements analysis: {{determining}} {{the conditions that}} need <b>to</b> <b>be</b> <b>met</b> ...|$|R
50|$|These demands had <b>to</b> <b>be</b> <b>met</b> 48 {{hours after}} reception.|$|R
5000|$|Requirements analysis: {{determining}} {{the conditions that}} need <b>to</b> <b>be</b> <b>met</b> ...|$|R
30|$|For the {{convenience}} in presentation, we here list the <b>assumption</b> <b>to</b> <b>be</b> {{used throughout the}} paper.|$|R
5000|$|Deficit in {{foodgrains}} {{storage capacity}} <b>to</b> <b>be</b> <b>met</b> with private-sector participation.|$|R
30|$|For {{the sake}} of convenience, we list the <b>assumptions</b> <b>to</b> <b>be</b> used in this paper as follows.|$|R
50|$|A simple model (with <b>assumptions</b> <b>to</b> <b>be</b> {{detailed}} later) <b>is</b> helpful <b>to</b> illustrate Tiebout's {{insight and}} theory.|$|R
5000|$|Appendix V: Criteria <b>to</b> <b>be</b> <b>met</b> when {{authorizing user}} {{inspectorates}} (article 14) ...|$|R
