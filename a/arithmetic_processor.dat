34|56|Public
40|$|SNAP [...] - the Stanford subnanosecond <b>arithmetic</b> <b>processor</b> [...] - is an {{interdisciplinary}} {{effort to develop}} theory, tools, and technology for realizing an <b>arithmetic</b> <b>processor</b> with execution rates under 1 ns. Specific improvements in clocking methods, floating-point addition algorithms, floating-point multiplication algorithms, division and higher-level function algorithms, design tools, and packaging technology were studied. These improvements have been demonstrated {{in the implementation of}} several VLSI designs. 1 Background Arithmetic operations including both integer and floating point add, subtract, multiply, and divide form the basic building blocks of scientific computation and signal processing. Yet usually the optimization of their performance is relegated to design exercises in which the building blocks, the circuits/package/technology have been predefined and only the algorithm itself and its implementation can be altered to achieve performance. The Stanford subnanosecond arithm [...] ...|$|E
40|$|The authors {{present a}} simple method, {{based on the}} {{principle}} of successive approxmation, of computing a function using a futed-point <b>arithmetic</b> <b>processor.</b> As an example, computing the square root function is considered and the performance is compared with Newton's method, in terms of accuracy and the number of instruction cycles required...|$|E
40|$|This paper {{describes}} an efficient <b>arithmetic</b> <b>processor</b> for elliptic curve cryptography. The proposed processor consists of special architectural components, {{the most important}} of which is a modular multiplication unit implemented using the systolic Montgomery multiplication algorithm. Another novelty of our proposed architecture is that it implements the field GF (3 m), which provides significant performance gains. 1...|$|E
50|$|The ES is {{a highly}} {{parallel}} vector supercomputer system of the distributed-memory type, and consisted of 160 processor nodes connected by Fat-Tree Network. Each Processor nodes is a system with a shared memory, consisting of 8 vector-type <b>arithmetic</b> <b>processors,</b> a 128-GB main memory system. The peak performance of each <b>Arithmetic</b> <b>processors</b> is 102.4Gflops. The ES as a whole thus consists of 1280 <b>arithmetic</b> <b>processors</b> with 20 TB of main memory and the theoretical performance of 131Tflops.|$|R
50|$|The {{format is}} not even limited to graphics, its {{definition}} allowing it {{to be used for}} arbitrary three-dimensional matrices of unsigned integers. Some programs of the Netpbm package, for example pamsummcol, function as crude matrix <b>arithmetic</b> <b>processors</b> and use the PAM format this way.|$|R
40|$|The single {{instruction}} stream, multiple {{data stream}} Massively Parallel Processor (MPP) unit consists of 16, 384 bit serial <b>arithmetic</b> <b>processors</b> configured as a 128 x 128 array whose speed can exceed that of current supercomputers (Cyber 205). The {{applicability of the}} MPP for solving reaction network problems is presented and discussed, including the mapping of the calculation to the architecture, and CPU timing comparisons...|$|R
40|$|This essay {{suggests}} that the computing profession should recognise {{that it has been}} responsible for quite a few serious blunders so that it can strike an appropriate balance between pride and humility. The essay looks at a blunder in each of seven areas: numeric encoding, text encoding, scientific programming, commercial programming, <b>arithmetic</b> <b>processor</b> design, keyboard design, and computing terminology...|$|E
40|$|This paper {{describes}} a hardware implementation of an <b>arithmetic</b> <b>processor</b> which is e#cient for elliptic curve (EC) cryptosystems, which {{are becoming increasingly}} popular as an alternative for public key cryptosystems based on factoring. The modular multiplication is implemented using a Montgomery modular multiplication in a systolic array architecture, which has the advantage that the clock frequency becomes independent of the bit length m...|$|E
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Many evolving application areas, such as digital multimedia systems, artificial intelligence, image processing and high performance graphics systems, require complex arithmetic calculations to be performed within a limited number of clock cycles. These new applications usually involve manipulating a large amount of input data, often requiring iterative calculations, just as in digital signal processing (DSP) applications. A traditional Arithmetic and Logic Unit would take too many clock cycles. By employing some special purpose coprocessors, the number of clock cycles can be significantly reduced. The Reconfigurable <b>Arithmetic</b> <b>Processor</b> (RAP) described in [11] is one such processor and it uses a very simple and slow multiplier. Also the main processor is forced to sample the output of a much simpler operation at the maximum clock frequency which was determined by the worst case processing delay. This meant that for the simpler operations like addition and subtraction, the processor utilization is around 50 %. The main objective of this thesis was to design, and implement a 16 -bit recon figurable <b>arithmetic</b> <b>processor,</b> which can be more efficiently used in place of the Multiplier-Accumulator's [5] and the RAP used in [11]. As the demand for high speed processing has been increasing with expanding computer applications, the performance of any coprocessor has to be enhanced by using a multiplier as a key component. A very fast multiplier is used in this <b>arithmetic</b> <b>processor</b> which adopts the modified Booth's algorithm and Wallace method. For the adder/subtract unit, the carry-look-ahead scheme is adopted due to its simplicity and modularity that make it particularly adaptable to integrated circuit implementation. By using a slightly complex control circuit, the same carry-look-ahead adder is used for both the multiplier and the adder module thus saving 1 / 5 of the total chip real estate. Also a control circuit is designed to generate dedicated clocks of different frequencies for the different functions. In this thesis, the complete design, layout and simulations of the 16 -bit <b>arithmetic</b> <b>processor</b> are presented...|$|E
40|$|The {{goal of this}} {{research}} was to develop designs for <b>arithmetic</b> <b>processors</b> wi th input and output data both In the Burst format. An ad-ditional requirement was that these designs be constructible from readily avai lable logic circuits such as IlL SSI and MSI and be competitive on both a cost and performance basis wi th the Block Stan Register (BSR) techniques first proposed for Burst Processing. The result is a family of <b>arithmetic</b> <b>processors</b> based on a design for adding Bursts known as the Perverted Adder (PA). The PA is a full adder and a flip-flop connected with the sum output of the full adder fed back to the carry input via the flip-flop. This simpl e circuit performs Burst addition. However, the output Bursts are uncompacted, i. e. the ones are no longer adjacent to one another, whereas BSR de-signs output compacted Bursts. Designs for a multipl ier and divider use a tree of perverted adders to perform the requisite additions in parallel. Several modifications o...|$|R
40|$|High speed {{attached}} processors are {{demanded in}} many real time applications; {{digital signal processing}} tasks are particularly suited to dedicated <b>arithmetic</b> <b>processors.</b> VLSI fabrication {{has made it possible}} to implement such processors and to enable them to operate at high speed. In some applications, reliability of processing results is of paramount importance and attention has to turn to fault-tolerant arithmetic processing systems. This will be increasingly the case as the designs become more compact and soft errors begin to be more predominant in the technology of sub geometries. This paper discusses a fault-tolerant mechanism for use with a new family of generic, finite ring, computational cells for systolic array processing of high speed DSP algorithms. The generic cell, and the fault-tolerant mechanism are based on a ROM/latch structure that is flexible, easy to design with and requires a low overhead fault detection circuit. The fault detection function is automatically distributed throughout the array, since the cell is used for all elements of the computation process. The fault correction circuitry is based {{on the use of the}} same generic cell, thus providing a universal solution to the protection of <b>arithmetic</b> <b>processors...</b>|$|R
40|$|A multinode parallel-processing {{computer}} {{is made up}} of a plurality of innerconnected, large capacity nodes each including a reconfigurable pipeline of functional units such as Integer <b>Arithmetic</b> Logic <b>Processors,</b> Floating Point <b>Arithmetic</b> <b>Processors,</b> Special Purpose Processors, etc. The reconfigurable pipeline of each node is connected to a multiplane memory by a Memory-ALU switch NETwork (MASNET). The reconfigurable pipeline includes three (3) basic substructures formed from functional units which {{have been found to be}} sufficient to perform the bulk of all calculations. The MASNET controls the flow of signals from the memory planes to the reconfigurable pipeline and vice versa. the nodes are connectable together by an internode data router (hyperspace router) so as to form a hypercube configuration. The capability of the nodes to conditionally configure the pipeline at each tick of the clock, without requiring a pipeline flush, permits many powerful algorithms to be implemented directly...|$|R
40|$|Abstract: Speed {{of digital}} <b>arithmetic</b> <b>processor</b> depends {{mainly on the}} speed of adders. This paper {{provides}} a technique {{so that we can}} increase the speed of addition. Hybrid signed digit number representation perform addition {{in such a way that}} the carry propagation chain is limited to single digit position and hence are used to speed up arithmetic operation. Also hybrid signed digit reduces the critical path delay by parallelizing. Hybrid signed digit can be appropriate to use, when output is redundant representation...|$|E
40|$|This paper {{describes}} a hardware implementation of an <b>arithmetic</b> <b>processor</b> which is efficient for bit-lengths suitable for both commonly used types of Public Key Cryptography (PKC), i. e., Elliptic Curve (EC) and RSA Cryptosystems. Montgomery modular multiplication in a systolic array architecture {{is used for}} modular multiplication. The processor consists of special operational blocks for Montgomery Modular Multiplication, modular addition /subtraction, EC Point doubling/addition, modular multiplicative inversion, EC point multiplier, projective to a#ne coordinates conversion and Montgomery to normal representation conversion...|$|E
40|$|Speed {{of digital}} <b>arithmetic</b> <b>processor</b> depends {{mainly on the}} speed of adders. This paper {{provides}} a technique {{so that we can}} increase the speed of addition. Hybrid signed digit number representation perform addition {{in such a way that}} the carry propagation chain is limited to single digit position and hence are used to speed up arithmetic operation. Also hybrid signed digit reduces the critical path delay by parallelizing. Hybrid signed digit can be appropriate to use, when output is redundant representation...|$|E
40|$|Upon {{completion}} of the course, students: 1. can combine MSI circuits into larger or more complex digital circuits; 2. can write a VHDL description of a combinational logic circuit; 3. can write a VHDL description of a sequential state machine; 4. can use current engineering software to compile and simulate circuits, including those described using VHDL; 5. know the architecture of a basic computer system; 6. know {{the operation of the}} components of a basic computer system include control, <b>arithmetic</b> <b>processors,</b> registers, and buses; 7. know the relationship between the hardware architecture and a computer's assembly language instruction set...|$|R
5000|$|In {{addition}} to the usual logical and <b>arithmetic</b> operations, the <b>processor</b> supported: ...|$|R
40|$|Hardware number radix {{converters}} may {{be required}} at the input and output interfaces of high-speed <b>arithmetic</b> <b>processors</b> implemented in VLSI. Systolic hardware realisation of the radix conversion process along with hardware implementation details are discussed in this paper. The main results are that it is always possible to accept one input digit in a continuous stream ofk-digit operands on every clock tick and that the total conversion delay (from {{the acceptance of the}} first digit of an operand to the emergence of the first digit in the corresponding k'-digit result) is k + k'/b clock ticks, where b is a design tradeoff parameter affecting the cell complexity, intercell communication, and possibly the clock period in the systolic converter...|$|R
40|$|Arithmetic {{algorithms}} for {{separate and}} nonseparate codes are considered. The nonseparate AN code is formed when an uncoded operand X is {{multiplied by the}} check modulus A to give the coded operand AX. The separate codes are the residue code, and the inverse-residue code, which has significant advantages in fault detection of repeated-use faults. A set of algorithms for low-cost AN-coded operands is discussed together with questions of their implementation in a byte-organized <b>arithmetic</b> <b>processor.</b> Algorithms for inverse-residue coded operands of the STAR computer are also examined...|$|E
40|$|High {{computing}} {{speed and}} modularity have made RNS-based <b>arithmetic</b> <b>processor</b> actractive {{for a long}} time, especially in signal processing, where additions and multiplications are very frequent. The VLSI tecnology renewed this interest because RNS-based circuits are becoming more feasible; however intermodular operations degradate their performance and a great effort result on this topic. In this paper {{we deal with the}} problem of performing the basic operation |X|&# 2012;m for large values of X, following an approximating and correcting approach, which guarantees the correctness of the result...|$|E
40|$|Abstract: In this paper, we {{deal with}} the {{designing}} of a 32 -bit floating point <b>arithmetic</b> <b>processor</b> for RISC/DSP processor applications. It is capable of representing real and decimal numbers. The floating point operations are incorporated into the design as functions. The logic for these is different from the ordinary arithmetic functions. The numbers in contention have to be first converted into the standard IEEE floating point standard representation before any sorts of operations are conducted on them. The floating point representation for a standard single precision number is a 32 -bit number that is segmented to represent the floating point number. The IEEE format consists of four fields, the sign of the exponent, the next seven bits are that of the exponent magnitude, and the remaining 24 bits represent the mantissa sign. The exponent in this IEEE standard is represented in excess- 127 format all the arithmetic functions like addition, subtraction, multiplication and division will be design by the processor. The main functional blocks of floating point <b>arithmetic</b> <b>processor</b> design includes, Arithmetic logic unit(ALU), Register organization, control & decoding unit, memory block, 32 -bit floating point addition, subtraction, multiplication and division blocks. This processor IP core can be embedded many places such as co-processor for embedded DSP and embedded RISC controller. The overall system architecture will be designed using HDL language and simulation, synthesis...|$|E
40|$|A {{method of}} {{concurrent}} error detection for <b>arithmetic</b> <b>processors</b> is described. Low-cost residue codes with check-length l and checkbase m = 2 to the l power - 1 are described for checking arithmetic operations of addition, subtraction, multiplication, division complement, shift, and rotate. Of the three number representations, the signed-magnitude representation is preferred for residue checking. Two methods of residue generation are described: the standard method of using modulo m adders and {{the method of}} using a self-testing residue tree. A simple single-bit parity-check code is described for checking the logical operations of XOR, OR, and AND, and also the arithmetic operations of complement, shift, and rotate. For checking complement, shift, and rotate, the single-bit parity-check code is simpler to implement than the residue codes...|$|R
40|$|AbstractÐTraditional {{computer}} systems often suffer from roundoff error and catastrophic cancellation in floating point computations. These systems produce apparently high precision results {{with little or}} no indication of the accuracy. This paper presents hardware designs, arithmetic algorithms, and software support for a family of variable-precision, interval <b>arithmetic</b> <b>processors.</b> These processors give the programmer the ability to detect and, if desired, to correct implicit errors in finite precision numerical computations. They also provide the ability to solve problems that cannot be solved efficiently using traditional floating point computations. Execution time estimates indicate that these processors are two to three orders of magnitude faster than software packages that provide similar functionality. Index TermsÐProcessors, hardware designs, variable-precision arithmetic, interval arithmetic, computer arithmetic, roundoff error, accuracy. ...|$|R
40|$|This chapter {{presents}} {{the design and}} analysis of variable-precision, interval <b>arithmetic</b> <b>processors.</b> The processors give the user the ability to specify the precision of the computation, determine {{the accuracy of the}} results, and recompute inaccurate results with higher precision. The processors support a wide variety of arithmetic operations on variable-precision floating point numbers and intervals. Efficient hardware algorithms and specially designed functional units increase the speed, accuracy, and reliability of numerical computations. Area and delay estimates indicate that the processors can be implemented with areas and cycle times that are comparable to conventional IEEE double-precision floating point coprocessors. Execution time estimates indicate that the processors are two to three orders of magnitude faster than a conventional software package for variable-precision, interval arithmetic. 1. 1 INTRODUCTION Floating point arithmetic provides a high-speed method for perform [...] ...|$|R
40|$|A technique, {{based on}} the residue number system (RNS) with diminished- 1 encoded channel, has being used for {{implementing}} a finite impulse response (FIR) digital filter. The proposed RNS architecture of the filter consists of three main blocks: forward and reverse converter and <b>arithmetic</b> <b>processor</b> for each channel. Architecture for residue to binary (reverse) convertor with diminished- 1 encoded channel has been proposed. Besides, for all RNS channels, the systolic design {{is used for the}} efficient  realization of FIR filter. A numerical example illustrates the principles of diminished- 1 residue arithmetic, signal processing, and decoding for FIR filters...|$|E
40|$|We {{review the}} eld of result-checking {{and suggest that}} it be {{extended}} to a methodology for enforcing hardware/software reliability. We thereby formulate a vision for -monitoring&quot; hardware/software whose reliability is augmented through embedded suites of run-time correct-ness checkers. In particular, we suggest that embedded checkers and correctors may beem-ployed to safeguard against arithmetic errors such as that which has bedeviled the Intel Pentium Microprocessor. We specify checkers and correctors suitable for monitoring the multiplication and division functionalities of an arbitrary <b>arithmetic</b> <b>processor</b> and seamlessly correcting erro-neous output which may occur for any reason during the lifetime of the chip...|$|E
40|$|For {{real time}} target detection, {{identification}} and tracking in high frame rates, large {{field of view}} images, a real-time image processing system is designed. A TMS 320 C 6678 DSP runs as the chief <b>arithmetic</b> <b>processor</b> of this system and FPGA as the secondary controller. C 6678 is {{compared with the same}} series C 6414 in image compression algorithm test. Experimental results show that the new system has a more effective construct, and higher reliability, and can provide a platform for the new high-speed image processing. © (2014) Trans Tech Publications, Switzerland. CEIS; IFST; National Chin-Yi University of Technology; Scientific. Ne...|$|E
40|$|The {{feasibility}} of implementing Residue Number System (RNS) based <b>arithmetic</b> <b>processors</b> has been {{motivated by the}} recent developments in microelectronics. In this report, new VLSI architectures are proposed for computing the integer modulo operation X mod m when m is restricted to the values 2 k; 2 k Σ 1 and composite numbers whose mutually prime factors fall in the above category. Two different design methodologies, namely, the recursive and partition methods are presented and their respective VLSI computational complexities are analyzed. A VLSI chip that computes X mod m where X is a 16 -bit number and m = 3; 5; 6; 7; 9 and 10 has been implemented using the proposed schemes in 3 ¯m CMOS technology and typical measurements have yielded a propagation delay of less than 109 ns...|$|R
40|$|Abstract-Arithmetic {{error codes}} {{constitute}} {{a class of}} error codes that are preserved during most arithmetic operations. Effectiveness studies for arithmetic error codes have shown their value for concurrent detection of faults in <b>arithmetic</b> <b>processors,</b> data transmission subsystems, and main storage units in fault-tolerant computers In this paper, it is shown that the same class of codes is also quite effective for detecting storage errors in both shift-register and magnetic-recording mass memories. Some of the results are more general and deal with properties of arithmetic error codes in detecting unidirectional failures. For example, it is shown that a low-cost arithmetic error code with check modulus A = 2 - 1 can detect any unidirectional failure which affects fewer than N bits. The use of arithmetic error codes for checking of mass memories is further justified since it {{eliminates the need for}} hard-core or self-checking code translators and reduces the number of different types of cod...|$|R
40|$|APEmille is a SIMD {{parallel}} processor {{under development}} at the Italian National Institute for Nuclear Physics (INFN). APEmille is very well suited for Lattice QCD applications, both for its hardware characteristics and for its software and language features. APEmille is an array of custom <b>arithmetic</b> <b>processors</b> arranged on a tridimensional torus. The replicated processor is a pipelined VLIW device performing integer and single/double precision IEEE floating point operations. The processor is optimized for complex computations and has a peak performance of 528 Mflop at 66 MHz and of 800 Mflop at 100 MHz. In principle an array of 2048 nodes is able to break the Tflops barrier. A powerful programming language named TAO is provided and is highly optimized for QCD. A C++ compiler is foreseen. Specific data structures, operators and even statements can be defined by the user for each different application. Effort {{has been made to}} define the language constructs for QCD. Comment: Talk presented at LATTICE 96 (machines...|$|R
40|$|The Stanford Nanosecond Arithmetic Project is {{targeted}} at realizing an <b>arithmetic</b> <b>processor</b> with performance approximately {{an order of}} magnitude faster than currently available technology. The realization of SNAP is predicated on an interdisciplinary approach and effort spanning research in algorithms, data representation, CAD, circuits and devices, and packaging. SNAP is visualized as an arithmetic coprocessor implemented on an active substrate containing several chips, each of which realize a particular arithmetic function. This year's report highlights recent results in the area of wave pipelining. We have fabricated a number of prototype die, implementing a multiplier slice. Cycle times below 5 ns were realized...|$|E
40|$|We {{review the}} field of result-checking and suggest that it be {{extended}} to a methodology for enforcing hardware/software reliability. We thereby formulate a vision for 2 ̆ 2 self-monitoring 2 ̆ 2 hardware/software whose reliability is augmented through embedded suites of run-time correctness checkers. In particular, we suggest that embedded checkers and correctors may be employed to safeguard against arithmetic errors such as that which has bedeviled the Intel Pentium Microprocessor. We specify checkers and correctors suitable for monitoring the multiplication and division functionalities of an arbitrary <b>arithmetic</b> <b>processor</b> and seamlessly correcting erroneous output which may occur for any reason during the lifetime of the chip...|$|E
40|$|The paper {{presents}} hardware {{solutions for}} the computation of inverse kinematics. They are based on an existing pipeline <b>arithmetic</b> <b>processor</b> which employs the CORDIC algorithm. It is shown that a slight modification of the standard CORDIC algorithm enables fast and efficient computation of the joint angles for the inverse kinematics. Due to this modification, the hardware amount and the computation time for the inverse kinematics computations are considerably reduced. The method yields an extended convergence range and a reduced number of iterations {{in comparison to the}} original CORDIC design. Another advantage of this method is that it yields an enhanced CORDIC functionality. Finally, a novel architecture for a new kinematic processor based on the modified CORDIC method is presented...|$|E
40|$|A new {{high-speed}} low-power multiplier {{has been}} presented in nanotechnology. Implementing multiplication algorithms in nanotechnology has a major effect in performance of <b>arithmetic</b> <b>processors.</b> A new design {{for the use of}} the number system { 0, 1, 2, 3 }, with a particular three bit coding of digits, is evaluated and some improvements are obtained, including the possibility of using a two bit coding, with a considerable reduction in the wiring of the multiplier structure. Non-regularities in the construction of conventional Wallace tree multipliers consequence in a large quantity of dissipated area when implemented in VLSI. A novel Wallace tree structure is proposed. An evaluation between the critical path and wiring overhead between the conventional and the modified Wallace tree is presented. A fast adder has been implemented. The proposed adder is based on carry chain schemes. A Wallace tree multiplier is designed and simulated in a 70 nm process. We used HSPICE and Synopsys for simulations. The latency has decreased by almost 18 % and power consumption decreased by 16 %. Our design reduced transistor count by 12 %...|$|R
40|$|As {{scientific}} computation {{continues to}} scale, {{it is crucial}} to use floating-point <b>arithmetic</b> <b>processors</b> as efficiently as possible. Lower precision allows streaming architectures to perform more operations per second and can reduce memory bandwidth pressure on all architectures. However, using a precision that is too low for a given algorithm and data set will result in inaccurate results. Thus, developers must balance speed and accuracy when choosing the floating-point precision of their subroutines and data structures. I am investigating techniques to help developers learn about the runtime floating-point behavior of their programs, and to help them make decisions concerning the choice of precision in implementation. I propose to develop methods that will generate floating-point precision configurations, automatically testing and validating them using binary instrumentation. The goal is ultimately to make a recommen-dation to the developer regarding which parts of the program can be reduced to single-precision. The central thesis is that automated analysis techniques can make recommendations regarding the precision levels that each part of a computer program must use to maintain overall accuracy, with the goal of improving performance on scientific codes...|$|R
40|$|As {{scientific}} computation {{continues to}} scale, {{it is crucial}} to use floating-point <b>arithmetic</b> <b>processors</b> as efficiently as possible. Lower precision allows streaming architectures to perform more operations per second and can reduce memory bandwidth pressure on all architectures. However, using a precision that is too low for a given algorithm and data set will result in inaccurate results. Thus, developers must balance speed and accuracy when choosing the floating-point precision of their subroutines and data structures. We are building tools to help developers learn about the runtime floating-point behavior of their programs, and to help them make decisions concerning the choice of precision in implementation. We propose a tool that performs automatic binary instrumentation of floating-point code to detect mathematical cancellations, as well as to automatically run calculations in alternate precisions. In particular, we show how our prototype can detect the variation in cancellation patterns for different pivoting strategies in Gaussian elimination, as well as how our prototype can detect a program’s sensitivity to ill-conditioned input sets. 1...|$|R
