1|35|Public
40|$|This is a {{tentative}} {{research on the}} education of stress management (SME) for university students that was focused on peer-pressure and cognition process. The purposes of this paper were to examine the process of SME in which the authors structured contents and exercises to adapt to characteristics of university students of today, and also to investigate {{the effect of the}} education on them. This SME had five sessions which consisted of teaching the concept of stress, discussion of their cognition processes, <b>assertion</b> <b>technique,</b> role playing and sharing. They could study the concept of stress and became conscious of their recognitions and responses, and could work to develop their forms of assertion. In analyzing differences between the pre-test and the post-test using some psychological measurements, significant differences were found in five factors : (1) an psychological stress response, (2) one of the interpersonal stress events, (3) cognitive estimates, (4) irrelevant beliefs and (5) assertiveness. This program of SME for university students proved effective. The authors discussed limit of those effects and tasks of this SME program...|$|E
40|$|There is some evidence, that <b>assertion</b> <b>techniques,</b> i. e., preconditions, postconditions and invariants have a {{positive}} effect on the overall software quality. Unfortunately only a limited number of commercially relevant programming languages support <b>assertion</b> <b>techniques</b> (e. g., Eiffel). Even modern programming languages like Java have very limited built-in support for assertions. Nevertheless a number of systems exist for the the Java programming language, that support <b>assertion</b> <b>techniques</b> in different ways (language extensions, preprocessors, metaprogramming approaches). In order to make these different approaches comparable we developed a set of criteria and used these criteria to evaluate these systems. ...|$|R
30|$|To handle those weak-points the Robust <b>Assertions</b> <b>technique</b> is proposed, whose {{effectiveness}} {{is shown by}} extensive fault injection experiments. With this technique a system follows a new failure model, that is called Fail-Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact distance depends on the output assertions used.|$|R
40|$|Abstract — This paper {{describes}} a System Verilog Verification Methodology Manual (VMM) test bench architecture that is structured to gain maximum efficiency from both constrained random and directed test case development. We specify how a novel form of directed traffic {{can be implemented}} in parallel to a complete random traffic generator inside a reusable directory structure which takes full advantage of coverage and <b>assertion</b> <b>techniques.</b> The paper uses an IEEE- 754 compliant Floating-Point adder model {{as part of a}} case study that illustrates a complete set of results from using this test bench solution...|$|R
40|$|There is {{evidence}} that “contracts, ” or <b>assertion</b> <b>techniques</b> involving preconditions, postconditions, and invariants, {{have a positive effect}} on overall software quality. Regrettably, very few programming languages support these techniques. Since the advent of Bertrand Meyer’s Design by Contract ™ method, introduced in the language Eiffel, a number of systems have been built to implement support for contracts in more commonly-used languages. Such support has not been satisfactorily implemented in C#. In this paper, we compare the different approaches of existing systems and introduce Contract Sharp, a tool that provides support for contracts in C#. ...|$|R
40|$|In {{this paper}} the {{behavior}} of assertion-based error detection mechanisms is characterized under faults injected according to a quite general fault model. Assertions based on {{the knowledge of the}} application can be very effective at detecting corruption of critical data caused by hardware faults. The main drawbacks of that approach are identified as being the lack of protection of data outside the section covered by assertions, namely during input and output, and the possible incorrect execution of the assertions. To handle those weak-points the Robust <b>Assertions</b> <b>technique</b> is proposed, whose effectiveness is shown by extensive fault injection experiments. With this technique a system follows a new failure model, that is called Fail- Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact distance depends on the output assertions used. Any kind of assertions can be considered, from simple likelihood tests to high coverage assertions such as those used in the Algorithm Based Fault Tolerance paradigm. We claim that this failure model is very useful to describe {{the behavior of}} many low-cost fault-tolerant systems, that have low hardware and software redundancy, like embedded systems, were cost is a severe restriction, yet full availability is expected...|$|R
50|$|Techniques of inquiry. The actual {{behaviors}} {{performed and}} the procedures followed in adducing evidence to verify a statement (warrant an <b>assertion)</b> are the <b>techniques</b> of an inquiry. Examples include conducting surveys, experimentation, drawing analogies, running simulations, locating documents, taking notes, classifying objects, defining terms, clarifying concepts, etc.|$|R
40|$|Assertion-based verification—that is, user {{specified}} {{properties and}} automatic property extraction combined with simulation and formal techniques—is {{likely to be}} the next revolution in hardware design verification. This paper explores a verification break-through prompted by multi-level specification and <b>assertion</b> verification <b>techniques.</b> The emerging Accellera formal property language, as well as the Open Verification Library standards and the important roles they will play in future assertion-based verification flows are discussed. Furthermore, automatic property extraction techniques are explored—and their important roles in validating semantic consistency {{in the context of an}} RTL signoff flow...|$|R
40|$|AbstractMetadata {{produced}} {{by members of}} a diverse community of peers tend to contain low-quality or even mutually inconsistent assertions. Trust values computed {{on the basis of}} users' feedback can improve metadata quality and reduce inconsistency, eliminating untrustworthy assertions. In this paper, we describe an approach to metadata creation and improvement, where community members express their opinions on the trustworthiness of each <b>assertion.</b> Our <b>technique</b> aggregates individual trustworthiness values to obtain a community-wide assessment of each assertion. We then apply a global trustworthiness threshold to eliminate some assertions to reduce the metadatabase's overall inconsistency...|$|R
5000|$|The earlier dialogues of Plato (424 - 348 BC), {{relating}} the debates of his teacher Socrates, raised {{the use of}} [...] arguments to a formal dialectical method (...) , now called the Socratic method which is taught in law schools. Typically Socrates' opponent would make an innocuous assertion, then Socrates by a step-by-step train of reasoning, bringing in other background assumptions, would make the person admit that the assertion resulted in an absurd or contradictory conclusion, forcing him to abandon his <b>assertion.</b> The <b>technique</b> was also a focus {{of the work of}} Aristotle (384 - 322 BC).|$|R
40|$|This paper {{presents}} {{a method for}} computer-aided veri cation of timing properties of real-time systems. A timed automaton model, along with invariant <b>assertion</b> and simulation <b>techniques</b> for proving properties of real-time systems, is formalized within the Larch Shared Language. This framework is then used to prove time bounds for two sample algorithms|a simple counter and Fischer's mutual exclusion protocol. The proofs are checked using the Larch Prover...|$|R
40|$|Abstract. We study {{confidential}} safety, {{a notion}} of secrecy that arises naturally in adversarial systems. We provide a formal definition {{in the context of}} a concurrent while-language. We provide techniques for establishing confidential safety, building on three ingredients: (1) We develop a novel view of correspondence assertions that focuses on their confidentiality (in contrast to the traditional view of correspondence assertions that uses their integrity to derive secrecy and authentication properties of protocols). (2) We establish the confidentiality of a correspondence <b>assertion</b> using <b>techniques</b> reminiscent of information flow, and incorporating cryptography using confounders. (3) We prove protocols are confidentially safe using techniques based on frame-bisimulation that exploit the noninterference properties of well-typed programs...|$|R
40|$|Mathematical {{modeling}} and simulation of complex physical systems are emerging as key technologies in engineering. Modern approaches to physical system simulation allow users to specify simulation models {{with the help of}} equation-based languages. Due to the highlevel declarative abstraction of these languages program errors are extremely hard to find. This paper presents an algorithmic automated debugging framework for equation-based modeling languages. We show how program slicing and dicing performed at the intermediate code level combined with <b>assertion</b> checking <b>techniques</b> can automate, to a large extent, the error finding process and behavior verification for physical system simulation models. Our prototype debugger prove that algorithmic debugging can enhance considerably a designer's capability to deal with the increasing complexity of today's physical system simulation models described by equation-based languages. 1...|$|R
40|$|Abstract: This paper surveys {{some new}} tools and methods for {{formally}} verifying time performance properties of systems that satisfy timing assumptions. The techniques are potentially of practical benet in the validation of real-time process control and communication systems. The tools and methods include nondeterministic timed automaton models, invariant <b>assertion</b> and simulation <b>techniques</b> for proving worst-case time bounds, probabilistic timed automaton models, and Markov-style techniques for proving probabilistic time bounds. All {{of these techniques}} are well suited for (partial) mechanization. ...|$|R
40|$|Includes bibliographical {{references}} (pages 29 - 35) Thirty-eight male inpatients in the Alcohol Treatment Program of the Sepulveda Veteran's Administration Hospital {{were randomly}} assigned to receive either assertion training, which utilized such techniques as behavioral rehearsal, modeling, and feedback in dealing with interpersonal problem situations, or {{to take part in a}} ???rap??? group in which problems of an interpersonal nature were discussed, but in which no <b>assertion</b> training <b>techniques</b> were employed. It was hypothesized that patients receiving the assertion training would show greater pre and post treatment attitudinal changes with respect to degree of assertiveness and locus of control over reinforcements. Both an analysis of variance and an analysis of covariance, utilizing age and severity of drinking as covariates, revealed a significant difference (p ???. 01) between the group mean difference scores for the two treatment groups on three separate assessment instruments. Implications of the results for possible future research were discussed...|$|R
40|$|Software faults and {{vulnerabilities}} {{continue to}} present significant obstacles to achieving reliable and secure software. The critical {{problem is that}} systems currently lack the capability to respond intelligently and automatically to attacks – especially attacks that exploit previously unknown vulnerabilities or are delivered by previously unseen inputs. Therefore, {{the goal of this}} thesis is to provide an environment where both supervision and automatic remediation can take place. Also provided is a mechanism to guide the supervision environment in detection and repair activities. This thesis supports the notion of Self-Healing Software by introducing three novel techniques: micro-sandboxing, micro-speculation, and self-correcting <b>assertions.</b> These <b>techniques</b> are combined in a kernel-level emulation framework to speculatively execute code that may contain faults or vulnerabilities and automatically repair such faults or exploited vulnerabilities. The framework, VPUF, introduces the concept of computation as an operating system service by providing control for an array of virtual processors in the Linux kernel (creating the concept of an endolithic kernel). This thesis introduces ROAR (Recognize, Orient, Adapt...|$|R
40|$|Technē/Technology is the {{up-to-date}} {{critical volume}} on the theories, philosophies, and debates on technology and their productivity for the fields of film and media studies. Comprehensive as well as innovative, it is not organised around a single thesis - except the <b>assertion</b> that <b>technique</b> {{is a major concern}} for film and media scholars, whether this is approached in terms of philosophy, techno-aesthetics, semiotics, apparatus theory, (new) film history, media archaeology, the industry or the sensory / cognitive experiences. Technē/Technology deliberately includes contributions by film and media experts working in very different ways {{on a wide range of}} technology-related issues. A major questions to be addressed in this book is how the new philosophies (of technology) created in relation to major technological transformations - such as the new philosophies of (media) technology formulated by Benjamin, Heidegger, McLuhan, Kittler, or Stiegler - could or did contribute in turn to the modification of film theory and some of its key concepts...|$|R
40|$|Executable assertions {{are used}} to test flight control software. The {{techniques}} used for testing flight software; however, {{are different from the}} techniques used to test other kinds of software. This is because of the redundant nature of flight software. An experimental setup for testing flight software using executable <b>assertions</b> is described. <b>Techniques</b> for writing and using executable assertions to test flight software are presented. The error detection capability of assertions is studied and many examples of assertions are given. The issues of placement and complexity of assertions and the language features to support efficient use of assertions are discussed...|$|R
40|$|International audienceThis article {{describes}} the system that participated in the Part-of-speech tagging subtask of the EmpiriST 2015 shared task on automatic linguistic annotation of computer-mediated communication / social media. The system combines a small <b>assertion</b> of trending <b>techniques,</b> which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of German CMC and Web corpus data; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Tiger v 2. 2 and EmpiriST) and unlabelled data (German Wikipedia) were used for training. The system is available under the APLv 2 open-source license...|$|R
40|$|Abstract—Bug-free first silicon is not {{guaranteed}} by the existing pre-silicon verification techniques. To have impeccable products, it is now required to identify any bug {{as soon as the}} first silicon becomes available. We consider the <b>Assertion</b> Based Verification <b>techniques</b> for the post-silicon debugging based on the insertion of hardware checkers in the debug infrastructure for complex systems on chip. This paper proposes a method to cluster hardware-assertion checkers using the graph partitioning approach. It turns out that having the clusters of hardwareassertions and controlling each cluster selectively during the debug mode and normal operation of the circuit makes integration of assertions inside the circuits easier, and causes lower energy consumption and efficient debug scheduling. I...|$|R
40|$|Abstract—We study {{a notion}} of secrecy that arises {{naturally}} in adversarial systems. Let all agents agree on a space of possible values. An honest agent chooses one of these values, and aims {{to make sure that}} this particular choice cannot be reliably guessed by an adversary, even in the context of a distributed protocol. An example is an agent that uses an honest mail server to send a message, wishing to keep the identity of the eventual recipient hidden from an adversary. We refer to this property as confidential safety. We provide a formal definition in the context of a concurrent whilelanguage. We provide techniques for establishing confidential safety, building on three ingredients: • We develop a novel view of correspondence assertions that focuses on their confidentiality (in contrast to the traditional view of correspondence assertions that uses their integrity to derive secrecy and authentication properties of protocols). • We establish the confidentiality of a correspondence <b>assertion</b> using <b>techniques</b> reminiscent of information flow, and incorporating cryptography using confounders. • We prove protocols (such as instances of the mail server above) are confidentially safe using techniques based on frame-bisimulation that exploit the non-interference properties of well-typed programs. I...|$|R
40|$|This paper surveys {{some new}} tools and methods for {{formally}} verifying time performance properties of systems that satisfy timing assumptions. The techniques are potentially of practical benefit in the validation of real-time process control and communication systems. The tools and methods include nondeterministic timed automaton models, invariant <b>assertion</b> and simulation <b>techniques</b> for proving worst-case time bounds, probabilistic timed automaton models, and Markov-style techniques for proving probabilistic time bounds. All {{of these techniques}} are well suited for (partial) mechanization. 1 Introduction A rich collection of formal methods have become well established for proving correctness properties [...] usually, safety and liveness properties [...] for asynchronous concurrent systems. The {{most important of these}} <b>techniques</b> are invariant <b>assertion</b> methods and simulation (refinement) methods for proving safety properties, and temporal logic methods for proving liveness properties. Other i [...] ...|$|R
40|$|In {{this paper}} {{we present a}} formal {{abstract}} specification for TCP/IP transport level protocols and formally verify that TCP satisfies this specification. We also present a formal description of an experimental protocol, T/TCP, which proposes to provide the same service as TCP, but with optimizations to make it efficient for transactions. We further show that this protocol does not provide the same service as TCP, and propose a weaker specification for this protocol. Our specifications are presented using an untimed automaton model, and we present the protocols using a timed automaton model. The formal verification is done using invariant <b>assertion</b> and simulation <b>techniques.</b> Keywords Verification, automata and languages, network protocols 1 INTRODUCTION The original motivation for this work was to do a formal verification of an experimental transport level protocol called T/TCP. This protocol, by Braden and Clark (Braden, 1992; Braden, 1994; Braden and Clark, 1993), {{is designed to be}} [...] ...|$|R
40|$|A true {{assertion}} {{about the}} input-output {{behavior of a}} Turing Machine M may be independent of (i. e., impossible to prove in) a theory T because the computational behavior of M is particularly opaque, or because the function or set computed by M is inherently subtle. The latter sorts of representation-independent independence results are more satisfying. For Π 2 <b>assertions,</b> the best-known <b>techniques</b> for proving independence yield representation-independent results {{as a matter of}} course. This paper illustrates current understanding of unprovability for Π 2 assertions by demonstrating that very weak conditions on classses of sets S and R guarantee that there exists a set L 0 2 R Γ S such that L 0 is not provably infinite (hence, not provably nonregular, nondeterministic, non-context-free, not in P, etc.). Under slightly stronger conditions, such L 0 s may be found within every L 2 R Γ S. 1 Introduction In a recent paper, Hartmanis shows how to use diagonalization techniques [...] ...|$|R
40|$|An {{important}} {{goal in the}} area of reliable software is to show that a program actually meets its specifications. This can be done using program verification <b>techniques.</b> <b>Assertions</b> are made about the expected behavior of a program, and intermediate program states are examined to ensure that the specifications of the program are never violated. However, proving that the intermediate program steps lead to the conclusion and, therefore, proving that the program is correct is difficult. In this paper we show how a constraint logic programming tool, CLP(R), can be used to help in determining whether the given pre- and postconditions of a program proof match the actual statement that is supposed to be performed. In addition to that, CLP(R) was used as part of a weakest precondition generator which was used to fill in program proofs that were incomplete. Keywords: program verification, constraint logic programming, proof checking, assertion-based reasoning, formal methods. 1 Introduction To [...] ...|$|R
40|$|Data-driven {{approaches}} to Machine Translation {{have come to}} the fore of Language Processing Research over the past decade. The relative success in terms of robustness of Example Based and Statistical approaches have given rise to a new optimism and an exploration of other data-driven approaches such as Maximum Entropy language modeling. Much of the work in the literature however, largely report on translation between languages within the European Family of languages. This research is an attempt to cross this language family divide in order to compare the performance of these techniques on Asian languages. In particular, this work reports on Statistical Machine Translation experiments carried out between language pairs of the three major languages of Sri Lanka: Sinhala, Tamil and English. Results indicate that current models perform significantly better for the Sinhala-Tamil pair than the English-Sinhala pair. This in turn appears to confirm the <b>assertion</b> that these <b>techniques</b> work better for languages that are not too distantly related to each other...|$|R
40|$|A {{variety of}} {{techniques}} {{have been proposed}} to verify stateful functional programs by developing Hoare logics for the state monad. For better automation, we explore a different point in the design space: we propose using affine types to model state, while relying on refinement type checking to prove <b>assertion</b> safety. Our <b>technique</b> is based on verification by translation, starting from FX, an imperative object-based surface language with specifications including object invariants and Hoare triple computation types, and translating into Fine, a functional language with dependent refinements and affine types. The core idea of the translation is the division of a stateful object into a pure value and an affine token whose type mentions {{the current state of}} the object. We prove our methodology sound via a simulation between imperative FX programs and their functional Fine translation. Our approach enables modular verification of FX programs supported by an SMT solver. We demonstrate its versatility by several examples, including verifying clients of stateful APIs, even in the presence of aliasing, and tracking information flow through sideeffecting computations. 1...|$|R
40|$|Abstract Allowing for {{copyright}} protection and ownership <b>assertion,</b> digital watermarking <b>techniques,</b> {{which have been}} successfully applied for classical media types like audio, images and videos, have recently been adapted for the newly emerged multimedia data type of 3 D geometry models. In particular, the widely used spreadspectrum methods can be generalized for 3 D datasets by transforming the original model to a frequency domain and perturbing the coefficients of the most dominant basis functions. Previous approaches employing this kind of spectral watermarking are mainly based on multiresolution mesh analysis, wavelet domain transformation or spectral mesh analysis. Though they already exhibit good resistance to many types of real-world attacks, they are often far too slow to cope with very large meshes due to their complicated numerical computations. In this paper, we present a novel spectral watermarking scheme using new orthogonal basis functions based on radial basis functions. With our proposed fast basis function orthogonalization, while observing similar persistence with respect to various attacks as other related approaches, our scheme runs faster by two orders of magnitude and thus can efficiently watermark very large models...|$|R
40|$|This paper gives a short {{introduction}} to digital watermarks {{and their use}} in copyright protection of audio, video, still images and similiar inexact data, but does not include discussion of specic algorithms. The specic uses covered are copyright assertion, usage control and content ngerprinting. Emphasis is given to invisible watermarking schemes, where it is not desirable for the original content to be perceivably altered {{in order to preserve}} its value. After consideration of the issue, one must conclude that while digital watermarking may prevent the casual pirate from copying the marked content, large-scale pirates cannot be stopped by them and even dedicated home pirates are not signicantly hampered except perhaps in the very short term. Additionally, the average home user may suer from these protection schemes by not being able to use the licensed content to the extent allowed by law (this, of course, varying from country to country). Copyright <b>assertion</b> and ngerprinting <b>techniques</b> are more eective, but, with current watermarking algorithms, mostly because they discourage potential attackers from attempting infringement by making it at least theoretically possible for them to be caught...|$|R
40|$|Large {{software}} systems require large test suites {{to achieve}} high coverage. Test suites often employ closed unit tests that are self-contained {{and have no}} input parameters. To achieve acceptable coverage with self-contained unit tests, developers often clone existing tests and reproduce both boilerplate and essential environment setup code as well as assertions. Existing technologies such as parametrized unit tests and theories could mitigate cloning in test suites. These technologies give developers new ways to express refactorings. However, they do not help detect refactorable clones in the first place, which requires tedious manual effort. This thesis proposes a novel <b>technique,</b> <b>assertion</b> fingerprints, for detecting clones based {{on the set of}} assertion/fail calls in test methods. Assertion fingerprints encode the control flow around the ordered set of assertions in methods. We have implemented clone set detection using assertion fingerprints and applied it to 10 test suites for open-source Java programs. We provide an empirical study and a qualitative analysis of our results. Assertion fingerprints enable the discovery of test clones that exhibit strong structural similarities and are amenable to refactoring. Our technique delivers an overall 75 % true positive rate on our benchmarks and identifies 44 % of the benchmark test methods as clones...|$|R
40|$|Test cases {{constitute}} around 30 % of the codebase of a num-ber {{of large}} software systems. Poor design of test suites hin-ders test comprehension and maintenance. Developers often copy-paste existing tests and reproduce both boilerplate and essential environment setup code {{as well as}} assertions. Test case refactoring would be valuable for developers aiming to control technical debt arising due to copy-pasted test cases. In the context of test code, identifying candidates for refactoring requires tedious manual effort. In this work, we specifically tailor static analysis techniques for test analy-sis. We present a novel <b>technique,</b> <b>assertion</b> fingerprints, for finding similar test cases based {{on the set of}} assertion calls in test methods. Assertion fingerprints encode the control flow around the ordered set of assertions in methods. We have implemented similar test case detection using as-sertion fingerprints and applied it to 10 test suites for open-source Java programs. We provide an empirical study and a qualitative analysis of our results. Assertion fingerprints enable the discovery of tests that exhibit strong structural similarities and are amenable to refactoring. Our technique delivers an overall 75 % true positive rate on our benchmarks and reports that 40 % of the benchmark test methods are po-tentially refactorable. CCS Concept...|$|R
40|$|This study {{aimed to}} provide South African public {{relations}} professionals with {{insights into the}} use of the Social Media News Release (SMNR) as a PR 2. 0 tool that has the potential to elicit consumer-driven dialogue in social media channels about information, a brand, product or service advocated by the particular social media news release. Drawing on literature from fields such as public relations, new media studies, marketing, and consumer studies, an analysis of two South African SMNR case studies was conducted including the Samsung Omnia i 900 SMNR and the Standard Bank Pro 20 2008 and 2009 SMNRs. An in-depth content analysis applying limited designations analysis and detailed <b>assertions</b> analysis <b>techniques</b> was performed on selected content from the dedicated social media platforms linked to in the SMNRs to determine the origins, tone and thematic nature of communications on the platforms. A total of 2071 messages was analysed by means of content analysis across six social media platforms in the two case studies. In order to triangulate and support data, an online survey was conducted with 43 social media users as respondents in order to determine social media users’ interactions with the social media platforms and SMNRs. The study found that the social media platforms linked to the SMNRs in the two case studies largely successfully elicited and hosted social media user-generated conversations about the themes advocated by the SMNR. The Blog, Facebook and YouTube platforms proved to be most successful in generating social media conversation, while the Flickr, Twitter and Delicious platforms were less effective among South African consumers. It was found that social media news releases are likely to elicit consumer-driven dialogue on the dedicated social media platforms linked to by the SMNRs if the platforms are managed correctly. Factors that were identified as important management considerations include ensuring the relevancy and timeliness of content on the social media platforms, the involvement by the platform creator in stimulating and encouraging participation from social media users where necessary, as well as the swift response to user comments, deleting of spam comments and pro-active management of negative perceptions that may arise from user comments on the platform...|$|R
40|$|This article {{comments}} {{on a book}} on management education, titled Managers Not MBAs: A Hard Look at the Soft Practice of Managing and Management Development, by Henry Mintzberg. In Part I of his book, Mintzberg provides a detailed critique of many issues surrounding management education. Because Master of Business Administration (MBA) programs cater for young people {{with little or no}} managerial experience, he argues that students are unable to use art or craft, and so they become spoon-fed with analysis and technique to emphasize the scientific aspects of management. This, he argues, results in MBA graduates who engage in too much analysis, reducing managing to decision making through analysis and <b>technique.</b> <b>Assertions</b> like these are repeated, throughout many of the early chapters. People with MBAs are referred to as being obsessed with facts, risk-averse, numbers-oriented, and favouring industries that rely on hard data and management: are left with the impression that management technique over knowledge of company context. In Part II of Managers not MBA, the practice of management development is covered in some detail before consideration of how management education can be reconceived and the emergent principles used to take management education and development to new dimensions. The ultimate goal is a shift from business to management education, and Mintzberg rightly draws a sharp distinction between these terms. He believes this shift can only be made by marrying the educational experience to the working environment, with management education being reserved for full-time practicing managers wishing to study on a part-time basis. He recommends blocked modules designed to encourage connections between managers' experiences and classroom concepts...|$|R
40|$|This study {{aimed to}} provide {{insights}} into {{the manner in which}} the representation of social media usage in relation to cyber-related crimes within selected South African newspapers can potentially shape the ideas and perceptions that society may have towards social networking channels. Drawing on the literature from fields such as developmental studies, new media studies, identity formation and cyber-criminality, an analysis of the Price Water House Coopers Global Economic Survey (2011) was used to provide some insight into the issue of cyber-crime within South Africa. The survey which was conducted by Price Water House Coopers revealed that South Africa is ranked second in the world with the highest rate of reported fraud cases. According to them this rate is comparatively higher than the escalating percentage of cases reported in the United States and other nations. In order to correlate and illustrate some of the findings of the survey and that which was found through primary research, an in-depth content analysis applying limited designations analysis and detailed <b>assertions</b> analysis <b>techniques</b> (Du Plooy, 2007) has been performed on selected content from local print and online publications such as The Herald, Algoa Sun, The Weekend Post, The Sunday Times and News 24, from the time period of January 2009 until January 2012. Herewith, a total of 125 articles were analysed in order to determine the tone and thematic nature of the communication within the respective platforms. Furthermore, the mass media has been argued as being the main platform of communication within society. Whereby, different communication techniques are used to communicate with different target audiences. On a theoretical level, the study explored whether or not social media perpetuates the prejudices of the modernisation theory or serves to challenge such prejudices. Furthermore, the study explored whether social media may potentially have an impact on the reported cyber-related crimes. Associated theory such as the representation theory, globalization, the privacy trust model, social contract theory, media richness theory, participatory theory, convergence, the digital divide, media-centricity, dependency and identity formation has been explored. It was found that social networking sites Facebook and Mxit have been represented as the most common platforms of cyber-related crime and women and teenagers are the most popular victims. The likelihood of individuals being exposed to cyber-crime within social networks is high due to the fact in order to develop online relationships, personal information needs to be shared. The Privacy Trust model was identified as being an important factor which shaped the findings of this study. This is due to the fact that a certain level of trust is held by social network subscribers to the Internet hosts who they entered into a social contract with and with their friends...|$|R
40|$|International audienceWe {{propose a}} {{probabilistic}} Hoare logic aHL {{based on the}} union bound, a tool from basic probability theory. While the union bound is simple, it is an extremely common tool for analyzing randomized algorithms. In formal verification terms, the union bound allows flexible and compos-itional reasoning over possible ways an algorithm may go wrong. It also enables a clean separation between reasoning about probabilities and reasoning about events, which are expressed as standard first-order formulas in our logic. Notably, assertions in our logic are non-probabilistic, even though we can conclude probabilistic facts from the judgments. Our logic can also prove accuracy properties for interactive programs, where the program must produce intermediate outputs as soon as pieces of the input arrive, rather than accessing the entire input at once. This setting also enables adaptivity, where later inputs may depend on earlier intermediate outputs. We show how to prove accuracy for several examples from the differential privacy literature, both interactive and non-interactive. 1998 ACM Subject Classification D. 2. 4 Software/Program Verification 1 Introduction Probabilistic computations arise naturally {{in many areas of}} computer science. For instance, they are widely used in cryptography, privacy, and security for achieving goals that lie beyond the reach of deterministic programs. However, the correctness of probabilistic programs can be quite subtle, often relying on complex reasoning about probabilistic events. Accordingly, probabilistic computations present an attractive target for formal verification. A long line of research, spanning more than four decades, has focused on expressive formalisms for reasoning about general probabilistic properties both for purely probabilistic programs and for programs that combine probabilistic and non-deterministic choice (see, e. g., [29, 34, 35]). More recent research investigates specialized formalisms that work with more restricted <b>assertions</b> and proof <b>techniques,</b> aiming to simplify formal verification. As perhaps the purest examples of this approach, some program logics prove probabilistic properties by working purely with non-probabilistic assertions; we call such systems lightweight logics. Examples include probabilistic relational Hoare logic [3] for proving the reductionist security of cryptographic constructions, and the related approximate probabilistic relational Hoare logic [4] for reasoning about differential privacy. These logics rely on the powerful abstraction of probabilistic couplings to derive probabilistic facts from non-probabilistic assertions [7]...|$|R
40|$|Client {{authentication}} {{on the web}} {{has remained}} in the internet-equivalent of the stone ages {{for the last two}} decades. Instead of adopting modern public-key-based authentication mechanisms, we seem to be stuck with traditional methods like passwords and cookies. These authentication methods are vulnerable {{to a wide range of}} attacks from simple password reuse to strong man-in-the-middle attackers that can inject themselves into the middle of encrypted communication channels. While many potential solutions have been proposed to sole the issues with the use of passwords and cookies for web authentication, most have failed to take hold. This lack of adoption stems from two issues. First, traditional password based authentication provides a very simple user experience. Any new technique must not increase user friction during login and provide a reasonable user experience. Secondly, a new authentication technique must not be difficult to implement in existing browsers and web applications or deploy to users. This thesis presents three techniques that provide protection against strong attackers while providing a low friction user experience. The first, Origin Bound Certificates, is a session hardening technique that cryptographically binds the user's authentication cookie to the TLS channel the cookie is presented over. This technique protects a user's session against strong attackers, requires no additional user interaction, requires little (or no) modification to existing web applications, and is compatible with existing data center infrastructure like TLS terminators. The second, Opportunistic Cryptographic Identity <b>Assertions,</b> is a <b>technique</b> in which the web browsers communicates with a user's cell phone in order to establish it as an opportunistic second factor in the initial login operation. This technique provides security assurances comparable or greater than conventional two factor authentication (i. e. phishing and password reuse prevention) while offering a simple user experience. Finally, I discuss a new federated login system that makes use of a new browser provided construct called the PostKey API. This interface allows the browser to create a cross certification that asserts ownership of client side keys to a trusted third party. The these cross certifications can be verified by an identity provider and used to harden existing federated login protocols as well as to create a new federation protocol that is resistant to man-in-the-middle attacks and leaked authentication tokens and provides relying parties with the means the better secure communication with the user...|$|R
