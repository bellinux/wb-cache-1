8841|10000|Public
5|$|EMA {{is used in}} {{linguistics}} and speech pathology to study articulation and in medicine to study oropharyngeal dysphagia. Unlike other methods of data collection, EMA does not expose subjects to ionizing radiation and allows for large <b>amounts</b> <b>of</b> <b>data</b> to be collected easily.|$|E
5|$|Palatography {{involves}} {{the painting of}} a colored substance onto the tongue which is then transferred onto the palate during articulation. A picture is then taken of the palate to record the location of contact and, if another palatogram is to be taken, the mouth is washed out and the tongue repainted. A particularly low cost method that is often used in fieldwork, {{it can be difficult}} to collect large <b>amounts</b> <b>of</b> <b>data.</b>|$|E
5|$|Instead, he practised a {{different}} style of science: systematically gathering data, discovering patterns common to whole groups of animals, and inferring possible causal explanations from these. This style is common in modern biology when large <b>amounts</b> <b>of</b> <b>data</b> become available in a new field, such as genomics. It does not result in the same certainty as experimental science, but it sets out testable hypotheses and constructs a narrative explanation of what is observed. In this sense, Aristotle's biology is scientific.|$|E
3000|$|The number <b>of</b> <b>data</b> {{forwarded}} in a node affects {{its energy}} consumption. The more data forwarded by a given node, the greater its energy consumption. Therefore, {{in order to}} increase the network lifetime, it is necessary to reduce the maximum <b>amount</b> <b>of</b> <b>data</b> forwarded by the nodes in the network. The <b>amount</b> <b>of</b> <b>data</b> forwarded by the node is usually composed of two parts: the <b>amount</b> <b>of</b> <b>data</b> generated by itself and the <b>amount</b> <b>of</b> <b>data</b> generated by other nodes. Therefore, reducing the <b>amount</b> <b>of</b> <b>data</b> generated by itself and the <b>amount</b> <b>of</b> <b>data</b> sent by other nodes can reduce the maximum energy consumption. The set of all nodes in the network is defined as S[*]=[*]{ 1,[*] 2,[*]⋯,[*]N}, the network lifetime is denoted by l, and the <b>amount</b> <b>of</b> <b>data</b> forwarded by each node is Di. We have [...]...|$|R
3000|$|... at any relay node. Since relay nodes {{are neither}} sources nor sinks, the <b>amount</b> <b>of</b> <b>data</b> in equals the <b>amount</b> <b>of</b> <b>data</b> out; [...]...|$|R
30|$|Proof: When the <b>amount</b> <b>of</b> {{non-redundant}} <b>data</b> <b>of</b> each node in {{the network}} is known, the <b>amount</b> <b>of</b> <b>data</b> Dx forwarded by each node {{can be obtained by}} Corollary 1 so as to obtain the maximum <b>amount</b> <b>of</b> <b>data</b> Dmax to be forwarded.|$|R
5|$|When an SSD {{is writing}} large <b>amounts</b> <b>of</b> <b>data</b> sequentially, the write {{amplification}} {{is equal to}} one meaning there is no write amplification. The reason is as the data is written, the entire block is filled sequentially with data related to the same file. If the OS determines that file is to be replaced or deleted, the entire block can be marked as invalid, {{and there is no}} need to read parts of it to garbage collect and rewrite into another block. It will need only to be erased, which is much easier and faster than the read-erase-modify-write process needed for randomly written data going through garbage collection.|$|E
5|$|Perl's text-handling {{capabilities}} can be {{used for}} generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large <b>amounts</b> <b>of</b> <b>data</b> for post-processing by a Perl subprogram.|$|E
25|$|The term is a misnomer, {{because the}} goal is the {{extraction}} of patterns and knowledge from large <b>amounts</b> <b>of</b> <b>data,</b> not the extraction (mining) of data itself.|$|E
5000|$|In {{comparison}} with ASFA, {{a system which}} transmits only a maximum <b>amount</b> <b>of</b> <b>data</b> per frequency, EBICAB uses electronic lookup table, the <b>amount</b> <b>of</b> <b>data</b> transmitted is much larger.|$|R
30|$|An {{evaluation}} of the online performance of our method reveals the required <b>amount</b> <b>of</b> <b>data</b> to estimate a crowd density. The required <b>amount</b> <b>of</b> <b>data</b> is closely connected to the required <b>amount</b> <b>of</b> pedestrians. These two aspects should be investigated jointly.|$|R
5000|$|The {{difference}} lies {{only with}} {{the ability of a}} Turing machine to manipulate an unbounded <b>amount</b> <b>of</b> <b>data.</b> However, given a finite <b>amount</b> <b>of</b> time, a Turing machine (like a real machine) can only manipulate a finite <b>amount</b> <b>of</b> <b>data.</b>|$|R
25|$|In applied economics, {{input-output}} models employing {{linear programming}} methods are quite common. Large <b>amounts</b> <b>of</b> <b>data</b> are run through computer programs to analyse {{the impact of}} certain policies; IMPLAN is one well-known example.|$|E
25|$|ESO telescopes {{generate}} large <b>amounts</b> <b>of</b> <b>data</b> {{at a high}} rate, {{which are}} stored in a permanent archive facility at ESO headquarters. The archive contains more than 1.5million images (or spectra) with a total volume of about 65terabytes (65,000,000,000,000bytes) of data.|$|E
25|$|In {{contrast}} to these views, Artificial Intelligence {{companies such as}} Amazon Mechanical Turk and CrowdFlower are using collective intelligence and crowdsourcing or consensus-based assessment to collect the enormous <b>amounts</b> <b>of</b> <b>data</b> for machine learning algorithms such as Keras and IBM Watson.|$|E
5000|$|Quantity-based pricing: This is the {{simplest}} model to implement. The vendors charge their customers {{based on the}} <b>amount</b> <b>of</b> <b>data</b> they want to use. Subscriptions for an unlimited <b>amount</b> <b>of</b> <b>data</b> {{is referred to as}} the fire-hose approach.|$|R
30|$|As the {{perspective}} of communication traffic, the malicious application purpose is to steal a lot <b>of</b> user <b>data,</b> so the <b>amount</b> <b>of</b> <b>data</b> uploaded is usually larger than the <b>amount</b> <b>of</b> <b>data</b> downloaded; the overall data flow is from the inside to the outside.|$|R
3000|$|... is {{the desired}} <b>amount</b> <b>of</b> fresh <b>data</b> per time period, ΔD(t) {{is the actual}} <b>amount</b> <b>of</b> <b>data</b> loaded during time period t. Initially phase t[*]=[*] 0,[*]f(0)[*]=[*]ΔD(0)/D [...]...|$|R
25|$|In {{the twenty-first}} century, AI {{techniques}} {{have experienced a}} resurgence following concurrent advances in computer power, large <b>amounts</b> <b>of</b> <b>data,</b> and theoretical understanding; and AI techniques have become {{an essential part of}} the technology industry, helping to solve many challenging problems in computer science.|$|E
25|$|On October 26, 2015, Cisco {{announced}} its intent to acquire ParStream, a privately held {{company based in}} Cologne, Germany, that provides an analytics database that allows companies to analyze large <b>amounts</b> <b>of</b> <b>data</b> and store it in near real time anywhere in the network.|$|E
25|$|In {{scientific}} computing, the GPGPU {{technique to}} pass large <b>amounts</b> <b>of</b> <b>data</b> bidirectionally between a GPU and CPU was invented; speeding up analysis on {{many kinds of}} bioinformatics and molecular biology experiments. The technique has also been used for Bitcoin mining and has applications in computer vision.|$|E
50|$|Respective <b>amount</b> <b>of</b> <b>data</b> for maintenance.|$|R
3000|$|... hence, the {{summation}} over l {{for each}} k gives the total <b>amount</b> <b>of</b> <b>data</b> generated at node-k. Since both the flows and the <b>amount</b> <b>of</b> <b>data</b> injected on each path are integer variables, the packets cannot be split (all packets are created as L [...]...|$|R
40|$|The {{exponential}} growth <b>of</b> the <b>amount</b> <b>of</b> <b>data</b> {{generated by the}} scientific research requires the implementing <b>of</b> a <b>data</b> management system. The article describe the development of such a system based on Open Source software for a small sized (and budgeted) research team. This first part concentrates on the system architecture. The fast growing <b>amount</b> <b>of</b> <b>data</b> that results from the activities of even a small sized research team impose the use <b>of</b> a <b>data</b> management system in order to provide:- reliable and secure storage <b>of</b> large <b>amount</b> <b>of</b> <b>data</b> from multipl...|$|R
25|$|Automated patch clamp {{systems have}} {{recently}} been developed, in order to collect large <b>amounts</b> <b>of</b> <b>data</b> inexpensively in a shorter period of time. Such systems typically include a single-use microfluidic device, either an injection molded or a polydimethylsiloxane (PDMS) cast chip, to capture a cell or cells, and an integrated electrode.|$|E
25|$|Transcriptomics studies {{generate}} large <b>amounts</b> <b>of</b> <b>data</b> {{that has}} potential applications {{far beyond the}} original aims of an experiment. As such, raw or processed data may be deposited in public databases to ensure their utility for the broader scientific community. For example, as of 2016, the Gene Expression Omnibus contained millions of experiments.|$|E
25|$|The {{challenges}} in particle physics have furthered major technological progress of widespread importance. For example, the World Wide Web {{began as a}} project to improve CERN's communication system. CERN's requirement to process massive <b>amounts</b> <b>of</b> <b>data</b> produced by the Large Hadron Collider also led to contributions to the fields of distributed and cloud computing.|$|E
30|$|Kahler et al. achieve realtime {{processing}} of KinectFusion on mobile devices [58]. To reduce a computational cost, {{they use a}} voxel block hashing in the mapping process. RGB-D vSLAM suffer from <b>amount</b> <b>of</b> <b>data.</b> In the literature [59], they reduce <b>amount</b> <b>of</b> <b>data</b> by unifying co-planar points.|$|R
30|$|For {{the energy}} {{consumption}} for transmitting non-redundant data, {{considering that the}} node’s non-redundant data is mx, Similar to Theorem 7, the <b>amount</b> <b>of</b> <b>data</b> that uses the retransmission mechanism can be obtained, and the <b>amount</b> <b>of</b> <b>data</b> forwarded by each node can be obtained according to (27) as D_x^non.|$|R
50|$|Because digital {{signature}} algorithms cannot sign a large <b>amount</b> <b>of</b> <b>data</b> efficiently, most implementations use a hash function to reduce ("compress") the <b>amount</b> <b>of</b> <b>data</b> {{that needs to}} be signed down to a constant size. Digital signature schemes are often vulnerable to hash collisions, unless using techniques like randomized hashing.|$|R
25|$|Large <b>amounts</b> <b>of</b> <b>data</b> {{have been}} {{gathered}} {{and these are}} collated into reports, of which a common type is the State of the Environment publications. A recent major report was the Millennium Ecosystem Assessment, with input from 1200 scientists and released in 2005, which showed {{the high level of}} impact that humans are having on ecosystem services.|$|E
25|$|The World Bank {{collects}} {{and processes}} large <b>amounts</b> <b>of</b> <b>data</b> and generates {{them on the}} basis of economic models. These data and models have gradually been made available to the public in a way that encourages reuse, whereas the recent publications describing them are available as open access under a Creative Commons Attribution License, for which the bank received the SPARC Innovator 2012 award.|$|E
25|$|The Federal Reserve {{records and}} publishes large <b>amounts</b> <b>of</b> <b>data.</b> A few {{websites}} where data is published {{are at the}} Board of Governors Economic Data and Research page, the Board of Governors statistical releases and historical data page, and at the St. Louis Fed's FRED (Federal Reserve Economic Data) page. The Federal Open Market Committee (FOMC) examines many economic indicators prior to determining monetary policy.|$|E
40|$|Studying the {{evolution}} of long lived processes such as the development history of a software system or the publication history of a research community, requires the analysis <b>of</b> a vast <b>amount</b> <b>of</b> <b>data.</b> Aggregation techniques and data specific techniques are usually used {{to cope with the}} large <b>amount</b> <b>of</b> <b>data...</b>|$|R
5000|$|<b>Amount</b> <b>of</b> usage (Duration <b>of</b> call, <b>amount</b> <b>of</b> <b>data,</b> number <b>of</b> messages, {{number of}} songs) ...|$|R
30|$|Session upload data/download data (upload/download, unit: none). The {{fact that}} during communication, the <b>amount</b> <b>of</b> {{uploaded}} <b>data</b> {{is larger than}} the <b>amount</b> <b>of</b> <b>data</b> downloaded; thus, the characteristic value is generally greater than 1.|$|R
