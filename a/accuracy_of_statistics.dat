14|10000|Public
25|$|The <b>accuracy</b> <b>of</b> <b>statistics</b> {{reporting}} on Afro-Latin Americans has been questioned, especially {{where they are}} derived from census reports in which the subjects choose their own designation, because in various countries the concept of African ancestry is viewed with differing attitudes.|$|E
2500|$|During his tenure, he {{proposed}} {{the merger of}} the OPCS and the CSO in August 1994, which was subsequently announced by Prime Minister John Major in September 1995 following a consultation period and took place on 1 April 1996 when the Office for National Statistics was launched. He persuaded the Chancellor of the Exchequer, Norman Lamont, to reduce ministerial access to economic statistics in advance of publication and to permit statistics to be released independently of ministers. [...] He produced the Official Statistics Code of Practice, first published in April 1995, which set good practice and principles for statisticians producing official statistics {{with the aim of}} promoting high standards and maintaining public confidence in official statistics. He led the CSO through its early years as a 'Next Steps Agency' with demanding and quantified targets for the <b>accuracy</b> <b>of</b> <b>statistics.</b>|$|E
50|$|The <b>accuracy</b> <b>of</b> <b>statistics</b> {{reporting}} on Afro-Latin Americans has been questioned, especially {{where they are}} derived from census reports in which the subjects choose their own designation, because in various countries the concept of African ancestry is viewed with differing attitudes.|$|E
50|$|There is a {{trade-off}} between the <b>accuracy</b> <b>of</b> the <b>statistics</b> estimated in a privacy-preserving manner, and the privacy parameter &epsilon;.|$|R
5000|$|... {{promote the}} collection, criticism, and {{interpretation}} <b>of</b> economic <b>statistics</b> ... by investigation <b>of</b> the sources and probable <b>accuracy</b> <b>of</b> existing <b>statistics</b> ... and by developing the application to economic <b>statistics</b> <b>of</b> modern methods of statistical analysis which have hitherto been utilized more extensively in other sciences than in economics.|$|R
30|$|In DS-NC, the {{decision}} <b>statistics</b> <b>of</b> each source node {{needs to be}} obtained at each receive antenna for BPA computation. However, due to SI, the decreased <b>accuracy</b> <b>of</b> decision <b>statistics</b> leads to the bad performance of DS-NC. Hence, the iteration method is adopted for BPA computation to mitigate this problem.|$|R
5000|$|The {{accuracy}} of Polaris’ data on human trafficking {{has been questioned}} by multiple sources. In 2011, Polaris was criticized for knowingly using false and misleading data to exaggerate the number of trafficked sex workers and understate their age of entry into sex work. Since then, Polaris has partnered with data analysis firm Palantir Technologies to improve the organization of data reported to the National Human Trafficking Resource Center and the <b>accuracy</b> <b>of</b> <b>statistics</b> released to the public.More recently, in 2015, Polaris was accused of using unreferenced and uncorroborated data to exaggerate the income and number of clients seen by street based and massage parlor based sex workers and the prevalence of [...] "pimps".|$|E
5000|$|During his tenure, he {{proposed}} {{the merger of}} the OPCS and the CSO in August 1994, which was subsequently announced by Prime Minister John Major in September 1995 following a consultation period and took place on 1 April 1996 when the Office for National Statistics was launched. He persuaded the Chancellor of the Exchequer, Norman Lamont, to reduce ministerial access to economic statistics in advance of publication and to permit statistics to be released independently of ministers. [...] He produced the Official Statistics Code of Practice, first published in April 1995, which set good practice and principles for statisticians producing official statistics {{with the aim of}} promoting high standards and maintaining public confidence in official statistics. He led the CSO through its early years as a 'Next Steps Agency' with demanding and quantified targets for the <b>accuracy</b> <b>of</b> <b>statistics.</b>|$|E
3000|$|... 10 However, {{there are}} {{considerable}} uncertainties regarding the <b>accuracy</b> <b>of</b> <b>statistics</b> relating to UK students studying abroad (Findlay et al. 2010).|$|E
40|$|While TB {{and malaria}} are {{currently}} {{the number one}} killers in India, inaction could make AIDS {{the leading cause of}} death. A decade after setting up the National AIDS Control Organisation, the nation is still debating the <b>accuracy</b> <b>of</b> HIV/AIDS <b>statistics.</b> Are current responses adequate to stem the spread of the disease...|$|R
50|$|CIPSEA did {{not give}} the {{statistical}} agencies new opportunities to use federal data on business taxes {{in combination with the}} other sources for statistical purposes. Such data is protected by Title 26 and new laws would be required to enable the other agencies to use such data, which could help them improve the classifications of business into industries and thus improve the <b>accuracy</b> <b>of</b> industry <b>statistics.</b> Later proposals address this prospect.|$|R
50|$|In addition, Passages' {{treatment}} {{philosophy is}} controversial both because it disputes {{the efficacy of}} multi-step treatment programs and also because the founders {{do not believe that}} addiction is a disease. Passages claims that its method produces above an 80-percent rehabilitation rate. However, the <b>accuracy</b> <b>of</b> these <b>statistics</b> have been questioned by other rehabilitation professionals, particularly because they include people who have been out of treatment for only 30 days.|$|R
30|$|Due to {{the lack}} of an {{existing}} rail line that contains continuous scars and crack defects, select different lines, respectively, at different speeds through the defect area, to detect the <b>accuracy</b> <b>of</b> <b>statistics.</b> Among them, 20 of the scar defects and 20 crack defects were tested by 0.5 m/s, 1 m/s, 2 m/s, 4 m/s, and 6 m/s to get the best detection speed and the detection accuracy of the rail surface defects.|$|E
40|$|Abstract. For Shanghai Tobacco Group Beijing Cigarette Factory {{computer}} consume material management, {{research and}} development a Computer Consume Material Management System (CCMMS). The system based on B/S mode, {{it is very important}} to improve the enterprise computer consume material management, enhance computer consume material utilization efficiency, standardized consume material management, improve the <b>accuracy</b> <b>of</b> <b>statistics</b> and reduce the work time of administrator. Also introduced the CCMMS function and structure design, management workflow optimize, developing tools and method...|$|E
40|$|Many machine {{learning}} algorithms {{are based on the}} assumption that training examples are drawn independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We show that the classic approach of ignoring this problem potentially can have a harmful effect on the <b>accuracy</b> <b>of</b> <b>statistics,</b> and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in this networked setting, providing significantly improved results. An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities...|$|E
2500|$|The {{federal and}} state governments have {{traditionally}} cooperated to some extent to improve vital statistics. From 1900 to 1946 the U.S. Census Bureau designed standard birth certificates, collected vital statistics on a national basis, and generally sought to improve the <b>accuracy</b> <b>of</b> vital <b>statistics.</b> In 1946 that responsibility was passed to the U.S. Public Health Service. Unlike the British system of recording all births in [...] "registers", the states file an individual document for each and every birth.|$|R
40|$|This paper {{examined}} bankruptcy predictive <b>accuracy</b> <b>of</b> five <b>statistics</b> models [...] {{discriminant analysis}} logistic regression, probit regression, neural networks, {{support vector machine}} (SVM), and genetic-based SVM (GA-SVM) that influenced by variable selection. Empirical {{results indicate that the}} SVM-based models are very promising models for predicting financial failure, in terms of both best predictive accuracy and generalization ability. In addition, variable selection had the lowest influence <b>of</b> predictive <b>accuracy</b> in the GA-SVM model with optimal values of parameters...|$|R
5000|$|The {{federal and}} state governments have {{traditionally}} cooperated to some extent to improve vital statistics. From 1900 to 1946 the U.S. Census Bureau designed standard birth certificates, collected vital statistics on a national basis, and generally sought to improve the <b>accuracy</b> <b>of</b> vital <b>statistics.</b> In 1946 that responsibility was passed to the U.S. Public Health Service. Unlike the British system of recording all births in [...] "registers", the states file an individual document for each and every birth.|$|R
40|$|Usage {{statistics}} are frequently used by repositories {{to justify their}} value to the management who decide about the funding to support the repository infrastructure. Another reason for collecting usage statistics at repositories is {{the increased use of}} webometrics in the process of assessing the impact of publications and researchers. Consequently, one of the worries repositories sometimes have about their content being aggregated is that they feel aggregations have a detrimental effect on the <b>accuracy</b> <b>of</b> <b>statistics</b> they collect. They believe that this potential decrease in reported usage can negatively influence the funding provided by their own institutions. This raises the fundamental question of whether repositories should allow aggregators to harvest their metadata and content. In this paper, we discuss the benefits of allowing content aggregations harvest repository content and investigate how to overcome the drawbacks...|$|E
40|$|This paper {{focuses on}} the needs of the end users of {{statistics}} for metadata about the statistics offered by the statistical producers. It is set within a framework proposed in an earlier paper [Nordbotten 2000 a]. Since the discussion is in the context of data editing, the discussion is mainly limited to metadata about the <b>accuracy</b> <b>of</b> <b>statistics</b> and statistical data editing. Several questions are addressed in this paper. The first is what are the end users' needs for metadata and how will the users of statistics react on metadata. A second question addressed is how should a statistical producer react to the information needed by the users and provide metadata, which might serve the needs. At the end of the paper, a short discussion is included about required research and development for implementation of a metadata service...|$|E
40|$|This {{paper is}} an invited {{paper to the}} ECE Work Session on Statistical Data Editing in Cardiff, 18 - 20 October 2000. The paper has been {{prepared}} in co-operation with and is funded by Statistics Sweden. More publications from the author:[URL] paper focuses {{on the needs of}} the end users of statistics for metadata about the statistics offered by the statistical producers. It is set within a framework proposed in an earlier paper [Nordbotten 2000 a]. Since the discussion is in the context of data editing, the discussion is mainly limited to metadata about the <b>accuracy</b> <b>of</b> <b>statistics</b> and statistical data editing. Several questions are addressed in this paper. The first is what are the end users' needs for metadata and how will the users of statistics react on metadata. A second question addressed is how should a statistical producer react to the information needed by the users and provide metadata, which might serve the needs. At the end of the paper, a short discussion is included about required research and development for implementation of a metadata service...|$|E
40|$|The <b>accuracy</b> <b>of</b> cause-of-death <b>statistics</b> {{substantially}} {{depends on}} the quality of cause-of-death information in death certificates, primarily completed by medical doctors. Deficiencies in cause-of-death certification have been observed across the world, and over time. Despite educational interventions targeting {{to improve the quality of}} death certification, their intended impacts are rarely evaluated. This review aims to provide empirical evidence that could guide the modification of existing educational programs, or the development of new interventions, which are necessary to improve the capacity of certifiers as well as the quality of cause-of-death certification, and thereby, the quality <b>of</b> mortality <b>statistics...</b>|$|R
40|$|This {{paper is}} devoted to {{thoroughly}} inves- tigating how to bootstrap the ROC curve, a widely used visual tool for evaluating the <b>accuracy</b> <b>of</b> test/scoring <b>statistics</b> s(X) in the bipartite setup. The issue of conﬁdence bands for the ROC curve is considered and a resampling procedure based on a smooth ver- sion of the empirical distribution called the ”smoothed bootstrap” is introduced. Theo- retical arguments and simulation results are presented {{to show that the}} ”smoothed boot- strap” is preferable to a ”naive” bootstrap in order to construct accurate conﬁdence bands...|$|R
40|$|This paper aims at {{measuring}} the robustness of Real Business Cycle international stylized facts across exchange rate regimes. I thus investigate th e {{impact of the}} Bretton Woods System and the ERM on the business cycle regularities. Thanks to bootstrap techniques, I measure the <b>accuracy</b> <b>of</b> the <b>statistics.</b> While exchange rate variability is sensitive {{to the nature of}} the exchange rate regime, the volatility of macroeconomic fundamentals does not differ across exchange rate regimes. Moreover, there is evidence that fixed-rates enhance international comovement of ouput, consumption and investment. EXCHANGE RATE; FINANCIAL POLICY; ECONOMIC MODELS...|$|R
40|$|Part 1 : Simulation, Optimization, Monitoring and Control TechnologyInternational audienceThe {{detection}} of pseudo-foreign fibers in cotton based on AVI(Automatic Visual Inspection) {{is crucial to}} improve the <b>accuracy</b> <b>of</b> <b>statistics</b> and classification of foreign fibers. To meet the requirement of textile factories, a new platform is introduced in which cotton bulks are floating with relative high speed of six meters per second, and the throughput of detected lint could be above 20 kg per hour. However, images captured by the new platform are blurred and not clear enough for post processes such as segmentation, feature extraction, target identification and statistics. Because thickness of the moving cotton bulks are not uniform, a part of or the whole object of pseudo-foreign fibers are blocked. Thus image enhancement algorithms should be investigated and implemented. In this paper {{the characteristics of the}} images acquired by the new platform are analyzed, and several image enhance algorithms are studied and compared on effectiveness and efficiency, which include Histogram Equalization, Wavelet Based Normalization, Homomorphic Filtering, Single Scale Retinex(SSR), Multiscale Retinex(MSR) and Variational Retinex. Result indicated that the Variational Retinex has a better performance and should be implemented in on-line pseudo-foreign fibers detection...|$|E
40|$|As no {{critical}} {{examination of}} agricultural statistics has been conducted, no {{progress has been}} made regarding a statistical analysis of agricultural growth during the early Meiji era of Hokkaido for many years. The amount of information related to the administration system during the early stage of development in Hokkaido has been increased remarkably by the analysis of public documents of the Hokkaido Development Agency, called hutatsu. This paper aims to clarify the institutional development of the agricultural statistics system during the early Meiji era of Hokkaido and evaluate the accuracy of the statistics by surveying the public documents. Although the agency began to collect statistics in 1870, the jurisdiction of statistical services and the statistics division in the agency were clarified by a regulation enacted in 1875. The new agricultural statistical information system, which covers areas in remote locations, was developed in 1877. Due to communication problems, reports on these areas often missed the scheduled deadlines. This is probably the main reason that data on Hokkaido were not included in published statistics such as nousanhyo. After the introduction of the new agricultural information system, nouji-tsushin, statistics gathering and publishing began to work effectively even in the last frontier of the Nemuro region. The <b>accuracy</b> <b>of</b> <b>statistics</b> had probably reached a level comparable with that of other prefectures in 1886...|$|E
40|$|Abstract—The {{nematode}} Caenorhabditis elegans is an im-portant model organism {{for many}} areas of biological research including genetics, development, and neurobiology. A common technique used in studying the locomotion of the worm is to take video of the worm in motion and analyze it to extract relevant data. A number of different software solutions exist to analyze these videos, {{yet there is no}} technique to determine the <b>accuracy</b> <b>of</b> the <b>statistics</b> being produced. We have developed a method to quantify the <b>accuracy</b> <b>of</b> a given analysis pipeline by using video of a biologically accurate simulation. Using this process we develop a metric to quantify the <b>accuracy</b> <b>of</b> a given pipeline, and we demonstrate this metric by comparing different implementations of a popular pipeline. Keywords-Image analysis; Feature extraction; Caenorhabdi-tis elegans; Simulatio...|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "In accordance with the Government Performance and Results Act of 1993, the Department of Justice (DOJ) provides Congress and the public with an annual performance report. These reports serve as an important measure of DOJ's progress related to its strategic goals and objectives, including statistics on its Fiscal Year 2000 Performance Report. We were asked to review the <b>accuracy</b> <b>of</b> DOJ's terrorism-related conviction statistics. Among other objectives, in this report we (1) identify how DOJ develops its terrorism-related conviction statistics and (2) assess whether DOJ has sufficient management oversight and internal controls in place to ensure the <b>accuracy</b> <b>of</b> terrorism-related <b>statistics</b> included in its annual performance reports. ...|$|R
25|$|The results {{indicated}} that users still favoured a traditional field-based census. There are newly emerging and increasing data requirements for local-decision-making. There {{is a need for}} more timely and regular statistics on a wider-range <b>of</b> themes. <b>Accuracy</b> <b>of</b> data and <b>statistics</b> based on the local geography is of higher importance than the frequency of data production. Genealogists are concerned about the potential loss of historical records.|$|R
5000|$|Brian Tamanaha, a law {{professor}} and legal theorist at Washington University, has questioned the <b>accuracy</b> <b>of</b> employment <b>statistics</b> provided by some law schools. He notes that employment and salary information provided by law schools is based on surveys of recent graduates. This information is consolidated and made available by the American Bar Association. In his book entitled [...] "Failing Law Schools," [...] Tamanaha concludes {{that because of the}} debt loads and job prospects facing law graduates, [...] "Many {{law professor}}s at many law schools across the country are selling a degree to their students that they would not recommend to people close to them." ...|$|R
40|$|We {{investigate}} the <b>accuracy</b> <b>of</b> trade <b>statistics</b> employing intra-industry trade analysis for 29 countries. We exploit {{the fact that}} country A's exports to country B are country B's imports from country A. The data published by the two sides can be compared with each other. Are A's or B's data more accurate? Our analysis {{is based on the}} average degree of 'similarity' between country A's and country B's data and the 'mirror' data of all the other countries in the sample. The ranking of the countries by the <b>accuracy</b> <b>of</b> their data yielded by our calculations mainly confirm our a priori expectations. We also investigate whether the data of 'good' reporting countries can be more trusted than some averages of the latter and the data reported by the 'bad' reporters. ...|$|R
40|$|Rapid {{economic}} growth and radical structural transformation pose a challenge to official statisticians as they seek to encompass new economic activities and phenomena. The <b>accuracy</b> <b>of</b> official <b>statistics</b> is liable to come into question. Urban unemployment in China is a good example. This paper estimates the urban unemployment rate using administrative statistics, population census data and a recent sample survey data set, and provides a critique showing in some detail how and why Chinese unemployment statistics are a minefield for the unwary and unemployment is so difficult to measure. Nevertheless, {{it is found that}} the urban unemployment rate rose rapidly over the 1990 s and exceeded 11 % in 1999 and 2000. The paper concludes by considering the implications of the findings for understanding unemployment, for policy, and for the collection <b>of</b> <b>statistics.</b> JEL Classifications: C 13, J 21, J 64, J 79, O 53,...|$|R
40|$|Despite {{tremendous}} {{debate and}} policy interest in software piracy, statistics {{compiled by the}} Business Software Alliance (BSA) have generally been accepted at face value by policy makers and scholars. However, the <b>accuracy</b> <b>of</b> BSA <b>statistics</b> has not been independently verified. Based on {{a review of the}} BSA methodology and empirical analysis, I conclude the following. A change in the BSA consultant and methodology around 2002 - 03 had systematic effects on published piracy rates. First, the trend rate of decrease of piracy rates fell from 2. 0 % points per year to 1. 1 % points per year. Second, piracy rates apparently became more sensitive to changes in income. Third, piracy rates depended on projections of software usage based on per capita incomes in the respective countries...|$|R
40|$|Despite {{tremendous}} {{debate and}} policy interest in software piracy, the <b>accuracy</b> <b>of</b> piracy <b>statistics</b> {{compiled by the}} Business Software Alliance (BSA) has been accepted at face value. Based on {{a review of the}} BSA methodology and empirical analysis, I conclude the following. First, prior cross-country studies of software piracy in the years 2002 and earlier were mis-specified: they more likely explained the demand for legitimate software relative to computers in use rather than piracy rates. Second, BSA statistics were biased on a cross-country basis either in the years 2002 and earlier, or the years 2003 and after, or both. Third, from 2003 onward, following a change in the BSA consultant and methodology, piracy rates across countries were inflated by an average of almost 4 % points...|$|R
40|$|Short-term {{statistics}} (STS) {{are important}} early indicators of economic activity. The statistics are obligatory for all EU countries and {{also serve as}} input to national accounts. In most countries, short-term Statistics are based on business surveys. However, in recent years {{a number of countries}} have gradually replaced their business surveys with business VAT registry data. An important question is whether these surveys and registries are representative of the populations and whether representativity is stable in time. We apply R-indicators and partial R-indicators to measure the representativity of both kinds of data sources. We find large differences between different months of the year and between the two data sources. We discuss dual frame approaches that optimize the <b>accuracy</b> <b>of</b> STS <b>statistics...</b>|$|R
40|$|Traditional query optimizers {{rely on the}} <b>accuracy</b> <b>of</b> {{estimated}} <b>statistics</b> {{to choose}} good execution plans. This design often leads to suboptimal plan choices for complex queries, since errors in estimates for intermediate subexpressions grow exponentially {{in the presence of}} skewed and correlated data distributions. Reoptimization is a promising technique to cope with such mistakes. Current re-optimizers first use a traditional optimizer to pick a plan, and then react to estimation errors and resulting suboptimalities detected in the plan during execution. The effectiveness of this approach is limited because traditional optimizers choose plans unaware of issues affecting reoptimization. We address this problem using proactive reoptimization, a new approach that incorporates three techniques: i) the uncertainty in estimates <b>of</b> <b>statistics</b> is computed in the form of bounding boxes around these estimates, ii) these bounding boxes are used to pick plans that are robust to deviations of actual values from their estimates, and iii) accurate measurements <b>of</b> <b>statistics</b> are collected quickly and efficiently during query execution. We present an extensive evaluation of these techniques using a prototype proactive re-optimizer named Rio. In our experiments Rio outperforms current re-optimizers by up to a factor of three. 1...|$|R
