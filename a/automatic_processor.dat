23|51|Public
50|$|Scott Mahlke {{from the}} University of Michigan, Ann Arbor, MI was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2015 for {{contributions}} to compiler code generation and <b>automatic</b> <b>processor</b> customization.|$|E
40|$|An <b>automatic</b> <b>processor</b> {{of written}} French {{language}} is described. This processor uses syntactic and semantic informations about words {{in order to}} construct a semantic net representing {{the meaning of the}} sentences. The structure of the network and the principles of the parser are explained. An application to the processing of the medical records is then discussed...|$|E
40|$|This thesis {{describes}} an optimising framework for <b>automatic</b> <b>processor</b> customisation. There are four major contributions: 1. First, {{we present a}} compiler that supports code generation and power aware pipeline scheduling for a processor with an extensible instruction set. Custom instructions can be introduced by the user, or be selected automatically at compile time to generate scheduled code for an Application Specific Instruction Processor. 2. Second, we describe ‘Similar sub-instructions’, a novel technique for <b>automatic</b> <b>processor</b> customisation that is implemented {{as part of our}} compiler. Similar sub-instructions is a heuristic approach that is unique in being able to build general instruction datapaths that can accelerate several structurally similar operations. 3. Third, we introduce a multi-threaded processor with a customisable instruction set as a target for our compiler. The processor is supported using a simulator driven by a synthesizable hardware model. This arrangement facilitates rapid hardware testing, allows the processor microarchitectur...|$|E
5000|$|POWER O.I.S. <b>automatic</b> {{utilisation}} with <b>processor</b> Venus Engine HD II ...|$|R
40|$|This {{study shows}} that using {{computational}} linguistic models is beneficial for descriptive linguistics and psycholinguistics. It applies two models to various English genres and learner language: 1) surprisal and 2) a syntactic parser, allowing us to investigate the role of ambiguity and the interplay between idiom and syntax principles. We find that surprisal and ambiguity are higher for learner language, while parser scores and model fit are lower. In addition, the random application of alternations leads to more ambiguous sentences. Failures to generate optimal orderings {{in the sense of}} relevance theory, such as nonnative-like utterances by language learners exhibit, increase processing load, both for human and <b>automatic</b> <b>processors.</b> As human and automatic parsing difficulties correlate, we suggest syntactic parsers as psycholinguistic processing models...|$|R
40|$|Odyssée is an <b>automatic</b> {{differentiation}} <b>processor</b> for fortran code which implements {{both the}} forward and reverse mode of automatic differentiation. This document presents the functionalities of Odyssée through an illustrative example. It presents the basic concepts and objects of Odyssée, and completely describes the command language of Odyssée and its graphical user interface...|$|R
40|$|This paper {{presents}} {{the design of}} highly optimized TTA architectures for image processing applications. An <b>automatic</b> <b>processor</b> design framework as described in [2] is used. Specialized hardware is used to improve the performance-cost ratio of the processors. An explorer searches the design space for solutions that are good in terms of cost and performance. We show that architectures can be found that efficiently execute very different algorithms at low cost. A hardware feasible architecture is presented that efficiently executes a set of image processing algorithms and performs almost equally or better than alternative, commercial-available solutions do...|$|E
40|$|Background: Accurate film {{processing}} is {{of paramount importance}} in acquiring a good diagnostic radiograph. Radiographic films show variations in densities and contrast, with changes in processing conditions, and also film type, all of which are interdependent. Therefore, this research was conducted to recognize the effect of time and temperature variations of <b>automatic</b> <b>processor</b> on the sensitometric properties of blue and green light sensitive screen films. The study also aimed to note the effect on sensitometric properties when mismatch occurred when using between the screen and film belonging to different manufacturers. Materials and Methods: Sixty green light sensitive and 60 blue light sensitive spectrally matched screen film combinations were used in the study. However, the films and the intensifying screens employed belonged to different manufacturers. These films were exposed to five different exposure times and subsequently processed in an <b>automatic</b> <b>processor,</b> using two different protocols. Initially, at constant processing time of 2. 5 min, five different processing temperatures were employed. Later, maintaining constant processing temperature of 35 °C and five different processing times were engaged. Density, contrast and speed were calculated, using H and D curve. Results: Results revealed increasing density, contrast and speed values with increasing processing times and temperatures of both green and blue sensitive films. Conclusion: This investigation clearly establishes the possibility of obtaining optimal sensitometric properties, despite using intensifying screens and films of different manufacturers, if spectral match is ensured...|$|E
30|$|Histological {{samples from}} the excised tissues, {{including}} the liver and the kidney, were subjected to light and electron microscopy. For light microscopy, the samples were initially fixed in a 10 % formalin buffer for 24 h at 4 °C and were then immediately dehydrated in a graded series of ethanol, immersed in xylene and embedded in paraffin wax using an <b>automatic</b> <b>processor.</b> Sections of 5 to 7 μm were then mounted. After deparaffinisation, the sections were rehydrated, stained with hematoxylin and eosin (Humason 1972) and mounted with Cristal/Mount (Sigma Aldrich, St. Louis, MO, USA). Subsequently, all of the tissues were examined microscopically, and their histological abnormalities were recorded.|$|E
40|$|Background and aims. The aim of {{this study}} was to {{evaluate}} x-ray protection methods in dental offices in Tabriz. Materials and methods. In this study 142 dental offices were evaluated. A questionnaire-based method was used. The data was analyzed by descriptive methods. Results. The least commonly used methods were leaded walls (4. 9 %) and film badges (16. 9 %) and the most commonly used methods were lead partitions (67. 6 %) and position-distance rule (68. 3 %). The most commonly used patient protection devices were E-speed films (84. 5 %) and long collimators (66. 2 %). The least commonly used methods, in this respect, were <b>automatic</b> <b>processors</b> (2. 1 %) and rectangular collimators (0 %). Conclusion. Regarding protection methods for the patient, results did not conform to international standards. Mostly, manual processing was used, resulting in extra radiation dose to patients. The methods which reduce the received dose of patients were disregarded in offices compared to educational centers, necessitating optimization of educational programs in these fields...|$|R
40|$|Electroencephalography (EEG) is a {{fundamental}} diagnostic instrument for many neurological disorders, {{and it is the}} main tool for the investigation of the cognitive or pathological activity of the brain through the bioelectromagnetic fields that it generates. The correct interpretation of the EEG is misleading, both for clinicians’ visual evaluation and for automated procedures, because of artifacts. As a consequence, artifact rejection in EEG is a key preprocessing step, and the quest for reliable <b>automatic</b> <b>processors</b> has been quickly growing in the last few years. Recently, a promising automatic methodology, known as automatic wavelet-independent component analysis (AWICA), has been proposed. In this paper, a more efficient and sensitive version, called enhanced-AWICA (EAWICA), is proposed, and an extensive performance comparison is carried out by a step of tuning the different parameters that are involved in artifact detection. EAWICA is shown to minimize information loss and to outperform AWICA in artifact removal, both on simulated and real experimental EEG recordings...|$|R
40|$|We {{intend to}} show in this paper an {{application}} made {{in the area of}} the Computational Linguistics: an <b>automatic</b> morphological <b>processor</b> of the spanish. We have explained the process of elaboration, since the building of the dictionaries and tables, which store lemmas and morphemes with their information, until the running of the computer formalisrn programmed in Turbo-Prolog, which makes the automatic analysis of the words...|$|R
30|$|The NODAGA-RGDyK is a GMP-produced monomeric integrin-targeting peptide {{purchased}} from Advanced Biochemical compounds ABX, Radeberg, Germany. As previously described [24], the NODAGA-RGDyK was radiolabelled with the 68 Ga eluate of a 68 Ge-generator IGC 100 (Eckert & Ziegler, Germany) using an <b>automatic</b> <b>processor</b> unit, Modular-Lab PharmTracer (Eckert & Ziegler, Germany). 68 Ga was eluted with 0.1  mol/l HCl. NODAGA-RGDyK (20  μg) was radiolabelled {{with the high}} activity 68 Ga fraction by incubation for 20  min at room temperature. After cartridge purification, the ready for use 68 Ga-NODAGA-RGDyK was eluted in 50 % ethanol through a 0.22 -μm sterile filter and diluted into NaCl solution. High-pressure liquid chromatography analysis was performed on a μ-Bondapak column (Waters C- 18) run with trifluoroacetic acid and acetonitrile.|$|E
40|$|Due to the {{decreasing}} {{feature size}} of VLSI technology, {{the amount of}} hardware which can be integrated into a single chip increases. As a result, future processor chips can execute tens of operations concurrently. Many applications can profit from these huge amounts of hardware parallelism by designing application specific processors. Two problems emerge however: 1) the design space gets large; {{it is difficult to}} chose a satisfactory solution, and 2) the design complexity increases and therefore design cycle gets too long. Solving these problems makes tools for <b>automatic</b> <b>processor</b> design necessary. In this paper we present such a tool. It generates processor layout for so called transport triggered processors. The processor configuration is largely parameterized. A working processor has already been produced using this generator. An important question concerns the quality of automatically generated designs: what do we pay in area and cycle time, when compared to a manual design? In orde [...] ...|$|E
40|$|The {{relationship}} between image quality and processing conditions was assessed {{in a survey}} of 26 primary health care clinics in Riyadh City. Each clinic is equipped with a basic X-ray room and a darkroom that has a small table-top <b>automatic</b> <b>processor.</b> Rooms were evaluated for the quality of safe light, light leakage, storage of films and chemicals and processor temperature setting. A relationship was obtained between the quality of these parameters and the analysis of characteristic curves (H and D curves) of images produced at each facility. Base plus fog indexes in 50 % of clinics were found to be above normal values. Contrast and speed indexes were above control in 46 and 19 % of clinics, respectively. The results showed that the image quality is negatively affected when the above conditions are unsatisfactory, even though the X-ray machine, cassettes and films used are in good condition. In conclusion, image quality can be improved significantly by applying quality control principles related to darkroom conditions...|$|E
30|$|The gonad (mantle tissue) {{from each}} mussel was {{dissected}} out after fixation, placed in histological cassettes in 70 ° ethanol and routinely processed for paraffin embedding in a Leica ASP 300 <b>automatic</b> tissue <b>processor</b> (Nusloch, Germany). Sections of 3 – 5  µm thick were {{cut in a}} Leitz 1512 microtome (Vienna, Austria) and stained with hematoxylin and eosin. The slides were examined using an Olympus BX 60 microscope equipped with a digital camera.|$|R
40|$|A {{survey of}} X-ray film {{processing}} departments revealed the main airborne contaminants to be sulphur dioxide and acetic acid at concentrations of about 0. 1 ppm. Glutaraldehyde was not detected {{either in the}} ambient air or in the exhaust duct from an <b>automatic</b> film <b>processor.</b> Laboratory studies confirmed sulphur dioxide and acetic acid as the main headspace constituents above working strength processing solutions but glutaraldehyde and butyraldehyde were also detected. (Crown copyright © 1996 Published by Hsevier Science Ltd...|$|R
40|$|Summary: A new commercially {{available}} Chemiluminescence immunoassay for the quantitative measurement of {{human chorionic gonadotropin}} and its-subunit in serum was compared with the enzyme immunoassay used in our routine laboratory. Human chorionic gonadotropin was determined in serum from pregnant women, {{as well as from}} women with abortus inaminens, suspected ectopic pregnancies or with molar pregnancies. The new human chorionic gonadotropin assay was also evaluated in combination with an <b>automatic</b> sample <b>processor</b> for distributing samples to the antibody-coated wells of the microtitre plates. The analytical precision, specificity and accuracy of the human chorionic gonadotropin assay were assessed with 152 sera, using the 60 min^incubation as well as the shorter 15 min version. Specificity was comparable with the conventional system, whereas the Chemiluminescence assay performed better with respect to the assay detection limit and measuring range. The enhanced Chemiluminescence system for the determination of human chorionic gonadotropin is an efficient assay which agrees well with our routine assay. In connection with an <b>automatic</b> sample <b>processor</b> it enables an advanced and versatile system for the determination of human chorionic gonadotropin in labo-ratories with large series. The system is rapid, easy to handle and apparently free from interference...|$|R
40|$|It is {{essential}} to have high geometric accuracy of airborne hyperspectral image data for multitemporal studies (e. g. change detection), the generation of mosaics and the comparison and integration with other georeferenced data. Direct georeferencing of airborne line scanner data (e. g. HyMap data) requires the combination of kinematic GPS-positioning and an inertial measurement unit (IMU). With these orientation observations the exterior orientation of the sensor is available simultaneous with the data acquisition. For accurate determination of the sensor’s exterior orientation, the so-called boresight misalignment angles have to be known. These describe the angular discrepancies between the sensor and the IMU coordinate system and can be determined using ground control points (GCPs). If the misalignment proves to be stable for each mounting of the sensor, {{it is possible to}} rectify the data without using extra GCPs for every single image strip. This would speed up and simplify the process of georeferencing. For this study, the misalignment angles were determined with the ortho-rectification software ORTHO developed by DLR, which {{is an essential part of}} the <b>automatic</b> <b>processor</b> of the forthcomin...|$|E
40|$|Graduation date: 1969 The {{objective}} {{of this study was}} to determine the relative speeds of available medical x-ray films exposed directly to x-rays and exposed in combination with various types of intensifying screens. The information obtained from this sensitometric data was used to obtain factors by which a radiographic exposure technique with one film-screen combination could be modified to obtain an adequate radiograph with any of the other films or film-screen combinations. All of the films were exposed using a conventional diagnostic x-ray machine and processed in an <b>automatic</b> <b>processor.</b> The sensitometric data were tested by producing radiographs with the various films and film-screen combinations using the basic exposure technique and the exposure factors obtained from the relative speeds. The radiographic subject was a phantom made of tissue equivalent plastic which encased human skeletal parts. The radiographic results of the testing indicated that adequate radiographs can be obtained using this data. Assuming that the basic radiograph has an average density of 1. 0, a second radiograph using a different film-screen combination will have an average density which will not deviate from 1. 0 by more than ± 0. 15...|$|E
40|$|Configurable {{processors}} enable dramatic {{gains in}} energy efficiency, relative to traditional fixed instruction-set processors. This energy advantage comes from three improvements. First, {{configuration of the}} instruction set permits a much closer fit of the processor to the target applications, {{reducing the number of}} execution cycles required. Second, configuring the processor removes unneeded features, reducing power and area overhead. Third, <b>automatic</b> <b>processor</b> generation tools enable logic optimization, signal switching reductions, and seamless mapping into low-voltage circuits and processes, for very low-power operation. The first improvement has been well-studied. Analysis of the second and third improvements requires detailed circuit and layout experiments, which is the primary focus of this paper. Starting from a range of existing available power saving options, this work explores the tradeoff and analyzes the results: the design priority tradeoff, the process technology impact, and implementing low-power configurable processor using commercial scaled-VDD cell libraries compatible with mainstream SOC practices. These real processor designs can achieve power dissipation approaching 20 µW/MHz at 0. 8 V and close to 10 µW/MHz at 0. 6 V, using production 0. 13 um libraries. Finally, this work quantifies the dramatic process, voltage and temperature dependence in post-layout leakage power for small processor designs...|$|E
30|$|Portions of {{the liver}} and kidney were fixed in 10 % neutral {{buffered}} formalin for histology. Thin sections {{of the liver}} were dissected and processed using Leica TP 2010 <b>automatic</b> tissue <b>processor</b> for 18  h. The processor passed the tissues through fixation, dehydration, dealcoholisation and paraffination. Ultra-thin sections of 5  μm were sliced from the paraffinated sections using a Thermo scientific semi-automated rotary microtome. The tissues were then subjected to hematoxylin and eosin staining and viewed under a microscope using 10 X and 40 X magnification.|$|R
5000|$|One {{columnist}} for Atlantic Monthly took {{the occasion to}} bemoan the effect of industrialized chicken production {{on the quality of}} the chicken that the United States was exporting, calling it a [...] "battery-bred, chemically fed, sanitized, porcelain-finished, money-back-if-you-can-taste-it bird."A cartoon accompanying the column portrays chicken being fed into a machine—the [...] "Instofreezo <b>Automatic</b> Food <b>Processor,</b> Packager & Deflavorizer, A Product of the U.S.A." [...] A production executive stands atop the machine as it pumps out cubes of generic chicken food product, which threaten to engulf the globe.|$|R
40|$|Four {{independent}} {{investigations are}} reported; in general these {{are concerned with}} improving and utilizing {{the correlation between the}} physical properties of natural materials as evidenced in laboratory spectra and spectral data collected by multispectral scanners. In one investigation, two theoretical models were devised that permit the calculation of spectral emittance spectra for rock and mineral surfaces of various particle sizes. The simpler of the two models can be used to qualitatively predict the effect of texture on the spectral emittance of rocks and minerals; it is also potentially useful as an aid in predicting the identification of natural atmospheric aerosol constituents. The second investigation determined, via an infrared ratio imaging technique, the best pair of infrared filters for silicate rock-type discrimination. In a third investigation, laboratory spectra of natural materials were compressed into 11 -digit ratio codes for use in feature selection, in searches for false alarm candidates, and eventually for use as training sets in completely <b>automatic</b> data <b>processors.</b> In the fourth investigation, general outlines of a ratio preprocessor and an <b>automatic</b> recognition map <b>processor</b> are developed for on-board data processing in the space shuttle era...|$|R
40|$|The {{airborne}} {{imaging spectrometer}} ARES (Airborne Reflective Emissive Spectrometer) {{is a new}} scanner available for the user community in 2006. The sensor will provide 160 channels in the solar reflective region (0. 47  2. 42 µm) and in the thermal region (8. 1  12. 1 µm) within the thematic focus of agriculture, vegetation, geology and soil science. An automatic processing system embedded in the Data Information and Management System DIMS of DLR is established, which includes system corrections, radiometric calibration, atmospheric correction and ortho image production. For the ortho image production a kinematic GPS positioning and inertial measurement unit IMU {{as well as a}} worldwide digital elevation database is used. Furthermore the stereo sensor ADS 40, installed in the aircraft together with the imaging sensor, is foreseen to produce high accuracy digital elevation models in combination with the DEM database. The boresight misalignment angles are calibrated using a ground control point field located at the DLR airbase. The paper describes the automatic processing chain for the different product levels, especially the direct georeferencing of the airborne data. Results and experiences of the <b>automatic</b> <b>processor</b> for the HyMap line scanner are shown. The long term stability of the boresight misalignment angles are investigated, which is essential for an automatic processing...|$|E
30|$|This {{study was}} {{conducted}} with the approval of the Ethical Committee of Suleyman Demirel University Faculty of Medicine, Isparta. The patient consent was obtained for using their teeth for research purposes. Freshly extracted human permanent molar teeth stored up to 3  months in the 0.2  % sodium azide solution (Merck, Darmstadt, Germany) at room temperature were used in this study. All teeth were hand-scaled to remove tissue remnants and debris, cleaned with a pumice slurry using a rubber cup, and air-dried. After visual examination and careful probing, sixty-nine extracted teeth with carious occlusal surfaces of different severity were radiographed. All images were acquired using bitewing projection geometry. The radiographs (E-speed, Ceadent, Strängnäs, Swiss) were taken for each tooth using a dental X-ray unit (Trophy Radiology, Marne La Vallee, France) operating at 65  kVp, 10  mA; an exposure time of 0.25  s, a film holder (Endo-Bite posterior, KerrHawe, Bioggio, Switzerland), and a focus-object distance of 36  cm was used. The films were processed in an <b>automatic</b> <b>processor</b> (Periomat Plus, Dürr Dental, Bietigheim-Bissingen, Germany). The radiographs were examined under standardized conditions by one examiner and occlusal caries lesions were diagnosed according to a five-point scale proposed by Espelid et al. (1994). The teeth were categorized as ‘Grade 4 ’ in which carious lesions in the middle third of the dentin and included in this study.|$|E
40|$|Many {{available}} systems-on-chip embedded processors can be specialised {{for a given}} application-domain {{by adding}} ad-hoc functional units. These functional units {{can be used to}} map clusters of elementary arithmetic or logic operations (sections of the dataflow graph); more complex hardware addons could attack the control flow (e. g., map loops onto sequential functional units). For <b>automatic</b> <b>processor</b> specialisation, the two strategies imply di#erent methods and have a di#erent complexity. Before incurring in the complexities of control flow, this paper attacks the question that appears so far unanswered: how much can one improve the performance of an embedded processor on specific algorithms by automatically mapping only dataflow sections of code on special functional units? The basic scope for performance improvement is assessed and broken out in di#erent sources: hardware parallelism; avoided quantisation of instructions in an integral number of cycles; and simplification of the logic due to constants. The scope for speedup is increased through additional manual optimisations (amenable to automation) : bit-width analysis and arithmetic optimisation. Finally, ILP techniques such as loop unrolling and predication are used to increase the size of the basic blocks and give more scope for the sources of improvement. The results show that significant improvements in speed (up to 6 times) can be targeted without necessarily mapping the control flow onto hardware...|$|E
40|$|A {{flexible}} {{digital architecture}} for a pulsed ultrawideband demodulator sampling below Nyquist rate is presented. The system {{is based on}} a complex Singular Value Decomposition implemented on a configurable systolic array of simple <b>processors.</b> <b>Automatic</b> code generation is applied to cut design time and rapidly assess the implementation cost of several architectures of the processors. status: publishe...|$|R
40|$|Two {{requirements}} for the registration of Earth Resources Technology Satellite (ERTS) data are discussed. These requirements are registration of ERTS data acquired on separate passes and registration of ERTS data to a ground reference. Performances of a semi-automatic warp algorithm and an automatic pipeline processing algorithm demonstrate that either procedure is useful, depending upon the user's requirements. In two cases where the time lapse between passes of the satellite were 90 days and 18 days the <b>automatic</b> pipeline <b>processor</b> reduced the mean radial registration error to 0. 28 and 0. 58 pixel, respectively. It is concluded that this technique is promissing for high-volume production processing...|$|R
40|$|Major {{public events}} such as open-air festivals are {{characterized}} by large concentrations of people in relatively small areas. These events represent challenging situations for public authorities concerned with organization, traffic and security and for festival organizers. In the research project VABENE++* an end-to-end monitoring system has been developed {{in order to provide}} situational and traffic related information in near real-time based on terrestrial and airborne platforms. In the present study, the potential of object-based image analysis (OBIA) for the extraction and classification of small scale features in the context of large scale public events is examined. The features to be identified involve different types of vehicles and tents. A first step involves a quantitative evaluation of segmentation quality and the investigation of the relationships between segmentation and machine learning algorithms. Methods are evaluated considering computational cost, thematic accuracy, robustness/transferability and the amount of user interaction. Finally, a methodological framework is developed as a basis for the development of (semi-) <b>automatic</b> <b>processors</b> to be implemented in the existing airborne monitoring system of VABENE++. Even though methods are predominately tested and compared using very high resolution aerial imagery, it is also assessed if and to what extent lower resolution imagery from optical satellites (e. g., WorldView- 2) represent appropriate alternatives for the same objectives. Possible application scenarios in the context of large scale events may involve the monitoring of the occupancy of parking and assembly areas, camping sites and the estimation of the number of overnight guests and the assessment of the accessability of emergency and escape routes...|$|R
40|$|International audienceIt is {{very common}} in todays’ social {{networks}} that several discussion threads around similar topics are opened {{at the same time}} in different distinct or overlapping communities. Being aware about these different threads may be difficult. Moreover, when new threads are created, it may be useful to provide the user with linked past tweets instead of generating new threads. Information linkage is the pro- cess by which different pieces of information are put together ac- cording to criteria and constraints to form a new information which is richer (i. e. increased) and which can be consumed by an user or automatically by another process. This linkage can: (i) ease the digestion of information, i. e. its perception by users, (ii) enable a better information management from the system perspective, and (iii) allow other third-party applications to draw more benefits from a social content which, in a disparate form, is useless. The problem we are tackling can be formulated as follows: Having a broad set of interactions between users of a social network with disparate messages and connections, how to link these interactions so that they are correlated consistently and significantly for either an end user or an <b>automatic</b> <b>processor</b> to navigate easier in this large content. We propose in this paper to handle the problem of overload in social interactions by grouping messages according to three important dimensions: (i) content (textual and hashtags), (ii) users, and (iii) time difference. This process will also allow to perform well other tasks, such as query recommendation, text understanding (i. e. summarization), and event detection. We evaluated our approach on a Twitter data set and we compared it to other existing approaches and the results are promising and encouraging...|$|E
40|$|One of {{the main}} {{processing}} steps of evaluating remote sensing data is the production of ortho images from the acquired raw scanner data. Since in most applications of thematic analysis, a rectified data set is required, {{there is a need}} for an effective – regarding time and accuracy- and generic – regarding different sensor systems – processor for performing this rectification for any desired sensor imagery. This is especially true when using the image data in Geographic Information Systems (GIS) and for data fusion and analysis with data from different sources or seasons. The accuracy of this rectification result is crucial for overlaying the data with existing data sets or maps and using them for evaluations like change detection, map updating a. o [...] Triggered by the demand of an <b>automatic</b> <b>processor</b> embedded in the Data Information and Management System DIMS of DLR a generic ortho image tool was developed. The generic ortho image processor supports the production of ortho images from airborne and spaceborne digital line scanner images, as well as images from frame cameras. It is based on the Direct Georeferencing model using measurements of the exterior orientation of the sensor platform or sensor itself, the interior orientation (sensor parameters) and a digital elevation model. For the interior orientation models for pushbroom, whiskbroom and frame cameras as well as sensor calibration tables are supported. For the exterior orientation local level co-ordinate frames (navigation frame, orbital frame), earth centred earth fixed (ECEF) co-ordinate frames and generic mapping frames are supported. An approximate processing in a map projection is provided for airborne scanner data. The boresight misalignment matrix and the lever arm values are part of the functional model. Map projections are included i...|$|E
40|$|High {{geometric}} {{accuracy of}} airborne hyperspectral image data {{is essential for}} multi-temporal studies, the generation of mosaics and the comparison and combination with other georeferenced data. Direct georeferencing of airborne line scanner data (e. g. HyMap data) implies the combination of kinematic GPS-positioning and an inertial measurement unit (IMU). With these orientation observations the exterior orientation of the sensor is available for each moment of the data acquisition. For the exact determination of the sensor’s exterior orientation, the so-called boresight misalignment angles have to be known, which describe the angular discrepancies between the sensor and the IMU coordinate system. They can be determined using ground control points (GCPs). If the misalignment proves to be stable for each mounting of the sensor, {{it is possible to}} rectify the data without the usage of extra GCPs for every image strip. This would reduce the expenditure of time and simplify the process of georefencing. The misalignment angles are considered within the ortho-rectification software ORTHO developed by DLR, which {{is an essential part of}} the <b>automatic</b> <b>processor</b> of the future ARES (Airborne Reflective Emmisive Spectrometer) sensor. Within this study, the long term stability of boresight misalignment angles has been investigated during the HyMap campaigns HyEurope 2003 and HyEurope 2004, where the mounting of the HyMap sensor remained unchanged for over 2 months. Several data takes over a test field around Oberpfaffenhofen with approximately 30 GCPs have been carried out to calculate the misalignment angles. The results were compared to each other and applied to different sets of HyMap data to investigate the stability and the specification of the accuracy of the boresight misalignment angles to fulfill the requested sub-pixel accuracy of the rectified image. Furthermore the study allowed an evaluation of the quality of the GCPs within the test field. The result was a significant improvement of the existing GCPs in addition with the definition and surveying of new GCPs...|$|E
30|$|At {{the end of}} {{the feeding}} trial, ten shrimp per {{treatment}} group were euthanized by immersion in cold water. Shrimp heads were collected, cut in half lengthwise, and then immediately preserved in Davidson’s fixative (Lightner et al. 2009). After fixation, samples were embedded in cassette molds and were processed overnight by <b>automatic</b> tissue <b>processors</b> (Leica, Nussloch, Germany). The samples were then embedded into paraffin blocks and cut to 3 -µm thickness with a sliding microtome (R. Jung AG, Heidelberg, Germany). The sections were stained with haematoxylin and eosin (H&E) (Bancroft 1967; Humason 1979). The stained sections were examined under a light microscope (Olympus CH 30, Olympus Corporation, Tokyo, Japan), and then a selected area in the stained section was imaged with an Olympus DP 71 digital camera and software (Olympus Corporation, Tokyo, Japan).|$|R
30|$|Samples (liver and ovary) for histomorphological {{observation}} {{were taken}} from the same fishes which were used for GSI estimation (as described above). Then the liver and ovary were preserved in 10  % neutral buffered formalin and Bouin’s fluid, respectively. Dehydration, cleaning and infiltration process were carried out in an <b>automatic</b> tissue <b>processor</b> (SHANDON, CITADEL 1000, England). Paraffin-embedded blocks were sliced by microtome machine (Leica Jung, RM 2035, Germany) at 5  µm size. The ribbons with samples were placed into a water bath (40  °C) and the best section was picked up by glass slides and placed on a slide drier (20  °C) for overnight. Then the slides were stained with haematoxylin and eosin dye. The stained sections were mounted with Canada balsam and photographed under a compound microscope (OPTIKA B- 350, Italy).|$|R
40|$|Summary. Streaming {{processing}} of XML transformations is practically needed es-pecially {{if we have}} large XML documents or XML data streams as the transformation input. In this paper, we present the design of an <b>automatic</b> streaming <b>processor</b> of transformations specified in XSLT language. Unlike other similar systems, our pro-cessor guarantees bounds on the resource usage for the {{processing of}} {{a particular type of}} transformation. This feature is achieved by employing tree transducers as the underlying formal base. The processor includes a set of streaming algorithms, each of them is associated with a tree transducer with specific resource usage (memory, number of passes), and thus captures different transformation subclass. The input XSLT stylesheet is analyzed in order to identify the transformation subclass to which it belongs. Then the lowest resource-consuming streaming algorithm capturing this subclass is applied. ...|$|R
