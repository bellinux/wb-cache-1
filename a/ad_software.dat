27|104|Public
5000|$|Some {{senior staff}} members of NebuAd had worked {{previously}} at a (now defunct) ad company, named Claria Corporation (formerly, the Gator Corporation), which {{was well known}} for <b>ad</b> <b>software</b> known as Gator. [...] Both Claria and NebuAd were located in Redwood City, California. The June 2006 creation of nebuad.com coincides with timing of Claria's decision {{to shut down the}} Gator service. NebuAd repeatedly denied any corporate connection to Claria, describing its hiring of Claria employees {{as a result of that}} company shedding employees in a tight market for experienced advertising sales staff in the Valley.|$|E
40|$|Many {{computational}} tasks {{require the}} determination of the Jacobian matrix, at a given argument, for a large nonlinear system of equations. Calculation or approximation of a Newton step is a related task. The development of robust automatic differentiation (<b>AD)</b> <b>software</b> allows for "painless" and accurate calculation of these quantities; however, straight forward application of <b>AD</b> <b>software</b> on large-scale problems can require an inordinate amount of computation. Fortunately, large-scale systems of nonlinear equations typically exhibit either sparsity or structure in their Jacobian matrices. In this paper we proffer general approaches for exploiting sparsity and structure to yield efficient ways to determine Jacobian matrices (and Newton steps) via automatic differentiation...|$|E
40|$|We {{demonstrate}} how the structure that arises in inverse and optimal design {{problems can be}} used to aid in the efficient application of automatic differentiation ideas. We discuss the program structure of generic inverse problems and then illustrate, with two examples (one example involves the heat equation, the other involves wave propagation) how structure can be used in combination with automatic differentiation. Finally, we report numerical results and describe the ADMIT- 2 software package which enables efficient derivative computation of structured problems. 1 Introduction Effective use of automatic differentiation (<b>AD)</b> <b>software</b> for realistic large-scale problems is often not "automatic". Indeed, performance gains of several orders of magnitude can sometimes be achieved by using AD in a selective manner (as opposed to straightforward use of <b>AD</b> <b>software).</b> In particular, large-scale problems typically exhibit structure: for AD to be used efficiently (or even feasibly) it is crucial [...] ...|$|E
5000|$|Currently, {{the website}} also blocks {{internet}} users using <b>ad</b> blocking <b>software</b> (such as Adblock Plus) from accessing articles, {{demanding that the}} website {{be put on the}} <b>ad</b> blocking <b>software's</b> whitelist before access is granted. [...] This is done because customers using <b>ad</b> blocking <b>software</b> do not contribute to the site's revenue. Malware attacks have been noted to occur from Forbes site.|$|R
50|$|Some {{websites}} {{have taken}} counter-measures against <b>ad</b> blocking <b>software,</b> such as attempting {{to detect the}} presence of ad blockers and informing users of their views, or outright preventing users from accessing the content unless they disable the <b>ad</b> blocking <b>software.</b> There have been several arguments supporting and opposing the assertion that blocking ads is wrong.|$|R
50|$|To create large (tens of {{thousands}} of people) crowds for the <b>ad</b> MASSIVE <b>software</b> with semi-independent AI actors from Weta Digital was used.|$|R
40|$|We give {{a gentle}} {{introduction}} to using various software tools for automatic differentiation (AD). Ready-to-use examples are discussed, {{and links to}} further information are presented. Our target audience includes all those {{who are looking for}} a straightforward way to get started using the available AD technology. The document is dynamic in the sense that its content will be updated as the <b>AD</b> <b>software</b> evolves. Comment: 23 page...|$|E
40|$|The authors give {{a gentle}} {{introduction}} to using various software tools for Automatic Differentiation (AD). Ready-to-use examples are discussed {{and links to}} further information are presented. The target audience includes all those {{who are looking for}} a straight-forward way to get started using the available AD technology. The document is supposed to be dynamic in the sense that its content will be kept up-to-date as the <b>AD</b> <b>software</b> covered is evolving...|$|E
40|$|AbstractAdjoint {{sensitivity}} computation of {{parameter estimation}} problems {{is a widely}} used technique {{in the field of}} computational science and engineering for retrieving derivatives of a cost functional with respect to parameters efficiently. Those derivatives can be used, e. g. for sensitivity analysis, optimization, or robustness analysis. Deriving and implementing adjoint code is an error-prone, non-trivial task which can be avoided by using Algorithmic Differentiation (<b>AD)</b> <b>software.</b> Generating adjoint code by <b>AD</b> <b>software</b> has the downside of usually requiring a huge amount of memory as well as a non-optimal run time. In this article, we couple two approaches for achieving both, a robust and efficient adjoint code: symbolically derived adjoint formulations and AD. Comparisons are carried out for a real-world case study originating from the remote atmospheric sensing simulation software JURASSIC developed at the Institute of Energy and Climate Research – Stratosphere, Research Center Jülich. We show, that the coupled approach outperforms the fully algorithmic approach by AD in terms of run time and memory requirement and argue that this can be achieved while still preserving the desireable feature of AD being automatic...|$|E
40|$|Version Date Description of changes(s) 0. 1 11 / 21 / 2007 Initial Draft Created with {{comments}} for further expansion. 0. 2 11 / 26 / 2007 Abstract added. Sent for review to the Instructor. 0. 3 11 / 27 / 2007 Added Literature and Theory of Upgradation parts. 0. 4 11 / 28 / 2007 <b>Added</b> <b>Software</b> Design and Implementation. 0. 5 11 / 29 / 2007 Added all other remaining parts. 0. 6 11 / 29 / 2007 Final review and applied changes...|$|R
50|$|Because of Cydoor's highly {{controversial}} practices of running <b>ads</b> in <b>software</b> programs, Cydoor software is often considered spyware — and many Anti-Spyware and Antivirus applications will flag the software as such.|$|R
40|$|We {{introduce}} a probabilistic framework for two-sample comparison {{based on a}} nonparametric process taking {{the form of a}} Markov model that transitions between a "divide" and a "merge" state on a multi-resolution partition tree of the sample space. Multi-scale two-sample comparison is achieved through inferring the underlying state of the process along the partition tree. The Markov design allows the process to incorporate spatial clustering of differential structures, which is commonly observed in two-sample problems but ignored by existing methods. Inference is carried out under the Bayesian paradigm through recursive propagation algorithms. We demonstrate the work of our method through simulated data and a real flow cytometry data set, and show that it substantially outperforms other state-of-the-art two-sample tests in several settings. Comment: Corrected typos. <b>Added</b> <b>Software</b> sectio...|$|R
40|$|This {{thesis is}} {{concerned}} with the efficient computation of Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of two directed edge separator methods, the weighted minimum separator and natural order separator methods, to exploit the structure of the computational graph of the nonlinear system. This allows for the efficient determination of the Jacobian matrix using <b>AD</b> <b>software.</b> We will illustrate the promise of this approach with computational experiments...|$|E
40|$|This {{paper is}} {{concerned}} with the efficient computation of sparse Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of a graph coloring technique, bi-coloring, to exploit the sparsity of the Jacobian matrix $J$ and thereby allow for the efficient determination of $J$ using <b>AD</b> <b>software.</b> We analyze both a direct scheme and a substitution process. We discuss the results of numerical experiments indicating significant practical potential of this approach...|$|E
40|$|A {{gradient-based}} optimisation of a parametric CAD model {{requires the}} calculation of shape sensitivities, i. e., the derivatives of surface points {{with respect to the}} design parameters. Typically, this information is not available within a CAD system and can be obtained by applying Automatic Differentiation (AD) to the CAD sources. This paper demonstrates the differentiated open-source CAD kernel OpenCascade Technology (OCCT) using the <b>AD</b> <b>software</b> tool ADOL-C (Automatic Differentiation by OverLoading in C++) for the optimisation of pressure loss in a U-bend pipe. The U-bend geometry is parametrised in OCCT and its derivatives are used in CFD optimisation loops...|$|E
2500|$|A {{few days}} after the release of iOS 9, <b>ad</b> {{blocking}} <b>software</b> had topped the App Store charts, with Marco Arment, developer of a Peace app, saying that [...] "web advertising and behavioral tracking is out of control. ... They're unacceptably creepy, bloated, annoying, and insecure, and they're getting worse at an alarming pace." ...|$|R
30|$|System and Application Vulnerabilities (SAV) are exploitable bugs {{arising from}} <b>software</b> <b>ad</b> {{configuration}} errors that an attacker {{can use to}} infiltrate and compromise a system.|$|R
50|$|To users, the {{benefits}} of <b>ad</b> blocking <b>software</b> include quicker loading and cleaner looking web pages with fewer distractions, lower resource waste (bandwidth, CPU, memory, etc.), and privacy benefits gained through {{the exclusion of the}} tracking and profiling systems of ad delivery platforms. Blocking ads can also save substantial amounts of electrical energy and lower users' power bills.|$|R
40|$|Given a {{numerical}} model for solving two-dimensional Shallow Water Equations, {{we are interested}} in the robustness of the simulation by identifying the rate of change of the water depths and discharges with respect to a change in the bottom friction coefficients. Such a sensitivity analysis can be carried out by computing the corresponding derivatives. Automatic Differentiation (AD) is an effcient numerical method, free of approximation errors, to evaluate derivatives of the objective function speciﬁed by the computer program, Rubar 20 for example. In this paper <b>AD</b> <b>software</b> tool Tapenade is used to compute forward derivatives. Numerical tests were done to show the robustness of the model and to demonstrate the effciency of these AD-derivatives...|$|E
40|$|AbstractAfter the Three Gorges Reservoir starts running, it can {{not only}} take into {{consideration}} the interest of departments such as flood control, power generation, water supply, and shipping, but also reduce or eliminate the adverse effects of pollutants by discharge regulation. The evolution of pollutant plumes under different operation schemes of the Three Gorges Reservoir and three kinds of pollutant discharge types were calculated with the MIKE 21 <b>AD</b> <b>software.</b> The feasibility and effectiveness of the reservoir emergency operation when pollution accidents occur were investigated. The results indicate that the emergency operation produces significant effects on the instantaneous discharge type with lesser effects on the constant discharge type, the impact time is shortened, and the concentration of pollutant is reduced. Meanwhile, the results show that the larger the discharge is and the shorter the operation duration is, the more favorable the result is...|$|E
40|$|Frequently of use in {{optimization}} problems, automatic differentiation {{may be used}} {{to generate}} Taylor coefficients. Specialized software tools generate Taylor series approximations, one term at a time, more efficiently than the general <b>AD</b> <b>software</b> used to compute (partial) derivatives. Through the use of operator overloading, these tools provide a relatively easy-to-use interface that minimizes the complications of working with both point and interval operations. Introduction. First, we briefly survey the tools of automatic differentiation and operator overloading used to compute point- and interval-valued Taylor coefficients. We assume that f is an analytic function f : R! R. Automatic differentiation (AD or computational differentiation) is the process of computing the derivatives of a function f at a point t = t 0 by applying rules of calculus for differentiation [10, 11, 18, 19]. One way to implement AD uses overloaded operators. Operator Overloading. An overloaded (or gener [...] ...|$|E
50|$|The company {{works with}} a “freemium” model, {{providing}} free software with advertisements, and a paid version without <b>ads.</b> The <b>software</b> is available for desktops, smart phones and tablet computers using Microsoft Windows, Mac OS X, Android and iOS operating systems. The software has been downloaded 120 million times and has 20 million active monthly users in 190 countries.|$|R
50|$|Publishers {{and their}} {{representative}} trade bodies, {{on the other}} hand, argue that Internet ads provide revenue to website owners, which enable the website owners to create or otherwise purchase content for the website. Publishers claim that the prevalent use of <b>ad</b> blocking <b>software</b> and devices could adversely affect website owner revenue and thus in turn lower the availability of free content on websites.|$|R
50|$|Rosemary Watson is an American {{voice over}} artist, actress, and singer-songwriter. Her voiceover {{work can be}} heard {{around the world in}} {{television}} and radio <b>ads,</b> in <b>software</b> training programs and educational videos, toys, gadgets, audio books and phone systems. Her satirical impersonations of Hillary Clinton, Sarah Palin, Michele Bachmann, Cindy McCain, James Carville, Diane Sawyer and others can be seen on her YouTube channel.|$|R
40|$|I hereby {{declare that}} I am the sole {{author of this}} thesis. This is a true copy of the thesis, {{including}} any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii This thesis {{is concerned with the}} efficient computation of Jacobian matrices of nonlin-ear vector maps using automatic differentiation (AD). Specificially, we propose the use of two directed edge separator methods, the weighted minimum separator and natural order separator methods, to exploit the structure of the computional graph of the nonlinear sys-tem. This allows for the efficient determination of the Jacobian matrix using <b>AD</b> <b>software.</b> We will illustrate the promise of this approach with computational experiments. iii Acknowledgements I am heartily thankful to my supervisor, Thomas F. Coleman, whose encouragement, guidance and support from the initial to the final level enabled me to develop an under...|$|E
40|$|This {{paper is}} {{concerned}} with the efficient computation of sparse Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of a graph coloring technique, bi-coloring, to exploit the sparsity of the Jacobian matrix J and thereby allow for the efficient determination of J using <b>AD</b> <b>software.</b> We analyze both a direct scheme and a substitution process. We discuss the results of numerical experiments indicating significant practical potential of this approach. Key words. sparse Jacobian matrices, nonlinear systems of equations, nonlinear least squares, graph coloring, bi-coloring, automatic differentiation, computational differentiation, sparse finite differencing, partition problem, NP-complete problems, ADOL-C AMS(MOS) subject classifications. 65 K 05, 65 K 10, 65 H 10, 90 C 30, 90 C 05, 68 L 10 1 Introduction The efficient numerical solution of nonlinear systems of algebraic equations, F (x) : ! n ! ! m, usually requires the repeated calc [...] ...|$|E
40|$|Full text of {{this paper}} is not {{available}} in the UHRAThis paper gives an introduction to a number of methods collectively known as Automatic Differentiation (AD). AD is the systematic application of the rules of calculus to computer programs yielding, as output, programs which accurately compute numerical values of first, second, or higher derivatives. AD is not symbolic differentiation nor a divided difference method. It is a process for evaluating derivatives which depends only on the algorithmic specification of the entire program or part of the program. <b>AD</b> <b>software</b> tools are now available for most languages employed in scientific computing including Fortran, Fortran 90, HPF, C++, etc. The tools have reached a sufficient maturity that they should be confidently used in applications which include processes which rely upon the generation of derivative information or error analysis. This paper will describe the basic ideas of AD and show how it may be extended for use in a parallel computing environment...|$|E
50|$|Cydoor Desktop Media is an Israeli adware company. Cydoor {{originally}} placed <b>ads</b> only in <b>software</b> {{programs such}} as Kazaa and iMesh, but has now expanded into running ads on websites as an advertising network.|$|R
50|$|The {{project is}} backed by {{donations}} and a single and optional <b>ad</b> for related <b>software</b> that is shown during installation. All donors get access to an ad-free installer including a few additional perks like an auto-updater.|$|R
50|$|Many {{find this}} form of {{advertising}} to be concerning and see these tactics as manipulative {{and a sense of}} discrimination (Toubiana et al., 2010). As a result of this, a number of methods have been introduced in order to avoid advertising (Johnson, 2013). Internet users employing ad blockers are rapidly growing in numbers. A study conducted by PageFair found that from 2013 to 2014, there was a 41% increase of people using <b>ad</b> blocking <b>software</b> globally (PageFair, 2015).|$|R
40|$|International audienceWe apply an {{optimized}} {{method to}} the adjoint generation of a time-evolving land ice model through algorithmic differentiation (AD). The optimization involves a special {{treatment of the}} fixed-point iteration required to solve the nonlinear stress balance, which differs from a straightforward application of <b>AD</b> <b>software,</b> and leads to smaller memory requirements {{and in some cases}} shorter computation times of the adjoint. The optimization is done via implementation of the algorithm of 5 Christianson [1994] for reverse accumulation of fixed-point problems, with the AD tool OpenAD. For test problems, the optimized adjoint is shown to have far lower memory requirements, potentially enabling larger problem sizes on memory-limited machines. In the case of the land ice model, implementation of the algorithm allows further optimization by having the adjoint model solve a sequence of linear systems with identical (as opposed to varying) matrices, greatly improving per- 10 formance. The methods introduced here will be of value to other efforts applying AD tools to ice models, particularly ones which solve a " hybrid " shallow ice / shallow shelf approximation to the Stokes equations...|$|E
40|$|We apply an {{optimized}} {{method to}} the adjoint generation of a time-evolving land ice model through algorithmic differentiation (AD). The optimization involves a special {{treatment of the}} fixed-point iteration required to solve the nonlinear stress balance, which differs from a straightforward application of <b>AD</b> <b>software,</b> and leads to smaller memory requirements {{and in some cases}} shorter computation times of the adjoint. The optimization is done via implementation of the algorithm of Christianson (1994) for reverse accumulation of fixed-point problems, with the AD tool OpenAD. For test problems, the optimized adjoint is shown to have far lower memory requirements, potentially enabling larger problem sizes on memory-limited machines. In the case of the land ice model, implementation of the algorithm allows further optimization by having the adjoint model solve a sequence of linear systems with identical (as opposed to varying) matrices, greatly improving performance. The methods introduced here will be of value to other efforts applying AD tools to ice models, particularly ones which solve a hybrid shallow ice/shallow shelf approximation to the Stokes equations...|$|E
30|$|As {{we saw in}} the worked examples, TDS {{technique}} is both accurate and powerful. Especially since parameters can be easily verified, is easy to use and is the best option for short test. Since most engineers prefer automatic pressure matching, we encourage doing so, once TDS technique has been used to find the range of the parameters used for the simulation. Actually, we assume that all the most popular computer software for transient-pressure analysis uses TDS technique. To avoid commercialism, they will be referred <b>ad</b> <b>software</b> 1, 2, 3 and 4. The first one is sold by a company and the standalone version has an option called “specialized lines” which can be displayed once the pressure derivative plot is built. See Fig.  9. The user can plot and move the different straight lines by choosing from the given menu. The same company—workstation version—has in the main menu a button “Tools” that displays the window given in Fig.  10. The user can select type of wellbore storage, well model, reservoir model and boundary model. Any of them will give a straight line on the pressure derivative plot and the results are given instantaneously.|$|E
2500|$|The {{cause of}} Facebook's low CTR has been {{attributed}} to younger users enabling <b>ad</b> blocking <b>software</b> and their adeptness at ignoring advertising messages, as well as the site's primary purpose being social communication rather than content viewing. According to digital consultancy iStrategy Labs in mid-January 2014, three million fewer users aged between 13 and 17 years were present on Facebook's Social Advertising platform compared to 2011. However, Time writer and reporter Christopher Matthews stated {{in the wake of the}} iStrategy Labs results: ...|$|R
50|$|Hoffman {{was born}} in Charlotte, North Carolina {{and grew up in}} Matthews, North Carolina, He {{attended}} high school at Charlotte Latin School. At the University of North Carolina at Chapel Hill, Hoffman was a manager of the men's basketball team. He also co-founded the UNC Darkside ultimate team. While in college he co-founded PrivNet, an internet privacy software company that pioneered <b>ad</b> blocking <b>software,</b> web cookie management, and search integrated into web browsers. PGP, Inc. acquired PrivNet in November 1996.|$|R
5000|$|The {{cause of}} Facebook's low CTR has been {{attributed}} to younger users enabling <b>ad</b> blocking <b>software</b> and their adeptness at ignoring advertising messages, as well as the site's primary purpose being social communication rather than content viewing. According to digital consultancy iStrategy Labs in mid-January 2014, three million fewer users aged between 13 and 17 years were present on Facebook's Social Advertising platform compared to 2011. However, Time writer and reporter Christopher Matthews stated {{in the wake of the}} iStrategy Labs results: ...|$|R
