7|970|Public
40|$|Web {{search engines}} gather {{information}} from the queries performed by the user {{in the form of}} query logs. These logs are extremely useful for research, marketing, or profiling, {{but at the same time}} they are a great threat to the user’s privacy. We provide a novel approach to anonymize query logs so they ensure user k-anonymity, by extending a common method used in statistical disclosure control: microaggregation. Furthermore, our microaggregation approach takes into account the semantics of the queries by relying on the Open Directory Project. We have tested our proposal with real data from <b>AOL</b> <b>query</b> <b>logs...</b>|$|E
40|$|Temporal text data {{is often}} {{generated}} by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative and conditional modeling of categorical temporal data such as time-stamped document sequences. The resulting model corresponds to a local n-gram model in the generative case and local logistic regression in the conditional case. We examine the asymptotic bias and variance of the local estimator and their implications to the optimal kernel bandwidth. We discuss various regularization schemes and demonstrate the proposed estimators using an experimental study on the Reuters RCV 1 news data and <b>AOL</b> <b>query</b> <b>logs.</b> ...|$|E
40|$|The recent {{release of}} the American Online (<b>AOL)</b> <b>Query</b> <b>Logs</b> {{highlighted}} the remarkable amount of private and identifying information that users are willing to reveal to a search engine. The {{release of the}}se types of log files therefore represents a significant liability and compromise of user privacy. However, without such data the academic community greatly suffers in their ability to conduct research on real search engines. This paper proposes two specific solutions (rather than an overly general framework) that attempts to balance the needs of certain types of research while individual privacy. The first solution, based on a threshold cryptography system, eliminates highly identifying queries, in real time, without preserving history or statistics about previous behavior. The second solution attempts to deal with sets of queries, that when taken in aggregate, are overly identifying. Both are novel and represent additional options for data anonymization...|$|E
40|$|Web <b>query</b> <b>log</b> data contain {{information}} {{useful to}} research; however, release of such data can re-identify the search engine users issuing the queries. These privacy concerns {{go far beyond}} removing explicitly identifying information such as name and address, since non-identifying personal data can be combined with publicly available information to pinpoint to an individual. In this work we model web <b>query</b> <b>logs</b> as unstructured transaction data and present a novel transaction anonymization technique based on clustering and generalization techniques to achieve the k-anonymity privacy. We conduct extensive experiments on the <b>AOL</b> <b>query</b> <b>log</b> data. Our results show that this method results in a higher data utility compared to the state of-the-art transaction anonymization methods. Comment: 9 page...|$|R
40|$|<b>Query</b> <b>logs</b> data, privacy-preserving data publishing, {{transaction}} data anonymization, item generalization Web <b>query</b> <b>log</b> data contain information useful to research; however, release of such data can re-identify the search engine users issuing the queries. These privacy concerns {{go far beyond}} removing explicitly identifying information such as name and address, since non-identifying personal data can be combined with publicly available information to pinpoint to an individual. In this work we model web <b>query</b> <b>logs</b> as unstructured {{transaction data}} and present a novel transaction anonymization technique based on clustering and generalization techniques to achieve the k-anonymity privacy. We conduct extensive experiments on the <b>AOL</b> <b>query</b> <b>log</b> data. Our results show that this method results in a higher data utility compared to the stateof-the-art transaction anonymization methods. ...|$|R
40|$|Abstract—Web <b>query</b> <b>logs</b> {{provide a}} rich wealth of information, but also present serious privacy risks. We {{consider}} publishing vocabularies, bags of query-terms extracted from web <b>query</b> <b>logs,</b> {{which has a}} variety of applications. We aim at preventing identity disclosure of such bag-valued data. The key feature of such data is the extreme sparsity, which renders conventional anonymization techniques not working well in retaining enough utility. We propose a semantic similarity based clustering approach to address the issue. We measure the semantic similarity between two vocabularies by a weighted bipartite matching and present a greedy algorithm to cluster vocabularies by the semantic similarities. Extensive experiments on the <b>AOL</b> <b>query</b> <b>log</b> show that our approach retains more data utility than existing approaches. Keywords-Anonymity; privacy; bag-valued data; <b>query</b> <b>logs</b> I...|$|R
40|$|Set-valued data, {{in which}} a set of values are {{associated}} with an individual, is common in databases ranging from market basket data, to medical databases of patients ’ symptoms and behaviors, to query engine search logs. Anonymizing this data is important {{if we are to}} reconcile the conflicting demands arising from the desire to release the data for study and the desire to protect the privacy of individuals represented in the data. Unfortunately, the bulk of existing anonymization techniques, which were developed for scenarios in which each individual is associated with only one sensitive value, are not well-suited for set-valued data. In this paper we propose a top-down, partition-based approach to anonymizing set-valued data that scales linearly with the input size and scores well on an information-loss data quality metric. We further note that our technique can be applied to anonymize the infamous <b>AOL</b> <b>query</b> <b>logs,</b> and discuss the merits and challenges in anonymizing query logs using our approach...|$|E
40|$|Abstract: In {{distributed}} databases {{there is}} an increasing need for sharing data that contain personal information. The existing system presented collaborative data publishing problem for anonymizing horizontally partitioned data at multiple data providers. M-privacy guarantees that anonymized data satisfies a given privacy constraint against any group of up to m colluding data providers. The heuristic algorithms exploiting monotonicity of privacy constraints for efficient checking of m-privacy for given group of records. The data provider-aware anonymization algorithm with adaptive m-privacy checking strategies to ensure high utility. But A new type of “insider attack ” occurred by colluding data providers. The proposed system m-partition privacy scheme to anonymizing set-valued data. This scales linearly with input size. The Scores well on an information-loss data quality metric applied to anonymize <b>AOL</b> <b>query</b> <b>logs.</b> To Divide-and-conquer techniques used in addressing data with multiple dimensions. The M-partition privacy algorithm is in a top-down manner by recursively separating set-valued data into groups where data in each partition share a generalized representation. The analysis were conducted on different Environment to measure the performance in terms of Loss of information, computational time and no of data provider...|$|E
40|$|Users {{frequently}} {{modify a}} previous search query {{in hope of}} retrieving better results. These modifications are called query reformulations or query refinements. Existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. In this paper, we aim to better understand how web searchers refine queries and form a theoretical foundation for query reformulation. We study users ’ reformulation strategies {{in the context of}} the <b>AOL</b> <b>query</b> <b>logs.</b> We create a taxonomy of query refinement strategies and build a high precision rule-based classifier to detect each type of reformulation. Effectiveness of reformulations is measured using user click behavior. Most reformulation strategies result in some benefit to the user. Certain strategies like add/remove words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. In contrast, users often click the same result as their previous query or select no results when forming acronyms and reordering words. Perhaps the most surprising finding is that some reformulations are better suited to helping users when the current results are already fruitful, while other reformulations are more effective when the results are lacking. Our findings inform the design of applications that can assist searchers; examples are described in this paper...|$|E
40|$|Concepts are {{sequences}} {{of words that}} represent real or imaginary entities or ideas that users are interested in. As a first step towards building a web of concepts that will form {{the backbone of the}} next generation of search technology, we develop a novel technique to extract concepts from large datasets. We approach the problem of concept extraction from corpora as a market-basket problem, adapting statistical measures of support and confidence. We evaluate our concept extraction algorithm on datasets containing data from a large number of users (e. g., the <b>AOL</b> <b>query</b> <b>log</b> data set), and we show that a high-precision concept set can be extracted. 1...|$|R
40|$|User click-throughs {{provide a}} search context for {{understanding}} the user need of complex information. This paper re-examines {{the effectiveness of this}} approach when based on partial clicked data using the language modeling framework. We expand the original query by topical terms derived from clicked Web pages and enhance early precision via a more compact document representation. Since our URLs of Web pages are stripped, we first reconstruct them at different levels based on different collections. Our experimental results on the GOV 2 test collection and <b>AOL</b> <b>query</b> <b>log</b> show improvement by 31. 7 % and 28. 3 % significantly in statMAP for two sources of reconstruction and 153 ad-hoc queries. Our model also outperforms pseudo relevance feedback...|$|R
40|$|Temporal {{distribution}} of the web query traffic has a signif-icant influence on the performance and scalability of large scale information systems. Most of the existing performance analysis assume a standard Poisson distribution. In this pa-per, we demonstrate that the temporal {{distribution of}} web <b>query</b> <b>log</b> traffic is statistically self-similar, and that the currently assumed Poisson query distribution does not cap-ture the statistical properties of the distribution. Also, we propose a high variability aggregation generative model as the physical explanation for the observed self-similarity, and support the model by rigorous statistical analysis on queries of the individual users from the <b>AOL</b> <b>query</b> <b>log.</b> We also empirically demonstrate the implications of our findings on capacity planning and performance evaluation of informa-tion systems based on queuing analysis. Our {{results show that the}} assumed Poisson distribution over-estimates the us-able capacity of web information systems significantly. 1...|$|R
40|$|Web {{search engines}} gather {{information}} from the queries performed by the user {{in the form of}} query logs. These logs are extremely useful for research, marketing, or profiling, {{but at the same time}} they are a great threat to the user's privacy. We provide a novel approach to anonymize query logs so they ensure user k-anonymity, by extending a common method used in statistical disclosure control: microaggregation. Furthermore, our microaggregation approach takes into account the semantics of the queries by relying on the Open Directory Project. We have tested our proposal with real data from <b>AOL</b> <b>query</b> <b>logs.</b> Partial support by the Spanish MICINN (projects eAEGIS TSI 2007 - 65406 -C 03 - 02, TSI 2007 - 65406 -C 03 - 01, ARES-CONSOLIDER INGENIO 2010 CSD 2007 - 00004, Audit Transparency Voting Process PT- 430000 - 2010 - 31 and N-KHRONOUS TIN 2010 - 15764), the Spanish Ministry of Industry, Commerce and Tourism (projects eVerification TSI- 020100 - 2009 - 720 and SeCloud TSI- 020302 - 2010 - 153), and the Government of Catalonia (grant 2009 SGR 1135) is acknowledged. G. Navarro-Arribas enjoyed a Juan de la Cierva grant (JCI- 2008 - 3162) from the Spanish MICINN. The authors are solely responsible for the views expressed in this paper, which do not necessarily reflect the position of UNESCO nor commit that organization. Peer Reviewe...|$|E
40|$|Many of today’s publish/subscribe (pub/sub) {{systems have}} been {{designed}} to cope with a large volume of subscriptions and high event arrival rate (velocity). However, in many novel applications (such as e-commerce), there is an increasing variety of items, each with different attributes. This leads to a very high-dimensional and sparse database that existing pub/sub systems can no longer support effectively. In this paper, we propose an efficient in-memory index that is scalable to the volume and update of subscriptions, the arrival rate of events and the variety of subscribable attributes. The index is also extensible to support complex scenarios such as prefix/suffix filtering and regular expression matching. We conduct extensive experiments on synthetic datasets and two real datasets (<b>AOL</b> <b>query</b> <b>log</b> and Ebay products). The results demonstrate the superiority of our index over state-of-the-art methods: our index incurs orders of magnitude less index construction time, consumes a small amount of memory and performs event matching efficiently. 1...|$|R
40|$|Query auto-completion (QAC) {{is being}} used by many of today's search engines. It helps searchers {{formulate}} queries by providing a list of query completions after entering an initial prefix of a query. To cater for a user's specific information needs, personalized QAC strategies use a searcher's search history and their profile. Is personalization consistently effective in different search contexts? We study the QAC problem by selectively personalizing the query completion list. Based on a lenient personalized QAC strategy that encodes the ranking signal as a trade-off between query popularity and search context, we propose a model for selectively personalizing query auto-completion (SP-QAC) to study this trade-off. We predict effective trade-offs based on a regression model, where the typed query prefix, clicked documents and preceding queries in the same session are used to weigh personalization in QAC. Experiments on the <b>AOL</b> <b>query</b> <b>log</b> show the SP-QAC model can significantly outperform a state-of-the-art personalized QAC approach...|$|R
40|$|Many of today’s {{search engines}} provide autocompletion while the user is typing a query string. This type of dynamic query {{suggestion}} can help users to formulate queries that better represent their search intent during Web search interactions. In this paper, we demonstrate our query suggestion system called CONQUER, which allows to efficiently suggest queries {{for a given}} partial query {{and a number of}} available query context observations. The context-awareness allows for suggesting queries tailored to a given context, e. g., the user location or the time of day. CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations. For this, the weight of a context in a query suggestion can be adjusted online, for example, based on the learned user behavior or user profiles. We demonstrate the functionality of CONQUER based on 6 million <b>queries</b> from an <b>AOL</b> <b>query</b> <b>log</b> using the time of day and the country domain of the clicked URLs in the search result as context observations. Categories and Subject Descriptor...|$|R
40|$|Users often try to {{accumulate}} {{information on a}} topic of interest from multiple information sources. In this case a user’s informational need might be {{expressed in terms of}} an available relevant document, e. g. a web-page or an e-mail attachment, rather than a query. Database search engines are mostly adapted to the queries manually created by the users. In case a user’s informational need is expressed in terms of a document, we need algorithms that map keyword queries automatically extracted from this document to the database content. In this paper we analyze the impact of selected document and database statistics on the effectiveness of keyword disambiguation for manually created as well as automatically extracted keyword queries. Our evaluation is performed using a set of user <b>queries</b> from the <b>AOL</b> <b>query</b> <b>log</b> and a set of queries automatically extracted from Wikipedia articles both executed against the Internet Movie Database (IMDB). Our experimental results show that (1) knowledge of the document context is crucial in order to extract meaningful keyword queries; (2) statistics which enable effective disambiguation of user queries are not sufficient to achieve the same quality for the automatically extracted requests...|$|R
40|$|<b>Query</b> <b>log</b> {{analysis}} has received substantial attention in recent years, {{in which the}} click graph is an important technique for describing the relationship between queries and URLs. State-of-the-art approaches based on the raw click frequencies for modeling the click graph, however, are not noise-eliminated. Nor do they handle heterogeneous query-URL pairs well. In this paper, we investigate and develop a novel entropy-biased framework for modeling click graphs. The intuition behind this model is that various query-URL pairs should be treated differently, i. e., common clicks on less frequent but more specific URLs are of greater value than common clicks on frequent and general URLs. Based on this intuition, we utilize the entropy information of the URLs and introduce a new concept, namely the inverse query frequency (IQF), to weigh the importance (discriminative ability) of a click on a certain URL. The IQF weighting scheme is never explicitly explored or statistically examined for any bipartite graphs in the information retrieval literature. We not only formally define and quantify this scheme, but also incorporate it with the click frequency and user frequency information on the click graph for an effective query representation. To illustrate our methodology, we conduct experiments with the <b>AOL</b> <b>query</b> <b>log</b> data for <b>query</b> similarity analysis and query suggestion tasks. Experimental results demonstrate that considerable improvements in performance are obtained with our entropy-biased models...|$|R
40|$|Recently many {{data types}} arising from data mining and Web search {{applications}} can be modeled as bipartite graphs. Examples include queries and URLs in <b>query</b> <b>logs,</b> and authors and papers in scientific literature. However, {{one of the}} issues is that previous algorithms only consider the content and link information {{from one side of the}} bipartite graph. There is a lack of constraints to make sure the final relevance of the score propagation on the graph, as there are many noisy edges within the bipartite graph. In this paper, we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance. Moreover, we investigate the algorithm based on two frameworks, including the iterative and the regularization frameworks, and illustrate the generalized Co-HITS algorithm from different views. For the iterative framework, it contains HITS and personalized PageRank as special cases. In the regularization framework, we successfully build a connection with HITS, and develop a new cost function to consider the direct relationship between two entity sets, which leads to a significant improvement over the baseline method. To illustrate our methodology, we apply the Co-HITS algorithm, with many different settings, to the application of query suggestion by mining the <b>AOL</b> <b>query</b> <b>log</b> data. Experimental results demonstrate that CoRegu- 0. 5 (i. e., a model of the regularization framework) achieves the best performance with consistent and promising improvements...|$|R
40|$|Query auto {{completion}} {{is known}} to provide poor predictions of the user’s query when her input prefix is very short (e. g., one or two characters). In this paper we show that context, such as the user’s recent queries, {{can be used to}} improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user’s input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. In order to evaluate our approach, we performed extensive experimentation over the public <b>AOL</b> <b>query</b> <b>log.</b> We demonstrate that when the recent user’s queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion’s MRR is 48 % higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion’s MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31. 5 % in MRR relative to MostPopular-Completion on average...|$|R
40|$|On the web, {{search engines}} {{represent}} a primary instrument through which users exercise their intent. Understanding the specific goals users express in search queries could improve our theoretical knowledge about strategies for search goal formulation and search behavior, and could equip search engine providers with better descriptions of users’ information needs. However, {{the degree to}} which goals are explicitly expressed in search queries can be suspected to exhibit considerable variety, which poses a series of challenges for researchers and search engine providers. This paper introduces a novel perspective on analyzing user goals in search <b>query</b> <b>logs</b> by proposing to study different degrees of intentional explicitness. To explore the implications of this perspective, we studied two different degrees of explicitness of user goals in the <b>AOL</b> search <b>query</b> <b>log</b> containing more than 20 million queries. Our results suggest that different degrees of intentional explicitness represent an orthogonal dimension to existing search query categories and that understanding these different degrees is essential for effective search. The overall contribution of this paper is the elaboration of a set of theoretical arguments and empirical evidence that makes a strong case for further studies of different degrees of intentional explicitness in search <b>query</b> <b>logs.</b> Author Keywords Web search, user goals, <b>query</b> <b>log</b> analysis, <b>AOL</b> searc...|$|R
40|$|Tremendous {{growth of}} the Web, lack of {{background}} knowledge about the Information Retrieval (IR), length of the input query keywords and its ambiguity, Query Recommendation is an important procedure which analyzes the real search intent of the user and recommends set of queries {{to be used in}} future to retrieve the relevant and required information. The proposed method recommends the queries by generating frequently accessed queries, rerank the recommended queries and evaluates the recommendation {{with the help of the}} ranking measures Normalized Discounted Cumulative Gain (NDCG) and Coefficient of Variance (CV). The proposed strategies are experimentally evaluated using real time American On Line (<b>AOL)</b> search engine <b>query</b> <b>log...</b>|$|R
40|$|Abstract. Generally, every Web {{search engine}} logs the user sessions. These records, called <b>query</b> <b>logs,</b> contain {{valuable}} {{information about the}} behaviour of Internet users and their language. There {{are only a few}} experiments on mining <b>query</b> <b>logs,</b> but they confirm that <b>query</b> <b>logs</b> are very useful for designing natural language applications in Web retrieval. This paper shows how lexical and semantic information can be extracted from <b>query</b> <b>logs</b> using statistical methods. I first summarize approaches in <b>query</b> <b>log</b> processing and mining for different purposes. After a short description of the used <b>query</b> <b>logs,</b> I present new domain- and language-independent methods for generating a compound dictionary and extracting semantically similar terms. The evaluation will shed light on the quality of proposed methods and show that the results are good enough to be directly integrated in query processing and improve information retrieval on the Web...|$|R
40|$|The {{practice}} of guiding {{a search engine}} based on <b>query</b> <b>logs</b> observed from the engine's user population provides large volumes of data but potentially also sacrifices {{the privacy of the}} user. In this paper, we ask the following question: Is it possible, given rich instrumented data from a panel and usability study data, to observe complete information without routinely analyzing <b>query</b> <b>logs?</b> What unique benefits to the user could hypothetically be derived from analyzing <b>query</b> <b>logs?</b> We demonstrate that three different modes of collecting data, the field study, the instrumented user panel, and the raw <b>query</b> <b>log,</b> provide complementary sources of data. The <b>query</b> <b>log</b> is the least rich source of data for individual events, but has irreplaceable information for understanding the scope of resources that a search engine needs to provide for the user...|$|R
40|$|As popular {{search engines}} face the {{sometimes}} conflicting interests of protecting privacy while retaining <b>query</b> <b>logs</b> {{for a variety}} of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the utility of <b>query</b> <b>logs.</b> This article seeks to assess seven of these techniques against three sets of criteria: (1) how well the technique protects privacy, (2) how well the technique preserves the utility of the <b>query</b> <b>logs,</b> and (3) how well the technique might be implemented as a user control. A user control is defined as a mechanism that allows individual Internet users to choose to have the technique applied to their own <b>query</b> <b>logs...</b>|$|R
40|$|Abstract. With organisations, local {{bodies and}} governments now {{releasing}} {{large amounts of}} linked data, {{there is a great}} opportunity for users and software agents to look for structured information, serving their information needs. <b>Query</b> <b>logs,</b> preserving such information needs can be harvested to understand what linked data consumers are looking for. Though statistical analysis of such <b>query</b> <b>logs</b> have been employed over the years to improve performance, visualising such analyses can provide a different way of exploration that could be invaluable to researchers, developers and linked data providers for discovering hidden trends and patterns. This paper presents our approach to analyse <b>query</b> <b>logs</b> and introduces SEMLEX, a tool that facilitates visual exploration of semantic <b>query</b> <b>log</b> analysis...|$|R
40|$|Access to {{knowledge}} about common human goals {{has been found}} critical for realizing the vision of intelligent agents acting upon user intent on the web. Yet, the acquisition of knowledge about common human goals represents a major challenge. In a departure from existing approaches, this paper investigates a novel resource for knowledge acquisition: The utilization of search <b>query</b> <b>logs</b> for this task. By relating goals contained in search <b>query</b> <b>logs</b> with goals contained in existing commonsense knowledge bases such as ConceptNet, we aim {{to shed light on}} the usefulness of search <b>query</b> <b>logs</b> for capturing knowledge about common human goals. The main contribution of this paper consists of insights generated from an empirical study comparing common human goals contained in two large search <b>query</b> <b>logs</b> (<b>AOL</b> and Microsoft Research) with goals contained in the commonsense knowledge base ConceptNet. The paper sketches ways how goals from search <b>query</b> <b>logs</b> could be used to address the goal acquisition and goal coverage problem related to commonsense knowledge bases...|$|R
40|$|This demo {{shows how}} usage {{information}} buried in <b>query</b> <b>logs</b> {{can play a}} central role in data integration and data exchange. More specifically, our system U-Map uses <b>query</b> <b>logs</b> to generate correspondences between the attributes of two different schemas and the complex mapping rules to transform and restructure data records from one of these schemas to another. We introduce several novel features showing the benefit of incorporating <b>query</b> <b>log</b> analysis into these key components of data integration and data exchange systems...|$|R
40|$|Academic {{researchers}} have very {{limited access to}} <b>query</b> <b>logs</b> of major web search engines. Studying and analyzing large-scale <b>query</b> <b>logs</b> is essential for advancing Web IR. We propose setting up review boards with clear rules for appropriate conduct, and allowing researchers access to logs within this framework...|$|R
40|$|Query {{reformulation}} techniques {{based on}} <b>query</b> <b>logs</b> {{have been studied}} {{as a method of}} capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary <b>query</b> <b>logs</b> and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a <b>query</b> <b>log</b> and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that logbased query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated <b>query</b> <b>log</b> is as least as effective as a real log for these techniques...|$|R
40|$|In this article, we {{demonstrate}} {{the value of}} long-term <b>query</b> <b>logs.</b> Most work on <b>query</b> <b>logs</b> to date considers only short-term (within-session) query information. In contrast, we show that long-term <b>query</b> <b>logs</b> {{can be used to}} learn about the world we live in. There are many applications of this that lead not only to improving the search engine for its users, but also potentially to advances in other disciplines such as medicine, sociology, economics, and more. In this article, we will show how long-term <b>query</b> <b>logs</b> can be used for these purposes, and that their potential is severely reduced if the logs are limited to short time horizons. We show that query effects are long-lasting, provide valuable information, and might be used to automatically make medical discoveries, build concept hierarchies, and generally learn about the sociological behavior of users. We believe these applications are only the beginning of what can be done with the information contained in long-term <b>query</b> <b>logs,</b> and see this work as a step toward unlocking their potential...|$|R
40|$|In this paper, {{we study}} {{a new problem}} of mining causal {{relation}} of queries in search engine <b>query</b> <b>logs.</b> Causal relation between two queries means event on one query is the causation of some event on the other. We first detect events in <b>query</b> <b>logs</b> by efficient statistical frequency threshold. Then the causal relation of queries is mined by the geometric features of the events. Finally the Granger Causality Test (GCT) is utilized to further re-rank the causal relation of queries according to their GCT coefficients. In addition, we develop a 2 -dimensional visualization tool to display the detected relationship of events in a more intuitive way. The experimental results on the MSN search engine <b>query</b> <b>logs</b> demonstrate that our approach can accurately detect the events in temporal <b>query</b> <b>logs</b> and the causal relation of queries is detected effectively...|$|R
40|$|International audienceLeveraging <b>query</b> <b>logs</b> beneﬁts {{the users}} {{analyzing}} large data warehouses. But so far nothing exists {{to allow the}} user to have concise and usable representation of {{what is in the}} log. In this paper, we propose a framework for summarizing OLAP <b>query</b> <b>logs.</b> This framework is {{based on the idea that}} a query can summarize another query and that a log can summarize another log. It includes a simple language to declaratively specify a summary, a measure to assess the quality of a summary and an algorithm for automatically computing a good quality summary of a <b>query</b> <b>log...</b>|$|R
40|$|In this lecture, we {{will focus}} on {{analyzing}} user goals in search <b>query</b> <b>logs.</b> Readings: M. Strohmaier, P. Prettenhofer, M. Lux, Different Degrees of Explicitness in Intentional Artifacts - Studying User Goals in a Large Search <b>Query</b> <b>Log,</b> CSKGOI' 08 International Workshop on Commonsense Knowledge and Goal Oriented Interfaces, in conjunction with IUI' 08, Canary Islands, Spain, 2008. ...|$|R
40|$|Releasing Web <b>query</b> <b>logs</b> {{which contain}} {{valuable}} infor-mation for research or marketing, can breach {{the privacy of}} search engine users. Therefore rendering <b>query</b> <b>logs</b> to limit linking a query to an individual while preserving the data usefulness for analysis, is an important research problem. This survey provides an overview and discus-sion on the recent studies on this direction. 1...|$|R
40|$|A better {{understanding}} of what motivates humans to perform certain actions is relevant {{for a range of}} research challenges including generating action sequences that implement goals (planning). A first step in this direction is the task of acquiring knowledge about human goals. In this work, we investigate whether Search <b>Query</b> <b>Logs</b> are a viable source for extracting expressions of human goals. For this purpose, we devise an algorithm that automatically identifies queries containing explicit goals such as find home to rent in Florida. Evaluation results of our algorithm achieve useful precision/recall values. We apply the classification algorithm to two large Search <b>Query</b> <b>Logs,</b> recorded by <b>AOL</b> and Microsoft Research in 2006, and obtain a set of ∼ 110. 000 queries containing explicit goals. To study the nature of human goals in Search <b>Query</b> <b>Logs,</b> we conduct qualitative, quantitative and comparative analyses. Our findings suggest that Search <b>Query</b> <b>Logs</b> (i) represent a viable source for extracting human goals, (ii) contain a great variety of human goals and (iii) contain human goals that can be employed to complement existing commonsense knowledge bases. Finally, we illustrate the potential of goal knowledge for addressing following application scenario: to refine and extend commonsense knowledge with human goals from Search <b>Query</b> <b>Logs.</b> This work is relevant for (i) knowledge engineers interested in acquiring human goals from textual corpora and constructing knowledge bases of human goals (ii) researchers interested in studying characteristics of human goals in Search <b>Query</b> <b>Logs...</b>|$|R
