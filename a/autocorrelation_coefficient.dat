202|212|Public
5000|$|When {{normalizing}} the autocovariance C of a weakly {{stationary process}} with its variance , one obtains the <b>autocorrelation</b> <b>coefficient</b> ...|$|E
5000|$|The <b>autocorrelation</b> <b>coefficient</b> at lag h {{is given}} bywhere ch is the {{autocovariance}} functionand c0 is the variance function ...|$|E
50|$|In signal processing, {{the above}} {{definition}} {{is often used}} without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is {{sometimes referred to as}} the <b>autocorrelation</b> <b>coefficient</b> or autocovariance function.|$|E
40|$|The {{behaviour}} of {{the sample}} <b>autocorrelation</b> <b>coefficients</b> {{is important for the}} identification of the time series process. In this note the exact expectations and distributions {{of the sample}} <b>autocorrelation</b> <b>coefficients</b> for a particular unstationary time series process IMA (1, 1) are evaluated numerically. It turns out that the exact expectation of the sample <b>autocorrelation</b> <b>coefficients</b> are smaller than the Wichern's estimates (1973), to which Box and Jenkins (1976, 200 - 1) referred. And also, from the exact distribution of the sample <b>autocorrelation</b> <b>coefficients,</b> we notice that using the <b>autocorrelation</b> <b>coefficients</b> in the identification of time series process is not necessarily exact...|$|R
5000|$|The power {{spectrum}} [...] {{can be expressed}} as a sum of known <b>autocorrelation</b> <b>coefficients</b> and unknown <b>autocorrelation</b> <b>coefficients.</b> By adjusting the values of unconstrained coefficients, the entropy can be maximized. The max entropy is of the form ...|$|R
40|$|It is {{generally}} {{believed that the}} lower-lag <b>autocorrelation</b> <b>coefficients</b> carry information about the spectral envelop and the higher-lag <b>autocorrelation</b> <b>coefficients</b> are more related to pitch information. In this paper, we use lower-lag and higher-lag ranges of the autocorrelation function separately for deriving speech recognition features, and investigate their role in terms of speech recognition performance. The state-of-the-art MFCC features use the whole autocorrelation function in their computation and are used here as a benchmark in our experiments. Our recognition results from the Aurora II corpus show that the higher-lag <b>autocorrelation</b> <b>coefficients</b> perform {{as well as the}} whole autocorrelation function for clean speech, and provide better performance for noisy speech, while lower-lag <b>autocorrelation</b> <b>coefficients</b> are not as effective in this aspect. 1...|$|R
40|$|The {{classical}} <b>autocorrelation</b> <b>coefficient</b> estimator in {{the time}} series context is very sensitive {{to the presence of}} outlying measurements in the data. This paper proposes several new robust estimators of the <b>autocorrelation</b> <b>coefficient.</b> First, we consider an autoregressive process of the first order AR(1) to be observed. Robust estimators of the <b>autocorrelation</b> <b>coefficient</b> are proposed in a straightforward way based on robust regression. Further, we consider the task of robust estimation of the <b>autocorrelation</b> <b>coefficient</b> of residuals of linear regression. The task is connected to verifying the assumption of independence of residuals and robust estimators of the <b>autocorrelation</b> <b>coefficient</b> are defined based on the Durbin-Watson test statistic for robust regression. The main result is obtained for the implicitly weighted <b>autocorrelation</b> <b>coefficient</b> with small weights assigned to outlying measurements. This estimator is based on the least weighted squares regression and we exploit its asymptotic properties to derive an asymptotic test that the <b>autocorrelation</b> <b>coefficient</b> is equal to 0. Finally, we illustrate different estimators on real economic data, which reveal the advantage of the approach based on the least weighted squares regression. The estimator turns out to be resistant against the presence of outlying measurements...|$|E
40|$|This article {{proposes a}} general method to build exact tests and {{confidence}} sets in linear regressions with first-order autoregressive Gaussian disturbances. Because of a nuisance parameter problem, {{we argue that}} generalized bounds tests and conservative confidence sets provide natural inference procedures in such a context. Given an exact confidence set for the <b>autocorrelation</b> <b>coefficient,</b> we describe how to obtain a similar simultaneous confidence set for the <b>autocorrelation</b> <b>coefficient</b> and any subvector of regression coefficient. Conservative confidence sets for the regression coefficients are then deduced by a projection method. For any hypothesis that specifies jointly {{the value of the}} <b>autocorrelation</b> <b>coefficient</b> and any set of linear restrictions on the regression coefficients, we get exact similar tests. For tesing linear hypotheses about the regression coefficients only, we suggest bounds-type procedures. Exact confidence sets for the <b>autocorrelation</b> <b>coefficient</b> are built by "inverting" autocorrelation tests. The method is illustrated with two examples. Copyright 1990 by The Econometric Society. ...|$|E
30|$|In {{order to}} predict the future price, we first {{calculate}} the <b>autocorrelation</b> <b>coefficient</b> [23] of historical prices. The value denotes the correlation of historical prices with itself at different time point. The value of autocorrelation function (ACF) lies between - 1 and 1, with - 1 indicating anti-correlation, 0 indicating no correlation, and 1 indicating perfect correlation. If the <b>autocorrelation</b> <b>coefficient</b> is higher than 0.4, then the future prices are predicted by using linear regression [23]; if the <b>autocorrelation</b> <b>coefficient</b> is lower than 0.4, the future prices are predicted by using the inverse cumulative distribution function of the normal distribution. The inverse cumulative distribution function return the value of x such that P(price<x) with the availability target p. We set p as 0.99 to obtain a high level of availability.|$|E
40|$|Statistical {{analysis}} of the turbulence measured in flight 6 of the NASA B- 57 B over Denver, Colorado, from July 7 to July 23, 1982 included the calculations of average turbulence parameters, integral length scales, probability density functions, single point <b>autocorrelation</b> <b>coefficients,</b> two point <b>autocorrelation</b> <b>coefficients,</b> normalized autospectra, normalized two point autospectra, and two point cross sectra for gust velocities. The single point <b>autocorrelation</b> <b>coefficients</b> were compared with the theoretical model developed by von Karman. Theoretical analyses were developed which address the effects spanwise gust distributions, using two point spatial turbulence correlations...|$|R
5000|$|... λ(l,m) must {{be chosen}} such that known <b>autocorrelation</b> <b>coefficients</b> are matched.|$|R
5000|$|... {{with the}} ideal {{autocorrelation}} property, {{such that the}} off-peak (non-cyclic) <b>autocorrelation</b> <b>coefficients</b> ...|$|R
3000|$|... where Gi(x) (x= 1, 2,…,n) are the {{historical}} channel gain records, and n= 400 is the sample size. Next, we will calculate the <b>autocorrelation</b> <b>coefficient</b> ρi(τ). We convert the time lag of τ {{to the number}} of time slots, i.e., ⌊τ/Ts⌋. Hence, ρi(τ) can be estimated as γi(⌊τ/Ts⌋), based on Eq. (1). In addition, in order to predict the channel condition for multiple future time slots, the hub should calculate an <b>autocorrelation</b> <b>coefficient</b> vector for each channel. In DSS-TA, the autocorrelation coefficients within the time lag of 500 ms are calculated. For instance, if the length of one time slot is 5 ms, each autocorrelation vector has 500 / 5 = 100 elements, which are denoted for the channel “ SNi-Hub” as γ⃗ ⃗_⃗i⃗ = (γ _i(1),γ _i(2),γ _i(3),..., γ _i(100)). Besides, when the time lag exceeds 500 ms, the <b>autocorrelation</b> <b>coefficient</b> is considered as γi(100).|$|E
3000|$|... is the {{expected}} value from the observations, {{calculated for the}} time variation (delay) k. The <b>autocorrelation</b> <b>coefficient</b> (ρ) of a TS varies between − 1 and 1.|$|E
40|$|This {{document}} clarifies {{the use of}} Moran’s <b>autocorrelation</b> <b>coefficient</b> {{to quantify}} whether the distribution of a trait among a set of species is affected or not by their phylogenetic relationships. 1 Theoretical Background Moran’s <b>autocorrelation</b> <b>coefficient</b> (often denoted as I) {{is an extension of}} Pearson product-moment correlation coefficient to a univariate series [2, 5]. Recall that Pearson’s correlation (denoted as ρ) between two variables x and y both of length n is: n∑ (xi − ¯x) (yi − ¯y) ρ = [n∑ i= 1 i= 1 (xi − ¯x) 2 n...|$|E
40|$|In {{this paper}} we {{investigate}} {{the performance of}} different feature extraction methods for facial expression recognition based on the higher-order local <b>autocorrelation</b> (HLAC) <b>coefficients</b> and Gabor wavelet filters. We use a Cohn-Kanade database of facial images, organized in training and testing sets, for evaluation. <b>Autocorrelation</b> <b>coefficients</b> are computationally inexpensive, inherently shift-invariant and quite robust against changes in facial expression. The {{focus is on the}} difficult problem of recognizing an expression in different resolutions. Results indicate that local <b>autocorrelation</b> <b>coefficients</b> have surprisingly high information content...|$|R
40|$|In this paper, {{a feature}} {{extraction}} method that is robust to additive background noise is proposed for automatic speech recognition. Since the background noise corrupts the <b>autocorrelation</b> <b>coefficients</b> {{of the speech}} signal mostly at the lower-time lags, while the higher-lag <b>autocorrelation</b> <b>coefficients</b> are least affected, this method discards the lower-lag <b>autocorrelation</b> <b>coefficients</b> and uses only the higher-lag <b>autocorrelation</b> <b>coefficients</b> for spectral estimation. The magnitude spectrum of the windowed higher-lag autocorrelation sequence is used here as {{an estimate of the}} power spectrum of the speech signal. This power spectral estimate is processed further (like the well-known Mel frequency cepstral coefficient (MFCC) procedure) by the Mel filter bank, log operation and the discrete cosine transform to get the cepstral coefficients. These cepstral coefficients are referred to as the autocorrelation Mel frequency cepstral coefficients (AMFCCs). We evaluate the speech recognition performance of the AMFCC features on the Aurora and the resource management databases and show that they perform as well as the MFCC features for clean speech and their recognition performance is better than the MFCC features for noisy speech. Finally, we show that the AMFCC features perform better than the features derived from the robust linear prediction-based methods for noisy speech. Griffith Sciences, Griffith School of EngineeringNo Full Tex...|$|R
40|$|First order {{periodic}} autoregressive {{models is}} of mostly used models in modeling of time dependency of hydrological flow processes. In these models, periodicity of the correlogram is preserved {{as well as}} time dependency of processes. However, the parameters of these models, namely, inter-monthly lag- 1 <b>autocorrelation</b> <b>coefficients</b> may be often estimated erroneously from short samples, since they are statistics of high order moments. Therefore, to constitute a regional model may be a solution that can produce more reliable and decisive estimates, and derive models and model parameters in any required point of the basin considered. In this study, definitions of homogeneous region for lag- 1 <b>autocorrelation</b> <b>coefficients</b> are made; five parametric and non parametric models are proposed to set regional models of lag- 1 <b>autocorrelation</b> <b>coefficients.</b> Regional models are applied on 30 stream flow gauging stations in Seyhan and Ceyhan basins, and tested by criteria of relative absolute bias, simple and relative root of mean square errors...|$|R
3000|$|... {{correspond}} to periods between 3 milliseconds and 16 milliseconds (which is the expected fundamental frequency range in voiced speech). The <b>autocorrelation</b> <b>coefficient</b> {{is defined as}} the value of this peak: [...]...|$|E
3000|$|The <b>autocorrelation</b> <b>coefficient</b> {{is defined}} as the highest peak in the {{short-time}} autocorrelation sequence and is used to evaluate how close the audio signal is to a periodic one. First, the normalized autocorrelation sequence of the frame is computed: [...]...|$|E
40|$|The Best linear {{unbiased}} estimate (BLUE) of Buys-Ballot estimates when trend-cycle component is linear {{are discussed in}} this paper. The estimates are those proposed by Iwueze and Nwogu (2004). Discussed are the Chain Based Estimation (CBE) method and the Fixed Based Estimation (FBE) method. The variates for the CBE method {{were found to have}} constant mean and variance but are correlated with only one significant <b>autocorrelation</b> <b>coefficient</b> at lag one. The variates for the FBE method were found to have constant mean, non-constant variance but with constant <b>autocorrelation</b> <b>coefficient</b> at all lags. Because the CBE variates exhibit stationarity, Best Linear unbiased estimators of the slope and intercept were derived. Numerical examples were used to illustrate the methods...|$|E
50|$|Advantages and Dis{{advantage}}s:-The {{advantage of}} this estimator is that errors in measuring or estimating the known <b>autocorrelation</b> <b>coefficients</b> can {{be taken into account}} since exact match is not required. The disadvantage is that too many computations are required.|$|R
30|$|To extract motion-based features, we {{calculate}} the Fourier transform of <b>autocorrelation</b> <b>coefficients</b> and extract {{peaks in the}} amplitude spectrum. Figures 10 and 11 plot amplitude spectrum of autocorrelation curves in Figures 8 and 9, respectively. We use six features for periodicity measurement as follows.|$|R
5000|$|In {{this method}} of {{spectral}} estimation, we {{try to find the}} spectral estimate whose inverse Fourier transform matches the known auto correlation coefficients. We maximize the entropy of the spectral estimate such that it matches the <b>autocorrelation</b> <b>coefficients.</b> The entropy equation is given as ...|$|R
40|$|Normal 0 MicrosoftInternetExplorer 4 The Best linear {{unbiased}} estimate (BLUE) of Buys-Ballot estimates when trend-cycle component is linear {{are discussed in}} this paper.   The estimates are those proposed by Iwueze and Nwogu (2004).   Discussed are the Chain Based Estimation (CBE) method and the Fixed Based Estimation (FBE) method.   The variates for the CBE method {{were found to have}} constant mean and variance but are correlated with only one significant <b>autocorrelation</b> <b>coefficient</b> at lag one. The variates for the FBE method were found to have constant mean, non-constant variance but with constant <b>autocorrelation</b> <b>coefficient</b> at all lags.   Because the CBE variates exhibit stationarity, Best Linear unbiased estimators of the slope and intercept were derived.   Numerical examples were used to illustrate the methods. </span...|$|E
40|$|This short note derives the {{probability}} limits of several estimators for panel AR(1) models under misspecification using sequential asymptotics. The {{results show that}} GMM estimators based on the forward orthogonal deviation transformation converge to the first-order <b>autocorrelation</b> <b>coefficient,</b> (C) 2008 Elsevier B. V. All rights reserved...|$|E
30|$|Wang et al. {{proposed}} a normalized model of pitch perception {{based on the}} unique resolution of the human ear to different frequency sources. This algorithm has great significance in multi-frequency estimation techniques [2]. Wu et al. simplified the pitch model by using the Bark scale to simulate the human ear frequency response, which greatly reduced the amount of computation. The method calculates the <b>autocorrelation</b> <b>coefficient</b> of the signal through two channels, and the multi-channel signal <b>autocorrelation</b> <b>coefficient</b> in the non-ERB scale, and finally calculates the respective fundamental frequency values one by one by scale stretching and linear interpolation. Similarly, the cycle-based algorithm was used by Staley and Sethares. Similarly, based on {{the establishment of the}} pitch model, Cheveigne et al. recursively screened the estimated fundamental frequencies in the poetry selection [3].|$|E
40|$|Abstract. A Barker {{sequence}} is a binary sequence with all non–trivial acyclic <b>autocorrelation</b> <b>coefficients</b> of size at most 1. It {{is widely believed}} (and confirmed up to size 10 22) that none exist of length greater than 13. A Barker polynomial is a polynomial whose coefficients form a Barker sequence. This problem {{has been around for}} at least 40 years. A K-Barker {{sequence is}} a binary sequence with all non–trivial acyclic <b>autocorrelation</b> <b>coefficients</b> of size at most K. It is widely believed that for eack K there are only finitely many such sequences (though this appears very hard and has been open for decades even for k:= 1). A K-Barker polynomial is a polynomial whose coefficients form a Barker sequence...|$|R
40|$|In {{this paper}} we provide {{polynomial}} time approximation techniques which {{allow us to}} calculate, to arbitrary levels of accuracy and with high probability of success, the spectral <b>coefficients</b> and <b>autocorrelation</b> <b>coefficients</b> of Boolean functions, given that those functions are expressed in either Sum-of-Products or Product-of-Sums form...|$|R
40|$|The {{ultrasonic}} emission, {{generated by}} rock samples loading, {{was subjected to}} autocorrelation analysis. The increase of <b>autocorrelation</b> <b>coefficients</b> and the tendency to their linear decrease is evidence of redistribution of stress in the sample, which occurs after stronger ultrasonic events. The above-mentioned parameters have a precursory character...|$|R
40|$|It {{is shown}} that the null {{distribution}} of the F-test in a linear regression is rather non-robust to spatial autocorrelation among the regression disturbances. In particular, the true size of the test tends to either zero or unity when the spatial <b>autocorrelation</b> <b>coefficient</b> approaches the boundary of the parameter space...|$|E
40|$|In this paper, {{we study}} the {{limiting}} distributions for the {{ordinary least squares}} (OLS), the fixed effects (FE), first difference (FD), and the generalized least squares (GLS) estimators in a linear time trend regression with a one-way error component model {{in the presence of}} serially correlated errors. We show that when the error term is I(0), the FE is asymptotically equivalent to GLS. However, when the error term is I(1), the GLS could be less efficient than FD or FE estimators and FD is the most efficient estimator. However, when the intercept is included in the model and the error term is I(0), the OLS, FE, and GLS are asymptotically equivalent. The limiting distribution of the GLS depends on the initial condition significantly when the error term is I(1) and an intercept is included in the regression. Monte Carlo experiments are employed to compare the performance of these estimators in finite samples. The main findings are: (1) the two-steps GLS estimators perform well if the variance component is small and close to zero when <b>autocorrelation</b> <b>coefficient</b> is less than one, (2) the FD estimator dominates the other estimators when <b>autocorrelation</b> <b>coefficient</b> equals to one for all values of variance component and (3) the FE estimator is recommended in practice since it performs pretty well for all values of the <b>autocorrelation</b> <b>coefficient</b> and variance component. Panel Time Series...|$|E
40|$|We propose an {{artificial}} market model based on deterministic agents. The agents modify their ask/bid price depending on past price changes. The temporal development of market price fluctuations is calculated numerically. A probability density function of market price changes has power law tails. <b>Autocorrelation</b> <b>coefficient</b> {{of the changes}} has an anti-correlation, and <b>autocorrelation</b> <b>coefficient</b> of squared changes (volatility correlation function) has a long time correlation. A probability density function of intervals between successive trading follows a geometric distribution. GARCH type stochastic process is theoretically derived from this market model in a limit case. We discuss factors of the market price fluctuations and a relation between the volatility of the market prices and a demand-supply curve. We conclude that the power law tails and the long time volatility result from mechanism of the GARCH type stochastic process. ...|$|E
40|$|The exact {{distribution}} of extremes of a non-gaussian stationary discrete process is obtained and their crossing intervals are studied {{in terms of}} the <b>autocorrelation</b> <b>coefficients</b> for any level of crossing. This process is an important model for some physical magnitudes. extreme distribution downcrossing and upcrossing intervals non-Gaussian process type 1 extremes distribution...|$|R
40|$|In {{this paper}} we provide {{polynomial}} time approximation techniques which {{allow us to}} calculate, to arbitrary levels of accuracy and with high probability of success, the spectral <b>coefficients</b> and <b>autocorrelation</b> <b>coefficients</b> of Boolean functions, given that those functions are expressed in either Sum-of-Products or Product-of-Sums form. 1 Introduction The focus {{of this paper is}} on the generation of spectral <b>coefficients</b> and <b>autocorrelation</b> <b>coefficients</b> for Boolean functions. Efficient calculation of those coefficients would allow digital logic analysts to draw on the large body of research already effectively employed in the area of signal processing. Utilisation of coefficient-based techniques in areas such as logic testing and synthesis has traditionally been hampered by the computational requirements for coefficient calculation. To reduce the computational demands, we use an approximation technique to estimate the coefficient values. As any specified level of accuracy can still be o [...] ...|$|R
30|$|The {{normalized}} <b>autocorrelation</b> <b>coefficients</b> can {{be regarded}} as textural features. They provide information about the spatial relationship between the texture patterns. For small primitives (texture elements), autocorrelation changes rapidly with change in the distance between the pixels. For the large primitives, it changes slowly. The presence of regular patterns makes regular periodic changes in the autocorrelation.|$|R
