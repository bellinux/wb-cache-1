48|111|Public
40|$|The climate <b>analogue</b> <b>approach</b> {{is often}} {{considered}} {{a valuable tool}} for climate change impact projection and adaptation planning, especially for complex systems that cannot be modelled reliably. Important examples are smallholder farming systems using agroforestry or other mixed-cropping approaches. For the projected climate at a particular site of interest, the <b>analogue</b> <b>approach</b> identifies locations where the current climate is similar to these projected conditions. By comparing baseline-analogue site pairs, information on climate impacts and opportunities for adaptation can be obtained. However, the climate <b>analogue</b> <b>approach</b> is only meaningful, if climate is a dominant driver of differences between baseline and analogue site pairs. For a smallholder farming setting on Mt. Elgon in Kenya, we tested this requirement by comparing yield potentials of maize and coffee (obtained from the IIASA Global Agro-ecological Zones dataset) among 50 close analogue sites for different future climate scenarios and models, and by comparing local ecological knowledge and farm characteristics for one baseline-analogue pair. Yield potentials among the 50 closest analogue locations varied strongly within all climate scenarios, hinting at factors other than climate as major drivers of what the <b>analogue</b> <b>approach</b> might interpret as climate effects. However, on average future climatic conditions seemed more favourable to maize and coffee cultivation than current conditions. The detailed site comparison revealed substantial differences between farms in important characteristics, such as farm size and presence of cash crops, casting doubt on {{the usefulness of the}} comparison for climate change analysis. Climatic constraints were similar between sites, so that no apparent lessons for adaptation could be derived. Pests and diseases were also similar, indicating that climate change may not lead to strong changes in biotic constraints at the baseline site in the near future. From both analyses, it appeared that differences between baseline and analogue sites were mostly explained by non-climatic factors. This does not bode well for using the analogue approac...|$|E
40|$|There exist {{a limited}} but {{growing number of}} {{biological}} metal centers whose properties lie conspicuously {{outside the realm of}} known inorganic chemistry. The synthetic <b>analogue</b> <b>approach,</b> broadly directed,[*]offers a powerful exploratory tool that can define intrinsic chemical possibilities for these sites while simultaneously expanding the frontiers of fundamental inorganic chemistry. This speculative application of analogue study is exemplified here in the evolution of synthetic efforts inspired by the cluster chemistry of biological nitrogen fixation...|$|E
40|$|AbstractEnergy sector {{industries}} included {{oil refinery}} industry was considered as big part to contribute enlarge gas emission. Nowadays CCS technology becoming effective method to reduce gas emission. During CO 2 capture process H 2 {{were included in}} oil refinery processing. In this study mixing gas of CO 2 and H 2 was use to observe the gas migration during CO 2 leakage based on natural <b>analogue</b> <b>approach.</b> We have constructed a test field with 5 test wells to investigate an optimal monitoring system against CO 2 leakage to carry the tracer gas injection into the wells...|$|E
5000|$|... #Subtitle level 3: Transition state <b>analogue</b> selection/screening <b>approach</b> ...|$|R
40|$|For his solo exhibition, Panoramic Decelerator, Jim Hobbs {{presents}} {{a selection of}} recent film, video, photographic and sculptural works completed over the past 2 years. The work moves across {{a wide variety of}} media, utilizing a combination of both digital and <b>analogue</b> <b>approaches</b> to explore complex relationships between place, memory and the fluctuations and materiality of time...|$|R
40|$|Inverse {{photogrammetry}} is {{a technique}} fairly old but rather seldom used. Reasons for this are its somehow limited applicability and, mainly, the tediousness of earlier <b>analogue</b> <b>approaches.</b> Today's digital processing techniques offer tools making its application both easier and more flexible. In this paper digital processing tools are considered and employed in order to generate more "realistic" end products o...|$|R
40|$|This paper {{discusses}} {{a technique}} that uses several different networks working together to find a relationship that approximates analogue training sets. Each network learns a different region of the training space and all these regions fit together, like pieces of a jigsaw puzzle, to cover the entire training space. This <b>analogue</b> <b>approach</b> is an extension to a technique previously developed for solving digital problems. The networks can be of any type (eg backprop, cascade). However, virtually any other technique {{can be used in}} place of the networks: evolved polynomials, DRS, Taboo search, Nearest Neighbour, and other statistical techniques...|$|E
40|$|In {{the first}} part this text deals with more {{thorough}} explanation of concepts such as analogue environment, digitalization or analogue and digital code in the cultural context. For example the difference between analogue and digital abstraction. It also points {{out the possibility of}} a pure analog transmission and record, as a counterpart of of selective and examining digitalization. Via these terms the interpretation of the film starts: Eisenstein's and Tarkovsky's approaches as the example of dichotomy of two ?natural? or ?organic? ?philosophies? in film theory. Eisenstein as a representative of digitization and Tarkovsky as an advocate of an <b>analogue</b> <b>approach.</b> Thesis leads into utopian micromanifest of the organic film...|$|E
40|$|The {{climate change}} {{information}} required for many impact studies is of a spatial scale much finer than that provided by climate models. For impact applications point specific climate projections are required. Therefore a gap exists between what climate models can predict about future {{climate change and}} the information relevant to our daily life. Statistical downscaling models are commonly used to fill this gap. Statistical downscaling models {{are based on the}} view that the regional climate is conditioned by two factors: the large scale climatic state, and local physiographic features. The <b>analogue</b> <b>approach</b> developed in the Bureau of Meteorology is one example of a more genera...|$|E
40|$|The {{ability to}} create thick tissues is a major tissue {{engineering}} challenge, requiring {{the development of a}} suitable vascular supply. Current trends are seeing the utilization of cells seeded into hybrid matrix/scaffold systems to create in vitro vascular <b>analogues.</b> <b>Approaches</b> that aim to create vasculature in vitro include the use of biological extracellular matrices such as collagen hydrogels, porous biodegradable polymeric scaffolds with macro- and micro-lumens and micro-channels, co-culture of cells, incorporation of growth factors, culture in dynamic bioreactor environments, and combinations of these. Of particular interest are those approaches that aim to create bioengineered tissues in vitro that can be readily connected to the host’s vasculature following implantation in order to maintain cell viability...|$|R
40|$|It is {{no great}} secret {{that climate change}} {{presents}} a massive challenge to agriculture. While some regions will develop a climate that favours agriculture, most will not. Even in places where conditions improve, many resource-poor smallholders will {{be hard pressed to}} take advantage of the additional production potential while, in areas where conditions deteriorate, farmers will have to adapt to their new circumstances as quickly as possible. This will be particularly difficult for smallholders who completely depend on agriculture for their food and livelihoods. Strategies and technologies for adapting to climate change in particular locations should ideally be grounded in knowledge of the future climatic conditions in those locations. Estimates hold that 70 % of future climates already exist somewhere in the world. That is where the <b>analogues</b> <b>approach</b> comes in...|$|R
40|$|The <b>analogues</b> <b>approach,</b> {{developed}} by CCAFS in R programming, {{is a novel}} way of supporting climate and crop models with on-the-ground empirical testing. In essence, the analogues tool connects sites with statistically similar (‘analogous’) climates, across space (i. e. between locations) and/or time (i. e. with past or future climates). A CCAFS dissimilarity index or Hallegatte index {{can be used to}} systematically identify climate analogues across the world, for certain regions, or among specific locations. Users may use default criteria or choose from a variety of global climate models (GCMs), scenarios, and input data. Once analogue sites are identified, information gathered from local field studies or databases can be used and compared to provide data for further studies, propose high-potential adaptation pathways, facilitate farmer-to-farmer exchange of knowledge, validate computational models, test new technologies and/or techniques, or enable us to learn from history. Users may manipulate the tool in the free, open-source R software, or access a simplified user-friendly version online...|$|R
40|$|In {{this paper}} we {{describe}} {{a part of}} our future project aiming to efficient identification of music score writers based on the automated image analysis approach. The idea to automate the process of identification of writers by analysing of music notation graphic features was developed at the Institute for Musicology at the University of Rostock. First tests of an <b>analogue</b> <b>approach</b> were applied on chosen music notations. These tests indicate that the recognition problem can be sufficiently solved only by computer-aided assistance. In our article we describe an approach for the automated image analysis and understanding as applied to digitised music scores. This approach is aimed to solve the automated identification of writers as a result of digital music notation matching...|$|E
40|$|Circum-Antarctic marine sediments {{contain a}} record of past climate and Southern Ocean {{circulation}} that both complements and considerably extends the record in the continental ice. Variations in primary biological production, reflecting changes in sea-ice cover and sea surface temperature, in bottom current strength {{and the size of}} the grounded continental ice sheet, all contribute to changes in sediment characteristics, in a record extending back m any million years. It is possible to assess both the value of the proxy record in Antarctic sediments, and the validity of the <b>analogue</b> <b>approach</b> to understanding climate change, by focusing on the last glacial cycle and, for comparison, on earlier periods that were significantly different: the Pliocene before 3 Ma ago that could provide an analogue for global warming, and the Oligocene before there was an Antarctic Circumpolar Current...|$|E
40|$|Synthetic Aperture Radar (SAR) is {{an active}} remote sensor. It utilises {{relative}} motion between an antenna and its target region, to synthesis a relatively long antenna, which can be further processed to achieve fine spatial. Pulsed Linear Frequency Modulation (LFM), or usually termed as chirp pulse, is the most commonly employed modulation scheme in SAR system. Several techniques to generate the chirp signal have been proposed {{over the past few}} decades. Generally, these techniques can be categorised into the <b>analogue</b> <b>approach</b> and the digital approach. Analogue waveform synthesis techniques are capable of generating high bandwidth signal, while digital waveform synthesis techniques allow the property of the digitally generated waveform (type of modulation, start and stop frequency, waveform output duration) be easily configurable by altering the firmware or its memory contents...|$|E
40|$|This report {{reviews the}} use of {{stepwise}} testing approaches for the prediction of skin and eye irritation and corrosion in a regulatory context. It is published as a companion report to the "Review of Literature-Based Models for Skin and Eye Irritation and Corrosion", an ECB report which reviewed the state-of-the-art of in silico and in vitro dermal and ocular irritation and corrosion human health hazard endpoints. In the former review, the focus was placed on reviewing alternative in silico approaches to assess acute local toxic effects, such as QSARs, SARs, chemical categories, and read-across and <b>analogue</b> <b>approaches.</b> Special {{emphasis was placed on}} literature-based (Q) SAR models for skin and eye irritation and corrosion and expert systems. In the present review, the emphasis is on different schemes (testing strategies) that have been conceived for the integrated use of different approaches, including in silico, in vitro and in vivo methods. JRC. I. 3 -Toxicology and chemical substance...|$|R
40|$|The Climate <b>Analogues</b> <b>approach,</b> {{developed}} by CCAFS in R programming, {{is a novel}} way of supporting climate and crop models with on-the-ground empirical testing. In essence, the analogues tool connects sites with statistically similar (‘analogous’) climates, across space (i. e. between locations) and/or time (i. e. with past or future climates). A CCAFS dissimilarity index is used to systematically identify climate analogues across the world, for certain regions, or among specific locations. Operators may use default criteria or choose {{from a variety of}} global climate models (GCMs), scenarios, and input data. Once analogue sites are identified, information gathered from local field studies or databases can be used and compared to provide data for further studies, propose high-potential adaptation pathways, facilitate farmer-to-farmer exchange of knowledge, validate computational models, test new technologies and/or techniques, or enable us to learn from history. Users may manipulate the tool in the free, open-source R software, or access a simplified user-friendly version online...|$|R
40|$|Schwarzschild {{black hole}} is the {{simplest}} black hole that is studied most in detail. Its behavior is best understood {{by looking at}} the geodesics of the particles under the influence of its gravitational field. In this paper, the focus of attention is giving a perspicuous description of the Schwarzschild geodesics by using <b>analogue</b> potential <b>approach.</b> Specifically we discuss geodesics of light and of a massive particle in the case that their angular momentum is non zero in the Schwarzschild spacetime. This discussion is done by defining analogue potentials out of geodesic equations and defining relevant dimensionless conserved quantities. Then, we designate how geodesics change in response to the change of these quantities. Our results indicate the relation between the particles' motion near black hole horizon and their angular momentum. Furthermore, we make a comparison between Newtonian Physics (NP) and General Relativity (GR) {{in the language of the}} <b>analogue</b> potential <b>approach.</b> Comment: 6 pages, 3 figure...|$|R
40|$|The {{performance}} of imaging systems containing analogue, digitizing and/or digital steps {{has been compared}} in regard to microimage quality and radiometric characteristics. It {{has been found that}} a conventional <b>analogue</b> <b>approach</b> based on the measurement of secondary film product (diapositive), a digital photogrammetric approach based on a digitized primary film product and a hypothetical digital photogrammetric camera give comparable image quality results under a number of made assumptions, the most important being the use of 7 m pixels, a medium contrast scene and residual uncorrected image motion of also 7 m. An analysis of the radiometric characteristics has led to the conclusion, that aerial photographs containing a density range not exceeding 2. 0 can be scanned to achieve a fully resolved 8 -bit density range from 0 to 2. 0 with an scanner providing a 12 -bit transmittance scan resolution (such as the Zeiss SCAI) ...|$|E
40|$|A {{new series}} of Pgp-dependent MDR inhibitors having a N,N-bis(cyclohexanol) amine {{scaffold}} was designed {{on the basis of}} the frozen <b>analogue</b> <b>approach.</b> The scaffold chosen gives origin to different geometrical isomers. The new compounds showed a wide range of potencies and efficacies on doxorubicin-resistant erythroleukemia K 562 cells in the pirarubicin uptake assay. The most interesting compounds (isomers of 3) were studied further evaluating their action on the ATPase activity present in rat small intestine membrane vesicles and doxorubicin cytotoxicity potentiation on K 562 cells. The latter assay was performed also on the isomers of 4. The four isomers of each set present different behavior in each of these tests. Compound 3 d shows the most promising properties as it was able to completely reverse Pgp-dependent pirarubicin extrusion at low nanomolar concentration, inhibited ATPase activity at 5 x 10 (- 9) and increased the cytotoxicity of doxorubicin with a reversal fold (RF) of 36. 4 at 3 microM concentration...|$|E
40|$|The {{microscopic}} ionization {{behavior of}} piroxicam was investigated using two different approaches, i. e., direct UV spectroscopy and an indirect <b>analogue</b> <b>approach</b> (deductive method). The best microscopic pK(a) values (PKa 12 = 4. 60, pK(a 21) = 5. 40, pK(a 22) = 2. 72, and pK(a 11) = 1. 92) {{were obtained by}} the deductive method using as pK(a 22) the pK(a) of the enolic O-methylated piroxicam 2. The results show remarkable electrostatic effects in the protonation/deprotonation equilibria, a marked increase in the acidity of the enolic function (2. 68 pK(a) units) being caused by the pyridinium group. The electronic structure of piroxicam was studied based on H- 1 -NMR chemical shifts at various ionization states, indicating an extended electron conjugation through the molecule. The partition measurements in octan- 1 -ol/H 2 O of zwitterionic compound 3 (the pyridyl N-methyl derivative of piroxicam (1)) suggest that the two opposite charges in zwitterionic piroxicam are indeed in a close intramolecular proximit...|$|E
40|$|Seasonal {{river flow}} {{forecasting}} methods {{are currently being}} developed for country-wide application in the United Kingdom, using several different techniques. In this paper, methods based on persistence and historical flow analogues are presented. New one- and three-month forecasts are made each month using monthly river flows at 93 stations with records at least 30 years long. The method that performs best is selected for each separate month, catchment and forecast duration. The forecasts based on persistence of the previous month’s flow generally outperform the <b>analogues</b> <b>approach,</b> particularly for slowly responding catchments (mainly in the southeast) with large underground water storage in aquifers. Historical analogues make a useful contribution to the forecasts in the northwest of the country. Correlations between hindcasts and observations that exceed 0. 23 and are significant at the 5 % level for a one-sided test are found for 81 % (70 %) of the station-month combinations for the one-month (three-month) forecast...|$|R
40|$|Discusses off-shell states {{guided by}} an <b>analogue</b> model <b>approach.</b> The authors are led, by {{algebraic}} considerations, to off-shell {{states in the}} Neveu-Schwarz-Ramond model, which obey the gauge conditions in the same critical dimension as the on-shell theory, the amplitudes factorizing on the usual positive definite states in 10 dimensions. Brief calculations reveal {{that some of the}} divergences present in the orbital model disappear in the fermion theory. (20 refs) ...|$|R
40|$|KEY WORDS: inverse photogrammetry, space resection, {{digital image}} {{processing}} Inverse photogrammetry is a technique fairly old but rather seldom used. Reasons for this are its somehow limited applicability and, mainly, the tediousness of earlier <b>analogue</b> <b>approaches.</b> Today’s digital processing techniques offer tools making its application both easier and more flexible. In this paper digital processing tools are considered and employed in order to generate more “realistic ” end products of inverse photogrammetric techniques. In this context, the practical application concerns the new buildings of our Department of Rural and Surveying Engineering at the Technical University of Athens, planned two years ago as extensions to the existing one. Suitable non-metric photographs were taken from various angles around the old building and, using inverse photogrammetry based on the final architectural design, the planned attachments were projected into them. Digital processing was employed in order to render the resulting raster products. The final products were assessed regarding their usefulness for conveying a concrete idea of “how it will look like ” and were also compared with respective photographs taken today, after {{the construction of the}} buildings. ...|$|R
40|$|Spatial {{variability}} of hydraulic conductivity exerts a predominant control on groundwater flow by influencing advective pathways, hydrodynamic dispersion, and density-dependent instabilities. Space-local spectral texture segmentation {{aids in the}} macroscale characterization of the spatial heterogeneity of natural porous media via an outcrop <b>analogue</b> <b>approach.</b> Detailed photographic data sets were obtained for a 45 m × 3 m vertical section of glacial-fluvial sand and gravel deposit in the Fanshawe Delta area (Ontario, Canada). High-resolution texture maps of the sedimentary exposure are generated using a texture segmentation routine based on the space-local S transform with the photographic data sets used as input. Geostatistical analyses of the texture maps reveal similarity between the spatial correlation structures of spectral texture and hydraulic conductivity as determined from constant-head permeameter testing of sediment cores. Conditioned on the permeameter measurements, texture maps {{can be used to}} provide local continuous estimates of the hydraulic conductivity field at a spatial resolution equal to the sediment core dimensions...|$|E
40|$|A {{literature}} has formed which has historically utilized an <b>analogue</b> <b>approach</b> {{to the study}} of prereferral intervention use and perceptions of acceptability and effectiveness. This methodology, however, may lack ecological validity if variables that mediate intervention selection and perceived efficacy are not linked to research design. If analogue research is to remain viable, supporting evidence of the concordance of naturalistic findings is expected. The current study utilizes an analogue and concomitant authentic approach to study the equivalence of these methods. A sample of 345 special education referral forms served as authentic data. Ninety-seven elementary general and special education teachers responded to an analogue survey for both academic and behavioral problems designed to mirror referral forms. Matched by grade and referral type, results suggest that while the selection of prereferral intervention strategies are not significantly different for analogue versus authentic data methods, the ratings of effectiveness do show differences according to methodology. Specifically, interventions are rated as more effective when presented via an analogue scenario compared with an equivalent authentic prereferral situation. Interventions are rated as more effective for academic referrals than for behavioral referrals. Special educators rate interventions as substantially more effective than do regular educators. In general, Process Instructional Adaptations (PIA) in the classroom are utilized with academic problems whereas Behavioral Conditioning Interventions (BCI) are applied most frequently with students with behavioral problems. A low percentage of students, however, were referred for behavioral problems in the case of authentic data. In the case of both authentic and analogue data, teachers do use interventions which they do not find to be effective and they also do not use interventions with frequency that they find to be quite effective. Self-efficacy perceptions are also related to some aspects of intervention effectiveness ratings. Teachers who demonstrate external efficacy perceptions reported lower levels of effectiveness for interventions they chose for behavioral problems while internal teachers rated interventions for students with behavioral problems to be effective. Implications with respect to the ecological validity of the <b>analogue</b> <b>approach</b> are discussed. The context of educational reform and the changing educational environment is also discussed. ...|$|E
40|$|A {{combined}} {{digital and}} <b>analogue</b> <b>approach</b> is introduced {{to establish a}} dynamic active voltage controller (AVC) for controlling an {{insulated gate bipolar transistor}} (IGBT), suitable for series connection of devices. In the AVC, the reference voltage dictates the switching trajectory of active voltage controlled IGBTs via a feedback loop. By means of employing adaptive and self-timing control methods to adjust the reference voltage profile according to the transient states on the power side, this new controller has achieved real-time optimization of the IGBT switching with lowest possible power losses. In particular, this new AVC has provided an efficient and flexible solution to addressing the diode reverse recovery and the IGBT-diode commutation during IGBT switch-on operation. The commonly seen voltage overshoot and extra power loss associated with diode reverse recovery voltage are greatly reduced in the new AVC. The optimal switching performance in experiments for both a single IGBT and IGBTs connected in series is given in this paper. This is an effective solution to IGBT control without snubber networks and shows the effectiveness of concurrent optimization of devices and circuits...|$|E
40|$|Three {{statistical}} downscaling {{methods were}} applied to NCEP/NCAR reanalysis (used as a surrogate for the best possible general circulation model), and the downscaled meteorology was used to drive a hydrologic model over California. The historic record was divided into an "observed" period of 1950 – 1976 to {{provide the basis for}} downscaling, and a "projected" period of 1977 – 1999 for assessing skill. The downscaling methods included a bias-correction/spatial downscaling method (BCSD), which relies solely on monthly large scale meteorology and resamples the historical record to obtain daily sequences, a constructed <b>analogues</b> <b>approach</b> (CA), which uses daily large-scale anomalies, and a hybrid method (BCCA) using a quantile-mapping bias correction on the large-scale data prior to the CA approach. At 11 sites we compared three simulated daily flow statistics: streamflow timing, 3 -day peak flow, and 7 -day low flow. While all downscaling methods produced reasonable streamflow statistics at most locations, the BCCA method consistently outperformed the other methods, capturing the daily large-scale skill and translating it to simulated streamflows that more skillfully reproduced observationally-driven streamflows...|$|R
40|$|Research on {{relationship}} {{management in}} digital political public relations is scarce. Departing from arelationship management perspective, then, this study {{seeks to contribute}} to the field of political public relations by investigating whether political parties take advantage of what digital media platforms offer in terms of long-term commitment and reciprocity utilizing the Swedish national election in 2010 as a case study. The results show that the political parties utilized social media outlets primarily during and just before the time of the election and that interaction between parties and constituents were scarce and shallow. All parties shared the same pattern of activity, although there were some differences in the frequencies of use. Additionally, although user commitment increased over time, there were relatively few users who chose to follow/friend the political parties, suggesting that the large majority of the voters could not easily be reached through these platforms. In essence, the results indicate that social media as a political public relations tool is, so far, dwarfed by more traditional and <b>analogue</b> <b>approaches...</b>|$|R
40|$|The use of {{a direct}} arene-exchange method for the {{synthesis}} of benzyl-tethered arene/Ru/TsDPEN complexes for use in asymmetric transfer hydrogenation is reported. A series of complexes tethered through a three-carbon linear chain was also prepared. The arene-exchange approach significantly simplifies the synthetic approach to this class of catalyst and permits the ready formation of modified <b>analogues.</b> The <b>approach</b> also provides a route to racemic catalysts for use in general reductions with either hydrogen or transfer hydrogenatio...|$|R
40|$|Snow and melting of the snowpack {{provide the}} {{principal}} {{supply of water}} {{to much of the}} Western United States. Whether global warming threatens this water supply is the focus of this research. This study builds upon a previous Global Climate Change Response Program investigation. Charts were generated of four geopotential height parameters for a domain covering the eastern North Pacific Ocean and western North America. Out of 131 total winter months (from 1946 - 89), 35 were selected as analogues. Monthly mean precipitation values for areas in western Montana, northern Utah, and east central Arizona were compared with median values for the 1946 - 89 period to determine if any significant differences existed. The results suggest that one regional impact of global warming may be a substantial reduction in wintertime precipitation in central and southern intermountain areas such as Utah and Arizona. The study also found the situation in western Montana to be unclear. Finally, a few examples are presented to highlight some of {{the strengths and weaknesses of}} the <b>analogue</b> <b>approach,</b> and several questions regarding other potential effects of global warming on winter precipitation are addressed...|$|E
40|$|This paper {{examines}} location planning as a valuation tool in retailing context. The broad aim of {{this paper}} is to ascertain the type, nature and extent to which location models are used Finnish retailers in their location decisions. More specifically, the objectives of the work are to review the literature on retail location models and ascertain whether or not Finnish retailers use any models for their location decisions. Empirical research is conducted by a survey addressed to all tenants in a large Finnish shopping centre. The aim is to find out the usage of five location planning methods; check list, analogue, financial analysis, regression and gravity model. Most of the sample retailers used the checklist analysis, <b>analogue</b> <b>approach</b> or financial analysis, in one form or another. The research indicated that the different location assessment procedures were complementary to each other, being used in sequence to maximise their overall effectiveness. According to this research, retailers operating in the target shopping centre use quite similar and relatively limited (quantitative) toolbox when making establishment decisions. The possible existence of several qualitative factors is certainly one recommended area for further research; what actually determines the establishment process if quantitative models are not used...|$|E
40|$|Recently, the multilinear PLS {{algorithm}} {{was presented}} by Bro and later implemented as a regression method in 3 D QSAR by Nilsson et al. In {{the present article}} a well-known set of (S) -N-[(1 -ethyl- 2 -pyrrolidinyl) methyl]- 6 -methoxybenzamides, with affinity towards the dopamine D- 2 receptor subtype, was utilised for the validation of the multilinear PLS method. After exhaustive conformational analyses on the ligands, the active <b>analogue</b> <b>approach</b> was employed to align them in their presumed pharmacologically active conformations, using (-) -piquindone as a template. Descriptors were then generated in the GRID program, and 40 calibration compounds and 18 test compounds were selected {{by means of a}} principal component analysis in the descriptor space. The final model was validated with different types of cross-validation experiments, e. g. leave one-our, leave-three-out and leave-five-out. The cross-validated Q(2) was 62 % for all experiments, confirming the stability of the model. The prediction of the test set with a predicted Q(2) of 62 % also established the predictive ability. Finally, the conformations and the alignment of the ligands in combination with multilinear PLS, obviously, played an important role for the success of our model...|$|E
40|$|Read-across is a {{data gap}} filling {{technique}} used within category and <b>analogue</b> <b>approaches.</b> It has been utilized {{as an alternative}} approach to address information requirements under various past and present regulatory {{programs such as the}} OECD High Production Volume Programme as well as the EU's Registration, Evaluation, Authorisation and restriction of CHemicals (REACH) regulation. Although read-across raises a number of expectations, many misconceptions still remain around what it truly represents; how to address its associated justification in a robust and scientifically credible manner; what challenges/issues exist in terms of its application and acceptance; and what future efforts are needed to resolve them. In terms of future enhancements, read-across is likely to embrace more biologically-orientated approaches consistent with the Toxicity in the 21 st Century vision (Tox- 21 c). This Food for Thought article, which is notably not a consensus report, aims to discuss a number of these aspects and, in doing so, to raise awareness of the ongoing efforts and activities to enhance read-across. It also intends to set the agenda for a CAAT read-across initiative in 2014 - 2015 to facilitate the proper use of this technique...|$|R
40|$|Abstract. Biological metal {{centers that}} consist of two {{fragments}} covalently connected by one more bridging atoms or groups {{are becoming increasingly}} recognized by physicochemical properties and protein crystallography. As a class, bridged biological metal assemblies pose challenging problems in chemical synthesis; at least some are potentially subject to further structural, electronic, and reactivity characterization provided the assembly itself or a close molecular simulation thereof can be prepared. Synthetic <b>analogue</b> <b>approaches</b> to three bridged assemblies are summarized here: nitro-genase FeMoN cofactor, the catalytic site of sulfitelnitrite reductase, and the heterometal CuFe site in cytochrome c oxidase. The cofactor cluster is approached by the clusters [MFe 4 S,(PEtJ 4 L] (M = Mo, V; L = Cl', RS-), in which a cuboidal Fe,S, unit {{is linked to the}} M site by three p 2 -S atoms. The sulfitelnitrite site analogue consists of an Fe 4 S, cluster linked to a heme group through an unsupported p 2 -S bridge. The binuclear site in the oxidized form of cytochrome c oxidase has been investigated by synthesis of the unit [Fe"O-Cun] in a molecular heme complex. The cyanide-inhibited form of the enzyn-e has been simulated by the preparation of a series of heme complexes containing the bridge unit [Fern-CN-Cun] in which the Fe atom is six-coordinate and low-spin...|$|R
40|$|This paper {{presents}} an effective <b>analogue</b> equalization <b>approach</b> {{to enhance the}} data modulation bandwidth of an {{organic light emitting diode}} (OLED) used for lighting and communications. The proposed approach is experimentally demonstrated to overcome the effect of the OLED's large capacitance induced by the large emitting area panel, thus offering higher data transmission rate for visible light communications system. The measured results show that a six time increase of bandwidth can be achieved and the large-size OLED can be used for on-off-keying non-return-to-zero data communications up to 2. 15 Mbit/s with the given bandwidth of 150 KHz...|$|R
