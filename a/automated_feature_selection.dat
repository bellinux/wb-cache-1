38|10000|Public
40|$|Accurate {{segmentation}} {{of brain}} white matter hyperintensities (WMHs) {{is important for}} prognosis and disease monitoring. To this end, classifiers are often trained – usually, using T 1 and FLAIR weighted MR images. Incorporating additional features, derived from diffusion weighted MRI, could improve classification. However, the multitude of diffusion-derived features requires selecting the most adequate. For this, <b>automated</b> <b>feature</b> <b>selection</b> is commonly employed, which can often be sub-optimal. In this work, we propose a different approach, introducing a semi-automated pipeline to select interactively features for WMH classification. The advantage of this solution is {{the integration of the}} knowledge and skills of experts in the process. In our pipeline, a Visual Analytics (VA) system is employed, to enable user-driven feature selection. The resulting features are T 1, FLAIR, Mean Diffusivity (MD), and Radial Diffusivity (RD) – and secondarily, Cs and Fractional Anisotropy (FA). The next step in the pipeline is to train a classifier with these features, and compare its results to a similar classifier, used in previous work with <b>automated</b> <b>feature</b> <b>selection.</b> Finally, VA is employed again, to analyze and understand the classifier performance and results...|$|E
40|$|In {{this study}} we have {{developed}} a supervised learning to automatically detect with high accuracy EEG reports that describe seizures and epileptiform discharges. We manually labeled 3, 277 documents as describing one or more seizures vs no seizures, and as describing epileptiform discharges vs no epileptiform discharges. We then used Naïve Bayes to develop a system able to automatically classify EEG reports into these categories. Our system consisted of normalization techniques, extraction of key sentences, and <b>automated</b> <b>feature</b> <b>selection</b> using cross validation. As candidate features we used key words and special word patterns called elastic word sequences (EWS). Final feature selection was accomplished via sequential backward selection. We used cross validation to predict out of sample performance. Our <b>automated</b> <b>feature</b> <b>selection</b> procedure resulted in a classifier with 38 features for seizure detection, and 23 features for epileptiform discharge detection. The average [95 % CI] area under the receiver operating curve was 99. 05 [98. 79, 99. 32]% for detecting reports with seizures, and 96. 15 [92. 31, 100. 00]% for detecting reports with epileptiform discharges. The methodology described herein greatly reduces the manual labor involved in identifying large cohorts of patients for retrospective neurophysiological studies of patients with epilepsy...|$|E
40|$|Abstract. The {{selection}} of gene subsets that retain high predictive accuracy for certain cell-type classification, poses a central problem in microarray data anal}^is. The appHcation and combination of various computational intelligence methods holds a great promise for <b>automated</b> <b>feature</b> <b>selection</b> and classification. In this paper, {{we present a}} new approach based on evolutionary algorithms that addresses the problem of very high dknensionahty of the data, by automatically selecting subsets of the most informative genes. The evolutionary algorithm is driven by a neural network classifier. Extensive experiments indicate that the proposed approach is both effective and reliable. ...|$|E
40|$|<b>Feature</b> <b>selection</b> is an {{important}} challenge in machine learning. Unfortunately, most methods for <b>automating</b> <b>feature</b> <b>selection</b> are designed for supervised learning tasks and are thus either inapplicable or impractical for reinforcement learning. This paper presents {{a new approach to}} <b>feature</b> <b>selection</b> specifically designed for the challenges of reinforcement learning. In our method, the agent learns a model, represented as a dynamic Bayesian network, of a factored Markov decision process, deduces a minimal feature set from this network, and efficiently computes a policy on this feature set using dynamic programming methods. Experiments in a stock-trading benchmark task demonstrate that this approach can reliably deduce minimal feature sets and that doing so can substantially improve performance and reduce the computational expense of planning...|$|R
40|$|Abstract—Feature {{selection}} {{is an important}} challenge in machine learning. Unfortunately, most methods for <b>automating</b> <b>feature</b> <b>selection</b> are designed for supervised learning tasks and are thus either inapplicable or impractical for reinforcement learning. This paper presents {{a new approach to}} <b>feature</b> <b>selection</b> specifically designed for the challenges of reinforcement learning. In our method, the agent learns a model, represented as a dynamic Bayesian network, of a factored Markov decision process, deduces a minimal feature set from this network, and efficiently computes a policy on this feature set using dynamic programming methods. Experiments in a stock-trading benchmark task demonstrate that this approach can reliably deduce minimal feature sets and that doing so can substantially improve performance and reduce the computational expense of planning. Keywords-Reinforcement learning; feature selection; factored MDPs I...|$|R
40|$|<b>Feature</b> <b>selection</b> is {{the process}} of finding the set of inputs to a machine {{learning}} algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to <b>automate</b> <b>feature</b> <b>selection</b> rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the <b>feature</b> <b>selection</b> problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the <b>feature</b> <b>selection</b> task it faces is made increasingly difficult...|$|R
40|$|International audienceA {{new method}} for <b>automated</b> <b>feature</b> <b>selection</b> is introduced. The {{application}} domain {{of this technique}} is fault diagnosis, where robust features are needed for modeling the wear level and therefore diagnosing it accurately. A robust feature in this field is one that exhibits a strong correlation with the wear level. The proposed method aims at selecting such robust features, {{while at the same}} time ascertain that they are as weakly correlated to each other as possible. The results of this technique on the extracted features for a real-world problem appear to be promising. It is possible to make use of the proposed technique for other feature selection applications, with minor adjustments to the original algorithm...|$|E
40|$|Over {{a century}} of {{research}} {{has resulted in a}} set of more than a hundred binary association measures. Many of them share similar properties. An overview of binary association measures is presented, focused on their order equivalences. Association measures are grouped according to their relations. Transformations between these measures are shown, both formally and visually. A generalization coefficient is proposed, based on joint probability and marginal probabilities. Combining association measures is one of recent trends in computer science. Measures are combined in linear and nonlinear discrimination models, <b>automated</b> <b>feature</b> <b>selection</b> or construction. Knowledge about their relations is particularly important to avoid problems of meaningless results, zeroed generalized variances, the curse of dimensionality, or simply to save tim...|$|E
40|$|Abstract. For {{statistical}} modelling of multivariate binary data, such as text documents, datum instances are typically represented as vectors over a global vocabulary of attributes. Apart from {{the issue of}} high dimensionality, this also faces us {{with the problem of}} uneven importance of various attribute presences/absences. This problem has been largely overlooked in the literature, however it may create difficulties in obtaining reliable estimates of unsupervised probabilistic representation models. In turn, the problem of <b>automated</b> <b>feature</b> <b>selection</b> and feature weighting in the context of unsupervised learning is challenging, because there is no known target to guide the search. In this paper we propose and study a relatively simple cluster-based generative model for multivariate binary data, equipped with automated feature weighting capability. Empirical results on both synthetic and real data sets are given and discussed. ...|$|E
40|$|Abstract. <b>Feature</b> <b>selection</b> is {{the process}} of finding the set of inputs to a machine {{learning}} algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to <b>automate</b> <b>feature</b> <b>selection</b> rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine the right set of inputs for the networks it evolves. By learning the network’s inputs, topology, and weights simultaneously, FS-NEAT addresses the <b>feature</b> <b>selection</b> problem without relying on meta-learning or labeled data. Initial experiments in a line orientation task demonstrate that FS-NEAT can learn networks with fewer inputs and better performance than traditional NEAT. Furthermore, it outperforms traditional NEAT even when the feature set does not contain extraneous features because it searches for networks in a lower-dimensional space. ...|$|R
40|$|Given {{sensors to}} detect object use, commonsense priors of object usage in {{activities}} {{can reduce the}} need for labeled data in learning activity models. It is often useful, however, to understand how an object is being used, i. e., the action performed on it. We show how to add personal sensor data (e. g., accelerometers) to obtain this detail, with little labeling and <b>feature</b> <b>selection</b> overhead. By synchronizing the personal sensor data with object-use data, it is possible to use easily specified commonsense models to minimize labeling overhead. Further, combining a generative common sense model of activity with a discriminative model of actions can <b>automate</b> <b>feature</b> <b>selection.</b> On observed activity data, automatically trained action classifiers give 40 / 85 % precision/recall on 10 actions. Adding actions to pure object-use improves precision/recall from 76 / 85 % to 81 / 90 % over 12 activities. ...|$|R
40|$|ABSTRACT <b>Feature</b> <b>selection</b> is {{the process}} of finding the set of inputs to a machine {{learning}} algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to <b>automate</b> <b>feature</b> <b>selection</b> rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the <b>feature</b> <b>selection</b> problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the <b>feature</b> <b>selection</b> task it faces is made increasingly difficult. Categories and Subject Descriptors I. 2. 6 [Artificial Intelligence]: Learning [...] Connectionism and neural net...|$|R
40|$|Considerable {{energy savings}} in {{industrial}} environment are possible {{in an industrial}} environment by detecting installations not working at their optimum operating point. The present paper proposes a new generalized data driven FDD method capable of automatically detecting the abnormal energy demand {{of different types of}} installations or machines based on process data. The paper contains a comprehensive overview of the research, focusing on a trade-off between performance and computing time together with minimizing the human input. The proposed method contains an <b>automated</b> <b>feature</b> <b>selection,</b> a hyper-parameter optimization of the chosen SVM regression algorithm and a residual control algorithm. The method was tested in several industrial installations and two case studies are presented to demonstrate the performance of the proposed method, while underlining the significance of a decent number of relevant features. status: publishe...|$|E
40|$|We {{performed}} <b>automated</b> <b>feature</b> <b>selection</b> for multi-stream (i. e., ensemble) automatic speech recognition, using a hill-climbing (HC) algorithm {{that changes}} one feature {{at a time}} if the change improves a performance score. For both clean and noisy data sets (using the OGI Numbers corpus), HC usually improved performance on held out data compared to the initial system it started with, even for noise types that were not seen during the HC process. Overall, we found that using Opitz’s scoring formula, which blends single-classifier word recogni-tion accuracy and ensemble diversity, worked better than en-semble accuracy as a performance score for guiding HC in cases of extreme mismatch between the SNR of training and test sets. Our noisy version of the Numbers corpus, our multi-layer-perceptron-based Numbers ASR system, and our HC scripts are available online. Index Terms: speech recognition, feature selection, ensemble 1...|$|E
40|$|For many feature {{selection}} problems, a human denes {{the features that}} are potentially useful, and then a subset is chosen from the original pool of features using an <b>automated</b> <b>feature</b> <b>selection</b> algorithm. In contrast to supervised learning, class information is not available to guide the feature search for unsupervised learning tasks. In this paper, we introduce Visual-FSSEM (Visual Feature Subset Selection using Expectation-Maximization Clustering), which incorporates visualization techniques, clustering, and user interaction to guide the feature subset search and to enable {{a deeper understanding of}} the data. Visual-FSSEM, serves both as an exploratory and multivariate-data visualization tool. We illustrate Visual-FSSEM on a high-resolution computed tomography lung image data set. 1. INTRODUCTION Most research in unsupervised clustering assumes that when creating the target data set, the data analyst in conjunction with the domain expert was able to identify a small relevant set of [...] ...|$|E
40|$|In this paper, we {{identify}} two {{issues involved in}} developing an <b>automated</b> <b>feature</b> subset <b>selection</b> algorithm for unlabeled data: the need for finding the number of clusters in conjunction with <b>feature</b> <b>selection,</b> {{and the need for}} normalizing the bias of <b>feature</b> <b>selection</b> criteria with respect to dimension. We explore the <b>feature</b> <b>selection</b> problem and these issues through FSSEM (<b>Feature</b> Subset <b>Selection</b> using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for <b>feature</b> <b>selection,</b> the need for addressing these two issues, and the effectiveness of our proposed solutions...|$|R
40|$|Natural Sciences and Engineering Research Council of Canada (NSERC); National Natural Science Foundation of China (NSFC) [41071257, 41001305]Using high-spatial-resolution {{multispectral}} imagery alone is insufficient for achieving highly accurate and reliable thematic mapping of urban areas. Integration of lidar-derived elevation information into image classification can considerably improve classification results. Additionally, traditional pixel-based classifiers have some limitations {{in regard to}} certain landscape and data types. In this study, we take advantage of current advances in object-based image analysis and machine learning algorithms to reduce manual image interpretation and <b>automate</b> <b>feature</b> <b>selection</b> in a classification process. A sequence of image segmentation, <b>feature</b> <b>selection,</b> and object classification is developed and tested by the data sets in two study areas (Mannheim, Germany and Niagara Falls, Canada). First, {{to improve the quality}} of segmentation, a range image of lidar data is incorporated in an image segmentation process. Among features derived from lidar data and aerial imagery, the random forest, a robust ensemble classifier, is then used to identify the best features using iterative feature elimination. On the condition that the number of samples is at least two or three times the number of features, a segmentation scale factor has no particular effect on the selected features or classification accuracies. The results of the two study areas demonstrate that the presented object-based classification method, compared with the pixel-based classification, improves by 0. 02 and 0. 05 in kappa statistics, and by 3. 9 % and 4. 5 % in overall accuracy, respectively...|$|R
40|$|Classification {{problems}} map {{an object}} {{to a particular}} class using a set of available features. The problem is how to choose the best subset of features that provide an accurate classification. Genetic algorithms may provide a novel yet powerful search method to <b>automate</b> <b>feature</b> subset <b>selection</b> on a classification problem. Two different types of genetic algorithms are compared on this domain. One of them, GENESIS, {{is based on the}} traditional simple genetic algorithm. The second, CHC, uses truncation selection, an adaptive uniform crossover operator and restarts. CHC and GENESIS have been used several times for different types of problems and demonstrate very good performance. Our results indicate that CHC yields better results on two applications. We also introduce a novel classifier, Euclidean Decision Tables. ...|$|R
40|$|ECCBR), {{is held in}} Alessandria, Italy, from July 19 through July 22, 2010. The Workshop Program enables dissemination, demonstration, and {{discussion}} of research in progress, facilitating interaction, feedback, and collaboration {{at the forefront of}} CBR research and development. Five workshops are included in this year’s program. Case-Based Reasoning for Computer Games: This is the third in a series of highly successful workshops focusing on CBR in computer gaming environments. The six papers included in this volume present work in meta-reasoning for adaptive behavior, case extraction from game replay repositories, <b>automated</b> <b>feature</b> <b>selection,</b> reinforcement learning, generic intelligent gaming frameworks, and domain-independent learning. Provenance-Aware CBR: Applications to Reasoning, Metareasoning, Maintenance and Explanation: New this year is an exploration of case provenance and its roles in trust and reputation, reasoning and metareasoning, and explanation. Provenance encompasses the sources of cases, case acquisition contexts...|$|E
40|$|It is {{estimated}} that {{ninety percent of the}} world’s species have yet to be discovered and described. The main reason for the slow pace of new species description is that the science of taxonomy, as traditionally practiced, can be very laborious. To formally describe a new species, taxonomists have to manually gather and analyze data from large numbers of specimens, often from broad geographic areas, and identify the smallest subset of external body characters that uniquely diagnoses the new species as distinct from all its known relatives. In this paper, we use an <b>automated</b> <b>feature</b> <b>selection</b> and classification approach to address the taxonomic impediment in new species discovery. The proposed computational framework can identify body shape characters that unite populations within species, as well as distinguishing among species. It also provides statistical “clues” for assisting taxonomists to identify new species or subspecies. 1...|$|E
40|$|This paper {{developed}} a CAD (Computer Aided Diagnosis) {{system based on}} neural network and a proposed feature selection method. The proposed feature selection method is Maximum Difference Feature Selection (MDFS). Digital mammography is reliable method for early detection of breast cancer. The most important step in breast cancer diagnosis is feature selection. Computer <b>automated</b> <b>feature</b> <b>selection</b> is reliable and also it helps to improve the classification accuracy. GLCM (Gray Level Co-occurrence Matrix) features are extracted from the mammogram. The extracted features are selected based on a proposed MDFS method. Experiments have been conducted on datasets from DDSM (Digital database for Screening Mammography) database. Several feature selection methods are available. The accuracy of the model depends on the relevant feature selection. The proposed MDFS method selects only essential features and eliminates the irrelevant features. The experiment results show that neural network based model with proposed feature selection method improved the classification accuracy...|$|E
40|$|This paper {{addresses}} {{the issue of}} "algorithm vs. representation" for case-based learning of linguistic knowledge. We first present empirical evidence {{that the success of}} case-based learning methods for natural language processing tasks depends to a large degree on the feature set used to describe the training instances. Next, we present a technique for <b>automating</b> <b>feature</b> set <b>selection</b> for case-based learning of linguistic knowledge. Given as input a baseline case representation, the method modifies the representation in response to a number of predefined linguistic biases by adding, deleting, and weighting features appropriately. We apply the linguistic bias approach to <b>feature</b> set <b>selection</b> to the problem of relative pronoun disambiguation and show that the casebased learning agorithm improves as relevant biases are incorporated into the underlying instance representation. Finally, we argue that the linguistic bias approach to <b>feature</b> set <b>selection</b> offers new possibilities for case-based learning of natural language: it simplifies the process of instance representation design and, in theory, obviates the need for separate instance representations for each linguistic knowledge acquisition task. More importantly, the approach offers a mechanism for explicitly combining the frequency information available from corpus-based techniques with linguistic bias information employed in traditional linguistic and knowledge-based approaches to natural language processing...|$|R
40|$|We {{applied a}} method called Distinction-Sensitive Learning Vector Quantization (DSLVQ) to the {{classification}} of footsteps. The measurements were made by a pressure-sensitive floor, {{which is part of}} the smart sensing living room in our research laboratory. The aim is to identify walkers based on their single footsteps. DSLVQ is an extended version of Learning Vector Quantization (LVQ), and it can be used for <b>automated</b> <b>feature</b> scaling and <b>selection</b> during the training of an LVQ codebook. The method shows improvements in the classification accuracies compared to a standard LVQ. In addition, due to its capability of automated input pruning, discarding the non-informative features, it was able to detect automatically the most significant features from a large set of features. This is important in an adaptive identification system, where the informative features might change. 1...|$|R
40|$|Abstract Introduction Breast {{cancer is}} the first {{leading cause of death}} for women in Brazil as well as in most {{countries}} in the world. Due to the relation between the breast density and the risk of breast cancer, in medical practice, the breast density classification is merely visual and dependent on professional experience, making this task very subjective. The {{purpose of this paper is}} to investigate image features based on histograms and Haralick texture descriptors so as to separate mammographic images into categories of breast density using an Artificial Neural Network. Methods We used 307 mammographic images from the INbreast digital database, extracting histogram features and texture descriptors of all mammograms and selecting them with the K-means technique. Then, these groups of selected features were used as inputs of an Artificial Neural Network to classify the images automatically into the four categories reported by radiologists. Results An average accuracy of 92. 9 % was obtained in a few tests using only some of the Haralick texture descriptors. Also, the accuracy rate increased to 98. 95 % when texture descriptors were mixed with some features based on a histogram. Conclusion Texture descriptors have proven to be better than gray levels features at differentiating the breast densities in mammographic images. From this paper, it was possible to <b>automate</b> the <b>feature</b> <b>selection</b> and the classification with acceptable error rates since the extraction of the features is suitable to the characteristics of the images involving the problem...|$|R
40|$|<b>Automated</b> <b>feature</b> <b>selection</b> is {{important}} for text categorization to reduce the feature size and {{to speed up the}} learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination (MD) and MD-χ^ 2 methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches. Comment: This paper has been submitted to the IEEE Trans. Knowledge and Data Engineering. 14 pages, 5 figure...|$|E
40|$|Abstract—Biologists {{could make}} {{valuable}} {{use of the}} wealth of specimen information in natural history museum databases. “Taxonomy via the Internet ” aims to build a centralized database where biologists can store, manipulate and retrieve biologically meaningful data from images of specimens and use the data to classify the specimens taxonomically. Multimedia information representation provides a new computational tool for extracting useful features from large databases of specimen images and has potential to expedite the pace of taxonomic research. In this paper, we use a taxonomic problem involving species of suckers in the genus Carpiodes {{to demonstrate the utility}} of this method. Logistic regression classifier with fully <b>automated</b> <b>feature</b> <b>selection</b> procedure is compared with the best landmark based classifier to illustrate how image quality affects classification accuracy. We discuss the need of creating a multimedia database using images of specimens from a fish collection. Keywords—taxonomy; feature selection; logistric regression; shape analysis; multimedia representation I...|$|E
40|$|This paper {{proposes a}} feature {{selection}} technique for software clustering {{which can be}} used in the architecture recovery of software systems. The recovered architecture can then be used in the subsequent phases of software maintenance, reuse and re-engineering. A number of diverse features could be extracted from the source code of software systems, however, some of the extracted features may have less information to use for calculating the entities, which result in dropping the quality of software clusters. Therefore, further research is required to select those features which have high relevancy in finding associations between entities. In this article first we propose a supervised feature selection technique for unlabeled data, and then we apply this technique for software clustering. A number of feature subset selection techniques in software architecture recovery have been proposed. However none of them focus on <b>automated</b> <b>feature</b> <b>selection</b> in this domain. Experimental results on three software test systems reveal that our proposed approach produces results which are closer to the decompositions prepared by human experts, as compared to those discovered by the well-known K-Means algorithm. 12 page(s...|$|E
40|$|The basic {{procedure}} executed by an AFIS (Automated Fingerprint Identification System) is stated. Figures of merit regarding crime countermeasures and security system applications {{in terms of}} notions from benchmark tests will be discussed. It is demonstrated how some most promising approaches of <b>automated</b> <b>feature</b> extraction as referenced in the current literature fit into the context of both the science of fingerprints an of advanced basic principles of pattern recognition. A continuation of the present paper, {{to be published in}} one of the next numbers of this journal, will be devoted to the problems of global <b>feature</b> <b>selection</b> and matching...|$|R
40|$|Generating discriminative input {{features}} {{is a key}} {{requirement for}} achieving highly accurate classifiers. The process of generating features from raw data is known as feature engineering and it can take significant manual effort. In this paper we propose <b>automated</b> <b>feature</b> engineering to derive a suite of additional features from a given set of basic features {{with the aim of}} both improving classifier accuracy through discriminative features, and to assist data scientists through automation. Our implementation is specific to HTTP computer network traffic. To measure the effectiveness of our proposal, we compare the performance of a supervised machine learning classifier built with <b>automated</b> <b>feature</b> engineering versus one using human-guided features. The classifier addresses a problem in computer network security, namely the detection of HTTP tunnels. We use Bro to process network traffic into base features and then apply <b>automated</b> <b>feature</b> engineering to calculate a larger set of derived features. The derived features are calculated without favour to any base feature and include entropy, length and N-grams for all string features, and counts and averages over time for all numeric <b>features.</b> <b>Feature</b> <b>selection</b> is then used to find the most relevant subset of these features. Testing showed that both classifiers achieved a detection rate above 99. 93 % at a false positive rate below 0. 01 %. For our datasets, we conclude that <b>automated</b> <b>feature</b> engineering can provide the advantages of increasing classifier development speed and reducing development technical difficulties through the removal of manual feature engineering. These are achieved while also maintaining classification accuracy...|$|R
40|$|Morphometrics from images, image analysis, {{may reveal}} {{differences}} between classes of objects {{present in the}} images. We have performed an image-features-based classification for the pathogenic yeast Cryptococcus neoformans. Building and analyzing image collections from the yeast under different environmental or genetic conditions may help to diagnose a new “unseen ” situation. Diagnosis here means that retrieval of the relevant information from the image collection is at hand each time a new “sample ” is presented. The basidiomycetous yeast Cryptococcus neoformans can cause infections such as meningitis or pneumonia. The presence of an extra-cellular capsule {{is known to be}} related to virulence. This paper reports on the approach towards developing classifiers for detecting potentially more or less virulent cells in a sample, i. e. an image, by using a range of features derived from the shape or density distribution. The classifier can henceforth be used for automating screening and annotating existing image collections. In addition we will present our methods for creating samples, collecting images, image preprocessing, identifying “yeast cells ” and creating feature extraction from the images. We compare various expertise based and fully <b>automated</b> methods of <b>feature</b> <b>selection</b> and benchmark a range of classification algorithms and illustrate successful application to this particular domain...|$|R
40|$|A time-frequency contour {{extraction}} and classiﬁcation algorithm {{was created}} to analyze humpback whale vocalizations. The algorithm automatically extracted contours of whale vocalization units by searching for gray-level discontinuities in the spectrogram images. The unit-to-unit similarity was quantiﬁed by cross-correlating the contour lines. A library of distinctive humpback units was then generated by applying an unsupervised, cluster-based learning algorithm. The {{purpose of this study}} was to provide a fast and <b>automated</b> <b>feature</b> <b>selection</b> tool to describe the vocal signatures of animal groups. This approach could beneﬁt a variety of applications such as species description, identiﬁcation, and evolution of song structures. The algorithm was tested on humpback whale song data recorded at various locations in Hawaii from 2002 to 2003. Results presented in this paper showed low probability of false alarm (0 %– 4 %) under noisy environments with small boat vessels and snapping shrimp. The classiﬁcation algorithm was tested on a controlled set of 30 units forming six unit types, and all the units were correctly classiﬁed. In a case study on humpback data collected in the Auau Chanel, Hawaii, in 2002, the algorithm extracted 951 units, which were classiﬁed into 12 distinctive types...|$|E
40|$|Political {{scientists}} {{in general and}} public law specialists in particular have only recently begun to exploit text classification using machine learning techniques to enable the reliable and detailed content analysis of political/ legal documents on a large scale. This article provides an overview and assessment of this methodology. We describe the basics of text classification, suggest applications of the technique to enhance empirical legal research (and political science more broadly), and report results of experiments designed to test {{the strengths and weaknesses}} of alternative approaches for classifying the positions and interpreting the content of advocacy briefs submitted to the U. S. Supreme Court. We find that the Wordscores method (introduced by Laver et al. 2003), and various models using a Naïve Bayes classifier, perform well at accurately classifying the ideological direction of amicus curiae briefs submitted in the Bakke (1978) and Bollinger (2003) affirmative action cases. We also find that <b>automated</b> <b>feature</b> <b>selection</b> tech-niques can enable the detection of disparate issue conceptualizations by opposing sides in a single case, and facilitate analysis of relative linguistic “reliance ” and “dominance ” over time. We conclude by discussing th...|$|E
40|$|The {{application}} of machine learning techniques to psychiatric neuroimaging offers the pos-sibility to identify robust, reliable and objective disease biomarkers {{both within and}} between contemporary syndromal diagnoses that could guide routine clinical practice. The use of quantitative methods to identify psychiatric biomarkers is consequently important, particu-larly {{with a view to}} making predictions relevant to individual patients, rather than at a group-level. Here, we describe predictions of treatment-refractory depression (TRD) diagnosis using structural T 1 -weighted brain scans obtained from twenty adult participants with TRD and 21 never depressed controls. We report 85 % accuracy of individual subject diagnostic prediction. Using an <b>automated</b> <b>feature</b> <b>selection</b> method, the major brain regions support-ing this significant classification were in the caudate, insula, habenula and periventricular grey matter. It was not, however, possible to predict the degree of ‘treatment resistance ’ in individual patients, at least as quantified by the Massachusetts General Hospital (MGH-S) clinical staging method; but the insula was again identified as a region of interest. Structural brain imaging data alone can be used to predict diagnostic status, but not MGH-S staging, {{with a high degree of}} accuracy in patients with TRD...|$|E
40|$|The {{objectives}} {{of this project}} were to develop a ROI (Region of Interest) detector using Haar-like feature similar to the face detection in Intel's OpenCV library, implement it in Matlab code, and test {{the performance of the}} new ROI detector against the existing ROI detector that uses Optimal Trade-off Maximum Average Correlation Height filter (OTMACH). The ROI detector included 3 parts: 1, <b>Automated</b> Haar-like <b>feature</b> <b>selection</b> in finding a small set of the most relevant Haar-like features for detecting ROIs that contained a target. 2, Having the small set of Haar-like features from the last step, a neural network needed to be trained to recognize ROIs with targets by taking the Haar-like features as inputs. 3, using the trained neural network from the last step, a filtering method needed to be developed to process the neural network responses into a small set of regions of interests. This needed to be coded in Matlab. All the 3 parts needed to be coded in Matlab. The parameters in the detector needed to be trained by machine learning and tested with specific datasets. Since OpenCV library and Haar-like feature were not available in Matlab, the Haar-like feature calculation needed to be implemented in Matlab. The codes for Adaptive Boosting and max/min filters in Matlab could to be found from the Internet but needed to be integrated to serve the purpose of this project. The performance of the new detector was tested by comparing the accuracy and the speed of the new detector against the existing OTMACH detector. The speed was referred as the average speed to find the regions of interests in an image. The accuracy was measured by the number of false positives (false alarms) at the same detection rate between the two detectors...|$|R
40|$|Abstract It is {{estimated}} that {{ninety percent of the}} world’s species have yet to be discovered and described. The main reason for the slow pace of new species description is that the science of taxonomy can be very laborious. To formally describe a new species, taxonomists have to manually gather and analyze data from large numbers of specimens and iden-tify the smallest subset of external body characters that uniquely diagnoses the new species as distinct from all its known rel-atives. In this paper, we present an <b>automated</b> <b>feature</b> selec-tion and classification scheme using logistic regression with controlled false discovery rate to address the taxonomic re-search need impediment in new species discovery. Unlike tra-ditional taxonomic practice, our scheme automatically selects body shape features from specimen samples with landmarks that unite populations within species, as well as distinguish-ing among species. It also provides probabilistic assessment of the classification accuracy using the selected features in identifying new species. We apply the scheme to a taxonomic problem involving species of suckers in the genus Carpiodes. The results confirm the necessity of <b>feature</b> <b>selection</b> for clas-sifier design and provide additional insight on the suspicious specimens which have traditionally been misdiagnosed as C. carpio but are in fact more close to C. cyprinus. We also com-pare the classification accuracy of our scheme with several well-known machine learning algorithms without and with <b>feature</b> <b>selection.</b> Key words <b>Feature</b> <b>selection,</b> false discovery rate, logistic regression, taxonomy, systematics. ...|$|R
40|$|This paper {{proposes a}} {{two-level}} <b>feature</b> <b>selection</b> to improves Naïve Bayes with kernel density estimation. The {{performance of the}} proposed <b>feature</b> <b>selection</b> is evaluated on question item set based on Bloom's cognitive levels. This two-level <b>feature</b> <b>selection</b> contains of filter and wrapper based <b>feature</b> <b>selection.</b> This paper uses chi square and information gain as the filter based <b>feature</b> <b>selection</b> and forward <b>feature</b> <b>selection</b> and backward <b>feature</b> elimination as the wrapper based <b>feature</b> <b>selection.</b> The result shows that the two-level <b>feature</b> <b>selection</b> improves the Naïve Bayes with kernel density estimation. The combination of chi square and backward feature elimination give more optimal quality than the other combination. IEEE The 5 th International Conference on Information Technology and Electrical Engineering (ICITEE), 2013 	 [URL]...|$|R
