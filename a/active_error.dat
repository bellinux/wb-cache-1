54|95|Public
5000|$|<b>Active</b> <b>Error</b> Flag: six {{dominant}} bits - Transmitted by a node detecting {{an error}} {{on the network}} that is in error state [...] "error active".|$|E
50|$|All {{fields in}} the frame are stuffed with the {{exception}} of the CRC delimiter, ACK field and end of frame which are a fixed size and are not stuffed. In the fields where bit stuffing is used, six consecutive bits of the same type (111111 or 000000) are considered an error. An <b>active</b> <b>error</b> flag can be transmitted by a node when an error has been detected. The <b>active</b> <b>error</b> flag consists of six consecutive dominant bits and violates the rule of bit stuffing.|$|E
5000|$|Passive Error Flag: six {{recessive}} bits - Transmitted by a node detecting an <b>active</b> <b>error</b> {{frame on}} the network that is in error state [...] "error passive".|$|E
50|$|<b>Active</b> <b>errors</b> {{occur at}} the sharp end of the process. The effect of <b>active</b> <b>errors</b> are felt almost immediately. <b>Active</b> <b>errors</b> could be; making a course change at the wrong position, pushing an {{incorrect}} button, forgetting to close a valve. Latent errors {{occur at the}} blunt end. These are errors, removed in both time and space from the operators at the sharp end, that may lie dormant within the system for a long time. Examples of latent errors may include; equipment design flaws that make the human-machine interface less than intuitive, or organizational flaws, such as staffing and training decisions made for fiscal reasons increasing the likelihood of errors. Latent errors are often unrecognized and {{have the capacity to}} result in multiple types of <b>active</b> <b>errors.</b> Analyses of major accidents involving many different areas of society indicate that latent errors pose the greatest risk to safety in a complex system. Such accidents include Three Mile Island accident, Heysel Stadium disaster, Bhopal disaster, Chernobyl disaster, Space Shuttle Challenger disaster, King's Cross fire, Piper Alpha and MS Herald of Free Enterprise.|$|R
5000|$|HFACS {{is based}} in the [...] "Swiss Cheese" [...] model of human error which looks at four levels of <b>active</b> <b>errors</b> and latent failures, {{including}} unsafe acts, preconditions for unsafe acts, unsafe supervision, and organizational influences. It is a comprehensive human error framework, that folded Reason's ideas into the applied setting, defining 19 causal categories within four levels of human failure.|$|R
40|$|Iatrogenic {{errors and}} patient safety in {{clinical}} processes are an increasing concern. The quality of process information in hardcopy or electronic form can heavily influence clinical behaviour {{and decision making}} errors. Little work has been undertaken to assess the safety impact of clinical process planning documents guiding the clinical actions and decisions. This paper investigates the clinical process documents used in elective surgery {{and their impact on}} latent and <b>active</b> clinical <b>errors.</b> Eight clinicians from a large health trust underwent extensive semi- structured interviews to understand their use of clinical documents, and their perceived impact on errors and patient safety. Samples of the key types of document used were analysed. Theories of latent organisational and <b>active</b> <b>errors</b> from the literature were combined with the EDA semiotics model of behaviour and decision making to propose the EDA Error Model. This model enabled us to identify perceptual, evaluation, knowledge and action error types and approaches to reducing their causes. The EDA error model was then used to analyse sample documents and identify error sources and controls. Types of knowledge artefact structures used in the documents were identified and assessed in terms of safety impact. This approach was combined with analysis of the questionnaire findings using existing error knowledge from the literature. The results identified a number of document and knowledge artefact issues that give rise to latent and <b>active</b> <b>errors</b> and also issues concerning medical culture and teamwork together with recommendations for further work...|$|R
50|$|The {{start of}} an {{overload}} frame due to case 1 is only {{allowed to be}} started at the first bit time of an expected intermission, whereas overload frames due to case 2 start one bit after detecting the dominant bit. Overload Flag consists of six dominant bits. The overall form corresponds {{to that of the}} <b>active</b> <b>error</b> flag. The overload flag’s form destroys the fixed form of the intermission field. As a consequence, all other stations also detect an overload condition and on their part start transmission of an overload flag. Overload Delimiter consists of eight recessive bits. The overload delimiter is of the same form as the error delimiter.|$|E
40|$|Abstract — Good quantum codes, such as quantum MDS codes, are {{typically}} nondegenerate, meaning that errors of small weight require active error-correction, which is—paradoxically—itself prone to errors. Decoherence free subspaces, {{on the other}} hand, do not require <b>active</b> <b>error</b> correction, but perform poorly in terms of minimum distance. In this paper, examples of degenerate quantum codes are constructed that have better minimum distance than decoherence free subspaces and allow some errors of small weight {{that do not require}} <b>active</b> <b>error</b> correction. In particular, two new families of [[n, 1, ≥ √ n]]q degenerate quantum codes are derived from classical duadic codes. I...|$|E
40|$|Kinship Analysis {{requires}} learning {{definitions of}} all kinship terms {{used in a}} target culture from data that is gathered incrementally through interviews with informants. We exploit a collection of previously learned kinship definitions from different cultures (logical “domain theories”) to minimize the cost of learning the definitions in a new domain theory. We use Transfer Learning and Inductive Logic Programming (ILP) to learn kinship definitions. We propose a novel method for identifying potential errors in the data by comparing our data with definitions in previously learned models of similar cultures. We actively ask informants to confirm or correct these potential errors. This <b>Active</b> <b>Error</b> Correction is enhanced by computing the similarity between the target domain theory {{and those of other}} cultures, and using these similarities to guide the search for definitions. Our similarity-guided <b>Active</b> <b>Error</b> Correction learns more than 90 % of the kin terms, even when training examples are up to 19 % incorrect. Suppressing error correction reduces learning accuracy by 29 %. Because <b>Active</b> <b>Error</b> Correction correctly identifies almost all of the errors in the data for human correction, it reduces the need to collect further data by 20 %. Furthermore, Similarity-Guided Search reduces repeat data requests of the User by 40 %. The combination of <b>Active</b> <b>Error</b> Correction and Similarity-Guided Search can be used to reduce data requirements for ILP learning problems. As a side benefit, this work has produced a machine readable collection of 50 kinship domain theories that may provide broader benefits in anthropology, by surfacing interesting anthropological issues. ...|$|E
40|$|In this paper, we {{identify}} 8 {{methods used to}} measure errors and adverse events {{in health care and}} discuss their strengths and weaknesses. We focus on {{the reliability and validity of}} each, as well as the ability to detect latent errors (or system <b>errors)</b> versus <b>active</b> <b>errors</b> and adverse events. We propose a general framework to help health care providers, researchers, and administrators choose the most appropriate methods to meet their patient safety measurement goals. KEY WORDS: medical error; adverse events; patient safety; measurement...|$|R
50|$|In {{standards}} of RT-component, RTC must have 4 {{states such as}} CREATED, INACTIVE, <b>ACTIVE,</b> and <b>ERROR.</b> When the state changes, corresponding event-handlers are called by the execution context which manages the RTCs' state machine.|$|R
5000|$|Wee is {{a protein}} that {{operates}} at the G2 to metaphase checkpoint. Wee becomes <b>active</b> if <b>errors</b> {{occur in the}} DNA synthesis phase. It blocks entry into metaphase until the problem is resolved. Like Rb, wee becomes inactive when hyperphosphorylated.|$|R
40|$|Using {{a rhythmic}} task where human {{subjects}} bounced a ball with a handheld racket, fine-grained analyses {{of stability and}} variability extricated contributions from open-loop control, noise strength, and <b>active</b> <b>error</b> compensation. Based on stability analyses of a stochastic-deterministic model of the task—a surface contacting the ball by periodic movements—open-loop or dynamic stability was assessed by the acceleration of the racket at contact. Autocovariance analyses of model and data were further used to gauge the contributions of open-loop stability and noise strength. Variability and regression analyses estimated <b>active</b> <b>error</b> compensation. Empirical results demonstrated that experienced actors exploited open-loop stability more than novices, had lower noise strength, and applied more <b>active</b> <b>error</b> compensations. By manipulating the model parameter coefficient of restitution, task stability was varied and showed that actors graded these three components {{as a function of}} task stability. It is concluded that actors tune into task stability when stability is high but use more active compensation when stability is reduced. Implications for the neural underpinnings for passive stability and active control are discussed. Further, results showed that stability and variability are not simply the inverse of each other but contain more quantitative information when combined with model analyses...|$|E
40|$|We {{consider}} {{a class of}} decoding algorithms that are applicable to error correction for both Abelian and non-Abelian anyons. This class includes multiple algorithms that have recently attracted attention, including the Bravyi-Haah RG decoder. They are applied to both the problem of single shot error correction (with perfect syndrome measurements) and that of <b>active</b> <b>error</b> correction (with noisy syndrome measurements). For Abelian models we provide a threshold proof in both cases, showing {{that there is a}} finite noise threshold under which errors can be arbitrarily suppressed when any decoder in this class is used. For non-Abelian models such a proof is found for the single shot case. The means by which decoding may be performed for <b>active</b> <b>error</b> correction of non-Abelian anyons is studied in detail. Differences with the Abelian case are discussed...|$|E
40|$|A {{supporting}} {{investigation of}} human operator errors is discussed. Existing classification systems (taxonomies) are mostly {{used to analyze}} the accident reports, and not for investigating the accidents themselves. These systems provide appropriate taxonomy primarily for Reason’s latent conditions and not so much for active errors. A model incorporating the <b>active</b> <b>error</b> characteristics of underlying cognitive sources along with the classification system for them is presented. It describes a human operator error as a group of frames adjusted to the activity of different operators, such as pilot or air traffic controller. Our model of <b>active</b> <b>error</b> and its classification system, combined with existing latent conditions classification systems, constitute a powerful tool – a decision support system for investigating human errors. Additionally, our model {{can be used as a}} basis for defining algorithms to analyze human performance in complex man-machine systems...|$|E
30|$|For a {{description}} of the FPM’s working principals, let us consider the general case of multiple detected and hence <b>active</b> driving <b>errors.</b> The logics of the FPM can best be understood by starting with the functionalities of the output and timing module.|$|R
50|$|Whenever the {{dorsal area}} was <b>active,</b> fewer <b>errors</b> were {{committed}} providing {{more evidence that}} the ACC is involved with effortful performance. The second finding showed that, during error trials, the ACC activated later than for correct responses, clearly indicating a kind of evaluative function.|$|R
30|$|According to Reason (1990) system’s {{accidents}} {{have their}} primary origin in fallible {{decisions made by}} designers and high-level management. The key {{factors that contribute to}} fallible decisions are safety and production goals which in turn are affected by money, equipment, personnel, and available time. An accident occurs when an unsafe act is committed {{in the presence of a}} potential hazard for which latent failures from decision makers, psychological precursors, and the defence coincide. Reason uses the word unpredictable to describe the coincidence of latent and <b>active</b> <b>errors</b> that cause an accident, which suggest quantification is not very meaningful. Similarly a review of 1000 shipping accidents concluded that accidents resulted from highly complex coincidences which could rarely be foreseen by the people involved (Reason 1990).|$|R
40|$|Software rarely {{works as}} {{intended}} {{while it is}} being written. Things go wrong {{in the midst of}} everyday practice, and developers are commonly understood to form theories and strategies for dealing with them. Errors in this sense are not bugs left behind in software, they are actively encountered and experienced. This paper reports findings of an ethnographically-informed study undertaken to examine error encountered at the desk. Films depicting paired open-source development practice {{over the course of a}} month were analyzed to identify and delineate instances of <b>active</b> <b>error.</b> Instances were interpreted within a framework of error handling drawn from psychology research. Analyses of representative instances are given and discussed in relation to software engineering research that examines practice at the desk. Findings demonstrate that the significance of <b>active</b> <b>error</b> in software development is personal, shaped by passing time, the emergence of preferred practices and environmental changes...|$|E
40|$|We {{describe}} a novel method for long distance quantum communication in realistic, lossy photonic channels. The method uses single emitters of light as intermediate nodes in the channel. One electronic spin and one nuclear spin {{degree of freedom}} associated with each emitter provide quantum memory and enable <b>active</b> <b>error</b> correction. We show that these two degrees of freedom, coupled via the contact hyperfine interaction, suffice to correct arbitrary errors, making our protocol robust to all realistic sources of decoherence. The method is particularly well suited for implementation using recently-developed solid-state nano-photonic devices...|$|E
40|$|Motion-induced {{errors in}} strapdown {{inertial}} navigation systems are discussed. The errors generated in single-degree-of-freedom gyros and accelerometers are treated in great detail. These sensor errors are related to errors at the system level and common pulse rebalance techniques are compared. Since instrument single transmission characteristics {{are found to be}} important, describing function analysis is applied to nonlinear pulse torqued inertial sensor loops, and the results are compared with simulations. Two means for reducing motion-induced errors are explored: (1) selection of inertial sensor parameters, and (2) <b>active</b> <b>error</b> compensation by measuring the environment, computing the errors generated, and correcting for them...|$|E
40|$|Training {{and dealing}} with errors or {{mistakes}} in medical practical procedures The complexity of treatment, procedures, interventions, and workload of modern, inpatient paediatric and neonatal care provides a setting where errors may, potentially, have serious adverse consequences for our patients. For the pur-pose of this article, an error is defined as clinical perform-ance which deviates from an ideal and, as a result, could (or does) lead to an accident or an iatrogenic incident. 1 2 <b>Active</b> <b>errors</b> are those that immediately precede an adverse event and latent errors are factors inherent to a system (for example, heavy workload, inadequate maintenance of equipment, or the prevailing professional culture) that pro-vide {{the conditions in which}} an accident is inevitable if given the right set of circumstances. Since all physician...|$|R
50|$|RT-components {{also have}} state, so the RT-components behaves as state machines. The states that RT-components can have are CREATED, INACTIVE, <b>ACTIVE,</b> and <b>ERROR.</b> States and {{behaviors}} {{are controlled by}} the execution-context. If developers {{want to change the}} behavior of their RT-components, the execution-context can be replaced at runtime.|$|R
30|$|The {{priority}} of the driving errors is determined dynamically by the prioritization module. It receives <b>active</b> <b>errors</b> from the driving error detection and reads saved ones from the data memory. An algorithm assigns a rating score to each error. The higher the score, the higher the {{priority of}} the error. The rating score consists of a constant term, which is specific for each driving error, and a dynamic term. While the constant term reflects {{the importance of the}} driving error with regard to its influence on fuel consumption as well as its criticality regarding a transmission in a timely manner, the dynamic term of the score will increase over time. The dynamic rating of a saved driving error is set to an even higher score, when its status changes from active to inactive (i.e., the very same error is not detected by the driving error detection anymore). By doing so, newly inactive, saved errors are handled with a higher priority in comparison to <b>active</b> driving <b>errors</b> and are thus more likely to be issued to the driver before expiration. Only after the advisory message of a corresponding driving error has been submitted to the display unit or the error has been considered inactive for too long, the driving error is removed from the data memory. As a result, the rating score is reset to its constant value.|$|R
40|$|The {{positioning}} accuracy of computer numer- ical control (CNC) machine tools is mainly {{limited by the}} manufacturing accuracy of their linear and circular motion axes and by the long-term dimensional stability of their structures. Maximizing this accuracy can {{prove to be a}} particularly challenging task, especially for large-sized systems. In fact, heat-induced deformations, long-period deformation of foundations and the manu- facturing process itself, these all cause time-dependent structural deformations of the machine body, which are difficult to model and to predict. The usual approach is a model-based prediction of structural deformations, which is followed by a compensation of positioning er- rors at CNC level. This approach is often limited by the complexity of the problem from both geometrical (sys- tem geometry can be very complex and it can vary in time) and physical (it is difficult to model and consider any possible load type and loading condition) point of view. As a consequence, only limited success has been achieved in <b>active</b> <b>error</b> compensation based on the modelling {{of the relationship between the}} generalized dynamic loads and the structural deformation field. This paper illustrates a different approach in <b>active</b> <b>error</b> compensation, which exploits a new measurement system able to provide real-time measurement of the displacement field of a given structural component, P. Bosetti (B) · S. Bruschi Department of Mechanical and Structural Engineering, University of Trento, 38123 Trento, Italy e-mail: paolo. bosetti@ing. unitn. it without any model about its dynamic/thermal structural behavior...|$|E
30|$|Reason (1997) further distinguishes error by <b>active</b> <b>error</b> whose {{effects are}} felt {{immediately}} and latent error whose adverse consequences may lie dormant {{within the system}} for a long time. In general active errors are associated with front-line operators while latent errors are caused by decision makers and management separated in time and space. Detailed analyses of accidents in complex systems such as nuclear power plants or industrial sites reveal that latent errors pose {{the greatest threat to}} safety. Examples of latent failures relevant to fire safety are the corroding sprinklers of Piper Alpha and the inability to realise the fire risk in London metro (Reason 1990; Akselsson 2011).|$|E
40|$|We {{present a}} unified {{approach}} to quantum error correction, called operator quantum error correction. This scheme {{relies on a}} generalized notion of noiseless subsystems that is not restricted to the commutant of the interaction algebra. We arrive at the unified approach, which incorporates the known techniques [...] i. e. the standard error correction model, the method of decoherence-free subspaces, and the noiseless subsystem method [...] as special cases, by combining <b>active</b> <b>error</b> correction with this generalized noiseless subsystem method. Moreover, we demonstrate that the quantum error correction condition from the standard model is {{a necessary condition for}} all known methods of quantum error correction. Comment: 5 page...|$|E
40|$|Motivation – Designers make {{decisions}} that later influence how users work with the systems that they have designed. When errors occur in use, {{it is tempting to}} focus on the <b>active</b> <b>errors</b> rather than on the latent design decisions that framed the context of error, and fixing latent conditions can have a more general (and future) impact than addressing particular active failures. Research approach – A constructive computer science approach is used, and results from a simulation reported. Research limitations – Error is a complex multidisciplinary field; this paper makes a new contribution complimentary to human factors engineering. Take away message – This paper shows that latent design decisions cause serious problems (including fatalities) in safety critical applications; the paper proposes UI discovery tools to identify and manage latent errors. UI discovery enables human factors engineers and programmers to work together to help eliminate broad classes of latent design errors...|$|R
40|$|In this survey-style {{paper we}} {{demonstrate}} {{the usefulness of}} the probabilistic modelling framework in solving not only the actual positioning problem, but also many related problems involving issues like calibration, <b>active</b> learning, <b>error</b> estimation and tracking with history. We also point out some interesting links between positioning research done in the area of robotics and in the area of wireless radio networks. ...|$|R
40|$|This paper briefly revises {{three books}} written by James Reason (namely «Human Error», «Managing {{the risks of}} {{organizational}} accidents», and «The human contribution»), a worldwide eminent expert of human error and organizational ergonomics. Through these books {{it is possible to}} understand how safety management in high risk socio-technical systems required a perspective shift in the last decades, concerning both safety culture and safety management practices, especially after enormous, tragic catastrophes. This fundamental change can be comprehended thanks to several important aspects: among them, the spreading of «organizational» models of accidents (which try to explain errors by focusing not only on first line operators), the adoption of a systemic perspective also in relation to preventive measures of safety management, and the consideration of latent conditions able to promote the inevitable <b>active</b> <b>errors</b> in operational contexts. All these aspects are discussed here, inviting the reader to the integral reading of the books reviewed, now translated to Spanish and published by Modus Laborandi, Madrid...|$|R
40|$|This paper {{describes}} {{research in}} active instruments for enhanced accuracy in microsurgery. The {{aim is to}} make accuracy enhancement as transparent to the surgeon as possible. Rather than using a robotic arm, we have taken the novel approach of developing a handheld instrument that senses its own movement, distinguishes between desired and undesired motion, and deflects its tip to perform active compensation of the undesired component. The research has therefore required work in quantification and modeling of instrument motion, filtering algorithms for tremor and other erroneous movements, and development of handheld electromechanical systems to perform <b>active</b> <b>error</b> compensation. The paper introduces the systems developed in this research and presents preliminary results...|$|E
40|$|An {{important}} bottleneck in spatial spectrum estimation {{theory is}} array errors. A new error self-calibration method is presented {{on the basis}} of <b>active</b> <b>error</b> calibration and online self-calibration theories. An algorithm combining Direction Of Arrival (DOA) estimation and array complex gain error calibration is developed using an auxiliary source without accurate spatial location information and numerical optimization calculation; the error calibration parameters can be used in the following spatial spectrum estimation. The method has a higher veracity than the active correction algorithm and is an offline calculation procedure, having the advantage of small computation amount, similar to the online self-calibration method, and avoiding accurate spatial location information in classical active calibration method...|$|E
40|$|We {{investigate}} effective noise {{channels for}} encoded quantum systems {{with and without}} <b>active</b> <b>error</b> correction. Noise acting on physical qubits forming a logical qubit is thereby described as a logical noise channel acting on the logical qubits, {{which leads to a}} significant decrease of the effective system dimension. This provides us with a powerful tool to study entanglement features of encoded quantum systems. We demonstrate this framework by calculating lower bounds on the lifetime of distillable entanglement and the negativity for encoded multipartite qubit states with different encodings. At the same time, this approach leads to a simple understanding of the functioning of (concatenated) error correction codes. Comment: 10 pages, 6 figure...|$|E
30|$|A review {{concerning}} {{road tunnel}} fire safety and risk is presented. In particular different perspectives and methods on safety and risk are discussed. Road tunnel fire safety usually involves high uncertainty and high-stakes decisions. Thus, a wider group of stakeholders and {{different types of}} knowledge {{should be included in}} the fire safety analysis and evaluation, than what is required by technical risk analyses. It is argued that the decision process should not be separated from the design and safety evaluation as they are strongly dependent and iterative processes. Decision theory can guide the design and decision process in negotiation with stakeholders. Key parameters for the decision can be analysed through a combination of functional requirements, societal and political values, safety engineering, safety factors and systems theory. By taking an organisational viewpoint, potential latent and <b>active</b> <b>errors</b> can be analysed and a good safety culture can be engineered. In order to improve the safety culture of truck companies, regulation ensuring proper maintenance, training and quality management may be necessary in a competitive global economy.|$|R
30|$|A BP burst-correcting {{convolution}} code (6, 5, 11) {{is constructed}} [16] {{for use with}} a fault-tolerant processing situation. A rate 1 / 3 (3, 1, 10) code is chosen from a standard text [16] which have a constraint parameter m = 10. Long simulations involving 250, 000 blocks of data {{over a wide range}} of variances are performed. For the rate 1 / 3 code, this represented 750, 000 samples, while for the rate 5 / 6 code case it implied 1.5 million samples. Burst and errors within each block are permitted. A burst in this context means that the standard deviations of all components in a block are raised to 10 % of the maximum standard deviation. On the other hand, when a burst is not <b>active,</b> <b>errors</b> are allowed with positions within a block chosen independently at random, and those selected had their standard deviations raised to 10 % of their maxima. The probability of a burst is 5 × 10 - 3, while intra block errors have probability 10 - 3. For long simulations, the basic parameter σ 2 (variance of error) is changed from 10 - 9 up to 3.2.|$|R
40|$|The {{purpose of}} {{accident}} investigation {{is to identify}} and organize {{a description of the}} process through which interacting elements and conditions result in an unfortunate event. Unfortunately, if accident investigators lack a cohesive definition of human factors issues, the investigation into human performance may be unsystematic, incomplete, or leave significant points unresolved. This may, in turn, hinder scientific analysis of the data from which future principles or procedural rules can be revealed. Case in point is the classification of Crew Resource Management (CRM) accidents in aviation. We analyzed the National Transportation Safety Board’s (NTSB) aviation accident data from 1990 - 2000 for CRM failure, based in part on James Reason’s theory of system disasters. We identified 95 human performance accidents involving flightcrews in large air carrier operations (FAR Part 121). Examination of the NTSB-assigned sequence of events revealed that 27 % of accidents involved a CRM-related latent error; however, an additional 16 % involved <b>active</b> <b>errors</b> committed by the flightcrew, not additionally coded as a latent crew failure. Without consistent and systematic classification of error from the initial investigation, the final analysis of CRM-related failures may ultimately be underrepresented, and thus the concept of CRM may remain ill-defined...|$|R
