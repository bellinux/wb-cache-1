12|46|Public
5000|$|LISA (Language for Instruction Set Architectures) is a {{language}} {{to describe the}} instruction set architecture of a processor. LISA captures the information required to generate software tools (compiler, <b>assembler,</b> <b>instruction</b> set simulator, ...) and implementation hardware (in VHDL or Verilog) of a given processor.|$|E
5000|$|... {{but that}} {{transform}} requires logarithm, square root, sine and cosine functions. On some processors, the cosine and sine {{of the same}} argument can be calculated in parallel using a single instruction. [...] Notably for Intel-based machines, one can use fsincos <b>assembler</b> <b>instruction</b> or the expi instruction (available e.g. in D), to calculate complex ...|$|E
5000|$|The {{basic form}} {{requires}} two multiplications, 1/2 logarithm, 1/2 square root, and one trigonometric function for each normal variate. On some processors, the cosine and sine {{of the same}} argument can be calculated in parallel using a single instruction. Notably for Intel-based machines, one can use the fsincos <b>assembler</b> <b>instruction</b> or the expi instruction (usually available from C as an intrinsic function), to calculate complex ...|$|E
5000|$|... manual static {{analysis}} {{techniques such as}} counting <b>assembler</b> <b>instructions</b> for each function, loop etc. and then combining them.|$|R
5000|$|The {{reference}} card contained details of all <b>assembler</b> <b>instructions</b> and other 360 [...] "essential facts" [...] condensed {{to a very}} convenient fold-up, pocket sized format:- ...|$|R
5000|$|Machine {{dependent}} optimizations: optimizations {{that depend}} {{on the details of}} the CPU architecture that the compiler targets. A prominent example is peephole optimizations, which rewrites short sequences of <b>assembler</b> <b>instructions</b> into more efficient instructions.|$|R
5000|$|One Operating System {{used in the}} {{key-to-disk}} system, {{was called}} FML-11 - which meant Field Modified Level 11. This OS was an interrupt driven, multitasking OS with individual device drivers for the peripheral hardware. The computer hardware did not support stacks, so subroutines were called by executing a [...] "return Jump" [...] RTJ <b>assembler</b> <b>instruction.</b> This instruction modified the code {{at the beginning of}} the call function and inserted the return address. Reentrancy was achieved by the use of a double linked list of buffers, which was maintained by the scheduler.|$|E
50|$|The {{floating}} point arithmetic modules were both multi-stage processors which were driven by explicit instructions. In the two-stage adder an <b>assembler</b> <b>instruction</b> such as FADD DX,DY would load values from data pads DX and DY into stage one of the adder. A subsequent FADD instruction {{would be required to}} present the result at the adder's output. This second FADD could be a dummy with no arguments, or it could be the next calculation in a sequence. In this fashion a stream of FADD operations could be performed in a pipeline, with a new result in every instruction cycle though every addition requires two cycles.|$|E
50|$|While the {{contents}} of the UCB has changed as MVS evolved, the concept has not. It is a representation to the channel command processor of an external device. Inside every UCB is a representation of a subchannel information block, that is used in the SSCH <b>assembler</b> <b>instruction</b> (put in the IRB, for input, or put in the ORB, for output), to start a chain of channel commands, known as CCWs. CCWs are queued onto the UCB with the STARTIO macro interface, although that reference does NOT discuss the STARTIO macro as that macro instruction is NOT an IBM-supported interface, not withstanding the fact that that interface has remained the same for at least the past three decades. The STARTIO interface will either start the operation immediately, should the Channel Queue be empty, or it will queue the request on the Channel Queue for deferred execution. Such deferred execution will be initiated immediately when the request is {{at the head of the}} queue and the device becomes available, even if another program is in control at that instant. Such is the basic design of Input/Output Supervisor (IOS).|$|E
5000|$|<b>Assembler</b> <b>instructions.</b> {{sometimes}} termed directives {{on other}} systems, are {{a request to}} the assembler to perform various operations during the assembly. For instance, CSECT means [...] "start a section of code here"; DC defines a constant {{to be placed in}} the object code.|$|R
5000|$|Just {{as there}} are {{multiple}} data models for 64-bit architectures, the 16-bit Intel architecture allows for different memory models—ways to access a particular memory location. The reason for using a specific memory model {{is the size of}} the <b>assembler</b> <b>instructions</b> or required storage for pointers. Compilers of the 16-bit era generally had the following type-width characteristic: ...|$|R
40|$|Design {{documentation}} and user documentation for function algorithms for the Massively Parallel Processor (MPP) are presented. The contract specifies development of MPP <b>assembler</b> <b>instructions</b> {{to perform the}} following functions: natural logarithm; exponential (e to the x power); square root; sine; cosine; and arctangent. To fulfill {{the requirements of the}} contract, parallel array and solar implementations for these functions were developed on the PDP 11 / 34 Program Development and Management Unit (PDMU) that is resident at the MPP testbed installation located at the NASA Goddard facility...|$|R
40|$|The Maximum Parsimony problem aims at reconstructing a {{phylogenetic tree}} from DNA, RNA or protein {{sequences}} while minimizing {{the number of}} evolutionary changes. Much work has been devoted by the Computer Science community to solve this NP-complete problem and many techniques have been used or designed in order to decrease the computation time necessary to obtain an acceptable solution. In this paper we report an improvement of {{the evaluation of the}} Fitch function for Maximum Parsimony using AVX 2 <b>assembler</b> <b>instruction</b> of Intel (TM)  processor...|$|E
30|$|At boot time, {{the devices}} {{are not yet}} {{configured}} {{and they do not}} know their own identifier. Indeed, the manufacturer does not know the bus on which the device will be plugged. However, when the firmware looks for all available devices (using the <b>assembler</b> <b>instruction</b> mov to read the PCI Express space mapped in memory), each memory access is processed by the host bridge. The host bridge then translates this access into a PCI Express configuration request which is routed to the corresponding bus. In particular, this request contains the identifier of the contacted device. If this device is available in the system, it receives this request and then knows its identifier. This step is important to allow a device to communicate.|$|E
40|$|In this article, {{the problem}} of finding a tight {{estimate}} on the worst-case execution time (WCET) of a hard realtime program is addressed. The analysis is focused on straight-line code (without loops and recursive function calls) which is quite commonly found in synthesised code for embedded systems. A comprehensive timing analysis system covering both low-level (<b>assembler</b> <b>instruction</b> level) as well as high-level aspects (programming language level) is presented. The low-level analysis covers all speed-up mechanisms used for modern superscalar processors: pipelining, instruction-level parallelism and caching. The high-level analysis uses {{the results from the}} low-level to compute the final estimate on the WCET. This is done by a heuristic for searching the longest really executable path in the control flow, based on the functional dependencies between various program parts...|$|E
50|$|Basic {{assembler}} language {{did not support}} macros.Later assembler versions allowed the programmer to group instructions together into macros and add them to a library, which can then be invoked in other programs, usually with parameters, like the preprocessor facilities in C and related languages. Macros can include conditional <b>assembler</b> <b>instructions,</b> such as AIF (an IF construct), used to generate different code according to the chosen parameters. That makes the macro facility of this assembler very powerful. While multiline macros in C are an exception, macro definitions in assembler can easily be hundreds of lines.|$|R
50|$|The <b>assembler</b> accepts <b>instruction</b> mnemonics, data {{declarations}} and directives and constructs {{an object}} file containing information readily understandable by the CPU {{of the target}} processor, in particular code instructions coded in binary.|$|R
5000|$|Microdata {{produced}} {{computers in}} which the microcode is accessible to the user; this allows the creation of custom <b>assembler</b> level <b>instructions.</b> Microdata's Reality operating system design makes extensive use of this capability.|$|R
40|$|Data Type {{with certain}} {{fundamental}} operations, e. g., initialize, insert, and retrieve. Conceptually, all insertions occur before any retrievals. 1 It {{is a useful}} data structure for representing static search sets. Static search sets occur frequently in software system applications. Typical static search sets include compiler reserved words, <b>assembler</b> <b>instruction</b> opcodes, and built-in shell interpreter commands. Search set members, called keywords, are inserted into the structure only once, usually during program initialization, and are not generally modified at run-time. Numerous static search structure implementations exist, e. g., arrays, linked lists, binary search trees, digital search tries, and hash tables. Different approaches offer trade-offs between space utilization and search time efficiency. For example, an $n$ element sorted array is space efficient, though the average-case time complexity for retrieval operations using binary search is proportional to $"log n$. Converse [...] ...|$|E
40|$|In this article, {{the problem}} of finding a tight {{estimate}} on the worst-case execution time (WCET) of a real-time program is addressed. The analysis is focused on straight-line code (i. e. code without loops and recursive function calls) which is quite commonly found in synthesised code of hard real-time embedded systems. The analysis exploits the very simple structure of these programs, resulting in a considerable processing time improvement compared to general-case analysis techniques. A comprehensive timing analysis system, called the Program Timing Analyser (PTA), covering low-level aspects (on the <b>assembler</b> <b>instruction</b> level) as well as high-level aspects (on the programming language level) is presented. On one hand the low-level analysis covers all speed-up mechanisms used for modern superscalar processors: pipelining, instruction-level parallelism and caching. It can handle a unified cache as well as separate caches for data and instructions. The pipelined and parallel execution of [...] ...|$|E
40|$|International audienceFault {{tolerance}} is {{an essential}} requirement for critical programming systems, due to potential catastrophic consequences of faults. Several approaches to evaluate system reliability parameters exist today; however, their work {{is based on the}} assumptions that hardware and software failures happen independently. The challenge in this field is {{to take into account the}} hardware-software interactions in the evaluation of the model. In the continuity of the CETIM project (Belhadaoui et al. 2007) whose principal objective is to define an integrated design of dependable mechatronic systems, this work evaluates important reliability parameters of an embedded application in a stack processor architecture using two dynamic models. The first one (stack processor emulator (Jallouli et al. 2007)) allows the study of dynamic performance and the evaluation of a fault-tolerant technique. The second one (information flow approach (Hamidi et al. 2005)) evaluates the failure probability for each <b>assembler</b> <b>instruction</b> and for some program loops. The main objective is to estimate the failure probability of the whole application. The hierarchically modelling with the information flow approach makes it possible to evaluate the efficiency of protection program loops. These loops ensure the fault tolerance policy by recovering imminent failures and allow the application to run successfully thanks to a permanent software recover mechanism: in case of a detected and not corrected error, the system returns to the last faultless state. This work is useful because it allows adjusting the architecture and shows the advantages of the hardware-software interactions during the co-design phase before the hardware implementation. It puts the hand on the critical points in term of reliability thanks to the scenarios of critical failure paths in the processor architecture...|$|E
2500|$|Some {{assemblers}} {{also support}} simple built-in macro-instructions that generate {{two or more}} machine instructions. For instance, with some Z80 <b>assemblers</b> the <b>instruction</b> [...] is recognized to generate [...] followed by [...] These are sometimes known as pseudo-opcodes.|$|R
40|$|Based on the vectorised and cache {{optimised}} kernel, {{a parallel}} lower upper decomposition {{with a novel}} communication avoiding pivoting scheme is developed to solve dense complex matrix equations generated by the method of moments. The fine-grain data rearrangement and <b>assembler</b> <b>instructions</b> are adopted to reduce memory accessing times and improve CPU cache utilisation, which also facilitate vectorisation of the code. Through grouping processes in a binary tree, a parallel pivoting scheme is designed to optimise the communication pattern and thus reduces the solving time of the proposed solver. Two large electromagnetic radiation problems are solved on two supercomputers, respectively, and the numerical results demonstrate that the proposed method outperforms those in open source and commercial libraries...|$|R
40|$|Morpheus is a {{special-purpose}} {{programming language}} that facilitates the efficient implementation of communication protocols. Protocols {{are divided into}} three categories, called shapes, {{so that they can}} inherit code and data structures based on their category; the programmer implements a particular protocol by refining the inherited structure. Morpheus optimization techniques reduce per-layer overhead on time-critical operations to a few <b>assembler</b> <b>instructions</b> even though the protocol stack is not determined until runtime. This supports divide-andconquer simplification of the programming task by minimizing the penalty for decomposing complex protocols into combinations of simpler protocols. 1 Introduction Network software is difficult to design and implement. As with any distributed concurrent program with complex functionality, correctness is difficult to achieve. This situation is exacerbated by the additional requirement of high performance. This paper introduces a new approach to [...] ...|$|R
50|$|A {{number of}} undocumented {{instructions}} and flags were discovered by two software engineers, Wolfgang Dehnhardt and Villy M. Sorensen {{in the process}} of developing an 8085 <b>assembler.</b> These <b>instructions</b> use 16-bit operands and include indirect loading and storing of a word, a subtraction, a shift, a rotate, and offset operations.|$|R
40|$|This paper {{presents}} {{some aspects}} of implementing a User-Mode Linux with as few changes to the original Linux kernel as possible. To port a Linux kernel to a User-Mode environment, basically {{all parts of the}} kernel directly interacting with the hardware must be changed. To accomplish this, we need an environment which simulates some hardware parts. This includes simulation of interfaces to device controllers such as keyboard, IDE, graphics or network controller and other devices such as the real-time-clock. Secondly a solution must be found for replacing the assembler code contained within the original kernel with functions. The implementation of the functions, which are called instead of the assembler code, should not become part of the kernel, but be part of the User-Mode environment, just like the implementation of the <b>assembler</b> <b>instructions</b> {{is not part of the}} kernel but part of the CPU...|$|R
40|$|Morpheus is {{special-purpose}} {{programming language}} that facilitates the efficient implementation of communication protocols. Protocols {{are divided into}} three categories, called shapes, {{so that they can}} inherit code and data structures based on their category; the programmer implements a particular protocol by refining the inherited structure. Morpheus optimization techniques reduce per-layer overhead on time-critical operations to a few <b>assembler</b> <b>instructions</b> even though the protocol stack is not determined until runtime. This supports divide-and-conquer simplification of the programming task by minimizing the penalty for decomposing complex protocols into combinations of simpler protocols. July 2, 1992 Department of Computer Science The University of Arizona Tucson, AZ 85721 1 This work DARPA Contract DABT 63 - 91 -C- 0030. 1 Introduction Network software is difficult to design and implement. As with any distributed concurrent program with complex functionality, correctness is difficult [...] ...|$|R
5000|$|Since ALGOL 68-R didn't compile to {{standard}} ICL semicompiled (link-ready) format {{it was necessary}} to extend the language to provide features in ALGOL 68-R to write code that would normally be written in <b>assembler.</b> Machine <b>instructions</b> could be written inside code ... edoc sections and the address manipulation operators inc, dec, dif, as were added.|$|R
2500|$|A {{channel program}} is a {{sequence}} of channel command words (CCWs) which are executed by the I/O channel subsystem in the IBM System/360 and subsequent architectures. [...] A channel program consists {{of one or more}} channel command words. [...] The operating system signals the I/O channel subsystem to begin executing the channel program with a SSCH (start sub-channel) instruction. [...] The central processor is then free to proceed with non-I/O instructions until interrupted. [...] When the channel operations are complete, the channel interrupts the central processor with an I/O interruption. [...] In earlier models of the IBM mainframe line, the channel unit was an identifiable component, one for each channel. In modern mainframes, the channels are implemented using an independent RISC processor, the channel processor, one for all channels. IBM System/370 Extended Architecture and its successors replaced the earlier SIO (start I/O) and SIOF (start I/O fast release) <b>assembler</b> <b>instructions</b> (System/360 and early System/370) with the SSCH (start sub-channel) instruction (late System/370 and successors).|$|R
40|$|Real time kernels are {{implemented}} today in software or {{in a separate}} processor. One of the disadvantages of current implementation approaches is that the execution times for the Service Calls have a minimum and a maximum value. The time gap can be large, and in real time systems the worst case time {{is one of the}} factors which determine the utilisation factor of the system. The execution time variations are dependent on the longest and shortest execution paths in the source code, the execution time for the <b>assembler</b> <b>instructions</b> and load variations such as number of tasks, interrupts, timeout queues etc. The real time Kernel in hardware is designated Real Time Unit (RTU). The RTU is designed to reduce the time gap between best and worst cases for the kernel's service calls and administration to zero. Real time administration such as scheduling of tasks and time queue handling is performed outside the CPU and therefore no clock tick administration routine interrupts executes in the CPU [...] . ...|$|R
40|$|The aim of {{this paper}} is twofold. First it reports on a {{two-dimensional}} assembler automaton (called CoreSys) which is able to support digital life without the need of memory protection. Secondly, we would like to discuss two conceptual hypotheses by using CoreSys as a concrete example of digital life: (a) Digital life, or in general strong articial life, should be studied in its own right. (b) Working with articial life creates pre-rational (implicit) knowledge about life and life-like processes. Introduction This work follows a line of research initiated by Dewdney, 1984 who suggested an assembler automaton as a world where assembler programs can compete. Rasmussen et al., 1990, Ray, 1992, and others have applied this idea to build computer simulations in order to understand life. In these systems the core memory is organized in a linear way and thus organisms consists of a linear sequence of <b>assembler</b> <b>instructions.</b> Avida (Adami and Brown, 1994) and other systems (e. g. (P [...] ...|$|R
5000|$|... == Channel program == A {{channel program}} is a {{sequence}} of channel command words (CCWs) which are executed by the I/O channel subsystem in the IBM System/360 and subsequent architectures. A channel program consists {{of one or more}} channel command words. The operating system signals the I/O channel subsystem to begin executing the channel program with a SSCH (start sub-channel) instruction. The central processor is then free to proceed with non-I/O instructions until interrupted. When the channel operations are complete, the channel interrupts the central processor with an I/O interruption. In earlier models of the IBM mainframe line, the channel unit was an identifiable component, one for each channel. In modern mainframes, the channels are implemented using an independent RISC processor, the channel processor, one for all channels. IBM System/370 Extended Architecture and its successors replaced the earlier SIO (start I/O) and SIOF (start I/O fast release) <b>assembler</b> <b>instructions</b> (System/360 and early System/370) with the SSCH (start sub-channel) instruction (late System/370 and successors).|$|R
2500|$|In {{assembly}} language, {{the term}} [...] "macro" [...] represents {{a more comprehensive}} concept than it does in some other contexts, {{such as in the}} C programming language, where its #define directive typically is used to create short single line macros. <b>Assembler</b> macro <b>instructions,</b> like macros in PL/I and some other languages, can be lengthy [...] "programs" [...] by themselves, executed by interpretation by the assembler during assembly.|$|R
50|$|Stage 1 is the {{compilation}} of {{a sequence of}} <b>assembler</b> macro <b>instructions</b> describing the configuration to be installed or updated. The assembler does not actually compile any object code, but instead compiles a series of PUNCH pseudo-ops in order to generate a job stream for Stage 2. As IBM changed the nomenclature for OS/360 options, it also changed the Sysgen macro definitions to use newer names for the options.|$|R
40|$|We {{report on}} a large formal {{verification}} effort in mechanically proving correct a compiling specification for a realistic bootstrap compiler from ComLisp (a subset of ANSI Common Lisp sufficiently expressive {{to serve as a}} compiler implementation language) to binary Transputer code using the PVS system. The compilation is carried out in five steps through a series of intermediate languages. In the first phase, ComLisp is translated into a stack intermediate language (SIL), where parameter passing is implemented by a stack technique. Expressions are transformed from a prefix notation into a postfix notation according to the stack principle. SIL is then compiled into C int where the ComLisp data structures (s-expressions) and operators are implemented in linear integer memory using a run-time stack and a heap. These two steps are machine independent. In the compiler’s backend, first control structures (loops, conditionals) of the intermediate language C int are implemented by linear assembler code with relative jumps, the infinite memory model of C int is realized on the finite Transputer memory, and the basic C int statements for accessing the stack and heap are implemented by a sequence of <b>assembler</b> <b>instructions.</b> The fourth phase consists of the implementation o...|$|R
40|$|This theory {{practically}} {{depends on}} the critical section problem. After studying the overview of CSP, it’s seen that {{there are lots of}} drawbacks in CSP but most of the different solutions are given by the different authors. But still no one has got the perfect solution to overcome this problem. But from the my point of view, it’s considered as “it can get a solution including all four possible conditions like (Mutual exclusion, No-preemption, Bounded waiting and starvation). {{the best way to go}} for this approach, it should be completely checked it out of all the instruction, but it’s very true no any process can move Parallely in critical section, practically one has to wait finally. According to these conditions it provides a great convenience to prove this problem. No one has proved the CSP till now, it’s proved by myself in each case of set instruction and algorithms. It provides a gateway to solve the problem of CSP in coming future. to accomplish the incrementing atomically, using only ordinary <b>assembler</b> <b>instructions.</b> The problem was defined and first solved by Edsgar Dijkstra. "Critical section routine " was his name for the code that solved the problem...|$|R
