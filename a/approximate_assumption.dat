15|105|Public
40|$|AbstractTypically, {{tasks of}} {{modeling}} complex systems are performed using a certain step size, and step size reduction {{can lead to}} an increase in working time. Additionally, with a low frame rate, the modeling process appears jerky. This situation can cause a problems with interpolation of frame. In this paper, an algorithm is presented for <b>approximate</b> <b>assumption</b> of the water front between flood modeling steps. An approach is described that produces smoother water front visualization instead of reducing the modeling step size...|$|E
40|$|Abstract. A fully {{discrete}} penalty {{finite element}} method is presented for the two-dimensional time-dependent Navier-Stokes equations. The time discretization of the penalty Navier-Stokes equations {{is based on the}} backward Euler scheme; the spatial discretization of the time discretized penalty Navier-Stokes equations is based on a finite element space pair (Xh,Mh) which satisfies some <b>approximate</b> <b>assumption.</b> An optimal error estimate of the numerical velocity and pressure is provided for the fully discrete penalty {{finite element method}} when the parameters ɛ, ∆t and h are sufficiently small...|$|E
40|$|An optimal error {{estimate}} of the numerical velocity, pressure and angular velocity, is proved for the fully discrete penalty finite element method of the micropolar equations, when the parameters ², ∆t and h are sufficiently small. In order to obtain above we present the time discretization of the penalty micropolar equation {{which is based on}} the backward Euler scheme; the spatial discretization of the time discretized penalty Micropolar equation is based on a finite elements space pair (Hh, Lh) which satisfies some <b>approximate</b> <b>assumption...</b>|$|E
3000|$|... is a nonexpansive mapping. They {{proved that}} under some <b>approximate</b> <b>assumptions</b> on the {{operators}} and parameters, the sequence [...]...|$|R
3000|$|This article aims to {{deal with}} a new {{modified}} iterative projection method for solving a hierarchical fixed point problem. It is shown that under certain <b>approximate</b> <b>assumptions</b> of the operators and parameters, the modified iterative sequence [...]...|$|R
50|$|The method {{described}} above for evaluating smoothness {{is based on}} information theory, and an assumption that {{the influence of the}} color of a voxel influencing the color of nearby voxels according to the normal distribution on the distance between points. The model is based on <b>approximate</b> <b>assumptions</b> about the world.|$|R
40|$|The project IRCAD-SARI, {{funded by}} the French Ministry of Transport, will {{experiment}} on some rural roads in France a warning sign system during adverse weather conditions. The safety stake is assessed by computing the added-risk in case of adverse weather conditions, and especially in case of rain on bends. The method for estimating rain exposure takes into account several types of meteorological information, and the additional weather information of the accident database. The traffic database, when available, enables to drop the <b>approximate</b> <b>assumption</b> that traffic is not correlated with rain. A method for computing the added risk due to rain is proposed and some related results are given in this paper...|$|E
40|$|A {{method is}} {{described}} that enables a chromatin fraction containing ribosomal DNA (DNA containing sequences coding for rRNA) {{to be prepared}} from the macronuclei of growing or stationary cultures of Tetrahymena pyriformis. This material is obtained in yields of between 25 and 75 % of the theoretical maximum. The DNA in this fraction was identified as ribosomal DNA {{on the basis of}} its density and molecular weight, and it appears not to be appreciably contaminated by other DNA. The method relies on the <b>approximate</b> <b>assumption</b> that ribosomal DNA is the smallest species of DNA in chromatin in the nucleus, and avoids the use of mechanical force, or enzyme action, to fractionate chromatin...|$|E
40|$|This {{paper will}} derive two {{equations}} for error rate prediction {{in the general}} M-toN biometric identification system: one for system false match rate, and one for system false non-match rate. Under the simplifying, but <b>approximate,</b> <b>assumption</b> of statistical independence of all errors, independent variables are bin error rate, penetration rate, sample-template ("genuine") and "impostor" distance distributions, number of active templates or user models in the database, N, {{and the number of}} samples submitted for each transaction, M Depending upon the system, each of the users might enroll one or many biometric measures or models. These measures might be different presentations of the same biometric pattern, or representations of independent biometric patterns, perhaps acquired using different biometric technologies. During use, each user might present multiple patterns for comparison with the database...|$|E
40|$|We {{analyze the}} {{properties}} of electroweak chiral effective Lagrangian with an extended SU(2) R gauge group. Right-handed SU(2) R gauge bosons affect electroweak observables by mixing with electroweak gauge bosons WL,μ and Bμ. We discuss all possible mass mixing terms and calculate the exact physical mass eigenvalues by diagonalization of mixing matrix without any <b>approximate</b> <b>assumptions.</b> The contributions to oblique radiative corrections parameters STU from SU(2) R fields are also presented...|$|R
5000|$|According to Carbon Monitoring for Action (CARMA), in 2007 Redbank emitted more {{climate change}} and global warming causing {{greenhouse}} gases per unit of electricity generated than any other power station in Australia. However no data from the actual plant, operator or Australian Government is actually used to base these <b>approximate</b> <b>assumptions</b> on. CARMA uses a statistical model that predicts [...] emissions given the size, age, fuel type, estimated capacity utilization, and engineering specifications of individual plants.|$|R
40|$|This paper {{deals with}} a modified {{iterative}} projection method for approximating a solution of hierarchical fixed point problems for nearly nonexpansive mappings. Some strong convergence theorems for the proposed method are presented under certain <b>approximate</b> <b>assumptions</b> of mappings and parameters. As a special case, this projection method solves some quadratic minimization problem. It {{should be noted that}} the proposed method can be regarded as a generalized version of Wang and Xu (2013), Ceng et al. (2011), Sahu et al. (2012), and many other authors...|$|R
40|$|Present work {{deals with}} the {{portfolio}} selection problem using mean-risk models. The main goal of this work is to investigate the convergence of approxi mate solutions using generated scenarios to the analytic solution and its sensitivity to chosen risk measure and probability distribution. The considered risk measures are: variance, VaR, cVaR, absolute deviation and semivariance. We present analytical solutions for all risk measures under the assumption of normal or Student distribution. For log-normal distribution, we use the <b>approximate</b> <b>assumption</b> that the sum of log-normal random variables has log-normal distribution. Optimization models for discrete scenarios are derived for all risk measures and compared with analytical solution. In case of approximate solution with scenarios, we repeat the procedure multiple times and present our own approach to nding the optimal solution using the cluster analysis. All optimization models are written in GAMS language. Testing and estimating are realized using an application developed in C++ language...|$|E
40|$|A {{mathematical}} model of synthesis and stabilization of ZnS nanoparticles in aqueous solutions of cetyltrimethylammonium bromide (CTAB) is presented. ZnS nanoparticles precipitated by {{the reaction of}} sodium sulphide and zinc acetate are significantly influenced by CTAB both in a stage of nucleation and in a stage of growth and stabilization. The suggested model assumes a dominating influence of the nucleation stage on future properties of emerging ZnS nanoparticles. On {{the basis of a}} calculated nucleation rate depending on a degree of the ZnS supersaturation, the model adopts an <b>approximate</b> <b>assumption</b> that ZnS nuclei are formed all at one moment. Mathematical formulation of the nucleation model attempts to explain a recently observed relationship between ZnS nanoparticles radii and the CTAB concentration. The dependence of a surface tension of zinc acetate and CTAB aqueous solutions on the CTAB concentration was measured and further applied to obtain the relation between the CTAB monomer concentration and the CTAB total concentration. The nucleation model assumed a key role of CTAB monomers, which were considered as nucleation centres. The predicted radiiWeb of Science 101039238...|$|E
40|$|The over {{representation}} of novice drivers in crashes is alarming. Research indicates {{that one in}} five drivers ’ crashes within {{their first year of}} driving. Driver training is one of the interventions aimed at decreasing the number of crashes that involve young drivers. Currently, {{there is a need to}} develop comprehensive driver evaluation system that benefits from the advances in Driver Assistance Systems. Since driving is dependent on fuzzy inputs from the driver (i. e. approximate distance calculation from the other vehicles, <b>approximate</b> <b>assumption</b> of the other vehicle speed), it is necessary that the evaluation system is based on criteria and rules that handles uncertain and fuzzy characteristics of the drive. This paper presents a system that evaluates the data stream acquired from multiple in-vehicle sensors (acquired from Driver Vehicle Environment-DVE) using fuzzy rules and classifies the driving manoeuvres (i. e. overtake, lane change and turn) as low risk or high risk. The fuzzy rules use parameters such as following distance, frequency of mirror checks, gaze depth and scan area, distance with respect to lane...|$|E
30|$|In this paper, {{we focused}} on {{approximate}} controllability results for impulsive neutral fuzzy stochastic differential equations with nonlocal conditions in a Banach space by using Schauder’s and Banach fixed point theorems. According to the hypotheses, it is proved {{that the system is}} approximately controllable under nonlocal conditions. Then, the result showed that these fixed point theorems can strongly be used in control problems to obtain the sufficient conditions. Upon making some <b>approximate</b> <b>assumptions,</b> by the ideas and techniques as suggested in this paper, one can establish the approximate controllability results for a wide class of linear fuzzy stochastic evolution equations.|$|R
40|$|Abstract. We study {{homomorphic}} authenticated encryption, where {{privacy and}} {{authenticity of data}} are protected simultaneously. We define homomorphic versions of various security notions for privacy and authenticity, and investigate relations between them. In particular, we show {{that it is possible}} to give a natural definition of IND-CCA for homomorphic authenticated encryption, unlike the case of homomorphic encryption. Also, we construct a homomorphic authenticated encryption scheme sup-porting arithmetic circuits, which is chosen-ciphertext secure both for privacy and authenticity. Our scheme is based on the error-free <b>approximate</b> GCD <b>assumption.</b> Key words: homomorphic authenticated encryption, homomorphic MAC, homomorphic encryption, <b>approximate</b> GCD <b>assumption...</b>|$|R
40|$|This paper {{presents}} an approach for controlling spacecraft equipped with control moment gyroscopes. A technique from feedback linearization theory {{is used to}} transform the original nonlinear problem to an equivalent linear form without <b>approximating</b> <b>assumptions.</b> In this form, the spacecraft dynamics appear linearly, and are decoupled from redundancy in the system of gyroscopes. A general approach to distributing control effort among the available actuators is described which includes provisions for redistribution of rotors, explicit bounds in gimbal rates, and guaranteed operation at or near singular configurations. A particular algorithm is developed for systems of double-gimbal devices, and demonstrated in two examples for which existing approaches fail to give adequate performance...|$|R
40|$|Transition state {{theory was}} {{originally}} {{developed in the}} 1930 s as method for calculating chemical reaction rates in simple systems using energetic barrier heights. Unfortunately its usefulness in complex high dimensional chemical processes such as protein dynamics is severely mitigated by the <b>approximate</b> <b>assumption</b> that reacting systems do not recross the transition state. This approximation has been improved for simple systems in modern variational transition state theory (VTST), which gives a close upper bound to the true reaction rate. For complex systems, location of the VTST separatrix is still theoretically challenging. We propose the definition of all alternative transition state separatrix for systems with high friction which has the property that the escape rate through the separatrix gives the exact reaction rate for the system, {{without the need for}} recrossing corrections, and, surprisingly, without requiring the escaping trajectories to cross the reaction barrier. However, locating this separatrix presently does require considering trajectories crossing the barrier. In order to lessen the computational challenge associated with barrier crossing, we provide an automatic method to enhance sampling of trajectories crossing the reaction barrier derived from the mathematics of super symmetry. This method provides an effective "ensemble magnifying glass," statistically inducing additional sampling of configurations in regions with large probability flux but without affecting transition rates...|$|E
40|$|Microphysical processes, {{such as the}} formation, growth, and {{evaporation}} of precipitation, {{interact with}} variability and covariances (e. g., fluxes) in moisture and heat content. For instance, evaporation of rain may produce cold pools, which in turn may trigger fresh convection and precipitation. These effects are usually omitted or else crudely parameterized at subgrid scales in weather and climate models. A more formal approach is pursued here, based on predictive, horizontally averaged equations for the variances, covariances, and fluxes of moisture and heat content. These higher-order moment equations contain microphysical source terms. The microphysics terms can be integrated analytically, given a suitably simple warm-rain microphysics scheme and an <b>approximate</b> <b>assumption</b> about the multivariate distribution of cloud-related and precipitation-related variables. Performing the integrations provides exact expressions within an idealized context. A large-eddy simulation (LES) of a shallow precipitating cumulus case is performed here, and it indicates that the microphysical effects on (co) variances and fluxes can be large. In some budgets and altitude ranges, they are dominant terms. The analytic expressions for the integrals are implemented in a single-column, higher-order closure model. Interactive single-column simulations agree qualitatively with the LES. The analytic integrations form a parameterization of microphysical effects in their own right, and they also serve as benchmark solutions that {{can be compared to}} non-analytic integration methods...|$|E
40|$|International audienceRATIONALEThis study {{analyzes}} {{molecular beam}} sampling by mass spectrometry by the Knudsen method using the so-called " restricted collimation device". This device, {{defined by the}} field and source apertures, was proposed in order to eliminate any additional contribution to the genuine molecular beam of surface vaporizations coming from {{the vicinity of the}} effusion orifice as usually detected by the ion source of a mass spectrometer. METHODSThe molecular transmission of the " restricted collimation device" was calculated using a vaporization law under vacuum taking into account the real surface where the molecules are emitted, i. e., the sample evaporation surface in the Knudsen cell or the effusion orifice section, towards the ion source inlet by integration of elementary solid angles. RESULTSAn optimum is observed depending on the pair of selected apertures that define the restricted collimation device, i. e., the field and source apertures. This optimum is different from that previously calculated when taking into account only the solid angle, as defined by the restricted collimation device. CONCLUSIONSThis difference is attributed to the previously <b>approximate</b> <b>assumption</b> that optimizing the restricted collimation solid angle automatically optimizes the sampling of the effused beam included in the restricted collimation angle. Moreover, the location of the evaporating surface for molecules traveling through the collimation device towards the ionization chamber remains an important factor: the distance between the sample evaporation surface and the field aperture is of paramount importance as it changes the molecular...|$|E
40|$|This paper {{deals with}} a {{modified}} iterative projection method for approximating a solution of the hierarchical fixed point problem for a sequene of nearly nonexpansive mappings {{with respect to a}} nonexpansive mapping. It is shown that under certain <b>approximate</b> <b>assumptions</b> on the operators and parameters, the modified iterative sequence converges strongly to a common element of the set of the common fixed points of nearly nonexpansive mappings. Also, this point solves some variational inequality. As a special case, this projection method can be used to find the minimum norm solution of the given variational inequality. The results here improve and extend some recent corresponding results of other authors. Comment: 11 page...|$|R
40|$|The {{effect of}} the {{thickness}} and the elastic properties of the adhesive layers in laminated structures is considered. The structure is assumed to consist of two sets of periodically arranged dissimilar layers which may contain cracks perpendicular to the interfaces. The crack problem is solved under the assumption of plane strain or generalized plane stress and by using two different models for the adhesive layers. In the first model the adhesive layer is approximated {{by a combination of}} tensile and shear springs. In the second the adhesive layer is considered to be an elastic continuum, hence involving no <b>approximating</b> <b>assumptions.</b> The results regarding the stress intensity and stress concentration factors obtained from these two models and that found by ignoring the adhesive layers are presented and some comparisons are made...|$|R
40|$|Let C be a nonempty closed convex {{subset of}} a real Hilbert space H. Let {T_{n}}:C›H be a {{sequence}} of nearly nonexpansive mappings such that F:=?_{i= 1 }^{?}F(T_{i}) ?Ø. Let V:C›H be a ?-Lipschitzian mapping and F:C›H be a L-Lipschitzian and ?-strongly monotone operator. This paper deals with a modified iterative projection method for approximating a solution of the hierarchical fixed point problem. It is shown that under certain <b>approximate</b> <b>assumptions</b> on the operators and parameters, the modified iterative sequence {x_{n}} converges strongly to x^{*}?F {{which is also the}} unique solution of the following variational inequality: 	 ? 0, ?x?F. As a special case, this projection method can be used to find the minimum norm solution of above variational inequality; namely, the unique solution x^{*} to the quadratic minimization problem: x^{*}=argmin_{x?F}?x?². The results here improve and extend some recent corresponding results of other authors...|$|R
40|$|We {{study the}} {{possibility}} to extract model independent information about {{the dynamics of the}} universe by using Cosmography. We intend to explore it systematically, to learn about its limitations and its real possibilities. Here we are sticking to the series expansion approach on which Cosmography is based. We apply it to different data sets: Supernovae Type Ia (SNeIa), Hubble parameter extracted from differential galaxy ages, Gamma Ray Bursts (GRBs) and the Baryon Acoustic Oscillations (BAO) data. We go beyond past results in the literature extending the series expansion up to the fourth order in the scale factor, which implies the analysis of the deceleration, q_{ 0 }, the jerk, j_{ 0 } and the snap, s_{ 0 }. We use the Markov Chain Monte Carlo Method (MCMC) to analyze the data statistically. We also try to relate direct results from Cosmography to dark energy (DE) dynamical models parameterized by the Chevalier-Polarski-Linder (CPL) model, extracting clues about the matter content and the dark energy parameters. The main results are: a) even if relying on a mathematical <b>approximate</b> <b>assumption</b> such as the scale factor series expansion in terms of time, cosmography can be extremely useful in assessing dynamical properties of the Universe; b) the deceleration parameter clearly confirms the present acceleration phase; c) the MCMC method can help giving narrower constraints in parameter estimation, in particular for higher order cosmographic parameters (the jerk and the snap), with respect to the literature; d) both the estimation of the jerk and the DE parameters, reflect the possibility of a deviation from the LCDM cosmological model. Comment: 24 pages, 7 figure...|$|E
40|$|The {{explanation}} of the accelerated expansion of the Universe poses {{one of the most}} fundamental questions in physics and cosmology today. If the acceleration is driven by some form of dark energy, {{and in the absence of}} a well-based theory to interpret the observations, one can try to constrain the parameters describing the kinematical state of the universe using a cosmographic approach, which is fundamental in that it requires only a minimal set of assumptions, namely to specify the metric, and it does not rely on the dynamical equations for gravity. Our high-redshift analysis allows us to put constraints on the cosmographic expansion up to the fifth order. It is based on the Union 2 Type Ia Supernovae (SNIa) data set, the Hubble diagram constructed from some Gamma Ray Bursts luminosity distance indicators, and gaussian priors on the distance from the Baryon Acoustic Oscillations (BAO), and the Hubble constant h (these priors have been included in order to help break the degeneracies among model parameters). To perform our statistical analysis and to explore the probability distributions of the cosmographic parameters we use the Markov Chain Monte Carlo Method (MCMC). We finally investigate implications of our results for the dark en- ergy, in particular, we focus on the parametrization of the dark energy equation of state (EOS). Actually, a possibility to investigate the nature of dark energy lies in measuring the dark energy equation of state, w, and its time (or redshift) dependence at high ac- curacy. However, since w(z) is not directly accessible to measurement, reconstruction methods are needed to extract it reliably from observations. Here we investigate differ- ent models of dark energy, described through several parametrizations of the equation of state, by comparing the cosmographic and the EOS series. The main results are: a) even if relying on a mathematical <b>approximate</b> <b>assumption</b> such as the scale factor series expansion in terms of time, cosmography can be extremely useful in assessing dynamical properties of the Universe; b) the deceleration parameter clearly confirms the present acceleration phase; c) the MCMC method provides stronger constraints for parameter estimation, in particular for higher order cosmographic parameters (the jerk and the snap), with respect to those presented in the literature; d) both the esti- mation of the jerk and the DE parameters, reflect the possibility of a deviation from the LambdaCDM cosmological model; e) there are indications that the dark energy equa- tion of state is evolving for all the parametrizations that we considered; f) the q(z) reconstruction provided by our cosmographic analysis allows a transient acceleration...|$|E
40|$|Automobiles have deeply {{impacted}} the way {{in which}} we travel but they have also contributed to many deaths and injury due to crashes. A number of reasons for these crashes have been pointed out by researchers. Inexperience has been identified as a contributing factor to road crashes. Driver’s driving abilities also play a vital role in judging the road environment and reacting in-time to avoid any possible collision. Therefore driver’s perceptual and motor skills remain the key factors impacting on road safety. Our failure to understand what is really important for learners, in terms of competent driving, is one of the many challenges for building better training programs. Driver training is one of the interventions aimed at decreasing the number of crashes that involve young drivers. Currently, there is a need to develop comprehensive driver evaluation system that benefits from the advances in Driver Assistance Systems. A multidisciplinary approach is necessary to explain how driving abilities evolves with on-road driving experience. To our knowledge, driver assistance systems have never been comprehensively used in a driver training context to assess the safety aspect of driving. The aim and novelty of this thesis is to develop and evaluate an Intelligent Driver Training System (IDTS) as an automated assessment tool that will help drivers and their trainers to comprehensively view complex driving manoeuvres and potentially provide effective feedback by post processing the data recorded during driving. This system is designed to help driver trainers to accurately evaluate driver performance and has the potential to provide valuable feedback to the drivers. Since driving is dependent on fuzzy inputs from the driver (i. e. approximate distance calculation from the other vehicles, <b>approximate</b> <b>assumption</b> of the other vehicle speed), it is necessary that the evaluation system is based on criteria and rules that handles uncertain and fuzzy characteristics of the driving tasks. Therefore, the proposed IDTS utilizes fuzzy set theory for the assessment of driver performance. The proposed research program focuses on integrating the multi-sensory information acquired from the vehicle, driver and environment to assess driving competencies. After information acquisition, the current research focuses on automated segmentation of the selected manoeuvres from the driving scenario. This leads to the creation of a model that determines a “competency” criterion through the driving performance protocol used by driver trainers (i. e. expert knowledge) to assess drivers. This is achieved by comprehensively evaluating and assessing the data stream acquired from multiple in-vehicle sensors using fuzzy rules and classifying the driving manoeuvres (i. e. overtake, lane change, T-crossing and turn) between low and high competency. The fuzzy rules use parameters such as following distance, gaze depth and scan area, distance with respect to lanes and excessive acceleration or braking during the manoeuvres to assess competency. These rules that identify driving competency were initially designed with the help of expert’s knowledge (i. e. driver trainers). In-order to fine tune these rules and the parameters that define these rules, a driving experiment was conducted to identify the empirical differences between novice and experienced drivers. The results from the driving experiment indicated that significant differences existed between novice and experienced driver, in terms of their gaze pattern and duration, speed, stop time at the T-crossing, lane keeping and the time spent in lanes while performing the selected manoeuvres. These differences were used to refine the fuzzy membership functions and rules that govern the assessments of the driving tasks. Next, this research focused on providing an integrated visual assessment interface to both driver trainers and their trainees. By providing a rich set of interactive graphical interfaces, displaying information about the driving tasks, Intelligent Driver Training System (IDTS) visualisation module has the potential to give empirical feedback to its users. Lastly, the validation of the IDTS system’s assessment was conducted by comparing IDTS objective assessments, for the driving experiment, with the subjective assessments of the driver trainers for particular manoeuvres. Results show that not only IDTS was able to match the subjective assessments made by driver trainers during the driving experiment but also identified some additional driving manoeuvres performed in low competency that were not identified by the driver trainers due to increased mental workload of trainers when assessing multiple variables that constitute driving. The validation of IDTS emphasized the need for an automated assessment tool that can segment the manoeuvres from the driving scenario, further investigate the variables within that manoeuvre to determine the manoeuvre’s competency and provide integrated visualisation regarding the manoeuvre to its users (i. e. trainers and trainees). Through analysis and validation it was shown that IDTS is a useful assistance tool for driver trainers to empirically assess and potentially provide feedback regarding the manoeuvres undertaken by the drivers...|$|E
40|$|Large spherical {{scintillation}} detectors {{are playing}} {{an increasingly important}} role in experimental neutrino physics studies. From the instrumental point of view the primary signal response of these set-ups is constituted by the time and amplitude of the anode pulses delivered by each individual phototube following a particle interaction in the scintillator. In this work, under some <b>approximate</b> <b>assumptions,</b> we derive a number of analytical formulas able to give a fairly accurate description {{of the most important}} timing features of these detectors, intended to complement the more complete Monte Carlo studies normally used for a full modelling approach. The paper is completed with a mathematical description of the event position distributions which can be inferred, through some inference algorithm, starting from the primary time measures of the photomultiplier tubes. Comment: 29 pages, 20 figures, accepted for publication on Nucl. Instr. and Meth. ...|$|R
40|$|A new {{technology}} of composite micromachining of laser and electrolysis is presented {{through a combination}} of technological advantages of laser processing and electrolytic machining. The implication of its method is that laser processing efficiently removes metallic materials and that pulse electrolytic machining removes recast layer and controls shape precisely. Machining accuracy and efficiency can be improved. The impacts that electrolyte fluid effectively cools the microstructure edge in the laser machining process and that gas-liquid two-phase flow makes the electrolyte conductivity produce uneven distribution in the electrolytic processing are considered. Some <b>approximate</b> <b>assumptions</b> are proposed on the actual conditions of machining process. The mathematical model of composite micromachining of laser and electrolysis based on the electrolyte fluid is built. The validity of the model can be verified by experimentation. The experimental results show that processing accuracy meets accuracy requirements which are ± 0. 05 [*]mm. Machining efficiency increases more than 20 percent compared to electrolytic processing...|$|R
40|$|One {{approach}} to feature detection is to match a parametric {{model of the}} feature to the image data. Naturally, the performance of such detectors is highly dependent upon the function {{used to measure the}} degree of fit between the feature model and the image data. In this paper, we first show how an existing detector can be extended to use a weighted L 2 norm as the matching function with minimal extra computation. Next, we propose optimality criteria for the two fundamental aspects of feature detection performance: feature detection robustness and parameter estimation accuracy. We also show how to combine these criteria in various ways. We analyze the optimality criterion for parameter estimation under the <b>approximating</b> <b>assumption</b> that the feature manifold is locally linear. We also present a numerical algorithm {{that can be used to}} estimate the optimal weighting functions for the other optimality criteria. We include the results of applying this algorithm for step edge, line, and corne [...] ...|$|R
40|$|Improvements of the {{mixing-length}} theory (MLT) of turbulent convection in stellar atmospheres {{are developed}} theoretically. It is {{pointed out that}} inaccuracies are introduced into MLT by the <b>approximating</b> <b>assumptions</b> of a single large eddy (rather than many eddies of different sizes) and of incompressibility. In the proposed new model, {{the full spectrum of}} turbulent eddies is determined using more recent turbulence models (e. g., the eddy-damped quasi-normal Markovian model of Orszag, 1977), and a new formula for the convective flux is obtained which gives values up to 10 times greater than those of the MLT at high convective efficiencies. The problem of compressibility is addressed by adding one of two new expressions (one with no free parameters) for the mixing length. Numerical results from simulations of a solar-type star and a 0. 8 -solar-mass globular-cluster star are presented in tables and graphs and discussed in detail; the agreement with observations is found to be better than with the MLT...|$|R
40|$|The {{method and}} {{characteristics}} of several approaches to the pricing of discretely monitored arithmetic Asian options on stocks with discrete, absolute dividends are described. The contrast between method behaviors for options with an Asian tail and those with monitoring throughout their lifespan is emphasized. Rates of convergence are confirmed, but greater focus is put on actual performance in regions of accuracy which are realistic for use by practitioners. A hybrid approach combining Curran's analytical approximation with a two-dimensional finite difference method is examined {{with respect to the}} errors caused by the <b>approximating</b> <b>assumptions.</b> For Asian tails of equidistant monitoring dates, this method performs very well, but as the scenario deviates from the method's ideal conditions, the errors in the approximation grow unfeasible. For general monitoring straightforward solution of the full three-dimensional partial differential equation by finite differences is highly accurate but suffers from rapid degradation in performance as the monitoring interval increases. For options with long monitoring intervals a randomized quasi-Monte Carlo method with control variate variance reduction stands out as a powerful alternative. Comment: 19 pages, 9 figures, 5 table...|$|R
40|$|The {{life time}} Of Y 2 O 3 {{stabilized}} ZrO 2 thermal barrier coatings (TBC) has been modelled using a rather simple fracture mechanical approach. The {{basis of the}} model, is a finite element analysis of the thermal stresses and <b>approximate</b> <b>assumptions</b> of crack growth along the bond coat (BC) -ceramics interface. The FE calculations show the influence of several microstructural features of the TBC system, as profile of the BC TBC interface, thickness of thermally grown oxide formed during thermal cycling and others, on the stress state. From these results, a specific way of crack growth is predicted and included into the model. The modelling results are compared to life times obtained from thermal cycling experiments. An analysis of the location of failure within the samples, {{as well as the}} influence of a variation of the roughness of the BC-ceramics interface on life time are presented. Both are in reasonable agreement with the modelling results. Finally, the shorter life times, which are predicted for samples exposed to an additional compressive mechanical strain, are discussed. (C) 2003 Elsevier Science B. V. All rights reserved...|$|R
40|$|In {{this paper}} we are {{concerned}} {{with a sample of}} asymptotically independent risks. Tail asymptotic probabilities for linear combinations of randomly weighted order statistics are <b>approximated</b> under various <b>assumptions,</b> where the individual tail behaviour has a crucial role. An application is provided for Log-Normal risks...|$|R
40|$|A random {{phenomenon}} {{may have}} two sources of random variation: an unstable identity {{and a set}} of external variation-generating factors. When only a single source is active, two mutually exclusive extreme scenarios may ensue that result in the exponential or the normal, the only truly univariate distributions. All other supposedly univariate random variation observed in nature is truly bivariate. In this article, we elaborate on this new paradigm for random variation and develop a general bivariate distribution to reflect it. It is shown that numerous current univariate distributions are special cases of an approximation to the new bivariate distribution. We first show that the exponential and the normal are special cases of a single distribution represented by a Response Modeling Methodology model. We then develop a general bivariate distribution commensurate with the new paradigm, its properties are discussed and its moments developed. An <b>approximating</b> <b>assumption</b> results in a univariate general distribution that is shown to include as exact special cases widely used distributions like generalized gamma, log-normal, F, t, and Cauchy. Compound distributions and their relationship to the new paradigm are addressed. Empirical observations that comply with predictions derived from the new paradigm corroborate its scientific validity. Comment: 20 pages;no figures or table...|$|R
40|$|In this work. {{an attempt}} {{has been made}} to develop a {{programming}} package for ride analysis of off-road vehicles based upon a finite-element formulation of vehicle suspension systems. Mathematical modelling of generalised suspension systems has been carried out with several non-linear aspects being investigated and implemented in the programming package. such as large deflection. non-linear characteristics of springs and dampers. bump stops and wheel separation. Different types of soi 1 have been considered together with an appropriate modelling of vehicle tracks. Several methods for time integration of dynamic equations have been investigated so as to deal wi th numerical instabi 1 i ty problems expected for off-road suspension systems which often have "stiff" differential equations of motion. Three ride analysis criteria have also been considered in the programming package. Several case studies have been analysed using the developed programming package. They consist of two simple case studies with known analytical solutions. an existing wheeled off-road vehicle with published analog computer resul t s, and an off-road tracked vehicle wi th known experimental results. The package has been validated and proved to be an acceptable tool for the ride analysis of off-road vehicles. within the <b>approximating</b> <b>assumptions</b> considered. Several measures for future development have also been suggested...|$|R
