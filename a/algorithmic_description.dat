179|66|Public
50|$|The TrueType font format {{introduced}} {{a scale from}} 100 through 900, which is also used in CSS and OpenType, where 400 is regular (roman or plain). The first <b>algorithmic</b> <b>description</b> of fonts was perhaps made by Donald Knuth in his Metafont and TeX system of programs.|$|E
50|$|In general, an {{algorithm}} can {{be performed}} over many clock cycles with few hardware resources, or over fewer clock cycles using {{a larger number of}} ALUs, registers and memories. Correspondingly, from one <b>algorithmic</b> <b>description,</b> a variety of hardware microarchitectures can be generated by an HLS compiler according to the directives given to the tool. This is the same trade off of execution speed for hardware complexity as seen when a given program is run on conventional processors of differing performance, yet all running at roughly the same clock frequency.|$|E
50|$|While logic {{synthesis}} uses an RTL {{description of}} the design, high-level synthesis works {{at a higher level}} of abstraction, starting with an <b>algorithmic</b> <b>description</b> in a high-level language such as SystemC and ANSI C/C++. The designer typically develops the module functionality and the interconnect protocol. The high-level synthesis tools handle the micro-architecture and transform untimed or partially timed functional code into fully timed RTL implementations, automatically creating cycle-by-cycle detail for hardware implementation. The (RTL) implementations are then used directly in a conventional logic synthesis flow to create a gate-level implementation.|$|E
5000|$|Step 1 of CT-QMC is {{to split}} the Hamiltonian into a exactly {{solvable}} term, , and the rest, [...] Different choices correspond to different expansions and thus different <b>algorithmic</b> <b>descriptions.</b> Common choices are: ...|$|R
30|$|The rest of {{the paper}} is {{organized}} as follows. We review the related work in Section 2. After that, we introduce our basic system model and formulate our investigate problem in Section 3. Section 4 introduces our proposed scheme with <b>algorithmic</b> <b>descriptions.</b> We present the experimental evaluation results in Section 5 and conclude the paper in Section 6.|$|R
40|$|This book organizes key concepts, theories, standards, methodologies, trends, {{challenges}} and applications of data mining and knowledge discovery in databases. It first surveys, then provides comprehensive yet concise <b>algorithmic</b> <b>descriptions</b> of methods, including classic methods plus the extensions and novel methods developed recently. It also gives in-depth descriptions of data mining applications in various interdisciplinary industries...|$|R
50|$|As {{a result}} of the {{efficiency}} gains realized using HDL, a majority of modern digital circuit design revolves around it. Most designs begin as a set of requirements or a high-level architectural diagram. Control and decision structures are often prototyped in flowchart applications, or entered in a state diagram editor. The process of writing the HDL description is highly dependent {{on the nature of the}} circuit and the designer's preference for coding style. The HDL is merely the 'capture language', often beginning with a high-level <b>algorithmic</b> <b>description</b> such as a C++ mathematical model. Designers often use scripting languages such as Perl to automatically generate repetitive circuit structures in the HDL language. Special text editors offer features for automatic indentation, syntax-dependent coloration, and macro-based expansion of the entity/architecture/signal declaration.|$|E
50|$|High-level {{synthesis}} (HLS), {{sometimes referred}} to as C synthesis, electronic system-level (ESL) synthesis, algorithmic synthesis, or behavioral synthesis, is an automated design process that interprets an <b>algorithmic</b> <b>description</b> of a desired behavior and creates digital hardware that implements that behavior. Synthesis begins with a high-level specification of the problem, where behavior is generally decoupled from e.g. clock-level timing. Early HLS explored a variety of input specification languages., although recent research and commercial applications generally accept synthesizable subsets of ANSI C/C++/SystemC/MATLAB. The code is analyzed, architecturally constrained, and scheduled to create a register-transfer level (RTL) hardware description language (HDL), which is then in turn commonly synthesized to the gate level by the use of a logic synthesis tool. The goal of HLS is to let hardware designers efficiently build and verify hardware, by giving them better control over optimization of their design architecture, and through the nature of allowing the designer to describe the design at a higher level of abstraction while the tool does the RTL implementation. Verification of the RTL {{is an important part of}} the process.|$|E
50|$|During the 20th century STV {{was used}} for {{elections}} to the Christchurch City Council in 1917, 1929, 1931 and 1933, and for Woolston Borough Council in 1917 and 1919. In business, Fonterra used STV for their Board of Directors and Shareholders' Council elections in 2002. The Local Electoral Act 2001 provided that STV was mandatory for District Health Board elections but offered local councils the choice of either staying with plurality at large or changing to STV. It also provided for a binding poll of voters in an area to be held to determine the which system would be used, either at the initiative of the council or by a citizen's initiative instigated by voters in an area. In practice very few local authorities adopted STV under the Act's provisions, and in those that did the use of STV was plagued by poor explanations of the STV process, which often gave little more information than an <b>algorithmic</b> <b>description</b> of how to place a vote. This left the unfortunate impression among voters that STV {{was little more than}} a gratuitously complex equivalent to existing voting mechanisms. Nonetheless New Zealand made history by becoming the first country in the world to use the advanced Meek's method of STV.|$|E
40|$|As the {{complexity}} of integrated circuit systems increases, automated hardware design from higherlevel abstraction {{is becoming more and}} more important. However, for many high-level programming languages, such as C/C++, the description of bitwise access and computation is not as direct as hardware description languages, and hardware synthesis of <b>algorithmic</b> <b>descriptions</b> may generate sub-optimal implementtations for bitwise computation-intensive applications. In this paper we introduce a bit-level transformation and optimization approach to assisting hardware synthesis of <b>algorithmic</b> <b>descriptions.</b> We introduce a bit-flow graph to capture bit-value information. Analysis and optimizing transformations can be performed on this representation, and the optimized results are transformed back to the standard data-flow graphs extended with a few instructions representing bitwise access. This allows high-level synthesis tools to automatically generate circuits with higher quality. Experiments show that our algorithm can reduce slice usage by 29. 8 % on average for a set of real-life benchmarks on Xilinx FPGAs. In the meantime, the clock period is reduced by 13. 6 % on average, with an 11. 4 % latency reduction. 1...|$|R
40|$|Pe~ormunce {{optimization}} is {{the primary}} design goal in most digital signal processing (DSP) and numerically intensive applications. The problem of mapping high-level <b>algorithmic</b> <b>descriptions</b> for these applications to special-ized instruction sets has only recently begun to receive attention. In fact, the problem of optimizing performance {{has yet to be}} addressed directly. This paper introduces a new approach to instruction set mapping (and template matching in general) targeted toward pe~ormance optimi-zation. Several novel issues are addressed inc[uding par-tial matching and automatic clock selection. ...|$|R
40|$|Tactical Temporalities is a semi-improvised live coding {{performance}} that manipulates <b>algorithmic</b> <b>descriptions</b> of musical {{processes in the}} context of an Algorave setting. The work employs algorithms developed through research into musically salient computational processes and insights from studies in music perception. These are combined into an interactive {{performance that}} manages the dance of agency between the human musician and semi-autonomous computational systems. Stylistically, Tactical Temporalities is metrically regular, harmonically diatonic and utilises electronic and sampled timbral elements typically of those employed in electronic dance music. Full Tex...|$|R
40|$|High-level or {{behavioral}} synthesis of digital circuits offers {{an effective way}} to deal with the increasing complexity of digital hardware design. A high-level synthesis tool transforms an abstract <b>algorithmic</b> <b>description</b> into a detailed register transfer level implementation. Since most of the times the <b>algorithmic</b> <b>description</b> is given in textual form, high-level synthesis transformations share common aspects with traditional compiler transformation...|$|E
40|$|The SYCO {{system is}} a silicon {{compiler}} for VLSI ASICs specified by algorithms. SYCO starts from an <b>algorithmic</b> <b>description</b> and produces a circuit that realises the algorithm. Although SYCO starts from a high-level hardware description language, the translation scheme from <b>algorithmic</b> <b>description</b> to layout is easy to understand, {{and there is a}} correspondence between the input description and the layout. Consequently the designer may easily modify the input description in order to force the compiler to produce a given result...|$|E
30|$|This article further {{discusses}} the initial technical results introduced in [22] and extends them by elaborating the <b>algorithmic</b> <b>description</b> in Sect. 2, enriching {{the results in}} Sect. 4 and introducing Sect. 5 completely.|$|E
40|$|Abstract Based on {{cognitive}} functionalities in human vision processing, we propose a computational cognitive model for object recognition with detailed <b>algorithmic</b> <b>descriptions.</b> The contribution {{of this paper}} is of two folds. Firstly, we present a systematic review on psychological and neurophysiological studies, which provide collective evidence for a distributed representation of 3 D objects in the human brain. Secondly, we present a computational model which simulates the distributed mechanism of object vision pathway. Experimental results show that the presented computational cognitive model outperforms five representative 3 D object recognition algorithms in computer science research...|$|R
40|$|We derive and {{implement}} an algorithm similar to (Huang and Chiang, 2005) for finding thenbest derivations in a weighted hypergraph. We prove the correctness and {{termination of the}} algorithm and we show experimental results concerning its runtime. Our work {{is different from the}} aforementioned one in the following respects: we consider labeled hypergraphs, allowing for tree-based language models (Maletti and Satta, 2009); we specifically handle the case of cyclic hypergraphs; we admit structured weight domains, allowing for multiple features to be processed; we use the paradigm of functional programming together with lazy evaluation, achieving concise <b>algorithmic</b> <b>descriptions.</b> ...|$|R
40|$|Automation of the floating-point to fixed-point {{conversion}} of <b>algorithmic</b> <b>descriptions</b> {{is expected to}} produce significant improvements in the design process of embedded systems, {{in terms of its}} efficiency and error resilience. Although several analytical and statistical approaches to help the designer make the conversion trade-off between design parameters and implementation costs do exist, none are able to replace the designer in making the tradeoff itself. Fixify, a toolset for automated floating-point to fixed-point conversion is presented here. It uses existing optimisation heuristics to perform the conversion tradeoff, disburdening the designer and significantly improving the design process...|$|R
30|$|The <b>algorithmic</b> <b>description</b> of {{the main}} {{algorithms}} discussed in the article along with the more precise complexity figures is presented in this section. Note that all implementations are directly available on line at [URL] telecom-paristech.fr/~grichard/EURASIP_Moreau 2011 /.|$|E
40|$|Abstract. We {{present a}} general {{methodology}} for non-deterministic programming based on pure functional programming. We construct families of automata constructions {{which are used}} as finite-state process descriptions. We use as <b>algorithmic</b> <b>description</b> language Pidgin ML, a core applicative subset of Objective Caml. ...|$|E
40|$|High-level or {{behavioral}} synthesis of digital circuits offers {{an effective way}} to deal with the increasing complexity of digital hardware design. A high-level synthesis tool transforms an abstract <b>algorithmic</b> <b>description</b> into a detailed register transfer level implementation. Since most of the times the <b>algorithmic</b> <b>description</b> is given in textual form, high-level synthesis transformations share common aspects with traditional compiler transformations (the process is also called hardware compilation). As a consequence, hardware compilation can be performed using attribute grammars. However, since modern high-level synthesis transformations are of high complexity, advanced attribute evaluation techniques are required. This paper presents recent advances in hardware compilation under the FNC- 2 attribute grammar system. It follows two different approaches. One that decorates the parse tree of the behavioral specication with implementation details, taking advantage of the use of a [...] ...|$|E
40|$|AbstractThe paper {{describes}} {{a system for}} executable papers for publishers enabling them to reuse content and to generate further advances of science and engineering. The executable <b>algorithmic</b> <b>descriptions</b> within a paper are presented in natural language sentences and basic code, thereby making long term compatibility absolute. Authors are required to use publicly numerical libraries on the Internet or references to publications with executable papers. As used by authors the system automatically creates a web of algorithmic knowledge on the Internet. Novelty of new algorithms in publications can be evaluated by automated tools available to authors, reviewers and readers of scientific papers published...|$|R
40|$|Algorithmic design {{must start}} with the {{observation}} of physical behaviours. When applied to designing spatial confi gurations, correlations between occupation, experience and spatial properties as implicit performances require mapping that can be abstracted into notations for algorithmic encoding. The <b>algorithmic</b> <b>descriptions</b> of space are calibrated by physical models. Both algorithms and catalogues of physical models are weighted and compiled into a heuristic design system that enables the generation of spatial confi gurations based on behaviours of occupation observed in real spaces. The final designs of this Implicit Space course constitute diagrammatic building configurations designed from the inside-out through encoded occupant experiences...|$|R
40|$|The paper {{describes}} {{a system for}} executable papers for publishers enabling them to reuse content and to generate further advances of science and engineering. The executable <b>algorithmic</b> <b>descriptions</b> within a paper are presented in natural language sentences and basic code, thereby making long term compatibility absolute. Authors are required to use publicly numerical libraries on the Internet or references to publications with executable papers. As used by authors the system automatically creates a web of algorithmic knowledge on the Internet. Novelty of new algorithms in publications can be evaluated by automated tools available to authors, reviewers and readers of scientific papers published...|$|R
40|$|AbstractIn {{this paper}} we discuss how any recursively {{enumerable}} language can be generated using a distributed splicing system with a fixed number of nine test tubes. This number has been recently reduced by other authors, and in this work we try to give an insightful <b>algorithmic</b> <b>description</b> {{of this kind of}} systems...|$|E
40|$|In this work, we have formalized and {{verified}} {{a hardware}} {{implementation of the}} Table-Driven algorithm for the floating-point exponential function. We have used a hierarchical approach enabling the verification of this function from the gate level implementation up to a behavioral specification adapted from the high level <b>algorithmic</b> <b>description</b> written by Harrison [3]...|$|E
40|$|We {{consider}} the classical resource-constrained project scheduling problem (RCPSP). The paper gives a detailed <b>algorithmic</b> <b>description</b> and provides theoretical results on two widely known problem specific heuristics: The serial and the parallel scheduling scheme. An in-depth computational study compares both schemes when applied as a deterministic single-pass and a probabilistic multi-pass (sampling) method...|$|E
40|$|The {{complexity}} of modern communication systems, {{particularly in the}} wireless domain, grows at an astounding rate, which causes increasing time-to-market. In order to provide a shorter timeto-market, mostly manually performed design tasks are to be overcome and an automation of the design process is needed. This paper presents approaches for automated design steps starting with high level <b>algorithmic</b> <b>descriptions.</b> An environment for automatic floating-point to fixedpoint conversion is presented. Based on the generation of evaluation metrics, automatic HW/SW partitioning is performed. Furthermore, the integration of those tools {{as part of a}} consistent development environment is described. ...|$|R
40|$|International audienceRecent computing-oriented FPGAs feature DSP blocks {{including}} small embedded multipliers. A large integer multiplier, {{for instance}} for a double-precision floating-point multiplier, consumes {{many of these}} DSP blocks. This article studies three non-standard implementation techniques of large multipliers: the Karatsuba-Ofman algorithm, non-standard multiplier tiling, and specialized squarers. They allow for large multipliers working at the peak frequency of the DSP blocks while reducing the DSP block usage. Their overhead in term of logic resources, if any, is much {{lower than that of}} emulating embedded multipliers. Their latency overhead, if any, is very small. Complete <b>algorithmic</b> <b>descriptions</b> are provided, carefully mapped on recent Xilinx and Altera devices, and validated by synthesis results...|$|R
40|$|Abstract — The {{growing number}} of {{transmission}} standards and technologies leads to an increased interest in software defined radios (SDRs). To provide the required computing power for these SDRs, multi processor architectures are considered as a realistic approach. To achieve an optimum speedup, the <b>algorithmic</b> <b>descriptions</b> of the digital baseband processing tasks have to be adequate for these architectures. In this work two approaches for parallel Viterbi decoding on a symmetric multi processor platform are presented, {{which are based on}} segmentation of the code trellis in state and time direction. The resulting speedup of these approaches is evaluated on an ARM MPCore platform. Index Terms—Viterbi decoder, symmetric multi processor, MPCore, overlapping, software defined radio I...|$|R
40|$|AbstractThis paper {{presents}} the prototype design of an algebraic computation system that manipulates algebraic quantities as generic objects using order-sorted algebra as the underlying model. The resulting programs have {{a form that}} {{is closely related to}} the <b>algorithmic</b> <b>description</b> of a problem, but with the security of full type checking in a compact, natural style...|$|E
40|$|Computer-aided {{synthesis}} of digital circuits from behavioural level specifications offers {{an effective way}} to deal with the increasing complexity of digital hardware design. A high-level synthesis tool transforms an abstract <b>algorithmic</b> <b>description</b> into a detailed register transfer level implementation. Even though considerable research has taken place, regarding high-level synthesis, practical implementations are just emerging. This happens due to the fac...|$|E
40|$|International audienceWe study {{rational}} ruled {{surfaces and}} μ-bases which were recently considered {{in a series}} of articles by Chen and coworkers. We give short and conceptual proofs with geometric insights and efficient algorithms. In particular, we provide a method to reparameterize an improper parameterization and we also briefly explain how to deal with approximate input data. Finally we provide an <b>algorithmic</b> <b>description</b> of self-intersection loci...|$|E
40|$|AbstractThe m-ary {{method for}} {{computing}} xE partitions {{the bits of}} the integer E into words of constant length, and then performs as many multiplications as there are nonzero words. Variable length partitioning strategies have been suggested {{to reduce the number}} of nonzero words, and thus, the total number of multiplications. Algorithms for exponentiation using such partitioning strategies are termed sliding window techniques. In this paper, we give <b>algorithmic</b> <b>descriptions</b> of two recently proposed sliding window techniques, and calculate the average number of multiplications by modeling the partitioning process as a Markov chain. We tabulate the optimal values of the partitioning parameters, and show that the sliding window algorithms require up to 8 % fewer multiplications than the m-ary method...|$|R
40|$|We {{are dealing}} with the optimal, i. e. densest {{packings}} of congruent circles into a unit square. In the recent years we built a numerically reliable method using interval arithmetic computations, which can be regarded as a `computer assisted proof'. A very efficient algorithm for eliminating large sets of suboptimal points is well known from earlier, non-interval computer methods. The paper presents an interval-based version of this tool, implemented as an accelerating device of an interval branch-and-bound optimization algorithm. In order to satisfy the requirements of a computer proof, detailed <b>algorithmic</b> <b>descriptions</b> and a proof of correctness are provided. The elimination method {{played a key role in}} solving the earlier open problems of packing 28, 29, and 30 circles...|$|R
40|$|This paper {{analyzes}} software radio processor {{load and}} algorithm speed for different OFDM physical layer tasks. Starting {{with a single}} core DSP implementation, the most time consuming tasks are evaluated. After that, the matrix inversion for Wiener filter channel estimation is {{used to evaluate the}} algorithm behaviour on a multi-processor platform. We will show that parallel algorithms used for hardware realizations (e. g. systolic arrays) are not tailored to multi-processor architectures, due to their fine granularity. This leads to the conclusion that different <b>algorithmic</b> <b>descriptions</b> for the physical layer tasks have to be developed to provide an optimum fit to multi-processor architectures. Exemplarily an alternative optimization network based channel decoding algorithm is presented...|$|R
