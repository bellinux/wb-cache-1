11|9|Public
40|$|Thesis (M. S.) [...] Wichita State University, College of Engineering, Dept. of Electrical and Computer Engineering. "December 2005. "Watermarking channel {{capacity}} {{is defined as}} the highest rate in bits per image of information that can be embedded with arbitrarily low probability of detection error under a given distortion between original and watermarked image. It is analogous to {{channel capacity}} defined in information theory. A mathematical method of decision-making called Game Theory, in which a competitive situation involving two parties is analyzed to determine the optimal solution for an interested party is very popular in information theory. Our approach for finding the amount of information that can be embedded in a digital image uses some principles of this theory. The two parties involved here are the information hider and the attacker. The information hider and attacker have a limit on the maximum amount of distortion each can introduce. The concept of game theory can be applied to watermarking as, information hider tries to embed as much information as he can constrained to maximum <b>allowable</b> <b>distortion</b> and the attacker tries to remove the embedded information by attacking the watermarked data constrained to maximum <b>allowable</b> <b>distortion.</b> Experiments are conducted by considering the above scenario. The results and conclusions are discussed in later sections of this thesis...|$|E
3000|$|With this {{sensor network}} {{configuration}} and modeling assumptions, we first present the optimum power allocation problem assuming CSIR and full CSIT in Section 2 -A and then formulate the problem assuming partial CSIT using quantized channel feedback in Section 2 -B. Note that power allocation here {{refers to the}} power control of CH transmitters for transmission over a single fading block {{as a function of}} CSIT, and long-term average power refers to the transmit power averaged over infinitely many fading blocks and over the number of CH transmitters. The performance metric used in this paper is distortion outage, or distortion outage probability, which is defined as the probability that the instantaneous distortion D at the FC (which, for a given fading block is a random variable) exceeds a maximum <b>allowable</b> <b>distortion</b> threshold D [...]...|$|E
40|$|Invisible Digital watermarks {{have been}} {{proposed}} as a method for discouraging illicit copying and distribution of copyright material. In recent years it has been recognized that embedding information in a transform domain leads to more robust watermarks. In particular, several approaches based on the Wavelet Transform {{have been proposed}} {{to address the problem}} of image watermarking. The advantage of the wavelet transform relative to the DFT or DCT is that it allows for localized watermarking of the image. A major di culty, however, in watermarking in any transform domain lies in the fact that constraints on the <b>allowable</b> <b>distortion</b> at any pixel are speci ed in the spatial domain. In order to insert an invisible watermark, the current trend has been to model the Human Visual System (HVS) and specify a masking function which yields the <b>allowable</b> <b>distortion</b> for any pixel. This complex function combines contrast, luminance, color, texture and edges. The watermark is then inserted in the transform domain and the inverse transform computed. The watermark is nally adjusted to satisfy the constraints on the pixel distortions. However this method is highly suboptimal since it leads to irreversible losses at the embedding stage because the watermark is being adjusted in the spatial domain with no care for the consequences in the transform domain. The central contribution of the paper is the proposal of an approach which takes into account the spatial domain constraints in an optimal fashion. The main idea is to structure the watermark embedding as a linear programming problem in which we wish to maximize the strength of the watermark subject to a set of linear constraints on the pixel distortions as determined by amasking function. We consider the Haar wavelet and Daubechies 4 -tap filter in conjunction with a masking function based on a non-stationary Gaussian model, but the algorithm is applicable to any combination of transform and masking functions. Our results indicate that the proposed approach performs well against lossy compression such as JPEG and other types of filtering which do not change the geometry of the image...|$|E
40|$|We {{propose a}} compress-forward (CF) {{strategy}} for relaying over parallel Gaussian channels. The proposed CF strategy compresses received signals at the relay under a constraint {{on the product}} of their <b>allowable</b> <b>distortions</b> and forwards them to the destination over all subchannels. Power allocation over all subchannels at both the source and the relay and bit allocation for compressing received signals at the relay are jointly optimized under a total power constraint on each node. The proposed CF strategy is shown to generalize a previous CF strategy in which the relay compresses the received signal from the i-th incoming subchannel and forwards it only through the i-th outgoing subchannel. We show that, if the incoming and outgoing subchannels at the relay are suitably matched, both CF strategies have similar performance. Otherwise, the gap between their performances becomes significant. Index Terms — Relay, compress-forward, resource allocation, OFDM, parallel Gaussian channel...|$|R
40|$|Distortion-based {{crest factor}} {{reduction}} (CFR) algorithms were studied in orthogonal frequency division multiplexing (OFDM) and multiple-input multiple-output (MIMO) OFDM systems {{to reduce the}} nonlinear distortion and improve the power efficiency of the transmitter front-end. First, definitions of peak-to-average-power ratio (PAR) were clarified based on the power efficiency improvement consideration in the MIMO-OFDM systems. Next, error vector magnitude (EVM) {{was used as the}} in-band performance-evaluating metric. Statistical analysis of EVM was performed to provide concrete thresholds for the amount of <b>allowable</b> <b>distortions</b> from each source to meet EVM requirements in the standard. Furthermore, an effective CFR technique, constrained clipping, was proposed to drastically reduce the PAR while satisfying any given in-band EVM and out-of-band spectral mask constraints. Constrained clipping has low computational complexity and can be easily extended to the multiple-user OFDM environment. Finally, signal-to-noise-and-distortion ratio (SNDR) analysis for transceiver nonlinearities in the additive white Gaussian noise channel was investigated. An analytical solution was presented for maximizing the transceiver SNDR for any given set of nonlinear transmitter polynomial coefficients. Additionally, mutually inverse pair of transceiver nonlinearities was shown to be SNDR-optimal only in the noise-free case. Ph. D. Committee Chair: Zhou, G. Tong; Committee Member: Kenney, J. Stevenson; Committee Member: Li, Ye (Geoffrey); Committee Member: Ma, Xiaoli; Committee Member: Yuan, Min...|$|R
40|$|We {{describe}} a methodology to maximize slow-light pulse delay {{subject to a}} constraint on the <b>allowable</b> pulse <b>distortion.</b> We show that optimizing over {{a larger number of}} physical variables can increase the distortion-constrained delay. We demonstrate these concepts by comparing the optimum slow-light pulse delay achievable using a single Lorentzian gain line with that achievable using a pair of closely-spaced gain lines. We predict that distortion management using a gain doublet can provide approximately a factor of 2 increase in slow-light pulse delay as compared with the optimum single-line delay. Experimental results employing Brillouin gain in optical fiber confirm our theoretical predictions...|$|R
40|$|We {{present a}} simple new {{criterion}} for classification, based on principles from lossy data compression. The criterion assigns a test sample {{to the class}} that uses the minimum number of additional bits to code the test sample, subject to an <b>allowable</b> <b>distortion.</b> We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classifiers. Theoretical results provide new insights into relationships among popular classifiers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classification criterion and its kernel and local versions perform competitively against existing classifiers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-specific information. ...|$|E
40|$|Multiple {{description}} coding (MDC) is {{an error}} resilient source coding scheme that creates multiple {{descriptions of the}} source {{with the aim of}} providing an acceptable reconstruction quality when only one description is received, and improved quality as more descriptions become available. Recently, we developed a matching pursuit multiple description video coder (MP-MDVC) based on a three loop structure originally proposed by Reibman and her colleagues (MTDC). While the MP-MDVC outperforms the discrete cosine transform based MTDC, it is not optimized for lossy environments. In this paper, we extend MP-MDVC by considering the network loss characteristics when coding multiple descriptions. In particular, we propose a fast steepest descent algorithm for creating multiple descriptions that results in minimum expected distortion, given network outage probability, bandwidth constraints, and maximum <b>allowable</b> <b>distortion</b> for each description. Analytical and experimental results show that by taking network loss characteristics into account, our approach outperforms existing MP-MDVC techniques. 1...|$|E
40|$|We {{propose a}} new pre-roll delay-distortion {{optimization}} (DDO) framework that allows {{determination of the}} minimum pre-roll delay and distortion while ensuring continuous playback for on-demand content-adaptive video streaming over limited bitrate networks. The input video is first divided into temporal segments, which are assigned a relevance weight and a maximum distortion level, called relevance-distortion policy, which may be specified by the user. The system then encodes the input video according to the specified relevance-distortion policy, whereby the optimal spatial and temporal resolutions and quantization parameters, also called encoding parameters, are selected for each temporal segment. The optimal encoding parameters are computed using a novel, multi-objective optimization formulation, where a relevance weighted distortion measure and pre-roll delay are jointly minimized under maximum allowable buffer size, continuous playback, and maximum <b>allowable</b> <b>distortion</b> constraints. The performance of the system has been demonstrated for on-demand streaming of soccer videos with substantial improvement in the weighted distortion without any increase in pre-roll delay over a very low-bitrate network using AVC/H. 264 encoding...|$|E
40|$|Requirements, parameters, and {{constraints}} influencing {{the design of}} a line scanner camera system for imaging from orbit are described. The driving requirements are the area of the site to be imaged in each pass and the resolution as determined by the size of a pixel projected on the planet surface. <b>Allowable</b> jitter and <b>distortion,</b> required SNR, maximum data rate, and maximum variation of the emission angle during imaging are among the constraints. Simplified equations incorporating these requirements, constraints, and parameters are developed and presented...|$|R
30|$|Recently, some {{works for}} calculating {{watermark}} capacity {{are reported in}} the literature. Moulin used the concept of information hiding to calculate the capacity by considering watermarking as an information channel between transmitter and receiver [1, 2]. Barni et al. in [3, 4] introduced methods for capacity estimation based on DCT. Voloshynovisky introduced Noise Visibility Function (NVF) which estimates the <b>allowable</b> invisible <b>distortion</b> in each pixel according to its neighbor's values [5, 6]. Zhang et al. in [7 – 9] and authors in [10, 11] showed how to use heuristic methods to determine the capacity. In addition, some works are reported in coding system and codebooks to reduce distortion in the watermarked image [12, 13]. We shall note that these methods use different approaches to find the capacity, and the estimated capacity values have a diverged range from 0.002 [*]bpp (bits per pixel) to 1.3 [*]bpp [9].|$|R
40|$|The article {{describes}} the results of assessing {{the provisions of the}} project «Energy Strategy of Ukraine till 2035 » in terms of the validity of the main target parameters of the strategy. The assessment is based on critical rethinking of the essence of choosing certain indicators as strategic target parameters. This gave the opportunity to prove the existing rigidity of approaches to developing energy policies in Ukraine, which is confirmed by studying the national energy policy foundations in accordance with the chronology of the emergence of the state energy strategies. In order to check the validity of the provisions of the project «Energy Strategy of Ukraine till 2035 » it has been proposed to use materiality guideline, as it enables to set a limit of <b>allowable</b> information <b>distortion,</b> exceedance of which becomes critical from the point of impact on the process of making strategic decisions. It has been found that the forecast balances of consuming the fuel and energy resources till 2035 were made up without taking into account the essential information about the energy intensity of GDP and its structure, which gives grounds to draw a conclusion on invalidity of some target parameters of the project...|$|R
40|$|Abstract. We {{present a}} simple new {{criterion}} for classification, based on principles from lossy data compression. The criterion assigns a test sample {{to the class}} that uses the minimum number of additional bits to code the test sample, subject to an <b>allowable</b> <b>distortion.</b> We demonstrate the asymptotic optimality of this criterion for Gaussian distributions and analyze its relationships to classical classifiers. The theoretical results clarify the connections between our approach and popular classifiers such as MAP, RDA, k-NN, and SVM, as well as unsupervised methods based on lossy coding. Our formulation induces several good effects on the resulting classifier. First, minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small sample setting. Second, compression provides a uniform means of handling classes of varying dimension. The new criterion and its kernel and local versions perform competitively on synthetic examples, {{as well as on}} real imagery data such as handwritten digits and face images. On these problems, the performance of our simple classifier approaches the best reported results, without using domain-specific information. All MATLAB code and classification results are publicly available for peer evaluation a...|$|E
40|$|Invisible Digital watermarks {{have been}} {{proposed}} as a method for discouraging illicit copying and distribution of copyright material. In recent years it has been recog-nized that embedding information in a transform domain leads to mo re robust watermarks. A major diÆculty in watermarking in a transform domain {{lies in the fact}} that constraints on the <b>allowable</b> <b>distortion</b> at any pixel may be specied in the spatial domain. The central contribution of the paper is the proposal of an approach which takes into account spatial domain constraints in an optimal fashion. The main idea is to structure the watermark embedding as a linear programming problem in which we wish to maximize the strength of the watermark subject to a set of lin-ear constraints on the pixel distortions as determined by a masking function. We consider the special cases of embedding in the DCT domain and wavelet domain using the Haar wavelet and Daubechies 4 -tap lter in conjunction with a masking function based on a non-stationary Gaussian model, but the algorithm is applicable to any combination of transform and masking functions. Our results indicate that the proposed approach performs well against lossy compression such as JPEG and other types of ltering which do not change the geometry of the image...|$|E
40|$|Optimal {{transform}} domain watermark embedding via linear programming PEREIRA, Shelby, VOLOSHYNOVSKYY, Svyatoslav, PUN, Thierry Invisible Digital watermarks {{have been proposed}} as a method for discouraging illicit copying and distribution of copyright material. In recent years it has been recognized that embedding information in a {{transform domain}} leads to mo re robust watermarks. A major difficulty inwatermarking in a transform domain {{lies in the fact}} that constraints on the <b>allowable</b> <b>distortion</b> at any pixel may be speci ed in the spatial domain. The central contribution of the paper is the proposal of an approach which takes into account spatial domain constraints in an optimal fashion. The main idea is to structure the watermark embedding as a linear programming problem in which we wish to maximize the strength of the watermark subject to a set of lin- ear constraints on the pixel distortions as determined by a masking function. We consider the special cases of embedding in the DCT domain and wavelet domain using the Haar wavelet and Daubechies 4 -tap lter in conjunction with a masking function based on a non-stationary Gaussian model, but the algorithm is applicable to any combination of transform and masking functions. Our results indicate that [ [...] . ...|$|E
40|$|International Telemetering Conference Proceedings / September 27 - 29, 1971 / Washington Hilton Hotel, Washington, D. C. Government and {{industrial}} telemetry users have the frequent requirement to procure telemetry receivers which will faithfully receive and demodulate physical and electrical phenomena occuring at the transmitting source. Such receiver output data accuracy requirements must {{be translated into}} meaningful receiver procurement specifications. Movement of the transmitting source, dynamic changes in the RF path through which the telemetry signal is transmitted, and the distorting effects of electrical circuitry in the receiver itself, all contribute to distortion of the baseband data. The parameters affecting baseband data quality must, therefore, be quantized in terms of telemetry receiver performance specifications. Where possible, the specifications should relate to input/ output parameters of the receiver; {{this may not be}} possible in all cases, however. The methodology for translation of telemetry signal data quality requirements into electrical specifications for a telemetry receiver is treated in this paper. Specification of RF and IF channel characteristics are covered, and AGC, AFC, and AM rejection circuitry requirements are analyzed in terms of received-signal amplitude and frequency dynamics. Overall distortion in the telemetry receiver is characterized for FDM signals by specifications whose reference is the noise-loading technique; for TDM signals distortion is characterized in terms of receiver transient response. FM demodulator specifications are treated, and specifications for <b>allowable</b> baseband <b>distortion</b> due to co-channel RF interference are introduced. Finally, the electrical performance requirements of the baseband channel are characterized...|$|R
40|$|Due {{to space}} {{limitations}} in urban areas, underground construction {{has become a}} common practice worldwide. When using deep excavations, excessive lateral movements are a major concern because they can lead to significant displacements and rotations in adjacent structures. Therefore, accurate predictions of lateral wall deflections and surface settlements are important design criteria in the analysis and design of excavation support systems. This {{research shows that the}} current design methods, based on plane strain analyses, are not accurate for designing excavation support systems and that fully three-dimensional (3 D) analyses including wall installation effects are needed. A complete 3 D finite element simulation of the wall installation at the Chicago and State Street excavation case history is carried out to show the effects of modeling: (i) the installation sequence of the supporting wall, (ii) the excavation method for the wall, and (iii) existing adjacent infrastructure. This model is the starting point of a series of parametric analyses that show the effects of the system stiffness on the resulting excavation-related ground movements. Furthermore, a deformation-based methodology for the analysis and design of excavation support systems is proposed in order to guide the engineer in the different stages of the design. The methodology is condensed in comprehensive flow charts that allow the designer to size the wall and supports, given the <b>allowable</b> soil <b>distortion</b> of adjacent structures or predict ground movements, given data about the soil and support system...|$|R
40|$|In the article, {{the authors}} {{developed}} {{a method for}} detecting speech activity for an automated system for recognizing critical use of speeches with wavelet parameterization of speech signal and classification at intervals of “language”/“pause” using a curvilinear neural network. The method of wavelet-parametrization proposed by the authors allows choosing the optimal parameters of wavelet transformation {{in accordance with the}} user-specified error of presentation of speech signal. Also, the method allows estimating the loss of information depending on the selected parameters of continuous wavelet transformation (NPP), which allowed {{to reduce the number of}} scalable coefficients of the LVP of the speech signal in order of magnitude with the <b>allowable</b> degree of <b>distortion</b> of the local spectrum of the LVP. An algorithm for detecting speech activity with a curvilinear neural network classifier is also proposed, which shows the high quality of segmentation of speech signals at intervals "language" / "pause" and is resistant to the presence in the speech signal of narrowband noise and technogenic noise due to the inherent properties of the curvilinear neural network...|$|R
40|$|Abstract—In this paper, {{based on}} ideas from lossy data coding and compression, {{we present a}} simple but {{effective}} technique for segmenting multivariate mixed data that are drawn from a mixture of Gaussian distributions, which are allowed to be almost degenerate. The goal {{is to find the}} optimal segmentation that minimizes the overall coding length of the segmented data, subject to a given distortion. By analyzing the coding length/rate of mixed data, we formally establish some strong connections of data segmentation to many fundamental concepts in lossy data compression and rate-distortion theory. We show that a deterministic segmentation is approximately the (asymptotically) optimal solution for compressing mixed data. We propose a very simple and effective algorithm that depends on a single parameter, the <b>allowable</b> <b>distortion.</b> At any given distortion, the algorithm automatically determines the corresponding number and dimension of the groups and does not involve any parameter estimation. Simulation results reveal intriguing phase-transition-like behaviors of the number of segments when changing the level of distortion or the amount of outliers. Finally, we demonstrate how this technique can be readily applied to segment real imagery and bioinformatic data. Index Terms—Multivariate mixed data, data segmentation, data clustering, rate distortion, lossy coding, lossy compression, image segmentation, microarray data clustering. ...|$|E
40|$|Future NASA {{missions}} demand increased {{data rates}} in satellite communications for near real-time transmission of {{large volumes of}} remote data. Increased data rates necessitate higher order digital modulation schemes and larger system bandwidth, which place stricter requirements on the <b>allowable</b> <b>distortion</b> caused by the high-power amplifier, or the traveling-wave-tube amplifier (TWTA). In particular, intersymbol interference caused by the TWTA becomes a major consideration for accurate data detection at the receiver. Experimentally investigating {{the effects of the}} physical TWTA on intersymbol interference would be prohibitively expensive, as it would require manufacturing numerous amplifiers in addition to acquiring the required digital hardware. Thus, an accurate computational model is essential to predict the effects of the TWTA on system-level performance when a communication system is being designed with adequate digital integrity for high data rates. A fully three-dimensional, time-dependent, TWT interaction model has been developed using the electromagnetic particle-in-cell code MAFIA (Solution of Maxwell's equations by the Finite-Integration-Algorithm). It comprehensively takes into account the effects of frequency-dependent AM (amplitude modulation) /AM and AM/PM (phase modulation) conversion, gain and phase ripple due to reflections, drive-induced oscillations, harmonic generation, intermodulation products, and backward waves. This physics-based TWT model can be used to give a direct description {{of the effects of the}} nonlinear TWT on the operational signal as a function of the physical device. Users can define arbitrary excitation functions so that higher order modulated digital signals can be used as input and that computations can directly correlate intersymbol interference with TWT parameters. Standard practice involves using communication-system-level software packages, such as SPW, to predict if adequate signal detection will be achieved. These models use a nonlinear, black-box model to represent the TWTA. The models vary in complexity, but most make several assumptions regarding the operation of the high-power amplifier. When the MAFIA TWT interaction model was used, these assumptions were found to be in significant error. In addition, digital signal performance, including intersymbol interference, was compared using direct data input into the MAFIA model and using the system-level analysis tool SPW for several higher order modulation schemes. Results show significant differences in predicted degradation between SPW and MAFIA simulations, demonstrating the significance of the TWTA approximations made in the SPW model on digital signal performance. For example, a comparison of the SPW and MAFIA output constellation diagrams for a 16 -ary quadrature amplitude modulation (16 -QAM) signal (data shown only for second and fourth quadrants) is shown. The upper-bound degradation was calculated from the corresponding eye diagrams. In comparison to SPW simulations, the MAFIA data resulted in a 3. 6 -dB larger degradation...|$|E

