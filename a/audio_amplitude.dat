13|15|Public
5000|$|<b>Audio</b> <b>amplitude</b> effects (normalization, {{fading in}} and out, amplification) helps to correct audio track; ...|$|E
40|$|The {{frequency}} subband scale-factors {{are fundamental}} components of MPEG- 1 audio encoded bitstreams. Examination of scale-factor weights {{is sufficient for}} the establishment of an <b>audio</b> <b>amplitude</b> profile of an audio track. If, for sports programme TV broadcasts, the <b>audio</b> <b>amplitude</b> is assumed to primarily reflect the noise level exhibited by the commentator (and/or attending spectators), then, this vocal reaction to the significance of unfolding events may be used as a basis for summarisation. i. e. by relying on the exhilaration, or otherwise, expressed by the commentator/spectators, individual clips of the programme (e. g. camera shots), may be ranked according to their relative significance. A summary may then be produced by amalgamating (chronologically) any number of these clips corresponding to selected audio peaks...|$|E
30|$|For volume change {{operation}} (all {{samples in}} value are scaled {{with the same}} factor), we have δ 1 = δ 2 = δ 3 = 0 and μ > 0. It indicates that w(i) can be recovered correctly under the linear change of <b>audio</b> <b>amplitude.</b> In other words, the watermark is immune to volume change attack.|$|E
5000|$|... 405-line is System A in the CCIR {{assignment}} of broadcast systems. The <b>audio</b> uses <b>amplitude</b> modulation {{rather than the}} frequency modulation in use on modern analogue systems. In addition, the system was broadcast in an aspect ratio of 5:4 until 3 April 1950, when it changed to the more common 4:3 format.|$|R
5000|$|The company sees {{its product}} range as an {{alternative}} to the traditional cone speaker or high <b>amplitude</b> <b>audio</b> exciters. Feonic drives result in non destructive vibrations and can be transmitted through existing or custom made structures and have various advantages over conventional technology, including: ...|$|R
40|$|Microphone {{used as a}} phonocardiograph {{transducer}} monitors small <b>amplitude</b> <b>audio</b> {{signals in}} the presence of large shock loads and high humidity. It contains a lead zirconate-lead titanate piezoelectric plate encapsulated in a flexible polyurethane resin. The resin is contained in a sealed nylon case having a diameter of less than one inch...|$|R
40|$|Waveform {{clipping}} is {{a common}} problem in digital audio signal processing. Clipping effect changes the <b>audio</b> <b>amplitude</b> distribution and brings noise, which results in the decrease of audio signal-to-noise ratio (SNR). To estimate the degree of clipping effect, we propose a method for SNR estimation of clipped signals base on <b>audio</b> <b>amplitude</b> distribution. We first locate the clipping intervals of audio signal with high precision based on {{the differences between the}} amplitude probability density functions (PDFs) of normal and clipped audio signals. Then, the clipped amplitude PDF is restored using Gamma distribution to estimate the non-clipped amplitude distribution. Finally, based on the restored PDF, the estimated SNR of distorted audio signals are calculated, which can indicate the degree of clipping effect. Experiments on 3200 audios show that our approach can achieve a low error of 0. 017 in clipped values locating, and an error less than 0. 5 dB in SNR estimation. Index Terms—amplitude PDF, Gamma distribution, clipping effect, SNR estimatio...|$|E
40|$|In today’s {{fast paced}} world, the time {{available}} to watch long sports programmes is decreasing, {{while the number}} of sports channels is rapidly increasing. Many viewers desire the facility to watch just the highlights of sports events. This paper presents a simple, but effective, method for generating sports video highlights summaries. Our method detects semantically important events in sports programmes by using the Scale Factors in the MPEG audio bitstream to generate an <b>audio</b> <b>amplitude</b> profile of the program. The Scale Factors for the subbands corresponding to the voice bandwidth give a strong indication of the level of commentator and/or spectator excitement. When periods of sustained high <b>audio</b> <b>amplitude</b> have been detected and ranked, the corresponding video shots may be concatenated to produce a summary of the program highlights. Our method uses only the Scale Factor information that is directly accessible from the MPEG bitstream, without any decoding, leading to highly efficient computation. It is also rather more generic than many existing techniques, being particularly suitable for the more popular sports televised in Ireland such as soccer, Gaelic football, hurling, rugby, horse racing and motor racing...|$|E
40|$|The {{imminent}} {{rapid expansion}} {{in the number of}} TV channels is driving the need for efficient digital video indexing, browsing and playback systems. For the past four years, the Centre for Digital Video Processing in DCU has been working towards the provision of such a system. The current stage of development is demonstrated on our Web-based digital video system called Físchlár [1, 2], which is now in hourly use by over 1000 registered users. At present a user can pre-set the recording of TV broadcast programmes and can choose from a set of different browser interfaces which allow navigation through the recorded programmes. As our research has developed we have been adding increased functionality such as personalisation and programme recommendation, automatic recording, SMS/WAP/PDA alerting, searching, summarising and so on. There have been many approaches to automatic highlight detection and summarization of sports programmes [3 - 6]. However, most methods are oriented to particular sports and/or specific edit effects or need much computational effort. Therefore we decided to take a more basic and generic approach based on the observation that, in most common sports coverage with a commentator and spectators, exciting events are usually characterized by an increase in the <b>audio</b> <b>amplitude,</b> particularly in the speech band. Measurement of the <b>audio</b> <b>amplitude</b> is described in Section II and a case study is presented in Section III. Section IV summarises results while conclusions and areas of continuing and future work are outlined in Section V...|$|E
40|$|Abstract—The {{potential}} use of non-linear speech features has {{not been}} investigated for music analysis although other commonly used speech features like Mel Frequency Ceptral Coefficients (MFCC) and pitch have been used extensively. In this paper, we assume an audio signal to be a sum of modulated sinusoidal and then use the energy separation algorithm to decompose the <b>audio</b> into <b>amplitude</b> and frequency modulation components using the non-linear Teager-Kaiser energy operator. We first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different Mel spaced frequency bands and show {{that they have the}} ability to discriminate. The proposed method based on Kullback-Leibler divergence measure is evaluated using a set of Indian classical songs from three different artists. Experimental results show that the discrimination ability is evident in certain low and mid frequency bands (200 - 1500 Hz) ...|$|R
40|$|The audio {{watermarking}} method presented below offers {{copyright protection}} to an audio signal by modifying its temporal characteristics. The amount of modification embedded {{is limited by}} the necessity that the output signal must not be perceptually different from the original one. The watermarking method presented here does not require the original signal for watermark detection. The watermark key is simply a seed known only by the copyright owner. This seed creates the watermark signal to be embedded. Watermark embedding depends also on the <b>audio</b> signal <b>amplitude</b> in a way that minimizes the audibility of the watermark signal. The embedded watermark is robust to MPEG audio coding, filtering, resampling and requantization. 1 Introduction The outstanding progress of digital technology has increased the ease with which digital data is reproduced and retransmitted. However, since the advantages of such a progress are broadly available, they offer equally increasing potential to both legal [...] ...|$|R
40|$|The {{potential}} use of non-linear speech features has {{not been}} investigated for music analysis although other commonly used speech features like Mel Frequency Ceptral Coefficients (MFCC) and pitch have been used extensively. In this paper, we assume an audio signal to be a sum of modulated sinusoidal and then use the energy separation algorithm to decompose the <b>audio</b> into <b>amplitude</b> and frequency modulation components using the non-linear Teager-Kaiser energy operator. We first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different Mel spaced frequency bands and show {{that they have the}} ability to discriminate. The proposed method based on Kullback-Leibler divergence measure is evaluated using a set of Indian classical songs from three different artists. Experimental results show that the discrimination ability is evident in certain low and mid frequency bands (200 - 1500 Hz). Comment: 5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics Applications (ISIEA...|$|R
40|$|Abstract. Extensive testing {{shows that}} the audio Zernike moments in lower orders are very robust to common signal {{processing}} operations, such as MP 3 compression, low-pass filtering, etc. Based on the observations, in this paper, a robust watermark scheme is proposed by embedding the bits into the low-order moments. By analyzing and deducting the linear relationship between the <b>audio</b> <b>amplitude</b> and moments, watermarking the low-order moments is achieved in time domain by scaling sample values directly. Thus, the degradation in audio reconstruction from {{a limited number of}} watermarked Zernike moments is avoided. Experimental works show that the proposed algorithm achieves strong robustness performance due to the superiorities of exploited low-order moments. ...|$|E
40|$|The {{frequency}} subband scalefactors {{are fundamental}} components of MPEG- 1 audio encoded bitstreams. Examination of scalefactor weights {{is sufficient for}} the establishment of an <b>audio</b> <b>amplitude</b> profile of an audio track. If, for sports programme TV broadcasts, the <b>audio</b> <b>amplitude</b> is assumed to primarily reflect the noise level exhibited by the commentator (and/or attending spectators), then, this vocal reaction to the significance of unfolding events may be used as a basis for summarisation. i. e. by relying on the exhilaration, or otherwise, expressed by the commentator/spectators, individual clips of the programme (e. g. camera shots), may be ranked according to their relative significance. A summary may then be produced by amalgamating (chronologically) any number of these clips corresponding to selected audio peaks. 1. BACKGROUND The Centre for Digital Video Processing at Dublin City University conducts concentrated research in developing innovative technologies fundamental to the realisation of efficient video content management. The current stage of development is demonstrated in the web-based digital video system, Fschlr [1]. Fschlr captures TV broadcast programmes and encodes them according to the MPEG- 1 (Layer-II) video standard. Fschlr then provides for efficient analysing, browsing, and viewing of the recorded content. At present, a user can pre-set the recording of programmes selected from an online TV broadcast schedule, and then choose from a set of different browser interfaces which allow navigation through the recorded material. As the research develops, increased options such as personalisation and programme recommendation, automatic recording, SMS/PDA/WAP alerting, searching, etc. are being plugged in and utilised. [...] ...|$|E
40|$|Abstract — Recorded {{meetings}} are useful only if people can find, access, and browse them easily. Key-frames and video skims are useful representations that can enable quick previewing {{of the content}} without actually watching a meeting recording from beginning to end. This paper proposes a new method for creating meeting video skims based on audio and visual activity analysis together with text analysis. Audio activity analysis is performed by analyzing sound directions — indicating different speakers — and <b>audio</b> <b>amplitude.</b> Detection of important visual events in a meeting is achieved by analyzing the localized luminance variations in consideration with the omni-directional property of the video captured by our meeting recording system. Text analysis {{is based on the}} Term Frequency–Inverse Document Frequency measure. The resulting video skims better capture the important meeting content compared to the skims obtained by uniform sampling. 1...|$|E
40|$|The {{purpose of}} this study was to {{determine}} the feasibility of using an ultrasound contrast agent test bolus to deter-mine optimum bolus timing for three-dimensional (3 D) gadolinium (Gd) -enhanced magnetic resonance angiogra-phy (MRA). Small test doses of ultrasound contrast agent (0. 3 ml Optison) were injected intravenously followed imme-diately by a 20 ml saline flush. Arrival of the contrast agent was detected by spectral Doppler ultrasound (US). This technique was implemented in patients undergoing periph-eral vascular MRA and carotid MRA. Arrival of the US contrast agent test bolus was readily detected by the change in amplitude of the Doppler spectrum and by a huge increase in the <b>audio</b> signal <b>amplitude.</b> This contrast travel time measurement accurately guided bolus timing for 3 D Gd MRA. Bolus timing for 3 D contrast-enhanced MRA can be performed using US, thereby eliminating the problems and MR scanner time required for injecting a test bolus o...|$|R
5000|$|Any digital object may {{be dragged}} and dropped {{anywhere}} on the timeline. Once on the timeline, video can be duplicated, split, cut, muted, cropped, flipped, rotated, played backward, resized, etc., its speed can be slowed down or increased; <b>audio</b> may experience <b>amplitude</b> and delay effects, filters, tempo and rate change, reverse effect, etc.; color corrected and enhanced. VSDC Free Video Editor gives the opportunity to save an output file to a computer hard disk drive with the resolution based on the targeted device (PC, mobile devices, iPhone/iPod, MP3/MP4, Blackberry, etc.), adjust bitrate, framerate, quality or burn to DVD disc. Supported output formats: ...|$|R
40|$|The {{present study}} was {{designed}} to assess evaluations of physicians interacting with patients via the telephone. Observers used ten adjective scales which resulted in three variables: empathic, dominant, and calm. Thirty doctor-patient interactions were presented in two different communication modes: audio-only and typed transcript-only. As predicted, female listeners rated doctors as more empathic dominant, and calm, and communication modes were significantly different with audio segments rated as more empathic, dominant, and calm. Middle phases of the conversation also were evaluated more positively than greeting phases. Significant interactions between temporal phase and mode indicated that audio segments were interpreted more positively during middle phases. Also, female listeners were more sensitive to <b>audio</b> segments. Physicians' <b>amplitude</b> and speech rate were positively correlated with dominance. vocal behavior telephone medicine communication mode gender differences...|$|R
40|$|An {{earlier study}} {{compared}} audiovisual perception of speech ’produced in environmental noise ’ (Lombard speech) and speech ’produced in quiet ’ {{with the same}} environmental noise added. The results and showed that listeners make differential use of the visual information depending on the recording condition, but gave no indication of how or why this might be so. A possible confound in that study was that high audio presentation levels might account for the small visual enhancements observed for Lombard speech. This paper reports results for a second perception study using much lower acoustic presentation levels, compares them {{with the results of}} the previous study, and integrates the perception results with analyses of the audiovisual production data: face and head motion, <b>audio</b> <b>amplitude</b> (RMS), and parameters of the spectral acoustics (line spectrum pairs). Index Terms: audiovisual speech, Lombard speech, production and perception links...|$|E
40|$|Due to its advantages, quantization-based {{embedding}} {{has been}} introduced into audio watermarking to improve robustness performance. Existing audio watermarking algorithms often focus on a given attack. However, in some transmission environments, digital audio files may suffer from the different attacks. For example, effects of D/A and A/D conversions (denoted as DA/AD in this paper) on audio watermarking may be modeled as modification of signal energy, phase changes, and noises corruption. These effects present an important challenge to audio watermarking. Extensive experiments show that phase changes of audio signals caused by the DA/AD may be represented as some extent of temporal scaling. Furthermore, we analyze the performance of quantization-based audio watermarking against these attacks in the DA/AD, and present corresponding calculation expressions of {em BER} (Bit Error Rate) in case of Gaussian noise and modification of <b>audio</b> <b>amplitude,</b> and then investigate the influences of temporal scaling caused by the DA/AD on audio watermarking. As a conclusion, audio watermarking quantization-based is very susceptible to the DA/AD...|$|E
40|$|The {{perception}} of musical meter is fundamental for rhythm production and perception in much music. Underlying {{structures such as}} pulse, meter, and metrical subdivisions are often described as successive points in time. This paper investigates whether experienced musical meter may not only include such points in time, but also trajectories between the points–that is, metrical shapes. Previous studies {{have pointed out that}} {{there seems to be a}} relationship between musical meter and periodic body motion like foot tapping, head nodding and dancing. This paper investigates musical meter in music styles with an intimate relationship with dance, and whether metrical points and trajectories can be understood by investigating performers’ periodic body motion. Two motion capture studies form the empirical basis of this paper; first, a percussionist and a dancer performing Brazilian samba; second, a fiddler and two dancers performing Norwegian telespringar. The analysis showed that it seemed to be a relationship between the periodic fluctuation in <b>audio</b> <b>amplitude</b> and the performers’ periodic foot motion on sixteenth note level in samba. Furthermore, motion analysis revealed similar periodic shapes in both percussionist and dancer foot motion. In telespringar there seemed to be a relationship between the metrical beat level and the fiddler’s foot stamping. In addition, the beat duration pattern, as indicated by the fiddler’s periodic foot stamping, seemed to correspond to the shape of the dancers’ vertical body motion. The results support the view that there is a close relationship between musical meter and performers’ periodic body motion. This suggest that the underlying meter may not only include metrical points in time, but that each metrical beat/subdivision duration has a corresponding metrical trajectory with a certain shape. If this is the case, then perceivers’ and performers’ implicit knowledge of the underlying reference structure in samba and telespringar might incorporate knowledge about the underlying metrical shape...|$|E
5000|$|In the mid 1980s, visual artist Ron Rocco, {{who also}} {{developed}} {{his work at}} CAVS, employed mirrors mounted to tiny servo motors, driven by the audio signal of a synthesizer and amplified by a tube amp to reflect the beam of a laser. This created light patterns which corresponded to the <b>audio's</b> frequency and <b>amplitude.</b> Using this beam to generate video feedback and computers to process the feedback signal, Rocco created his [...] "Andro-media" [...] series of installations. Rocco later formed a collaboration with musician David Hykes, who practiced a form of Mongolian overtone chanting with The Harmonic Choir, to generate cymatic images {{from a pool of}} liquid mercury, which functioned as a liquid mirror to modulate the beam of a Helium-Neon laser from the sound thus generated. Photographs of this work {{can be found in the}} Ars Electronica catalog of 1987.|$|R
30|$|An {{alternative}} to in-encoder techniques is the post-encoder (or in-stream) techniques. To survive audio encoders, authors in[41] have embedded {{data in the}} bitstream of an ACELP codec. This technique hides data jointly with the analysis-by-synthesis codebook search. The authors applied the concept on the AMR encoder {{at a rate of}} 12.2 kbit/s and were able to hide 2 kbit/s of data in the bitstream. The quality of the stego speech is evaluated in terms of signal to noise ratio at 20.3 dB. A lossless steganography technique for G. 711 -PCMU telephony encoder has been proposed in[42]. Data in this case is represented by folded binary code which codes each sample with a value between - 127 and 127 including - 0 and + 0. One bit is embedded in 8 -bits sample which absolute amplitude is zero. Depending on the number of samples with absolute amplitudes of 0, a potential hiding rate ranging from 24 to 400 bps is obtained. To increase the hiding capacity, the same authors have introduced a semi-lossless technique for G. 711 -PCMU[43], where <b>audio</b> sample <b>amplitudes</b> are amplified with a pre-defined level ’i’. The audio signal samples with absolute amplitudes vary from 0 to i are utilized in the hiding process. For a greater hiding capacity,[44] suggested to embed data in the inactive frames of low bit-rate audio streams (i.e., 6.3 kbps) encoded by G. 723.1 source codec.|$|R
40|$|We {{measured}} the linear vestibulo-ocular reflex (LVOR) and vergence, using binocular search coils, in 3 humans. The subjects were accelerated sinusoidally at 0. 5 Hz and 0. 2 g peak acceleration, in complete darkness, while performing three different tasks: i) mental arithmetic; ii) tracking a remembered target at either 0. 34 m or 0. 14 m distance; and iii) maintaining vergence at {{either of these}} distances by means of audio biofeedback based on vergence. Subjects could control vergence using the audio feedback; there was greater convergence with the near audio target. However, {{there was no significant}} difference in vergence between the near and far remembered target conditions. With <b>audio</b> feedback, the <b>amplitude</b> of smooth tracking was not consistently different for the near and the far conditions. However, the amplitude of tracking (saccades and smooth component) in the remembered target conditions was greater for near than for far targets. These results suggest that linear VOR amplitude is not determined by vergence alone...|$|R
40|$|Version 2. 11 - Time Warp' 3 rd Nov, 2016 (view commits) This {{release is}} {{the biggest and most}} adventurous release yet. There are as many (invisible) {{modifications}} and improvements to the internal systems as there are new external features that you can see and play with. The aim was to create a solid foundation for new and exciting features both in this release and in preparation for future releases. We also open our arms to welcome two new Core Team members - Luis Lloret and Adrian Cheater. Both have made generous and substantial contributions to this release. Thank-you. Sadly we also say farewell to Jeremy Weatherford. Please extend your kind thoughts and gratitude to Jeremy for all of his contributions - in particular for turning the Windows release from a possibility into a reality. Luckily Luis has kindly stepped in to maintain the Windows installer. The main visible feature is the new scope visualisers. The overall audio output can now be visually monitored by one of three wave form visualisers. Firstly there is the individual left and right channels, next is a single mono scope which is mixed down from the stereo channels using RMS and finally there is a Lissajous scope which displays phase differences between the left and right channels. Typically the mono output will be most useful. Use the preferences pane to hide and show each of these scopes. All of them may be viewed at the same time if necessary. Thanks to Adrian Cheater for the core work behind this feature. We now have support for multi-channel input (up to 16 channels) via the new sound_in* synths for systems that have audio in. This opens up the possibility to use Sonic Pi as an FX unit for vocals, guitars and any other audio source. Another exciting new feature is the sample opt onset: - which lets you play a specific percussive part of a sample. This uses an automatic onset detection algorithm to determine all the points in the sample that go from quiet to loud quickly such as a drum, synth or bass hit. For example, this allows you to take a complex drum sample and trigger each of the individual drums in your own order and to your own timing. Finally, translations are now crowd-sourced and small or large contributions for any language can be made here: [URL] If your language isn't yet available or you would like to improve things, please join in the effort. Thanks to Hanno Zulla for making this possible. Breaking Changes sample now supports the opt path: which enables the sample's path to be overridden. use_sample_pack is now deprecated and no longer available. Consider using the new filter system. See documentation for sample for more details. current_sample_pack is now deprecated and no longer available. inspect has been removed. (Standard printing now calls Object#inspect by default) load_sample now only loads the first matched sample. load_samples now loads all matched samples. Remove SuperCollider server automatic reboot system as it was badly conflicting with machines that went into a 'sleep state' (for example, when a laptop is closed). The fn reboot is still supported and may still be triggered manually if required. Calls to play, synth and sample now consume all their arguments before testing to see if the synth should be triggered. This ensures all declared rands are consumed. This change might therefore potentially modify your random stream consumption. Consider using rand_back or rand_skip to re-align the stream if necessary. New threads now start with a fresh set of tick counters and a new random stream. It is no longer possible to use lambdas as values for synth defaults. This is because synth defaults are shared across thread boundaries and there is now a new safety system that only allows immutable/serialisable values to be used. Unfortunately Ruby has no notion of a 'pure' function and each lambda captures over its environment and therefore may contain free variables pointing to mutable data. A replacement system for describing a simple set of pure functions is being designed. New Fns reset - resets the user's thread locals (such as ticks and rand stream index) to the snapshot of the state as recorded {{at the start of the}} current thread. clear - clears all the user's thread locals to a blank state. time_warp- allows whole blocks of code to be shifted forward or backwards in time up to the value of current_sched_ahead_time. rand_look - generate a random number without consuming a rand by looking ahead in the random stream. rand_i_look - generate a random integer without consuming a rand by looking ahead in the random stream. run_file - Runs the contents of file at path as if it was in the current buffer. run_code - Runs the contents of the specified string as if it was in the current buffer. Numeric#clamp - max and minimum bound (will clamp self to a value = - 1 *other set_recording_bit_depth! - set the bit depth for WAV files generated by recording the audio. Default is 16 bits, and can now be set to one of 8, 16, 24 and 32. Larger bit depths will result in better quality audio files but also much larger file sizes for the same duration. scsynth_info - obtain information about the running audio synthesis server SuperCollider such as the number of available busses and buffers. Synths & FX New synth :tech_saws - an implementation of a slightly modified supersaw. New synth :sound_in - reads audio from the sound card. New synth :sound_in_stereo - reads audio from the sound card. All FX now have a pre_mix: opt. This allows the audio flow to completely bypass a given FX (unlike mix: which passes the audio through the FX but modifies the amplitude afterwards). Teach control to manipulate the last triggered synth by default. For example, control amp: 3 will set the amp: opt of the last triggered synth to 3. However, control foo, amp: 3 will still specifically control synth foo. Samples New opt slice: - lets you play a specific slice of a sample. The default number of slices is 16 which may be changed with the num_slices: opt. Sample is divided equally into the number of slices without regard for audio content and onset points. The slice: opt also works with pick for triggering random sample slices: sample :loop_amen, slice: pick. New opt onset: - lets you play a specific percussive part of a sample. Uses automatic onset detection to determine the points in the sample that go from quiet to loud quickly. Unlike slice:, onset: does not necessarily divide a sample into equal onsets - some onsets will be smaller or bigger than others and the number of onsets is determined by the algorithm and isn't known in advance. GUI New scope visualisers. Allow files to be dragged from the OS into the text area. This inserts the file/folder name as a string. GUI now remembers the last directory you saved or opened a file to/from as the default location for next time. Swap align button for a scope button. Given that alignment now happens automatically, a specific button seems somewhat redundant. Instead we now have a button for toggling the visibility of the scope(s). Loading multiple samples simultaneously is now much faster. Preferences have been slightly re-organised. Preferences now has a Master volume slider which controls Sonic Pi's <b>audio</b> <b>amplitude</b> independently from the system volume. All buttons now display status message + shortcut where available. Enable app transparency slider for Windows. Dark and light theme colours have been slightly polished and unified to use the same logic. On multi-screen systems, fullscreen mode now defaults to the app's current screen. Documentation Translations are now crowd-sourced. See: [URL] Improve docstring for live_loop. Add 3 new MagPi articles on amplitude modulation, performance and practice techniques. Add missing pulse_width: opt to flanger FX doc. Improvements Improve log messages written to ~/. sonic-pi/log Improve booting on Mac in the case that the audio card's rate can't be determined. Massively improve boot stability on Windows. Improve error message for play_chord when notes isn't list like. The number of samples that may be loaded at any one time has been increased from 1000 to 4000. However, memory limitations still apply (4000 1 MB samples will require 4000 MB of free system memory) pick now returns a lambda if no list is given as the first argument (which makes it useful for using with sample's onset: and slice: opts. Audio server is now paused when app is not in use - reducing CPU load and battery consumption. Error messages now report names matching the editor tabs such as buffer 0. Bugfixes Decrease duration of :loop_tabla so that it correctly loops. (Length reduced to 10. 674 seconds). Enforce UTF- 8 encoding of all incoming text. Fix :reverb FX's mix: opt to ensure it's in the range 0 to 1. sample nil now no longer plays a sample - it was incorrectly defaulting to the first built-in sample (:ambi_choir) pick's skip: opt now works as expected: pick(5). drop(1) == pick(5, skip: 1) sample now prints a 'no sample found' message with both sample nil and sample [] rather than incorrectly playing the first built-in wav file. Limit :piano synth to notes less than 231 as higher values crash the audio server...|$|E
5000|$|All {{these early}} {{technologies}} {{were replaced by}} vacuum tube transmitters in the 1920s, which used the feedback oscillator invented by Edwin Armstrong and Alexander Meissner around 1912, based on the Audion (triode) vacuum tube invented by Lee De Forest in 1906. Vacuum tube transmitters took over because they were inexpensive and produced continuous waves, which could be modulated to transmit <b>audio</b> (sound) using <b>amplitude</b> modulation (AM). This made possible AM radio broadcasting, which began in about 1920. Practical frequency modulation (FM) transmission was invented by Edwin Armstrong in 1933, who showed that it was less vulnerable to noise and static than AM, and the first FM radio station was licensed in 1937. Experimental television transmission had been conducted by radio stations since the late 1920s, but practical television broadcasting didn't begin until the 1930s. The development of radar during World War II was a great stimulus {{to the evolution of}} high frequency transmitters in the UHF and microwave ranges, using new devices such as the magnetron, klystron, and traveling wave tube.|$|R
40|$|This thesis {{presents}} an academic {{study of the}} development and the instrumentation setup of gamelan Melayu in Malaysia. It draws to attention the current instrumentation set-up in the ensemble, and how restructuring it could enhance and {{improve the quality of}} sound projection in the performance of the gamelan Melayu ensemble. The purpose of this research is to produce alternative solutions to improve the sound projection in gamelan Melayu. This is achieved through orchestrating new gamelan setups, based on the musicology study through the analysis of amplitude data obtained through music technology, and subsequently strengthening it with western orchestration theory. This research finds out the amplitude of each gamelan instrument by detecting the sound using an instrument microphone (Shure SM 57), and then recording the results of the <b>audio</b> level (<b>amplitude)</b> displayed in the digital mixer (Behringer X 32). Then, a few samples of instrumentation setup are proposed based on the analysis of the results. This is further supported by the theory of western orchestration, which can determine the priority ranking of the gamelan instruments. In addition, gamelan repertoires in traditional balok (cipher) notation are transcribed into western notation system, as the western notation is more comprehendible by most musicians. This research provide guidelines for future gamelan enthusiasts who would like to understand more about the gamelan Melayu before purchasing a new set of gamelan in the near future, as well as a reference for gamelan enthusiasts, and promote awareness among readers to explore more in-depth into gamelan instruments. It is hope that through this research, the beautiful sound of gamelan can be mold into a better shape. iv This research is divided into five chapters. Chapter 1 contains an introduction to the research, where the research objective and research question is determined. Chapter 2 presents the history and literature review of this research, which covers the literatures of the development of the instrumentation setup in gamelan Melayu, as well as various western orchestration theories that could be applied into the aforementioned. The research methodologies are explained thoroughly in chapter 3. Chapter 4 displays the results and analysis of the amplitude, as well as the discussion about a few different samples of instrumentation setup. A summary of conclusions and findings are highlighted in chapter 5, as well as suggestions for future researches...|$|R
40|$|The {{components}} of a sound system, {{whether it is a}} small system for an electronic device or a high power one, need to be designed to meet certain power requirements. For this, manufacturers often design these components using test-signals, which have an unchanging amplitude. These are very different from real audio signals, because, in <b>audio,</b> the <b>amplitude</b> is continuously changing over the time, and high power peaks can be found, which have a small duration in comparison with the continuous low power values. This project follows the work done by a group of PhD students and professors of the Technical University of Denmark, where the power requirements of a loudspeaker were investigated when it is reproducing different audio files. For this, over 400 songs were used, and the simulations were done with the mathematical linear model of the loud-speaker, which will also be explained in this document. The results of the simulations showed, as expected, that loudspeakers have a high power consumption for short amounts of time, and a low continuous power consumption. The goal of the investigation is to avoid oversizing and unnecessary costs when designing all the units of a sound system, such as the power supply unit, the audio amplifier and the loudspeaker driver. The main goal of this project is to validate the functionality of the mathematical model of the loudspeaker, {{to make sure that the}} simulations on this model give the same results as measurements in a real loudspeaker. For this, a circuit has been designed and built, which allows to send an audio signal to a real loudspeaker system and measure the power consumption of the loudspeaker driver. Measurements taken on this circuit have been compared to simulations using the linear and non-linear models, which are also explained in the document. The results show, indeed, that the models simulate the behaviour of the real loudspeaker with high precision, so the results from the simulations are absolutely valid. Moreover, the difference between the measurements on the linear and the non-linear models is very small, which confirms that the linear model is enough for the simulations, which is more simple than the non-linear one. The validity of the mathematical model has been confirmed making measurements with various audio files belonging to different musical styles. The power requirements give very similar results both for the real loudspeaker and the simulation models. Finally, a database of over 100 loudspeaker drivers of the same size has been taken, and simulations have been done on each one of them: all the loudspeakers must have the same sound pressure level response, so a filter is applied to each one to make them all sound the same. Then, simulations have been run and the power requirements have been analysed. The results show how the range of power consumption can be quite wide for the loudspeakers, even if they are similar and are forced to have the same sound response. This explains why design for the worst case needs to be done...|$|R

