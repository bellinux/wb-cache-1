3|130|Public
40|$|Abstract. The NASA {{scatterometer}} (NSCAT) {{estimates the}} wind speed {{and direction of}} near-surface ocean wind. Several possible wind vectors (termed ambiguities) are estimated for each resolution element known as a wind vector cell (WVC). Typically, the speeds of the possible wind vectors are nearly the same, but the directions are very different. The correct wind must be distinguished in a step called ambiguity removal. Unfortunately, ambiguity removal algorithms are subject to error. In an attempt to evaluate {{the accuracy of the}} Jet Propulsion Laboratory NSCAT product, we use a new model-based quality assurance algorithm that uses only NSCAT data. The algorithm segments the swath into overlapping 12 � 12 WVC regions and classifies each region according to estimated quality. The 9 -month NSCAT mission data set is analyzed. In 82 % of the regions the ambiguity removal is over 99 % effective, with the ambiguity errors correctable using a model-based correction technique. In 5 % of the regions, areas of significant <b>ambiguity</b> <b>error</b> are found. For remaining regions, all of which have root-mean-square (rms) wind speeds less than 4 ms � 1, there is too much uncertainty in the wind field model or too much noise in the measurements to uniquely evaluate ambiguity selection with sufficient confidence. We thus conservatively conclude that for the set of regions with rms wind speed greater than 4 ms � 1, NSCAT ambiguity removal is at least 95 % effective. 1...|$|E
40|$|Precise Point Positioning (PPP) is an {{increasingly}} recognized precisely the GPS/GNSS positioning technique. In {{order to improve}} the accuracy of PPP, the error sources in PPP measurements should be reduced {{as much as possible}} and the ambiguities should be correctly resolved. The correct ambiguity resolution requires a careful control of residual errors that are normally categorized into random and systematic errors. To understand effects from two categorized errors on the PPP ambiguity resolution, those two GPS datasets are simulated by generating in locations in South Korea (denoted as SUWN) and Hong Kong (PolyU). Both simulation cases are studied for each dataset; the first case is that all the satellites are affected by systematic and random errors, and the second case is that only a few satellites are affected. In the first case with random errors only, when the magnitude of random errors is increased, L 1 ambiguities have a much higher chance to be incorrectly fixed. However, the size of <b>ambiguity</b> <b>error</b> is not exactly proportional to the magnitude of random error. Satellite geometry has more impacts on the L 1 ambiguity resolution than the magnitude of random errors. In the first case when all the satellites have both random and systematic errors, the accuracy of fixed ambiguities is considerably affected by the systematic error. A pseudorange systematic error of 5 cm is the much more detrimental to ambiguity resolutions than carrier phase systematic error of 2 mm. In the 2 nd case when only a portion o...|$|E
40|$|Industries which utilize Computer Aided Design, (CAD), are in {{a similar}} {{situation}} to the film industry, where the use of Computer Graphics, (CG), has reached such a level of reality that audiences often do not spot where CG has been used. This has resulted in a general attitude among critics of: “CG is what you expect in a film, but what we often lack is a decent plot”. Over a similar period, CAD software has become a powerful tool with proficient users, whilst the marketplace for such services now takes such facilities for granted. The ‘wow factor’ has faded. The special effects used in films has contributed to this dulling of presentation impact, which leads us to question where we stand in relation to a competitive edge, with the realization that: “CAD is what you expect from a firm, but what we often lack is clear intent. ” The questioning of competitive edge draws us into some complex issues, concerning the reduction of compromise for design intent, where priorities fight for first place. There is no disputing the importance of time to market, yet the time compression technologies may no longer be providing a sufficient cutting edge. Even if new technologies facilitate even shorter lead-times we will always face the threat of a time management trap and potential loss of design quality. As a high-risk strategy for competitive advantage, contractual agreements for specified short lead-time deliveries, in some cases with penalty clauses written in, have established an expectation among the client base. Such a strategy leads us to effectively burn our bridges, in sacrificing margins for schedule 3 slippage and error compensation, leaving us nowhere to go but back. With such a lean approach to product development we have to improve our focus on the plot and its intent for design quality. The more investment we make at the front end, to enable the decision making process, the more likely we are to avoid pain at the back-end. Presently, decisions are made on a resource of available quality and quantity of data, using a perspective which is based on the experience, tacit knowledge and intuition of those involved. Whilst intuition is a good starting point or fall-back, as with tacit knowledge, it often proves difficult to substantiate. Background experience is the most valuable asset here but proves ineffectual when faced with low quality data, either through <b>ambiguity,</b> <b>error</b> or lack of substance. The improvement of quality standards require that we look closely at the production and presentation of data in the context of decision making and establish a process by which quality decisions can be made quickly and efficiently. This paper focuses on the process of communication between designers and their colleagues and clients, concerning the presentation of CAD models, from a cognitive perspective. It first establishes a context for individual differences in the management of auditory and visual information for decision making. This is followed by a discussion of five approaches to the communication of design intent and concludes with a checklist, to aid selection of an effective approach to communication...|$|E
40|$|The {{scatterometer}} wind {{retrieval process}} produces several pos-sible wind vector choices or ambiguities at each resolution cell. Ambiguity selection routines are generally ad hoc and often re-sult in <b>ambiguity</b> selection <b>errors.</b> It {{is important to}} locate areas of <b>ambiguity</b> selection <b>error</b> to assess the quality of scatterom-eter wind data. A quality assurance algorithm is presented based on comparing ambiguity-selected winds from SeaWinds on QuikSCAT to a low order wind field model fit. Regions exceeding error thresholds are rated and flagged as possible <b>ambiguity</b> selection <b>errors.</b> Appropriate error thresholds and additional flagging criteria are set through an analysis of false alarms versus missed detections on a manually-inspected train-ing data set. The algorithm correctly identifies 97 % of the regions manually flagged as <b>ambiguity</b> selection <b>errors</b> in the training set with a false alarm rate of less than 2 %. Applying the algorithm to the entire QuikSCAT data set, we conclude that the ambiguity selection is over 95 % effective on regions of rms wind speed greater than 3. 5 m/s. The algorithm validates that higher noise occurs at nadir and in low wind speed regions. Additionally, fewer estimated <b>ambiguity</b> selection <b>errors</b> occur at nadir and on the swath edges due to a larger ambiguity set in those regions. The percentage of <b>ambiguity</b> selection <b>errors</b> {{are found to be}} highly correlated with cyclonic storm and rain occurences. ...|$|R
40|$|The SeaWinds on QuikScat {{scatterometer}} is {{the first}} in a series of new scanning pencil-beam Ku-band scatterometers. The viewing geometry is significantly different than previous fan beam instruments, resulting in different characteristics in the retrieved winds. In this paper we provide an assessment of the reliability of the SeaWinds ambiguity selection using a SeaWinds data-only algorithm. An ambiguity selection quality assurance algorithm developed for NASA Scatterometer (NSCAT) data is modified for use with SeaWinds data. The algorithm uses the selected ambiguity field to estimate the parameters of a simple wind field model and examines significant differences between the fields, enabling detection of possible <b>ambiguity</b> <b>errors.</b> Tests against subjectively analyzed selection errors suggest that the algorithm correctly detects more than 94 % of all <b>ambiguity</b> <b>errors.</b> Applying the algorithm, we find that the ambiguity selection accuracy exceeds 93 %...|$|R
50|$|Historically, {{engineering}} and manufacturing activities {{have relied on}} hardcopy and/or digital documents (including 2D drawings) to convey engineering data and drive manufacturing processes. These documents required interpretation by skilled practitioners, often leading to <b>ambiguities</b> and <b>errors.</b>|$|R
40|$|Ordinary {{requirements}} come in many forms, natural languages, equations, tables, charts, predecessor systems, {{and ideas}} {{in the minds of}} domain experts. All forms can contain <b>ambiguities,</b> <b>errors,</b> and omissions. They change both during and after a phase of development. Sequence-based specification has in many field applications been effective in converting ordinary requirements to precise specifications through a constructive process. Algorithms for managing requirements changes meet a very great need in applications of sequence-based specification. In this paper we explore the change theory developed with the aid of an axiom system for sequence-based specification, and present algorithms for managing requirements changes of various kinds. This has established the basis for maximizing potential automation support and producing benefits in field applications as well as further development of sequencebased specification...|$|R
40|$|Jossberger, H., Brand-Gruwel, S., Boshuizen, H. P. A., & Van de Wiel, M. (2010, August). Monitoring: A {{strategy}} to detect imminent mistakes. In D. Sembill (Chair), Human Fallibility: The <b>Ambiguity</b> of <b>Errors</b> for Work and Learning. Symposium {{conducted at the}} EARLI Learning and Professional Development SIG Conference, Munich, Germany...|$|R
3000|$|Compensating {{for this}} error {{requires}} {{the availability of}} sound velocity profile data during localisation. Figure 2 shows that in very shallow water environments with relatively benign velocity profiles and short ranges (i.e., under 4000 [*]m), the error caused by sound refraction is relatively small (i.e., 0.02 % of the range) compared to the <b>ambiguity</b> <b>errors</b> that are potentially introduced by {{a large percentage of}} missing pairwise distances (typically in the order of 10 to 20 % of range). Another common compensation approach is by using a set of available anchor points of which the precise coordinates, and hence shortest inter-node distances, are used to predict a best-fit velocity profile by matching against ranging measurements. These anchor points are also used for mapping the relative locations onto geo-coordinates by using m-axis vector translations along with [...]...|$|R
25|$|Anatomical {{terminology}} uses many unique terms, suffixes, and prefixes {{deriving from}} Ancient Greek and Latin. These terms can be confusing to those unfamiliar with them, {{but can be}} more precise reducing <b>ambiguity</b> and <b>errors.</b> Also, since these anatomical terms are not used in everyday conversation, their meanings {{are less likely to}} change, and less likely to be misinterpreted.|$|R
40|$|Conventional {{non-contact}} optical {{methods of}} surface profiling {{are limited in}} the range of surface heights that can be accurately measured due to phase <b>ambiguity</b> <b>errors</b> on steep local slopes. Instruments that have been developed thus far to avoid the problems with local slope typically suffer from poor measurement height resolution and slow measurement speeds. Contact profilometers such as stylus-based instruments suffer from poor lateral resolution due to the finite radius of the stylus tip, and slow measurement speeds, especially when two-dimensional scans of the surface are required. Stylus tips can also scratch delicate surfaces during the measurement. We propose a new method of optical, non-contact profiling of rough surfaces without the limitations on local slope that other methods suffer from. This new method utilizes interferometric techniques as well as digital signal processing algorithms to produce fast, accurate, and repeatable three-dimensional surface profile measurements {{on a wide variety of}} surfaces...|$|R
40|$|We present here {{a project}} aimed at {{developing}} {{a system for}} collocation extraction based on contextual morpho-syntactic properties. Collocations are lexical expressions composed {{of two or more}} items, each with their own at times unpredictable syntactic and semantic behavior. Collocation is a crucial feature of idiomatic language use, and represents an important source of <b>ambiguities</b> and <b>errors</b> for NLP application...|$|R
40|$|Abstract. Specification defects are a {{major source}} of <b>ambiguities</b> and <b>errors.</b> We propose a new {{weighted}} requirement mining framework to ad-dress this problem. We define weighted and parametric weighted signal temporal logic (STL) as the underlying formalism to improve the ex-pressiveness. This also improves convergence of the mining process. We provide some primitive results to show the benefit of the new framework. ...|$|R
40|$|The eminent role of {{knowledge}} for successful professional performance is beyond dispute. Many modern professions, however, include {{a multitude of}} complex activities, so that even highly skilled professionals meet ambiguous demands and are prone to errors. This is particularly true for rapidly developing professions in which interaction with other people plays a central role, like in company consultation. On one side, <b>ambiguity</b> and <b>errors</b> pose challenges for the validity of one’s domain {{of knowledge}}, because this knowledge has to be conditionalised according to professional constraints, {{and on the other}} <b>ambiguity</b> and <b>errors</b> define a new area of knowledge, that is, knowledge of errors, that has to be considered as relevant part of expertise. In professional practice, too little {{attention has been paid to}} knowledge about errors as part of professional knowledge. In this chapter, the underlying theoretical approaches are discussed, from which educational conclusions as well as guidelines for organisational error management are drawn...|$|R
50|$|The {{authors of}} the ALGOL 60 Report met in Rome, Italy in April 1962 to resolve most of the <b>ambiguities</b> and <b>errors</b> known at the time, {{resulting}} in the Revised Report on the Algorithmic Language ALGOL 60. During that meeting, the authors decided to institutionalize {{the responsibility for the}} continued support and maintenance of ALGOL 60 by transferring it to the young international IFIP organization.|$|R
40|$|We {{put forth}} {{in this paper}} a geometrically {{motivated}} 3 -D motion error analysis which is capable of supporting investigation of global effect such as inherent <b>ambiguities.</b> The <b>error</b> expression that we derive allows us to predict the exact conditions likely to cause ambiguities and how these ambiguities vary with motion types such as lateral or forward motion. Our formulation, though geometrically motivated, is employed to model the effect of noise. ...|$|R
40|$|We define natural A_infinity-transformations and {{construct}} A_infinity-category of A_infinity-functors. The notion of non-strict units in an A_infinity-category is introduced. The 2 -category of (unital) A_infinity-categories, (unital) functors and transformations is described. Comment: 51 pages, LaTeX, uses Paul Taylor's diagrams. sty. It is the published text plus my comments in {{blue in the}} places {{that are hard to}} read: omissions, difficult proofs, <b>ambiguities.</b> An <b>error</b> is corrected in re...|$|R
40|$|NSCAT) {{estimates}} the wind speed {{and direction of}} nearsurface ocean wind. This is done by directing microwaves toward the earth's surface and measuring the backscattered radiation. From this, several possible wind vectors are identified for each point over the swath. The correct wind must be distinguished from these in a step called ambiguity removal. Unfortunately, ambiguity removal algorithms are subject to error. Because the true wind is not known, where these errors occur is difficult to determine, {{and there is little}} information in the measurements alone to detect the errors in this removal step. We have developed a method to assess the accuracy of the ambiguity removal algorithm by comparing the point-wise retrieved wind to winds inferred with a wind field model. The performance of the algorithm achieves its goal to identify at least 95 % of regions containing <b>ambiguity</b> removal <b>errors.</b> The algorithm provides a very simple tool to indicate regions of possible <b>ambiguity</b> removal <b>errors</b> in the point-wise retrieved winds for NSCAT data. This paper describes this algorithm and its performance for real NSCAT data...|$|R
40|$|Three-dimensional RNA models fitted into crystallographic density maps exhibit {{pervasive}} conformational <b>ambiguities,</b> geometric <b>errors</b> and steric clashes. To {{address these}} problems, we present enumerative real-space refinement assisted by electron density under Rosetta (ERRASER), coupled to Python-based hierarchical environment for integrated 'xtallography' (PHENIX) diffraction-based refinement. On 24 data sets, ERRASER automatically corrects {{the majority of}} MolProbity-assessed errors, improves the average Rfree factor, resolves functionally important discrepancies in noncanonical structure and refines low-resolution models to better match higher-resolution models...|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. This document describes common issues seen in Remote Authentication Dial In User Service (RADIUS) implementations and suggests some fixes. Where applicable, <b>ambiguities</b> and <b>errors</b> in previous RADIU...|$|R
40|$|A {{contig map}} is a {{physical}} map that shows the native order of a library of overlapping genomic clones. One common method for creating such maps involves using hybridization to detect clone overlaps. False- positive and false-negative hybridization errors, the presence of chimeric clones, and gaps in library coverage lead to <b>ambiguity</b> and <b>error</b> in the clone order. Genomes with good genetic maps, such as Neurospora crassa, provide a means for reducing <b>ambiguities</b> and <b>errors</b> when constructing contig maps if clones can be anchored with genetic markers to the genetic map. A software application called ODS 2 for creating contig maps based on clone-clone hybridization data is presented. This application is also designed to exploit partial ordering information provided by anchorage of clones to a genetic map. This information, along with clone-clone hybridization data, is used by a clone ordering algorithm and is represented graphically, allowing users to interactively align physical and genetic maps. ODS 2 has a graphical user interface and is implemented entirely in Java, so it runs on multiple platforms. Other features include the flexibility of storing data in a local file or relational database {{and the ability to}} create full or minimum tiling contig maps...|$|R
40|$|Evaluation of the launch-version {{algorithms}} {{used by the}} European Space Agency (ESA) to derive wind field and ocean wave estimates from measurements of sensors aboard the European Remote Sensing satellite, ERS- 1, has been accomplished through comparison of the derived parameters with coincident measurements made by 24 open ocean buoys maintained by the National Oceanic and Atmospheric Administration). During the period from November 1, 1991 through February 28, 1992, data bases with 577 and 485 pairs of coincident sensor/buoy wind and wave measurements were collected for the Active Microwave Instrument (AMI) and Radar Altimeter (RA) respectively. Based on these data, algorithm retrieval accuracy {{is estimated to be}} plus or minus 4 m/s for AMI wind speed, plus or minus 3 m/s for RA wind speed and plus or minus 0. 6 m for RA wave height. After removing 180 degree <b>ambiguity</b> <b>errors,</b> the AMI wind direction retrieval accuracy was estimated at plus or minus 28 degrees. All of the ERS- 1 wind and wave retrievals are relatively unbiased. These results should be viewed as interim since improved algorithms are under development. As final versions are implemented, additional assessments should be conducted to complete the validation...|$|R
40|$|Concerning {{different}} approaches to automatic PoS tagging: EngCG- 2, a constraintbased morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The ex- periments show that {{for the same amount}} of remaining <b>ambiguity,</b> the <b>error</b> rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed...|$|R
5000|$|Triple {{differencing}} can {{be performed}} by taking the difference of double differencing performed at time [...] with that performed at time [...] This will eliminate the ambiguity associated with the integral number of wavelengths in carrier phase provided this ambiguity does not change with time. Thus the triple difference result has eliminated all or practically all clock bias errors and the integer <b>ambiguity.</b> Also <b>errors</b> associated with atmospheric delay and satellite ephemeris have been significantly reduced. This triple difference is: ...|$|R
30|$|Despite {{the large}} amount of work {{conducted}} in this field, big challenges still remain in many applications due to noise and ambiguities. From a likely noisy set of detections, the algorithm must construct an unknown number of trajectories. This task causes <b>ambiguities</b> and thereby <b>errors</b> or inaccuracies.|$|R
50|$|Fuzzy set QCA aims {{to handle}} variables, such as GDP per capita, where {{the number of}} categories, decimal values of {{monetary}} units, becomes too large to use mvQCA, or in cases were uncertainty or <b>ambiguity</b> or measurement <b>error</b> in the classification of a case needs to be acknowledged.|$|R
40|$|International audienceMany project-specific languages, {{including}} in particular filtering languages, are defined using non-formal specifications written in natural languages. This leads to <b>ambiguities</b> and <b>errors</b> in the specification of those languages. This paper reports on an industrial experiment on using a tool-supported language specification framework (K) for the formal specification of the syntax and semantics of a filtering language having a complexity {{similar to those}} of real-life projects. This experimentation aims at estimating, in a specific industrial setting, the difficulty and benefits of formally specifying a packet filtering language using a tool-supported formal approach...|$|R
40|$|International audienceWe {{validate}} the RBAC ANSI 2012 standard using the B method. Numerous problems are identified: logical <b>errors,</b> inconsisten- cies, <b>ambiguities,</b> typing <b>errors,</b> missing preconditions, invariant violation, inappropriate specification notation. A clean {{version of the}} standard written in the B notation is proposed. We argue that the ad hoc mathematical notation used in the standard is inappropriate and we propose that a more methodological and tool-supported approach must definitely be used for writing standards, {{in order to avoid}} the issues identified in the paper. Human reviewing is insufficient to produce error-free international standard...|$|R
50|$|Soon {{after the}} {{publication}} of the original ALGOL 60 Report in 1960, issues arose that needed some form of authoritative resolution. ALGOL 60 had been chosen by the Communications of the ACM, then a leading scientific journal, as the publication language for algorithms, then {{an important part of the}} items published in the Communications. Computer manufacturers and academic groups were laboring to produce implementations. There were issues that needed clarification, such as <b>ambiguities</b> and <b>errors</b> in the Report. Another urgent issue was the complete absence of even basic I/O facilities.|$|R
40|$|A {{framework}} for the logical and statistical analysis and annotation of dynamic scenes containing occlusion and other uncertainties is presented. This framework consists of three elements; an object tracker module, an object recognition/classification module and a logical consistency, <b>ambiguity</b> and <b>error</b> reasoning engine. The principle behind the object tracker and object recognition modules is to reduce <b>error</b> by increasing <b>ambiguity</b> (by merging objects in close proximity and presenting multiple hypotheses). The reasoning engine deals with <b>error,</b> <b>ambiguity</b> and occlusion in a unified framework to produce a hypothesis that satisfies fundamental constraints on the spatio-temporal continuity of objects. Our algorithm finds a globally consistent model of an extended video sequence that is maximally supported by a voting function based on the output of a statistical classifier. The system results in an annotation that is significantly more accurate than what would be obtained by frame-by-frame evaluation of the classifier output. The framework has been implemented and applied successfully {{to the analysis of}} team sports with a single camera. Key words: Visua...|$|R
40|$|We {{present an}} {{automated}} surface profiling {{system based on}} a shearing interferometer, in which precise measurement of the polarization states eliminates fringe <b>ambiguity.</b> A full <b>error</b> correction based on Mueller matrices allows comparatively inaccurate but rapidly switchable liquid-crystal wave plates to be used, enabling unambiguous profile information to be obtained in real time...|$|R
40|$|Many project-specific languages, {{including}} in particular filtering languages, are defined using non-formal specifications written in natural languages. This leads to <b>ambiguities</b> and <b>errors</b> in the specification of those languages. This paper reports on an industrial experiment on using a tool-supported language specification framework (K) for the formal specification of the syntax and semantics of a filtering language having a complexity {{similar to those}} of real-life projects. This experimentation aims at estimating, in a specific industrial setting, the difficulty and benefits of formally specifying a packet filtering language using a tool-supported formal approach. Comment: In Proceedings F-IDE 2016, arXiv: 1701. 0792...|$|R
40|$|The {{objective}} of the TanDEM-X Mission is the generation of a global high resolution Digital Elevation Model (DEM). To carry out this goal, two interferograms with different baselines will be acquired. We propose a novel two-stage multibaseline phase unwrapping method. Maximum Likelihood Estimation (MLE) is used to reduce the <b>ambiguity</b> and <b>errors</b> in gradient estimation on a pixel-by-pixel basis. Based on these estimates, Minimum Cost Flow (MCF) is used to unwrap the phase accounting for the overall conservative condition of the gradient. Hence the advantages of both techniques are efficiently integrated. Results on simlated data using TerraSAR-X parameters are reported...|$|R
40|$|GPS/GLONASS <b>ambiguity</b> {{resolution}} and <b>error</b> mitigation methods for carrier phase-based kinematic positioning over short-, medium-, and long-range, GPS attitude determination and {{the integration of}} GPS, INS and Pseudolites. He is Chairman of the International Association of Geodesy Special Study Group 1. 179 “Wide (regional) area modelling for precise satellite positioning”, and has authored over 100 journal and conference publications...|$|R
40|$|Attempts at {{discourse}} {{processing of}} spontaneously spoken dialogue face several difficulties: multiple hypotheses {{that result from}} the parser's attempts {{to make sense of}} the output from the speech recognizer, ambiguity that results from segmentation of multi-sentence utterances, and cumulative error [...] - errors in the discourse context which cause further errors when subsequent sentences are processed. In this paper we will describe our robust parsers, our procedures for segmenting long utterances, and two approaches to discourse processing that attempt to deal with <b>ambiguity</b> and cumulative <b>error.</b> 1. Introduction In this paper we describe how the JANUS [10] multi-lingual speechto -speech translation system addresses problems that arise in discourse processing of spontaneous speech. The analysis of spoken dialogues requires discourse processors that can deal with <b>ambiguity</b> and cumulative <b>error</b> [...] errors in the discourse context which cause further errors when subsequent sentences are pr [...] ...|$|R
40|$|The {{recently}} launched NASA Scatterometer (NSCAT) estimates the wind speed {{and direction of}} near-surface ocean wind. Several possible wind vectors are identified for that location of the earth known as a cell. Typically, the speeds of the possible wind vectors are the same, but the directions are very different. The correct wind must be distinguished from these in a step called ambiguity removal. Unfortunately, ambiguity removal algorithms are subject to error. Because the true wind is not known, where these errors occur is difficult to determine, {{and there is little}} information about how to detect the errors in this removal step. One method we have developed to assess the accuracy of the ambiguity removal algorithm is to compare the point-wise retrieved wind to wind retrieved using a model. The model is fit to the point-wise retrieved wind to determine if the observed wind is realistic or containing possible <b>ambiguity</b> removal <b>errors.</b> The algorithm's performance achieves its goal to identify at least 95 % of the regions that contain errors. The algorithm provides a very simple tool to indicate regions of possible <b>ambiguity</b> removal <b>errors</b> in the point-wise retrieved winds for NSCAT data. This paper describes this algorithm and its performance for real NSCAT data. It also outlines an algorithm to correct some of the errors. This is done by either choosing the alias closest to the model-fit or by simply replacing the erroneous wind vectors with those produced by the model-fit...|$|R
