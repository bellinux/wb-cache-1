3|66|Public
2500|$|A simple example: A useful symbol {{duration}} TU = 1ms {{would require}} a sub-carrier spacing of [...] (or an integer multiple of that) for orthogonality. N = 1,000 sub-carriers {{would result in a}} total passband bandwidth of NΔf = 1MHz. For this symbol time, the required bandwidth in theory according to Nyquist is [...] (half of the achieved bandwidth required by our scheme), where R is the bit rate and where N = 1,000 samples per symbol by FFT. If a guard interval is applied (see below), Nyquist bandwidth requirement would be even lower. The FFT would result in N = 1,000 samples per symbol. If no guard interval was applied, this would result in a base band complex valued signal with a sample rate of 1MHz, which {{would require a}} baseband bandwidth of 0.5MHz according to Nyquist. However, the passband RF signal is produced by multiplying the baseband signal with a carrier waveform (i.e., double-sideband quadrature amplitude-modulation) resulting in a passband bandwidth of 1MHz. A single-side band (SSB) or vestigial sideband (VSB) modulation scheme would achieve almost half that bandwidth for the same symbol rate (i.e., twice as high spectral efficiency for the same symbol <b>alphabet</b> <b>length).</b> It is however more sensitive to multipath interference.|$|E
5000|$|A simple example: A useful symbol {{duration}} TU = 1 ms {{would require}} a sub-carrier spacing of [...] (or an integer multiple of that) for orthogonality. N = 1,000 sub-carriers {{would result in a}} total passband bandwidth of NΔf = 1 MHz. For this symbol time, the required bandwidth in theory according to Nyquist is [...] (i.e., half of the achieved bandwidth required by our scheme). If a guard interval is applied (see below), Nyquist bandwidth requirement would be even lower. The FFT would result in N = 1,000 samples per symbol. If no guard interval was applied, this would result in a base band complex valued signal with a sample rate of 1 MHz, which {{would require a}} baseband bandwidth of 0.5 MHz according to Nyquist. However, the passband RF signal is produced by multiplying the baseband signal with a carrier waveform (i.e., double-sideband quadrature amplitude-modulation) resulting in a passband bandwidth of 1 MHz. A single-side band (SSB) or vestigial sideband (VSB) modulation scheme would achieve almost half that bandwidth for the same symbol rate (i.e., twice as high spectral efficiency for the same symbol <b>alphabet</b> <b>length).</b> It is however more sensitive to multipath interference.|$|E
40|$|There {{are several}} types of finite automata on {{infinite}} words, differing in their acceptance conditions. As each type has its own advantages, there is an extensive research on the size blowup involved in translating one automaton type to another. Of special interest is the Muller type, providing the most detailed acceptance condition. It turns {{out that there is}} inconsistency and incompleteness in the literature results regarding the translations to and from Muller automata. Considering the automaton size, some results take into account, in addition to the number of states, the <b>alphabet</b> <b>length</b> and the number of transitions while ignoring the length of the acceptance condition, whereas other results consider the length of the acceptance condition while ignoring the two other parameters. We establish a full picture of the translations to and from Muller automata, enhancing known results and adding new ones. Overall, Muller automata can be considered less succinct than parity, Rabin, and Streett automata: translating nondeterministic Muller automata to the other nondeterministic types involves a polynomial size blowup, while the other way round is exponential; translating between the deterministic versions is exponential in both directions; and translating nondeterministic automata of all types to deterministic Muller automata is doubly exponential, as opposed to a single exponent in the translations to the other deterministic types...|$|E
5000|$|In the Korean Hangul <b>alphabet,</b> vowel <b>length</b> is not {{distinguished}} {{in normal}} writing. Some dictionaries use a double dot, , for example [...] “Daikon radish”.|$|R
2500|$|In general, let [...] be the <b>alphabet</b> of <b>length</b> [...] Denote by [...] {{the length}} of key. Then Vigenère {{encryption}} and decryption can be written as follows: ...|$|R
5000|$|The International Phonetic <b>Alphabet</b> (IPA) denotes <b>length</b> by {{doubling}} the letter or by diacritics above {{or after the}} letters: ...|$|R
2500|$|The NK model {{defines a}} {{combinatorial}} phase space, consisting of every string (chosen from a given <b>alphabet)</b> of <b>length</b> [...] For each string in this search space, a scalar value (called the fitness) is defined. If a distance metric is defined between strings, the resulting structure is a landscape.|$|R
3000|$|..., i.e. {{the minimum}} number of {{character}} insertions, deletions and substitutions needed to transform one string into the other. Note that edit distance is well defined on larger <b>alphabet</b> and variable <b>length</b> strings. The scheme can be extended to these cases.|$|R
5000|$|An {{inscription}} of 38 {{words is}} found on {{parts of the}} rim and {{the lid of the}} sarcophagus. It is written in the Old Phoenician dialect of Byblos and is the oldest witness to the Phoenician <b>alphabet</b> of considerable <b>length</b> discovered to date: ...|$|R
40|$|One of the oustanding open {{problems}} {{at the heart of}} gapped sequence alignment is the longest common subsequence (LCS) problem which is stated as follows: given two random sequences (in a finite <b>alphabet)</b> of <b>length</b> N, what is the longest common subsequence shared by the two sequences? Although the expected value and variance of the distribution are known to lie within certain limits, their exact values and the exact distribution over all possible length subsequences k, 0 N, for all sequence lengths N are unknown. In the simples...|$|R
5000|$|Methods vary on {{the exact}} form the table for the bad {{character}} rule should take, but a simple constant-time lookup solution is as follows: create a 2D table which is indexed first by the index of the character [...] in the alphabet and second by the index [...] in the pattern. This lookup will return the occurrence of [...] in [...] with the next-highest index [...] or -1 {{if there is no}} such occurrence. The proposed shift will then be , with [...] lookup time and [...] space, assuming a finite <b>alphabet</b> of <b>length</b> [...]|$|R
5000|$|The table below {{lists the}} RM(r, m) codes of lengths up to 32 for {{alphabet}} of size 2 annotated with standard coding theory notation for block codesThe Reed - Muller code is a -code, that is, it is a linear code over a binary <b>alphabet,</b> has block <b>length</b> , message length (or dimension) , and minimum distance [...]|$|R
40|$|Transaction {{sequences}} in market-basket analysis {{have large}} set of <b>alphabets</b> with small <b>length,</b> whereas bio-sequences have small set of <b>alphabets</b> of long <b>length</b> with gap. There {{is the difference}} in pattern finding algorithms of these two sequences. The chances of repeatedly occurring small patterns are high in bio-sequences than in the transaction sequences. These repeatedly occurring small patterns are called as Frequent Contiguous Patterns (FCP). The challenging task in pattern finding of bio-sequences is to find FCP. FCP gives clues for genetic discovery, functional analysis and also helps to assemble a whole genome of species. Most of the existing FCP algorithms are all based on Apriori method. They require repeated scanning of the database and large number of intermediate tables to produce the results. So, these algorithms require large space and high computational time. In this paper, we are analyzing few of the currently available FCP algorithms with their advantages and disadvantages...|$|R
40|$|A (fractional) {{repetition}} {{in a word}} w is a subword {{with the}} period of at most half of the subword length. We study maximal repetitions occurring in w, that is those for which any extended subword of w has a bigger period. The set of such repetitions represents in a compact way all repetitions in w. We first study maximal repetitions in Fibonacci words [...] we count their exact number, and estimate the sum of their exponents. These quantities {{turn out to be}} linearly-bounded in the length of the word. We then prove that the maximal number of maximal repetitions in general words (on arbitrary <b>alphabet)</b> of <b>length</b> n is linearly-bounded in n, and we mention some applications and consequences of this result...|$|R
5000|$|Due to {{the data}} {{structures}} required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), and also prefers inputs over a small alphabet. Once it has been implemented for a given <b>alphabet</b> and word <b>length</b> m, however, its running time is completely predictable [...] - [...] it runs in O(mn) operations, no matter {{the structure of the}} text or the pattern.|$|R
5000|$|Given an {{alphabet}} , {{the set of}} all strings {{over the}} <b>alphabet</b> [...] of <b>length</b> [...] is indicated by [...] The set [...] of all finite strings (regardless of their length) is indicated by the Kleene star operator as , and is also called the Kleene closure of [...] The notation [...] indicates the set of all infinite sequences over the alphabet , and [...] indicates the set [...] of all finite or infinite sequences.|$|R
40|$|We {{show that}} for every n ≥ 1 and over any finite alphabet, there is a word whose {{circular}} factors of length n have a one-to-one correspondence with the set of primitive words. In particular, we prove that such a word {{can be obtained by}} a greedy algorithm, or by concatenating all Lyndon words of length n in increasing lexicographic order. We also look into connections between de Bruijn graphs of primitive words and Lyndon graphs. Finally, we also show that the shortest word that contains every p-power of length pn over a k-letter <b>alphabet</b> has <b>length</b> between pk^n and roughly (p+ 1 /k) k^n, for all integers p ≥ 1. An algorithm that generates a word which achieves the upper bound is provided. Comment: Revisions made for publicatio...|$|R
40|$|Colloque avec actes et comité de lecture. A (fractional) {{repetition}} {{in a word}} $w$ is a subword {{with the}} period of at most half of the subword length. We study maximal repetitions occurring in $w$, that is those for which any extended subword of $w$ has a bigger period. The set of such repetitions represents in a compact way all repetitions in $w$. We first study maximal repetitions in Fibonacci words [...] we count their exact number, and estimate the sum of their exponents. These quantities {{turn out to be}} linearly-bounded in the length of the word. We then prove that the maximal number of maximal repetitions in general words (on arbitrary <b>alphabet)</b> of <b>length</b> $n$ is linearly-bounded in $n$, and we mention some applications and consequences of this result...|$|R
40|$|A novel fast {{recursive}} coding {{technique is}} proposed. It operates with only integer values not longer 8 bits and is multiplication free. Recursion the algorithm {{is based on}} indirectly provides rather effective coding of symbols for very large <b>alphabets.</b> The code <b>length</b> for the proposed technique can be up to 20 - 30 % less than for arithmetic coding and, in the worst case it is only by 1 - 3 % larger. Comment: 3 pages, submitted to IEEE Transactions on Information Theor...|$|R
50|$|If S is finite, then a group G = &lang;S&rang; {{is called}} finitely generated. The {{structure}} of finitely generated abelian groups {{in particular is}} easily described. Many theorems that are true for finitely generated groups fail for groups in general. It has been proven that if a finite group is generated by a subset S, then each group element may be expressed as a word from the <b>alphabet</b> S of <b>length</b> {{less than or equal}} to the order of the group.|$|R
40|$|Abstract — A novel fast {{recursive}} coding {{technique is}} proposed. It operates with only integer values not longer 8 bits and is multiplication free. Recursion the algorithm {{is based on}} indirectly provides rather effective coding of symbols for very large <b>alphabets.</b> The code <b>length</b> for the proposed technique can be up to 20 - 30 % less than for arithmetic coding and, in the worst case it is only by 1 - 3 % larger. Index Terms—Arithmetic coding, Huffman codes, data compression, fast algorithms...|$|R
40|$|We {{consider}} the following variant of Huffman coding in which {{the costs of the}} letters, rather than the probabilities of the words, are non-uniform: Given an <b>alphabet</b> of unequal. <b>length</b> letters, find a minimum-average-length prefix-free set of n codewords over the alphabet. We show new structural properties of such codes, leading to an O(n log 2 r) time algorithm for finding them. This new algorithm is simpler and faster than the previously best known O(nr min{log n, r}) one due to Perl, Garey, and Even [5]...|$|R
40|$|Richard Stanely proposed, in {{a recent}} Amer. Math. Monthly Problem, to prove a nice {{explicit}} formula for the generating function {{for the number of}} n-letter words in {H,T} that have as many occurrences of HT as HH. In this article, we show how to prove this problem automatically, and ANY problem of that type, regardless {{of the size of the}} <b>alphabet</b> and the <b>length</b> of the two chosen stringsComment: Accompanied by Maple package <A HREF="[URL]. Sample input and output files can be gotten from <a href="[URL] page</a...|$|R
40|$|SummaryIt was proved by Ahlswede (1971) that codes whose codewords form a {{group or}} even a linear space do not achieve Shannon's {{capacity}} for discrete memoryless channels even if the decoding procedure is arbitrary. Sharper results were obtained in Part I of this paper. For practical purposes, one is interested not only in codes which allow a short encoding procedure but also an efficient decoding procedure. Linear codes—the codewords form a linear space and the decoding is done by coset leader decoding — have a fairly efficient decoding procedure. But {{in order to achieve}} high rates the following slight generalization turns out to be very useful: We allow the encoder to use a coset of a linear space as a set of codewords. We call these codes shifted linear codes or coset codes. They were implicitly used by Dobrushin (1963). This new code concept has all the advantages of the previous one with respect to encoding and decoding efficiency and enables us to achieve positive rate on discrete memoryless channels whenever Shannon's channel capacity is positive and the <b>length</b> of the <b>alphabet</b> is less or equal to 5 (Theorem 3. 1. 1). (The result holds very likely also for all <b>alphabets</b> with a <b>length</b> a = ps, p prime, s positive integer). A disadvantage of the concepts of linear codes and of shifted linear codes is that they can be defined only for <b>alphabets</b> whose <b>length</b> is a prime power. In order to overcome this difficulty, we introduce generalized shifted linear codes. With these codes we can achieve a positive rate on arbitrary discrete memoryless channels if Shannon's capacity is positive (Theorem 3. 2. 1...|$|R
40|$|Problems {{associated}} with finding strings that are within [...] . In this paper, we use techniques from parameterized computational complexity to assess non-polynomial time algorithmic options {{for three of}} these problems, namely p-EXACT SUBSTRING (pES), APPROXIMATE SUBSTRING (1 AS), and p-APPROXIMATE SUBSTRING (pAS). These problems vary whether the substring must be an exact match, and also whether a single substring or a set of substrings (of cardinality p) is required. Our analyses indicate under which parameter restrictions useful algorithms are possible, and include both class membership and parameterized reductions to prove class hardness. Since variation in parameter restrictions will lead to different algorithms being preferable, we give a variety of algorithms for the fixed parameter tractable problem variations. One of these, for 1 AS with <b>alphabet,</b> substring <b>length,</b> and distance all fixed, is an improvement {{of one of the}} best previously known exact algorithms (under these restrictions). Other algorithms solve parameterized variants previously unexplored. We also prove that pES is NP-hard, and show inapproximability for pES and aAS...|$|R
40|$|Error Tree {{is a novel}} tree {{structure}} that is mainly oriented to solve the approximate pattern matching problems, Hamming and edit distances, {{as well as the}} wildcards matching problem. The input is a text of length n over a fixed <b>alphabet</b> of <b>length</b> Σ, a pattern of length m, and k. The output is to find all positions that have ≤ k Hamming distance, edit distance, or wildcards matching with P. The algorithm proposes for Hamming distance and wildcards matching a {{tree structure}} that needs O(nlog_Σ ^kn/k!) words and takes O(m^k/k! + occ) (O(m + log_Σ ^kn/k! + occ) in the average case) of query time for any online/offline pattern, where occ is the number of outputs. As well, a tree structure of O(2 ^knlog_Σ ^kn/k!) words and O(m^k/k! + 3 ^kocc) (O(m + log_Σ ^kn/k! + 3 ^kocc) in the average case) query time for edit distance for any online/offline pattern...|$|R
40|$|Each {{data string}} over a finite <b>alphabet</b> of <b>length</b> {{a power of}} two is {{represented}} via a binary tree called a bisection tree. The nodes of the bisection tree correspond {{to the members of}} the smallest class of substrings of the data string which contains the data string and is closed with respect to bisection. A data string can be perfectly reconstructed from its bisection tree. A lossless data compression algorithm is presented which compresses the data string by compressing its bisection tree. This algorithm is shown to be a universal algorithm in the sense that it yields a compression performance at least as good as the compression performance provided by any finite-state sequential lossless data compression algorithm, asymptotically as the length of the data string goes to infinity. Index Terms: lossless data compression, arithmetic coding, entropy, universal algorithms I Introduction Throughout this paper, let A be a generic symbol denoting a finite alphabet containing at least tw [...] ...|$|R
40|$|In {{wireless}} communication, {{the minimum}} Euclidean distance between codewords {{is a major}} factor for the ability to correct errors in messages, and it is of interest to maximize the minimum Euclidean distance. The thesis improves previously established general upper bounds on minimum Euclidean distance of phase shift keying block codes. There are no requirements on structure of codes, as the bound depends only on <b>alphabet</b> size, word <b>length</b> and code size. Prior to this thesis, bounds found by use of a method of Elias, had been improved by generalization of Elias' method. The method used here is an attempt to optimize that generalization...|$|R
40|$|The Shortest Common Supersequence (SCS) {{problem is}} a {{classical}} problem from stringology which has applications e. g. in Arti cial Intelligence (speci cally planning), mechanical engineering and data compression. The SCS problem is NP-complete under various restrictions concerning the <b>alphabet</b> size, the <b>length</b> of the given strings, or their structure. Using a Genetic Algorithm to solve SCS is not easy, e. g. because the search space only contains a few valid solutions of reasonable length and a natural representation would lead to varying string lengths. To circumvent these diculties, we base the Genetic Algorithm on a slightly modi ed well known heuristic...|$|R
40|$|We prove {{a central}} limit theorem for non-commutative random {{variables}} in a von Neumann algebra with a tracial state: Any non-commutative polynomial of averages of i. i. d. samples converges to a classical limit. The proof {{is based on a}} {{central limit theorem}} for ordered joint distributions together with a commutator estimate related to the Baker-Campbell-Hausdorff expansion. The result can be considered a generalization of Johansson's theorem on the limiting distribution of the shape of a random word in a fixed <b>alphabet</b> as its <b>length</b> goes to infinity [math. CO/ 9906120,math. PR/ 9909104]. Comment: 7 page...|$|R
40|$|After Büchi it {{has become}} very natural to interprete {{formulae}} of certain logical theories as finite automata, i. e., as recognizing devices. This recognition aspect though, was neglected by {{the inventor of the}} concept and the study of the families of linear structures that could be accepted in the language theory sense of the term, was carried out by other authors. The most popular field of application of Büchi type automata is nowadays connected with model checking by considering a process as a possibly infinite sequence of events. For over a decade, the original model has been enriched by adding a time parameter in order to model reactive systems and their properties. Originally Büchi was interested in the monadic second order theory with the successor over ω but he later considered the theory of countable ordinals for which he was led to propose new notions of finite automata. Again these constructs can be viewed as recognizing devices of words over a finite <b>alphabet</b> whose <b>length</b> are countable ordinals. They were studied by other authors, mainly Chouek...|$|R
40|$|We {{show that}} the Fixed Alphabet Shortest Common Supersequence (SCS) and the Fixed Alphabet Longest Common Subsequence (LCS) {{problems}} parameterized {{in the number of}} strings are W [1]-hard. Unless W [1] = FPT, this rules out the existence of algorithms with time complexity of O(f(k) n α) for those problems. Here n is the size of the problem instance, α is constant, k is the number of strings and f is any function of k. The fixed alphabet version of the LCS problem is of particular interest considering the importance of sequence comparison (e. g. multiple sequence alignment) in the fixed <b>length</b> <b>alphabet</b> world of DNA and protein sequences...|$|R
40|$|AbstractWe {{show that}} the fixed {{alphabet}} shortest common supersequence (SCS) and the fixed alphabet longest common subsequence (LCS) problems parameterized {{in the number of}} strings are W[1]-hard. Unless W[1]=FPT, this rules out the existence of algorithms with time complexity of O(f(k) nα) for those problems. Here n is the size of the problem instance, α is constant, k is the number of strings and f is any function of k. The fixed alphabet version of the LCS problem is of particular interest considering the importance of sequence comparison (e. g. multiple sequence alignment) in the fixed <b>length</b> <b>alphabet</b> world of DNA and protein sequences...|$|R
40|$|All {{sufficiently}} long binary words {{contain a}} square {{but there are}} infinite binary words having only the short squares 00, 11 and 0101. Recently it was shown by J. Currie that there exist cyclically square-free words in a ternary <b>alphabet</b> except for <b>lengths</b> 5, 7, 9, 10, 14, and 17. We consider binary words all conjugates of which contain only short squares. We show that the number c(n) of these binary words of length n grows unboundedly. In order for this, we show that there are morphisms that preserve circular square-free words in the ternary alphabet. Key words: combinatorics on words, repetitions, conjugates, square-free words, cyclically square-free, almost square-free word...|$|R
40|$|This paper generalizes {{previous}} optimal upper bounds on {{the minimum}} Euclidean distance for {{phase shift keying}} (PSK) block codes, that are explicit in three parameters: <b>alphabet</b> size, block <b>length</b> and code size. The bounds are primarily generalized from codes over symmetric PSK to codes over asymmetric PSK and also to general alphabet size. Furthermore, block codes are optimized {{in the presence of}} other types of noise than Gaussian, which induces also non-Euclidean distance measures. In some instances, codes over asymmetric PSK prove to give higher Euclidean distance than any code over symmetric PSK with the same parameters. We also provide certain classes of codes that are optimal among codes over symmetric PSK...|$|R
40|$|Abstract. In this paper, {{we study}} {{the problem of}} mining {{frequent}} di-amond episodes efficiently from an input event sequence with sliding a window. Here, a diamond episode is of the form a → E → b, which means that every event of E follows an event a and is followed by an event b. Then, we design a polynomial-delay and polynomial-space algorithm PolyFreqDmd that finds all of the frequent diamond episodes without duplicates from an event sequence in O(|Σ| 2 n) time per an episode and in O(|Σ | + n) space, where Σ and n are an <b>alphabet</b> and the <b>length</b> the event sequence, respectively. Finally, we give experimental results on artificial event sequences with varying several mining parameters to evaluate {{the efficiency of the}} algorithm. ...|$|R
