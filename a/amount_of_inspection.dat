10|10000|Public
5000|$|Another {{component}} of an [...] "intelligent" [...] aircraft structure {{is the ability}} to sense and diagnose potential threats to its structural integrity. This differs from conventional non-destructive testing (NDT) by the fact that Structural Health Monitoring (SHM) [...] uses sensors that are permanently bonded or embedded in the structure. Composite materials, which are highly susceptible to hidden internal flaws which may occur during manufacturing and processing of the material or while the structure is subjected to service loads, require a substantial <b>amount</b> <b>of</b> <b>inspection</b> and defect monitoring at regular intervals. Thus, the increasing use of composite materials for aircraft primary structure aircraft components increases substantially their life cycle cost. According to some estimates, over 25% of the life cycle cost of an aircraft or aerospace structure, which includes pre-production, production, and post-production costs, can be attributed to operation and support, involving inspection and maintenance. With sensing technology reducing in cost, size and weight, and sensor signal processing power continuously increasing, a variety of approaches have been developed allowing integration of such sensing options onto or into structural components.|$|E
40|$|Abstract. Increasing the <b>amount</b> <b>of</b> <b>inspection</b> {{activities}} and inspecting a large {{fraction of the}} items are two approaches {{that are used to}} improve products ’ quality. In this paper, as in a prece-dent article by Jaraiedi, et al. [5], 100 % inspection in combination with multiple-criteria decision (MCD) response is considered. Three different inspection procedures for the multi-stage inspec-tion are presented. Performance of these three procedures are derived and the Average Outgoin...|$|E
40|$|Increasing the <b>amount</b> <b>of</b> <b>inspection</b> {{activities}} and inspecting a large {{fraction of the}} items are two approaches {{that are used to}} improve products' quality. In this paper, as in a precedent article by Jaraiedi, et al. [5], 100 % inspection in combination with multiple-criteria decision (MCD) response is considered. Three different inspection procedures for the multi-stage inspection are presented. Performance of these three procedures are derived and the Average Outgoing Quality (AOQ), Average False Rejected (AFR), and Overall Average Fraction Inspected (SAFI) for all procedures are compared. Two examples are discussed in depth to illustrate numerical comparisons of these procedures...|$|E
40|$|In this {{contribution}} we outline a {{new project}} {{in the field of}} service robotics. The goal is to develop teams of mobile robots that perform routine inspection, health monitoring, and early fault detection in large chemical process plants or in public utility networks. We envisage a general-purpose assistant for the maintenance engineer to collect, compress and screen large <b>amounts</b> <b>of</b> <b>inspection</b> data. It will provide fast and extensiv...|$|R
40|$|At {{the end of}} 2013, the {{environmental}} administration of Karlstad municipality still had 975 regulatory hours left to perform which they had already charged the food businesses for. The environmental administration {{is in need of}} a more effective approach since they have been struggling {{with the same kind of}} problem for years.   The introduction of the control receipt has proved to be good in two ways. The environmental administration has succeeded in getting a more efficient approach and the business operators has increased their understanding of the authority’s work.     For maintaining and/or even increasing the efficiency it is important for the inspectors to possess competence and professionalism, which is gained through continuous use of utilities from the National Food Agency. The food inspections influence on restaurant business operators is significant as well and affected by inspection frequency, treatment and the type <b>of</b> <b>inspection</b> (announced/unannounced). Therefore, {{the environmental}} administration may consider increasing both the total <b>amount</b> <b>of</b> <b>inspections</b> and the <b>amount</b> <b>of</b> announced <b>inspections.</b> Systems for incentives may even be considered as well...|$|R
40|$|This paper {{presents}} the analysed findings of experimental research {{carried out at}} a Caterpillar Remanufacturing facility in the UK into core inspection at the initial stage of remanufacture – Receive Core. It concludes that the initial analysis of the results show an increase in efficiency with increase <b>of</b> <b>inspection</b> but that this increase is finite and over-inspection leads to an overall decrease of efficiency {{and an increase in}} costs. However, it also shows that the <b>amount</b> <b>of</b> viable <b>inspection</b> is linked to the cost of the component rather than the facility operating costs. A generic methodology for core inspection is being developed to improve the overall efficiency of the remanufacturing operation...|$|R
40|$|This paper {{develops}} {{a model of}} job search in which jobs differ in the <b>amount</b> <b>of</b> <b>inspection</b> and experience uncertainty. Under these circumstances, reservation wages may not remain constant or decrease over an unemployment spell. Even if job offer rates are constant over an unemployment spell, negative duration dependence of the reemployment hazard can occur when individuals search amongst a variety of jobs. Since this duration dependence {{is a result of}} time-varying unobserved heterogeneity, conventional methods for controlling for unobserved heterogeneity in reduced-form hazard models are inappropriate. Copyright 1994 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association. ...|$|E
40|$|Plant {{reliability}} and profitability are important performance indicators for oil refineries. The <b>amount</b> <b>of</b> <b>inspection</b> and maintenance resources that {{is devoted to}} process equipment effects the overall plant performance. Dedicating {{the same level of}} resources to inspecting and maintaining each piece of equipment is not cost effective and can lead to oversight of high risk equipment. Although in an operating plant, the number of high risk items usually exists in lower percentages than low risk items, an oversight in the inspection and maintenance of high risk equipment may produce catastrophic results. Risk based inspection (RBI) provides a methodology for prudent assignment of resources to assess and maintain equipment integrity based on their risk levels. The application of RBI to pressure vessels and aboveground storage tanks are demonstrated through two examples related to a crude distillation column and a crude storage tank...|$|E
40|$|Abstract—The {{inspection}} of software products {{has proven to}} be an effective approach to find defects. Inspecting requirements documents offers especially large benefits as it removes defects very early in the development process. On the other hand inspection is also an expensive and sometimes cumbersome process resulting in a large <b>amount</b> <b>of</b> <b>inspection</b> material that has to be sorted, searched, and consolidated. Existing inspection tools whose success has been empirically evaluated are focused on code inspection and fall short for inspection needs of early life cycle documents like requirements specifications. Based (a) on empirical data from our experiments with paperbased {{inspection of}} requirements documents and (b) on our experience with groupware support for software requirements negotiation, we have developed a concept for a groupwaresupported requirements inspection process. In this paper we present our concept, discuss potential benefits for software requirements inspection, and propose an approach for empirical evaluation. Index Terms—Inspection, requirements, groupware, empirical evaluation criteria...|$|E
40|$|This report {{presents}} {{a comprehensive review}} of the maritime safety regimes and provides recommendations on how to improve the system. The results show a complex legal framework which generates a high <b>amount</b> <b>of</b> <b>inspections</b> and overlapping <b>of</b> <b>inspection</b> areas where no cross-recognition is established by the various stakeholders. While the safety system seems to be successful in eliminating substandard vessels and while average insurance claims costs are substantially lower for inspected vessels than not inspected vessels, the results indicate that the economic conditions of the shipping market also have an effect on safety quality besides the frequency <b>of</b> <b>inspections.</b> No significant differences can be found between industry inspections and port state control inspections with respect to decreasing the probability of casualty. The system could be made more effective by combining data sources on inspections and use them respectively to improve risk profiling and to decrease the frequency <b>of</b> <b>inspections</b> performed on ship types such as tankers. The results further indicate a lack of proper implementation of the International Safety Management Code (ISM code) and conventions with reference to working and living conditions of crew (ILO 147). A revision of the ISM code and more emphasis on enforcement of ILO 147 could further enhance the level of safety at sea. The {{authors would like to thank}} several inspection regimes for their cooperation in providing inspection data and in allowing the observation <b>of</b> surveys and <b>inspections</b> on 26 vessels. In addition, the authors would like to acknowledge the data providers for the casualty data, Clarkson’s for the economic data as well as two P&I Clubs in making data on insurance claims available...|$|R
30|$|Supply-chain finance {{involves}} an extensive <b>amount</b> <b>of</b> manual <b>inspections</b> and paper-based transactions. The process also has numerous intermediaries, {{a high risk}} of illegal transactions, high costs, and low efficiency. Blockchain technology can drastically reduce manual interventions and employ smart contracts in order to digitize procedures that rely heavily on paperwork. This would greatly improve the efficiency of supply-chain finance and reduce manual operational risks. With the supplier, buyer, and bank as the main trading parties, and the sharing of contractual information on a decentralized distributed ledger, smart contracts can ensure that payments are made automatically once a predetermined time and result is reached.|$|R
40|$|From 1990 to 1993, {{a series}} of tests on {{numerical}} reliability ofdata analysis systems has been carried out. The tests are based onL. Wilkinson's "Statistics Quiz". Systems under test included BMDP, Data Desk,Excel, GLIM, ISP, SAS, SPSS, S-PLUS,STATGRAPHICS. The results showconsiderable problems even in basic features of well-known systems. For allour test exercises, the computational solutions are well known. The omissionsand failures observed here give some suspicions of what happens in lesswell-understood problem areas of computational statistics. We cannot takeresults of data analysis systems at face value, but have to submit them to alarge <b>amount</b> <b>of</b> informed <b>inspection.</b> Quality awareness still needs improvement...|$|R
40|$|Single {{sampling}} inspection plans proposed by Dodge and Romig have been critically examined {{in respect of}} the distribution of the number of defectives in a sample as and also the requirement of customer’s risk being equal to 0. 10. The problem of determining the plan parameters has been formulated as a non-linear integer programming, a non-zero-sum (bi-matrix) game problem between the producer and the customer, and also as problem in (non-statistical) decision analysis. Further, the requirement about a given customer’s risk has been considered as a fuzzy constraint in a fuzzy nonlinear integer programming problem which can be eventually solved in terms of two non-linear integer programming problems, assuming a trapezoidal membership function. Different optimality criteria for determining the plan parameters as constituting the strategies in a decision matrix, where states of nature correspond to various plausible values of lot fraction defective and the pay-off is the average <b>amount</b> <b>of</b> <b>inspection,</b> have been considered. Finally, the author indicates how this analysis is relevant to a wide class of statistical decision problems, particularly many non-parametric test...|$|E
40|$|This {{research}} addresses several {{quality control}} problems which arise {{in a variety}} of manufacturing, healthcare, service, finance, and other industries given the existence of human and automated attribute detection error. Several mathematical and economic models are developed for various types of single and multiple inspection screening policies in order first to examine inherent tradeoffs between type I errors, type II errors, and all associated inspection, false-rejection, and false-acceptance costs and then, ultimately, to help identify the minimum expected total cost policy and the optimal <b>amount</b> <b>of</b> <b>inspection</b> for any particular scenario. While originally motivated by industrial problems, these models also have been adapted to various non-manufacturing concerns, including service processes and laboratory cancer screening policies. In particular, similar methods are developed and used to analyze the policy for screening Pap smears for early indications of cervical cancer currently required by the Congressional Laboratory Improvements Amendments Act of 1988 (CLIA 2 ̆ 788), to compare this policy with possible alternatives, and to develop an algorithm that identifies the optimal policy in any given scenario. ^ Results show that the mandated CLIA policy never is optimal and always increases total costs, that overall sensitivity of CLIA never can be improved beyond a certain mathematical bound, that CLIA 2 ̆ 7 s 10...|$|E
40|$|Doctor of PhilosophyDepartment of Electrical and Computer EngineeringDon M. GruenbacherCaterina M. ScoglioCyber{{security}} {{is playing}} {{a vital role in}} today's network. We can use security devices, such as a deep packet inspection (DPI) device, to enhance cybersecurity. However, a DPI has a limited <b>amount</b> <b>of</b> <b>inspection</b> capability, which cannot catch up with the ever-increasing volume of network traffic, and that gap is getting even larger. Therefore, inspecting every single packet using DPI is impractical. Our objective is to find a tradeoff between network security and network performance. More explicitly, we aim at maximizing the utilization of security devices, while not decreasing network throughput. We propose two prototypes to address this issue in a demilitarized zone (DMZ) architecture. Our first prototype involves a flow-size based DMZ criterion. In a campus network elephant flows, flows with large data rate, are usually science data and they are mostly safe. Moreover, the majority of the network bandwidth is consumed by elephant flows. Therefore, we propose a DMZ prototype that we inspect elephant flows for a few seconds, and then we allow them to bypass DPI inspection, {{as long as they are}} identified as safe flows; and they can be periodically inspected to ensure they remain safe. Our second prototype is a congestion-aware DMZ scheme. Instead of determining whether a flow is safe or not by its size, we treat all flows identically. We measure the data rates of all flows, and use a global optimization algorithm to determine which flows are allowed to safely bypass a DPI. The objective is to maximize DPI utilization. Both prototypes are implemented using OpenFlow in this work, and extensive experiments are performed to test both prototypes' feasibility. The results attest that the two prototypes are effective in ensuring network security while not compromising network performance. A number of tools for SDN network configuring and testing are also developed...|$|E
40|$|This paper {{examines}} the South Carolina Office of Regulatory Staff's Natural Gas Pipeline Safety Program {{and how it}} can improve its efficiency. lf the Program can decrease the <b>amount</b> <b>of</b> time spent associated with hard-paper processes, then the Program can increase the <b>amount</b> <b>of</b> time performing <b>inspections.</b> Additionally, if the Program can eliminate hard-paper processes, then there will be a decrease in equipment cost (i. e. vehicle wear, office supplies, cameras, printers, etc.) ...|$|R
40|$|This paper {{presents}} {{the findings of}} experimental research carried out at a Caterpillar Remanufacturing facility in the UK into core inspection at the initial stage of remanufacture – Receive Core. The initial analysis of results shows an increase in productivity with increase <b>of</b> <b>inspection</b> but that this increase is finite and over-inspection leads to an overall decrease of productivity {{and an increase in}} costs. However the results also show that the <b>amount</b> <b>of</b> viable <b>inspection</b> can be more closely linked to the cost of the component rather than the facility operating costs. This is important because remanufacturers traditionally base their pricing and product recovery on their operating costs. The new knowledge concerning the factors affecting the efficacy <b>of</b> core <b>inspection</b> is being used to develop a generic decision-making methodology for core inspection at component level to improve the overall efficiency (in terms of increased productivity and cost reduction) of the remanufacturing operation...|$|R
40|$|Research in text {{classification}} (a. k. a. predictive coding) usually {{focuses on}} the design of algorithms for training a text classifier from manually coded data, and for automatically classifying, via the trained classifier, large <b>amounts</b> <b>of</b> uncoded data. Very little attention, if any, has been given to what comes next, i. e., to supporting human annotators in inspecting (and correcting if appropriate) the automatically classified documents with the goal <b>of</b> reducing the <b>amount</b> <b>of</b> classification error present in the data. In this talk I will present recent research aimed at minimizing the <b>amount</b> <b>of</b> human <b>inspection</b> effort needed to reduce the classification error down to a desired level. The fact that for many applications false positives and false negatives weigh differently calls for an approach to this task based on utility theory...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "Since the attacks of September 11, 2001, combating terrorism {{has been one of}} the nation's highest priorities. As part of that effort, preventing nuclear and radioactive material from being smuggled into the United States [...] perhaps to be used by terrorists in a nuclear weapon or in a radiological dispersal device (a "dirty bomb") [...] has become a key national security objective. On April 15, 2005, the president directed the establishment, within the Department of Homeland Security (DHS), of the Domestic Nuclear Detection Office (DNDO), whose duties include acquiring and supporting the deployment of radiation detection equipment. In October 2006, Congress enacted the SAFE Port Act, which made DNDO responsible for the development, testing, acquisition and deployment of a system to detect radiation at U. S. ports of entry. An important component of this system is the deployment of radiation portal monitors, large stationary detectors through which cargo containers and trucks pass as they enter the United States. Prior to DNDO's creation, another DHS agency [...] U. S. Customs and Border Protection (CBP) [...] managed programs for deployment of radiation detection equipment. In 2002, CBP began the radiation portal monitor project, deploying radiation detection equipment at U. S. ports of entry. This program initially deployed portal monitors, known as polyvinyl toluene monitors (PVT), and handheld detection technologies, such as radioactive isotope identification devices (RIID). CBP also established a system of standard operating procedures to guide its officers in the use of this equipment. Current procedures include conducting primary inspections with PVTs to detect the presence of radioactivity, and secondary inspections with PVTs and RIIDs to confirm and identify the source and determine whether it constitutes a threat. After its creation, DNDO assumed responsibility for the development, testing, and deployment of radiation detection equipment, while CBP maintained its role of operating the equipment at U. S. ports of entry. Currently deployed PVTs are capable of detecting radiation, but they have an inherent limitation because they are unable to identify specific radioactive isotopes and therefore cannot distinguish between dangerous and benign materials. CBP officers also use RIIDs to identify different types of radioactive material. However, RIIDs are limited in their ability to identify nuclear material. DNDO believes that these deficiencies may delay legitimate commerce at ports of entry, and that CBP may use an inordinate <b>amount</b> <b>of</b> <b>inspection</b> resources for radiation detection at the expense of other missions, such as drug interdiction. ...|$|E
40|$|Research in text {{classification}} (a. k. a. verbatim coding) mostly {{focuses on}} the design of software systems for classifying large <b>amounts</b> <b>of</b> uncoded data. Some involve a training phase, whereby a text classifier "learns" to code verbatims from manually coded examples. Scant attention has been given to designing software that supports what often come next: further human editing and even correction of the data to reduce classification errors. In this presentation I will present recent research aimed at optimizing the <b>amount</b> <b>of</b> human <b>inspection</b> effort needed to reduce the classification error down to a desired level. The fact that, for many applications, false positives and false negatives weigh differently on what one perceives "error" to be, calls for an approach to this task based on utility theory...|$|R
50|$|Reusable launch systems require maintenance, {{which is}} often substantial. The Space Shuttle system {{required}} extensive refurbishing between flights, primarily dealing with the silica tile TPS and the high performance LH2/LOX burning main engines. Both systems require a significant <b>amount</b> <b>of</b> detailed <b>inspection,</b> rebuilding and parts replacement between flights, and account for over 75% of the maintenance costs of the Shuttle system. These costs, far in excess {{of what had been}} anticipated when the system was constructed, have cut the maximum flight rate of Shuttle to 1/4 of that planned. This has also quadrupled the cost per pound of payload to orbit, making Shuttle economically infeasible in today's launch market for any but the largest payloads, {{for which there is no}} competition.|$|R
40|$|The Rokkasho Spent Fuel Receipt and Storage (RSFS) Facility at the Rokkasho Reprocessing Plant (RRP) in Japan is {{expected}} to begin operations in 1998. Effective safeguarding by International Atomic Energy Agency (IAEA) and Japan Atomic Energy Bureau (JAEB) inspectors requires monitoring the time of transfer, direction of movement, and number of spent fuel assemblies transferred. At peak throughput, up to 1, 000 spent fuel assemblies will be accepted by the facility in a 90 -day period. In order for the safeguards inspector to efficiently review the resulting large <b>amounts</b> <b>of</b> <b>inspection</b> information, an unattended monitoring system was developed that integrates containment and surveillance (C/S) video with radiation monitors. This allows for an integrated review of the facility`s radiation data, C/S video, and operator declaration data. This paper presents an outline of the integrated unattended monitoring hardware and associated data reviewing software. The hardware consists of a multicamera optical surveillance (MOS) system radiation monitoring gamma-ray and neutron detector (GRAND) electronics, and an intelligent local operating network (ILON). The ILON was used for time synchronization and MOS video triggers. The new software consists of a suite of tools, each one specific to a single data type: radiation data, surveillance video, and operator declarations. Each tool {{can be used in a}} stand-alone mode as a separate ion application or configured to communicate and match time-synchronized data with any of the other tools. A data summary and comparison application (Integrated Review System [IRS]) coordinates the use of all of the data-specific review tools under a single-user interface. It therefore automates and simplifies the importation of data and the data-specific analyses...|$|R
40|$|The {{advances}} of computational methods and tools can greatly support other areas in doing tasks {{from the most}} tedious or repetitive to the most complex. In this paper, these advances were manipulated in civil structures maintenance specifically in pipeline corrosion assessment. This paper describes mechanize method developed to automatically detect and quantitY important parameters for future prediction of corrosion growth using In-line inspection (Ill) data. The focal process in this system includes data conversion, data filtering, parameter tolerance or sizing configuration, matching, and data trimming. A sensitivity analysis using linear regression method was used to correlates defects from one inspection to the next. Issues and advantage gain from this mechanize system is threefold~ Firstly, timeliness (manual matching procedure consumed {{a great deal of}} time). Secondly, accuracy and consistencies in data sampling (current implementation, different researcher obtain a different number of sample even though the method used in matching were the same), and finally, reduction of data matching error (manual matching was prone to human error due to the masses <b>of</b> <b>inspection</b> data to be match. Furthermore this method is impractical when facing the large <b>amount</b> <b>of</b> real <b>inspection</b> data) ...|$|R
40|$|Abstract—Previous {{attempts}} at supply chain risk management are often non-technical and {{rely heavily on}} policies/procedures to provide security assurances. This is particularity worrisome as there are vast volumes of data that must be analyzed and data continues to grow at unprecedented rates. In order to mitigate these issues and minimize the <b>amount</b> <b>of</b> manual <b>inspection</b> required, we propose the development of mathematically-based automated screening methods that {{can be incorporated into}} supply chain risk management. In particular, we look at methods for identifying deception and deceptive practices that may be present in the supply chain. We examine two classes of constraints faced by deceivers, cognitive/computational limitations and strategic tradeoffs, which can be used to developed graph-based metrics to represent entity behavior. By using these metrics with novel machine learning algorithms, we can robustly detect deceptive behavior and identify potential supply chain issues...|$|R
40|$|One of {{the main}} issues in modern supply chain {{management}} is the recovery of value {{from the end of}} life (EOL) or defective products by re-manufacturing, reassembly, re-use and recycling. Despite the fact that reverse logistics would impose extra <b>amount</b> <b>of</b> complexity to the supply chain, it has captured a lot of attention as it is possible to recycle the materials where there are limited resources. Through reverse logistics companies will be able to minimize the overall production costs through reclaiming the unsold or defective products’ values which in turn may lead to more productivity and growth, and more importantly reverse logistics may improve the quality of end products by finding the faults of the system and the points which directly or indirectly affect the ultimate product. However, a number of challenges arise with reverse logistics; integration of the whole supply chain including both inbound activities and outbound activities, creating incentives for return and reuse, huge <b>amount</b> <b>of</b> <b>inspections</b> and imposed complexity to the supply chain as a whole since the number of partners may increase. On the other hand, technologies such as barcodes, radio frequency identification (RFID), global positioning system (GPS), etc, have made it easier to cope with the aforementioned challenges and complexities of reverse supply chains. In this thesis, our goal is to examine the potential of radio frequency identification (RFID) technology on dis-assembly operations of aircraft at the End of Life using system dynamics simulation. In particular, a case study on how RFID technology affects the time of dis-assembly of a single helicopter has been conducted in cooperation with Bell Helicopters. The proposed System dynamics simulation model is developed using “AnyLogic”. The results of our study show that employing RFID technology will lead to a reduction in total dis-assembly time of a helicopter. However, bringing motivations to the market to employ RFID technology in industries and developing trust in the promising benefits and results will require more challenging planning and managerial activities. Keywords: Reverse logistics (RL), RFID, aviation industry, end of life products (EOL), System Dynamics simulation...|$|R
40|$|This {{paper is}} to present a security-related {{motivation}} for compiler verification, and in particular for binary compiler implementation verification. We will prove that source level verification is not sufficient in order to guarantee compiler correctness. For this, we will adopt the scenario of a well-known attack to Unix operating system programs due to intruded Trojan Horses in compiler executables. Such a compiler will pass nearly every test, {{state of the art}} compiler validation, the strong bootstrap test, any <b>amount</b> <b>of</b> source code <b>inspection</b> and verification, but for all that, it nevertheless might eventually cause a catastrophe...|$|R
40|$|This paper {{contributes}} {{an integrated}} {{survey of the}} work in the area <b>of</b> software <b>inspection.</b> It consists <b>of</b> two main sections. The first introduces {{a detailed description of the}} core concepts and relationships that together define the field <b>of</b> software <b>inspection.</b> The second elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail. After Fagan's seminal work presented in 1976, the body of work in software inspection has greatly increased and reached measured maturity. Yet, there is still no encompassing and systematic view of this research body driven from a life-cycle perspective. This perspective is important since inspection methods and refinements are most often aligned to particular life-cycle artifacts. It also provides practitioners with a road-map available in their terms. To provide a systematic and encompassing view of the research and practice body in software <b>inspection,</b> the contribution <b>of</b> this survey is, in a first step, to introduce in detail the core concepts and relationships that together embody the field <b>of</b> software <b>inspection.</b> This lays out the field key ideas and benefits as well as to elicit a common vocabulary. There, we make a strong effort to unify the relevant vocabulary used in available literature sources. In a second step, we use this vocabulary to build a contextual map of the field in the form of a taxonomy indexed by the different development stages of a generic process. This contextual map can guide practitioners and focus their attention to the inspection work most relevant to the introduction or development <b>of</b> <b>inspections</b> at the level of their particular development stage; or to help motivate the use <b>of</b> software <b>inspection</b> earlier in their development cycle. Our work provides three distinct, practical benefits: First, the index taxonomy can help practitioners identify inspection experience directly related to a particular life-cycle stage. Second, our work allows one to structure the large <b>amount</b> <b>of</b> published <b>inspection</b> work. Third, such taxonomy can help researchers compare and assess existing inspection methods and refinements to identify fruitful areas of future work...|$|R
40|$|This study {{investigated}} the extension of existing single acceptance sampling plans indexed by lot tolerance percent defective (LTPD) or limiting quality (LQ) {{with respect to their}} applicability to high precision processes. LTPD/LQ indexed sampling plans were extended to cover the very low fraction defective levels of high precision processes. Plans based on the ISO 2859 - 2 LQ indexed plans, Dodge-Romig LTPD indexed plans, and lot sensitive plans (LSP) were used as bases for the extensions. Target levels for LTPD/LQ were set at defective parts per million levels ranging from 20 ppm to 5000 ppm. The performance of the extended plans was evaluated using measures relating to level of protection afforded by the plans and efficiency in terms <b>of</b> <b>amount</b> <b>of</b> required <b>inspection.</b> The extended Dodge-Romig LTPD plans showed best performance among the three plans generated. Three selected plans from the extended Dodge-Romig LTPD sampling scheme were then subjected to simulation studies. Comparison of the theoretical and simulation data indicated that the plans behave more strictly than expected from theoretical calculations. The specific demands of high precision processes for appropriate sampling inspection plans that cover lower fraction defective levels, {{and at the same time}} satisfy the requirement for economy and efficiency <b>of</b> <b>inspection</b> were shown to be provided by this plan. These results contributed significantly to the manufacturing industry as it continuously strives to improve process yields and decreases fraction defective levels to respond to customer demands of better quality and improved performance...|$|R
40|$|The {{objective}} {{of this research is}} to detect change in urban areas using two satellite images (from 2001 and 2010) covering the city of Shanghai, China. These satellite images were acquired by Landsat- 7 and HJ- 1 B, two satellites with different sensors. Two change detection algorithms were tested: image differencing and post-classification comparison. For image differencing the difference image was classified using unsupervised k-means classification, the classes were then aggregated into change and no change by visual inspection. For post-classification comparison the images were classified using supervised maximum likelihood classification and then the difference image of the two classifications were classified into change and no change also by visual inspection. Image differencing produced result with poor overall accuracy (band 2 : 24. 07 %, band 3 : 25. 96 %, band 4 : 46. 93 %), while post-classification comparison produced result with better overall accuracy (90. 96 %). Post-classification comparison works well with images from different sensors, but it relies heavily on the accuracy of the classification. The major downside of the methodology of both algorithms was the large <b>amount</b> <b>of</b> visual <b>inspection...</b>|$|R
40|$|PII, GE Oil & Gas, {{facility}} in the UK holds a vast <b>amount</b> <b>of</b> historic pipeline <b>inspection</b> data which is required each time it performs a reinspection of a client’s pipeline. This data can be requested from any of its locations around the world. The inspection data is stored on old 3480 Tape Cartridges that are physically deteriorating. Besides, only a few experienced analysts remain in the company who can perform the translation re-processing tasks. For these reasons PII deemed {{the development of a}} project necessary to stablish policies for the global retention requirements <b>of</b> <b>inspection</b> data both historic and future. The following thesis is developed basing on the case study executed during a six-month internship at PII in Cramlington. It has an introduction to learn about the reasons of the study and then is divided in six chapters. First two chapters introduce the GE business segment of the project, locations, activities and inspections technologies. In chapter 3 pictures <b>of</b> an <b>inspection</b> are reported. In chapter 4 media, software and processes used by PII for data retention and procedures for transferring data on new tapes are described. In chapters 5 and 6 the project is introduced (team, objectives, times, resources) and studies are described in detail. Then, in chapter 7 the outcomes of enquiries and the different solutions that can be developed are axpounded...|$|R
30|$|Further {{analysis}} of the test {{results showed that the}} overall ratings of the nonspeech experts were slightly higher than the ratings given by the speech experts (Mann-Whitney test significance = 0.8). Furthermore, similar to the results of the first experiment, the female participants reported slightly better ratings in comparison to the male subjects (Mann-Whitney test significance = 0.9). As the subjects were the same as for the first experiment, this difference is again likely to be caused by the limited number of female viewers in comparison to the <b>amount</b> <b>of</b> male participants. <b>Inspection</b> <b>of</b> the overall ratings of each participant showed that in this second experiment like in the first one some participants reported in general higher ratings than other participants (Kruskal-Wallis test significance = 0.99). Nevertheless, also for this test we were mostly interested in the pairwise comparisons among the synthesis strategies for a single sentence rated by each particular viewer.|$|R
50|$|In {{addition}} to the final pressure testing, each hose is subjected to a variety <b>of</b> <b>inspections</b> and tests at each stage of manufacture. Some <b>of</b> these <b>inspections</b> and tests include visual inspections, ozone resistance tests, accelerated aging tests, adhesion tests of the bond between the liner and inner jacket, determination <b>of</b> the <b>amount</b> <b>of</b> hose twist under pressure, dimensional checks, and many more.|$|R
40|$|System {{developed}} that monitors integrity of composite-material structural components of aircraft in service. Includes strain gauges and accelerometers installed permanently in components to monitor vibrations, microprocessor-based data-acquisition system to process outputs of these vibration sensors, and desktop computer to analyze acquired data. By automating significant part <b>of</b> <b>inspection</b> process, system reduces <b>amount</b> <b>of</b> time needed for <b>inspection</b> and cost <b>of</b> <b>inspection</b> equipment. Contributes to safety by giving timely warning of hidden flaws that necessitate early, detailed <b>inspection</b> <b>of</b> critical components {{to determine whether}} components should be replaced immediately...|$|R
40|$|The {{personal}} {{distribution of}} income {{is assumed to be}} a function of the values of factors of production and their distribution among households. In the paper's first part, simple linear regressions on variables representing land, labor, education, and milchstock account for 36 % to 87 % of the variance in household incomes per capita, for data from nine surveys. The influence of caste is found to be primarily indirect, through different caste groups possessing different <b>amounts</b> <b>of</b> resources. Closer <b>inspection</b> reveals that the role of economic factors varied greatly with occupation. In the paper's second part, an original method, based on correction for the systematic accumulation of errors, refines the estimated income distribution obtained from the set of regression equation predictions for individual households. Inequality of land distribution is shown to be the only important factor in explaining income inequalities. The income distribution effects of a hypothetical land reform are simulated...|$|R
40|$|AbstractIn {{recognition}} that pressure equipment welds {{that have not}} undergone a full volumetric inspection may contain internal defects, a weld efficiency is introduced. For spot volumetric inspection which is typically 10 % of the weld length the weld efficiency is taken as 0. 85. Of interest is that this factor is universally adopted in all known pressure equipment codes around the world. Its origins are obscure and to the authors’ best knowledge, has gone unchallenged for the past 88 years. Additional interest {{is the use of}} 0. 7 weld efficiency for a weld that has undergone no volumetric inspection. This is prevalent in many international pressure equipment code, but not all. This paper revisits these factors. It considers how they were developed and explores a more rigorous probabilistic approach based on the <b>amount</b> <b>of</b> volumetric <b>inspection</b> and the likelihood of defects. It also considers the closely associated design factors. Understanding also that weld technology has developed since the early 1900 and in particular with the introduction of new technologies such as submerged arc welding, it may not be unreasonable to expect these weld efficiencies to differ from that initially developed. While the paper is not conclusive in its findings, it highlights there is justification to question the weld efficiencies adopted and proposes a program to develop more rigorous values...|$|R
40|$|This {{case study}} mainly {{focuses on the}} {{execution}} of programs. In particular we study the execution of compiler machine programs on an abstract machine that we implement in ACL 2. But this article also presents a security-related motivation for compiler verification and in particular for binary compiler implementation verification. We will prove that source level verification is not sucient to guarantee compiler correctness. For this, we will adopt the scenario of a well-known attack to Unix operating system programs due to intruded Trojan Horses in compiler executables. Such a compiler will pass nearly every test: {{state of the art}} compiler validation, the bootstrap test, any <b>amount</b> <b>of</b> source code <b>inspection</b> and verification. But for all that, it nevertheless might eventually cause a catastrophe. We will show such a program in detail; it is surprisingly easy to construct such a program. In that, we share a common experience with Ken Thompson, who initially documented this kind of attack in 1984 in his Turing Award Lecture [11]...|$|R
