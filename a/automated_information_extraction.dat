63|7629|Public
5000|$|OpenCalais is an <b>automated</b> <b>information</b> <b>extraction</b> {{web service}} from Thomson Reuters (Free limited version) ...|$|E
50|$|CiteSeerx uses <b>automated</b> <b>information</b> <b>extraction</b> tools, usually {{built on}} machine {{learning}} methods such ParsCit, to extract scholarly document metadata such as title, authors, abstract, citations, etc. As such, there are sometime errors in authors and titles. Other academic search engines have similar errors.|$|E
50|$|In 2006, Julien Chaveau of the University of Angers cited Jaikoz as an {{exemplar of}} an <b>automated</b> <b>information</b> <b>extraction</b> system, and in 2008 Badawia Albassuny of King Abdulaziz University {{included}} Jaikoz in his survey of automatic metadata generators {{for its ability}} to generate metadata by analyzing content.|$|E
40|$|This {{thesis is}} a step towards <b>automating</b> <b>information</b> <b>extraction</b> from {{clinical}} free-text. It establishes a Cost-efficient Enhanced Active Learning framework to significantly reduce annotation cost, while ensuring high-quality extracted information. The practical significance {{of this research is}} three-fold: (1) benefitting the overall patient healthcare by facilitating downstream eHealth workflows such as supporting clinical information processing and efficient decision making, (2) benefitting the research in medical informatics by facilitating the development of rich annotated corpora from clinical free text resources, and (3) benefitting the research in machine learning by developing domain-independent and effective active learning approaches...|$|R
40|$|Objective To {{develop a}} {{semantic}} representation for clinical research eligibility criteria to <b>automate</b> semistructured <b>information</b> <b>extraction</b> from eligibility criteria text. Materials and Methods An analysis pipeline called eligibility criteria extraction and representation (EliXR) was developed that integrates syntactic parsing and tree pattern mining to discover common semantic patterns in 1000 eligibility criteria randomly selected fro...|$|R
40|$|<b>Automated</b> road <b>information</b> <b>extraction</b> has {{significant}} applicability in transportation. It {{provides a means}} for creating, maintaining, and updating transportation network databases that are needed for purposes ranging from traffic management to automated vehicle navigation and guidance. This paper is to review literature {{on the subject of}} road extraction and to describe a study of an optimization-based method for automated road network extraction...|$|R
40|$|An {{investigation}} {{to examine the}} utility of spaceborne radar image data to malaria vector control programs is described. Specific tasks involve an analysis of radar illumination geometry vs information content, the synergy of radar and multispectral data mergers, and <b>automated</b> <b>information</b> <b>extraction</b> techniques...|$|E
40|$|<b>Automated</b> <b>information</b> <b>extraction</b> {{procedures}} {{for analysis of}} multitemporal Landsat data in non-U. S. crop inventory and monitoring are reviewed. Experiments to develop and evaluate crop area estimation technologies for spring small grains, summer crops, corn, and soybeans are discussed. Previously announced in STAR as N 82 - 3279...|$|E
40|$|Computational social {{sciences}} {{is a research}} discipline at the interface between computer science and the traditional {{social sciences}}. This interdisciplinary and emerging scientific field uses computationally methods to analyze and model social phenomena, social structures, and collective behavior. The main computational approaches to the social sciences are social network analysis, <b>automated</b> <b>information</b> <b>extraction</b> systems, social geographic information systems, complexity modeling, and social simulation models. info:eu-repo/semantics/publishedVersio...|$|E
40|$|This paper {{presents}} RoadRunner, {{a research}} project that aims at developing solutions for automatically extracting data from large HTML data sources. The target of our research are data-intensive Web sites, i. e., HTML-based sites with a fairly complex structure, that publish large amounts of data. The paper describes the top-level software architecture of the Road RunnerSystem, and the novel research challenges posed by the attempt to <b>automate</b> the <b>information</b> <b>extraction</b> process...|$|R
50|$|Ontology {{learning}} is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, {{there is great}} motivation to <b>automate</b> the process. <b>Information</b> <b>extraction</b> and text mining methods have been explored to automatically link ontologies to documents, e.g. {{in the context of}} the BioCreative challenges.|$|R
40|$|Detailed {{information}} on tree cover structure {{is critical for}} research and monitoring programs targeting African woodlands, including agroforestry parklands. High spatial resolution satellite imagery represents a potentially effective alternative to field-based surveys, but requires the development of accurate methods to <b>automate</b> <b>information</b> <b>extraction.</b> This study presents a method for tree crown mapping based on Geographic Object Based Image Analysis (GEOBIA) that use spectral and geometric information to detect and delineate individual tree crowns and crown clusters. The method was implemented on a WorldView- 2 image acquired over the parklands of Saponé, Burkina Faso, and rigorously evaluated against field reference data. The overall detection rate was 85. 4 % for individual tree crowns and crown clusters, with lower accuracies in areas with high tree density and dense understory vegetation. The overall delineation error (expressed as the difference between area of delineated object and crown area measured in the field) was 45. 6 % for individual tree crowns and 61. 5 % for crown clusters. Delineation accuracies were higher for medium (35 – 100 m 2) and large (> 100 m 2) trees compared to small (< 35 m 2) trees. The results indicate potential of GEOBIA and WorldView- 2 imagery for tree crown mapping in parkland landscapes and similar woodland areas. ...|$|R
40|$|Abstract — The {{information}} content of ENVISAT ASAR alternating polarization data {{is evaluated with}} respect to operations at the Canadian Ice Service. A data set covering an entire ice season is shown {{to have a higher}} {{information content}} compared to single polarization data. The potential for <b>automated</b> <b>information</b> <b>extraction</b> is also investigated, in particular problems caused by system noise and its variation over the swath. Keywords- ASAR; cross-polarization; sea ice I...|$|E
40|$|We {{present an}} {{information}} extraction system for <b>automated</b> <b>information</b> <b>extraction</b> from free text. The system, XAR, available open-source to the community, abstracts an extraction application developer from low level text processing details, permitting developing applications with high-level extraction rules. The system also incorporates semantic {{information in the}} form of integrity constraints {{to improve the quality of}} the extraction. We describe the XAR system and present a summary of experimental results demonstrating the effectiveness of our approach. 1...|$|E
40|$|Abstract. The {{purpose of}} this entry is to bring in an {{extension}} of ontologies {{so that they can}} be utilized in the process of <b>automated</b> <b>information</b> <b>extraction</b> from the web documents. Major part of it is dedicated to a proposition and derivation of an inference model for evaluation of the pattern matches and their combination. Further is proposed a simple naïve method of wrapper induction which is able to use the results of the first part...|$|E
40|$|Abstract. <b>Automated</b> road <b>information</b> <b>extraction</b> {{enables the}} ready creation, maintenance, and update of the {{transportation}} network databases used for traffic management and automated vehicle navigation. This paper presents a semi-automatic method for road network extraction from high-resolution satellite images. First, we focus on detecting the seed points in candidate road regions using a Kohonen-type self-organizing map (SOM). Then, an approach to road tracking is presented, searching for connected points in the direction and candidate domain of a road. A study of Geographical Information Systems (GIS) with high-resolution satellite images is presented in this paper. Experimental results verified the effectiveness and efficiency of this approach. Keywords: Road network extraction, Self-organizing map, High-resolution satellite imagery 1. Introduction. Th...|$|R
40|$|We present new {{techniques}} for supervised wrapper generation and <b>automated</b> web <b>information</b> <b>extraction,</b> {{and a system}} called Lixto implementing these techniques [6]. Our system can generate wrappers which translate relevant pieces of HTML pages into XML. Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface. In this convenient user-interface very expressive extraction programs can be created. Internally, this functionality is reflected by the new logic-based declarative language Elog. Users never {{have to deal with}} Elog and even familiarity with HTML is not required. Lixto can be used to create an "XML-Companion" for an HTML web page with changing content, containing the continually updated XML translation of the relevant information. 1...|$|R
40|$|Motivated by {{the need}} to <b>automate</b> medical <b>information</b> <b>extraction</b> from free-text {{radiological}} reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks. Comment: LOUHI 2016 conference proceeding...|$|R
40|$|We present HIGGINS, {{a system}} for Knowledge Acquisition (KA), placing {{emphasis}} on its architecture. The distinguishing characteristic and novelty of HIGGINS lies in its blending of two engines: an <b>automated</b> <b>Information</b> <b>Extraction</b> (IE) engine, aided by semantic resources and statistics, and a game-based Human Computing (HC) engine. We focus on KA from web pages and text sources and, in particular, on deriving relationships between entities. As a running application we utilize movie narratives, from which we wish to derive relationships among movie characters...|$|E
30|$|Modern <b>automated</b> <b>information</b> <b>extraction</b> (IE) systems {{usually are}} based on machine-learning models, which require large amount of {{manually}} annotated data to specify the model according to the task at hand. Unfortunately, particularly in the medical domain, experts have obligations with higher priorities, thus it is very expensive and cumbersome to annotate {{a large number of}} training examples. In order to alleviate this problem, {{there is a need for}} an approach where human annotators are facilitated to annotate faster than the traditional way, in order to produce required annotations in less time.|$|E
3000|$|Perhaps {{the most}} {{currently}} available technologies most similar to KEF are [...] "web 2.0 " [...] information stores. Examples include encyclopedic {{resources such as}} Wikipedia and Knol that rely on the [...] "wisdom of the crowds [25]" [...] to build and maintain a knowledge base of information. Such resources rarely utilize automated processes to extract semantic relations and add these as additional metadata that can aid in the discovery process. Like KEF, some of these systems use tags to provide an informal taxonomy, but the domain scale is typically very wide (in the case of Wikipedia, {{the goal is to}} provide an encyclopedia’s worth of knowledge). Project Halo [26] is a specific instance of an information store that aims to develop an application capable of answering novel questions and solving advanced problems in a broad range of scientific disciplines (e.g., biology, physics, and chemistry). The mechanism for inserting knowledge into the data store (i.e., using graduate students with domain knowledge) requires significant effort, however. The KEF approach is to share the load between <b>automated</b> <b>information</b> <b>extraction</b> tools and domain experts. While we acknowledge the limitations of <b>automated</b> <b>information</b> <b>extraction</b> technologies, we believe an approach that leverages automated means while encouraging users to make corrections and provide their own annotations provides significant semantic markup and encourages SME engagement.|$|E
40|$|Email {{is one of}} {{the most}} {{ubiquitous}} applications used regularly by millions of people worldwide. Professionals have to manage hundreds of emails on a daily basis, sometimes leading to overload and stress. Lots of emails are unanswered and sometimes remain unattended as the time pass by. Managing every single email takes a lot of effort especially when the size of email transaction log is very large. This work is focused on creating better ways of automatically organizing personal email messages. In this paper, a methodology for <b>automated</b> event <b>information</b> <b>extraction</b> from incoming email messages is proposed. The proposed methodology/algorithm and the software based on the above, has helped to improve the email management leading to reduction in the stress and timely response of emails. ...|$|R
40|$|Within the {{framework}} of Virtual Organisations (VO), a decision aid approach was developed to support the identification of collaborative corporate networks. This approach {{is based on an}} <b>automated</b> procedure of <b>information</b> <b>extraction</b> to identify key features of potential partners. The added value of this research is to operate in an "open universe " of potential partners, using the company internet sites as the main source of information on firms. The key features extracted concern the activity fields and the competencies of the firms...|$|R
40|$|Although OCR {{technology}} is now commonplace, character recognition errors {{are still a}} problem, in particular, in <b>automated</b> systems for <b>information</b> <b>extraction</b> from printed documents. This paper proposes a method for the automatic detection and correction of OCR errors in an <b>information</b> <b>extraction</b> system. Our algorithm uses domain-knowledge about possible misrecognition of characters to propose corrections; then it exploits knowledge about the type of the extracted information to perform syntactic and semantic checks in order to validate the proposed corrections. We assess our proposal on a real-world, highly challenging dataset composed of nearly 800 values extracted from approximately 100 commercial invoices and we obtained very good results...|$|R
40|$|In {{the scope}} of this thesis we are {{addressing}} two challenges, <b>automated</b> <b>information</b> <b>extraction</b> and its comprehensible visualization. The first part is concerned with identification of possibly interesting financial data from newspaper articles and their extraction. We are proposing a new approach to recognize named entities using DBpedia, we are identifying financial relations and extracting monetary data. Next, to communicate obtained information clearly, a tool for visualizing the financial cost of recognized entities is introduced. As we developed an advanced system, which helps people to reinforce their finance-related knowledge, we believe that the thesis?s objective was met. </p...|$|E
40|$|Abstract: Motivated by a {{continually}} {{increasing demand}} for applications {{that depend on}} machine comprehension of text-based content, researchers, in both academia and industry, have developed innovative solutions for <b>automated</b> <b>information</b> <b>extraction</b> from text. In this article, we focus on a subset of such tools – i. e., semantic taggers – that not only extract and disambiguate entities mentioned in the text, but also identify topics that unambiguously describe the text’s main themes. We offer insight into the process of semantic tagging, the capabilities and specificities of today’s semantic taggers, and also indicate some of the criteria to be considered when choosing a tagger to use...|$|E
40|$|Abstract. This paper {{describes}} {{the work on}} <b>automated</b> <b>Information</b> <b>Extraction</b> that accepts arbitrary text and extracts information from the text. A new approach to implement Information Extraction system is proposed in this paper. Firstly, the article will be decomposed according to paragraph, sentence and phrase. Every sentence will {{be compared with the}} knowledge node, and then append the information extracted to the knowledge model. Finally, the answers are generated to the questions about the input text. With the experimental corpus the accuracy rate of knowledge matching is 63. 5 %, and accuracy rate of question answering is 65. 0 % with the system knowledge model...|$|E
40|$|Abstract- “Email {{is one of}} {{the most}} {{ubiquitous}} applications used regularly by millions of people worldwide. Professionals have to manage hundreds of emails on a daily basis, sometimes leading to overload and stress. Lots of emails are unanswered and sometimes remain unattended as the time pass by. Managing every single email takes a lot of effort especially when the size of email transaction log is very large. This work is focused on creating better ways of automatically organizing personal email messages. In this paper, a methodology for <b>automated</b> event <b>information</b> <b>extraction</b> from incoming email messages is proposed. The proposed methodology/algorithm and the software based on the above, has helped to improve the email management leading to reduction in the stress and timely response of emails. ” Keywords-Information management; periodic access; mail organizer; email client; text mining; EIA algorithm I...|$|R
40|$|The {{access to}} {{huge amount of}} {{information}} sources on the internet has been limited to browsing and searching due to the heterogeneity {{and the lack of}} structure of the web information sources. This has resulted in the need for <b>automated</b> Web <b>Information</b> <b>Extraction</b> (IE) tools that analyze the Web pages and harvest useful information from noisy content for any further analysis. The goal of this survey is to provide a comprehensive review of the major Web IE tools that used for Web text and based on Document Object Model for representing the web pages. This paper compares them in three dimensions: (1) the source of content extraction, (2) the techniques used, and (3) the features of the tools, moreover the advantages and disadvantages for each tool. Based on this survey, we can decide which suitable Web IE tool will be integrated in our future work in Web Text Mining...|$|R
40|$|A {{process for}} the {{analysis}} and collection of information of software systems has been defined. It extracts relevant information of project source files from an online repository and stores that meta-information in a database for further processing. Then, according to the meta-information in the database, it downloads the source files and writes the feedback information back to the database as well. Now the data {{can be used as}} input for various analysis tools, in our case a tool called VizzAnalyzer, which reads the project source code and performs a series of software quality analyses. But actually, the process, which is mentioned above needs, a lot of manual work, makes the work inefficient and the analysis of large numbers of projects impossible. Thus, a series of thesis projects has been devised to automate the whole process. This thesis aims at <b>automating</b> the <b>information</b> <b>extraction</b> and source file download work, which will make the latter preparation of the analysis task much easier and more efficient...|$|R
40|$|Abstract: This paper {{presents}} {{methods for}} workout tracking and recording. <b>Automated</b> <b>information</b> <b>extraction</b> from recorded data helps identifying details such {{of the number}} of repetitions in a set. Proposed methods are implemented for mobile phones with accelerometer sensors. Peculiarities of fitness workout are identified to support proposed methods. Activity in an ordinary workout is divided into homogenous areas. Body acceleration data is collect from accelerometer sensor. Statistical properties are assessed for small intervals of data and repetition movement is identified. Further the number of repetition in a set is identified by statistical means. Results show that the workout data is successfully recorded and the number of repetitions is extracted correctly and accurately...|$|E
40|$|Foreign {{and defense}} policy is {{typically}} affected by complex issues that {{can benefit from}} advanced social science methodologies. In this paper we identify a salient set of issues and the contribution—current and potential—that the main analytical approaches and methods from computational social science might bring to bear on such issues. The issues consist of terrorism, WMD proliferation, state failure, and global issues ranging from human rights to global change. The main computational methods consist of <b>automated</b> <b>information</b> <b>extraction</b> systems, social network analysis, social GIS, complexity modeling, and social simulation models. After a review of issues and methods, we discuss some present and future trends {{in the application of}} computational analysis to national security policy. 1...|$|E
40|$|In this paper, {{we present}} a fast and {{scalable}} Spreading Activation Network (SAN) framework for improving weakly annotated data which is typically generated by an <b>automated</b> <b>information</b> <b>extraction</b> system from Web documents. Weakly annotated data suffers from two major problems; (i) might contain incorrect ontological role assignments, and (ii) might have many missing attributes. The SAN model described here is shown to substantially improve the Web data annotations for Web document collections. Our experimental evaluations with the TAP data set indicate that our model can improve the accuracy of role assignments up to 75 % even with 60 % and (35 %, 35 %) distortion, and can recover {{more than half of}} the 35 % missing attributes. ...|$|E
40|$|We {{present an}} {{overview}} of DNA microarray image re-quirements for <b>automated</b> processing and <b>information</b> <b>extraction</b> from spotted glass slides. Motivation of our review comes from the need to automate high-throughput microarray data processing due to exponentially growing amounts of microarray data. In order to automate mi-croarray image processing and draw biologically mean-ingful conclusions from experiments, one has to under-stand the processing flow, modeling assumptions, uncer-tainties involved, and the computational tradeoffs of mul-tiple approaches. We present a model of an ideal mi-croarray image and microarray deviations from the model in real experiments. In the summary, we discuss several open problems and the current challenges of high-throughput microarray image processing...|$|R
40|$|Abstract: The {{development}} of content based retrieval mechanisms {{is a very}} active research area. Present studies are mainly focused on <b>automating</b> the <b>information</b> <b>extraction</b> and indexing processes. Usually for the development and evaluation of such mechanisms {{there is always a}} need for a ground-truth database. In this paper we present a software tool named qp that is able to semi-automatically produce a collection of random 3 D vessels, with morphological characteristics similar to those found in ancient Greek pottery, a ceramic group exhibited worldwide with great impact to scholars as well as general public. A 3 D vessel collection has been produced by qp and {{can be used as a}} test bed dataset for the {{development of}} shape-based 3 D descriptors applicable to pottery. Additionally, qp can be considered as a 3 D vessel modelling software tool which can be used by people not related to computer graphics technology and particularly to 3 D modelling. Keywords: Hermite Cubic Splines/ 2 D curve approximation/ancient Greek pottery/ VRML/XML/MPEG- 7 /OpenGL/ 3 D extrusion/ 3 D revolve/point cloud triangulation 1...|$|R
40|$|Background: Good {{automatic}} <b>information</b> <b>extraction</b> tools offer {{hope for}} automatic processing of the exploding biomedical literature, and successful named entity recognition {{is a key}} component for such tools. Methods: We present a maximum-entropy based system incorporating a diverse set of features for identifying gene and protein names in biomedical abstracts. Results: This system was entered in the BioCreative comparative evaluation and achieved a precision of 0. 83 and recall of 0. 84 in the &quot;open &quot; evaluation and a precision of 0. 78 and recall of 0. 85 in the &quot;closed &quot; evaluation. Conclusion: Central contributions are rich use of features derived from the training data at multiple levels of granularity, a focus on correctly identifying entity boundaries, and the innovative use of several external knowledge sources including full MEDLINE abstracts and web searches. Background The explosion of information in the biomedical domain and particularly in genetics has highlighted the need for <b>automated</b> text <b>information</b> <b>extraction</b> techniques. MEDLINE, the primary research database serving the biomedical community, currently contains over 14 million abstracts, with 60, 000 new abstracts appearing each month. There is also an impressive number of molecular biological databases covering an array of information on genes, proteins, nucleotide and amino acid sequences, both generally (GenBank, Swiss-Prot) and for particular species (FlyBase, Mouse Genome Informatics, WormBase, Saccharomyces Genome Database), each containin...|$|R
