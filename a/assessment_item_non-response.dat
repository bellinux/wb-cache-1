0|996|Public
40|$|STUDY OBJECTIVES: These were as follows: {{to study}} incompleteness of data, herein called <b>item</b> <b>non-response,</b> {{generated}} by a self completion questionnaire; to identify the characteristics of item non-responders {{and the types of}} questions liable to high <b>item</b> <b>non-response</b> rates; and to discuss possible reasons for <b>item</b> <b>non-response.</b> DESIGN: <b>Item</b> <b>non-response</b> patterns in 12, 307 responders (62 %) to a representative postal survey based on a stratified sample drawn from family health services authorities' (FHSA) registers were investigated. MAIN OUTCOME MEASURES: Data were analysed for <b>item</b> <b>non-response</b> in three groups depending on when the questionnaire was returned (wave analysis). The overall completion rate of the questionnaire was examined and the natural logarithm of the proportion of completed questions was used as an outcome variable in multiple regression analysis. Item nonresponse to key questions and questions of different types was examined. RESULTS: Wave analysis: the overall completion rate of the questionnaire was 86 % in questionnaires returned before the first reminder and 83 %- 84 % in those sent back after subsequent reminders. Overall pattern of item non-response; respondents failed to complete a mean of 15 % and a median of 10 % of the questionnaire. All questions in the questionnaire had some <b>item</b> <b>non-response,</b> ranging from 1 % to 85 %. Completion rates were associated with gender, age, indicators of lower socioeconomic status, and general health status. Individual questions: particular types of questions were liable to have higher <b>item</b> <b>non-response,</b> for example, linked binary questions. CONCLUSIONS: <b>Item</b> <b>non-response</b> in population postal surveys is likely to present problems in the interpretation of data by introducing bias additional to that of total <b>non-response.</b> <b>Item</b> <b>non-response</b> does not increase greatly with later returns, suggesting that the quality of data across responses generated by two reminders is similar. There are obstacles to reducing <b>item</b> <b>non-response,</b> such as respondent error or socioeconomic and health characteristics of the general population, that cannot be totally overcome. However, the evidence that individuals tend to complete only options within questions that apply to them and their positive behaviour is useful information for those designing questionnaires and interpreting survey data...|$|R
40|$|After {{reviewing}} {{the literature on}} <b>item</b> <b>non-response</b> we focus on three issues: First, is there significant heterogeneity in <b>item</b> <b>non-response</b> across financial questions and in the association of covariates with <b>item</b> <b>non-response</b> across outcomes? Second, can the informational value of surveys be improved by matching interviewers and respondents based on their characteristics? Third, how does offering a "don't know" answer option affect respondent behavior? The questions are answered based on detailed survey and interviewer data from the German Socioeconomic Panel, considering a broad set of income and wealth outcomes...|$|R
50|$|Firstly, <b>item</b> <b>non-{{response}}</b> {{needs to}} be addressed. Non-response can either be 'unit'- where a person gave no response {{for any of the}} n items, or 'item'- i.e., individual question. Unit non-response is generally dealt with exclusion. <b>Item</b> <b>non-response</b> should be handled by imputation- the method used can vary between test and questionnaire items. Literature about the most appropriate method to use and when can be found here.|$|R
40|$|Sampling {{surveys are}} more than often {{affected}} by non-response. More so, the process of data collection is practically never free of non-response. Because of the damage that it has on survey estimates, taking measures of prevention and the treating for non-response it is clearly necessary. Considering the fact that non-response is a two-level phenomenon, i. e. <b>item</b> and unit <b>non-response,</b> the measures differ on {{the two types of}} non-response. The paper presents an overview of the methods used for prevention and control for <b>item</b> <b>non-response</b> and unit non-response. Key words: survey <b>non-response,</b> <b>item</b> <b>non-response,</b> unit non-response, prevention methods...|$|R
40|$|Abstract Background Health surveys provide {{important}} {{information on the}} burden and secular trends of risk factors and disease. Several factors including survey and <b>item</b> <b>non-response</b> can affect data quality. There are few reports on efficiency, validity {{and the impact of}} <b>item</b> <b>non-response,</b> from developing countries. This report examines factors associated with <b>item</b> <b>non-response</b> and study efficiency in a national health survey in a developing Caribbean island. Methods A national sample of participants aged 15 – 74 years was selected in a multi-stage sampling design accounting for 4 health regions and 14 parishes using enumeration districts as primary sampling units. Means and proportions of the variables of interest were compared between various categories. Non-response was defined as failure to provide an analyzable response. Linear and logistic regression models accounting for sample design and post-stratification weighting were used to identify independent correlates of recruitment efficiency and <b>item</b> <b>non-response.</b> Results We recruited 2012 15 – 74 year-olds (66. 2 % females) at a response rate of 87. 6 % with significant variation between regions (80. 9 % to 97. 6 %; p Conclusion Informative health surveys are possible in developing countries. While survey response rates may be satisfactory, <b>item</b> <b>non-response</b> was high in respect of income and sexual practice. In contrast to developed countries, non-response to questions on income is higher and has different correlates. These findings can inform future surveys. </p...|$|R
30|$|No {{significant}} differences in <b>item</b> <b>non-response</b> rates between the test and retest phase were found {{for any of the}} 54 items.|$|R
40|$|Background: The {{development}} of consumer experience measures (the so-called consumer quality index or CQ-index) in The Netherlands {{has not taken}} into account the data quality, instrument reliability and validity among ethnic minorities. To estimate the quality of data for these groups, ethnic differences in <b>item</b> <b>non-response</b> were assessed. Possible explanations for any ethnic differences were explored. Method: To measure the impact of ethnic background on possible differences in <b>item</b> <b>non-response,</b> multilevel linear regression models were fit to the data of 23058 respondents who filled out the hospital version of CQ-index. The association of ethnicity with overall percentage of <b>non-response</b> on core <b>items</b> was explored, while accounting for the possible explanatory effects of education and Dutch language reading proficiency. Results: There were ethnic differences in <b>item</b> <b>non-response,</b> with <b>item</b> <b>non-response</b> being higher for the Turkish, Moroccan and other non-western ethnic minorities. This effect could only be explained by their education and Dutch language reading proficiency for Moroccan respondents. Discussion: Higher <b>item</b> <b>non-response</b> for ethnic minorities on core aspects of the hospital CQ index results in lower data quality for these respondents. This is probably explained by unmeasured factors, although measuring respondents’ Dutch language proficiency through a written question in Dutch is problematic. Further research into these and related issues should include thorough cognitive testing and extensive response analysis to ensure the reliability, validity and usability of the CQ-index among multi-ethnic groups in the Netherlands. (aut. ref. ...|$|R
40|$|<b>Item</b> <b>non-response</b> is a {{challenge}} faced by virtually all surveys. <b>Item</b> <b>non-response</b> occurs when a respondent skips over a question, refuses to answer a question, or indicates {{that they do not}} know the answer to a question. Hot deck imputation is one of the primary <b>item</b> <b>non-response</b> imputation tools used by survey statisticians. Recently, new competitor in the field of Weighted Sequential Hotdeck Imputation has arrived: PROC HOTDECK of SUDAAN®, version 10. We compared the results of imputation using the new procedure with the results of the Hotdeck SAS® Macro with respect to: a) how close the post-imputation weighted distributions and standard errors of the estimates are to those of the item respondent data; b) whether there is a difference in the number of times donors contribute to the imputation...|$|R
40|$|This study {{investigates the}} {{mechanisms}} determining <b>item</b> <b>non-response</b> focusing on three issues: First, is there significant heterogeneity in <b>item</b> <b>non-response</b> across financial questions {{and in the}} association of covariates with <b>item</b> <b>non-response</b> across outcomes? Second, can the informational value of surveys be improved by matching interviewers and respondents based on their characteristics? Third, how does offering a "don't know " answer option affect respondent behavior? The questions are answered based on detailed survey and interviewer data from the German Socioeconomic Panel using a broad set of income and wealth outcomes. We find considerable heterogeneity in <b>non-response</b> across financial <b>items,</b> little explanatory power of interviewer-respondent matches and strong evidence that "don't know " answers result from mechanisms that differ from those yielding valid responses and outright refusals to respond...|$|R
40|$|The paper {{systematically}} reviews {{existing literature}} {{on the relationship between}} the level of effort to recruit a sampled person and the measurement quality of survey data. Hypotheses proposed for this relationship are reviewed. Empirical findings for the relationship between level of effort as measured by paradata (the number of follow-up attempts, refusal conversion and time in the field) and question-specific <b>item</b> <b>non-response</b> rates, aggregate measures of <b>item</b> <b>non-response</b> rates, response accuracy and various measurement errors on attitudinal questions are examined through a qualitative review...|$|R
40|$|This paper reports two {{experiments}} {{in which the}} prominence of university sponsorship on Web surveys was systematically manipulated, {{and its effects on}} dropout and <b>item</b> <b>non-response</b> were observed. In Study 1, 498 participants were randomised to online surveys with either high or low university sponsorship. Overall, 13. 9 percent of participants commenced, but did not complete the surveys, and {{there was no difference between}} the proportions of participants dropping out of each condition. However, counter to our predictions, participants in the high sponsorship condition displayed significantly higher <b>item</b> <b>non-response.</b> In Study 2 (N = 159), which addressed a rival explanation for the findings in Study 1, the overall dropout rate was 23. 9 percent and sponsorship prominence had no effect on either outcome variable. Overall, these findings suggest that hosting information pages on university Web sites, placing university logos on survey pages, and including the name of the university in survey URLs do not reliably impact on dropout or <b>item</b> <b>non-response.</b> Although it may seem disappointing that enhancing sponsor visibility is not sufficient to reduce dropout and <b>item</b> <b>non-response,</b> researchers without ready access to university Web servers or branding will appreciate these findings, as they indicate that minimally visible sponsorship does not necessarily compromise data quality...|$|R
40|$|Missing {{survey data}} occur because of unit and <b>item</b> <b>non-response.</b> This is {{practically}} {{independent of the}} method of data collection. As {{a result of the}} bias that non-response sometimes introduces in survey estimates, identifying factors that promote it, and taking measures of prevention and correction methods are clearly necessary. The standard method to compensate for unit non-response is by weighting adjustment, while <b>item</b> <b>non-responses</b> are handled by some form of imputation. This paper reviews factors that give rise to nonresponse and the corresponding methods used for its prevention and control. It also discusses their properties...|$|R
40|$|Missing {{data are}} an {{unwanted}} reality in most {{forms of social}} science research, including institutional research. Like troublesome “guests ” at a family gathering, missing data are a nuisance; they impose themselves on research designs and undermine the methodological assumptions of an analysis plan. The primary problems associated with missing data are the threats that they pose to a study’s internal validity (primarily issues of statistical power) and external validity (being able to generalize results to a target population). Even when investigators employ appropriate strategies for coping with missing data, different approaches may lead to substantially different conclusions (Cohen, Cohen, West, and Aiken, 2003). In this chapter we focus on one type of missing data – <b>item</b> <b>non-response.</b> <b>Item</b> <b>non-response,</b> as opposed to participant or unit non-response, occurs when only partial data are available for survey participants or subjects. <b>Item</b> <b>non-response</b> can occur for a multitude of reasons. Mistakes {{can be made in}} coding or data entry, respondents may fail or be unable t...|$|R
3000|$|... 5 Weight {{and height}} are {{originally}} reported in pounds and inches, respectively, in the PSID. The pounds/inches BMI formula is: Weight (in pounds) × 704.5 divided by Height (in inches) × Height (in inches). Oreffice and Quintana-Domeque (2010) shown that non-response to body size questions {{appears to be}} very small in the PSID data. Specifically, <b>item</b> <b>non-response</b> for husband’s height is below 1.4 % in each year, for wife’s height is below 1.4 % in each year, and for husband’s weight is below 2.2 % in each year. Regarding wife’s weight, <b>item</b> <b>non-response</b> is below 5.5 % in each year.|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references: p. 78 - 79. Issued also on microfiche from Lange Micrographics. Information obtained through the collection and analysis of origin-destination travel surveys is used to develop trip rate information for input to travel demand models. Three major sources of systematic error can exist relative to typical sample survey data sets: survey <b>non-response,</b> <b>item</b> <b>non-response,</b> and inaccurate reporting. This thesis presents the results of a study examining the effect of <b>item</b> <b>non-response</b> on trip rates for a household survey conducted by the Houston-Galveston Area Council from October 1994 to June 1995. The variables of age cohort, household size, household income, and auto availability were used to stratify the trip rates. These four variables were used individually and in combination to examine internal vehicular person trip rates for home based work, home based other, non-home based, and all three trip purposes combined and for internal auto driver trip rates for all three trip purposes combined. The results were evaluated to determine the bias introduced into the trip rates by <b>item</b> <b>non-response</b> with regard {{to the quality of the}} collected data. No attempt is made to quantify the bias for the purpose of establishing or recommending correction factors or adjusted trip rates for the survey data. Statistical evidence in the form of significantly different trip rates in the removed questionable households indicate that <b>item</b> <b>non-response</b> bias may have resulted in a change in mean trip rates. Empirical evidence was also found to indicate that bias due to trip <b>item</b> <b>non-response</b> caused a change in total trips and total vehicle miles traveled were observed after the removal of the questionable samples. A reduction in the variation of the sample data within stratification cells also resulted from the removal of the questionable samples...|$|R
25|$|A {{particular}} {{problem is that}} of non-response. Two major types of non-response exist: unit nonresponse (referring to lack of completion of {{any part of the}} survey) and <b>item</b> <b>non-response</b> (submission or participation in survey but failing to complete one or more components/questions of the survey).|$|R
30|$|The {{information}} elicited in {{the survey}} comprises personal details, training, job experience, wage and wage expectations of the 483 respondents in German- and French-speaking Switzerland. About {{one half of the}} 483 teachers (230) were pursuing a degree qualifying them to be full-time vocational teachers. The other half of the respondents (253) was working toward a certificate qualifying them to be sideline vocational teachers. Despite the 100 % response rate, some data were missing on account of <b>item</b> <b>non-response.</b> We excluded 93 observations (19 %) from the analysis because of missing wage information or other important data, leaving a final dataset of 390 vocational teachers. <b>Item</b> <b>non-response</b> analyses show that the exclusion of the 93 observations should not influence or bias our resultse.|$|R
3000|$|As noted earlier, {{the birth}} {{intention}} variables {{have a limited}} number of missing values. After imputation, the number of <b>item</b> <b>non-responses</b> for the first-child intention fell from 33 to just 1, and the one for the second child intention fell from 72 to none. 6 [...]...|$|R
40|$|Missing data due to <b>item</b> <b>non-response</b> is a {{pervasive}} problem {{in social and}} economic data. Often economists simply throw out observations with missing data. Estimates based on the remaining samples can be inefficient or even biased. A large and growing literature in statistics show to impute values fo...|$|R
30|$|Income is {{self-reported}} {{and measured}} in the national currency, RON. The <b>item</b> <b>non-response</b> for this variable is high for both samples: for the childless individuals group, the <b>item</b> <b>non-response</b> is 26 % and for the one-child parents group it is 16 %. Moreover, these missing cases can hardly be assumed to be at random because respondents with either a high or very low income {{are less likely to}} report their incomes (Soley-Bori 2013), suggesting that the probability of the missing values depends on some unobserved variables. Thus, a previous treatment of the data to reduce this lack of information has been performed, applying the already mentioned ‘hot-deck’ imputation method (see ‘Methods and model specification’ section). After correction, the percentage of non-responses for income fell to around 10 % for both groups.|$|R
40|$|An {{important}} {{issue for the}} stated choice method {{is the effect of}} the number of choice sets on responses. Based on a study of this issue in a mailed survey, results indicate that the number of choice sets does not affect survey response rates or <b>item</b> <b>non-response</b> rates. Research Methods/ Statistical Methods,...|$|R
40|$|A {{three-way}} treatment {{design is}} used to compare contingent valuation response formats. Respondents are asked to value an endangered species (the red-cockaded woodpecker) and the restoration of its habitat following a natural disaster. For three question formats (open-ended, payment card, and double-bounded dichotomous choice), differences in survey response rates, <b>item</b> <b>non-response</b> rates, and protest bids are examined. Bootstrap techniques are used to compare means across formats and to explore differences in willingness to pay (WTP) distribution functions. Convergent validity is found in a comparison of mean WTP values, although some differences are apparent in the cumulative distribution functions. Differences across formats are also identified in <b>item</b> <b>non-response</b> rates and proportion of protest bids. Overall, the payment card format exhibits desirable properties relative {{to the other two}} formats. Copyright Kluwer Academic Publishers 1999 contingent valuation, endangered species, question format, red-cockaded woodpecker,...|$|R
40|$|Statistical {{surveys are}} always {{affected}} by <b>non-response.</b> There is <b>item</b> <b>non-response,</b> {{in which only}} the answers to some questions are missing, {{and there is also}} unit non-response, in which the answers to all questions are missing. For many years now Statistics Netherlands has been confronted with severe non-response problems. This is mainly unit non-response. Consequently, research has concentrated o...|$|R
40|$|Non-response {{adjustment}} in the Israeli Social Survey (ISS) {{is based on}} the MAR assumption. Association of non-response with key socio-economic characteristics (individual's economic status and degree of religiosity) which do not correlate strongly with standard survey design and calibration variables may corrupt the MAR assumption validity. We analyze survey and <b>item</b> <b>non-response</b> in ISS by estimating non-parametric sharp bounds for conditional means of key ISS variables. Statistical tests for checking validity of MCAR and MAR assumptions are proposed, where the test statistics are based on the width of the interval between the estimated bounds. We find significant departures from MAR assumption in the ISS data. Non-response propensity varies significantly between population groups assumed to be homogenous according to the survey design. We propose to utilize information about income and religiosity, available on individual or neighborhood level, for improving the ISS design. Key Words: survey <b>non-response,</b> <b>item</b> <b>non-response,</b> MAR assumption, sharp bounds...|$|R
40|$|Sample {{selection}} bias is a chronic problem in longitudinal studies {{that is particularly}} problematic for studies concerning the relationship between health and socio-economic status. This paper adopts two alternate methods for handling sample {{selection bias}} attributable to survey attrition and <b>item</b> <b>non-response.</b> Both methods are applied to examine the magnitude of bias in the effects of childhood cognition and behavior on the adult socio-economic gradient in health. A method for sample selection correction with multiple imputation for <b>item</b> <b>non-response</b> is implemented to account for different sources of sample selection bias over time. Estimates of a life course model of health and socioeconomic attainment demonstrate that sample selection bias inflates estimates of socioeconomic gradients. The proposed correction for sample selection bias also suggests {{that the effects of}} early child non-cognitive skills rather than cognitive skills may {{play an important role in}} the early life origins of adult socioeconomic gradients...|$|R
40|$|Household {{surveys are}} often plagued by <b>item</b> <b>non-response</b> on {{economic}} {{variables of interest}} like income, savings or the amount of wealth. Manski (1989, 1994, 1995) shows how, {{in the presence of}} such non-response, bounds on conditional quantiles of the variable of interest can be derived, allowing for any type of non-random response behavior. Including follow up categorical questions in the form of unfolding brackets for initial item non-respondents, is an effective way to reduce complete <b>item</b> <b>non-response.</b> Recent evidence, however, suggests that such design is vulnerable to a psychometric bias known as the anchoring effect. In this paper, we extend the approach by Manski to take account of the information provided by the bracket respondents. We derive bounds which do and do not allow for the anchoring effect. These bounds are applied to earnings in the 1996 wave of the Health and Retirement Survey (HRS). The results show that the categorical questions can be useful to increase precision of the bounds, even if anchoring is allowed for. ...|$|R
30|$|Even in {{the case}} of the {{dependent}} variables listwise deletion is not a proper procedure (Rubin 1986). For this reason, we imputed the <b>item</b> <b>non-responses</b> using the NN procedure described in the footnote n. 5. Following one of the reviewers’ suggestion, we ran the same regression models by deleting the missing values of the dependent variables: neither the significance nor the direction of the coefficients changed.|$|R
40|$|Efficient {{actions to}} fight elder abuse are highly {{dependent}} on reliable dimensions of the phenomenon. Accurate measures are nevertheless difficult to achieve owing to {{the sensitivity of the}} topic. Different research endeavours indicate varying prevalence rates, which are explained by different research designs and definitions used, but little is known about measurement errors such as <b>item</b> <b>non-responses</b> and how outcomes are affected by modes of administration...|$|R
30|$|In {{the context}} of PISA and TALIS, we can {{consider}} two types of missing data; unit and <b>item</b> <b>non‐response.</b> However, when considering the fusion of the two data sets, a very large amount of unit missing data obtains because the surveys contain different items and units of analysis. What is required {{to move forward with}} data fusion is a general theoretical framework for the problem of missing data.|$|R
40|$|Monetary {{incentives}} are one approach for increasing response rates in contingent valuation surveys. We {{present the results}} of a case study desgined to assess the effect of incentives on response rates and respondent behavior. We compare response rates and quality of answers for five incentive levels. Including incentives increased the response rate, decreased <b>item</b> <b>non-response</b> rates, but had not effect on stated willingness-to-pay. mail surveys, contingent valuation, monetary incentives...|$|R
40|$|Abstract. A {{three-way}} treatment {{design is}} used to compare contingent valuation response formats. Respondents are asked to value an endangered species (the red-cockaded woodpecker) and the restoration of its habitat following a natural disaster. For three question formats (open-ended, payment card, and double-bounded dichotomous choice), differences in survey response rates, <b>item</b> <b>non-response</b> rates, and protest bids are examined. Bootstrap techniques are used to compare means across formats and to explore differences in willingness to pay (WTP) distribution functions. Convergent validity is found in a comparison of mean WTP values, although some differences are apparent in the cumulative distribution functions. Differences across formats are also identified in <b>item</b> <b>non-response</b> rates and proportion of protest bids. Overall, the payment card format exhibits desirable properties relative {{to the other two}} formats. Key words: contingent valuation, endangered species, question format, red-cockaded woodpecker JEL classification: 426 With a growing interest in ecosystem protection and restoration, economic research has continued to seek improved methods for valuing environmental amenities. One non-market valuation tool, the contingent valuation method (CVM), relies o...|$|R
40|$|Background: With {{the growth}} in {{international}} comparative {{research in the field}} of health economics, new tools to collect comparative data from disparate countries are required to measure and compare the costs of illnesses. Methods: A multistep approach was used to translate an English questionnaire to measure the costs of food allergies across 12 European countries, using WHO guidelines as a minimum standard. Greek, Polish and Spanish translations are presented as case studies. Survey response rates and <b>item</b> <b>non-response</b> rates were analysed to evaluate the process. Results: Questionnaires were adapted to reflect different health professions and health settings in each country. Spain achieved the highest response rate (85 %) and lowest <b>item</b> <b>non-response</b> rate (85 %; 1. 52 %) compared to Poland (68 %; 4. 97 %) and Greece (38 %; 10. 64 %). Spain implemented a more complex translation protocol than Poland and Greece. Conclusion: More complex translation protocols yield better results, but this paper concludes that good channels of communication between originators of questionnaires and translators is most effective way of ensuring good quality outcomes...|$|R
40|$|The Household Finance and Consumption Survey (HFCS) {{provides}} information about household wealth (real and financial assets {{as well as}} liabilities) from 15 Euro-countries after the financial crisis of 2007 / 8. The survey will be the central dataset in this topic in the future. However, several aspects point to potential methodological constraints regarding crosscountry comparability. Therefore {{the aim of this}} paper is to get a better insight in the data quality of this important data source. We will first present a synopsis of cross-country differences, which is the core of the paper. We will compare the sampling processes, the interview modes, the oversampling techniques, the unit and <b>item</b> <b>non-response</b> rates and how itis dealt with them via weighing and imputation as well as further points which might restrict country comparability. In addition we give a first insight in the selectivity of item nonresponse in a cross-national setting. We make use of logit models as well as apply a decomposition method suggested by Fairlie (1999, 2005) to identify differences in characteristics as well as structural (cultural) differences in the <b>item</b> <b>non-response</b> missing process...|$|R
40|$|Abstract Background There is a {{need for}} local level health data for local {{government}} and health bodies, for health surveillance and planning and monitoring of policies and interventions. The Health Survey for England (HSE) is a nationally-representative survey of the English population living in private households, but sub-national analyses can be performed only at a regional level because of sample size. A boost of the HSE was commissioned to address the need for local level data in London but a different mode of data collection was used to maximise participant numbers for a given cost. This study examines the effects on survey and item response of the different survey modes. Methods Household and individual level data are collected in HSE primarily through interviews plus individual measures through a nurse visit. For the London Boost, brief household level data were collected through interviews and individual level data through a longer self-completion questionnaire left by the interviewer and collected later. Sampling and recruitment methods were identical, and both surveys were conducted by the same organisation. There was no nurse visit in the London Boost. Data were analysed to assess the effects of differential response rates, <b>item</b> <b>non-response,</b> and characteristics of respondents. Results Household response rates were higher in the 'Boost' (61 %) than 'Core' (HSE participants in London) sample (58 %), but the individual response rate was considerably higher in the Core (85 %) than Boost (65 %). There were few differences in participant characteristics between the Core and Boost samples, with the exception of ethnicity and educational qualifications. <b>Item</b> <b>non-response</b> was similar for both samples, except for educational level. Differences in ethnicity were corrected with non-response weights, but differences in educational qualifications persisted after non-response weights were applied. When <b>item</b> <b>non-response</b> was added to those reporting no qualification, participants' educational levels were similar in the two samples. Conclusion Although household response rates were similar, individual response rates were lower using the London Boost method. This may be due to features of London that are particularly associated with lower response rates for the self-completion element of the Boost method, such as the multi-lingual population. Nevertheless, statistical adjustments can overcome most of the demographic differences for analysis. Care must be taken when designing self-completion questionnaires to minimise <b>item</b> <b>non-response.</b> </p...|$|R
40|$|Background. Greater {{attention}} {{is being paid}} to data quality in surveys of older age groups. In this paper patterns of <b>item</b> <b>non-response</b> are examined in a health risk appraisal instrument administered to an elderly cohort participating in a randomized preventive intervention study. Methods. The association between demographic and health status factors {{with the number of}} non-responses out of 174 items was examined at baseline and at the 12 -month follow-up on 1791 subjects. Results. Overall, non-response decreased from baseline to 12 months. The pattern was consistent across the seven major components of the questionnaire. Univariate analyses at baseline found that <b>item</b> <b>non-response</b> increased significantly (P < 0. 05) with age, being female, being unmarried, lower annual income, less education, and poorer personal health ranking. Polychotomous logistic regression identified age and personal health ranking as statistically significant at both baseline and 12 -month follow-up assessments after controlling for all other factors. In addition, education was significant at baseline. Conclusions. These results help to identify subgroups of elderly participants who contribute to non-random patterns of missing data. Sample surveys and intervention studies among elderly populations are increasing. Although it is recognize...|$|R
40|$|Abstract Background Surveys {{of doctors}} are an {{important}} data collection method in health services research. Ways to improve response rates, minimise survey response bias and <b>item</b> <b>non-response,</b> within a given budget, have not previously been addressed in the same study. The aim {{of this paper is}} to compare the effects and costs of three different modes of survey administration in a national survey of doctors. Methods A stratified random sample of 4. 9 % (2, 702 / 54, 160) of doctors undertaking clinical practice was drawn from a national directory of all doctors in Australia. Stratification was by four doctor types: general practitioners, specialists, specialists-in-training, and hospital non-specialists, and by six rural/remote categories. A three-arm parallel trial design with equal randomisation across arms was used. Doctors were randomly allocated to: online questionnaire (902); simultaneous mixed mode (a paper questionnaire and login details sent together) (900); or, sequential mixed mode (online followed by a paper questionnaire with the reminder) (900). Analysis was by intention to treat, as within each primary mode, doctors could choose either paper or online. Primary outcome measures were response rate, survey response bias, <b>item</b> <b>non-response,</b> and cost. Results The online mode had a response rate 12. 95 %, followed by the simultaneous mixed mode with 19. 7 %, and the sequential mixed mode with 20. 7 %. After adjusting for observed differences between the groups, the online mode had a 7 percentage point lower response rate compared to the simultaneous mixed mode, and a 7. 7 percentage point lower response rate compared to sequential mixed mode. The difference in response rate between the sequential and simultaneous modes was not statistically significant. Both mixed modes showed evidence of response bias, whilst the characteristics of online respondents were similar to the population. However, the online mode had a higher rate of <b>item</b> <b>non-response</b> compared to both mixed modes. The total cost of the online survey was 38 % lower than simultaneous mixed mode and 22 % lower than sequential mixed mode. The cost of the sequential mixed mode was 14 % lower than simultaneous mixed mode. Compared to the online mode, the sequential mixed mode was the most cost-effective, although exhibiting some evidence of response bias. Conclusions Decisions on which survey mode to use depend on response rates, response bias, <b>item</b> <b>non-response</b> and costs. The sequential mixed mode appears to be the most cost-effective mode of survey administration for surveys of the population of doctors, if one is prepared to accept a degree of response bias. Online surveys are not yet suitable to be used exclusively for surveys of the doctor population. </p...|$|R
