7|5|Public
5000|$|<b>Adaptive</b> <b>DPCM</b> (ADPCM) is {{a variant}} of DPCM that varies {{the size of the}} {{quantization}} step, to allow further reduction of the required bandwidth for a given signal-to-noise ratio.|$|E
40|$|A {{number of}} {{adaptive}} data compression techniques are considered {{for reducing the}} bandwidth of multispectral data. They include adaptive transform coding, <b>adaptive</b> <b>DPCM,</b> adaptive cluster coding, and a hybrid method. The techniques are simulated and their performance in compressing the bandwidth of Landsat multispectral images is evaluated and compared using signal-to-noise ratio and classification consistency as fidelity criteria...|$|E
40|$|Image {{and audio}} data are {{examples}} of domains in which DPCM (Differential Pulse Coded Modulation) is an effective method for removing much of the linear correlation between data samples. However, most simple DPCM schemes cannot simultaneously cater for smooth signals, noisy signals, and discontinuities such as edges in images. To overcome this, many <b>adaptive</b> <b>DPCM</b> schemes have been proposed, including median predictors, gradient-based switching predictors and gradient-based blending predictors. In this paper we generalize the idea of blending predictors, and describe a powerful technique for creating locally adaptive compound predictors. The result is a prediction scheme which works well over a range of data types froms smooth to noisy, and requires very few tunable parameters. We apply this scheme to greyscale image data, colour image data, and audio data, and compare results {{with some of the}} current best <b>adaptive</b> <b>DPCM</b> predictors. 1 Introduction Image and audio data {{are examples of}} dom [...] ...|$|E
40|$|In the Differential Pulse-code Modulation (DPCM) image coding, the {{intensity}} of a pixel is predicted as a linear combination {{of a set of}} surrounding pixels and the prediction error is encoded. In this paper, we propose the <b>adaptive</b> residual <b>DPCM</b> (ARDPCM) for intra lossless coding. In the ARDPCM, intra residual samples are predicted using <b>adaptive</b> mode-dependent <b>DPCM</b> weights. The weights are estimated by minimizing the Mean Squared Error (MSE) of coded data and they are synchronized at the encoder and the decoder. The proposed method is implemented on the High Efficiency Video Coding (HEVC) reference software. Experimental results show that the ARDPCM significantly outperforms HEVC lossless coding and HEVC with the DPCM. The proposed method is also computationally efficient...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy at Loughborough University. Predictive coding, which {{is often associated with}} differential pulse code modulation (DPCM), can offer an attractive means of data compression of television signals. This thesis describes investigations of various <b>adaptive</b> and non-adaptive <b>DPCM</b> schemes {{in order to reduce the}} high bit-rate required for the transmission of broadcast quality colour television signals. [Continues. ...|$|R
40|$|Nonuniform quantizers {{for just}} not visible {{reconstruction}} errors in an <b>adaptive</b> intra-/interframe <b>DPCM</b> scheme for componentcoded color television signals are presented, both for conventional DPCM and for noise-shaping DPCM. Noise feedback filters that minimize {{the visibility of}} reconstruction errors by spectral shaping are designed for Y, R-Y, and B-Y. A dosed-form description of the "masking function" is derived {{which leads to the}} one-parameter "b quantizer" characteristic. Subjective tests that were carried out to determine visibility thresholds for reconstruction errors for conventional DPCM and for noise shaping DPCM show significant gains b noise shaping. For a transmission rate of around 30 Mbits/s, reconstruction errors are almost always below the visibility threshold if variable length encoding of the prediction error is combined with noise shaping within a 3 : 1 : 1 system...|$|R
40|$|The general {{problem of}} image data {{compression}} is discussed briefly with {{attention given to}} the use of Karhunen-Loeve transforms, suboptimal systems, and block quantization. A survey is then conducted encompassing the four categories of adaptive systems: (1) adaptive transform coding (adaptive sampling, adaptive quantization, etc.), (2) adaptive predictive coding (adaptive delta modulation, <b>adaptive</b> <b>DPCM</b> encoding, etc.), (3) adaptive cluster coding (blob algorithms and the multispectral cluster coding technique), and (4) adaptive entropy coding...|$|E
40|$|The {{discrete}} cosine transform (DCT) {{has been}} shown as an optimum encoder for sharp edges in an image (Andrew and Ogunbona, 1997). A conventional lossless coder employing {{differential pulse code modulation}} (DPCM) suffers from significant deficiencies in regions of discontinuity, because the simple model cannot capture the edge information. This problem can be partially solved by partitioning the image into blocks that are supposedly statistically stationary. A hybrid lossless <b>adaptive</b> <b>DPCM</b> (ADPCM) /DCT coder is presented, in which the edge blocks are encoded with DCT, and ADPCM is used for the non-edge blocks. The proposed scheme divides each input image into small blocks and classifies them, using shape vector quantisation (VQ), as either edge or smooth. The edge blocks are further vector quantised, and the side information of the coefficient matrix is saved through the shape-VQ index. Evaluation of the compression performance of the proposed method reveals its superiority over other lossless coders...|$|E
40|$|Lossless image {{compression}} {{continues to be}} the focus of the medical picture archiving system designers because of the possibility of reducing the bandwidth required to transmit medical images. The lossless Differential Pulse Code Modulation (DPCM) and Hierarchical Interpolation (HINT) have been suggested as solutions to this problem. However, there are limitations due to the inability of these schemes to adapt to local image statistics. Efforts to alleviate this problem can be seen in various adaptive schemes found in the literature. This paper introduces a new <b>adaptive</b> <b>DPCM</b> (ADPCM) scheme based on the shape of the region of support (ROS) of the predictor. The shape information of the local region is obtained through a universal Vector Quantization (VQ) scheme. The proposed lossless encoding scheme switches predictor type depending on the local shape. Simulation results show that improvements of about 0. 4 bits/ pixel over basic DPCM and 0. 2 bits/pixel over HINT can be obtained. Comparison with lossless JPEG indicates that the proposed scheme can cope more easily with the changes in local image statistics. The computation required is moderate, since a universal VQ is used in encoding the shape information...|$|E
40|$|The {{performance}} of adaptive image compression techniques and {{the applicability of}} a variety of techniques to the various steps in the data dissemination process are examined in depth. It is concluded that the bandwidth of imagery generated by scanners can be reduced without introducing significant degradation such that the data can be transmitted over an S-band channel. This corresponds to a compression ratio equivalent to 1. 84 bits per pixel. It is also shown that this can be achieved using at least two fairly simple techniques with weight-power requirements well within the constraints of the LANDSAT-D satellite. These are the <b>adaptive</b> 2 D <b>DPCM</b> and <b>adaptive</b> hybrid techniques...|$|R
40|$|In this dissertation, we {{investigate}} several embedded zerotree wavelet (EZW) coding techniques for designing image and video coders. Four topics addressed include: (1) EZW coding using non-uniform quantization, (2) Adaptive EZW coding using rate-distortion criterion, (3) Modified “set partitioning in hierarchical trees” (SPIHT) and (4) Video coding using segmentation, regional wavelet packet, and adaptive EZW. The first three topics are applications for image compression, {{and the last}} topic is an application for video compression. ^ The embedded zerotree wavelet image compression algorithm developed by Shapiro {{is the most popular}} wavelet-based image coder to date. First, we aim to modify the quantization characteristics of EZW by the following two approaches: (1) Non-uniform quantizer: we design two non-uniform quantization schemes, nonuniform EZW and Lloyd-Max EZW. (2) Rate-distortion criterion: we develop adaptive EZW, where we introduce adaptive step sizes for each subband. The best set of step sizes is found by using Lagrangian optimization, where two coding environments, independent and dependent, are considered. ^ The proposed image coder retains all the good features of EZW, namely, embedded coding, progressive transmission, order of importance. Experimental results show that the proposed image coders perform significantly better than the standard EZW algorithm. ^ Then, we aim at the significance map coding of EZW by designing a new set partition algorithm. We adopt and modify the framework of the SPIHT. The new set partition algorithm can catch more insignificant coefficients than the original algorithm. The experimental results show that the proposed algorithms achieve significant improvement over the standard EZW and SPIHT algorithms. ^ Finally, we present a new video coding algorithm. This algorithm can effectively exploit the temporal, spatial and frequency information within a video sequence through a combination of segmentation, regional wavelet packet, <b>adaptive</b> EZW, and <b>DPCM</b> techniques. The proposed video coder provides notable improvement in R-D characteristics over similar algorithms. ...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. Conventional pulse-code and differential pulse-code modulators for encoding video signals are difficult to realise economically. To alleviate this problem, a technique which divides the modulators into two stages is proposed. The first stage is a two-bit instantaneously adaptive delta modulator operating at a high clock rate and using low-precision components. Two-bit signals conveying polarity and magnitude information are produced by this delta modulator and used as the input to the second stage, a code converter. The code converter transforms, digitally, delta modulated signals into Pulse Code Modulation (PCM) or Differential Pulse Code Modulation (DPCM) signals. The resolution of the final PCM or DPCM encoder depends {{on the performance of}} the delta modulator used as the input stage. For that reason, the performance of the two-bit Instantaneously Adaptive Delta Modulation (2 BIADM) encoder is evaluated. This evaluation is made in two steps. First, a semi-empirical anaysis of the High Information Delta Modulation (HIDM) is made, because the 2 BIADM system is derived from the HIDM. Then the performance of the 2 BIADM is derived considering the HIDM as a reference. For the HIDM and 2 BIADM modulators operating at the same sampling frequency, the 2 BIADM presents an improvement in peak signal-to-noise ratio (SNR) Of 6 to 8 dB. Expressions are established to enable SNR to be calculated for the HIDM {{as a function of the}} encoding parameters. The expressions also apply to Constant Factor Delta Modulation, and represent the only known method of estimating numerically the SNR for instantaneously adaptive delta modulators. The 2 BIADM was tested, built and operated at a low sampling rate. This gave an insight into the operation of the proposed system, and complemented the computer simuLation analyses. The principles for the code conversion from the 2 BIADM to PCM or DPCM are fully discussed. The 2 BIADM does not impose restrictions on the values that the coefficients of the digital low-pass filter required in the code converter can assume. For low bandwidth expansion rates, it was verified that a 2 BIADM-to-PCM conversion filter with 5 stages performs better than a HIDM-to-PCM conversion with a filter having 256 stages (both encoders operating at the same word-rate). A generalization of the 2 -bit encoder to a N-bit <b>adaptive</b> <b>DPCM</b> system is outlined...|$|E

