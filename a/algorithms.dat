10000|10000|Public
5|$|American mathematicians Stan Wagon and Stanley Rabinowitz {{produced}} a simple spigot algorithm in 1995. Its speed {{is comparable to}} arctan <b>algorithms,</b> but not as fast as iterative <b>algorithms.</b>|$|E
5|$|Analysis of <b>algorithms</b> is {{a branch}} of {{computer}} science that studies the performance of <b>algorithms</b> (computer programs solving a certain problem). Logarithms are valuable for describing <b>algorithms</b> that divide a problem into smaller ones, and join the solutions of the subproblems.|$|E
5|$|Integer sorting <b>algorithms</b> {{including}} pigeonhole sort, counting sort, and radix sort {{are widely}} used and practical. Other integer sorting <b>algorithms</b> with smaller worst-case time bounds are not believed to be practical for computer architectures with 64 or fewer bits per word. Many such <b>algorithms</b> are known, with performance depending {{on a combination of}} the number of items to be sorted, number of bits per key, and number of bits per word of the computer performing the sorting algorithm.|$|E
3000|$|... {{which has}} been already listed in <b>Algorithm</b> 4, <b>Algorithm</b> 5, <b>Algorithm</b> 6, <b>Algorithm</b> 7, <b>Algorithm</b> 8 and <b>Algorithm</b> 12 as one {{alternative}} of A [...]...|$|R
25|$|The <b>algorithm</b> is {{also known}} as Floyd's <b>algorithm,</b> the Roy–Warshall <b>algorithm,</b> the Roy–Floyd <b>algorithm,</b> or the WFI <b>algorithm.</b>|$|R
50|$|The <b>algorithm</b> is {{also known}} as Floyd's <b>algorithm,</b> the Roy-Warshall <b>algorithm,</b> the Roy-Floyd <b>algorithm,</b> or the WFI <b>algorithm.</b>|$|R
25|$|Every {{field of}} science has its own {{problems}} and needs efficient <b>algorithms.</b> Related problems in one field are often studied together. Some example classes are search <b>algorithms,</b> sorting <b>algorithms,</b> merge <b>algorithms,</b> numerical <b>algorithms,</b> graph <b>algorithms,</b> string <b>algorithms,</b> computational geometric <b>algorithms,</b> combinatorial <b>algorithms,</b> medical <b>algorithms,</b> machine learning, cryptography, data compression <b>algorithms</b> and parsing techniques.|$|E
25|$|<b>Algorithms</b> {{are usually}} {{discussed}} {{with the assumption}} that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel <b>algorithms</b> or distributed <b>algorithms.</b> Parallel <b>algorithms</b> take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed <b>algorithms</b> utilize multiple machines connected with a network. Parallel or distributed <b>algorithms</b> divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such <b>algorithms</b> is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting <b>algorithms</b> can be parallelized efficiently, but their communication overhead is expensive. Iterative <b>algorithms</b> are generally parallelizable. Some problems have no parallel <b>algorithms,</b> and are called inherently serial problems.|$|E
25|$|Some {{problems}} may have multiple <b>algorithms</b> of differing complexity, while other problems might have no <b>algorithms</b> or no known efficient <b>algorithms.</b> There are also mappings from some problems to other problems. Owing to this, {{it was found}} to be more suitable to classify the problems themselves instead of the <b>algorithms</b> into equivalence classes based on the complexity of the best possible <b>algorithms</b> for them.|$|E
40|$|Mehrotra's <b>algorithm</b> {{has been}} the most {{successful}} infeasible interior-point <b>algorithm</b> for linear programming since 1990. Most popular interior-point software packages for linear programming are based on Mehrotra's <b>algorithm.</b> This paper proposes an alternative <b>algorithm,</b> arc-search infeasible interior-point <b>algorithm.</b> We will demonstrate, by testing Netlib problems and comparing the test results obtained by arc-search infeasible interior-point <b>algorithm</b> and Mehrotra's <b>algorithm,</b> that the proposed arc-search infeasible interior-point <b>algorithm</b> is a more efficient <b>algorithm</b> than Mehrotra's <b>algorithm...</b>|$|R
30|$|A {{different}} diffusion-based <b>algorithm</b> {{was proposed}} in[14] using the recursive least squares (RLS) <b>algorithm</b> {{to obtain the}} diffusion RLS (DRLS) <b>algorithm.</b> This DRLS <b>algorithm</b> provided exceptional results in both speed and performance. Another RLS-based distributed estimation <b>algorithm</b> has been studied in[15, 16]. The latter <b>algorithm</b> is hierarchical in nature, which makes its complexity {{higher than that of}} the DRLS <b>algorithm.</b> The RLS <b>algorithm</b> is inherently far more complex compared with the LMS <b>algorithm.</b> In this work, it is shown that despite the LMS <b>algorithm</b> being inferior to the RLS <b>algorithm,</b> using a variable step-size allows the LMS <b>algorithm</b> to achieve performance very close to that of the RLS <b>algorithm.</b>|$|R
30|$|Based on the {{previous}} study of MUSIC <b>algorithm</b> and ESPRIT <b>algorithm,</b> {{we know that they}} both belong to subspace class <b>algorithm,</b> so similar to MUSIC <b>algorithm,</b> ESPRIT <b>algorithm</b> can also be combined with time-frequency analysis integrated polarization information, which is polarimetric time-frequency ESPRIT (PTF-ESPRIT) <b>algorithm.</b> The biggest advantage of the MUSIC <b>algorithm</b> is the high accuracy of estimation, while the ESPRIT <b>algorithm</b> has the biggest advantage of fast calculation. Therefore, the PTF-ESPRIT <b>algorithm</b> should have a faster calculation speed than PTF-MUSIC <b>algorithm</b> in theory. The TLS-ESPRIT <b>algorithm</b> is used in this section.|$|R
25|$|<b>Algorithms</b> can be {{expressed}} in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of <b>algorithms</b> tend to be verbose and ambiguous, and are rarely used for complex or technical <b>algorithms.</b> Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express <b>algorithms</b> that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing <b>algorithms</b> {{in a form that}} can be executed by a computer, but are often used as a way to define or document <b>algorithms.</b>|$|E
25|$|In 2003, {{the entire}} {{editorial}} {{board of the}} Journal of <b>Algorithms</b> resigned to start ACM Transactions on <b>Algorithms</b> with a different, lower-priced, not-for-profit publisher, {{at the suggestion of}} Journal of <b>Algorithms</b> founder Donald Knuth. The Journal of <b>Algorithms</b> continued under Elsevier with a new editorial board until October 2009, when it was discontinued.|$|E
25|$|Sorting <b>algorithms</b> are {{prevalent}} in introductory computer science classes, where {{the abundance of}} <b>algorithms</b> for the problem provides a gentle introduction {{to a variety of}} core algorithm concepts, such as big O notation, divide and conquer <b>algorithms,</b> data structures such as heaps and binary trees, randomized <b>algorithms,</b> best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.|$|E
40|$|In this paper, we {{proposed}} an efficient <b>algorithm</b> based on SPFA(shortest path faster <b>algorithm),</b> {{which is an}} improved the Bellman-Ford <b>algorithm.</b> The Bellman-Ford <b>algorithm</b> {{can be used on}} graphs with negative edge weights unlike Dijkstra's <b>algorithm.</b> And SPFA <b>algorithm</b> used a queue to store the nodes, to avoid redundancy, though the Bellman-Ford <b>algorithm</b> {{takes a long time to}} update the nodes table. In this improved <b>algorithm,</b> an adjacency list is also used to store each vertex of the graph, applying dynamic optimal approach. And a queue is used to store the data. The improved <b>algorithm</b> can find the optimal path by continuous relaxation operation to the new node. Simulations to compare the efficiencies for Dijkstra's <b>algorithm,</b> SPFA <b>algorithm</b> and improved Bellman-Ford were taken. The result shows that Dijkstra's <b>algorithm,</b> SPFA <b>algorithm</b> have almost same efficiency on the random graphs, the improved <b>algorithm,</b> although the improved <b>algorithm</b> is not desirable, on grid maps the proposed <b>algorithm</b> is very efficient. The proposed <b>algorithm</b> has reduced two-third times processing time than SPFA <b>algorithm...</b>|$|R
40|$|This paper {{presents}} a new partitioned <b>algorithm</b> for LU decomposition with partial pivoting. The new <b>algorithm,</b> called the recursively partitioned <b>algorithm,</b> {{is based on}} a recursive partitioning of the matrix. The paper analyzes the locality of reference in the new <b>algorithm</b> and the locality of reference in a known and widely used partitioned <b>algorithm</b> for LU decomposition called the right-looking <b>algorithm.</b> The analysis reveals that the new <b>algorithm</b> performs a factor of Θ(√(M/n)) fewer I/O operations (or cache misses) than the right-looking <b>algorithm,</b> where n is the order of the matrix and M is the size of primary memory. The analysis also determines the optimal block size for the right-looking <b>algorithm.</b> Experimental comparisons between the new <b>algorithm</b> and the right-looking <b>algorithm</b> show that an implementation of the new <b>algorithm</b> outperforms a similarly coded right-looking <b>algorithm</b> on six different RISC architectures, that the new <b>algorithm</b> performs fewer cache misses than any other <b>algorithm</b> tested, and that it benefits more from Strassen's matrix-multiplication <b>algorithm...</b>|$|R
40|$|Job {{scheduling}} is a NP –hard {{problem in}} which we have to minimize the makespan time. Scheduling is the process of assigning resources to the jobs {{in such a way that}} all jobs get required resource in fairly manner without affecting one another. In this paper we have analysed different – different optimization <b>algorithm</b> cuckoo search <b>algorithm,</b> genetic <b>algorithm,</b> particle swarm optimization <b>algorithm</b> and hybrid <b>algorithm</b> and hybrid <b>algorithm</b> for job scheduling. Keywords-job scheduling, cuckoo search <b>algorithm,</b> genetic <b>algorithm,</b> particle optimization <b>algorithm,</b> hybrid <b>algorithm...</b>|$|R
25|$|Other <b>algorithms</b> {{for solving}} linear-programming {{problems}} {{are described in}} the linear-programming article. Another basis-exchange pivoting algorithm is the criss-cross algorithm. There are polynomial-time <b>algorithms</b> for linear programming that use interior point methods: these include Khachiyan's ellipsoidal algorithm, Karmarkar's projective algorithm, and path-following <b>algorithms.</b>|$|E
25|$|Search-match <b>algorithms</b> compare {{selected}} test {{reflections of}} an unknown crystal phase with entries in the database. Intensity-driven <b>algorithms</b> utilize the three most intense lines (so-called ‘Hanawalt search’), while d-spacing-driven <b>algorithms</b> {{are based on the}} eight to ten largest d-spacings (so-called ‘Fink search’).|$|E
25|$|For <b>algorithms,</b> this {{primarily}} {{consists of}} ensuring that <b>algorithms</b> are constant O(1), logarithmic O(log n), linear O(n), {{or in some cases}} log-linear O(n log n) in the input (both in space and time). <b>Algorithms</b> with quadratic complexity O(n2) fail to scale, and even linear <b>algorithms</b> cause problems if repeatedly called, and are typically replaced with constant or logarithmic if possible.|$|E
30|$|Our main <b>algorithm</b> is {{basically}} <b>algorithm</b> 6 and 10 which are our main binary consensus and average consensus. <b>Algorithm</b> 3, 4, 5, and 6 are for binary consensus <b>algorithm.</b> In <b>Algorithm</b> 6, we used <b>algorithm</b> 3, 4, and 5.|$|R
40|$|Abstract. Aimed at the {{inadequacy}} of the standard BP <b>algorithm,</b> a near optimal learning rate BP <b>algorithm</b> (NOLRBP) is presented. Selecting the learning rate of the <b>algorithm</b> based on one-dimensional search <b>algorithm</b> of optimization theory avoids the blindness in determining the learning rate. Simulations show that the <b>algorithm</b> is superior to the standard BP <b>algorithm</b> (SDBP), momentum BP <b>algorithm</b> (MOBP) and variable learning rate BP <b>algorithm</b> (VLBP) ...|$|R
3000|$|The {{implementation}} of <b>Algorithm</b> <b>Algorithm</b> 2 Basic ZMSSDł::bel alg:Basic-ZMSSD becomes <b>Algorithm</b> <b>Algorithm</b> 3 Efficient scalar ZMSSD: [...]...|$|R
25|$|The {{separator}} based {{divide and}} conquer paradigm has also been used to design data structures for dynamic graph <b>algorithms</b> and point location, <b>algorithms</b> for polygon triangulation, shortest paths, {{and the construction of}} nearest neighbor graphs, and approximation <b>algorithms</b> for the maximum independent set of a planar graph.|$|E
25|$|Although {{bubble sort}} {{is one of}} the {{simplest}} sorting <b>algorithms</b> to understand and implement, its O(n2) complexity means that its efficiency decreases dramatically on lists of more than a small number of elements. Even among simple O(n2) sorting <b>algorithms,</b> <b>algorithms</b> like insertion sort are usually considerably more efficient.|$|E
25|$|JVM {{improvements}} include: synchronization and compiler performance optimizations, new <b>algorithms</b> and upgrades {{to existing}} garbage collection <b>algorithms,</b> and application start-up performance.|$|E
40|$|Abstract. The waste-recycling Monte Carlo (WR) <b>algorithm,</b> {{introduced}} by Frenkel, is {{a modification of}} the Metropolis-Hastings <b>algorithm,</b> which makes use of all the proposals, whereas the standard Metropolis-Hastings <b>algorithm</b> only uses the accepted proposals. We prove the convergence of the WR <b>algorithm</b> and its asymptotic normality. We give an example which shows that in general the WR <b>algorithm</b> is not asymptotically better than the Metropolis-Hastings algorithm: the WR <b>algorithm</b> can have an asymptotic variance larger than {{the one of the}} Metropolis-Hastings <b>algorithm.</b> However, in the particular case of the Metropolis-Hastings <b>algorithm</b> called Boltzmann <b>algorithm,</b> we prove that the WR <b>algorithm</b> is asymptotically better than the Metropolis-Hastings <b>algorithm.</b> 1...|$|R
5000|$|The MaxClique <b>algorithm</b> [...] is {{the basic}} <b>algorithm</b> of MaxCliqueDyn <b>algorithm.</b> The pseudo code of the <b>algorithm</b> is: ...|$|R
40|$|Abstract. This paper {{proposed}} an improved Web text classification <b>algorithm.</b> It combined SVM <b>algorithm</b> and KNN <b>algorithm</b> {{and used the}} KNN <b>algorithm</b> {{to compensate for the}} deficiencies of traditional SVM <b>algorithm</b> and took simple ideas and smaller cost to improve traditional SVM <b>algorithm.</b> The experiments show that the proposed <b>algorithm</b> in this paper has gained good effect...|$|R
25|$|Two {{important}} {{classes of}} claytronics <b>algorithms</b> are shape sculpting and localization <b>algorithms.</b> The {{ultimate goal of}} claytronics research is creating dynamic motion in three-dimensional poses. All the research on catom motion, collective actuation and hierarchical motion planning require shape sculpting <b>algorithms</b> to convert catoms into the necessary structure, which will give structural strength and fluid movement to the dynamic ensemble. Meanwhile, localization <b>algorithms</b> enable catoms to localize their positions in an ensemble. A localization algorithm should provide accurate relational knowledge of catoms to the whole matrix based on noisy observation in a fully distributed manner.|$|E
25|$|While these <b>algorithms</b> are {{asymptotically}} efficient on random data, {{for practical}} efficiency on real-world data various modifications are used. First, the overhead of these <b>algorithms</b> becomes significant on smaller data, so often a hybrid algorithm is used, commonly switching to insertion sort once {{the data is}} small enough. Second, the <b>algorithms</b> often perform poorly on already sorted data or almost sorted data – these are common in real-world data, and can be sorted in O(n) time by appropriate <b>algorithms.</b> Finally, {{they may also be}} unstable, and stability is often a desirable property in a sort. Thus more sophisticated <b>algorithms</b> are often employed, such as Timsort (based on merge sort) or introsort (based on quicksort, falling back to heap sort).|$|E
25|$|The {{algorithm}} (and {{therefore the}} program code) is simpler than other <b>algorithms,</b> especially compared to strong <b>algorithms</b> that ensure {{a solution to}} the most difficult puzzles.|$|E
40|$|<b>Algorithm</b> finds {{frequency}} and phase of sinudoidal signal in presence of noise. <b>Algorithm</b> is special case of more-general, adaptive-paramenter-estimation techniques. Computational requirements of <b>algorithm</b> comparable to corresponding fast-Fourier-transform (FFT) <b>algorithm.</b> <b>Algorithm</b> works directly in time domain, whereas FFT <b>algorithm</b> transforms data into frequency domain for estimation and detection and requires secondary <b>algorithm</b> to interpolate between frequencies...|$|R
40|$|A {{deadlock}} avoidance <b>algorithm</b> for a centralized resource allocation system is presented. Unlike the Banker's <b>algorithm,</b> this proposed <b>algorithm</b> {{makes use of}} the state of the previous safe sequence to construct a new safe sequence. The performance of this proposed <b>algorithm</b> is compared to that of both the Banker's <b>algorithm</b> and an efficient <b>algorithm</b> proposed by Belik. The simulation results show that our <b>algorithm's</b> execution time is significantly better than the Banker's <b>algorithm</b> and is very competitive with Belik's <b>algorithm.</b> In addition, our Modified Banker's <b>Algorithm</b> produces optimal results unlike Belik's approach which sometimes deems a safe allocation request unsafe. This centralized <b>algorithm</b> combined with an <b>algorithm</b> by Moser, is extended for use in distributed systems. Compared with Moser's <b>algorithm,</b> this <b>algorithm</b> is less restrictive in acquiring resources from other processes and allowing increase in its maximum resource requirement as confirmed by our analysis and simulation results...|$|R
50|$|Elwyn Berlekamp invented an <b>algorithm</b> for {{decoding}} Bose-Chaudhuri-Hocquenghem (BCH) codes. James Massey recognized {{its application}} to linear feedback shift registers and simplified the <b>algorithm.</b> Massey termed the <b>algorithm</b> the LFSR Synthesis <b>Algorithm</b> (Berlekamp Iterative <b>Algorithm),</b> but it {{is now known as}} the Berlekamp-Massey <b>algorithm.</b>|$|R
