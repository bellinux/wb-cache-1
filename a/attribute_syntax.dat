6|31|Public
5|$|C# {{provides}} properties as syntactic sugar for {{a common}} pattern in which a pair of methods, accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class. No redundant method signatures for the getter/setter implementations need be written, and the property may be accessed using <b>attribute</b> <b>syntax</b> rather than more verbose method calls.|$|E
50|$|If the {{language}} uses the <b>attribute</b> <b>syntax</b> the syntax {{may look like}} this.|$|E
5000|$|The XML Enabled Directory allows {{directory}} {{entries to}} contain XML formatted data as attribute values. Furthermore, the <b>attribute</b> <b>syntax</b> {{can be specified}} {{in any one of}} a variety of XML schema languages that the directory understands.|$|E
5000|$|... {{the ability}} to accept at run time, user defined <b>attribute</b> <b>syntaxes</b> {{specified}} {{in a variety of}} XML schema languages, ...|$|R
5000|$|Robbins {{developed}} the Quipu directory {{part of the}} ISO Development Environment (ISODE) while a research assistant at UCL working for Peter Kirstein, he became custodian of ISODE from Marshall Rose in 1991.Quipu was used to prototype DIXIE and DASED, which merged to invent LDAP of which Robbins wrote the String Representation of Standard <b>Attribute</b> <b>Syntaxes</b> [...] element defined in RFCs 1448 and 1778 published by the IETF.Robbins wrote part 3 of Steve Kille's book Implementing X.400 and X.500: the PP and QUIPU Systems.|$|R
50|$|Here is {{an example}} of the TinyButStrong {{standard}} syntax for a field that should change the value of an HTML <b>attribute.</b> This <b>syntax</b> interferes with the HTML structure.|$|R
50|$|C# {{provides}} properties as syntactic sugar for {{a common}} pattern in which a pair of methods, accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class. No redundant method signatures for the getter/setter implementations need be written, and the property may be accessed using <b>attribute</b> <b>syntax</b> rather than more verbose method calls.|$|E
40|$|Status of this Memo This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. The Lightweight Directory Access Protocol (LDAP) [9] requires that the contents of AttributeValue fields in protocol elements be octet strings. This document defines the requirements that must be satisfied by encoding rules used to render X. 500 Directory attribute syntaxes into a form suitable {{for use in the}} LDAP, then goes on to define the encoding rules for the standard set of attribute syntaxes defined in [1, 2] and [3]. 1. <b>Attribute</b> <b>Syntax</b> Encoding Requirements. This section defines general requirements for lightweight directory protocol <b>attribute</b> <b>syntax</b> encodings. All documents defining <b>attribute</b> <b>syntax</b> encodings for use by the lightweight directory protocols are expected to conform to these requirements. The encoding rules defined for a given <b>attribute</b> <b>syntax</b> must produce octet strings. To the greatest extent possible, encoded octet strings should be usable in their native encoded form for display purposes. In particular, encoding rules for attribute syntaxes defining non-binary values should produce strings that can be displayed with little or no translation by clients implementing the lightweight directory protocols...|$|E
40|$|Integrated {{information}} systems provide users {{with a single}} unified view to heterogeneous data sources. As the resolution of schema level conflicts and the detection of fuzzy duplicates has been looked at more comprehensively, the problem of resolving data level conflicts still remains. We propose a relational data fusion operator, which fuses tuples representing the same real world entity by resolving conflicts in the <b>attributes.</b> <b>Syntax</b> and semantics of the operator are given {{as well as an}} extension of Sql. Furthermore, optimization issues involving transformations of logical query plans involving fusion and enabling cost based optimization for fusion are addressed. An implementation of the operator as part of a research prototype is under way...|$|R
5000|$|Pointers are {{available}} for all types, and the pointer-to- type is denoted [...] Address-taking and indirection use the [...] and [...] operators as in C, or happen implicitly through the method call or <b>attribute</b> access <b>syntax.</b> There is no pointer arithmetic, except via the special [...] type in the standard library.|$|R
40|$|The {{attribute}} dependence graph of a {{syntax tree}} may be partitioned into disjoint regions. Attribute instances in dierent regions {{are independent of}} one other. The advantages of partitioning the attribute dependence graph include simplifying the attribute grammar conceptually and allowing the possibility of parallel evaluation. We present a static partitioning algorithm for attribute grammars. The algorithm builds the set of all feasible partitions for every production by analyzing the grammar. After the <b>attributed</b> <b>syntax</b> tree is constructed, one of the feasible partitions is chosen for each production instance in the syntax tree. Gluing together the selected partitions for individual production instances results in a partition of the attribute dependence graph of the syntax tree. No further merging or partitioning is needed at evaluation time. In addition to static partitioning, the algorithm always produces the finest partition of every attribute dependence graph. An application of the partitioning technique is the strictness analysis for a simple programming language that contains no higher-order functions. 7 200...|$|R
5000|$|Attribute Types—Define {{an object}} {{identifier}} (OID) {{and a set}} of names that may be used to refer to a given attribute, and associates that <b>attribute</b> with a <b>syntax</b> and set of matching rules.|$|R
50|$|The {{optional}} <b>syntax</b> <b>attribute</b> {{was added}} to SLINK, ALINK, and TLINK. Syntax {{can be used to}} hold CDATA, but is generally only used by annotation programs to hold the data that {{led to the creation of}} the tag.|$|R
5000|$|Developers can add {{metadata}} {{to their}} code through attributes. There {{are two types}} of attributes, custom and pseudo custom attributes, and to the developer these have the same <b>syntax.</b> <b>Attributes</b> in code are messages to the compiler to generate metadata. In CIL, metadata such as inheritance modifiers, scope modifiers, and almost anything that isn't either opcodes or streams, are also referred to as attributes.|$|R
5000|$|Attribute grammars define {{systems that}} {{systematically}} compute [...] "metadata" [...] (called attributes) {{for the various}} cases of the language's <b>syntax.</b> <b>Attribute</b> grammars {{can be understood as}} a denotational semantics where the target language is simply the original language enriched with attribute annotations. Aside from formal semantics, attribute grammars have also been used for code generation in compilers, and to augment regular or context-free grammars with context-sensitive conditions; ...|$|R
40|$|In this paper, we expand Morzycki (2009) ’s {{claims that}} degree {{readings}} of size adjectives are <b>attributed</b> to <b>syntax.</b> We introduce a corpus-based analysis in Dutch to verify and extend his claim into the semantic domain. Using the LASSY Treebank, we extract syntactic and semantic properties of noun phrases {{consisting of the}} adjectives “gigantisch”, “kolossaal”, and “reusachtig ” and manually annotate each adjective-noun pair with a gradable or nongradable label. Using these features, we construct a statistical model based on logistic regression and find that the grammatical role, definiteness, and particular semantic noun groups derived from Cornetto (a Dutch WordNet with referential relations) {{have a significant effect}} on the likelihood that an adjective-noun pair is interpreted by the reader to have a degree reading. 1...|$|R
40|$|AbstractWe study {{issues in}} verifying compilers for modern {{imperative}} and object-oriented languages. We take {{the view that}} it is not the compiler but the code generated by it which must be correct. It is this subtle difference that allows for reusing standard compiler architecture, construction methods and tools also in a verifying compiler. Program checking is the main technique for avoiding the cumbersome task of verifying most parts of a compiler and the tools by which they are generated. Program checking remaps the result of a compiler phase to its origin, the input of this phase, in a provably correct manner. We then only have to compare the actual input to its regenerated form, a basically syntactic process. The correctness proof of the generation of the result is replaced by the correctness proof of the remapping process. The latter turns out to be far easier than proving the generating process correct. The only part of a compiler where program checking does not seem to work is the transformation step which replaces source language constructs and their semantics, given, e. g., by an <b>attributed</b> <b>syntax</b> tree, by an intermediate representation, e. g., in SSA-form, which is expressing the same program but in terms of the target machine. This transformation phase must be directly proven using Hoare logic and/or theorem-provers. However, we can show that given the features of today's programming languages and hardware architectures this transformation is to a large extent universal: it can be reused for any pair of source and target language. To achieve this goal we investigate annotating the syntax tree as well as the intermediate representation with constraints for exhibiting specific properties of the source language. Such annotations are necessary during code optimization anyway...|$|R
40|$|We present our {{approach}} to support program understanding by a tool that generates static and dynamic analysis algorithms from design pattern specifications to detect design patterns in legacy code. We therefore specify the static and dynamic aspects of patterns as predicates, and represent legacy code by predicates that encode its <b>attributed</b> abstract <b>syntax</b> trees. Given these representations, the static analysis is performed on the legacy code representation as a query derived from the specification of the static pattern aspects. It provides us with pattern candidates in the legacy code. The dynamic specification represents state sequences expected when using a pattern. We monitor {{the execution of the}} candidates and check their conformance to this expectation. We demonstrate {{our approach}} and evaluate our tool by detecting instances of the Observer, Composite and Decorator patterns in Java code using Prolog to define predicates and queries. 1...|$|R
40|$|Please {{refer to}} the errata for this document, which may include some {{normative}} corrections. This document is also available in these non-normative formats: Diff from Previous Recommendation, Postscript version, and PDF version Copyright Â © 2007 - 2013 W 3 CÂ ® (MIT, ERCIM, Keio, Beihang), All Rights Reserved. W 3 C liability, trademark and document use rules apply. RDFa Core 1. 1 [RDFA-CORE [p. 61]] defines <b>attributes</b> and <b>syntax</b> for embedding semantic markup in Host Languages. This document defines one such Host Language. This language is a superset of XHTML 1. 1 [XHTML 11 - 2 e [p. 61]], integrating the attributes as defined in RDFa Core 1. 1. This document is intended for authors who want to create XHTML Family documents that embed rich semantic markup. Status of This Document This section describes the status of this document {{at the time of}} its publication. Other documents may supersede this document. A list of current W 3 C publications and the latest revision of thi...|$|R
40|$|In {{the context}} of Alma (a system for program {{visualization}} and algorithm animation), we use an internal representation - based {{on the concept of}} an <b>attributed</b> abstract <b>syntax</b> tree decorated with attribute values, a DAST - to associate (static) figures to grammar rules (productions) and to step over program dynamics executing state changes in order to perform its animation. We do not rely upon any source program annotations (visual/animation statements, or parameters), neither on any special visual data types. On account of such principle, the approach becomes source language independent. It means that we can apply the same visualizer and animator, that is the Alma's back-end, to different programming languages; all that we need is di erent front-ends to parse each program into the DAST we use. In this paper we discuss Alma design goals and architecture, and we present the two mappings that associate to productions figures and rewriting rules to systematically draw a visual representation (exhibiting data and control flow) of a given source program and to animate its execution...|$|R
40|$|As edit, analyze, measure or {{transform}} attribute grammars by hand is {{an exhaustive}} task, {{it would be}} great if it could be automatized, specially for those who work in Language Engineering. However, currently there are no editors oriented to grammar development that cover all our needs. In this paper we describe the architecture and the development stages of AGile, a structured editor, analyzer, metric calculator and transformer for attribute grammars. It is intended, with this tool, to fill the existing gap. An AnTLR based <b>attribute</b> grammar <b>syntax</b> was used to define the input for this system. As soon as the user types the grammar, the input is parsed and kept in an intermediate structure in memory which holds the important information about the input grammar. This intermediate structure can be used to calculate all the metrics or to transform the input grammar. This system can be a valorous tool for those who need to improve the performance or functionalities of their language processor, speeding up the difficult task of defining and managing a language. Features like highlighting, automatic indentation, on-the-fly error detection, etc., also adds efficiency...|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (1999). All Rights Reserved. The Lightweight Directory Access Protocol [1] provides a means for clients to interrogate and modify information stored in a distributed directory system. The information in the directory is maintained as attributes [2] of entries. Most of these <b>attributes</b> have <b>syntaxes</b> which are human-readable strings, and it is desirable {{to be able to}} indicate the natural language associated with attribute values. This document describes how language codes [3] are carried in LDAP and are to be interpreted by LDAP servers. All implementations MUST be prepared to accept language codes in the LDAP protocols. Servers {{may or may not be}} capable of storing attributes with language codes in the directory. This document does not specify how to determine whether particular attributes can or cannot have language codes...|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2004). It is often desirable {{to be able to}} indicate the natural language associated with values held in a directory {{and to be able to}} query the directory for values which fulfill the user’s language needs. This document details the use of Language Tags and Ranges in the Lightweight Directory Access Protocol (LDAP). 1. Background and Intended Use The Lightweight Directory Access Protocol (LDAP) [RFC 3377] provides a means for clients to interrogate and modify information stored in a distributed directory system. The information in the directory is maintained as attributes of entries. Most of these <b>attributes</b> have <b>syntaxes</b> which are human-readable strings, and it is desirable to be able to indicate the natural language associated with attribute values. This document describes how language tags and ranges [RFC 3066] are carried in LDAP and are to be interpreted by LDAP implementations. All LDAP implementations MUST be prepared to accept language tags and ranges...|$|R
40|$|Abstract: A Log is {{consists}} of much helpful data regarding activities or events of systems and networks and these data having number of <b>attributes</b> and own <b>syntax.</b> These logs are made-up of events {{which has been}} done by users on systems or in networks. These information is very expensive for organizations. These logs are used for finding problems, to optimize performance, to record all events, and to investigate malicious activity in systems or networks. So, protection from attackers is required. Hence organization should maintained integrity, confidentiality, security of logs. The capital expenses will be very less to maintain logs for organizations for longer period. Hence in this paper, we propose more effective secure cloud based log management to decrease cost and provide security of logs from attackers. By using encryption and MAC provide secured log...|$|R
40|$|Cognitive {{science in}} general and {{linguistics}} in particular necessitates some analog component of descriptive device, in order {{to account for the}} continuous gradation of the preferences among interpretations of utterances, for example. This paper proposes a descriptive formalism consisting of two layers, i. e., a system of symbolic aspect of constraints based on first order logic and a theory of cost or potential energy of the symbolic constraints. A grammar fragment is presented which accounts for some basic phenomena concerning complementation, adjunction and quantification of Japanese. This grammar reduces the combinatorial complexity, especially that of <b>syntax,</b> <b>attributing</b> relevant constraints to semantics and the analog component of the theory. A formal theory of the analog component is also sketched, and some implication of our approach with respect to cognitive science is discussed. ...|$|R
5000|$|Barthes {{discusses}} the art patterns as narratives of cultural coexistence (for details see: Introduction to structural analysis of narratives). However, Spivey summarizes that cultural coexistence {{is not the}} single reason to explain the phenomenon of art being recursive. Chomsky at al. argued that the core property of human communication (in a [...] "narrow" [...] sense, including language) is recursion. According to Chomsky at al. recursion is <b>attributed</b> to limited <b>syntax</b> in the conception, with a finite set of elements to yield a potentially infinite array of discrete expressions. Thomas explains the art recursion (in a [...] "broad" [...] sense) with implosion of archetypal structures existing beyond the faculty of human communication. Studying Persian-Sassanide art patterns and possibly their early Nomadic conceptions is uncovering their symbols (symbolism) and creative imagination.|$|R
40|$|The aim of {{this paper}} is to show the {{approaches}} involved in the implementation of two tools of PCVIA project that can be used for Program Comprehension. Both tools use known compiler techniques to inspect code in order to visualize and understand programs’ execution but one of them modifies the source code and the other not. In the non-invasive approach, we convert the source program into an internal decorated (or <b>attributed)</b> abstract <b>syntax</b> tree and then we visualize the structure traversing it, and applying visualization rules at each node according to a pre-defined rule-base. No changes are made in the source code, and the execution is simulated. In the invasive approach, we traverse the source program and instrument it with inspection functions. Those functions, also known as inspectors, provide information about the function-call flow and data usage at runtime (during the actual program execution). This information is collected and gathered in an information repository that is then displayed in a suitable form for navigation. These two different approaches are used respectively by Alma (generic program animation system) and CEAR (C Rooting Algorithm Visualization tool). For each tool several examples of visualization are shown in order to discuss the information that is included in the visualizations, visualization types and the use of Program Animation for Program Comprehension...|$|R
40|$|The {{notion of}} {{attribute-based}} communication seems promising to model and analyse systems with {{huge numbers of}} interacting components that dynamically adjust and combine their behaviour to achieve specific goals. A basic process calculus, named AbC, is introduced that has as primitive construct exactly attribute-based communication {{and its impact on}} the above mentioned kind of systems is considered. An AbC system consists of a set of parallel components each of which is equipped with a set of attributes. Communication takes place in a broadcast fashion and communication links among components are dynamically established by taking into account interdependences determined by predicates over <b>attributes.</b> First, the <b>syntax</b> and the reduction semantics of AbC are presented, then its expressiveness and effectiveness is demonstrated by modelling two scenarios from the realm of TV streaming channels. An example of how well-established process calculi could be encoded into AbC is given by considering the translation into AbC of a prototypical π-calculus process...|$|R
40|$|We {{show how}} new {{syntactic}} forms and static analysis {{can be added}} to a programming language to support abstractions provided by libraries. Libraries have the important characteristic that programmers can use multiple libraries in a single program. Thus, any attempt to extend a language’s syntax and analysis should be done in a composable manner so that similar extensions that support other libraries can be used by the programmer in the same program. To accomplish this we have developed an extensible attribute grammar specification of Java 1. 4 written in the attribute grammar specification language Silver. Library writers can specify, as an <b>attribute</b> grammar, new <b>syntax</b> and analysis that extends the language and supports their library. The Silver tools automatically compose the grammars defining the language and the programmer-selected language extensions (for their chosen libraries) into a specification for a new custom language that has language-level support for the libraries. We demonstrate how syntax and analysis are added to a language by extending Java with syntax from the query language SQL and static analysis of these constructs so that syntax and type errors in SQL queries can be detected at compile-time. 1...|$|R
40|$|This paper {{addresses}} the long-standing {{question of the}} restricted distribution of subjects in wh-questions in languages like Italian: *Chi Gianni ha invitato?, *Chi ha Gianni invitato?, Chi ha invitato, Gianni? “whom Gianni has invited?”. It is commonly assumed that this restriction is {{a direct consequence of}} the special <b>syntax</b> <b>attributed</b> to the subject in Null Subject languages (NSLs). I will show that this is incorrect. The following generalizations regarding the occurrence of subjects in wh-questions seem to hold: -	the restriction against subjects occurring between a wh-phrase and the verb is very specific and only concerns the highest subjects in the syntactic tree (DPs and strong pronouns); -	the restricted distribution of these subjects in wh-questions does not correlate with pro-drop, since it is also found in non-NSLs; -	the restriction, instead, seems to correlate with verb movement: although in Italian the verb raises higher in interrogatives than in declaratives, it does not reach the C layer; in other words, no T-to-C occurs; -	the postverbal subject occurring in wh-questions is marginalized, i. e. destressed in situ. In this paper, I will concentrate on questions in main clauses...|$|R
40|$|Promotor: Grzegorz J. Nalepa. Recenzent: Marcin Szpyrka, Ngoc Thanh Nguyen. Niepublikowana praca doktorska. Tyt. z ekranu tyt. Praca doktorska. AGH University of Science and Technology in Kraków. Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, 2015. Zawiera bibliogr. Dostępna również w wersji drukowanej. Tryb dostępu: Internet. State {{of the art}} in rule representation, {{knowledge}} representation with rules, selected {{knowledge representation}} methods, expert systems, production systems Shells, business rules approach, rules on semantic web, rules in software engineering, formalization of rules, propositional logic, First-Order Predicate Calculus, common logic, description logics, attributive logic, F-LOGIC, modal logics, knowledge engineering processes, problem identification, knowledge acquisition, knowledge modeling, inference process, knowledge verification and validation, knowledge interoperability, rule interoperability methods, knowledge interchange framework, rule interchange framework, production rule representation, rule markup language, REWERSE rule markup language, languages for production rules, important features of rule languages, Polish Liability Insurance Use Case Example, CLIPS, Jess, DROOLS, XTT 2, comparison of rule languages, CLIPS versus JESS, CLIPS versus DROOLS, JESS versus DROOLS, all versus XTT 2, existing approaches to translation of the selected rule languages, model of production rule representation, multilevel approach to rule interoperability, definition of the formalized model, data types and objects, facts, system state and trajectory, variables, taxonomy of formulae and operators, semantics of formulae and operators, rules, modules, knowledge base, model-based knowledge translation, identification of semantically equivalent features, translation of rule base structure, modules, rule level features, submodules, evaluation of the approach, evaluation procedure, definition and translation of the formal model of the PLI use case, definitions of types, initial state, definitions of rules, modules definitions, implementation of translation tool, identified challenges, syntax of rule languages, XML schema of the Model Concrete Synatx, CLIPS Syntax in BNF, data types, variables and expressions, Constructs, deffacts Construct, deftemplate Construct, fact specification, defrule Construct, defglobal Construct, deffunction Construct, defgeneric Construct, defmethod Construct, defclass Construct, defmessage-handler Construct, definstances Construct, defmodule Construct, Constraint <b>Attributes,</b> JESS <b>Syntax,</b> deffacts Construct, deffunction Construct, defglobal Construct, defmodule Construct, defquery Construct, defrule Construct, deftemplate Construct, Complete Models of the Selected Use Cases, Model of PLI Use Case, Formal Model of System, Model in CLIPS, Model in JESS, Model in DROOLS, Model in XTT 2, Model of UserV Use Case, Formal Model of Syste...|$|R
40|$|AbstractThe {{general idea}} of Space Syntax {{approach}} postulates that, the total network of public spaces which {{is defined by}} the natural form of the urban tissue in every settlement or city, can be considered as a single and continuous spatial system. This system can be divided into components, analyzed as networks of choices, and then represented as maps and graphs that describe the relative connectivity of those spaces. Among the Space Syntax analysis methods the most widely used is the integration analysis which identifies the preferred shortest route between two points in a given street network. Although paradoxes arise using this method, many research results from European and American cities of different morphology have claimed strong connectivity of preferred routes and traffic volume generated by city planning choices (land use, population density, etc). The proposed adaptation of the chosen Space Syntax methodology is applied to Xanthi, a Greek medium size city in Northern Greece with a very interesting variety in the form and density of its urban tissue. It is the purpose of this application (which uses the DepthMap program) to verify and interpret the functional structure of the city through identifying the form of its core, the principal and most congested road axes, the allocation of the central land use etc. Moreover, it aims to verify the argument that the space <b>syntax</b> <b>attributes</b> of the urban public space {{have an impact on the}} accomplishment of the objectives in the regeneration studies and are able to affect the development and structure of its central functions...|$|R
40|$|Relative clauses (RCs) {{have been}} studied {{extensively}} in language acquisition and adult processing. Studies show that both children and adults find object relatives harder than subject relatives [1, 2]. Despite the similar pattern, adult difficulty is taken to reflect the increased processing demands of object relatives [1] while child difficulty is often interpreted as evidence for children's lack of adult-like knowledge of the structure, <b>attributed</b> to under-developed <b>syntax</b> [2] or {{to the use of}} non-adult processing heuristics which are abandoned as the parser develops [3]. In this paper, I suggest that children's difficulty reflects similarity between the child and the adult parser: children display adult-like processing preferences. The study focuses on the effect of the NP type appearing inside the relative clause. This factor was shown to influence adult processing: adult difficulty with object relatives is reduced when the embedded NP is a pronoun rather than a Lexical NP [4]. Two experiments demonstrate the influence of this factor on child performance. Together, the experiments show that just like adults children are better at comprehending what they (a) hear more frequently and (b) requires less processing resources. Furthermore, the results suggest that previous assessments of child performance have underestimated children's knowledge by testing them on relative clauses that are especially taxing even for adults. The first experiment examined the distribution of NP types in child and child-directed speech in Hebrew. The results reveal a very similar distribution: both children and adults rarely produce object relatives with Lexical NPs. The second experiment manipulated the NP type in a comprehension task (full NP: the monkey that the girl fed, vs. pronoun: the monkey that I fed) ...|$|R
40|$|Network {{diagrams}} {{of ecological}} cycles, eg, carbon and nitrogen cycles, {{are a common}} feature in science textbooks for 14 - 18 years age groups. From an information design perspective these diagrams raise a particularly interesting challenge; that of categorising up to six types of biological information using two graphic syntactic roles – nodes and connecting arrows – whilst ensuring an efective and unambiguous message. This practice-led thesis reviews the precision of information categorisation in 209 network diagrams collected from UK and Danish science textbooks (1935 - 2009). Visual content analysis and graphic syntax theory (Engelhardt, 2002) is applied to review the existing information categorisation in relation to four types of graphic inefectiveness: 1) implicit nodes, 2) imprecise relative spatial positioning of graphic objects, 3) polysemy, and 4) inconsistent visual <b>attributes</b> or verbal <b>syntax.</b> This review finds 29 types of ineffective graphic tactics, which may result in ambiguous messages due to illogical linking sequences, implicit circulating elements, and confusion about chemical transfer and transformations. Based on these analysis indings, the design process in educational publishing is investigated. This identifies the rationale informing the transformation of information into network diagrams, based on semi- structured interviews with 19 editors, authors, designers, and illustrators in six publishing houses (3 in UK, 3 in Denmark). The rationale is mapped using phenomenographic analysis method and existing theories on the design process, namely brief development and translation stages (Crilly, 2005), choice points and the problem setting process (Schön, 2006), problem- solution co-evolution (Dorst and Cross, 2001), and design constraints (Lawson, 2006). The curriculum purpose of the ecological cycle network diagram is found to tightly constrain the identiied rationale and the graphic decision-making based mainly on tacit knowledge. In a final discussion the research indings are integrated by identifying models of design activities (Dumas and Mintzberg, 1993) present in investigated professional practice. This reveals how design decisions may inluence the occurrence of inefective graphic tactics. Recommendations for alternative information transformation strategies are then presented, centred on integrating graphic syntax knowledge into the current processes. These recommendations are anchored in suggestions by the interviewed participants...|$|R
40|$|The general aim of {{this study}} is to {{investigate}} the similarities and differences in knowledge and processing of Japanese passive constructions by heritage speakers and second language (L 2) learners of Japanese. These groups acquire language differently in terms of age and context/manner of acquisition, and comparing their linguistic behaviour allows us to examine whether heritage speakers have an advantage over L 2 learners due to their early exposure and natural context of input. In order to examine this issue, the linguistic knowledge of Japanese passives and the way in which they are processed were compared between the two populations. I tested the two different types of passives that are available in Japanese: the type that involves the syntax-semantics-discourse interface (indirect passive and ni-direct passive), and the one that does not involve that interface (niyotte passive). It has been found that advanced heritage speakers and L 2 learners have difficulties with structures involving different structural levels, especially structures at the interface with discourse (Laleko & Polinsky, 2013; Montrul & Polinsky, 2011, among many others), as the interfaces involving an external cognitive domain (e. g., syntax-discourse) require more processing resources than linguistic internal interfaces (e. g., syntax-semantics) (Sorace, 2011). While such representational and processing difficulties have been reported for several languages, previous studies on the acquisition of Japanese passives by both L 1 and L 2 learners have found the opposite: namylt, that the niyotte passive, which does not involve an external interface, being acquired later than the other two passives, which are discourse dependent (Harada & Fukuda 1998 for L 1; Hara 2002 for L 2). These results may be <b>attributed</b> to <b>syntax</b> derivation or frequency of use. The niyotte passive is considered to be derived by movement, while the other two are said to be base-generated. Thus, both the complexity of the syntactic derivation and the fact that usage of niyotte passive is usually limited to formal speech or written texts may delay acquisition. Examining the acquisition and processing of Japanese passives allows us to analyse the factors that play a crucial role in determining the difficulty of acquisition. In order to investigate these factors, I used two experimental tasks, an acceptability judgment task (AJT) and a self-paced listening task (SPL). The former investigated heritage speakers’ and Japanese as a foreign language (JFL) learners’ knowledge of each type of Japanese passives. The latter allowed us to test whether there are any differences in the processing of the two types of passives; specifically, whether the passives with an external interface are more difficult for L 2 learners and heritage speakers to process. A control group of native speakers and a group of first generation immigrants to Canada were also tested to compare their results to those of the two experimental groups, allowing us to investigate whether heritage speakers have knowledge and processing patterns similar to those of the control group due to their early language exposure to the language and contextualized input. 	The results of the AJT showed that each experimental group displayed a stronger knowledge of different aspects of Japanese passives. While the heritage speakers recognised the pragmatic features of the ni-direct passive, the JFL learners did not. In contrast, the JFL learners showed syntactic and semantic knowledge of the indirect passive, unlike the heritage speakers. These contrastive results indicate that different manners of input lead to different acquisition outcomes. Furthermore, neither group demonstrated knowledge of the low frequency niyotte passive, and thus input frequency, rather than the discourse-related interface, appear to be more critical for the acquisition of Japanese passives. With respect to the SPL, the speakers’ performance was native-like in the case of the heritage speakers but non-native with the JFL learners, indicating that early age of exposure has an effect on language processing. Taken together, the results from the both tasks showcase the importance of both implicit and explicit manner of input, especially in the case of low frequency structures, as well as the early age of acquisition of a language...|$|R

