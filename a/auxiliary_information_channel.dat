1|5407|Public
40|$|Abstract: The {{protocol}} implementation {{was tested}} as black box before. Due {{to the complexity}} of routing protocols, the black box model can not fulfil the requirements to test them. Based on the analysis and perception of routing protocols, the windowed black box model is raised. The controllability and the observability are enhanced through other modules that interact with the module under test using standard interfaces. The multiple channels test method is put forward accordingly with more channels such as configuration channel and <b>auxiliary</b> <b>information</b> <b>channel.</b> The coverage and the efficiency of the test are increased considerably. With techniques like packet extension, the test suite is organized and described uniformly and integrally, which guarantees the flexibility and the universality and paves the way for the transition from single centered test suite to a distributed one. This model and this test method have achieved desirable results from routing protocol test and also give valuable references for the test of other protocols...|$|E
30|$|A {{multitude}} of imaging probes such as scanning {{transmission electron microscopy}} (STEM) have reached the requisite spatial resolution, at least in two dimensions, to directly distinguish the individual structural microstate of a material, namely an atom and its local neighbors [1]. In addition, the prevalence of <b>auxiliary</b> <b>information</b> <b>channels</b> such as electron energy-loss spectra acquired at similar spatial resolutions allows one to append to these structural microstates additional chemical/electronic state information [2 – 4]. The data that emanate from such modalities reveal a wealth of information regarding the static modulation of material properties by local structural deviations [5], competing structural ground states [6], and even dynamic phase transformations or ensuing structural reordering during in situ atomic resolution imaging of materials growth [7]. These imaging modalities are crucial to fundamental investigations of modern materials, which often display a range of structural configurations and order parameter phases. In many cases, some structural phases are not directly discernible by the diffraction-based methods of X-rays and neutron scattering [8, 9] due to either their small volume fraction and/or their lack of long-range periodicity, and therefore require an imaging approach [10, 11] for identification.|$|R
40|$|In {{this paper}} we {{describe}} our TRECVID 2008 video retrieval experiments. The MediaMill team participated in three tasks: concept detection, automatic search, and interac- tive search. Rather than continuing {{to increase the}} number of concept detectors available for retrieval, our TRECVID 2008 experiments focus on increasing the robustness of a small set of detectors using a bag-of-words approach. To that end, our concept detection experiments emphasize in particular the role of visual sampling, the value of color in- variant features, the influence of codebook construction, and the effectiveness of kernel-based learning parameters. For retrieval, a robust but limited set of concept detectors ne- cessitates the need to rely on as many <b>auxiliary</b> <b>information</b> <b>channels</b> as possible. Therefore, our automatic search ex- periments focus on predicting which <b>information</b> <b>channel</b> to trust given a certain topic, leading to a novel framework for predictive video retrieval. To improve the video retrieval re- sults further, our interactive search experiments investigate the roles of visualizing preview results for a certain browse- dimension and active learning mechanisms that learn to solve complex search topics by analysis from user brows- ing behavior. The 2008 edition of the TRECVID bench- mark has been the most successful MediaMill participation to date, resulting in the top ranking for both concept de- tection and interactive search, and a runner-up ranking for automatic retrieval. Again a lot has been learned during this year’s TRECVID campaign; we highlight the most im- portant lessons at the end of this paper...|$|R
40|$|In {{this paper}} we {{describe}} our TRECVID 2009 video re- trieval experiments. The MediaMill team participated in three tasks: concept detection, automatic search, and in- teractive search. The {{starting point for}} the MediaMill con- cept detection approach is our top-performing bag-of-words system of last year, which uses multiple color descriptors, codebooks with soft-assignment, and kernel-based supervised learning. We improve upon this baseline system by explor- ing two novel research directions. Firstly, we study a multi- modal extension by including 20 audio concepts and fusion using two novel multi-kernel supervised learning methods. Secondly, with the help of recently proposed algorithmic re- nements of bag-of-word representations, a GPU implemen- tation, and compute clusters, we scale-up the amount of vi- sual information analyzed by an order of magnitude, to a total of 1, 000, 000 i-frames. Our experiments evaluate the merit of these new components, ultimately leading to 64 ro- bust concept detectors for video retrieval. For retrieval, a robust but limited set of concept detectors justi es the need to rely on as many <b>auxiliary</b> <b>information</b> <b>channels</b> as pos- sible. For automatic search we therefore explore how we can learn to rank various <b>information</b> <b>channels</b> simultane- ously to maximize video search results for a given topic. To further improve the video retrieval results, our interactive search experiments investigate the roles of visualizing pre- view results for a certain browse-dimension and relevance feedback mechanisms that learn to solve complex search top- ics by analysis from user browsing behavior. The 2009 edi- tion of the TRECVID benchmark has again been a fruitful participation for the MediaMill team, resulting in the top ranking for both concept detection and interactive search. Again a lot has been learned during this year's TRECVID campaign; we highlight the most important lessons {{at the end of this}} paper...|$|R
40|$|In {{this paper}} we {{describe}} our TRECVID 2010 video retrieval experiments. The MediaMill team participated in three tasks: semantic indexing, known-item search, and instance search. The {{starting point for}} the MediaMill concept detection approach is our top-performing bag-of-words system of last year, which uses multiple color SIFT descriptors, sparse codebooks with spatial pyramids, kernel-based machine learning, and multi-frame video processing. We improve upon this baseline system by further improving its execution times for both training and classification using GPU-optimized algorithms, approximated histogram intersection kernels, and several multi-frame combination methods. Being more efficient allowed us to supplement the Internet video training collection with positively labeled examples from international news broadcasts and Dutch documentary video from the TRECVID 2005 - 2009 benchmarks. Our experimental setup covered a huge training set of 170 thousand keyframes and a test set of 600 thousand keyframes in total. Ultimately leading to 130 robust concept detectors for video retrieval. For retrieval, a robust but limited set of concept detectors justifies the need to rely on as many <b>auxiliary</b> <b>information</b> <b>channels</b> as possible. For automatic known item search we therefore explore how we can learn to rank various <b>information</b> <b>channels</b> simultaneously to maximize video search results for a given topic. To further improve the video retrieval results, our interactive known item search experiments investigate how to combine metadata search and visualization into a single interface. The 2010 edition of the TRECVID benchmark has again been a fruitful participation for the MediaMill team, resulting in the top ranking for concept detection in the semantic indexing task. Again a lot has been learned during this year’s TRECVID campaign; we highlight the most important lessons {{at the end of this}} paper. ...|$|R
30|$|The {{concept of}} our rate {{adaptation}} scheme {{is simple and}} novel. Since traditional rate adaptation algorithms are designed for general purposes, it has no <b>auxiliary</b> <b>information</b> about the <b>channel</b> quality in general situations. They rely on certain measurements, such as packet error rate or signal strength, to recognize the channel’s quality and can only be a passive adaptor to the channel. By contrast, since {{we know that the}} channel condition is different when a train passes through a station in each phase, AARA identifies the train’s current movement phases to proactively adopt suitable transmission strategies. In other words, the proposed mechanism is a two-tier design. We use the information of the train’s movement phases estimated by accelerometer to deal with long-term channel variation, and make use of the rate adaptation scheme, such as SampleRate algorithm, to handle the channel variation in short-term time.|$|R
40|$|Orthogonal {{frequency}} division multiplexing (OFDM) {{is a technique}} which {{are used in the}} next-generation wireless communication. Channel estimation in the OFDM technique is one of the big challenges, ever since high-resolution channel estimation can significantly improve the equalization at the receiver and consequently enhance the communication performances. Channel computation using superimposed pilot sequences is also a fully new area, idea for using superimposed pilot sequences has been proposed by various authors for different applications. In this paper, we are introduced a high accurate, low complexity compressive sensing (CS) based <b>channel</b> estimation namely <b>Auxiliary</b> <b>information</b> based Subspace Pursuit (ASP) in TFT-OFDM systems. ASP based channel estimation in TFT-OFDM system is based on two steps. First is, by exploiting the signal structure of recently proposed TDM-OFDM scheme, the supporting <b>channel</b> <b>information</b> is obtained. Second is, we propose the supporting information based subspace pursuit (SP) algorithm to use a very small amount of frequency domain pilots embedded in the OFDM block used for the exact channel estimation. Moreover, the obtained <b>auxiliary</b> <b>channel</b> <b>information</b> is adopted to reduce the complexity of the conventional SP algorithm. Simulation results demonstrate a important reduction of the number of pilots relative to least-squares channel estimation and supporting high-order modulations like 256 QAM...|$|R
30|$|When no <b>auxiliary</b> <b>information</b> is {{exploited}} (HT), {{the performance}} tends to improve as the forest cover increases. When <b>auxiliary</b> <b>information</b> is exploited, the performance tends to improve as {{the correlation between}} auxiliary and survey variable increases.|$|R
40|$|Abstract [...] In this paper, {{we propose}} a novel method for {{improving}} digital image recovery from print – scan channel. A printed photograph {{is not sufficient}} to recover the original digital image because of distortions introduced in scanning process. The proposed approach recovers the print – scan images with improved quality. We solve this approximation problem by combining both the photograph and the digital <b>auxiliary</b> <b>information</b> in the same printed material. Digital <b>auxiliary</b> <b>information</b> is composed of small amount of digital data, which enables accurate image recovery. The proposed technique consists of two main processes namely encoding and decoding. Encoding process generates <b>auxiliary</b> <b>information</b> and it is kept together with the print. Decoding process uses the <b>Auxiliary</b> <b>information</b> along with the scan of the printed photograph to eliminate the distortions introduced during scanning process. Our experimental results confirm that the proposed approach recovers the digital image with high quality by using reduced amount of <b>auxiliary</b> <b>information...</b>|$|R
5000|$|... #Subtitle level 2: <b>Auxiliary</b> <b>Information</b> in Collaborative Filtering ...|$|R
2500|$|Availability of <b>auxiliary</b> <b>information</b> about {{units on}} the frame ...|$|R
40|$|AbstractThis paper {{presents}} program {{analyses and}} transformations for strengthening invariants {{for the purpose}} of efficient computation. Finding the stronger invariants corresponds to discovering a general class of <b>auxiliary</b> <b>information</b> for any incremental computation problem. Combining the techniques with previous techniques for caching intermediate results, we obtain a systematic approach that transforms non-incremental programs into efficient incremental programs that use and maintain useful <b>auxiliary</b> <b>information</b> as well as useful intermediate results. The use of <b>auxiliary</b> <b>information</b> allows us to achieve a greater degree of incrementality than otherwise possible. Applications of the approach include strength reduction in optimizing compilers and finite differencing in transformational programming...|$|R
5000|$|DisplayPort carries {{digital audio}} and video, {{as well as}} <b>auxiliary</b> <b>information</b> ...|$|R
40|$|Automatic speech recognition (ASR) {{is a very}} {{challenging}} problem due to {{the wide variety of}} the data that it must be able to deal with. Being the standard tool for ASR, hidden Markov models (HMMs) have proven to work well for ASR when there are controls over the variety of the data. Being relatively new to ASR, dynamic Bayesian networks (DBNs) are more generic models with algorithms that are more flexible than those of HMMs. Various assumptions can be changed without modifying the underlying algorithm and code, unlike in HMMs; these assumptions relate to the variables to be modeled, the statistical dependencies between these variables, and the observations which are available for certain of the variables. The main objective of this thesis, therefore, is to examine some areas where DBNs can be used to change HMMs' assumptions so as to have models that are more robust to the variety of data that ASR must deal with. HMMs model the standard observed features by jointly modeling them with a hidden discrete state variable and by having certain restraints placed upon the states and features. Some of the areas where DBNs can generalize this modeling framework of HMMs involve the incorporation of even more "auxiliary" variables to help the modeling which HMMs typically can only do with the two variables under certain restraints. The DBN framework is more flexible in how this auxiliary variable is introduced in different ways. First, this <b>auxiliary</b> <b>information</b> aids the modeling due to its correlation with the standard features. As such, in the DBN framework, we can make it directly condition the distribution of the standard features. Second, some types of <b>auxiliary</b> <b>information</b> are not strongly correlated with the hidden state. So, in the DBN framework we may want to consider the auxiliary variable to be conditionally independent of the hidden state variable. Third, as <b>auxiliary</b> <b>information</b> tends to be strongly correlated with its previous values in time, I show DBNs using discretized auxiliary variables that model the evolution of the <b>auxiliary</b> <b>information</b> over time. Finally, as <b>auxiliary</b> <b>information</b> can be missing or noisy in using a trained system, the DBNs can do recognition using just its prior distribution, learned on <b>auxiliary</b> <b>information</b> observations during training. I investigate these different advantages of DBN-based ASR using <b>auxiliary</b> <b>information</b> involving articulator positions, estimated pitch, estimated rate-of-speech, and energy. I also show DBNs to be better at incorporating <b>auxiliary</b> <b>information</b> than hybrid HMM/ANN ASR, using artificial neural networks (ANNs). I show how <b>auxiliary</b> <b>information</b> is best introduced in a time-dependent manner. Finally, DBNs with <b>auxiliary</b> <b>information</b> are better able than standard HMM approaches to handling noisy speech; specifically, DBNs with hidden energy as <b>auxiliary</b> <b>information</b> [...] that conditions the distribution of the standard features and which is conditionally independent of the state [...] are more robust to noisy speech than HMMs are...|$|R
40|$|Abstract. We {{study the}} problem of secure two-party and multiparty {{computation}} (MPC) in a setting where a cheating polynomial-time ad-versary can corrupt an arbitrary subset of parties and, in addition, learn arbitrary <b>auxiliary</b> <b>information</b> on the entire states of all honest par-ties (including their inputs and random coins), in an adaptive manner, throughout the protocol execution. We formalize a definition of multiparty computation secure against adaptive <b>auxiliary</b> <b>information</b> (AAI-MPC), that intuitively guarantees that such an adversary learns {{no more than the}} function output and the adaptive <b>auxiliary</b> <b>information.</b> In particular, if the <b>auxiliary</b> <b>information</b> contains only partial, “noisy, ” or computa-tionally invertible information on secret inputs, then only such informa-tion should be revealed. We construct a universally composable AAI two-party and multiparty computation protocol that realizes any (efficiently computable) func-tionality against malicious adversaries in the common reference string model, based on the linear assumption over bilinear groups and the n-th residuosity assumption. Apart from theoretical interest, our result has interesting applications to the regime of leakage-resilient cryptography. At the heart of our construction is a new two-round oblivious transfer protocol secure against malicious adversaries who may receive adaptive <b>auxiliary</b> <b>information.</b> This may be of independent interest. ...|$|R
40|$|In this article, {{we propose}} and explore a multivariate {{logistic}} regression model for analyzing multiple binary outcomes with incomplete covariate data where <b>auxiliary</b> <b>information</b> is available. The auxiliary data are extraneous to the regression model of interest but predictive of the covariate with missing data. Horton and Laird [N. J. Horton, N. M. Laird, Maximum likelihood analysis of logistic regression models with incomplete covariate data and <b>auxiliary</b> <b>information,</b> Biometrics 57 (2001) 34 - 42] describe how the <b>auxiliary</b> <b>information</b> {{can be incorporated}} into a regression model for a single binary outcome with missing covariates, and hence the efficiency of the regression estimators can be improved. We consider extending the method of [9] to the case of a multivariate logistic regression model for multiple correlated outcomes, and with missing covariates and completely observed <b>auxiliary</b> <b>information.</b> We demonstrate {{that in the case of}} moderate to strong associations among the multiple outcomes, one can achieve considerable gains in efficiency from estimators in a multivariate model as compared to the marginal estimators of the same parameters. Asymptotic relative efficiency <b>Auxiliary</b> <b>information</b> Incomplete data Logistic regression model Missing covariates Multiple outcomes...|$|R
40|$|A {{technique}} is proposed {{to test for}} the presence of clusters when both spatial and <b>auxiliary</b> <b>information</b> is available. The test is based on randomizing the <b>auxiliary</b> <b>information</b> over the spatial locations. It requires no model assumptions (such as a Poisson process) for the spatial distribution. The procedure may be used to screen data for possible clustering...|$|R
40|$|We {{consider}} secure sketch {{construction in}} an asymmetric setting, that is, multiple samples are acquired during enrollment, {{but only a}} single sample is obtained during verification. Known protection methods apply secure sketch constructions on {{the average of the}} samples, while publishing the <b>auxiliary</b> <b>information</b> extracted from the set of samples, such as variances or weights of the features, in clear. Since the <b>auxiliary</b> <b>information</b> is revealed, an adversary can potentially use it to determine the relationship among multiple sketches, and gather information on the identity of the sketches. In this paper, we give a formal formulation of secure sketch under the asymmetric setting, and propose two schemes that mix the identity-dependent <b>auxiliary</b> <b>information</b> within the sketch. Our analysis shows that while our schemes maintain similar bounds of information loss compared to schemes that reveal the <b>auxiliary</b> <b>information,</b> they offer better privacy protection by limiting the linkages among sketches. 1...|$|R
40|$|Random {{processes}} are monitored over {{space and time}} by a network of stations distributed across a spatial region. <b>Auxiliary</b> <b>information</b> is often gathered {{not only at the}} stations but at other points across the region. The incorporation of <b>auxiliary</b> <b>information</b> in some interpolation techniques has show improvement on the interpolation results. The Empirical Orthogonal Functions (EOF) model is a well-known eigenvector based prediction technique widely used in meteorology and oceanography for modeling the variability of the observed spatio-temporal random process. Similarity matrices are constructed using available <b>auxiliary</b> <b>information</b> and included in the EOF model to develop a spatial interpolation method. The resulting interpolation technique will be applied to real data set and the results compared to ordinary kriging. Random {{processes are}} monitored over space and time by a network of stations distributed across a spatial region. <b>Auxiliary</b> <b>information</b> is often gathered not only at the stations but at other points across the region. The incorporation of <b>auxiliary</b> <b>information</b> in some interpolation techniques has show improvement on the interpolation results. The Empirical Orthogonal Functions (EOF) model is a well-known eigenvector based prediction technique widely used in meteorology and oceanography for modeling the variability of the observed spatio-temporal random process. Similarity matrices are constructed using available <b>auxiliary</b> <b>information</b> and included in the EOF model to develop a spatial interpolation method. The resulting interpolation technique will be applied to real data set and the results compared to ordinary kriging...|$|R
40|$|Calibration is {{commonly}} used in survey sampling to include <b>auxiliary</b> <b>information</b> to increase the precision of the estimates of population parameter. In this paper, we newly propose various calibration approach ratio estimators and derive the estimator of the variance of the calibration approach ratio estimators in stratified sampling. Calibration approach Stratified sampling Estimation of variance Ratio and regression-type estimator <b>Auxiliary</b> <b>information...</b>|$|R
40|$|International audienceThis {{paper is}} {{dedicated}} to the use of <b>auxiliary</b> <b>information</b> in order to help a classical acoustic-based speaker identification system in the specific context of TV shows. The underlying assumption is that <b>auxiliary</b> <b>information</b> could help (1) to re-rank n-best speaker hypotheses provided by the acoustic-based only speaker identification system, (2) to provide confidence score to refine a rejection process (open-set identification task), and finally, (3) to identify speakers not covered by the speaker dictionary (out-of-dictionary speakers) used by the speaker identification system (full-set verification task); the last point being one of the main issue when dealing with TV shows. In this paper, the <b>auxiliary</b> <b>information</b> is based on person names detected in overlaid text and speech. Experiments conducted in three different datasets issued from the REPERE evaluation campaign have highlighted the interest of the <b>auxiliary</b> <b>information</b> used here, and notably the use of overlaid person names to identify out-of-dictionary speakers, confirming the key assumptions made...|$|R
40|$|SI is {{generated}} at the decoder by the motion-compensated interpolation (MCI) {{from the past}} and future key frames under the assumption that the motion trajectory between the adjacent frames is translational with constant velocity. However, this assumption is not always true and thus, the coding efficiency for WZ coding is often unsatisfactory in video with high and/or irregular motion. This situation becomes more serious in low-delay applications since only motion-compensated extrapolation (MCE) can be applied to yield SI. In this paper, a spatial-aided Wyner-Ziv video coding (WZVC) in low-delay application is proposed. In SA-WZVC, at the encoder, each WZ frame is coded as performed in the existing common Wyner-Ziv video coding scheme and meanwhile, the <b>auxiliary</b> <b>information</b> is also coded with the low-complexity DPCM. At the decoder, for the WZ frame decoding, <b>auxiliary</b> <b>information</b> should be decoded firstly and then SI {{is generated}} with the help of this <b>auxiliary</b> <b>information</b> by the spatial-aided motion-compensated extrapolation (SA-MCE). Theoretical analysis proved that when a good tradeoff between the <b>auxiliary</b> <b>information</b> coding and WZ frame coding is achieved, SA-WZVC is able to achieve better rate distortion performance than the conventional MCE-based WZVC without <b>auxiliary</b> <b>information.</b> Experimental results also demonstrate that SA-WZVC can efficiently improve the coding performance of WZVC in low-delay application...|$|R
3000|$|Owing to {{the strong}} {{correlation}} between map and reference data, the exploitation of map data as <b>auxiliary</b> <b>information</b> in the D estimator proves to be highly effective {{with respect to the}} HT estimator, in which no <b>auxiliary</b> <b>information</b> is exploited. With SRSWOR the decrease in RSE varies from about 20 % to 75 % and are more marked for the P 3 population when the y [...]...|$|R
30|$|In {{this special}} issue, three {{contributions}} address different means to improve coding efficiency. In [38], Wu et al. address the shortcoming {{of the common}} motion-compensated temporal interpolation which assumes that the motion remains translational and constant between key frames. In this paper, a spatial-aided Wyner-Ziv video coding is proposed. More specifically, <b>auxiliary</b> <b>information</b> is encoded with DPCM at the encoder and transmitted along with WZ bitstream. At the decoder, SI is generated by spatial-aided motion-compensated extrapolation exploiting this <b>auxiliary</b> <b>information.</b> It is shown that the proposed scheme achieves better rate distortion performance than conventional motion-compensated extrapolation-based WZ coding without <b>auxiliary</b> <b>information.</b> It is also demonstrated that the scheme efficiently improves WZ coding performance for low-delay applications.|$|R
40|$|Given {{a program}} f and an input change ⊕, {{we wish to}} obtain an {{incremental}} program that computes f(x⊕ y) efficiently by making use {{of the value of}} f(x), the intermediate results computed in computing f(x), and <b>auxiliary</b> <b>information</b> about x that can be inexpensively maintained. Obtaining such incremental programs {{is an essential part of}} the transformational-programming approach to software development and enhancement. This paper presents a systematic approach that discovers a general class of useful <b>auxiliary</b> <b>information,</b> combines it with useful intermediate results, and obtains an efficient incremental program that uses and maintains these intermediate results and <b>auxiliary</b> <b>information.</b> We give a number of examples from list processing, VLSI circuit design, image processing, etc...|$|R
40|$|In this paper, we {{employ the}} method of {{empirical}} likelihood to construct confidence intervals for M-functionals {{in the presence of}} <b>auxiliary</b> <b>information</b> under a nonparametric setting. The modified empirical likelihood confidence intervals which make use of the knowledge of <b>auxiliary</b> <b>information</b> are asymptotically at least as narrow as the standard ones which do not utilize <b>auxiliary</b> <b>information.</b> For testing a hypothesis about a M-functional, the power of a test statistic based on the modified empirical likelihood ratio is larger than the one based on the standard empirical likelihood ratio. A simulation study is presented to demonstrate the performance of the modified empirical likelihood confidence intervals for small samples. Coverage probability Empirical likelihood ratio Hypothesis Power...|$|R
40|$|The {{estimation}} of quantiles {{in the presence}} of <b>auxiliary</b> <b>information</b> is discussed. Calibration and poststratification techniques provide simple and practical procedures for incorporating <b>auxiliary</b> <b>information</b> into the {{estimation of}} distribution functions, which can offer some useful gains in efficiency. The estimator proposed combines these techniques and possesses a number of desirable properties, including yielding a genuine distribution function, providing simplicity of computation and generalizing Silva and Skinner's estimator. This proposed procedure is compared to alternative methods. On the basis of simulation studies, the proposed post-stratified calibration estimator presents a good level of performance and comprises a valid alternative to other estimators of the distribution function. Post-stratified estimator Calibration Finite distribution function <b>Auxiliary</b> <b>information</b> Bahadur representation...|$|R
30|$|In {{distributed}} video coding, {{the side}} information (SI) quality {{plays an important}} role in Wyner-Ziv (WZ) frame coding. Usually, SI is generated at the decoder by the motion-compensated interpolation (MCI) from the past and future key frames under the assumption that the motion trajectory between the adjacent frames is translational with constant velocity. However, this assumption is not always true and thus, the coding efficiency for WZ coding is often unsatisfactory in video with high and/or irregular motion. This situation becomes more serious in low-delay applications since only motion-compensated extrapolation (MCE) can be applied to yield SI. In this paper, a spatial-aided Wyner-Ziv video coding (WZVC) in low-delay application is proposed. In SA-WZVC, at the encoder, each WZ frame is coded as performed in the existing common Wyner-Ziv video coding scheme and meanwhile, the <b>auxiliary</b> <b>information</b> is also coded with the low-complexity DPCM. At the decoder, for the WZ frame decoding, <b>auxiliary</b> <b>information</b> should be decoded firstly and then SI is generated with the help of this <b>auxiliary</b> <b>information</b> by the spatial-aided motion-compensated extrapolation (SA-MCE). Theoretical analysis proved that when a good tradeoff between the <b>auxiliary</b> <b>information</b> coding and WZ frame coding is achieved, SA-WZVC is able to achieve better rate distortion performance than the conventional MCE-based WZVC without <b>auxiliary</b> <b>information.</b> Experimental results also demonstrate that SA-WZVC can efficiently improve the coding performance of WZVC in low-delay application.|$|R
40|$|The {{problem of}} {{estimating}} the population variance is presented using <b>auxiliary</b> <b>information</b> {{in the presence}} of measurement errors. The estimators in this article use <b>auxiliary</b> <b>information</b> to improve efficiency and assume that measurement error is present both in study and auxiliary variable. A numerical study is carried out to compare the performance of the proposed estimator with other estimators and the variance per unit estimator {{in the presence of}} measurement errors...|$|R
40|$|Abstract—Finding ways of {{incorporating}} <b>auxiliary</b> <b>information</b> or <b>auxiliary</b> data into {{the learning process}} has been the topic of active data mining and machine learning research in recent years. In this work we study and develop a new framework for classification learning problem in which, in addition to class labels, the learner is provided with an <b>auxiliary</b> (probabilistic) <b>information</b> that reflects how strong the expert feels about the class label. This approach can be extremely useful for many practical classification tasks that rely on subjective label assessment and where the cost of acquiring additional <b>auxiliary</b> <b>information</b> is negligible {{when compared to the}} cost of the example analysis and labelling. We develop classification algorithms capable of using the <b>auxiliary</b> <b>information</b> to make the learning process more efficient in terms of the sample complexity. We demonstrate the benefit of the approach on a number of synthetic and real world data sets by comparing it to the learning with class labels only. Keywords-classification learning; sample complexity; learning with <b>auxiliary</b> label <b>information</b> I...|$|R
40|$|AbstractIn this article, {{we propose}} and explore a multivariate {{logistic}} regression model for analyzing multiple binary outcomes with incomplete covariate data where <b>auxiliary</b> <b>information</b> is available. The auxiliary data are extraneous to the regression model of interest but predictive of the covariate with missing data. Horton and Laird [N. J. Horton, N. M. Laird, Maximum likelihood analysis of logistic regression models with incomplete covariate data and <b>auxiliary</b> <b>information,</b> Biometrics 57 (2001) 34 – 42] describe how the <b>auxiliary</b> <b>information</b> {{can be incorporated}} into a regression model for a single binary outcome with missing covariates, and hence the efficiency of the regression estimators can be improved. We consider extending the method of [9] to the case of a multivariate logistic regression model for multiple correlated outcomes, and with missing covariates and completely observed <b>auxiliary</b> <b>information.</b> We demonstrate {{that in the case of}} moderate to strong associations among the multiple outcomes, one can achieve considerable gains in efficiency from estimators in a multivariate model as compared to the marginal estimators of the same parameters...|$|R
40|$|This paper {{introduces}} {{a new class}} of M-estimators based on generalised em-pirical likelihood (GEL) estimation with some <b>auxiliary</b> <b>information</b> available in the sample. The resulting class of estimators is e ¢ cient {{in the sense that it}} achieves the same asymptotic lower bound as that of the e ¢ cient generalised method of moment (GMM) estimator with the same <b>auxiliary</b> <b>information.</b> The paper also shows that in case of smooth estimating equations the proposed es-timators enjoy a small second order bias property compared to both e ¢ cient GMM and full GEL estimators. Analytical formulae to obtain bias corrected estimators are also provided. Simulations show that with correctly speci 8 ̆ 5 ed <b>auxiliary</b> <b>information</b> the proposed estimators and in particular those based on empirical likelihood outperform standard M and e ¢ cient GMM estimators both in terms of 8 ̆ 5 nite sample bias and e ¢ ciency. On the other hand with moder-ately misspeci 8 ̆ 5 ed <b>auxiliary</b> <b>information</b> estimators based on the nonparametric tilting method are typically charactersed by the best 8 ̆ 5 nite sample properties...|$|R
40|$|We {{consider}} {{the number of}} queries required to identify a regular set given an oracle for the set and some <b>auxiliary</b> <b>information</b> about the set. If the <b>auxiliary</b> <b>information</b> is n, the number of states of the canonical finite state acceptor for the language, then {{the upper and lower}} bounds on the number of queries are exponential in n. If the <b>auxiliary</b> <b>information</b> consists of a set of strings guaranteed to reach every live state of the canonical acceptor for the language, then the upper and lower bounds are polynomial in n {{and the size of the}} given set of strings. As a corollary, the problem considered by Pao and Carr (1978) is shown to be solvable in a polynomial number of queries...|$|R
40|$|Recent {{studies in}} speaker {{recognition}} {{have shown that}} scorelevel combination of subsystems can yield significant performance gains over individual subsystems. We explore the use of <b>auxiliary</b> <b>information</b> to aid the combination procedure. We propose a modified linear logistic regression procedure that conditions combination weights on the <b>auxiliary</b> <b>information.</b> A regularization procedure is used to control {{the complexity of the}} extended model. Several auxiliary features are explored. Results are presented for data from the 2006 NIST speaker recognition evaluation (SRE). When an estimated degree of nonnativeness for the speaker is used as <b>auxiliary</b> <b>information,</b> the proposed combination results in a 15 % relative reduction in equal error rate over methods based on standard linear logistic regression, support vector machines, and neural networks...|$|R
40|$|Small area {{estimation}} methods typically combine direct estimates from a survey with predictions from {{a model in}} order to obtain estimates of population quantities with reduced mean squared error. When the <b>auxiliary</b> <b>information</b> used in the model is measured with error, using a small area estimator such as the Fay [...] Herriot estimator while ignoring measurement error may be worse than simply using the direct estimator. We propose a new small area estimator that accounts for sampling variability in the <b>auxiliary</b> <b>information,</b> and derive its properties, in particular showing that it is approximately unbiased. The estimator is applied to predict quantities measured in the U. S. National Health and Nutrition Examination Survey, with <b>auxiliary</b> <b>information</b> from the U. S. National Health Interview Survey. Copyright 2008, Oxford University Press. ...|$|R
3000|$|... s are {{referred}} to as map data and will be used as <b>auxiliary</b> <b>information</b> in forest cover estimation, while the y [...]...|$|R
