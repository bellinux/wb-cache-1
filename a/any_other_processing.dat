22|10000|Public
50|$|The C {{preprocessor}} replaces all {{occurrences of}} the following nine trigraph sequences by their single-character equivalents before <b>any</b> <b>other</b> <b>processing.</b>|$|E
50|$|During this phase, {{the most}} {{practical}} alternatives for processing {{in case of a}} disaster are researched and evaluated. All aspects of the organization are considered, including physical facilities, computer hardware and software, communications links, data files and databases, customer services provided, user operations, the overall management information systems (MIS) structure, end-user systems, and <b>any</b> <b>other</b> <b>processing</b> operations.|$|E
50|$|It is {{possible}} to play back digital sound samples at a resolution of approximately 5-bit by sending a stream of values to the sound chip. This technique is very processor-intensive and hard to combine with <b>any</b> <b>other</b> <b>processing.</b> Examples are the title screens or other non-playable scenes of games like Chase H.Q., Meltdown, and RoboCop. The later Plus models incorporated a DMA engine in order to offload this processing.|$|E
40|$|Abstract. A novel {{design of}} the {{frequency}} invariant beamformer based on a rectangular array is proposed. There are two unique features about this design: there is no taped delay-line (TDL) or <b>any</b> <b>other</b> temporal <b>processing</b> involved and the resultant beamformer has a full 360 ◦ azimuth angle coverage. Two design examples are provided with a satisfactory frequency invariant property...|$|R
50|$|After {{the line}} trim, the signal is {{processed}} by the mixing board's EQ, filter, compressor, limiter, de-esser, delay, reverb, and <b>any</b> <b>other</b> signal <b>processing</b> features the mixing board has available {{and that the}} mix engineer chooses to use. The processed signal is then sent to the mix bus, where it is combined {{with all the other}} signals coming from the stage. The balance of signals is controlled by faders.|$|R
50|$|Often {{when the}} {{processing}} requirement is not real-time, processing is economically done with an existing general-purpose {{computer and the}} signal data (either input or output) exists in data files. This is essentially no different from <b>any</b> <b>other</b> data <b>processing,</b> except DSP mathematical techniques (such as the FFT) are used, and the sampled data is usually assumed to be uniformly sampled in time or space. For example: processing digital photographs with software such as Photoshop.|$|R
50|$|The Mockingboard (the {{name is a}} pun on the Mockingbird) is a {{sound card}} for the Apple II family of {{microcomputers}} built by Sweet Micro Systems. The standard Apple II machines never had particularly good sound, especially when compared to competitors like the SID chip-featuring Commodore 64. With the notable exception of the Apple IIGS, all an Apple II programmer could do was to form sounds out of single clicks sent to the speaker at specific moments, which made the creation of complex sounds extremely difficult to program and made it mostly impossible to do <b>any</b> <b>other</b> <b>processing</b> during the creation of sounds. Early (1978) hardware accessories such as ALF's Apple Music Synthesizer focused on producing only music. In 1981, Sweet Micro Systems began designing products not only for creating music, but speech and general sound effects as well. Its specialized hardware allowed programmers to create complex, high-quality sound without need for constant CPU attention. The Mockingboard could {{be connected to the}} Apple's built-in speaker or to external speakers. However, as the quality of the built-in speaker was not high, the instruction manual recommended obtaining external speakers.|$|E
40|$|AbstractA multidiode {{simulation}} {{determines the}} absolute solar cell efficiency gain due to gettering. The simulation predicts current/voltage characteristics of lateral inhomogeneous industrial solar cells from luminescence images measured {{before and after}} gettering or <b>any</b> <b>other</b> <b>processing</b> step. Improved lifetime distributions lead to a predicted increase in solar cell efficiency Δηabs = + 0. 46 % for a POCl 3 diffusion on multicrystalline silicon wafers. The simulative approach determines the gettering efficiency of any process step on any wafer material without preselection...|$|E
30|$|Each new {{traffic flow}} {{that belongs to}} a service class that cannot be {{supported}} by the network will be dropped by the SLA Enforcer (see Section 4.2. 2). This functionality needs to be performed in the SLA Enforcer, before <b>any</b> <b>other</b> <b>processing.</b> If this would only be done {{in any of the}} next modules, those traffic flows would unfairly be taken into account by the SLA Enforcer and add up to the consumed data volume or data rate. This would result in less bandwidth or data volume than end users are entitled to.|$|E
5000|$|The first ballot marking devices {{specifically}} designed for use in elections emerged in the late 19th century along with proposals to use various punched-card ballot forms. Kennedy Dougan filed for patents on a punched-card system using a ballot marking device in 1890. [...] Urban Iles filed {{a proposal for a}} more sophisticated system in 1892. [...] The patents for these machines suggest that their primary goal was to provide for mechanical vote tabulation while retaining paper ballots {{that could be used to}} verify the operation of the tabulator in the event of any question. The punched cards used by these early machines were not designed to be compatible with <b>any</b> <b>other</b> data <b>processing</b> equipment.|$|R
40|$|Text {{alignment}} {{and text}} quality {{are critical to}} the accuracy of Machine Translation (MT) systems, some NLP tools, and <b>any</b> <b>other</b> text <b>processing</b> tasks requiring bilingual data. This research proposes a language independent bi-sentence filtering approach based on Polish (not a position-sensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of them leverage synonyms and semantic and structural analysis of text as additional information. Minimization of data loss was ensured. An improvement in MT system score with text processed using the tool is discussed. Comment: arXiv admin note: text overlap with arXiv: 1509. 09093, arXiv: 1509. 0888...|$|R
5000|$|In Unicode, the em dash is U+2014 (decimal 8212). In HTML, one {{may use the}} numeric forms [...] {{or there}} is also the HTML entity [...] In TeX, the em dash may {{normally}} be input as a triple hyphen-minus (...) [...] On any Mac, most keyboard layouts map an em dash to [...] On Microsoft Windows, an em dash may be entered as Alt+0151, where the digits are typed on the numeric keypad while holding the Alt key down. In the X Window System, it may be entered using the compose key by pressing the compose key and three hyphens. In Microsoft Word, the standard shortcut is [...] But in this or <b>any</b> <b>other</b> word <b>processing</b> app, one can also easily create custom shortcuts, such that, for example, [...] keeps the fingers near the home row.|$|R
40|$|International audienceHigh Dynamic Range (HDR) {{images are}} common in {{computer}} graphics. In these images, speckles can appear for various reasons, such as high variance for Monte-Carlo-based rendering engines. These speckles are responsible for large artefacts if non-robust post-processing methods are used, such as tonemapping operators relying on a maximal or average luminance value. Ensuring that all post-processing operators are robust is tedious, therefore we propose to handle these speckles before <b>any</b> <b>other</b> <b>processing</b> is done. This way, we ensure that any HDR image post-processing pipeline produces acceptable results...|$|E
40|$|The thesis {{concerns}} error {{evaluation of}} target elevation in a 3 D surveillance radar using a monopulse technique. Using of monopulse technique {{is a special}} case, because the monopulse processor has to work in collaboration with other usual blocks of a surveillance radar, such as Doppler filter, pulse compression, CFAR, and others. All these signal processing techniques have influence on errors with which the elevation is evaluated. In the thesis, there is an error analysis in elevation for cases when the monopulse processor processes signals without <b>any</b> <b>other</b> <b>processing</b> and signal output by Doppler filter. Special attention {{is given to the}} case, when signals from Doppler filter output from more sweeps are used. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
30|$|The {{utility of}} PMOD and FSL to process [18 F]flutemetamol images and derive SUVR values, for {{conversion}} to Centiloid, was also assessed in this study. Both PMOD and FSL performed well, producing data that was {{comparable to the}} standard SPM 8 methods. Validation of each pipeline against the GAAIN data again gave excellent correlation {{when compared with the}} published data, with both pipelines fulfilling the required acceptance criteria. The equations derived for each method were then transformed to ‘Standard’ SPM 8 -equivalent Centiloids, allowing a true comparison of values. This shows that Centiloid scaling is robust and can be implemented on a number of platforms, not just the initially recommended SPM 8 version. Furthermore, this work provides a straightforward framework for implementation and validation of Centiloid scaling using <b>any</b> <b>other</b> <b>processing</b> pipeline.|$|E
30|$|The {{mining and}} {{cleaning}} of coal at local processing sites creates {{large quantities of}} ambient particulate matter (Ghose and Banerjee 1995; Ghose and Majee 2000). Opencast mining operations contribute major air pollutants to the atmosphere and are responsible for environmental degradation by deteriorating the air quality in respect to dust, fine coal particles and other gaseous pollutants (Mukhopadhyay et al. 2010). The major sources of air pollution in coal mining area include: drilling and blasting, loading and unloading of coal and overburden, movements of heavy vehicles on haul roads, dragline operations, crushing of coal in feeder breakers, presence of fire, exposed pit faces, wind erosion and exhaust of heavy earthmover machinery (Nair and Sinha 1987; Ghose and Majee 2007; Huertas et al. 2011). According to Ghose and Banerjee (1997), air pollution caused by washeries is more acute than <b>any</b> <b>other</b> coal <b>processing</b> operations.|$|R
40|$|Because {{atmospheric}} effects {{can have a}} significant impact on the data obtained from multi-spectral satellite remote sensing, it is frequently necessary to make corrections before <b>any</b> <b>other</b> image <b>processing</b> can be started. This paper describes a robust and relatively simple atmospheric correction method that uses pseudo-invariant targets (PITs) in conjunction with the empirical line method. The method is based on the selection of a number of suitable generic PITs, on the basis that they are large, distinctive in shape, and occur in many geographical areas. Whereas the multi-temporal normalization method corrects all images to a selected reference image, in this method images are simultaneously corrected using targets with a range of estimated surface reflectance values. The paper describes some applications of the method for a range of environmental studies involving water quality and air pollution monitoring, and mapping land-cover changes<br/...|$|R
2500|$|Keyboard shortcuts vary by {{operating}} system and by application. In TeX, the en dash may normally (depending on the font) be input as a double hyphen-minus (--). LaTeX has the macro (\textendash). On macOS, most keyboard layouts map an en dash to [...] On Microsoft Windows, an en dash may be entered as Alt+0150 (where the digits are typed on the numeric keypad while holding down the Alt key). In Linux (GTK+ v. 2.10+ applications only, see Unicode input), it is entered by holding down Ctrl+Shift and typing U followed by its Unicode code point, 2013, or using the compose key by pressing the compose key, two hyphens, and a period. In Microsoft Word, the standard shortcut is [...] But in this or <b>any</b> <b>other</b> word <b>processing</b> app, {{one can also}} easily create custom shortcuts, such that, for example, [...] keeps the fingers near the home row.|$|R
40|$|Abstract—In DMVC, we {{have more}} than one options of sources {{available}} for construction of side information. The newer techniques make use of both the techniques simultaneously by constructing a bitmask that determines the source of every block or pixel of the side information. A lot of computation is done to determine each bit in the bitmask. In this paper, we have tried to define areas that can only be well predicted by temporal interpolation and not by multiview interpolation or synthesis. We predict that all such areas that are not covered by two cameras cannot be appropriately predicted by multiview synthesis and if we can identify such areas in the first place, we don’t need to go through the script of computations for all the pixels that lie in those areas. Moreover, this paper also defines a technique based on KLT to mark the above mentioned areas before <b>any</b> <b>other</b> <b>processing</b> is done on the side view...|$|E
40|$|High {{valuable}} enantiomerically pure compounds {{are potential}} chiral {{building blocks for}} the synthesis of pharmaceutically important molecules, agrochemicals, flavors and asymmetric chiral ligands. A novel methodology coined “botanochemistry” investigates {{the possibility of using}} directly freshly cut pieces of various vegetables, without <b>any</b> <b>other</b> <b>processing</b> to perform important chemical transformations. The present work aims to optimize the time required for the preparation of the biocatalysts and to screen among fruit and vegetable organs for this purpose. It was observed that most of those biocatalysts contain specific enzymes that hydrolyze exogenous organic substrates. The red beetroot skin has been selected as the most promising biocatalyst to perform the kinetic resolution of (±) - 1 -phenylethyl acetate with excellent enantiomeric excess. The main advantage of this approach, as compared to the chemical procedures (metal- and organo-catalysis), is that the final consequences of these transformations are more ecologically friendly, economically promising and also enables the valorization of by-products issued from agriculture and food industries...|$|E
40|$|Background: Numerosity {{estimation}} {{is a basic}} preverbal {{ability that}} humans share with many animal species and that {{is believed to be}} foundational of numeracy skills. It is notoriously difficult, however, to establish whether numerosity estimation is based on numerosity itself, or on one or more non-numerical cues like—in visual stimuli—spatial extent and density. Frequently, different non-numerical cues are held constant on different trials. This strategy, however, still allows numerosity estimation to be based on a combination of non-numerical cues rather than on any particular one by itself. Methodology/Principal Findings: Here we introduce a novel method, based on second-order (contrast-based) visual motion, to create stimuli that exclude all first-order (luminance-based) cues to numerosity. We show that numerosities can be estimated almost as well in second-order motion as in first-order motion. Conclusions/Significance: The results show that numerosity estimation need not be based on first-order spatial filtering, first-order density perception, or <b>any</b> <b>other</b> <b>processing</b> of luminance-based cues to numerosity. Our method can be used as an effective tool to control non-numerical variables in studies of numerosity estimation...|$|E
40|$|Tyt. z nagłówka. Bibliogr. s. 182 - 184. Text {{alignment}} {{and text}} quality {{are critical to}} the accuracy of Machine Translation (MT) systems, some NLP tools, and <b>any</b> <b>other</b> text <b>processing</b> tasks requiring bilingual data. This research proposes a language-independent bisentence filtering approach based on Polish (not a position-sensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of the heuristics leverage synonyms as well as semantic and structural analysis of text as additional information. Minimization of data loss has been? ensured. An improvement in MT system scores with text processed using this tool is discussed. Dostępny również w formie drukowanej. KEYWORDS: statistical machine translation, NLP, comparable corpora, text filtering...|$|R
40|$|International audiencePage {{segmentation}} and classification is {{very important}} in document layout analysis system before it is presented to an OCR system or for <b>any</b> <b>other</b> subsequent <b>processing</b> steps. In this paper, we propose an accurate and suitably designed system for complex documents segmentation. This system is based on steerable pyramid transform. The features extracted from pyramid sub-bands serve to locate and classify regions into text (either machine-printed or handwritten) and non-text (images, graphics, drawings or paintings) in some noise-infected, deformed, multilingual, multi-script document images. These documents contain tabular structures, logos, stamps, handwritten script blocks, photographs, etc. The encouraging and promising results obtained on 1, 000 official complex document images data set are presented in this research paper. We compared our results with those from existing state-of-the-art methods. This comparison shows that the proposed method performs consistently well on large sets of complex document images...|$|R
5000|$|Keyboard shortcuts vary by {{operating}} system and by application. In TeX, the en dash may normally (depending on the font) be input as a double hyphen-minus (...) [...] In LaTeX {{you can also}} use the macro (...) [...] On macOS, most keyboard layouts map an en dash to [...] On Microsoft Windows, an en dash may be entered as Alt+0150 (where the digits are typed on the numeric keypad while holding down the Alt key). In Linux (GTK+ v. 2.10+ applications only, see Unicode input), it is entered by holding down Ctrl+Shift and typing U followed by its Unicode code point, 2013, or using the compose key by pressing the compose key, two hyphens, and a period. In Microsoft Word, the standard shortcut is [...] But in this or <b>any</b> <b>other</b> word <b>processing</b> app, one can also easily create custom shortcuts, such that, for example, [...] keeps the fingers near the home row.|$|R
40|$|Abstract This thesis {{describes}} {{the design and}} construction of digital multi-meter using PIC microcontroller. In this system a typical multi-meter may include features {{such as the ability}} to measure ACDC voltage DC current resistance temperature diodes frequency and connectivity. This design uses of the PIC microcontroller voltage rectifiers voltage divide potentiometer LCD and other instruments to complete the measure. When we used what we have learned of microprocessors and adjust the program to calculate and show the measures in the LCD keypad selected the modes. The software programming has been incorporated using MPLAB and PROTEUS. In this system the analogue input is taken directly to the analogue input pin of the microcontroller without <b>any</b> <b>other</b> <b>processing.</b> So the input range is from 0 V to 5 V the maximum source impedance is 2 k 5 for testing use a 1 k pot. To improve the circuit adds an op-amp in front to present greater impedance to the circuit under test. The output impedance of the op-amp will be low which a requirement of the PIC analogue input is...|$|E
40|$|Abstract — From {{the very}} {{beginning}} various measures are taken or consider for better utilization of available limited resources in the computer system for operational environment, this is came in consideration {{because most of the}} time our system get free and not able to exploit the system resource/capabilities as whole cause low performance. Parallel Computing can work efficiently, where operations are handled by multi-processors independently or efficiently, without <b>any</b> <b>other</b> <b>processing</b> capabilities. All processing unit’s works in a parallel fashioned and increases the system throughput without any resource allocation problem among different processing units. But this is limited and effective within a single machine. Today in this computing world, maintaining and establishing high speed computational work environment in a distributed scenario seems to be a challenging task because this environment made all operations by not depending on single resources but by interacting with other resources in the vast network architecture. All current resource management system can only work smoothly if they apply these resources within their clusters, local organizations or disputed among many users who needs processing power, but for vast distributed environment performing various operational activities seems to be difficult because data is physically not maintained in a centralized location, it is geographically dispersed on multiple remote computers systems. Computers in the distribute...|$|E
40|$|When {{assessing}} {{the quality of}} a workflow and, in particular, its uncertainty propagation, the management of the quality description of the components instantiated within the workflow is essential. This is the case whether one is using clients for quality visualisation or computational services for the error propagation and eventually for the execution of the workflow. This paper describes the benefits and pitfalls - in term of interoperability for data and services - when using a Web Processing Service (WPS) consuming the workflow description to derive the quality and uncertainty propagation and particularly using the meta-propagation paradigm,. The preferred solution, a metadata-profile driven web service implementation, is detailed. This solution, towards a standard interface for workflow services, has the advantage of being easily compliant with any workflow description language standard such as XPDL, BPEL or BPMN 2. 0. A change of workflow language only implies a server side adaptation for the XML parser method enabling capture of the workflow structure. This metadata-profile driven solution is illustrated by the meta-propagation of uncertainty implemented within the WPS. This interface can be used for generic access, at workflow or sub-workflow level, enabling <b>any</b> <b>other</b> <b>processing</b> upon the workflow elements and therefore allowing general management including its own execution and also data uncertainty sampling, sensitivity analysis, error propagation, visualisations, etc. N/A 30 m...|$|E
30|$|From {{early days}} of {{graphene}} research, LPE has been anticipated as the most desirable mass-production method for graphene. The principal attraction of this method is that, it is a straightforward and scalable process where pristine graphite or expandable graphite (obtained by thermal or microwave expansion of graphite intercalation compounds) is directly subjected to a solvent treatment to weaken the van der Waals attractive forces between graphene interlayers. External driving force such as ultrasonication, electric field or shearing {{can be applied to}} facilitate the spontaneous exfoliation into graphene sheets. Another significant advantage of this method is the production of exfoliated graphene sheets in the form of solvent suspension that allows an immediate utilization for spin-coating, spray painting or <b>any</b> <b>other</b> solution <b>processing.</b> For instance, simple vacuum filtration of the as-obtained graphene suspensions can be used for the fabrication of thin films with high conductivities [19]. Novel graphene/polymer composites can be easily prepared by direct solution mixing. As such, LPE method addresses all crucial prospects for viable industrial applications.|$|R
40|$|Abstract. For the machine-reading task of {{biomedical}} texts {{about the}} Alz-heimer disease {{we have used}} a Question-Answering approach by adapting func-tionalities of Question-Answering (Q-A) engine EAGLi. We didn’t involve <b>any</b> <b>other</b> Natural Language <b>Processing</b> method. As a knowledge store we used the biggest resource of biomedical literature- MEDLINE. Our final results showed that the best run was without using the filter of “stop words ” in queries. Run 1 and Run 2 provided answers to all 40 Question, while Run 3 and 4 provided an-swers to 5 questions; Run 5 answered to 6 questions. These results can be tenta-tively explained by the limits of the Boolean search we chose in the Q-A en-gine...|$|R
40|$|An {{important}} {{proportion of}} {{the information about the}} medications a patient is taking is mentioned only in narrative text in the electronic health record. Automated information extraction can make this information accessible for decision-support, research, or <b>any</b> <b>other</b> automated <b>processing.</b> In the context of the “i 2 b 2 medication extraction challenge, ” we have developed a new NLP application called Textractor to automatically extract medications and details about them (e. g., dosage, frequency, reason for their prescription). This application and its evaluation with part of the reference standard for this “challenge ” are presented here, along with an analysis of the development of this reference standard. During this evaluation, Textractor reached a system-level overall F 1 -measure, the reference metric for this challenge, of about 77 % for exact matches. The best performance was measured with medication routes (F 1 -measure 86. 4 %), and the worst with prescription reasons (F 1 -measure 29 %). These results are consistent with the agreement observed between human annotators when developing the reference standard, and with other published research...|$|R
40|$|Surfaces {{coated with}} {{nanoscale}} filaments such as silicon nanowires and carbon nanotubes are potentially compelling for high‐performance battery and capacitor electrodes, photovoltaics, electrical interconnects, substrates for engineered cell growth, dry adhesives, and other smart materials. However, {{many of these}} applications require a wet environment or involve wet processing during their synthesis. The capillary forces introduced by these wet environments can lead to undesirable aggregation of nanoscale filaments, but control of capillary forces can enable manipulation of the filaments into discrete aggregates and novel hierarchical structures. Recent {{studies suggest that the}} elastocapillary self‐assembly of nanofilaments can be a versatile and scalable means to build complex and robust surface architectures. To enable a wider understanding and use of elastocapillary self‐assembly as a fabrication technology, we give an overview of the underlying fundamentals and classify typical implementations and surface designs for nanowires, nanotubes, and nanopillars made {{from a wide variety of}} materials. Finally, we discuss exemplary applications and future opportunities to realize new engineered surfaces by the elastocapillary self‐assembly of nanofilaments. New insights in capillary interactions between nanofilaments have led to versatile and scalable methods to build complex structures that cannot be achieved by <b>any</b> <b>other</b> <b>processing</b> technique. Understanding the control of this process is conducive to the development of high‐performance battery and capacitor electrodes as well as photovoltaics, electrical interconnects, and other smart materials...|$|E
40|$|Abstract Leachate {{treatment}} {{is an alternative}} in order to apply the concept of environmental management. Mojorejo landfill {{is the only place}} for district final processing in Sukoharjo. Mojorejo landfill has a leachate processing plant (IPL) which consists of 4 pieces leachate retention ponds, without <b>any</b> <b>other</b> <b>processing</b> installations. This installation is no longer functioning because most of the surface of the pool has been covered by heaps of garbage. The checking laboratory results from samples taken shortly on May 2, 2011, showed that the quality of leachate in the IPL exceeded the quality standard outlet. Parameters which still exceeded the quality standard is TSS (1130 mg /L), TDS (12, 700 mg /L), Ammonia (926. 8 mg /L), BOD (270 mg /L) and COD (3333. 33 mg/L). The problem increases with the presence of leachate drainage system is not functioning properly so that not all of the leachate to flow into the IPL. In planning the distribution and processing leachate consists of several stages, namely: 1) Planned pipeline from the zone of landfill leachate to the leachate processing tub. 2) Determine the optimum alternative leachate treatment 3) Designing and building processing leachate into account construction costs. Leachate processing buildings must be able to process the leachate so that the effluent coming out in accordance with the applicable quality standard Java Government Regulations No. 10 Tahun 2004. Key words: leachate, the characteristics of leachate, leachate treatment unit...|$|E
40|$|Large high-dimensional {{datasets}} {{are becoming}} more and more popular in an increasing number of research areas. Processing the high dimensional data incurs a high computational cost and is inherently inefficient since many of the values that describe a data object are redundant due to noise and inner correlations. Consequently, the dimensionality, i. e. the number of values that are used to describe a data object, needs to be reduced prior to <b>any</b> <b>other</b> <b>processing</b> of the data. The dimensionality reduction removes, in most cases, noise from the data and reduces substantially the computational cost of algorithms that are applied to the data. In this thesis, a novel coherent integrated methodology is introduced (theory, algorithm and applications) to reduce the dimensionality of high-dimensional datasets. The method constructs a diffusion process among the data coordinates via a random walk. The dimensionality reduction is obtained based on the eigen-decomposition of the Markov matrix that is associated with the random walk. The proposed method is utilized for: (a) segmentation and detection of anomalies in hyper-spectral images; (b) segmentation of multi-contrast MRI images; and (c) segmentation of video sequences. We also present algorithms for: (a) the characterization of materials using their spectral signatures to enable their identification; (b) detection of vehicles according to their acoustic signatures; and (c) classification of vascular vessels recordings to detect hyper-tension and cardio-vascular diseases. The proposed methodology and algorithms produce excellent results that successfully compete with current state-of-the-art algorithms. Comment: PhD Thesis, Tel Aviv Univ, 200...|$|E
40|$|Additive Manufacturing (AM), also {{commonly}} known as 3 D Printing or Rapid Prototyping, is a method of manufacturing that provides for {{the ability to make}} intricate internal features and easily customizable parts. The concept is to break a Computer Aided Design (CAD) file into a series of thin layers that are sent to the machine and laid down one layer at a time. Just like <b>any</b> <b>other</b> form of <b>processing,</b> material properties can alter by undergoing this process. Manipulating various parameters of the AM process can allow for different properties to be achieved. For this reason, an in depth study will be done by Lawrence Livermore National Laboratory to discover what parameters are relevant, how those parameters interact with one another, and what affect those parameters have on the material...|$|R
50|$|Some small translators {{operated}} by direct conversion {{of a parent}} station's signal to another frequency for rebroadcast, without <b>any</b> <b>other</b> local signal <b>processing</b> or demodulation. For example, W07BA, a 16-watt repeater for Syracuse, New York station WSYR-TV, was by design a very simple piece of broadcast apparatus; it merely shifted the main station's signal from channel nine to channel seven to cover a small valley in Dewitt. After digital transition, Syracuse became a UHF island and WSYR-TV's main ABC signal became a 100 kW digital broadcast on channel 17. Therefore, {{there is no longer}} a channel 9 signal in any format available to feed the tiny repeater. Translators in remote locations, where no commercial power is available, were also expected to have problems in deploying extra equipment to handle an uplink's digital conversion. While many translators continue analog broadcasts (and a minority transitioned to digital themselves), some distant rural communities expected to find all local translator signals gone as a result of originating stations' transition to digital.|$|R
40|$|Previous reports {{highlighted}} the onion solid wastes as abundant, residual material that might contain a significant load of antioxidant polyphenols. Although {{there have been}} studies pertaining to polyphenol recovery from onion wastes, the effect of temperature has not been adequately addressed. In this line, this study was undertaken {{with the aim of}} establishing a correlation between the extraction yield in total polyphenols and the extraction temperature, using acidified aqueous ethanol as the solvent system. Extraction of polyphenols from onion solid wastes was found to obey 2 nd -order kinetics. On such a basis, the yield in total polyphenols at saturation could be very effectively determined and correlated with temperature using non-linear regression. The results indicated that the extraction yield at saturation is highly correlated with temperature, following a quadratic function. The extract obtained at optimal temperature (40 °C) had a total polyphenol yield of 21. 10 mg gallic acid equivalents per gram of dry weight, and it was further analysed by liquid chromatography-mass spectroscopy to characterise its major constituents. The polyphenols detected were quercetin glucosides, as well as quercetin oxidation derivatives, including certain degradation products and dimers. The outcome of this study outlined that temperatures above 40 °C are rather not favourable for polyphenol extraction from onion solid wastes, as suggested by the model established through kinetics. The extract obtained under optimal conditions contained peculiar polyphenolic composition, not encountered in <b>any</b> <b>other</b> food <b>processing</b> residue. ...|$|R
