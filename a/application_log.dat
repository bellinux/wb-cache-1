19|424|Public
50|$|JSON is {{a popular}} format for exchanging object data between systems. Frequently there's {{a need for a}} stream of objects to be sent over a single connection, such as a stock ticker or <b>application</b> <b>log</b> records. In these cases there's a need to {{identify}} where one JSON encoded object ends and the next begins. Technically this is known as framing.|$|E
50|$|Note {{that this}} {{transformation}} {{does not address}} durability. Gbcast treats durable state as a property of the application, not the protocol, whereas Paxos logs events {{to a set of}} durable command logs, and hence can still recover its state even after the whole service is shut down and restarted. The equivalent behavior with Gbcast involves having the <b>application</b> <b>log</b> all received messages, but that case will not be considered here.|$|E
5000|$|The {{second tier}} {{consists}} of Element Management Systems (EMSs) which are computerized operations systems (OSs) used {{to manage the}} elements {{that are in the}} first tier. The different EMSs are collectively called FASTAR Element Management Systems (FASTEMS). The two major FASTEMS are the DACS Element Management Systems (DEMS) and the RNC Element Management Systems(RNC-EMS). DEMS is designed to assist NOC with management of DACSs. In the event of a change in the status of the network due to a fiber failure, RAPID forwards this status change to DEMS, which triggers DEMS to isolate the problem. The RNC-EMS monitors the RNCs directly via the data communication network and indirectly monitors the RTE, LTE, and DASC III, and their links to the RNC, via agents residing in the RNC. It consists two components: the manager and the agent. The manager software daemon (NMd) runs on the RNC-EMS machine and is responsible for polling the RNCs. Every RNC is polled twice, once over each of the data communication networks. The agent software daemon (NAd) runs on every RNC as part of the application software. It accesses the RNC <b>application</b> <b>log</b> to respond to manager queries, and has the ability to send autonomous alarms to the manager.|$|E
40|$|As threats {{shift toward}} {{applications}} and as more companies struggle with compliance mandates, {{the need for}} useful, comprehensive <b>application</b> <b>logging</b> can only increase. Here we provide guidance on <b>application</b> <b>logging</b> to <b>application</b> developers and architects and to security professionals. <b>Application</b> <b>Logging</b> Today Organizations have finally gotten network device logging and—to some extent—server logging under control. However, after getting used to neat Cisco Adaptive Security Appliance or other firewal...|$|R
50|$|Each <b>application</b> <b>logs</b> into CCOW {{using its}} secret {{passcode}} (and unique application name).|$|R
50|$|A Logger is {{an object}} that allows the <b>application</b> to <b>log</b> without regard to where the output is sent/stored. The <b>application</b> <b>logs</b> a message by passing an object or an object and an {{exception}} with an optional severity level to the logger object under a given a name/identifier.|$|R
40|$|Abstract—Application {{recovery}} in Mobile Database Systems (MDS) {{is more complex}} because of an unlimited geographical mobility of mobile units. The mobility of these units makes it tricky to store <b>application</b> <b>log</b> and access it for recovery. This paper presents an <b>application</b> <b>log</b> management scheme, which uses a mobile-agent-based framework to facilitate seamless logging of application activities for recovery from transaction or system failure. We compare the performance of our scheme with lazy, pessimistic, and frequency-based schemes through simulation and show that compared to these schemes, our scheme reduces overall recovery time by efficiently managing resources and handoffs. Index Terms—MDS, mobile agents, PCS, coordinators, log unification, recovery...|$|E
40|$|The paper {{discusses}} {{our research}} in development of general and systematic methods for intrusion prevention. The key {{idea is to}} use data mining techniques to discover repeated patterns of system features that describe program and user behavior. Server systems customarily write comprehensive activity logs whose value is useful in detecting intrusion. Unfortunately, production volumes overwhelm the capacity and manageability of traditional approach. This paper discusses the issues involving largescale log processing that helps to analyze log records. Here, we propose to analyze intersections of firewall log files with <b>application</b> <b>log</b> files installed on one computer, as well as intersections resulting from firewall log files with <b>application</b> <b>log</b> files coming from different computers. Intersections of log files are substantially shorter than full logs and consist of records that indicate abnormalities in accessing single computer or set of computers. The paper concludes with some lessons we learned in building the system...|$|E
40|$|Because in the {{learning}} process at the Faculty of Computer and Information science, Moodle web application is used and when it is accessed via web browser on mobile device, large amount of irrelevant information for user is transferred and as nowadays almost every mobile application uses data transfer for their normal operation, {{it is important that the}} amount of data transferred is reduced. This mobile application was developed, to solve the problem with large amount of transferred data, which in addition to the Moodle web <b>application</b> <b>log</b> in, it also allows advertising. Later upgrades could include other functionalities important for students. Because this mobile application uses advertising it was necessary to create web application to allow creation and managing this advertising messages and web services to access them. This thesis is divided into three parts. In the initial part the mobile application development is described which allows the Moodlee web <b>application</b> <b>log</b> in. This part also describes how advertising messages are sorted. The second part describes web application for adding and managing advertisement messages and how is for its safety taken care of. The third part describes how web services are made and how do they work. ...|$|E
30|$|The logger <b>application</b> <b>logs</b> power-related records {{using the}} smart battery {{interface}} inside the device. The datasets logged overtime are {{then used to}} develop power models using neural network techniques.|$|R
40|$|This paper {{deals with}} the problem of {{analyzing}} <b>application</b> event <b>logs</b> in relevance to dependability evaluation. We present the significance of <b>application</b> <b>logs</b> as a valuable source of information on operational profiles, anomalies and errors. They can enhance classical approaches based on monitoring system logs and performance variables. Keywords; event monitoring, operational profiles, anomaliesComment: 2 pages in two column format, will be presented as FasAbstract during EDCC 2012 conference, May 8 - 11, 201...|$|R
40|$|Abstract—Trustworthy system <b>logs</b> and <b>application</b> <b>logs</b> {{are crucial}} for digital forensics. Researchers have {{proposed}} different security mechanisms {{to ensure the}} integrity and confidentiality of logs. However, applying current secure logging schemes on heterogeneous formats of logs is tedious. Here, we propose FAL, a domain-specific language (DSL) through which we can apply a secure logging mechanism on any format of logs. Using FAL, we can define log structure, which represents the format of logs and ensures the security properties of a chosen secure logging scheme. This log structure can be later used by FAL to serve two purposes: {{it can be used}} to store system logs securely, and it will help application developers for secure <b>application</b> <b>logging</b> by generating required source code...|$|R
30|$|Precursor or Indication Sources: The cloud {{provider}} deploys, maintains and manages the cloud infrastructure. The provider also develops required security sensors, logging and monitoring mechanisms to gather enough data for incident detection and analysis at the infrastructure level. As an example, security agents, intrusion monitoring sensors, <b>application</b> <b>log</b> files, report repository, firewall statistics and logs {{are all part}} of security relevant indication sources. In case of a security incident, the {{cloud provider}} should provide raw data from these sources to affected customers and stakeholders. Thus they will be capable of analyzing raw data and characterizing incident properties.|$|E
40|$|The paper {{reports on}} a set of non-intrusive tools {{that can be used for}} the {{performance}} evaluation of "classical" Internet applications such as Email, WWW, and FTP. At the application layer, key factors influencing the events are stochastic aspects of user behavior, protocol characteristics and sizes of contents transferred. A multi pronged approach involving <b>application</b> <b>log</b> analysis, direct probe of the WWW pages, and network flow analysis has been used. The measurement infrastructure is presented and diverse software tools for gathering of application layer properties are described in detail. The motivation is to build stochastic models for the key elements of application layer that can be further used to build up an integrated end-to-end performance testbed for conducting traffic engineering experiments...|$|E
40|$|AbstractPersonal {{computer}} {{is one of}} the indispensable tools at work and in everyday life. Some of application programs in the computer are habitually used or launched in a particular time. Even though their invocations can be predicted in advance, they are executed manually in each time, hence, this results in a deterioration of the usability in computer operation. In this paper, a considerable application prediction system with Artificial Neural Network, which recommends a useful application for the user at the time, will be proposed. It refers to an application ontology and uses an <b>application</b> <b>log</b> obtained from the user's personal computer. Moreover, the effectiveness of the proposed system will be discussed showing the prediction accuracy of about 90 % in recommending useful applications when the user utilizes the computer in daily life...|$|E
30|$|We {{collected}} smartphone {{usage data}} including call logs, SMS <b>logs,</b> location <b>logs,</b> <b>application</b> usage <b>logs,</b> music play list history, physical activity logs, and battery charging logs from 24 participants {{for over a}} month.|$|R
2500|$|Additionally [...] "c:geo - opensource" [...] {{is a free}} opensource full {{function}} application for Android phones that is very popular. [...] This app includes similar features to the official Geocaching mobile application, such as: View caches on a live map (Google Maps or OpenStreet Maps), navigation using a compass, map, or other <b>applications,</b> <b>logging</b> finds online and offline, etc.|$|R
50|$|A cloud {{security}} {{operations center}} (CloudSOC) may be set up to monitor cloud service use within an enterprise (and keep the Shadow IT problem under control), or parse and audit IT infrastructure and <b>application</b> <b>logs</b> via SIEM technologies and machine data platforms (such as LogRhythm, Splunk, IBM QRadar, HP ArcSight, CYBERShark and Elastica) to provide alerts and details of suspicious activity.|$|R
40|$|Many {{applications}} {{within the}} Flexyz network generate {{a lot of}} log data. This data used {{to be difficult to}} reach and search. It was therefore not used unless a problem was reported by a user. One goal of this project was to make this data available in a single location so that it becomes easy to search and visualize it. Additionally, automatic analysis can be performed on the log data so that problems can be detected before users notice them. This analysis is the core of this project and is the topic of a case study in the domain of <b>application</b> <b>log</b> data analysis. We compare four algorithms that take different approaches to this problem. We perform experiments with both artificial and real world data. It turns out that the relatively simple KNN algorithm gives the best performance, although it still produces a lot of false positives. However, {{there are several ways to}} improve these results in future research...|$|E
40|$|Upon {{completion}} of this chapter, {{you will be}} able to: 1. Discuss the major steps in developing an EC application. 2. Describe the major EC applications and list their major functionalities. 3. List the major EC application development options along with their benefits and limitations. 4. Discuss various EC application outsourcing options, including application service providers (ASPs), software as a service (SaaS), and utility computing. 5. Discuss the major EC software packages and EC application suites. 6. Describe various methods for connecting an EC application to back-end systems and databases. 7. Discuss the value and technical foundation of Web Services and their evolution into second-generation tools in EC applications. 8. Understand service-oriented architecture (SOA) and virtualization and their relationship to EC application development. 9. Describe the criteria used in selecting an outsourcing vendor and package. 10. Understand the value and uses of EC <b>application</b> <b>log</b> files. 11. Discuss the importance of usage analysis and site management...|$|E
40|$|This dataset {{contains}} {{training and}} competition EEG data (GDF format) and <b>application</b> <b>log</b> files {{for the two}} pilots of the Brain Tweakers team in the Cybathlon BCI race event. The Cybathlon is the first international competition for bionic technologies. It was held in Zurich, Switzerland in October 2016 and encompassed the BCI race event, where 4 disabled contestants were driving their avatar by means of 3 mental commands towards the finish line of a race track. Pilot MA 25 VE was {{the winner of the}} competition and pilot AN 14 VE the recordman. More information on the event can be found at: [URL] All EEG data were recorded with a gTec gUSBamp 16 -channel system at 512 Hz on scalp locations (10 - 20 system, in order) : Fz,FC 3,FC 1,FCz,FC 2,FC 4,C 3,C 1,Cz,C 2,C 4,CP 3,CP 1,CPz,CP 2,CP 4. The brain-computer interface used was a motor-imagery BCI. This dataset will be linked to a publication upon acceptance of the latter. For more information on the dataset, please contact the authors...|$|E
40|$|The field {{collection}} of data for precision agriculture applications is well suited for a handheld computer connected to a GPS receiver. An integrated suite of data collection tools has been developed by the University of Kentucky based on ArcPad 6 technology using ArcPad Application Builder. The tool suite includes field boundary and feature mapping, crop scouting features, grid soil sampling tools, and nutrient and pesticide <b>application</b> <b>logging...</b>|$|R
5000|$|Fluentd is marketed for {{big data}} for semi- or {{unstructured}} data sets Apache Kafka, it analyzes event <b>logs,</b> <b>application</b> <b>logs,</b> and clickstreams. According to Suonsyrjä and Mikkonen, the [...] "core idea of Fluentd {{is to be}} the unifying layer between different types of log inputs and outputs." [...] According to its official website, Fluentd is available on Linux and Mac OSX and experimentally on Windows.|$|R
50|$|Manual or {{automated}} system or <b>application</b> transaction <b>logs</b> should be maintained, which record all processed system commands or application transactions.|$|R
40|$|Abstract — Event logs or {{log files}} form an {{essential}} part of any network management and administration setup. While log files are invaluable to a network administrator, the vast amount of data they sometimes contain can be overwhelming and can sometimes hinder rather than facilitate the tasks of a network administrator. For this reason several event clustering algorithms for log files have been proposed, one of which is the event clustering algorithm proposed by Risto Vaarandi, on which his Simple Log file Clustering Tool (SLCT) is based. The aim of this work is to develop a visualization tool {{that can be used to}} view log files based on the clusters produced by SLCT. The proposed visualization tool, which is called LogView, utilizes treemaps to visualize the hierarchical structure of the clusters produced by SLCT. Our results based on different <b>application</b> <b>log</b> files show that LogView can ease the summarization of vast amount of data contained in the log files. This in turn can help to speed up the analysis of event data in order to detect any security issues on a given application. I...|$|E
40|$|In {{automatic}} {{speech recognition}} (ASR) <b>application,</b> <b>log</b> likelihood ratio testing (LRT) {{is one of the}} most popular techniques to obtain confidence measure (CM). Unlike traditional (log likelihood ratio) LLR related method, we apply non-linear transformations towards LLRs before computing string-level CMs. Different phonemes may have different transformation functions. Through suitable LLR transformations, the verification performances of those string-level CMs may increase. Transformation functions are implemented by Multi Layer Perceptron (MLP). Two algorithms are used to optimize the parameters of MLPs: One is the Minimum Verification Error (MVE) algorithm [2]; another is the Figure-of-Merit (FOM) training algorithm [3]. In our mandarin command recognition system, the two methods remarkably improve the performances of confidence measures for out-ofvocabulary words rejection compared with the performances of standard LRT related CMs, and we obtain a best 45. 5 % relative reduction in equal error rate (EER). In addition, in our mandarin command recognition experiments, the FOM training algorithm outperforms the MVE algorithm even they share an approximately same best performance, while due to limited experimental setups in our experiments, which algorithm is the better still needs to be explored. 1...|$|E
40|$|Mobile devices have, in many ways, {{replaced}} traditional desktops in usability, usefulness, and availability. Improvements to computational power, battery life, device capabilities, and {{user experience}} {{will continue to}} drive people to stop using desktops and solely use mobile devices. Applications are vital to maximize usefulness of these devices. Development of these applications proceeds with a rapidity that surpasses the development pace of the devices themselves. Current methods are inadequate when attempting to verify and validate {{the behavior of the}} applications to ensure they perform correctly as the customer expect and correctly with respect to the software specifications. The current V&V methods are limited to environments that do not reflect the typical operational environment for mobile devices. These methods lead to false beliefs that the results of V&V tests prove correctness of the software, when they are only proving that the software works in a non-mobile environment. To solve this problem, we propose that <b>application</b> <b>log</b> files be used to capture the execution behavior while operating in their typical environment. The log file along with customer requirements, represented formally as statechart assertions, will provide a mechanism to conduct automated V&V on the behavior of the application while it was operating in its planned, mobile environment. Lieutenant, United States Nav...|$|E
40|$|Climatic {{conditions}} in recent years, table grapes grown in INCDBH Ştefăneşti-Arges have suffered greatly. Thus, after {{the winter of}} 2011 - 201, were most affected, after frosts in February 2012 (- 20. 9 ° C). To avoid loss of production must improve technology applied in plots with table grape varieties. In this paper, these improvements were the differentiated <b>application</b> <b>logging,</b> leading to increased production {{quantity and quality of}} table grapes. Varieties take...|$|R
40|$|<b>Application</b> console <b>logs</b> are a {{ubiquitous}} {{tool for}} diagnosing system failures and anomalies. While several techniques exist to interpret logs, describing and assessing log quality remains relatively unexplored. In this paper, we describe an abstract graphical representation of console logs called the identifier graph and a visualization {{based on this}} representation. Our representation breaks logs into message types and identifier fields and shows the interrelation between the two. We describe two applications of this visualization. We apply it to Hadoop logs from two different deployments, showing that we capture important properties of Hadoop’s logging as well as relevant {{differences between the two}} sites. We also apply our technique to logs from two other systems under development. We show that our representation helps highlight flaws in the underlying <b>application</b> <b>logging...</b>|$|R
30|$|For further {{analysis}} of the usability of our AR application, we asked {{the participants in the}} AR group to answer the HARUS which measures general system usability, ease of handling the AR application, and ease of understanding the portrayed information. Lastly, both AR and non-AR <b>applications</b> <b>logged</b> time-stamped button pushes, words studied, and tablet acceleration and orientation based on the built-in sensors. We did not notice any burden on the application due to the system logging even after extended use.|$|R
40|$|Abstract. Software {{log file}} {{analysis}} is involved heavily in both Software development and maintenance phases. It serves for various purposes such as verifying the conformance {{of the software}} functionality to the specification, software quality check and troubleshooting. <b>Application</b> <b>log</b> files or the logs generated by other monitoring tools are subjected to analysis for extracting {{information that can be}} vital in an investigation. These tasks demand expertise to a great deal and are labor intensive when performed manually. The lack of a commonly used technique to record expert knowledge stands as an impediment to automate the analysis tasks. The need for correlating information extracted from different locations in the same log file or multiple log files further ads to this complexity. This paper describes a framework based on mind maps which formulates a homogeneous platform for recording expert knowledge as well as for performing other tasks such as extracting information from log files, drawing inferences and creating reports. The framework includes a scripting language, a parallel application programming interface and a set of tools. Usage is illustrated by a proof of concept system built using the framework that creates a useful report after analyzing a log file generated by a widely used software monitoring tool...|$|E
40|$|This thesis {{examines}} {{the utility of}} automated image registration techniques developed by the author. The major thrusts of this research include using the Laplacian of Gaussian (LoG) filter to automatically determine ground control points (GCPs) and wavelet theory for multiresolution analysis. Additionally, advances in both composite and predictive transformations will be covered. The defense will include {{an overview of the}} processes involved in general image registration and specifically how they pertain to automation with the techniques utilized in this thesis. Use of the LoG filter to extract semi- invariant GCPs, development of automated point matching schemas, and the use of matrix transformations for efficient management of affine image relationships will be explained in detail. Additionally, the ability to apply statistical analysis to both local and image wide sets of GCPs will be discussed. The student developed software <b>application,</b> <b>LoG</b> Wavelet Registration (LoGWaR). will demonstrate the utility of these techniques for processing large datasets such as LANDSAT and how integration of these features can provide both power and flexibility when registering multiresolution and/or multisensor images. Automation techniques will be highlighted, demonstrating the strengths and weaknesses when applied to images with high degrees of parallax, cloud-cover, and other types of temporal change. Specific applications, such as 2 ̆ 2 waveletsharpening 2 ̆ 2 and 2 ̆ 2 spectral will be addressed as it pertains to current research...|$|E
40|$|Flaws in system {{security}} may persist {{for the foreseeable}} future. Yet, software developers and system administrators are not learning from security mistakes because identifying {{the cause of a}} computer intrusion is time-consuming, tedious and unlikely to yield definitive results. Investigations are fraught with data volatility, privacy and legal issues as well. When intrusions are detected, computer forensics analysts are swamped in evidence because of the large volume of data encountered, the dearth of trained investigators and the lack of automated techniques to analyze computer crime data. An expert system with a decision tree that uses predetermined invariant relationships between redundant digital objects (like an <b>application</b> <b>log</b> entry and an audit trail) to detect semantic incongruities could augment a computer crime investigator's efforts. By analyzing data from a system and searching for violations of known data relationships, an attacker's changes to the system may be automatically identified. Examples of such invariant data relationships are provided, as are techniques to identify new, useful ones. A requirement for such a system is to have the evidence available in a standard machine-readable format. A prototype of this general approach has been written, integrating The Coroner's Toolkit and JESS, The Expert System Shell for the Java Platform, that automatically identifies files that have been modified, accessed or changed when their owners were not logged in. By automatically identifying relevant evidence, experts can focus on the relevant files, users, times and other facts first...|$|E
40|$|Intrusion {{detection}} {{relies on}} the information provided {{by a number of}} sensors deployed throughout a protected network. Sensors operate on different event streams, such as network packets and <b>application</b> <b>logs,</b> and provide information at different abstraction levels, such as low-level warnings and correlated alerts. In addition, sensors range from lightweight probes and simple log parsers to complex software artifacts that perform sophisticated analysis. Therefore, deploying, configuring, and managing, a large number of heterogeneous sensors is a complex, expensive, and error-prone activity...|$|R
40|$|Lots of {{interesting}} content on the web. How can these content creators be rewarded? Help {{not only the}} most famous sites, but also the small content creators that contribute interesting things. Tuesday, May 25, 2010 Tipsy: How does it work? Content creators embed some RDFa on their webpages describing the content and how they’d like to get paid Browser <b>application</b> <b>logs</b> websites visited runs a lottery after a given time period pays the winner an allotted amount of money via PayPa...|$|R
40|$|Abstract. There is {{currently}} a trend in giving access to users of online services to their own data. In this paper, we consider in particular the data which is generated from the interaction between a user and an organisation online: activity data as held in websites and Web <b>applications</b> <b>logs.</b> We show how we use semantic technologies including RDF integration of log data, SPARQL and lightweight ontology reasoning to aggregate, integrate and analyse activity data from a user-centric point of view. ...|$|R
