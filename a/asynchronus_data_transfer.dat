0|10000|Public
40|$|This paper {{presents}} the architecture, implementation {{and evaluation of}} IFTD, a multiprotocol <b>data</b> <b>transfer</b> service. The architecture allows a single implementation of a <b>data</b> <b>transfer</b> protocol {{to be used by}} any local application for its <b>data</b> <b>transfer</b> needs. This way, IFTD separates the development of application logic from <b>data</b> <b>transfer</b> logic, allowing application developers to avoid re-implementing <b>data</b> <b>transfer</b> protocols for each additional application. Also, applications that use IFTD will immediately benefit from additional <b>data</b> <b>transfer</b> protocols added to it. This paper first provides an argument for the design considerations and requirements for creating IFTD based upon common features of real-world <b>data</b> <b>transfer</b> scenarios. It then gives a discription of how IFTD meets these requirements, and then assesses its effectiveness at performing <b>data</b> <b>transfers</b> in a series of four performance evaluations with respect to the performances of existing <b>data</b> <b>transfer</b> software. Finally, it discusses additional considerations that may be used to algorithmically determine the best <b>data</b> <b>transfer</b> protocol for given data and a given transfer history. ...|$|R
40|$|<b>Data</b> <b>transfer</b> is an {{indispensable}} step that is widely {{involved in the}} maintenance and processing of Cloud data. Due to rapid growth in Cloud data, methods of reducing the huge energy consumption of <b>data</b> <b>transfer</b> in the Cloud have become a challenge. In this paper, we propose a novel energy-efficient <b>data</b> <b>transfer</b> strategy called LRCDT (Link Rate Controlled <b>Data</b> <b>Transfer).</b> By scheduling bandwidth in a link rate controlled fashion, LRCDT intends to reduce the energy consumption specifically for <b>data</b> <b>transfer</b> that {{does not require the}} maximum transfer speed, which is referred to as 'lazy' <b>data</b> <b>transfer,</b> so to achieve the energy-efficient <b>data</b> <b>transfer</b> goal in the overall term. The result in our simulation indicates that LRCDT is able to reduce energy consumption by up to 63 % when compared to existing <b>data</b> <b>transfer</b> strategies...|$|R
30|$|A GPGPU {{execution}} {{consists of}} three phases: <b>data</b> <b>transfer</b> to GPU, kernel execution, and <b>data</b> <b>transfer</b> back from GPU to CPU phase after kernel execution. We would like to note that we have deliberately overlapped <b>data</b> <b>transfer</b> and GPU execution phases in the time measurement {{to take into account}} the overhead of GPU-CPU <b>data</b> <b>transfer.</b>|$|R
50|$|UDT {{is widely}} used in {{high-performance}} computing to support high-speed <b>data</b> <b>transfer</b> over optical networks. For example, GridFTP, a popular <b>data</b> <b>transfer</b> tool in grid computing, has UDT available as a <b>data</b> <b>transfer</b> protocol.|$|R
5000|$|A high {{performance}} <b>Data</b> <b>Transfer</b> Node (DTN) running parallel <b>data</b> <b>transfer</b> {{tools such as}} GridFTP ...|$|R
40|$|AbstractMemory {{storage devices}} are getting cheaper,and are coming with high {{capacity}} non-volatile in nature. The significance of high capacity flash {{is to have}} more dataand there will be frequent huge <b>data</b> <b>transfer</b> occurring between these devices to computer. It is also demand of time {{that there should be}} faster <b>data</b> <b>transfer</b> between these devices, otherwise use of high capacity feature will go in vague. Present paper will undergo the concept of Efficient <b>Data</b> <b>Transfer</b> system using which performance improvement in <b>data</b> <b>transfer</b> can be achieved based on EDT hardware and software logic throughDMA <b>data</b> <b>transfer...</b>|$|R
5000|$|... 500 kB/sec <b>data</b> <b>transfer</b> rate (uncompressed) / 1.0 MB/sec <b>data</b> <b>transfer</b> rate (assuming a 2:1 {{compression}} ratio) ...|$|R
50|$|Dynamic Compression in <b>data</b> <b>transfer</b> {{is another}} example which uses {{computational}} resources to minimize the bandwidth requirements of <b>data</b> <b>transfer.</b>|$|R
40|$|In this paper, a new network {{interface}} with distributed memory is proposed for clustering WorkStation (WS) or Personal Computer (PC) {{to reduce the}} <b>data</b> <b>transfer</b> within a node of a cluster. Clustering WS or PC is expected to realize high performance computing (HPC). However, the limited bandwidth and high network latency restrict the transfer speed for huge amount of data among nodes in a cluster. Therefore, {{it is important for}} cluster system to reduce <b>data</b> <b>transfer</b> as much as possible. <b>Data</b> <b>transfer</b> speed among nodes of a cluster has been improved significantly in recent years using giga-bit network layer. On the other hand, the <b>data</b> <b>transfer</b> speed within each node of a cluster is still a bottleneck, thus it is important to reduce the <b>data</b> <b>transfer</b> within a node. In order to reduce the <b>data</b> <b>transfer,</b> out {{network interface}} has a memory on the interface. For effective data storing into the memory on network interface, two functions Put and Back are also defined and they reduce the <b>data</b> <b>transfer</b> between main memory and network interface. The simulation results of matrix multiplication indicate that <b>data</b> <b>transfer</b> of message communication can be reduced dramatically by using our new network interface. リサーチレポート（北陸先端科学技術大学院大学情報科学研究科...|$|R
40|$|In {{standard}} OpenCL programming, hosts {{are supposed}} to control their compute devices. Since compute devices are dedicated to kernel computation, only hosts can execute several kinds of <b>data</b> <b>transfers</b> such as internode communication and file access. These <b>data</b> <b>transfers</b> require one host to simultaneously play two or more roles due {{to the need for}} collaboration between the host and devices. The codes for such <b>data</b> <b>transfers</b> are likely to be system-specific, resulting in low portability. This paper proposes an OpenCL extension that incorporates such <b>data</b> <b>transfers</b> into the OpenCL event management mechanism. Unlike the current OpenCL standard, the main thread running on the host is not blocked to serialize dependent operations. Hence, an application can easily use the opportunities to overlap parallel activities of hosts and compute devices. In addition, the implementation details of <b>data</b> <b>transfers</b> are hidden behind the extension, and application programmers can use the optimized <b>data</b> <b>transfers</b> without any tricky programming techniques. The evaluation results show that the proposed extension can use the optimized <b>data</b> <b>transfer</b> implementation and thereby increase the sustained <b>data</b> <b>transfer</b> performance by about 18 % for a real application accessing a big data file...|$|R
5000|$|<b>Data</b> <b>transfer</b> rate - The rate {{at which}} user <b>data</b> bits are <b>transferred</b> from or to the medium. Technically, this would more {{accurately}} be entitled the [...] "gross" [...] <b>data</b> <b>transfer</b> rate.|$|R
50|$|Early {{versions}} of Chapweske’s technology {{were based on}} peer-to-peer <b>data</b> <b>transfer</b> techniques, but today Swarmcast does not utilize any form of P2P <b>data</b> <b>transfer.</b>|$|R
40|$|We {{introduce}} {{another view}} of group {{theory in the}} field of interconnection networks. With this approach it is possible to specify application specific network topologies for permutation <b>data</b> <b>transfers.</b> Routing of <b>data</b> <b>transfers</b> is generated and all possible permutation <b>data</b> <b>transfers</b> are guaranteed. We present the approach by means of a kind of SIMD DSP...|$|R
5000|$|For Link 16, <b>data</b> <b>transfer</b> rate {{is between}} 26.8 kbit/s (26,880 bit/s) and 107.5 kbit/s (107,520 bit/s), {{depending}} on the data packing structure. For Link 22, the UHF fixed frequency <b>data</b> <b>transfer</b> rate is 12.7 kbit/s (12,666 bit/s). Link 22 can use multiple networks for one data stream to increase the <b>data</b> <b>transfer</b> rate.|$|R
40|$|Abstract [...] In a {{reconfigurable}} platform {{such as the}} E 2 R platform, interconnectivity and <b>data</b> <b>transfer</b> {{between the}} different components in the platform {{is a very important}} task. In this paper we propose the interconnect as a reconfigurable resource which can be configured to the <b>data</b> <b>transfer</b> needs of the platform. Functionalities of the components in the resource’s Logical Device Drivers are elaborated. Additionally, with a NoC based C-CEM as an example, initialization, <b>data</b> <b>transfer</b> and legacy <b>data</b> <b>transfer</b> transactions support are explained in detail...|$|R
40|$|Abstract This paper {{introduces}} two alternative algorithms for efficient <b>data</b> <b>transfer</b> in the Grid environment. For <b>data</b> <b>transfer</b> from {{a source}} node to the destination node, the algorithms can construct multiple dynamic paths by selecting some other nodes as data relays. The bandwidth available in different paths can be aggregated thus to significantly speed up the <b>data</b> <b>transfer</b> process. The proposed algorithms differ {{from each other in}} whether the global networking information should be considered. Experimental results indicate that both algorithms can provide efficient <b>data</b> <b>transfer</b> under various circumstances...|$|R
40|$|Abstract: The Serial Front Panel Data Port (SFPDP) {{protocol}} for high speed <b>data</b> <b>transfer</b> presents in this paper. High-speed <b>data</b> <b>transfer</b> finds application in most modern day communication systems. This design has been mainly done for <b>data</b> <b>transfer</b> in radar systems {{but can be}} programmed and used for variety of applications involving high-speed <b>data</b> <b>transfer.</b> The design follows a systematic approach with design of SFPDP protocol and implementation on FPGA and explains all these stages of design in detail. The design can be programmed to work at different speeds as required by different systems and thus {{can be used in}} variety of systems involving high-speed <b>data</b> <b>transfers.</b> The efficient use of customized IP cores and resources of FPGA delivers high level of performance and area efficiency...|$|R
50|$|Some ISPs offer {{flat rates}} for mobile access. <b>Data</b> <b>transfer</b> limits apply to most mobile {{broadband}} access {{which is usually}} shaped to a certain speed after the <b>data</b> <b>transfer</b> limit is reached.|$|R
30|$|Strategies for <b>data</b> <b>transfer,</b> cloud {{resource}} provisioning and job allocation {{have been}} proposed in the literature. A Stream-based <b>data</b> <b>transfer</b> strategy presented in [21] proposed a data compression technique to reduce the <b>data</b> <b>transfer</b> overhead associated with large data. In [30], various strategies for provisioning of virtual machines and workflow scheduling are described. Another similar survey in [9] tested various on-demand execution and waiting time provisioning policies.|$|R
30|$|The limited {{bandwidth}} between CPU and Intel Phi {{may represent a}} major bottleneck {{to the use of}} the coprocessor, especially for data intensive applications [32]. The <b>data</b> <b>transfers</b> necessary to run a computation in the Intel Phi are typically carried out synchronously (sync), by pipelining the input <b>data</b> <b>transfer</b> (host to device), the computation in the coprocessor, and the output <b>data</b> <b>transfer</b> (device to host).|$|R
40|$|There are {{analyzed}} integrated syringe pumps control system (ISPCS) usage advantages. Necessity of such system is analyzed and motivated. There are presented concept of ISPCS integration device (ID) and broad-brush ID functioning algorithm. Syringe pumps connection with integration device variants {{are analyzed}}. There are presented existing <b>data</b> <b>transfer</b> technologies comparison and analysis. <b>Data</b> <b>transfer</b> protocol realization aspects and false <b>data</b> <b>transfer</b> probability minimization ways are approached...|$|R
30|$|Decision module(DM): The DM {{has been}} {{designed}} to decide the destination of the controller after completing the <b>data</b> <b>transfer</b> operation viz. whether the control will go to the IM or to the CINM. It is required for successive sequence of <b>data</b> <b>transfer.</b> After performing the <b>data</b> <b>transfer</b> process, the control comes to the DM. The DM constantly monitors the Reset signal and depending on the status of the Reset signal, it decides whether the control will go to the IM for performing the next <b>data</b> <b>transfer</b> operation or it will go to the CINM for initialization of the card.|$|R
40|$|Abstract—One of {{the issues}} in <b>data</b> <b>transfer</b> between host CPU and its eMMC {{connected}} subsystem is to determine when to send data from host to subsystem and when to receive data from subsystem to host with least CPU interference. The conventional approach to achieving <b>data</b> <b>transfer</b> synchronization is by polling, which impacts CPU bandwidth and potentially affects the subsystem’s performance. This paper proposes an automatic two-way <b>data</b> <b>transfer</b> synchronization scheme that requires no involvement of host CPU in <b>data</b> <b>transfer</b> synchronization and offers real-time synchronization performance on subsystem side. (Abstract) Keywords-eMMC; subsystem; ECSS; DDR, ASRHA; ASR, synchronization; Input Buffer; Output Buffe...|$|R
40|$|Hadoop {{distributed}} {{file system}} (HDFS) {{is becoming more}} popular {{in recent years as}} a key building block of integrated grid storage solution in the field of scientific computing. Wide Area Network (WAN) <b>data</b> <b>transfer</b> is one of the important data operations for large high energy physics experiments to manage, share and process datasets of PetaBytes scale in a highly distributed grid computing environment. In this paper, we present the experience of high throughput WAN <b>data</b> <b>transfer</b> with HDFS-based Storage Element. Two protocols, GridFTP and fast <b>data</b> <b>transfer</b> (FDT), are used to characterize the network performance of WAN <b>data</b> <b>transfer...</b>|$|R
40|$|FREQUENCY HOPPING In this paper, {{we propose}} a new {{encryption}} algorithm for communication of Bluetooth Sensor Systems. It {{is well known}} that Bluetooth sensor systems are commonly used in low range <b>data</b> <b>transfer</b> and communication applications where security and <b>data</b> <b>transfer</b> speed are the primary concerns. Several algorithms were proposed in this regard. Analyzing {{the strengths and weaknesses of}} the current encryption algorithms, we develop a new 128 bit encryption scheme which is designed only for the Bluetooth <b>data</b> <b>transfer</b> scenario by taking advantage of the Fast Frequency Hopping Scheme and thereby achieving greater <b>data</b> <b>transfer</b> speed and security...|$|R
40|$|This {{is widely}} {{accepted}} that Network-on-Chip represents a promising solution for forthcoming complex embedded systems. The current SoC Solutions are built from heterogeneous hardware and Software components integrated around a complex communication infrastructure. The crossbar {{is a vital}} component of in any NoC router. In this work, we have designed a crossbar interconnect for serial bit <b>data</b> <b>transfer</b> and 128 -parallel bit <b>data</b> <b>transfer.</b> We have shown comparison between power and delay for the serial bit and parallel bit <b>data</b> <b>transfer</b> through crossbar switch. The design is implemented in 0. 180 micron TSMC technology. The bit rate achieved in serial transfer is slow as compared with parallel <b>data</b> <b>transfer.</b> The simulation {{results show that the}} critical path delay is less for parallel bit <b>data</b> <b>transfer</b> but power dissipation is high. ...|$|R
50|$|Note: The {{effective}} <b>data</b> <b>transfer</b> rate {{is usually}} expressed in bits, characters, blocks, or frames per second. The effective <b>data</b> <b>transfer</b> rate may be averaged {{over a period}} of seconds, minutes, or hours.|$|R
50|$|UDP-based <b>Data</b> <b>Transfer</b> Protocol (UDT), is a {{high-performance}} <b>data</b> <b>transfer</b> protocol designed for transferring large volumetric datasets over high-speed wide area networks. Such settings are typically disadvantageous {{for the more}} common TCP protocol.|$|R
40|$|This unit is {{designed}} to enable candidates to gain knowledge and understanding to apply a high level programming language solution to parallel and serial <b>data</b> <b>transfer</b> problems in an engineering environment. It is also intended to introduce the principles of data logging. Outcomes Outcome 1 introduces the principles of parallel <b>data</b> <b>transfer</b> with Outcome 2 concentrating on serial <b>data</b> <b>transfer.</b> Outcome 3 introduces the principles of file creation, reading and closing. ...|$|R
50|$|In Web hosting service, {{the term}} {{bandwidth}} is often incorrectly {{used to describe}} the amount of <b>data</b> <b>transferred</b> to or from the website or server within a prescribed period of time, for example bandwidth consumption accumulated over a month measured in gigabytes per month. The more accurate phrase used for this meaning of a maximum amount of <b>data</b> <b>transfer</b> each month or given period is monthly <b>data</b> <b>transfer.</b>|$|R
40|$|In {{distributed}} systems, {{the lack}} of global information about <b>data</b> <b>transfer</b> between clients and servers makes implementation of parallel I/O a challenging task. In this paper, we propose two distributed algorithms for scheduling <b>data</b> <b>transfer</b> in parallel I/O with non-uniform data sizes, the Maximum-Size/Maximum-Load (MS/ML) algorithm and the Minimum-Size/Earliest-Completion-First (MS/ECF) algorithm. Experimental results indicate that both algorithms achieve good performance, compared with the results achieved by their centralized counterparts. Both algorithms yielded parallel performances within 6 % of the centralized solutions. We also compare the performance of our algorithms with a distributed Highest Degree First (HDF) method, which handles non-uniform <b>data</b> <b>transfers</b> by dividing them into units of fixed-sized blocks which are then scheduled and transferred one at a time. Experimental results show that our algorithms require less scheduling and <b>data</b> <b>transfer</b> time, resulting in better overall parallel I/O performance. Our simulations also show that MS/ML is more suitable for parallel I/O with lighter <b>data</b> <b>transfer</b> traffic, while MS/ECF is more suitable for parallel I/O with heavy <b>data</b> <b>transfer</b> traffic. 1...|$|R
40|$|Abstract—Graphics {{processing}} units (GPUs) embrace many-core compute devices where {{massively parallel}} compute threads are offloaded from CPUs. This heterogeneous nature of GPU computing raises non-trivial <b>data</b> <b>transfer</b> problems especially against latency-critical real-time systems. However even the basic characteristics of <b>data</b> <b>transfers</b> associated with GPU computing {{are not well}} studied in the literature. In this paper, we investigate and characterize currently-achievable <b>data</b> <b>transfer</b> methods of cutting-edge GPU technology. We implement these methods using open-source software to compare their performance and latency for real-world systems. Our experimental {{results show that the}} hardware-assisted direct memory access (DMA) and the I/O read-and-write access methods are usually the most effective, while on-chip microcontrollers inside the GPU are useful in terms of reducing the <b>data</b> <b>transfer</b> latency for concurrent multiple data streams. We also disclose that CPU priorities can protect the performance of GPU <b>data</b> <b>transfers...</b>|$|R
40|$|An {{expanding}} mobile {{cellular network}} <b>data</b> <b>transfer</b> service offers cheaper wireless solutions for various <b>data</b> <b>transfer</b> needs. This paper presents an experimental testing of <b>data</b> <b>transfer</b> performance in 3 G and 4 G modes. The purpose of testing {{was to check}} the possibility of real-time and critical <b>data</b> <b>transfer</b> over the mobile cellular networks. The testing was performed in Riga in July and August 2016 using the most popular mobile service operators in Latvia: Tele 2 -LV, BITE-LV and LMT. The testing confirmed that the overload of Riga’s 4 G networks causes serious service deterioration or even interruption. Riga’s 3 G networks are more stable. However, 3 G network service quality depends on a cell load. Lightly loaded 3 G network meets real-time <b>data</b> <b>transfer</b> requirements of 100 ms one-way delay of the small packet traffic...|$|R
30|$|A common {{approach}} used {{for reducing the}} impact of such transfer costs to performance is to overlap <b>data</b> <b>transfer</b> operations with the execution of other tasks. This technique receives the name of double buffering [38, 39] and {{has been used in}} several domains. In our problem, {{it can be used to}} reduce both the CPU and Intel Phi idle times during the transfers, and may be implemented using an asynchronous <b>data</b> <b>transfer</b> (async) mechanism available in the Phi. In this strategy, while the Intel Phi is in the computation stage of the sync pipeline, we concurrently launch the input <b>data</b> <b>transfer</b> of a batch of IP addresses while a second buffer is being processed in the Intel Phi. The same also occurs for the output <b>data</b> <b>transfers.</b> This will allow for the overlapping of computation and <b>data</b> <b>transfers.</b>|$|R
40|$|While service-oriented cloud {{workflow}} shows potentials of inherent scalability, {{loose coupling}} and expenditure reduction, {{such issues as}} <b>data</b> <b>transfer</b> and efficiency have popped up as major concerns. In this paper, a dataflow opt imisation mechanism is proposed, including <b>data</b> <b>transfer</b> strategy and <b>data</b> placement strategy. The <b>data</b> <b>transfer</b> strategy uses integrated data structures to enable {{the elimination of the}} SOAP serialisation and de-serialisation within the web service invocation by the exchange of their references. In additi on, the data placement strategy groups the existing datasets and dynamically cluste rs newly generated datase ts to minimise <b>data</b> <b>transfers</b> while keeping a polynomial time complexity. By integrating the optimisation mechanism into service-oriented cloud workflow system, we can expect efficiency improvements in <b>data</b> <b>transfer</b> and workflow execution. Experiments and analysis supported our efforts. Peng Zhang, Yanbo Han, Muhammad Ali Baba...|$|R
40|$|<b>Data</b> <b>transfer</b> means <b>transferring</b> <b>data</b> fields from a sender to a receiver. It is a {{fundamental}} and frequently used operation of a coupler. Most versions of state-of-the-art couplers currently use an implementation based on the point-to-point (P 2 P) communication of the message passing interface (MPI) (referred to as “P 2 P implementation” hereafter). In this paper, we reveal the drawbacks of the P 2 P implementation when the parallel decompositions of the sender and the receiver are different, including low communication bandwidth due to small message size, variable and high number of MPI messages, as well as network contention. To overcome these drawbacks, we propose a butterfly implementation for <b>data</b> <b>transfer.</b> Although the butterfly implementation outperforms the P 2 P implementation in many cases, it degrades the performance when the sender and the receiver have similar parallel decompositions or {{when the number of}} processes used for running models is small. To ensure <b>data</b> <b>transfer</b> with optimal performance, we design and implement an adaptive <b>data</b> <b>transfer</b> library that combines the advantages of both butterfly implementation and P 2 P implementation. As the adaptive <b>data</b> <b>transfer</b> library automatically uses the best implementation for <b>data</b> <b>transfer,</b> it outperforms the P 2 P implementation in many cases while it does not decrease the performance in any cases. Now, the adaptive <b>data</b> <b>transfer</b> library {{is open to the public}} and has been imported into the C-Coupler 1 coupler for performance improvement of <b>data</b> <b>transfer.</b> We believe that other couplers can also benefit from this...|$|R
