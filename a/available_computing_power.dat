191|9438|Public
500|$|As {{computers}} {{have become more}} powerful, {{the size of the}} initial data sets has increased and newer atmospheric models have been developed {{to take advantage of the}} added <b>available</b> <b>computing</b> <b>power.</b> These newer models include more physical processes in the simplifications of the equations of motion in numerical simulations of the atmosphere. [...] In 1966, West Germany and the United States began producing operational forecasts based on primitive-equation models, followed by the United Kingdom in 1972 and Australia in 1977. [...] The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclones as well as air quality in the 1970s and 1980s. [...] By the early 1980s models began to include the interactions of soil and vegetation with the atmosphere, which led to more realistic forecasts.|$|E
2500|$|Earlier {{challenges}} in training deep neural networks were successfully addressed with {{methods such as}} unsupervised pre-training, while <b>available</b> <b>computing</b> <b>power</b> increased {{through the use of}} GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as [...] "deep learning", although deep learning is not strictly synonymous with deep neural networks.|$|E
50|$|The {{number of}} {{bootstrap}} samples recommended in literature has increased as <b>available</b> <b>computing</b> <b>power</b> has increased. If {{the results may}} have substantial real-world consequences, then one should use as many samples as is reasonable, given <b>available</b> <b>computing</b> <b>power</b> and time. Increasing the number of samples cannot {{increase the amount of}} information in the original data; it can only reduce the effects of random sampling errors which can arise from a bootstrap procedure itself.|$|E
40|$|Introduction The {{effectiveness}} of task scheduling in a distributed environment is critically {{dependent on the}} timely identification of the least loaded nodes. Whether the issue of interest is load-sharing or distributed parallel computation, overall system performance is determined {{in large part by}} the characteristics of the nodes participating in a particular computation. The diversity of node characteristics across the network frequently results in a spectrum of <b>available</b> <b>compute</b> <b>powers</b> on different nodes. Due to the high cost of task migration, effective evaluation of the relative <b>available</b> <b>compute</b> <b>powers</b> of the nodes in the network and the use of that information in task distribution are essential components of successful task scheduling in a distributed environment. A few implementations of systems for distributing large scale computations over a network of computers rely on schemes based on the current as well as the prior state information on each node to make better t...|$|R
40|$|This {{full day}} course {{will provide a}} {{detailed}} overview of {{state of the art}} in Monte Carlo ray tracing. Recent advances in algorithms and <b>available</b> <b>compute</b> <b>power</b> have made Monte Carlo ray tracing based methods widely used for simulating global illumination. This course will review the fundamentals of Monte Carlo methods, and provide {{a detailed description of the}} theory behind the latest techniques and algorithms used in realistic image synthesis. This includes path tracing, bidirectional path tracing, Metropolis light transport, irradiance caching and photon mapping. Course Syllabu...|$|R
50|$|Bigger {{networks}} are only {{limited by the}} amount of equipment <b>available,</b> and <b>computing</b> <b>power</b> needed to resolve the resulting data. The latest acoustic networks used in the marine seismic industry can resolve a network of some 16,000 individual ranges in a matter of seconds.|$|R
50|$|Multiple Terminals - The {{number of}} online {{terminals}} is {{limited only by}} the <b>available</b> <b>computing</b> <b>power</b> and memory for system structures.|$|E
5000|$|The {{status of}} the Human Proteome Folding Project caused some {{discussion}} on the grid.org forums. Most members wanted to see all <b>available</b> <b>computing</b> <b>power</b> directed toward the still-active cancer project, but UD representative Robby Brewer asserted that [...] "some users like the screensaver". As noted above, {{in the end the}} redundant HPF1 work on grid.org was halted.|$|E
5000|$|Earlier {{challenges}} in training deep neural networks were successfully addressed with {{methods such as}} unsupervised pre-training, while <b>available</b> <b>computing</b> <b>power</b> increased {{through the use of}} GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as [...] "deep learning", although deep learning is not strictly synonymous with deep neural networks.|$|E
40|$|Service grids and desktop grids {{are both}} {{promoted}} by their supportive communities as great solutions for solving the <b>available</b> <b>compute</b> <b>power</b> problem {{and helping to}} balance loads across network systems. Little work, however, has been undertaken to blend these two technologies together. In this paper we introduce a new EU project, that is building technological bridges to facilitate service and desktop grid interoperability. We provide a taxonomy and background into service grids, such as EGEE and desktop grids or volunteer computing platforms, such as BOINC and XtremWeb. We then describe our approach for identifying translation technologies between service and desktop grids. The individual themes discuss the actual bridging technologies employed and the distributed data issues surrounding deployment...|$|R
40|$|Developers of {{simulation}} {{packages are}} {{now able to}} take advantage of the increase in <b>available</b> desktop <b>computing</b> <b>power</b> to expand the capabilities and usability of their programs. This paper will illustrate these opportunities by discussing the different techniques the developers of the TRNSYS software package have used to try and create a synergy between TRNSYS and external programs and between the developers and users of the program...|$|R
5000|$|Computational <b>power</b> <b>available,</b> i.e., the <b>computing</b> <b>power</b> {{which can}} be brought to bear on the problem. It is {{important}} to note that average performance/capacity of a single computer is not the only factor to consider. An adversary can use multiple computers at once, for instance, to increase the speed of exhaustive search for a key (i.e., [...] "brute force" [...] attack) substantially.|$|R
50|$|The {{design of}} a shell is guided by {{cognitive}} ergonomics and {{the goal is to}} achieve the best workflow possible for the intended tasks; the design can be constricted by the <b>available</b> <b>computing</b> <b>power</b> (for example, of the CPU) or the available amount of graphics memory. The {{design of a}} shell is also dictated by the employed computer periphery, such as computer keyboard, pointing device (a mouse with one button, or one with five buttons, or a 3D mouse) or touchscreen, which is the direct human-machine interface.|$|E
5000|$|In contrast, direct methods {{attempt to}} solve the problem by a finite {{sequence}} of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations [...] by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best <b>available</b> <b>computing</b> <b>power.</b>|$|E
50|$|Purpose-shaping {{can be seen}} in {{the design}} of surface {{faceting}} on the F-117A Nighthawk stealth fighter. This aircraft, designed in the late 1970s though only revealed to the public in 1988, uses a multitude of flat surfaces to reflect incident radar energy away from the source. Yue suggests that limited <b>available</b> <b>computing</b> <b>power</b> for the design phase kept the number of surfaces to a minimum. The B-2 Spirit stealth bomber benefited from increased computing power, enabling its contoured shapes and further reduction in RCS. The F-22 Raptor and F-35 Lightning II continue the trend in purpose shaping and promise to have even smaller monostatic RCS.|$|E
40|$|Virtualization is a {{well-known}} technique to facilitate a variety of use cases in both desktop and server environments. Increasingly, security becomes {{an important aspect of}} virtualiza-tion because it allows for consolidating multiple workloads on a single physical machine in a protected manner. Recent processor development shifted from increasing single-core performance to integrating multiple cores onto one chip, resulting from the physical limits imposed on the design of microprocessors. It was only a question of time until virtualization followed this trend and supported running several virtual machines truly in parallel, at first multiple single-core ones, then even virtual multi-core. In the full virtualization solution of the TU Dresden, the NOVA OS Virtualization Architecture (NOVA), SMP is only supported in the first variant, while multiple CPUs of a guest system have to share the same physi-cal core. The goal of this thesis is to explore how this limitation can be eliminated with maximum efficiency, passing on as much <b>available</b> <b>compute</b> <b>power</b> to the guest as possi-ble. By identifying and enhancing the critical synchronization mechanisms inside the VMM, the presented solution accounts for maximum scalability in the current environment whil...|$|R
40|$|Molecular {{dynamics}} programs {{simulate the}} behavior of biomolecular systems, leading to understanding of their functions. However, the computational complexity of such simulations is enormous. Parallel machines provide the potential to meet this computational challenge. To harness this potential, {{it is necessary to}} develop a scalable program. It is also necessary that the program be easily modified by application– domain programmers. The NAMD 2 program presented in this paper seeks to provide these desirable features. It uses spatial decomposition combined with force decomposition to enhance scalability. It uses intelligent periodic load balancing, so as to maximally utilize the <b>available</b> <b>compute</b> <b>power.</b> It is modularly organized, and implemented using Charm++, a parallel C++ dialect, so as to enhance its modifiability. It uses a combination of numerical techniques and algorithms to ensure that energy drifts are minimized, ensuring accuracy in long running calculations. NAMD 2 uses a portable run-time framework called Converse that also supports interoperability among multiple parallel paradigms. As a result, different components of applications can be written in the most appropriate parallel paradigms. NAMD 2 runs on most parallel machines including workstation clusters and has yielded speedups in excess of 180 on 220 processors. This paper also describes the performance obtained on some benchmark applications. c ○ 1999 Academic Pres...|$|R
40|$|Abstract. For {{the actual}} Video-On-Demand (VOD) service environ-ment, we {{implement}} a cluster-based VOD server composed of general PCs and adopt the parallel processing for MPEG movies. For the imple-mented VOD server, a video block recovery mechanism is designed on the RAID- 3 and the RAID- 4 algorithms. However, without considering {{the architecture of}} cluster-based VOD server, the application of these ba-sic RAID techniques causes the performance bottleneck of the internal network for recovery. To solve these problems, the new failure recovery mechanism based on the pipeline computing concept is proposed. The proposed method distributes the network traffics invoked by recovery operations and utilizes the <b>available</b> CPU <b>computing</b> <b>power</b> of cluster nodes. ...|$|R
50|$|For a long time, it was {{a widely}} held opinion that {{computer}} Go posed a problem fundamentally different from computer chess. It was believed that methods relying on fast global search with relatively little domain knowledge would not be effective against human experts. Therefore, {{a large part of}} the computer Go development effort was during these times focused on ways of representing human-like expert knowledge and combining this with local search to answer questions of a tactical nature. The result of this were programs that handled many situations well but which had very pronounced weaknesses compared to their overall handling of the game. Also, these classical programs gained almost nothing from increases in <b>available</b> <b>computing</b> <b>power</b> per se and progress in the field was generally slow.|$|E
5000|$|As {{computers}} {{have become more}} powerful, {{the size of the}} initial data sets has increased and newer atmospheric models have been developed {{to take advantage of the}} added <b>available</b> <b>computing</b> <b>power.</b> These newer models include more physical processes in the simplifications of the equations of motion in numerical simulations of the atmosphere. [...] In 1966, West Germany and the United States began producing operational forecasts based on primitive-equation models, followed by the United Kingdom in 1972 and Australia in 1977. [...] The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclones as well as air quality in the 1970s and 1980s. [...] By the early 1980s models began to include the interactions of soil and vegetation with the atmosphere, which led to more realistic forecasts.|$|E
40|$|In recent years, the {{development}} of large-scale distributed-memory computers has given the user community unprecedented levels of computing power. In order to effectively use the <b>available</b> <b>computing</b> <b>power,</b> processor scheduling algorithms have been developed that allow many users to share distributed computing resources while obtaining the best possible job turnaround time. However, not all existing scheduling techniques {{take full advantage of}} <b>available</b> <b>computing</b> <b>power.</b> For example, in hypercubes, a cluster must normally be allocated as an entire subcube, which can result in high internal fragmentation, as well as poor job performance. Although the distributed workstation environment has recently become popular as a choice for a distributed-memory parallel computer, the problem of scheduling specifically for parallel job execution has not been well studied in this environment. In this thesis, we pr [...] ...|$|E
40|$|Summary. Processor {{technology}} is still dramatically advancing and promises further enormous improvements in processing {{data for the}} next decade. On the other hand, much lower advances in moving data are expected such that the efficiency of many numerical software tools for Partial Differential Equations (PDE’s) are restricted by the cost for memory access. We demonstrate how data locality and pipelining can achieve {{a significant percentage of}} the <b>available</b> huge <b>computing</b> <b>power,</b> and we explain the influence of processor technology on recent and future numerical PDE simulation tools. Exemplarily, we describe hardware-oriented concepts for adaptive error control, multigrid/domain decomposition schemes and incompressible flow solvers and discuss their numerical and computational characteristics. ...|$|R
40|$|In this paper, we aim {{to present}} the {{fundamental}} properties of a community-driven adaptive P 2 P streaming scheme. We show that if the participants in a community network agree to not only share their bandwidth, but also their computing resources according to the design principles mentioned here, then mobile and heterogeneous devices can be accommodated in the P 2 P paradigm ensuring adequate resource utilization, with respect to resilience to peer dynamics. We present simple design principles for a multimodal P 2 P system considering <b>available</b> bandwidth, <b>computing</b> <b>power,</b> and delay to build the video overlays. Some evaluation results supporting our design principles are also presented. Index Terms – Overlay streaming, Peer-to-Peer system, Video adaptation...|$|R
40|$|As {{parallel}} {{systems from}} traditional mainframe vendors join offerings from specialist parallel computer manufacturers {{in the commercial}} marketplace, this paper examines the issues facing a business end-user considering migration to parallel systems. The results of a 1992 survey of major UK companies are presented, highlighting {{a number of factors}} which are perceived as inhibiting the wider uptake of parallel computing in commerce. 1 INTRODUCTION The fundamental economic motivation for parallel computing is the improvement in cost performance which it can offer over sequential alternatives. This permits cheaper solutions of existing applications, or, within the limits of scalability, makes <b>available</b> increased <b>computing</b> <b>power</b> for a fixed budget. Cheaper solutions can offer more than a simple economic saving, by representing an opportunity for potentially radical change in {{the ways in which the}} system is deployed and exploited within an organization. An increase in <b>computing</b> <b>power</b> can b [...] ...|$|R
40|$|Computing environments change: {{everyone}} has portable computing devices (in form of mobile phones) {{and access to}} large servers (in the cloud). This change presents fundamental challenge of outsourcing computation, which is motivated by the asymmetry of the <b>available</b> <b>computing</b> <b>power.</b> In recent computing scenarios clients are trusted (but weak), while computationally strong servers ar...|$|E
40|$|CAUT) funded {{a project}} to develop an {{integrated}} computationally rich first year mathematics course built {{on the use of}} APL {{as an integral part of}} the instruction. This article outlines how the standard syllabus was amended to respond to the <b>available</b> <b>computing</b> <b>power</b> for elucidating mathematical topics and what mathematical skills were improved or neglected by the novel presentation. ...|$|E
40|$|Distributed Computing, the {{exploitation}} of idle cycles on pervasive desktop PC systems offers the opportunity to increase the <b>available</b> <b>computing</b> <b>power</b> by orders of magnitude (10 x - 1000 x). However, for desktop PC distributed computing to be widely accepted within the enterprise, the systems much achieve high levels of robustness, security, scalability, unobtrusiveness, and manageability...|$|E
40|$|Processor {{technology}} is dramatically advancing and promises enormous improvements in processing {{data for the}} next decade. On the other hand, much lower advances in moving data are expected such that the efficiency of many numerical software tools for Partial Differential Equations are restricted by the cost for memory access. We demonstrate how data locality and pipelining can achieve {{a significant percentage of}} the <b>available</b> huge <b>computing</b> <b>power,</b> and we explain the influence of processor technology on recent and future numerical simulation tools. Exemplarily, we describe hardware-related concepts for adaptive error control, multigrid/domain decomposition schemes and incompressible flow solvers and discuss their numerical and computational characteristics. (orig.) Available from TIB Hannover: RR 1606 (99 - 31) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|The {{original}} publication {{is available}} at www. springerlink. com???. Copyright Springer. DOI: 10. 1007 / 3 - 540 - 44597 - 8 _ 14 [Full text {{of this article is}} not available in the UHRA]The capability of creating artificial neural networks with biologically-plausible characteristics, is becoming ever more attainable through the greater understanding of biological neural systems and the constant increases in <b>available</b> desktop <b>computing</b> <b>power.</b> Designing and implementing such neural systems still however remains a complex and challenging problem. This chapter introduces a design methodology, inspired by embryonic neural development, which is capable of creating 3 dimensional morphologies of both individual neurons and networks. Examples of such morphologies are given, which are created by evolving the parameters of the developmental model using an evolutionary algorithm...|$|R
40|$|Room {{acoustic}} simulations commonly use simple {{models for}} sound scattering on surfaces in the scene. However, the continuing increase of <b>available</b> parallel <b>computing</b> <b>power</b> {{makes it possible}} to apply more sophisticated mod-els. We present a method to precompute the distribution of the reflected sound off a structured surface described by a height map and normal map using the Kirchhoff approximation. Our precomputation and interpolation scheme, based on representing the reflected pressure with von-Mises-Fisher functions, is able to retain many directional and spectral features of the reflected pressure while keeping the computational and storage requirements low. We discuss our model and demonstrate applications of our precomputed functions in acoustic ray tracing and a novel interactive method suitable for applications such as architectural walk-throughs and video games...|$|R
40|$|Software parallelization allows an {{efficient}} use of <b>available</b> <b>computing</b> <b>power</b> to in- crease the performance of applications. In a case study we have investigated the parallelization of high-energy physics event reconstruction software in terms of costs (effort, computing resource requirements), benefits (performance increase), and the feasibility of a systematic parallelization approach. Guidelines facilitating a parallel implementation are proposed for future software development...|$|E
40|$|This paper {{presents}} an experience in {{use of a}} parallel mathematical library, ScaLAPACK, on a network composed by heterogeneous workstations. The good performance results have been obtained {{by means of a}} distributed programming environment which is able to dynamically evaluate <b>available</b> <b>computing</b> <b>power</b> at each workstation and to distribute accordingly the set of parallel processes. Keywords: ScaLAPACK, Networks of Workstations, Heterogeneous Networks. 1...|$|E
40|$|Moore's Law, {{defined in}} the sixties, predicts a {{monotonic}} increase in <b>available</b> <b>computing</b> <b>power</b> with time. With the commodification of high performancemicroprocessors, very large amounts of computing power are now readily available in the open market, at very modest cost. This paper will exploresomeof {{the implications of this}} phenomenon for IW in the coming decades, and propose some guidelines for IW strategists and planners...|$|E
40|$|Biological applications, from {{genomics}} to ecology, {{deal with}} graphs {{that represents the}} structure of interactions. Analyzing such data requires searching for subgraphs in collections of graphs. This task is computationally expensive. Even though multicore architectures, from commodity computers to more advanced symmetric multiprocessing (SMP), offer scalable <b>computing</b> <b>power,</b> currently published software implementations for indexing and graph matching are fundamentally sequential. As a consequence, such software implementations (i) do not fully exploit <b>available</b> parallel <b>computing</b> <b>power</b> and (ii) they do not scale {{with respect to the}} size of graphs in the database. We present GRAPES, software for parallel searching on databases of large biological graphs. GRAPES implements a parallel version of well-established graph searching algorithms, and introduces new strategies which naturally lead to a faster parallel searching system especially for large graphs. GRAPES decomposes graphs into subcomponents that can be efficiently searched in parallel. We show the performance of GRAPES on representative biological datasets containing antiviral chemical compounds, DNA, RNA, proteins, protein contact maps and protein interactions networks...|$|R
40|$|Utilizing the {{computational}} {{power of}} a few thousand processors on a BlueGene/P, we have explored the folding mechanism of the 67 -residue protein GS-alpha(3) W. Results from our large-scale simulation indicate a diffusion-collision mechanism for folding. However, the lower-than-expected frequency of native-like configurations at physiological temperatures indicates shortcomings of our energy function. Our results suggest that computational studies of large proteins call for redevelopment and reparametrization of force fields that in turn require extensive simulations only possible with the newly <b>available</b> supercomputers with <b>computing</b> <b>powers</b> reaching the petaflop range...|$|R
5000|$|However, {{the birth}} of {{functional}} genomics in the 1990s meant that large quantities of high-quality data became <b>available,</b> while the <b>computing</b> <b>power</b> exploded, making more realistic models possible. In 1992, then 1994, serial articles [...] on systems medicine, systems genetics, and systems biological engineering by B. J. Zeng was published in China and was giving a lecture on biosystems theory and systems-approach research at the First International Conference on Transgenic Animals, Beijing, 1996. In 1997, the group of Masaru Tomita published the first quantitative model of the metabolism of a whole (hypothetical) cell.|$|R
