31|217|Public
5000|$|Variable {{declarations}} {{are followed}} by an <b>attribute</b> <b>list.</b> The attributes allowed are , , , , , , , , [...] and [...] The <b>attribute</b> <b>list</b> is followed by , {{which is part of}} the syntax. F also allows for optional initialization in the list of objects. All items in a list will have the same attributes in a given type declaration statement. In addition, declarations are attribute oriented instead of entity oriented.|$|E
5000|$|An <b>attribute</b> <b>list</b> {{specifies}} {{for a given}} {{element type}} the list of all possible attribute associated with that type. For each possible attribute, it contains: ...|$|E
50|$|A non-validating parser may, however, elect {{not to read}} {{parsable}} external entities (including {{the external}} subset), and {{does not have to}} honor the content model restrictions defined in element declarations and in <b>attribute</b> <b>list</b> declarations.|$|E
30|$|Each {{consumer}} household remembers varying {{attributes of}} both stores and products. The Miller (1956) range of seven {{plus or minus}} two bounds the sizes of the consumer <b>attribute</b> <b>lists.</b> The store and product selection processes depend on these <b>attribute</b> <b>lists.</b> These boundedly rational processes are O(⌈[*]log 2 (N)⌉) {{in time and space}} for each agent.|$|R
5000|$|Perform an [...] "or" [...] {{search for}} [...] "Bill White*" [...] against the default <b>attributes</b> <b>listed</b> above.|$|R
3000|$|..., the CH- 149 Helicopter. CH- 149 is a {{helicopter}} {{which has the}} <b>attributes</b> <b>listed</b> below. For instance, OpticalSensor [...]...|$|R
50|$|Note that <b>attribute</b> <b>list</b> {{declarations}} {{are ignored}} by non-validating SGML and XML parsers (in which cases any attribute is accepted within {{all elements of}} the parsed document), but these declarations are still checked for well-formedness and validity.|$|E
5000|$|Given its URL, the {{attributes}} of a service can be requested. In standard SLP {{the attributes}} are not returned in the query result and must be fetched separately. The <b>Attribute</b> <b>List</b> Extension (RFC 3059) fixes this problem.|$|E
50|$|In {{practical}} terms, it can {{be roughly}} thought of as picking a subset of all available columns. For example, if the attributes are (name, age), then projection of the relation {(Alice, 5), (Bob, 8)} onto <b>attribute</b> <b>list</b> (age) yields {5,8} - we have discarded the names, and only know what ages are present.|$|E
50|$|DTD markup {{declarations}} declare which element types, <b>attribute</b> <b>lists,</b> entities, and notations {{are allowed}} {{in the structure of}} the corresponding class of XML documents.|$|R
50|$|While {{builders}} hardware {{is classified}} by supplying {{at least one}} of the three <b>attributes</b> <b>listed</b> above, it is usually broken down by where it is used, or by usage.|$|R
40|$|The goal of our {{research}} is to distinguish vet-erinary message board posts that describe a case involving a specific patient from posts that ask a general question. We create a text classifier that incorporates automatically gen-erated <b>attribute</b> <b>lists</b> for veterinary patients to tackle this problem. Using {{a small amount of}} annotated data, we train an information extrac-tion (IE) system to identify veterinary patient attributes. We then apply the IE system to a large collection of unannotated texts to pro-duce a lexicon of veterinary patient attribute terms. Our experimental results show that us-ing the learned <b>attribute</b> <b>lists</b> to encode pa-tient information in the text classifier yields improved performance on this task. ...|$|R
5000|$|The {{filename}} attributes {{stored in}} the <b>attribute</b> <b>list</b> do not make the file immediately accessible through the hierarchical file system. In fact, all the filenames must be indexed separately {{in at least one}} separate directory on the same volume, with its own MFT record and its own security descriptors and attributes, that will reference the MFT record number for that file. This allows the same file or directory to be [...] "hardlinked" [...] several times from several containers on the same volume, possibly with distinct filenames.|$|E
50|$|Most XML schema {{languages}} are only replacements for element declarations and <b>attribute</b> <b>list</b> declarations, {{in such a}} way that it becomes possible to parse XML documents with non-validating XML parsers (if the only purpose of the external DTD subset was to define the schema). In addition, documents for these XML schema languages must be parsed separately, so validating the schema of XML documents in pure standalone mode is not really possible with these languages: the document type declaration remains necessary for at least identifying (with a XML Catalog) the schema used in the parsed XML document and that is validated in another language.|$|E
5000|$|XCS {{inspired}} {{the development of}} a whole new generation of LCS algorithms and applications. In 1995, Congdon was the first to apply LCS to real-world epidemiological investigations of disease [...] followed closely by Holmes who developed the BOOLE++, EpiCS, and later EpiXCS for epidemiological classification. These early works inspired later interest in applying LCS algorithms to complex and large-scale data mining tasks epitomized by bioinformatics applications. In 1998, Stolzmann introduced anticipatory classifier systems (ACS) which included rules in the form of 'condition-action-effect, rather than the classic 'condition-action' representation. [...] ACS was designed to predict the perceptual consequences of an action in all possible situations in an environment. In other words, the system evolves a model that specifies not only what to do in a given situation, but also provides information of what will happen after a specific action will be executed. This family of LCS algorithms is best suited to multi-step problems, planning, speeding up learning, or disambiguating perceptual aliasing (i.e. where the same observation is obtained in distinct states but requires different actions). Butz later pursued this anticipatory family of LCS developing a number of improvements to the original method. [...] In 2002, Wilson introduced XCSF, adding a computed action in order to perform function approximation. [...] In 2003, Bernado-Mansilla introduced a sUpervised Classifier System (UCS), which specialized the XCS algorithm to the task of supervised learning, single-step problems, and forming a best action set. UCS removed the reinforcement learning strategy in favor of a simple, accuracy-based rule fitness as well as the explore/exploit learning phases, characteristic of many reinforcement learners. Bull introduced a simple accuracy-based LCS (YCS) and a simple strength-based LCS Minimal Classifier System (MCS) in order to develop a better theoretical understanding of the LCS framework. Bacardit introduced GAssist and BioHEL, Pittsburgh-style LCSs designed for data mining and scalability to large datasets in bioinformatics applications. In 2008, Drugowitsch published the book titled [...] "Design and Analysis of Learning Classifier Systems" [...] including some theoretical examination of LCS algorithms. [...] Butz introduced the first rule online learning visualization within a GUI for XCSF (see the image {{at the top of this}} page). Urbanowicz extended the UCS framework and introduced ExSTraCS, explicitly designed for supervised learning in noisy problem domains (e.g. epidemiology and bioinformatics). [...] ExSTraCS integrated (1) expert knowledge to drive covering and genetic algorithm towards important features in the data, (2) a form of long-term memory referred to as attribute tracking, allowing for more efficient learning and the characterization of heterogeneous data patterns, and (3) a flexible rule representation similar to Bacardit's mixed discrete-continuous <b>attribute</b> <b>list</b> representation. [...] Both Bacardit and Urbanowicz explored statistical and visualization strategies to interpret LCS rules and perform knowledge discovery for data mining. [...] Browne and Iqbal explored the concept of reusing building blocks in the form of code fragments and were the first to solve the 135-bit mulitplexer benchmark problem by first learning useful building blocks from simpler multiplexer problems. ExSTraCS 2.0 was later introduced to improve Michigan-style LCS scalability, successfully solving the 135-bit multiplexer benchmark problem for the first time directly. [...] The n-bit multiplexer problem is highly epistatic and heterogeneous, making it a very challenging machine learning task.|$|E
30|$|The stub for {{the aspect}} Timing is {{presented}} in Listing 2. It emulates two advices and one inter-type declaration of an <b>attribute.</b> <b>Listing</b> 3 shows the stub for the class Timer, emulating two attributes and three methods.|$|R
3000|$|Many of the <b>attributes</b> <b>listed</b> in Table  1 {{can already}} be easily derived from {{existing}} models {{as will be}} described in “Result # 2 : automation and discovery” section, where we automate the process for CityGML files [...]...|$|R
30|$|The same {{principle}} {{can be applied}} to decompose residual inequality; the price function g(w|x,T[*]=[*]t) is replaced with the residual price function g(ε|x,T[*]=[*]t). The residuals, ε, are obtained from a regression of log wages on the same set of <b>attributes</b> <b>listed</b> above.|$|R
30|$|<b>Attribute</b> <b>list</b> A: {{a set of}} {{candidate}} attributes.|$|E
30|$|Remove the {{splitting}} attribute X {{from the}} <b>attribute</b> <b>list.</b>|$|E
30|$|Else {{attach the}} node {{returned}} by a generate decision tree (Dj, <b>attribute</b> <b>list,</b> selected splitting criterion) to node Nd.|$|E
40|$|<b>Attribute</b> <b>listing</b> is {{a common}} method of {{exploring}} the nature of natural categories. In this article, the content of <b>listed</b> <b>attributes</b> were examined. One hundred and twenty-six categories were used; 11 artifacts and 7 natural kind superordinate category terms, and 108 basic level category terms. (From each of superordinate category, 6 basic level categories were used.) Forty-four subjects <b>listed</b> <b>attributes</b> for each item. <b>Listed</b> <b>attributes</b> were classified into 8 categories; visual, audio, perceputual (non visual-audio), functionaluse, ability-state, source, part, and categorical attributes. The number of <b>listed</b> <b>attributes</b> of each attribute categories were compared by naturalness (artifacts or natural kinds) and by level of categories (basic level or superordinate level). The result mainly indicated that basic level categories elicit more visual and part attributes than superordinate categories, and natural kinds elicit more visual, perceptual, part and categorical attributes than artifacts...|$|R
5000|$|An {{undeclared}} variable is, by default, declared {{by first}} occurrence—thus misspelling {{might lead to}} unpredictable results. This [...] "implicit declaration" [...] {{is no different from}} FORTRAN programs. For PL/I(F), however, an <b>attribute</b> <b>listing</b> enables the programmer to detect any misspelled or undeclared variable.|$|R
30|$|The product <b>{{attributes}}</b> <b>listed</b> on {{the website}} are not always attributes in a strict sense. Some of the attributes are “basket” attributes, such as “characteristics,” which is too vague {{to be considered in}} the experiment. Moreover, different attribute names may refer to the same attribute.|$|R
30|$|If <b>attribute</b> <b>list</b> is empty, {{then return}} Nd as leaf node labeled {{with the most}} {{frequent}} class value in the observations of this node.|$|E
40|$|Datasets with a {{large number}} of {{attributes}} are a difficult challenge for evolutionary learning techniques. The recently proposed <b>attribute</b> <b>list</b> rule representation has shown to be able to significantly improve the overall performance (e. g. run-time, accuracy, rule set size) of the BioHEL Iterative Evolutionary Rule Learning system. In this paper we, first, extend the <b>attribute</b> <b>list</b> rule representation so it can handle not only continuous domains, but also datasets with {{a very large number of}} mixed discrete-continuous attributes. Secondly, we benchmark the new representation with a diverse set of large-scale datasets and, third, we compare the new algorithms with several well-known machine learning methods. The experimental results we describe in the paper show that the new representation is equal or better than the stateof-the-art in evolutionary rule representations both in terms of the accuracy obtained with the benchmark datasets used, as well as in terms of the computational time requirements needed to achieve these improved accuracies. The new <b>attribute</b> <b>list</b> representation puts BioHEL on an equal footing with other well-established machine learning techniques in terms of accuracy. In the paper, we also analyse and discuss the current weaknesses behind the current representation and indicate potential avenues for correcting them...|$|E
30|$|There {{are some}} main {{differences}} between our work and previous vector representations. Firstly, our representation of word meaning actually has two levels: the explicit level of concept vector {{which is similar}} to a property (<b>attribute)</b> <b>list</b> and an implicit level of similarity position, which could be viewed as an “untraditional” distributional vector in which each dimension is the similarity with other words. Secondly, compared with Gärdenfors’ work, this paper focuses on the computation of concept vectors, while Gärdenfors’ work is in the cognitive science domain and “not developing algorithms” as noted in the preface of Gärdenfors (2004).|$|E
5000|$|MVA, multi-value <b>attributes</b> (variable-length <b>lists</b> of 32-bit {{unsigned}} integers).|$|R
50|$|The {{documentation}} {{style of}} an issue log may differ from project to project. Some of <b>attributes</b> <b>listed</b> above may be considered unimportant to record, while other additional attributes may be necessary. However, main attributes such as description, author, priority, status, and resolution should always be included. Further, the sequence of attributes may differ as well.|$|R
50|$|For each file (or directory) {{described}} in the MFT record, there is a linear repository of stream descriptors (also named attributes), packed together {{in one or more}} MFT records (containing the so-called <b>attributes</b> <b>list),</b> with extra padding to fill the fixed 1 KB size of every MFT record, and that fully describes the effective streams associated with that file.|$|R
40|$|Abstract—Soft {{computing}} methodologies {{are characterized}} {{by the use of}} inexact solutions to computationally challenging tasks. One of the typical applications of Soft computing techniques is in finding the relation of input attributes to a target item. In this paper we intend to find the significant attributes from larger <b>attribute</b> <b>list</b> to find its impact on a target item. Using Support vector machine (SVM), the system has achieved a good prediction accuracy of 98. 06 %. This would aid the marketing team to predict their target customersby focusing on relevant components in data to plan an expert marketing strategy...|$|E
30|$|Shafer et al. {{proposed}} the SPRINT decision tree algorithm, {{which is based}} on SLIQ, in 1996 [34]. The SPRINT algorithm combines the property list and category list. The property list is used to store attribute values, a histogram plot is used to record the category distributions of the partition before and after a specified node, and a hash table is used instead of SLIQ to record the attribute sub-node information of the training tuple. The <b>attribute</b> <b>list</b> and histogram plot data structures do not require storage or memory, which eliminates the size limitation of the memorization capability. The SPRINT algorithm is not only simple, accurate, and fast, it also improves the data structure, which makes mining problems easier to solve.|$|E
40|$|Abstract In {{order to}} ease mobility, users {{should be able}} to access {{available}} services and resources of the local network without manually recongure its terminal at each visited network. Therefore, some kind of automatic mechanism is needed to dynamically locate the best available services from any network. The eXtensi-ble Service Discovery Framework (XSDF) is a novel solution to this problem. XSDF is an evolution of the Service Location Protocol (SLP) architecture, that also integrates the load balancing and high-availability capabilities from the Re-liable Server Pooling (Rserpool) framework in order to bridge together scalable service discovery with extensible load sharing selection policies. This paper pro-vides a brief overview of XSDF, and compares it against SLPv 2 (including its <b>Attribute</b> <b>List</b> Extension) employing several simulations in different scenarios. 1...|$|E
40|$|Logically, {{weighting}} is transitive, but similarity is not, so clustering {{cannot be}} either. Entailments must help {{a child to}} review <b>attribute</b> <b>lists</b> more efficiently. Children's understanding of exceptions to generic claims precedes their ability to articulate explanations. So agency, as enabling constraint, may show coherent covariation with attributes, as mere extensional, observable effect of intensional entailments...|$|R
5000|$|Inputs: Project Scope Statement, Activity <b>List,</b> Activity <b>Attributes,</b> Milestones <b>List,</b> Approved change {{requests}} ...|$|R
30|$|We need a “fine-grained” {{similarity}} {{model to}} capture the difference between within one category because product <b>attributes</b> <b>listed</b> on the website only represent part of the intension of the product. Generally, it is the part that it differs with other products in the same category. An initial idea is to combine different similarity models, especially those utilizes different resources, such as ontology or corpus (word vectors).|$|R
