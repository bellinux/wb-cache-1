0|48|Public
40|$|In {{the context}} of wider debates {{about the role of}} {{uncertainty}} in environmental science and the development of environmental policy, we use a Generalised Likelihood Uncertainty Estimate (GLUE) approach to address the uncertainty in both acid deposition model predictions and in the sensitivity of the soils to assess the likely success of policy actions to reduce acid deposition damage across Great Britain. A subset of 11, 699 acid deposition model runs that adequately represented observed deposition data were used to provide acid deposition distributions for 2005 and 2020, following a substantial reduction in SO 2 and NOx emissions. Uncertain critical loads data for soils were then combined with these deposition data to derive estimates of the <b>accumulated</b> <b>exceedance</b> (AE) of critical loads for 2005 and 2020. For the more sensitive soils, the differences in <b>accumulated</b> <b>exceedance</b> between 2005 and 2020 were such that we could be sure that they were significant and a meaningful environmental improvement would result. For the least sensitive soils, critical loads were largely met by 2020, hence uncertainties in the differences in <b>accumulated</b> <b>exceedance</b> were of little policy relevance. Our approach of combining estimates of uncertainty in both a pollution model and an effects model, shows that even taking these combined uncertainties into account, policy-makers can be sure that the substantial planned reduction in acidic emissions will reduce critical loads exceedances. The use of <b>accumulated</b> <b>exceedance</b> as a relative measure of environmental protection provides additional information to policy makers in tackling this ‘wicked problem’...|$|R
40|$|Atmospheric {{deposition}} of nitrogen {{is one of}} the most important threats to nature areas in the Netherlands. At a large scale the critical loads for nitrogen are exceeded to a great extend. Ammonia emissions from agricultural activities contribute over 50 % to the nitrogen deposition in the Netherlands. For nature areas, this contribution can be even higher because agricultural areas are often situated close to nature areas. This offers special opportunities for (sub) national policy makers to reduce the large nitrogen loads at a local scale by relocating ammonia emissions. In this study the ammonia emissions are relocated in such a way that nature areas are maximally protected given a national emission ceiling. This is carried out by minimising the exceedances of the critical loads for nitrogen for the nature areas and taken into account the atmospheric {{deposition of}} nitrogen from other sources and countries. This optimisation is carried out at a spatial scale of 1 x 1 km 2. For the ammonia emissions for 2010 (93 kton) an optimisation of the distribution of the emissions leads to a reduction in the <b>accumulated</b> <b>exceedances</b> of about 30 - 40 %. The percentages of the nature areas that is protected against the atmospheric deposition of nitrogen increases from 30 % to 40 - 50 %. An outlook for 2030 shows that the reductions in the <b>accumulated</b> <b>exceedances</b> are 40 - 60 % but that the percentage of protection is increased by only a few percent. The results are highly dependent on the assumptions made about the development of the agricultural practice. The effect of these relocations is used in the evaluation on whether local measures are effective compared to generic measures in reducing agricultural ammonia emissions...|$|R
40|$|The {{quality of}} surface water is rapidly {{changing}} due to climatic variations, natural processes, and anthropogenic activ i ties. The objectives {{of this study}} were to classify and analyze the surface water quality of 12 major rivers of Alberta on the basis of 17 parameters during the period of five years (i. e., 2004 - 2008) using principal component analysis (PCA), total <b>exceedance</b> <b>model</b> and clustering technique. Seven major principal components (PCs) with variability of about 89 % were identified. These PCs were the indicators of watershed geology, mineralization and anthropogenic activities related to land use/cover. The seven dominant parameters revealed from the seven PCs were total dissolved solids (TDS), true color (TC), pH, iron (Fe), fecal coliform (FC), dissolved oxygen (DO), and turbidity (TUR). The normal ized data of dominant parameters were used to develop a model for obtaining total exceedance. The exceedance values acquired from the total <b>exceedance</b> <b>model</b> were used to determine the patterns for the development of five clusters. The performance of the clusters was compared with the classes obtained in Canadian Water Quality Index (CWQI). Cluster 1, cluster 2, cluster 3, cluster 4 and cluster 5 showed agreements of 85. 71 %, 83. 54 %, 90. 22 %, 80. 74 %, and 83. 40 % with their respective CWQI classes {{on the basis of the}} data for all rivers during 2004 - 2008. The water quality was deterio rated in growing season due to snow melting. This methodology could be applied to classify the raw surface water quality, analyze the spatio-temporal trends and study the impacts of the factors affecting the water quality anywhere in the world. </p...|$|R
40|$|An {{efficient}} {{frequency response}} function (FRF) bounding method is proposed using asymptotic extreme-value theory. The method exploits a small random sample of realised FRFs obtained from nominally identical structures to predict corresponding FRF bounds for a substantially larger batch. This is useful for predicting forced-vibration levels in automotive vehicle bodies when parameters are assumed to vary statistically. Small samples are assumed to come either from Monte Carlo simulation using a vibration model, or via measurements from real structures. The basis of the method is to undertake a hypothesis test and if justified, repeatedly fit inverted Type I asymptotic threshold <b>exceedance</b> <b>models</b> at discrete frequencies, for which the models are not locked to a block size (as in classical extreme-value models). The chosen FRF `bound is predicted from the inverse model {{in the form of}} the `m-observational return level, namely the level that will be exceeded on average once in every m structures realised...|$|R
40|$|To date, national- and regional-scale flood risk {{assessments}} have provided valuable {{information about the}} annual expected consequences of flooding, but not the exposure to widespread concurrent flooding that could have damaging consequences for people and the economy. We present a new method for flood risk assessment that accommodates the risk of widespread flooding. It {{is based on a}} statistical conditional <b>exceedance</b> <b>model,</b> which is fitted to gauged data and describes the joint probability of extreme river flows or sea levels at multiple locations. The method can be applied together with data from models for flood defence systems and economic damages to calculate a risk profile describing the probability distribution of economic losses or other consequences aggregated over a region. The method has the potential to augment national or regional {{risk assessments}} of expected annual damage with new information about the likelihoods, extent and impacts of events that could contribute to the risk...|$|R
30|$|The {{same holds}} true for the maximum {{estimated}} exposure to N deposition below which significant harmful effects on specified sensitive elements of the environment are not assumed to occur according to present knowledge [1, 17 – 20]: such critical loads specifying that dose that can be deposited in ecosystems per area unit and period without any long-term harmful effect vary spatially, depending on receptor-specific characteristics. The area at risk due to the exceedance of critical loads of eutrophication and the average <b>accumulated</b> <b>exceedance</b> in EU 27 in 2000 and 2020 under the baseline scenario relying on national reports, i.e. representing current legislation, were estimated to amount for 74 % and 61 %, respectively. Under the maximum feasible reduction scenario, the area at risk in EU 27 could be 24 % [18]. In part, ecotoxicologically critical input levels, critical loads, for N are exceeded extensively. However, long-term exceedances of the critical N input rate can lead to an imbalance of nutrients and to changes in the species composition in sensitive ecosystems [4, 6, 17, 21, 22]. Hence, 15 % of the natural area within in the EU 27 countries can be seen at risk of significant change of biodiversity in 2000. This area is expected to be reduced to about 6 % under the Baseline scenario and to approximately 1 % under the Maximum Feasible Reduction scenario, respectively [18]. Nutrient imbalances can increase the sensitivity of plants to climatic extremes and to biotic pests [4, 6, 17, 22].|$|R
40|$|Multivariate {{challenges}} 3 Non-stationary extremes Penalised B-splines Quantile {{regression model}} for extreme value threshold Poisson model for rate of threshold <b>exceedance</b> Generalised Pareto <b>model</b> for size of threshold exceedance Return values 4 Current developments Extremal dependence Conditional extreme...|$|R
40|$|Abstract: The RAINS {{model is}} used to {{calculate}} cost minimising abatement policies subject to European-wide spatial restrictions on pollution. The principle for choosing environmental targets for the 1994 Oslo Protocol was closing a gap between benchmark- and critical loads for each grid with a uniform percentage. During the negotiations for the 1999 Gothenburg Protocol <b>accumulated</b> ecosystems <b>exceedances</b> was adapted as basis for gap closure, and overshooting of the constraints allowed as an option, provided compensation could be found within the same country. A theoretical discussion of this compensation mechanism is provided. A simulation study, using the full RAINS model, of the impact of different levels of targets for troublesome Norwegian grids is presented, and results in the form of changes in accumulated acidity excesses and costs for the participating countries are reported...|$|R
40|$|Flooding is {{a natural}} {{phenomenon}} that regularly causes financial and human devastation around the world. In many countries the risk of flooding is managed by society {{through a combination of}} governmental agencies and the insurance industry. For both these types of organisation an estimate of the largest, or most widespread, events that can be expected to occur is useful. Such estimates can be used to help in preparing or co-ordinating flood mitigation activities and by the insurance and re-insurance industries to assess financial risk. In this paper we develop a method to simulate a set of synthetic flood events {{that can be used to}} estimate the probability of widespread floods. We demonstrate this method using data from a set of UK river flow gauges. The model used in this simulation process is based on the conditional <b>exceedance</b> <b>model</b> of Heffernan and Tawn, extended to incorporate features typically found in the data for extreme river floods. We also present an improved estimation method for the model parameters and demonstrate its advantages through the results of a simulation study. The benefits of the method over previous models used are that it provides a theoretical basis for extrapolation and is flexible enough to account for varying strengths of extremal dependence that are observed in flood data...|$|R
40|$|The RAINS {{model is}} used to {{calculate}} cost minimising abatement policies subject to European-wide spatial restrictions on pollution. The principle for choosing environmental targets for the 1994 Oslo Protocol was closing a gap between benchmark- and critical loads for each grid with a uniform percentage. During the negotiations for the 1999 Gothenburg Protocol <b>accumulated</b> ecosystems <b>exceedances</b> was adapted as basis for gap closure, and overshooting of the constraints allowed as an option, provided compensation could be found within the same country. A theoretical discussion of this compensation mechanism is provided. A simulation study, using the full RAINS model, of the impact of different levels of targets for troublesome Norwegian grids is presented, and results in the form of changes in accumulated acidity excesses and costs for the participating countries are reported. Acid rain; RAINS; critical loads; gap closure; accumulated exceedances; compensation mechanism...|$|R
40|$|Critical loads {{have been}} used to develop {{international}} agreements on acidifying air pollution abatement, and within the UK and other countries, to develop national policies for pollution abatement. The Environment Agency (England and Wales) has regulatory obligations to protect sites of high conservation value from the threat of acidification, and hence requires a practical methodology for acidification assessments at the site-specific scale. The Environment Agency has therefore posed the question: Are the national critical load <b>exceedance</b> <b>models</b> sufficiently robust to form the basis for methods to assess harm to individual sites or are they only useful for national policy development? In order to provide one measure of the appropriateness of applying the models at the site-specific scale we incorporated estimates of uncertainty in both national and site-specific data into the calculation of critical load exceedance for individual sites. The exceedance calculations use data {{from a wide range of}} sources and the accuracy of the exceedance will be influenced by the accuracy of the input data sets. Using Monte Carlo methods to incorporate the uncertainty in the input data sets into the calculation a distribution of critical load exceedance values is generated rather than a single point estimate. This paper compares uncertainty analyses for coniferous forested sites in England and Wales using both national scale and site-specific data sets and uncertainty ranges...|$|R
40|$|Tools {{are needed}} that can add value to {{existing}} drought information and customize it for specific drought management contexts. This study develops a generalized framework {{that can be}} used to link local impacts with readily available drought information, thus increasing the usability of existing drought products in decision making. We offer a three-step risk-based framework that can be applied to specific decision-making contexts: (i) identify hydrologic impact thresholds, (ii) develop threshold <b>exceedance</b> <b>model,</b> and (iii) evaluate exceedance likelihood. The framework is demonstrated using a study site in south-central Oklahoma, which is highly susceptible to drought and faces management challenges. Stakeholder input from interviews are used to identify “moderate” and “extreme” thresholds below which water needs are not met for important uses. A logistic regression model translates existing drought information to the likelihood of exceeding the identified thresholds. The logistic model offers an improvement over climatology, and the 12 -month Standardized Precipitation Index is shown to be the best drought index predictor. The logistic model is used in conjunction with historical drought information to give a retrospective look at the risk of drought impacts from the beginning of the century. Results show the 1980 s to early 2000 s to be an anomalously wet period, and that recent drying trends and impacts do not appear to be unusual for the 20 th century. This drought risk analysis can be used as a baseline by local managers to guide future decision making under climate uncertainty...|$|R
40|$|The {{problem of}} {{statistically}} bounding {{the response of}} an engineering structure with random boundary conditions is addressed across the entire frequency range: from the low, through the mid, to the high frequency region. Extreme-value-based bounding of both the FRF and the energy density response is examined for a rectangular linear plate with harmonic point forcing. The proposed extreme-value (EV) approach, previously tested only in the low frequency region for uncoupled and acoustically-coupled uncertain structures, is examined here in the mid and high frequency regions, {{in addition to testing}} at low frequencies. EV-based bounding uses an asymptotic threshold <b>exceedance</b> <b>model</b> of Type-I, to extrapolate the m-observational return period to an arbitrarily-large batch of structures. It does this by repeatedly calibrating the threshold model at discrete frequencies using a small sample of response data generated by Monte Carlo simulation or measurement. Here the discrete singular convolution (DSC) method a transfrequency computation approach for deterministic vibration - is used to generate Monte Carlo samples. The accuracy of the DSC method is first verified i) in terms of the spatial distribution of total energy density, and ii) across the frequency range, by comparison with a mode superposition method and Statistical Energy Analysis (SEA). EV-based bound extrapolations of the receptance FRF and total energy density are then compared with: i) directly-estimated bounds using a full set of Monte Carlo simulations, and ii) with total mean energy levels obtained with SEA. The paper shows that for a rectangular plate structure with random boundary conditions, EV-based statistical bounding of both the FRF and total energy density response is generally applicable across the entire frequency range...|$|R
40|$|Attenuation {{measurement}} on Ku- band satellite signal in {{a tropical}} site, Fiji is presented. Rain-attenuation prediction by ITU-R and the Crane Global models showed noticeable deviation to the measured values. Unlike the monotonic decrease predicted by these <b>models,</b> <b>exceedance</b> of rain-rate and attenuation in Fiji and other tropical regions showed {{the presence of}} breakpoints. For Suva, the breakpoint in rain-rate and attenuation were at 58 mm/h and 9. 4 dB with exceedances of 0. 009 and 0. 018...|$|R
40|$|Sediment {{transport}} is {{a fundamental}} component of research into river morphology and related engineering practices. The relationship between flow and sediment particle entrainment underpins many of the empirical models used to estimate sediment transport dynamics. The scientific literature reports a research gap specific to the thresholds of mobility of different sized particles in non-gravel bed systems, including those in bedrock channels. Particle tracer technology was used to study coarse sediment entrainment and transport dynamics in an urban, bedrock controlled stream channel in Toronto, Ontario, Canada. Passive integrated transponders were inserted in constrained and unconstrained particles within an incised reach of stream. The distribution of particles transport distances conformed to a two-parameter gamma distribution model, which assumes integrations of the travelled series of steps and rests. Size selective dependency of path length was found to increase for coarser clasts, as compared to observed conditions for gravel-bed systems. Coarser particles were also found to transport in an unconstrained mode, as compared to finer grains. A force <b>exceedance</b> <b>model</b> was applied to further test the performance of reported size selective transport relationships for the study site. Many particles were found to transport at critical shear ratios less than 1, when assuming a modified Shields’s based model for entrainment. Field data was then used to determine a reference shear based on the smallest magnitude competent storm. The results show that, when compared to alluvial gravel-bed conditions, finer particles require larger thresholds to mobilize and the inverse is true for coarser particles. Using the reference shear conditions, rates of sediment transport were calculated and compared to common models for coarse particle transport. The results confirm size selectivity by grain class and indicate differentiations between fine and coarse transport relationships for the site. This research confirms non-conformity of particle entrainment and transport relationships for the study site, when compared to common empirical model for gravel-bed rivers. The results {{may be used to}} obtain critical entrainment parameters and sediment transport relationships, which can then be used to inform design criteria for regional watercourses having like lithology and morphology...|$|R
40|$|Thesis (Ph. D.), Engineering Science, Washington State UniversityMountain lakes {{are among}} the most {{sensitive}} and policy-relevant ecological indicators of atmospheric nitrogen deposition, but limited information is available about the effects of nitrogen deposition on Pacific Northwest mountain lakes. This dissertation assesses the sensitivity of Pacific Northwest mountain lakes to nitrogen deposition. The research herein is designed to generate information needed by air quality managers in federal land management agencies. Three aspects of N deposition effects in the Pacific Northwest are addressed. Chapter 2 evaluates the operational performance of several air quality models frequently used by researchers and air quality managers to estimate N deposition rates and test for critical load <b>exceedances</b> (<b>model</b> deposition > critical load). Results demonstrate that model uncertainty is frequently large relative to critical load values for diatoms and lichens in the Pacific Northwest, and could potentially cause false positive or false negative exceedance test results. Chapter 2 includes several recommendations regional air quality managers could use to minimize uncertainty in exceedance tests. Chapter 3 describes results of in situ nutrient enrichment bioassays conducted in mountain lakes within Mount Rainier, North Cascades, and Olympic National Parks. Experiments characterized phytoplankton species and biomass responses to nitrogen enrichment, and associated nitrogen concentration thresholds. This information is necessary to calculate critical loads and assess lake sensitivity to N deposition. Approximately 75 % of sampled lakes across the three parks have concentrations below experimentally-defined thresholds, suggesting many park mountain lakes are sensitive to future N deposition increases. Chapter 4 tests for differences in nitrate chemistry, phytoplankton biomass, and phytoplankton community structure between lakes with and without a glacier in their watershed at North Cascades National Park (NOCA). Glacier melting has increased nitrate concentrations in downstream streams and lakes in many mountainous regions, but its effects in the Pacific Northwest are not known. If glacier melting enriches mountain lakes with nitrate, nitrate may reduce sensitivity of glacier-fed lakes to future N deposition increases. Glacier-fed lakes at NOCA had slightly higher nitrate concentrations than snow-fed lakes, but there were no differences in phytoplankton biomass or community structure between lake types. Washington State University, Engineering ScienceBy student request, this dissertation cannot be exposed to search engines and is, therefore, only accessible to Washington State University users...|$|R
40|$|This study aims to {{undertake}} a statistical study to evaluate the accuracy of nine models that have been previously proposed for estimating the ultimate resistance of plate girders subjected to patch loading. For each model, mean errors and standard errors, {{as well as the}} probability of underestimating or overestimating patch load resistance, are estimated and the resultant values are compared one to another. Prior to that, the models are initially calibrated in order to improve interaction formulae using an experimental data set collected from the literature. The models are then analyzed by computing design factors associated with a target risk level (probability of <b>exceedance).</b> These <b>models</b> are compared one to another considering uncertainties existed in material and geometrical properties. The Monte Carlo simulation method is used to generate random variables. The statistical parameters of the calibrated models are calculated for various coefficients of variations regardless of their correlation with the random resistance variables. These probabilistic results are very useful for evaluating the stochastic sensitivity of the calibrated models...|$|R
40|$|Generalized linear (GL-) {{statistics}} {{are defined as}} functionals of an U-quantile process and unify different classes of statistics such as U-statistics and L-statistics. We derive a central limit theorem for GL-statistics of strongly mixing sequences and arbitrary dimension of the underlying kernel. For this purpose we establish a limit theorem for U-statistics and an invariance principle for U-processes together with a convergence rate for the remaining term of the Bahadur representation. An application is given by the generalized median estimator for the tail-parameter of the Pareto distribution, which is commonly used to <b>model</b> <b>exceedances</b> of high thresholds. We use subsampling to calculate confidence intervals and investigate its behaviour under independence and strong mixing in simulations...|$|R
40|$|This paper {{presents}} {{a strategy to}} quantify the influence major point sources in a region have on extreme pollution values observed {{at each of the}} monitors in the network. We focus on the number of hours in a day the levels at a monitor exceed a specified health threshold. The number of daily <b>exceedances</b> are <b>modeled</b> using observation-driven negative binomial time series regression models, allowing for a zero-inflation component to characterize the probability of no exceedances in a particular day. The spatial nature of the problem is addressed {{through the use of a}} Gaussian plume model for atmospheric dispersion computed at locations of known emissions, creating covariates that impact exceedances. In order to isolate the influence of emitters at individual monitors, we fit separate regression models to the series of counts from each monitor. We apply a final model clustering step to group monitor series that exhibit similar behavior with respect to mean, variability, and common contributors to support policy decision making. The methodology is applied to eight benzene pollution series measured at air quality monitors around the Houston ship channel, a major industrial port...|$|R
40|$|The lower Colorado River basin {{is located}} {{in an area of}} known El Nino-Southern Oscillation (ENSO) influence. A streamflow {{forecast}} is developed using Pacific Ocean Sea Surface Temperatures (SSTs) as predictors in addition to a traditional ENSO predictor, such as the Southern Oscillation Index (SOI). Significant regions of SST influence on streamflow were determined using linear correlations (LC). These significant SST regions are then used as predictors in a statistically based <b>exceedance</b> probability <b>model</b> previously applied to streamflow stations in Australia and the U. S. Long lead-time (3 and 6 month) streamflow forecasts were developed for El Nino, La Nina and non-ENSO years for the winter-spring (January-February-March - JFM) season. The use of the SSTs resulted in improved forecasts, based on cross-validated skill scores, when compared to forecasts using the SOI. Additionally, forecast leadtimes were increased when using the SSTs as predictors due to the inability of the SOI to provide an acceptable forecast. Also, the use of SSTs provided an improved forecast for all lead times for non-ENSO seasons when compared to the SOI forecasts. Following the methodology presented, water resource planners in ENSO influenced areas are provided a useful tool for forecasting streamflow...|$|R
40|$|Statistical {{methods for}} {{modelling}} extremes of stationary sequences have received much attention. The most common {{method is to}} model the rate and size of exceedances of some high constant threshold; the size of <b>exceedances</b> is <b>modelled</b> by using a generalized Pareto distribution. Frequently, data sets display non-stationarity; this is especially common in environmental applications. The ozone data set that is presented here {{is an example of}} such a data set. Surface level ozone levels display complex seasonal patterns and trends due to the mechanisms that are involved in ozone formation. The standard methods of modelling the extremes of a non-stationary process focus on retaining a constant threshold but using covariate models in the rate and generalized Pareto distribution parameters. We suggest an alternative approach that uses preprocessing methods to model the non-stationarity {{in the body of the}} process and then uses standard methods to model the extremes of the preprocessed data. We illustrate both the standard and the preprocessing methods by using a simulation study and a study of the ozone data. We suggest that the preprocessing method gives a model that better incorporates the underlying mechanisms that generate the process, produces a simpler and more efficient fit and allows easier computation. Copyright (c) 2009 Royal Statistical Society. ...|$|R
40|$|A Bayesian {{multiple}} change-point {{model is}} proposed to analyse violations of {{air quality standards}} by pollutants such as nitrogen oxides (NO 2 and NO) and carbon monoxide (CO). Threshold <b>exceedance</b> occurrences are <b>modelled</b> by a step rate Poisson process fitted after short-range correlations in the exceedance data are removed via declusterisation. The change-points are identified, and the rate function is estimated, using a reversible jump MCMC algorithm adapted from Green (1995). This technique {{is applied to the}} daily concentration data collected in Leeds, UK (1993 – 2009). Results are validated by running the MCMC estimator on the posterior-replicated data. Findings are discussed {{in the context of the}} past environmental actions and events. The proposed methodology may be useful for the air quality management by providing quantitative means to measure the efficacy of pollution control programmes...|$|R
40|$|Statistical {{inference}} for extremes {{has been}} a subject of intensive research during {{the past couple of}} decades. One approach is based on <b>modeling</b> <b>exceedances</b> of a random variable over a high threshold with the Generalized Pareto (GP) distribution. This has shown to be an important way to apply extreme value theory in practice and is widely used. In this paper we introduce a multivariate analogue of the GP distribution and show that it is characterized by each of following two properties: (i) exceedances asymptotically have a multivariate GP distribution if and only if maxima asymptotically are Extreme Value (EV) distributed, and (ii) the multivariate GP distribution is the only one which is preserved under change of exceedance levels. We also give a number of examples and discuss lowerdimensional marginal distribution...|$|R
40|$|We model extreme {{river flow}} data from five UK rivers with {{distinct}} hydrological properties. The data exhibit significant and complex nonstationarity, which we model using a nonlinear function of hydrological covariates corresponding to soil saturation, latent {{flow of the}} river and rainfall. We additionally consider season as a covariate, although the hydrological covariates explain most of the seasonal effect directly. The standard approach to modelling data of this kind is to fix a threshold and to <b>model</b> <b>exceedances</b> of this threshold using the generalised Pareto distribution. We identify a number of problems with this approach in nonstationary cases. To overcome these issues, we propose the use of a censored generalised extreme value distribution for threshold exceedances. The data analysis illustrates a number of features of model fit and in particular the stability of the model parameters and return levels to threshold choice...|$|R
40|$|It is {{difficult}} to find an existing single model which is able to simultaneously <b>model</b> <b>exceedances</b> over thresholds in multivariate financial time series. A new modeling approach, which is a combination of max-stable processes, GARCH processes, and Markov processes, is proposed. Combining Markov processes and max-stable processes defines a new statistical model which has the flexibility of modeling cross-sectional tail dependencies between risk factors and tail dependencies across time. The new model also models asymmetric behaviors of negative and positive returns on financial assets. An important application of the proposed method is to calculate value at risk (VaR) and evaluate portfolio combinations under VaR constraints. Result comparisons between VaRs based on the new approach and VaRs based on some existing methods such as variance–covariance approach and historical simulation approach suggest that some existing methods substantially underestimate the risks during recession and expansion time...|$|R
40|$|The {{likelihood}} of extreme daily changes in London Interbank Offer rates are estimated using the peaks-over-threshold method developed from extreme value theory. Value {{at risk and}} expected shortfall for high quantiles are produced for {{the left and right}} tails of the distributions for each maturity. The Generalized Pareto distribution of the peaks-over-threshold method is found to be unsuitable for <b>modeling</b> <b>exceedances</b> above a high threshold for samples of simple daily changes in the LIBOR. When the series are transformed to logarithmic daily changes, extreme value analysis proceeds smoothly and yields useful information about the relative frequency or magnitudes of extreme events. The main consequence of this is that the risk statistics associated with a given change in the LIBOR depend on the initial rate level; at higher (lower) interest rates, changes of a given size are more (less) likely to occur. ...|$|R
40|$|For {{more than}} a decade, {{anthropogenic}} sulfur (S) and nitrogen (N) deposition {{has been identified as}} a key pollutant in the Arctic. In this study new critical loads of acidity (S and N) were estimated for terrestrial ecosystems north of 60 A degrees latitude by applying the Simple Mass Balance (SMB) model using two critical chemical criteria (Al/Bc = 1 and ANC(le) = 0). Critical loads were exceeded in large areas of northern Europe and the Norilsk region in western Siberia during the 1990 s, with the more stringent criterion (ANC(le) = 0) showing the larger area of <b>exceedance.</b> However, <b>modeled</b> deposition estimates indicate that mean concentrations of sulfur oxides and total S deposition within the Arctic almost halved between 1990 and 2000. The modeled exceeded area is much reduced when currently agreed emission reductions are applied, and almost disappears under the implementation of maximum technically feasible reductions by 2020. In northern North America there was no exceedance under any of the deposition scenarios applied. Modeled N deposition was less than 5 kg ha(- 1) y(- 1) almost across the entire study area for all scenarios; and therefore empirical critical loads for the eutrophying impact of nitrogen are unlikely to be exceeded. The reduction in critical load exceedances is supported by observed improvements in surface water quality, whereas the observed extensive damage of terrestrial vegetation around the mining and smelter complexes in the area is mainly caused by direct impacts of air pollution and metals...|$|R
40|$|The revised Bathing Water Directive (rBWD) (2006 / 7 /EC) of the European Parliament {{requires}} {{monitoring of}} {{bathing water quality}} and, if early-warnings are provided to the public, it is permissible to discount a percentage of exceedance events from the monitoring process. This paper describes {{the development and implementation}} of both Decision Tree (DT) and Artificial Neural Network (ANN) based machine learning models for 8 beaches in south-west England, UK, as bases for early warning systems (EWS) and compares their performance for one beach. Weekly bacteria-count samples were gathered by the Environment Agency of England (EA) over a 12 -year period from 2000 - 2011 during the 20 -week bathing season and this data is used to calibrate and test the models. Daily sampling data were also collected at 5 of the beaches during the 2012 season to provide more robust validation of the models. As a benchmark, models are also compared with use of simple thresholds of antecedent rainfall to classify water quality exceedances. Evolutionary Algorithm-based optimisation of the ANN models is employed using single-objective approach using area under the Receiver Operating Characteristic (ROC) curve as fitness function. The optimum operating point is established using a weighting factor for the relative importance placed on false positives (passes) and false negatives (<b>exceedances).</b> The <b>models</b> use a number of input factors, including antecedent rainfall for the catchment adjacent to each bathing beach. A possible technique for automating selection of inputs is also discussed. Environment Agency (SW...|$|R
40|$|The {{generalized}} Pareto distribution (GPD) is a two-parameter {{family of}} distributions {{which can be}} used to <b>model</b> <b>exceedances</b> over a threshold. We compare the empirical coverage of some standard bootstrap and likelihood-based confidence intervals for the parameters and upper p-quantiles of the GPD. Simulation results indicate that none of the bootstrap methods give satisfactory intervals for small sample sizes. By applying a general method of D. N. Lawley, correction factors for likelihood ratio statistics of parameters and quantiles of the GPD have been calculated. Simulations show that for small sample sizes accuracy of confidence intervals can be improved by incorporating the computed correction factors to the likelihood-based confidence intervals. While the modified likelihood method has better empirical coverage probability, the mean length of produced intervals are not longer than corresponding bootstrap confidence intervals. This article also investigates the performance of some bootstrap methods for estimation of accuracy measures of maximum likelihood estimators of parameters and quantiles of the GPD...|$|R
40|$|Fire {{emissions}} {{associated with}} tropical land use change and maintenance influence atmospheric composition, air quality, and climate. In this study, we explore {{the effects of}} representing fire emissions at daily versus monthly resolution in a global composition-climate model. We find that simulations of aerosols are impacted more by the temporal resolution of fire emissions than trace gases such as carbon monoxide or ozone. Daily-resolved datasets concentrate emissions from fire events over shorter time periods {{and allow them to}} more realistically interact with model meteorology, reducing how often emissions are concurrently released with precipitation events and in turn increasing peak aerosol concentrations. The magnitude of this effect varies across tropical ecosystem types, ranging from smaller changes in modeling the low intensity, frequent burning typical of savanna ecosystems to larger differences when modeling the short-term, intense fires that characterize deforestation events. The utility of modeling fire emissions at a daily resolution also depends on the application, such as <b>modeling</b> <b>exceedances</b> of particulate matter concentrations over air quality guidelines or simulating regional atmospheric heating patterns...|$|R
40|$|In {{environmental}} applications it {{is found}} frequently that the extremes of a variable of interest are non-stationary, varying systematically in space, time or with the values of covariates. Multi-site datasets are common, and in such cases there {{is likely to be}} non-negligible inter-site dependence. We consider applications in which multi-site data are used to infer the marginal behaviour of the extremes at individual sites, while making proper adjustment for inter-site dependence. For reasons of statistical efficiency, modern extreme value analyses often <b>model</b> <b>exceedances</b> of a high threshold. Choosing an appro-priate threshold can be problematic, particularly if the extremes are non-stationary. We propose a method for setting covariate-dependent threshold using quantile regression. We consider how the quantile regression model and extreme value models fitted to threshold exceedances should be parameterised, in order that they are compatible. These consid-erations also suggest a new technique for selecting the level of extreme value thresholds. We adjust estimates of uncertainty for spatial dependence using methodology proposed recently. These methods are illustrated using time series of storm peak significant wave heights from 72 sites in the Gulf of Mexico...|$|R
40|$|Extreme value theory {{studies the}} tail {{behavior}} of a stochastic process, and {{plays a key role}} in a wide range of applications. Understanding and quantifying the behavior of rare events and the associated uncertainties is practically important for risk assessment, since such unexpected events can result in massive losses of wealth and high cost in human life. In this dissertation, we present a Bayesian nonparametric mixture modeling framework for the analysis of extremes with applications in financial industry and environmental sciences. In particular, the modeling is built from the point process approach to analysis of extremes, under which the pairwise observations, comprising the time of excesses and the exceedances over a high threshold, are assumed to arise from a non-homogeneous Poisson process. To relax the time homogeneity restriction, implicit in traditional parametric methods, a nonparametric Dirichlet process mixture model is presented to provide flexibility in estimation of the joint intensity of extremes, the marginal intensity over time, and different types of return level curves for one financial market. This class of models is then expanded to assess the effect of systemic risk in multiple financial markets. In this case, the process generating the extremes is modeled as a superposition of two Poisson process. This approach provides a decomposition of the risk associated with each individual market into two components: a systemic risk component and an idiosyncratic risk component. Finally, we extend the point process framework to model spatio-temporal extremes from environmental processes observed at multiple spatial locations over a certain time interval. Specifically, a spatially varying mixing distribution, assigned a spatial Dirichlet process prior, is incorporated into the model to develop inference for spatial interpolation of risk assessment quantities for high-level <b>exceedances.</b> The <b>modeling</b> approaches are illustrated with a number of simulated and real data examples...|$|R
40|$|Nutrient credit {{trading is}} a market-based policy {{currently}} proposed {{in several countries}} to mitigate nutrient pollution underpinning eutrophication and anoxic “dead zones” in coastal waters and lakes. Under nutrient credit trading programs regulated sources of nutrient pollution with high discharge compliance costs, such as wastewater treatment plants, are allowed to meet discharge restrictions by purchasing credits representing discharge reductions from other sources with low compliance costs, such as agricultural operations. The potential for cost savings is huge; also nutrient credit trading can, in theory, incentivize traditionally non-regulated sources, such as agriculture, to voluntarily adopt better nutrient management practices and thereby reduce nutrient pollution at an aggregate scale. Despite its intuitive appeal, and the successes of emissions trading markets for some atmospheric pollutants, the performance of nutrient credit trading is widely considered to be disappointing. A recent international survey by the World Resources Institute found over 50 nutrient trading programs at some early stage of development, yet very few trades had actually occurred. Many of the reasons offered by researchers for the apparent stagnation of nutrient credit trading appear {{to be due to}} learning impediments. Nutrient credit trading presents a difficult learning environment featuring multiple agents with disparate decision making criteria, complex feedback and stock-and-flow structures, and lengthy time lags between causes and effects. This research develops a system dynamics model to facilitate understanding of the dynamic complexity in nutrient credit trading systems. It is anticipated that following models developed on similar principles, and configured into appropriate user-friendly formats, may provide valuable insights to designers of these systems. The model explicitly takes into account feedback, stock-and-flow effects, time lags, and agent decision-making processes. A guiding principle in the model development was to capture the essential dynamic complexity of the nutrient trading system while striving to maximize model transparency and comprehensibility. A range of policy experiments were made with the primary objective of developing a realistic design that attains aggregate nutrient loading targets recommended by researchers. These experiments focus on settings and interactions between policy elements that include loading caps, trading ratios, exceedance penalties, investment of funds <b>accumulated</b> from <b>exceedance</b> penalty payments. Specific design recommendations are offered; however, these recommendations should not be considered definitive but exploratory in nature. The primary value of the research is viewed to be demonstration {{of the value of the}} system dynamics modeling approach to support trading policy design through simulation experiments and enhanced learning...|$|R
40|$|A rather recent {{approach}} for modelling extreme events is {{the so called}} peak over threshold (POT) method. The generalised Pareto distribution (GPD) is a twoparameter family of distributions {{which can be used}} to <b>model</b> <b>exceedances</b> over a threshold. We compare the empirical coverage of standard bootstrap and likelihood based confidence intervals for these parameters. Simulation results indicate that none of the methods give satisfactory intervals for small sample sizes. By applying a general method of D. N. Lawley, small sample correction factors for likelihood ratio statistics of parameters and quantiles of the GPD have been calculated. In many applications, lower limited condence interval for the shape parameter and upper limited confidence interval for the scale parameter of the GPD are of main interest. Simulations show that for sample sizes larger than 30 such intervals can be best constructed by incorporating the computed correction factors to the likelihood-based confidence intervals. While corrected likelihood method has better empirical coverage probability, the mean length of produced intervals are not longer than corresponding bootstrap confidence intervals. This article also investigates the performance of some bootstrap methods for estimation of accuracy measures of maximum likelihood estimators of parameters and quantiles of the GPD...|$|R
40|$|Mixed-mode {{buildings}} operate along {{a spectrum}} from sealed heating, ventilation and air-conditioning to 100 % naturally ventilated, but {{little is known}} about their occupants' comfort expectations and experiences. Exceedance metrics, which quantify the percentage of time that a building's environment falls outside an expected thermal comfort zone, can help address the comfort trade-offs in building design and operation. Practitioners were polled on exceedance use in practice and comfort <b>models</b> and <b>exceedance</b> metrics were analysed: several comfort standards using EnergyPlus simulations of a mixed-mode building with radiant cooling in California's 16 climate zones. Results indicate that comfort models from ASHRAE Standard 55, EN 15251, and the Dutch NPR-CR 1752 frequently differed by 10 percentage points, often with 2 - 4 percentage points across the adaptive <b>models.</b> Yet, recommended <b>exceedance</b> limits often fall between 3 and 5 % total. Exceedance predictions are also sensitive to uncertainties in predicted neutral comfort temperatures and variations in building envelop performance, solar heat gain, thermal mass, and control precision. Future work is needed to characterize comfort better in support of improved comfort <b>modelling,</b> <b>exceedance</b> targets, building design and building operation, and development of related codes and standards...|$|R
40|$|Parametric {{inference}} for spatial max-stable processes {{is difficult}} since the related likelihoods are unavailable. A composite likelihood approach {{based on the}} bivariate distribution of block maxima has been recently proposed in the literature. However modeling block maxima is a wasteful approach provided that other information is available. Moreover an approach based on block, typically annual, maxima is unable {{to take into account}} the fact that maxima occur or not simultaneously. If time series of, say, daily data are available, then estimation procedures based on exceedances of a high threshold could mitigate such problems. In this paper we focus on two approaches for composing likelihoods based on pairs of exceedances. The first one comes from the tail approximation for bivariate distribution proposed by Ledford and Tawn (1996) when both pairs of observations exceed the fixed threshold. The second one uses the bivariate extension (Rootzen and Tajvidi, 2006) of the generalized Pareto distribution which allows to <b>model</b> <b>exceedances</b> when at least one of the components is over the threshold. The two approaches are compared through a simulation study according to different degrees of spatial dependency. Results show that both the strength of the spatial dependencies and the threshold choice play a fundamental role in determining which is the best estimating procedure...|$|R
