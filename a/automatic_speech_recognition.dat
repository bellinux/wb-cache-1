4067|10000|Public
25|$|Hidden Markov {{models are}} the basis for most modern <b>automatic</b> <b>speech</b> <b>recognition</b> systems.|$|E
25|$|In {{order to}} {{minimize}} the cost of power networks, wiring connections, piping, <b>automatic</b> <b>speech</b> <b>recognition,</b> etc., people often use algorithms that gradually build a spanning tree (or many such trees) as intermediate steps {{in the process of}} finding the minimum spanning tree.|$|E
25|$|Similar to this concept, eigenvoices {{represent}} {{the general direction}} of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in <b>automatic</b> <b>speech</b> <b>recognition</b> systems for speaker adaptation.|$|E
40|$|In this chapter, we {{describe}} our recent advances on representation and modelling of <b>speech</b> signals for <b>automatic</b> <b>speech</b> and speaker <b>recognition</b> in noisy conditions. The research {{is motivated by}} the need for improvements in these research areas in order the <b>automatic</b> <b>speech</b> and speaker <b>recognition</b> systems could be fully employed in real-world application...|$|R
40|$|We {{approached}} {{our first}} participation in TREC {{with an interest}} in performing retrieval on the output of <b>automatic</b> speech-to-text (<b>speech</b> <b>recognition)</b> systems and a background in performing topic-labeling on such output. Our primary thrust, therefore, was to participate in the SDR track. In conformance with the rules, we also participated in the Ad Hoc text-retrieval task, to create...|$|R
40|$|This paper {{introduces}} our developed noise robust speech communication {{techniques and}} describes its implementation to a smart info-media system, i. e., a small robot. Our designed speech communication system consists of <b>automatic</b> <b>speech</b> detection, <b>recognition,</b> and rejection. By using <b>automatic</b> <b>speech</b> detection and <b>recognition,</b> an observed <b>speech</b> waveform can be recognized without a manual trigger. In addition, using speech rejection, this system only accepts registered speech phrases and rejects any other words. In other words, although an arbitrary input speech waveform can be fed into this system and recognized, the system responds {{only to the}} registered speech phrases. The developed noise robust speech processing can reduce various noises in many environments. In addition {{to the design of}} noise robust <b>speech</b> <b>recognition,</b> the LSI design of this system has been introduced. By using the design of <b>speech</b> <b>recognition</b> application specific IC (ASIC), we can simultaneously realize low power consumption and real-time processing. This paper describes the LSI architecture of this system and its performances in some field experiments. In terms of current <b>speech</b> <b>recognition</b> accuracy, the system can realize 85 - 99 % under 0 - 20 dB SNR and echo environments...|$|R
2500|$|Frederick Jelinek (18 November 1932 – 14 September 2010) was a Czech-American {{researcher}} in information theory, <b>automatic</b> <b>speech</b> <b>recognition,</b> and natural language processing. He {{is well known}} for his oft-quoted statement, [...] "Every time I fire a linguist, the performance of the speech recognizer goes up".|$|E
5000|$|... #Subtitle level 2: Text-to-speech and <b>Automatic</b> <b>Speech</b> <b>Recognition</b> support ...|$|E
50|$|Early in his career, he {{concentrated}} on <b>automatic</b> <b>speech</b> <b>recognition</b> and signal processing.|$|E
40|$|Proceedings of: IberSPEECH 2012 Conference, Madrid, Spain, November 21 - 23, 2012. A speech {{denoising}} method {{based on}} Non-Negative Matrix Factorization (NMF) {{is presented in}} this paper. With respect to previous related works, this paper makes two contributions. First, our method does not assume a priori knowledge {{about the nature of}} the noise. Second, it combines the use of the Kullback-Leibler divergence with sparseness constraints on the activation matrix, improving the performance of similar techniques that minimize the Euclidean distance and/or do not consider any sparsification. We evaluate the proposed method for both, <b>speech</b> enhancement and <b>automatic</b> <b>speech</b> <b>recognitions</b> tasks, and compare it to conventional spectral subtraction, showing improvements in <b>speech</b> quality and <b>recognition</b> accuracy, respectively, for different noisy conditions. This work has been partially supported by the Spanish Government grants TSI- 020110 - 2009 - 103 and TEC 2011 - 26807. Publicad...|$|R
40|$|In this paper, a {{cross-media}} browsing demonstrator named InfoLink is described. InfoLink automatically {{links the}} content of Dutch broadcast news videos to related information sources in parallel collections containing text and/or video. <b>Automatic</b> segmentation, <b>speech</b> <b>recognition</b> and available meta-data are used to index and link items. The concept is visualised using SMIL-scripts for presenting the streaming broadcast news video and the information links. 1...|$|R
40|$|Research in <b>automatic</b> <b>speech</b> {{and speaker}} <b>recognition</b> has now spanned five decades. This paper surveys the major themes and {{advances}} {{made in the}} past fifty years of research so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. Although many techniques have been developed, many challenges have yet to be overcome before we can achieve the ultimate goal of creating machines that can communicate naturally with people. Such a machine {{needs to be able to}} deliver a satisfactory performance under a broad range of operating conditions. A much greater understanding of the human speech process is required before <b>automatic</b> <b>speech</b> and speaker <b>recognition</b> systems can approach human performance. 1...|$|R
50|$|Hidden Markov {{models are}} the basis for most modern <b>automatic</b> <b>speech</b> <b>recognition</b> systems.|$|E
50|$|Frederick Jelinek, a Czechoslovakian-born {{researcher}} in information theory, <b>automatic</b> <b>speech</b> <b>recognition,</b> and natural language processing.|$|E
5000|$|Vaissière, Jacqueline. 1985. The use of {{prosodic}} parameters in <b>automatic</b> <b>speech</b> <b>recognition.</b> Computer, Speech and language. Prentice Hall International.|$|E
40|$|Abstract- In {{the field}} of human {{computer}} interaction <b>automatic</b> <b>speech</b> emotion <b>recognition</b> is a current research topic. Emotion <b>recognition</b> in <b>speech</b> is a challenging problem because it is unclear that which features are effective for <b>speech</b> emotion <b>recognition.</b> In this paper we proposed an approach in which we extract the features of energy, spectral and acoustic domains and then merging these features by Principal Component Analysis(PCA) and then we get a new hybrid feature that feed into a Gaussian mixture model with kernel approach and analysis its accuracy, precision, recall and ROC curve...|$|R
40|$|<b>Automatic</b> speaker-independent <b>speech</b> <b>recognition</b> {{has made}} {{significant}} progress {{from the days of}} isolated word recognition. Today state of the art systems are capable of performing large-vocabulary continuous <b>speech</b> <b>recognition</b> (LVCSR) over complex domains such as news broadcasts and telephone conversations. A significant contribution to this advancement in technology is due to the development of search techniques that support efficient, sub-optimal decoding over large search spaces and complex statistical models. Moreover, these decoding strategies are capable of dynamically integrating information from a number of diverse knowledge sources to determine the correct word hypothesis...|$|R
40|$|In this paper, {{we present}} a semi-supervised method for <b>automatic</b> <b>speech</b> act <b>recognition</b> in email and forums. The major {{challenge}} of this task is {{due to lack of}} labeled data in these two genres. Our method leverages labeled data in the Switchboard-DAMSL and the Meeting Recorder Dialog Act database and applies simple domain adaptation techniques over a large amount of unlabeled email and forum data to address this problem. Our method uses automatically extracted features such as phrases and dependency trees, called subtree features, for semi-supervised learning. Empirical results demonstrate that our model is effective in email and forum <b>speech</b> act <b>recognition.</b> ...|$|R
50|$|A speech {{translation}} system would typically integrate {{the following three}} software technologies: <b>automatic</b> <b>speech</b> <b>recognition</b> (ASR), machine translation (MT) and voice synthesis (TTS).|$|E
50|$|AppTek {{has been}} {{involved}} in <b>automatic</b> <b>speech</b> <b>recognition</b> since the acquisition of AIXPLAIN, covering broadcast voice and conversational telephony in various languages and dialects.|$|E
50|$|Morgan was the {{principal}} investigator of the IARPA-funded project Outing Unfortunate Characteristics of HMMs, which sought to identify problems in <b>automatic</b> <b>speech</b> <b>recognition</b> technology.|$|E
3000|$|... of the {{underlying}} Markov chain, independently for any n. The applications of Hidden Markov processes include <b>automatic</b> character and <b>speech</b> <b>recognition,</b> information theory, statistics and bioinformatics, see [4, 13]. The particular example (1.1) we consider in the present paper {{is probably one of}} simplest examples, and often used as a benchmark for testing algorithms.|$|R
40|$|AbstractIn {{this paper}} {{we present a}} new wavelet-based {{algorithm}} for low-cost computation of the cepstrum. It {{can be used for}} real time precise pitch determination in <b>automatic</b> <b>speech</b> and speaker <b>recognition</b> systems. Many wavelet families are examined to determine the one that works best. The results confirm the efficacy and accuracy of the proposed technique for pitch extraction...|$|R
40|$|International audienceThis paper {{presents}} a new architecture for <b>automatic</b> continuous <b>speech</b> <b>recognition</b> called HEAR - Hybrid Episodic-Abstract speech Recognizer. HEAR relies on both parametric speech models (HMMs) and episodic memory. We propose an evaluation on the Wall Street Journal corpus, a standard continuous <b>speech</b> <b>recognition</b> task, {{and compare the}} results with a state-of-the-art HMM baseline. HEAR is {{shown to be a}} viable and a competitive architecture. While the HMMs have been studied and optimized during decades, their performance seems to converge to a limit which is lower than human performance. On the contrary, episodic memory modeling for <b>speech</b> <b>recognition</b> as applied in HEAR offers flexibility to enrich the recognizer with information the HMMs lack. This opportunity as well as future work are exposed in a discussion...|$|R
50|$|LumenVox {{is one of}} {{the major}} {{providers}} of <b>automatic</b> <b>speech</b> <b>recognition</b> for telephone systems, and as of 2006, became the second largest provider of speech recognition software.|$|E
5000|$|Amazon Lex {{was opened}} to {{developers}} in April 2017. It involves {{natural language understanding}} technology combined with <b>automatic</b> <b>speech</b> <b>recognition</b> and had been introduced in November 2016.|$|E
50|$|Applications Technology (AppTek) is a U.S. {{software}} company specializing in human language technology (<b>automatic</b> <b>speech</b> <b>recognition,</b> machine translation, NLP, machine learning and artificial intelligence), headquartered in McLean, Virginia.|$|E
40|$|What is the {{appropriate}} spatial scale for image representation? In the primate visual system, receptive fields are small at early stages of processing (area V 1), and larger at late stages of processing (areas MT, IT). In the current work, we explore the efficiency of local and global image representations on an <b>automatic</b> visual <b>speech</b> <b>recognition</b> task using an HMM as the recognition system. We compare local and global principal component and independent component image representations for the task. Local representations consistently and significantly outperformed global representations in terms of generalization to new speakers...|$|R
40|$|The {{pronunciation}} variability is {{an important}} issue that must be faced with when developing practical <b>automatic</b> spontaneous <b>speech</b> <b>recognition</b> systems. By studying the initial/final (IF) characteristics of Chinese language and developing the Bayesian equation, we propose the concepts of generalized initial/final (GIF) and generalized syllable (GS), the GIF modeling and the IF-GIF modeling, as well as the context-dependent pronunciation weighting. By using these methods, the IF-GIF modeling reduces the Chinese syllable error rate by 6. 3 % and 4. 2 % compared with the GIF modeling and IF modeling respectively...|$|R
40|$|This {{paper is}} about {{different}} methods and algorithms {{that were used}} for speaker identification from the video recordings of TV broadcast news transcription. The information from visual speaker identification were used in our complex system for <b>automatic</b> continuous <b>speech</b> <b>recognition</b> of TV broadcast programs because {{it is possible to}} use speaker adapted (SA) Hidden Markov Models (HMMs) if we have the information about a speaker and the resulting recognition rate is higher than if only the speaker independent (SI) HMMs are used. These methods for visual speaker identification were evaluated according to the highest recognition rate in this work. 1...|$|R
5000|$|VoiceXML has tags that {{instruct}} the voice browser to provide speech synthesis, <b>automatic</b> <b>speech</b> <b>recognition,</b> dialog management, and audio playback. The {{following is an}} example of a VoiceXML document: ...|$|E
50|$|Emeritus Professor Mary Josephine O'Kane , (born in 1954 in , Queensland) an Australian scientist, is the Chief Scientist and Engineer to Government of New South Wales. O'Kane is {{an expert}} in <b>automatic</b> <b>speech</b> <b>recognition.</b>|$|E
50|$|The toolkit {{includes}} {{newly developed}} speech recognition {{technology for the}} development of <b>automatic</b> <b>speech</b> <b>recognition</b> systems. It has been developed by the Human Language Technology and Pattern Recognition Group at RWTH Aachen University.|$|E
40|$|The {{problem of}} {{information}} overload {{can be solved}} by the application of information filtering to the huge amount of data. Information {{on radio and television}} can be filtered using <b>speech</b> <b>recognition</b> of the audio track. A prototype system using closed captions has been developed on top of the INQUERY information access system. The challange of integrating <b>speech</b> <b>recognition</b> and information retrieval into a working system is a big one. The open problems are the selection of a document representation model, the recognition and selection of indexing features for speech retrieval and dealing with the erroneous output of recognition processes. Keywords: multimedia, multimedia representation, content-based retrieval, information filtering, <b>automatic</b> indexing, <b>speech</b> <b>recognition,</b> content analysis, probabilistic information retrieval. 1 Introduction The Indexing a Sea of Audio project forms a foundation for multimedia information filtering projects at the Cambridge Research Laboratory (CRL [...] ...|$|R
40|$|The {{study of}} {{emotions}} in human-computer interaction {{is a growing}} research area. Focusing on automatic emotion recognition, work is being performed {{in order to achieve}} good results particularly in speech and facial gesture recognition. In this paper we present a study performed to analyze different machine learning techniques validity in <b>automatic</b> <b>speech</b> emotion <b>recognition</b> area. Using a bilingual affective database, different speech parameters have been calculated for each audio recording. Then, several machine learning techniques have been applied to evaluate their usefulness in <b>speech</b> emotion <b>recognition.</b> In this particular case, techniques based on evolutive algorithms (EDA) have been used to select speech feature subsets that optimize automatic emotion recognition success rate. Achieved experimental results show a representative increase in the abovementioned success rate. 1...|$|R
40|$|<b>Automatic</b> <b>Speech</b> Emotion <b>Recognition</b> (SER) is {{a current}} {{research}} {{topic in the}} field of Human Computer Interaction (HCI) with wide range of applications. The speech features such as, Mel Frequency cepstrum coefficients (MFCC) and Mel Energy Spectrum Dynamic Coefficients (MEDC) are extracted from speech utterance. The Support Vector Machine (SVM) is used as classifier to classify different emotional states such as anger, happiness, sadness, neutral, fear, from Berlin emotional database. The LIBSVM is used for classification of emotions. It gives 93. 75 % classification accuracy for Gender independent case 94. 73 % for male and 100 % for female speech. Categories and Subject Descriptor...|$|R
