1|4638|Public
40|$|Sampling {{theories}} lie at {{the heart}} of signal processing devices and communication systems. To accommodate high operating rates while retaining low computational cost, efficient analog-to digital (ADC) converters must be developed. Many of limitations encountered in current converters are due to a traditional assumption that the sampling state needs to acquire the data at the Nyquist rate, corresponding to twice the signal bandwidth. In this thesis a method of sampling far below the Nyquist rate for sparse spectrum multiband signals is investigated. The method is called periodic non-uniform sampling, and it is useful in a variety of applications such as data converters, sensor array imaging and image compression. Firstly, a model for the sampling system in the frequency domain is prepared. It relates the Fourier transform of observed compressed samples with the unknown spectrum of the signal. Next, the reconstruction process based on the topic of compressed sensing is provided. We show that the sampling parameters play an important role on the <b>average</b> <b>sample</b> <b>ratio</b> and the quality of the reconstructed signal. The concept of condition number and its effect on the reconstructed signal in the presence of noise is introduced, and a feasible approach for choosing a sample pattern with a low condition number is given. We distinguish between the cases of known spectrum and unknown spectrum signals respectively. One of the model parameters is determined by the signal band locations that in case of unknown spectrum signals should be estimated from sampled data. Therefore, we applied both subspace methods and non-linear least square methods for estimation of this parameter. We also used the information theoretic criteria (Akaike and MDL) and the exponential fitting test techniques for model order selection in this case...|$|E
40|$|Periodic {{nonuniform}} sampling {{has been}} considered in literature as an effective approach to reduce the sampling rate far below the Nyquist rate for sparse spectrum multiband signals. In the presence of non-ideality the sampling parameters {{play an important role}} on the quality of reconstructed signal. Also the <b>average</b> <b>sampling</b> <b>ratio</b> is directly dependent on the sampling parameters that they should be chosen for a minimum rate and complexity. In this paper we consider the effect of sampling parameters on the reconstruction error and the <b>sampling</b> <b>ratio</b> and suggest feasible approaches for achieving an optimal sampling and reconstruction...|$|R
40|$|This study aims {{to better}} {{understand}} the effect of catchment scale and climate on the statistical properties of regional flood frequency distributions. A database of L-moment ratios of annual maximum series (AMS) of peak discharges from Austria, Italy and Slovakia, involving a total of 813 catchments with more than 25 yr of record length is presented, together with mean annual precipitation (MAP) and basin area as catchment descriptors surrogates of climate and scale controls. A purely data-based investigation performed on the database shows that the generalized extreme value (GEV) distribution provides a better representation of the <b>averaged</b> <b>sample</b> L-moment <b>ratios</b> compared to the other distributions considered, for catchments with medium to higher values of MAP independently of catchment area, while the three-parameter lognormal distribution is probably a more appropriate choice for drier (lower MAP) intermediate-sized catchments, which presented higher skewness values. <b>Sample</b> L-moment <b>ratios</b> do not follow systematically any of the theoretical two-parameter distributions. In particular, the averaged values of L-coefficient of skewness (L-Cs) are always larger than Gumbel 's fixed L-Cs. The results presented in this paper contribute to the progress in defining a set of "process-driven" pan-European flood frequency distributions and to assess possible effects of environmental change on its properties...|$|R
40|$|This study {{addresses}} {{the question of}} the existence of a parent flood frequency distribution on a European scale and aims to better understand the effect of catchment scale and climate on the statistical properties of regional flood frequency distributions. A new database of L-moment ratios of annual maximum series (AMS) of peak discharges from 4105 catchments was compiled by joining 13 national datasets. Using this database and additional Monte Carlo simulations, the Generalised Extreme Value (GEV) distribution appears as a potential pan-European flood frequency distribution, being the 3 -parameter statistical model with the closest resemblance to the estimated <b>average</b> of the <b>sample</b> L-moment <b>ratios,</b> but failing to represent the kurtosis dispersion, especially for high skewness values. A more detailed investigation performed on a subset of the database (Austria, Italy and Slovakia, involving a total of 813 catchments with more than 25 yr of record length) confirms that the GEV distribution provides a better representation of the <b>averaged</b> <b>sample</b> L-moment <b>ratios</b> compared to the other distributions considered, for catchments with medium to high values of mean annual precipitation (MAP) independently of catchment area, while the 3 -parameter Lognormal distribution is probably a more appropriate choice for dry (low MAP) intermediate-sized catchments, which presented higher skewness values. <b>Sample</b> L-moment <b>ratios</b> do not follow systematically any of the theoretical 2 -parameter distributions. In particular, the averaged values of L-coefficient of skewness (L- Cs) are always larger than Gumbel 's fixed L- Cs. The results presented in this paper contribute to progress towards the definition of a set of pan-European flood frequency distributions and to assess possible effects of environmental change on its properties...|$|R
40|$|Currently, {{a lack of}} {{accurate}} emission data exits for {{particulate matter}} (PM) in agricultural air quality studies (USDA-AAQTF, 2000). PM samplers, however, tend to over estimate the concentration of most agricultural dusts because of {{the interaction of the}} particle size distribution (PSD) and performance characteristics of the sampler (Buser, 2004). This research attempts to find a practical method to characterize and correct this error for the Federal Reference Method (FRM) PM 10 sampler. First, a new dust wind tunnel testing facility that satisfies the USEPA?s requirement of testing PM 10 samplers was designed, built, and evaluated. Second, the wind tunnel testing protocol using poly-dispersed aerosol as the test dust was proved to be able to provide results consistent with mono-dispersed dusts. Third, this study quantified the variation of over <b>sampling</b> <b>ratios</b> for the various cut point and slopes of FRM PM 10 samplers and proposed an <b>averaged</b> over <b>sampling</b> <b>ratio</b> as a correction factor for various ranges of PSD. Finally, a method of using total suspended particle (TSP) samplers as a field reference for determining PM 10 concentrations and aerosol PSD was explored computationally. Overall, this dissertation developed successfully the methodology to correct the sampling error associated with the FRM PM 10 sampler: (1) wind tunnel testing facilities and protocol for experimental evaluation of samplers; (2) the variation of the oversampling ratios of FRM PM 10 samplers for computational evaluation of samplers; (3) the evaluation of TSP sampler effectiveness as a potential field reference for field evaluation of samplers...|$|R
40|$|We present alpha {{element to}} iron {{abundance}} ratios, [α/Fe], for four {{stars in the}} outer stellar halo of the Andromeda Galaxy (M 31). The stars were identified as high-likelihood field halo stars by Gilbert et al. (2012) and lie at projected distances between 70 and 140 kpc from M 31 's center. These are the first alpha abundances measured for a halo star in a galaxy beyond the Milky Way. The stars range in metallicity between [Fe/H]= - 2. 2 and [Fe/H]= - 1. 4. The <b>sample's</b> <b>average</b> [α/Fe] <b>ratio</b> is + 0. 20 +/- 0. 20. The best-fit average value is elevated above solar {{which is consistent with}} rapid chemical enrichment from Type II supernovae. The mean [α/Fe] ratio of our M 31 outer halo sample agrees (within the uncertainties) with that of Milky Way inner/outer halo stars that have a comparable range of [Fe/H]. Comment: 6 pages, 5 figures, resubmitted to ApJ Letters after referee repor...|$|R
3000|$|... via {{two or more}} {{sampling}} in some {{order with}} given <b>sampling</b> <b>ratios.</b> The <b>sampling</b> <b>ratios</b> of the finally sampled point set [...]...|$|R
40|$|It is {{well known}} that {{samplers}} are linear time varying systems, so in general, the commutativity of samplers does not hold. There are some existing results on the commutativity of conventional decimators and expanders, block samplers with the same integer block lengths but different integer <b>sampling</b> <b>ratios,</b> and block samplers with different integer block lengths and integer <b>sampling</b> <b>ratios.</b> This paper extends the existing results to a necessary and sufficient condition for the commutativity of block decimators and expanders with arbitrary rational <b>sampling</b> <b>ratios</b> and block lengths. Â© 2012 Elsevier Inc. All rights reserved...|$|R
30|$|The {{noise figure}} {{of the system is}} further {{improved}} by using the Q enhancement technique. Although the receiver has already exhibited good performance with the simple ASK modulation and the <b>sampling</b> <b>ratio</b> of 20, more advanced modulation and the higher <b>sampling</b> <b>ratio</b> can be used to further improve the performance of the system.|$|R
30|$|It is {{not clear}} what <b>sampling</b> <b>ratios</b> (90 : 10, 75 : 25, etc.) were used with RUS, ROS, and SMOTE, {{limiting}} the study without investigating the impact of various <b>sampling</b> <b>ratio</b> values of classification performance; impact of reducing the number of features to 90 (from 631) on the various experiments is not discussed.|$|R
40|$|Received; {{accepted}} We use XMM-Newton data {{to carry}} out a detailed study of the Si, Fe and Ni abundances in the cool cores of a representative sample of 26 local clusters. We have performed a careful evaluation of the systematic uncertainties related to the instruments, the plasma codes and the spectral modeling finding that the major source of uncertainty is in the plasma codes. Our Si, Fe, Ni, Si/Fe and Ni/Fe distributions feature only moderate spreads (from 20 % to 30 %) around their mean values strongly suggesting similar enrichment processes at work in all our cluster cores. Our <b>sample</b> <b>averaged</b> Si/Fe <b>ratio</b> is comparable to those measured in samples of groups and high luminosity ellipticals implying that the enrichment process in ellipticals, dominant galaxies in groups and BCGs in clusters is quite similar. Although our Si/Fe and Ni/Fe abundance ratios are fairly well constrained, the large uncertainties in the supernovae yields prevent us from making a firm assessment of the relative contribution of type Ia and core-collapsed supernovae to the enrichment process. All that can really be said with some certainty is that both contribute to the enrichment of cluster cores...|$|R
5000|$|... where ri' is {{the value}} of the <b>sample</b> <b>ratio</b> with the ith group omitted.|$|R
40|$|Abstract. Although {{much work}} has been done on {{duplicate}} document detection (DDD) and its applications, we observe the absence of a systematic study of the performance and scalability of large-scale DDD. It is still unclear how various parameters of DDD, such as similarity threshold, precision/recall requirement, <b>sampling</b> <b>ratio,</b> document size, correlate mutually. In this paper, correlations among several most important parameters of DDD are studied and the impact of <b>sampling</b> <b>ratio</b> is of most interest since it heavily affects the accuracy and scalability of DDD algorithms. An empirical analysis is conducted on a million documents from the TREC. GOV collection. Experimental results show that even using the same <b>sampling</b> <b>ratio,</b> the precision of DDD varies greatly on documents with different size. Based on this observation, an adaptive sampling strategy for DDD is proposed, which minimizes the <b>sampling</b> <b>ratio</b> within the constraint of a given precision threshold. We believe the insights from our analysis are helpful for guiding the future large scale DDD work. ...|$|R
40|$|AbstractAverage {{sampling}} {{is motivated}} by realistic needs, e. g., physical limitation of acquisition devices or characteristics of sampling procedure. As {{an extension of the}} <b>average</b> <b>sampling,</b> we study generalized <b>average</b> <b>sampling</b> in shift invariant spaces with frame generators, in which averages are taken from suitable channeled version of signals. Illustrative examples are given...|$|R
5000|$|Methods and Apparatus of Obtaining <b>Average</b> <b>Samples</b> and Temperature of Tank Liquids (1932) ...|$|R
3000|$|... s, and the {{integration}} {{window of the}} charge sampling architecture can be depended on <b>sampling</b> <b>ratio</b> f/f [...]...|$|R
40|$|Graduation date: 1984 The <b>sample</b> <b>ratio</b> of sublegal {{to legal}} male crabs {{retained}} in crab pots {{was used in}} calculations to predict future harvest of Dungeness crabs, Cancer magister, along the Oregon coast. Accurate predictions by this method require a <b>sample</b> <b>ratio</b> representative of the population <b>ratio.</b> I <b>sampled</b> the Oregon Dungeness crab population out of the ports of Astoria and Newport during the 1974 - 75 and 1975 - 76 crab seasons. Samples were taken with commercial crab pots modified to retain sublegal as well as legal crabs. My predictions of future harvest of Dungeness crab were inaccurate because the <b>sample</b> <b>ratio</b> was an inaccurate and biased estimator of the population ratio of sublegal to legal crabs. My {{results indicate that the}} samples obtained by my sampling gear and methods were not representative of the number of sublegal and legal crabs in the population. The <b>sample</b> <b>ratio</b> may be improved as an estimator of the population ratio by decreasing the time the modified crab pots are allowed to fish and by random sampling of the Dungeness crab population...|$|R
3000|$|... sense. In 2009, Song, Wang and Xie [18] {{proved that}} second-order moment {{processes}} can {{be approximated by}} <b>average</b> <b>sampling.</b> Recently, He and Song [19] proved that a real-valued weak stationary process can be approximated by its local average in the almost sure sense. For reference to the results on <b>average</b> <b>sampling</b> for deterministic signals, see Gröchenig [20], Djokovic and Vaidyanathan [21], Aldroubi [22], Sun and Zhou [23].|$|R
40|$|The {{work is to}} {{determine}} water and dry matter content by the gravimetric method on Chamaecyparis lawsoniana Blue Piramidal (A. Murr.) Parl. To achieve an accurate assessment an <b>average</b> <b>sample</b> of plant material was taken. This sample is obtained by mixing as homogeneous sample of at least ten parts using the quarter’s method. <b>Average</b> <b>sample</b> is obtained by cleaning of the impurities and finely grinding the materials...|$|R
40|$|We use XMM-Newton data {{to carry}} out a {{detailed}} study of the Si, Fe and Ni abundances in the cool cores of a representative sample of 26 local clusters. We have performed a careful evaluation of the systematic uncertainties related to the instruments, the plasma codes and the spectral modeling finding that the major source of uncertainty is in the plasma codes. Our Si, Fe, Ni, Si/Fe and Ni/Fe distributions feature only moderate spreads (from 20 % to 30 %) around their mean values strongly suggesting similar enrichment processes at work in all our cluster cores. Our <b>sample</b> <b>averaged</b> Si/Fe <b>ratio</b> is comparable to those measured in samples of groups and high luminosity ellipticals implying that the enrichment process in ellipticals, dominant galaxies in groups and BCGs in clusters is quite similar. Although our Si/Fe and Ni/Fe abundance ratios are fairly well constrained, the large uncertainties in the supernovae yields prevent us from making a firm assessment of the relative contribution of type Ia and core-collapsed supernovae to the enrichment process. All that can really be said with some certainty is that both contribute to the enrichment of cluster cores. Comment: 14 pages, accepted for publication in Astronomy and Astrophysic...|$|R
3000|$|The {{consecutive}} <b>sample</b> <b>ratio</b> β is {{a trade-off}} parameter. The optimal sparse recovery performance {{is to be}} expected for the case that A=N [...]...|$|R
40|$|AbstractIn this paper, {{we study}} the {{reconstruction}} of functions in shift invariant subspaces from local averages with symmetric averaging functions. We present an <b>average</b> <b>sampling</b> theorem for shift invariant subspaces and give quantitative results on the aliasing error and the truncation error. We show that every square integrable function can be approximated by its <b>average</b> <b>sampling</b> series. As special cases we also obtain new error bounds for regular sampling. Examples are given...|$|R
40|$|Abstract. From {{an average}} (ideal) sampling/reconstruction process, the {{question}} arises whether and how the original signal can be recovered from its <b>average</b> (ideal) <b>samples.</b> We consider the above question {{under the assumption that}} the original signal comes from a prototypical space modelling signals with finite rate of innovation, which includes finitely-generated shift-invariant spaces, twisted shift-invariant spaces associated with Gabor frames and Wilson bases, and spaces of polynomial splines with non-uniform knots as its special cases. We show that the displayer associated with an average (ideal) sampling/reconstruction process, that has well-localized <b>average</b> <b>sampler,</b> can be found to be well-localized. We prove that the reconstruction process associated with an <b>average</b> (ideal) <b>sampling</b> process is robust, locally behaved, and finitely implementable, and thus we conclude that the original signal can be approximately recovered from its incomplete <b>average</b> (ideal) <b>samples</b> with noise in real time. Most of our results in this paper are new even for the special case that the original signal comes from a finitely-generated shift-invariant space. <b>average</b> <b>sampling,</b> ideal sampling, signals with finite rate of innovation, shift-Key words. invariant space...|$|R
40|$|The {{advent of}} {{computers}} {{and their impact on}} the graphic arts and printing industry has, and will continue to, change the methodology of working and workflow in prepress operations. The conversion of analog materials (prints, artwork, transparencies, studio work) into a digital format requires the use of scanners or digital cameras, coupled with the knowledge of output requirements as related to client expectations. The chosen input <b>sampling</b> <b>ratio</b> (<b>sampling</b> rate in relation to halftone screening) impacts output quality, as well as many aspects of prepress workflow efficiency. The ability to predict printed results begins with the correct conversion of originals into digital information and then an appropriate conversion into the output materials for the intended press condition. This conversion of originals into digital information can be broken down into four general components. First, the image must be scanned {{to the size of the}} final output. Second, the input <b>sampling</b> <b>ratio</b> must be determined, in relation to the screening requirements of the job. This ratio should be appropriate to the needs of the printing condition for the final press sheet. Third, the highlight, highlight to midtone and shadow placement points must be determined in order to achieve the correct tone reproduction. Fourth, decisions must be made as to the image correction system to be employed in order to obtain consistent digital files from the scanner and prepress workflow. Factors relating to image correction and enhancement include such details as gray balance, color cast correction, dot gain, ink trapping, hue error, unsharp masking, all areas that impact quality. These are generally applied from within software packages that work with the scanner, or from within image manipulation software after the digital conversion is complete. The question of what is the necessary input <b>sampling</b> <b>ratio</b> for traditional AM screening has traditionally been based on the Nyquist Sampling Theorem. The basis for determining input <b>sampling</b> <b>ratio</b> requirements for frequency modulated (FM) screening is less clear. The Nyquist Theorem (originally from electrical engineering and communications research) has been applied to the graphic arts, leading to the general acceptance of a standard 2 : 1 ratio for most prepress scanning work. The ratio means that the sampling rate should be twice the screen frequency. This thesis set out to determine if there are dif ferences in input <b>sampling</b> <b>ratio</b> scanning requirements, based on the screen frequency rx selection (lOOlpi AM, 1751 pi AM and 21 |lFM used in this study), when generating films and/or plates for printing, that might question this interpretation of the Nyquist Sam pling Theorem as it relates to the graphic arts. Five images were tonally balanced over three different screening frequencies and six different <b>sampling</b> <b>ratios.</b> A reference image was generated for each condition using the Nyquist <b>Sampling</b> <b>ratio</b> of 2 : 1. Observers were then asked to rate the images in terms of quality against the standard. Statistical analysis was then applied to the data in order to observe interactions, similarities and differences. A pilot study was first run in order to determine the amount of unsharp masking to use on the images that would be manipulated in the main study. Seven images were pre sented from which four were selected for the final study. Thirty observers were asked for their preference on the amount of sharpening to use. It was found that for this condition (7 images) observers preferred the same amount of sharpening for the 1751 pi AM and 21 u FM screens, but slightly more sharpening for the lOOlpi AM screen. This information was then applied to the main study images. An additional image previously published was added after the pilot study, as it contained elements not found in the other images The unsharp masking applied to this image was the same as at the time of publication. The main study focused on the interaction of image type, screen frequency and varia tions of input scanner <b>sampling</b> <b>ratios</b> as it relates to output. The results indicated that image type, <b>sampling</b> <b>ratio,</b> <b>sampling</b> <b>ratio</b> - frequency interaction were factors, but fre quency alone was not. However, viewing the interaction chart of frequency and <b>sampling</b> <b>ratio</b> for the 1751 pi AM and 21 u FM screens alone, an insignificant difference was indi cated (at a 95 % confidence level). The conclusion can therefore be drawn that at the higher screen frequencies tested in this study, viewer observations showed that the input <b>sampling</b> <b>ratios</b> should be the same for 1751 pi and 21) 1 FM screens. Continuous tone orginals should be scanned at a sam pling ratio of 1. 75 : 1. This answered the question of whether FM screening technology can withstand a reduced input <b>sampling</b> <b>ratio</b> and maintain quality, which this study finds cannot. At the lower screen ruling of lOOlpi the input scanner <b>sampling</b> <b>ratio</b> requirement, based on viewer preferences of the five images presented, can be reduced to a 1. 5 : 1...|$|R
40|$|The joint {{statistics}} of <b>averaged</b> <b>sample</b> auto- and cross-spectra from a heave-pitch-roll-time series is studied. It is {{found that the}} corresponding joint probability distribution belongs to the class of Wishart distributions. Using this result, a maximum likelihood scheme is proposed for the estimation of target heave-pitch-roll auto and cross-spectra, from given <b>averaged</b> <b>sample</b> spectra, {{taking into account the}} statistical variability of the latter as well as the energy constraint between auto-spectra. These maximum likelihood estimates are then used for retrieving the directional wave spectrum; a parametric model is used for the latter, permitting the represent unimodal, bimodal or skewed spectral distributions, per frequency band. It is numerically confirmed that the use of the maximum likelihood estimated, instead of the corresponding <b>averaged</b> <b>sample</b> spectra, improves the estimate of the directional wave spectrum...|$|R
30|$|As {{shown in}} Fig. 7, a ScanSAR image and Stripmap beam F 2 - 5, F 2 - 7, and post June 1, 2015 F 2 - 6 images have {{frequency}} overlap. Here, {{we can make}} a Stripmap-ScanSAR interferogram (Ortiz and Zebker 2007). In this case, a ScanSAR image can be used as same as a Stripmap image. One problem for performing a Stripmap-ScanSAR interferometry from L 1.1 images is that they are observed with different PRF, range/azimuth <b>sampling</b> <b>ratio</b> (ratio of the ground distance per pixel), and coverage area in order to minimize the range and the azimuth ambiguity. That is, the PRF of the PALSAR- 2 image depends on the off-nadir angle of the beam and the range <b>sampling</b> <b>ratio</b> depends on the frequency bandwidth. The azimuth <b>sampling</b> <b>ratio</b> is fixed for each mode if we use the L 1.1 standard product.|$|R
30|$|As {{shown in}} Fig. 6, {{we can see}} that the PSNR of the reconstructed Lena at {{different}} <b>sampling</b> <b>ratios</b> was better than the other two low-memory techniques.|$|R
40|$|In this paper, we {{investigate}} a sequential test for binary hypothesis testing for stationary, first-order Markov dependent observations in steady state. Wald's {{first and second}} lemmas are generalized. For a Markov chain with symmetric transition probability matrix the <b>average</b> <b>sample</b> number required by the test to decide a hypothesis is derived. Numerical analysis shows that accounting for a positive correlation in the observations results in {{a significant decrease in}} the <b>average</b> <b>sample</b> number for fixed error probabilities...|$|R
30|$|As {{evident in}} the above literatures, several {{researchers}} have done extensive work {{on the performance of}} various discriminant and classification functions under skewed or non normal distributions. However, not much attention has been focused on studying and evaluating the performance of these classifiers using three populations under skewed distribution considering different <b>sampling</b> <b>ratios,</b> under different centroid separators and under varying variable selections. This study therefore seeks to investigate the performance of a single classifier (i.e the QDF) under skewed distribution considering different variable selections, varying <b>sampling</b> <b>ratios</b> and varying centroid separators considering three groups/populations.|$|R
40|$|Weighted <b>average</b> <b>sampling</b> is more {{practical}} and numerically {{more stable than}} sampling at single points as in the classical Shannon sampling framework. Using the frame theory, one can completely reconstruct a bandlimited function from its suitably-chosen <b>average</b> <b>sample</b> data. When only finitely many sample data are available, truncating the complete reconstruction series with the standard dual frame results in very slow convergence. We present in this note a method of reconstructing a bandlimited function from finite average oversampling with an exponentially-decaying approximation error...|$|R
40|$|In this paper, {{we present}} the {{designing}} of the skip-lot sampling plan including the re-inspection called SkSP-R. The plan {{parameters of the}} pro-posed plan are determined through a nonlinear optimization problem by minimizing the <b>average</b> <b>sample</b> number satisfying both the producer’s risk and the consumer’s risks. The proposed plan is shown to perform better than the existing sampling plans {{in terms of the}} <b>average</b> <b>sample</b> number. The application of the proposed plan is explained with the help of illustra-tive examples...|$|R
30|$|In addition, we used data {{augmentation}} {{to adjust}} {{the positive and negative}} <b>sample</b> <b>ratios</b> and {{to increase the number of}} training samples. The optimization function momentum[*]=[*] 0.9, weight decay 0.0001.|$|R
30|$|Of the 339 {{questionnaires}} {{distributed to}} active auditors in Taiwan, 326 valid responses were returned, representing {{a very high}} response rate of 96.17  % and a <b>sampling</b> <b>ratio</b> of 48.95  %.|$|R
3000|$|... {{in small}} time intervals. In 2007, Song, Sun, Yang etc. [16, 17] gave some {{surprising}} {{results on the}} <b>average</b> <b>sampling</b> theorems for univariate bandlimited processes in the [...]...|$|R
5000|$|The {{following}} four contingency tables contain observed cell counts, {{along with the}} corresponding <b>sample</b> odds <b>ratio</b> (OR) and <b>sample</b> log odds <b>ratio</b> (LOR): ...|$|R
30|$|This paper {{investigated}} the asymptotic performance of QDF on skewed training data for three populations (π _i,i= 1, 2, 3) with increasing group centroid (δ), with chosen variables and <b>sample</b> size <b>ratios.</b> Results {{from the study}} indicates that, the QDF performed quite poorly {{with an increase in}} error rates under <b>sample</b> <b>ratios</b> 1 : 2 : 2 and 1 : 2 : 3 for δ = 1 –δ = 3. Other results also indicates that, the QDF performs better under an equal <b>sample</b> size <b>ratio</b> (1 : 1 : 1) resulting in a reduced misclassification rate with minimized error rates. The group centroid separators increased with decreasing group error rates and sample sizes. In other words, the QDF performed better in classifying the observations into their respective groups when the group centroid separators were increased. Also with increasing number of variables, from 4 to 8, the average error rate for evaluating the performance of the QDF dropped under δ = 3, 4 for <b>sample</b> <b>ratios</b> 1 : 2 : 2 and 1 : 2 : 3.|$|R
