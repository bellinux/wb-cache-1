208|223|Public
40|$|In {{a recent}} article [Rao, A. G., A. Shapiro. 1970. <b>Adaptive</b> <b>smoothing</b> using {{evolutionary}} spectra. Management Sci. 17 (3) 208 - 218. ] Ambar G. Rao and Arthur Shapiro have proposed a scheme for <b>adaptive</b> <b>smoothing</b> using evolutionary spectra. As the authors admit, this technique demands considerable computer storage and computer time due to the increased number of additions, multiplications and exponentiations {{when compared to the}} <b>adaptive</b> <b>smoothing</b> technique proposed by Trigg and Leach [Trigg, D. W., A. G. Leach. 1967. Exponential smoothing with an adaptive response rate. O. R. Quart. 18 (1) 53 - 59. ]. ...|$|E
40|$|In this thesis, {{we study}} Gaussian Markov random field {{representation}} of the non-homogenous integrated Wiener process, {{for the purpose of}} doing <b>adaptive</b> <b>smoothing</b> of temporal data. We demonstrate that these representations are consistent for irregular locations, and derive Bayesian inferential algorithms with computational cost of only O(n), using numerical algorithms for band-matrices. We outline a more general purpose with the aim of doing more general generic <b>adaptive</b> <b>smoothing</b> of temporal data...|$|E
40|$|In this paper, {{a common}} {{framework}} is outlined for nonlinear diffusion, <b>adaptive</b> <b>smoothing,</b> bilateral filtering and mean shift procedure. Previously, {{the relationship between}} bilateral filtering and the nonlinear diffusion equation was explored by using a consistent <b>adaptive</b> <b>smoothing</b> formulation. However, both nonlinear diffusion and <b>adaptive</b> <b>smoothing</b> were treated as local processes applying a window at each iteration. Here, these two approaches are extended to an arbitrary window, showing their equivalence and stressing the importance of using large windows for edge-preserving smoothing. Subsequently, it follows that bilateral filtering is a particular choice of weights in the extended diffusion process that is obtained from geometrical considerations. We then show that kernel density estimation applied in the joint spatial-range domain yields a powerful processing paradigm - the mean shift procedure, related to bilateral filtering but having additional flexibility. This establishes an attractive relationship between the theory of statistics and that of diffusion and energy minimization. We experimentally compare the discussed methods and give insights on their performance. Keywords: Nonlinear Diffusion, <b>Adaptive</b> <b>Smoothing,</b> Bilateral filtering, Mean Shift Procedure. ...|$|E
40|$|In this research, {{we propose}} {{to use the}} <b>smooth</b> <b>adaptive</b> {{estimator}} to estimate the process mean. Since the <b>smooth</b> <b>adaptive</b> estimator is robust for non-normal processes and outliers, {{it can be used}} in the estimator of the Process Capability Indices (PCIs). The resulting indices are called <b>smooth</b> <b>adaptive</b> PCIs, which will also be robust estimators. We simu late extensively the biases and mean square errors of these <b>smooth</b> <b>adaptive</b> PCIs. In many cases the mean square errors of the <b>smooth</b> <b>adaptive</b> PCIs are smaller than those of the classical estimators...|$|R
3000|$|... {{previously}} calculated. The final smoothed trajectory {{corresponds to}} the concatenation of points {{for each of the}} generated trajectories, and the k-th trajectory contributes with its k-th point. Thus, an <b>adaptive</b> <b>smoothed</b> path is obtained. This process is applied only to the translation and rotation paths.|$|R
40|$|Abstract. We {{summarize}} {{the ideas that}} led to the <b>Adaptive</b> <b>Smoothed</b> Particle Hydrodynamics (ASPH) algorithm, with anisotropic smoothing and shock-tracking. We then identify a serious new problem for SPH simulations with shocks and radiative cooling — false cooling — and discuss a possible solution based on the shock-tracking ability of ASPH. 1...|$|R
40|$|A novel <b>adaptive</b> <b>smoothing</b> {{method is}} {{proposed}} for noise removal and feature preservation. Unlike the previous methods, we adopt two proposed measures to detect discontinuities in an image. Spatial variance is employed {{to form a}} measure for detecting contextual discontinuities {{that can be used}} for feature preservation and control of the smoothing speed, and another measure is proposed to detect variable local discontinuities during smoothing. As a result, the two discontinuity measures are jointly used in the proposed feature-preserving <b>adaptive</b> <b>smoothing</b> scheme. Due to the use of the contextual discontinuity, our smoothing scheme is insensitive to termination times, and the resulting images in a wide range of iterations are applicable to achieve the identical results in various early vision tasks. Relations between recent <b>adaptive</b> <b>smoothing</b> algorithms and ours are discussed. Simulation results show that our algorithm yields favorable smoothing results for various kinds of imag [...] ...|$|E
40|$|The {{algorithm}} of <b>adaptive</b> <b>smoothing</b> {{of images}} subject to their gradients is offered. It {{is based on}} accurate calculation and automatic binarization of the gradient and adaptive Gaussian smoothing of images with preservation of borders of homogeneous regions. The algorithm is developed specially to process noisy aeroand the space images that usually contain {{a large number of}} objects of small size. Local characteristics of the noise and the gradient are used to determine adaptive window for each pixel. The windows of <b>adaptive</b> <b>smoothing</b> are fitted so that they do not cross binarized gradient...|$|E
40|$|Abstract Ultrasound imaging is an {{important}} technique with great potential for accurate nondestructive testing. Previously, we proposed a high-resolution 3 -D imaging algorithm for UWB radar, the Envelope algorithm with <b>adaptive</b> <b>smoothing</b> techniques. The Envelope imaging algorithm utilizes the principle that the target boundary is expressed as an envelope of spheres determined by observed ranges, while the range smoothing techniques stabilize the estimated image produced by the Envelope algorithm by adaptively changing the range smoothing method. Numerical simulations confirm that the maximum error of this algorithm is less than 0. 015 center wavelength. In this study, we apply the Envelope algorithm with <b>adaptive</b> <b>smoothing</b> techniques to high resolution 3 -D acoustic imaging, which is a promising form of accurate nondestructive testing, and evaluate the performance of this algo-rithm with experimental data. The results verify that the proposed method achieves high-resolution 3 -D acoustic imaging. The mean error of the estimated image is 6. 1 µm which corresponds to 8. 2 × 10 − 3 center wavelength. Key words Ultrasound imaging, high-resolution 3 -D imaging, Envelope, <b>adaptive</b> <b>smoothing</b> 1...|$|E
40|$|We {{present a}} {{simplified}} structure of 3 -D <b>adaptive</b> LUM <b>smoother</b> for the impulse noise suppression in image sequences. The proposed reduced filter structure is optimized for possible hardware implementation. Introduced simplifications significantly decrease a filter complexity, however the <b>adaptive</b> LUM <b>smoother</b> {{based on the}} proposed reduced set of smoothing levels still provides excellent smoothing capability. 1...|$|R
40|$|We {{present an}} <b>adaptive</b> <b>smoother</b> for linear {{state-space}} models with unknown process and measurement noise covariances. The proposed method utilizes the variational Bayes technique to perform approximate inference. The resulting smoother is computationally efficient, easy to implement, {{and can be}} applied to high dimensional linear systems. The performance of the algorithm is illustrated on a target tracking example. Comment: Derivations for the smoother can found here: [URL]...|$|R
40|$|Abstract:- A {{technique}} for <b>adaptive</b> conjugate <b>smoothing</b> of discontinuous fields {{is presented in}} the paper. The described technique is applicable in various engineering problems and is especially effective when hybrid numerical – experimental methodologies are used. <b>Adaptive</b> image <b>smoothing</b> and stretching strategy is illustrated for a discontinuous plain stress field problem when photoelastic fringes representing the variation of stress are constructed in virtual projection plane...|$|R
40|$|This study refers {{mainly to}} the {{characteristics}} of symmetry and texture features in order to correctly locate a face within an image. Since we target facial expression and illumination variation in a facial image, this first requires an equalization process of <b>adaptive</b> <b>smoothing</b> of {{the shadows of the}} face caused by varying illumination. Following this, for symmetry axis detection, the study will address: Gradient Detection, Image Width and Location of Symmetry Axes, Symmetry Axes for Gradient Histogram (SAGH) and Selection; Weight is also added to strengthen symmetry characteristics. In order to verify the accuracy of the method, this study will use 6 experimental methods, namely SAGH, SAPG, WSAGH, WSAPG, WSAGH for no <b>adaptive</b> <b>smoothing,</b> and WSAPG for no <b>adaptive</b> <b>smoothing.</b> The image database used for this experiment is the Yale Face Database, with facial images that are subjected to different illumination, masked by shelters and displaying varying facial expressions. The experiment results show that the WSAPG method is the most accurate; achieving a 96. 36 % LM value, with the lowest GM value; it was the most success...|$|E
40|$|In this paper, an <b>adaptive</b> <b>smoothing</b> {{forecasting}} approach {{based on}} evolutionary spectra as developed by Rao and Shapiro (1970) {{is applied to}} the 3003 time series of various types and lengths used in the M 3 -Competition (Makridakis and Hibon, 2000). Comparisons of out-of-sample forecasts are made with other methods used in the M 3 -Competition via the symmetric mean absolute percentage error (SMAPE). It will be seen that this method does appear to perform very well when applied specifically to yearly, quarterly and monthly macro time series and to yearly and monthly demographic time series used in the competition. Evolutionary Spectra, <b>Adaptive</b> <b>Smoothing,</b> M 3 -Competition...|$|E
40|$|International audienceWe {{develop a}} new {{perturbation}} theory (PT) treatment that can describe gravitational dynamics of large-scale structure after shell-crossing in the one-dimensional cosmological case. Starting with cold initial conditions, the motion of matter distribution follows at early stages the single-stream regime, which can, in one dimension, be described exactly by the first-order Lagrangian perturbation, i. e. the Zel'dovich solution. However, the single-stream flow no longer holds after shell-crossing and a proper account of the multi-stream flow is essential for post-collapse dynamics. In this paper, extending previous work by Colombi (2015, MNRAS 446, 2902), we present a perturbative description for the multi-stream flow after shell-crossing in a cosmological setup. In addition, we introduce an <b>adaptive</b> <b>smoothing</b> scheme {{to deal with the}} bulk properties of phase-space structures. The filtering scales in this scheme are linked to the next-crossing time in the post-collapse region, estimated from our PT calculations. Our PT treatment combined with <b>adaptive</b> <b>smoothing</b> is illustrated in several cases. Predictions are compared to simulations and we find that post-collapse PT with <b>adaptive</b> <b>smoothing</b> reproduces the power spectrum and phase-space structures remarkably well even at small scales, where Zel'dovich solution substantially deviates from simulations...|$|E
40|$|We {{summarize}} {{the ideas that}} led to the <b>Adaptive</b> <b>Smoothed</b> Particle Hydrodynamics (ASPH) algorithm, with anisotropic smoothing and shock-tracking. We then identify a serious new problem for SPH simulations with shocks and radiative cooling [...] - false cooling [...] - and discuss a possible solution based on the shock-tracking ability of ASPH. Comment: 8 pages, 5 figures, to appear in Proceedings of IAU Symposium 208, Astrophysical Supercomputing Using Particles, eds. J. Makino and P. Hu...|$|R
40|$|We {{describe}} our new {{methodology for}} <b>Adaptive</b> <b>Smoothed</b> Particle Hydrodynamics (ASPH) and {{its application to}} problems in modeling material failure. We find that ASPH is often crucial for properly modeling such experiments, since {{in most cases the}} strain placed on materials is non-isotropic (such as a stretching rod), and without the directional adaptability of ASPH numerical failure due to SPH nodes losing contact in the straining direction can compete with or exceed the physical process of failure...|$|R
30|$|The ER is {{preserved}} {{for the sake}} of rendering performance. For non-ERs, a Gaussian filter and <b>adaptive</b> window <b>smoothing</b> filter are used, respectively.|$|R
40|$|Imaging modality, that {{measures}} diffusion {{of water in}} tissues like the human brain. The package contains R-functions to process diffusion-weighted data. The functionality includes diffusion tensor imaging (DTI), structural <b>adaptive</b> <b>smoothing</b> in in case of (DTI) (K. Tabelow, J. Polzehl, V. Spokoiny, an...|$|E
40|$|Abstract [...] An <b>adaptive</b> <b>smoothing</b> {{algorithm}} {{has been}} described which is capable of performing various tasks, such as removing salt and pepper noise, preserving roof edges, stretching (enhancing) step edges and reducing variations in low intensity varied regions. While iteration advances, it approximates both isotropic and anisotropic heat diffusion processes in performing these tasks. A region topography index has been defined for guiding the algorithm under different situations. Further, an image quality index is proposed which provides a criterion for automatic termination of the algorithm. This criterion {{can also be used}} with other iterative smoothing algorithms. The superiority of the method over some other similar techniques has been established for both synthetic and real images. <b>Adaptive</b> <b>smoothing</b> lsotropic/anisotropic diffusion Edge stretching Quality index. Topography inde...|$|E
40|$|Includes bibliographical {{references}} (pages 72 - 73) This thesis examines {{and enhances}} {{the area of}} adaptive exponential smoothing. A new smoothing methodology, Double <b>Adaptive</b> <b>Smoothing,</b> is developed. This method is designed to address a complex forecasting model including linear trend and multiplicative seasonal components. The problem of the complex model is handled by separately tracking the errors from each component of the model. These errors are then used to develop smoothing parameters for each component. This study goes on to compare this new <b>adaptive</b> <b>smoothing</b> method with those developed by Trigg/Leach and Whybark. A simulated constant time series and a simulated time series with linear trend and multiplicative seasonal indexes are used for this comparison. These comparisons include step, ramp, impulse and trigonometric changes in the time series. Next, comparison tests are run for these time series that {{were used for the}} forecasting tournament at the Los Angeles ORSA/TIMS conference. The final comparison was made using airline traffic data from the years 1960 through 1970. Results from these tests indicate that a benefit is gained by separating the components of a complex model. This separate monitoring allows the forecasting system to respond quickly to a change in the time series. The Double <b>Adaptive</b> <b>Smoothing</b> model proved to give the most accurate forecasts in almost every test case that was performed...|$|E
40|$|Abstract. In this paper, we {{highlight}} new multigrid solver {{advances in}} the Terascale Optimal PDE Simulations (TOPS) project in the Scienti¯c Discovery Through Advanced Computing (SciDAC) program. We discuss two new algebraic multigrid (AMG) developments in TOPS: the <b>adaptive</b> <b>smoothed</b> aggregation method (®SA) and a coarse-grid selection algorithm based on compatible relaxation (CR). The ®SA method is showing promising results in initial studies for Quantum Chromodynamics (QCD) applications. The CR method {{has the potential to}} greatly improve the applicability of AMG. 1...|$|R
40|$|In this paper, the {{material}} flow around the pin during {{friction stir welding}} (FSW) is simulated using a 2 D plane strain model. A pin rotates without translation in a disc with elasto-viscoplastic material properties and the outer boundary of the disc is clamped. Two numerical methods are used {{to solve this problem}} and an analytical solution is derived. The analytical model is complementary to validate the two numerical methods, i. e. the arbitrary Lagrangian-Eulerian (ALE) method and the <b>adaptive</b> <b>smoothed</b> finite elements method (ASFEM) ...|$|R
40|$|This paper {{addresses}} {{the problem of}} regulating the dynamic model of a nonholonomic underactuated autonomous underwater vehicle (AUV) to a point with a desired orientation. A time-invariant discontinuous controller is proposed that yields convergence of the trajectories of the closed-loop system {{in the presence of}} parametric modeling uncertainty. Controller design relies on a non smooth coordinate transformation in the original state space followed by the derivation of a Lyapunov-based, <b>adaptive,</b> <b>smooth</b> control law in the new coordinates. Convergence of the regulation system is analyzed and simulation results are presented...|$|R
40|$|Abstract <b>Adaptive</b> <b>smoothing</b> {{has been}} {{proposed}} for curve-fitting problems where the underlying function is spatially inhomogeneous. Two Bayesian adaptive smooth-ing models, Bayesian <b>adaptive</b> <b>smoothing</b> splines on a lattice and Bayesian adaptive P-splines, are studied in this paper. Estimation is fully Bayesian and carried out by efficient Gibbs sampling. Choice of prior is critical in any Bayesian non-parametric regression method. We use objective priors on the first level parameters where feasible, specifically independent Jeffreys priors (right Haar priors) on the implied base linear model and error variance, and we derive sufficient conditions on higher level compo-nents {{to ensure that the}} posterior is proper. Through simulation, we demonstrate that the common practice of approximating improper priors by proper but diffuse priors may lead to invalid inference, and we show how appropriate choices of proper but only weakly informative priors yields satisfactory inference...|$|E
40|$|This article {{describes}} {{an approach to}} adaptive video coding for video surveillance applications. Using a combination of low-level features with low computational cost, we show {{how it is possible}} to control the quality of video compression so that semantically meaningful elements of the scene are encoded with higher fidelity, while background elements are allocated fewer bits in the transmitted representation. Our approach is based on <b>adaptive</b> <b>smoothing</b> of individual video frames so that image features highly correlated to semantically interesting objects are preserved. Using only low-level image features on individual frames, this <b>adaptive</b> <b>smoothing</b> can be seamlessly inserted into a video coding pipeline as a pre-processing state. Experiments show that our technique is efficient, outperforms standard H. 264 encoding at comparable bitrates, and preserves features critical for downstream detection and recognition. Keywords-Video coding; video analysis; video-surveillance; H. 264. I...|$|E
40|$|The {{purpose of}} the package fmri is the {{analysis}} of single subject {{functional magnetic resonance imaging}} (fMRI) data. It provides fMRI analysis from time series modeling by a linear model to signal detection and publication quality images. Specifically, it implements structural <b>adaptive</b> <b>smoothing</b> methods with signal detection for adaptive noise reduction which avoids blurring of activation areas. Within this paper we describe the complete pipeline for fMRI analysis using fmri. We describe data reading from various medical imaging formats and the linear modeling used to create the statistical parametric maps. We review the rationale behind the structural <b>adaptive</b> <b>smoothing</b> algorithms and explain their usage from the package fmri. We demonstrate the results of such analysis using two experimental datasets. Finally, we report on the usage of a graphical user interface for some of the package functions...|$|E
40|$|We {{introduce}} <b>Adaptive</b> <b>Smooth</b> Multicast Protocol (ASMP), for multimedia transmission over best-effort networks. The smoothness lays in {{the calculation}} and {{adaptation of the}} transmission rate, {{which is based on}} dynamic estimation of protocol parameters and dynamic adjustment of the “smoothness factor”. ASMP key attributes are: a) adaptive scalability to large sets of receivers, b) TCP-friendly behavior, c) high bandwidth utilization, and finally d) smooth transmission rates, which are suitable for multimedia applications. We evaluate the performance of ASMP and investigate its behavior under various network conditions through extensive simulations, conducted with the network simulator software (ns 2) ...|$|R
40|$|We {{introduce}} <b>Adaptive</b> <b>Smooth</b> Simulcast Protocol (ASSP) for simulcast transmission over best-effort networks. ASSP {{is a new}} multiple-rate protocol that implements {{a single}} rate TCP-friendly protocol as the underlying congestion control mechanism for each simulcast stream. ASSP is build {{on top of the}} RTP/RTCP protocol and exploits the RTCP sender and receiver reports for the dissemination of feedback information. The key attributes of ASSP are: a) TCPfriendly behavior, b) adaptive per-stream transmission rates, c) adaptive scalability to large sets of receivers and finally d) smooth transmission rates that are suitable for multimedia applications. We evaluate the performance of ASSP and investigate its behavio...|$|R
40|$|Summary. The linear systems arising in lattice {{quantum chromodynamics}} (QCD) pose {{significant}} challenges for traditional iterative solvers. The Dirac operator as-sociated with these systems is nearly singular, indicating {{the need for}} efficient pre-conditioners. Multilevel preconditioners cannot, however, be easily constructed for these systems becasue the Dirac operator has multiple locally distinct near-kernel components (the so-called slow-to-converge error components of relaxation) that are generally both oscillatory and not known a priori. This paper presents heuristic ar-guments and numerical results demonstrating that the recently developed <b>adaptive</b> <b>smoothed</b> aggregation (αSA) [BFM+ 04] methodology {{can be used to}} overcome the challenges posed by these systems. ...|$|R
40|$|The Los Alamos 3 D grid toolbox handles grid {{maintenance}} {{chores and}} {{provides access to}} a sophisticated set of optimization algorithms for unstructured grids. The application of these tools to semiconductor problems is illustrated in three examples: grain growth, topographic deposition and electrostatics. These examples demonstrate <b>adaptive</b> <b>smoothing,</b> front tracking, and automatic, adaptive refinement/derefinement...|$|E
40|$|This paper {{considers}} {{the development of}} spatially <b>adaptive</b> <b>smoothing</b> splines for the esti-mation of a regression function with nonhomogeneous smoothness across the domain. Two challenging issues arising in this context are {{the evaluation of the}} equivalent kernel and the deter-mination of a local penalty. The penalty {{is a function of the}} design points in order to accommodate local behaviour of the regression function. We show that the spatially <b>adaptive</b> <b>smoothing</b> spline estimator is approximately a kernel estimator, and that the equivalent kernel is spatially depen-dent. The equivalent kernels for traditional smoothing splines are a special case of this general solution. With the aid of the Green’s function for a two-point boundary value problem, explicit forms of the asymptotic mean and variance are obtained for any interior point. Thus, the optimal roughness penalty function is obtained by approximately minimizing the asymptotic integrated mean squared error. Simulation results and an application illustrate the performance of the pro-posed estimator...|$|E
40|$|This thesis {{consists}} of three chapters. The first chapter focuses on <b>adaptive</b> <b>smoothing</b> splines for fitting functions with varying roughness. In {{the first part of}} the first chapter, we study an asymptotically optimal procedure to choose the value of a discretized version of the variable smoothing parameter in <b>adaptive</b> <b>smoothing</b> splines. With the choice given by the multivariate version of the generalized cross validation, the resulting <b>adaptive</b> <b>smoothing</b> spline estimator is shown to be consistent and asymptotically optimal under some general conditions. In the second part, we derive the asymptotically optimal local penalty function, which is subsequently used for the derivation of the locally optimal smoothing spline estimator. In the second chapter, we propose a Lipschitz regularity based statistical model, and apply it to coordinate measuring machine (CMM) data to estimate the form error of a manufactured product and to determine the optimal sampling positions of CMM measurements. Our proposed wavelet-based model takes advantage of the fact that the Lipschitz regularity holds for the CMM data. The third chapter focuses on the classification of functional data which are known to be well separable within a particular interval. We propose an interval based classifier. We first estimate a baseline of each class via convex optimization, and then identify an optimal interval that maximizes the difference among the baselines. Our interval based classifier is constructed based on the identified optimal interval. The derived classifier can be implemented via a low-order-of-complexity algorithm. PhDCommittee Chair: Huo, Xiaoming; Committee Member: Lu, Jye-Chyi; Committee Member: Peng, Liang; Committee Member: Shi, Jianjun; Committee Member: Vengazhiyil, Roshan Josep...|$|E
40|$|A {{successful}} {{simulation of}} a bulk forming process with finite elements {{can be difficult}} due to distortion of the finite elements. Nodal smoothed Finite Elements (NSFEM) are an interesting option for such a process since they show good distortion insensitivity and moreover have locking-free behavior and good computational efficiency. In this paper a method is proposed which {{takes advantage of the}} nodally smoothed field. This method, named <b>adaptive</b> <b>smoothed</b> finite elements (ASFEM), revises the mesh for every step of a simulation without mapping the history dependent material parameters. In this paper an updated-Lagrangian implementation is presented. Several examples are given to illustrate the method and to show its properties...|$|R
40|$|This paper {{addresses}} {{the problem of}} stabilizing the dynamic model of a nonholonomic wheeled robot of the unicycle type to a point with a desired orientation. A simple controller is derived that yields global stability and convergence of the trajectories of the closed loop system {{in the presence of}} parametric modeling uncertainty. Controller design relies on a non smooth coordinate transformation in the original state space, followed by the derivation of a Lyapunov-based, <b>adaptive,</b> <b>smooth</b> control law in the new coordinates. Stability and convergence to the origin are analyzed and simulation results are presented. Keywords: Nonholonomic Systems, Mobile Robots, Nonlinear Adaptive Control, Stabilization 1...|$|R
40|$|Rates of {{convergence}} for nonparametric regression estimators {{based on}} recursive partitioning schemes are derived. The central {{idea is to}} consider the tree-structured regression estimator as a wavelet estimator based on the orthogonal system of Haar functions. A locally <b>adaptive</b> data-driven <b>smoothing</b> method is proposed and its performance is studied. ...|$|R
