48|13|Public
50|$|LAME A {{high quality}} MP3 (Layer III) <b>audio</b> <b>encoder.</b>|$|E
50|$|TooLAME A {{high quality}} MPEG-1 Layer II <b>audio</b> <b>encoder.</b>|$|E
5000|$|An <b>audio</b> <b>encoder</b> {{converts}} analog audio {{to digital}} audio signals ...|$|E
50|$|MP3 is a frequency-domain <b>audio</b> {{transform}} <b>encoder.</b> Even {{though it}} utilizes {{some of the}} lower layer functions, MP3 {{is quite different from}} Layer II/MP2.|$|R
50|$|The RTAudio encoder {{is capable}} of {{encoding}} single-channel (mono), 16 bit per sample <b>audio</b> signals. The <b>encoder</b> can be configured to operate either in the Narrow Band mode (8 kHz sampling rate) or the Wide Band mode (16 kHz sampling rate). The RTAudio decoder has a built-in jitter control module {{as well as an}} error concealment module.|$|R
50|$|Packetized Elementary Stream (PES) is a {{specification}} in the MPEG-2 Part 1 (Systems) (ISO/IEC 13818-1) and ITU-T H.222.0 {{that defines}} carrying of elementary streams (usually {{the output of}} an <b>audio</b> or video <b>encoder)</b> in packets within MPEG program streams and MPEG transport streams. The elementary stream is packetized by encapsulating sequential data bytes from the elementary stream inside PES packet headers.|$|R
5000|$|MP2 is a {{sub-band}} <b>audio</b> <b>encoder,</b> {{which means}} that compression {{takes place in the}} time domain with a low-delay filter bank producing 32 frequency domain components. By comparison, MP3 is a transform <b>audio</b> <b>encoder</b> with hybrid filter bank, {{which means that}} compression takes place in the frequency domain after a hybrid (double) transformation from the time domain.|$|E
50|$|On September 10, 2014, Fraunhofer IIS {{demonstrated}} a real time MPEG-H 3D <b>audio</b> <b>encoder.</b>|$|E
50|$|This {{includes}} the ISO Dist10 <b>audio</b> <b>encoder</b> code, which LAME and TooLAME were originally based upon.|$|E
40|$|We {{present a}} single-chip, MPEG- 2 Main Profile at Main Level, <b>audio</b> and video <b>encoder</b> and decoder. It {{combines}} a RISC core, a 24 -bit DSP, {{video and audio}} interface units, and several dedicated processing units. A programmable video interface unit supports multiple modes of pre- and post-processing and on-screen display (OSD). The codec has been implemented using a standard-cell library in 0. 18 #m CMOS technology...|$|R
40|$|An <b>audio</b> signal <b>encoder</b> (100) {{comprises}} a transform-domain path (12) configured {{to obtain}} a set of spectral coefficients (124) and noise-shaping information (126) {{on the basis of}} a time-domain representation (122) of a portion of the audio content to be encoded in a transform-domain mode. The transform-domain path comprises a time-domain-to-frequency-domain converter (130) configured to window a time-domain representation of the audio content, or a pre-processed version thereof, {{to obtain a}} windowed representation of the audio content, and to apply a time-domain-to-frequency-domain conversion, to derive a set of spectral coefficients from the windowed time-domain representation of the audio content. The audio signal decoder comprises a CELP path (140) configured to obtain an code-excitation information (144) and a linear-prediction-domain parameter information (146) {{on the basis of a}} portion of the audio content to be encoded in a CELP mode. The time-domain-to-frequency-domain converter (136) is configured to apply a predetermined asymmetric analysis window (520) for a windowing of a current portion of the audio content to be encoded in the transform-domain mode and following a portion of the audio content encoded in the transform-domain mode both if the current portion of the audio content is followed by a subsequent portion of the audio content to be encoded in the transform-domain mode and if the current portion of the audio content is followed by a subsequent portion of the audio content to be encoded in the CELP mode. The <b>audio</b> signal <b>encoder</b> is configured to selectively provide an aliasing cancellation information (164) if the current portion of the audio content is followed by a subsequent portion of the audio content to be encoded in the CELP mode...|$|R
5000|$|An {{elementary}} stream (ES) {{as defined by}} the MPEG communication protocol is usually the output of an <b>audio</b> or video <b>encoder.</b> ES contains only one kind of data, e.g. audio, video or closed caption. An {{elementary stream}} {{is often referred to as}} [...] "elementary", [...] "data", [...] "audio", or [...] "video" [...] bitstreams or streams. The format of the elementary stream depends upon the codec or data carried in the stream, but will often carry a common header when packetized into a packetized elementary stream.|$|R
50|$|Altacast (formerly {{known as}} Edcast and Oddcast) {{is a free}} and open source <b>audio</b> <b>encoder</b> {{that can be used}} to create Internet streams of varying types. Many {{independent}} and commercial broadcasters use Altacast to create Internet radio stations, such as those listed on the Icecast, Loudcaster and Shoutcast station directories.|$|E
50|$|TooLAME {{was mainly}} a {{standalone}} <b>audio</b> <b>encoder,</b> accepting PCM files in RAW/AIFF/WAV format. However, {{in the final}} TooLAME release from Cheng (TooLAME 0.2 m beta 8), support {{for use as a}} library was included. Cheng repeatedly resisted the addition of features like libsndfile integration for support of a much wider variety of input formats.|$|E
50|$|A typical {{method of}} {{transmitting}} elementary stream {{data from a}} video or <b>audio</b> <b>encoder</b> is to first create PES packets from the elementary stream data and then to encapsulate these PES packets inside Transport Stream (TS) packets or Program Stream (PS) packets. The TS packets can then be multiplexed and transmitted using broadcasting techniques, such as those used in an ATSC and DVB.|$|E
5000|$|VisualOn {{provided}} a simple Advanced <b>Audio</b> Coding (AAC) <b>encoder</b> in early versions of Android. The encoder {{was derived from}} the 3GPP reference encoder and supported only the AAC-LC profile in mono or stereo. Google added a more advanced AAC codec library to Android as of the 4.1 Jelly Bean release. The VisualOn encoder remains in the Android source code, but development appears to have ended. VisualOn no longer markets any commercial encoder products. VisualOn software codecs listed on its website are software based decoders and not encoders.|$|R
40|$|A novel {{approach}} is proposed for effective high frequency regeneration in audio coding, {{which is based}} on a sinusoids plus noise model. It assumes a standard high efficiency advanced <b>audio</b> coding (HE-AAC) <b>encoder,</b> and modifies the decoder to exploit all available information in estimating the model parameters. From the lower band reconstruction of core AAC, frequency parameters of the high band sinusoids are estimated. Side information about spectral energy and the regenerated high band of standard HE-AAC are employed in estimating the magnitude parameters of the high band sinusoids as well as noise model parameters. The gains achieved by the proposed technique, over conventional HE-AAC, are demonstrated by subjective quality tests that were carried out on audio signals with significant harmonics in the high band. 1...|$|R
40|$|In this paper, we {{investigated}} {{the influence of}} stereo coding on Japanese speech localized in 3 -D virtual space. We encoded localized speech using Joint Stereo and Parametric Stereo modes within the HE-AAC (High-Efficiency Advanced <b>Audio</b> Coding) <b>encoder</b> at identical data rates. First, the sound quality of the localized speech signal was checked using MUSHRA subjective tests. The result showed that the speech quality for Joint Stereo is higher than Parametric Stereo when localized at 45 (where 0 refers to localization {{directly in front of}} the listener) by 20 to 30 MUSHRA score points. The scores for Joint Stereo were relatively proportional to bit rate. However, Parametric Stereo scores were not proportional to bit rate, and remained fairly constant with bit rate. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic Rhyme Tests (JDRT). Test speech was localized in front, while competing noise were localized at various angles. The result showed that speech could not be separated from the noise for Joint Stereo when the noise was in located in the frontal region, from 45 to 45, and intelligibility degrades significantly. However at other azimuth, the intelligibility improves dramatically. On the other hand, intelligibility with Parametric Stereo remained constant, at about 70 to 80 %. 1...|$|R
50|$|TooLAME {{is a free}} {{software}} MPEG-1 Layer II (MP2) <b>audio</b> <b>encoder</b> written primarily by Mike Cheng. While there are innumerable MP2 encoders, TooLAME is well-known and widely used for its particularly high audio quality. It has been unmaintained since 2003, but is directly succeeded by the TwoLAME code fork (the latest version, TwoLAME 0.3.13, was released January 21, 2011). The name TooLAME is a play on LAME and Layer II.|$|E
50|$|SBC, or low-complexity subband codec, is {{an audio}} subband codec {{specified}} by the Bluetooth Special Interest Group (SIG) for the Advanced Audio Distribution Profile (A2DP). SBC is a digital <b>audio</b> <b>encoder</b> and decoder used to transfer data to Bluetooth audio output devices like headphones or loudspeakers. It {{can also be used}} on the Internet. It was designed to obtain a reasonably good audio quality at medium bit rates while keeping low computational complexity, having Bluetooth bandwidth limitations and processing power in mind. As of A2DP version 1.3, the Low Complexity Subband Coding remains the default codec and its implementation is mandatory for devices supporting that profile. Other codecs can be run on top of A2DP along with SBC though.|$|E
50|$|MPEG Surround encoder {{receives}} a multichannel audio signal,x1 to xN where {{the number of}} input channels is N. The {{most important aspect of}} the encoding process is that a downmix signal, xt1 and xt2, which is typically stereo, is derived from the multichannel input signal, and it is this downmix signal that is compressed for transmission over the channel rather than the multichannel signal. The encoder may be able to exploit the downmix process so as to be more advantageous. It not only creates a faithful equivalent of the multichannel signal in the mono or stereo downmix, but also creates the best possible multichannel decoding based on the downmix and encoded spatial cues as well. Alternatively, the downmix could be supplied externally (Artistic Downmix in before Diagram Block). The MPEG Surround encoding process could be ignored by the compression algorithm used for the transmitted channels (<b>Audio</b> <b>Encoder</b> and Audio Decoder in before Diagram Block). It could be any type of high-performance compression algorithms such as MPEG-1 Layer III, MPEG-4 AAC or MPEG-4 High Efficiency AAC, or it could even be PCM.|$|E
40|$|A {{multi-mode}} {{audio signal}} decoder for providing a decoded representation of an audio content {{on the basis}} of an encoded representation of the audio content comprises a spectral value determinator configured to obtain sets of decoded spectral coefficients for a plurality of portions of the audio content. The audio signal decoder also comprises a spectrum processor configured to apply a spectral shaping to a set of spectral coefficients, or to a pre-processed version thereof, in dependence on a set of linear-prediction-domain parameters for a portion of the audio content encoded in a linear-prediction mode, and to apply a spectral shaping to a set of decoded spectral coefficients, or a pre-processed version thereof, in dependence on a set of scale factor parameters for a portion of the audio content encoded in a frequency-domain mode. The audio signal decoder comprises a frequency-domain-to-time-domain converter configured to obtain a time-domain representation of the audio content {{on the basis of}} a spectrally-shaped set of decoded spectral coefficients for a portion of the audio content encoded in the linear-prediction mode, and to obtain a time domain representation of the audio content {{on the basis of a}} spectrally shaped set of decoded spectral coefficients for a portion of the audio content encoded in the frequency domain mode. An <b>audio</b> signal <b>encoder</b> is also described...|$|R
40|$|Presented at the 15 th International Conference on Auditory Display (ICAD 2009), Copenhagen, Denmark, May 18 - 22, 2009 In this paper, we {{investigated}} {{the influence of}} stereo coding on Japanese speech localized in 3 -D virtual space. We encoded localized speech using Joint Stereo and Parametric Stereo modes within the HE-AAC (High-Efficiency Advanced <b>Audio</b> Coding) <b>encoder</b> at identical data rates. First, the sound quality of the localized speech signal was checked using MUSHRA subjective tests. The result showed that the speech quality for Joint Stereo is higher than Parametric Stereo when localized at 45 (where 0 refers to localization {{directly in front of}} the listener) by 20 to 30 MUSHRA score points. The scores for Joint Stereo were relatively proportional to bit rate. However, Parametric Stereo scores were not proportional to bit rate, and remained fairly constant with bit rate. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic Rhyme Tests (JDRT). Test speech was localized in front, while competing noise were localized at various angles. The result showed that speech could not be separated from the noise for Joint Stereo when the noise was in located in the frontal region, from 45 to 45, and intelligibility degrades significantly. However at other azimuth, the intelligibility improves dramatically. On the other hand, intelligibility with Parametric Stereo remained constant, at about 70 to 80 %...|$|R
40|$|Implementations {{of audio}} {{algorithms}} on embedded devices {{are required to}} consume minimal memory and processing power. Such applications can usually tolerate numerical imprecisions (distortion) {{as long as the}} resulting perceived quality is not degraded. By taking advantage of this error-tolerant nature the algorithmic complexity can be reduced greatly. In the context of real-time audio coding, these algorithms can benefit from parametrization to adapt rate-distortion-complexity (R-D-C) trade-offs. We propose a modification to the rate-distortion loop in the quantization and coding stage of a fixed-point implementation of the Advanced <b>Audio</b> Coding (AAC) <b>encoder</b> to include complexity scaling. This parametrization could allow the control of algorithmic complexity through instantaneous workload measurements using the target processor’s task scheduler to better assign processing resources. Results show that this framework can be tuned to reduce a significant amount of the additional workload caused by the rate-distortion loop while remaining perceptually equivalent to the full-complexity version. Additionally, the modification allows a graceful degradation when transparency cannot be met due to limited computational capabilities...|$|R
40|$|An <b>audio</b> <b>encoder</b> (10) {{adapted for}} {{encoding}} frames of a sampled audio signal to obtain encoded frames, wherein a frame comprises {{a number of}} time domain audio samples. The <b>audio</b> <b>encoder</b> (10) comprises a predictive coding analysis stage (12) for determining information on coefficients of a synthesis filter and a prediction domain frame based on a frame of audio samples. The <b>audio</b> <b>encoder</b> (10) further comprises a time-aliasing introducing transformer (14) for transforming overlapping prediction domain frames to the frequency domain to obtain prediction domain frame spectra, wherein the time-aliasing introducing transformer (14) is adapted for transforming the overlapping prediction domain frames in a critically-sampled way. Moreover, the <b>audio</b> <b>encoder</b> (10) comprises a redundancy reducing encoder (16) for encoding the prediction domain frame spectra to obtain the encoded frames based on the coefficients and the encoded prediction domain frame spectra...|$|E
40|$|An <b>audio</b> <b>encoder</b> (100) for {{encoding}} {{segments of}} coefficients, the segments of coefficients representing different time or frequency resolutions of a sampled audio signal, the <b>audio</b> <b>encoder</b> (100) comprising a processor (110) for deriving a coding context for a currently encoded coefficient of a current segment {{based on a}} previously encoded coefficient of a previous segment, the previously encoded coefficient representing a different time or frequency resolution than the currently encoded coefficient. The <b>audio</b> <b>encoder</b> (100) further comprises an entropy encoder (120) for entropy encoding the current coefficient based on the coding context to obtain an encoded audio stream...|$|E
40|$|In this paper, it is {{described}} a combined {{hardware and software}} solution for MPEG 1 <b>audio</b> <b>encoder</b> and decoder system imple-mented on samsung 16 bit fixed-point DSP(Digital Signal Processor) and 2 K gate FPGA(Field Programmable Gate Array) logics. The MPEG 1 <b>audio</b> <b>encoder</b> and decoder (layer I & II) implemented on SSP 1605 1 and a compact/cost effective board are running in real time. The devel-oped code satisfies the MPEGl standard requirement. 1...|$|E
40|$|Abstract. We {{propose a}} method to {{estimate}} the shape parameter p in the generalized Gaussian distribution. Our estimator is an explicit approximate solution to the trascendental estimator obtained by the method of moments. An estimator for p, based on the method of moments, does not always exist, however we show {{that it is possible}} to find such an estimator with high probability for most of the practical situations. A numeric-analytical procedure to obtain the confidence intervals for p is also presented. We illustrate our procedures on data obtained from the different subbands of the <b>audio</b> MP 3 <b>encoder.</b> Key words: generalized Gaussian distribution, method of moments, generalized Gaussian ratio function (ggrf), sampled generalized Gaussian ratio function The Gaussian distribution is a typical model for signals and noise in many applications in science and engineering. However, there are some applications where this Gaussian assumption departs from the actual random behavior. For instance, the samples of a speech signal are modeled by a Laplacian distribution, and the generalized Gaussian distributio...|$|R
40|$|An <b>audio</b> <b>encoder</b> (100) for {{encoding}} audio samples, comprising a {{first time}} domain aliasing introducing encoder (110) for encoding audio samples {{in a first}} encoding domain, the first time domain aliasing introducing encoder (110) having a first framing rule, a start window and a stop window. The <b>audio</b> <b>encoder</b> (100) further comprises a second encoder (120) for encoding samples in a second encoding domain, the second encoder (120) having a different second framing rule.; The <b>audio</b> <b>encoder</b> (100) further comprises a controller (130) switching from the first encoder (110) to the second encoder (120) in response to characteristic of the audio samples, and for modifying the second framing rule in response to switching from the first encoder (110) to the second encoder (120) or for modifying the start window or the stop window of the first encoder (110), wherein the second framing rule remains unmodified...|$|E
40|$|An <b>audio</b> <b>encoder</b> (100) {{adapted for}} {{encoding}} frames of a sampled audio signal to obtain encoded frames, wherein a frame comprises {{a number of}} time domain audio samples, comprising a predictive coding analysis stage (110) for determining information on coefficients of a synthesis filter and information on a prediction domain frame based on a frame of audio samples. The <b>audio</b> <b>encoder</b> (100) further comprises a frequency domain transformer (120) for transforming a frame of audio samples to the frequency domain to obtain a frame spectrum and an encoding domain decider (130). Moreover, the <b>audio</b> <b>encoder</b> (100) comprises a controller (140) for determining an information on a switching coefficient when the encoding domain decider decides that encoded data of a current frame {{is based on the}} information on the coefficients and the information on the prediction domain frame when encoded data of a previous frame was encoded based on a previous frame spectrum...|$|E
40|$|An <b>audio</b> <b>encoder</b> for {{encoding}} {{an audio}} signal includes an impulse extractor (10) for extracting an impulse-like portion from the audio signal. This impulse-like portion is encoded and forwarded to an output interface (22). Furthermore, the <b>audio</b> <b>encoder</b> includes a signal encoder (16) which encodes a residual signal {{derived from the}} original audio signal so that the impulse-like portion is reduced or eliminated in the residual audio signal. The output interface (22) forwards both, the encoded signals, i. e., the encoded impulse signal (12) and the encoded residual signal (20) for transmission or storage. On the decoder-side, both signal portions are separately decoded and then combined to obtain a decoded audio signal...|$|E
40|$|Discusses the {{fundamentals}} of audio signals. The authors refer to adaptation to the human ear sensitivity spectrum. They describe the AAC <b>audio</b> <b>encoder</b> for broadcast quality (ISO/IEC 13818 - 7), and the MPEG- 2 -AAC compression standard. They also describe the MPEG- 4 standard for audio-visual encoding, which includes scalability and contents-based interaction...|$|E
40|$|This paper {{describes}} the ongoing effort on development {{of high quality}} AC- 3 <b>audio</b> <b>encoder</b> on a DSP Core. The task involves formulation of the core compression algorithms; implementation of the algorithms using a high level language; subjective listening test for evaluation of the compression techniques; simulation with various fixed point word-lengths; and final implementation into the firmware of a DSP Core...|$|E
40|$|An <b>audio</b> <b>encoder</b> for {{encoding}} {{an audio}} signal comprises a first coding branch (400), the first coding branch comprising a first converter (410) for converting {{a signal from}} a time domain into a frequency domain. Furthermore, the <b>audio</b> <b>encoder</b> comprises a second coding branch (500) comprising a second time/frequency converter (523). Additionally, a signal analyzer (300 / 525) for analyzing the audio signal is provided. The signal analyzer, on the hand, determines whether an audio portion is effective in the encoder output signal as a first encoded signal from the first encoding branch or as a second encoded signal from a second encoding branch. On the other hand, the signal analyzer determines a time/frequency resolution to be applied by the converters (410, 523) when generating the encoded signals.; An output interface includes, {{in addition to the}} first encoded signal and the second encoded signal, a resolution information identifying the resolution used by the first time/frequency converter and used by the second time/frequency converter...|$|E
40|$|In this paper, a {{terrestrial}} digital multimedia broadcasting (T-DMB) multichannel audio broadcasting {{system based}} on spatial audio coding is presented. The proposed system provides realistic multichannel audio service via T-DMB with a small increase of data rate as well as backward compatibility with the conventional stereo-based T-DMB player. To reduce the data rate for additional multichannel audio signals, we compress the multichannel audio signals using the sound source location cue coding algorithm, which is an efficient parametric multichannel audio compression technique. For compatibility, we use the dependent property of an elementary stream descriptor, and this property should be ignored in a conventional T-DMB player. To verify the feasibility of the proposed system, we implement the T-DMB multichannel <b>audio</b> <b>encoder</b> and a prototype player. We perform a compatibility test using the T-DMB multichannel <b>audio</b> <b>encoder</b> and conventional T-DMB players. The test demonstrates that the proposed system is compatible with a conventional T-DMB player and that it can provide a promisingly rich audio service...|$|E
40|$|In {{this report}} I {{describe}} {{the design and}} implementation of an extensible ‘System-on-a-Chip ’ (SoC) architecture which can be targeted at accelerating specific applications. To demonstrate its effectiveness this is {{used to create a}} completely self-contained digital <b>audio</b> <b>encoder,</b> based around a high-quality general purpose audio compression algorithm called Ogg Vorbis. The resulting <b>audio</b> <b>encoder</b> is able to operate 33 % faster than the original software only algorithm, and when running at 25 MHz on FPGA hardware is able to achieve compression of 8 KHz, mono, audio data in real-time. To summarise, this project has 5 distinct achievements: • The design of an extensible framework for creating application specific SoCs. This allows the addition of custom instructions and data processors to a general purpose CPU in a way abstracted from the underlying architecture. • An implementation of such a framework based around an off-the-shelf soft-core microprocessor. This has specific support for fast memor...|$|E
