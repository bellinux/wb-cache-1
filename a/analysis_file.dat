30|773|Public
50|$|Seed7 {{has many}} libraries, {{covering}} areas including containers, numeric functions, lexical <b>analysis,</b> <b>file</b> manipulation, networking (sockets, Transport Layer Security (TLS/SSL), Hypertext Transfer Protocol (HTTP), HTTP Secure (HTTPS), File Transfer Protocol (FTP), Simple Mail Transfer Protocol (SMTP), etc.), graphics, pixmap and vector fonts, database access (MySQL-MariaDB, SQLite, PostgreSQL, Oracle, Open Database Connectivity (ODBC)), Common Gateway Interface (CGI) support, data compression, character encoding, time and date handling, XML processing, message digests and more. These libraries {{reduce the need}} to use operating system features and third-party libraries directly. Seed7 libraries contain abstraction layers for hardware, operating system and third-party libraries, e.g. graphic and database libraries. In other words, no changes are needed to move Seed7 programs between different processors or operating systems.|$|E
40|$|Excel data {{spreadsheets}} and Prism <b>analysis</b> <b>file</b> for all electrophysiology {{data used}} in publication. This research data supports Structural Domains Underlying the Activation of Acid-Sensing Ion Channel 2 a, {{which has been}} published in the Molecular Pharmacology journal. This work was supported by the BBSRC [grant number BB/J 014540 / 1], and the Isaac Newton Trust/Wellcome Trust Institutional Strategic Support Fund/University of Cambridge Joint Research Grants Scheme...|$|E
40|$|Directory Tree <b>Analysis</b> <b>File</b> Generator is a Practical Extraction and Reporting Language (PERL) {{script that}} {{simplifies}} and automates {{the collection of}} information for forensic analysis of compromised computer systems. During such an analysis, it is sometimes necessary to collect and analyze information about files on a specific directory tree. Directory Tree <b>Analysis</b> <b>File</b> Generator collects information of this type (except information about directories) and writes it to a text file. In particular, the script asks the user for {{the root of the}} directory tree to be processed, the name of the output file, and the number of subtree levels to process. The script then processes the directory tree and puts out the aforementioned text file. The format of the text file is designed to enable the submission of the file as input to a spreadsheet program, wherein the forensic analysis is performed. The analysis usually consists of sorting files and examination of such characteristics of files as ownership, time of creation, and time of most recent access, all of which characteristics are among the data included in the text file...|$|E
5000|$|All {{analysis}} {{results are presented}} in a web report, which contains Excel annotation and enrichment sheets, PowerPoint slides, and custom <b>analysis</b> <b>files</b> (e.g., [...]cys file by Cytoscape, [...]svg by Circos) for further offline analysis or processing.|$|R
40|$|Over {{many years}} {{transaction}} log analysis (also called log <b>analysis,</b> log <b>file</b> <b>analysis,</b> or log tracking, and more lately web logging, web log <b>file</b> <b>analysis,</b> and web tracking) {{has been used}} to collect information on how information systems such as online library catalogues (OPACs), online and CD-ROM databases, and web-base...|$|R
40|$|This dataset {{contains}} data <b>files</b> and <b>analysis</b> code {{associated with}} manuscript entitled "Parallel reverse genetic screening in mutant human cells using transcriptomics". Data files include expression profiles for over 1800 RNA-seq samples and annotations. <b>Analysis</b> <b>files</b> include R scripts to generate summary figures...|$|R
30|$|The modal results, {{specifically}} modal periods, are {{the main}} parameters used in time history analyses. Moreover, geometric nonlinearity was included in all dynamic analyses and damping was also considered using Rayleigh damping. Since the transient analysis was often unable to converge to a solution, a solution procedure script was developed within the <b>analysis</b> <b>file</b> that tried {{a number of different}} solution algorithms, time steps, and convergence criterion until a solution could be achieved.|$|E
40|$|To perform {{behavior}} based malware analysis, behavior capturing is {{an important}} prerequisite. In this paper, we present Osiris system which is a tool to capture behaviors of executable files in Windows system. It collects API calls invoked not only by main process of the <b>analysis</b> <b>file,</b> but also API calls invoked by child processes which are created by main process, injected processes if process injection happens, and service processes if the main process creates services. By modifying the source code of Qemu, Osiris is implemented at the virtual machine monitor layer and has the following advantages. First, it does not rewrite the binary code of <b>analysis</b> <b>file</b> or interfere with its normal execution, so that behavior data are obtained more stealthily and transparently. Second, it employs a multi-virtual machine framework to simulate the network environment for malware analysis, so that network behaviors of a malware are stimulated to a large extend. Third, besides network environment, it also simulates most common host events to stimulate potential malicious behaviors of a malware. The experimental results show that Osiris automates the malware analysis process and provides good behavior data for the following detection algorithm...|$|E
30|$|Our {{first task}} was to {{identify}} VR applicants and track them from their application date {{for as long as}} we could. We identified first-time non-beneficiary applicants for VR services from 2002 through 2005 and followed their SSDI and SSI program activity for the 48  months after VR application using a constructed dataset containing linked administrative data from the RSA Case Service Report (RSA- 911) files, the SSA 831 File, and the 2009 SSA Ticket Research File (TRF, now called the Disability <b>Analysis</b> <b>File).</b>|$|E
50|$|Valkyrie Cloud <b>File</b> <b>Analysis</b> Platform - The Valkyrie {{cloud-based}} <b>file</b> <b>analysis</b> platform uses multiple {{techniques to}} provide a verdict of good or bad on unknown files.|$|R
5000|$|<b>File</b> system <b>analysis</b> - <b>file</b> name, file type, file size, date attributes, last {{accessed}} & last modified ...|$|R
40|$|Forecasting {{possibility}} of convective phenomena arising time, place and type was estimated {{with the aid}} of calculated convection indexes. To calculate these parameters HIRLAM (High Resolution Local Area Model) forecast and <b>analysis</b> <b>files</b> have been processed. Six indexes chosen for study were divided into 3 groups. Indexes from the first diagnostic group estimate atmospheric stability state. There’s a parameter having triggering function for convection arising. Third group gives information on possible convective phenomena type. Analyses an...|$|R
40|$|What's New in v 3. 0 ? Version 3. 0 fixes a {{significant}} bug {{present in the}} previous versions, adds several exciting new features, and contains several major performance improvements. Major New Features: Added the ability to shrink the lattice by a specified integer factor Added {{the ability to use}} a 3 D checkerboard starting configuration instead of a random blend Added the ability to modify the interaction energy in one of the directions to allow anisotropic domain growth Improved the <b>analysis</b> <b>file</b> output to include a list of the properties of all morphologies in the set Added output of the average correlation function for the morphology set to the correlation_data_avg. txt file Minor New Features: Added the ability to extend the correlation function calculation out to the second correlation maximum Added characterization of domain anisotropy as a standard metric Improved the <b>analysis</b> <b>file</b> to include a header that specifies which version of the software was used to generate the morphology set Improved the <b>analysis</b> <b>file</b> output to also specify which morphology has the median domain size and the median tortuosity Performance Improvements: Increased the speed of the phase separation process by increasing the speed of the energy calculation during the Ising swapping stage Improved the speed of the tortuosity calculation by increasing the the speed of the pathfinding and path distance calculations Added the ability to enable a reduced memory usage tortuosity calculation algorithm. This algorithm is significantly slower, but it is useful when simulating very large lattices where memory limits are reached. Bug Fixes: Corrected a major bug with the random number generator used to choose which neighboring site to use for a swapping attempt (This bug caused anisotropic domain growth in the previous versions of Ising_OPV) Improved the correlation function to catch rare cases where the correlation function does not cross the mix fraction value, and in these cases the domain size is set to the position of the first correlation minimu...|$|E
40|$|This report {{compares the}} {{estimated}} impacts {{of an education}} intervention based on an experimental design to the estimated impacts of the same intervention using a regression discontinuity (RD) design. The analysis uses data from two large-scale randomized controlled trials (RCTs) of education interventions—data from the IES restricted-use file from the Educational Technology Study and from the contractor-provided Teach for America Study <b>analysis</b> <b>file.</b> We found that the RD and experimental designs produced impact estimates that {{were not significantly different}} from one another, although the differences between the point estimates of impacts from the experimental and RD designs sometimes were nontrivial in size. We also found that manipulation of the assignment variable in RD designs can substantially influence RD impact estimates, particularly if manipulation is related to the outcome and occurs close to the assignment variable’s cutoff value...|$|E
40|$|Many large image {{databases}} are {{springing up}} as cheap optical technology finally lets users store {{large numbers of}} images. The database community has few {{solutions to the problem}} of finding one image, picture, or face out of a million. We present a solution which is simple, intuitive, and permits image-based searching on a large database by selection. Keywords: interfaces, image databases, pictures, images, statistics, clustering, vector spaces, face recognition, image <b>analysis,</b> <b>file</b> identification, file indexing. 1 Introduction When designing interfaces for image databases, current database ideology suggests querybased interfaces expressed in SQL or similar languages (e. g. [4].) We believe this approach is not well suited to typical use. Query-based interfaces are hard for most non-experts to use, and wearying even for experts. Users must possess knowledge about the domain of the database, the functionality of the retrieval system, and the classification scheme employed by the databa [...] ...|$|E
40|$|This release helps {{reduce the}} initial file sizes of your <b>analysis</b> <b>files</b> by {{compressing}} the objects {{to recreate the}} alchemical systems. This gives over 10 x reduction in the initial file sizes, and saves minutes off the initial start up before simulations start. Automatic Expanded Cutoff Distance Selection Compressed stored systems drastically reduce initial file sizes Requires OpenMMTools 0. 11. 2, minimum version updated Use C Yaml Dumper and Loaders to speed up YAML object processin...|$|R
50|$|Antivirus {{can be used}} {{to prevent}} {{propagation}} of malicious code. Most computer viruses have similar characteristics which allow for signature based detection. Heuristics such as <b>file</b> <b>analysis</b> and <b>file</b> emulation are also used to identify and remove malicious programs. Virus definitions should be regularly updated in addition to applying operating system hotfixes, service packs, and patches to keep computers on a network secure.|$|R
25|$|The Archival Cytometry Standard (ACS) {{is being}} {{developed}} to bundle data with different components describing cytometry experiments. It captures relations among data, metadata, <b>analysis</b> <b>files</b> and other components, and includes support for audit trails, versioning and digital signatures. The ACS container {{is based on the}} ZIP file format with an XML-based table of contents specifying relations among files in the container. The XML Signature W3C Recommendation has been adopted to allow for digital signatures of components within the ACS container.|$|R
40|$|Typically, large {{scientific}} datasets (order of terabytes) {{are generated}} at large computational centers, and stored on mass storage systems. However, large subsets {{of the data}} need to be moved to facilities available to application scientists for <b>analysis.</b> <b>File</b> replication of thousands of files is a tedious, error prone, but extremely important task in scientific applications. The automation of the file replication task requires automatic space acquisition and reuse, and monitoring the progress of staging thousands of files from the source mass storage system, transferring them over the network, archiving them at the target mass storage system or disk systems, and recovering from transient system failures. We have developed a robust replication system, called DataMover, which is now in regular use in High-Energy-Physics and Climate modeling experiments. Only a single command is necessary to request multifile replication or the replication of an entire directory. A web-based tool was developed to dynamically monitor {{the progress of the}} multi-file replication process. 1...|$|E
40|$|AbstractIn this paper, using APDL {{language}} {{to generate the}} <b>analysis</b> <b>file</b> the LED lamp heat sink of finite element model is established. The length, width and numbers of the heat sink fins being design variables, the maximum junction temperature of LED being the objective function, the mathematical model is set up and optimized using ANSYS thermal analysis software. The {{results show that the}} longer the length of heat sink fins, the lower the maximum junction temperature of the chip; the wider the fins width, the higher the maximum chip junction temperature; with the increasing of the numbers of fins, the maximum junction temperature will go down; but when reaching a certain value, the junction temperature will slowly ncrease again. Under the conditions of the LED chip junction temperature not exceeding 60 °C, the heat sink structure optimized values were: fin length is 62. 5 mm, fin width is 1 mm, the number of fins is 20...|$|E
40|$|Abstract: File system traces {{have been}} used for file system evaluation, user {{behavior}} <b>analysis,</b> <b>file</b> system debugging and intrusion detection. Current file system tracing tools are hardly applicable in real environments, because they require restarting the applications or are designed for specific kinds of file systems. We developed VFS Interceptor, a low-overhead tool for capturing file system traces dynamically. First, it detects VFS function pointers by recursive detecting, replaces them with stackable ones, in which traces are recorded. The detecting and replacing process is completely transparent to the applications. Second, VFS Interceptor uses a low-overhead compressing method {{to reduce the size of}} trace files, thus to reduce I/O overhead and save disk space. Finally, implemented between the VFS layer and the low-level file system, it can capture all important information and can be used for any low-level file system. The evaluations showed that the overhead of VFS Interceptor is no more than 4 % on various kinds of file systems. The compressing method can achieve a compressing ratio of up to 9 times and slightly reduce the overall overhead simultaneously...|$|E
40|$|Paper to be {{presented}} at the Midwest Economics Association meetings, March 27, 1999, Nashville, Tennessee. The excellent research assistance of Noyna DebBurman and excellent clerical support of Claire Black are gratefully acknowledged. The data used in this paper come from the National Household Education Survey of 1995, available from the National Center for Education Statistics of the U. S. Department of Education. The programs and <b>analysis</b> <b>files</b> used to produce the estimates reported here {{are available from the}} author. ECONOMIC PAYOFFS TO ADULT EDUCATIO...|$|R
40|$|In {{this report}} three {{subjects}} were addressed: (1) an efficiently applicable calculation method; (2) selection of `likely' research variables; and (3) {{the possibilities for}} an interactive analysis method. A method was worked out to apply efficient calculation techniques to the <b>analysis</b> <b>files,</b> which would calculate statistically reliable estimates for risks, including estimates for associated sound deviations. Also considered in doing so was {{the possibility of being}} able to arrive at these estimates adequately by using modules from the available software. It was important to inventory only those variables which would make a definite contribution to the interpretation of differences between road stretches or intersections. road stretches or intersections. Determining the class definitions to be distinguished during the inventory also had to take place as efficiently as possible. The developed calculation method was used to analyse the available existing <b>analysis</b> <b>files</b> containing data collected approximately ten years previously. A calculation method that could be elaborated into an interactive analysis method was worked out. By using this method, researchers or policy-making officials with specific questions in mind could determine whether a certain road characteristic would yield statistical differences in risk and in which classes these differences would turn up. It proved possible to draw up a prototype for acquiring experience. See also C 7633 - 7536...|$|R
30|$|Set {{analysis}} conditions. According to the <b>analysis</b> condition <b>file</b> (i.e. condition.txt in Table  1), {{the network}} parameters are changed from their base-state values to their fault-state values.|$|R
40|$|Current {{techniques}} for sound analysis and synthesis make powerful tools for designing new music instruments. Yet, the tools for investigating {{the possibility of}} creating new sounds from existing ones are rarely interactive by design, commonly geared towards either scientific or commercial applications. As music instruments need to be interactive {{in order to allow}} for expression, it is difficult to expand the usability of these powerful tools in new directions. A unique and exciting method is introduced within this thesis for exploring spectra data created through the well-known and powerful analysis/synthesis methods known as SMS (Spectral Modeling Synthesis). The system is designed to be as interactive as can be, exposing many low level parameters within an environment that can be manipulated in real-time. The main reason why this system is unique is that arbitrary time/frame specification is indefinitaly allowed. Modifications are then performed on {{some or all of the}} data in any number of ways, while not requiring a large <b>analysis</b> <b>file</b> to create realistic and interesting sounds. In order to mak...|$|E
40|$|This thesis {{presents}} {{the development of}} an intelligent CAD system (ICADS) for DC machines. This system was implemented on a PC using Common Lisp provided by Gold Works II and connected with a Sun Workstation working under the Unix system for file transfer. Both frame-based and rule-based knowledge representation techniques have been used to capture the knowledge about the design of electric machines. The system has its own inference engine specially designed to handle both decision making and numerical computation. Forward-chain reasoning and procedural attachment were used to construct this inference engine. The knowledge base management system provides a user interface for knowledge acquisition, knowledge representation, man-machine communication, design document preparation, etc. The design results are translated to a finite element <b>analysis</b> <b>file</b> and sent to MagNet 2 D, a powerful electromagnetic field analysis package for design checking and refinement. The system provides different levels of design automation to suit the needs of the system user. As examples the design results of a series of universal motors and a small DC motor are provided in the thesis...|$|E
40|$|This paper {{details the}} {{conceptual}} design previous term optimisation next {{term of the}} configuration and previous term composite next term lay-ups used to replace the conventional honeycomb stiffened previous term structure next term of a previous term Krueger flap. next term The multiple previous term composite next term laminates selected for redesigning the lay-ups within an initial symmetrical quasi-isotropic ply configuration of [0 / 45 /- 45 / 90]s, had to demonstrate full orthotropic characteristics. In order to construct a numerical process to optimise the required multi-layered previous term composite shells,next term a commercial finite element code, Ansys, was used to develop a previous term parametric next term <b>analysis</b> <b>file.</b> This analysis subroutine was then integrated into an Ansys previous term Parametric next term Design Language code embedding {{the objective of the}} previous term optimisation next term process "mass minimisation" as well as all the constraints and the allowable domains of the parameters. The paper, in its conclusion, presents a comparison between the original product and the optimal design, and reviews the advantages of the future implementation of this design...|$|E
5000|$|Automation of many {{traditionally}} manual {{steps in}} the localization process, including: filtering and segmentation, TM leveraging, <b>analysis,</b> costing, <b>file</b> handoffs, email notifications, TM update, target file generation ...|$|R
40|$|This work {{created a}} {{database}} for tracking data <b>analysis</b> <b>files</b> from multiple lab techniques and equipment stored on a central file server. Experimental details appropriate for each file type are {{pulled from the}} file header and stored in a searchable database. The database also stores specific location and self-directory structure for each data file. Queries can be run on the database according to file type, sample type or other experimental parameters. The database was constructed in Microsoft Access and Visual Basic was used for extraction of information from the file header...|$|R
40|$|A current {{focus of}} {{proteomics}} {{research is the}} establishment of acceptable confidence measures in the assignment of protein identifications in an unknown sample. Development of new algorithmic approaches would greatly benefit from a standard reference set of spectra for known proteins {{for the purpose of}} testing and training. Here we describe an openly available library of mass spectra generated on an ABI 4700 MALDI TOF/TOF from 246 known, individually purified and trypsin-digested protein samples. The initial full release of the Aurum Dataset includes gel images, peak lists, spectra, search result <b>files,</b> decoy database <b>analysis</b> <b>files,</b> FASTA file of protein sequences, manual curation, and summary pages describing protein coverage and peptides matched by MS/MS followed by decoy database analysis using Mascot, Sequest, and X!Tandem. The data are publicly available for use at ProteomeCommons. org...|$|R
40|$|This {{repository}} contains {{code and}} manuscripts {{for a project}} in the lab relating to predicting the amount of diet induced obesity. This PredictorsDietInducedObesity data is made available under the Open Data Commons Attribution License: [URL] - See more at: [URL] For more information see the LICENSE. txt file in this directory. Raw Data [...] - All raw data is in the data/raw folder. This data is automatically obtained from our internal LIMS system and can only be updated from that point. The script files use this raw data to do the analysis. Data which has been processed is saved in the data/processed folder. Script files [...] This code base includes the raw data and reproducible R code for these analyses within the scripts folder. All <b>analysis</b> <b>file</b> are R scripts as Rmarkdown (Rmd) files. These files can be run inside RStudio [URL] and will generate the md and html files which include the processed data. For more information on using R and using these files see [URL] Publications [...] All posters, manuscripts or external presentations are in the publications folder. This includes revisions, and where possible reviewer comments and responses...|$|E
40|$|BackgroundUnderstanding glioma {{etiology}} requires {{determining which}} environmental factors {{are associated with}} glioma. Upper Midwest Health Study case 22 ̆ 0 ac 2 ̆ 01 ccontrol participant work histories collected 199522 ̆ 0 ac 2 ̆ 01 c 1998 were evaluated for occupational associations with glioma. 22 ̆ 0 ac 1 ̆ 53 Exposures of interest 22 ̆ 0 ac? from our study protocol comprise our a priori hypotheses. Materials and MethodsYear-long or longer jobs for 1, 973 participants were assigned Standard Occupational Classifications (SOC) and Standard Industrial Classifications (SIC). The <b>analysis</b> <b>file</b> includes 8, 078 SIC- and SOC-coded jobs. For each individual, SAS 9. 2 programs collated employment with identical SIC-SOC coding. Distributions of longest 22 ̆ 0 ac 1 ̆ 53 total employment duration 22 ̆ 0 ac? (total years worked in jobs with identical industry and occupation codes, including multiple jobs, and non-consecutive jobs) were compared between cases and controls, using an industrial hygiene algorithm to group occupations. ResultsLongest employment duration was calculated for 780 cases and 1, 156 controls. More case than control longest total employment duration was in the 22 ̆ 0 ac 1 ̆ 53 engineer, architect 22 ̆ 0 ac? occupational group [16 cases, 10 controls, odds ratio (OR) 2. 50, adjusted for age group, sex, age and education, 95...|$|E
40|$|Although age is {{recognized}} as the strongest predictor of mortality in chronic disease epidemiology, a calendar-based approach is often employed when evaluating time-related variables. An age-based <b>analysis</b> <b>file,</b> created by determining the value of each time-dependent variable for each age that a cohort member is followed, provides a clear definition of age at exposure and allows development of diverse analytic models. To demonstrate methods, the relationship between cancer mortality and external radiation was analyzed with Poisson regression for 14, 095 Oak Ridge National Laboratory workers. Based on previous analysis of this cohort, a model with ten-year lagged cumulative radiation doses partitioned by receipt before (dose-young) or after (dose-old) age 45 was examined. Dose-response estimates were similar to calendar-year-based results with elevated risk for dose-old, but not when film badge readings were weekly before 1957. Complementary results showed increasing risk with older hire ages and earlier birth cohorts, since workers hired after age 45 were born before 1915, and dose-young and dose-old were distributed differently by birth cohorts. Risks were generally higher for smokingrelated than non-smoking-related cancers. It was difficult to single out specific variables associated with elevated cancer mortality because of: (1) birth cohort differences in hire age and mortality experience completeness, and (2) time-period differences in working conditions, dose potential, and exposure assessment. This research demonstrated the utility and versatility of the age-based approach...|$|E
40|$|Central Michigan University, whose {{financial}} support is gratefully acknowledged. Jean Kimmel {{made a number}} of comments that substantially improved the paper. The excellent research assistance of Becky Jacobs and excellent clerical support of Claire Vogelsong are also acknowledged. The data used in this paper come from the National Household Education Survey, available from the U. S. Department of Education, and the January 1991 Current Population Survey, available from the U. S. Department of Labor. The programs and <b>analysis</b> <b>files</b> used to produce the estimates reported here are available from the author upon request. 1 THE ECONOMIC PAYOFFS TO WORKPLACE LITERACY 1...|$|R
5000|$|UniSay: Sophisticated Post-production <b>file</b> <b>analysis</b> {{and audio}} {{processing}} based on MPEG-7.|$|R
50|$|Currently, runtime {{verification}} {{techniques are}} often presented with various alternative names, such as runtime monitoring, runtime checking, runtime reflection, runtime analysis, dynamic analysis, runtime/dynamic symbolic analysis, trace <b>analysis,</b> log <b>file</b> <b>analysis,</b> etc., all referring to instances {{of the same}} high-level concept applied either to different areas or by scholars from different communities. Runtime verification is intimately related to other well-established areas, such as testing (particularly model-based testing) when used before deployment and fault-tolerant systems when used during deployment.|$|R
