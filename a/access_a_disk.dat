6|10000|Public
25|$|By {{integrating}} sockets {{with the}} Unix operating system's file descriptors, it became almost {{as easy to}} read and write data across a network as it was to <b>access</b> <b>a</b> <b>disk.</b> The AT laboratory eventually released their own STREAMS library, which incorporated much of the same functionality in a software stack with a different architecture, but the wide distribution of the existing sockets library reduced the impact of the new API. Early versions of BSD were used to form Sun Microsystems' SunOS, founding the first wave of popular Unix workstations.|$|E
50|$|Buffers can {{increase}} application performance by allowing synchronous operations such as file reads or writes to complete quickly instead of blocking {{while waiting for}} hardware interrupts to access a physical disk subsystem; instead, an operating system can immediately return a successful result from an API call, allowing an application to continue processing while the kernel completes the disk operation in the background. Further benefits can be achieved if the application is reading or writing small blocks of data that do not correspond to the block size of the disk subsystem, allowing a buffer {{to be used to}} aggregate many smaller read or write operations into block sizes that are more efficient for the disk subsystem, {{or in the case of}} a read, sometimes to completely avoid having to physically <b>access</b> <b>a</b> <b>disk.</b>|$|E
5000|$|The Motorola 6800 and the Intel 8080 were {{designed}} {{at the same}} time and were similar in function. The 8080 was an extension and enhancement of the Intel 8008, which in turn was an LSI implementation of the TTL-based CPU design used in the Datapoint 2200. The 6800 architecture was modeled after the DEC PDP-11 processor. Both the 8080 and the 6800 were TTL compatible, had an 8-bit bidirectional data bus, a 16-bit stack pointer, a 16-bit address bus that could address 64 KB of memory, and came in a 40-pin DIP package. The 6800 had two accumulators and a 16-bit index register. The direct addressing mode allowed fast access to the first 256 bytes of memory. I/O devices were addressed as memory so there were no special I/O instructions. The 8080 had more internal registers and instructions for dedicated I/O ports. When the 8080 was reset, the program counter was cleared and the processor started at memory location 0000. The 6800 loaded the program counter from the highest address and started at the memory location stored there. The 6800 had a three-state control that would disable the address bus to allow another device direct memory <b>access.</b> <b>A</b> <b>disk</b> controller could therefore transfer data into memory with no load on the processor. It was even possible to have two 6800 processors access the same memory. [...] However, in practice systems of such complexity usually required the use of external bus transceivers to drive the system bus; in such circuits the on-processor bus control was disabled entirely in favor of using the similar capabilities of the bus transceiver. In contrast, the 6802 dispensed with this on-chip control entirely in order to free up pins for other functions in the same 40-pin package as the 6800, but this functionality could still be achieved using an external bus transceiver.|$|E
5000|$|The games {{themselves}} may not <b>access</b> the <b>disk</b> (<b>a</b> GameMaker limit) ...|$|R
50|$|On some {{operating}} systems, {{a network}} block device is a device node whose content {{is provided by}} a remote machine. Typically, network block devices are used to <b>access</b> <b>a</b> storage device that does not physically reside in the local machine but on a remote one. As an example, a local machine can <b>access</b> <b>a</b> hard <b>disk</b> drive that is attached to another computer.|$|R
5000|$|The 2841 was a microprogrammed {{control unit}} [...] "intended {{for use in}} {{controlling}} <b>access</b> to <b>a</b> <b>disk</b> or strip file or a slow-speed drum storage unit." [...] It connected {{to one or two}} standard System/360 channels, or could also be attached to an IBM 1130 or IBM 1800 Data Acquisition and Control System to add support for 2311 disks.|$|R
40|$|Disk drives are {{the most}} {{commonly}} used secondary storage devices in computer systems. The way operating systems access these devices leads {{to a wide range of}} variability in access time. In this paper we study the detailed temporal characteristics of disk drives. We describe a comprehensive set of experiments designed to build a model for the disk drive. Simulation is used to validate the model. This disk model will help design a device driver which can achieve a high degree of temporal determinacy. 1 Introduction During the recent years, disk drives have tremendously improved in terms of capacity, speed, reliability, and physical size. Even though several other secondary storage technologies have emerged, disk drives remain the dominant choice. However, the traditional method to <b>access</b> <b>a</b> <b>disk</b> has not changed. Applications send a read/write request with the proper disk address to an operating system process called the device driver. The application process requesting the disk service i [...] ...|$|E
40|$|Retrieving {{sequential}} rich media {{content from}} modern commodity disks is a challenging task. As disk capacity increases, {{there is a}} need to increase the number of streams that are allocated to each disk. However, when multiple streams are accessing a single disk, throughput is dramati-cally reduced because of disk head seek overhead, resulting in requirements for more disks. Thus, there is a tradeoff between how many streams should be allowed to <b>access</b> <b>a</b> <b>disk</b> and the total throughput that can be achieved. In this work we examine this tradeoff and provide an understanding of issues along with a practical solution. We use Disksim, a detailed architectural simulator, to examine several aspects of a modern I/O subsystem and we show the effect of various disk parameters on system performance under multiple sequential streams. Then, we propose a solution that dynamically adjusts I/O request streams, based on host and I/O subsystem parameters. We implement our approach in a real system and perform experiments with a small and a large disk configuration. Our approach improves disk throughput up to a factor of 4 with a workload of 100 sequential streams, without requiring large amounts of memory on the storage node. Moreover, it is able to adjust (statically) to different storage node configurations, essentially making the I/O subsystem insensitive to the number of I/O streams used. 1...|$|E
5000|$|A {{tape drive}} {{provides}} sequential <b>access</b> storage, unlike <b>a</b> hard <b>disk</b> drive, which provides direct <b>access</b> storage. <b>A</b> <b>disk</b> drive can move to any {{position on the}} <b>disk</b> in <b>a</b> few milliseconds, but a tape drive must physically wind tape between reels to read any one particular piece of data. As a result, tape drives have very slow average seek times to data. However, tape drives can stream data very quickly off a tape when the required position has been reached. For example, [...] Linear Tape-Open (LTO) supported continuous data transfer rates of up to 140 MB/s, comparable to hard disk drives.|$|R
40|$|We have {{developed}} a scheme to secure networkattached storage systems against many types of attacks. Our system uses strong cryptography to hide data from unauthorized users; someone gaining complete <b>access</b> to <b>a</b> <b>disk</b> cannot obtain any useful data from the system, and backups can be done without allowing the super-user access to cleartext. While insider denial-of-service attacks cannot be prevented (an insider can physically destroy the storage devices), our system detects attempts to forge data. The system was developed using <b>a</b> raw <b>disk,</b> and can be integrated into common file systems...|$|R
25|$|Tries can be slower in {{some cases}} than hash tables for looking up data, {{especially}} if the data is directly <b>accessed</b> on <b>a</b> hard <b>disk</b> drive or some other secondary storage device where the random-access time is high compared to main memory.|$|R
50|$|A notable {{example is}} {{in the context of}} {{platform}} virtualization, and a feature of certain virtualization software is the ability to <b>access</b> <b>a</b> hard <b>disk</b> at the raw level. Virtualization software may typically function via the usage of a virtual drive format like OVF, but some users may want the virtualization software to be able to run an operating system that has been installed on another disk or disk partition. In order to do this, the virtualization software must allow raw disk access to that disk or disk partition, and then allow that entire operating system to boot within the virtualization window.|$|R
40|$|We {{consider}} {{the problem of}} removing <b>a</b> given <b>disk</b> from <b>a</b> collection of unit disks in the plane. At each step, we allow <b>a</b> <b>disk</b> to be removed by a collision-free translation to infinity, and {{the goal is to}} <b>access</b> <b>a</b> given <b>disk</b> using as few steps as possible. This <b>Disks</b> problem is <b>a</b> version of a common task in assembly sequencing, namely removing a given part from a fully assembled product. Recently there has been a focus on optimizing assembly sequences over various cost measures, however with very limited algorithmic success. We explain this lack of success, proving strong inapproximability results in this simple geometric setting. Namely, we show that approximating the number of steps required to within a factor of 2 log 1 -# n for any # > 0 is quasi-NP-hard. This provides the first inapproximability results for assembly sequencing, realized in a geometric setting. As a stepping stone, we study the approximability of scheduling with and/or precedence constraints. The Disks problem [...] ...|$|R
50|$|SATA (and older PATA) {{hard drives}} use the Advanced Technology Attachment (ATA) {{protocol}} to issue commands, such as read, write, and status. AoE encapsulates those commands inside Ethernet frames and lets them travel over an Ethernet network {{instead of a}} SATA or 40-pin ribbon cable. Although internally AoE uses the ATA protocol, it presents the disks as SCSI to the operating system. Also the actual disks can be SCSI or any other kind, AoE {{is not limited to}} disks that use the ATA command set. By using an AoE driver, the host operating system is able to <b>access</b> <b>a</b> remote <b>disk</b> as if it were directly attached.|$|R
5000|$|In 1974, IMS was {{contacted}} by a client which wanted a [...] "workstation system" [...] that could complete jobs for any General Motors car dealership. IMS planned a system including a terminal, small computer, printer, and special software. Five of these workstations {{were to have}} common <b>access</b> to <b>a</b> hard <b>disk</b> drive, which would be controlled by a small computer. Eventually product development was stopped.|$|R
40|$|Abstract. We {{consider}} {{the problem of}} removing <b>a</b> given <b>disk</b> from <b>a</b> collection of unit disks in the plane. At each step, we allow <b>a</b> <b>disk</b> to be removed by a collision-free translation to infinity, and {{the goal is to}} <b>access</b> <b>a</b> given <b>disk</b> using as few steps as possible. This <b>Disks</b> problem is <b>a</b> version of a common task in assembly sequencing, namely removing a given part from a fully assembled product. Recently there has been a focus on optimizing assembly sequences over various cost measures, however with very limited algorithmic success. We explain this lack of success, proving strong inapproximability results in this simple geometric setting. Namely, we show that approximating the number of steps required to within a factor of 2 log 1 −γ n for any γ> 0 is quasi-NP-hard. This provides the first inapproximability results for assembly sequencing, realized in a geometric setting. As a stepping stone, we study the approximability of scheduling with and/or precedence constraints. The Disks problem can be formulate...|$|R
50|$|Hard disks {{and modern}} linear {{serpentine}} tape drives do not precisely fit into either category. Both have many parallel tracks across {{the width of}} the media and the read/write heads take time to switch between tracks and to scan within tracks. Different spots on the storage media take different amounts of time to <b>access.</b> For <b>a</b> hard <b>disk</b> this time is typically less than 10 ms, but tapes might take as much as 100 s.|$|R
40|$|Disk {{throughput}} {{is one of}} {{the main}} impediments to improving performance for data-intensive servers. In this paper, we propose two management techniques for the disk controller cache that can significantly increase disk throughput. Both techniques employ a block-based, rather than the traditional segment-based, organization of the disk controller cache. The first technique, called File-Oriented Read-ahead (FOR), adjusts the number of read-ahead blocks brought into the cache according to file system information. The second technique, called Largest Distance First (LDF), replaces a block according to the spatial distance between the block and the last block <b>accessed</b> in <b>a</b> <b>disk</b> stream. Our detailed simulations of real server workloads show that FOR and LDF can improve server throughput by as much as 21 % and 150 %, respectively, in comparison to conventional cache management. ...|$|R
5000|$|In 1974, IMS was {{contacted}} by a client which wanted a [...] "workstation system" [...] that could complete jobs for any General Motors new-car dealership. IMS planned a system including a terminal, small computer, printer, and special software. Five of these work stations {{were to have}} common <b>access</b> to <b>a</b> hard <b>disk,</b> which would be controlled by a small computer. Eventually product development was stopped. Millard and his chief engineer Joe Killian turned to the microprocessor.|$|R
40|$|We have {{developed}} a scheme to secure networkattached storage systems against many types of attacks. Our system uses strong cryptography to hide data from unauthorized users; someone gaining complete <b>access</b> to <b>a</b> <b>disk</b> cannot obtain any useful data from the system, and backups can be done without allowing the superuser access to unencrypted data. While denial-of-service attacks cannot be prevented, our system detects forged data. The system was developed using <b>a</b> raw <b>disk,</b> and can be integrated into common file systems. We discuss the design and security tradeoffs such a distributed file system makes. Our design guards against both remote intruders and those who gain physical access to the disk, using just enough security to thwart both types of attacks. This security can be achieved with little penalty to performance. We discuss the security operations that are necessary {{for each type of}} operation, and show that {{there is no longer any}} reason not to include strong encryption and authentication in network file systems. 1...|$|R
40|$|As {{part of our}} {{research}} into improving synthetic, blocklevel I/O workloads, we developed an algorithm that generates <b>a</b> synthetic <b>disk</b> <b>access</b> pattern based on given distributions of both (1) sectors accessed and (2) “jump distances” between successive <b>disk</b> <b>accesses.</b> Generating <b>a</b> synthetic <b>disk</b> <b>access</b> pattern that maintains both distributions exactly is an NP-complete problem (similar to the Traveling Salesman problem). In this paper, we (1) discuss our approximation algorithm, (2) show that it runs in {{a reasonable amount of}} time, (3) show that it reproduces both distributions with reasonable accuracy, and (4) demonstrate its overall effect on the quality of block-level synthetic workloads. 1...|$|R
40|$|Processing and {{analysing}} {{large volumes}} of data plays an increasingly important role in many domains of scientific research. The complexity and irregularity of datasets in many domains make the task of developing such processing applications tedious and error-prone. We propose use of high-level abstractions for hiding the irregularities in these datasets and enabling rapid development of correct, but not necessarily efficient, data processing applications. We present two execution strategies {{and a set of}} compiler analysis techniques for obtaining high performance from applications written using our proposed high-level abstractions. Our execution strategies achieve high locality in <b>disk</b> <b>accesses.</b> Once <b>a</b> <b>disk</b> block is read from the disk, all iterations that read any of the elements from this disk block are performed. To support our execution strategies and improve the performance, we have developed static analysis techniques for: 1) computing the set of iterations that <b>access</b> <b>a</b> particular righ-hand-side element, 2) generating a function that {{can be applied to the}} meta-data associated with each disk block, for determining if that disk block needs to be read, and 3) performing code hoisting of conditionals. Cross-referenced as UMIACS-TR- 2001 - 5...|$|R
40|$|Processing and {{analyzing}} {{large volumes of}} data plays an increasingly important role in many domains of scientific research. The complexity and irregularity of datasets in many domains make the task of developing such processing applications tedious and errorprone. We propose use of high-level abstractions for hiding the irregularities in these datasets and enabling rapid development of correct data processing applications. We present two execution strategies {{and a set of}} compiler analysis techniques for obtaining high performance from applications written using our proposed high-level abstractions. Our execution strategies achieve high locality in <b>disk</b> <b>accesses.</b> Once <b>a</b> <b>disk</b> block is read from the disk, all iterations that access any of the elements from this disk block are performed. To support our execution strategies and improve the performance, we have developed static analysis techniques for: 1) computing the set of iterations that <b>access</b> <b>a</b> particular right-hand-side element, 2) generating a function that {{can be applied to the}} meta-data associated with each disk block, for determining if that disk block needs to be read, and 3) performing code hoisting of conditionals. We present experimental results from a prototype compiler implementing our techniques to demonstrate the effectiveness of our approach...|$|R
40|$|Introduction The work {{presented}} in this dissertation was motivated by two recent changes in technology: networks and large memories. The introduction of networks {{has led to a}} move away from centralized timesharing operating systems towards network operating systems. In these network operating systems each user has a personal highperformance workstation and communicates with other users across a network. Data that was once stored on a single set of disks in the timesharing systems is now distributed amongst the disks of several workstations. In fact, many of the workstations do not have any disk at all; the data for these diskless workstations is stored across the network on the disks of other workstations. The move towards network operating systems poses two problems: how to provide users with high performance and how to allow users to easily share data. Performance is a problem in network environments because each access of data may require both <b>a</b> network <b>access</b> and <b>a</b> <b>disk</b> ac...|$|R
40|$|This study empirically {{investigates the}} usage of a {{customized}} discrete firefly algorithm (DFA) for ordering the disk requests to minimize the total access time. The procedure simulates the movement of each firefly within a population towards others using a variation of edge-based mutation. The algorithm was applied to randomized standard disk sequences with varying length of input disk requests. Owing to the greater impact of seek time in the determination of <b>access</b> time for <b>a</b> <b>disk</b> having sizable number of tracks, this has been taken as the primary performance factor in the scheduling of tasks. The analysis of the results obtained establishes the relative advantage of using the firefly optimization method over the traditional disk scheduling algorithms...|$|R
40|$|This paper {{presents}} {{a family of}} programming projects appropriate to a sophomore-level data structures course, centered around {{the concept of a}} buffer pool serving as the <b>access</b> intermediary to <b>a</b> <b>disk</b> file. These projects provide a meaningful vehicle for practicing object-oriented design techniques and teach fundamental material on file processing and manipulating binary data. I begin with a concrete example, a heap stored on disk and mediated by a buffer pool. Several important intellectual concepts introduced by such a project are enumerated. Significant extensions and alternatives to the basic project are then described. I conclude with some observations on the role of file processing in modern CS curricula, and the significance of recent trends away from coverage of these topics...|$|R
40|$|Abstract-Memory {{resident}} database systems (MMDB’s) store 2) Main {{memory is}} normally volatile, while disk storage is their data in main physical memory and provide very high-speed not. However, {{it is possible}} (at some cost) to construct access. Conventional database systems are optimized for the particular characteristics of disk storage mechanisms. Memory resident systems, on the other hand, use different optimizations to structure and organize data, {{as well as to}} make it reliable. nonvolatile main memory. 3) <b>Disks</b> have <b>a</b> high, fixed cost per access that does not depend on the amount of data that is retrieved during the This paper surveys the major memory residence optimizations access. For this reason, disks are block-oriented storage and briefly discusses some of the memory have been designed or implemented. resident systems that devices. Main memory is not block oriented. 4) The layout of data on <b>a</b> <b>disk</b> is much more critical than Index Terms- Access methods, application programming in-the layout of data in main memory, since sequential terface, commit processing, concurrency control, data clustering, data representation, main memory database system (MMDB), query processing, recovery. <b>access</b> to <b>a</b> <b>disk</b> is faster than random access. Sequential access is not as important in main memories. 5) Main memory is normally directly accessible by the processor(s), while disks are not. This may make data in I...|$|R
40|$|Abstract. Full-text searching {{consists}} in locating the occurrences {{of a given}} pattern P[1 [...] m] in a text T[1 [...] u], both sequences over an alphabet of size σ. In this paper we define a new index for full-text searching on secondary storage, based on the Lempel-Ziv compression algorithm and requiring 8 uHk +o(u log σ) bits of space, where Hk denotes the k-th order empirical entropy of T, for any k = o(log σ u). Our experimental results show that our index is significantly smaller than any other practical secondary-memory data structure: 1. 4 – 2. 3 times the text size including the text, which means 39 %– 65 % the size of traditional indexes like String B-trees [Ferragina and Grossi, JACM 1999]. In exchange, our index requires more disk access to locate the pattern occurrences. Our index is able to report up to 600 occurrences per <b>disk</b> <b>access,</b> for <b>a</b> <b>disk</b> page of 32 kilobytes. If we only need to count pattern occurrences, the space {{can be reduced to}} about 1. 04 – 1. 68 times the text size, requiring about 20 – 60 disk accesses, depending on the pattern length...|$|R
40|$|This paper {{describes}} a general-purpose file system {{that uses a}} writeonce -read-many (WORM) optical <b>disk</b> <b>accessed</b> via <b>a</b> magnetic <b>disk</b> cache. The cache enables blocks to be modified multiple times before they are written to the WORM and increases performance. Snapshots of the file system can be made at any time without limiting the users' access to files. These snapshots reside entirely on the WORM, are accessible to the user via a second read-only file system, do not contain multiple copies of unchanged data, {{and can be used}} to rebuild the file system in the event that the disk cache is destroyed. The file system has been implemented as part of Plan 9, an experimental operating system under development at AT&T Bell Laboratories...|$|R
40|$|Compression is {{becoming}} more important as more information is stored on, and transferred between computers. Some applications of compression require high throughput, such as the <b>access</b> to <b>a</b> hard <b>disk</b> drive or <b>a</b> Local Area Network. The Acorn RISC Machine (ARM) is a general purpose VLSI microprocessor with a very competitive price/performance ratio. Its architecture is particularly suitable for applications requiring frequent bitwise operations. It has been used here to construct a fast and inexpensive text decompressor. The algorithm used {{is a type of}} Ziv-Lempe! compression scheme and has the properties of good compression and a fast decode strategy. The resulting device is very fast, yet inexpensive. It can operate at the speed of Local Area Networks and hard disk drives and lends itself to applications where text is stored once and read many times...|$|R
40|$|Today’s {{enterprise}} computing systems routinely {{employ a}} large number of computers for tasks ranging from supporting daily business operations to mission-critical backend applications. These computers consume a lot of energy whose monetary cost accounts for a significant portion of an enterprise’s operating budget. Consequently, enterprises employ energy saving techniques such as turning machines off overnight and dynamic energy management during the business hours. Unfortunately, dynamic energy management, especially that for disks, introduces delays when <b>an</b> <b>accessed</b> <b>disk</b> is in <b>a</b> low power state and needs to be brought into an active state. Existing techniques mainly focus on reducing energy consumption and do not take advantage of enterprisewide resources to mitigate the associated delays. Thus, systems designers are faced with a critical trade-off: saving energ...|$|R
40|$|This paper {{presents}} {{some thoughts}} on problems arising in I/O systems. Firstly, the I/O needs of processors and multiprocessors and currently-used solutions are outlined. Next, for a multiprocessor architecture with Uniform Memory <b>Access</b> (UMA) <b>a</b> set of RAID-type disks is proposed whose access model is called Uniform <b>Disk</b> <b>Access</b> (UDA). <b>A</b> multiport <b>disk</b> cache with ultra-high-rate serial links carries out the interface between the multiprocessor and the set of <b>disks,</b> behaving as <b>a</b> virtual crossbar. In this way the bottleneck of I/Os can be eliminated, and their latency time can be considerably reduced. In this architecture data transfer is no longer performed using an SCSI bus but by using ultra-high-rate serial links, thus improving the transfer throughput of the I/O system-processor bus to a rate higher {{than or equal to}} that of the memory-processor bus. Out of this new approach a throughput of more than 1 Gbit/sec per processor is obtained. Key words: I/O system, serial multiportdisk cac [...] ...|$|R
50|$|<b>A</b> hard <b>disk</b> drive {{failure occurs}} when <b>a</b> hard <b>disk</b> drive {{malfunctions}} and the stored information cannot be <b>accessed</b> with <b>a</b> properly configured computer.|$|R
5000|$|... #Caption: <b>A</b> hard <b>disk</b> head on <b>an</b> <b>access</b> arm {{resting on}} <b>a</b> hard <b>disk</b> platter ...|$|R
40|$|This paper {{looks at}} {{optimising}} the energy costs for storing user-generated content when accesses are highly skewed towards a few “popular ” items, but the popularity ranks vary dynamically. Using traces from a video-sharing website {{and a social}} news website, it is shown that the non-popular content, which constitute the majority by numbers, tend to have accesses which spread locally in the social network, in a viral fashion. Based on the proportion of viral accesses, popular data is separated onto <b>a</b> few <b>disks</b> on storage. The popular disks receive the majority of accesses, allowing other disks to be spun down {{when there are no}} requests, saving energy. Our technique, SpinThrift, improves upon Popular Data Concentration (PDC), which, in contrast with our binary separation between popular and unpopular items, directs the majority of <b>accesses</b> to <b>a</b> few <b>disks</b> by arranging data according to popularity rank. Disregarding the energy required for data reorganisation, SpinThrift and PDC display similar energy savings. However, because of the dyamically changing popularity ranks, SpinThrift requires less than half the number of data reorderings compared to PDC...|$|R
40|$|Abstract—Modern {{enterprises}} employ {{hundreds of}} workstations for daily business operations, which consume {{a lot of}} energy and thus have significant operating costs. To reduce such costs, dynamic energy management is often employed. However, dynamic energy management, especially that for disks, introduces delays when <b>an</b> <b>accessed</b> <b>disk</b> is in <b>a</b> low power state and needs to be brought into active state. In this paper, we propose System-wide Alternative Retrieval of Data (SARD) that exploits the large number of machines in an enterprise environment to transparently retrieve binaries from other nodes, thus avoiding access delays when the local <b>disk</b> is in <b>a</b> low power mode. SARD uses a software-based approach to reduce spin-up delays while eliminating the need for major operating system changes, custom buffering, or shared memory infrastructure. I...|$|R
