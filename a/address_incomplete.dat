14|67|Public
40|$|One of Semantic Web {{strengths}} is {{the ability}} to <b>address</b> <b>incomplete</b> knowledge. However, at present, it cannot handle incomplete knowledge directly. Also, it cannot handle non-monotonic reasoning. In this paper, we extend ALC − Defeasible Description Logic with existential quantifier, i. e., ALE Defeasible Description Logic. Also, we modify some parts of the logic, resulting in an increasing efficiency in its reasoning...|$|E
40|$|Applicants must be {{at least}} 16 years of age at the time of filing an application. Please PRINT CLEARLY all answers on this application, and file it by mail or in person at the above <b>address.</b> (<b>INCOMPLETE</b> OR UNSIGNED APPLICATIONS WILL NOT BE ACCEPTED). Labor Service {{registration}} is valid for FIVE YEARS subject to all provisions of Civil Service Law and Rules. If you wish to renew your registration beyond that time, you must notify the Human Resources Department in writing no earlier than six months before, or no later than six months after the fifth anniversary of your registration. Failure to provide such notification will result in removal from the labor registration list...|$|E
30|$|Unlike {{other methods}} used to <b>address</b> <b>incomplete</b> data {{relating}} to social networks [4, 5], the question at hand is not if a rivalry exists, but rather to which rivalry a violent event belongs. This structure of between gang rivalries {{can be viewed as}} a social network [6] often embedded in space [1, 2]. Violent events involving gangs tend to be dyadic, and so we can formulate these events as a realization of a stochastic process occurring on the edges of the rivalry network. For each edge in the network there exists a different stochastic process. In our analysis however, we use identical parameters to generate synthetic data. The method does not assume that the underlying parameters generating each process are identical.|$|E
30|$|The organic blue, {{possibly}} turnsole, will be <b>addressed</b> in “The <b>incomplete</b> recipe of katasol”.|$|R
40|$|Microfinance Institutions (MFIs) provide {{loans to}} low income individuals. The credit scoring systems of MFIs, if they exist, are {{strictly}} financial. Although many MFIs consider the social {{impact of their}} loans, they do not incorporate formal systems to estimate this social impact. This paper proposes that their creditworthiness evaluations should be coherent with their social mission and should, accordingly, estimate the social impact of microcredit. Thus, a decision support system to facilitate microcredit granting is proposed, and multicriteria evaluation is used to translate MFI’s social mission into numbers. The assessment of social impact is performed by calculating the Social Net Present Value (SNPV). The system captures credit officers’ experience and <b>addresses</b> <b>incomplete</b> and intangible information. The model has been tested in a microfinance institution. The paper illustrates an example of its use in practice. info:eu-repo/semantics/publishe...|$|R
40|$|Examines how popular, oppositional {{historical}} {{consciousness is}} formed and expressed among poor and marginalized rural proletarians of Haitian descent {{living in the}} Dominican Republic. Author <b>addresses</b> the <b>incomplete</b> and contested nature of hegemony in the Caribbean and {{raises questions about the}} relationship between 'vernacular' and 'official' knowledge about the past, whether they always oppose each other or whether there is overlap...|$|R
40|$|We were {{motivated}} {{to define and}} build a sophisticated satellite simulation capability for the evaluation at a satellite operations automated environment called IntelliSTAR. This architecture, and associated prototype, addresses the entire spacecraft operations cycle including planning, scheduling, task execution, and analysis. It is aimed at increasing the autonomous capability of current and future spacecraft. It utilizes advanced software techniques to <b>address</b> <b>incomplete</b> and conflicting data for making decisions. It also encompasses critical response time requirements, complex relationships among multiple systems, and dynamically changing objectives. Given the extreme scope of activities that are targeted, a sophisticated, flexible, and dynamic simulation environment was required to drive this prototype. In particular the derived requirements for evaluating the IntelliSTAR prototype include realistic and dynamic environment, easily reconfigurable, and multiple levels of fidelity...|$|E
40|$|Present {{research}} work evaluates {{the influence of}} both density and size on the treatability of Aluminum-based (6000 series) foam-parts subjected to a typical solid solution heat treatment (water quenching). The results are compared with those obtained for the bulk alloy, evaluating the fulfilment of cooling requirements. Density of the foams was modeled by tomography analysis and the thermal properties calculated, based on validated density-scaled models. With this basis, cooling velocity maps during water quenching were predicted by finite element modeling (FEM) in which boundary conditions were obtained by solving the inverse heat conduction problem. Simulations under such conditions have been validated experimentally. Obtained results <b>address</b> <b>incomplete</b> matrix hardening for foam-parts bigger than 70 mm in diameter with a density below 650 kg/m 3. An excellent agreement {{has been found in}} between the predicted cooling maps and final measured microhardness profiles...|$|E
40|$|ABSTRACT: Comprehensive {{two-dimensional}} chromatography is {{a powerful}} technology for analyzing the patterns of constituent compounds in complex samples, but matching chromatographic features for comparative analysis across large sample sets is difficult. Various methods have been described for pairwise peak matching between two chromatograms, but the peaks indicated by these pairwise matches commonly are incomplete or inconsistent across many chromatograms. This paper describes a new, automated method for postprocessing the results of pairwise peak matching to <b>address</b> <b>incomplete</b> and inconsistent peak matches and thereby select chromatographic peaks that reliably correspond across many chromatograms. Reliably corresponding peaks can be used both for directly comparing relative compositions across large numbers of samples and for aligning chromatographic data for comprehensive comparative analyses. To select reliable features {{for a set of}} chromatograms, the Consistent Cliques Method (CCM) represents all peaks from all chromatograms and all pairwise pea...|$|E
50|$|An address must be {{complete}} {{in order to}} be valid. This means that it must have a street, city, state and ZIP code. Whenever possible, <b>addresses</b> that are <b>incomplete</b> have the missing information added.|$|R
5000|$|Learning {{from early}} projects, Avanti {{produced}} practical working documentation and tested the methods on live projects. This information supplemented the CPIC document Code of Procedure for the Construction Industry and <b>addressed</b> problems of <b>incomplete,</b> inaccurate and ambiguous information. Avanti guidance included: ...|$|R
40|$|Obtaining {{complete}} and accurate {{models for the}} formal verification of systems is often hard or impossible. We present a data-based verification approach, for properties expressed in a probabilistic logic, that <b>addresses</b> <b>incomplete</b> model knowledge. We obtain experimental data from a system that can be modelled as a parametric Markov chain. We propose a novel verification algorithm to quantify the confidence the underlying system satisfies a given property of interest by using this data. Given a parameterised model of the system, the procedure first generates a feasible set of parameters corresponding to model instances satisfying a given probabilistic property. Simultaneously, we use Bayesian inference to obtain a probability distribution over the model parameter set from data sampled from the underlying system. The results of both steps are combined to compute a confidence the underlying system satisfies the property. The amount data required is minimised by exploiting partial knowledge of the system. Our approach offers a framework to integrate Bayesian inference and formal verification, and in our experiments our new approach requires one order of magnitude less data than standard statistical model checking {{to achieve the same}} confidence...|$|R
40|$|Dictionaries {{of terms}} and phrases (e. g. common person or {{organization}} names) are integral to information extraction systems that extract structured information from unstructured text. Using noisy or unrefined dictionaries {{may lead to}} many incorrect results even when highly precise and sophisticated extraction rules are used. In general, {{the results of the}} system are dependent on dictionary entries in arbitrary complex ways, and removal of a set of entries can remove both correct and incorrect results. Further, any such refinement critically requires laborious manual labeling of the results. In this paper, we study the dictionary refinement problem and address the above challenges. Using provenance of the outputs in terms of the dictionary entries, we formalize an optimization problem of maximizing the quality of the system with respect to the refined dictionaries, study complexity of this problem, and give efficient algorithms. We also propose solutions to <b>address</b> <b>incomplete</b> labeling of the results where we estimate the missing labels assuming a statistical model. We conclude with a detailed experimental evaluation using several real-world extractors and competition datasets to validate our solutions. Beyond information extraction, our provenance-based techniques and solutions may find applications in view-maintenance in general relational settings...|$|E
40|$|Metabolic network reconstructions define {{metabolic}} information {{within a}} target organism and {{can therefore be}} used to <b>address</b> <b>incomplete</b> metabolic information. In the present study we used a computational approach to identify human metabolites whose metabolism is incomplete {{on the basis of}} their detection in humans but exclusion from the human metabolic network reconstruction RECON 1. Candidate solutions, composed of metabolic reactions capable of explaining the metabolism of these compounds, were then identified computationally from a global biochemical reaction database. Solutions were characterized with respect to how metabolites were incorporated into RECON 1 and their biological relevance. Through detailed case studies we show that biologically plausible non-intuitive hypotheses regarding the metabolism of these compounds can be proposed in a semi-automated manner, in an approach that is similar to de novo network reconstruction. We subsequently experimentally validated one of the proposed hypotheses and report that C 9 orf 103, previously identified as a candidate tumour suppressor gene, encodes a functional human gluconokinase. The results of the present study demonstrate how semi-automatic gap filling can be used to refine and extend metabolic reconstructions, thereby increasing their biological scope. Furthermore, we illustrate how incomplete human metabolic knowledge can be coupled with gene annotation in order to prioritize and confirm gene functions...|$|E
40|$|Efst á síðunni er hægt að nálgast greinina í heild sinni með því að smella á hlekkinnMetabolic network reconstructions define {{metabolic}} information {{within a}} target organism and {{can therefore be}} used to <b>address</b> <b>incomplete</b> metabolic information. In the present study we used a computational approach to identify human metabolites whose metabolism is incomplete {{on the basis of}} their detection in humans but exclusion from the human metabolic network reconstruction RECON 1. Candidate solutions, composed of metabolic reactions capable of explaining the metabolism of these compounds, were then identified computationally from a global biochemical reaction database. Solutions were characterized with respect to how metabolites were incorporated into RECON 1 and their biological relevance. Through detailed case studies we show that biologically plausible non-intuitive hypotheses regarding the metabolism of these compounds can be proposed in a semi-automated manner, in an approach that is similar to de novo network reconstruction. We subsequently experimentally validated one of the proposed hypotheses and report that C 9 orf 103, previously identified as a candidate tumour suppressor gene, encodes a functional human gluconokinase. The results of the present study demonstrate how semi-automatic gap filling can be used to refine and extend metabolic reconstructions, thereby increasing their biological scope. Furthermore, we illustrate how incomplete human metabolic knowledge can be coupled with gene annotation in order to prioritize and confirm gene functions...|$|E
30|$|The {{quality of}} the {{selected}} randomized controlled trials (RCTs) was established from the randomized clinical trial checklist of the Cochrane Center and CONSORT (Consolidated Standards of Reporting Trials) statement, which provided guidelines for the following parameters: (1) sequence generation, (2) allocation concealment method, (3) masking of the examiner, (4) <b>address</b> of <b>incomplete</b> outcome data, and (5) free of selective outcome reporting. The Newcastle-Ottawa scale (NOS) {{was used to assess}} the risk of bias of non-randomized studies. This was performed by two investigators (GMR and BE). Cohen ́s kappa coefficient was used to assess inter-rater agreement.|$|R
40|$|Most of {{statistical}} approaches in cardiovascular research {{were based on}} variance analysis (ANOVA). However, most of the time, the assumption that data are independent is violated since several measures are performed on the same subject (repeated measures). In addition, the presence of intra- and inter-observers variability can potentially obscure significant differences. The linear mixed model (LMM) is an extended multivariate linear regression method of analysis that accounts for both fixed and random effects. LMM allows for <b>addressing</b> <b>incomplete</b> design cases. In this paper, LMM was applied to two sets of cardiovascular research data and compared to ANOVA. The first example is an analysis of heart rate in mice after atropine and propranolol injections. LMM shows an important mouse random effects that depends on pharmacological treatment and provides with accurate estimates for each significant experimental factors. When randomly suppressing observations from the data sets (20 - 30 %) the time factor of Anova model becomes non significant while LMM still remains significant. The second example is the analysis of isolated coronary-perfused pressure of transgenic mice hearts. LMM evidenced a significant transgenic effect in {{both male and female}} animals, while, with ANOVA, the transgenic effects was limited to male mice only. In both cases, as compared to ANOVA, the LMM separately accounts for fixed and random effects, allowing thus for studying more adequately incomplete designs on repeated measures...|$|R
40|$|Target {{extraction}} is {{an important}} task in opinion mining. In this task, a complete target consists of an aspect and its corresponding object. However, previous work has always simply regarded the aspect as the target itself and has ignored the important "object" element. Thus, these studies have <b>addressed</b> <b>incomplete</b> targets, which are of limited use for practical applications. This paper proposes a novel and important sentiment analysis task, termed aspect-object alignment, to solve the "object neglect" problem. The objective of this task is to obtain the correct corresponding object for each aspect. We design a two-step framework for this task. We first provide an aspect-object alignment classifier that incorporates three sets of features, namely, the basic, relational, and special target features. However, the objects that are assigned to aspects in a sentence often contradict each other and possess many complicated features {{that are difficult to}} incorporate into a classifier. To resolve these conflicts, we impose two types of constraints in the second step: intra-sentence constraints and inter-sentence constraints. These constraints are encoded as linear formulations, and Integer Linear Programming (ILP) is used as an inference procedure to obtain a final global decision that is consistent with the constraints. Experiments on a corpus in the camera domain demonstrate that the three feature sets used in the aspect-object alignment classifier are effective in improving its performance. Moreover, the classifier with ILP inference performs better than the classifier without it, thereby illustrating that the two types of constraints that we impose are beneficial...|$|R
40|$|The {{decision-making}} {{assumption of}} all experts {{being able to}} express their preferences on all available alternatives of a decision-making problem might be considered unrealistic. This is specially true {{when the number of}} alternatives is considerable high and/or when sources of information are conflicting and dynamic. Thus, the presence of incomplete information, which is not equivalent to low quality information, is worth investigation and its processing within decision-making processes desirable. A consistency based approach to deal with incomplete fuzzy linguistic preferences is the focus of this contribution. Consistency is considered here as linked to the transitivity of preferences, and in particular to Tanino’s multiplicative transitivity property of reciprocal fuzzy preference relations. The first result presented is the formal modelling and representation of Tanino’s multiplicative transitivity property to the case of fuzzy linguistic preference relations. This is done via Zadeh’s extension principle and the representation theorem of fuzzy sets. The second result derives the multiplicative transitivity property of reciprocal intuitionistic fuzzy preference relations, which can be isomorphically mapped to a particular type of linguistic preference relation: reciprocal interval-valued fuzzy preference relations. The third result is the computation of the consistency based estimated reciprocal intuitionistic fuzzy preference values using an indirect chain of alternatives, which can be used to <b>address</b> <b>incomplete</b> information in decision-making problems with this type of preference relations...|$|E
40|$|The {{issue of}} missing or {{incomplete}} information arises in many National Environmental Policy Act (NEPA) processes. Council on Environmental Quality (CEQ) regulations provide some guidance on how to <b>address</b> <b>incomplete</b> information at 40 C. F. R. § 1502. 22, but this provision {{can be difficult to}} interpret and is frequently misapplied. Departmental and agency implementing regulations tend to provide little instructions regarding {{missing or incomplete}} information, and there is scant applicable case law. Federal agencies and NEPA practitioners are left with inadequate guidance on this difficult and often controversial issue. Uncertainty about how to address missing or incomplete information can weaken NEPA analyses, obfuscate important environmental issues and also undermine the legal defensibility of NEPA documents and the agency decisions they support. This paper proposes a systematic process for addressing incomplete information in Environmental Impact Statements developed pursuant to NEPA. This solution flows from careful interpretation of relevant provisions of NEPA and CEQ regulations, in particular CEQ regulations at 40 C. F. R. § 1502. 22. The product of this effort is a sequential process that is simple enough to illustrate in the form a flow chart, yet expansive enough to contemplate the full spectrum of missing or incomplete information that may be encountered in an EIS process. This approach was recently utilized in a high-profile EIS and received strong praise from the U. S. Environmental Protection Agency (EPA) ...|$|E
40|$|Comprehensive {{two-dimensional}} chromatography is {{a powerful}} technology for analyzing the patterns of constituent compounds in complex samples, but matching chromatographic features for comparative analysis across large sample sets is difficult. Various methods have been described for pairwise peak matching between two chromatograms, but the peaks indicated by these pairwise matches commonly are incomplete or inconsistent across many chromatograms. This paper describes a new, automated method for postprocessing the results of pairwise peak matching to <b>address</b> <b>incomplete</b> and inconsistent peak matches and thereby select chromatographic peaks that reliably correspond across many chromatograms. Reliably corresponding peaks can be used both for directly comparing relative compositions across large numbers of samples and for aligning chromatographic data for comprehensive comparative analyses. To select reliable features {{for a set of}} chromatograms, the Consistent Cliques Method (CCM) represents all peaks from all chromatograms and all pairwise peak matches in a graph, finds the maximal cliques, and then combines cliques with shared peaks to extract reliable features. The parameters of CCM are the minimum number of chromatograms with complete pairwise peak matches and the desired number of reliable peaks. A particular threshold for the minimum number of chromatograms with complete pairwise matches ensures that there are no conflicts among the pairwise matches for reliable peaks. Experimental results with samples of complex bio-oils analyzed by comprehensive two-dimensional gas chromatography (GCxGC) coupled with mass spectrometry (GCxGC−MS) indicate that CCM provides a good foundation for comparative analysis of complex chemical mixtures...|$|E
40|$|International audienceLaboratory {{studies of}} {{atmospheric}} chemistry characterize {{the nature of}} atmospherically relevant processes down to the molecular level, providing fundamental information used to assess how human activities drive environmental phenomena such as climate change, urban air pollution, ecosystem health, indoor air quality, and stratospheric ozone depletion. Laboratory studies have {{a central role in}} <b>addressing</b> the <b>incomplete</b> fundamental knowledge of atmospheric chemistry. This article highlights the evolving science needs for this community and emphasizes how our knowledge is far from complete, hindering our ability to predict the future state of our atmosphere and to respond to emerging global environmental change issues. Laboratory studies provide rich opportunities to expand our understanding of the atmosphere via collaborative research with the modeling and field measurement communities, and with neighboring disciplines...|$|R
40|$|We {{study the}} scalar-tensor theory of gravity profoundly {{in the action}} level {{as well as in}} the {{thermodynamic}} level. Contrary to the usual description in the literature about the equivalence in the two conformally connected frames, this paper <b>addresses</b> several <b>incomplete</b> inferences regarding it and mentions some inequivalences which were not pointed out earlier. In the thermodynamic level, our analysis shows the two frames are equivalent. In that process, we identify the entropy, the energy and the temperature for the thermodynamic description, and we find these quantities are conformally invariant even without any prior assumption. The same conclusion is reached from the gravitational action as well as from the Gibbons-Hawking-York boundary term, establishing the result in a more convincing manner. Comment: Typos corrected, published in Phys. Rev. ...|$|R
50|$|When {{running for}} reelection in 2014 the {{signature}} forms to get Jen Flanagan on the primary ballot were filed with an <b>incomplete</b> <b>address,</b> so she got on the general election ballot with a write-in campaign during the primary. There was no candidate for the seat on either the Republican or Democratic primary ballots. Her general election opponent, Richard Bastien, also got on the general election ballot with a write-in campaign in the primary.|$|R
40|$|Realistically {{modelling}} uncertainties in {{structural analysis}} {{is vital for}} structural safety assessment. Stochastic modelling is a general approach for quantifying uncertainties when probability density function can be justified by sufficient statistical information. In engineering practice, however, available information is frequently imprecise, ambiguous or inadequate. Selection of probability density function for these uncertainties in stochastic modelling can lead to subjective results and even significant errors in structural reliability assessment. Consequently, how to tackle incomplete information about uncertain parameters is crucial for structural safety. This dissertation aims to propose a new methodology to <b>address</b> <b>incomplete</b> information about uncertain parameters for structural analysis by using a hybrid stochastic and interval approach. Uncertain parameters with sufficient statistical information are modelled as random variables, while other parameters with incomplete information are represented by interval variables. An efficient propagation strategy is formulated by employing perturbation method and moment method. Lower and upper bounds of mean value and variance of system outputs are then determined by interval arithmetic or simulation methods. As a result, possible ranges of probability density function and cumulative distribution function of system outputs can be achieved. Beyond this application, this methodology is expandable to hybrid stochastic and fuzzy approach through &# 945;-level cut, and also capable of solving pure stochastic, interval and fuzzy problems. Static response, dynamic characteristics, dynamic response and reliability analyses of structures with mixed random and interval parameters, or mixed random and fuzzy parameters are investigated by applying the developed methodology, and results are verified by Monte Carlo Simulation method. The capability of solving pure stochastic, interval and fuzzy problems are also demonstrated by comparison with peer research outcomes. The methodology developed in this dissertation provides a new tool to address uncertainties, particularly mixed uncertainties in predication of structural behaviour and assessment of structural safety...|$|E
40|$|Ankara : The Department of Electrical and Electronics Engineering and the Institute of Engineering and Sciences of Bilkent University, 2010. Thesis (Ph. D.) [...] Bilkent University, 2010. Includes bibliographical {{references}} leaves 171 - 187. A popular {{method to}} study electromagnetic scattering and radiation of threedimensional electromagnetics problems is to solve discretized surface integral equations, which {{give rise to}} dense linear systems. Iterative solution of such linear systems using Krylov subspace iterative methods and the multilevel fast multipole algorithm (MLFMA) {{has been a very}} attractive approach for large problems because of the reduced complexity of the solution. This scheme works well, however, only if the number of iterations required for convergence of the iterative solver is not too high. Unfortunately, {{this is not the case}} for many practical problems. In particular, discretizations of open-surface problems and complex real-life targets yield ill-conditioned linear systems. The iterative solutions of such problems are not tractable without preconditioners, which can be roughly defined as easily invertible approximations of the system matrices. In this dissertation, we present our efforts to design effective preconditioners for large-scale surface-integral-equation problems. We first <b>address</b> <b>incomplete</b> LU (ILU) preconditioning, which is the most commonly used and well-established preconditioning method. We show how to use these preconditioners in a blackbox form and safe manner. Despite their important advantages, ILU preconditioners are inherently sequential. Hence, for parallel solutions, a sparseapproximate-inverse (SAI) preconditioner has been developed. We propose a novel load-balancing scheme for SAI, which is crucial for parallel scalability. Then, we improve the performance of the SAI preconditioner by using it for the iterative solution of the near-field matrix system, which is used to precondition the dense linear system in an inner-outer solution scheme. The last preconditioner we develop for perfectly-electric-conductor (PEC) problems uses the same inner-outer solution scheme, but employs an approximate version of MLFMA for inner solutions. In this way, we succeed to solve many complex real-life problems including helicopters and metamaterial structures with moderate iteration counts and short solution times. Finally, we consider preconditioning of linear systems obtained from the discretization of dielectric problems. Unlike the PEC case, those linear systems are in a partitioned structure. We exploit the partitioned structure for preconditioning by employing Schur complement reduction. In this way, we develop effective preconditioners, which render the solution of difficult real-life problems solvable, such as dielectric photonic crystals. Malas, TahirPh. D...|$|E
40|$|Computer {{games in}} general and Real-Time Strategy (RTS) games in {{particular}} provide a rich challenge for both human- and computer controlled players, often denoted as bots. The player or bot controls {{a large number of}} units that have to navigate in partially unknown dynamic worlds to pursue a goal. Navigation in such worlds can be complex and require much computational resources. Typically it is solved by using some sort of path planning algorithm, and a lot of research has been conducted to improve the performance of such algorithms in dynamic worlds. The main goal of this thesis is to investigate an alternative approach for RTS bots based on Artificial Potential Fields, an area originating from robotics. In robotics the technique has successfully been used for navigation in dynamic environments, and we show {{that it is possible to}} use Artificial Potential Fields for navigation in an RTS game setting without any need of path planning. In the first three papers we define and demonstrate a methodology for creating multi-agent potential field based bots for an RTS game scenario where two tank armies battle each other. The fourth paper <b>addresses</b> <b>incomplete</b> information about the game world, referred to as the fog of war, and show how Potential Field based bots can handle such environments. The final paper shows how a Potential Field based bot can be evolved to handle a more complex full RTS scenario. It addresses resource gathering, construction of bases, technological development and construction of an army consisting of different types of units. We show that Artificial Potential Fields is a viable option for several RTS game scenarios and that the performance, both in terms of being able to win a game and computational resources used, can match and even surpass those of traditional approaches based on path planning...|$|R
50|$|Dead letter mail or {{undeliverable}} mail is mail that cannot {{be delivered to}} the addressee or returned to the sender. This is usually {{due to lack of}} compliance with postal regulations, an <b>incomplete</b> <b>address</b> and return address, or the inability to forward the mail when both correspondents move before the letter can be delivered. Largely based on the British model that emerged in the late eighteenth century, many countries developed similar systems for processing {{undeliverable mail}}.|$|R
40|$|War and the Crisis of Youth in Sierra Leone <b>addresses</b> the {{currently}} <b>incomplete</b> {{understanding of the}} conflict in Sierra Leone {{by focusing on the}} direct experiences and interpretations of protagonists. The data presented challenges the widely canvassed notion of this conflict as a war motivated by "greed, not grievance," pointing instead to a rural crisis expressed in terms of unresolved tensions between landowners and marginalized rural youth, further reinforced and triggered by a collapsing patrimonial state...|$|R
40|$|Web Service (WS) domains {{constitute}} an application field where automated planning can significantly contribute towards achieving customisable and adaptable compositions. Following {{the vision of}} using domain-independent planning and declarative complex goals to generate compositions based on atomic service descriptions, we apply a planning framework based on Constraint Satisfaction techniques to a domain consisting of WSs with diverse functionalities. One of the key requirements of such domains {{is the ability to}} <b>address</b> the <b>incomplete</b> knowledge problem, as well as recovering from failures that may occur during execution. We propose an algorithm for interleaving planning, monitoring and execution, where continual planning via altering the CSP is performed, under the light of the feedback acquired at runtime. The system is evaluated against a number of scenarios including real WSs, demonstrating the leverage of situations that can be effectively tackled with respect to previous approaches. ...|$|R
40|$|In {{this paper}} we {{describe}} an iterative development process {{to deal with}} speculative constraint optimization projects. Speculative projects are ill-defined in nature, as they are often new to the customer organization who wishes to anticipate market changes, but also because their main complexity lies in completing the problem definition. We consider one speculative application tackled in the Chic- 2 Esprit project by different partners: Risk management {{in the area of}} energy trading 1. The ill-defined and incomplete components of the problem are: 1) model adequacy, 2) multi-criteria objective, and 3) forecast data. We show how the constraint technology can help refining the problem definition by building iterative models and methods to <b>address</b> the <b>incomplete</b> parts incrementally. Each partner applied his own technology using constraint programming, mathematical programming and local search techniques. Finally we discuss the new potentials for constraint technology to deal with stochast [...] ...|$|R
40|$|The use of anthropological {{qualitative}} {{methods to}} validate and improve health surveillance data is demonstrated through {{an examination of}} the process of birth registration in Gaza. Theoretically, the importance of understanding the link between historical events and microlevel decision-making is emphasized both in general terms and specifically {{in the context of the}} Gaza Strip today. In the course of interviewing a sample of mother/infant pairs selected from a register of births in the Gaza Strip it became evident that 100 % of the <b>addresses</b> were <b>incomplete.</b> Using qualitative methods in the form of field visits and interviews with physicians, clerks and nurses, an understanding of the information pathway for birth registration data was developed. It was also established that there was some erroneous recording of birthweight. An intervention was designed which failed to improve the accuracy of addresses but did improve the recording of birthweight. Vital registration Qualitative Palestinians Intervention Validity...|$|R
40|$|In {{this work}} {{disassembler}} for monolithic microproces or (micro-controllers) Microchip PIC was created. For typical programs this disasambler statically determines values of runtime address registers, thus complting the address from partial address in instruction. On its basis the disassembler recognizes procedures, creates procedure call-graph and recognizes control structures. Described disassembler separates usage of a register us do for variables of multiple procedures and sorts them to procedures inputs, locally modified variables and returned variables. Contemporary disassemblers for this architecture restrict themselves to printing instructions with <b>incomplete</b> <b>addresses,</b> {{because they do}} not perform any analysis. Powered by TCPDF (www. tcpdf. org...|$|R
5000|$|When he {{returned}} to civilian society, he opened a free clinic and began to practice exclusively using his method. With the support of leading figures (members of the Japanese imperial family, consuls, and other relationships acquired thanks to his high military rank) he quickly became successful. The demands increased so much that consultations had {{to be limited to}} 100 per day; his popularity became such that he got mail with <b>incomplete</b> <b>addresses</b> such as [...] "For the Anti-Doctor Doctor, Tokyo," [...] "Doctor Vegetables, Tokyo" [...] or [...] "Doctor Daikon, Tokyo" [...] (as he often prescribed daikon).|$|R
40|$|Abstract- Many {{data mining}} methods are {{dependent}} on recognizing frequent patterns. Frequent pat-terns lead {{to the discovery of}} association rules, strong rules, sequential episodes, and multi-dimen-sional patterns. All can {{play a critical role in}} helping corporate and scientific institutions to under-stand and analyze their data. Patterns should be discovered in time and space efficient manner. Dis-covered patterns have authentic value when they accurately describe data trends; and, do not exclu-sively reflect noise or chance encounters. Vertical data mining algorithms key advantage is that they can outperform their horizontal counterparts in terms of both time and space efficiency. Little work has <b>addressed</b> how <b>incomplete</b> data influences vertical data mining. Consequently, the quality and utility of vertical mining algorithms results remains ambiguous as real data sets often contain incom-plete data. This paper considers how to establish methodologies that deal with incomplete data in vertical mining; additionally, it seeks to develop strategies for determining the maximal utilization that can be mined from a dataset based on how much and what data is missing...|$|R
40|$|We have {{composed}} {{a sample of}} 68 massive stars in our galaxy whose projected rotational velocity, effective temperature and gravity are available from high-precision spectroscopic measurements. The additional seven observed variables considered here are their surface nitrogen abundance, rotational frequency, magnetic field strength, and the amplitude and frequency of their dominant acoustic and gravity mode of oscillation. Multiple linear regression to estimate the nitrogen abundance combined with principal components analysis, after <b>addressing</b> the <b>incomplete</b> and truncated nature of the data, reveals that the effective temperature {{and the frequency of}} the dominant acoustic oscillation mode are the only two significant predictors for the nitrogen abundance, while the projected rotational velocity and the rotational frequency have no predictive power. The dominant gravity mode and the magnetic field strength are correlated with the effective temperature but have no predictive power for the nitrogen abundance. Our findings are completely based on observations and their proper statistical treatment and call for a new strategy in evaluating the outcome of stellar evolution computations. Comment: Paper contains 3 figures and 3 tables. Accepted for publication in The Astrophysical Journa...|$|R
