0|31|Public
50|$|There {{are several}} types of disk imaging {{software}} available that use single instancing technology to reduce the time, bandwidth, and storage required to capture and <b>archive</b> <b>disk</b> images. This {{makes it possible to}} rebuild and transfer information-rich disk images at lightning speeds, which is a significant improvement over the days when programmers spent hours configuring each machine within an organization.|$|R
50|$|Holographic drives store {{information}} in a manner different from normal DVD drives. While DVDs hold information on the surface or just below the surface, holographic drives pack data in a gel, using a pair of laser beams that cross through {{the entirety of the}} gel. A single pulse can etch one million bits, which is four times the write speed of Blu-ray and HD DVD drives. The gel is durable enough to last for 50 years. Professional <b>archiving</b> <b>disks</b> created by Maxwell can hold 300 gigabytes of memory.|$|R
5000|$|<b>Disk</b> <b>Archive</b> - {{portable}} robust {{program for}} archiving and backup ...|$|R
5000|$|... dar (<b>disk</b> <b>archive)</b> is a {{computer}} program, a command-line archiving tool intended {{as a replacement for}} tar in Unix-like operating systems.|$|R
5000|$|A {{well known}} problem {{with one of}} the telescope's derotators casing error {{messages}} and shifted values was eventually dealt with in February 2009. The ejection function of the instrument computer's DVD burner unit for the CD archive stopped working in May. A rubber band was replaced. A web based image gallery was added to the telescope web pages in Mars. A second image gallery was installed on the side doors in the dome as a support to guided visitor groups at the telescope and a replacement for a temporary gallery. A relatively new HDD <b>archive</b> <b>disk</b> failed in Mars and most of the archive had to be reconstructed from CD. Although the disk was mirrored the hardware and computer software did not give a warning until the second disk started to degrade.|$|R
50|$|The {{complete}} soundtrack to the International Sound Version (which {{includes all}} of the original songs) survives at the UCLA Film and Television <b>Archive</b> on Vitaphone <b>disks.</b>|$|R
50|$|The XAD {{system is}} an {{open-source}} client-based unarchiving {{system for the}} Amiga. This means there is a master library called xadmaster.library which provides an interface between the client and the user application and there are clients handling the special archive formats. Three different types to handle file and <b>disk</b> <b>archives</b> and also <b>disk</b> image files (filesystem) are possible. They {{can be made by}} anyone. The master library itself includes some of these clients internally to make the work somewhat easier for the package maintainer and the user installing it.|$|R
40|$|Abstract—Data {{movement}} within {{high performance}} environ-ments {{can be a}} large bottleneck to the overall performance of programs. With the addition of continuous storage and usage of older data, the back end storage is becoming a larger problem than the improving network and computational nodes. This has led us to develop a Distributed Shared Disk Cache, DiSK, to reduce the dependence on these back end storage systems. With DiSK requested files will be distributed across nodes {{in order to reduce}} the amount of requests directed to the <b>archives.</b> <b>DiSK</b> has two key components. One is a Distributed Metadata Management, DIMM, scheme that allows a centralized manager to access what data is available in the system. This is accomplished through the use of a counter-based bloomfilter with locality checks in order to reduce false positives and false negatives. The second component is a method of replication called Differentiable Replication, DiR. The novelty of DiR is that the requirements of the files and capabilities of underlying nodes are taken into consideration for replication. This allows for a varying degree of replication depending on the file. This customization of DiSK yields better performance than the conventional archive system. I...|$|R
5000|$|The June 2013 {{release of}} the 5th {{generation}} models features a name change to AirPort Time Capsule, and a redesign with measurements 3.85 in square, and 6.6 in high. The square dimensions echo the size of both the latest AirPort Express and Apple TVs (2nd generation onwards), just with the height being significantly higher. The 2013 models feature the same [...] ports on the back as previous generations, and come in the same capacities as the 4th generation of 2 TB & 3 TB, but have introduced the newest Wi-Fi standard 802.11ac. The AirPort Extreme released {{at the same time}} is exactly the same in dimensions and I/O ports, just without the internal harddrive of the AirPort Time Capsule. 2013 models feature faster download speed, beam-forming improvements and wireless or desktop network control with iCloud integration. Airport is compatible with devices using the 802.11a, 802.11b, 802.11g, 802.11n and 802.11ac specifications. Also improved, Airport Utility has added one click Time Capsule format from the utility's Airport Time Capsule, Edit, Disks menu, allowing easy and rapid Erase <b>Disk</b> and <b>Archive</b> <b>Disk</b> to start over or configure Network. Disk Erase includes up to 35 passes and device includes encrypted storage plus optional WAN sharing, making Airport extremely secure and flexible for home, class and office environments. Airport Utility is a free download.|$|R
40|$|International Telemetering Conference Proceedings / October 29 -November 02, 1990 / Riviera Hotel and Convention Center, Las Vegas, NevadaThis paper {{discusses}} {{the benefits of}} using UNIX in a telemetry and satellite control product and some specific features implemented in UNIX-based workstations and file servers. Features discussed include real-time <b>disk</b> <b>archiving</b> and playback using UNIX and single-point-of-failure issues...|$|R
50|$|Amplidata {{technology}} aims {{to overcome}} the limitations of RAID by the increasing length of time required to rebuild larger capacity drives. Rebuild times for 2TB drives are already known to take four hours or longer to complete, and in some cases—depending on how busy the storage system is—it can take days for a rebuild. There is also {{the need to keep}} all disks in a RAID group spinning so no power savings can be realized. Spin down is likely to become more important {{in the years to come}} as more data is <b>archived</b> to <b>disk,</b> with it likely becoming a function of the storage array to intelligently manage and place the archived data on these drives—as opposed to the software—to facilitate the spin down of drives.|$|R
50|$|The {{ability to}} suspend a {{complete}} state of a multi-machine configuration {{and make a}} snapshot (a copy {{at a point in}} time) is a major benefit of virtualization. This is especially useful for application development teams because when a bug is found a configuration snapshot to be taken at the point of failure and a link to the configuration added to a defect report. Instead of a developer spending hours to reproduce the defect, he or she can restore the configuration and start debugging the issue within minutes.The ability to control the life cycle automatically is beneficial for web based service. For example, an environment can run for two hours and than automatically be <b>archived</b> to <b>disk.</b> This reduces the costs of running the environments and allows for multiple business scenarios.|$|R
40|$|The {{measurements}} of MIPAS/ENVISAT, {{in its original}} nominal observation mode, are analyzed with the GMTR retrieval system {{in order to obtain}} 2 -D fields of pressure, temperature and Volume Mixing Ratio of H 2 O, O 3, HNO 3, CH 4, N 2 O and NO 2. The MIPAS Level- 1 b data are <b>archived</b> in the <b>disks</b> cluster of the MIPAS Bologna Facility. Samples of the obtained results are presented and compared with the corresponding ESA Level- 2 MIPAS products. RATIONALE The GMTR (Geo-fit Multi-Target Retrieval) reduction tool for MIPAS measurements has been delivered to ESA and is now an open source code part of the BEAT tools [1...|$|R
5000|$|The unit is {{developed}} by Gideon Zweijtzer {{and is a}} cartridge that can emulate other cartridges such as the Commodore REU, Action Replay, The Final Cartridge III, Super Snapshot, Retro Replay or TurboAss with Codenet-support, and an almost fully compatible (including JiffyDOS support FPGA-cloned Commodore 1541 (including 1541, 1541C, and 1541 II models) floppy disk unit that can use Commodore 64-compatible files like [...]D64/.G64 disc images or [...]PRG files via a SD card reader. Additionally, the 1541 Ultimate is suitable for making <b>archives</b> of floppy <b>disks.</b> All units after the initial production have 32 megabytes of RAM, while the original production run only had 16 megabytes.|$|R
40|$|Recent {{modifications}} to the computer network at DIII-D enhance the collection and distribution of newly acquired and archived experimental data. Linked clients and servers route new data from diagnostic computers to centralized mass storage and distribute data on demand to local and remote workstations and computers. Capacity for data handling exceeds the upper limit of DIII-D Tokamak data production of about 4 GBytes per day. Network users have fast access to new data stored on line. An interactive program handles requests for restoration of data <b>archived</b> off line. <b>Disk</b> management procedures retain selected data on line in preference to other data. Redundancy of all components on the archiving path from the network to magnetic media has prevented loss of data. Older data are rearchived as dictated by limited media life...|$|R
5000|$|In 1993, PKWARE {{released}} PKZIP 2.0. This {{new version}} dispensed with the miscellaneous compression methods of PKZIP 1.x and {{replaced them with}} a single new compression method which Katz called [...] "deflating" [...] (although several compression levels of deflating were provided by the program). The resulting file format has since become ubiquitous on Microsoft Windows and on the Internet almost all files with the [...]ZIP (or [...]zip) extension are in PKZIP 2.x format, and utilities to read and write these files are available on all common platforms. PKZIP 2.x also supported spanning <b>archives</b> to multiple <b>disk,</b> which simply split the files into multiple pieces, and using volume label on each drive to differentiate each other. A new Authenticity Verification (AV) signature format was used. Registered version included PKUNZJR, PK Safe ANSI, PKCFG utilities.|$|R
40|$|EOS is an {{open source}} {{distributed}} disk storage system in production since 2011 at CERN. Development {{focus has been on}} low-latency analysis use cases for LHC(1) and non- LHC experiments and life-cycle management using JBOD(2) hardware for multi PB storage installations. The EOS design implies a split of hot and cold storage and introduced a change of the traditional HSM(3) functionality based workflows at CERN. The 2015 deployment brings storage at CERN to a new scale and foresees to breach 100 PB of disk storage in a distributed environment using tens of thousands of (heterogeneous) hard drives. EOS has brought to CERN major improvements compared to past storage solutions by allowing quick changes in the quality of service of the storage pools. This allows the data centre to quickly meet the changing performance and reliability requirements of the LHC experiments with minimal data movements and dynamic reconfiguration. For example, the software stack has met the specific needs of the dual computing centre set-up required by CERN and allowed the fast design of new workflows accommodating the separation of long-term tape <b>archive</b> and <b>disk</b> storage required for the LHC Run II. This paper will give a high-level state of the art overview of EOS with respect to Run II, introduce new tools and use cases and set the roadmap for the next storage solutions to come...|$|R
40|$|For three decades, Kryder’s Law correctly predicted an {{exponential}} {{increase in}} bit density on disk platters, {{leading to an}} exponential drop in cost per gigabyte.  However, disk now is over 7 times as expensive as it would have been had Kryder’s law continued unchanged from  2010, and industry projections suggest that in 2020 the gap will reach 200 times.  Entrenched expectations {{of the cost of}} storing data for the long-term are being disrupted because of slow storage density growth. We use an economic model of long-term storage to investigate the implications of this disruption, including a comparison of archives based upon traditional disk media with alternative media such as flash. Our model shows that archives based upon alternative media are surprisingly cost competitive with <b>archives</b> based upon traditional <b>disk</b> media over the long-term. We propose using Archival Flash for long-term data preservation, with the trade off between longer data retention period and lower write cycles...|$|R
40|$|A {{location}} and positioning system {{was developed and}} implemented in the anechoic chamber of the Structural Acoustics Loads and Transmission (SALT) facility to accurately determine the coordinates of points in three-dimensional space. Transfer functions were measured between a shaker source at two different panel locations and the vibrational response distributed over the panel surface using a scanning laser vibrometer. The binaural simulation test matrix included test runs for several locations of the measuring microphones, various attitudes of the mannequin, two locations of the shaker excitation and three different shaker inputs including pulse, broadband random, and pseudo-random. Transfer functions, auto spectra, and coherence functions were acquired for the pseudo-random excitation. Time histories were acquired for the pulse and broadband random input to the shaker. The tests were repeated with a reflective surface installed. Binary data files were converted to universal format and <b>archived</b> on compact <b>disk...</b>|$|R
40|$|Increasingly {{sophisticated}} National Aeronautics and Space Administration (NASA) Earth science missions {{have driven}} their associated data and data management systems from providing simple point-to-point archiving and retrieval to performing user-responsive distributed multisensor information extraction. To fully maximize {{the use of}} remote-sensor-generated Earth science data, NASA recognized the need for data systems that provide data access and manipulation capabilities responsive to research brought forth by advancing scientific analysis {{and the need to}} maximize the use and usability of the data. The decision by NASA to purposely evolve the Earth Observing System Data and Information System (EOSDIS) at the Goddard Space Flight Center (GSFC) Earth Sciences (GES) Data and Information Services Center (DISC) and other information management facilities was timely and appropriate. The GES DISC evolution was focused on replacing the EOSDIS Core System (ECS) by reusing the In-house developed disk-based Simple, Scalable, Script-based Science Product Archive (S 4 PA) data management system and migrating data to the <b>disk</b> <b>archives.</b> Transition was completed in December 200...|$|R
40|$|Land-observing {{satellites}} {{with multiple}} thematic mappers will produce data at rates of 100 to 300 Mbps. When {{coupled with a}} high daily scene production rate, these rates will require new approaches to ground processing. Consideration is given here to future downlink rates and data volumes, and requirements peculiar to the future user community are discussed. The advanced technologies required to attain an operational system in the years 1985 - 1990 are considered, together with advances foreseen in communications, mass storage, bulk memories, and data processing. Using advanced devices, a centralized data processing system capable of handling the 100 Mbps data rate is described. New approaches, among them a parallel pipelined calibration front-end, real-time browse image production, a high bandwidth optical <b>disk</b> <b>archive,</b> regional image broadcast and massively parallel product production, are considered. A distributed system capable of handling the 300 Mbps data rate is then described. Designs for a hub system and a regional processing center are presented...|$|R
40|$|We {{describe}} {{the architecture of}} the data archiving and distribution of the Virgo antenna for gravitational wave detection. The main characteristic of this system is the modularity of the architecture. This solution allows system upgrades without dramatic changes of the hardware and software components. The main performances are: 1. Maximum sustained data flow of 10 Mbyte/s on DLT tapes (35 / 70 Gbyte) for the raw data archiving; 2. Up to 1 Tbyte data <b>archiving</b> capacity on <b>disk</b> at a maximum sustained data flow of 25 Mbyte/s for the online data distribution; 3. Up to 10 Mbyte/s data retrieval flow for the on-line data distribution. The basic architecture of the system consists of two sections: an acquisition and storage section and a data management section. The former is a LynxOS based system with the disks directly connected to the CPU slave boards, the latter is a DEC-Unix Alpha Server (Data Server), NFS mounting the LynxOS disks through a Fast Ethernet networ...|$|R
40|$|International Telemetering Conference Proceedings / October 13 - 15, 1981 / Bahia Hotel, San Diego, CaliforniaThis paper {{describes}} the telemetry processing {{capability of the}} Data System Modernization (DSM) system being developed at the Air Force Satellite Control Facility (AFSCF) for real-time or off-line processing, storage, and display of satellite telemetry data. The system accepts multiple satellite streams from the AFSCF Remote Tracking Stations (RTS) through a wide-band communications segment to the communication interface support equipment (IBM Series 1 computers) for processing analysis and display in the Mission Control Complexes (MCC). Dual IBM 370 series computers process the telemetry data for real-time command and control, while <b>archiving</b> on the <b>disk</b> and tape for future mission planning and post flight analysis. Real-time displays {{are located in the}} MCC’s or program dedicated work areas. The network is configured for support by RCC where generic checkout of the entire data distribution and telemetry system is performed prior to release for mission support to an MCC. The system is designed for the large diverse satellite population of the AFSCF with growth for future requirements...|$|R
50|$|In 1985, Jon Walker from Marshfield, WI {{converted}} Eamon to the PC {{and released}} several converted and new adventures. Eventually the disks were released through various shareware organizations and garnered a minor following. While the adventures attracted some modest interest, the direct {{conversion of the}} adventure creation program prompted much criticism among PC users for being too difficult to use and poorly written (the latter in later years by people who were probably never constrained by early versions of the BASIC language). By the time a serious {{attempt was made to}} upgrade the Eamon system to Foxbase, other adventures containing graphics and real time action were becoming popular and the project was scrapped. In 1997 an attempt was made to bring Eamon to the World Wide Web; however, the project never got beyond a beta version because of the rapid pace at which the web and content distribution was developing. The original PC GW-BASIC source code is almost impossible to find online anymore, and the author has {{only a few of the}} original <b>disks</b> <b>archived</b> on CD.|$|R
40|$|Developed for the NASA Johnson Space Center and Life Sciences Directorate by GE Government Services, the Microcomputer Integrated Real-time Acquisition Ground Equipment (MIRAGE) {{system is}} a {{portable}} ground support system for Spacelab life sciences experiments. The MIRAGE system can acquire digital or analog data. Digital data may be NRZ-formatted telemetry packets of packets from a network interface. Analog signal are digitized and stored in experimental packet format. Data packets from any acquisition source are <b>archived</b> to a <b>disk</b> as they are received. Meta-parameters are generated from the data packet parameters by applying mathematical and logical operators. Parameters are displayed in text and graphical form or output to analog devices. Experiment data packets may be retransmitted through the network interface. Data stream definition, experiment parameter format, parameter displays, and other variables are configured using spreadsheet database. A database can be developed to support virtually any data packet format. The user interface provides menu- and icon-driven program control. The MIRAGE system can be integrated with other workstations to perform a variety of functions. The generic capabilities, adaptability {{and ease of use}} make the MIRAGE a cost-effective solution to many experimental data processing requirements...|$|R
40|$|There is an {{increasing}} use {{in the business community}} of multifunction devices that provide network printing, scanning and faxing. These devices are frequently being deployed within business with little thought of the security implications of devices that bridge the network and phone line, potentially offering a backdoor to both the network and confidential information via “cross channel ” communications. This paper examines the security of the Ricoh Aficio 450 E Multifunction device (hereafter known as Ricoh 450 E) that provides the following functions: Key • Manual fingerprint and = optional AF 19 FA 27 network 2 F 94 998 D faxingFDB 5 DE 3 D F 8 B 5 06 E 4 A 169 4 E 46 • Network printing • Manual and network scanning • The ability to store faxes, scans & print jobs to memory • The ability to <b>archive</b> to hard <b>disk</b> • The ability to store and forward received faxes A security assessment of the Ricoh 450 E reveals many of the security issues encountered with workstations and servers. These issues can endanger the confidentiality, integrity and availability (CIA) of data passing through the device. These issues require implementation of countermeasures appropriate for the business environment...|$|R
40|$|Although Lidar (light {{detection}} and ranging) technology has {{a variety of}} applications, such as nautical charting, coastal zone monitoring, habitat mapping, pollution modelling, its principle function is the same and is based on light theory. Especially concerning the airborne Lidar bathymetry [ALB], and despite its high perspectives, the whole project faces limitations. The key to its effectiveness is considered to be water clarity, which can be determined satisfactorily with a secchi disk [SD]. The aim of this project is to determine whether ALB could be beneficially applicable in the Aegean Sea or not, regarding water clarity. To achieve this goal, the general trend of it was examined by sampling three different coastal areas with a SD. The Hellenic Navy Hydrographic Service [HNHS] provided an <b>archive</b> of secchi <b>disk</b> depths [SDDs] (1978 to 1982) for these areas. Two of them are located {{in the northern part of}} the Aegean Sea and the third one in the central west part. At the same time, a few more measurements referring to the south east part of the Aegean Sea were obtained through the SD application of Plymouth University. The data were displayed through a Geographic Information System [GIS] (ArcMap 10. 1) and analysed statistically. The results showed that ALB surveys would be effective and applicable in the Aegean Sea and that any project should be carried out during the summer when water transparencies are increased. Furthermore, studying the area’s particularities showed that even if the Black Sea affects the water clarity of the north-eastern Aegean, transparencies are good enough to allow the implementation of effective ALB surveys. However, they should not be conducted too close to rivers’ mouths where mixing waters result in density discontinuities and therefore, decreased water transparencies. In collaboration with the Hellenic Navy Hydrographic Servic...|$|R
40|$|Although the {{fundamental}} principles of spin stabilization are well established, uncertainty regarding {{the potential for}} rapid nutation growth caused by onboard liquids is a continuing concern. NASA and other organizations regularly encounter the issue of rapid nutation growth due to energy dissipation by liquids on spinning vehicles. Of concern is the stability of spinning upper stages and of spacecraft that spin for part or all of their missions. Several missions have required last-minute hardware or operational changes to deal with rapid nutation divergences that were identified late in the program. In some instances, major schedule slips were barely averted. In at least two cases, {{it was determined that}} a spinning upper stage was not a viable option. Historically, the "slosh" issue has been addressed by each space vehicle project individually, if it has been addressed at all. Due to budgetary and programmatic constraints, individual projects are unable to address the problem globally. Hence, there has been little effort to collect available test and flight data and use that data to make a coherent, unified picture of the "slosh" effect and how to deal with it. To some extent, each project has had to "reinvent the wheel", which can be both costly and risky. This study is a step toward correcting the situation. Specifically, the goal was to identify and collect available flight and test data for spinning vehicles with onboard liquid propellants. A total of 149 flight data points and 1, 692 test points were collected as part of this study. This data was analyzed, correlated, and is presented here in a normalized form. In most cases, the normalization involves a dimensionless nutation time constant {{that can be used to}} predict performance of other vehicles with the same type of tank. For some configurations, it was also possible to identify conditions that can lead to resonance between nutational motion and liquid modes. Gaps in the knowledge base are identified and approaches to filling those gaps are outlined. The data presented here has two different but related uses. First, it can be applied directly to current and future spacecraft programs. Second, it can provide truth models for testing analytical techniques. Experience has shown that purely analytical models of the liquid "slosh" effect on spinning vehicles are unreliable unless they are validated against flight or test data. To the author's knowledge, this report contains the most extensive and varied data set available. As such, it should be a good resource for anyone seeking to develop and validate improved analytical techniques. All of the original digital data sets have been <b>archived</b> on <b>disk,</b> with copies provided to NASA/KSC. With some restrictions, many of these data sets can be made available to researchers within the United States. Whenever possible, spacecraft are identified by name in this report. However, several organizations provided access to data with the explicit proviso that their programs not be identified and that parameters be presented only in normalized form. These constraints have been respected...|$|R
40|$|International Telemetering Conference Proceedings / November 04 - 07, 1991 / Riviera Hotel and Convention Center, Las Vegas, NevadaNASA’s {{reaction}} to {{requirements for the}} Space Station Freedom era’s telemetry data systems has been the continuing effort to combine a modular design approach with stateof-the-art VLSI technology for developing telemetry data processing systems. As part of this effort, NASA’s Data Systems Technology Division, in cooperation with Clemson University, is developing a Macintosh II based Telemetry and Command (MacTAC) system. This system performs telemetry data processing functions including frame synchronization, Reed-Solomon decoding, and packet reassembly at moderate data rates of 5 Mbps (20 Mbps burst). The MacTAC is a low-cost, transportable, easy to use, compact system designed to meet requirements specified by the Consultative Committee for Space Data Systems (CCSDS) while remaining flexible enough to support {{a wide variety of}} other user specific telemetry processing requirements (e. g., TDM data). In addition, the MacTAC can accept or generate forward data (such as spacecraft commands), calculate and append a Polynomial Check Code (PCC), and output this data to NASCOM to provide full Telemetry and Command (TAC) capability. Semi-custom VLSI gate arrays perform the return link functions of NASCOM deblocking, correlation, and frame synchronization. Reed-Solomon decoding (for error detection) and packet reassembly are also performed by modern microprocessor and semi-custom VLSI components. The local user interface is a standard Macintosh application with the wellknown look and feel of the Macintosh environment. A remote interface is possible via Ethernet which allows the system to be completely controlled from any location capable of generating the required remote operating commands. Return link data may be viewed in real time on the local or remote user interface screen in a variety of formats along with system status information. In addition, data may also be <b>archived</b> on SCSI <b>disks</b> for later retrieval and analysis as needed. This paper describes the general architecture and functionality of this MacTAC system including the particular custom telemetry cards, the various input/output interfaces, and the icon driven user interface...|$|R
40|$|Downwelling shortwave and longwave {{irradiation}} {{are being}} continuously monitored at Palisades, New York {{as part of}} the First International Satellite Cloud Climatology Project (ISCCP) Regional Experiment (FIRE) Extended Time/Limited Area Initiative. In addition, fisheye (180 degree) sky photographs are taken at the times of NOAA 9 and LANDSAT satellite overpasses on select days, particularly when cirrus clouds are present. Measurements of incoming shortwave (0. 28 to 2. 80 microns) hemispheric and diffuse, hemispheric near infrared (0. 7 to 2. 80 microns), and downwelling hemispheric infrared (4. 0 to 50. 0 microns) irradiation have been made from a rooftop location {{on the grounds of the}} Lamont-Doherty Geological Observatory since December 1986. The three Eppley Precision Spectral Pyranometers and the Eppley Pyrgeometer used to measure these variables were calibrated with Colorado State University instruments at Madison, Wisconsin {{as part of the}} FIRE Intensive Laboratory. Pyrgeometer output contains an adjustment for body temperature but not for dome temperature. Data are transmitted to a Campbell CR- 21 Digital Recorder, where one minute averages of ten second samples are stored and subsequently dumped to a cassette recorder. Using a Campbell C- 20 Cassette Interface, these data are transferred to an Apple Macintosh computer for analysis and for <b>archiving</b> on floppy <b>disks.</b> In addition to the raw irradiances collected, variables derived from these data are generated and stored. These include: the ratio of near infrared irradiation to visible irradiation and the fraction of the full shortwave irradiation which is diffuse; and will soon include: shortwave transmissivity and optical depth in the shortwave. Sky photographs are taken with an Olympus OM 2 -N 35 mm camera and are timed to be coincident with overpassing NOAA and LANDSAT satellites. Palisades is within the field of view of the NOAA 9 daily in the middle to late afternoon. The satellite viewing angle is within 45 degrees of nadir over Palisades on approximately half of the passes...|$|R
40|$|The Geological Survey of Canada (GSC) {{is making}} {{preparations}} for Canadian participation in GSETT 3 {{but will be}} unable to make a formal commitment until the necessary resources have been secured. As Canada is expected to provide at least four alpha stations, and a significant number of beta stations, the financial resources that will be needed are substantial, even though in many respccts the GSC is, with the recent modernization of the Yellowknife array and the ongoing installation of the Canadian National Seismograph Network (CNSN), well positioned to make a significant contribution to GSETT 3. The CNSN currently (October 1993) consists of 17 broad band stations and will grow to 23 and 33 such stations by December 1993 and December 1994 respectively. Some 40 50 short period stations will complete the network. Data from all sites are continuously telemetered in real time to network acquisition centres in Ottawa and Sidney, British Columbia, <b>archived</b> to optical <b>disk,</b> and kept on line in a 72 h ring buffer. Most of the broadband sites could serve as either alpha or beta stations once the necessary software for continuous data transfer, or on request provision, of data from the selected sites has been completed. This software wili be configured so that changes in station selection are easy to implement, and this will provide considerable flexibility to the GSETT 3 planning and operations working groups in selecting the optimum network. Backup stations can be designated in the case of station failures, and the network centre in British Columbia will serve, at least for beta stations, as a backup NDC to that in Ottawa. Data from. the Yellowknife array are collected in Yellowknife and forwarded in ten minute files to Ottawa, where processing is completed and the results archived. This arrangement would not meet the deadlines for receipt of alpha station data at the IDC and new hardware and software will be needed to forward the data more immediately from Yellowknife to Ottawa. Although the procedures and formats for both alpha and beta station data have not yet been agreed upon, or even discussed, by the GSE, it is apparent that new facilities will be re quired in Ottawa to multiplex and reformat data for transmission to the IDC. We anticipate that a dedicated 56 kbaud link will be needed between Ottawa. and Washington...|$|R

