39|10000|Public
5000|$|Determine {{the lowest}} level (granularity) of summary in <b>a</b> <b>fact</b> <b>table</b> (e.g. sales dollars).|$|E
50|$|In data warehousing, <b>a</b> <b>Fact</b> <b>table</b> {{consists}} of the measurements, metrics or facts of a business process. It {{is located at the}} center of a star schema or a snowflake schema surrounded by dimension tables. Where multiple fact tables are used, these are arranged as a fact constellation schema. <b>A</b> <b>fact</b> <b>table</b> typically has two types of columns: those that contain facts and those that are a foreign key to dimension tables. The primary key of <b>a</b> <b>fact</b> <b>table</b> is usually a composite key that is made up of all of its foreign keys. Fact tables contain the content of the data warehouse and store different types of measures like additive, non additive, and semi additive measures.|$|E
50|$|In data warehousing, a {{dimension}} table {{is one of}} the set of companion tables to <b>a</b> <b>fact</b> <b>table.</b>|$|E
5000|$|A {{transactional}} {{table is}} the most basic and fundamental. The grain associated with <b>a</b> transactional <b>fact</b> <b>table</b> is usually specified as [...] "one row per line in a transaction", e.g., every line on a receipt. Typically <b>a</b> transactional <b>fact</b> <b>table</b> holds data of the most detailed level, causing it {{to have a great}} number of dimensions associated with it.|$|R
50|$|Bitmap indexes {{are also}} useful in data {{warehousing}} applications for joining <b>a</b> large <b>fact</b> <b>table</b> to smaller dimension tables {{such as those}} arranged in a star schema.|$|R
5000|$|The planner {{is capable}} of using {{multiple}} indexes together to satisfy complex queries, using temporary in-memory bitmap index operations (useful in data warehousing applications for joining <b>a</b> large <b>fact</b> <b>table</b> to smaller dimension tables such as those arranged in a star schema).|$|R
50|$|<b>A</b> <b>fact</b> <b>table</b> might contain either detail level {{facts or}} facts {{that have been}} {{aggregated}} (fact tables that contain aggregated facts are often instead called summary tables).|$|E
5000|$|In {{the real}} world, it is {{possible}} to have <b>a</b> <b>fact</b> <b>table</b> that contains no measures or facts. These tables are called [...] "factless fact tables", or [...] "junction tables".|$|E
50|$|The star schema {{gets its}} name from the {{physical}} model's resemblance to a star shape with <b>a</b> <b>fact</b> <b>table</b> at its center and the dimension tables surrounding it representing the star's points.|$|E
30|$|It {{is these}} schemas {{that form the}} basis of any Data Warehouse. In Fig.  2, we firstly see the star schema in which <b>a</b> single <b>fact</b> <b>table</b> {{containing}} all numerical and summative values resides. Any number of dimensions then describes each row in the <b>fact</b> <b>table.</b> Dimensions typically represent (in the scope of education) dates, courses, modules, topics, etc. The snowflake schema is a logical extension that allows for greater granularity of querying, i.e., instead of just dates, they can be decomposed into years, semesters, weeks, days, etc.|$|R
40|$|Recent work {{proposed}} {{extending the}} OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform <b>a</b> given imprecise <b>fact</b> <b>table</b> into <b>a</b> form, called the Extended Database, {{that can be}} readily used to answer OLAP aggregation queries. In this work, we present scalable, efficient algorithms for creating the Extended Database (i. e., performing allocation) for <b>a</b> given imprecise <b>fact</b> <b>table.</b> Many allocation policies require multiple iterations over the imprecise <b>fact</b> <b>table,</b> and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, {{and has not been}} considered previously {{to the best of our}} knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straightfoward approaches...|$|R
40|$|The Indian Materials Database (IMDB) is a {{national}} project aiming to develop a database through compilation of materials property data available in different laboratories in India. The database contains data on mechanical, corrosion, nondestructive evaluation, thermal and optical properties {{of a wide variety}} of materials. Selecting the appropriate data modeling technique is crucial for the successful deployment of such a database. Dimensional modeling is a logical design technique to present data in a standard, intuitive framework that allows for high-performance access. Dimensional modeling of data results in a „Star Schema‟, where the data constitutes <b>a</b> central <b>fact</b> <b>table</b> surrounded by dimension tables. This paper discusses the model and architecture of the material database using a „Snowflake Schema‟, which is a variation of „Star Schema‟, where some of the dimensions are normalized into multiple related tables. The database contains <b>a</b> central <b>fact</b> <b>table</b> linked to multiple dimensions namely, 1) Materials 2) Properties of materials 3) Details of experiments conducted on materials and 4) Source from which data is obtained...|$|R
5000|$|Dimensional {{analysis}} of data can use representation terms for creating data warehouses. Representation terms such as Code and Indicator can be converted into dimensions and Amounts and Measures {{can be converted to}} measures in <b>a</b> <b>fact</b> <b>table.</b>|$|E
5000|$|<b>A</b> <b>Fact</b> <b>Table</b> {{contains}} {{expressions of}} any facts, {{each of which}} is accompanied by a number of auxiliary facts that provide additional information relevant for the main facts. Examples of auxiliary facts are: the intention, status, author, creation date, etc.|$|E
5000|$|By {{applying}} Temporal Database {{theory and}} modeling techniques the Temporal Snapshot Fact Table [...] allows {{to have the}} equivalent of daily snapshots without really having daily snapshots. It introduces the concept of Time Intervals into <b>a</b> <b>fact</b> <b>table,</b> allowing to {{save a lot of}} space, optimizing performances while allowing the end user to have the logical equivalent of the [...] "picture of the moment" [...] they are interested in.|$|E
5000|$|A table {{designed}} for the games snooker and English billiards is usually called a snooker table, although more accurately it is in <b>fact</b> <b>a</b> Billiards <b>table</b> (the baulkline is not used in snooker).|$|R
40|$|Over {{the past}} ten to fifteen years, data {{warehouse}} platforms have grown enormously, {{both in terms of}} their importance and their sheer size. Traditionally, such systems have been based upon a dimensional model known as the Star Schema that consists of <b>a</b> central <b>fact</b> <b>table</b> and <b>a</b> series of related dimension tables. Given the enormous size of the <b>fact</b> <b>table,</b> virtually all current systems augment the primary <b>fact</b> <b>table</b> with <b>a</b> small number of focused summary tables. Previous research has addressed the issue of the selection or identification of the most cost-effective summaries. However, the problem of efficiently computing a given view subset has received far less attention. In this paper, we present a suite of greedy algorithms for the construction of these view subsets. Experimental results demonstrate cost savings of between 20 % and 70 % relative to the naive alternatives, depending upon the degree of materialization required...|$|R
50|$|The Error Event schema holds {{records of}} all error events thrown {{by the quality}} screens. It {{consists}} of <b>an</b> Error Event <b>Fact</b> <b>table</b> with foreign keys to three dimension tables that represent date (when), batch job (where) and screen (who produced error). It also holds information about exactly when the error occurred and {{the severity of the}} error. In addition there is an Error Event Detail <b>Fact</b> <b>table</b> with <b>a</b> foreign key to the main table that contains detailed information about in which table, record and field the error occurred and the error condition.|$|R
50|$|A {{degenerate}} {{dimension is}} a key, {{such as a}} transaction number, invoice number, ticket number, or bill-of-lading number, that has no attributes and hence does not join to an actual dimension table. Degenerate dimensions are very common when the grain of <b>a</b> <b>fact</b> <b>table</b> represents a single transaction item or line item because the degenerate dimension represents the unique identifier of the parent. Degenerate dimensions often play an integral role in the fact table's primary key.|$|E
5000|$|A full Gellish Message {{table is}} in fact a {{combination}} of a Naming Table and <b>a</b> <b>Fact</b> <b>Table.</b> It contains not only columns for the expression of facts, but also columns for the names of the related objects and the additional columns to express auxiliary facts. This enables the use of a single table, also for the specification and use of synonyms and homonyms, multiple languages, etcetera. The core of a Message Table is illustrated in the following table: ...|$|E
50|$|One {{solution}} {{is to create a}} new dimension for each of the remaining attributes, but due to their nature, it could be necessary to create a vast number of new dimensions resulting in <b>a</b> <b>fact</b> <b>table</b> with {{a very large number of}} foreign keys. The designer could also decide to leave the remaining attributes in the fact table but this could make the row length of the table unnecessarily large if, for example, the attributes is a long text string.|$|E
40|$|When changes {{occur on}} data organization, {{conventional}} multidimensional structures are not adapted because dimensions {{are supposed to}} be static. In many cases, especially when time covered by the data warehouse is large, dimensions of the hypercube must be redesigned in order to integrate evolutions. We propose an approach allowing to track history but also to compare data, mapped into static structures. We define a conceptual model building <b>a</b> Mutiversion <b>Fact</b> <b>Table</b> from the Temporal Multidimensional Schema and we introduce the notion of temporal modes of representation corresponding to different ways to analyze data and their evolution...|$|R
5000|$|This data {{representation}} {{is analogous to}} space-efficient methods of storing a sparse matrix, where only non-empty values are stored. In an EAV data model, each attribute-value pair is <b>a</b> <b>fact</b> describing <b>an</b> entity, and a row in an EAV <b>table</b> stores <b>a</b> single <b>fact.</b> EAV <b>tables</b> are often described as [...] "long and skinny": [...] "long" [...] refers {{to the number of}} rows, [...] "skinny" [...] to the few columns.|$|R
40|$|OLAP {{applications}} use precomputation of {{aggregate data}} to improve query response time. While this {{problem has been}} well-studied in the recent database literature, to our knowledge all previous work has focussed on the special case in which all aggregates are computed from a single cube (in a star schema, this corresponds to there being <b>a</b> single <b>fact</b> <b>table).</b> This is unfortunate, because many real world applications require aggregates over multiple <b>fact</b> <b>tables.</b> In this paper, we attempt to fill this lack of discussion about the issues arising in multi-cube data models by analyzing these issues. Then we examine performance issues by studying the precomputation problem for multi-cube systems. We show that this problem is significantly {{more complex than the}} single cube precomputation problem, and that algorithms and cost models developed for single cube precomputation must be extended to deal well with the multi-cube case. Our results from a prototype implementation show that for multi-cube workloads substantial performance improvements can be realized by using the multi-cube algorithms...|$|R
5000|$|Fact tables record {{measurements}} or metrics for {{a specific}} event. Fact tables generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept.Fact tables are designed to {{a low level of}} uniform detail (referred to as [...] "granularity" [...] or [...] "grain"), meaning facts can record events at a very atomic level. This can result in the accumulation {{of a large number of}} records in <b>a</b> <b>fact</b> <b>table</b> over time. Fact tables are defined as one of three types: ...|$|E
50|$|For example, a {{database}} may contain <b>a</b> <b>fact</b> <b>table</b> that stores sales records. This fact table would {{be linked to}} dimensions by means of foreign keys. One of these dimensions may contain data about the company's salespeople: e.g., the regional offices in which they work. However, the salespeople are sometimes transferred from one regional office to another. For historical sales reporting purposes {{it may be necessary}} to keep a record of the fact that a particular sales person had been assigned to a particular regional office at an earlier date, whereas that sales person is now assigned to a different regional office.|$|E
50|$|If {{the rows}} in <b>a</b> <b>fact</b> <b>table</b> {{are coming from}} several timezones, it might be useful to store date and time in both local time and a {{standard}} time. This {{can be done by}} having two dimensions for each date/time dimension needed - one for local time, and one for standard time. Storing date/time in both local and standard time, will allow for analysis on when facts are created in a local setting and in a global setting as well. The standard time chosen can be a global standard time (ex. UTC), it can be the local time of the business’ headquarter, or any other time zone that would make sense to use.|$|E
40|$|STREETS: Augusta; Centre [Quarry to Sycamore]; Congress; Hamilton; Hotel; Ohio; Quarry [North Side]; Sophia; Sycamore; Washington; SPECIALS: Clasgens J. & H. Woolen Mills; Clermont Flour Mills; Clermont Woolen Mills; Keyser A. & Co. Distillery; Methodist Episc. Ch. Quarry St.; Roettinger P. & Son Tannery; Shaw W. A. & <b>A.</b> J. <b>Table</b> <b>Fact...</b>|$|R
40|$|Abstract. Most {{existing}} {{data mining}} (DM) approaches look for pat-terns {{in a single}} table. Multi-relational DM approaches, on the other hand, look for patterns that involve multiple tables. In recent years, the most common DM techniques have been extended to the multi-relational case, but there are few dedicated to star schemas. These schemas are com-posed of <b>a</b> central <b>fact</b> <b>table,</b> linking <b>a</b> set of dimension tables, and joining all the tables before mining {{may not be a}} feasible solution. This work proposes a method for frequent pattern mining in a star schema based on FP-Growth. It does not materialize the entire join between the tables. Instead, it constructs an FP-Tree for each dimension and then combines them to form a super FP-Tree, that will serve as input to FP-Growth. ...|$|R
40|$|Database {{design for}} data {{warehouses}} {{is based on}} the notion of the snowflake schema and its important special case, the star schema. The snowflake schema represents a dimensional model which is composed of <b>a</b> central <b>fact</b> <b>table</b> and <b>a</b> set of constituent dimension tables which can be further broken up into subdimension tables. We formalise the concept of a snowflake schema in terms of an acyclic database schema whose join tree satisfies certain structural properties. We then define a normal form for snowflake schemas which captures its intuitive meaning with respect to a set of functional and inclusion dependencies. We show that snowflake schemas in this normal form are independent as well as separable when the relation schemas are pairwise incomparable. This implies that relations in the data warehouse can be updated independently of each other as long as referential integrity is maintained. In addition, we show that a data warehouse in snowflake normal form can be queried by joining the relation over the <b>fact</b> <b>table</b> with the relations over its dimension and subdimension tables. We also examine an informationtheoretic interpretation of the snowflake schema and show that the redundancy of the primary key of the <b>fact</b> <b>table</b> is zero. Key words. Data warehouse design, star and snowflake schema, independent and separable database schema, acyclic database schema. ...|$|R
5000|$|Fact tables {{provide the}} (usually) {{additive}} values {{that act as}} independent variables by which dimensional attributes are analyzed. Fact tables are often defined by their grain. The grain of <b>a</b> <b>fact</b> <b>table</b> represents the most atomic level by which the facts may be defined. The grain of a SALES fact table might be stated as [...] "Sales volume by Day by Product by Store". Each record in this fact table is therefore uniquely defined by a day, product and store. Other dimensions might be members of this fact table (such as location/region) but these add nothing to {{the uniqueness of the}} fact records. These [...] "affiliate dimensions" [...] allow for additional slices of the independent facts but generally provide insights at a higher level of aggregation (a region contains many stores).|$|E
5000|$|A Gellish Fact Table {{consists}} of columns {{for the main}} fact {{and a number of}} columns for auxiliary facts. The auxiliary facts enable to specify things such as roles, cardinalities, validity contexts, units of measure, date of latest change, author, references, etcetera.: The columns for the main fact in <b>a</b> <b>Fact</b> <b>Table</b> are: - a UID of the fact that is expressed on this row in the table [...] - a UID of the intention with which the fact is communicated or stored (e.g. as a statement, a query, etc.) [...] - a UID of a left-hand object [...] - a UID of a relation type [...] - a UID of a right-hand object [...] - a UID of a unit of measure (optional) [...] - a string that forms a description (textual definition) of the left hand object.|$|E
3000|$|The {{conceptual}} model {{used for this}} integrated approach {{is based on a}} star dimensional structure, which provides <b>a</b> <b>fact</b> <b>table</b> (OLAP-MCDA cube) as evaluation table that contains observable, measurable and digital data (Kimball and Ross 2002) circled by dimensions including the specific needs of decision makers as mentioned below: [...]...|$|E
5000|$|Eloise finally {{walks in}} to the {{reception}} hall and meets the other denizens of Table 19, who are Jerry and Bina Kepp, who own a diner and are Facebook friends with the groom's father due to his owning diner chain; Renzo Eckberg, whose parents are acquaintances {{and came to the}} wedding in the hopes of meeting a girl; Jo Flanagan, Francie's childhood nanny; and Walter Thimble, a distant nephew who is currently on parole after he was tricked into stealing $125,000 from his last job. The table debates whether table 19 is a [...] "good table" [...] to which Eloise responds and said that before getting dumped she planned half the wedding and knows for <b>a</b> <b>fact</b> that <b>table</b> 19 is for [...] "guests that should have known not to show up." ...|$|R
40|$|EXtensible Markup Language (XML) {{has emerged}} as the {{dominant}} standard in describing and exchanging data among heterogeneous data sources. The increasing presence of large volumes of data appearing creates the need to investigate XML Document Warehouses as a means of handling the data. In this paper our focus is twofold. First we utilise Object Oriented (OO) concepts to develop and propose a conceptual design formalism to build meaningful XML Document Warehouses (XDW). This includes: (1) XML (warehouse) repository (xFACT) using OO concepts followed by the transformation of XML Schema constructs and (2) Conceptual Virtual Dimensions (VDims) using Conceptual views (Rajugan, Chang, Dillon, & Feng, 2003, 2004). Secondly we address several important outstanding issues related to our proposed design of an XML Document Warehouse. Specifically we note that the xFACT portion is now a complex structure, involving several entities and relationships as opposed to being <b>a</b> simple <b>FACT</b> <b>table</b> {{as was the case in}} relational data warehouses, and the notion of Virtual Dimensions (VDims) has considerably greater complexity...|$|R
40|$|Traditional {{data mining}} {{approaches}} look for patterns {{in a single}} table, while multi-relational data mining aims for identifying patterns that involve multiple tables. In recent years, the most common mining techniques have been extended to the multi-relational context, but there are few dedicated to deal with data stored following the multi-dimensional model, in particular the star schema. These schemas are composed of <b>a</b> central huge <b>fact</b> <b>table</b> linking <b>a</b> set of small dimension tables. Joining all the tables before mining {{may not be a}} feasible solution due to the usual massive number of records. This work proposes a method for mining frequent patterns on data following a star schema that does not materialize the join between the tables. As it extends the algorithm FP-Growth, it constructs an FP-Tree for each dimension and then combines them through the records in the <b>fact</b> <b>table</b> to form <b>a</b> super FP-Tree. This tree is then mined with FP-growth to find all frequent patterns. The paper presents a case study on bibliographic data, comparing efficiency and scalability of our algorithm against FP-Growth...|$|R
