34|33|Public
60|$|To this I {{was forced}} to agree. I here {{definitely}} declare that the psychic was not concerned with the flight of the cone in any way known to biology. If she produced the voices, they too must have been examples of supernormal ventriloquism, for they came through the megaphone. Of that I am as certain as one can be of an <b>auditory</b> <b>impression.</b>|$|E
5000|$|... "Regret" [...] is a maxi-single by The Gazette. It was {{released}} as two different types: the Optical Impression and <b>Auditory</b> <b>Impression,</b> the first coming with a DVD {{with the music}} video for the song [...] "Regret", and the second with a bonus track.|$|E
5000|$|... "Filth in the Beauty" [...] is a maxi-single by The Gazette. It was {{released}} as two different types: the Optical Impression and <b>Auditory</b> <b>Impression,</b> the first coming with a DVD {{with the music}} video for the song [...] "Filth in the Beauty", and the second with a bonus track.|$|E
30|$|Sounds {{infinite}} in variety surround us {{throughout our}} lives. When we describe sounds {{to others in}} our daily lives, onomatopoeic representations related to the actual acoustic properties of the sounds they represent are often used. Moreover, because the acoustic properties of sounds induce <b>auditory</b> <b>impressions</b> in listeners, onomatopoeic representations and the <b>auditory</b> <b>impressions</b> associated with actual sounds may be related.|$|R
30|$|In {{practical}} {{situations in}} which people communicate sound information to others using onomatopoeic representation, {{it is necessary that}} the receivers of onomatopoeic representations (e.g., engineers in the above-mentioned case) be able to identify the acoustic properties and <b>auditory</b> <b>impressions</b> of the sounds that onomatopoeic representations represent. The present paper examines this issue. Experiments were carried out in which participants evaluated the auditory imagery associated with onomatopoeic representations. The auditory imagery of onomatopoeic representations was compared with the <b>auditory</b> <b>impressions</b> for their corresponding actual sound stimuli, which were obtained in our previous study [7].|$|R
30|$|These {{results suggest}} that the {{receiver}} of onomatopoeic representations can more accurately guess <b>auditory</b> <b>impressions</b> of muddiness, brightness, and sharpness (or clearness, darkness and dullness) for real sounds from their heard onomatopoeic representations. Conversely, it seems difficult for listeners to report impressions of strength and powerfulness for sounds using onomatopoeic representations.|$|R
5000|$|... "Distress and Coma" [...] is the 15th maxi-single by The Gazette. The single {{comes in}} two {{different}} types: the Optical Impression and <b>Auditory</b> <b>Impression,</b> the first contains a DVD with the music video for the song [...] "Distress and Coma", and the latter comes with a bonus track instead. In accordance {{with the release of}} the single, The Gazette hosted a special event which a lucky purchaser got to attend.|$|E
5000|$|When {{discussing}} the <b>auditory</b> <b>impression</b> {{created by the}} sound of the decasyllabic quatrain, Ralph Waldo Emerson described how he would hum the tune created by the pattern of the rhyme scheme then long to fill the sounds in with the words of a poem. However, Henry David Thoreau, when writing about Emerson's [...] "Ode to Beauty" [...] criticizes the use of the decasyllabic quatrain by suggesting that its tune is unworthy of the thoughts expressed.|$|E
5000|$|... "Hyena" [...] is {{the fourth}} major and 12th overall single {{released}} by The Gazette. It is their lone single of 2007. It was released in two different versions: an Optical Impression (CD+DVD) edition and an <b>Auditory</b> <b>Impression</b> edition. The CD+DVD edition comes with a DVD of the title song's PV, while the CD-only edition comes with the B-side song [...] "Defective Tragedy". [...] "Chizuru" [...] {{was used as the}} theme song for the film [...] Apartment.|$|E
30|$|The {{obtained}} rating {{scores were}} averaged across participants for each scale {{and for each}} onomatopoeic stimulus. To compare impressions between actual sound stimuli and onomatopoeic representations, factor analysis {{was applied to the}} averaged scores for onomatopoeic representations together with those for the sound stimuli (i.e., the rating results of <b>auditory</b> <b>impressions)</b> obtained in our previous experiments [7].|$|R
50|$|The film {{does not}} make a claim to realism or {{objective}} expertise regarding the collapse of Communism but instead examines the individual stories of characters in intimate ways, surveying their fear of the surrounding political opacity.Akerman presents a continuous, nonsynchronous montage of images and sounds, provoking unfiltered optical and <b>auditory</b> <b>impressions.</b> The smoothness of the visual continuity is used as a tactic to accentuate the effects of narrative disjunction and discontinuity.|$|R
30|$|In our {{previous}} study, {{we found that}} the powerfulness impressions of sounds were significantly correlated with the number of voiced consonants [7]. However, as shown in Figure 1 (c), the auditory imagery of onomatopoeic stimuli containing voiced consonants (i.e., nos. 26 and 35) was different from the <b>auditory</b> <b>impressions</b> evoked by real sounds. Thus, we can conclude {{that it is difficult to}} communicate the powerfulness impression of sounds by voiced consonants alone.|$|R
3000|$|In {{the present}} work, {{considering}} that our voice quality data {{is based on}} <b>auditory</b> <b>impression,</b> we use the terms [...] "breathy" [...] and [...] "breathiness" [...] in a broad sense, indicating all utterances where turbulent noise is audibly perceived in the vowel segments. However, the term [...] "whispery" [...] is also used in the paper, when the <b>auditory</b> <b>impression</b> of the turbulent noise is closer to whisper, rather than to normal phonation.|$|E
40|$|It {{has been}} said that there are certain {{differences}} between English /s / and Japanese /s/ in their articulation, and also in their <b>auditory</b> <b>impression</b> that the former sounds stronger than the latter. This paper is trying to reveal the differences/similarities of the acoustic characteristics of English and Japanese voiceless alveolar fricatives in their L 1 and L 2 speech and to know how the articulatory differences and the <b>auditory</b> <b>impression</b> would be reflected in acoustical properties such as their amplitude, duration, formant frequency, and spectral shape. Three native British English speakers whose L 2 is Japanese, and three native Japanese speakers whose L 2 is English were asked to read out word lists in their first and second languages. As a result, it was found that there was an interesting prominence in the lower frequency region of the spectral shape that can be seen only in the spectra of alveolar fricatives uttered by the English speakers but not in those uttered by the Japanese speakers. 1...|$|E
40|$|Early in 1960 {{it dawned}} on me that the sensory impression, proffered by a medium like movie or radio, was not the sensory effect obtained. Radio, for example, has an intense visual effect on listeners. But then there is the {{telephone}} which also proffers an <b>auditory</b> <b>impression,</b> but has no visual effect. In the same way television is watched but has a very different effect from movies. (McLuhan, 1960, emphasis in original...|$|E
5000|$|Originally {{printed in}} an edition of 300, the {{original}} book became even scarcer when the River Arno flooded in 1966, destroying {{many of the}} remaining copies.'Of the futurist poets, Ardengo Soffici (1879 - 1965) produced {{some of the best}} experimental verse of the time. Painter, art and literary critic, Soffici was an active mediator between Italy and the French avant-garde.... BÏF§ZF+18 Simultaneità e Chimismi lirici, his collection of Futurist poems, is a collage of visual and <b>auditory</b> <b>impressions</b> that combine memory and sensation to give the effect of poetry in motion.' Peter Brand & Lino Pertile ...|$|R
30|$|In {{previous}} studies, {{relationships between}} the temporal and spectral acoustic properties of sounds and onomatopoeic features have been discussed [1 – 4]. We have also conducted psychoacoustical experiments to confirm the validity of using onomatopoeic representations to identify the acoustic properties of operating sounds emitted from office equipment and audio signals emitted from domestic electronic appliances [5, 6]. We found relationships between subjective impressions, such as the product imagery and functional imagery evoked by machine operation sounds, audio signals, and the onomatopoeic features. Furthermore, in a separate previous study, we investigated the validity of using onomatopoeic representations to identify the acoustic properties and <b>auditory</b> <b>impressions</b> of various kinds of environmental sounds [7].|$|R
40|$|In Noh, a {{traditional}} performing art of Japan, extremely expressive voice quality {{is used to}} convey an emotional message. Aperiodicity of voice appears responsible for these special effects. Acoustic signals were recorded for selected portions of dramatic singing in order to study the acoustic effects of delicate voice control by {{a master of the}} Konparu school. Using a signal analysis-synthesis algorithm, TANDEM-STRAIGHT, to represent multiple candidates for pitch perception, signals deviating from the harmonic structure have been successfully displayed, corresponding to <b>auditory</b> <b>impressions</b> of pitch movements, even when narrow-band spectrograms failed to show the perceived events. Strong interaction between vocal tract resonance and vocal fold vibration seems {{to play a major role}} in producing these expressive voice qualities...|$|R
40|$|Evidence is {{presented}} {{that suggests that}} articulation should be considered separately from acoustics (or the <b>auditory</b> <b>impression)</b> in investigations of the interface between phonetics and phonology. We use Ultrasound Tongue Imaging to show that onset and coda versions of Dutch /r/ can have secondary articulations, categorical allophones, and subtle or covert articulations which have few acoustic implications. Covert rhotic (retroflex) articulation was observed in one speaker, who displayed acoustic derhoticisation. We also consider this finding in relation to ongoing work in Scottish English...|$|E
30|$|In {{the present}} work, {{we focus on}} breathy and whispery voice qualities, which are {{characterized}} by an <b>auditory</b> <b>impression</b> of turbulent noise, caused by an air escape through the glottis, and analyze their communication roles (i.e., the variations in paralinguistic information) in spontaneous dialogue speech, for several speakers. Note that fricative consonants are also characterized by turbulent noise, but it is produced by a constriction in the vocal tract, while turbulence in breathy and whispery voices is produced by a constriction at the glottis.|$|E
3000|$|Breathiness is {{generally}} {{treated as a}} continuum {{that is difficult to}} separate into [...] "breathy" [...] and [...] "modal" [...] whether sorting is based on perceived quality, acoustics, or the underlying glottal configuration [10]. The transition from [...] "breathy" [...] to [...] "whispery" [...] seems also to be part of an auditory continuum [1]. Although breathy and whispery voices have distinct definitions in terms of the phonation settings, they are often confused, probably because they are similarly characterized by the <b>auditory</b> <b>impression</b> of turbulent noise (aspiration noise).|$|E
40|$|This study investigates {{examples}} of horse race commentaries and compares the acoustic properties with an auditorily based {{description of the}} typical suspense pattern from calm to very excited at the finish and relaxation after the finish. With the exception of tempo, the <b>auditory</b> <b>impressions</b> were basically confirmed. The examination shows further {{that the results of}} the investigated prosodic parameters pause duration, pausing and breathing rate, F 0 level and range, intensity, and spectral tilt fit well with other forms of excitement such as anger or elation. Additionally, it is discussed how the specific speaking style of horse race commentators can be classified. Finally, the role of prosodic descriptions for modelling those speaking styles and emotions, especially for speech technology, is considered. 1...|$|R
40|$|Abstract: The near-death {{experience}} (NDE) of Pam Reynolds {{is one of}} the most impressive and medically well-documented NDEs in the literature. It took place during an operation to remove a brain aneurism, and it included almost all the aspects of a classic NDE, including accurate visual perception of the operating theater. Furthermore, parts of the experience would seem to have occurred when no brain activity whatsoever was possible. Despite testimony to the contrary by the medical personnel involved, Gerald Woerlee has attempted to explain Reynold's experience as a result of <b>auditory</b> <b>impressions</b> combined with an anesthesia-induced fantasy. I argue here that Woerlee's attempted explanation is simply unsupported by the documented facts of the case. I also invite Woerlee to accompany me to the Barrow Neurological Institute to participate in an empirical test under the exact auditory conditions Reynolds experienced...|$|R
40|$|It {{has proven}} {{practical}} {{over a long}} history of research on language sound systems to rationalize phonological units and processes in terms of speech articulation. The Sanskrit grammarians, for example, focused on vocal anatomy and articulatory processes to the exclusion of descriptions of acoustic or <b>auditory</b> <b>impressions</b> produced by speech sounds (Allen, 1953). Similarly, the 19 th century linguists Bell (1867), Sweet (1877), Sievers (1881), Passy (1890), and Rousselot (1897 - 1901) all focused primarily on speech articulation to explain sound change, describe similarities and differences across languages and in language teaching. For example, the Sweet/Bell system of vowel classification (which is still widely used in phonological description) and their iconic phonetic alphabets were based on speech articulation. This tradition of articulatory phonetics also formed the basis for the structuralists' approach to phonetics and phonology (Pike, 1943). It is arguably t...|$|R
40|$|Is Mandarin Chinese a syllable-timed language? Based on <b>auditory</b> <b>impression,</b> {{traditional}} analyses say it is. However, {{this question}} {{has rarely been}} investigated {{from the perspective of}} acoustic phonetics. Following Ramus et al. (1999) and Grabe and Low (2002), we measured the four rhythmic correlates: vowel percentage, consonant standard deviation, rPVI and nPVI in passage readings and conversations of native Mandarin speakers. Except those for nPVI, the results confirmed the impression that Mandarin is a syllable-timed language. Keywords: rhythm, syllable-timed, stress-timed, acoustic phonetics, consonant duration, vowel duration...|$|E
40|$|This paper proposes an {{objective}} speech distortion measure {{as a substitute}} for human auditory systems. Simultaneous and temporal masking effects are introduced into this measure called auditory-oriented Spectral Distortion(ASD). We calculate the ASD using spectral components over masked thresholds {{in the same way as}} the Spectral Distortion(SD). We confirmed that the ASD is more compatible to subjective mean opinion score that represents distortions on <b>auditory</b> <b>impression</b> than the SD. We applied the ASD to optimize a noise reduction algorithm proposed by the authors, and confirmed that this optimized algorithm reduces noises appearing to the ear. ASD is sure to be an available guide to design noise reduction algorithms...|$|E
40|$|This paper {{investigates the}} phonetic {{features}} of vowels with a diacritic for voiced obstruents (dakuten) in Japanese, which are phonologically and orthographically nonstandard but often observed recently in informal linguistic media. Recorded data of these vowels was analysed {{in terms of}} <b>auditory</b> <b>impression,</b> visual inspection, formant frequencies, phonation type, F 0 and acoustic intensity. It {{was found that the}} production of /a / with a dakuten exhibited positive spectral tilt in the low frequency range and lowering of F 0, both of which are indicative of creaky voice. On the other hand, an increase in acoustic intensity, which has been claimed by some previous work, was not consistently observed in this analysis...|$|E
25|$|The grave/acute {{distinction}} {{has lost}} its relevance in modern phonetics (though it may still be relevant to other disciplines). It dates from relatively early {{in the days of}} acoustic phonetics, when some phonologists believed that one could categorize all speech sounds by means of a finite set of acoustically-defined distinctive features. These were supposed to correspond to <b>auditory</b> <b>impressions</b> of sounds. The pioneering publication for this was Jakobson, Fant and Halle (1951) Preliminaries to Speech Analysis (MIT). The feature(s) grave/acute were defined primarily in acoustic terms (with some reference to auditory qualities), but were given a secondary description (or gloss) in terms of their articulation. Features like grave/acute could be used to divide speech sounds into broad classes. For most phoneticians, the JF features had been superseded by 1968 by the articulatory features set out in Chomsky and Halle’s Sound Pattern of English and by competing articulatory features devised by Ladefoged in such publications as Preliminaries to Linguistic Phonetics (1971).|$|R
40|$|We {{introduce}} a new control problem: the control of motion simulating devices (Virtual Motion Systems, or VMS) for walking and running humans and robots in a fashion that feels most "realistic," that is, like locomoting on ground. After developing simplified dynamical models for the VMS, the human/robot and the resulting coupled system, we cast the problem {{in terms of a}} performance index. This approach permits application of standard optimal control theory. We present two solutions and discuss upcoming problems in the task domain of virtual motion control. Figure 1 : Legged robot on a treadmill 1 Introduction Sensation in Artificial Reality environments could be considerably enriched if we could provide a realistic simulation of locomotion [6]. People could walk and run in any direction, without limitation, while guided by visual and <b>auditory</b> <b>impressions</b> from their head mounted displays. At the same time, however, they would not go anywhere, because they are moving on a "virtual motion [...] ...|$|R
50|$|The grave/acute {{distinction}} {{has lost}} its relevance in modern phonetics (though it may still be relevant to other disciplines). It dates from relatively early {{in the days of}} acoustic phonetics, when some phonologists believed that one could categorize all speech sounds by means of a finite set of acoustically-defined distinctive features. These were supposed to correspond to <b>auditory</b> <b>impressions</b> of sounds. The pioneering publication for this was Jakobson, Fant and Halle (1951) Preliminaries to Speech Analysis (MIT). The feature(s) grave/acute were defined primarily in acoustic terms (with some reference to auditory qualities), but were given a secondary description (or gloss) in terms of their articulation. Features like grave/acute could be used to divide speech sounds into broad classes. For most phoneticians, the JF&H features had been superseded by 1968 by the articulatory features set out in Chomsky and Halle’s Sound Pattern of English and by competing articulatory features devised by Ladefoged in such publications as Preliminaries to Linguistic Phonetics (1971).|$|R
3000|$|For the utterances where breathiness was {{perceived}} (excluding the laughing speech utterances), {{a more detailed}} segmentation and annotation of voice quality was conducted for evaluating the acoustic features introduced in Section 2. The segmentation was conducted by the first author, based on visual inspection of the spectrograms and on <b>auditory</b> <b>impression.</b> The segment categories are [...] "br/wh voiced" [...] (for breathy and whispery voiced segments), [...] "br/wh" [...] (for unvoiced whispered or aspirated segments), [...] "br/wh?" [...] (for segments with acoustic and auditory properties intermediate between breathy/whispery and other voice qualities), [...] "modal" [...] (for normal phonation in voiced segments), [...] "fricative" [...] (for fricative and affricative consonants), [...] "aspirated consonant" [...] (for /h/), [...] "nasal", and [...] "rough" [...] (for rough quality segments, including vocal fry, creaky, and period-doubled segments).|$|E
40|$|Geometric {{acoustic}} modeling systems spatialize sounds {{according to}} reverberation paths from a sound source to a receiver {{to give a}} realistic <b>auditory</b> <b>impression</b> of a virtual 3 D environment. These systems are useful for concert hall design, teleconferencing, training and simulation, and interactive virtual environments. In many cases, such as in an interactive walkthrough program, the reverberation paths must be updated within strict timing constraints [...] e. g., as the sound receiver position moves under interactive control by a user. In this paper, we describe a geometric acoustic modeling algorithm that uses a priority queue to trace polyhedral beams representing reverberation paths in best-first order up to some termination criteria (e. g., expired time-slice). The advantage of this algorithm {{is that it is}} more likely to find the highest priority reverberation paths within a fixed time-slice, avoiding many geometric computations for lower-priority beams. Yet, there is overhead [...] ...|$|E
40|$|Human {{perception}} of the geometry and spatial layout of an environment is a multi-sensory process. In addition to sight, the human brain is also particularly adept at subconsciously processing echoes and using these reflected sounds to provide some indication of the dimensions of an environment. This <b>auditory</b> <b>impression</b> {{of the size of}} an environment will incorporate surfaces not only to the front, but also to the sides and rear of the person and thus currently hidden from his/her view. So while computer graphics can provide an image of what a person can currently see, the level of perceptual realism may be significantly improved by incorporating auditory effects as well. This paper describes a method for combining the computation of lighting and acoustics to provide enhanced rendering of virtual environments. Keywords : Computer Graphics, Acoustic Rendering, Virtual Reality, Particle Tracing, Perception. 1 INTRODUCTION Our {{perception of}} the geometry and size of virtual environ [...] ...|$|E
3000|$|Furthermore, one of {{the most}} {{primitive}} human behaviors related to sounds is the identification of sound sources [11]. Gygi et al. [12] reported that the important factors affecting the identification of environmental sounds involve spectral information, especially the frequency contents around 1 - 2 [*]kHz, and temporal information such as envelope and periodicity. If we do indeed recognize events related to everyday sounds using acoustic cues [13 – 15], then {{it may be possible to}} also recognize sound sources from onomatopoeic features instead of acoustic cues. Moreover, such recognition of the source may affect the auditory imagery evoked by onomatopoeia. Although Fujisawa et al. [16] examined the auditory imagery evoked by simple onomatopoeia with two morae such as /don/ and /pan/ ("mora" [...] is a standard unit of rhythm in Japanese speech), sound source recognition was not discussed in their study. In the present paper, therefore, we took sound source recognition into consideration while comparing the auditory imagery of onomatopoeic representations to the <b>auditory</b> <b>impressions</b> induced by their corresponding real sounds.|$|R
40|$|Presented at the 10 th International Conference on Auditory Display (ICAD 2004) Spatial {{impression}} {{refers to}} the attributes of subjective space beyond localization. In the field of auditorium acoustics, <b>auditory</b> spatial <b>impression</b> is often divided into `apparent source width', `envelopment' and sometimes `intimacy'. In separate experiments, this study considers how visual and <b>auditory</b> spatial <b>impression</b> vary within two auditoria, and hence similarities between these two sensory modes. In the visual experiment, the `spaciousness', `envelopment', `stage dominance', `intimacy' and target distance were judged by subjects using grayscale projected photographs, taken from various positions in the audience areas of the two auditoria when a visual target was on stage. In the auditory experiment, the `apparent source width', `envelopment', `intimacy' and performer distance were judged using an anechoic orchestral recording convolved with binaural impulse responses measured from the same positions in the two auditoria. Results show target distance to be of primary importance in auditory and visual spatial impression – thereby providing a basis for covariance between some attributes of auditory and visual spatial impression. Nevertheless, some attributes of spatial impression diverge between the senses...|$|R
40|$|It is too {{complicated}} to teach supra-segmental phonology, namely rhythm, intonation and {{the stress of}} a foreign language analytically. One tool for learning these may be the frequent imitation of model sounds. In creating authentic software or a textbook of English pronunciation training, voices of some native speakers of the language are usually recorded as models. In cooperation with staff members at Konan University, I am developing computer-based listening and pronunciation software {{which is designed to}} improve learners 2 ̆ 7 ability to sense and monitor some prosodic features of English. Before practicing with the software, 20 students had taken a pretest. We have already created another set of software for the pretest which was designed to record students 2 ̆ 7 voices. Ten sentences out of forty were recorded without model sounds. The same sentences were also recorded after they listened to model sounds. The aim {{of this study is to}} compare students 2 ̆ 7 own creative reading sounds with their imitating reading sounds. To investigate which element of speech sounds changed or did not change is especially important when creating aural/oral training software. The fundamental frequencies of each sentence and the duration of some target consonants were measured by speech analyzers. <b>Auditory</b> <b>impressions</b> of two native teachers of English and two Japanese teachers of English were also examined. Fundamental frequencies apparently changed after listening to model sounds as predicted while some consonant confusion in voicing and manner was not reduced. Both groups of teachers observed that students 2 ̆ 7 pronunciation had improved in most cases. after they listened to model sounds. The aim of this study is to compare students 2 ̆ 7 own creative reading sounds with their imitating reading sounds. To investigate which element of speech sounds changed or did not change is especially important when creating aural/oral training software. The fundamental frequencies of each sentence and the duration of some target consonants were measured by speech analyzers. <b>Auditory</b> <b>impressions</b> of two native teachers of English and two Japanese teachers of English were also examined. Fundamental frequencies apparently changed after listening to model sounds as predicted while some consonant confusion in voicing and manner was not reduced. Both groups of teachers observed that students 2 ̆ 7 pronunciation had improved in most cases...|$|R
