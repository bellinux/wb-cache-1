5|984|Public
40|$|To {{facilitate}} networked {{discovery and}} information retrieval in the biomedical domain, we have designed {{a system for}} automatic assignment of Medical Subject Headings to documents retrieved from the World-Wide Web. Our prototype implementations show significant promise. We describe our methods and discuss the further development of a completely <b>automated</b> <b>indexing</b> <b>tool</b> called the "Web-MeSH Medibot. ...|$|E
40|$|We {{analyzed}} {{the effect of}} crystallographic anisotropy on the morphological evolution of a 12 -nm-thick gold film during solid-state dewetting at high temperatures using <b>automated</b> <b>indexing</b> <b>tool</b> in a transmission electron microscopy. Dewetting initiated at grain-boundary triple junctions adjacent to large grains resulting from abnormal grain growth driven by (111) texture development. Voids at the junctions developed shapes with faceted edges bounded by low-index crystal planes. The kinetic mobility of the edges varied with the crystal orientation normal to the edges, with a predominance of specific edges with the slowest retraction rates as the annealing time was increased...|$|E
40|$|Automatic {{indexing}} is evaluated as an aid/replacement to manual indexing for biomedical literature. Manual indexing {{is costly}} and labour intensive. Technological innovations {{have the potential}} to increase efficiency and reduce costs. British Library produces a bibliographic database of allied and complementary medicine (AMED). This study compares articles which have been indexed manually for AMED with the same documents submitted to an <b>automated</b> <b>indexing</b> <b>tool.</b> The indexing tool selected was Helping Interdisciplinary Vocabulary Engineering, (HIVE) which is a jointly funded project by the University of North Carolina and the National Evolutionary Synthesis Center, North Carolina. A random selection of 100 records from a total of 1059 articles was selected. Each manually indexed document was compared with results returned by HIVE. Data analysis was made using SPSS. Results showed that HIVE does not provide a suitable replacement for the skills of a human indexer. Continued development of automatic indexing tools is recommended...|$|E
5000|$|Most Word {{processors}} have integrated <b>automated</b> <b>indexing</b> functions. These <b>tools</b> build a concordance or word lists from processed files. They {{have often}} limited usage.|$|R
40|$|The index {{selection}} problem is NP-Complete and therefore requires heuristic to be used. Heuristic {{must be based}} on the query optimizer principles because the indices decided should be usable by the query optimizer. It should be noted that since the query optimization principles of commercial DBMSs vary to a great extent, it is infeasible to implement a general purpose <b>index</b> selection <b>tool.</b> We present a tool MAESTRO 7 (METU <b>Automated</b> <b>indEx</b> Selection <b>Tool</b> foR Oracle 7) that assists the database administrator in designing an index configuration for a given database application for Oracle 7. It decides on the complete set of indices both primary and secondary by considering the index maintenance costs. MAESTRO 7 automatically derives valid SQL statements and their usage statistics during regular database sessions through SQL Trace Facility. It classifies the SQL statements that will produce the same execution plan and accumulates their weights. MAESTRO 7 also decides on plausible columns for indexing at the beginning, before the more time consuming processing on the queries start, to improve performance of the tool. The selection of primary indices cannot be done independently for each table for multi table queries. Therefore in choosing primary indices, all combinations of plausible primary indices are considered. The benefit of using the indices are calculated by obtaining the cost of executing the queries from the query optimizer. Although this tool is specific to Oracle 7, a similar tool for any DBMS that has a cost based optimizer can be developed very easily following the ideas presented in this paper. I...|$|R
40|$|The Hazardous Substances Data Bank (HSDB), {{produced}} and {{maintained by the}} National Library of Medicine (NLM), contains over 4600 records on potentially hazardous chemicals. To enhance information retrieval from HSDB, NLM has undertaken {{the development of an}} <b>automated</b> HSDB <b>indexing</b> protocol as part of its Indexing Initiative. The NLM Indexing Initiative investigates methods whereby <b>automated</b> <b>indexing</b> may partially or completely substitute for human indexing. The posterâ€™s purpose is to describe the HSDB <b>Automated</b> <b>Indexing</b> Project...|$|R
40|$|In {{contrast}} to {{other kinds of}} libraries, software libraries need to be conceptually organized. When looking for a component, the main concern of users is the functionality of the desired component; implementation details are secondary. Software reuse would be enhanced with conceptually organized large libraries of software components. In this paper, we present GURU, a tool that allows automatical building of such large software libraries from documented software components. We focus here on GURU's indexing component which extracts conceptual attributes from natural language documentation. This indexing method is based on words' co-occurrences. It first uses EXTRACT, a co-occurrence knowledge compiler for extracting potential attributes from textual documents. Conceptually relevant collocations are then selected according to their resolving power, which scales down the noise due to context words. This fully <b>automated</b> <b>indexing</b> <b>tool</b> thus goes further than keyword-based tools {{in the understanding of}} a document without the brittleness of knowledge based tools. The indexing component of GURU is fully implemented, and some results are given in the paper...|$|E
40|$|Health and {{functional}} status data {{have been shown}} to have clinical utility in predicting outcome. Various metadata registries in the form of patient self-administered health assessment questionnaires have been incorporated into routine clinical care and clinical research of patients with rheumatic disease. Examples of such health assessment instruments are the Clinical Health Assessment Questionnaire (CLINHAQ) and the Modified Health Assessment Questionnaire (MHAQ). These instruments contain concepts that {{are an integral part of}} the health {{and functional}} status domain. Using an <b>automated</b> <b>indexing</b> <b>tool</b> we examined the clinical content coverage by SNOMED RT and the Unified Medical Language System (UMLS) Metathesaurus for health and functional status concepts identified in the MHAQ and CLINHAQ. Significant differences existed between the overall representational ability of SNOMED and UMLS for concepts identified in the MHAQ (49 %, vs. 77 % respectively, p <. 005) and for concepts identified in the CLINHAQ (30 % vs. 64 % respectively p <. 005). Representational capability by SNOMED-RT and UMLS for concepts in a given health assessment instrument was carried across four semantic classes of "attitudes", "symptoms", "activities", and "social attributes". The conceptual content coverage of health status assessment concepts contained in the MHAQ and CLINHAQ by SNOMED-RT and UMLS was incomplete but better for UMLS with its panoply of vocabulary sources. This observed overall improved representation by UMLS appeared to be due to better representation of concepts in "activities" and "social attributes" semantic classes. Representation of health or functional status concepts in a computerized medical record should be founded on a universally agreed concept model of that domain. Established functional and health status metadata registries can serve as important sources for concepts and candidate classes within that domain...|$|E
40|$|This study {{explores the}} {{potential}} for <b>automated</b> <b>indices</b> related to speech delivery, language use, and topic development to model human judgments of TOEFL speaking proficiency in second language (L 2) speech samples. For this study, 244 transcribed TOEFL speech samples taken from 244 L 2 learners were analyzed using <b>automated</b> <b>indices</b> taken from Coh-Metrix, CPIDR, and LIWC. A stepwise linear regression was used to explain the variance in human judgments of independent speaking ability and overall speaking proficiency. <b>Automated</b> <b>indices</b> related to word type counts, causal cohesion, and lexical diversity predicted 52 % {{of the variance in}} human ratings for the independent speech samples. <b>Automated</b> <b>indices</b> related to word type counts and word frequency predicted 61 % of the variance of the human scores of overall speaking proficiency. These analyses demonstrate that, {{even in the absence of}} indices related to pronunciation and prosody (e. g., phonological accuracy, intonation, and stress), <b>automated</b> <b>indices</b> related to vocabulary size, causality, and word frequency can predict a significant amount of the variance in human ratings of speaking proficiency. These findings have important implications for understanding the construct of speaking proficiency and for the development of automatic scoring techniques...|$|R
40|$|AbstractThe recent {{improvements}} in capabilities of desktop computers and communications networks give {{impetus for the}} development of clinical image repositories {{that can be used for}} patient care and medical education. A challenge in the use of these systems is the accurate indexing of images for retrieval performance acceptable to users. This paper describes a series of experiments aiming to adapt the SAPHIRE system, which matches text to concepts in the UMLS Metathesaurus, for the <b>automated</b> <b>indexing</b> of image reports. A series of enhancements to the baseline system resulted in a recall of 63 % but a precision of only 30 % in detecting concepts. At this level of performance, such a system might be problematic for users in a purely <b>automated</b> <b>indexing</b> environment. However, if the ability to retrieve images in repositories based on content in their reports is desired by clinical users, and no other current systems offer this functionality, then follow-up research questions include whether these imperfect results would be useful in a completely or partially <b>automated</b> <b>indexing</b> environment and/or whether other approaches can improve upon them...|$|R
40|$|This paper {{clinical}} images {{was extremely}} limited. But {{with the ability}} describes a series of experiments aiming to adapt the SAPHIRE system, of modern desktop computers to display high-quality images, which matches text to concepts in the UMLS Metathesaurus, for the <b>automated</b> <b>indexing</b> of image reports. A series of enhancements to the along with large disks to store them on servers and fast baseline system resulted in a recall of 63 % but a precision of only networks to allow their widespread viewing, the door has 30 % in detecting concepts. At this level of performance, such a system been opened to new applications of clinical image repositor- might be problematic for users in a purely <b>automated</b> <b>indexing</b> environ- ies for clinical care and education. For clinical care, these ment. However, if the ability to retrieve images in repositories based collections allow clinicians to compare a current case against on content in their reports is desired by clinical users, and no other previous instances of the suspected finding or diagnosis, current systems offer this functionality, then follow-up research ques- while for education they make the use of images for teaching tions include whether these imperfect results would be useful in a purposes much easier. completely or partially <b>automated</b> <b>indexing</b> environment and/or whether other approaches can improve upon them. # 2001 Elsevier There is, however, a major challenge to the effective use Science (USA) of clinical image repositories, which is that images must be Key Words: natural language processing; <b>automated</b> indexing; properly <b>indexed</b> for accurate retrieval. <b>Automated</b> <b>indexing</b> SAPHIRE; unified medical language system (UMLS); Metathe- of radiology images involves processing either the image saurus; evaluatio...|$|R
40|$|Abstractâ€”Modern {{traceability}} tools employ information re-trieval (IR) {{methods to}} generate candidate traceability links. These methods track textual signs {{embedded in the}} system to establish relationships between software artifacts. However, as software systems evolve, new and inconsistent terminology finds {{its way into the}} systemâ€™s taxonomy, thus corrupting its lexical structure and distorting its traceability tracks. In this paper, we argue that the distorted lexical tracks of the system can be systematically re-established through refactoring, a set of behavior-preserving transformations for keeping the system qual-ity under control during evolution. To test this novel hypothesis, we investigate the effect of integrating various types of refactoring on the performance of requirements-to-code automated tracing methods. In particular, we identify the problems of missing, misplaced, and duplicated signs in software artifacts, and then ex-amine to what extent refactorings that restore, move, and remove textual information can overcome these problems respectively. We conduct our experimental analysis using three datasets from different application domains. Results show that restoring textual information in the system has a positive impact on tracing. In contrast, refactorings that remove redundant information impact tracing negatively. Refactorings that move information among the system modules are found to have no significant effect. Our findings address several issues related to code and requirements evolution, as well as refactoring as a mechanism to enhance the practicality of <b>automated</b> tracing <b>tools.</b> <b>Index</b> Termsâ€”Refactoring, traceability, information retrieval. I...|$|R
40|$|We {{developed}} a prototype information retrieval system which uses advanced {{natural language processing}} techniques to enhance the effectiveness of traditional key-word based document retrieval. The backbone of our system is a statistical retrieval engine which performs <b>automated</b> <b>indexing</b> of documents, then search and ranking in response to user queries...|$|R
5000|$|The library curates the BNCF Thesaurus (...) , a [...] "subject <b>indexing</b> <b>tool</b> {{for various}} types of resources".|$|R
5000|$|PlagScan {{built an}} <b>indexing</b> <b>tool</b> based on Apache Solr and relies on Microsoft Bingâ€™s search index for web documents.|$|R
5000|$|The {{formula for}} calculating the <b>automated</b> {{readability}} <b>index</b> is given below: ...|$|R
40|$|The {{functional}} {{characteristics of a}} prototype Image DataBase system under development are presented and discussed. This system {{is based on the}} integration of tools and methodologies which support the interactive processing, classification and browsing of medical images, as well as methodologies for the efficient <b>automated</b> <b>indexing,</b> storage and retrieval of such images by content...|$|R
40|$|While {{the problem}} of Content Based Image Retrieval (CBIR) and <b>automated</b> image <b>indexing</b> systems has been widely studied in the past years they still {{represent}} a chal-lenging research field. Indeed capturing high level seman-tics from digital images basing on low level basic descrip-tors remains an issue. A review of existing systems shows that edge descriptors {{are among the most}} popular features. While color features have led to extensive work, edge fea-tures havenâ€™t produced such active research and most cur-rent systems rather rely on completing basic edge informa-tion with other, more computationally expensive features such as texture. In this paper we propose to work on a more accurate edge feature while keeping a relatively low computation cost. We will begin with a review of common edge features used in CBIR and <b>automated</b> <b>indexing</b> sys-tems, we will then explain our Enhanced Fast Hough Trans-form algorithm and the edge descriptor we derived from it. Through a study of computational complexity, we will ex-plain that the computational burden is kept minimal and ex-perimental results using a sample <b>automated</b> <b>indexing</b> sys-tem will show that our new edge feature significantly im-proves over more traditional descriptors...|$|R
40|$|Includes bibliographical {{references}} and index. What are taxonomies? [...] Who are taxonomists? [...] Creating terms [...] Creating relationships [...] Software for taxonomy creation and management [...] Taxonomies for human indexing [...] Taxonomies for <b>automated</b> <b>indexing</b> [...] Taxonomy structures [...] Taxonomy displays [...] Taxonomy planning, design, and creation [...] Taxonomy implementation and evolution [...] Taxonomy {{work and the}} profession...|$|R
40|$|<b>Automated</b> <b>indexing</b> â€“ using a {{computer}} to look at individual documents and assign metadata without a person looking at every documentâ€“ was used to build an interactive online database to store and retrieve pages from a looseleaf resource (i. e., a resource which changes state over time). A database was designed and more than 30, 000 pages in the database were indexed. Digitization, optical character recognition, and computer scripting to extract metadata were the methods used to assign most metadata. In places where the computer program could not assign metadata, a person looking at the document assigned metadata. The index was audited for errors. The computer script and the human indexer had comparable error rates but computer indexing spent much less time per value assigned. It is recommended that <b>automated</b> <b>indexing</b> be considered in indexing projects, especially where {{a large number of}} similar documents are to be indexed...|$|R
5000|$|Cedefop's {{library and}} {{documentation}} service: responsible for European and international references, also provides guidelines for collection development, and the <b>indexing</b> <b>tool</b> European Training Thesaurus ...|$|R
40|$|This paper {{examines}} {{the use of}} three of the dimensions of semantic locality in the UMLS in order to find the MeSH terms most closely related to any given UMLS concept. This work is part of the Indexing Initiative, an ongoing effort of the National Library of Medicine to investigate <b>automated</b> <b>indexing</b> methods as a partial or complete substitute for current indexing practice...|$|R
5000|$|Commercial {{software}} packets {{are available}} for aiding an indexer in building a book index. [...] There are several dedicated, indexing software programs available {{to assist with the}} special sorting and copying needs involved in index preparation. The most widely known include Cindex, Macrex and SkyIndex. TExtract is a hybrid semi-automatic program combining conventional manual <b>indexing</b> with <b>automated</b> <b>indexing</b> features and text linking.|$|R
40|$|Literature <b>indexing</b> <b>tools</b> provide {{researchers}} with {{a means to}} navigate through the network of scholarly scientific articles in a subject domain. We propose that more effective <b>indexing</b> <b>tools</b> may be designed using the links between articles provided by citations. With the explosion {{in the amount of}} scientific literature and with the advent of artifacts requiring more sophisticated indexing, a means to provide more information about the citation relation in order to give more intelligent control to the navigation process is warranted. In order to navigat...|$|R
40|$|Abstract â€” Text {{categorization}} {{is a task}} of automatically sorting a set {{of documents}} into categories from a predefined set. Text categorization also known as text classification. This task has several applications, including <b>automated</b> <b>indexing</b> of scientific articles according to predefined thesauri of technical terms, filing patents into patent directories, spam filtering, identification of document genre etc. In this paper we discuss several techniques of text categorization...|$|R
50|$|Military {{taxonomy}} {{encompasses the}} domains of weapons, equipment, organizations, strategies, and tactics. The use of taxonomies in the military extends beyond its value as an <b>indexing</b> <b>tool</b> or record-keeping template.|$|R
30|$|The {{major benefit}} {{of this work}} is a novel {{approach}} that enables the <b>automated</b> <b>indexing</b> of unconstrained wildlife video. As an additional information, our approach provides the spatial location and complete tracking information for each detection. This makes the approach a sound basis for higher-level analysis tasks, from the automated estimation of group sizes, to the identification of animals, and to the automated recognition of different activities and behaviors.|$|R
40|$|We {{propose a}} simple {{technique}} for extracting camera motion parameters from {{a sequence of}} images. The method can estimate qualitatively camera pan, tilt, zoom, roll, and horizontal and vertical tracking. Unlike most other comparable techniques, the present method can distinguish pan from horizontal tracking, and tilt from vertical tracking. The technique {{can be applied to}} the <b>automated</b> <b>indexing</b> of video and film sequences. (C) 1997 Pattern Recognition Society...|$|R
40|$|Indexing {{the content}} of medical reports {{requires}} that the data to be organized into appropriate data structures which are capable of recording information at various levels. SNOMED {{is presented as a}} data structure suitable for housing medical information and serving as a knowledge base for medical information. Issues related to classification of data and problems related to the computerized processing and <b>automated</b> <b>indexing</b> of natural language are discussed...|$|R
40|$|Calling {{on their}} {{experience}} in developing algorithms and {{software for the}} <b>automated</b> <b>indexing</b> model known as LSI, the organizers of SIAMâ€™s Atlanta meeting made short work {{of what can be}} a frustrating, time-consuming task: organizing the meetingâ€™s contributed paper sessions. By Michael Berry and Jack Dongarra If you have ever been on the organizing committee for a large meeting, you know that organizers encounter many rewards and frustrationsâ€”before, during, and after the meetings. The story we relate here, part of the before side of the 1999 SIAM Annual Meeting, held in Atlanta in May, {{turned out to be a}} terrific experience, one that could become standard procedure for future meetings. One of the pre-meeting chores of the organizing committee is to assemble the contributed paper sessions. This involves reading the abstracts and trying to form groupings of related papers. Having spent several years developing algorithms and software for an <b>automated</b> <b>indexing</b> approach known as Latent Semantic Indexing (or LSI), we decided to automate the grouping of talks, especially since we were running close to the program deadline...|$|R
30|$|The {{differences}} in C-index between BSI and manual PET <b>index,</b> BSI and <b>automated</b> PET 15 <b>index,</b> and manual PET <b>index</b> and <b>automated</b> PET 15 <b>index</b> {{were not statistically}} significant (p[*]=[*] 0.60, 0.89, and 0.75, respectively).|$|R
5000|$|More generally, Andy Hunt and Dave Thomas {{note the}} {{importance}} of version control, dependency management, text <b>indexing</b> <b>tools</b> such as GLIMPSE and SWISH-E, and [...] "drawing a map as you begin exploring." ...|$|R
40|$|Indexing with {{computers}} {{is a far}} cry from indexing with the first <b>indexing</b> <b>tool,</b> the manual card sorter. With the aid of computer-aided lexicography, both <b>indexing</b> and <b>indexing</b> <b>tools</b> can provide standardization, consistency, and accuracy, resulting in greater quality control than ever before. A brief survey of computer activity in indexing is presented with detailed illustrations from NASA activity. Applications from techniques mentioned, such as Retrospective Indexing (RI), can be made to many indexing systems. In addition to improving the quality of indexing {{with computers}}, the improved efficiency with which certain tasks can be done is demonstrated...|$|R
5000|$|In 2010, an <b>automated</b> <b>index</b> {{created by}} the Library of Congress, {{compiled}} all the names mentioned in the respective indices and allows a comprehensive search of all volumes at one time. (Previously, {{the structure of the}} work provided an index in each volume. As the work progressed, supplemental names were added, breaking the alphabetical continuum. Without perusing all the volumes one could never be sure that an article may not have been missed.) ...|$|R
2500|$|An image {{database}} and <b>indexing</b> <b>tool</b> comprising some 13,000 documents scanned chiefly from photocopies of original {{documents from the}} Rare Book & Manuscript Library, Columbia University in the City of New York and approximately 90 other institutions.|$|R
40|$|This paper {{describes}} the Olive project {{which aims to}} support <b>automated</b> <b>indexing</b> of video material by use of human language technologies. Olive is making use of speech recognition to automatically derive transcriptions of the sound tracks, generating time-coded linguistic elements which {{serve as the basis}} for text-based retrieval functionality. The retrieval demonstrator builds on and extends the architecture from the Pop-Eye project, a system applying human language technology on subtitles for the disclosure of video fragments...|$|R
