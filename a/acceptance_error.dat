9|42|Public
40|$|Approved {{for public}} release; {{distribution}} is unlimited. This study evaluates an inexpensive personal computer {{access control system}} that relies on biometric keystroke typing dynamics technology, BioPassword Model 2100 (BioPassword). Enrollment time, verification time, false rejection error rate, false <b>acceptance</b> <b>error</b> rate, and user acceptance were evaluated for this system. The results show that BioPassword provides multilayer security through the inclusion of privilege control, audit functions, passwords, and verification of a personal behavioral characteristic, the rate and variation of typing a given password string. Enrollment and verification times were considered satisfactorily fast. Overall false rejection error rate was 22. 5 %, while false <b>acceptance</b> <b>error</b> rate was 3. 4 %. The false rejection error rate for acceptance {{as a function of}} trial number from one trial to five trials were 4. 4 %, 1. 4 %, O. 7 %, O. 4 %, andO. 3 % respectively. These values were achieved under relatively uncontrolled conditions and should be improved on by using recommendations that are included. Users generally reported satisfaction with the system, which should be acceptable as part of an office automation system when used in conjunction with other standard security measures. BioPassword Model 21 00, Biometric technology, Keystroke Typing Dynamics, False Rejection Error Rate, False <b>Acceptance</b> <b>Error</b> Rate, Enrollment Time, Verification Time[URL] Republic of China NavyLieutenant, United States Nav...|$|E
40|$|Abstract. In speaker verification, a claimed speaker’s {{score is}} {{computed}} to accept or reject the speaker claim. Most {{of the current}} normalisation methods compute the score as {{the ratio of the}} claimed speaker’s and the impostors ’ likelihood functions. Based on analysing false <b>acceptance</b> <b>error</b> occured by the current methods, we propose a fuzzy c-means clustering-based normalisation method to find a better score which can reduce that error. Experiments performed on the TI 46 and the ANDOSL speech corpora show better results for the proposed method. ...|$|E
40|$|Communicated by Qingling Zhang) Abstract. Balanced {{truncation}} {{is one of}} {{commonly used}} methods for model order reduction. Positive-real balanced truncation is a particular BT proce-dure that preserves passivity and stability. In general, Positive-real balanced truncation requires solving two algebraic Riccati equations, whose computa-tional complexity limits its practical use in large-scale systems. This paper discusses the problem of continuous descriptor system model order reduction by the Positive-real balanced truncation method. A new solution has been gotten by the iteration using relaxation factor based on the positive realness lemma. The reduction order dimension can be obtained under the <b>acceptance</b> <b>error</b> bounds for descriptor systems. Finally, a simulation example is carried out to show {{the efficiency of the}} proposed method...|$|E
40|$|Signature {{verification}} {{system is}} to match the tested signature with a claimed signature. This paper proposes time series based for feature extraction method and dynamic time warping for match method. The system made by process of testing 900 signatures belong to 50 participants, 3 signatures for reference and 5 signatures from original user, simple imposters and trained imposters for signatures test. The final result system was tested with 50 participants with 3 references. This test obtained that system accuracy without imposters is 90, 44897959 % at threshold 44 with rejection errors (FNMR) is 5, 2 % and <b>acceptance</b> <b>errors</b> (FMR) is 4, 35102 %, when with imposters system accuracy is 80, 1361 % at threshold 27 with error rejection (FNMR) is 15, 6 % and <b>acceptance</b> <b>errors</b> (average FMR) is 4, 263946 %, with details as follows: <b>acceptance</b> <b>errors</b> is 0, 391837 %, <b>acceptance</b> <b>errors</b> simple imposters is 3, 2 % and <b>acceptance</b> <b>errors</b> trained imposters is 9, 2 %...|$|R
40|$|Abstract. The {{normalisation}} {{method for}} speaker verification proposed {{in this paper}} {{is based on the}} idea of the noise clustering method in fuzzy clustering. The proposed method can reduce false <b>acceptance</b> <b>errors</b> and apply to all current normalisation scores. Experiments performed on the ANDOSL and YOHO speech corpora show better results for the proposed method. ...|$|R
40|$|AbstractÐIn this paper, {{an on-line}} {{signature}} veriÞcation {{scheme based on}} similarity measurement of logarithmic spectrum is proposed. The principal components of the logarithmic spectrum of each signature are extracted. We then compute the similarity of logarithmic spectrum between input signature and the reference template. By comparing the similarity of logarithmic spectrum with the veriÞcation threshold, we can determine {{the authenticity of the}} input signature. Based on the experimentation, the rates of false rejection <b>errors</b> and false <b>acceptance</b> <b>errors</b> are as low as 1. 4 and 2. 8 %, respectively. This demonstrates the e¤ectiveness of the proposed scheme. (1998 Published by Elsevier Science Ltd on behalf of the Pattern Recognition Society. All rights reserved. Signature veriÞcation Similarity measurement Logarithmic spectrum Scatter matrix Principal component 1...|$|R
40|$|Multibiometrics {{provides}} high recognition {{accuracy and}} population coverage by combining different biometric sources. However, some multibiometrics may obtain smaller-than-expected improvement of recognition accuracy if the combined biometric sources are dependent {{in terms of}} a false acceptance by mistakenly perceiving biometric features from two different persons as being from the same person. In this paper, we evaluate whether or not features of multiple fingerprints are statistically independent. By evaluating false <b>acceptance</b> <b>error</b> using matchign scores obtained by Verifinger SDK, we confirmed that these features are dependent in some degree and have no small effect on the FAR obtained by their fusion. Mathematics and Computer Science : Proceedings of Annual Workshop on Mathematics and Computer Science, held at Josai University on March 25 in 2014 / edited by Masatoshi IIDA, Manabu INUMA, Kiyoko NISHIZAW...|$|E
30|$|The {{richness}} of the Futures Map is the special target {{of the first and}} the third criteria of the pragmatic external validity. Using the concept of Ackoff [1], their main function is to avoid the omission error. The omission error is one formulation of the so-called third kind of error in the statistical inference. In the statistical inference, you make the <b>acceptance</b> <b>error</b> if you accept something that you should reject (the type I error). The type II error is the rejection error. You do not accept something that you should accept. In the Futures Map frame, you make the type I or II error if you wrongly either add or reject to add a hypothetical scenario path to the futures map. Using the metaphor of the geographical map, your Futures Map has a type I error if it includes a path that in practice is not available. It has a type II error if your Futures Map does not describe a hypothetical path that is available in the futures landscape.|$|E
40|$|In hot {{stamping}} process, similar die {{is used as}} in cold stamping process but with additional cooling channels. The cooling channel systems are integrated into the die design to control the cooling rate for quenching process of hot blanks. During quenching process, the die is effectively cooled to achieve the optimum cooling rate and homogeneous temperature distribution on hot blanks. In this paper, heuristic method with finite element analysis (FEA) of static analysis and thermal analysis are applied to determine the cooling channel size, pitch size between channels and channel distance to the blanks surface. This static analysis identifies either the tool able to stand the pressure applied or not, while the thermal analysis is to ensure the die obtains the high cooling efficiency with homogenous temperature distribution. In this heuristic method, each parameter of the cooling channels inside the die are optimised and benchmarked with traditional Taguchi method. The {{results showed that the}} heuristic method coincides with Taguchi method even better and achieved the <b>acceptance</b> <b>error</b> between FEA in temperature distributions...|$|E
40|$|In {{this paper}} we propose a new {{formulation}} of minimum verification error training and apply it to the problem of topic verification as an example. In topic verification, a decision is made as to whether a document truly belongs to a particular topic of interest. Such a decision typically depends on a comparison between a model for the desired topic and a model for hackground topics, using a decision threshold. We propose modeling the background topics as a cohort model consisting of a weighted combination of the M closest topics discovered from the training data. The weights and the decision threshold arc optimized using the generalized probabilistic descent algorithm to explicitly minimize the verification error rate, which is defined to he a weighted sum of the Type I (false rejection) and Type 11 (false <b>acceptance)</b> <b>errors...</b>|$|R
40|$|We {{propose a}} novel {{classification}} method to identify boar spermatozoid heads which present an intracellular intensity distribution {{similar to a}} model. From semen sample images, head images are isolated and normalized. We define a model intensity distribution averaging a set of head images assumed as normal by veterinary experts. Two training sets are also formed: one with images {{that are similar to}} the model and another with non-normal head images according to experts. Deviations from the model are computed for each set, obtaining low values for normal heads and higher values for heads assumed as non-normal. There is also an overlapping area. The decision criterion is determined to minimize the sum of the obtained false rejected and false <b>acceptance</b> <b>errors.</b> Experiments with a test set of normal and non-normal head images give a global error of 20. 40 %. The false rejection and the false acceptance rates are 13. 68 % and 6. 72 % respectively. ...|$|R
40|$|A simple {{evolutionarily}} stable strategy (ESS) {{model of}} identity advertisement is presented, which is applicable to many different situations, ranging from parental recognition of young to recognition of kin by workers in social insect colonies that comprise several genetically distinct lineages. The model {{assumes that the}} receiver may respond favourably or unfavourably to the signaller, but that it cannot immediately determine which type of response is appropriate. The signaller, who always benefits by eliciting a favourable response (but is unaware of which type of response is appropriate for the receiver), may choose to reveal or conceal its identity, making the task of discrimination easier or harder for the receiver. The evolutionarily stable outcome of the model depends on the probability that an unfavourable response is appropriate, the relative costs to the receiver of <b>acceptance</b> and rejection <b>errors,</b> and the relative benefits to the signaller of eliciting a favourable response when this is appropriate {{and when it is}} inappropriate for the receiver. High costs of <b>acceptance</b> <b>errors</b> to the receiver, and high benefits of appropriate favourable responses to the signaller, favour provision of distinctive identity cues: so, paradoxically, does a high probability that an unfavourable response is appropriate. However, under a wide range of conditions, selection favours the withholding of signature cues, which prevents discrimination by the receiver. Finally, if the signaller must provide some information to elicit a favourable response, but stands to gain more from such a response when it is undesirable, it may do best to provide partial but incomplete information about its identity...|$|R
40|$|This {{paper is}} {{dedicated}} to the performance analysis of content-based identification using binary fingerprints and constrained list-based decoding. We formulate content-based identification as a multiple hypothesis test and develop analytical models of its performance in terms of probabilities of correct detection/miss and false acceptance for a class of statistical models, which captures the correlation between elements of either the content or its extracted features. Furthermore, {{in order to determine the}} block/codeword length impact on the identification’s accuracy, we analyse exponents of these probabilities of errors. Finally, we develop a probabilistic model, justifying the accuracy of identification based on list decoding by evaluating the position of the queried entry on the output list. The obtained results make it possible to characterize the performance of traditional unique decoding, based on the maximum likelihood for the situations when the decoder fails to produce the correct index. This paper also contains experimental results that confirm theoretical findings. Index Terms Content-based identification, digital fingerprint, constrained list-based decoding, order statistics, miss error exponent, false <b>acceptance</b> <b>error</b> exponent. I...|$|E
40|$|Context {{plays an}} {{important}} role in a discriminator's ability to make appropriate recognition decisions, such as accepting what is acceptable and rejecting what is not acceptable. Previously it was shown that in both honey bees and stingless bees, discriminating workers (guards) make more errors towards conspecific non-nestmates when the guards are removed from the natural hive entrance. However, it may be that guards, in addition to making incorrect recognition decisions, also may adopt non-guarding behaviours. Here, we tested honey bee guards in two contexts (natural versus unnatural) against five types of introduced arthropods (conspecific nestmates and non-nestmates; allospecific wasps, beetles and woodlice), which should be rejected without error. We scored a guard's response as accept, reject, avoid and ignore. Total errors significantly increased from natural to unnatural contexts. Specifically, guards were significantly more likely to make an <b>acceptance</b> <b>error,</b> guarding and accepting both conspecific and allospecific non-nestmates, in the unnatural context. Importantly, guards were significantly more likely to adopt a non-guarding behaviour in the unnatural context, which usually involved ignoring or avoiding, where a guard makes contact but then immediately retreats, the introduced arthropod. Overall, these data demonstrate the context is important. Removing a guard from the home that it protects elicits either incorrect discrimination or, additionally, a complete lack of discriminator behaviour altogether...|$|E
40|$|The {{beaconing}} {{approach is}} the key function in geographic routing to disseminate the location. However, the node mobility is a prominent challenge to the beacon based location broadcasting schemes resulting in high routing overhead. The conventional methods allow some errors on location prediction. As a result, the mobile nodes update their location when the predicted location exceeds the allowable error range. However, the prediction error is more sensible for boundary nodes than adjacent nodes, as the boundary nodes located in the proximity area act as greedy nodes. Consequently, allowing the static prediction-error for all nodes does not efficiently reduce the overhead while maintaining the neighbor list accuracy. To deal with these issues, this work proposes a system called “MObility pattern free Dynamic and Effective Location update” (MODEL) {{for the maintenance of}} the trade-off between overhead and precision. Instead of allowing the static prediction-error, the Dynamic <b>Acceptance</b> <b>Error</b> Rate (DAR) in MODEL dynamically calculates the error range to the boundary and adjacent nodes and enhances the neighbor list accuracy with routing overhead. Due to the sensitivity of boundary nodes to the location being accurate, the MODEL efficiently exploits the fuzzy algorithm to allow a minimum error in predicting location rather than in adjacent nodes. This work simulates the proposed MODEL in NS 2 simulator and compares the performance of the existing Load Balanced-Dynamic Beaconing Greedy Perimeter Stateless Routing (LB-DB-GPSR) ...|$|E
40|$|This paper {{investigates the}} problem that some Arabic names can be written in {{multiple}} ways. When someone searches for only one form of a name, neither exact nor approximate matching is appropriate for returning the multiple variants of the name. Exact matching requires the user to enter all forms of the name for the search, and approximate matching yields names not among the variations of the one being sought. In this paper, we attempt {{to solve the problem}} with a dictionary of all Arabic names mapped to their different (alternative) writing forms. We generated alternatives based on rules we derived from reviewing the first names of 9. 9 million citizens and former citizens of Jordan. This dictionary can be used for both standardizing the written form when inserting a new name into a database and for searching for the name and all its alternative written forms. Creating the dictionary automatically based on rules resulted in at least 7 % erroneous <b>acceptance</b> <b>errors</b> and 7. 9 % erroneous rejection errors. We addressed the errors by manually editing the dictionary. The dictionary can be of help to real world-databases, with the qualification that manual editing does not guarantee 100 % correctness...|$|R
30|$|False <b>acceptance</b> {{rate and}} <b>error</b> {{rejection}} rate: The false acceptance rate (FAR) is {{the probability that}} the wrong key correctly decrypts the original image. The false rejection rate (FRR) is {{the probability that the}} original image cannot be correctly decrypted with the correct key.|$|R
50|$|In February 1891 Thayer {{published}} a lecture {{in which he}} expressed disagreement with the position of Biblical inerrancy, asserting that his own <b>acceptance</b> of various <b>errors</b> of history and science in the Bible did not materially detract from {{his belief in the}} overall soundness of Christianity.|$|R
5000|$|Error and indifferentism are avoided. Caparros et al. {{comments}} that, [...] "this {{is a basic}} criterion, {{expressed in}} [...] 26" [...] which states that participation in worship [...] "which harms {{the unity of the}} Church or involves formal <b>acceptance</b> of <b>error</b> or the danger of aberration in the faith, of scandal and indifferentism, is forbidden." [...] In cases of doubt, the recipient should seek references elsewhere. A Catholic recipient who manifests indifference could not apply the provisions of this canon to licitly receive a sacrament from a non-Catholic minister.|$|R
50|$|Further {{problems}} {{occurred with}} the approval of the ETCS system. So in the course of test and <b>acceptance</b> runs, software <b>errors</b> and the need to adjust the track configuration data have been observed. After the elimination of the errors, authorisation to use the ETCS system could be given on 10 September 2015.|$|R
40|$|We {{describe}} techniques {{based on}} {{natural language generation}} which allow a user to author a document in controlled language for multiple natural languages. The author {{is expected to be}} an expert in the application domain but not in the controlled language or in more than one of the supported natural languages. Because the system can produce multiple expressions of the same input in multiple languages, the author can choose among alternative expressions satisfying the constraints of the controlled language. Because the system offers only legitimate choices of wording, correction is unnecessary. Consequently, <b>acceptance</b> of <b>error</b> reports and corrections by trained authors are non-issues...|$|R
40|$|This {{document}} {{is an early}} draft {{and should not be}} considered as a final version. The authors are non-native English speakers and they ask for your <b>acceptance</b> of any <b>errors</b> in grammar and syntax The views expressed in this article {{are those of the authors}} and do not necessarily reflect the views of th...|$|R
5000|$|Equal {{error rate}} or {{crossover}} error rate (EER or CER): {{the rate at}} which both <b>acceptance</b> and rejection <b>errors</b> are equal. The value of the EER can be easily obtained from the ROC curve. The EER is a quick way to compare the accuracy of devices with different ROC curves. In general, the device with the lowest EER is the most accurate.|$|R
40|$|The <b>acceptance</b> test <b>errors</b> of a {{computer}} software project {{to determine if the}} errors could be detected or avoided in earlier phases of development. GROAGSS (Gamma Ray Observatory Attitude Ground Support System) was selected as the software project to be examined. The development of the software followed the standard Flight Dynamics Software Development methods. GROAGSS was developed between August 1985 and April 1989. The project is approximately 250, 000 lines of code of which approximately 43, 000 lines are reused from previous projects. GROAGSS had a total of 1715 Change Report Forms (CRFs) submitted during the entire development and testing. These changes contained 936 errors. Of these 936 errors, 374 were found during the acceptance testing. These <b>acceptance</b> test <b>errors</b> were first categorized into methods of avoidance including: more clearly written requirements; detail review; code reading; structural unit testing; and functional system integration testing. The errors were later broken down in terms of effort to detect and correct, class of error, and probability that the prescribed detection method would be successful. These determinations were based on Software Engineering Laboratory (SEL) documents and interviews with the project programmers. A summary {{of the results of the}} categorizations is presented. The number of programming errors at the beginning of acceptance testing can be significantly reduced. The results of the existing development methodology are examined for ways of improvements. A basis is provided for the definition is a new development/testing paradigm. Monitoring of the new scheme will objectively determine its effectiveness on avoiding and detecting errors...|$|R
30|$|It is {{deducible}} {{from the}} above discussion that the errors in known support T will reduce the RP of modified-CS. However, the first, under certain number of samples, to recover a sparse vector with ℓ nonzero entries, how many errors in set T can the modified-CS bear? The second, within the <b>acceptance</b> range of <b>errors,</b> whether the modified-CS can guarantee the recoverability? Hereinafter, we consider these problems and reach following results.|$|R
25|$|In October 2006, {{just before}} the D66 party {{congress}} and its 40th anniversary as a party, D66 founder Hans van Mierlo asked the question whether D66 has still political legitimacy. He believes that many errors were made in recent history and that only the <b>acceptance</b> of these <b>errors</b> can provide for any credibility to D66. Van Mierlo has put his support behind party leader Pechtold, who in his view can provide for such credibility.|$|R
40|$|This {{executive}} summary presents {{a summary of}} the findings of all study phases conducted to develop recommendations for the development of specifications for subgrade acceptance based on measured deflections. The rolling wheel deflectomter (RWD), portable truck-mounted deflection measurement systems, and dynamic cone penetrometer (DCP) were utilized on numerous subgrade construction projects between the 1998 and 2001 construction seasons. Comparative nuclear density and soil stiffness gauge readings were also obtained at selected locations on many of the included construction projects. The research findings indicate that deflection test results may be appropriate for identifying areas of poor in-place stability within constructed subgrades. However, deflection testing alone may not provide all of the data necessary to properly differentiate acceptable and non-acceptable subgrade stabilities. It {{is important to note that}} deflection test results are related to the moisturedensity conditions at the time of testing. Soils that show acceptable results (i. e., low deflections) may subsequently weaken due to changes in moisture content, freezing/thawing, etc. In instances where subgrade acceptance is well in advance of base course application, subgrade moisture changes may result in decreased soil support. For those conditions where soil compaction has been conducted at a moisture state near optimum, surface deflections should be correlated to the achieved level of compaction. Based on the deflection data gathered during this research study from test areas which were considered as passing based on visual observations, a deflection acceptance threshold of 1. 50 inches was selected as reasonable to limit associated <b>acceptance</b> <b>errors.</b> For use within project implementations, this threshold value was recommended for use to identify potentially “failed” test locations. It was recommended that the project engineer retain the right to require corrective actions to improve subgrade conditions based on the magnitude and extent of failed readings...|$|R
30|$|The {{setup for}} {{transmittance}} measurement is composed by a calibrated length cell {{and by a}} chopped He-Ne laser. The light transmitted through the cell was measured with a photodiode and a lock-in amplifier. The experimental setup {{was similar to that}} of Ref. [43] with an acceptance angle of the detection system of 7 mrad. With this small <b>acceptance</b> angle the <b>error</b> on the extinction coefficient due to the unavoidable fraction of scattered received power was negligible, and the specific extinction coefficient has been obtained with an error smaller than 0.5 %.|$|R
40|$|We present {{classical}} simulation {{techniques for}} measure once quantum branching programs. For bounded error syntactic quantum branching program of width $w$ that computes a function with error $delta$ {{we present a}} classical deterministic branching program of the same length and width at most $(1 + 2 /(1 - 2 delta)) ^{ 2 w}$ that computes the same function. Second technique is a classical stochastic simulation technique for bounded error and unbounded error quantum branching programs. Our result {{is that it is}} possible stochastically-classically simulate quantum branching programs with the same length and almost the same width, but we lost bounded <b>error</b> <b>acceptance</b> property...|$|R
40|$|In {{the era of}} Basel II a {{powerful}} tool for bankruptcy prognosis is vital for banks. The tool must be precise but also easily adaptable to the bank’s objections regarding the relation of false <b>acceptances</b> (Type I <b>error)</b> and false rejections (Type II error). We explore the suitability of Smooth Support Vector Machines (SSVM), and investigate how important factors such as selection of appropriate accounting ratios (predictors), length of training period and structure of the training sample influence the precision of prediction. Furthermore we show that oversampling can be employed to gear the tradeoff between error types. Finally, we illustrate graphically how different variants of SSVM can be used jointly to support the decision task of loan officers...|$|R
40|$|Numerous {{studies have}} been done about {{creative}} people. Some of these studies point to the importance of cultural differences between countries that might enhance or block creative achievement. The authors study Portuguese people whose work has been recognized as creative in the past 6 years, looking for particularities in their psychological makeup and work style. The current work presents the preliminary results that point to great similarities with the reviewed literature (teamwork, informal relationships, peer feedback, mentors or inspiring figures, action centred, intrinsic motivation, constant dedication, work life balance, match between the challenges and the growing competences, <b>error</b> <b>acceptance,</b> domain knowledge and humility). However the participants also refer that the quality of their work is much more easily recognized abroad...|$|R
40|$|This paper {{deals with}} the problem of {{landmark}} detection for application of Simultaneous Localization and Mapping (SLAM) for agricultural robots. A new method of using a fuzzy rule base for fast evaluation of landmarks based on its measurement error, amount of dynamic nature, need for a landmark and evaluation time has been proposed. The output of the fuzzy system is a multiplier which represents the degree of uncertainty of the landmark. High values of uncertainty results in landmarks being discarded, average values results in revaluation, and smaller values result in <b>acceptance</b> with the <b>error</b> covariance being a product of the multiplier and the measured covariance. Selected landmarks with corresponding error covariance are used to update the Kalman Filter. Simulations show the effectiveness of the fuzzy system in improving the relative position uncertainty...|$|R
40|$|This paper {{attempts}} to advance new understandings of female cinematic agency by interrogating its connection to patterns of cultural colonialism in Australian film. The visual presence of female Aboriginality in contemporary Australian film undermines, in subtle and explicit ways, {{the possibility of}} a truly secure white identity tied to the Australian environment. It does so through the introduction of the complexities of Aboriginal difference, through the subversion of white cinematic narratives and mythologies, and through physical agency and action. In this way, the anti-colonial impulse in the cinema emerges, in films which effectively ‘unearth’ the continuing cinematic metaphors of colonial power. The key narrative and visual link between the analyses of films in this paper is the cinematic metaphor of the homestead as the locus of white belonging, values and ownership. Along the way, the motifs of fire and the the lost female child are considered as fundamental components of an anti-colonial cinematic rhetoric. The term ‘postcolonial’ may be seen to ‘cover all the culture affected by the imperial process from the moment of colonisation to the present day’ (Ashcroft, Griffiths and Tiffin, 1989 : 2) and to challenge or invert the ‘ideological <b>acceptance</b> of <b>error</b> as truth’ (Spivak, 1988 : 109). However, the term ‘anti-colonial’ is used to specifically emphasise Aboriginal female instrumentality throughout this paper. This distinction enables a clear focus on the Aboriginal refusal of colonial influence in the cinema, which cannot be confused by problematic definitions of postcolonial politics and criticism. 16 page(s...|$|R
40|$|In this study, {{we propose}} a discriminative {{training}} algorithm to jointly minimize mispronunciation detection errors (i. e., false rejection and false <b>acceptances)</b> and diagnosis <b>errors</b> (i. e., correctly pinpointing mispronunciations but incorrectly stating {{how they are}} wrong). An optimization procedure, similar to Minimum Word Error (MWE) discriminative training, is developed to refine the ML-trained HMMs. The errors to be minimized are obtained by comparing transcribed training utterances (including mispronunciations) with Extended Recognition Networks [3] which contain both canonical pronunciations and explicitly modeled mispronunciations. The ERN is compiled by handcrafted rules, or data-driven rules. Several conclusions {{can be drawn from}} the experiments: (1) data-driven rules are more effective than hand-crafted ones in capturing mispronunciations; (2) compared with the ML training baseline, discriminative training can reduce false rejections and diagnostic <b>errors,</b> though false <b>acceptances</b> increase slightly due to a small number of false-acceptance samples in the training set. Index Terms: CAPT, mispronunciation detection and diagnosis, discriminative training, data-driven phonological rule extractio...|$|R
40|$|Variations between {{plans and}} reality {{can hardly be}} avoided in {{construction}} projects. These variations may be accepted when they {{are still in the}} acceptance limits, or rejected when they are beyond the acceptance limits. The former is called variation in acceptance and the later is called error. Distinguishing these two in a completed construction work is very important, as rejected variations (errors) may be considered as breaching a contract, thus may have legal implications. The research aims to evaluate the traditional calculation method (MeTrad) and to promote variation <b>acceptance</b> and <b>error</b> approach (MeVE) that is able to draw a clear line whether a particular work to be accepted and rejected. The research was conducted by comparing the results of calculation of the aggregate volume between MeTrad and MeVE. The data was collected from Mauponggo- Puuwada road project in Nangakeo in 2010. The results show that {{there was no difference in}} the volumes calculated using both methods. The volume of Class B aggregate based on MeTrad calculation is 386. 01 m 3 and the volume based on MeVE is 386. 01 m 3; where 16. 67 m 3 of which (3. 34 % of the planned volume) is in the range of acceptance and 369. 34 m 3 of which (73. 94 % of the planned volume) is the volume that is rejected (error). The results suggest that MeTrad cannot distinguish clearly whether the work is within the limits of acceptance or rejection, while on the other hand MeVE can describe in detail parts of the work to be accepted or rejected. Using this MeVE calculation, an assessment of a construction workis expected to be more objective and detailed...|$|R
40|$|The {{desire to}} predict the effort in {{developing}} or explaining the quality of software {{has led to the}} proposal of several metrics. As a step toward validating these metrics, the Software Engineering Laboratory (SEL) has analyzed the software science metrics, cyclomatic complexity, and various standard program measures for their relation to effort (including design through <b>acceptance</b> testing), development <b>errors</b> (both discrete and weighted according to the amount of time to locate and fix), and one another. The data investigated are collected from a project FORTRAN environment and examined across several projects at once, within individual projects and by reporting accuracy checks demonstrating the need to validate a database. When the data comes from individual programmers or certain validated projects, the metrics' correlations with actual effort seem to be strongest. For modules developed entirely by individual programmers, the validity ratios induce a statistically significant ordering of several of the metrics' correlations. When comparing the strongest correlations, neither software science's E metric cyclomatic complexity not source lines of code appears to relate convincingly better with effort than the others...|$|R
