233|10000|Public
25|$|The Agency {{also added}} an <b>Automated</b> <b>Data</b> <b>Processing</b> (ADP) Center on February 19, a Dissemination Center on March 31, and a Scientific and Technical Intelligence Directorate on April 30, 1963. DIA assumed the staff support {{functions}} of the J-2, Joint Staff, on July 1, 1963. Two years later, on July 1, 1965, DIA accepted responsibility for the Defense Attaché System—the last function the Services transferred to DIA.|$|E
25|$|The {{tabulating machine}} is an {{electrical}} device designed {{to assist in}} summarizing information and, later, accounting. The results of a tabulation are electrically coupled with a sorter while displayed on clock-like dials. The concept of <b>automated</b> <b>data</b> <b>processing</b> had been born. In 1890, Herman Hollerith invented the mechanical tabulating machine, a design used during the 1890 Census which stored and processed demographic and statistical information on punched cards.|$|E
2500|$|The manual {{extraction}} of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power {{of computer technology}} has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct [...] "hands-on" [...] data analysis has increasingly been augmented with indirect, <b>automated</b> <b>data</b> <b>processing,</b> aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining {{is the process of}} applying these methods with the intention of uncovering hidden patterns in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.|$|E
50|$|This method can <b>automate</b> <b>data</b> <b>processing</b> {{by using}} pre-defined {{templates}} and configurations. A template in this case, {{would be a}} map of the document, detailing where the data fields are located within the form or document. As compared to the manual data entry process, automatic form input systems are more preferable, since they help reduce the problems faced during manual <b>data</b> <b>processing.</b>|$|R
5000|$|<b>Automated,</b> dynamic, {{real-time}} <b>data</b> <b>processing</b> and visualization capability ...|$|R
50|$|Based in Manchester, New Hampshire, Contact Telecom offers carriers, service providers, {{telecommunication}} companies, Internet service providers, wireless, {{voice over}} IP, competitive local exchange carriers, incumbent local exchange carriers, cable companies and cloud computing providers {{a way to}} manage their invoice and usage data through cost management, revenue assurance and customer care initiatives. Its flagship service, Billing <b>Data</b> Analyzer (BDA), <b>automates</b> <b>data</b> <b>processing</b> to further drive a company's overall profitability models.|$|R
5000|$|Mobilizes communications, <b>automated</b> <b>data</b> <b>processing,</b> {{and combat}} {{logistics}} for United States Northern Command (USNORTHCOM) commanders.|$|E
50|$|Yushchenko {{worked on}} {{probability}} theory, algorithmic languages and programming languages, and developing methods of <b>automated</b> <b>data</b> <b>processing</b> systems.|$|E
50|$|The 153d Command and Control Squadron (153 CACS) mobilizes communications, <b>automated</b> <b>data</b> <b>processing,</b> {{and combat}} {{logistics}} for United States Northern Command (USNORTHCOM) commanders.|$|E
40|$|The go-to guidebook for {{deploying}} Big Data solutions with Hadoop Today's enterprise architects need {{to understand}} how the Hadoop frameworks and APIs fit together, and how they can be integrated to deliver real-world solutions. This book is a practical, detailed guide to building and implementing those solutions, with code-level instruction in the popular Wrox tradition. It covers storing data with HDFS and Hbase, <b>processing</b> <b>data</b> with MapReduce, and <b>automating</b> <b>data</b> <b>processing</b> with Oozie. Hadoop security, running Hadoop with Amazon Web Services, best practices, and automating Hadoop processes...|$|R
30|$|Companies {{specializing in}} the {{development}} of commercial kits of peptides for SRM analysis have emerged in the market. Peptides not interfering with components of the biological matrix as well as peptides used in retention time calibration are prerequisites for commercial kits. Commercial kits are typically delivered with specialized software <b>automating</b> <b>data</b> <b>processing</b> such as Skyline (MacCoss Lab of Biological Mass Spectrometry, University of Washington, Seattle, WA, USA) [7], which does not require the studying of complex software documentation and is focused on assay development and subsequent results analysis in “plug-and-play” mode.|$|R
40|$|This article {{presents}} the Internet Query Language (IQL) tool – an open-source software for <b>automated</b> <b>data</b> mining, <b>processing,</b> presentation and formatting. It automates complex multi-threaded tasks through a specialized query language adapted to hierarchical data while employing <b>data</b> <b>processing</b> techniques {{common to the}} internet medium and familiar to current developers. The tool is modular and it's various components can be employed for other use cases...|$|R
50|$|The JCS J-3 Command Systems Operations Division {{manages the}} {{operations}} of the J-3 information system facilities and maintains operational control of the Crisis Management <b>Automated</b> <b>Data</b> <b>Processing</b> System for the National Military Command Center.|$|E
50|$|As {{initially}} established, WWMCCS was {{an arrangement}} of personnel, equipment (including <b>Automated</b> <b>Data</b> <b>Processing</b> equipment and hardware), communications, facilities, and procedures employed in planning, directing, coordinating, and controlling the operational activities of U.S. military forces.|$|E
50|$|More recently, {{his work}} has been {{focussed}} on Dark Web and <b>automated</b> <b>data</b> <b>processing</b> technologies and {{has been working with}} DARPA and NASA JPL on the Memex project which involves data discovery and dissemination from the Dark Web.|$|E
40|$|Abstract: This article {{presents}} the Internet Query Language (IQL) tool – an open-source software for <b>automated</b> <b>data</b> mining, <b>processing,</b> presentation and formatting. It automates complex multi-threaded tasks through a specialized query language adapted to hierarchical data while employing <b>data</b> <b>processing</b> techniques {{common to the}} internet medium and familiar to current developers. The tool is modular and it's various components can be employed for other use cases. Key-Words: IQL, data-mining, data-processing, hierarchy, parallelis...|$|R
40|$|We present our protein-protein {{interaction}} (PPI) network visualization system RobinViz (reliability-oriented bioinformatic networks visualization). Clustering the PPI {{network based}} on gene ontology (GO) annotations or biclustered gene expression data, providing a clustered visualization model {{based on a}} central/peripheral duality, computing layouts with algorithms specialized for interaction reliabilities represented as weights, completely <b>automated</b> <b>data</b> acquisition, <b>processing</b> are notable features of the system...|$|R
40|$|Tumor size or {{volume is}} often the primary {{endpoint}} in preclinical efficacy studies of anticancer drugs. Efficient and accurate measurement of such tumors is crucial to rapid evaluation of novel drug candidates. Currently available techniques for acquiring high-throughput data on tumor volume are time-consuming and prone to various inaccuracies and errors. The laser-scanning technology we describe here provides a convenient, high-throughput system for tumor measurement that reduces interoperator variability and bias while providing <b>automated</b> <b>data</b> collection, <b>processing</b> and analysis...|$|R
50|$|Unipept {{consists}} of a web application and a stand-alone command line tool. The web application uses interactive data visualizations to explore datasets. The command line tool contains the same functionality, but is designed for use in <b>automated</b> <b>data</b> <b>processing</b> pipelines.|$|E
50|$|Dedicated {{physical}} spaces may {{be provided}} in the control center for certain mission support roles, such as flight dynamics and network control, or these roles may be handled via remote terminals outside the control center. As on-board computing power and flight software complexity have increased, there is a trend toward performing more <b>automated</b> <b>data</b> <b>processing</b> on board the spacecraft.|$|E
50|$|The Department of Veterans Affairs (VA) has had <b>automated</b> <b>data</b> <b>processing</b> systems, {{including}} extensive {{clinical and}} administrative capabilities, within its medical facilities since before 1985. Initially called the Decentralized Hospital Computer Program (DHCP) information system, DHCP was enshrined as {{a recipient of}} the Computerworld Smithsonian Award for best use of Information Technology in Medicine in 1995.|$|E
40|$|The {{generalized}} {{data about}} directions of information technologies use in sports practice are {{cited in the}} article. The opportunities of information-methodical system "Athlete" use in system of bodybuilder's preparation are examined in details. Program "Athlete" {{is a collection of}} tools, methods of <b>automated</b> <b>data</b> collection, <b>processing,</b> storage and use of information, with access to various information resources, including Internet. The prospects of computer technologies use in educational-training process of the sportsmen specializing in bodybuilding are designated...|$|R
40|$|Traditional {{methods for}} {{deriving}} performance models of customer flow in real-life systems are manual, timeconsuming {{and prone to}} human error. This paper proposes an <b>automated</b> four-stage <b>data</b> <b>processing</b> pipeline which takes as input raw high-precision location tracking data and which outputs a queueing network model of customer flow. The pipeline estimates both {{the structure of the}} network and the underlying interarrival and service time distributions of its component service centres. We evaluate our method’s effectiveness and accuracy in four experimental case studies...|$|R
40|$|Shells, as command interpreters, are the {{classical}} way {{for humans to}} interact with computing systems, and modern shell features have extended this basic functionality with higher-level programming language constructs. Although implementing compilation in these shell languages is generally unprofitable and intractable, many advantages, such as isolation, filesystem abstraction, security, portability, parallelization and locality optimization are possible, using standard compilation techniques. While compilation is not possible for all scripts, there exist shell scripts of a class that are, in practice, both profitable and tractable to compile and execute. This class of scripts is prevalent in the scientific computing community, where scripts are commonly used to <b>automate</b> <b>data</b> <b>processing</b> sequences. We describe a prototype shell compilation and implementation for these scripts, noting advantages and challenges, and illustrating the significant performance potential available. Our results show that shell compilation is a viable means of automatically identifying and exploiting high-level program parallelism using existing sequential script specification and without requiring reimplementation. Key words: programming languages, shell languages, domain-specific language, command interpreters, optimizing compiler, locality optimization, paralle...|$|R
50|$|Behind the {{projection}} room is the operations team area containing the {{automatic data processing}} equipment and seats and console work areas for 29 staff members. The consoles are configured {{to provide access to}} or from the <b>automated</b> <b>data</b> <b>processing,</b> automatic switchboard, direct access telephone and radio circuits, direct ("hot") lines, monitor panel for switchboard lines, staff, and operator inter-phone and audio recorder.|$|E
50|$|The Agency {{also added}} an <b>Automated</b> <b>Data</b> <b>Processing</b> (ADP) Center on February 19, a Dissemination Center on March 31, and a Scientific and Technical Intelligence Directorate on April 30, 1963. DIA assumed the staff support {{functions}} of the J-2, Joint Staff, on July 1, 1963. Two years later, on July 1, 1965, DIA accepted responsibility for the Defense Attaché System—the last function the Services transferred to DIA.|$|E
5000|$|The NMCC {{includes}} several war rooms, uses more than 300 operational personnel, and houses the United State's {{side of the}} 1963 Moscow-Washington hotline which links the Pentagon and the Kremlin. Data into the NMCC includes warning [...] "on the size, origin, and targeting of an attack" [...] (e.g., from the NORAD/NORTHCOM Command Center). The NMCC's Crisis Management <b>Automated</b> <b>Data</b> <b>Processing</b> Systems are under control of the J-3 Command Systems Operations Division.|$|E
40|$|This bachelor's thesis {{presents}} {{the design and}} implementation of a framework for <b>automated</b> multimedia <b>data</b> <b>processing</b> in Python. The created framework enables the user to decompose the editing process into atomic tasks, which can be run in a computer cluster, and handles their interconnection including automatic dependency resolution. The framework's functionality has been successfully tested using tasks to process recordings of lectures. Also, a web interface has been created, which enables the administrator to manage the system and lecturers to submit lecture recordings for processing...|$|R
40|$|XDSAPP is a Tcl Tk based {{graphical}} {{user interface}} for the easy and convenient <b>processing</b> of diffraction <b>data</b> sets using XDS. It provides easy access to all XDS functionalities, <b>automates</b> the <b>data</b> <b>processing</b> and generates graphical plots of various data set statistics provided by XDS. By incorporating additional software, further information on certain features of the data set, such as radiation decay during data collection or the presence of pseudo translational symmetry and or twinning, can be obtained. Intensity files suitable for CCP 4, CNS and SHELX are generate...|$|R
40|$|Comparative Case Analysis (CCA) is an {{important}} tool for criminal investigation and crime theory extraction. It analyzes the commonalities and differences between a collection of crime reports {{in order to understand}} crime patterns and identify abnormal cases. A big challenge of CCA is the <b>data</b> <b>processing</b> and exploration. Traditional manual approach can no longer cope with the increasing volume and complexity of the data. In this paper we introduce a novel visual analytics system, Spherical Similarity Explorer (SSE) that <b>automates</b> the <b>data</b> <b>processing</b> process and provides interactive visualizations to support the data exploration. We illustrate the use of the system with uses cases that involve real world application data and evaluate the system with criminal intelligence analysts...|$|R
50|$|Beginning in 1978, HQ AFCS {{was given}} {{responsibility}} for the design, operation and maintenance of Air Force <b>automated</b> <b>data</b> <b>processing</b> systems. The Air Force Chief of Staff directed {{the integration of the}} communications, data automation and office automation disciplines Air Force-wide {{to take advantage of the}} then-emerging technologies. AFCS undertook major efforts to plan replacement of older early 1960s computer systems in base support functions such as supply, maintenance, personnel and finance.|$|E
5000|$|The term [...] "near real-time" [...] or [...] "nearly real-time" [...] (NRT), in {{telecommunications}} and computing, {{refers to the}} time delay introduced, by <b>automated</b> <b>data</b> <b>processing</b> or network transmission, between the occurrence of an event {{and the use of}} the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.|$|E
5000|$|When the National Security Agency (NSA) {{was formed}} in 1952, Rowlett became chief of cryptanalysis. The primary problem facing {{research}} and development in the post-war period was development of high-speed processing equipment. Kullback supervised a staff of about 60, including such innovative thinkers in <b>automated</b> <b>data</b> <b>processing</b> development as Leo Rosen and Sam Snyder. His staff pioneered new forms of input and memory, such as magnetic tape and drum memory, and compilers to make machines truly [...] "multi-purpose." [...] Kullback gave priority to using computers to generate communications security (COMSEC) materials.|$|E
40|$|MS-based {{proteomics}} produces {{large amounts}} of mass spectra that require processing, identification and possibly quantification before interpretation can be undertaken. High-throughput studies require automation of these various steps, {{and management of the}} data in association with the results obtained. We here present ms_lims ([URL] a freely available, open-source system based on a central database to <b>automate</b> <b>data</b> management and <b>processing</b> in MS-driven proteomics analyses...|$|R
40|$|This paper {{experimentally}} {{evaluates the}} assessment of precise point positioning (PPP) using the Natural Resources Canada (NRCan) Ultra-Rapid GPS products {{to serve as a}} short latency time-transfer tool to assist timing laboratories in operational maintenance of frequency standards and time scale dissemination. An <b>automated</b> <b>data</b> exchange and <b>processing</b> system has been set up to serve the international community for efficient, nearly real-time clock comparison and monitoring purposes...|$|R
40|$|AbstractThe {{investigation}} {{presented in}} this paper aims on a quantitative analysis of spatter formation in laser beam welding with superposed beam oscillation. After a discussion of design space limitations, which result from the scanner dynamics and theoretical considerations on the welding process itself, an optimal experimental design is created. By the use of high speed camera imaging, spatters were captured during statistically designed welding experiments and correlations between the number of spatters and the welding parameters have been derived. To evaluate the spatter characteristics in the high speed videos, a state space approach was applied, which is based on <b>automated</b> image <b>data</b> <b>processing...</b>|$|R
