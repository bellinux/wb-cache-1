6|49|Public
40|$|Objective: To {{identify}} the overall survival and prognostic factors of malignant lymphoma {{of the oral}} cavity and the maxillofacial region. Study Design: Clinical records data were obtained {{in order to determine}} overall survival at 2 and 5 years, the individual survival percentage of each possible prognostic factor with the <b>actuarial</b> <b>technique,</b> and the survival regarding the possible prognostic factors with the <b>actuarial</b> <b>technique</b> and the Log-rank and Cox's regression tests. Results: Of 151 subjects, an overall survival was 60 % at 2 years, and 45 % at 5 years. The multivariate analysis demonstrated statistically significant differences for clinical stage (p= 0. 002), extranodal involvement (p= 0. 030), presence of human immunodeficiency virus (p= 0. 032), and presence of Epstein-Barr virus (p= 0. 010). Conclusion: The advanced clinical stage and the larger number of involved extranodular sites are related to a lower overall survival, as well as, the presence of previous infections such as the human immunodeficiency and the Epstein-Barr virus...|$|E
40|$|The current article {{addresses}} the ongoing {{debate about whether}} individuals can perform as well as actuarial techniques when confronted with real world, consequential decisions. A single experiment tested the ability of participants (N 215) and an <b>actuarial</b> <b>technique</b> to accurately predict the residential locations of serial offenders based on information about where their crimes were committed. Results indicated that participants introduced to a ‘Circle ’ or ‘Decay ’ heuristic showed a significant improvement in the accuracy of predictions, and that their post-training performance {{did not differ significantly}} from the predictions of one leading <b>actuarial</b> <b>technique.</b> Further analysis of individual performances indicated that approximately 50 % of participants used appropriate heuristics that typically led to accurate predictions even before they received training, while nearly 75 % improved their predictive accuracy once introduced to either of the two heuristics. Several possible explanations for participants’ accurate performances are discussed and the practical implications for police investigations are highlighted. Copyright # 2004 John Wiley & Sons, Ltd. A more efficient use of resources and an increase in offender apprehension are the rewards for the police decision-maker who is able to predict accurately the location of an offender’s residence from information about where his or her crimes were committed...|$|E
40|$|I n {{this issue}} of Peritoneal Dialysis Internatianal, Jindal and Hirsch report an 86 % {{technique}} survival at three years in their home peritoneal dialysis program (1). There were 155 patients included in an analysis, of which 150 were on continuous ambulatory peritoneal dialysis (CAPD) and 5 were on lifetime cycler therapy. Cardiovascular disease was present in 33 % and diabetes was present in 35 % of these patients. The mean age at study entry was 53 years. The period of study was from 1987 to 1990, {{and a variety of}} connection systems were used. There was a gradual transition to disconnect devices after 1988. Of 14 failures, 8 were related to peritonitis. The <b>actuarial</b> <b>technique</b> survival as calculated represents the percentage of patients at risk who were still on home peritoneal dialysis; the events of death, transplant, and transfer out of the program were considered losses to risk. This may be the highest technique survival rate fo...|$|E
50|$|His {{work has}} {{attempted}} to predict criminal recidivism using <b>actuarial</b> <b>techniques.</b>|$|R
50|$|The Diploma in <b>Actuarial</b> <b>Techniques</b> {{is sent to}} {{students}} on completion of all Core Technical stage subjects: CT1, CT2, CT3, CT4, CT5, CT6, CT7, CT8 and CT9.|$|R
40|$|AbstractLife {{insurers}} use {{accounting and}} <b>actuarial</b> <b>techniques</b> to smooth reporting of firm assets and liabilities, seeking to transfer surpluses in good years to cover benefit payouts in bad years. Yet these techniques {{have been criticized}} as they {{make it difficult to}} assess insurers’ true financial status. We develop stylized and realistically-calibrated models of a participating life annuity, an insurance product that pays retirees guaranteed lifelong benefits along with variable non-guaranteed surplus. Our goal is to illustrate how accounting and <b>actuarial</b> <b>techniques</b> for this type of financial contract shape policyholder wellbeing, along with insurer profitability and stability. Smoothing adds value to both the annuitant and the insurer, so curtailing smoothing could undermine the market for long-term retirement payout products...|$|R
40|$|The {{single most}} {{important}} number in the accounts of a non-life insurance company {{is likely to be}} the estimate of the outlying liabilities. Since non-life insurance is a major part of our financial industry (amounting to up to 5 % of BNP in western countries), it is perhaps surprising that mathematical statisticians and experts of operational research (the natural experts of the underlying problem) have left the intellectual work on estimating this number to actuaries. This paper establishes this important problem in a vocabulary accessible to experts of operations research and mathematical statistics and it can be seen as an open invitation to these two important groups of scholars to join this research. The paper introduces a number of new methodologies and approaches to estimating outstanding liabilities in non-life insurance. In particular it reformulates the classical <b>actuarial</b> <b>technique</b> as a histogram type of approach and improves this classical technique by replacing this histogram by a kernel smoother...|$|E
40|$|Stochastic {{models for}} {{triangular}} data are derived {{and applied to}} claims reserving data. The standard <b>actuarial</b> <b>technique,</b> the so-called "chain-ladder technique" is given a sound statistical foundation and considered as a linear model. This linear model, the '"Chain Ladder Linear Model" is extended to encompass Bayesian, empirical Bayes and dynamic estimation. The empirical Bayes results are given a credibility theory interpretation, and {{the advantages and disadvantages}} of the various approaches are highlighted. Finally, the methods are extended to two-dimensional systems and results based on classical time series and Kalman filtering theory are produced. The empirical Bayes estimation results are very useful, practically, and can be compared to the Kalman filter estimates. They have the advantage that no prior information is required: the Kalman filter method requires the state and observation variances to be specified. For illustration purposes the estimates from the empirical Bayes procedure are used. The empirical Bayes results can also be compared with credibility theory estimators, although they retain the general statistical advantages of the linear modelling approach. For the classical theory, unbiased estimates of outstanding claims, reserves and variances are derived, and prediction intervals for total outstanding claims are produced. Maximum likelihood theory is utilised to derive the distributions of quantities relating to the column parameters which have actuarial interpretations. The row totals are also considered. Bayesian estimates of similar quantities are derived for the methods based on Bayes theory...|$|E
40|$|The {{purpose of}} this study was to {{determine}} the survival and prognostic factors of patients with diffuse large B-cell lymphoma (DLBCL) of the oral cavity and maxillofacial region. Retrospectively, the clinical records of patients with a primary diagnosis of DLBCL of the oral cavity and maxillofacial region treated at the A. C. Camargo Hospital for Cancer, S&# 227;o Paulo, Brazil, between January 1980 and December 2005 were evaluated to determine (A) overall survival (OS) at 2 and 5 years and the individual survival percentage for each possible prognostic factor by means of the <b>actuarial</b> <b>technique</b> (also known as mortality tables), and the Kaplan Meier product limit method (which provided the survival value curves for each possible prognostic factor); (B) prognostic factors subject to univariate evaluation with the log-rank test (also known as Mantel-Cox), and multivariate analysis with Cox's regression model (all the variables together). The data were considered significant at p &# 8804; 0. 05. From 1980 to 2005, 3513 new cases of lymphomas were treated, of which 151 (4. 3 %) occurred in the oral cavity and maxillofacial region. Of these 151 lesions, 48 were diffuse large B-cell lymphoma, with 64 % for OS at 2 years and 45 % for OS at 5 years. Of the variables studied as possible prognostic factors, multivariate analysis found the following variables have statistically significant values: age (p = 0. 042), clinical stage (p = 0. 007) and performance status (p = 0. 031). These data suggest that patients have a higher risk of mortality if they are older, at a later clinical stage, and have a higher performance status. </p...|$|E
40|$|This study aims to {{question}} the assurances companies establishment in Porto in the liberal period {{in the context of}} the <b>actuarial</b> <b>techniques</b> development and its generalization in the maritime, fire and life branches of assurance. After giving a general outline of the assurance practices in Porto, we proceed to recapitulate some aspects of Segurança, Garantia and Previdente companies, seen as case studies...|$|R
40|$|Motivation. There is {{a growing}} need for effective, {{practical}} methods of operational risk analysis in all industries. Risk professionals are learning to develop of business-unit level risk distributions, combine those distributions into an aggregate risk model, and use that aggregate risk model to either assign risk charges back to the business units, or to evaluate cost-benefit of mitigation strategies. Operational risk modeling is structurally similar to actuarial risk modeling. The operational risk community will benefit from learning <b>actuarial</b> <b>techniques</b> {{that can be applied}} to operational risk modeling. Method. First, the paper will outline how operational risk management is similar to an internal insurance program. Second, it will discuss an internal risk modeling framework that ties together risk exposure, likelihood, severity, and correlation into an aggregate loss model. Finally, using the model output, it will present several methods to transparently reward risk mitigation efforts and attribute risk costs to their sources. Conclusions. This paper will demonstrate the potential synergies from applying <b>actuarial</b> <b>techniques</b> to operational risk analysis. It will also demonstrate practical <b>techniques,</b> grounded in <b>actuarial</b> science, to solve operational risk management problems such as risk cost allocation and risk mitigation cost-benefit analysis...|$|R
50|$|No Fault Automobile Insurance. A {{driving force}} in the {{introduction}} of a pure no-fault automobile insurance system (for bodily injuries only) in Israel (1976), Kahane conducted the first feasibility study and the setting of the initial rate structure for this line of business, and provided counsel to a parliamentary committee that was studying the topic. Later, {{he was in charge of}} determining the loss reserves for this line by employing modern and sophisticated <b>actuarial</b> <b>techniques</b> that have been the key for setting no-fault automobile insurance rates in Israel for three decades.|$|R
40|$|Despite recent gains, the U. S. remains behind {{most other}} {{affluent}} countries in life expectancy. Even within the U. S., {{the gap between}} the life expectancies of Caucasians and African-Americans remains significant. At the same time, firearm deaths in the U. S. far exceed peer nations, and disproportionately affect African-American males. In this Issue Brief, Dr. Lemaire explores whether deaths from firearms explain some of these international and racial disparities in life expectancy. He uses <b>actuarial</b> <b>techniques</b> to calculate the “cost” of firearm deaths in the U. S., both in terms of reduced life expectancy and increased life insurance premiums...|$|R
50|$|In 2015, John Barry Forman and Michael J. Sabin, using modern <b>actuarial</b> <b>techniques</b> to {{calculate}} fair transfer payments when participants are different ages {{and have made}} different contributions, proposed a new structure of pension plan on the tontine model, through which large employers could provide retirement income for their employees. They argued that tontine pensions would have two major advantages over traditional pensions, as they would always be fully funded, and the plan sponsor would {{not be required to}} bear the investment and actuarial risks. Similar arguments were put forward in the same year by Moshe Milevsky.|$|R
50|$|The chain-ladder or {{development}} {{method is}} a prominent <b>actuarial</b> loss reserving <b>technique.</b>|$|R
40|$|Before {{applying}} <b>actuarial</b> <b>techniques</b> {{to determine}} different subportfolios and adjusted insurance premiums for contracts {{that belong to}} {{a more or less}} heterogeneous portfolio, e. g. using credibility theory, it is worthwhile performing a statistical analysis on the relevant factors influencing the risk in the portfolio. Also the distributional behaviour of the Portfolio should be examined. In this paper such a programme is presented for car insurance data using logistic regression, correspondence analysis, and statistical techniques from survival analysis. The specific mechanisms governing large claims in such portfolios will also be described. This work is based on a representative sample from Belgian car insurance data from 1989. status: publishe...|$|R
40|$|Increases in NASA {{mission costs}} {{have led to}} {{analysis}} of the causes and magnitude of historical mission overruns as well as mitigation and prevention attempts. This paper hypothesizes that one cause is that the availability of reserves may reduce incentives to control costs. We draw a comparison to the insurance concept of moral hazard, and we use <b>actuarial</b> <b>techniques</b> {{to better understand the}} increase in mission costs due to the availability of reserves. NASA's CADRe database provided the data against which we tested our hypothesis and discovered that there is correlation between the amount of available reserves and project overruns, particularly for mission hardware cost increases. We address {{the question of how to}} prevent reserves from increasing mission spending without increasing cost risk to projects...|$|R
40|$|Thispaper {{discusses}} {{methods and}} data {{that can be used}} to quantify insurers’potential liabilities arising from pollution (as specifically definedl. It provides background information on the genesis of the liabilities and then discusses why traditional <b>actuarial</b> <b>techniques</b> fail in analyzing the problem and why analyses that rely on analogies to asbestos are weak. It outlines a typical analysis, including both aggregate quantification techniques and a more detailed model of the potential liabilities. It then comments on the critical issues involved in modelling reported claims and IBNR, data requirements andproblems, and reinsurance issues. A list of references and a discussion of pollution claims database issues are also included. Submitted in response to the 1993 CAS call for discussion papers on environmental liability and other mass action reserving topics...|$|R
40|$|We {{consider}} evaluation {{methods for}} payoffs with an inherent financial risk as encountered for instance for portfolios held by {{pension funds and}} insurance companies. Pricing such payoffs in a way consistent to market prices typically involves combining <b>actuarial</b> <b>techniques</b> with methods from mathematical finance. We propose to extend standard actuarial principles by a new market-consistent evaluation procedure which we call "two-step market evaluation. " This procedure preserves the structure of standard evaluation techniques and has many other appealing properties. We give a complete axiomatic characterization for two-step market evaluations. We show further that in a dynamic setting with continuous stock prices every evaluation which is time-consistent and market-consistent is a two-step market evaluation. We also give characterization results and examples in terms of g-expectations in a Brownian-Poisson setting...|$|R
40|$|Abstract: Generalized Linear Model [GLM] {{theory is}} a {{commonly}} accepted framework for building insurance pricing and scoring models. A helpful {{feature of the}} GLM framework is the “offset ” option. An offset is a model variable with a known or pre-specified coefficient. This paper presents several sample applications of offsets in property-casualty modeling applications. In addition, we will connect the offset option with more traditional <b>actuarial</b> <b>techniques</b> such as exposure and premium adjustments. A recurring theme of the discussion is that actuarial modelers have at their disposal several conceptually related techniques {{that can be used}} to eliminate the impact of variables that (for whatever reason) are not intended for inclusion in a model, despite the fact that they might be correlated with both the target variable and other predictive variables. Examples discussed in this paper include a class plan analysis as well as a tier scoring application. Sample SAS code for fitting GLMs will be provided in the body of the paper...|$|R
40|$|In {{the present}} work we study the {{alternatives}} in the valuation of technical provisions under the Solvency II. We are concerned on set of proposals which {{are about the}} usage of proxies released in the fourth Quantitative Impact Study. The proxy is an approach for the calculation of the best estimate for those companies which {{do not have the}} sufficient statistical data in order to carry out a proper actuarial calculation. This work is based on application of proxy to the traditional <b>actuarial</b> <b>techniques.</b> There is also a description of supervisors procedure how to derive market parameters based on claims development scheme of each insurance company. The next chapter is focused on model error calculation and gives us an information whether the proxy method is proper and reliable. There is also a need of risk margin calculation to meet the insurers obligations. This work also enumerates a number of risk margin's proxies. Department of Probability and Mathematical StatisticsKatedra pravděpodobnosti a matematické statistikyFaculty of Mathematics and PhysicsMatematicko-fyzikální fakult...|$|R
40|$|The Basel Capital Accord II {{establishes}} a new {{framework for the}} management of risks in the banking sector, giving a particular importance to the development of internal models of credit risk measurement. This sort of risk has been traditionally measured through statistical models and <b>actuarial</b> <b>techniques,</b> nevertheless the new competitive environment, characterised by the presence of multiple and incomplete information and the interaction of dynamic market forces, reduces its accuracy and efficiency. Thus way, it is necessary to develop new models which must fit two important characteristics: to get a low rate of error and to generate clear decision rules in order to be understood by managers. To get it, in this work a new model is proposed, based on both symbolic Machine Learning paradigms and new boosting techniques, which improve final results through the combination of multiple simple but specialised models. This approach is applied to a well-known credit risk database, obtaining optimal results with a minimum cost...|$|R
40|$|This text {{outlines}} basic property/casualty insurance ratemaking {{concepts and}} techniques. It {{is intended to}} be a single educational text to prepare actuarial candidates practicing around the world for basic ratemaking. A key concept in the text is the fundamental insurance equation, which balances the expected future income and outgo of an insurance operation. Various chapters discuss the individual components of the equation (e. g., premium, loss, expense, profit), and other chapters review how to assess whether the equation is in balance in the aggregate and by customer segment. The text focuses on quantitative analysis as well as practical considerations in the ratemaking process. Finally, the text provides consistent definitions of terms and examples that underlie the ratemaking techniques discussed. i FOREWORD Ratemaking is a key driver of property and casualty (P&C) insurance profitability and hence a primary actuarial responsibility. Actuaries employ a variety of ratemaking techniques depending on specific circumstances. For example, techniques used to price short-tailed lines of insurance (e. g., personal automobile) are different than techniques used in long-tailed lines (e. g., workers compensation). Even within the same insurance product, <b>actuarial</b> <b>techniques</b> may differ due to regulatory requirements an...|$|R
40|$|The {{purpose of}} the I 996 Geo-Coding Survey {{was to assess the}} current usage of geocoded data in the {{casualty}} actuarial profession, and to foster development of new <b>actuarial</b> <b>techniques</b> using such data. A total of 152 CAS members returned a completed survey. The following are the key findings of the Geo-Coding Survey:. Nearly four in ten (36. 8 %) respondents reported they were currently using geo-coded data for the monitoring of catastrophe exposures, while nearly one-third (30. 9 %) reported current use in the definition of rating territories. Over one in five (2 I. 1 %) respondents reported they were currently using geo-coded data for the determination of unexpected insurance costs for specific locations, and the same number reported use for marketing/underwriting. Close to half (48 %) of all respondents reported they were not currently using geo-coded data for any purpose. Zip code data was named most frequently by respondents when asked the type of geographic data they were using for listed purposes. Zip code data was the most popular response for six of the seven listed purposes, such as the monitoring of catastrophe exposures or the definition of rating territories. ...|$|R
40|$|Recently, {{several authors}} have {{suggested}} that only by incorporating findings from actuarial risk assessment instruments (ARAIs) can mental health experts provide evidence-based testimony in mental health commitment hearings. Determining eligibility for involuntary hospitalization seems like an appropriate, natural, obvious application of ARAIs. Similar instruments are used frequently in decision-making about sex offender commitments, where (as with mental health commitment) social policy ostensibly aims {{to protect the public}} from harmful acts by persons with mental abnormalities. Also, all evidence suggests that <b>actuarial</b> <b>techniques</b> for judging dangerousness are superior to other methods of assessing the risk of future violence. Yet in many jurisdictions, case law or mental health commitment statutes require clear and convincing evidence showing that a respondent actually did something (that is, committed an “overt act”) that did or could have caused harm. Such requirements may preclude using probabilistic considerations about future behavior as the sole ground for a mental health commitment. This article considers whether U. S. jurisdictions might allow mental health experts to use ARAIs as the primary evidence supporting their opinions in favor of involuntary psychiatric hospitalization. Our findings have important implications for the potential relevance of ARAIs in mental health commitment proceedings...|$|R
40|$|ABSTRACT: Existing <b>actuarial</b> <b>techniques</b> for {{automobile}} warranty ratemaking and reserving {{rely heavily}} on emerging experience (loss development) for the pricing and unearned premium reserving of these products. Since terms for automobile warranties can extend up to 10 years, such data is typically not available or not credible {{to the degree that}} the actuary can take great reliance on it. In addition, changing coverage terms in the auto warranty products can often make past development even less meaningful. Exposure techniques that have been developed (Cheng, 1993) rely on overall averages for some critical assumptions instead of dismbutions or individual policy characteristics. We propose a "miles-driven " approach in which claims are assumed to arise from auto warranties in proportion to the miles driven times a weight assigned to the overall mileage of the vehicle. The method we employ is much more complex than traditional methods, but relies on data that is typically available at waxranty writers. Important data dements would include the mileage of the vehicle at the time of a claim and if the contract cancels. In addition, the underlying manufacturer's warranty is also critical. In order to provide an accurate model of pricing, a distributional approach is utilized fo...|$|R
40|$|In general, current life {{insurance}} pricing techniques {{rely on the}} calculation of unit profits using a "cost-plus" pricing paradigm. Over the past decade, {{it has become increasingly}} obvious that this methodology is not well suited to decision-making in a competitive nviron-ment. This paper outlines the limitations inherent in current <b>actuarial</b> pricing <b>techniques</b> and describes a product development process that can produce optimal decisions in competitive environments. I. Traditional Pricing Models Although actuarial literature describes a multitude of product-pricing methodologies, current techniques exhibit hree characteristics: 1. Product profitability is measured on a per-unit basis...|$|R
40|$|Our {{experience}} with ball valve {{replacement of the}} mitral valve {{during the past decade}} is presented in terms that allow comparison with other techniques. The use ofsuch prostheses is characterized by ease of implantation, with an overall operative mortality of II per cent for isolated mitral re-placement and 13 per cent for multiple valve replacement. The operative mortality for isolated mitral valve replacement during I 969 and thus far in 1970 has been nil. The late mortality was 13 per cent for isolated mitral replacement and 20 per cent for multiple valve replacement. Forty-three per cent of the total late deaths were clearly unrelated to the prosthetic device itself. The overall incidence of late infection and leak is less than I per cent and the immediate haemodynamic benefit is not altered by loss of structural integrity of the prosthesis. The most serious problem after mitral valve replacement with the ball valve prosthesis is that of thromboembolic complications. While thrombotic stenosis of the prosthesis is a rarity, embolic episodes, usually cerebral in type, have been noted in 63 per cent of the patients surviving mitral valve replacement with the earliest model ball valve from August I 960 to February I 966. Improvements in valve design have resulted in a remarkable decrease in this incidence as examined by <b>actuarial</b> <b>techniques</b> and taking into account the duration offollow-up. The extension of th...|$|R
40|$|Abstract: Reserve ranges {{and risk}} capital {{requirements}} {{can be related}} to statistical interval estimates. While not all sources of uncertainty are readily incorporated into an interval estimate, such intervals give a lower bound {{on the size of the}} required interval. We discuss the calculation of interval estimates, for both the estimate of the mean and for the liability process itself, show how to tell if the model is a reasonable description of the data and show that when it is not, the interval estimates may sometimes be disastrously wrong. Many practitioners are now using probabilistic versions of standard <b>actuarial</b> <b>techniques,</b> sometimes employing quite sophisticated tools in their estimation. However, none of these developments avoid the need for stringent checking of the suitability of model assumptions, a necessity that is often overlooked. We discuss some of the statistical models underlying a variety of standard methods, construct a number of diagnostics for model assessment for several models and discuss how the underlying ideas carry over to many other methods for the estimation of liabilities. These tools are easy to implement and use. They allow practitioners to use the corresponding models with greater confidence, and gain additional information about the triangle. This information can have important consequences for the insurer. We illustrate that some popular approaches—the Mack chain ladder, the quasi-Poisson GLM—an...|$|R
40|$|In Canada, {{appointed}} actuaries {{are required}} to opine on {{the adequacy of the}} policy liabilities for property-casualty insurers. Policy liabilities include both claims and premium liabilities. Several papers have been written and <b>actuarial</b> <b>techniques</b> have been developed to estimate claims liabilities. Premium liabilities, however, have received little, virtually no attention in the actuarial literature. To date, we believe that only Canadian actuaries have been evaluating these liabilities. However, other countries are following that lead. We understand that in some states, regulators will soon require actuarial opinions on the adequacy of unearned premiums for policies with terms exceeding twelve months. The evaluation of premium liabilities consists of examining all related assets and liabilities to ensure that the anticipated net costs to discharge an insurer's obligations with respect to its insurance and reinsurance contracts, except its claim liabilities, are provided for. This paper intends to provide the practicing actuary with some guidelines on the evaluation of the premium liabilities. We will review the individual components of the premium liabilities and the related regulatory requirements and CIA recommendations. Finally, we will present an actuarial approach to evaluate the equity in the unearned premium, the unearned premium deficiency and the deferred policy acquisition expenses. Acknowledgement The authors would like to express their gratitude to Jean-Luc Allard, Richard Belleau, Jean Côté, Bernard Dupont and Betty-Jo Hill for their thorough review and constructive comments...|$|R
40|$|GESIS. This article {{examines}} the stratifying effects of economic classifications. We argue that in the neoliberal era market institutions increasingly use <b>actuarial</b> <b>techniques</b> to split and sort individuals into classification situations that shape life-chances. While this is a general and increasingly pervasive process, our main empirical illustration comes from {{the transformation of the}} credit market in the United States. This market works as both as a leveling force and as a condenser of new forms of social difference. The U. S. banking and credit system has greatly broadened its scope over the past twenty years to incorporate previously excluded groups. We observe this leveling tendency in the expansion of credit amongst lower-income households, the systematization of overdraft protections, and the unexpected and rapid growth of the fringe banking sector. But while access to credit has democratized, it has also differentiated. Scoring technologies classify and price people according to credit risk. This has allowed multiple new distinctions to be made amongst the creditworthy, as scores get attached to different interest rates and loan structures. Scores have also expanded into markets beyond consumer credit, such as insurance, real estate, employment, and elsewhere. The result is a cumulative pattern of advantage and disadvantage with both objectively measured and subjectively experienced aspects. We argue these private classificatory tools are increasingly central to the generation of "market-situations", and thus an important and overlooked force that structures individual life-chances. In short, classification situations may have become the engine of modern class situations...|$|R
40|$|The goal of {{this paper}} is to develop simple, {{quantitative}} methods to generate a range of reserves for an aggregate insurance portfolio, and provide a basis for selecting the best estimate of the aggregate reserves, given assumptions by accident period or by line of business. The basic assumption of the paper is that the range of reserves generated by the application of various <b>actuarial</b> <b>techniques</b> (for example projection of paid losses, projection of incurred losses, Bornhuetter-Ferguson techniques) can be used to generate parameters for loss distributions by accident year or by line of business. Accident year or line of business parameters generated based on the reserve projections are then used, along with simulation techniques, to generate a range for the aggregate reserves. The first section of the paper describes some of the current statistical, and ad-hoc methodologies used by actuaries to generate reserve ranges. The second section describes some distribution functions that could be used in the simulation process. It focuses on four relatively simple and common distribution functions (uniform, triangle, normal, and lognormal) and describes situations where they would be most suitable. The third section describes the simulation methodology and explores techniques for generating the parameters to be used in the actual simulations. In addition, it explores the issue of what would be an appropriate number of simulations. The final section presents results from simulations for three different types of insurance portfolios. Results are plotted graphically, and comparisons are made of the simulated and unsimulated ranges...|$|R
40|$|ARTICLES Principles and Application of Credibility • Vincent Goulet <b>Actuarial</b> <b>Techniques</b> in Risk Pricing and Cash Flow Analysis for U. K. Bank Loans • Philip Booth and Duncan E. P. Walsh Stability of Representative Crediting Rate Scenarios Under Monte Carlo Simulations • Sarah L. M. Christiansen and Kelley Buchacker Outlier Analysis of Annual Retail Price Inflation: A Cross-Country Study • Wai-Sum Cha An Analysis of Australian Pensioner Mortality by Pre-Retirement Income • David Knox and Andrew Tomlin Using Parametric Statistical Models to Estimate Mortality Structure: The Case of Taiwan • Shih-Chieh Chang A Frailty Model for Projection of Human Mortality Improvements • Shaun S. Wang and Robert L. Brown Editor - Colin Ramsay, University of Nebraska. Associate Editors: Robert Brown, University of Waterloo ○ Cecil Bykerk, Mutual of Omaha ○ Ruy Cardoso, Actuarial Frameworks ○ Samuel Cox, Georgia State University ○ David Cummins, University of Pennsylvania ○ Robert Finger, Retired ○ Charles Fuhrer, The Segal Company ○ Farrokh Guiahi, Hofstra University ○ Steven Haberman, City University ○ Merlin Jetton, Retired ○ Eric Klieber, Buck Consultants ○ Edward Mailander, WeIlpoint Health Networks ○ Charles McClenahan, Mercer Oliver Wyman ○ Robert Myers, Temple University ○ Norman Nodulman, Retired ○ François Outreville, United Nations ○ Timothy Pfeifer, Milliman USA ○ Esther Portnoy, University of Illinois ○ Robert Reitano, John Hancock Financial Services ○ Alice Rosenblatt, WeIlpoint Health Networks ○ Arnold Shapiro, Penn State University ○ Elias Shiu, University of Iowa ○ Michael Sze, Sze Associates Ltd. ○ Joseph Tan, National Actuarial Network ○ Ronnie Tan, Great Eastern Life ○ Richard Wendt, Tower Perrin. Margo Young, Technical Edito...|$|R
40|$|This {{publication}} is a {{tool for}} designers of new civil service pension schemes {{in central and eastern}} Europe. It presents civil service pension schemes in five OECD Member countries and ten central and eastern European countries. In most central and eastern European countries, people employed in the public administration are covered under common national pension schemes, usually defined in a common pension law. As part of efforts to improve the professionalisation and quality of public administration, countries are defining civil service categories of personnel through civil service legislation. Some countries will introduce specific pension provisions for the public administration employees subject to this legislation. There are at least three obvious reasons for this: to secure the independence of civil servants, to make a public sector career more attractive, and to shift the costs of current remuneration into the future. In most OECD Member countries, civil servants have separate and specially designed pension schemes. These are either totally independent of the common national pension schemes or complementary to them. Conditions vary between countries and so do principles for financing. In one country there might also be several schemes for various categories of state officials and employees. “Pay-as-you-go” schemes financed by the annual state budget exist in several OECD Member countries. When they were introduced, national civil services were small and common pension schemes for the working population at large were lacking. Over the last 30 years, the rapid growth of western public services has not had any major impact on pension costs in pay-as-you-go schemes for demographic reasons and, until recently, the financing of pensions has in many countries stayed unchanged. The long-term nature of pension schemes and the strong interest that civil servants and their unions have in keeping them intact has added to the difficulty of changing them. With changing demography, pensions are becoming a heavy burden on the budget. Pensions imply both considerable running costs and heavy long-term liabilities. That is why many OECD Member countries today are trying to find new solutions to fund the financing costs. Different funding and <b>actuarial</b> <b>techniques</b> can be used to achieve this. ...|$|R
40|$|AbstractObjectiveTo {{assess the}} {{adequacy}} of peritoneal dialysis in Chinese by analyzing the relationship between weekly urea kinetics (Kt/V) and clinical outcomes. MethodsA total of 146 patients on continuous ambulatory peritoneal dialysis for more than 6 months in the Shanghai Renji Hospital between July 1997 and March 1999 were enrolled into this study. They were assigned to three groups according to weekly Kt/V: Group A, Kt/V less than 1. 7; Group B, Kt/V between 1. 7 and 2; and Group C, Kt/V greater than 2. Patient and technique survivals were analyzed by using the log rank method. ResultsThe overall 2 -year <b>actuarial</b> patient and <b>technique</b> survivals were 90 % and 76 %, respectively. The 2 -year actuarial patient survival was 78 % for Group A, 97 % for Group B, and 96 % for Group C (p< 0. 05). The 2 -year technique survival was 56 % for Group A, 88 % for Group B, and 88 % for Group C. Both <b>actuarial</b> patient and <b>technique</b> survivals in Group A were significantly lower (p< 0. 05) compared {{with the other two}} groups. ConclusionThe study showed that clinical outcomes in Groups B and C patients were similar. However, patients with weekly Kt/V values less than 1. 7 had poorer clinical outcomes compared with patients from groups B and C. We conclude that Chinese patients who were receiving peritoneal ambulatory dialysis may benefit from weekly Kt/V greater than 1. 7...|$|R
40|$|Increases in NASA {{mission costs}} are well-noted but not well-understood, {{and there is}} little {{evidence}} that they are decreasing in frequency or amount over time. The need to control spending has led to analysis of the causes and magnitude of historical mission overruns, and many program control efforts are being implemented to attempt to prevent or mitigate the problem (NPR 7120). However, cost overruns have not abated, and while some direct causes of increased spending may be obvious (requirements creep, launch delays, directed changes, etc.), the underlying impetus to spend past the original budget may be more subtle. Gaining better insight into the causes of cost overruns will help NASA and its contracting organizations to avoid. them. This paper hypothesizes that one cause of NASA mission cost overruns is that the availability of reserves gives project team members an incentive to make decisions and behave in ways that increase costs. We theorize that the presence of reserves is a contributing factor to cost overruns because it causes organizations to use their funds less efficiently or to control spending less effectively. We draw a comparison to the insurance industry concept of moral hazard, the phenomenon that the presence of insurance causes insureds to have more frequent and higher insurance losses, and we attempt to apply <b>actuarial</b> <b>techniques</b> to quantifY the increase in the expected cost of a mission due to the availability of reserves. We create a theoretical model of reserve spending motivation by defining a variable ReserveSpending as a function of total reserves. This function has a positive slope; for every dollar of reserves available, there is a positive probability of spending it. Finally, the function should be concave down; the probability of spending each incremental dollar of reserves decreases progressively. We test the model against available NASA CADRe data by examining missions with reserve dollars initially available and testing whether they are more likely to spend those dollars, and whether larger levels of reserves lead to higher cost overruns. Finally, we address the question of how to prevent reserves from increasing mission spending without increasing cost risk to projects budgeted without any reserves. Is there a "sweet spot"? How can we derive the maximum benefit associated with risk reduction from reserves while minimizing the effects of reserve spending motivation...|$|R
