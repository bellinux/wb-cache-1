0|4953|Public
40|$|Abstract. In Behavior Driven Development (BDD), <b>acceptance</b> <b>tests</b> {{provide the}} {{starting}} point for the software design flow and serve as a basis for the communication between designers and stakeholders. In this agile software de-velopment technique, <b>acceptance</b> <b>tests</b> are written in natural language in order to ensure a common understanding between all members of the project. As a consequence, mapping the sentences to actual source code is the first step of the design flow, which is usually done manually. However, the scenarios described by the <b>acceptance</b> <b>tests</b> provide enough in-formation in order to automatize the extraction of both the structure of the implementation and the test cases. In this work, we propose an assisted flow for BDD where the user enters into a dialog with the computer which suggests code pieces extracted from the sentences. For this purpose, natural language processing techniques are exploited. This allows for a semi-automatic transfor-mation from <b>acceptance</b> <b>tests</b> to source <b>code</b> stubs and thus provides a first step towards an automatization of BDD. ...|$|R
40|$|While {{there are}} many {{excellent}} <b>acceptance</b> <b>testing</b> tools and frame-works available today, this paper presents an alternative approach, involving generating <b>code</b> from <b>tests</b> specified in a declarative tabular format within Excel spreadsheets. While this is a general approach, it is most applicable to difficult-to-test situations. Two such situations are presented: one involving complex fixture setup, and another involving complex application workflow concerns. Key Words: automated <b>testing,</b> <b>code</b> generation, domain specific testing lan-guage, test automation patterns, <b>testing</b> strategy, user <b>acceptance</b> <b>testing,</b> XML...|$|R
50|$|The preeminent <b>code</b> for <b>testing</b> fired steam {{generators}} in the USA is the American Society of Mechanical Engineers (ASME) performance <b>test</b> <b>code,</b> PTC 4. A related component is the regenerative air heater. A major revision {{to the performance}} <b>test</b> <b>code</b> for air heaters {{will be published in}} 2013. Copies of the draft are available for review. The European standards for <b>acceptance</b> <b>test</b> of steam boilers are EN 12952-15 and EN 12953-11. The British standards BS 845-1 and BS 845-2 remain also in use in the UK.|$|R
50|$|British, German, other {{national}} and international <b>test</b> <b>codes</b> are used to standardize the procedures and definitions used to test gas turbines. Selection of the <b>test</b> <b>code</b> to be used is an agreement between the purchaser and the manufacturer, and has some significance {{to the design of}} the turbine and associated systems. In the United States, ASME has produced several performance <b>test</b> <b>codes</b> on gas turbines. This includes ASME PTC 22-2014. These ASME performance <b>test</b> <b>codes</b> have gained international recognition and <b>acceptance</b> for <b>testing</b> gas turbines. The single most important and differentiating characteristic of ASME performance <b>test</b> <b>codes,</b> including PTC 22, is that the test uncertainty of the measurement indicates the quality of the test and is not {{to be used as a}} commercial tolerance.|$|R
40|$|The Fit {{framework}} {{is a well}} established tool for creating early and automated <b>acceptance</b> <b>tests.</b> Available Eclipse plug-ins like FITpro support for new requirements and new <b>code</b> writing of <b>test</b> data and creation of test stubs quite well. In our project we faced the problem, that a large legacy system should undergo a major refactoring. Before this, <b>acceptance</b> <b>tests</b> had {{to be added to}} the system to ensure equivalent program behavior before and after the changes. Writing <b>acceptance</b> <b>tests</b> manually for existing code is very laborious, cumbersome and very costly. However reverse generation of fit tests based on legacy code is not foreseen in the current Fit framework, and there are no other tools available to do so. So we decided to develop a tool which allows generation of the complete Fit <b>test</b> <b>code</b> and <b>test</b> specification based on existing code. The tool also includes automatic refactoring of test data when refactoring production code and vice versa, when changing the Fit test specification, it also updates production code accordingly. This reduces the maintenance effort of Fit tests in general and we hope, this will help to spread the usage of Fit for <b>acceptance</b> and integration <b>testing</b> even more...|$|R
40|$|User <b>Acceptance</b> <b>Testing</b> is {{typically}} {{the final phase}} in a software development {{process in which the}} software is given to the intended audience or domain experts. These domain experts know the functional requirements of the application and write user <b>acceptance</b> <b>tests</b> (UAT) in their natural language. A normal UAT test case in English typically follows an imperative sentence structure, i. e. a sentence that gives advice or instructions, or that expresses a request or command. We propose a methodology to write UAT <b>test</b> automation <b>code</b> using natural language processing techniques on test scripts written in free form English text by using the assumption that test cases are written in an imperative style. We have also built a proof of concept tool, the Autotestbot, to demonstrate the feasibility of our idea. In addition, with the help of Autotestbot, we also demonstrate the feasibility of our proposed approach to semi-automate the time consuming and cumbersome manual UAT <b>test</b> <b>code</b> generation process. The scope of this thesis is restricted to automating Web applications...|$|R
50|$|British, German, other {{national}} and international <b>test</b> <b>codes</b> are used to standardize the procedures and definitions used to test steam turbines. Selection of the <b>test</b> <b>code</b> to be used is an agreement between the purchaser and the manufacturer, and has some significance {{to the design of}} the turbine and associated systems. In the United States, ASME has produced several performance <b>test</b> <b>codes</b> on steam turbines. These include ASME PTC 6-2004, Steam Turbines, ASME PTC 6.2-2011, Steam Turbines in Combined Cycles, PTC 6S-1988, Procedures for Routine Performance Test of Steam Turbines. These ASME performance <b>test</b> <b>codes</b> have gained international recognition and <b>acceptance</b> for <b>testing</b> steam turbines. The single most important and differentiating characteristic of ASME performance <b>test</b> <b>codes,</b> including PTC 6, is that the test uncertainty of the measurement indicates the quality of the test and is not {{to be used as a}} commercial tolerance.|$|R
40|$|Software <b>acceptance</b> <b>testing</b> is an {{industry}} best practice. Most development teams do it poorly. This is because teams often misunderstand what <b>acceptance</b> <b>testing</b> is and why it is important. Furthermore, these teams often {{do not have an}} extensible framework for automated <b>acceptance</b> <b>testing.</b> In this paper, we define <b>acceptance</b> <b>testing</b> and discuss why it is important, and we describe our custom automated <b>acceptance</b> <b>testing</b> framework...|$|R
40|$|Abstract. In Executable Acceptance Test Driven Development, <b>acceptance</b> <b>tests</b> {{represent}} {{the requirements of}} a software system. As requirements change over time, the <b>acceptance</b> <b>tests</b> have to be updated and maintained. This process can be time-consuming and risky as <b>acceptance</b> <b>tests</b> lack the regression safety net that production code has. Refactoring of <b>acceptance</b> <b>tests</b> is used to keep the fixtures and the <b>acceptance</b> <b>test</b> definitions consistent...|$|R
50|$|The {{customer}} specifies scenarios to test when a user {{story has}} been correctly implemented. A story can have one or many <b>acceptance</b> <b>tests,</b> {{whatever it takes to}} ensure the functionality works. <b>Acceptance</b> <b>tests</b> are black-box system <b>tests.</b> Each <b>acceptance</b> <b>test</b> represents some expected result from the system. Customers are responsible for verifying the correctness of the <b>acceptance</b> <b>tests</b> and reviewing test scores to decide which failed tests are of highest priority. <b>Acceptance</b> <b>tests</b> are also used as regression tests prior to a production release. A user story is not considered complete until it has passed its <b>acceptance</b> <b>tests.</b> This means that new <b>acceptance</b> <b>tests</b> must be created for each iteration or the development team will report zero progress.|$|R
50|$|In {{software}} testing the ISTQB defines <b>acceptance</b> as: formal <b>testing</b> {{with respect to}} user needs, requirements, and business processes conducted {{to determine whether a}} system satisfies the acceptance criteria and to enable the user, customers or other authorized entity {{to determine whether or not}} to accept the system. <b>Acceptance</b> <b>testing</b> is also known as user <b>acceptance</b> <b>testing</b> (UAT), end-user <b>testing,</b> operational <b>acceptance</b> <b>testing</b> (OAT) or field (<b>acceptance)</b> <b>testing.</b>|$|R
5000|$|<b>Acceptance</b> <b>testing</b> {{performed}} by the customer, often in their lab environment on their own hardware, is known as user <b>acceptance</b> <b>testing</b> (UAT). <b>Acceptance</b> <b>testing</b> may be performed {{as part of the}} hand-off process between any two phases of development.|$|R
40|$|The {{purpose of}} this interim staff {{guidance}} (ISG) is to supplement standard review plan guidance for evaluating the helium leakage <b>testing</b> and ASME <b>Code</b> 1 required pressure (hydrostatic/pneumatic) testing that is specified for the dry storage system (DSS) confinement boundary. These <b>acceptance</b> <b>tests</b> are necessary to clearly demonstrate that the DS...|$|R
40|$|Functionally {{generated}} <b>acceptance</b> <b>tests</b> {{are examined}} using structural coverage metrics. A method of comparing <b>acceptance</b> <b>tests</b> and operational usage was generated. <b>Acceptance</b> <b>tests</b> are prepresentative of operational usage {{except for the}} mix of statement types. Structural coverage metrics may provide insight into software faults...|$|R
40|$|Abstract. User <b>acceptance</b> <b>testing</b> {{is finally}} getting the {{attention}} and tool support it deserves. It is imperative that <b>acceptance</b> <b>tests</b> follow the best practices and embody the critical success factors that have been established over the years for automated unit testing. However, it is often challenging for <b>acceptance</b> <b>tests</b> to be repeatable, readable, and maintainable {{due to the nature}} of the tests and the tools currently available for automation. The key contributions this paper makes to the agile community are: first, it provides concrete examples of applying test automation patterns to user <b>acceptance</b> <b>testing,</b> and secondly it provides a description of various extensions to the WebTest <b>acceptance</b> <b>testing</b> framework that facilitate developing automated <b>acceptance</b> <b>tests</b> according to these established best practices...|$|R
40|$|The overall {{objective}} of the <b>acceptance</b> <b>test</b> is to demonstrate a combined system. This includes associated tools and equipment necessary to perform cleaning in the 105 K East Basin (KE) for achieving optimum reduction {{in the level of}} contamination/dose rate on canisters prior to removal from the KE Basin and subsequent packaging for disposal. <b>Acceptance</b> <b>tests</b> shall include necessary hardware to achieve acceptance of the cleaning phase of canisters. This <b>acceptance</b> <b>test</b> procedure will define the <b>acceptance</b> <b>testing</b> criteria of the high pressure water jet cleaning fixture. The focus of this procedure will be to provide guidelines and instructions to control, evaluate and document the <b>acceptance</b> <b>testing</b> for cleaning effectiveness and method(s) of removing the contaminated surface layer from the canister presently identified in KE Basin. Additionally, the desired result of the <b>acceptance</b> <b>test</b> will be to deliver to K Basins a thoroughly tested and proven system for underwater decontamination and dose reduction. This report discusses the <b>acceptance</b> <b>test</b> procedure for the High Pressure Water Jet...|$|R
40|$|The <b>acceptance</b> <b>test</b> {{procedure}} is described for the Lockheed Electronics Elevon Servoactuator Simulator {{to be used}} in the Shuttle Avionics Integration Laboratory (SAIL). The intent of this <b>acceptance</b> <b>test</b> {{procedure is}} to comply with the technical Shuttle Actuators Simulator Requirements. <b>Acceptance</b> <b>tests</b> will be performed on each Elevon Servoactuator Simulator...|$|R
5000|$|... system {{development}} lifecycle - including {{the topics of}} internal kickoff, requirements, design, development, unit/module & integration <b>testing,</b> factory <b>acceptance</b> <b>testing,</b> system shipping, installation, commissioning and site <b>acceptance</b> <b>testing</b> ...|$|R
30|$|Adjudicators {{generally}} {{come in two}} flavours, {{voters and}} <b>Acceptance</b> <b>Tests</b> (ATs). A brief description of voter characteristics and differences are presented in Section 5, {{in the context of}} the proposed taxonomy. We refer to Pullum ([2001]) for a more detailed description about the various types of adjudicators and their operations (pages 269 - 324). For example, specific adjudicators covered by Pullum ([2001]) are exact majority, consensus, formal consensus, formal majority, median, mean, weighted, and dynamic voters; <b>acceptance</b> <b>tests</b> can be based on satisfaction of requirements, accounting <b>tests,</b> computer run-time <b>acceptance</b> <b>tests</b> and reasonableness <b>acceptance</b> <b>tests.</b>|$|R
40|$|In this paper, we {{show that}} two {{recently}} published on-line <b>acceptance</b> <b>tests</b> for guaranteeing hard deadline aperiodic tasks scheduled under the Slack Stealing algorithm are insufficient: they may guarantee tasks which will then miss their deadlines. Further, we derive a sufficient <b>acceptance</b> <b>test</b> which is both efficient and effective. An evaluation {{of the performance of}} this <b>acceptance</b> <b>test</b> is given in the paper...|$|R
40|$|Prior to {{commercial}} operation, large solar systems in utility-size power plants {{need to pass}} a performance <b>acceptance</b> <b>test</b> conducted by the engineering, procurement, and construction (EPC) contractor or owners. In lieu of the present absence of ASME or other international <b>test</b> <b>codes</b> developed for this purpose, the National Renewable Energy Laboratory has undertaken the development of interim guidelines to provide recommendations for test procedures that can yield results of {{a high level of}} accuracy consistent with good engineering knowledge and practice. The Guidelines contained here are specifically written for parabolic trough collector systems with a heat-transport system using a high-temperature synthetic oil, but the basic principles are relevant to other CSP systems...|$|R
5000|$|ATDD {{is closely}} related to test-driven {{development}} (TDD). [...] It differs by the emphasis on developer-tester-business customer collaboration. ATDD encompasses <b>acceptance</b> <b>testing,</b> but highlights writing <b>acceptance</b> <b>tests</b> before developers begin coding.|$|R
50|$|Aircrew {{training}} include Class A (Experimental and <b>Acceptance</b> <b>Testing)</b> or {{the shorter}} Class B (<b>Acceptance</b> <b>Testing)</b> courses on either fixed or rotary-wing aircraft. A light aircraft test pilot course is also offered.|$|R
40|$|Abstract. We {{present results}} of a case study looking at how domain {{knowledge}} is communicated to developers using executable <b>acceptance</b> <b>test</b> driven development at a large software development company. We collected and analyzed qualitative data on a large software development team's testing practices and their use of a custom-built executable <b>acceptance</b> <b>testing</b> tool. Our findings suggest that executable <b>acceptance</b> <b>tests</b> (1) helps communicate domain knowledge required to write software and (2) can help software developers to communicate {{the status of the}} software implementation better. In addition to presenting these findings, we discuss several human aspects involved in facilitating executable <b>acceptance</b> <b>test</b> driven development...|$|R
40|$|In this paper, {{we argue}} that {{executable}} <b>acceptance</b> <b>test</b> driven development (EATDD) allows tighter integration between the software requirements and the implementation. We argue that EATDD improves communication between all project stakeholders. We give an overview of why previous approaches to requirements specifications are less than impressive and how executable <b>acceptance</b> <b>tests</b> help fix problems. In addition, we argue for multi-modal executable <b>acceptance</b> <b>tests</b> {{and how it can}} help improve the requirements specification. We provide some of the immediate research questions {{that need to be addressed}} in order to push forward more wide-spread use of executable <b>acceptance</b> <b>test</b> driven development...|$|R
5000|$|If {{the test}} is successful, the product is copied to an <b>Acceptance</b> <b>test</b> environment. During the <b>Acceptance</b> <b>test,</b> the {{customer}} will test the product in this environment to verify whether it meets their expectations.|$|R
50|$|<b>Acceptance</b> <b>tests</b> {{are a part}} of {{an overall}} testing strategy. They are the {{customer}} tests that demonstrate the business intent of a system. Component <b>tests</b> are technical <b>acceptance</b> <b>tests</b> developed by an architect that specify the behavior of large modules. Unit tests are created by the developer to drive easy-to-maintain code. They are often derived from <b>acceptance</b> <b>tests</b> and unit tests. Cross-functional testing includes usability testing, exploratory testing, and property testing (scaling and security).|$|R
40|$|The {{development}} and <b>acceptance</b> <b>testing</b> of the 4 -band Multispectral Scanners to be flown on LANDSAT D and LANDSAT D Earth resources satellites are summarized. Emphasis {{is placed on}} the <b>acceptance</b> <b>test</b> phase of the program. Test history and <b>acceptance</b> <b>test</b> algorithms are discussed. Trend data of all the key performance parameters are included and discussed separately for each of the two multispectral scanner instruments. Anomalies encountered and their resolutions are included...|$|R
5000|$|In {{contract}} <b>acceptance</b> <b>testing,</b> {{a system}} is <b>tested</b> against <b>acceptance</b> criteria as documented in a contract, before the system is accepted. In regulation <b>acceptance</b> <b>testing,</b> {{a system is}} tested to ensure it meets governmental, legal and safety standards.|$|R
50|$|In 2014, AUTOSAR <b>Acceptance</b> <b>Tests</b> were {{introduced}} to minimize test effort and <b>test</b> costs. <b>Acceptance</b> Test Specifications are system test specifications with interfaces to the application and the bus. The specification of standard <b>acceptance</b> <b>tests</b> contribute to these objectives.|$|R
5000|$|This {{may include}} factory <b>acceptance</b> <b>testing</b> (FAT), i.e. the testing {{done by a}} vendor before the product or system is moved to its {{destination}} site, after which site <b>acceptance</b> <b>testing</b> (SAT) may be performed by the users at the site.|$|R
40|$|<b>Acceptance</b> <b>Testing</b> will be {{performed}} by a medical physicist before the scanner is used to scan patients. The goal of the <b>acceptance</b> <b>testing</b> {{is to ensure that}} the scanner meets or exceeds the manufacturer’s specifications as is required by the CAR Standards for Magnetic Resonance Imaging. The references {{at the end of this}} section provide guidelines for the <b>acceptance</b> <b>tests.</b> However, actual tests may deviate from the guidelines due to the capabilities and limitations of the scanner and phantoms...|$|R
5000|$|In {{addition}} to <b>acceptance</b> <b>tests</b> for requirements, <b>acceptance</b> <b>tests</b> {{can be used}} on a project as a whole. For example, if this requirement was part of a library book checkout project, there could be <b>acceptance</b> <b>tests</b> for the whole project. These are often termed SMART objectives. An example test is [...] "When the new library system is in production, the users will be able to check books in and out three times as fast as they do today".|$|R
40|$|Abstract. We {{conducted}} a survey on Executable Acceptance Test Driven Development (or: Story Test Driven Development). The results {{show that there is}} often a substantial delay between defining an <b>acceptance</b> <b>test</b> and its first successful pass. Therefore, it becomes important for teams to easily be able to distinguish between tasks that were never tackled before and tasks that were already completed but whose tests are now failing again. We then describe our FitClipse tool that extends Fit by maintaining a history of <b>acceptance</b> <b>test</b> results. Based on the history, FitClipse is able to generate reports that show when an <b>acceptance</b> <b>test</b> is suddenly failing again. Keywords: Executable Acceptance Test-Driven Development (EATDD), executable <b>acceptance</b> <b>test,</b> Fit...|$|R
40|$|Abstract—During <b>acceptance</b> <b>testing</b> {{customers}} {{assess whether}} a system meets their expectations and often identify issues {{that should be}} improved. These findings have to be communicated to the developers – a task we observed to be error prone, especially in distributed teams. Here, it is normally not possible to have developer representatives from every site attend the test. Developers who were not present might misunderstand insufficiently documented findings. This hinders fixing the issues and endangers customer satisfaction. Integrated feedback systems promise to mitigate this problem. They allow to easily capture findings and their context. Correctly applied, this technique could improve feedback, while reducing customer effort. This paper collects our experiences from comparing <b>acceptance</b> <b>testing</b> with and without feedback systems in a distributed project. Our results indicate that this technique can improve <b>acceptance</b> <b>testing</b> – if certain requirements are met. We identify key requirements feedback systems should meet to support <b>acceptance</b> <b>testing.</b> Keywords-distributed software development; requirements engineering; <b>acceptance</b> <b>testing</b> I...|$|R
40|$|The {{same kind}} of {{standard}} and controls are established that are currently in use for the procurement of new analog, digital, and IBM/IBM compatible 3480 tape cartridges, and 1 in wide channel video magnetic tapes. The Magnetic Tape Certification Facility (MTCF) maintains a Qualified Products List (QPL) for the procurement of new magnetic media and uses the following specifications for the QPL and Acceptance Tests: (1) NASA TM- 79724 {{is used for the}} QPL and <b>Acceptance</b> <b>Testing</b> of new analog magnetic tapes; (2) NASA TM- 80599 is used for the QPL and <b>Acceptance</b> <b>Testing</b> of new digital magnetic tapes; (3) NASA TM- 100702 is used for the QPL and <b>Acceptance</b> <b>Testing</b> of new IBM/IBM compatible 3840 magnetic tape cartridges; and (4) NASA TM- 100712 is used for the QPL and <b>Acceptance</b> <b>Testing</b> of new 1 in wide channel video magnetic tapes. This document will be used for the QPL and <b>Acceptance</b> <b>Testing</b> of new Helical Scan 8 mm digital data tape cartridges...|$|R
