1|27|Public
40|$|The linear <b>array</b> <b>push</b> broom imaging mode {{is widely}} used for high {{resolution}} optical satellites (HROS). Using double-cameras attached by a high-rigidity support along with push broom imaging is one method to enlarge {{the field of view}} while ensuring high resolution. High accuracy image mosaicking is the key factor of the geometrical quality of complete stitched satellite imagery. This paper proposes a high accuracy image mosaicking approach based on the big virtual camera (BVC) in the double-camera system on the GaoFen 2 optical remote sensing satellite (GF 2). A big virtual camera can be built according to the rigorous imaging model of a single camera; then, each single image strip obtained by each TDI-CCD detector can be re-projected to the virtual detector of the big virtual camera coordinate system using forward-projection and backward-projection to obtain the corresponding single virtual image. After an on-orbit calibration and relative orientation, the complete final virtual image can be obtained by stitching the single virtual images together based on their coordinate information on the big virtual detector image plane. The paper subtly uses the concept of the big virtual camera to obtain a stitched image and the corresponding high accuracy rational function model (RFM) for concurrent post processing. Experiments verified that the proposed method can achieve seamless mosaicking while maintaining the geometric accuracy...|$|E
40|$|Recent work on {{embedded}} domain specific languages (EDSLs) {{for high}} performance array programming {{has given rise}} to a number of array representations. In Feldspar and Obsidian there are two different kinds of arrays, called Pull and <b>Push</b> <b>arrays.</b> Both Pull and <b>Push</b> <b>arrays</b> are deferred; they are methods of computing arrays, rather than elements stored in memory. The reason for having multiple array types is to obtain code that performs better. Pull and <b>Push</b> <b>arrays</b> provide this by guaranteeing that operations fuse automatically. It is also the case that some operations are easily implemented and perform well on Pull arrays, while for some operations, <b>Push</b> <b>arrays</b> provide better implementations. But do we really need to have more than one array representation? In this paper we derive a new <b>array</b> representation from <b>Push</b> <b>arrays</b> that have all the good qualities of Pull and <b>Push</b> <b>arrays</b> combined. This new array representation is obtained via defunctionalization of a <b>Push</b> <b>array</b> API. Categories and Subject Descriptors CR-number [subcategory]: third-leve...|$|R
40|$|Recent work on domain {{specific}} languages (DSLs) {{for high}} per-formance array programming {{has given rise}} to a number of array representations. In Feldspar and Obsidian there are two different kinds of arrays, called Pull and <b>Push</b> <b>arrays</b> and in Repa there is a even higher number of different array types. The reason for hav-ing multiple array types is to obtain code that performs better. Pull-and <b>Push</b> <b>arrays,</b> that are present in Feldspar and Obsidian, provide this by guaranteeing that operations fuse automatically. It is also the case that some operations are easily implemented and perform well on Pull arrays, while for some operations, <b>Push</b> <b>arrays</b> provide better implementations. Repa has Pull arrays, called delayed arrays for the same reason and so-called cursored arrays which are impor-tant for performance in stencil operations. But do we really need to have more than one array representation? In this paper we derive a new <b>array</b> representation from <b>Push</b> <b>arrays</b> that have all the good qualities of Pull- and <b>Push</b> <b>arrays</b> combined. This new array repre-sentation is obtain via defunctionalization of a <b>Push</b> <b>array</b> API. Categories and Subject Descriptors CR-number [subcategory]: third-leve...|$|R
5000|$|In 1875, Henry Spratt of Kent {{received}} a U.S. patent for a voting machine that presented the ballot as an <b>array</b> of <b>push</b> buttons, one per candidate. [...] Spratt's machine {{was designed for}} a typical British election with a single plurality race on the ballot. In 1881, Anthony Beranek of Chicago patented the first voting machine appropriate {{for use in a}} general election in the United States. [...] Beranek's machine presented an <b>array</b> of <b>push</b> buttons to the voter, with one row per office on the ballot, and one column per party. Interlocks behind each row prevented voting for more than one candidate per race, and an interlock with the door of the voting booth reset the machine for the next voter as each voter left the booth.|$|R
40|$|Single-photon {{detectors}} play a {{key role}} in many research fields such as biology, chemistry, medicine, and space technology, and in recent years, single-photon avalanche diodes (SPADs) have become a valid alternative to photo multiplier tubes (PMTs). Moreover, scientific research has recently focused on single-photon detector <b>arrays,</b> <b>pushed</b> by a growing demand for multichannel systems. In this scenario, we developed a compact 32 -channel system for time-resolved single-photon counting applications. The system is divided into two independent modules: a photon detection head including a 32 $times$ 1 SPAD array built in custom technology, featuring high time resolution, high photon detection efficiency (44 % at 550 nm), and low dark count rate (mean value $ 400 cps at $- 10 ^{circ}hbox{C}$) at 6 -V excess bias voltage and a 32 -channel acquisition system able to perform time-correlated single-photon counting (TCSPC) measurements. The TCSPC module includes eight four-channel time-to-amplitude converter (TAC) arrays, built-in 0. 35 -$mu$m Si-Ge BiCMOS technology, characterized by low differential non-linearity (rms value lower than 0. 15 % of the time bin width) and variable full-scale range. The system response function of this TCSPC instrumentation achieves a mean time resolution of 63 $hbox{ps}_{rm FWHM}$, considering a mean count rate of 1 Mcps...|$|R
50|$|Later in the 1970s, Plessey—now Roke Manor Research Limited—of Great Britain {{developed}} their smaller, more economical <b>Pusher</b> CDAA <b>array.</b> At least 25 Pusher CDAAs were installed {{in many countries}} around the world. Several <b>Pusher</b> <b>arrays</b> were installed in U.S. military facilities, where the array {{is known as the}} AN/FRD-13.|$|R
2500|$|HyperPodX, a German {{team with}} a pod {{designed}} to levitate using a series of fixed magnets following a Halbach <b>array</b> and a <b>pusher</b> with 4 electric motors for acceleration to high velocities The team is comprised from the conjoined effort from Engineering Physics students from the University of Oldenburg and the Hochschule Emden/Leer (...) ...|$|R
50|$|Unlike {{nearly every}} other sort, items are never written {{elsewhere}} in the <b>array</b> simply to <b>push</b> {{them out of the}} way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.|$|R
40|$|Nowadays, many {{research}} fields like biology, chemistry, {{medicine and}} space technology rely on high sensitivity imaging instruments that allow to exploit modern measurement techniques; among these, Time-Correlated Single-Photon Counting (TCSPC) provides extremely high time resolution. Single-photon detectors {{play a key}} role in these advanced imaging systems, and in recent years Single-Photon Avalanche Diodes (SPADs) have become a valid alternative to Photo Multiplier Tubes (PMTs). Moreover scientific research has recently focused on single photon detector <b>arrays,</b> <b>pushed</b> by a growing demand for multichannel systems. In this scenario, we developed a compact, stand-alone, 32 -channel system for time-resolved single-photon counting applications. The system core is represented by a 32 × 1 SPAD array built in custom technology, featuring high time resolution, high photon detection efficiency (> 45 %) and low dark count rate. The SPAD avalanche signal is exported through an integrated inverter which is placed close to the photo detector, this way the avalanche event is detected with high time resolution while achieving negligible crosstalk between adjacent pixels. SPAD proper operation is guaranteed by a 32 × 1 mixed passive-active quenching circuit (AQC) array built in 0. 18 μm HV-CMOS technology; its digital outputs are fed to an FPGA that performs on-board processing of photon counting information. On the contrary, photon timing information is directly extracted from the pixel array and exported in Current Mode Logic (CML) standard. Preliminary experiments have been carried out on the developed system, resulting in a high time resolution (< 60 ps FWHM) and mean dark count rate lower than 8500 counts/s at 25 °C...|$|R
30|$|Further {{investigations}} {{are made to}} these three factors by increasing the levels of each factor from two to five and the same procedure is repeated by taking modified orthogonal <b>array.</b> The <b>pushing</b> zone temperature, dimmer speed, die head temperature in the extrusion process again correlated significantly to the quality characteristic. This research concentrated on parameter design developed by Dr. Taguchi to calculate the S/N ratio. After all the quality values were converted to S/N ratios, the consequent analysis was conducted using the statistic estimation to estimate S/N ratios under different parameter-level combinations. The highest value of S/N ratio was conformed to be the optimum parameter-level combination, because it had the minimum variance. After Taguchi’s S/N analysis and the following experiments, the combination of A 4, B 5 and C 2 {{was chosen as the}} optimum factor level which was as follows: pushing zone temperature 166  °C, dimmer speed 08  rpm, and die head temperature 192  °C.|$|R
40|$|DNA editing offers new {{possibilities}} in synthetic biology and biomedicine for modulation or modification of cellular functions to organisms. However, inaccuracy {{in this process}} may lead to genome damage. To address this important problem, a strategy allowing specific gene modification has been achieved through the addition, removal or exchange of DNA sequences using customized proteins and the endogenous DNA-repair machinery. Therefore, the engineering of specific protein-DNA interactions in protein scaffolds is key to providing 'toolkits' for precise genome modification or regulation of gene expression. In a search for putative DNA-binding domains, BurrH, a protein that recognizes a 19 bp DNA target, was identified. Here, its apo and DNA-bound crystal structures are reported, revealing a central region containing 19 repeats of a helix-loop-helix modular domain (BurrH domain; BuD), which identifies the DNA target by a single residue-to-nucleotide code, thus facilitating its redesign for gene targeting. New DNA-binding specificities have been engineered in this template, showing that BuD-derived nucleases (BuDNs) induce high levels of gene targeting in a locus of the human haemoglobin beta (HBB) gene close to mutations responsible for sickle-cell anaemia. Hence, the unique combination of high efficiency and specificity of the BuD <b>arrays</b> can <b>push</b> forward diverse genome-modification approaches for cell or organism redesign, opening new avenues for gene editing...|$|R
40|$|Abstract — The Navy has an {{immediate}} need for large-scale, 3 D, nonlinear simulations of broadband sonar pro-jectors. The example considered {{here is a}} volume array of high-power, electrostrictive (PMN) flextensionals. Analysis and design of these complex <b>arrays</b> are clearly <b>pushing</b> the limits of simplified models. The analytical burden should be shifted from clever but overextended designers to computers. Very large-scale models, broad-band response and nonlinearity favor explicit time-domain methods over implicit time- or frequency-domain meth-ods. We demonstrate comprehensive finite element mod-eling of an icosahedral array of 12 electrostrictive flextensionals. Behavior of the PMN driver is illustrated with a 1 D finite element (nonlinear harmonic oscillator) and generalized to a 3 D element. Full-scale, SMP simu-lations are shown for individual flextensionals and the icosahedral array including tow-body structure...|$|R
40|$|We {{present the}} results of our {{investigations}} into options for the computing platform for the imaging pipeline in the CHILES project, an ultra-deep HI pathfinder for the era of the Square Kilometre <b>Array.</b> CHILES <b>pushes</b> the current computing infrastructure to its limits and understanding how to deliver the images from this project is clarifying the Science Data Processing requirements for the SKA. We have tested three platforms: a moderately sized cluster, a massive High Performance Computing (HPC) system, and the Amazon Web Services (AWS) cloud computing platform. We have used well-established tools for data reduction and performance measurement to investigate the behaviour of these platforms for the complicated access patterns of real-life Radio Astronomy data reduction. All of these platforms have strengths and weaknesses and the system tools allow us to identify and evaluate them in a quantitative manner. With the insights from these tests we are able to complete the imaging pipeline processing on both the HPC platform and also on the cloud computing platform, which paves the way for meeting big data challenges in the era of SKA in the field of Radio Astronomy. We discuss the implications that all similar projects will have to consider, in both performance and costs, to make recommendations for the planning of Radio Astronomy imaging workflows. Comment: Accepted Astronomy and Computin...|$|R
40|$|In {{the focal}} plane of a pushbroom imager, a linear array of pixels is scanned across the scene, {{building}} up the image one row at a time. For the Multispectral Thermal Imager (MTI), each of fifteen di#erent spectral bands has its own linear array. These <b>arrays</b> are <b>pushed</b> across the scene together, but since each band's array is at a di#erent position on the focal plane, a separate image is produced for each band. The standard MTI data products (LEVEL 1 B R COREG and LEVEL 1 B R GEO) resample these separate images to a common grid and produce coregistered multispectral image cubes. The coregistration software employs a direct "dead reckoning" approach. Every pixel in the calibrated image is mapped to an absolute position {{on the surface of}} the earth, and these are resampled to produce an undistorted coregistered image of the scene. To do this requires extensive information regarding the satellite position and pointing as a function of time, the precise configuration of the focal plane, and the distortion due to the optics. These must be combined with knowledge about the position and altitude of the target on the rotating ellipsoidal earth. We will discuss the direct approach to MTI coregistration, as well as more recent attempts to "tweak" the precision of the band-to-band registration using correlations in the imagery itself...|$|R
40|$|We {{describe}} {{the implementation of}} an inexpensive spin-coating system to deposit thin films of materials dissolved in a volatile solvent. The system can be easily built with interdisciplinary knowledge of mechanics, fluid mechanics and electronics at undergraduate level. The system allows the deposition of thin films of up to 5 cm 2 in area and is constructed from a commercial DVD player drive motor and an electronic circuit designed to control the spinning speed and spinning time up to 10, 000 rpm and 60 seg, respectively. In our design, both variables can be adjusted manually through an <b>array</b> of micro <b>push</b> button switches and a varistor. To illustrate the use of our spin-coating system, were prepared films of MDMO-PPV conjugated polymer from solutions in chlorobenzene and tetrahydrofuran and their optical absorption and photoluminescence properties are analyzed and discussed...|$|R
40|$|This paper {{argues for}} a new {{methodology}} for writing high per-formance Haskell programs by using Embedded Domain Specific Languages. We exemplify the methodology by describing a complete li-brary, meta-repa, which is a reimplementation of parts of the repa library. The paper describes the implementation of meta-repa and contrasts it with the standard approach to writing high performance libraries. We conclude {{that even though the}} embedded language approach has an initial cost of defining the language and some syntactic overhead it gives a more tailored programming model, stronger performance guarantees, better control over optimizations, simpler implementation of fusion and inlining and allows for mov-ing type level programming down to value level programming in some cases. We also provide benchmarks showing that meta-repa is as fast, or faster, than repa. Furthermore, meta-repa also includes <b>push</b> <b>arrays</b> and we demonstrate their usefulness for writing certain high performance kernels such as FFT...|$|R
50|$|The {{development}} of the modern telephone keypad is attributed to research in the 1950s by Richard Deininger under the directorship of John Karlin at the Human Factors Engineering Department of Bell Labs. The contemporary keypad is {{laid out in a}} rectangular <b>array</b> of twelve <b>push</b> buttons arranged as four rows and three columns of keys. For military applications, a fourth, right-most column of keys was added for priority signaling in the Autovon system in the 1960s. Initially, between 1963 and 1968, the keypads for civilian subscriber service had keys installed in only ten positions, omitting the lower left and lower right keys that commonly are assigned to the star (✻) and number sign (#) signals, respectively. These keys were added to provide signals for anticipated data entry purposes in business applications, but found use in Custom Calling Services (CLASS) features installed in electronic switching systems.|$|R
40|$|The {{extraordinary}} astrometric {{accuracy of}} radio interferometry creates {{an important and}} unique opportunity for the discovery and characterization of exo-planets. Currently, the Very Long Baseline Array can routinely achieve better than 100 microarcsecond accuracy, and can approach 10 microarcsecond with careful calibration. We describe here RIPL, the Radio Interferometric PLanet search, a new program with the VLBA and the Green Bank 100 m telescope that will survey 29 low-mass, active stars over 3 years with sub-Jovian planet mass sensitivity at 1 AU. An upgrade of the VLBA bandwidth will increase astrometric accuracy by an order of magnitude. Ultimately, the colossal collecting area of the Square Kilometer <b>Array</b> could <b>push</b> astrometric accuracy to 1 microarcsecond, making detection and characterizaiton of Earth mass planets possible. RIPL and other future radio astrometric planet searches occupy a unique volume in planet discovery and characterization parameter space. The parameter space of astrometric searches gives greater sensitivity to planets at large radii than radial velocity searches. For the VLBA and the expanded VLBA, the targets of radio astrometric surveys are by necessity nearby, low-mass, active stars, which cannot be studied efficiently through the radial velocity method, coronagraphy, or optical interferometry. For the SKA, detection sensitivity will extend to solar-type stars. Planets discovered through radio astrometric methods will be suitable for characterization through extreme adaptive optics. The complementarity of radio astrometric techniques with other methods demonstrates that radio astrometry {{can play an important}} role in the roadmap for exoplanet discovery and characterization...|$|R
40|$|Process {{variations}} {{are a major}} bottleneck {{for digital}} CMOS integrated circuits manufacturability and yield. That is why regular techniques with different degrees of regularity are emerging as possible solutions. Our proposal is a new regular layout design technique called Via-Configurable Transistors <b>Array</b> (VCTA) that <b>pushes</b> to the limit circuit layout regularity for devices and interconnects {{in order to maximize}} regularity benefits. VCTA is predicted to perform worse than the Standard Cell approach designs for a certain technology node but it will allow the use of a future technology on an earlier time. Our objective is to optimize VCTA for it to be comparable to the Standard Cell design in an older technology. Simulations for the first unoptimized version of our VCTA of delay and energy consumption for a Full Adder circuit in the 90 nm technology node are presented and also the extrapolation for Carry-Ripple Adders from 4 bits to 64 bits. Peer ReviewedPostprint (published version...|$|R
40|$|Graphics Processing Units (GPUs) are {{powerful}} computing devices {{that with the}} advent of CUDA/OpenCL are becomming useful for general purpose computations. Obsidian is an embedded domain specific language that generates CUDA kernels from functional descriptions. A symbolic array construction allows us to guarantee that intermediate arrays are fused away. However, the current array construction has some drawbacks; in particular, arrays cannot be combined efficiently. We add a new type of <b>push</b> <b>arrays</b> to the existing Obsidian system in order to solve this problem. The two array types complement each other, and enable the definition of combinators that both take apart and combine arrays, and that result in efficient generated code. This extension to Obsidian is demonstrated on a sequence of sorting kernels, with good results. The case study also illustrates the use of combinators for expressing the structure of parallel algorithms. The work presented is preliminary, and the combinators presented must be generalised. However, the raw speed of the generated kernels bodes well...|$|R
40|$|Topographic {{maps are}} {{compiled}} by manually operated stereoplotters that recreate {{the geometry of}} two wide-angle overlapping stereo frame photographs. Continuous imaging systems such as strip cameras, electro-optical scanners, or linear <b>arrays</b> of detectors (<b>push</b> brooms) can also create stereo coverage from which topography can be compiled; however, the instability of an aircraft in the atmosphere makes this approach impractical. The benign environment of space permits a satellite to orbit the Earth with very high stability as long as no local perturbing forces are involved. Solid-state linear-array sensors have no moving parts and create no perturbing force on the satellite. Digital data from highly stabilized stereo linear arrays are amenable to simplified processing to produce both planimetric imagery and elevation data. A proposed satellite, called MAPSAT, could accomplish automated mapping in near real time. Image maps as large as 1 : 50, 000 scale with contours as close as 20 -m interval may be produced from MAPSAT data...|$|R
40|$|Abstract: The Telescope Array {{observatory}} {{has been}} collecting ultra high energy cosmic ray data since 2007. It {{consists of three}} telescope stations {{at the corners of}} a 30 km triangle and an array of 507 scintillator detectors filling the central part of this area. The scintillator detectors measure the footprint of the extensive air show generated when a cosmic ray interacts with the atmosphere. The 38 telescopes at the three stations observe the longitudinal development of the showers above the scintillator array. However, the existing experiment was designed with a threshold of 1019 eV. While {{we have been able to}} extend analysis down to about 1018 eV, this is insufficient to fully observe the galactic to extra galactic transition. In addition, it is optimal to observe cosmic rays from LHC energies through the second knee and up to the GZK cutoff with one well cross-calibrated detector. TALE, the low energy extension to the Telescope Array, is designed to lower the energy threshold to about 1016. 5 eV. To do this, we installed an additional 10 telescopes viewing up to 57 degrees in elevation and a new graded array of scintillator detectors. This extension will enable the Telescope Array to measure the energy and composition of cosmic rays to much lower energies while cross calibrated with the detectors of the main Telescope <b>Array.</b> By <b>pushing</b> the energy threshold down to 1016. 5 eV, we hope to sort out the galactic and extragalactic contributions to the cosmic ray flux. The detectors, their status, and first measurements will be presented...|$|R
40|$|Nowadays an {{increasing}} number of applications require high-performance analytical instruments capable to detect the temporal trend of weak and fast light signals with picosecond time resolution. The Time-Correlated Single-Photon Counting (TCSPC) technique is currently one of the preferable solutions when such critical optical signals have to be analyzed and it is fully exploited in biomedical and chemical research fields as well as in security and space applications. Recent progress in the field of single-photon detector <b>arrays</b> is <b>pushing</b> research towards the development of high performance multichannel TCSPC systems opening the way to modern time-resolved multi-dimensional optical analysis. In this paper we describe a new 8 -channel high-performance TCSPC acquisition system designed to be compact and versatile to be used in modern TCSPC measurement setups. We designed a novel integrated circuit including a multichannel Time-to-Amplitude Converter with variable full-scale range a D/A converter and a parallel adder stage. The latter is used to adapt each converter output to the input dynamic range of a commercial 8 -channel Analog-to-Digital Converter while the integrated DAC implements the dithering technique with as small as possible area occupation. The use of this monolithic circuit made the design of a scalable system of very small dimensions (95 × 40 mm) and low power consumption (6 W) possible. Data acquired from the TCSPC measurement are digitally processed and stored inside an FPGA (Field-Programmable Gate Array) while a USB transceiver allows real-time transmission of up to eight TCSPC histograms to a remote PC. Eventually the experimental results demonstrate that the acquisition system performs TCSPC measurements with high conversion rate (up to 5 MHz/channel) extremely low differential nonlinearity (< 0. 04 peak-to-peak of the time bin width) high time resolution (down to 20 ps Full-Width Half-Maximum) and very low crosstalk between channels...|$|R
40|$|The {{structural}} integrity of seamed fossil {{high energy piping}} {{has become a major}} safety and O&M issue again with eight recent failures of seam-welded piping since 1992. These include failure of six superheat link piping segments, two of them catastrophic, and the failures of two long-seamed bends in hot reheat lines. Advanced methods of inspecting piping welds with ultrasonic techniques, such as Time Of Flight Diffraction and Focused/Phased <b>Arrays,</b> is <b>pushing</b> back the envelop of detection to earlier stages of creep damage. But these are still very expensive and involve considerable logistics planning and downtime to perform. EPRI has sponsored development activities since 1986 to mature the utilization of a real-time online evaluation method for seam-welded piping: Acoustic Emission (AE) Guidelines were published in 1995, and over 90 full-scale tests have been performed from 1996 - 2003 to develop a database and correlate results with other established evaluation methods. Tests to date have shown high sensitivity to early stage creep damage, which is evidenced by development of cavities (cavitation) around nonmetallic inclusions and carbides in the grain boundaries of the weld heat affected zone and fusion zone. Successful double-blind testing with advanced ultrasonsic inspection methods, and additional confirmation with advanced cryo-cracking metallography, have proven both the reliability and sensitivity of the AE technique. The economics of the method are highly favorable. Only small areas of insulation need to be removed every 4. 6 - 6. 1 m (15 - 20 ft) to weld “waveguides” to the piping surface. These form a linear location array along the length of piping, providing global coverage of the piping system. Testing is performed online with normal peak loading and load cycling. No outage schedule is required to perform the AE examination. The ASTM E 07. 04 Subcommittee on Acoustic Emission is currently developing a standard based on the EPRI testing database...|$|R
40|$|The {{structural}} integrity of seam-welded fossil high-energy piping {{has remained a}} major safety and operations-maintenance issue for US utility companies. Several failures of seamwelded superheat and hot reheat piping segments have occurred since 1992, two of them catastrophic. Advanced methods of inspecting piping welds with ultrasonic testing (UT), such as time-of-flight diffraction and focused/phased <b>arrays,</b> are <b>pushing</b> back the envelope of detection to earlier stages of creep damage, but these are still very expensive, and involve considerable logistical planning and downtime to perform. The Electric Power Research Institute (EPRI) has sponsored development activities since 1986 to mature the utilization of a real-time online evaluation method for seam-welded piping: Acoustic emission (AE) testing guidelines were published by EPRI in 1995, and over 100 full-scale tests have been performed to develop a database and correlate results with other established evaluation methods. An effort was begun in 2002 to standardize the testing method within ASTM (AE Subcommittee of E 07. 04) utilizing the developed database. Tests to date have shown high sensitivity to early stage creep damage, which is evidenced by development of cavities around inclusions in the grain boundaries. Successful double-blind testing with advanced ultrasonic inspection methods has proven both the reliability and sensitivity of the AE technique. The economics of the method are highly favorable. Only small areas of insulation need to be removed every 4. 6 - 6 m to weld waveguides to the piping surface. These form a linear location array {{along the length of}} piping, providing global coverage of the piping system. Testing is performed online with normal peak loading and load cycling. No outage schedule is required to perform the AE examination. Results will be presented showing that the AE method has become a reliable and economical field evaluation tool for seam-welded high energy piping. Keywords: Seam-welded piping, fossil power plants, online monitoring, high temperature cree...|$|R
40|$|New telescopes {{like the}} Square Kilometre <b>Array</b> (SKA) will <b>push</b> {{into a new}} {{sensitivity}} regime and expose systematics, such as direction-dependent effects, that could previously be ignored. Current methods for handling such systematics rely on alternating best estimates of instrumental calibration and models of the underlying sky, {{which can lead to}} inadequate uncertainty estimates and biased results because any correlations between parameters are ignored. These deconvolution algorithms produce a single image that is assumed to be a true representation of the sky, when in fact it is just one realization of an infinite ensemble of images compatible with the noise in the data. In contrast, here we report a Bayesian formalism that simultaneously infers both systematics and science. Our technique, Bayesian Inference for Radio Observations (BIRO), determines all parameters directly from the raw data, bypassing image-making entirely, by sampling from the joint posterior probability distribution. This enables it to derive both correlations and accurate uncertainties, making use of the flexible software meqtrees to model the sky and telescope simultaneously. We demonstrate BIRO with two simulated sets of Westerbork Synthesis Radio Telescope data sets. In the first, we perform joint estimates of 103 scientific (flux densities of sources) and instrumental (pointing errors, beamwidth and noise) parameters. In the second example, we perform source separation with BIRO. Using the Bayesian evidence, we can accurately select between a single point source, two point sources and an extended Gaussian source, allowing for ‘super-resolution' on scales much smaller than the synthesized bea...|$|R
40|$|The {{mainframe}} of {{this thesis}} is about DNA nanotechnology in {{the perspectives of}} materials science and molecular architecture. My first project was to develop a new motif for structural DNA nanotechnology. The new motif, derived from the triple crossover motif, is rigid and triangular prism shaped. The corresponding 1 D and 2 D arrays were self-assembled and observed by transmission electron microscopy, whereas the 3 D arrays are still in blueprints. With a strong interest in 3 D DNA architecture, I developed a 3 D DNA object. In the study, a specific single-strand DNA was designed to self-fold into a tetrahedron. The formation of the tetrahedron was proved by ligation, and more implicitly, by restriction enzyme digestion. It is the first single strand 3 D DNA geometrical object that has been obtained insofar. Along the road of DNA nanotechnology research, we found it necessary to develop a computer system for structural DNA nanotechnology design. One such system, Uniquimer, was developed by our team and used for the sequence design in our DNA research. To meet the new challenges of structural DNA nanotechnology design, we have upgraded Uniquimer to Uniquimer 3 D, a system with 3 D visualization, internal energy minimization, sequence generation, and motif <b>array</b> simulation functionalities. <b>Pushing</b> my graduate study forward, I took part in the project of investigating DNA hydrogel’s applications in drug delivery. In our hydrogel system, aptamer segment was engineered into the crosslinker DNA to enable its specific binding with target protein. Sol-gel transition and protein capture/release were carefully studied. Most of these projects are still open for further exploration and I feel very lucky to have initiated all these possibilities for exciting future scientific adventures in the new frontiers...|$|R

