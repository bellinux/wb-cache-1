37|43|Public
50|$|The {{frequency}} {{range of a}} system is the range over which it is considered to provide satisfactory performance, such as a useful level of signal with <b>acceptable</b> <b>distortion</b> characteristics. A listing of {{the upper and lower}} limits of frequency limits for a system is not useful without a criterion for what the range represents.|$|E
50|$|The rugged {{elements}} of moving-coil microphones {{can have a}} dynamic range of up to 140 dB (at increased distortion), while condenser microphones are limited by the overloading of their associated electronic circuitry. Practical considerations of <b>acceptable</b> <b>distortion</b> levels in microphones combined with typical practices in a recording studio result in a useful operating range of 125 dB.|$|E
50|$|Typically, an amplifier's power {{specifications}} {{are calculated}} by measuring its RMS output voltage, with a continuous sine wave signal, {{at the onset}} of clipping—defined arbitrarily as a stated percentage of total harmonic distortion (THD), usually 1%, into specified load resistances. Typical loads used are 8 and 4 ohms per channel; many amplifiers used in professional audio are also specified at 2 ohms. Considerably more power can be delivered if distortion is allowed to increase; some manufacturers quote maximum power at a higher distortion, like 10%, making their equipment appear more powerful than if measured at an <b>acceptable</b> <b>distortion</b> level.|$|E
3000|$|During {{the course}} of transmission, the {{original}} music signal might experience various <b>acceptable</b> signal <b>distortions</b> or malicious cropping, adding, replacing, etc. Therefore, at the verification end, the received music will not be segmented into exactly {{the same set of}} beats as the original ones in most cases. That is, let B = {B [...]...|$|R
40|$|In this paper, we {{proposed}} a hybrid data hiding scheme to embed secret data into block truncation coding codes. The hybrid scheme is a combined lossless and lossy hiding with considerations on <b>acceptable</b> <b>distortions.</b> The secret data was first embedded lossless in the complex image blocks and any amounts exceeding would be lossy embedded based on optimal bitmap replacement on the smooth image blocks. The amount of hiding is adjustable based on the acceptable PSNRs. The embedded secret bits can be extracted directly from the compressed domain without the original cover image. Experimental {{results showed that the}} visual quality of reconstructed images had significantly higher PSNRs. Test results from eight images showed an average gain of 0. 05 bpp in payloads. Furthermore, the recovered images showed significantly higher PSNRs for the same amount of payloads. For example, Lena showed a gain of 3. 9 dB and 1. 4 dB under the embedding capacity 0. 2 bpp and 0. 8 bpp, respectively...|$|R
40|$|This paper {{focuses on}} the use of nested lattice codes for {{effective}} analysis and design of semi-fragile watermarking schemes for content authentication applications. We provide a design framework for digital watermarking which is semi-fragile to any form of <b>acceptable</b> <b>distortions,</b> random or deterministic, such that both objectives of robustness and fragility can be effectively controlled and achieved. Robustness and fragility are characterized as two types of authentication errors. The encoder and decoder structures of semi-fragile schemes are derived and implemented using nested lattice codes to minimize these two types of errors. We then extend the framework to allow the legitimate and illegitimate distortions to be modelled as random noise. In addition, we investigate semifragile signature generation methods such that the signature is invariant to watermark embedding and legitimate distortion. A new approach, called MSB signature generation, is proposed which is shown to be more secure than the traditional dual subspace approach. Simulations of semi-fragile systems on real images are provided to demonstrate the effectiveness of nested lattice codes in achieving design objectives. Keywords: Semi-fragile Watermarking, Authentication, Lattice Codes 1...|$|R
50|$|Prior to WWII, {{almost all}} {{electronic}} amplifiers were triodes used without feedback. The inherent, albeit imperfect, linearity of tubes {{makes it possible}} to get <b>acceptable</b> <b>distortion</b> performance without correction. Amplitude distortion in a class A triode stage can be small if care is taken to prevent the anode current from becoming too small and ensuring that grid current does not flow at any point. In this case, distortion is largely relatively unobjectionable second harmonic, with percentage closely proportional to the output amplitude. Adding modest negative feedback improves linearity further. Pentodes of the same power dissipation are capable of higher output power than triodes, but distortion is higher and more objectionable.|$|E
30|$|We {{consider}} that all sensor values are correct, without noise. As the sensed process is reconstructed at sink, DECA imposes that the reconstruction error {{has to be}} kept within an <b>acceptable</b> <b>distortion</b> criterion, defined by δ.|$|E
3000|$|... must {{consider}} how the sink reconstructs the process from the measurements the sink receives. As a consequence, in order to guarantee a trusty reconstruction of the process, we impose the constraint of keeping the reconstruction error within an <b>acceptable</b> <b>distortion</b> criterion at the sink and not solely at the sensor. Therefore, since each node takes its own decision, the proposed strategy is decentralized and distributed.|$|E
50|$|The lens {{body has}} a plastic construction, {{including}} the lens mount. Generally, however, {{this version of}} the lens is soft and must be stopped down to gain <b>acceptable</b> sharpness. Barrel <b>distortion</b> becomes quite noticeable at the wide-angle setting and chromatic aberration (purple fringing) is common.|$|R
3000|$|... where Rbudget is the {{available}} total bit rate to encode two descriptions and dbudget is the maximum <b>distortion</b> <b>acceptable</b> for central decoder reconstruction. The encoding optimization module {{is based on}} the above function. With the constrain on the total bit rate and the central distortion, N is adjusted accordingly to minimize the side distortion.|$|R
40|$|The {{control of}} {{harmonics}} in power systems {{continues to be}} a major concern in the telecommunications industry. AC/DC telecommunication conversion equipment has rarely been thought of as playing {{a major role in the}} harmonic interaction problem. Yet, in today's sensitive electronics, the input conversion is likely to generate high harmonic currents upstream into the power source. The output side may be expected to handle a non-sinusoidal current and is to do so without causing voltage distortion. This paper reviews the origin and causes of harmonics, the bad effects of harmonics, the <b>acceptable</b> harmonic <b>distortion</b> limits in the telecommunication power system and the best methods for harmonic detection and mitigation...|$|R
30|$|We should {{highlight}} that DECA {{differs from}} existing proposals {{in the literature}} as (i) DECA is applied in the application layer; (ii) DECA does not impose an uniform sampling interval for the measurements time-series; (iii) by design, DECA imposes the reconstruction error of the sensed variable to be within an <b>acceptable</b> <b>distortion</b> criterion; (iv) DECA is decentralized and distributed, since each and every node decides by itself for how long to sleep, although considering information about the nodes it forwards packets for; and (v) DECA uses a deterministic approach for computing the node sleeping/inactivity period.|$|E
40|$|International audienceIn this paper, a blind digital {{watermarking}} scheme for Portable Document Format (PDF) documents is proposed. The proposed method {{is based on}} a variant Quantization Index Modulation (QIM) method called Spread Transform Dither Modulation (STDM). Each bit of the secret message is embedded into a group of characters, more specifically in their x-coordinate values. The method exhibits experiments of two opposite objectives: transparency and robustness, and is motivated to present an <b>acceptable</b> <b>distortion</b> value that shows sufficient robust-ness under high density noises attacks while preserving sufficient transparency...|$|E
40|$|Rapid {{growth in}} {{wireless}} networks is fueling demand for video services from mobile users. In the wireless environment, transmission power management {{is an important}} consideration. In this paper, we consider {{the interaction of the}} video coding and transmission parameters. We formulate an optimization problem that corresponds to minimizing the transmission energy required to deliver a video frame with an <b>acceptable</b> <b>distortion</b> and delay. We present a Dynamic Programming formulation that jointly considers the number of macroblocks and coding parameters of each video packet as well as the transmission rate. We present results illustrating the advantage of adjusting the number of MBs per video packet. The proposed approach reduces energy consumption by 20 - 25 %...|$|E
40|$|Steganography {{techniques}} are useful to convey hidden information using {{various types of}} typically-transmitted multimedia data as cover file to conceal communication. When using JPEG as a cover file, normally message is hidden in the AC values of quantized DCT coefficients. However, concealment of message in quantization table {{is yet to be}} done. In this paper, a novel in-DQT technique for message hiding in JPEG’s quantization table using bytewise insertion is proposed. By using in-DQT technique on standard JPEG test images for cover files with up to 24 bytes of message, results show that steganography image with minimal <b>acceptable</b> image <b>distortion.</b> Thus, in-DQT technique provides alternative for embedding message in quantization table...|$|R
40|$|NSF of China [60972053]; CSTC (Chongqing Science & Technology Commission) [2010 AC 3060]; NHA University; Hong Kong Research Grants Council [CityU 117 / 10 E]Designing a non-ideal {{delay line}} (DL) with phase {{distortion}} in a transmitted-reference ultra-wideband system with an autocorrelation receiver {{is a great}} technical challenge. Differing from the currently empirical design method of DL, a semi-analytic approach is proposed through Gaussian approximation of the expression for conditional bit error rate (BER), based on investigation {{on the degradation of}} average BER caused by a group delay ripple range (GDRR) over independent Nakagami-m fading channels. This GDRR-based design method can directly evaluate its effects on the system performance and determine the <b>acceptable</b> phase <b>distortion</b> level to trade-off the BER performance and system complexity...|$|R
40|$|A few {{water-based}} polymer formulations with precipitation {{temperature of}} 74 'C have been, arrived at for the heat treatment of Al- based Aircraft alloys. Two {{of them were}} prepared in bulk and tested as quenching media for L- 73 sheets and L- 65 bars/angles by the Heat Treatment Section of the HAL, Bangalore. While the mechanical properties of the quenched alloys were <b>acceptable,</b> the <b>distortion</b> of the sheets quenched in NAL formulations was more. The latter was assigned by the HAL engineers to the lower viscosity of our formulations. This has been improved later and some quenching studies made in NAL. Further heat treatment, tests, etc. and improvement are {{to be done in}} co-operation with the HAL engineers and funding. The formulations can replace the presently used UCON Quenchant A, imported from the UNION CARBIDE CORPORATION, U. S. A...|$|R
40|$|A {{wideband}} imaging architecture {{based on}} subbanding and Doppler filter bank using FFT is developed and its performance is analyzed in detail. The theoretical analysis shows that owing to Doppler dispersion, the target’s range profile will produce distortion {{with the increase}} in target velocity. The distortion includes range shift and amplitude deformation. At the same time, two related theoretical formulas are deduced for the calculation of the range shift value and evaluation of the amplitude deformation extent of a moving target's range profile formed by the proposed imaging architecture and thereby the maximum critical velocity is derived. When target velocity is less than the maximum critical velocity, a moving target’s range profile with <b>acceptable</b> <b>distortion</b> can be obtained. Specific conclusions are verified with some simulations...|$|E
40|$|Abstract—We {{introduce}} {{the concept of}} depth-based blurring to achieve an aesthetically <b>acceptable</b> <b>distortion</b> when reducing the bitrate in image coding. The proposed depth-based blurring is a prefiltering that reduces high frequency components by mimicking the limited depth of field effect that occurs in cameras. To cope {{with the challenge of}} avoiding intensity leakage at the boundaries of objects when blurring at different depth levels, we introduce a selective blurring algorithm that simulates occlusion effects as occur in natural blurring. The proposed algorithm can handle any number of blurring and occlusion levels. Subjective experiments show that the proposed algorithm outperforms foveation filtering, which is the dominant approach for bitrate reduction by space-variant prefiltering. Index Terms—Image coding, depth of field, foveated image coding, space-variant image processing. I...|$|E
40|$|The latest base sta-This article {{provides}} design tion amplifier de-and measured perforsigns for the wiremance details for a power less telecom industry amplifier using a recently- offer much higher data developed technique for rate services with lower distortion cancellation levels of distortion than preceding designs, providing high efficiency and reliable long life at lower cost. These amplifiers are also {{required to be}} economical and work reliably over {{a wider range of}} environmental conditions than the typical air-conditioned ground-based stations. Efficiency and thermal issues are critical to the type of power amplifier selected for these newer applications. There are many amplifier types and architectures used for amplifying complex modulated signals. Each type has different levels of efficiency, <b>acceptable</b> <b>distortion,</b> complexity and cost...|$|E
40|$|The peak-to-average ratio (PAR) of {{a signal}} is {{commonly}} used for estimating the backoff required for an radio-frequency/microwave power amplifier to exhibit <b>acceptable</b> intermodulation <b>distortion.</b> In this paper, it is shown that the PAR is an inaccurate metric for predicting the backoff {{and can lead to}} improper choices for modulation with respect to the linearity [...] efficiency tradeoff. A specific case is presented, based on the IS- 94 code-division multiple-access communication (CDMA) reverse-link and IS- 95 CDMA forward-link wireless standards. Using simulation and load [...] pull measurements, it is illustrated that although quaternary phase-shift keying has a higher PAR than offset QPSK (O-QPSK), it has lower adjacent-channel power, for constant average power. This observation, contrary to what is expected, is explained by introducing the envelope distribution function, which characterizes the saturation of an amplifier based on the time-domain statistics of the applied signal. Index Terms- [...] ...|$|R
40|$|Transmitter ener!ly is a {{valuable}} resource in wireless networks. Transmitter power management {{can have an impact}} on battery life for mobile users, link level Qo$ and network capacity. We consider elcient use of transmitter ener!ly in a streaming application. We formulate an optimization problem that cor'esponds to minimizing the transmission ener!ly required to achieve an <b>acceptable</b> level of <b>distortion</b> subject to a delay constraint. By considering jointly the selection of coding parameters and transmitter power we can formulate an optimal policy. We present results illustrating the advantages of jointly considering these two variables...|$|R
30|$|This {{criterion}} {{measures the}} perceptual {{distortion of the}} cipher image (or video) {{with respect to the}} plain image (or video). It assumes that the cipher image (or video) can be decoded and viewed without decryption. This assumption is not satisfied for all existing algorithms. In some applications, it could be desirable to achieve enough visual degradation, so that an attacker would still understand the content but prefer to pay to access the unencrypted content. However, for sensitive data (e.g., military images/videos), high visual degradation could be desirable to completely disguise the visual content. For this reason, tunability property is very important to be able to tune the visual degradation of the encrypted content depending on the target application and requirements. The peak signal-to-noise ratio (PSNR) is the main metric used in the literature to measure visual degradation. Visual degradation is a subjective criterion that is why it is difficult to define a threshold for <b>acceptable</b> visual <b>distortion</b> regarding a given application.|$|R
40|$|The optimal {{scheduling}} of load {{tap changers}} (LTCs) and switched shunt capacitors for simultaneously minimizing energy loss and improving voltage profile while taking harmonics into account {{is performed using}} evolutionary-based algorithms (EAs). The proposed algorithm is capable of optimizing large distribution systems with different types of nonlinear loads. The decouple approach is employed for harmonic power-flow calculations, while two EAs are developed to determine the load interval division and near-optimal schedule. The inclusion of harmonics provides the optimization benefits while maintaining the <b>acceptable</b> <b>distortion</b> levels. The scheduling is carried out for the IEEE 123 -bus with 14 shunt capacitors and 12 nonlinear loads. The main contributions are inclusion of harmonics in the optimal scheduling problem and its application to large distribution systems with multiple nonlinear loads...|$|E
40|$|A paintbrush-like image {{transformation}} is proposed in this paper. It {{is based on}} a random searching to insert brush-strokes into a generated image at decreasing scale of brush-sizes, without predefined models or interaction. One of the goals of the method is to transform the image into a representation that is very similar to the human sensation of artistic images. We introduce a sequential multiscale image decomposition method, based on simulated rectangular-shaped paintbrush strokes. The resulting images look like good-quality paintings with well-defined contours, at an <b>acceptable</b> <b>distortion</b> compared to the original image. The image can be described with the parameters of the consecutive paintbrush strokes, resulting in a parameter-series {{that can be used for}} compression. The painting process can be applied for scale-space image representation, segmentation and contour detection, and image representation for retrieval purposes...|$|E
40|$|We are {{studying}} some fundamental {{properties of the}} interface between control and data planes in Information-Centric Networks. We try to evaluate the traffic between these two planes based on allowing a minimum level of <b>acceptable</b> <b>distortion</b> in the network state representation in the control plane. We apply our framework to content distribution, and {{see how we can}} compute the overhead of maintaining the location of content in the control plane. This is of importance to evaluate content-oriented network architectures: we identify scenarios where the cost of updating the control plane for content routing overwhelms the benefit of fetching a nearby copy. We also show how to minimize the cost of this overhead when associating costs to peering traffic and to internal traffic for operator-driven CDNs. Comment: 10 pages, 12 figure...|$|E
40|$|International audienceThis paper {{presents}} an efficient algorithm {{for a global}} parameterization of triangular surface meshes. In contrast to previous techniques which achieve global parameterization through the optimization of non-linear systems of equations, our algorithm is solely based on solving at most two linear equation systems, in the least square sense. Therefore, in terms of running time the unfolding procedure is highly efficient. Our approach is direct – it solves for the planar UV coordinates of each vertex directly – hence avoiding any numerically challenging planar reconstruction in a post-process. This results in a robust unfolding algorithm. Curvature prescription for user-provided cone singularities can either be specified manually, or suggested automatically by our approach. Experiments {{on a variety of}} surface meshes demonstrate the runtime efficiency of our algorithm and the quality of its unfolding. To demonstrate the utility and versatility of our approach, we apply it to seamless texturing. The proposed algorithm is computationally efficient, robust and results in a parameterization with <b>acceptable</b> metric <b>distortion...</b>|$|R
40|$|An {{effective}} way to minimise harmonic pollution in power systems is by careful design of the equipment connected to them. It is important for designers of equipment associated with emerging technologies {{to be aware of}} the potential impact of their designs on power system quality. One such upcoming technology is electric vehicle (EV) battery charging which may contribute to high harmonic distortion in the power system during the charging period. The literature notes total harmonic distortion of up to 50 %. These findings are the impetus behind the present paper, where an EV battery charger has been designed, with an inherent power quality control feature. A parallel power circuit topology has been proposed on an existing ferroresonant charger, which ensures that the THD of the input current remains within the <b>acceptable</b> harmonic <b>distortion</b> limits of the distribution system. The design and control of the battery charger are elaborated upon in the paper and simulation results are presented which confirm the performance of the charger...|$|R
40|$|This paper {{presents}} an efficient algorithm {{for a global}} parameterization of triangular surface meshes. In contrast to previous techniques which achieve global parameterization through the optimization of non-linear systems of equations, our algorithm is solely based on solving at most two linear equation systems, in the least square sense. Therefore, in terms of running time the unfolding procedure is highly efficient. Our approach is direct – it solves for the planar UV coordinates of each vertex directly – hence avoiding any numerically challenging planar reconstruction in a post-process. This results in a robust unfolding algorithm. Curvature prescription for user-provided cone singularities can either be specified manually, or suggested automatically by our approach. Experiments {{on a variety of}} surface meshes demonstrate the runtime efficiency of our algorithm and the quality of its unfolding. To demonstrate the utility and versatility of our approach, we apply it to seamless texturing. The proposed algorithm is computationally efficient, robust and results in a parameterization with <b>acceptable</b> metric <b>distortion...</b>|$|R
40|$|The {{field of}} image and video {{compression}} {{has gone through}} rapid growth during the past thirty years, leading to various coding standards. The main goal of continuous efforts on image/video coding standardization is to achieve low bit rate for data storage and transmission, while maintaining <b>acceptable</b> <b>distortion.</b> In this paper, various developmental stages of image and video compression standards are reviewed, including JPEG and JPEG 2000 image standards, MPEG- 1, MPEG- 2, MPEG- 4, H. 261, H. 263, H. 264 /MPEG- 4 AVC, and the latest international video standard HEVC as well as Chinese video coding standard AVS. Key features and major applications of the standards will be briefly introduced and the compression performance of the standards at each stage will be compared and discussed. © 2013 APSIPA...|$|E
30|$|Most of the {{above-mentioned}} steganographic methods use the nonoverlapping {{blocks of the}} DCT coefficients for hiding secret message. Such a blockwise embedding scheme divides both the stream of the DCT coefficients and hidden message into the separate blocks and solves the equations for hiding data for each block individually. Recent methods like MME [14], BCH-based steganography methods [15 – 17] may produce several alternative solutions. Thus, such a data hiding method can choose a solution with the lowest distortion impact. Past investigation over the BCH data hiding scheme finds that BCH usually allows redundant number of possible solutions. It means that a solution with <b>acceptable</b> <b>distortion</b> impact can be achieved from the reduced set of possible solutions. Hence, the embedding efficiency of the BCH steganographic methods can be increased by {{reducing the number of}} possible solutions and keeping similar distortion impact compared to the original approach.|$|E
40|$|Optical pulse {{distortion}} and propagation through quasi-periodic structures generally {{and especially for}} Fibonacci-class, is investigated analytically and simulated numerically. In this analysis, the transfer matrix method (TMM) for distortion evaluation is used. The simulated results for Fibonacci-class quasi-periodic structures and pure periodic multilayer stacks are compared. We show that the Fibonacci-class quasi-periodic structure has a large dispersion coecient {{with respect to a}} similar case in the periodic structure. So, quasi-periodic structures will destroy the incident pulse shape for smaller stack length than a periodic case in a similar situation. Finally, using output power, the distortion limit can be estimated and the maximum number of layers with <b>acceptable</b> <b>distortion</b> can be determined. Also, we have calculated the second order (D) and third order (B) dispersion coecients for Fibonacci-class quasi-periodic structures around 1 : 55 m for dispersion compensation purposes. Key Words: Pulse distortion, dispersion, quasi-periodic structures, and Fibonacci-class. 1...|$|E
40|$|Distributors need to {{allocate}} a maximum allowed level of harmonic current to medium-voltage customers to keep voltage <b>distortion</b> <b>acceptable.</b> Tlie paper describes a new approach, {{based on the}} concept of voltage droop, requiring much less calculation and data than required by the present approach based on an IEC technical report. The discrepancy between the new method and the present is studied by comparing some carefully selected scenarios. It is shown that the proposed method gives results within 20 % of the standards-based approach which makes it a very attractive alternative for harmonic allocation. © Institution of Engineers Australia, 2013...|$|R
3000|$|We {{present and}} {{evaluate}} {{the performance of a}} reduced complexity variation to the source encoding assisted multiple access (SEAMA) protocol for integrating voice and data over a wireless network. This protocol, denoted as slow movable-boundary SEAMA (SMB-SEAMA), uses the same embedded and multistate voice encoder used in the original SEAMA protocol. However, in SMB-SEAMA, the movable voice/data boundary is not set based on the frame-by-frame bandwidth demand of the voice subsystem, but on the number of ongoing voice calls and the <b>acceptable</b> average <b>distortion</b> level. This results in a protocol that, at the network layer, is packet switched for both voice and data; however, from the data traffic point of view, voice looks like circuit switched. Analytical results show that SMB-SEAMA is a very efficient MAC protocol and present a model for analyzing the performance of queuing systems with a variable number of servers, each with constant service time. Consequently, while reducing the refreshing rate of the movable boundary by three orders of magnitude, simulation results demonstrate that SMB-SEAMA does not significantly degrade the system performance (less than [...]...|$|R
40|$|HEVC and VP 9 are {{the current}} {{state-of-the-art}} in video compression, since their bit-streams were recently finalized in January and May 2013, respectively. These codecs are the generational successors of the currently most widely-used video codecs, H. 264 /AVC and VP 8, respectively. Consequently, {{it is expected}} that in the near future these new codecs will replace their predecessors. However, the process of converting video contents compressed with one standard to those using another standard is highly computationally expensive, since a priori the video contents must be decompressed and compressed with the target video encoder. Nevertheless, it is known that some information can be extracted from the decoding process in order to accelerate the encoding process. In this paper, some techniques for transcoding from HEVC to VP 9 are presented. By using some information extracted from the HEVC decoding process some coding modes will be discarded from being checked in the VP 9 encoder. By applying the proposed approaches, a reduction of about 60 % of the coding complexity is achieved with <b>acceptable</b> Rate <b>Distortion</b> penalties...|$|R
