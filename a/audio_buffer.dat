10|33|Public
25|$|The new audio stack runs at user level, thus {{increasing}} stability. The Windows Vista audio {{engine is}} designed to run faster than the Windows XP audio engine, and has tighter requirements on <b>audio</b> <b>buffer</b> position accuracy. Also, the new Universal Audio Architecture (UAA) model has been introduced, replacing WDM audio, which allows compliant audio hardware to automatically work under Windows without needing device drivers from the audio hardware vendor.|$|E
25|$|A natural {{development}} from digital delay-processing hardware was {{the appearance of}} software-based delay systems. In large part, this coincided with the popularity of both professional and consumer audio editing software. Software delays, in many cases, offer much greater flexibility than even the most recent digital hardware delays. Abundant system memory on modern personal computers offers practically limitless storage for the <b>audio</b> <b>buffer,</b> and the natural efficiency of audio delay algorithms has made the implementation trivial for delays offering shifting or random delay times, or the insertion of other audio effects during the feedback process. Many authors of software plugins have added functionality to emulate {{the sounds of the}} earlier analog units.|$|E
50|$|There {{are several}} ways to keep the {{computer}} sending audio data to the modem at a rate to keep up with playback without overrunning the <b>audio</b> <b>buffer.</b>|$|E
2500|$|On {{supported}} sound cards, DirectSound {{would try}} to use [...] "hardware accelerated" [...] buffers, i.e. the ones which either can be placed in local sound card memory, or can be accessed by the sound card from the system memory. If hardware acceleration is not available, DirectSound would create <b>audio</b> <b>buffers</b> in the system memory and use purely software mixing.|$|R
40|$|International audienceIn {{this article}} we present an API {{and a set of}} Javascript modules for the {{synchronized}} scheduling and aligned playback of predetermined sequences of events such as notes, audio segments, and parameter changes as well as media streams (e. g. <b>audio</b> <b>buffers)</b> based on the Web Audio API logical time. The API has been designed to facilitate the development on both ends, the implementation of modules which generate event sequences or media streams as well as the integration of such modules into complex audio applications that require flexible scheduling, playback and synchronization...|$|R
50|$|The general {{functionality}} of OpenAL is {{encoded in}} source objects, <b>audio</b> <b>buffers</b> {{and a single}} listener. A source object contains a pointer to a buffer, the velocity, position {{and direction of the}} sound, and the intensity of the sound. The listener object contains the velocity, position and direction of the listener, and the general gain applied to all sound. <b>Buffers</b> contain <b>audio</b> data in PCM format, either 8- or 16-bit, in either monaural or stereo format. The rendering engine performs all necessary calculations as far as distance attenuation, Doppler effect, etc.|$|R
5000|$|Audio Interface (AI) reads {{data from}} the <b>audio</b> <b>buffer</b> using a fixed time interval, and sends it to the DA (digital-to-analog) {{converter}} (audio DAC) to produce the sound output.|$|E
50|$|A natural {{development}} from digital delay-processing hardware was {{the appearance of}} software-based delay systems. In large part, this coincided with the popularity of both professional and consumer audio editing software. Software delays, in many cases, offer much greater flexibility than even the most recent digital hardware delays. Abundant system memory on modern personal computers offers practically limitless storage for the <b>audio</b> <b>buffer,</b> and the natural efficiency of audio delay algorithms has made the implementation trivial for delays offering shifting or random delay times, or the insertion of other audio effects during the feedback process. Many authors of software plugins have added functionality to emulate {{the sounds of the}} earlier analog units.|$|E
50|$|Windows Vista {{features}} a completely re-written audio stack {{designed to provide}} low-latency 32-bit floating point audio, higher-quality digital signal processing, bit-for-bit sample level accuracy, up to 144 dB of dynamic range and new audio APIs created by a team including Steve Ball and Larry Osterman.The new audio stack runs at user level, thus increasing stability. The Windows Vista audio engine is designed to run faster than the Windows XP audio engine, and has tighter requirements on <b>audio</b> <b>buffer</b> position accuracy. Also, the new Universal Audio Architecture (UAA) model has been introduced, replacing WDM audio, which allows compliant audio hardware to automatically work under Windows without needing device drivers from the audio hardware vendor.|$|E
50|$|Because {{of audio}} data compression, optical discs {{do not have}} to spin all of the time, {{potentially}} saving battery power; however, decompressing the audio takes more processor time. The <b>audio</b> is <b>buffered</b> in random-access memory, which also provides protection against skipping.|$|R
40|$|Abstract. We {{present a}} HMM based system for {{real-time}} gesture analysis. The system outputs continuously parameters {{relative to the}} gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system {{in the context of}} music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing <b>audio</b> <b>buffers</b> or videos...|$|R
50|$|Alternately, Paula may {{signal the}} CPU to load a new sample {{into any of}} the four <b>audio</b> output <b>buffers</b> by {{generating}} an interrupt when a new sample is needed. This allows for output rates that exceed 57 kHz per channel and increases the number of possible voices (simultaneous sounds) through software mixing.|$|R
50|$|Computers can use {{software}} to generate sounds, {{which are then}} passed through a digital-to-analog converter (DAC) to a power amplifier and loudspeaker system. The number of sounds that can be played simultaneously (the polyphony) {{is dependent on the}} power of the computer's CPU, as are the sample rate and bit depth of playback, which directly affect the quality of the sound. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause synchronization problems, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay known as latency in their sound generation, because computers use an <b>audio</b> <b>buffer</b> that delays playback and disrupts MIDI timing.|$|E
30|$|Audio frame: A {{group of}} {{consecutive}} audio buffers. All the algorithms described here operate on overlapping, fixed-sized frames of audio. These frames are four audio buffers (2, 048 samples) in duration, {{consisting of the}} most recent <b>audio</b> <b>buffer</b> which is passed directly to the algorithm, combined with the previous three buffers which are saved in memory. The start of each frame is separated by a fixed number of samples, which is equal to the buffer size.|$|E
40|$|In {{this paper}} we {{introduce}} {{a simple and}} fast method for realtime recognition of multiple pitches produced by multiple musical instruments. Our proposed method is based on two important facts: (1) that timbral information of any instrument is pitch-dependant and (2) that the modulation spectrum of the same pitch seems to result into a persistent representation {{of the characteristics of}} the instrumental family. Using these basic facts, we construct a learning algorithm to obtain pitch templates of all possible notes on various instruments and then devise an online algorithm to decompose a realtime <b>audio</b> <b>buffer</b> using the learned templates. The learning and decomposition proposed here are inspired by non-negative matrix factorization methods but differ by introduction of an explicit sparsity control. Our test results show promising recognition rates for a realtime system on real music recordings. We discuss further improvements that can be made over the proposed system. 1...|$|E
40|$|The {{end to end}} {{delay is}} a {{critical}} factor in the perceived quality of service for Voice over IP applications. Sics ophone is a complete VoIP system that couples the low level features of audio hardware with a standard jitter buffer playout algorithm. Using the sound card directly eliminates intermediate buffering as well as providing fine control over timers needed by a soft real-time application such as VoIP. A statistical based approach for inserting packets into <b>audio</b> <b>buffers</b> is used in conjunction with a scheme for inhibiting unnecessary fluctuations in the system. We also present mouth-to-ear delay measurements for selected VoIP applications and show that several hundreds of milliseconds can be saved by using the techniques described in this paper. A prototype for both UNIX and Windows platforms has been implemented, demonstrating that our system adapts to network conditions whilst maintaining low delays...|$|R
40|$|Existing {{real-time}} audio rendering architectures provide rigid development frameworks {{which are}} not adapted {{to a wide range}} of applications. In particular, experimenting with new rendering techniques is virtually impossible. In this paper, we present a novel, platform-independent software architecture that is well suited for experimenting with multichannel audio mixing, geometrical acoustics and 3 D audio processing in a single framework. Our architecture is divided into two layers. A low level DSP layer is responsible for streaming and processing <b>audio</b> <b>buffers</b> using a general filter-based formalism. Built on top is an audio rendering layer responsible for general geometry-based audio rendering and configuration of the rendering setup. In particular, we introduce sequence objects which we use to define and control arbitrary sound propagation paths in a geometrical 3 D virtual environment. We discuss implementation details and present a variety of prototype applications which can be efficiently designed within the proposed framework. 1...|$|R
30|$|The {{time between}} an onset {{occurring}} in the input audio stream and the system correctly registering an onset occurrence must {{be no more than}} 50 ms. This value was chosen to allow for the difficulty in specifying reference onsets, which is described in more detail in Section 2.1. 1. All of the onset-detection schemes that are described in this article have latency of 1, 024 samples (the size of two <b>audio</b> <b>buffers),</b> except for the peak amplitude difference method (given in Section 4.3) which has an additional latency of 512 samples, or 1, 536 samples of latency in total. This corresponds to latency times of 23.2 and 34.8 ms respectively, at a sampling rate of 44.1 kHz. The reason for the 1, 024 sample delay on all the onset-detection systems is explained in Section 2.2. 2, while the cause of the additional latency for the peak amplitude difference method is given in Section 4.3.|$|R
40|$|International audienceIn {{this paper}} we {{introduce}} {{a simple and}} fast method for realtime recognition of multiple-pitches produced by multiple musical instruments. Our proposed method is based on two important facts: one that timbral information of any instrument is pitch-dependant and two, that the modulation spectrum of the same pitch seems to result into a persistent representation {{of the characteristics of}} the instrumental family, as discussed in the paper. Using these basic facts, we construct a learning algorithm to obtain pitch templates of all possible notes on various instruments and then devise an online algorithm to decompose a realtime <b>audio</b> <b>buffer</b> using the learned templates. The learning and decomposition proposed here are inspired by non-negative matrix factorization methods but differ by introduction of an explicit sparsity control. Our test results show significant recognition rate for a realtime system and on real music recordings. We discuss further improvements that can be made over the proposed system...|$|E
40|$|Presented at the 7 th International Conference on Auditory Display (ICAD), Espoo, Finland, July 29 -August 1, 2001. Existing {{real-time}} audio rendering architectures provide rigid development frameworks {{which are}} not adapted {{to a wide range}} of applications. In particular, experimenting with new rendering techniques is virtually impossible. In this paper, we present a novel, platform-independent software architecture that is well suited for experimenting with multichannel audio mixing, geometrical acoustics and 3 D audio processing in a single framework. Our architecture is divided into two layers. A low level DSP layer is responsible for streaming and processing <b>audio</b> <b>buffers</b> using a general filter-based formalism. Built on top is an audio rendering layer responsible for general geometry-based audio rendering and configuration of the rendering setup. In particular, we introduce sequence objects which we use to define and control arbitrary sound propagation paths in a geometrical 3 D virtual environment. We discuss implementation details and present a variety of prototype applications which can be efficiently designed within the proposed framework...|$|R
40|$|The {{end to end}} {{delay is}} a {{critical}} factor in the perceived quality of service for Voice over IP applications. The described solution is a complete system-level platform and complements QoS work in the network and application areas. We describe a VoIP system that couples the low level features of audio hardware with a jitter buffer playout algorithm. Using the sound card directly eliminates intermediate buffering as well as providing fine control over timers needed by a soft real-time application such as VoIP. A statistical based approach for inserting packets into <b>audio</b> <b>buffers</b> is used in conjunction with a scheme for inhibiting unnecessary fluctuations in the system. We give comparisons for the performance of the playout algorithm against idealised playout conditions. We also present mouth to ear delay measurements for selected VoIP applications and show that several hundreds of milliseconds can be saved by using the techniques described in this paper. A prototype for both UNIX and Windows platforms (NT and 9 X) has been implemented, demonstrating that our system adapts to network conditions whilst maintaining low delays. ...|$|R
40|$|International audienceScheduling for {{real-time}} {{interactive multimedia}} systems (IMS) raises specific challenges that require particular attention. Examples are triggering and coordination of heterogeneous tasks, especially for IMS that use a physical time, {{but also a}} musical time that depends on a particular performance, and how tasks that deal with audio processing interact with control tasks. Moreover, IMS have to ensure a timed scenario, for instance specified in an augmented musical score, and current IMS do not deal with their reliability and predictability. We present how to formally interleave audio processing with control by using buffer types that represent <b>audio</b> <b>buffers</b> and the way of interrupting computations that occur on them, and how to check the property of time-safety of IMS timed scenarios, in particular augmented scores for the IMS Antescofo for automatic accompaniment developed at Ircam. Our approach {{is based on the}} extension of an intermediate representation similar to the E code of the real-time embedded programming language Giotto, and on static analysis procedures run on the graph of the intermediate representation...|$|R
50|$|Many older audio {{players on}} {{personal}} computers do not implement the required buffering to play gapless audio. Some of these rely on third-party gapless <b>audio</b> plug-ins to <b>buffer</b> output. Most recent players and newer versions of old players now support gapless playback directly.|$|R
50|$|XNA Game Studio 4.0 was {{released}} on September 16, 2010. It added support for the Windows Phone platform (including 3D hardware acceleration), framework hardware profiles, configurable effects, built-in state objects, graphics device scalars and orientation, cross-platform and multi-touch input, microphone input and <b>buffered</b> <b>audio</b> playback, and Visual Studio 2010 integration.|$|R
40|$|For live {{digital audio}} systems with {{high-resolution}} multichannel functionalities, {{it is desirable}} to have accurate latency control and estimation over all the stages of digital audio processing chain. The evaluation system we designed supports 12 channel, 24 bits Sigma Delta based ADC/DAC, incorporating both a programmable FPGA and a Digital Signal Processor. It {{can be used for}} testing and evaluation of different ADC/DAC digital filter architectures, <b>audio</b> sample <b>buffer</b> subsystem design, interrupt and scheduling, high level audio processing algorithm and other system factors, which might cause the latency effects. It also can estimate the synchronization and delay of multiple channels...|$|R
5000|$|A voice {{interception}} program codenamed MYSTIC {{began in}} 2009. Along with RETRO, short for [...] "retrospective retrieval" [...] (RETRO is voice <b>audio</b> recording <b>buffer</b> that allows retrieval of captured content up to 30 {{days into the}} past), the MYSTIC program is capable of recording [...] "100 percent" [...] of a foreign country's telephone calls, enabling the NSA to rewind and review conversations up to 30 days and the relating metadata. With the capability to store up to 30 days of recorded conversations MYSTIC enables the NSA to pull an instant history of the person's movements, associates and plans.|$|R
50|$|When {{working with}} {{streaming}} audio or video that uses interrupts, DPCs {{are used to}} process the <b>audio</b> in each <b>buffer</b> as they stream in. If another DPC (from a poorly written driver) takes too long and another interrupt generates a new buffer of data, before the first one can be processed, a drop-out results.|$|R
50|$|SPIRITâ€™s VoIP {{software}} {{products are}} media processing libraries. They include standard (like G.723, G.729, H.264, MPEG-4) and proprietary (SPIRIT IPMR) voice and video codecs for speech and video compression / decompression, RTP packetizers, echo and noise cancellation, packet loss concealment and error correction, adaptive jitter <b>buffer,</b> <b>audio</b> and video synchronization, CPU load and playback rate control, etc. These components are {{integrated into a}} module within application framework.|$|R
5000|$|XNA Game Studio 4.0 was {{announced}} and initially released as a [...] "Community Technical Preview" [...] at Game Developers Conference (GDC) on March 9, 2010, {{and in its}} final form on September 16, 2010. It adds support for the Windows Phone platform (including 3D hardware acceleration), framework hardware profiles, configurable effects, built-in state objects, graphics device scalars and orientation, cross-platform and multi-touch input, microphone input and <b>buffered</b> <b>audio</b> playback, and Visual Studio 2010 integration.|$|R
25|$|For audio professionals, a new WaveRT port driver {{has been}} {{introduced}} that strives to achieve real-time performance by using the multimedia class scheduler and supports audio applications that reduce the latency of audio streams. Consequently, user mode applications can completely govern streams of audio without any code execution in the kernel during runtime. WaveRT allows the user mode application {{direct access to the}} internal <b>audio</b> hardware <b>buffers</b> and sample position counters (data in the memory that is mapped to the audio hardware DMA engine). It allows applications to poll the current position in the DMA memory window that the hardware is accessing. WaveRT also supports the notion of a hardware generated clock notification event, similar to the ASIO API, so that applications need not poll for current position if they don't want to.|$|R
40|$|As the Internet is a {{best-effort}} delivery network, audio packets may {{be delayed}} or lost {{en route to}} the receiver due to network congestion. To compensate for the variation in network delay, <b>audio</b> applications <b>buffer</b> received packets before playing them out. Basic algorithms adjust the packet playout time during periods of silence such that all packets within a talkspurt are equally delayed. Another approach is to scale individual voice packets using a dynamic time-scale modification technique based on the WSOLA algorithm. In this work, an adaptive playout algorithm based on the normalized least mean square algorithm, is improved by introducing a spike-detection mode to rapidly adjust to delay spikes. Simulations on Internet traces show that the enhanced bi-modal playout algorithm improves performance by reducing both the average delay and the loss rate as compared to the original algorithm...|$|R
25|$|RedPhone: A {{stand-alone}} {{application for}} encrypted voice calling on Android. RedPhone {{integrated with the}} system dialer to make calls, but used ZRTP {{to set up an}} end-to-end encrypted VoIP channel for the actual call. RedPhone was designed specifically for mobile devices, using <b>audio</b> codecs and <b>buffer</b> algorithms tuned to the characteristics of mobile networks, and used push notifications to preserve the user's device's battery life while still remaining responsive. RedPhone was merged into TextSecure on November 2, 2015. TextSecure was then renamed as Signal for Android. RedPhone's source code was available under the GPLv3 license.|$|R
50|$|During playback, it is {{necessary}} to send the audio data at a rate that keeps the audio playing smoothly, but without sending it faster than the modem can handle it. It is also desirable to make sure the modem can always abort playback and discard any <b>buffered</b> <b>audio</b> in case a message is to be canceled. Message cancellation is expected by callers who already know the answers to voice prompts and provide their answer early (and who would become irritated at being forced to listen to a prompt they've already responded to).|$|R
50|$|RedPhone: A {{stand-alone}} {{application for}} encrypted voice calling on Android. RedPhone {{integrated with the}} system dialer to make calls, but used ZRTP {{to set up an}} end-to-end encrypted VoIP channel for the actual call. RedPhone was designed specifically for mobile devices, using <b>audio</b> codecs and <b>buffer</b> algorithms tuned to the characteristics of mobile networks, and used push notifications to preserve the user's device's battery life while still remaining responsive. RedPhone was merged into TextSecure on November 2, 2015. TextSecure was then renamed as Signal for Android. RedPhone's source code was available under the GPLv3 license.|$|R
40|$|The {{invention}} {{relates to}} a device for calculating speaker signals for {{a plurality of}} speakers using a plurality of audio sources, wherein an audio source has an audio signal (10), said device comprising: a forward-transformation stage (100) for transforming each audio signal (10) in stages into a spectral range {{in order to obtain}} a plurality of temporally successive short-term spectra for each <b>audio</b> signal; a <b>buffer</b> (200) for storing a number of temporally successive short-term spectra for each <b>audio</b> signal; a <b>buffer</b> access control (600) for accessing a specific short-term spectrum from the plurality of short-term spectra for a combination of a speaker and an audio signal based on a delay value (701); a filter stage (300) for filtering the specific short-term spectrum for the combination of audio signal and speaker with a filter that is provided for the combination of audio signal and speaker such that a filtered short-term spectrum is obtained for each combination of one audio signal and one speaker; a summation stage (400) for adding up the filtered short-term spectra for a speaker in order to obtain the sum of short-term spectra for each speaker; and a back-transformation stage (800) for the back transformation of added short-term spectra in stages for the speaker in a time domain in order to obtain the speaker signals...|$|R
50|$|The {{larger the}} buffer is, {{the more time}} it takes to fill it by digital <b>audio</b> data. Large <b>buffers</b> {{increase}} the time required for processing audio in computer, this delay is usually called latency. Every system has certain limitations - too small buffers involving negligible latencies cannot be smoothly processed by computer, so the reasonable size starts at about 32 samples. The processor load does not affect latency directly (it means, once you set certain buffer size, the latency is constant), but with very high processor loads the processing starts dropping out. Increasing buffer size or quitting other application helps to keep playback smooth.|$|R
