8|2|Public
40|$|In {{this paper}} we present the design, implementation, and {{evaluation}} of a pluggable <b>autoscaler</b> within an open cloud platform-as-a-service (PaaS). We redefine high availability (HA) as the dynamic use of virtual machines to keep services available to users, making it a subset of elasticity (the dynamic use of virtual machines). This {{makes it possible to}} investigate autoscalers that simultaneously address HA and elasticity. We present and evaluate autoscalers within this pluggable system that are HA-aware and Quality-of-Service (QoS) -aware for web applications written in different programming languages. Hot spares can also be utilized to provide both HA and improve QoS to web users. Within the open source AppScale PaaS, hot spares can increase the amount of web traffic that the QoS-aware <b>autoscaler</b> serves to users by up to 32 %...|$|E
40|$|Large-scale {{deployments}} of IoT {{devices are}} subject to energy and performance issues. Fortunately, offloading is a promising technique to enhance those aspects. However, several problems still remain open regarding cloud deployment and provisioning. In this paper, we {{address the problem of}} provi- sioning offloading as a service in large-scale IoT deployments. We design and develop an <b>AutoScaler,</b> an essential component for our offloading architecture to handle offloading workload. In addition, we also develop an offloading simulator to generate dynamic offloading workload of multiple devices. With this toolkit, we study the effect of task acceleration in different cloud servers and analyze the capacity of several cloud servers to handle multiple concurrent requests. We conduct multiple experiments in a real testbed to evaluate the system and present our experiences and lessons learned. From the results, we find that the <b>AutoScaler</b> component introduces a very small overhead of ≈ 150 milliseconds in the total response time of a request, which is a fair price to pay to empower the offloading architectures with multi-tenancy ability and dynamic horizontal scaling for IoT scenarios...|$|E
40|$|Software {{engineering}} considers {{performance evaluation}} {{to be one}} of the key portions of software quality assurance. Unfortunately, there seems to be a lack of standard methodologies for performance evaluation even in the scope of experimental computer science. Inspired by the concept of "instantiation" in object-oriented programming, we distinguish the generic performance evaluation logic from the distributed and ad-hoc relevant studies, and develop an abstract evaluation methodology (by analogy of "class") we name Domain Knowledge-driven Methodology (DoKnowMe). By replacing five predefined domain-specific knowledge artefacts, DoKnowMe could be instantiated into specific methodologies (by analogy of "object") to guide evaluators in performance evaluation of different software and even computing systems. We also propose a generic validation framework with four indicators (i. e. ~usefulness, feasibility, effectiveness and repeatability), and use it to validate DoKnowMe in the Cloud services evaluation domain. Given the positive and promising validation result, we plan to integrate more common evaluation strategies to improve DoKnowMe and further focus on the performance evaluation of Cloud <b>autoscaler</b> systems...|$|E
40|$|An {{automated}} {{solution to}} horizontal vs. vertical elastic-ity problem {{is central to}} make cloud <b>autoscalers</b> truly au-tonomous. Today’s cloud <b>autoscalers</b> are typically varying the capacity allocated by increasing and decreasing the num-ber of virtual machines (VMs) of a predefined size (horizon-tal elasticity), not {{taking into account that}} as load varies it may be advantageous not only to vary the number but also the size of VMs (vertical elasticity). We analyze the price/performance effects achieved by different strategies for selecting VM-sizes for handling increasing load and we pro-pose a cost-benefit based approach to determine when to (partly) replace a current set of VMs with a different set. We evaluate our repacking approach in combination with different auto-scaling strategies. Our results show a range of 7 % up to 60 % cost saving in total resource utilization cost of our sample applications and workloads...|$|R
40|$|One of {{the most}} useful {{features}} of a microservices architecture is its versatility to scale horizontally. However, not all services scale in or out uniformly. The performance of an application composed of microservices depends largely on a suitable combination of replica count and resource capacity. In practice, this implies limitations to the efficiency of <b>autoscalers</b> which often overscale based on an isolated consideration of single service metrics. Consequently, application providers pay more than necessary despite zero gain in overall performance. Solving this issue requires an application-specific determination of scaling limits due to the general infeasibility of an application-agnostic solution. In this paper, we study microservices scalability, the auto-scaling of containers as microservice implementations and {{the relation between the}} number of replicas and the resulting application task performance. We contribute a replica count determination solution with a mathematical approach. Furthermore, we offer a calibration software tool which places scalability boundaries into declarative composition descriptions of applications ready to be consumed by cloud platforms...|$|R
40|$|Finding visual {{correspondence}} between local features {{is key to}} many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose <b>AutoScaler,</b> a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB- 2011. We also show that our method can generalize to improve hand-crafted descriptors (e. g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps...|$|E
40|$|Network {{functions}} virtualization provides {{opportunities to}} design, deploy, and manage networking services. It utilizes cloud computing virtualization services {{that run on}} high-volume servers, switches, and storage hardware to virtualize network functions. Virtualization techniques {{can be used in}} IP multimedia subsystem (IMS) cloud computing to develop different networking functions (e. g., load balancing and call admission control). IMS network signaling happens through session initiation protocol (SIP). An open issue is the control of overload that occurs when an SIP server lacks sufficient CPU and memory resources to process all messages. This paper proposes a virtual load balanced call admission controller (VLB-CAC) for the cloud-hosted SIP servers. VLB-CAC determines the optimal call admission rates and signaling paths for admitted calls along with the optimal allocation of CPU and memory resources of the SIP servers. This optimal solution is derived through a new linear programming model. This model requires some critical information of SIP servers as input. Further, VLB-CAC is equipped with an <b>autoscaler</b> to overcome resource limitations. The proposed scheme is implemented in smart applications on virtual infrastructure (SAVI) which serves as a virtual testbed. An assessment of the numerical and experimental results demonstrates the efficiency of the proposed work...|$|E
40|$|International audience— Network {{functions}} virtualization provides {{opportunities to}} design, deploy, and manage networking services. It utilizes Cloud computing virtualization services {{that run on}} high-volume servers, switches and storage hardware to virtualize network functions. Virtualization techniques {{can be used in}} IP Multimedia Subsystem (IMS) cloud computing to develop different networking functions (e. g. load balancing and call admission control). IMS network signaling happens through Session Initiation Protocol (SIP). An open issue is the control of overload that occurs when an SIP server lacks sufficient CPU and memory resources to process all messages. This paper proposes a virtual load balanced call admission controller (VLB-CAC) for the cloud-hosted SIP servers. VLB-CAC determines the optimal " call admission rates " and " signaling paths " for admitted calls along with the optimal allocation of CPU and memory resources of the SIP servers. This optimal solution is derived through a new linear programming model. This model requires some critical information of SIP servers as input. Further, VLB-CAC is equipped with an <b>autoscaler</b> to overcome resource limitations. The proposed scheme is implemented in SAVI (Smart Applications on Virtual Infrastructure) which serves as a virtual testbed. An assessment of the numerical and experimental results demonstrates the efficiency of the proposed work...|$|E
40|$|Abstract—This {{document}} presents ongoing work {{on creating}} a computing {{system that can}} run two types of workloads on a private cloud computing cluster, namely web servers and batch computing jobs, {{in a way that}} would maximize utilization of the computing infrastructure. To this end, a queue engine called Cloud Gunther has been developed. This application improves upon current practices of running batch computations in the cloud by integrating control of virtual machine provisioning within the job scheduler. For managing web server workloads, we present ScaleGuru, which has been modeled after Amazon Auto Scaler for easier transition from public to private cloud. Both these tools are tested to run over the Eucalyptus cloud system. Further research has been done in the area of Time Series Forecasting, which enables to predict the load of a system based on past observations. Due to the periodic nature of the interactive load, predictions can be made in the horizon of days with reasonable accuracy. Two forecasting models (Holt-Winters exponential smoothing and Box-Jenkins autoregressive) have been studied and evaluated on six server load time series. The <b>autoscaler</b> and queue engine are not yet integrated. Meanwhile, the prediction can be used to decide how many servers to turn off at night or as an internal component for the autoscaling system...|$|E
40|$|The {{purpose of}} this thesis {{is to create an}} {{automatic}} scaling implementation for Cassandra clusters. The automatic scaler should never lower the overall performance of the cluster in a way that results in a bad user experience. It should also be able to successfully scale up and down nodes, and the cluster should continue as if nothing happened. Last but not least, it is desirable that the automatic scaler performs equally, or better than, the person {{who is in charge of}} administrating the database. In this thesis we have developed an early version of an <b>autoscaler</b> that may run alongside a Cassandra instance. The implementation is split into two separate implementations: a master-, and an agent-implementation. The master will be deployed to the same server as the application using the cluster, even though this is not required. The agent implementation will be deployed to, and run alongside, all nodes that are a part of the cluster. The agent will monitor the node`s resource usage, and send messages back to the master if the usage increases above, or decreases below certain thresholds. We performed a set of test cases to prove that the implementation works as intended. The test cases recorded the nodes resource-usage to determine the impact our implementation makes to the overall performance...|$|E

