532|36|Public
2500|$|Fearnley-Sander, Desmond, and Stokes, Timothy, 1996, [...] "". <b>Automated</b> <b>Deduction</b> in Geometry: 141–70 ...|$|E
2500|$|In proof {{theory and}} {{mathematical}} logic, sequent calculus {{is a family}} of formal systems sharing a certain style of inference and certain formal properties. [...] The first sequent calculi, systems LK and LJ, were introduced in 1934/1935 by Gerhard Gentzen {{as a tool for}} studying natural deduction in first-order logic (in classical and intuitionistic versions, respectively). [...] Gentzen's so-called [...] "Main Theorem" [...] (Hauptsatz) about LK and LJ was the cut-elimination theorem, a result with far-reaching meta-theoretic consequences, including consistency. [...] Gentzen further demonstrated the power and flexibility of this technique a few years later, applying a cut-elimination argument to give a (transfinite) proof of the consistency of Peano arithmetic, in surprising response to Gödel's incompleteness theorems. [...] Since this early work, sequent calculi, also called Gentzen systems, and the general concepts relating to them, have been widely applied in the fields of proof theory, mathematical logic, and <b>automated</b> <b>deduction.</b>|$|E
5000|$|Herbrand Award [...] - [...] by the Conference on <b>Automated</b> <b>Deduction,</b> for <b>automated</b> <b>deduction</b> ...|$|E
40|$|<b>Automated</b> forward <b>deduction</b> engines for {{discovery}} and prediction must get valid conclusions in acceptable time, {{because the system}} that cannot satisfy the requirements of validity of conclusions and acceptability of time is useless at all in pragmatic applications. However, any forward de-duction process is essentially an inefficient process, because it {{has to deal with}} a lot of intermediates. Hence excessive amount of execution time and main memory are needed in execution of <b>automated</b> forward <b>deduction.</b> This problem is generally involved in any <b>automated</b> forward <b>deduction</b> en-gines. This paper presents our approach to improving the efficiency of <b>automated</b> forward <b>deduction</b> engines by par-allel processing. We present a parallel computing model of <b>automated</b> forward <b>deduction</b> and our implementation of an <b>automated</b> forward <b>deduction</b> system for general-purpose entailment calculus on clusters of PCs according to the model...|$|R
40|$|In the eld {{of formal}} methods, {{rewriting}} techniques and provers by consistency in particular appear as powerful tools for <b>automating</b> <b>deduction.</b> However, these provers su er limitations as they only give a (nonreadable) trace of their progress and a yes/no answer where the user would expect a detailed explicit proof. Therefore, we propose a general mechanism {{to build an}} explicit proof from the running of a generic class of inductionless induction provers. We then show howitapplies to Bouhoula's SPIKE prover, and give examples of proofs built by this method...|$|R
40|$|Recent {{efforts in}} the Conceptual Modelling {{community}} have been devoted to properly capturing time-varying information, and several proposals of temporally enhanced Entity-Relationship (ER) exist. This work gives a logical formalisation of the various properties that characterise and extend different temporal ER models which are found in literature. The formalisation we propose is based on Description Logics (DL), which have been proved useful for a logical reconstruction {{of the most popular}} conceptual data modelling formalisms. The proposed DL has the ability to express both enhanced temporal ER schemas and integrity constraints in the form of complex inclusion dependencies. Reasoning in the devised logic is decidable, thus allowing for <b>automated</b> <b>deductions</b> over the whole conceptual representation, which includes both the ER schema and the integrity constraints over it. ...|$|R
5000|$|Herbrand Award [...] - [...] by the Conference on <b>Automated</b> <b>Deduction,</b> for {{contributions}} {{in the field of}} <b>automated</b> <b>deduction</b> ...|$|E
50|$|In June 2012 Melvin Fitting {{was given}} the Herbrand Award by CADE, for {{distinguished}} contributions to <b>automated</b> <b>deduction.</b>|$|E
5000|$|Fearnley-Sander, Desmond, and Stokes, Timothy, 1996, [...] "Area in Grassmann Geometry [...] ". <b>Automated</b> <b>Deduction</b> in Geometry: 141-70 ...|$|E
40|$|We {{present a}} sound and {{complete}} calculus CL_S for the first-order logic L_S without equality but dynamic sorts. By dynamic sorts we understand a sort structure which is exploited during the deduction process {{and is not}} fixed from the beginning. Therefore the logic allows arbitrary sort declarations and every first-order logic formula can be compiled into our logic. Deductions in the calculus {{turn out to be}} very efficient as will be shown by examples. Thus the new logic signifacantly improves the efficiency of <b>automated</b> <b>deductions</b> if one-place predicates (sorts) are involved in the reasoning process. Furthermore the calculus CL_S is conservative extension of the existing approaches to sorted logic. That means if the sorted information could be represented in the known approaches the more general calculus CL_S has the same behaviour as the existing calculi...|$|R
40|$|This paper {{presents}} the fundamental design ideas, working principles, {{and implementation of}} an <b>automated</b> forward <b>deduction</b> system for general-purpose entailment calculus, named EnCal, shows its potential applications in knowledge acquisition, reasoning rule generation, and theorem finding, reports some current results of our experiments with EnCal, and suggests some important research problems...|$|R
40|$|Recent eorts in the Conceptual Modelling {{community}} have {{been devoted to}} properly capturing time-varying information. Various temporally enhanced Entity-Relationship (ER) models have been proposed that are intended to model the temporal aspects of database conceptual schemas. This work gives a logical formalisation of the various properties that characterise and extend dierent temporal ER models which are found in literature. The formalisation we propose is based on Description Logics (DL), which have been proved useful for a logical reconstruction {{of the most popular}} conceptual data modelling formalisms. The proposed DL has the ability to express both enhanced temporal ER schemas and integrity constraints in the form of complex inclusion dependencies. Reasoning in the devised logic is decidable, thus allowing for <b>automated</b> <b>deductions</b> over the whole conceptual representation, which includes both the ER schema and the integrity constraints over it. 1 Introduction Tempo [...] ...|$|R
50|$|He is {{the winner}} of the 2007 IJCAI Award for Research Excellence and Herbrand Award for Distinguished Contributions to <b>Automated</b> <b>Deduction.</b>|$|E
50|$|The first CASC, CASC-13, {{was held}} {{as part of}} the 13th Conference on <b>Automated</b> <b>Deduction</b> at Rutgers University, New Brunswick, NJ, in 1996.|$|E
50|$|He {{was elected}} {{fellow of the}} American Association for Artificial Intelligence in 1992 and {{received}} the Herbrand Award for his contributions to <b>automated</b> <b>deduction</b> in 2002.|$|E
40|$|AbstractThough {{circumscription}} {{was introduced}} by McCarthy over a decade ago, there has been relatively little work on algorithms for computing circumscriptive databases. In this paper, we develop algorithms to compute the preferred models of circumscriptive databases at compile-time using mixed integer linear programming techniques. Two advantages of this (bottom-up) approach are that it makes efficient re-use of previous computations and it provides much faster run-time performance. Some other advantages of using linear programming to <b>automate</b> <b>deduction</b> at compile time are that its re-optimization facilities elegantly accommodate database updates and also that it leads to a completely declarative formulation in which ordering of rules and literals in rule bodies plays no real role. Finally, we plan to use a standard relational database system as our run-time environment; this should yield relatively fast run-time processing, and provide a more expressive query language in which aggregates and the like can be expressed easily...|$|R
40|$|In {{the field}} of formal methods, {{rewriting}} techniques and provers by consistency in particular appear as powerful tools for <b>automating</b> <b>deduction.</b> However, these provers suffer limitations as they only give a (nonreadable) trace of their progress and a yes/no answer where the user would expect a detailed explicit proof. Therefore, we propose a general mechanism to build an explicit proof from the running of a generic class of inductionless induction provers. We then show how it applies to Bouhoula's SPIKE prover, and give examples of proofs built by this method. Keywords: Rewriting, theorem prover, natural deduction, inductionless induction, SPIKE, Coq R'esum'e Dans le domaine des m'ethodes formelles, les techniques de r'e'ecriture et les prouveurs par r'ecurrence implicite sont des outils puissants pour automatiser le processus de preuve. Cependant, ces prouveurs donnent une trace du d'eroulement de la preuve peu compr'ehensible, alors que l'utilisateur lambda aimerait av [...] ...|$|R
40|$|Automated theorem finding (ATF for short) {{is one of}} 33 basic {{research}} problems in automated? reasoning that was originally proposed by Wos in 1988. It is still a completely open problem until now. We have proposed an approach to the ATF problem that one should use strong relevant logic for mathematical knowledge representation and reasoning and find theorems by forward deduction based on strong relevant logic. However, at present, {{it is not clear}} that how this approach is really effective to solving the ATF problem. In this paper, we present our case study to find theorems in von Neumann-Bernays-Godel (NBG for short) set theory by <b>automated</b> forward <b>deduction</b> based on strong relevant logic. This case study showed that strong relevant logic is suitable to automated theorem finding in the sense that from the viewpoint of NBG set theory theorems reasoned out by <b>automated</b> forward <b>deduction</b> based on strong relevant logic are valid and no theorem reasoned out by the deduction is paradoxical...|$|R
50|$|The Conference on <b>Automated</b> <b>Deduction</b> (CADE) is {{the premier}} {{academic}} conference on <b>automated</b> <b>deduction</b> and related fields. The first CADE was organized in 1974 at the Argonne National Laboratory near Chicago. Most CADE meetings {{have been held}} in Europe and the United States. However, conferences have been held all over the world. Since 1996, CADE has been held yearly. In 2001, CADE was, for the first time, merged into the International Joint Conference on Automated Reasoning (IJCAR). This has been repeated biannually since 2004.|$|E
50|$|The Herbrand Award is a prize {{given by}} CADE Inc. to honour persons or groups for {{important}} {{contributions to the}} field of <b>automated</b> <b>deduction.</b> The prize is $1000.|$|E
5000|$|International Joint Conference on Automated Reasoning (IJCAR) that itself {{conglomerates}} Conference on <b>Automated</b> <b>Deduction</b> (CADE), Theorem Proving in Higher-Order Logics (TPHOLs), and Automated Reasoning with Analytic Tableaux and Related Methods (TABLEAUX).|$|E
40|$|In {{this paper}} we present an {{approach}} to prove the equality between terms in a goaldirected way developed {{in the field of}} inductive theorem proving. The two terms to be equated are syntactically split into expressions which are common to both and those which occur only in one term. According to the computed differences we apply appropriate equations to the terms {{in order to reduce the}} differences in a goal-directed way. Although this approach was developed for purposes of inductive theorem proving - we use this technique to manipulate the conclusion of an induction step to enable the use of the hypothesis - it is a powerful method for the control of equational reasoning in general. 1. Introduction The automation of equational reasoning is one of the most important obstacles in the field of <b>automating</b> <b>deductions.</b> Even small equational problems result in a huge search space, and finding a proof often fails due to the combinatorial explosion. Proving (conditional) equations by inductio [...] ...|$|R
40|$|AbstractClassical {{mathematical}} logic includes {{a lot of}} “implicational paradoxes” as its logic theorems. This paper uses the property of strong relevance as the criterion to identify implicational paradoxes in logical theorems of classical {{mathematical logic}}, and enumerates logical theorem schemata of classical mathematical logic that do not satisfy the strong relevance. This quantitative analysis shows that classical mathematical logic is by far not a suitable logical basis for <b>automated</b> forward <b>deduction...</b>|$|R
50|$|It is also {{possible}} to represent logical descriptions using semantic networks such as the existential graphs of Charles Sanders Peirce or the related conceptual graphs of John F. Sowa. These have expressive power equal to or exceeding standard first-order predicate logic. Unlike WordNet or other lexical or browsing networks, semantic networks using these representations {{can be used for}} reliable <b>automated</b> logical <b>deduction.</b> Some <b>automated</b> reasoners exploit the graph-theoretic features of the networks during processing.|$|R
5000|$|In 2008, he {{was elected}} a Fellow of the Association for the Advancement of Artificial Intelligence for [...] "significant and {{sustained}} contributions to <b>automated</b> <b>deduction</b> and constraint programming, and for extraordinary service to the AI community".|$|E
50|$|In 2015, his {{contributions}} {{to the field of}} automated reasoning were recognized with the Herbrand Award. He was won 25 division titles in the CADE ATP System Competition (CASC) at the Conference on <b>Automated</b> <b>Deduction</b> (CADE) since 1999.|$|E
50|$|In {{the late}} 1960s {{agencies}} funding research in <b>automated</b> <b>deduction</b> began {{to emphasize the}} need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied {{to the problem of}} verifying the correctness of computer programs in languages such as Pascal, Ada, Java etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using J.A. Robinson's resolution Principle. This was the first <b>automated</b> <b>deduction</b> system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.|$|E
40|$|In {{this article}} we propose an {{extension}} of term rewriting techniques to <b>automate</b> the <b>deduction</b> in monotone pre-order theories. To prove an inclusion a ⊆ b from a given set I of them, we generate from I, using a completion procedure, a bi-rewrite system 〈R⊆, R⊇〉, that is, a pair of rewrite relations −−− → R ⊆ and −−− → R ⊇, and seek a common term c such that a −−−→ R ⊆ c and b −−−...|$|R
40|$|Two {{apparently}} {{different approaches}} to <b>automating</b> <b>deduction</b> are mentioned in the title; they {{are the subject of}} a debate on "big engines vs. little engines of proof". The contributions in this thesis advocate that these two strands of research can interplay in subtle and sometimes unexpected ways, such that mutual pervasion can lead to intriguing results: Firstly, superposition can be run on top of decision procedures. This we demonstrate for the class of Shostak theories, incorporating a little engine into a big one. As another instance of decision procedures within superposition, we show that ground confluent rewrite systems, which decide entailment problems in equational logic, can be harnessed for detecting redundancies in superposition derivations. Secondly, superposition can be employed as proof-theoretic means underneath combined decision procedures: We re-establish the correctness of the Nelson-Oppen procedure as an instance of the completeness of superposition. Thirdly, superposition {{can be used as a}} decision procedure for many interesting theories, turning a big engine into a little one. For the theory of bits and of fixed-size bitvectors, we suggest a rephrased axiomatization combined with a transformation of conjectures, based on which superposition decides the universal fragment. Furthermore, with a modification of lifting, we adapt superposition to the theory of bounded domains and give a decision procedure, which captures the Bernays-Schönfinkel class as well...|$|R
40|$|The Fellegi-Holt method {{automatically}} "corrects" {{data that}} fail some predefined requirements. Computer implementations {{of the method}} were used in many national statistics agencies but are less used now because they are slow. We recast the method in propositional logic, and show that many of its results are well-known results in propositional logic. In particular we show that the Fellegi-Holt method of "edit generation" {{is essentially the same}} as a technique for <b>automating</b> logical <b>deduction</b> called resolution. Since modern implementations of resolution are capable of handling large problems efficiently, they might lead to more efficient implementations of the Fellegi-Holt method...|$|R
50|$|Automated {{theorem proving}} (also known as ATP or <b>automated</b> <b>deduction)</b> is a {{subfield}} of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof {{was a major}} impetus {{for the development of}} computer science.|$|E
50|$|The CADE ATP System Competition (CASC) is {{a yearly}} {{competition}} of fully automated theorem provers for classical first order logic. CASC {{is associated with}} the Conference on <b>Automated</b> <b>Deduction</b> and the International Joint Conference on Automated Reasoning organized by the Association for Automated Reasoning.|$|E
5000|$|David Luckham Stanford University, Developed the Stanford Resolution Theorem Prover 1968, {{the first}} <b>automated</b> <b>deduction</b> system used to solve {{problems}} announced in the Notices of the AMS, and subsequently developed the Stanford Pascal Verifier, the first program verification system for Pascal, and a widely distributed program verification system, 1968-75 ...|$|E
40|$|THINKER is an <b>automated</b> natural <b>deduction</b> first-order {{theorem proving}} program. This paper reports {{on how it}} was adapted so as to prove theorems in modal logic. The method {{employed}} is an &quot;indirect semantic method&quot;, obtained by considering the semantic conditions involved in being a valid argument in these modal logics. The method is extended from propositional modal logic to predicate modal logic, and issues concerning the domain of quantification and &quot;existence in a world's domain &quot; are discussed. Finally, we took at the very interesting issues involved with adding identity to the theorem prover {{in the realm of}} modal predicate logic. Various alternatives are discussed...|$|R
40|$|The {{commutative}} {{version of}} the Gröbner Basis Algorithm {{has become one of}} the most powerful tools in computer algebra. The recent adaptation of the algorithm to noncommuting variables has potential applications to matrix and operator expressions. A Forth-based research system was used to implement and study applications of a noncommutative variant of the Gröbner Basis Algorithm. It has been used in <b>automated</b> simplification and <b>deduction</b> for formulas in engineering. The paper will discuss this work and the use of Forth in producing systems for mathematics research...|$|R
40|$|In {{this paper}} we {{investigate}} {{a new approach}} to formalizing interpretation of and reasoning with visual languages based on linear logic. We argue that an approach based on logic makes it possible to deal with different computational tasks in the usage of visual notations, from parsing and animation to reasoning about diagrams. However, classical first order logic, being monotonic, is not a suitable basis for such an approach. The paper therefore explores linear logic as an alternative. We demonstrate how parsing corresponds to linear proofs and prove the soundness and correctness of this mapping. As our mapping of grammars is into a subset of a linear logic programming language, we also demonstrate how multi-dimensional parsing can be understood as <b>automated</b> linear <b>deduction.</b> We proceed to discuss how the same framework can be used as the foundation of more complex forms of reasoning with and about diagrams...|$|R
