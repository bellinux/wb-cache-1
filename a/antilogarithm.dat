18|14|Public
500|$|For manual {{calculations}} {{that demand}} any appreciable precision, performing the lookups {{of the two}} logarithms, calculating their sum or difference, and looking up the <b>antilogarithm</b> is much faster than performing the multiplication by earlier methods such as prosthaphaeresis, which relies on trigonometric identities. Calculations of powers and roots are reduced to multiplications or divisions and look-ups by ...|$|E
500|$|A key {{tool that}} enabled the {{practical}} use of logarithms before calculators and computers was {{the table of}} logarithms. The first such table was compiled by Henry Briggs in 1617, immediately after Napier's invention. Subsequently, tables with increasing scope were written. These tables listed the values of [...] and [...] for any number x in a certain range, at a certain precision, for a certain base b (usually [...] ). For example, Briggs' first table contained the common logarithms of all integers in the range 1–1000, with a precision of 14 digits. As the function [...] is the inverse function of logb'x, it {{has been called the}} <b>antilogarithm.</b> The product and quotient of two positive numbers c and d were routinely calculated as the sum and difference of their logarithms. The product cd or quotient c/d came from looking up the <b>antilogarithm</b> of the sum or difference, also via the same table: ...|$|E
500|$|Four {{constants}} {{are printed}} on the calculator case for easy reference. [...] For converting to and from base 10 logarithms and natural logarithms the natural logarithm of 10 (2.30259) and e (2.71828) are {{printed on the}} case. Pi (3.14159) and 57.2958 (180 / Pi) {{are also on the}} case for trigonometry calculations. [...] There was not enough internal memory to store these constants internally. [...] Angles are computed using radians; degree values must be converted to radians by dividing by 57.2958. [...] As an example, to calculate 25 sin (600*0.05°) one would enter C 6 E 2 + 0 0 5 × 5 7 2 9 5 8 E 1 ÷ ▲ + 2 5 E 1 × to get a result of 1.2500 01 (representing 12.5 which is equal to 25 sin(30°) [...] ). [...] Sine is selected with the combination of the [...] "▲" [...] key followed by the [...] "+" [...] key. [...] The [...] "▼" [...] (down) and [...] "▲" [...] (up) arrow keys are function select keys. [...] The four operation keys ("-, +, ÷ and ×") all have two other function activated by using one of the arrow keys. [...] The function available are Sine, Arcsine, Cosine, Arccosine, Tangent, Arctangent, Logarithm and <b>Antilogarithm.</b>|$|E
50|$|When taking <b>antilogarithms,</b> the {{resulting}} number should {{have as many}} significant figures as the mantissa in the logarithm.|$|R
40|$|This paper {{presents}} {{a method to}} improve the accuracy of a logarithmic multiplier, based on Mitchell’s algorithms for calculating logarithms and <b>antilogarithms.</b> The method developed offers an area saving of approximately 50 % and a power saving of 71 % for larger input widths. A filter based on the multiplier is also presented...|$|R
50|$|With John Pell he {{computed}} {{the first}} table of <b>antilogarithms</b> in the 1630s. John Aubrey, relying on Pell's testimony, states that Warner had {{claimed to have}} anticipated William Harvey's discovery of the circulation of the blood, and that Harvey must have heard of it through a Mr Prothero. Pell also mentioned that Warner had been born without a left hand.|$|R
50|$|A {{person can}} use {{logarithm}} tables to divide two numbers, by subtracting the two numbers' logarithms, then looking up the <b>antilogarithm</b> of the result.|$|E
5000|$|Fold number {{refers to}} how many double folds that are {{required}} to cause rupture of a paper test piece under standardized conditions. Fold number is defined in ISO 5626:1993 as the <b>antilogarithm</b> of the mean folding endurance: ...|$|E
5000|$|In {{the former}} Swedish {{standard}} SS 152005 ("Pappersordlista") from 1992, with paper related terms defined in Swedish and English, fold number is explained as [...] "the number of double folds which a test strip withstands under specified conditions before a break {{occurs in the}} strip"; that is, not the <b>antilogarithm</b> of the mean folding endurance.|$|E
50|$|Tycho had {{intended}} that the tables {{should have a}} dedication to Emperor Rudolf II, but by 1627, when the tables were published, Rudolf II had died, so instead the tables were dedicated to Emperor Ferdinand II but are named after Rudolph II. They contain positions for the 1,005 stars measured by Tycho Brahe, and more than 400 stars from Ptolemy and Johann Bayer, with directions and tables for locating the planets of the solar system. The tables included many function tables of logarithms and <b>antilogarithms,</b> and instructive examples for computing planetary positions.|$|R
5000|$|In {{both cases}} the result is {{accurate}} to same order of magnitude as the inputs (-20 and -10, respectively). In the second case, the answer seems to have one significant digit, which would amount to loss of significance. However, in computer floating point arithmetic, all operations {{can be viewed as}} being performed on <b>antilogarithms,</b> for which the rules for significant figures indicate that the number of significant figures remains the same as the smallest number of significant figures in the mantissas. The way to indicate this and represent the answer to 10 significant figures is: ...|$|R
40|$|There {{are applied}} power {{mappings}} in algebras with logarithms induced by a given linear operator D {{in order to}} study particular properties of powers of logarithms. Main results of this paper will be concerned with the case when an algebra under consideration is commutative and has a unit and the operator D satisfies the Leibniz condition, i. e. D(xy) = xDy + yDx for x, y ∈ dom D. Note that in the Number Theory there are well-known several formulae expressed by means of some combinations of powers of logarithmic and antilogarithmic mappings or powers of logarithms and <b>antilogarithms</b> (cf. for instance, the survey of Schinzel S[1]...|$|R
5000|$|For manual {{calculations}} {{that demand}} any appreciable precision, performing the lookups {{of the two}} logarithms, calculating their sum or difference, and looking up the <b>antilogarithm</b> is much faster than performing the multiplication by earlier methods such as prosthaphaeresis, which relies on trigonometric identities. Calculations of powers and roots are reduced to multiplications or divisions and look-ups by ...|$|E
5000|$|A key {{tool that}} enabled the {{practical}} use of logarithms before calculators and computers was {{the table of}} logarithms. The first such table was compiled by Henry Briggs in 1617, immediately after Napier's invention. Subsequently, tables with increasing scope were written. These tables listed the values of [...] and [...] for any number x in a certain range, at a certain precision, for a certain base b (usually [...] ). For example, Briggs' first table contained the common logarithms of all integers in the range 1-1000, with a precision of 14 digits. As the function [...] is the inverse function of logb(x), it {{has been called the}} <b>antilogarithm.</b> The product and quotient of two positive numbers c and d were routinely calculated as the sum and difference of their logarithms. The product cd or quotient c/d came from looking up the <b>antilogarithm</b> of the sum or difference, also via the same table:and ...|$|E
5000|$|The graph of [...] is upward-sloping, and {{increases}} faster as [...] increases. The graph always lies above the -axis but can get arbitrarily {{close to it}} for negative thus, the -axis is a horizontal asymptote. The slope of the tangent to the graph at each point is equal to its -coordinate at that point, as implied by its derivative function (see above). Its inverse function is the natural logarithm, denoted , , or because of this, some old texts refer to the exponential function as the <b>antilogarithm.</b>|$|E
40|$|The {{distribution}} of 0 - 8 M HNO 3 between aqueous and 5 - 100 vol % solutions of TBP in Amsco 125 - 82 has been measured {{and used to}} obtain a quantitative description of this extraction process. Up to- 5 M aqueous HN 031 a l imit arbitrarily chosen because of un-certainties i n HNO 3 act ivi ty coefficients, the data of this report a t any particular con-centration of TBP i n ~ m s c o are accurately described by the equation log (HNOCJ) ~~S/ ([HNO~],q(3. 75493 -(~~ 03) 0 rg)) = A + B (HN 03) org,, where ~arentheses refer to concentrat~ons, square brackets to activities, and subscr~pts org and aq to organic and aqueous phases. The <b>antilogarithms</b> of the constants A for the six TBP-diluent solutions studied {{can be described as}} a linear function of (Yo + Y;B,) l the sum of the mole fractions, Yo H 2 0 and YTBp of water and TBP i n the aHc?%free water-saturated organic phase. These <b>antilogarithms</b> were interpreted as the product K 1 Y p where K 1 i s the thermodynamic equilibrium constant for the extraction reaction and i s the mean act ivi ty coefficient T of TBP and TBP. H 20 i n the acid-free, water-saturated organic phase. As the concentra-tion of TBP in Amsco 125 - 82 increases from 0 to 100 %, KIYTl i n molal units, varies from 0. 2 to 1. 5. The quantities B of the above equation are proportional to (YfBp+ % 20.) 1 /...|$|R
50|$|Bürgi {{constructed}} {{a table of}} progressions which {{can be considered a}} table of <b>antilogarithms</b> independently of John Napier, whose publication (1614) was known by the time Bürgi published at the behest of Johannes Kepler. We know that Bürgi had some way of simplifying calculations around 1588, but most likely this way was the use of prosthaphaeresis, and not the use of his table of progressions which probably goes back to about 1600. Indeed Wittich, who was in Kassel from 1584 to 1586, brought with him knowledge of prosthaphaeresis, a method by which multiplications and divisions can be replaced by additions and subtractions of trigonometrical values... This procedure achieves the same as the logarithms will a few years later.|$|R
40|$|AbstractIn {{order to}} solve {{ordinary}} differential equations, we use an equation {{with the so-called}} logarithmic derivative. Similar equations with a linear operator permit us to define logarithmic and antilogarithmic mappings and obtain some results unknown before for the classical derivation operator. This method and its applications are exposed in detail in the author's book [1] and may be treated as an introduction into the 21 st century logarithmo-technia (according to the original meaning of this word). In the first section, there are given basic notions and facts of algebraic analysis (without proofs). The second section consists {{of the most important}} definitions and theorems (without proofs) concerning logarithms and <b>antilogarithms.</b> The third section is concerned with properties of multiplicative true shifts. In the last section is given a generalization of the binomial theorem for harmonic logarithms...|$|R
50|$|There {{has been}} {{interest}} in tristimulus reporting in the brewing community {{in recent years}} and the ASBC has an approved Method of Analysis MOA for tristimulus characterization. The absorption of the sample is measured in 1 cm at 81 wavelengths separated by 5 nm starting at 380 nm and extending to 780 nm. These are converted to transmission values (by taking the <b>antilogarithm</b> of each absorption) and inserting the results into ASTM E-308. The reported tristimulus values are in L*a*b* color space and describe what is seen under Illuminant C (daylight) by a 10° observer when the path is 1 cm. The choice of path, illuminant, observer and color space does not represent a limitation of E-308 but rather the ASBC's need to standardize reporting.|$|E
5000|$|Four {{constants}} {{are printed}} on the calculator case for easy reference. For converting to and from base 10 logarithms and natural logarithms the natural logarithm of 10 (2.30259) and e (2.71828) are {{printed on the}} case. Pi (3.14159) and 57.2958 (180 / Pi) {{are also on the}} case for trigonometry calculations. There was not enough internal memory to store these constants internally. Angles are computed using radians; degree values must be converted to radians by dividing by 57.2958. As an example, to calculate 25 sin (600*0.05°) one would enter C 6 E 2 + 0 0 5 × 5 7 2 9 5 8 E 1 ÷ ▲ + 2 5 E 1 × to get a result of 1.2500 01 (representing 12.5 which is equal to 25 sin(30°) [...] ). Sine is selected with the combination of the [...] "▲" [...] key followed by the [...] "+" [...] key. The [...] "▼" [...] (down) and [...] "▲" [...] (up) arrow keys are function select keys. The four operation keys ("-, +, ÷ and ×") all have two other function activated by using one of the arrow keys. The function available are Sine, Arcsine, Cosine, Arccosine, Tangent, Arctangent, Logarithm and <b>Antilogarithm.</b>|$|E
40|$|A {{general design}} {{methodology}} for arithmetic operators in current mode multiple value logic is described. It {{is based on}} the interconnection of single output functions of one current, quantizers, through summing nodes and current replicator circuits. Some quantizers are known already: radix 4 sum and carry circuits. The authors present new circuits for the discrete pseudologarithm and <b>antilogarithm,</b> which together give a single digit multiply circuit. These form a complete set that allows any arithmetic circuit to be constructe...|$|E
5000|$|The primary {{calculation}} in GRAPE hardware is a {{summation of}} the forces between a particular star and every other star in the simulation. Several versions (GRAPE-1, GRAPE-3 and GRAPE-5) use the Logarithmic Number System (LNS) in the pipeline to calculate the approximate force between two stars, and take the <b>antilogarithms</b> of the x, y and z components before adding them to their corresponding total. [...] The GRAPE-2, GRAPE-4 and GRAPE-6 use floating point arithmetic for more accurate calculation of such forces. The advantage of the logarithmic-arithmetic versions is they allow more and faster parallel pipes for a given hardware cost because all but the sum portion of the GRAPE algorithm (1.5 power of {{the sum of the}} squares of the input data divided by the input data) is easy to perform with LNS. GRAPE-DR consists {{of a large number of}} simple processors, all operating in the SIMD fashion.|$|R
50|$|Warner was {{unpublished}} in his lifetime, {{but well}} known, {{in particular to}} Marin Mersenne who published some of his optical work in Universae geometriae (1646). He was an atomist, and a believer in an infinite universe. He was both a theoretical and practical chemist, and wrote psychological works based on Bruno and Lullism. Many manuscripts of his survive, and show eclectic interests; they include works related to the circulation of the blood. Some of Warner's papers {{ended up in the}} Pell manuscripts collected by Richard Busby; after his death the bulk of his papers were seized in 1644 by superstitious sequestrators. George John Gray, writing in the Dictionary of National Biography, states that the table of 11-figure <b>antilogarithms</b> later published by James Dodson was believed to have passed to Herbert Thorndike, and then to Busby; Pell's account in 1644 was that Warner had been bankrupt, and the creditors were likely to destroy the work.|$|R
5000|$|Bürgi {{constructed}} {{a table of}} progressions what is now understood as <b>antilogarithms</b> independently of John Napier, through a method distinct from Napier's. Napier published his discovery in 1614, and this publication was widely disseminated in Europe by the time Bürgi published {{at the behest of}} Johannes Kepler. Bürgi may have constructed his table of progressions around 1600, but Bürgi's work is not a theoretical basis for logarithms, although his table serves the same purpose as Napier's. One source claims that Bürgi did not develop a clear notion of a logarithmic function and can therefore not be viewed as an inventor of logarithms. Bürgi's method is different from that of Napier and was clearly invented independently. Kepler wrote about Bürgi's logarithms in the introduction to his Rudolphine Tables (1627): [...] "... as aids to calculation Justus Byrgius was led to these very logarithms many years before Napier's system appeared; but being an indolent man, and very uncommunicative, instead of rearing up his child for the public benefit he deserted it at birth." ...|$|R
40|$|Densitometric and {{gravimetric}} methods are employed {{to measure the}} percent of ink trapped in a wet-on-wet transfer. This study seeks to discover {{the relationship of the}} two methods {{and if there is a}} significant difference between the percent gravimetric and percent apparent trapping. An IGT Printability tester (Model-A 2) with two printing discs is used to apply ink onto paper. When using the gravimetric method the amount of ink on the disc and ink transferred onto paper are weighed with an analytical balance. The author then calculates the percent ink trapping. When using the densitometric methods densities are measured by polarizing and non-polarizing reflection densitometers which are used to generate data to determine percent trapping. The author also applies an <b>antilogarithm</b> trapping formula to determine percent trapping. The experimental factors studied here are the three tack levels of both ink layers, which are defined as low, medium, and high; the three different types of paper, which are newsprint, uncoated paper, and coated paper; and the four measurement methods. These methods are gravimetric, apparent, polarizing apparent, and <b>antilogarithm</b> trapping. The study results reveal that there is significant difference between the four measurement methods. The regression analysis establishes the relationship equations between percent gravimetric and apparant trapping. All of the equations are rejected because they lack accuracy in predicting percent gravimetric trapping using percent apparent trapping. Improved equations for percent trapping on newsprint, uncoated paper, and coated paper are presented. These equations estimate the percent gravimetric trapping on the basis of densitometric data but they are complex, and therefore only useful in the laboratory at this time. The author also determines percent gravimetric trapping using densities in terms of the exponential function and establishes prediction equations of percent trapping for each type of experimental paper. Further study will be needed to develop equations useful to the printing industry...|$|E
40|$|Abstract—A 32 -bit fixed-point {{logarithmic}} {{arithmetic unit}} is {{proposed for the}} possible application to mobile three-dimensional (3 -D) graphics system. The proposed logarithmic arithmetic unit performs division, reciprocal, square-root, reciprocal-square-root and square operations in two clock cycles and powering operation in four clock cycles. It can program its number range for accurate computation flexibility of 3 -D graphics pipeline and eight-region piecewise linear approximation model for logarithmic and antilog-arithmic conversion to reduce the operation error under 0. 2 %. Its test chip is implemented by 1 -poly 6 -metal 0. 18 - m CMOS technology with 9 -k gates. It operates at the maximum frequency of 231 MHz and consumes 2. 18 mW at 1. 8 -V supply. Index Terms—ALU, <b>antilogarithm,</b> logarithmic number system (LNS), 3 -D graphics. I...|$|E
30|$|Mean {{velocity}} in {{the middle}} cerebral artery (VmMCA) was measured using a 2 -MHz TCD probe through the temporal bone window {{on both sides of}} the skull, twice within the first 48  h after the confirmation of sepsis diagnosis for the septic group or after ICU admission for patients in the control group. An interval period of more than 20  h between the two measurements was ensured. Each measurement on each side of the brain was repeated three times and the highest value was considered for our analysis. The average of the two values on the two brain sides was registered. A difference in depth of 0.5  cm between the two sides was considered acceptable. At the time of the measurements, the patients were in a stable hemodynamic status. Pulsatility index (PI) (PI[*]=[*]Velocity systolic-Velocity diastolic/mean velocity)[19] and Cerebrovascular resistant index (RI) (RI[*]=[*]Velocity Systolic–Velocity diastolic/Velocity Systolic)[20] were calculated. Previous experimental data showed a strong inverse correlation between PI and the logarithm of regional cerebral blood flow measured by laser Doppler (r 2 [*]=[*] 0.81)[21]. From the formula that was provided (logCBF[*]=[*] 1.11 - 0.17 *PI), we found that the <b>antilogarithm</b> of CBF was related to the PI with the following formula CBF[*]=[*] 10 / 1.47 PI. Based on this formula, and given that cerebral blood flow (CBF) is estimated by the formula Perfusion Pressure/cerebrovascular resistances, we considered PI relate to cerebral resistances (CBVR) accordingly: CBVR[*]≈[*] 1.47 PI/ 10. We evaluated the ratio MAPx 10 / 1.47 PI as an index of CBF.|$|E
30|$|Summary {{statistics}} of PK parameters and concentrations included all treated subjects. Only subjects completing the study (i.e. PK data available for THC/CBD spray and THC/CBD spray plus interacting drug), {{were included in}} the statistical analyses of the interaction effects of rifampicin (Group 1), ketoconazole (Group 2), and omeprazole (Group 3). Data from the 3 groups were analyzed separately and no comparisons were made between groups. For each group, the PK parameters Cmax, AUC(0 -t) and AUC(0 -inf) were statistically analyzed using an analysis of variance model (ANOVA, SAS PROC MIXED). The traditional two-period crossover design was implemented. The model included effects of treatment, period, sequence, and subject within sequence. The log-transformed AUC and Cmax data was analyzed using a general linear mixed model. The model included fixed terms for treatment, sequence, period and a random term for subject within sequence. Point estimates and 90 % confidence intervals (CIs) for the ratios of the treatment means were calculated. The two one-sided hypotheses were tested at a 5 % level for Cmax, AUC(0 -t) and AUC(0 -inf) by constructing 90 % CIs for the ratio of the treatment means. The 90 % CIs were obtained from the <b>antilogarithms</b> of the lower and upper bounds of the 90 % CIs for the differences in the least-squares means of the log-transformed data. No significant interaction with respect to the log-transformed Cmax, AUC(0 -t) and AUC(0 -inf) was concluded if the 90 % CI of the ratio of the geometric means fell within the range of 0.80 to 1.25. The summaries and descriptive statistics were calculated using WinNonlin ® Professional, version 4.1 b and SAS ®, version 9.1.|$|R
40|$|Odds ratios (ORs), unlike chi 2 tests, provide direct {{insight into}} the {{strength}} of the relationship between treatment modalities and treatment effects. Multiple regression models can reduce the data spread due to certain patient characteristics and thus improve the precision of the treatment comparison. Despite these advantages, the use of these methods in clinical trials is relatively uncommon. Our objectives were (1) to emphasize the great potential of ORs and multiple regression models as a basis of modern methods; (2) to illustrate their ease of use; and (3) to familiarize nonmathematical readers with these important methods. Advantages of ORs are multiple. (1) They describe the probability that people with a certain treatment will have an event, versus those without the treatment, and are therefore a welcome alternative to the widely used chi 2 tests for analyzing binary data in clinical trials. (2) statistical software of ORs is widely available. (3) Computations using risk ratios (RRs) are less sensitive than those using ORs. (4) ORs are the basis for modern methods such as meta-analyses, propensity scores, logistic regression, and Cox regression. For analysis, logarithms of the ORs have to be used; results are obtained by calculating <b>antilogarithms.</b> A limitation of the ORs is that they present relative benefits but not absolute benefits. ORs, despite a fairly complex mathematical background, are easy to use, even for nonmathematicians. Both linear and logistic regression models can be adequately applied for the purpose of improving precision of parameter estimates such as treatment effects. We caution that, although application of these models is very easy with computer programs widely available, the fit of the regression models should always be carefully checked, and the covariate selection should be carefully considered and sparse. We do hope that this article will stimulate clinical investigators to use ORs and multiple regression models more ofte...|$|R
30|$|The log-based {{arithmetic}} unit {{embedded in the}} designed FPU utilizes the carry save adder for computing all arithmetic operations. It uses simple log principles, along with operational switches, to select the inputs based on the operation needs. If the adder operator is fed to the switch, the addition computation phenomenon is carried out by merely adding or subtracting the mantissa bits according to the exponent and sign bits. The difference of the two exponents is calculated. If any, perform the mantissa shift and set the larger exponent as the tentative exponent of the result. Shift the mantissa of the smaller exponent to the right by {{the difference in the}} exponents. According to the sign bit, perform addition (if equal) or subtraction (if unequal) on the mantissas to get the tentative mantissa as the result. Normalize and round off the mantissa result. If there is an overflow due to rounding, shift right and increment the exponent by 1 bit. Have the highest of the sign bits be the sign bit of the result. Similarly, a multiplication computation procedure is chosen for multiplier input that is fed to the operator switch. The overall data path involved in the multiplier component of this FPU architecture gets simplified. This is a mere computation with only mapping involved. Hence, this simplifies the overall stages involved in multiplications. The mantissas of the input data are mapped to the corresponding logarithmic number in the LUT. This is followed by adding the logarithms. If any overflow shifts the result to the right, then map with <b>antilogarithm</b> LUT to obtain the mantissa of the result. The exponent of the result is obtained by mere addition of the exponent bits, and the sign bit of the result is obtained by the Ex-or-ing both sign bits.|$|E
40|$|Nowadays, {{there are}} many {{commercial}} demands for decimal floating-point (DFP) arithmetic operations such as financial analysis, tax calculation, currency conversion, Internet based applications, and e-commerce. This trend gives rise to further development on DFP arithmetic units which can perform accurate computations with exact decimal operands. Due to the significance of DFP arithmetic, the IEEE 754 - 2008 standard for floating-point arithmetic includes it in its specifications. The basic decimal arithmetic unit, such as decimal adder, subtracter, multiplier, divider or square-root unit, as a main part of a decimal microprocessor, is attracting more and more researchers' attentions. Recently, the decimal-encoded formats and DFP arithmetic units have been implemented in IBM's system z 900, POWER 6, and z 10 microprocessors. Increasing chip densities and transistor count provide more room for designers to add more essential functions on application domains into upcoming microprocessors. Decimal transcendental functions, such as DFP logarithm, <b>antilogarithm,</b> exponential, reciprocal and trigonometric, etc, as useful arithmetic operations {{in many areas of}} science and engineering, has been specified as the recommended arithmetic in the IEEE 754 - 2008 standard. Thus, virtually all the computing systems that are compliant with the IEEE 754 - 2008 standard could include a DFP mathematical library providing transcendental function computation. Based on the development of basic decimal arithmetic units, more complex DFP transcendental arithmetic will be the next building blocks in microprocessors. In this dissertation, we researched and developed several new decimal algorithms and architectures for the DFP transcendental function computation. These designs are composed of several different methods: 1) the decimal transcendental function computation based on the table-based first-order polynomial approximation method; 2) DFP logarithmic and antilogarithmic converters based on the decimal digit-recurrence algorithm with selection by rounding; 3) a decimal reciprocal unit using the efficient table look-up based on Newton-Raphson iterations; and 4) a first radix- 100 division unit based on the non-restoring algorithm with pre-scaling method. Most decimal algorithms and architectures for the DFP transcendental function computation developed in this dissertation have been the first attempt to analyze and implement the DFP transcendental arithmetic in order to achieve faithful results of DFP operands, specified in IEEE 754 - 2008. To help researchers evaluate the hardware performance of DFP transcendental arithmetic units, the proposed architectures based on the different methods are modeled, verified and synthesized using FPGAs or with CMOS standard cells libraries in ASIC. Some of implementation results are compared with those of the binary radix- 16 logarithmic and exponential converters; recent developed high performance decimal CORDIC based architecture; and Intel's DFP transcendental function computation software library. The comparison results show that the proposed architectures have significant speed-up in contrast to the above designs in terms of the latency. The algorithms and architectures developed in this dissertation provide a useful starting point for future hardware-oriented DFP transcendental function computation researches...|$|E
40|$|Recently, high {{performance}} hardware intellectual property (IP) cores for arithmetic operations are highly {{required to meet}} the increasing demand of digital signal processing (DSP) applications in multimedia, wireless communications, mobile and handheld devices. Traditionally, the {{high performance}} arithmetic function generators can be implemented by logic circuits with some advanced techniques such as the parallel architecture. However, these techniques have to tradeoff between the computation performance and system complexity. In other words, the high performance computation circuits may occupy much hardware area and lead to high power consumption. As a result, some alternative approaches should be proposed for modern and future DSP applications. On the other hand, the look-up table (LUT) - based computation approach is promisingly suitable to be an alternative approach. LUT-based computation circuits provide the output by only accessing the pre-stored tables other than actual computations in real time. This LUT access-only operation results in the high speed and the low switching rate {{that leads to the}} low power consumption. Moreover, according to the report of the International Technology Roadmap for Semiconductors (ITRS), embedded memories are becoming faster, having higher density, lower dynamic power consumption and will dominate the content of the future System-on- Chip DSP applications. Also, advanced LUT technologies lead to the high improvements in LUT performance and density. Memorybased computation and LUT-based computation are also well suited for many DSP algorithms which require squaring, constant multiplications and elementary function computations. However, in the simple direct LUT-based computation, the LUT size grows exponentially when the operand width increases. Therefore, many researches focus on reducing the LUT size. Although there have been some existing methods for LUT-based computation components and systems, the new and more improved methods are desired because the future DSP applications require more efficient computation circuits. The research presented in this dissertation focuses on the efficient hardware IP cores based on the LUT architectures for DSP applications. Four methods for efficient specific hardware IP cores are proposed. Moreover, some detail design examples and a computation system based on these IP cores are developed. The proposed IP cores include both basic arithmetic operations (such as multiplication and squaring) and elementary functions (such as sine and logarithm/antilogarithm function computations). Two methods are proposed for the design of efficient LUT-based multiplier and squarer circuits. Firstly, a novel and efficient LUTbased truncated multiplier is presented for specific DSP applications in which the full width results are not required. This method combines two approaches of LUT-based computation and truncated multipliers for DSP applications. An LUT optimization algorithm is also developed to find the optimal parameters and LUT content. Secondly, an improved hybrid LUT-based architecture is employed for the fixedwidth squarer circuits. This hybrid technique takes the advantages of both LUT-based and conventional logic circuits to achieve the good trade-off of the squarer performance, error and complexity. For the computation of elementary unctions using LUT-based architecture, two novel architectures which combine LUT-based computation and linear difference approach are proposed. The first one is the improved linear difference method for the sine function computation which can be used in digital frequency synthesizer, adaptive signal processing and sine function generator. The numerical analysis and optimization are employed to find the optimal parameters which minimize the LUT size and hardware complexity while remain the same error performance compared with other methods. The second one is a novel quasi-symmetrical approach for the logarithm and <b>antilogarithm</b> computation. An optimization algorithm is also developed to find the optimal parameters for the hardware architecture. Thanks to the optimization algorithm and the LUT size reduction method, the hardware complexity and computation delay of the logarithm and anti-logarithm computation modules can be reduced significantly with the same accuracy compared with other methods. This method can be applied for both logarithm/anti-logarithm function generators in DSP systems and the domain converters in hybrid number system processors. Finally, some specific applications and a prototype computation system based on these IP cores are developed to clarify the improvements of proposed methods. With the improvements achieved, the proposed methods can be considered as potential candidates for modern and future DSP systems. 電気通信大学 201...|$|E

