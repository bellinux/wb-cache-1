247|185|Public
50|$|A popular {{method is}} based on the {{calculation}} of the quantum Jensen-Shannon divergence between all pairs of layers, which is then exploited for its metric properties to build a distance matrix and hierarchically cluster the layers. Layers are successively aggregated according to the resulting hierarchical tree and the <b>aggregation</b> <b>procedure</b> is stopped when the objective function, based on the entropy of the network, gets a global maximum. This greedy approach is necessary because the underlying problem would require to verify all possible layer groups of any size, requiring a huge number of possibile combinations (which is given by the Bell number and scales super-exponentially with the number of units). Nevertheless, for multilayer systems with a small number of layers, {{it has been shown that}} the method performs optimally in the majority of cases.|$|E
30|$|Algorithm 1 : <b>Aggregation</b> <b>procedure.</b>|$|E
40|$|In the investigation, the {{problems}} of utility evaluation methodology application for transport development projects evaluation are described. The main steps of development of impact <b>aggregation</b> <b>procedure</b> for sustainable transport system are described in the article. The new 3 stage environmental impact <b>aggregation</b> <b>procedure</b> is suggested...|$|E
40|$|Many axiomatic results {{concerning}} <b>aggregation</b> <b>procedures</b> {{in decision}} aiding {{have been obtained}} {{in the framework of}} conjoint measurement or social choice theory. We show that these frameworks, although they helped us to better understand some <b>aggregation</b> <b>procedures,</b> are not totally appropriate for decision aiding. We propose a new framework, very general, in which most <b>aggregation</b> <b>procedures</b> can fit and more appropriate for the axiomatization of <b>aggregation</b> <b>procedures</b> in decision aiding context. We present some axiomatic results obtained in this framework and showing its interest...|$|R
40|$|This thesis {{addresses}} the {{mechanisms by which}} groups of agents can track the truth, particularly in political situations. I argue that the mechanisms which allow groups of agents to track the truth operate in two stages: firstly, there are search procedures; and secondly, there are <b>aggregation</b> <b>procedures.</b> Search <b>procedures</b> and <b>aggregation</b> <b>procedures</b> work in concert. The search procedures allow agents to extract information from the environment. At {{the conclusion of a}} search procedure the information will be dispersed among different agents in the group. <b>Aggregation</b> <b>procedures,</b> such as majority rule, expert dictatorship and negative reliability unanimity rule, then pool these pieces of information into a social choice. The institutional features of both search <b>procedures</b> and <b>aggregation</b> <b>procedures</b> account for the ability of groups to track the truth and amount to social epistemic mechanisms. Large numbers of agents are crucial for the epistemic capacities of both search <b>procedures</b> and <b>aggregation</b> <b>procedures.</b> This thesis makes two main contributions to the literature on social epistemology and epistemic democracy. Firstly, most current accounts focus on the Condorcet Jury Theorem and its extensions as the relevant epistemic mechanism that can operate in groups of political agents. The introduction of search procedures to epistemic democracy is (mostly) new. Secondly, the thesis introduces a two-stage framework to the process of group truth-tracking. In 4 addition to showing how the two procedures of search and aggregation can operate in concert, the framework highlights the complexity of social choice situations. Careful consideration of different types of social choice situation shows that different <b>aggregation</b> <b>procedures</b> will be optimal truth-trackers in different situations. Importantly, there will be some situations in which <b>aggregation</b> <b>procedures</b> other than majority rule will be best at tracking the truth...|$|R
40|$|This paper {{stresses}} that standard multicriteria <b>aggregation</b> <b>procedures</b> either do not assume any structure in data or this structure {{is in fact}} assumed linear. Nevertheless, many decision making problems are based upon a family of data with a well defined spatial structure, which is simply not taken into account. Hence, such <b>aggregation</b> <b>procedures</b> may be misleading. Therefore, we propose an alternative model where the aggregation of criteria assumes a certain structure, according to remote sensing data. ...|$|R
3000|$|... [2]. There cannot exist any {{preference}} <b>aggregation</b> <b>procedure</b> that simultaneously satisfies certain consistency conditions [...]...|$|E
3000|$|We {{like our}} <b>aggregation</b> <b>procedure</b> to be {{strategy}} proof. Impossibility theorems proved by Dietrich and List [...]...|$|E
3000|$|We first {{discuss the}} {{complexity}} of the <b>aggregation</b> <b>procedure.</b> Based on Algorithm 1, the procedure between line 5 to line 12 is iterated [...]...|$|E
30|$|In {{addition}} to the above mentioned <b>aggregation</b> <b>procedures</b> and synthesizing criteria, fuzzy logic [109], decision rules [110], multi-objective mathematical programming [111], and objective classification [112] have been employed to improve {{the performance of the}} MCDM approaches.|$|R
40|$|The {{results from}} the U. S. corn/soybeans {{exploratory}} experiment which was completed during FY 1980 are summarized. The experiment consisted of two parts: the classification procedures verification test and the simulated aggregation test. Evaluations of labeling, proportion estimation, and <b>aggregation</b> <b>procedures</b> are presented...|$|R
40|$|In the paper, we {{deal with}} {{cardinal}} preferences of experts when these are expressed by means of Pairwise Comparison Matrices (PCMs). In order to obtain general results, suitable for several kinds of PCMs proposed in literature, we focus on PCMs defined over a general unifying framework, that is an Abelian linearly ordered group. In this framework, firstly, we aggregate several PCMs and we analyse how the aggregated PCM preserves some coherence levels, such as transitivity, weak consistency and consistency. Then, we reformulate Arrow’s conditions in terms of PCMs, and we provide two preference <b>aggregation</b> <b>procedures</b> for representing group preferences that give a social PCM and a social cardinal ranking, respectively. Finally, we analyse how these preference <b>aggregation</b> <b>procedures</b> satisfy reformulated Arrow’s conditions...|$|R
40|$|Given {{stationary}} {{time series}} data, we study {{the problem of}} finding the best linear combination {{of a set of}} lag window spectral density estimators with respect to the mean squared risk. We present an <b>aggregation</b> <b>procedure</b> and prove a sharp oracle inequality for its risk. We also provide simulations demonstrating the performance of our <b>aggregation</b> <b>procedure,</b> given Bartlett and other estimators of varying bandwidths as input. This extends work by P. Rigollet and A. Tsybakov on aggregation of density estimators...|$|E
40|$|As human {{immunodeficiency}} virus (HIV) begins to replicate within hosts, immune responses are elicited against it. Escape mutations in viral epitopes—immunogenic peptide parts presented {{on the surface of}} infected cells—allow HIV to partially evade these responses, and thus rapidly go to fixation. The faster they go to fixation, i. e., the higher their escape rate, the larger the selective pressure exerted by the immune system is assumed to be. This relation underpins the rationale for using escapes to assess the strength of immune responses. However, escape rate estimates are often obtained by employing an <b>aggregation</b> <b>procedure,</b> where several mutations that affect the same epitope are aggregated into a single, composite epitope mutation. The <b>aggregation</b> <b>procedure</b> thus rests upon the assumption that all within-epitope mutations have indistinguishable effects on immune recognition. In this study, we investigate how violation of this assumption affects escape rate estimates. To this end, we extend a previously developed simulation model of HIV that accounts for mutation, selection, and recombination to include different distributions of fitness effects (DFEs) and inter-mutational genomic distances. We use this discrete time Wright–Fisher based model to simulate early within-host evolution of HIV for DFEs and apply standard estimation methods to infer the escape rates. We then compare true with estimated escape rate values. We also compare escape rate values obtained by applying the <b>aggregation</b> <b>procedure</b> with values estimated without use of that procedure. We find that across the DFEs analyzed, the <b>aggregation</b> <b>procedure</b> alters the detectability of escape mutations: large-effect mutations are overrepresented while small-effect mutations are concealed. The effect of the <b>aggregation</b> <b>procedure</b> is similar to extracting the largest-effect mutation appearing within an epitope. Furthermore, the more pronounced the over-exponential decay of the DFEs, the more severely true escape rates are underestimated. We conclude that the <b>aggregation</b> <b>procedure</b> has two main consequences. On the one hand, it leads to a misrepresentation of the DFE of fixed mutations. On the other hand, it conceals within-epitope interactions that may generate irregularities in mutation frequency trajectories that are thus left unexplained...|$|E
3000|$|The <b>aggregation</b> <b>procedure</b> aims {{to reduce}} the {{cardinality}} of the set of shared regions, thus greatly reduces the solution space. Given a set of shared regions [...]...|$|E
40|$|In {{supply chain}} {{management}} production planning is often separated into several planning levels. These levels have different degrees of detail and are usually connected by aggregation-disaggregation devices. Particularly aggregate parameters, like aggregate holding cost and capacity consumption rates, are to be determined in an optimal way. Following the aggregation levels of the traditional (MIT) hierarchical production planning approach we are analysing different aggregation devices for an aggregate linear capacity adaptation model and an adjoining detailed mixed integer production model. In particular, focusing on the aggregation of demand we are developing two <b>aggregation</b> <b>procedures</b> in using shadow prices of the inventory balance constraints. An extensive numerical investigation is comparing these new aggregation devices for mixed integer models with the more traditional approaches based on an aggregation of work content and on even simpler <b>aggregation</b> <b>procedures</b> in practice...|$|R
40|$|A {{new method}} for the formal {{description}} of systems {{in terms of}} “Unit”, “Function”, and “Object” based on the comparative study and integration of tools from the Grenander algebraic theory of pat terns and Milner’s process calculus is considered. Decomposition and <b>aggregation</b> <b>procedures</b> are formalized for the graphic-analytical models “Unit-Function-Object” using the proposed approachyesBelgorod State Universit...|$|R
40|$|International audienceMaximin {{expected}} {{utility model}} for individual decision making under ambiguity prescribes {{that the individual}} posits independently a utility function {{and a set of}} probability distributions over events to represent the values and belief, respectively. It assumes that individual evaluates each act {{on the basis of its}} minimum expected utility over this class of distributions. In this paper, we attempt to generalize the model to social decision making. It is assumed that the society’s belief is formed through a linear aggregation of individual beliefs and society’s values through a linear aggregation of individual values. We propose principles which characterize such separate <b>aggregation</b> <b>procedures.</b> We also generalize Choquet expected utility model, which posits a nonadditive measure over events and a utility function to represent belief and values, respectively. We prove that the only <b>aggregation</b> <b>procedures</b> that respect our principles are the separate linear aggregations of beliefs and value...|$|R
40|$|Smoothed {{aggregation}} multigrid {{method is}} considered for computing stationary distributions of Markov chains. A judgement which determines whether {{to implement the}} whole <b>aggregation</b> <b>procedure</b> is proposed. Through this strategy, {{a large amount of}} time in the <b>aggregation</b> <b>procedure</b> is saved without affecting the convergence behavior. Besides this, we explain the shortage and irrationality of the Neighborhood-Based aggregation which is commonly used in multigrid methods. Then a modified version is presented to remedy and improve it. Numerical experiments on some typical Markov chain problems are reported to illustrate the performance of these methods...|$|E
40|$|Given {{a finite}} set F of {{functions}} and a learning sample, {{the aim of}} an <b>aggregation</b> <b>procedure</b> {{is to have a}} risk {{as close as possible to}} risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the <b>aggregation</b> <b>procedure.</b> Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal <b>aggregation</b> <b>procedure</b> in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures...|$|E
40|$|The paper aims {{to discuss}} two {{heuristic}} approaches for cell formation techniques, cell <b>aggregation</b> <b>procedure</b> and inter cell flow reduction procedure are compared {{on the basis}} of efficiency measures. Theglobal efficiency, group efficiency, and group technology efficiency are the measures to compare the methods with part routing sequence inputs. The study suggests that inter cell flow reduction procedureprovides better results than cell <b>aggregation</b> <b>procedure</b> in terms of efficiency. The study further suggests when an alternative part routing procedure is used, which is based on the variance of busy time of machines of same type, it is observed that the workload of the machines is balanced up to some extent but the efficiency is decreased...|$|E
40|$|In this paper, {{we present}} an {{empirical}} methodology to determine aggregate numerical criteria weights from group ordinal ranks of multiple decision criteria. Assuming that such ordinal ranks are obtained from several decision makers, <b>aggregation</b> <b>procedures</b> are proposed to combine individual rank inputs into group criteria weights. In this process, we use previous empirical results {{for an individual}} decision maker, in which a simple function provides the weight for each criterion {{as a function of}} its rank and the total number of criteria. Using a set of experiments, weight <b>aggregation</b> <b>procedures</b> are proposed and empirically compared for two cases: (i) when all the decision makers rank the same set of criteria, and (ii) when they rank different subsets of criteria. The proposed methodology can be used to determine relative weights for any set of criteria, given only criteria ranks provided by several decision makers. Multi-criteria, decision making/process, group decisions...|$|R
40|$|The {{variability}} of protection rates within sectors is frequently particularly high in agriculture relative to non-agriculture. Standard <b>aggregation</b> <b>procedures</b> ignore the variability within sectors, and underweight {{the importance of}} highly protected sectors. It therefore seems likely that they underestimate {{the potential benefits of}} agricultural trade reform relative to non-agricultural reform. This study examines this question using a new procedure for aggregating trade distortions. It finds that the key impact of using better aggregators is to increase the benefits of both agricultural and non-agricultural reform. It finds that using optimal <b>aggregation</b> <b>procedures</b> increases the measured importance of agricultural trade reform relative to non-agricultural reform from a very high initial level, but only by around two percentage points. agricultural trade, nonagricultural trade, trade distortions, tariffs, aggregation, World Trade Organization, WTO, trade reform, Food Security and Poverty, International Relations/Trade, F 13, F 14, Q 13, Q 17, Q 18,...|$|R
40|$|The primary {{responsibility}} of the crop assessment subsystem (CAS) during the three phases of LACIE was to produce crop reports that included estimates of wheat area, yield, and production, {{as well as a}} specified set of associated statistical descriptors. The operations of CAS are described with emphasis on sampling strategy, input/output data, evolution of aggregation/reporting system capabilities, and CAS <b>aggregation</b> <b>procedures...</b>|$|R
40|$|The welfare {{implications}} of alternative monetary aggregation procedures are investigated {{by providing a}} comparison among simple-sum, Divisia, and currency equivalent monetary aggregates {{at different levels of}} monetary aggregation. Evidence is found that the choice of monetary <b>aggregation</b> <b>procedure</b> is crucial in evaluating the welfare cost of inflation. ...|$|E
40|$|International audienceOne {{main focus}} of {{learning}} theory is to find optimal rates of convergence. In classification, {{it is possible to}} obtain optimal fast rates (faster than n− 1 / 2) in a minimax sense. Moreover, using an <b>aggregation</b> <b>procedure,</b> the algorithms are adaptive to the parameters of the class of distributions. Here, we investigate this issue in the bipartite ranking framework. We design a ranking rule by aggregating estimators of the regression function. We use exponential weights based on the empirical ranking risk. Under several assumptions on the class of distribution, we show that this procedure is adaptive to the margin parameter and smoothness parameter and achieves the same rates as in the classification framework. Moreover, we state a minimax lower bound that establishes the optimality of the <b>aggregation</b> <b>procedure</b> in a specific case...|$|E
40|$|The usual {{procedure}} in Social Choice Theory consists in postulating some desirable properties which a <b>aggregation</b> <b>procedure</b> should verify and from them to derive {{the features of}} the corresponding social choice function and the outcomes that arise at each possible pro¯le of preferences. In this paper we invert this line of reasoning and try to infer, up from what we call social situations (each one consisting of a pro¯le and the associated social ordering) the criteria veri¯ed in the implicit <b>aggregation</b> <b>procedure.</b> Furthermore we derive them in axiomatic form. This inference process, which extracts intentional from extensional information can be seen as an exercise in social choicetheoretic &quot;. The fact that complete intentional characterizations of the aggregation process cannot be derived in such way can be easily seen {{as a consequence of the}} procedure...|$|E
40|$|Abstract—With the {{proliferation}} of mobile devices, mobile mashups promise great data aggregation and processing capabil-ities for all end users. During the data collection and <b>aggregation</b> <b>procedures,</b> some data providers fail to protect confidentiality and privacy of user queries and transmit information in plain text. This enables attackers to eavesdrop on wireless networks and compromise user information. Since mobile mashups can adopt server-side, client-side, or hybrid architectures, no one-size-fits-all solutions can be designed to solve this problem. In this paper, we propose to design two mechanisms using mobile clouds to preserve data query privacy in mobile mashups. For server-side mashups, we propose to use dynamically created virtual machines as proxies to process data collection and aggregation {{in order to prevent}} information leakage through eavesdropping. For client-side mashups, we propose to use live migration of the application level virtual machines into mobile cloud to hide the data collection and <b>aggregation</b> <b>procedures</b> from attackers. We will evaluate the proposed approaches through both analysis and experiments on real platforms. I...|$|R
40|$|The use of {{different}} objective functions in hierarchical <b>aggregation</b> <b>procedures</b> is {{examined in this}} paper. Specifically, we analyse {{the use of the}} original Intramax objective function, the sum-of-flows objective function, the sum-of-proportions-to-intra-regional-flows objective function, Smart’s weighted interaction index, the first and second CURDS weighted interaction indices, and Tolbert and Killian’s interaction index. The results of the functional regionalisation have been evaluated by self-containment statistics, and they show that the use of the original Intramax procedure tends to delineate operationally the most persuasive and balanced regions that, regarding the intra-regional flows, homogeneously cover the analysed territory. The other objective functions give statistically better but operationally less suitable results. Functional regions modelled using the original Intramax procedure were compared to the regions at NUTS 2 and NUTS 3 levels, as well as to administrative units in Slovenia. We conclude that there are some promising directions for further research on functional regionalisation using hierarchical <b>aggregation</b> <b>procedures...</b>|$|R
40|$|The {{aggregation}} {{of consistent}} individual judgments on logically interconnected propositions into a collective judgment on those propositions has recently drawn much attention. Seemingly reasonable <b>aggregation</b> <b>procedures,</b> such as propositionwise majority voting, cannot ensure an equally consistent collective conclusion. The literature on judgment aggregation refers to that problem as the discursive dilemma. In this paper, we motivate that many groups do not {{only want to}} reach a factually right conclusion, but also want to correctly evaluate the reasons for that conclusion. In other words, we {{address the problem of}} tracking the true situation instead of merely selecting the right outcome. We set up a probabilistic model analogous to Bovens and Rabinowicz (2006) and compare several <b>aggregation</b> <b>procedures</b> by means of theoretical results, numerical simulations and practical considerations. Among them are the premise-based, the situation-based and the distance-based procedure. Our findings confirm the conjecture in Hartmann, Pigozzi and Sprenger (2008) that the premise-based procedure is a crude, but reliable and sometimes even optimal form of judgment aggregation. ...|$|R
40|$|For {{a social}} {{decision}} problem we define a new <b>aggregation</b> <b>procedure</b> [...] the threshold rule [...] {{for the construction}} of an output ranking from the individual m-graded rankings with an arbitrary integer mÂ >=Â  2. An axiomatic characterization of the procedure is given. Social decision problem Ranking Aggregation Threshold rule Representation theorem...|$|E
3000|$|In {{order to}} obtain the optimal {{solution}} as a benchmark for our GICO algorithm, we need to search all possible combinations of the shared regions. However, even after employing the <b>aggregation</b> <b>procedure</b> {{to reduce the size}} of solution space, the complexity for searching the optimal solution could still be as high as [...]...|$|E
40|$|We {{consider}} {{the problem of}} adaptation to the margin and to complexity in binary classification. We suggest an exponential weighting aggregation scheme. We use this <b>aggregation</b> <b>procedure</b> to construct classifiers which adapt automatically to margin and complexity. Two main examples are worked out in which adaptivity is achieved in frameworks proposed b...|$|E
30|$|This paper {{assesses the}} {{predictive}} {{value of a}} novel measure of consensus among agents’ expectations. This metric presents several advantages over alternative <b>aggregation</b> <b>procedures</b> of qualitative survey expectations. On the one hand, as it gives the percentage of agreement among respondents, it is directly interpretable. On the other hand, it allows incorporating information from respondents who do not expect any change in questions with three response options.|$|R
3000|$|... [10] {{used the}} concept of fuzzy set theory to handle {{imprecision}} (or uncertainty) to solve the ambiguity and vagueness in human preference. Fuzzy logic can help in <b>aggregation</b> <b>procedures</b> to make the collective preference set democratic in nature. Trapezoidal fuzzy numbers are {{the best way to}} model preferences. Recently, a lot of work on trapezoidal fuzzy numbers has been done by Li et al.|$|R
40|$|This paper {{considers}} {{the effect of}} aggregation on the variance of parameter estimates for a linear regression model with random coefficients and an additive error term. Aggregate and microvariances are compared and measures of relative efficiency are introduced. Necessary conditions for efficient <b>aggregation</b> <b>procedures</b> are obtained from the Theil aggregation weights and from measures of synchronization related {{to the work of}} Grunfeld and Griliches. ...|$|R
