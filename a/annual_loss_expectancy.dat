7|2192|Public
40|$|In {{this thesis}} it isshown {{how to measure}} the <b>annual</b> <b>loss</b> <b>expectancy</b> of {{computer}} networks due to therisk of cyber attacks. With the development of metrics for measuring theexploitation difficulty of identified software vulnerabilities, it is possibleto make a measurement of the <b>annual</b> <b>loss</b> <b>expectancy</b> for computer networks usingBayesian networks. To enable the computations, computer net-work vulnerabilitydata {{in the form of}} vulnerability model descriptions, vulnerable dataconnectivity relations and intrusion detection system measurements aretransformed into vector based numerical form. This data is then used to generatea probabilistic attack graph which is a Bayesian network of an attack graph. The probabilistic attack graph forms the basis for computing the annualizedloss expectancy of a computer network. Further, it is shown how to compute anoptimized order of vulnerability patching to mitigate the annual lossexpectancy. An example of computation of the <b>annual</b> <b>loss</b> <b>expectancy</b> is providedfor a small invented example networ...|$|E
30|$|The fixed {{parameter}} {{includes the}} <b>annual</b> <b>loss</b> <b>expectancy</b> (ALE), which characterizes the intrusion or attack and is directly acquirable from expert knowledge through interviews, as performed by us and presented in Section 6.|$|E
40|$|The goal-oriented {{security}} process {{helps to}} alleviate {{risks associated with}} security and other continuity goals such as scalability, availability, reliability and compliance. System assets, threats, and potential safeguards are modeled in a goal tree to show the likelihood and impact of individual threats and the efficacy of safeguards to protect against them. This paper describes the process of selecting a set of safeguards that optimize security and continuity goals. To mitigate the combination problem introduced by a potentially large set of candidate safeguards, a heuristic method and a genetic algorithm approach for safeguard selection are described An experimental analysis of these approaches is described in which simulated goal trees were generated, and performance, accuracy, and utility of the safeguard selection techniques were evaluated. The paper also describes and assesses different objective functions including optimization of net present value, depth of defense, and target <b>annual</b> <b>loss</b> <b>expectancy...</b>|$|E
5000|$|The annualized <b>loss</b> <b>expectancy</b> (ALE) is {{the product}} of the annual rate of {{occurrence}} (ARO) and the single <b>loss</b> <b>expectancy</b> (SLE). It is mathematically expressed as: ...|$|R
50|$|For {{an annual}} rate of {{occurrence}} of one, the annualized <b>loss</b> <b>expectancy</b> is 1 * $25,000, or $25,000.|$|R
50|$|In {{quantitative}} {{risk assessment}} an annualized <b>loss</b> <b>expectancy</b> (ALE) {{may be used to}} justify the cost of implementing countermeasures to protect an asset. This may be calculated by multiplying the single <b>loss</b> <b>expectancy</b> (SLE), which is the loss of value based on a single security incident, with the annualized rate of occurrence (ARO), which is an estimate of how often a threat would be successful in exploiting a vulnerability.|$|R
30|$|To acquire all {{parameters}} and {{models for the}} RFIA requires a great level of accuracy in estimations, i.e., requires detailed analyses of, e.g., monetary values. As the RFIA is based on direct calculations on acquired parameters, results can only be as accurate as the forecasts of loss event frequencies on which they rely. A significant amount of parameters, e.g., the <b>annual</b> <b>loss</b> <b>expectancy</b> (ALE), and the effectiveness (EF) used to compute the risk mitigation level, rely on expert knowledge, which will involve human errors. Effectively, an RFIA requires various estimated parameters whose kind {{is very similar to}} classical business economical operating figures. This means that, on the one hand side, a large amount of expert knowledge is required and many parameters must be manually assessed by experts. However, on the other hand side, experts from which this knowledge is acquirable, are trained business experts that are deeply familiar with this kind of estimation: estimating business operating figures. Even though the targeted security of an IT infrastructure is outside of the expert’s subject area, the kind of required knowledge lies in their expertise.|$|E
40|$|Information {{security}} {{investment has}} been getting increasing attention in recent years. Various methods have been proposed to determine the effective level of security investment. However, traditional expected value methods (such as <b>annual</b> <b>loss</b> <b>expectancy)</b> cannot fully characterize the information security risk confronted by organizations, considering some extremal yet perhaps relatively rare cases in which a security failure may be critical and cause high losses. In this research note we introduce the concept of value-at-risk to measure the risk of daily losses an organization faces due to security exploits and use extreme value analysis to quantitatively estimate the value at risk. We collect a set of internal daily activity data from a large financial institution in the northeast United States and then simulate its daily losses with information based on data snapshots and interviews with security managers at the institution. We illustrate our methods using these simulated daily losses. With this approach, decision makers can make a proper investment choice {{based on their own}} risk preference instead of pursuing a solution that minimizes only the expected cost...|$|E
40|$|Producing a {{cost-benefit}} {{analyses of}} security solutions {{has always been}} hard, because the benefits are difficult to assess and often {{only a part of}} the overall cost is clear. Despite this, today the provision of economic evaluations of security technology investments is a requirement that more and more customers ask vendors to satisfy. In this paper, we consider the typical calculation of a ReturnOn -Investment (ROI) index based on the evaluation of the <b>Annual</b> <b>Loss</b> <b>Expectancy</b> (ALE), as the one provided usually by vendors of IT security. Our motivating assumption is that such classical index, the ROI, provides a partial characterization of investments in information security technology, because it lacks to explicitly consider attackers' behavior. We suggest that to better evaluate security technology investments, the ROI index should be coupled with a corresponding index aimed at measuring the convenience of attacks, the Return-On-Attack (ROA) expecially in situation where different technologies are combined or where the possible degradation of a security solution's efficiency over time must be taken into account...|$|E
50|$|In {{financial}} terms, quantitative {{risk assessments}} include a {{calculation of the}} single <b>loss</b> <b>expectancy</b> of monetary value of an asset.|$|R
5000|$|The annualized <b>loss</b> <b>expectancy</b> is {{the product}} of the annual rate of {{occurrence}} (ARO) and the single loss expectancy.ALE = ARO * SLE ...|$|R
50|$|Suppose that {{an asset}} {{is valued at}} $100,000, and the Exposure Factor (EF) for this asset is 25%. The single <b>loss</b> <b>expectancy</b> (SLE) then, is 25% * $100,000, or $25,000.|$|R
40|$|The {{starting}} point of this research essay is a critical review of two methods to conduct a quantitative analysis of information systems security risks: 1) Management of Risk: Guidance for Practitioners and 2) a cost model based on <b>annual</b> <b>loss</b> <b>expectancy.</b> We are focusing on these methods with a perspective that highlights the limits of both empiricism and the theoretical elements that underlie them. From an epistemological point of view we have considered the logical syntax of the two models, the semantics included in statements and the pragmatics of the scientific discourse: the use of models to demonstrate the risk assessment thesis, {{to solve the problems}} of risks in the human judgment versus mathematical calculus controversy. The major issues that we are discussing in this article imply various perspectives on scientific criteria, the choice among various theories and the structuring of problems proposed to be solved. We argue that the models that have been developed so far, the top-down approach (which involves well defined and well understood rules), as well as the demonstrations based on the induction method, cannot be applied in a lot of scenarios, because information systems, considered as a complex whole made up of various components, is primarily not a positivistic science solely described by mathematics. The main research question to be answered in this paper is: What are the limits of knowledge in probabilistic computations for information systems security risk assessment? Our purpose is to demonstrate the epistemological limits of the two models and the error of generalizing probability calculus using the interpretive approach...|$|E
30|$|Separate average {{waveforms}} {{were obtained}} for four identified experimental conditions (High <b>Expectancy</b> <b>Loss,</b> High <b>Expectancy</b> Win, Low <b>Expectancy</b> <b>Loss,</b> and Low <b>Expectancy</b> Win). The High Expectancy Win condition {{was taken from the}} fourth epochal feedback window during consistent win presentation signaling correct pattern establishment and, presumably, a higher expectation of subsequent win feedback. The High <b>Expectancy</b> <b>Loss</b> condition was taken from the epochal feedback window following the first incorrect trial following consistent pattern establishment, signaling a pattern shift to the participant. The Low Expectancy Win condition was taken from the epochal window following the first correct trial following a pattern shift, while the Low <b>Expectancy</b> <b>Loss</b> condition was taken from the window following the first incorrect trial following the loss signaling a pattern shift.|$|R
25|$|By way of contrast, during 1991–2007, {{commercial}} banks' average <b>annual</b> <b>loss</b> rate on single-family mortgages {{was about}} 15 basis points. During 2008–2011, <b>annual</b> <b>losses</b> were 184 basis points.|$|R
25|$|In August 2016, {{the company}} {{recorded}} its worst <b>annual</b> <b>loss</b> in history, $6.4 billion.|$|R
50|$|This pest causes US $334 million <b>annual</b> <b>loss</b> to sorghum {{alone in}} the {{semiarid}} tropics.|$|R
50|$|For example, if {{performing}} activity X has {{a probability}} of 0.01 of suffering {{an accident of}} A, {{with a loss of}} 1000, and a probability of 0.000001 of suffering an accident of type B, with a loss of 2,000,000, then total <b>loss</b> <b>expectancy</b> is 12, which is equal to a loss of 10 from an accident of type A and 2 from an accident of type B.|$|R
25|$|Typical <b>annual</b> <b>loss</b> {{of thermal}} energy through {{distribution}} is around 10%, {{as seen in}} Norway's district heating network.|$|R
50|$|By 1999, United States offline {{and online}} credit card fraud <b>annual</b> <b>losses</b> were {{estimated}} at between $500,000 and $2 million.|$|R
50|$|The dam's current {{problem is}} the <b>annual</b> <b>loss</b> of {{reservoir}} capacity due to the erosion of soil in upstream areas.|$|R
5000|$|Ablation zone :Area of {{a glacier}} {{in which the}} <b>annual</b> <b>loss</b> of ice through {{ablation}} exceeds the annual gain from precipitation.|$|R
5000|$|Estimated {{direct and}} {{consequential}} <b>annual</b> <b>loss</b> to {{industries in the}} USA due to wear is approximately 1-2% of GDP. (Heinz, 1987).|$|R
40|$|Under the Basel II standards, the Operational Risk (OpRisk) {{advanced}} measurement {{approach is}} not prescriptive regarding the class of statistical model utilised to undertake capital estimation. It has however become well accepted to utlise a Loss Distributional Approach (LDA) paradigm to model the individual OpRisk loss process corresponding to the Basel II Business line/event type. In this paper we derive a novel class of doubly stochastic alpha-stable family LDA models. These models provide the ability to capture the heavy tailed loss process typical of OpRisk whilst also providing analytic expressions for the compound process <b>annual</b> <b>loss</b> density and distributions {{as well as the}} aggregated compound process <b>annual</b> <b>loss</b> models. In particular we develop models of the <b>annual</b> <b>loss</b> process in two scenarios. The first scenario considers the loss process with a stochastic intensity parameter, resulting in an inhomogeneous compound Poisson processes annually. The resulting arrival process of losses under such a model will have independent counts over increments within the year. The second scenario considers discretization of the <b>annual</b> <b>loss</b> process into monthly increments with dependent time increments as captured by a Binomial process with a stochastic probability of success changing annually. Each of these models will be coupled under an LDA framework with heavy-tailed severity models comprised of α-stable severities for the loss amounts per loss event. In this paper we will derive analytic results for the <b>annual</b> <b>loss</b> distribution density and distribution under each of these models and study their properties. ...|$|R
25|$|The GSE {{business}} model has outperformed any other {{real estate business}} throughout its existence. According to the Annual Report to Congress, filed by the Federal Housing Finance Agency, over a span of 37 years, from 1971 through 2007, Fannie Mae's average <b>annual</b> <b>loss</b> rate on its mortgage book was about four basis points. Losses were disproportionately worse during the crisis years, 2008 through 2011, when Fannie's average <b>annual</b> <b>loss</b> rate was 52 basis points. Freddie Mac's results are comparable.|$|R
50|$|Thus, tenant incomes {{have been}} disappointing, {{but it is}} {{doubtful}} whether incomes {{would have been better}} with higher yields, as a trade-off exist between the tenant incomes and the <b>annual</b> <b>loss</b> that the project makes. In 1990, the World Bank pointed out that the project made an <b>annual</b> <b>loss</b> of Kshs 17,000 per settler. If the project wanted to be financially viable, then the tenants were expected to pay this <b>annual</b> <b>loss</b> through their service charges. At Bura, the tenants paid Kshs 3,000 per settler, but to reach a break-even point, the tenants should have paid a much higher amount, meaning that tenants would not earn any income. The project management finds itself in a balancing act that on the one hand it has to keep the charges at such a level that the tenants do not lose interest in farming, whereas {{at the same time it}} needs to keep the <b>annual</b> <b>loss</b> on its operations as low as possible. In the beginning the project got adequate government support to take care of the losses, although a side effect was that the operation and maintenance of the project suffered. Later the government support was reduced.|$|R
50|$|In June 2009, Ryanair {{reported}} their first <b>annual</b> <b>loss,</b> {{with a loss}} posted of €169 million for the financial year ending 31 March.|$|R
40|$|On {{completion}} of this article, the reader {{should be able}} to (1) discuss the effect that level of injury has on <b>loss</b> of life <b>expectancy</b> after gun-related spinal cord injury, (2) describe the effects that gender and race have on additional life <b>expectancy</b> <b>loss</b> after gun-related spinal cord and brain injury, and (3) describe the additional <b>loss</b> of life <b>expectancy</b> attributable to gun-related spinal cord injury and traumatic brain injury in th...|$|R
50|$|In {{business}} it {{is imperative}} to be able to present the findings of risk assessments in financial, market, or schedule terms. Robert Courtney Jr. (IBM, 1970) proposed a formula for presenting risks in financial terms. The Courtney formula was accepted as the official risk analysis method for the US governmental agencies. The formula proposes calculation of ALE (annualized <b>loss</b> <b>expectancy)</b> and compares the expected loss value to the security control implementation costs (cost-benefit analysis).|$|R
50|$|The lack of new burials {{left the}} {{cemetery}} {{in a difficult}} financial situation. The cemetery was experiencing an <b>annual</b> <b>loss</b> of $3,000 a year.|$|R
40|$|The clover-seed chalcis fly {{which is}} {{generally}} termed the alfalfa-seed chalcis-fly by alfalfa-seed growers, has been increasing so rapidly that its destructive work is now causing a large <b>annual</b> <b>loss,</b> and in some sections even threatening the production of alfalfa seed [...] This bulletin will serve to give the alfalfa-seed grower a general knowledge of the chalcis-fly, together with such information as will direct him in adopting measures for reducing the large <b>annual</b> <b>loss</b> due to it work. " [...] p. 1 -...|$|R
5000|$|... #Caption: Wage theft versus other {{property}} crimes (robbery, burglary, larceny, and auto theft) in the United States {{in terms of}} total <b>annual</b> <b>loss</b> in billions.|$|R
40|$|This paper simulates the {{increase}} in the average <b>annual</b> <b>loss</b> from tropical cyclones in the North Atlantic for the years 2015 and 2050. The simulation is based on assumptions concerning wealth trends in the regions affected by the storms, considered by the change in material assets (capital stock). Further assumptions are made about the trend in storm intensity resulting from anthropogenic climate change. The simulations use a stochastic model that models the <b>annual</b> storm <b>loss</b> from the number of storms and the loss per storm event. The paper demonstrates that increasing wealth will continue to be the principle loss driver in the future (average <b>annual</b> <b>loss</b> in 2015 + 32 %, in 2050 + 308 %). But climate change will also lead to higher <b>losses</b> (average <b>annual</b> <b>loss</b> in 2015 + 4 %, in 2050 + 11 %). In order to reduce the uncertainties surrounding the assumptions on the trend in capital stock and storm intensity, a sensitivity analysis was carried out, based on the assumptions from current studies on the future costs for tropical storms. climate change, tropical cyclones, natural catastrophes, insurance...|$|R
40|$|A set of indices was {{developed}} in order to classify the vulnerability of agricultural land to water and nitrogen losses (LOS), setting {{a basis for the}} integrated water resources management in agricultural systems. To calibrate the indices using multiple regression analysis, the simulation results of GLEAMS model for combinations of different soil properties, topography and climatic conditions of a reference field-crop were used as “observed values”. GLEAMS quantified (i) the <b>annual</b> <b>losses</b> of the percolated water beneath the root zone, (ii) the <b>annual</b> <b>losses</b> of the surface runoff, (iii) the <b>annual</b> <b>losses</b> of the nitrogen leaching beneath the root zone and (iv) the <b>annual</b> <b>losses</b> of nitrogen through the surface runoff, which were used to calibrate the following indices LOSW-P, LOSW-R, LOSN-PN and LOSN-RN, respectively. All the simulations to gain the LOS indices were carried out for the same reference field-crop, the same nitrogen fertilization and the same irrigation practice, in order to obtain the intrinsic vulnerability of agricultural land to water and nitrogen losses. The LOS indices were also combined to derive nitrogen concentrations in the percolated and in the runoff water. Finally, the connection of LOS indices with the groundwater was performed using an additional equation, which determines the minimum transit time of the percolated water to reach the groundwater table...|$|R
40|$|Objectives: Bone loss around dental {{implants}} {{is generally}} measured by monitoring changes in marginal bone level using radiographs. After {{the first year}} of implantation, an implant should have < 0. 2 [*]mm <b>annual</b> <b>loss</b> of marginal bone level to satisfy the criteria of success. However, the process of measuring marginal bone level on radiographs has a precision of 0. 2 [*]mm (or more) owing to variations in exposure geometry, exposure time and observer perception. Therefore, the value of the <b>annual</b> <b>loss</b> may vary considerably, especially when short intervals are considered. This study investigates how the success rate of dental implants depends on the way <b>annual</b> bone <b>loss</b> is calculated. Methods: Panoramic radiographs of 82 implant patients with an average follow-up of 10. 4 years were analysed. Marginal bone levels near the implants were indicated by one observer. The <b>annual</b> <b>loss</b> of marginal bone level was determined according to four different calculation methods. Results: The methods yielded success rates of 9 %, 45 %, 81 % and 89 %. Conclusions: The success rate of dental implants measured on radiographs greatly depends on the details of the calculation method. Without rigorous standardization, <b>annual</b> bone <b>loss</b> and implant success rate are not well defined...|$|R
50|$|Purely {{quantitative}} {{risk assessment}} is a mathematical calculation based on security metrics on the asset (system or application).For each risk scenario, taking into consideration the different risk factors a Single <b>loss</b> <b>expectancy</b> (SLE) is determined. Then, considering the probability of occurrence on a given period basis, for example the annual rate of occurrence (ARO), the Annualized <b>Loss</b> <b>Expectancy</b> is determined {{as the product of}} ARO X SLE.It is {{important to point out that}} the values of assets to be considered are those of all involved assets, not only the value of the directly affected resource.For example, if you consider the risk scenario of a Laptop theft threat, you should consider the value of the data (a related asset) contained in the computer and the reputation and liability of the company (other assets) deriving from the lost of availability and confidentiality of the data that could be involved.It is easy to understand that intangible assets (data, reputation, liability) can be worth much more than physical resources at risk (the laptop hardware in the example).Intangible asset value can be huge, but is not easy to evaluate: this can be a consideration against a pure quantitative approach.|$|R
