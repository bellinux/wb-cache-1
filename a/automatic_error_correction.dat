38|10000|Public
50|$|ARQ-M was {{developed}} to automatically deal with errors. The {{automatic repeat request system}} was invented by Hendrik van Duuren of the Netherlands in the 1940s and so it became known as the van Duuren <b>automatic</b> <b>error</b> <b>correction</b> system. The seven unit code used was called the van Duuren code.|$|E
5000|$|X.PC is a {{deprecated}} {{communications protocol}} developed by McDonnell-Douglas for connecting {{a personal computer}} to its Tymnet packet-switched public data telecommunications network. It is a subset of X.25, a CCITT standard for packet-switched networks. It is a full-duplex, [...] and error-correcting network protocol that supports up to 15 simultaneous channels. It maintains <b>automatic</b> <b>error</b> <b>correction</b> during any communications session between two or more computers.|$|E
50|$|In {{mathematical}} logic, Kalmár {{proved that}} certain classes of formulas {{of the first}} order predicate calculus were decidable. In 1936, he proved that the predicate calculus could be formulated using a single binary predicate, if the recursive definition of a term was sufficiently rich. (This result is commonly attributed to a 1954 paper of Quine's.) He discovered an alternative form of primitive recursive arithmetic, known as elementary recursive arithmetic, based on primitive functions that differ from the usual kind. He did his utmost to promote computers and computer science in Hungary. He wrote on theoretical computer science, including programming languages, <b>automatic</b> <b>error</b> <b>correction,</b> non-numerical applications of computers, and the connection between computer science and mathematical logic.|$|E
5000|$|... "End to End Error Code Correction" [...] (e2eECC) - all {{memory storage}} and {{internal}} transfers may {{be protected by}} <b>error</b> <b>correction</b> encoding having a Hamming Distance of 4, that distance providing <b>automatic</b> single bit-flip <b>error</b> <b>correction</b> and double bit-flip error detection.|$|R
40|$|The paper {{presents}} the results of the first large-scale human evaluation of <b>automatic</b> grammatical <b>error</b> <b>correction</b> (GEC) sys-tems. Twelve participating systems and the unchanged input of the CoNLL- 2014 shared task have been reassessed in a WMT-inspired human evaluation proce-dure. Methods introduced for the Work-shop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical <b>error</b> <b>correction</b> in terms of correlation with human judgment. ...|$|R
2500|$|... (Catt 1988) and (Catt and Hirst 1990) {{created a}} model of grammaticality based around a {{computer}} program developed for computer-assisted language instruction {{which was designed to}} perform <b>automatic</b> <b>error</b> diagnosis and <b>correction</b> of ungrammaticalities produced by second-language ...|$|R
40|$|Abstract. The design {{consists}} of STC 89 C 51 MCU, the GPS 15 -XL receivers, LED display, servo motors, voice timekeeping and other components. And {{it is the}} GPS control system for tower clock, with timekeeping, display, <b>automatic</b> <b>error</b> <b>correction</b> functions. The design captures accurate time information from satellites by GPS 15 XL-W receiver chip, and collects and deals with time information received from SCM. And then it adjusts the clock. The SCM system consists master clock, and drives the secondary clock through giving out pulse to a servo motor. The system can also achieve the <b>automatic</b> <b>error</b> <b>correction</b> after power on, to change inconvenience of the traditional correction, reduce the mechanical errors. And the tower clock accurate up to ± 1 µs. There is no cumulative error...|$|E
40|$|The {{bachelor}} thesis {{focuses on}} the creation of the automatic error cleansing software which clears the errors generated by the ordering system. Based on the error analysis (e. g. their causes and possibilities of their correction) the software enables the <b>automatic</b> <b>error</b> <b>correction</b> without the necessity of any manual action resulting in minimal delay of customers’ orders...|$|E
40|$|In {{this paper}} an {{extended}} scalability condition is proposed {{to achieve the}} ground-state stability for a class of multipartite quantum systems which may involve two-body interactions, and an explicit procedure to construct the dissipation control is presented. Moreover, we show that dissipation control {{can be used for}} <b>automatic</b> <b>error</b> <b>correction</b> in addition to stabilization. We demonstrate the stabilization and error correction of three-qubit repetition code states using dissipation control. Comment: 7 pages, 3 figure...|$|E
40|$|ISBN 978 - 1 - 937284 - 71 - 8 This volume {{contains}} papers {{describing the}} CoNLL- 2013 Shared Task and the participating systems. This year, we continue {{the tradition of}} the Conference on Computational Natural Language Learning (CoNLL) of having a high profile shared task in natural language processing, centered on <b>automatic</b> grammatical <b>error</b> <b>correction</b> of English essays. This task has gained popularity recently with th...|$|R
40|$|Abstract. A core {{problem in}} Model Driven Engineering is model {{consistency}} achievement: all models must satisfy relationships constraining them. Active consistency techniques monitor and control models edition for preventing inconsistencies, e. g., using <b>automatic</b> <b>errors</b> <b>correction.</b> The main problem {{of these approaches}} is that strict enforcement of consistency narrows the modeler’s possibilities for exploring conflicting or tradeoff solutions; this is just what temporaries inconsistencies enable. In this article, we propose a hybrid approach capitalizing on active consistency characteristics while allowing the user to edit inconsistent models in a managed mode: at any moment {{we are able to}} propose a sequence of modelling operations that, when executed, make the model consistent. The solution consists in defining a set of automatons capturing a sufficient part of the model state space for managing any inconsistent situation. We illustrate this approach on a consistency relationship implied by the application of a security design pattern impacting both class and sequence diagrams of a UML 2 model. ...|$|R
50|$|DMX512 {{does not}} include <b>automatic</b> <b>error</b> {{checking}} and <b>correction,</b> and so is not an appropriate control for hazardous applications, such as pyrotechnics or movement of theatrical rigging. False triggering {{may be caused by}} electromagnetic interference, static electricity discharges, improper cable termination, excessively long cables, or poor quality cables.|$|R
40|$|Site de la Conférence : [URL] audienceThe Discrete Controller Synthesis {{technique}} {{is used for}} automatic correction of design bugs in discrete event systems. A design method is proposed and illustrated extensively on a simple yet realistic example, modelling a serial to parallel converter. Simulation results show that this <b>automatic</b> <b>error</b> <b>correction</b> method can effectively mask errors without human intervention inside the design code. This automatic approach is more efficient and reliable than the traditional manual bug correction...|$|E
40|$|We {{present a}} {{data-driven}} approach which exploits word alignment {{in a large}} parallel corpus {{with the objective of}} identifying those verb- and adjective-preposition combinations which are difficult for L 2 language learners. This allows us, on the one hand, to provide language-specific ranked lists in order to help learners to focus on particularly challenging combinations given their native language (L 1). On the other hand, we provide extensive statistics on such combinations with the objective of facilitating <b>automatic</b> <b>error</b> <b>correction</b> for preposition use in learner texts. We evaluate these lists, first manually, and secondly automatically by applying our statistics to an error-correction task...|$|E
40|$|Abstract In {{the case}} of {{large-scale}} surveys, such as a Census, data may contain errors or missing values. An <b>automatic</b> <b>error</b> <b>correction</b> procedure is therefore needed. We focus {{on the problem of}} restoring the consistency of agricultural data concerning cultivation areas and number of livestock, and we propose here an approach to this balancing problem based on Optimization. Possible alternative models, either linear, quadratic or mixed integer, are presented. The mixed integer linear one has been preferred and used for the treatment of possibly unbalanced data records. Results on real-world Agricultural Census data show the effectiveness of the proposed approach...|$|E
40|$|A beam {{position}} monitor (BPM) upgrade at the KEK Accelerator Test Facility (ATF) {{damping ring}} {{has been accomplished}} in its first stage, carried out by a KEK/FNAL/SLAC collaboration {{under the umbrella of}} the global ILC R&D effort. The upgrade consists of a high resolution, high reproducibility read-out system, based on analog and digital donwconversion techniques, digital signal processing, and also tests a new <b>automatic</b> gain <b>error</b> <b>correction</b> schema. The technical concept and realization, as well as preliminary results of beam studies are presented...|$|R
40|$|We {{present a}} novel {{approach}} for <b>automatic</b> collocation <b>error</b> <b>correction</b> in learner English {{which is based on}} paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L 1 language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L 1 -induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms. ...|$|R
40|$|A beam {{position}} monitor (BPM) upgrade at the KEK Accelerator Test Facility (ATF) {{damping ring}} has been accomplished, {{carried out by}} a KEK/FNAL/SLAC collaboration {{under the umbrella of}} the global ILC R&D effort. The upgrade consists of a high resolution, high reproducibility read-out system, based on analog and processing, and also implements a new <b>automatic</b> gain <b>error</b> <b>correction</b> schema. The technical concept and realization as well as results of beam studies are presented. Comment: 3 pp. 10 th European Workshop on Beam Diagnostics and Instrumentation for Particle Accelerators DIPAC 2011, 16 - 18 May 2011. Hamburg, German...|$|R
40|$|In this article, self-adjusted Multi-sensor Information Fusion {{measuring}} method of electric energy based on neural networks has been thoroughly given. This paper studies {{the method of}} <b>automatic</b> <b>error</b> <b>correction</b> of electric power measurement also. The effective learning algorithm of the neural network based on gradient algorithm and Newton algorithm is combined with the LEA discriminant method. The {{results show that the}} method can improve the learning efficiency. The hardware model of adaptive real-time fast power measurement is constructed by using DSP device. The experimental results show that the adaptive power measurement model is better than the traditional power meter...|$|E
40|$|We {{describe}} a paradigm for combining manual and <b>automatic</b> <b>error</b> <b>correction</b> of noisy structured lexicographic data. Modifications {{to the structure}} and underlying text of the lexicographic data are expressed in a simple, interpreted programming language. Dictionary Manipulation Language (DML) commands identify nodes by unique identifiers, and manipulations are performed using simple commands such as create, move, set text, etc. Corrected lexicons are produced by applying sequences of DML commands to the source version of the lexicon. DML commands can be written manually to repair one-off errors or generated automatically to correct recurring problems. We discuss advantages of the paradigm for the task of editing digital bilingual dictionaries...|$|E
40|$|Abstract. Analysis {{of short}} message corpus is an {{important}} foundation for research of automatic short message processing technology. Based on large scale short message corpus, this paper firstly presents statistical data and performs analysis in detail on basic information of short message corpus and special language phenomena in it. The distributions of the corpus parameters and special language phenomena are also given out. The statistical results presented in the paper are meaningful for research of robust short message understanding and implementation of short message based manmachine dialog system and short message based machine translation system. And we also build an <b>automatic</b> <b>error</b> <b>correction</b> system on mobile phone to correct the misapplication of Chinese character in short messages. The preliminary results show that our method is effective...|$|E
30|$|The {{proposed}} {{model is}} {{compared with other}} language modeling approaches based on three different evaluation approaches. The first one is the model computation complexity presented in “Model complexity” section. The second one in “Models perplexity results” section is the proposed model perplexity enhancement and word error rate. The results are shown for English automatic speech recognition using the AMI meeting corpus and the model perplexity enhancement plus the entropy reduction result for the Online Open Source Arabic language corpuss experimental results. Finally, in “Arabic automatic spelling correction application” section, the Arabic <b>automatic</b> spelling <b>error</b> <b>correction</b> results are presented using the Qatar Arabic Language Bank (QALP) corpus.|$|R
40|$|An {{important}} {{problem in}} real-time DSP (digital signal-processing) systems with highly integrated components is {{the capability of}} <b>automatic</b> <b>error</b> detection and <b>correction.</b> The use of residue number arithmetic allows <b>error</b> detection and <b>correction</b> because of its unweighted nature. A single-error-correction procedure is proposed {{which is based on}} the use of redundant residue number systems (RRNS) and the base extension operation. The proposed method uses a small decision table and works in parallel mode; therefore it is suitable for high-speed VLSI circuit realization. A parallel architecture which realizes the method is also introduce...|$|R
40|$|In {{the case}} of {{large-scale}} surveys, such as a Census, data may contain errors or missing values. An <b>automatic</b> <b>error</b> detection and <b>correction</b> procedure is therefore needed. We propose here an approach to this problem based on Discrete Optimization. The treatment of each data record is converted into a mixed integer linear programming model and solved by means of state-of-the-art branch and cut procedures. Results on real-world Agricultural Census data show {{the effectiveness of the}} proposed procedure...|$|R
40|$|We {{developed}} a {{text entry method}} for touchscreen devices using a Graffiti-like alphabet combined with <b>automatic</b> <b>error</b> <b>correction.</b> The method is novel in that the user does not receive {{the results of the}} recognition process, except {{at the end of a}} phrase. The method is justified over soft keyboards in terms of a Frame Model of Visual Attention, which reveals both the presence and advantage of reduced visual attention. With less on-going feedback to monitor, there is a tendency for the user to enter gestures more quickly. Preliminary testing reveals reasonably quick text entry speeds (> 20 wpm) with low errors rates (< 5 %). Keywords Text entry; gestural input; Graffiti; unistrokes; automatic error correction; visual attention ACM Classification H. 5. 2. [Information Interfaces and Presentation]...|$|E
30|$|In {{the last}} few years, {{telecommunication}} industry has been growing explosively. The beginning of modern telecommunication was marked by the invention of telegraph in the eighteenth century. At that time, the message transmission was performed in a rudimentary way, in {{that there was no}} mechanism for <b>automatic</b> <b>error</b> <b>correction</b> and interference management. However, people had used it for about 100 years, until Thomas Edison invented telephone in 1876. The telephone network, known as public switched telephone network (PSTN), was rapidly established in Western counties. From that moment, a lot of novel concept and techniques, such as modulation, demodulation, and cellular concept, had been developed {{to improve the quality of}} telecommunication services. These inventions constitute the cornerstone of the first mobile telephone service which was introduced by America in 1946 [1, 2].|$|E
40|$|We {{describe}} a paradigm for combining manual and <b>automatic</b> <b>error</b> <b>correction</b> of noisy structured lexicographic data. Modifications {{to the structure}} and underlying text of the lexicographic data are expressed in a simple, interpreted programming language. Dictionary Manipulation Language (DML) commands identify nodes by unique identifiers, and manipulations are performed using simple commands such as create, move, set text, etc. Corrected lexicons are produced by applying sequences of DML commands to the source version of the lexicon. DML commands can be written manually to repair one-off errors or generated automatically to correct recurring problems. We discuss advantages of the paradigm for the task of editing digital bilingual dictionaries. Comment: 5 pages, 3 figures, 1 table; appeared in Proceedings of Electronic Lexicography in the 21 st Century (eLex), November 201...|$|E
5000|$|Forward <b>error</b> <b>correction</b> - used in high-data-rate UWB pulse systems - {{can provide}} channel {{performance}} approaching the Shannon limit.OFDM receivers typically fix most errors {{with a low}} density parity check code inner code followed by some other outer code that fixes the occasional errors (the [...] "error floor") that get past the LDPC correction inner code even at low bit-error rates.For example:The Reed-Solomon code with LDPC Coded Modulation (RS-LCM) adds a Reed-Solomon <b>error</b> <b>correction</b> outer code.The DVB-T2 standard and the DVB-C2 standard use a BCH code outer code to mop up residual errors after LDPC decoding.WiMedia over a UWB channel uses a Hybrid <b>automatic</b> repeat request:inner <b>error</b> <b>correction</b> using convolutional and Reed-Solomon coding,outer <b>error</b> <b>correction</b> using a frame check sequence that,when the check fails, triggers automatic repeat-request (ARQ).|$|R
40|$|Background: Endothelial {{examination}} of organ culture stored corneas is usually done manually and on several mosaic zones. Some banks use an image analyser that takes account {{of only one}} zone. This method is restricted by image quality, and may be inaccurate if endothelial cell density (ECD) within the mosaic is not homogeneous. The authors have developed an analyser that has tools for <b>automatic</b> <b>error</b> detection and <b>correction,</b> and can measure ECD and perform morphometry on multiple zones of three images of the endothelial mosaic...|$|R
40|$|The GCF upgrade {{task for}} NASA's Deep Space Network (DSN) is described. The DSN is a multimission {{telecommunications}} and radiometric data facility {{used to support}} space science, exploration, and applications by communicating with spacecraft and observing other radio sources. The GCF upgrade task is a six-year project designed to increase the data rate throughout capacity from 280 kbps to 2. 272 Mbps from each tracking area to JPL, and 224 kbps from JPL to each tracking area. This would be accomplished by using a selective <b>automatic</b> repeat request <b>error</b> <b>correction</b> scheme in all throughput data...|$|R
40|$|<b>Automatic</b> <b>error</b> <b>correction</b> {{systems for}} English as a Second Language(ESL) {{speakers}} often {{rely on the}} use of a confusion set to limit the choices of possible correction candidates. Typically, the confusion sets are either manually constructed or extracted from a corpus of manually corrected ESL writings. Both options require the involvement of English teachers. This paper proposes a method to automatically construct confusion sets for commonly used prepositions from non-ESL corpus without manual intervention. The proposed method simulates how ESL learners learn both the intensions and extensions of English words from standard English text. Our experimental results suggest that the automatically constructed confusion sets based on the similarities between the learned words ’ intensions is competitive with those directly learned from an ESL corpus containing about 150 K preposition usages...|$|E
40|$|Learner corpora {{consist of}} texts {{produced}} by non-native speakers. In {{addition to these}} texts, some learner corpora also contain error annotations, which can reveal common errors made by language learners, and provide training material for <b>automatic</b> <b>error</b> <b>correction.</b> We present a novel type of error-annotated learner corpus containing sequences of revised essay drafts written by non-native speakers of English. Sentences in these drafts are annotated with comments by language tutors, and are aligned to sentences in subsequent drafts. We describe the compilation process of our corpus, present its encoding in TEI XML, and report agreement levels on the error annotations. Further, we demonstrate {{the potential of the}} corpus to facilitate research on textual revision in L 2 writing, by conducting a case study on verb tenses using ANNIS, a corpus search and visualization platform. 1...|$|E
40|$|As {{the number}} of {{learners}} of English is constantly growing, <b>automatic</b> <b>error</b> <b>correction</b> of ESL learners ’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One {{of the main reasons}} why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction. ...|$|E
40|$|Abstract—In this paper, {{we present}} a novel {{debugging}} method for imperative software, featuring both <b>automatic</b> <b>error</b> localization and <b>correction.</b> The input of our method is an incorrect program and a corresponding specification, which can be given in form of assertions or as a reference implementation. We use symbolic execution for program analysis. This allows {{for a wide range}} of different trade-offs between resource requirements and accuracy of results. Our error localization method rests upon model-based diagnosis and SMT-solving. <b>Error</b> <b>correction</b> is done using a template-based approach which ensures that the computed repairs are readable. Our method can handle all sorts of incorrect expressions, not only under a single-fault assumption but also for multiple faults. Finally, we present experimental results, where an implementation for C programs is used to debug mutants of the TCAS case study of the Siemens suite. I...|$|R
40|$|Abstract. In {{conversational}} speech, {{irregularities in}} the speech such as overlaps and disruptions {{make it difficult to}} decide what is a sentence. Thus, despite very precise guidelines on how to label conversational speech with dialog acts (DA), labeling inconsistencies are likely to appear. In this work, we present various methods to detect labeling inconsistencies in the ICSI meeting corpus. We show that by automatically detecting and removing the inconsistent examples from the training data, we significantly improve the sentence segmentation accuracy. We then manually analyze 200 of noisy examples detected by the system and observe that only 13 % of them are labeling inconsitencies, while the rest are errors done by the classifier. The errors naturally cluster into 5 main classes for each of which we give hints on how the system can be improved to avoid these mistakes. Key words: <b>automatic</b> relabeling, <b>error</b> <b>correction,</b> boosting, sentence segmentation, noisy data. ...|$|R
40|$|In this work, {{we study}} {{parameter}} tuning towards the M^ 2 metric, the standard metric for <b>automatic</b> grammar <b>error</b> <b>correction</b> (GEC) tasks. After implementing M^ 2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning {{strategies for the}} CoNLL- 2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^ 2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL- 2014 test set by a large margin (46. 37 % M^ 2 over previously 41. 75 %, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49. 49 % M^ 2. Comment: Accepted for publication at EMNLP 201...|$|R
