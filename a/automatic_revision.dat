9|14|Public
5000|$|Among the {{innovations}} introduced by {{him in the}} company's products, worth mentioning are the global policies, which allow the cover of many risks with only one calculation of the premium, the preliminary estimate of the risk in the fire insurance, the <b>automatic</b> <b>revision</b> of the risks in the insurances for civil risks, the doubling of the life capital in case of accident, the guarantee of the payment of debts in case of death, the participation of policy holders to the profits of the life branch, the doubling {{in the event of}} death of the savings deposits at the Cassa di Risparmio delle Province Lombarde, [...] the group life insurance for the employees of the same company, the increase every three years of insured life capitals with a reduced rate, the insurance against the risk of machine assembly, the hail insurance for citrus grove.|$|E
40|$|For the <b>automatic</b> <b>revision</b> of {{homework}} assignments in Prolog programming courses, in general {{one has to}} rely on testing or on validating programs {{with respect to a}} specification. Here, we present a pragmatic and flexible method for the partial specification of program properties. Within the AT(P) system, partial specifications can be used for automatic analysis of student solutions to Prolog exercises, yielding automatically generated feedback to the student. AT(P) is integrated into the Virtual University system of the FernUniversität in Hagen. ...|$|E
40|$|International audienceWe are {{interested}} in this paper in the <b>automatic</b> <b>revision</b> of procedural knowledge for generalisation systems based on the agent paradigm. Our approach consists in analysing the system execution logs and extracting new knowledge from thee logs using machine learning techniques. The objective {{is not only to}} improve the system in terms of efficiency and of effectiveness but also to allow it to automatically adapt itself to various uses {{and to be able to}} evolve when adding new elements. A first experiment has been carried out on the generalisation of housing estates to validate the approach...|$|E
40|$|SNePS is {{a mature}} {{knowledge}} representation, reasoning, and acting {{system that has}} long contained a belief revision subsystem, called SNeBR. SNeBR is triggered when an explicit contradiction is introduced into the SNePS belief space, either because of a user's new assertion, or because of a user's query. SNeBR then makes the user decide what belief to remove from the belief space in order to restore consistency, although it provides information to help the user in making that decision. We have recently added <b>automatic</b> belief <b>revision</b> to SNeBR, by which, under certain circumstances, SNeBR decides by itself which belief to remove, and then informs the user of the decision and its consequences. We have used the well-known belief revision integrity constraints as a guide in designing <b>automatic</b> belief <b>revision,</b> taking into account, however, that SNePS's belief space is not deductively closed, {{and that it would}} be infeasible to form the deductive closure in order to decide w [...] ...|$|R
40|$|Abstract. The AgentSpeak agent-oriented {{programming}} language {{has recently been}} extended with various new features, such as speech-act based communication, internal belief additions, and support for reasoning with ontological knowledge, which imply the need for belief revision within an AgentSpeak agent. In this paper, we show how a polynomial-time belief-revision algorithm {{can be incorporated into}} the Jason AgentSpeak interpreter by making use of Jason’s language constructs and customisation features. This {{is one of the first}} attempts to include <b>automatic</b> belief <b>revision</b> within an interpreter for a practical agent {{programming language}}. ...|$|R
40|$|This paper {{presents}} {{a new approach}} to case-based reasoning and knowledge representation. The scheme is based on special assumptions about the data ontology: how the information structures are manifested in the observation data. It turns out that some of the old paradoxes can be attacked in this framework, like the shift from novice to expert. As an example, an industrial instrument selection database is modeled along the presented guidelines. The low-dimensional data model makes it easier to evaluate the consistency of the knowledge base, and it also facilitates <b>automatic</b> rule <b>revision...</b>|$|R
40|$|The {{automation}} {{of scientific}} method {{is a subject}} of increasing intellectual and practical interest, with potentially great benefits to science and society. This paper discusses four key challenges in this task and explains how they have been addressed within a functional genomics project known as the Robot Scientist. In so doing, it describes how abduction and induction have enabled the <b>automatic</b> <b>revision</b> of metabolic models through a synthesis of cutting edge artificial intelligence and laboratory robotics. Our aim is to summarise the progress which has already been made and to set out an agenda for further technological and social changes that are needed to turn the automation of science into a truly useful reality. ...|$|E
40|$|In this paper, {{we focus}} on the problem of <b>automatic</b> <b>revision</b> of legacy {{real-time}} programs. We consider this problem in two contexts. First, we investigate the problem of automated addition of properties expressed in Metric Temporal Logic (MTL) formulas to existing real-time programs modeled in Alur and Dill timed automata. Then, we consider transformation problems, where we design synthesis methods to add fault-tolerance to existing fault-intolerant realtime programs. While both problems have been addressed in the literature for untimed programs in theory and practice, there is much to be done for real-time programs. To this end, we concentrate on filling the gap between theory and practice of automated methods for synthesizing realtime programs by characterizing the class of real-time programs and properties, where program synthesis is practically feasible...|$|E
40|$|Abstract. This paper {{presents}} a nonmonotonic ILP approach for the <b>automatic</b> <b>revision</b> of metabolic networks through the logical analysis of experimental data. The method extends previous work in two respects: by suggesting revisions that involve both the addition and removal of information; and by suggesting revisions that involve combinations of gene functions, enzyme inhibitions, and metabolic reactions. Our proposal {{is based on}} a new declarative model of metabolism expressed in a nonmonotonic logic programming formalism. With respect to this model, a mixture of abductive and inductive inference is used to compute a set of minimal revisions needed to make a given network consistent with some observed data. In this way, we describe how a reasoning system called XHAIL was able to correctly revise a state-of-the-art metabolic pathway in the light of real-world experimental data acquired by an autonomous laboratory platform called the Robot Scientist. ...|$|E
40|$|Institutions {{provide an}} {{effective}} mechanism to govern agents in open distributed systems by specifying {{a set of}} norms (in terms of permissions, empowerments and obligations) regarding certain goals. However, when several institutions have to cooperate to govern the same entities simultaneously, norm conflicts are very likely to occur because institutions are typically designed independently with different goals. In this thesis, we aim to: · identify the different ways to combine institutions. · model those ways formally and computationally. · detect conflicts in different combinations automatically. · resolve those conflicts via <b>automatic</b> norm <b>revision</b> using an approach based on inductive learning...|$|R
40|$|This report {{presents}} an initial attempt {{to run a}} battle scenario with built in inconsistencies on the SNePS knowledge representation and reasoning system. This system alerts the user to the inconsistencies {{as soon as they}} are detected and offers an opportunity to correct the base hypotheses as well and, consequently, the beliefs that were derived from them. In this scenario, <b>automatic</b> belief <b>revision</b> is able to narrow down its culprit choices to one proposition each time it is called, so it removes those propositions. The system automatically stops believing any derived beliefs whose justifications rely on the removed beliefs. ...|$|R
40|$|We {{discuss some}} belief {{revision}} theories and {{the implementation of}} {{some of them in}} SNeBR, the belief revision subsystem of the SNePS knowledge representation and reasoning system. The following guidelines are important to belief revision: 1. minimize information removal 2. retract less-plausible beliefs before more-plausible ones. Alterations to SNeBR that provide users with information to help them follow these guidelines include the incorporation of sources into the knowledge base, as well as ordering of both sources and individual propositions, allowing partially ordered propositional epistemic entrenchment. We also developed an <b>automatic</b> belief <b>revision</b> option that performs the retraction of a belief if one culprit can be singled out based on the guidelines above. 1. Introduction Belief revision, or belief change, is the term used to describe any change in a knowledge base. The form of belief revision discussed in this paper is removal of propositions from an inconsistent know [...] ...|$|R
40|$|This paper {{examines}} {{data from}} different sensors regarding {{their potential for}} an <b>automatic</b> <b>revision</b> of topographic databases. The data which have to be updated are from the German national topographic cartographic database (ATKIS) and were captured in the scale of 1 : 25, 000. After a brief introduction into ATKIS the used approach is discussed. Results are shown on examples of data from several sensors: scanned analogue aerial photos, an airborne digital line scanner (DPA camera system), the Indian satellite IRS- 1 C, the MOMS- 2 P camera and from a laser scanning system as an additional information source. 1. ATKIS DATA 1. 1 The Basics ATKIS, the Authoritative Topographic-Cartographic Information System (Amtliches Topographisch -Kartographisches Informationssystem) {{is one of the}} common projects of the Federal Republic of Germany State Survey Working Committee (Arbeitsgemeinschaft der Vermessungsverwaltungen der Lnder der Bundesrepublik Deutschland (AdV)). This committee was establishe [...] ...|$|E
40|$|Current {{intelligent}} writing assistance tools (e. g. Grammarly, Turnitin, etc.) typically work by {{locating the}} problems of essays for users (grammar, spelling, argument, etc.) and providing possible solutions. These tools focus on providing feedback on a single draft, while ignoring feedback on an author’s changes between drafts (revision). This thesis argues that {{it is also important}} to provide feedback on authors’ revision, as such information can not only improve the quality of the writing but also improve the rewriting skill of the authors. Thus, it is desirable to build an intelligent assistant that focuses on providing feedback to revisions. This thesis presents work from two perspectives towards the building of such an assistant: 1) a study of the revision’s impact on writings, which includes the development of a sentence-level revision schema, the annotation of corpora based on the schema and data analysis on the created corpora; a prototype revision assistant was built to provide revision feedback based on the schema and a user study was conducted to investigate whether the assistant could influence the users’ rewriting behaviors. 2) the development of algorithms for <b>automatic</b> <b>revision</b> identification, which includes the automatic extraction of the revised content and the automatic classification of revision types; we first investigated the two problems separately in a pipeline manner and then explored a joint approach that solves the two problems at the same time...|$|E
40|$|This article {{describes}} {{a comprehensive approach}} to <b>automatic</b> theory <b>revision.</b> Given an imperfect theory, the approach combines explanation attempts for incorrectly classified examples {{in order to identify}} the failing portions of the theory. For each theory fault, correlated subsets of the examples are used to inductively generate a correction. Because the corrections are focused, they tend to preserve the structure of the original theory. Because the system starts with an approximate domain theory, in general fewer training examples are required to attain a given level of performance (classification accuracy) compared to a purely empirical system. The approach applies to classification systems employing a propositional Horn-clause theory. The system has been tested in a variety of application domains, and results are presented for problems in the domains of molecular biology and plant disease diagnosis. 1 INTRODUCTION 2 1 Introduction One of the most difficult problems in the develo [...] ...|$|R
40|$|Over {{the last}} years, Wikis have arisen as {{powerful}} tools for collaborative documentation on the Internet. The Encyclopaedia Wikipedia {{has become a}} reference, {{and the power of}} community editing in a Wiki allows for capture of knowledge from contributors all over the world. Use of a Wiki for Technical Documentation, along with hyper-links to other data sources such as a Product Lifecycle Management (PLM) system, provides a very effective collaboration tool as information can be easily feed into the system throughout the project life-cycle. In particular for software- and hardware projects with rapidly evolving documentation, the Wiki approach has proved to be successful. Certain Wiki implementations, such as TWiki, are project-oriented and include functionality such as <b>automatic</b> page <b>revisioning.</b> This paper addresses the use of TWiki to document hardware and software projects at CERN, from the requirements and brain-storming phase to end-product documentation. 2 examples are covered: large scale engineering for the ATLAS Experiment, and a network management software project...|$|R
40|$|Programming environments support {{revision}} control in several guises. Explicitly, {{revision control}} software manages the trees of revisions that grow as software is modified. Implicitly, editors retain past versions by automatically saving backup copies and by allowing users to undo commands. This paper describes an editor {{that offers a}} uniform solution to these problems by never destroying the old version of the file being edited. It represents files using a generalization of AVL trees called “AVL dags, ” which makes it affordable to automatically retain past versions of files. <b>Automatic</b> retention makes <b>revision</b> maintenance transparent to users. The editor also uses the same command language to edit both text and revision trees...|$|R
40|$|International audienceHumans {{frequently}} {{have to face}} complex problems. A classical approach to solve them is to search the solution {{by means of a}} trial and error method. This approach is often used with success by artificial systems. However, when facing highly complex problems, it becomes necessary to introduce control knowledge (heuristics) in order {{to limit the number of}} trials needed to find the optimal solution. Unfortunately, acquiring and maintaining such knowledge can be fastidious. In this paper, we propose an <b>automatic</b> knowledge <b>revision</b> approach for systems based on a trial and error method. Our approach allows to revise the knowledge off-line by means of experiments. It is based on the analysis of solved instances of the considered problem and on the exploration of the knowledge space. Indeed, we formulate the revision problem as a search problem: we search the knowledge set that maximises the performances of the system on a sample of problem instances. Our knowledge revision approach has been implemented for a real-world industrial application: automated cartographic generalisation, a complex task of the cartography domain. In this implementation, we demonstrate that our approach improves the quality of the knowledge and thus the performance of the system...|$|R
40|$|This paper {{presents}} {{a framework for}} controlling the evolution of complex software systems concurrently developed by teams of software engineers. A general technique for fine-grained revision control of hierarchically structured information, such as programs and documents, is described and evaluated. All levels in the hierarchy are revision controlled, leaves as well as branch nodes. The technique supports sharing of unchanged nodes among <b>revisions,</b> <b>automatic</b> change propagation, and change-oriented representation of differences. Its use in a software development environment is presented, facilitating optimistic check-out of revisions and alternatives, check-in with incremental merge support, visualization of change propagation, and an integrated flexible diff-ing technique providing group awareness for team members. KEYWORDS Software development, version and configuration control, incremental merge, teamware, CSCW, group awareness 1 INTRODUCTION Despite the fact that software systems [...] ...|$|R
40|$|Automating the {{generalisation}} process, a {{major issue}} for national mapping agencies, is extremely complex. Several works have proposed {{to deal with this}} complexity using a trial and error strategy. The performance of systems based on such a strategy is directly dependent {{on the quality of the}} control knowledge (i. e. heuristics) used to guide the trials. Unfortunately, most of the time, the definition and updation of knowledge is a fastidious task. In this context, <b>automatic</b> knowledge <b>revision</b> can not only improve the performance of the generalisation, but also allow it to automatically adapt to various usages and evolve when new elements are introduced. In this article, an offline knowledge revision approach is proposed, based on a logging of the system and on the analysis of outcoming logs. This approach is dedicated to the revision of control knowledge expressed by production rules. We have implemented and tested this approach for the automated generalisation of groups of buildings within a generalisation model called AGENT, from initial data that reference a scale of approximately 1 : 15, 000 compared with the target map's scale of 1 : 50, 000. The results show that our approach improves the quality of the control knowledge and thus the performance of the system. Moreover, the approach proposed is generic and can be applied to other systems based on a trial and error strategy, dedicated to generalisation or not...|$|R
40|$|ABSTRACT The {{complexity}} of academic activity, the eminence {{of the freedom}} to research and the reverence due {{to the authority of}} knowledge require full autonomy for the university departments. Above that, although dependent on central decisions on organization and resources availability, it is the action of Faculty and students within the departments that determines the results of the educational process. For this reason, it is at departmental level that must be driven the main effort to improve quality in higher education and the most important virtue of any academic evaluation system is its power to call attention of the departments to the importance of quality management and providing them information that may drive their efforts in this direction. This paper presents an approach to access the productivity of academic departments by combining model building and parameters estimation, in an iterative manner. Validation and outliers selection techniques can then be applied to feed <b>automatic</b> model <b>revision</b> procedures that will preserve continuity in the evaluation process. We start with a simple linear approximation to the ideal production function of each department with coefficients depending on quality of work explanatory variables. We follow the evolution of these coefficients estimates along time, as local observations become available. Departments with extreme estimated coefficients are taken as object of study aiming to detect useful innovation or any other cause of its different observed behavior. An iterative algorithm for the estimation of the parameters of the dynamic hierarchical model thus generated is described here. An approach to combine the estimates of productivity with respect to each particular output in a global measurement is also proposed. Examples of different ways to model the academic activity are, finally, presented...|$|R

