8|13|Public
40|$|Photogrammetry and {{computer}} vision are coherent in research content, {{but since they}} are come from different subject so there are great differences in researching method and application. The paper reviews the space resection, relative orientation and absolute orientation which is the kernel content of Photogrammetry 3 D space computational, and comparisons it with the computer vision. As while as, explain its strongpoint and its shortage and given {{an example of the}} combine of the two methods. It can be seen from this comparing that the most theory and algorithm of computer vision are great value to digital photogrammetry, and the same that photogrammetry <b>adjustment</b> <b>computation</b> method has also important meaning to improve the measure precision of computer vision. 1...|$|E
40|$|AbstractIn this study, {{a classic}} survey <b>adjustment</b> <b>computation</b> method {{was used for}} data {{obtained}} in the Inner Mongolia and Ningxia gravimetric networks between September 2013 and April 2015 so as to investigate the variation of gravity before the Alxa Zuoqi M 5. 8 earthquake. The relationship between gravity variation and the Alxa Zuoqi M 5. 8 earthquake was analyzed. The results showed that: (1) the severe variation in gravity field at the test sites before the Alxa Zuoqi M 5. 8 earthquake, {{as well as the}} subsequent accelerated rising, might be an earthquake precursor; (2) the Alxa Zuoqi M 5. 8 earthquake occurred at the turning point where the high-gravity gradient zone changed from the NE direction to NW...|$|E
40|$|Geodetic {{deformation}} {{surveys have}} been used in many tasks such as the monitoring of engineering structures, land subsidence and other related applications. As results, the analysis of deformation has become a subject of intensive studies of many professional groups which besides surveyors and geodesist include geotechnical and structural engineers, as well as geophysists and geologists. One of the main tasks in deformation analysis is the handling of least squares adjustment of the two-epochs of geodetic observations data. Another important aspect in such analysis is on the question of selecting a proper control point as datum in the <b>adjustment</b> <b>computation.</b> The adjustment results were then further analyzed to obtain a clear picture of deformation. One way of depicting the deformation is by creating a graphical trend analysis. This report describes the effort of developing Fredy 04, a software for analyzing the deformation using geodetic methods. The computational methodology adopted in the software is based on the methodology known as the Fredericton Approach. The working procedures of the software were then implemented to analyze the deformation of a simulation deformation network which consist of geodetic observations in the form angle and distance measurements...|$|E
50|$|After the arrival, {{more than}} 50 {{scientific}} papers of him were {{published by the}} IGM, Argentine Association of Geophysicists and Geodesists, Pan-American Institute for Geography and History, and Direction for Geodesy of the Province of Buenos Aires, besides many other papers published in his mother country. Horvat's research interests chiefly deal with the <b>adjustment</b> <b>computations</b> of geodetic networks, general as well as geodetic cartography, ellipsoid geometry, {{and the development of}} feasible methods for automatic computation. Horvat is noted as the first person to use hyperbolic functions to ease and simplify problems of numeric calculations in the field of geodetic cartography.|$|R
30|$|Let us finally {{mention the}} {{important}} issue of netting of outstanding contracts between counterparties, which means that, in principle, every new deal should be valued not in isolation, {{but rather as}} a new component added to the portfolio of existing contracts. Needless to say, this issue is highly challenging, in both theory ad practice, and thus it is left for future work. Last but not least, it should be acknowledged that the valuation of derivatives based on arbitrage-free replication (or superhedging) should not be seen as the most realistic pricing approach, but rather a mathematical idealization of a much more complex situation, and thus other pricing paradigms should also be examined. The interested reader is referred to Kenyon and Green (2013) for a discussion of a regulatory-compliant derivatives pricing and to Albanese and Crépey (2017) for a novel balance-sheet approach to XVA with the special emphasis on KVA (capital value <b>adjustment)</b> <b>computations.</b>|$|R
40|$|Abstract. This paper {{presents}} {{analysis of}} the influence of the virtualization mechanism of IBM pSeries servers on dynamic resource allocation in AIX 5 L operating system. It raises issues of economic use of resources and distribution of processing power. Some innovative solutions are proposed as methods for dynamic resource allocation. These are: micro-partitioning and partition load manager utility. Some results of experiments are presented. The goal of these experiments was to estimate the influence of selected AIX 5 L operating system <b>adjustments</b> on <b>computation</b> efficiency...|$|R
40|$|The diploma thesis {{presents}} {{the process of}} homogenization of positional accuracy of land cadastral index map, where the so called membrane method {{as one of the}} possible approaches to achieve the goal of homogenization has been used. In Slovenia, the cadastral index maps are characterized by considerable inhomogeneity as a result of different technologies used for land surveying and maintenance of cadastral maps during several decades. In this thesis, the software solutions of Slovenian providers are presented, which are most frequently used in the land cadastre, and the software solution from the German providers is introduced, which was used in this research for homogenization of positional accuracy of cadastral index map. The whole process is presented starting with data acquisition, data processing, <b>adjustment</b> <b>computation</b> to the analysis of results, we obtained with Systra software. The main purpose was to determine the optimal number of observations of additional land cadastral points, which determine the land plot border, to achieve improvement of positional and geometrical accuracy of the cadastral index map in the study area of cadastral community Bočna. The particularity of the study area is that there was a new cadastral surveying conducted, which included also the land plots’ rearrangements, and this has definitely bought some specialities when dealing with the geometrical and positional characteristics of the cadastral index map...|$|E
40|$|A lunar global control network {{provides}} {{geodetic datum}} and control points for mapping {{of the lunar}} surface. The widely used Unified Lunar Control Network 2005 (ULCN 2005) was built based on a combined photogrammetric solution of Clementine images acquired in 1994 and earlier photographic data. In this research, we propose an initiative for construction of a new-generation lunar global control network using multi-mission data newly acquired in the 21 st century, which have much better resolution and precision than the old data acquired in the last century. The new control network {{will be based on}} a combined photogrammetric solution of an extended global image and laser altimetry network. The five lunar laser ranging retro-reflectors, which can be identified in LROC NAC images and have cm level 3 D position accuracy, will be used as absolute control points in the least squares photogrammetric adjustment. Recently, a new radio total phase ranging method has been developed and used for high-precision positioning of Chang’e- 3 lander; this shall offer a new absolute control point. Systematic methods and key techniques will be developed or enhanced, including rigorous and generic geometric modeling of orbital images, multi-scale feature extraction and matching among heterogeneous multi-mission remote sensing data, optimal selection of images at areas of multiple image coverages, and large-scale <b>adjustment</b> <b>computation,</b> etc. Based on the high-resolution new datasets and developed new techniques, the new generation of global control network is expected to have much higher accuracy and point density than the ULCN 2005...|$|E
40|$|Cadastral {{surveying}} in Korea {{is based}} on the cadastral triangulation points which are originated from the old surveying network established in early years of 20 th century. The datum is different from New Korea Geodetic Datum 2000 (NKGD 2000) which employs ITRF 97 and GRS 80 ellipsoid. In order to improve quality of old cadastral surveying network, which will not be accurate enough to meet modern needs, the network is investigated by GPS measurements and trilateration adjustment is carried out. In this process, coordinate transformation between the old and NKGD 2000 datum, and local geoid model is used to find out accurate control points. The <b>adjustment</b> <b>computation</b> by using the GPS observations on 32 triangulation points distributed over Gyunggi province (100 km x 100 km) has shown the control points employed in cadastral surveying has coordinate error up to one meter or more. The computation also has shown the estimated coordinate error of the adjusted points is within 5 cm, highly accurate as well as highly consistent. In Korea, more than 90 % of cadastral surveying has been carried out by graphic method. And all the map sheets have been digitalized by the year 2003. Because the graphic map was digitalized independently of control points and of adjoining maps, several problems arise in surveying where the adjoining two maps are improperly connected. To solve the problem, we are going to update or re-establish the old control networks, and develop a method tying parcel boundary points to nearby control points. TS 3 – GNSS...|$|E
40|$|A puzzling, {{slightly}} submerged, single tidal notch {{has been}} reported from the carbonate coasts of the northeastern Adriatic Sea. This paper attempts to explain the origin and the recent evolution of this marine erosion feature {{and the reasons for}} its uniqueness in the late Holocene. After reviewing how tidal notches are usually formed, a comparison of recent measurements of bioerosion rates carried out in the area show that when the contribution of dissolution processes can be neglected, bioerosion rates of the deepening of tidal-notch profiles appear to be very low (often < 0. 1 mm/y) along the coasts of Istria, in contrast to higher rates (between 0. 2 and 1. 0 mm/y) generally reported in other Mediterranean areas. Such a low rate of bioerosion implies a long period favourable to tidal-notch development. Several glacial isostatic <b>adjustment</b> <b>computations</b> show that relative sea level changes in the area during the last few millennia may correspond to a period of equilibrium between the regional tectonic subsidence and hydro-isostatic emergence during which relative sea level changes were limited, permitting development of the tidal notch observed. The submergence of the notch is consistent with a coseismic subsidence in late Roman time. After this, a new tidal notch could not form at the present sea level because of the limited amount of local bioerosion and the relatively large rate of sea level rise...|$|R
50|$|In {{marking-to-market}} a derivatives account, at pre-determined periodic intervals, each counterparty exchanges {{the change}} in the market value of their account in cash. For Over-The-Counter (OTC) derivatives, when one counterparty defaults, the sequence of events that follows is governed by an ISDA contract. When using models to compute the ongoing exposure, FAS 157 requires that the entity consider the default risk ("nonperformance risk") of the counterparty and make a necessary <b>adjustment</b> to its <b>computations.</b>|$|R
40|$|Deformation {{analysis}} {{is one of}} the most challenging tasks in engineering survey. The analysis of deformation is essential to monitor the movement of established network or of object under study for many purposes such as tectonic movement or monitoring of large engineering structures (e. g. dams, bridges and etc). This study will concentrate on the development of computational software for the purpose of 2 D geodetic network deformation analysis. The tasks in the study are divided into two parts – the least squares <b>adjustment</b> (LSA) <b>computation</b> and the analysis of deformation. The LSA is initially done by employing minimum constraint technique. The analysis of deformation is been performed using robust technique known as Iterative Weighted Similarity Transformation (IWST). The software developed in this study was implemented for deformation analysis by utilizing two epochs of measurement data performed on a deformation network. Results obtained from this computation show a very good agreement with results of a previous exercise computed using the same data set...|$|R
40|$|Our work {{is driven}} by a class of {{practical}} problems of sequential decision making {{in the context of}} electric power generation under uncertainties. These problems are usually treated as receding horizon deterministic optimization problems, and/or as scenario-based stochastic programs. Stochastic programming allows to compute a first stage decision that is hedged against the possible futures and [...] if a possibility of recourse exists [...] this decision can then be particularized to possible future scenarios thanks to the information gathered until the recourse opportunity. Although many decomposition techniques exist, stochastic programming is currently not tractable in the context of day-ahead electric power generation and furthermore does not provide an explicit recourse strategy. The latter observation also makes this approach cumbersome when one wants to evaluate its value on independent scenarios. We propose a supervised learning methodology to learn an explicit recourse strategy for a given generation schedule, from optimal adjustments of the system under simulated perturbed conditions. This methodology may thus be complementary to a stochastic programming based approach. With respect to a receding horizon optimization, it has the advantages of transferring the heavy computation offline, while providing the ability to quickly infer decisions during online exploitation of the generation system. Furthermore the learned strategy can be validated offline on an independent set of scenarios. On a realistic instance of the intra-day electricity generation rescheduling problem, we explain how to generate disturbance scenarios, how to compute adjusted schedules, how to formulate the supervised learning problem to obtain a recourse strategy, how to restore feasibility of the predicted adjustments and how to evaluate the recourse strategy on independent scenarios. We analyze different settings, namely either to predict the detailed adjustment of all the generation units, or to predict more qualitative variables that allow to speed up the <b>adjustment</b> <b>computation</b> procedure by facilitating the ``classical'' optimization problem. Our approach is intrinsically scalable to large-scale generation management problems, and may in principle handle all kinds of uncertainties and practical constraints. Our results show the feasibility of the approach and are also promising in terms of economic efficiency of the resulting strategies. The solutions of the optimization problem of generation (re) scheduling must satisfy many constraints. However, a classical learning algorithm that is (by nature) unaware of the constraints the data is subject to may indeed successfully capture the sensitivity of the solution to the model parameters. This has nevertheless raised our attention on one particular aspect of the relation between machine learning algorithms and optimization algorithms. When we apply a supervised learning algorithm to search in a hypothesis space based on data that satisfies a known set of constraints, can we guarantee that the hypothesis that we select will make predictions that satisfy the constraints? Can we at least benefit from our knowledge of the constraints to eliminate some hypotheses while learning and thus hope that the selected hypothesis has a better generalization error? In the second part of this thesis, where we try to answer these questions, we propose a generic extension of tree-based ensemble methods that allows incorporating incomplete data but also prior knowledge about the problem. The framework is based on a convex optimization problem allowing to regularize a tree-based ensemble model by adjusting either (or both) the labels attached to the leaves of an ensemble of regression trees or the outputs of the observations of the training sample. It allows to incorporate weak additional information in the form of partial information about output labels (like in censored data or semi-supervised learning) or [...] more generally [...] to cope with observations of varying degree of precision, or strong priors in the form of structural knowledge about the sought model. In addition to enhancing the precision by exploiting information that cannot be used by classical supervised learning algorithms, the proposed approach may be used to produce models which naturally comply with feasibility constraints that must be satisfied in many practical decision making problems, especially in contexts where the output space is of high-dimension and/or structured by invariances, symmetries and other kinds of constraints...|$|E
40|$|We {{depart from}} the usual methods for pricing {{contracts}} with the counterparty credit risk found {{in most of the}} existing literature. In effect, typically, these models do not account for either systemic effects or at-first-default contagion and postulate that the contract value at default equals either the risk-free value or the pre-default value. We propose instead a fairly general framework, which allows us to perform effective Credit Value <b>Adjustment</b> (CVA) <b>computations</b> for a contract with bilateral counterparty risk in the presence of systemic and wrong or right way risks. Our general methodology focuses on the role of alternative settlement clauses, but it is also aimed to cover various features of margin agreements. A comparative analysis of numerical results reported in the final section supports our initial conjecture that alternative specifications of settlement values have a non-negligible impact on the CVA computation for contracts with bilateral counterparty risk. This emphasizes the practical importance of more sophisticated models that are capable of fully reflecting the actual features of financial contracts, as well as the influence of the market environment...|$|R
40|$|The {{generalization}} of simple correspondence analysis, for two categorical variables, to multiple correspondence analysis {{where they may}} be three or more variables, is not straighforward, both from a mathematical and computational point of view. In this paper we detail the exact computational steps involved in performing a multiple correspondence analysis, including the special aspects of adjusting the principal inertias to correct the percentages of inertia, supplementary points and subset analysis. Furthermore, we give the algorithm for joint correspondence analysis where the cross-tabulations of all unique pairs of variables are analysed jointly. The code in the R language for {{every step of the}} computations is given, as well as the results of each <b>computation.</b> <b>Adjustment</b> of principal inertias, Burt matrix, correspondence analysis, multiple correspondence analysis, R language, singular value decomposition, subset analysis...|$|R
40|$|Image feature {{points are}} the basis for {{numerous}} computer vision tasks, such as pose estimation or object detection. State of the art algorithms detect features that are invariant to scale and orientation changes. While feature detectors and descriptors have been widely studied in terms of stability and repeatability, their localisation error has often been assumed to be uniform and insignificant. We argue that this assumption does not hold for scale-invariant feature detectors and demonstrate that the detection of features at different image scales actually {{has an influence on}} the localisation accuracy. A general framework to determine the uncertainty of multi-scale image features is introduced. This uncertainty is represented via anisotropic covariances with varying orientation and magnitude. We apply our framework to the well-known SIFT and SURF algorithms, detail its implementation and make it available 1. Finally, the usefulness of such covariance estimates for bundle <b>adjustment</b> and homography <b>computation</b> is illustrated. ...|$|R
40|$|This paper {{analyzes}} {{the relationship between}} the U. S. dollar and the U. S. current account, dealing with issues of sustainability and the mechanics of current account adjustment. The analysis differs from other work in several respects. First, it emphasizes the dynamics of current account <b>adjustment,</b> going beyond <b>computations</b> of the real depreciation required to achieve sustainability. The analysis shows that, even if foreigners’ net demand for U. S. assets continues to increase significantly, the current account deficit is likely to fall steeply in the not too distant future. Second, the paper uses international evidence to explore the likelihood of an abrupt decline in capital flows into the United States. Third, it {{analyzes the}} international evidence on current account reversals, to investigate the potential consequences of a sudden stop of capital inflows. This analysis suggests that adjustment of the U. S. external accounts is likely to result in a significant reduction in growth. macroeconomics, U. S. Current Account Deficit, Sustainable, Adjustment...|$|R
40|$|In {{this paper}} I analyze the {{relationship}} between the U. S. dollar and the U. S. current account. I deal with issues of sustainability, and I discuss the mechanics of current account adjustment. The analysis presented in this paper differs from other work in several respects: First, I emphasis the dynamics of the current account <b>adjustment,</b> going beyond <b>computations</b> of the "required" real depreciation of the dollar to achieve sustainability. I show that even if foreigners' (net) demand for U. S. assets continues to increase significantly, the current account deficit is likely to experience a large decline in the (not too distant) future. Second, I rely on international evidence to explore the likelihood of an abrupt decline in capital flows into the U. S. And third, I analyze the international evidence on current account reversals, to investigate the potential consequences of a (possible) sudden stop of capital flows into the U. S. This analysis suggests that the future adjustment of the U. S. external accounts is likely to result in a significant reduction in growth. ...|$|R
40|$|In the Bayesian {{framework}} {{a standard}} approach to model criticism {{is to compare}} some function of the observed data to a reference predictive distribution. The result of the comparison can be summarized {{in the form of}} a p-value, and computation of some kinds of Bayesian predictive p-values can be challenging. The use of regression <b>adjustment</b> approximate Bayesian <b>computation</b> (ABC) methods is explored for this task. Two problems are considered. The first is approximation of distributions of prior predictive p-values for the purpose of choosing weakly informative priors in the case where the model checking statistic is expensive to compute. Here the computation is difficult because of the need to repeatedly sample from a prior predictive distribution for different values of a prior hyperparameter. The second problem considered is the calibration of posterior predictive p-values so that they are uniformly distributed under some reference distribution for the data. Computation is difficult because the calibration process requires repeated approximation of the posterior for different data sets under the reference distribution. In both these problems we argue that high accuracy in the computations is not required, which makes fast approximations such as regression adjustment ABC very useful. We illustrate our methods with several examples...|$|R
40|$|The {{objective}} {{of this research was}} development and testing of a method for estimating cattle carrying capacities. A series of studies were conducted in developing this method. Range site and vegetation production data were grouped by topographic position and multiple linear regression equations were calculated for predicting vegetation production as a site deviated from the average case of a given range site. Overstory-understory relationships from the literature were adapted into overstory canopy cover classes for predicting understory production and tested on a variety of range sites. Use of these classes produced understory biomass estimates within 13 % of measured biomass. Range condition class and understory aspect dominance by forage vs. non-forage species were investigated as estimators of forage value of the understory vegetation. Both were significantly related to amount of forage in the understory. However, understory aspect proved to be a better estimator when individual comparisons were examined. The previous findings, along with Soil Conservation Service range site guides, were used to calculate resource value ratings. Adjustment factors to be applied to the resource value ratings were calculated, using data from the literature, to account for the effects of slope and distance from water on forage utilization by cattle. These resource value ratings and adjustment factors form the basis of the carrying capacity estimation method. Pastures identified as properly utilized were used in testing the method developed. Pastures were mapped for range site, vegetation, slope and water location. Maps were converted to digital form and analyzed using the Map Analysis Package (MAP) computer program (Tomlin, 1975). Construction of a final range site-vegetation-slope-distance from water map, assigning of resource value ratings and <b>adjustment</b> factors, and <b>computation</b> of final carrying capacity estimates were accomplished using MAP. Carrying capacity estimates from the developed method were well correlated to estimates from ocular reconnaissance and area allowable use methods, r =. 87 and. 97, respectively, and with the actual use (perceived proper use), r =. 95. These estimates were accomplished without intensive field sampling. The only information required was range site designation, amount of overstory canopy cover, understory aspect class, percent slope and water location...|$|R

