3|33|Public
50|$|The PDP-6 was infamous {{because of}} the 6205 board, a large (11 × 9 inches) board which {{contained}} 1 bit of <b>arithmetic</b> <b>register</b> (AR), memory buffer (MB), and multiplier-quotient register (MQ) (thus there were 36 of these). It had 88 transistors, a two-sided PC etch, two 18-pin and two 22-pin connectors (two {{on each side of}} the module). Because of all these connectors, swapping this module was a major undertaking, and the mechanical coupling made it highly likely that fixing one fault would cause another. There was also a great fear of powering off a PDP-6, since it would generally result in at least one 6205 board failing.The experience with the 6205 led the designers of the first models of PDP-10, the KA10 and KI10, to use only small boards. It was not until the KL10 that large boards were used again.|$|E
5000|$|The 2200 Series {{architecture}} provides many registers. Base registers logically {{contain a}} virtual address that {{points to a}} word in a code or data bank (segment). They may point {{to the beginning of}} the bank or to any word within the bank. Index registers are used by instructions to modify the offset of the specified or assumed base register. Simple arithmetic (add, subtract) may be performed on all index registers. In addition, index registers consist of a lower offset portion and an upper increment portion. An instruction may both use the offset value in an index register as part of an address and specify that the increment is to be added to the offset. This allows loops to be accomplished with fewer instructions as incrementing the index by the step size can be accomplished without a separate instruction. Arithmetic registers allow the full set of computational instructions including all floating point operations. Some of those instructions work on adjacent pairs of registers to perform double-precision operations. There are no even-odd constraints. Any two registers may be used as a double-precision value. Four of the arithmetic registers are also index registers (the sets overlap - index register X12 is <b>arithmetic</b> <b>register</b> A0). This allows the full range of calculations to be performed on indexes without having to move the results. The rest of the registers, known as R registers, are used as fast temporary storage and for certain special functions. R1 holds the repeat count for those instructions that may be repeated (block transfer, execute repeated, etc.). R2 holds a bit mask for a few instructions that perform a bitwise logical operation in addition to some other functions (e.g., masked load upper) ...|$|E
40|$|Korea-Japan Joint VLBI Correlator (KJJVC) {{is being}} {{developed}} by collaborating KASI (Korea Astronomy and Space Science Institute), Korea, and NAOJ(National Observatory of Japan), Japan. In early 2010, KJJVC will work in normal operation. In this study, we developed the software correlator {{which is based on}} VCS (VLBI Correlation Subsystem) hardware specification as the core component of KJJVC. The main specification of software correlator is 8 Gbps, 8192 output channels, and 262, 144 -points FFT (Fast Fourier Transform) function same as VCS. And the functional algorithm which is same as specification of VCS and <b>arithmetic</b> <b>register</b> are adopted in this software correlator. To verify the performance of developed software correlator, the correlation experiments were carried out using the spectral line and continuum sources which were observed by VERA (VLBI Exploration of Radio Astrometry), NAOJ. And the experimental results were compared to the output of Mitaka FX correlator by referring spectrum shape, phase rate, and fringe detection and so on. Through the experimental results, we confirmed that the correlation results of software correlator are the same as Mitaka FX correlator and verified the effectiveness of it. In future, we expect that the developed software correlator will be the possible software correlator of KVN (Korean VLBI Network) with KJJVC by introducing the correlation post-processing and modifying the user interface as like GUI (Graphic User Interface) ...|$|E
5000|$|... the English Electric KDF9 machine. First {{delivered}} in 1964, the KDF9 had a 19-deep pushdown stack of <b>arithmetic</b> <b>registers,</b> and a 17-deep stack for subroutine return addresses ...|$|R
50|$|The most {{interesting}} {{feature of the}} AN/UYK-1 instruction set was that the machine-language instructions had two operators that could simultaneously manipulate the <b>arithmetic</b> <b>registers,</b> for example complementing the contents of one register while loading or storing another. It also {{may have been the}} first computer that implemented a single-cycle indirect addressing ability.|$|R
5000|$|... "The <b>arithmetic</b> <b>registers</b> of the ALWAC {{computer}} are circulating {{loops of}} information one word in length. The information is changed when the magnetic writing head is switched to {{an input device}} or to some reading head {{other than the one}} which makes up the loop with it. In addition to four one-word registers, the drum has four 32-word channels of working storage from which instructions are picked up to be carried out and data picked up to be operated on in the <b>arithmetic</b> <b>registers.</b> The working storage, through what is essentially a loop arrangement, has faster access than the remainder of the drum, the main storage."Main storage consists of 128 channels, 32 words in each. Any main-storage channel may be copied in a block to one of the working-storage channels, and a working-storage channel may be copied in a block to any main channel." ...|$|R
50|$|<b>Arithmetic</b> <b>registers</b> were {{provided}} for both {{halves of the}} data word and included an accumulator, an A register that held the data value retrieved from memory, and a B register that held the least significant bits of a multiplication, the magnitude of a division, as well as shifted bits. There was also a program counter, four index registers, and a 16-bit real-time clock register which was incremented 32 times a second. Trigonometric sine and cosine functions used 1.4 degree precision (256 values) via look-up tables.|$|R
5000|$|RISC — <b>arithmetic</b> {{instructions}} use <b>registers</b> only, so explicit 2-operand load/store {{instructions are}} needed: ...|$|R
5000|$|A key {{challenge}} for full virtualization is the interception and simulation of privileged operations, such as I/O instructions. The effects of every operation performed {{within a given}} virtual machine must be kept within that virtual machine [...] - [...] virtual operations cannot be allowed to alter the state of any other virtual machine, the control program, or the hardware. Some machine instructions can be executed directly by the hardware, since their effects are entirely contained within the elements managed by the control program, such as memory locations and <b>arithmetic</b> <b>registers.</b> But other instructions that would [...] "pierce the virtual machine" [...] cannot be allowed to execute directly; they must instead be trapped and simulated. Such instructions either access or affect state information that is outside the virtual machine.|$|R
50|$|The CPU {{architecture}} featured three register sets. The Nest was a 16-deep pushdown {{stack of}} <b>arithmetic</b> <b>registers,</b> The SJNS (Subroutine Jump Nesting Store) was a similar stack of return addresses. The Q Store {{was a set}} of 16 index registers, each of 48 bits divided into Counter (C), Increment (I) and Modifier (M) parts of 16 bits each. Flags on a memory-reference instruction specified whether the address should be modified by the M part of a Q Store, and, if so, whether the C part should be decremented by 1 and the M part incremented by {{the contents of the}} I part. This made the coding of counting loops very efficient. Three additional Nest levels and one additional SJNS level were reserved to Director, the Operating System, allowing short-path interrupts to be handled without explicit register saving and restoring. As a result the interrupt overhead was only 3 clock cycles.|$|R
40|$|The {{design of}} an {{asynchronous}} clone of a MIPS R 3000 microprocessor is presented. In 0 : 6 m CMOS, we expect performance close to 280 MIPS, for a power consumption of 7 W. The paper describes {{the structure of}} a high-performance asynchronous pipeline, in particular precise exceptions, pipelined caches, <b>arithmetic,</b> and <b>registers,</b> and the circuit techniques developed to achieve high throughput. ...|$|R
5000|$|Microelectronics Integrated Circuits; Layout design; Design of CMOS cells; From gate to <b>arithmetic</b> {{circuit and}} <b>register</b> file; Low power design at the CMOS level; Design of MEMS sensors; Lab: cell design.|$|R
50|$|Accumulator (A-register) - The {{accumulator}} {{serves as}} the main register of the computer and holds the results of all <b>arithmetic</b> operations. This <b>register</b> serves as an output register for telemetry and character outputs.|$|R
40|$|In this paper, {{we present}} first results from EliAD, a new {{automatic}} differentiation tool. EliAD uses the Elimination approach for Automatic Differentation first advocated by Griewank and Reese (Automatic Differentiation of Algorithms, SIAM, 1991 p 126 - 135). EliAD implements this technique via source-transformation, writing new Fortran code for the Jacobians of functions defined by existing Fortran code. Our results are from applying EliAD to the Roe flux routine {{commonly used in}} computational fluid dynamics. We show that we can produce code that calculates the associated flux Jacobian approaching or in excess of twice the speed of current state-of-the-art automatic differentiation tools. However, {{in order to do}} so we must take into account the architecture on which we are running our code. In particular, on processors that do not support out-of-order execution, we must reorder our derivative code so that values may be reused while in <b>arithmetic</b> <b>registers</b> in order that the floating point arithmetic pipeline may be kept full. Presented at The 2002 International Conference on Computational Science ICCS 2002 Special Session on Automatic Differentiation and Applications Amsterdam, The Netherlands, April 21 - 24, 200...|$|R
50|$|Logic {{circuits}} {{include such}} devices as multiplexers, <b>registers,</b> <b>arithmetic</b> logic units (ALUs), and computer memory, {{all the way}} up through complete microprocessors, which may contain more than 100 million gates. In modern practice, most gates are made from field-effect transistors (FETs), particularly metal-oxide-semiconductor field-effect transistors (MOSFETs).|$|R
40|$|Abstract-The most {{creative}} step in synthesizing data paths executing software descriptions is the hardware allocation process. New algorithms for the simultaneous costlresource constrained allocation of <b>registers,</b> <b>arithmetic</b> units, and interconnect in a data path have been developed. The entire allocation {{process can be}} formulated as a twodimensional placement problem of microinstructions in space and time. This formulation readily lends itself {{to the use of}} a variety of heuristics for solving the allocation problem. We present simulated-annealingbased algorithms which provide excellent solutions to this formulation of the allocation problem. These algorithms operate under a variety of user-specifable constraints on hardware resources and costs. They also incorporate conditional resource sharing and simultaneously address all aspects of the allocation problem, namely <b>register,</b> <b>arithmetic</b> unit and interconnect allocation, while effectively exploring the existing tradeoffs in the design space. I...|$|R
40|$|Upon {{completion}} of the course, students: 1. can combine MSI circuits into larger or more complex digital circuits; 2. can write a VHDL description of a combinational logic circuit; 3. can write a VHDL description of a sequential state machine; 4. can use current engineering software to compile and simulate circuits, including those described using VHDL; 5. know the architecture of a basic computer system; 6. know {{the operation of the}} components of a basic computer system include control, <b>arithmetic</b> processors, <b>registers,</b> and buses; 7. know the relationship between the hardware architecture and a computer's assembly language instruction set...|$|R
40|$|Data paths are {{collections}} of <b>arithmetic</b> elements, buses, <b>registers</b> and multiplexers that usually account for roughly 80 {{percent of the}} area of a complex chip. Unfortunately, gate-level synthesis does not work well for these elements. Data-path synthesis solves this problem by keeping the design at a conceptually higher level than gate-level synthesis, thus allowing much larger designs...|$|R
50|$|The Data General Nova, Motorola 6800 family, and MOS Technology 6502 {{family of}} {{processors}} were families of processors {{with very few}} internal <b>registers.</b> <b>Arithmetic</b> and logical instructions were mostly performed against values in memory as opposed to internal registers. As a result, many instructions required a two-byte (16-bit) location to memory. Given that opcodes on these processors were only one byte (8 bits) in length, memory addresses could make up {{a significant part of}} code size.|$|R
50|$|In the {{original}} instruction set, all operations were memory-to-memory only, with no visible data <b>registers.</b> <b>Arithmetic</b> was done serially, one digit at a time, beginning with most-significant digits then working rightwards to least-significant digits. This is backwards from manual right-to-left methods and more complicated, but it allowed all result writing to be suppressed in overflow cases. Serial arithmetic {{worked very well}} for COBOL. But for languages like FORTRAN or BPL, it was much less efficient than standard word-oriented computers.|$|R
40|$|Traditional {{compiler}} {{data flow}} analysis techniques {{are used to}} transform the intermediate representation of a decompiled program to a higher representation that eliminates low-level concepts such as registers and condition codes, and reintroduces the high-level concept of expression. Summary data flow information is collected on condition codes and registers, and is propagated across basic blocks and subroutine boundaries to find boolean and <b>arithmetic</b> expressions, <b>register</b> arguments, function return registers, actual arguments, and propagate data types whenever required. The elimination of condition codes is performed by an extension of a reach algorithm. The elimination of registers and intermediate instructions is performed by an extended copy propagation algorithm {{that is based on}} intra and interprocedural analysis of the program's control flow graph. The methods presented in this paper have been implemented in dcc, a prototype decompiler for the Intel i 80286 architecture. Experi [...] ...|$|R
40|$|An {{architecture}} {{has been}} developed for a high performance VLSI digital signal processor that is highly reliable, fault tolerant and radiation hard. The signal processor, part of a spacecraft receiver designed to support uplink radio science experiments at the outer planets, organizes the connections between redundant <b>arithmetic</b> resources, <b>register</b> files and memory through a shuffle exchange communication network. The configuration of the network {{and the state of}} the processor resources are all under microprogram control which both maps the resources according to algorithmic needs and reconfigures the processing should a failure occur. In addition, the microprogram is reloadable through the uplink to accommodate changes in the science objectives throughout the course of the mission. The processor will be implemented with silicon compiler tools and its design will be verified through silicon compilation simulation at all levels from the resources to full functionality. By blending reconf [...] ...|$|R
40|$|The {{design of}} an {{asynchronous}} clone of a MIPS R 3000 microprocessor is presented. In 0 : 6 ¯m CMOS, we expect performance close to 280 MIPS, for a power consumption of 7 W. The paper describes {{the structure of}} a high-performance asynchronous pipeline, in particular precise exceptions, pipelined caches, <b>arithmetic,</b> and <b>registers,</b> and the circuit techniques developed to achieve high throughput. 1 Introduction This paper describes the architectural algorithms and circuit techniques used in the design of an asynchronous MIPS R 3000 microprocessor. The project has two main goals. First, we are investigating issues in asynchronous processor architecture that we have not tackled in the Caltech Asynchronous Microprocessor [6]: caches, precise exceptions, register bypassing, branch-delay slot and branch prediction. Secondly, we are developing new techniques for asynchronous digital VLSI [...] -based on very fine pipelining [...] -that can meet high throughput requirements without sacrificing the low-power ad [...] ...|$|R
5000|$|Posing as minimalists, we reduce all the {{registers}} excepting the accumulator A and indirection register N e.g. r = { r0, r1, r2, ... } to an unbounded {{string of}} (very-) bounded-capacity pigeon-holes. These {{will do nothing}} but hold (very-) bounded numbers e.g. a lone bit with value { 0, 1 }. Likewise we shrink the accumulator to a single bit. We restrict any <b>arithmetic</b> to the <b>registers</b> { A, N }, use indirect operations to pull the contents of registers into the accumulator and write 0 or 1 from the accumulator to a register: ...|$|R
40|$|This paper {{describes}} {{the design of}} a 17 bit 4 stage pipelined Reduced Instruction Set Computer (RISC) processor using Verilog HDL in Xilinx. The processor implements the Harvard memory architecture, so the instruction and data memory spaces are both physically and logically separate. There is 5 bit opcode with totally 23 set of instructions. The CPU designed by using the pipelining and it will increase speed of processor with CPI= 1. The pipeline stages such as fetch, decode, execute and store are used. The RISC processor architecture presented in this paper is designed by using <b>Registers,</b> <b>arithmetic</b> and logical unit, Memory with pipeline techniques. Memory access is done only by using Load and Store instructions...|$|R
40|$|When {{computational}} {{resources are}} limited, especially multipliers, distributed arithmetic (DA) {{is used in}} lieu of the typical multiplier-based filtering structures. However, DA is not well suited for adaptive applications. The bottleneck is updating the memory table. Several attempts have been done to accelerate updating the memory, but at the expense of additional memory usage and of convergence speed. To develop an adaptive DA filter with an uncompromised convergence rate, the memory table must be fully updated. In this research, an efficient method for fully updating a DA memory table is proposed. The proposed update method is based on exploiting the temporal locality of the stored data and subexpression sharing. The proposed update method reduces the computational workload and requires no additional memory resources. DA using the proposed update method is called conjugate distributed arithmetic. Filters can also be constructed from analog components. Often, for lower precision computations, analog circuits use less power and less chip area than their digital counterparts. However, digital components are often used because of their ease of reprogrammability. Achieving such reprogrammability in analog is possible, but at the expense of additional chip area. A reprogrammable mixed-signal DA finite impulse response (FIR) filter is proposed to address the issues with reprogrammable analog FIR filters that are constructing compact reprogrammable filtering structures, non-symmetric and imprecise filter coefficients, inconsistent sampling of the input data, and input sample data corruption. These issues are successfully addressed using distributed <b>arithmetic,</b> digital <b>registers,</b> and epots. Also, a mixed-signal DA second-order section (SOS), which is used as the building block for higher order infinite impulse response filters, was proposed. The type of issues with an analog SOS filter {{are similar to those of}} an analog FIR filter, which are the lack of a compact reprogrammable filtering structure, the imprecise filter coefficients, the inconsistent sampling of the data, and the corruption of the data samples. These issues are successfully addressed using distributed <b>arithmetic</b> and digital <b>registers.</b> Ph. D. Committee Chair: Anderson, David V.; Committee Member: Ferri, Bonnie H.; Committee Member: Hasler, Paul E.; Committee Member: Kang, Sung Ha; Committee Member: McClellan, James H.; Committee Member: Wolf, Wayne H...|$|R
40|$|Secondary {{multilevel}} analyses on {{the data}} set of the national evaluation of the Dutch Educational Priority Program (EPP) were applied to select schools that are high, average and low effective across grades and school years. Analyses were carried out on arithmetic achievement data of some 50, 000 pupils in 560 primary schools. These children were tested both in 1988 and 1990. Differences in school and instruction characteristics between {{the three types of}} schools were explored to explain differences in effectiveness. Results show that only one school characteristic, educational leadership, was related (negatively) to effectiveness. With regard to instruction characteristics the picture only partly is consistent with earlier research: teachers in high effective schools give more whole-class instruction, more often have the same minimum goals for all pupils and spend more time on learning activities and evaluation of learning tasks. They also offer more learning content during the school year. On the other hand they spend less hours during the week on <b>arithmetic</b> and <b>register</b> less frequently the mastering of learning content. It is concluded that efficient allocation of opportunity and time to learn within arithmetic lessons seems to be especially important in explaining differences in pupils' arithmetic achievement among schools...|$|R
40|$|Abstract—Some {{processors}} {{designed for}} consumer applications, such as Graphics Processing Units (GPUs) and the CELL processor, promise outstanding floating-point performance for scientific applications at commodity prices. However, IEEE single precision {{is the most}} precise floating-point data type these processors directly support in hardware. Pairs of native floatingpoint numbers {{can be used to}} represent a base result and a residual term to increase accuracy, but the resulting order of magnitude slowdown dramatically reduces the price/performance advantage of these systems. By adding a few simple microarchitectural features, acceptable accuracy can be obtained with relatively little performance penalty. To reduce the cost of native-pair <b>arithmetic,</b> a residual <b>register</b> is used to hold information that would normally have been discarded after each floating-point computation. The residual register dramatically simplifies the code, providing both lower latency and better instruction-level parallelism. I...|$|R
5000|$|The NCR 315 Data Processing System, {{released}} in January 1962 by NCR, was a second-generation computer. All {{printed circuit boards}} used resistor-transistor logic to create the various logic elements. It used 12-bit slab memory structure using core memory. The instructions could use a memory slab as either two 6-bit alphanumeric characters or as three 4-bit BCD characters. Basic memory was 5000 [...] "slabs" [...] of handmade core memory, which was expandable {{to a maximum of}} 40,000 slabs in four refrigerator-size cabinets. The main processor included three cabinets and a console section that housed the power supply, keyboard, output writer (an IBM electric typewriter), and a panel of lights that indicated the current status of the program counter, <b>registers,</b> <b>arithmetic</b> accumulator, and system errors. Input/Output was by direct parallel connections to each type of peripheral through a two-cable bundle with 1-inch-thick cables. Some devices like magnetic tape and the CRAM were daisy-chained to allow multiple drives to be connected.|$|R
40|$|Some of the {{potentially}} fastest processors {{that could be}} used for scientific computing do not have efficient floatingpoint hardware support for precisions higher than 32 -bits. This is true of the CELL processor, all current commodity Graphics Processing Units (GPUs), various Digital Signal Processing (DSP) chips, etc. Acceptably high accuracy can be obtained without extra hardware by using pairs of native floating-point numbers to represent the base result and a residual error term, but an order of magnitude slowdown dramatically reduces the price/performance advantage of these systems. By adding a few simple microarchitectural features, acceptable accuracy can be obtained with relatively little performance penalty. To reduce the cost of native-pair <b>arithmetic,</b> a residual <b>register</b> is used to hold information that would normally have been discarded after each floating-point computation. The residual register dramatically simplifies the code, providing both lower latency and better instruction-level parallelism. To support speculative use of faster, lower precision, arithmetic, a peak exponent monitor and an absorption counter are added to measure potential loss of accuracy. ...|$|R
40|$|This paper {{deals with}} the design and {{implementation}} of the 32 -bit floating point Digital signal processor with MIPS (microcomputer with out interlocked pipeline stages). The designed DSP has 32 floating point MIPS instructions, instruction sets suitable for processing digital signals and consists of super Harvard architecture, 40 -bit ALU, 5 level pipelines, 17 -bit X 17 -bit parallel multiplier for single-cycle MAC operation, 8 addressing modes, 8 auxiliary registers, 2 auxiliary <b>register</b> <b>arithmetic</b> units, two 40 -bit accumulators and 2 address generators. The VHSIC HDL coded synthesizable RTL code of the DSP core has a complexity of 80, 670 in the two input NAND gates. We verified the functions of the DSP by a simulation with a single instruction test as the first step and then implemented the DSP with the FPGA. The test vectors have a single instruction test, combination of single instructions and algorithm applications, ADPCM vocoder and the MP 3 decoder. After FPGA verification, the DSP core carried out three test vector sets which are tested at FPGA at the 106 MHz clock rates. General Terms Digital signal processor that can execute millions of instructions per secon...|$|R
40|$|Abstract: In this paper, we {{deal with}} the {{designing}} of a 32 -bit floating point arithmetic processor for RISC/DSP processor applications. It is capable of representing real and decimal numbers. The floating point operations are incorporated into the design as functions. The logic for these is different from the ordinary arithmetic functions. The numbers in contention have to be first converted into the standard IEEE floating point standard representation before any sorts of operations are conducted on them. The floating point representation for a standard single precision number is a 32 -bit number that is segmented to represent the floating point number. The IEEE format consists of four fields, the sign of the exponent, the next seven bits are that of the exponent magnitude, and the remaining 24 bits represent the mantissa sign. The exponent in this IEEE standard is represented in excess- 127 format all the arithmetic functions like addition, subtraction, multiplication and division will be design by the processor. The main functional blocks of floating point arithmetic processor design includes, <b>Arithmetic</b> logic unit(ALU), <b>Register</b> organization, control & decoding unit, memory block, 32 -bit floating point addition, subtraction, multiplication and division blocks. This processor IP core can be embedded many places such as co-processor for embedded DSP and embedded RISC controller. The overall system architecture will be designed using HDL language and simulation, synthesis...|$|R
40|$|This thesis investigates novel robust {{hardware}} {{elements for}} weightless artificial neural systems with a bias towards high integrity avionics applications. The author initially reviews {{the building blocks}} of physiological neural systems and then chronologically describes the development of weightless artificial neural systems. Several new design methodologies for the implementation of robust binary sum-and-threshold neurons are presented. The new techniques do not rely on weighted binary counters or <b>registered</b> <b>arithmetic</b> units for their operation making them less susceptible to transient single event upsets. They employ Boolean, weightless binary, asynchronous elements throughout thus increasing robustness in the presence of impulsive noise. Hierarchies formed from these neural elements are studied and a weightless probabilisitic activation function proposed for non-deterministic applications. Neuroram, an auto-associative memory created using these weightless neurons is described and analysed. The signal-to-noise ratio characteristics are compared with the traditional Hamming distance metric. This led to the proposal that neuroram can form a threshold logic based digital signal filter. Two weightless autoassociative memory based neuro-filters are presented and their filtration properties studied and compared with a traditional median filter. Eachn novel architecture was emulated using weightless numericM ATLAB code prior to schematic design and functional simulation. Several neural elements were implemented and validated using FPGA technology. A preliminary robustness evaluation was performed. The large scale particle accelerator at the Theodor Svedberg Laboratory at the University of Uppsala, Sweden, was used to generate transienut psetsin an FPGA performing a weightless binary neural function. One paper,two letters and five international patents have been published {{during the course of this}} research. The author has significantly contributed to the field of weightless artificial neural systems in high integrity hardware applications...|$|R
50|$|A {{pseudocode}} {{comparison of}} imperative, procedural, and object oriented approaches {{used to calculate}} the area of a circle (πr²), assuming no subroutine inlining, no macro preprocessors, <b>register</b> <b>arithmetic,</b> and weighting each instruction 'step' as only 1 instruction - as a crude measure of instruction path length - is presented below. The instruction step that is conceptually performing the state change is highlighted in bold typeface in each case. The arithmetic operations used to compute the area of the circle are the same in all three paradigms, with the difference being that the procedural and object-oriented paradigms wrap those operations in a subroutine call that makes the computation general and reusable. The same effect could be achieved in a purely imperative program using a macro preprocessor at only the cost of increased program size (only at each macro invocation site) without a corresponding pro rata runtime cost (proportional to n invocations - that may be situated within an inner loop for instance). Conversely, subroutine inlining by a compiler could reduce procedural programs to something similar in size to the purely imperative code. However, for object-oriented programs, even with inlining, messages still must be built (from copies of the arguments) for processing by the object-oriented methods. The overhead of calls, virtual or otherwise, is not dominated by the control flow alteration - but by the surrounding calling convention costs, like prologue and epilogue code, stack setup and argument passing (see here for more realistic instruction path length, stack and other costs associated with calls on an x86 platform). See also here for a slide presentation by Eric S. Roberts ("The Allocation of Memory to Variables", chapter 7) - illustrating the use of stack and heap memory use when summing three rational numbers in the Java object-oriented language.|$|R
40|$|This thesis {{extends the}} work and {{application}} of Taylor Expansion Diagrams (TED) {{as a framework for}} high level synthesis and verification of data-flow and arithmetic-intensive designs. It shows that TEDs can be used for different high level optimization objectives and that TED-based optimizations and transformations translate into actual gains in hardware. The first metric it tackles is area and latency optimization. This optimization has been studied in previous works, but reported gains were marginal and not driven by actual hardware performance. Specifically, the previously used metrics for decomposition targeted the minimization of the number of operations in the arithmetic data-path, assuming that this optimization would translate into latency or area gains. We show that such minimization does not produce noticeable hardware gains in terms of resource utilization or performance. We demonstrate that we can obtain more efficient hardware implementations with other decomposition methods which directly address those tangible hardware metrics. In this manner we demonstrate that TED is a valid framework for area and latency optimization and put a closure on this subject. Furthermore, we propose an extension to Taylor Expansion Diagrams to represent sequential <b>arithmetic</b> data-paths with <b>registers.</b> This extension allows TED to be used for register and delay minimization, {{while at the same time}} performing factorizations and common subexpression eliminations in the data-flow graph. It also enables transformation of one type of design into another, which is particularly useful in DSP design and filter optimizations. TED is shown as a valid framework for retiming and throughput optimizations. Finally, we show that TED can also be used to evaluate accuracy (and compute round-off error bounds) in fixed-point hardware architecture and to optimize the architecture in the presence of such errors under accuracy constrains. The proposed framework may be used to: (1) automatically compute the accuracy of a given architecture and certify (through a formal proof) that a given architecture meets or exceeds the required accuracy; and (2) optimize the architecture (in terms of delay, area, or power consumption) subject to the required accuracy. TED framework can provide a fast architectural exploration for DSP designs implemented in fixed-point hardware. In summary, this thesis will show that TED can be used as a kernel tool in high level synthesis that targets different optimizations and design transformations of data-flow designs. It can also be used as a vehicle to perform verification between the different forms of designs. ...|$|R

