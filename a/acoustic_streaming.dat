402|83|Public
50|$|As said, the <b>acoustic</b> <b>streaming</b> {{is driven}} by mass and force sources {{originating}} from the acoustic attenuation. However, {{they are not the}} only driven forces for the <b>acoustic</b> <b>streaming.</b> The boundary vibration may also have contribution to the <b>acoustic</b> <b>streaming,</b> especially to “boundary driven streaming”. For these cases, the boundary condition should also be processed by perturbation approach and be imposed to the two order equations accordingly.|$|E
50|$|For {{applications}} in a static flow, the fluid velocity {{comes from the}} <b>acoustic</b> <b>streaming.</b> The magnitude of <b>acoustic</b> <b>streaming</b> depends on the power and frequency of input. Also, the properties of fluid media affect the value as well. For typical acoustic-based microdevices, the operating frequency ranges from ~kHz to ~MHz. The vibration amplitude is in range of 0.1 nm~1 µm. The fluid used is water. The estimated magnitude of <b>acoustic</b> <b>streaming</b> is in range of 1 µm/s~ 1 mm/s. Thus, the <b>acoustic</b> <b>streaming</b> should be smaller than the main flow for most continuous flow applications. The drag force is mainly induced by the main flow in those applications.|$|E
5000|$|We can {{decompose}} {{the velocity}} field in a vibration part {{and a steady}} part [...]The vibration part [...] is due to sound, while the steady part is the <b>acoustic</b> <b>streaming</b> velocity (average velocity).The Navier-Stokes equations implies for the <b>acoustic</b> <b>streaming</b> velocity: ...|$|E
5000|$|... libdca (formerly libdts) {{is a free}} {{library for}} {{decoding}} DTS Coherent <b>Acoustics</b> <b>streams.</b> It is released {{under the terms of}} the GNU General Public License license, and is developed by Gildas Bazin of the VideoLAN team. The library is based on the DTS Coherent Acoustics standard (ETSI 102 114 v1.2.1).|$|R
40|$|The work {{described}} in this paper has been motivated by consideration of both parsimony in the representation of speech acoustics and observations of the degradation of automatic speech recognition (ASR) performance when speaking rate changes. The acoustic-phonetic processing within an ASR system involves the matching of {{a representation of the}} <b>acoustic</b> <b>stream</b> with a phonem...|$|R
40|$|We are {{building}} an aid {{for individuals with}} hearing impairments which converts continuous speech into an animated visual display. A speech analysis system continuously estimates phoneme probabilities from the input <b>acoustic</b> <b>stream.</b> Phoneme symbols are displayed graphically with brightness in proportion to estimated phoneme probabilities. We use an automated layout algorithm to design the display to group acoustically confusable phonemes together in the graphical display...|$|R
50|$|To get the <b>acoustic</b> <b>streaming,</b> the first-order {{equations}} {{should be}} solved first. Since Navier-Stokes equations {{can only be}} solved for some simple cases analytically, numerical methods are the most used option to work them out in engineering. Finite element method (FEM) {{is one of the}} most used numerical method. It can be employed to simulate the <b>acoustic</b> <b>streaming</b> phenomena. Figure 3 is one example of <b>acoustic</b> <b>streaming</b> around a solid circular pillar, which is calculated by FEM method.|$|E
5000|$|... #Subtitle level 2: Order of {{magnitude}} of <b>acoustic</b> <b>streaming</b> velocities ...|$|E
5000|$|<b>Acoustic</b> <b>streaming</b> is {{a steady}} flow {{generated}} by a nonlinear effect in an acoustic field. Depending on the mechanisms, the <b>acoustic</b> <b>streaming</b> can be categorized into two general types, Eckert streaming and Rayleigh streaming. Eckert streaming is driven by a time-average momentum flux created when high amplitude acoustic wave propagates and attenuates in fluid. Rayleigh streaming, also called “boundary driven streaming”, is forced by a shear viscosity close to a solid boundary. Both of the driven mechanisms come from a time-average nonlinear effect. Regarding to the nonlinearity of <b>acoustic</b> <b>streaming,</b> a so-called perturbation approach is used to analyze this phenomenon. The governing equations for this problem are mass conservation and Navier-Stokes equations： ...|$|E
3000|$|..., {{with the}} subindex connoting the {{semantic}} or the <b>acoustic</b> <b>stream,</b> respectively. The large bi-directed arrow {{between the two}} matrices H indicates that a common matrix is sought for H, and thus common loads on recurrent patterns which are co-occurring between the two data streams. The finding of recurrent patterns, co-occurring between the two data streams, {{lies at the heart}} of the learning procedure (cf. the section ‘Non-negative matrix factorisation’), where idiosyncratic expressions are parsed and linked to operations on a device. The steps and algorithms are explained with more detail in the following sections.|$|R
40|$|This report {{describes}} {{the demonstration of}} the Audio-Visual based person authentication system. The system records the audio-visual data and splits it into <b>acoustic</b> and visual <b>streams.</b> The spectral features are derived from the <b>acoustic</b> <b>stream</b> and are represented by Weighted Linear Prediction Cepstral Coefficients(WLPCC). The system uses motion information to detect the face region in the visual stream, and the region is processed in YCrCb color space to determine {{the location of the}} eyes. The system extracts the gray level features relative to the location of the eyes. Autoassociative Neural Network(AANN) models are used to capture the distribution of the extracted acoustic and visual features. The proposed system can be used to recognize the identity of a test subject in addition to authentication. The performance of the system is invariant to size, and tilt of the face and is also not sensitive to variations in natural lighting conditions. 1...|$|R
40|$|Parsing {{continuous}} <b>acoustic</b> <b>streams</b> into perceptual units {{is fundamental}} to auditory perception. Previous studies have uncovered a cortical entrainment mechanism in the delta and theta bands (~ 1 - 8 Hz) that correlates with formation of perceptual units in speech, music, and other quasi-rhythmic stimuli. Whether cortical oscillations in the delta-theta bands are passively entrained by regular acoustic patterns or {{play an active role}} in parsing the <b>acoustic</b> <b>stream</b> is debated. Here, we investigate cortical oscillations using novel stimuli with 1 /f modulation spectra. These 1 /f signals have no rhythmic structure but contain information over many timescales because of their broadband modulation characteristics. We chose 1 /f modulation spectra with varying exponents of f, which simulate the dynamics of environmental noise, speech, vocalizations, and music. While undergoing magnetoencephalography (MEG) recording, participants listened to 1 /f stimuli and detected embedded target tones. Tone detection performance varied across stimuli of different exponents and can be explained by local signal-to-noise ratio computed using a temporal window around 200 ms. Furthermore, theta band oscillations, surprisingly, were observed for all stimuli, but robust phase coherence was preferentially displayed by stimuli with exponents 1 and 1. 5. We constructed an auditory processing model to quantify acoustic information on various timescales and correlated the model outputs with the neural results. We show that cortical oscillations reflect a chunking of segments, > 200 ms. These results suggest an active auditory segmentation mechanism, complementary to entrainment, operating on a timescale of ~ 200 ms to organize acoustic information...|$|R
5000|$|For the second-order equations, {{they can}} be {{considered}} as governing equations using to describe the motion of fluid with mass source [...] and force source [...] Generally, the <b>acoustic</b> <b>streaming</b> is a steady mean flow, whose response time scale is much smaller than the one of the acoustic vibration. The time-average term [...] is normally used to present the <b>acoustic</b> <b>streaming.</b> By time average the second-order equations and using that [...] , the time-average second-order equations can be obtained, ...|$|E
50|$|Even if {{viscosity}} {{is responsible}} for <b>acoustic</b> <b>streaming,</b> the value of viscosity disappears from the resulting streaming velocities {{in the case of}} near-boundary acoustic steaming.|$|E
50|$|The {{detailed}} {{motion of}} the powder is actually due to an effect called <b>acoustic</b> <b>streaming</b> caused by {{the interaction of the}} sound wave with the boundary layer of air at the surface of the tube.|$|E
40|$|We {{report on}} an {{architecture}} for the unsupervised discovery of talker-invariant subword embeddings. It {{is made out}} of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated frag-ments in the <b>acoustic</b> <b>streams</b> while the DNN is trained to mini-mize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of dif-ferent clusters. We use additional side information regarding the average duration of phonemic units, as well as talker iden-tity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improve-ment over the baseline in subword unit modeling. Index Terms: zero resource speech challenge, feature extrac-tion, deep learning 1...|$|R
40|$|International audienceAnimal {{and human}} studies have {{suggested}} that posterior temporal, parietal, and frontal regions are specifically involved in auditory spatial (location and motion) processing, forming a putative dorsal "where" pathway. We used scalp EEG and current density mapping to investigate the dynamics of this network in human subjects presented with a varying <b>acoustic</b> <b>stream</b> in a two-factor paradigm: spatial versus pitch variations, focused versus diverted attention. The main findings were: (i) a temporo-parieto-frontal network was activated during the whole duration of the stream in all conditions and modulated by attention; (ii) the left superior temporal cortex was the only region showing different activations for pitch and spatial variations. Therefore, parietal and frontal regions would be involved in task-related processes (attention and motor preparation), whereas the differential processing of acoustic spatial and object-related features seems to take place at the temporal level...|$|R
40|$|This paper {{presents}} {{results of}} our efforts on combining standard mixture of Gaussians acoustic modeling [10] with a context-dependent hybrid connectionist HME/HMM architecture [3, 4] for the Switchboard corpus. Using a score normalization scheme which is independent of the stream's modeling paradigm and adaptive methods for combining multiple probability distributions, we achieve a relative decrease in word error rate of 3. 5 % and 9. 3 %, compared {{to each of the}} single stream systems. As opposed to multiple <b>acoustic</b> <b>streams</b> based on mixture of Gaussians, the integration of hybrid NN/HMM based modeling appears to be advantageous since the differences in modeling techniques and training algorithms allow to capture different aspects of the speech signal. Small dependence among emission probability estimates is considered essential for potential gains in interpolated systems. 1. INTRODUCTION Recognizing spontaneous conversational telephone speech {{is one of the most}} challenging fields being t [...] ...|$|R
50|$|<b>Acoustic</b> <b>streaming</b> is {{a steady}} flow in a fluid driven by the {{absorption}} of high amplitude acoustic oscillations. This phenomenon can be observed near sound emitters, or in the standing waves within a Kundt's tube.It is the less-known opposite of sound generation by a flow.|$|E
50|$|He was {{appointed}} docent at the University of Bergen in 1960, and {{was promoted to}} professor in 1963. He succeeded Oddvar Bjørgum, and had responsibility for the university's education in applied mathematics. His fields of research include plasma, nonlinear acoustics, hydroacoustics and <b>acoustic</b> <b>streaming.</b> He was also {{the dean of the}} Faculty of Mathematics and Natural Sciences from 1975 to 1977, and has held positions in NAVF, NTVF, and in the national committee of the International Union of Theoretical and Applied Mechanics. He has also been a visiting scholar at the University of Texas.|$|E
50|$|There {{are three}} primary {{benefits}} to ultrasound. The {{first is the}} speeding up of the healing process from the increase in blood flow in the treated area. The second is the decrease in pain from the reduction of swelling and edema. The third is the gentle massage of muscle tendons and/ or ligaments in the treated area because no strain is added and any scar tissue is softened. These three benefits are achieved by two main effects of therapeutic ultrasound. The two types of effects are: thermal and non thermal effects. Thermal effects are due to the absorption of the sound waves. Non thermal effects are from cavitation, microstreaming and <b>acoustic</b> <b>streaming.</b>|$|E
40|$|The entropy metric {{derived from}} {{information}} theory {{provides a means}} to quantify {{the amount of information}} transmitted in <b>acoustic</b> <b>streams</b> like speech or music. By systematically varying the entropy of pitch sequences, we sought brain areas where neural activity and energetic demands increase as a function of entropy. Such a relationship is predicted to occur in an efficient encoding mechanism that uses less computational resource when less information is present in the signal: we specifically tested the hypothesis that such a relationship is present in the planum temporale (PT). In two convergent functional MRI studies, we demonstrated this relationship in PT for encoding, while furthermore showing that a distributed fronto-parietal network for retrieval of acoustic information is independent of entropy. The results establish PT as an efficient neural engine that demands less computational resource to encode redundant signals than those with high information content...|$|R
40|$|This paper {{describes}} a lightweight method for the automatic insertion of intra-sentence punctuation into text. Despite the intuition that pauses in an <b>acoustic</b> <b>stream</b> are a positive indicator for {{some types of}} punctuation, this work will demonstrate the feasibility of a system which relies solely on lexical information. Besides its potential role in a speech recognition system, such a system could serve equally well in non-speech applications such as automatic grammar correction in a word processor and parsing of spoken text. After describing {{the design of a}} punctuationrestoration system, which relies on a trigram language model and a straightforward application of the Viterbi algorithm, we summarize results, both quantitative and subjective, of the performance and behavior of a prototype system. 1. INTRODUCTION The requirement that conventional speech dictation systems impose on the user to enunciate punctuation can often be an annoyance and in some situations even an impossibility. [...] ...|$|R
40|$|We are {{building}} an aid {{for individuals with}} hearing impairments which converts continuous speech into an animated visual display. A speech analysis system continuously estimates phoneme probabilities from the input <b>acoustic</b> <b>stream.</b> Phoneme symbols are displayed graphically with brightness in proportion to estimated phoneme probabilities. We use an automated layout algorithm to design the display to group acoustically confusable phonemes together in the graphical display. Introduction We are working on automatic speech analysis techniques {{which can be used}} to augment the communication abilities of hearing-impaired individuals. For example an individual might wear a portable device which includes a microphone to record the communication partner's speech, and some sort of tactile, visual or electrical stimulus output device which displays the speech after some preprocessing. A basic design decision in building such a hearing aid is to determine how much interpretation of the speech sig [...] ...|$|R
5000|$|He is {{especially}} renowned for his {{application of the}} theory of Sir Michael James Lighthill, for his important contributions to the understanding of nonlinear scattering of sound by sound, and for his discoveries of the parametric array and the laser-excited thermoacoustic array. [...] His lifetime of physics research spans other aspects of acoustics as well, include the contributions to the understanding of acoustic radiation pressure, which has applications to Acoustic levitation and other devices which exploit macrosonic phenomena and <b>acoustic</b> <b>streaming,</b> as well as to several other fields of Physics (with example references shown here), including General Relativity, (primarily in the area of gravitational waves), including Gravitational phenomena analogous to the parametric array, Cosmology, low temperature physics the Physics of Sound in Liquid Helium, and High Energy Particle Physics (primarily in the area of cosmic ray particle detectors.|$|E
5000|$|Sonoelectrochemistry is the {{application}} of ultrasound in electrochemistry. Like sonochemistry, sonoelectrochemistry was discovered in the early 20th century. The effects of power ultrasound on electrochemical systems and important electrochemical parameters were originally demonstrated by Moriguchi and then by Schmid and Ehert [...] when the researchers investigated the influence of ultrasound on concentration polarisation, metal passivation {{and the production of}} electrolytic gases in aqueous solutions. In the late 1950s, Kolb and Nyborg showed that the electrochemical solution (or electroanalyte) hydrodynamics in an electrochemical cell was greatly increased in the presence of ultrasound and described this phenomenon as <b>acoustic</b> <b>streaming.</b> In 1959, Penn et al. demonstrated that sonication had a great effect on the electrode surface activity and electroanalyte species concentration profile throughout the solution. In the early 1960s, the electrochemist Allen J. Bard showed in controlled potential coulometry experiments that ultrasound significantly enhances mass transport of electrochemical species from the bulk solution to the electroactive surface. In the range of ultrasonic frequencies kHz - 2 MHz, ultrasound has been applied to many electrochemical systems, processes and areas of electrochemistry (to name but a few: electroplating, electrodeposition, electropolymerisation, electrocoagulation, organic electrosynthesis, materials electrochemistry, environmental electrochemistry, electroanalytical chemistry, hydrogen energy and fuel cell technology) both in academia and industry, as this technology offers several benefits over traditional technologies. The advantages are as follows: significant thinning of the diffusion layer thickness (δ) at the electrode surface; increase in electrodeposit/electroplating thickness; increase in electrochemical rates, yields and efficiencies; increase in electrodeposit porosity and hardness; increase in gas removal from electrochemical solutions; increase in electrode cleanliness and hence electrode surface activation; lowerering in electrode overpotentials (due to metal depassivation and gas bubble removal generated at the electrode surface induced by cavitation and acoustic streaming); and suppression in electrode fouling (depending on the ultrasonic frequency and power).|$|E
40|$|Progressive {{ultrasonic}} waves cause <b>acoustic</b> <b>streaming</b> in a liquid. Although {{theoretical and}} experimental studies on 	<b>acoustic</b> <b>streaming</b> for liquid phase {{have been carried}} out, the <b>acoustic</b> <b>streaming</b> for a solid-liquid mixture dose not 	seems to be investigated. The {{purpose of this study}} is to clarify the velocity distribution of <b>acoustic</b> <b>streaming</b> in a 	solid-liquid mixture. An ultrasonic wave with a frequency of 485 kHz was irradiated on tap water or tap water with 	aluminum particles in a cylindrical container with a diameter of 120 mm whose orientation was kept horizontal; the 	<b>acoustic</b> <b>streaming</b> velocities were measured with the irradiation time of ultrasonic wave, particle concentration, and 	particle shape as the parameters. The followings were obtained: (a) The higher the particle concentration is, the faster 	the <b>acoustic</b> <b>streaming</b> velocity of solid-liquid mixture becomes; (b) When ultrasonic waves were irradiated on a liquid 	with heavier solid particles, the <b>acoustic</b> <b>streaming</b> velocity of the solid-liquid mixture decreases with irradiation time to a certain extent...|$|E
30|$|We {{describe}} {{a method to}} convert a virtual human avatar (animated through key frames and interpolation) into a more naturalistic talking head. In fact, speech articulation cannot be accurately replicated using interpolation between key frames and talking heads with good speech capabilities are derived from real speech production data. Motion capture data are commonly used to provide accurate facial motion for visible speech articulators (jaw and lips) synchronous with acoustics. To access tongue trajectories (partially occluded speech articulator), electromagnetic articulography (EMA) is often used. We recorded a large database of phonetically-balanced English sentences with synchronous EMA, motion capture data, and acoustics. An articulatory model was computed on this database to recover missing data and to provide ‘normalized’ animation (i.e., articulatory) parameters. In addition, semi-automatic segmentation {{was performed on the}} <b>acoustic</b> <b>stream.</b> A dictionary of multimodal Australian English diphones was created. It is composed of the variation of the articulatory parameters between all the successive stable allophones.|$|R
40|$|Most {{conversational}} understanding (CU) systems today {{employ a}} cascade approach, {{where the best}} hypothesis from automatic speech recognizer (ASR) is fed into spoken language understanding (SLU) module, whose best hypothesis is then fed into other systems such as interpreter or dialog manager. In such approaches, errors from one statistical module irreversibly propagates into another module causing a serious degradation in the overall performance of the conversational understanding system. Thus it is desirable to jointly optimize all the statistical modules together. As a first step towards this, in this paper, we propose a joint decoding framework in which we predict the optimal word as well as slot (semantic tag) sequence jointly given the input <b>acoustic</b> <b>stream.</b> On Microsoft’s CU system, we show 1. 3 % absolute reduction in word error rate (WER) and 1. 2 % absolute improvement in F measure for slot prediction when compared to a very strong cascade baseline comprising of the state-of-the-art recognizer followed by a slot sequence tagger...|$|R
40|$|To write a language, {{one must}} first {{abstract}} the unit {{to be used}} from the <b>acoustic</b> <b>stream</b> of speech. Writing systems based on the meaningless units, syllables and phonemes, were late developments {{in the history of}} written language. The alphabetic system, which requires abstraction of the phonemic unit of speech, was the last to appear, evolved from a syllabary and, unlike the other systems, was apparently invented only once. It might therefore be supposed that phoneme segmentation is particularly difficult and more difficult, indeed, than syllable segmentation. Speech research sug-gests reasons why this may be so. The present study provides direct evidence of a similar developmental ordering of syllable and phoneme segmentation abilities in the young child. By means of a task which required preschool, kindergarten, and first-grade children to tap out the number of segments in spoken utterances, it was found that, though ability in both syllable and phoneme segmentation increased with grade level, analysis into phonemes was significantly harder and perfected later tha...|$|R
40|$|The {{presence}} of <b>acoustic</b> <b>streaming</b> in ovarian cysts {{can be determined}} by ultrasound imaging and provides {{an indication of the}} viscosity of the cyst content. It has previously been demonstrated that the absence of <b>acoustic</b> <b>streaming</b> in endometriomas could be a pathognomonic feature of endometriomas. In this report we demonstrate the {{presence of}} <b>acoustic</b> <b>streaming</b> in two endometriomas. status: publishe...|$|E
40|$|Objective To {{determine}} {{the ability of}} <b>acoustic</b> <b>streaming</b> to discriminate between endometriomas and other adnexal masses. Methods We used data from 1938 patients with an adnexal mass included in Phase 2 of the International Ovarian Tumor Analysis (IOTA) study. All patients had been examined by transvaginal gray-scale and Doppler ultrasound following a standardized research protocol. Assessment of <b>acoustic</b> <b>streaming</b> was voluntary and was carried out only in lesions containing echogenic cyst fluid. <b>Acoustic</b> <b>streaming</b> was defined as movement of particles inside the cyst fluid during gray-scale and/or color Doppler examination provided that the probe had been held still for two seconds {{to ensure that the}} movement of the particles was not caused by movement of the probe or the patient. Only centers where <b>acoustic</b> <b>streaming</b> had been evaluated in > 90 % of cases were included. Sensitivity, specificity, positive and negative likelihood ratios (LR+, LR-), and positive and negative predictive values (PPV and NPV) of <b>acoustic</b> <b>streaming</b> with regard to endometrioma were calculated. Results 460 (24 %) masses were excluded because they were examined in centers where <= 90 % of the masses with echogenic cyst fluid had been evaluated for the presence of <b>acoustic</b> <b>streaming.</b> <b>Acoustic</b> <b>streaming</b> was evaluated in 633 of 646 lesions containing echogenic cyst fluid. It was present in 19 (9 %) of 209 endometriomas and in 55 (13 %) of 424 other lesions. This corresponds to a sensitivity of absent <b>acoustic</b> <b>streaming</b> with regard to endometrioma of 91 % (190 / 209), a specificity of 13 % (55 / 424), LR+ of 1. 04, LR- of 0. 69, PPV of 34 % (190 / 559) and NPV of 74 % (55 / 74). Conclusions <b>Acoustic</b> <b>streaming</b> cannot discriminate reliably between endometrioinas and other adnexal lesions, and the presence of <b>acoustic</b> <b>streaming</b> does not exclude an endometrioma. Copyright (C) 2009 ISUOG. Published by John Wiley & Sons, Ltd...|$|E
30|$|In {{the present}} work, {{the tumor was}} located close to the blood vessel with the {{diameter}} d= 7 mm. For smaller blood vessel diameters, the effect of <b>acoustic</b> <b>streaming</b> will be smaller. In [24, 25], the importance of blood flow cooling and <b>acoustic</b> <b>streaming</b> was studied for parallel and perpendicular blood vessel orientations. Intermediate intensity range was considered. It was shown that when {{the distance between the}} tumor and blood vessel wall is less than several millimeters, the convective cooling should be taken into account and the homogenization assumption becomes no longer acceptable. If the blood vessel is inside the area of the beam, inside the half-pressure maximum (- 6 dB) contours, the effect of <b>acoustic</b> <b>streaming</b> should be taken into account. For the case of high intensities, the effect of <b>acoustic</b> <b>streaming</b> becomes important in the larger area. For blood vessels with a smaller diameter, the effect of <b>acoustic</b> <b>streaming</b> becomes less important. It was shown that if the diameter of the blood vessel is smaller than 1 mm <b>acoustic</b> <b>streaming</b> velocity magnitude is smaller than blood flow velocity [24]. However, only intermediate intensity range was considered. For the high-intensity focused ultrasound, <b>acoustic</b> <b>streaming</b> velocity magnitude will be larger and additional parametric studies may be necessary. In the near future, it is highly interesting to investigate the threshold size of blood vessel and the distance between the tumor and blood vessel wall beyond which the <b>acoustic</b> <b>streaming</b> effects should be considered negligibly small in the modeling and calculations.|$|E
40|$|During speech listening, {{the brain}} parses a {{continuous}} <b>acoustic</b> <b>stream</b> of information into computational units (e. g. syllables or words) necessary for speech comprehension. Recent hypotheses {{have proposed that}} neural oscillations contribute to speech parsing but whether they do so {{on the basis of}} acoustic cues (bottom-up acoustic parsing) or as a function of available linguistic representations (top-down linguistic parsing) is unknown. In this magnetoencephalography study, we contrasted acoustic and linguistic parsing using bistable speech sequences. While listening to speech sequences, participants were asked to maintain one of the two possible speech percepts through volitional control. We predicted that the tracking of speech dynamics by neural oscillations would not solely follow the acoustic properties but also shift in time according to participant’s conscious speech percept. Our results show two issociable markers of neural-speech tracking under endogenous control: small modulations in low-frequency oscillations and variable latencies of high-frequency activity (sp. beta and gamma bands). While changes in low-frequency neural oscillations are compatible with the encoding of pre-lexical segmentation cues, high-frequency activity specifically informed on an individual’s conscious speech percept...|$|R
40|$|Speech {{contains}} {{information of}} at least three sources- the message that is being communicated, the speaker who is communicating and the environment. In this work we propose several approaches to improve the recognition of speech sounds that convey information about the message. We use phonemes which occur at the rate of few tens of milliseconds in the speech signal as basic units. Improvements in recognizing these units result in considerable performance gains in applications like automatic speech recognition (ASR) where the goal is to transcribe the message into text and automatic speaker verification that uses information in the speaker component to verify the the speaker’s claimed identity. We propose several approaches to improve phoneme posterior estimates from artificial neural networks. These include combination of information for multiple <b>acoustic</b> <b>streams</b> and different neural network training architectures. For speech recognition, especially in lowresource scenarios where the amount training data is limited (for example 1 hour of training), features extracted from better phoneme posteriors using the proposed techniques provide significant word recognition improvements. For speaker recognition these posteriors are used in...|$|R
5000|$|... "Clean the Slate" [...] (Two {{separate}} <b>acoustic</b> recordings were <b>streamed</b> in 2007 and 2010 by the band, {{although the}} track was ultimately only released in electric full band form on The Beautiful Sounds of Revenge.) ...|$|R
