11|64|Public
40|$|We {{address the}} problem of {{estimating}} the <b>alignment</b> <b>pose</b> between two models using structure-specific local descriptors. Our descriptors are generated using a combination of 2 D image data and 3 D contextual shape data, resulting in a set of semi-local descriptors containing rich appearance and shape information for both edge and texture structures. This is achieved by defining feature space relations which describe the neighborhood of a descriptor. By quantitative evaluations, we show that our descriptors provide high discriminative power compared to state of the art approaches. In addition, we show how to utilize this for the estimation of the <b>alignment</b> <b>pose</b> between two point sets. We present experiments both in controlled and real-life scenarios to validate our approach...|$|E
40|$|We {{present a}} {{multi-purpose}} algorithm for simultaneous face detection, face <b>alignment,</b> <b>pose</b> estimation, gender recognition, smile detection, age estimation and face recognition using a single deep {{convolutional neural network}} (CNN). The proposed method employs a multi-task learning framework that regularizes the shared parameters of CNN and builds a synergy among different domains and tasks. Extensive experiments show that the network has a better understanding of face and achieves state-of-the-art result for most of these tasks...|$|E
40|$|Abstract — We {{address the}} problem of {{estimating}} the align-ment pose between two models using structure-specific local descriptors. Our descriptors are generated using a combination of 2 D image data and 3 D contextual shape data, resulting in a set of semi-local descriptors containing rich appearance and shape information for both edge and texture structures. This is achieved by defining feature space relations which describe the neighborhood of a descriptor. By quantitative evaluations, we show that our descriptors provide high discriminative power compared to state of the art approaches. In addition, we show how to utilize this for the estimation of the <b>alignment</b> <b>pose</b> between two point sets. We present experiments both in controlled and real-life scenarios to validate our approach. I...|$|E
30|$|Such {{high degree}} of <b>alignment</b> <b>poses</b> {{challenges}} to classical projection methods since the progress made in each iteration is clearly humble. A much more favorable situation applies when the normal vectors’ directions are spread close to evenly over the unit circle so as to lower the conditioning of the problem. The system depicted on the right in Figure  1 is obtained from the previous ill-conditioned one through the easily invertible Singular Value Homogenization (SVH) transformation (described below) and visibly features such better condition. Also plotted is the progress made by the classical Kaczmarz projection method which confirms the improved run-time (left: first 50 iterations without convergence, right: convergence after seven steps).|$|R
40|$|A {{framework}} for pose-invariant face recognition using the <b>pose</b> <b>alignment</b> method {{is described in}} this paper. The main idea is to normalize the face view in depth to frontal view as the input of face recognition framework. Concretely, an inputted face image is first normalized using the irises information, and then the pose subspace algorithm is employed to perform the pose estimation. To well model the pose-invariant, the face region {{is divided into three}} rectangles with different mapping parameters in this <b>pose</b> <b>alignment</b> algorithm. So the affine transformation parameters associated with the different poses can be used to align the input pose image to frontal view. To evaluate this algorithm objectively, the views after the <b>pose</b> <b>alignment</b> are incorporated into the frontal face recognition system. Experimental results show that it has the better performance and it increases the recognition rate statistically by 17. 75 % under the pose that rotated within 30 degree. Keywords: Pose-invariant face recognition, <b>pose</b> estimation, <b>pose</b> <b>alignment,</b> affine transformation, face recognition 1...|$|R
40|$|A {{comprehensive}} {{data set}} of aligned ligands with highly similar binding pockets from the Protein Data Bank has been built. Based on this data set, a scoring function for recognizing good <b>alignment</b> <b>poses</b> for small molecules has been developed. This function {{is based on}} atoms and hydrogen-bond projected features. The concept is simply that atoms and features of a similar type (hydrogen-bond acceptors/donors and hydrophobic) tend to occupy the same space in a binding pocket and atoms of incompatible types often tend to avoid the same space. Comparison with some recently published results of small molecule alignments shows that the current scoring function can lead to performance better than those of several existing methods...|$|R
40|$|In this paper, {{we argue}} that the most {{difficult}} face recognition problems (unconstrained face recognition) will be solved by simultaneously leveraging the solutions to multiple vision problems including segmentation, <b>alignment,</b> <b>pose</b> estimation, and the estimation of other hidden variables such as gender and hair color. While in theory a single unified principle could solve all these problems simultaneously in a giant hidden variable model, we believe that such an approach will be computationally, and more importantly, statistically, intractable. Instead, we promote studying the interactions among mid-level vision features, such as segmentations and pose estimates, as a route toward solving very difficult recognition problems. In this paper, we discuss and provide results showing how pose and face segmentations mutually influence each other, and provide a surprisingly simple method for estimating pose from segmentations. 1...|$|E
40|$|Abstract:Nose tip {{detection}} is {{an essential}} point in 3 d face <b>alignment,</b> <b>pose</b> estimation and the discrimination process. A method to determine nose tip location based on exploiting local descriptors(shape index) from each point cloud on the 3 d surface mesh is presented in this paper. Furthermore, theeffective energy constraint was appliedover each 3 d mesh data point. Then NN-Nearest Neighbour classification methodwas applied in order to locate the nose tip and nose region based 3 d feature extraction techniques. The detection method is also characterized by the refining of candidate points for nose tip regions and is robust against pose and expression variations. All computations {{were based on the}} pure 3 D mesh data points of a face without texture information. The proposed method was tested using a 3 d face image sample database FRAV 3 d, and GAVADB database. The experimental results show 96 % and 76 % correct nose tip detection for FRAV 3 d and GAVADB databases respectively. I...|$|E
40|$|Given {{an area of}} {{interest}} in a video sequence, one may want to manipulate or edit the area, e. g. remove occlu-sions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultane-ously deal with temporal <b>alignment,</b> <b>pose</b> rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rec-tifying {{a small number of}} initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust align-ment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Ex-perimental results on both simulated and real data demon-strate the accurate and robust performance of our method. 1...|$|E
40|$|This report {{introduces}} a basic alignment scheme for the GEM muon detector. Optical straightness monitors are described, and their application discussed. Alternative alignment technologies are suggested and techniques are identified {{that can provide}} multipoint measurements. The problem of global <b>alignment</b> is <b>posed,</b> and several concepts are presented to attain the required precision...|$|R
40|$|Abstract. Many {{recent efforts}} {{have shown the}} {{effectiveness}} of dictionary learning methods in solving several computer vision problems. However, when designing dictionaries, training and testing domains may be different, due to different view points and illumination conditions. In this paper, we present a function learning framework for the task of transforming a dictionary learned from one visual domain to the other, while maintaining a domain-invariant sparse representation of a signal. Domain dictionaries are modeled by a linear or non-linear parametric function. The dictionary function parameters and domain-invariant sparse codes are then jointly learned by solving an optimization problem. Experiments on real datasets demonstrate the effectiveness of our approach for applications such as face recognition, <b>pose</b> <b>alignment</b> and <b>pose</b> estimation. ...|$|R
40|$|Dilacerated {{teeth are}} {{commonly}} {{seen in the}} maxillary anterior region. They are {{a cause for concern}} to both patients as well as parents when such teeth do not erupt or erupt in an unusual position. Careful planning is required while aligning such teeth. Orthodontists often hesitate aligning severely dilacerated teeth due to high chances of failure. A case of a dilacerated and malposed right central incisor in an eleven-year-old male patient is presented here. The tooth was orthodontically repositioned despite its unusual position and severely dilacerated root. Key words: Dilacerated teeth, Malposed teeth, Dilacerated central incisor. Dilacerated roots are a common finding in the maxillary central incisor region especially in the permanent dentition 1, 2. However their <b>alignment</b> <b>poses</b> a clinical challenge to the orthodontists especially when they are severely dilacerated, and malposed or impacted. Treatment depends upon the early diagnosis 3, degree of dilaceration, position of the tooth and patients motivation 4. A case of severely malposed and dilacerate...|$|R
40|$|Most {{contemporary}} {{face recognition}} algorithms work well under laboratory conditions but degrade when tested in less-controlled environments. This is mostly {{due to the}} difficulty of simultaneously handling variations in illumination, <b>alignment,</b> <b>pose,</b> and occlusion. In this paper, we propose a simple and practical face recognition system that achieves a high degree of robustness and stability to all these variations. We demonstrate how to use tools from sparse representation to align a test face image with a set of frontal training images in the presence of significant registration error and occlusion. We thoroughly characterize the region of attraction for our alignment algorithm on public face datasets such as Multi-PIE. We further study how to obtain a sufficient set of training illuminations for linearly interpolating practical lighting conditions. We have implemented a complete face recognition system, including a projectorbased training acquisition system, in order to evaluate how our algorithms work under practical testing conditions. We show that our system can efficiently and effectively recognize faces under a variety of realistic conditions, using only frontal images under the proposed illuminations as training. 1...|$|E
40|$|We {{propose a}} general {{framework}} for aligning continuous (oblique) video onto 3 D sensor data. We align a point cloud computed from the video onto the point cloud directly {{obtained from a}} 3 D sensor. This {{is in contrast to}} existing techniques where the 2 D images are aligned to a 3 D model derived from the 3 D sensor data. Using point clouds enables the alignment for scenes full of objects that are difficult to model, for example, trees. To compute 3 D point clouds from video, motion stereo is used along with a state-of-the-art algorithm for camera pose estimation. Our experiments with real data demonstrate the advantages of the proposed registration algorithm for texturing models in large-scale semi-urban environments. The capability to align video before a 3 D model is built from the 3 D sensor data offers new practical opportunities for 3 D modeling. We introduce a novel modeling-through-registration approach that fuses 3 D information from both the 3 D sensor and the video. Initial experiments with real data illustrate the potential of the proposed approach. Index Terms — <b>Alignment,</b> <b>pose</b> estimation, motion stereo, range data, sensor fusion, 3 D model & visualization I...|$|E
40|$|Abstract—Many {{computer}} vision problems (e. g., camera calibration, image alignment, structure from motion) are solved with nonlinear optimization methods. It is generally accepted that second order descent methods {{are the most}} robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, {{in the context of}} {{computer vision}}, second order descent methods have two main drawbacks: (1) the function might not be analytically differentiable and numerical approximations are impractical, and (2) the Hessian may be large and not positive definite. To address these issues, this paper proposes generic descent maps, which are average “descent directions ” and rescaling factors learned in a supervised fashion. Using generic descent maps, we derive a practical algorithm- Supervised Descent Method (SDM) - for minimizing Nonlinear Least Squares (NLS) problems. During training, SDM learns a sequence of decent maps that minimize the NLS. In testing, SDM minimizes the NLS objective using the learned descent maps without computing the Jacobian or the Hessian. We prove the conditions under which the SDM is guaranteed to converge. We illustrate the effectiveness and accuracy of SDM in three computer vision problems: rigid image alignment, non-rigid image alignment, and 3 D pose estimation. In particular, we show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code has been made available at www. humansensing. cs. cmu. edu/intraface. Index Terms—Newton’s method, Lucas-Kanade, nonlinear least squares, face alignment, image <b>alignment,</b> <b>pose</b> estimation F...|$|E
40|$|In {{this paper}} {{we present a}} novel joint {{approach}} for optimising surface curvature and <b>pose</b> <b>alignment.</b> We present two implementations of this joint optimisation strategy, including a fast implementation that uses two frames and an offline multi-frame approach. We demonstrate {{an order of magnitude}} improvement in simulation over state of the art dense relative point-to-plane Iterative Closest Point (ICP) <b>pose</b> <b>alignment</b> using our dense joint frame-to-frame approach and show comparable pose drift to dense point-to-plane ICP bundle adjustment using low-cost depth sensors. Additionally our improved joint quadric based approach can be used to more accurately estimate surface curvature on noisy point clouds than previous approaches...|$|R
40|$|Motivation: Current {{methods that}} {{annotate}} conserved transcription factor binding sites in an alignment of two regulatory regions perform the alignment and annotation step separately and combine {{the results in}} the end. If the site descriptions are weak or the sequence similarity is low, the local gap structure of the <b>alignment</b> <b>poses</b> a problem in detecting the conserved sites. It is therefore desirable to have an approach that is able to simultaneously consider the alignment as well as possibly matching site locations. Results: With SimAnn we have developed a tool that serves exactly this purpose. By combining the annotation step and the alignment of the two sequences into one algorithm, it detects conserved sites more clearly. It has the additional advantage that all parameters are calculated based on statistical considerations. This allows for its successful application with any binding site model of interest. We present the algorithm and the approach for parameter selection and compare its performance with that of other, non-simultaneous methods on both simulated and real data. Availability: A command-line based C++ implementation of SimAnn {{is available from the}} authors upon request. In addition, we provide Perl scripts for calculating the input parameters based on statistical considerations...|$|R
40|$|THESIS ABSTRACT How do {{school districts}} prepare for {{large-scale}} standards-based educational reform? This {{study focused on}} one school district???s effort to plan and implement standards-based educational reform. Teachers from one elementary, one middle, and one {{high school in the}} district were surveyed to ascertain their perceptions of the district???s preparation and their own readiness to implement the Common Core State Standards (CCSS). The director of Curriculum and Instruction was interviewed to provide detailed analysis of the district???s overall framework and timeline for implementation of the CCSS reform initiative. The director???s answers were compared with results from teacher surveys to ascertain the effectiveness of the district???s plan as perceived by the teachers. To ensure successful implementation of such large-scale reforms as the CCSS, teacher perceptions and opinions, along with the district???s vision must be transparent, shared and understood to avoid miscommunication, inadequate planning and a resulting decrease in motivation and relationships. Findings from the research indicated collaborative time, availability of adequate and applicable materials as essential parts of successful implementation. Furthermore the uncertainty of online assessment, application of technology and curriculum <b>alignment</b> <b>posed</b> some of the most evident challenges for the district. While the study was small, focusing on three schools in one district, the implications of the findings and concurring recommendations could provide other school districts with pertinent information that may apply or be used for consideration in CCSS planning in all school districts with similar goals. School of Educatio...|$|R
40|$|International audienceFacial {{analysis}} {{is one of}} the challenges of a Human Machine Interactive (HMI) system. It includes face <b>alignment,</b> <b>pose</b> estimation, face recognition, facial expression detection etc. This paper proposes an efficient optimization technique by the hybridization of genetic algorithm (GA) with gradient descent (GD) to make a robust, efficient and real time face alignment system. It employs 2. 5 D Active Appearance Model (AAM) for the face search, in which the face model is generated by taking 3 D landmarks and 2 D texture of the facial image. Facial large lateral movements requires to optimize 6 DOF (Degrees of Freedom) pose parameters which make the facial search space of AAM non-convex. This non-convex multidimensional search space requires an efficient optimization methodology. Gradient descent also known as deterministic optimization method can optimize the parameters as long as the search space remains convex. However in non-convex search space GA is able search face globally while GD can help GA to search face locally. In other words exploitation property of GD and exploration property of GA are combined to make an efficient optimization method. For this hybrid optimization we propose a gradient operator in GA, which functions in conjunction with the existing genetic operator of mutation. Thus it does not increase the computational cost of the system. We compare it with classical search algorithm by gradient descent and the hybrid optimization of GA with Simplex. Facial databases of SUPELEC' 08, Pointing' 04 and synthetic characters comprising of different facial poses are analyzed. Simulation results validate the efficiency, accuracy and robustness achieved...|$|E
40|$|Restricted until 18 Nov. 2008. This {{dissertation}} aims at {{lowering the}} entrance threshold for laymen {{to contribute to}} the Semantic Web. To be more specific, it aims at lowering the difficulty for laymen to perform two basic and tightly related tasks on the Semantic Web: semantic data creation and ontology alignment.; The content of the current Web [...] plain texts mixed with HTML tags [...] is difficult for machines to interpret. Researchers from universities and industries have been working on the development of next generation Web [...] the Semantic Web. The Semantic Web aims at creating and connecting a web of machine-understandable semantic data, which allows for intelligent processing and could bring the Web to a higher level.; In order for the Semantic Web to succeed, it has to be easy for laymen to make contributions. It has to be easy for laymen to contribute semantic data and operate on heterogeneous data.; Semantic data is structured data marked up with ontology terms. A piece of semantic data specifying one's telephone number could be represented as (JohnSmith, O 1 : phoneNumber," 123 - 456 - 7890 "), where "phoneNumber" is an ontological term in ontology "O 1 ". Ontology alignment is the process of matching terms from different ontologies. For example, "phoneNumber" in one ontology corresponds to "telephoneNumber" in a second ontology.; The realization of the Semantic Web is dependent on the creation of massive amount of semantic data. Semantic data, being encoded in a structured form and marked up with ontologies, is much more machine-friendly and makes more intelligent processing feasible.; Aligning terms from different ontologies is the key to integrating heterogeneous semantic data in different ontologies. Given the openness of the Web, it is unrealistic to expect that all semantic data is created in the same ontology. Ontology alignment could help semantic data creation as well by supplying existing ontologies and semantic data in the intended domain.; Many tools have been proposed to create semantic data and align ontologies. However, the lack of semantic data is still the biggest problem with the current Semantic Web development. In addition, no alignment tool has gained widespread use among ordinary users, and there is little ontology alignment data available on the current Semantic Web.; In this dissertation, we argue that the conventional tools and techniques for semantic data creation and ontology <b>alignment</b> <b>pose</b> a high barrier of entry to laymen. We argue that conventional tools and techniques share a common characteristic: they are all topdown, ontology-based tools. As a result, they are difficult to use by laymen because of the inherent difficulty in dealing with ontologies: ontologies are abstract, generalized, high-level entities.; Instead, we proposed a bottom-up, data-centric approach to semantic data creation and ontology alignment. In bottom-up data creation, users create weakly-structured data first, and gradually refine their data. An ontology is derived as a summary of created data so far. In bottom-up ontology alignment, laymen, rather than ontology experts, align their semantic data (for their own purposes) within end applications, the inferred implicit and sometimes imprecise ontology alignments are then integrated and mined to produce higher accuracy ontology alignments. In both tasks, the difficulty of carrying out the task is significantly reduced, making it easier for laymen {{to contribute to the}} development of the Semantic Web...|$|E
40|$|In this thesis, {{a number}} of new schemes are {{presented}} which address current problems and shortcomings within the area of visual cryptography. Visual cryptography provides a very powerful means by which a secret, {{in the form of a}} digital image, can be distributed (encoded) into two or more pieces known as shares. When these shares are xeroxed onto transparencies and superimposed exactly together, the original secret can be recovered (decoded) without the necessity for computation. Traditionally, visual cryptography allows effective and efficient sharing of a single secret between {{a number of}} trusted parties. One aspect of the research within this thesis specifically addresses the issues of embedding more than two secrets within a set of two shares. <b>Alignment</b> <b>poses</b> a further problem. The placement of the shares must be specific. In order to ease ali~ent, the techniques developed within this thesis for sharing multiple secrets relaxes this restriction. The result is a scheme in which the shares can be superimposed upon one another in a multitude of positions and alignment styles which enables multiple secret recovery. Applications of visual cryptography are also examined and presented. This is an area within visual cryptography that has had very little attention in terms of research. The primary focus of the work presented within this thesis concentrates on applications of visual cryptography in real world scenarios. For such a simple and effective method of sharing secrets, practical applications are as yet, limited. A number of novel uses for visual cryptography are presented that use theoretical techniques in a practical way. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
3000|$|... is {{a generic}} 3 D object either {{belonging}} to the gallery or the probe set. Without any loss of generality 3 D models are characterized {{by a set of}} vertices which may be meshed in order to form a closed surface or compact manifold of intrinsic dimension two. Other notations and terminologies will be introduced as we go through different sections of this paper which is organized as follows. Section 2 introduces the <b>alignment</b> and <b>pose</b> normalization process. Section 3 presents the global and the local multiscale contour convexity/concavity signatures. The matching process together with pruning strategies are introduced in Section 4, ending with experiments and comparison on the Princeton Shape Benchmark and the very recent Shrec' 09 international benchmark in Section 5.|$|R
40|$|Abstract. We {{propose a}} {{homography}} estimation method from {{the contours of}} planar regions. Standard projective invariants such as cross ratios or canonical frames based on hot points obtained from local differential properties are extremely unstable in real images suffering from pixelization, thresholding artifacts, and other noise sources. We explore alternative constructions based on global convexity properties of the contour such as discrete tangents and concavities. We show that a projective frame can be robustly extracted from arbitrary shapes {{with at least one}} appreciable concavity. Algorithmic complexity and stability are theoretically discussed and experimentally evaluated in a number of real applications including projective shape matching, <b>alignment</b> and <b>pose</b> estimation. We conclude that the procedure is computationally efficient and notably robust given the ill-conditioned nature of the problem. ...|$|R
40|$|This chapter {{moves on}} to the next major topic: {{recognising}} individual stones from their silhouettes. The key idea is to apply the ET-based pose optimisation described in Chapter 6 to pairs of silhouette sets. If the residual ET error after <b>pose</b> <b>alignment</b> is sufficiently low, then the pair of silhouette sets is classified as a match (i. e., produced by the same stone); otherwise, the pair of silhouette sets is classified as a mismatc...|$|R
40|$|This paper {{presents}} a novel system to estimate body pose configuration {{from a single}} depth map. It combines both pose detection and pose refinement. The input depth map is matched {{with a set of}} pre-captured motion exemplars to generate a body configuration estimation, as well as semantic labeling of the input point cloud. The initial estimation is then refined by directly fitting the body configuration with the observation (e. g., the input depth). In addition to the new system architecture, our other contributions include modifying a point cloud smoothing technique to deal with very noisy input depth maps, a point cloud <b>alignment</b> and <b>pose</b> search algorithm that is view-independent and efficient. Experiments on a public dataset show that our approach achieves significantly higher accuracy than previous state-of-art methods. 1...|$|R
40|$|This paper {{describes}} a structural method for object <b>alignment</b> by <b>pose</b> clustering. The idea underlying pose clustering is to decompose the objects under consideration into k-tuples of primitive parts. By bringing pairs of k-tuples into correspondence, sets of alignment parameters are estimated. The global alignment {{corresponds to the}} set of parameters with maximum votes. The work reported here oers two novel contributions. Firstly, we impose structural constraints on {{the arrangement of the}} k-tuples of primitives used for pose clustering. This limits problems of combinatorial nature and eases the search for consistent pose clusters. Secondly, we use the EM algorithm to estimate maximum likelihood alignment parameters. Here we fit a mixture model to the set of transformation parameter votes. We control the order of the underlying mixture model using a minimum description length criterion. The new alignment method is illustrate...|$|R
40|$|Elucidating the {{mechanisms}} of transcriptional regulation relies heavily on the sequence annotation of the binding sites of DNA-binding proteins called transcription factors. With the rationale that binding sites conserved across di erent species {{are more likely to}} be functional, the standard approach is to employ cross-species comparisons and focus the search to conserved regions. Usually, computational methods that annotate conserved binding sites perform the alignment and binding site annotation steps separately and combine the results in the end. If the binding site descriptions are weak or the sequence similarity is low, the local gap structure of the <b>alignment</b> <b>poses</b> a problem in detecting the conserved sites. In this thesis, we introduce a novel method that integrates the two axes of sequence conservation and binding site annotation in a simultaneous approach yielding annotated alignments – pairwise alignments with parts annotated as putative conserved transcription factor binding sites. Standard pairwise alignments are extended to include additional states for binding site profiles. A statistical framework that estimates profile-related parameters based on desired type I and type II errors is prescribed. This forms the core of the tool SimAnn. As an extension, we use existing probabilistic models to demonstrate how the framework can be adapted to consider position-specific evolutionary characteristics of binding sites during parameter estimation. This underlies the tool eSimAnn. Through simulations and real data analysis, we study the influence of considering a simultaneous approach as opposed to a multi-step one on resulting predictions. The former enables a local rearrangement in the alignment structure to bring forth perfectly aligned binding sites. This precludes the necessity of adopting post-processing steps to handle errors in pre-computed alignments, as is usually done in multi-step approaches. Additionally, the framework for parameter estimation is applicable to any novel profile of interest. Especially for instances with poor sequence conservation or profile quality, the simultaneous approach stands out. As a by-product of the analysis, we also model the annotated alignment problem as an extended pair Hidden Markov Model and illustrate the correspondence between the various theoretical concepts...|$|R
40|$|In this paper, a novel {{approach}} for fast protein-protein docking based on geometric complementarity is introduced. Complementarity matching is achieved using a rotation-invariant 3 D shape descriptor, the Shape Impact Descriptor (SID). Rotation invariance enables matching of equally-sized surface patches {{without the need for}} initial <b>alignment.</b> The candidate <b>poses</b> are computed using an intuitive alignment method, which is much faster than exhaustively searching the translational and rotational space of the ligand. The method yields competitive results when compared to other well-known geometry-based, rigid-docking approaches. © 2011 IEEE...|$|R
40|$|In {{this paper}} we present an {{approach}} for 3 D face recognition from range data {{based on the}} principal curvature, kmax, and Hausdorff distance. We use the principal curvature, kmax, to represent the face image as a 3 D binary image called ridge image. The ridge image shows {{the locations of the}} ridge lines around the important facial regions on the face (i. e. the eyes, the nose, and the mouth). We utilize Hausdorff distance to match the ridge image of a given probe to the created ridge images of the subjects in the gallery. For <b>pose</b> <b>alignment,</b> we extract the locations of three feature points, the inner corners of the two eyes and the tip of the nose using Gaussian curvature. These three feature points plus an auxiliary point {{in the center of the}} triangle, made by averaging the coordinates of the three feature points, are used for initial 3 D face alignment. In the face recognition stage, we find the optimum <b>pose</b> <b>alignment</b> between the probe image and the gallery, which gives the minimum Hasusdorff distance between the two sets of features. This approach is used for identification of both neutral faces and faces with smile expression. Experiments on a public face database of 61 subjects resulted in 93. 5 % ranked one recognition rate for neutral expression and 82. 0 % for the faces with smile expression...|$|R
40|$|Abstract. While {{the problem}} of grain <b>alignment</b> was <b>posed</b> more than 60 years ago the {{quantitative}} model of grain alignment that can account for the observed polarization arising from aligned grains has been formulated only recently. The quantitative predictions of the radiative torque mechanism, which is currently accepted as the dominant mechanism of grain alignment, open avenues to tracing magnetic fields in various astrophysical environments, including diffuse and dense interstellar gas, molecular clouds, circumstellar environments, accretion disks, comet tails, Zodiacal dust etc. At the same time, measurements of the absolute value of polarization and its variations can, in addition, provide unique information about the dust composition and dust environment. In the review I describe the analytical model describing well radiative torques acting on irregular grains and discuss how the alignment induced by radiative torques varies {{in the presence of}} superparamagnetic inclusions and pinwheel torques, e. g. arising from the H 2 formation over grain surface. I also describe observations that can establish whether grains are superparamagnetic and whether recoil...|$|R
40|$|In {{contrast}} to comparing faces via single exemplars, matching sets of face images increases robustness and discrimination performance. Recent image set matching approaches typically measure similarities between subspaces or manifolds, while representing faces in a rigid and holistic manner. Such representations are easily affected by variations {{in terms of}} <b>alignment,</b> illumination, <b>pose</b> and expression. While local feature based representations are considerably more robust to such variations, they have received little attention within the image set matching area. We propose a novel image set matching technique, comprised of three aspects: (i) robust descriptors of face regions based on local features, partly inspired by the hierarchy in the human visual system, (ii) use of several subspace and exemplar metrics to compare corresponding face regions, (iii) jointly learning which regions are the most discriminative while finding the optimal mixing weights for combining metrics. Experiments on LFW, PIE and MOBIO face datasets show that the proposed algorithm obtains considerably better performance than several recent state-of-theart techniques, such as Local Principal Angle and the Kernel Affine Hull Method. 1...|$|R
40|$|We {{present a}} {{framework}} for pose-invariant face recognition using parametric linear subspace models as stored representations of known individuals. Each model can be fit to an input, resulting in faces of known people whose head pose is aligned to the input face. The model's continuous nature enables the <b>pose</b> <b>alignment</b> to be very accurate, improving recognition performance, while its generalization to unknown poses enables the models to be compact. As a demonstration, recognition systems with two types of parametric linear model are compared using a database of 20 persons. The experimental results showed our system's robust recognition of faces with +- 50 degree range of full 3 D head rotation, while compressing the data bya factor of 20 and more...|$|R
40|$|Abstract. To {{assemble}} easily damaged micro-parts {{safely in}} interference fit way, the assembly process is designed {{and the interaction}} force in assembly process is analyzed. The pose of micro-parts is aligned based on calibration of three microscopic vision systems from different directions. In addition, a control strategy based on feedback of vision and force is proposed to assemble safely. In the case of two micro-parts just contact, position of micro-part is adjusted based on force information to compensate <b>pose</b> <b>alignment</b> error and keep force in safe range. When micro-part is deformed as increased contact force, position of micro-part is adjusted based on force in the interference fit process and deformation of micro-part. Experimental results demonstrate the effectiveness of proposed methods. 1...|$|R
40|$|Abstract. Discriminative methods (e. g., kernel regression, SVM) {{have been}} {{extensively}} used {{to solve problems}} such as object recognition, image <b>alignment</b> and <b>pose</b> estimation from images. Regression methods typically map image features (X) to continuous (e. g., pose) or discrete (e. g., object category) values. A major drawback of existing regression methods is that samples are directly projected onto a subspace and hence fail to account for outliers which are common in realistic training sets due to occlusion, specular reflections or noise. It is important to notice that in existing regression methods, and discriminative methods in general, the regressor variables X {{are assumed to be}} noise free. Due to this assumption, discriminative methods experience significant degrades in performance when gross outliers are present. Despite its obvious importance, the problem of robust discriminative learning has been relatively unexplored in computer vision. This paper develops the theory of Robust Regression (RR) and presents an effective convex approach that uses recent advances on rank minimization. The framework applies to a variety of problems in computer vision including robust linear discriminant analysis, multi-label classification and head pose estimation from images. Several synthetic and real world examples are used to illustrate the benefits of RR. Key words: Robust methods, errors in variables, intra-sample outliers...|$|R
40|$|Abstract—One goal of {{statistical}} shape analysis is the discrimination between two populations of objects. Whereas traditional shape analysis was mostly concerned with single objects, analysis of multi-object complexes presents new challenges related to <b>alignment</b> and <b>pose.</b> In this paper, {{we present a}} methodology for discriminant analysis of multiple objects represented by sampled medial manifolds. Non-euclidean metrics that describe geodesic distances between sets of sampled representations are used for alignment and discrimination. Our choice of discriminant method is the distance-weighted discriminant because of its generalization ability in high-dimensional, low sample size settings. Using an unbiased, soft discrimination score, we associate a statistical hypothesis test with the discrimination results. We explore the effectiveness of different choices of features as input to the discriminant analysis, using measures like volume, pose, shape, and the combination of pose and shape. Our method is applied to a longitudinal pediatric autism study with 10 subcortical brain structures in a population of 70 subjects. It is shown that the choices of type of global alignment and of intrinsic versus extrinsic shape features, the latter being sensitive to relative pose, are crucial factors for group discrimination and also for explaining the nature of shape change {{in terms of the}} application domain. Index Terms—Shape, size and shape, shape analysis. Ç...|$|R
40|$|Discriminative methods (e. g., kernel regression, SVM) {{have been}} {{extensively}} used {{to solve problems}} such as object recognition, image <b>alignment</b> and <b>pose</b> estimation from images. These methods typically map image features (X) to continuous (e. g., pose) or discrete (e. g., object category) values. A major drawback of existing discriminative methods is that samples are directly projected onto a subspace and hence fail to account for outliers common in realistic training sets due to occlusion, specular reflections or noise. It is important to notice that existing discriminative approaches assume the input variables X to be noise free. Thus, discriminative methods experience significant performance degradation when gross outliers are present. Despite its obvious importance, the problem of robust discriminative learning has been relatively unexplored in computer vision. This paper develops the theory of robust regression (RR) and presents an effective convex approach that uses recent advances on rank minimization. The framework applies {{to a variety of}} problems in computer vision including robust linear discriminant analysis, regression with missing data, and multi-label classification. Several synthetic and real examples with applications to head pose estimation from images, image and video classification and facial attribute classification with missing data are used to illustrate the benefits of RR...|$|R
