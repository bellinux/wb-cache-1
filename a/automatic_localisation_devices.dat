0|49|Public
3000|$|... [...]. The results {{reported}} in this paper have been obtained by applying a geometric face normalisation based on the eyes positions. The eyes positions were localised either manually or automatically. A fast method of face detection and eyes localisation {{was used for the}} <b>automatic</b> <b>localisation</b> of eyes centre [26]. The XM 2 VTS database [27] was used for calculating the LDA and PCA projection matrices.|$|R
40|$|In this paper, a {{regional}} minimal cost approach without parameters {{is used for}} contour tracking with a good robustness. Dynamic programming is exploited for its efficiency. This general method {{is applied to the}} extraction of the cranial contour on higth resolution X-Ray image. Ellipse is then fitted on the extracted contour to represent it as a first step for <b>automatic</b> <b>localisation</b> of cephalometric points. This method is tested on 424 X-Ray images, with different acquisition parameters...|$|R
30|$|Gawel et al. [5] {{introduced}} {{a method for}} automatic segmentation of vertebral column tissue based on machine learning with cascade classifiers, active appearance model and principal component analysis. Further approaches have been reported for <b>automatic</b> <b>localisation</b> and segmentation of vertebral bodies on MRI. For instance, Chu et al. [6] used a random forest regression and classification framework and Hille et al. [7] used computed appearance-based vertebral body probability maps with a subsequent hybrid level-set segmentation. Available segmentation methods have been summarised in a review by Rak et al. [8].|$|R
40|$|International audienceCross-subject {{analysis}} of structural and/or functional images often requires accurate localisation of cortical convolutions (sulci and gyri). This is particularly relevant {{to better understand}} the anatomical/functional relationships within the brain and to increase accuracy in neuro-surgical planning. Internal brain structures are easily identified on high resolution MRI, using the Talairach space, thanks to their low variability. Unfortunately, localisation and identification of sulci are very difficult tasks because of the high variability of cortical structures. In this work, we propose an approach for <b>automatic</b> <b>localisation</b> and identification of sulci on MR images...|$|R
40|$|Improving the {{feedback}} {{quality of a}} computer-based system for pronunciation training requires rather detailed and precise knowledge about the place {{and the nature of}} actual mispronunciations in a student's utterance. To be able to provide this kind of information, components for the <b>automatic</b> <b>localisation</b> and correction of pronunciation errors have been developed. This work was part of a project aimed at integrating state-of-the-art speech recognition technology into a pronunciation training environment for adult, intermediate level learners. Although the technologies described here are in principle valid for any language pairs, the current system focuses on Italian and German learners of English. 1...|$|R
40|$|The direct {{measurement}} of local strains during a dynamic loading is proposed from the spectral {{analysis of a}} crossed grating using optical diffraction under oblique incidence This procedure is experimentally achieved with acousto-optic modulators which allow to record 24 strain states at a maximum frequency near to 1 MHz. The diffraction figures can be recorded on a photographic film or directly on a C. C. D. camera. The <b>automatic</b> <b>localisation</b> of the diffracted spots {{with the help of}} data image processing gives the components of the strain tensor. The strain measurement method is performed for compression impact tests using an Hopkinson bar loading...|$|R
40|$|Chinese fengshui {{evolved from}} a {{classical}} tradition aimed at achieving harmony within the environment. However, case studies and fieldwork undertaken in this thesis reveal that in contemporary architecture, fengshui is often used as a Chinese <b>localisation</b> <b>device</b> in {{a significant departure from}} classical practice. This thesis analyses examples of Chinese cultural identification within transnational architecture in Hong Kong and Shanghai, during the 20 th and early 21 st centuries. The outcome provides architects and urban designers with a new perspective on the contemporary use of fengshui in China...|$|R
30|$|Concerning the {{treatment}} response prediction with DWI, {{in the light}} of good results obtained by using whole-volume tumour segmentations [32, 43, 44], a growing interest has developed in the literature to explore the feasibility of automatic segmentation tools. The main advantage of this approach would be that manual segmentation of the whole tumour is a time-consuming procedure during the everyday clinical practice, requiring up to 18  min [45] and high-level experience. Trebeschi et al. [46] used deep learning methods (convolutional neural networks) to obtain an <b>automatic</b> <b>localisation</b> and segmentation of rectal cancer on multiparametric MRI with promising results. In detail, in this study, the AUC of the probability maps resulting from the validation of the computational neural network classifier reached 0.99.|$|R
40|$|The Gaussian plume {{model is}} the core of most {{regulatory}} atmospheric dispersion models. The parameters of the model include the source characteristics (e. g. location, strength) and environmental parameters (wind speed, direction, atmospheric stability conditions). The paper presents a theoretical analysis of the best achievable accuracy in estimation of Gaussian plume parameters {{in the context of a}} continuous point-source release and using a binary sensor network for acquisition of measurements. The problem is relevant for <b>automatic</b> <b>localisation</b> of atmospheric pollutants with applications in public health and defence. The theoretical bounds of achievable accuracy provide a guideline for sensor network deployment and its performance under various environmental conditions. The bounds are compared with empirical errors obtained using a Markov chain Monte Carlo (MCMC) parameter estimation technique...|$|R
40|$|Introduction Cross-subject {{analysis}} of structural and/or functional images often requires accurate localisation of cortical convolutions (sulci and gyri). This is particularly relevant {{to better understand}} the anatomical/functional relationships within the brain and to increase accuracy in neuro-surgical planning. Internal brain structures are easily identified on high resolution MRI, using the Talairach space, thanks to their low variability. Unfortunately, localisation and identification of sulci are very difficult tasks because of the high variability of cortical structures. In this work, we propose an approach for <b>automatic</b> <b>localisation</b> and identification of sulci on MR images. Methods Brain is automatically extracted and segmented into cerebro-spinal fluid (CSF), grey matter and white matter using morphological operators (thresolding, filtering, erosion, reconstruction, dilatation). 3 D skeletonization and curve thinning are applied on CSF. Sulci are then described as 3 D curve...|$|R
40|$|In {{this paper}} {{we present a}} system for the virtual {{reconstruction}} of 3 D scenes from range images. For this purpose a laser range camera is mounted on a suitable device (tripod, rotary table) that allows an exact orientation at given angles. The camera is then rotated horizontally around it's optical center {{and a set of}} overlapping range images is acquired. The images are translated into sets of three-dimensional points in a common coordinate system according to their original viewing angle, followed by a fine-registration step that uses a derivative of the widely-used ICP algorithm. Finally, the resulting sets are integrated into a VRML model for visualisation. The results of this system can serve as a pre-requisite for the <b>automatic</b> <b>localisation</b> and recognition of objects in an environment...|$|R
40|$|The {{paper will}} {{introduce}} into {{the subject of}} recognition of typical patterns in road networks. We will first describe the design and lay-out of roads. Applications to detect patterns and to use them for finding more knowledge in vector data are shown. We will familiarise the reader with different patterns in road networks and approaches for the automatic detection of those patterns in vector data. The objective of the present work consists in the <b>automatic</b> <b>localisation</b> of a city centre, which {{can be seen as}} one possibility to derive new knowledge by analysing existing information. We will show, that the combination and interaction of simple patterns which will be described in the following sections can lead to higher-ranking patterns or explicit new information, which is hidden in the data. 1...|$|R
40|$|<b>Automatic</b> <b>localisation</b> of correspondences for the {{construction}} of Statistical Shape Models from examples {{has been the focus of}} intense research during the last decade. Several algorithms are available and benchmarking is needed to rank the different algorithms. Prior work has focused on evaluating the quality of the models produced by the algorithms by measuring compactness, generality and specificity. In this paper problems with these standard measures are discussed. We propose that a ground truth correspondence measure (gcm) is used for benchmarking and in this paper benchmarking is performed on several state of the art algorithms. Minimum Description Length (MDL) with a curvature cost comes out as the winner of the automatic methods. Hand marked models turn out to be best but a semi-automatic method is shown to lie in between the best automatic method and the hand built models in performance...|$|R
40|$|<b>Automatic</b> <b>localisation</b> of correspondences for the {{construction}} of Statistical Shape Models from examples {{has been the focus of}} intense research during the last decade. Several algorithms are available and benchmarking is needed to rank the different algorithms. Prior work has argued that the quality of the models produced by the algorithms can be evaluated by measuring compactness, generality and specificity. In this paper severe problems of these measures are discussed and a new measure called Ground truth Correspondence Measure (gcm) is instead proposed for benchmarking. The key point with gcm is that from a database with known true correspondences, algorithms can be evaluated by measuring how close the correspondences produced by the algorithms are to the true correspondences. Minimum Description Length (MDL) with a curvature cost comes out as the winner of the automatic methods. Handmarked models turn out to be best but a semiautomatic method is shown to be nearly as good...|$|R
40|$|In {{this work}} we propose a novel {{approach}} to perform segmentation by leveraging the abstraction capabilities of convolutional neural networks (CNNs). Our method is based on Hough voting, a strategy that allows for fully <b>automatic</b> <b>localisation</b> and segmentation of the anatomies of interest. This approach does not only use the CNN classification outcomes, but it also implements voting by exploiting the features produced by the deepest portion of the network. We show that this learning-based segmentation method is robust, multi-region, flexible and can be easily adapted to different modalities. In the attempt to show the capabilities and the behaviour of CNNs when they are applied to medical image analysis, we perform a systematic study of the performances of six different network architectures, conceived according to state-of-the-art criteria, in various situations. We evaluate the impact of both different amount of training data and different data dimensionality (2 D, 2. 5 D and 3 D) on the final results. We show results on both MRI and transcranial US volumes depicting respectively 26 regions of the basal ganglia and the midbrain...|$|R
40|$|Facing {{the dynamic}} field of {{warehouse}} logistics {{and the rising}} customer process demands combined with cost pressure then a constant change management and continuous, customer-oriented improvement process must take place in 3 PL warehouse business. Hence technological change, especially in regards of <b>automatic</b> object <b>localisation</b> and product tracking along the supply chain, gains in importance. That raises the question if a change from barcode technology to radio frequency identification generates profitability in 3 PL warehouse busines...|$|R
40|$|Abstract — Dynamic contrast-enhanced {{magnetic}} resonance imaging (DCE-MRI) has become {{an important source of}} information to aid cancer diagnosis. Nevertheless, due to the multitemporal nature of the 3 D volume data obtained from DCE-MRI, evaluation of the image data is a challenging task and tools are required to support the human expert. We investigate an approach for <b>automatic</b> <b>localisation</b> and characterisation of suspicious lesions in DCE-MRI data. It applies an artificial neural network (ANN) architecture which combines unsupervised and supervised techniques for voxel-by-voxel classification of temporal kinetic signals. The algorithm is easy to implement, allows for fast training and application even for huge data sets and can be directly used to augment the display of DCE-MRI data. To demonstrate that the system provides a reasonable assessment of kinetic signals, the outcome is compared with the results obtained from the model-based three-time-points (3 TP) technique which represents a clinical standard protocol for analysing breast cancer lesions. The evaluation based on the DCE-MRI data of twelve cases indicates that, although the ANN is trained with imprecisely labelled data, the approach leads to an outcome conforming with 3 TP without presupposing an explicit model of the underlying physiological process...|$|R
40|$|Abstract—In In this paper, {{algorithms}} for the <b>automatic</b> <b>localisation</b> of two anatomical {{soft tissue}} landmarks of the head, the medial canthus (inner {{corner of the}} eye) and the tragus (a small, pointed, cartilaginous flap of the ear), in CT images are described. These landmarks are {{to be used as}} a basis for an automated image-to-patient image registration system we are developing. The landmark andmarks are localised on a surface model extracted from CT images, based on surface curvature and a rule based system that incorporates prior knowledge of the landmark characteristics. The approach was tested on a dataset of near isotropic CT images of 95 patients. The position of the automatically localised landmarks was compared to the position of the manually localised landmarks. The average difference was 1. 5 mm and 0. 8 mm for the medial canthus and tragus, with a maximum difference of 4. 5 mm and 2. 6 mm m respectively. The medial canthus and tragus can be automatically localised in i CT images, with performance comparable mparable to manual localisation. M. Ovinis is a PhD scholar in the Department of Mechanical an...|$|R
40|$|Colloque avec actes et comité de lecture. internationale. International audienceThis paper {{presents}} a vision based localization system {{which can be}} used {{on a wide range of}} mobile vehicles. The principle of the algorithm developed is to merge the information given by a vision system with the data extracted from the moves of the vehicle. The image processing level is performed by using principal components analysis that allows low cost position estimation by using a representative set of images obtained during an initial exploration of the environment. This set of images makes it possible to represent the environment as a partially observable Markov decision process. The originality of this approach is the resulting data fusion process that uses both image matching and decision made by the robot in order to estimate the set of plausible positions of the robot and the associated probabilities. Furthermore, this stochastic <b>localisation</b> <b>device</b> shows better results compared with the classical methods such as static neighborhoods. The main characteristics of this localization device are its robustness, its accuracy and its low cost compared with usual localization methods...|$|R
40|$|Research question: Facing {{the dynamic}} field of {{warehouse}} logistics {{and the rising}} customer process demands combined with cost pressure then a constant change management and continuous, customer-oriented improvement process must take place in 3 PL warehouse business. Hence technological change, especially in regards of <b>automatic</b> object <b>localisation</b> and product tracking along the supply chain, gains in importance. That raises the question if a change from barcode technology to radio frequency identification generates profitability in 3 PL warehouse business. Methods: Based {{on the principles of}} continuous process improvement, chang...|$|R
40|$|Purpose: Computer {{vision has}} been widely used in the {{inspection}} of electronic components. This paper proposes a computer vision system for the <b>automatic</b> detection, <b>localisation,</b> and segmentation of solder joints on Printed Circuit Boards (PCBs) under different illumination conditions. Design/methodology/approach: An illumination normalization approach is applied to an image, which can effectively and efficiently eliminate the effect of uneven illumination while keeping {{the properties of the}} processed image the same as in the corresponding image under normal lighting conditions. Consequently special lighting and instrumental setup can be reduced in order to detect solder joints. These normalised images are insensitive to illumination variations and are used for the subsequent solder joint detection stages. In the segmentation approach, the PCB image is transformed from an RGB color space to a YIQ color space for the effective detection of solder joints from the background. Findings: The segmentation results show that the proposed approach improves the performance significantly for images under varying illumination conditions. Research limitations/implications: This paper proposes a front-end system for the <b>automatic</b> detection, <b>localisation,</b> and segmentation of solder joint defects. Further research is required to complete the full system including the classification of solder joint defects. Practical implications: The methodology presented in this paper can be an effective method to reduce cost and improve quality in production of PCBs in the manufacturing industry. Originality/value: This research proposes the automatic location, identification and segmentation of solder joints under different illumination conditions...|$|R
40|$|Identification of {{anatomical}} landmarks on skeletal tissue reconstructed from CT/MR {{images is}} indispensable in patient-specific preoperative planning (turnout referencing, deformity evaluation, resection planning, and implant alignment and anchoring) {{as well as}} intra-operative navigation (bone registration and instruments referencing). Interactive localisation of landmarks on patient-specific anatomical models is time-consuming and may lack in repeatability and accuracy. We present a computer graphics-based method for <b>automatic</b> <b>localisation</b> and identification (labelling) of anatomical landmarks on a 3 D model of bone reconstructed from CT images of a patient. The model surface is segmented into different landmark regions (peak, ridge, pit and ravine) based on surface curvature. These regions are labelled automatically by an iterative process using a spatial adjacency relationship matrix between the landmarks. The methodology has been implemented in a software program and its results (automatically identified landmarks) are compared with those manually palpated by three experienced orthopaeclic surgeons, on three 3 D reconstructed bone models. The variability in location of landmarks {{was found to be}} in the range of 2. 15 - 5. 98 mm by manual method (inter surgeon) and 1. 92 - 4. 88 mm by our program. Both methods performed well in identifying sharp features. Overall, the performance of the automated methodology was better or similar to the manual method and its results were reproducible. It is expected to have a variety of applications in surgery planning and intra-operative navigation. (C) 200...|$|R
40|$|Airborne thermal {{infrared}} (TIR) imaging systems are being increasingly used for wild fire tactical monitoring since they show important advantages over spaceborne platforms and visible sensors while becoming much more affordable and much lighter than multispectral cameras. However, {{the analysis of}} aerial TIR images entails a number of difficulties which have thus far prevented monitoring tasks from being totally automated. One of these issues {{that needs to be}} addressed is the appearance of flame projections during the geo-correction of off-nadir images. Filtering these flames is essential in order to accurately estimate the geographical location of the fuel burning interface. Therefore, we present a methodology which allows the <b>automatic</b> <b>localisation</b> of the active fire contour free of flame projections. The actively burning area is detected in TIR georeferenced images through a combination of intensity thresholding techniques, morphological processing and active contours. Subsequently, flame projections are filtered out by the temporal frequency analysis of the appropriate contour descriptors. The proposed algorithm was tested on footages acquired during three large-scale field experimental burns. Results suggest this methodology may be suitable to automatise the acquisition of quantitative data about the fire evolution. As future work, a revision of the low-pass filter implemented for the temporal analysis (currently a median filter) was recommended. The availability of up-to-date information about the fire state would improve situational awareness during an emergency response and may be used to calibrate data-driven simulators capable of emitting short-term accurate forecasts of the subsequent fire evolution. Postprint (author's final draft...|$|R
40|$|An MR-only {{radiotherapy}} planning (RTP) workflow {{would reduce}} the cost, radiation exposure and uncertainties introduced by CT-MRI registrations. In the case of prostate treatment, one of the remaining challenges currently holding back the implementation of an RTP workflow is the MR-based localisation of intraprostatic gold fiducial markers (FMs), which is crucial for accurate patient positioning.   Currently, MR-based FM localisation is clinically performed manually. This is sub-optimal, as manual interaction increases the workload. Attempts to perform automatic FM detection often rely {{on being able to}} detect signal voids induced by the FMs in magnitude images. However, signal voids may not always be sufficiently specific, hampering accurate and robust <b>automatic</b> FM <b>localisation.</b>   Here, we present an approach that aims at <b>automatic</b> MR-based FM <b>localisation.</b> This method is based on template matching using a library of simulated complex-valued templates, and exploiting the behaviour of the complex MR signal {{in the vicinity of the}} FM. Clinical evaluation was performed on seventeen prostate cancer patients undergoing external beam radiotherapy treatment. <b>Automatic</b> MR-based FM <b>localisation</b> was compared to manual MR-based and semi-automatic CT-based localisation (the current gold standard) in terms of detection rate and the spatial accuracy and precision of localisation. The proposed method correctly detected all three FMs in 15 / 17 patients. The spatial accuracy (mean) and precision (STD) were 0. 9 mm and 0. 5 mm respectively, which is below the voxel size of 1. 1 × 1. 1 × 1. 2 mm 3 and comparable to MR-based manual localisation. FM localisation failed (3 / 51 FMs) in the presence of bleeding or calcifications in the direct vicinity of the FM.   The method was found to be spatially accurate and precise, which is essential for clinical use. To overcome any missed detection, we envision the use of the proposed method along with verification by an observer. This will result in a semi-automatic workflow facilitating the introduction of an MR-only workflow...|$|R
40|$|Lip {{reading is}} used to {{understand}} or interpret speech without hearing it, a technique especially mastered by people with hearing difficulties. The ability to lip read enables {{a person with a}} hearing impairment to communicate with others and to engage in social activities, which otherwise would be difficult. Recent advances in the fields of computer vision, pattern recognition, and signal processing has led to a growing interest in automating this challenging task of lip reading. Indeed, automating the human ability to lip read, a process referred to as visual speech recognition, could open the door for other novel applications. This thesis investigates various issues faced by an automated lip-reading system and proposes a novel "visual words" based approach to automatic lip reading. The proposed approach includes a novel <b>automatic</b> face <b>localisation</b> scheme and a lip localisation method...|$|R
40|$|Indoor {{localisation}} {{based on}} ubiquitous WLAN has exhibited {{the capability of}} being a cheap and relatively precise technology and has been verified by many successful examples. Its performance is subject to change due to multipath propagation {{and changes in the}} environment (people, building layouts, antenna characteristics etc.) which cannot be easily eliminated. This thesis addresses the <b>automatic</b> <b>localisation</b> of indoor user and proposes solutions for both positioning and seamlessly tracking a user using WLAN technology in addition to image sensing. By fusing these modalities we obtain better performance than using them individually. A fusion function designed to merge both analysis results into one semantic interpretation of user location is presented. Also a tracking approach based on an adaptive function that converts times between locations into probabilities and employs a Viterbi-based solution is proposed. An indoor localisation algorithm is described which is based on the creation of a database of WLAN signal strengths at pre-chosen calibration points (CPs). The need for fewer CPs than in standard methods is achieved due to the use of a novel interpolation algorithm, based on the specification of robust range and angle-dependent likelihood functions that describe the probability of a user being in the vicinity of each CP. The actual location of the user is estimated by solving a system of equations with two unknowns derived for a pair of CPs. Different pairs of CPs can be chosen to make several estimates which can then be combined to increase the accuracy of the estimate. The effectiveness of the fusion and the tracking approaches is evaluated on a very challenging dataset throughout a university building. Results that are presented demonstrate high accuracy that can be achieved. The methods are compared to several competing localisation methods and are shown to give superior results. The potential usefulness of this work is envisaged in a range of ambient assisted living applications including lifelogging and as an assistive technology for the memory or the visually impaired...|$|R
40|$|Abstract. The <b>automatic</b> {{subcellular}} <b>localisation</b> {{of proteins}} in living cells {{is a critical}} step in determining their function. The evaluation of fluorescence images constitutes a common method of localising these proteins. For this, additional knowledge about {{the position of the}} con-sidered cells within an image is required. In an automated system, it is advantageous to recognise these cells in bright-field microscope images taken in parallel with the regarded fluorescence micrographs. Unfortu-nately, currently available cell recognition methods are only of limited use within the context of protein localisation, since they frequently re-quire microscopy techniques that enable images of higher contrast (e. g. phase contrast microscopy or additional dyes) or can only be employed with too low magnifications. Therefore, this article introduces a novel approach to the robust automatic recognition of unstained living cells in bright-field microscope images. Here, {{the focus is on the}} automatic segmentation of cells. ...|$|R
40|$|Data {{collected}} by statistical offices generally contain errors, which {{have to be}} corrected before reliable data can be published. This correction process {{is referred to as}} statistical data editing. At statistical offices, certain rules, so-called edits, are often used during the editing process to determine whether a record is consistent or not. Inconsistent records are considered to contain errors, while consistent records are considered error-free. In this article we focus on <b>automatic</b> error <b>localisation</b> based on the Fellegi-Holt paradigm, which says that the data should be made to satisfy all edits by changing the fewest possible number of fields. Adoption of this paradigm leads to a mathematical optimisation problem. We propose an algorithm for solving this optimisation problem for a mix of categorical, continuous and integer-valued data. We also propose a heuristic procedure based on the exact algorithm. For five realistic data sets involving only integer-valued variables we evaluate the performance of this heuristic procedure. Peer Reviewe...|$|R
40|$|Unilateral severe-to-profound hearing loss, or {{single-sided}} deafness (SSD), impairs listening abilities {{supported by}} the use of two ears, including speech perception in background noise and sound <b>localisation.</b> Hearing-assistive <b>devices</b> can aid listening by re-routing sounds from the impaired to the non-impaired ear or by restoring input to the impaired ear. A systematic review of the literature examined the impact of hearing-assistive devices on the health-related quality of life (HRQoL) of adults with SSD as measured using generic and disease-specific instruments. A majority of studies used observational designs, {{and the quality of the}} evidence was low to moderate. Only two studies used generic instruments. A mixed-effect meta-analysis of disease-specific measures suggested that hearing-assistive devices have a small-to-medium impact on HRQoL. The Speech, Spatial and Qualities of Hearing Scale and the Health Utilities Index Mark 3 (HUI 3) were identified as instruments that are sensitive to device-related changes in disease-specific and generic HRQoL, respectively...|$|R
40|$|The SARBAU {{project is}} a study on a highly self config-uring {{building}} automation and control network using IP as field level protocol. UPnP {{will be used for}} device op-eration and control. The paper gives an overview on the SARBAU approach. Current work-in-progress including wired and wireless experimental device hardware is pre-sented. A focus of this paper is “nearly automatic config-uration”. Using intelligent commissioning software and optional <b>device</b> <b>localisation,</b> we propose highly automated device commissioning and binding schemes. 1. Background Building automation and control (BAU) systems are commonly employed in commercial buildings to automate light, heating and other control. Besides the fact that com...|$|R
30|$|Interventional oncology, the {{youngest}} and most rapidly growing offshoot of interventional radiology, has successfully established itself as an essential and independent pillar within the firmament of multidisciplinary oncologic care, alongside medical, surgical and radiation oncology [1]. Interventional oncology deals with the {{diagnosis and treatment of}} cancer and cancer-related problems using targeted minimally invasive procedures performed under image guidance. Among these, during the past 25  years, image-guided thermal ablation has been validated and increasingly used for the treatment of neoplastic diseases because of its low invasiveness, efficacy, repeatability and low cost [2]. However, precise image guidance is {{critical to the success of}} interventional oncology procedures. Navigational tools can enhance the interventional precision by improving <b>localisation</b> of <b>devices</b> in relation to the target. Currently available navigational tools for interventional radiology include electromagnetic, optical, laser and robotic guidance systems as well as image fusion platforms [3]. Such automation, navigation and visualisation tools may eventually optimise needle-based ablation procedures and decrease variability among operators, thus facilitating the diffusion of novel image-guided therapies [4].|$|R
40|$|The {{potential}} {{capabilities of}} resonators {{based on two}} dimensional arrays of re-entrant posts is demonstrated. Such posts may be regarded as magnetically-coupled lumped element microwave harmonic oscillators, arranged in a 2 D lattices structure, which is enclosed in a 3 D cavity. By arranging these elements in certain 2 D patterns, we demonstrate how to achieve certain requirements with respect to field <b>localisation</b> and <b>device</b> spectra. Special {{attention is paid to}} symmetries of the lattices, mechanical tuning, design of areas of high localisation of magnetic energy, which in turn creates unique discrete mode spectra. We demonstrate analogies between systems designed on the proposed platform and well known physical phenomena such as polarisation, frustration and Whispering Gallery Modes. The mechanical tunability of the cavity with multiple posts is analysed and its consequences to optomechanical applications is calculated. One particular application to quantum memory is demonstrated with a cavity design consisting of separate resonators analogous to discrete Fabry-Pérot resonators. Finally, we propose a generalised approach to a microwave system design based on the concept of Programmable Cavity Arrays...|$|R
40|$|Over {{the last}} years {{adaptive}} Web personalization has become a widespread service and all the major players of the WWW are providing it in various forms. Ephemeral personalization, in particular, deals with short time interests which are often tacitly entailed from user browsing behaviour or contextual information. Such personalization can be found almost anywhere in the Web in several forms, ranging from targeting advertising to <b>automatic</b> language <b>localisation</b> of content. In order to present personalized content a user model is typically built and maintained at server-side by collecting, explicitly or implicitly, user data. In the case of ephemeral personalization this means storing at server-side {{a huge amount of}} user behaviour data, which raises severe privacy concerns. The evolution of the semantic Web and the growing availability of semantic metadata embedded in Web pages allow a role reversal in the traditional personalization scenario. In this paper we present a novel approach towards ephemeral Web personalization consisting in a client-side semantic user model built by aggregating RDF data encountered by the user in his/her browsing activity and enriching them with triples extracted from DBpedia. Such user model is then queried by a server application via SPARQL to identify a user stereotype and finally address personalized content...|$|R
40|$|Abstract: The {{development}} of modern edit and imputation (E&I) methods and software {{is one of}} the spearheads of the Methods and Informatics Department of Statistics Netherlands. Many aspects of E&I are covered by the work that is currently being carried out. Software development focuses on the further {{development of}} SLICE, a general software framework for automatic E&I. At the moment, SLICE is being extended with the new Cherry Pie module for <b>automatic</b> error <b>localisation</b> in a mixture of categorical and continuous data, with a module for regression imputation of missing continuous data, and with the EC System module for modification of imputed values so that all user-specified edits become satisfied. Besides SLICE, the WAID program for donor imputation of missing values is being developed further. Finally, a software tool for graphical macro-editing based on the functionality offered by SPSS enhanced with Visual Basic modules that are integrated into the SPSS environment is being developed. Methodological research focuses on selective editing and automatic E&I. The merits of classification/regression trees and (logistic) regression for selective editing purposes are currently being investigated. For automatic E&I an ambitious research project is planned to start early 2002. This project will last four years. Aim of this research project is to develop an approach for automatic E&I that integrates the approach based on the Fellegi-Holt paradigm and the NIM approach. Like NIM ou...|$|R
40|$|In this paper, {{we present}} an {{acoustic}} localization system for multiple devices. In contrast to systems which localise a device relative {{to one or}} several anchor points, {{we focus on the}} joint <b>localisation</b> of several <b>devices</b> relative to each other. We present a prototype of our system on off-the-shelf smartphones. No user interaction is required, the phones emit acoustic pulses according to a precomputed schedule. Using the elapsed time between two times of arrivals (ETOA) method with sample counting, distances between the devices are estimated. These, possibly incomplete, distances are the input to an efficient and robust multi-dimensional scaling algorithm returning a position for each phone. We evaluated our system in real-world scenarios, achieving error margins of 15 cm in an office environment...|$|R
40|$|The {{introduction}} of wireless {{networks in the}} factory floor offers many advantages. Besides a new flexibility for automation, also features like the <b>localisation</b> of wireless <b>devices</b> ease {{the use of this}} technology. However, for the application on the factory floor real-time guarantees have to be given, which can be ensured by schemes like TDMA, which is based on implicit or explicit clock synchronization. This is typically supported by a high accurate timestamping of incoming packets for the reduction of jitter effects introduced by the protocol layers. This paper 1 introduces an open platform, which supports research on receiver and timestamper design. The receiver is implemented in a flexible fashion, in order to support simultaneous multi-channel monitoring as well as easy reconfiguration for technologies other than IEEE 802. 11 b/g for efficient deployment in automation systems. ...|$|R
