10|10000|Public
500|$|Kouno had {{attempted}} to present the idea at pitch meetings twice {{in the early part}} of 2005 but was turned away. [...] While management was able to understand the mechanic of tilting the world, they could not understand Kouno's vision of <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the LocoRoco or other creatures in the game. On the second rejection, the management staff suggested that Kouno return with something more concrete to explain his ideas. Kouno spent one month with a four-person team to create a simple pre-prototype version of the game that demonstrated the rolling gameplay aspect. [...] The pre-prototype version was well received, and Kouno was given further resources to develop the full game. A complete prototype was created by an eight-person team over three months to establish the rest of the game's core mechanics, including the joining and splitting of the Loco Roco and the dynamic music. [...] The remainder of the game was completed in the following 11 months by the full 16-person staff at Sony Computer Entertainment Japan.|$|E
5000|$|... 1983. Learning Systems and Pattern Recognition in Industrial Control: <b>Applying</b> <b>Artificial</b> <b>Intelligence</b> <b>to</b> Industrial Control Proceedings of the Ninth Annual Advanced Control Conference, West Lafayette, Indiana, September 19-21, 1983. With E. J. Kompass ...|$|E
50|$|Chun {{started his}} career in Boston as Project Manager/Senior Scientist at HoneywellBull while {{finishing}} up his PhD thesis. After obtaining his PhD in 1987, he joined MIT’s Prof. Patrick Winston’s new startup Ascent Technology, and worked on the design and implementation of mission critical IT systems for airline, airport, and defense industries. He {{was a member of}} the Ascent software development team, participated in projects for several major airlines around the world, <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> mathematical optimization and scheduling (computing).|$|E
50|$|Prior to joining Sun, Polese {{worked on}} expert systems at IntelliCorp Inc., helping Fortune 500 {{companies}} <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> solving complex business challenges.|$|R
50|$|In 2012, {{he became}} a {{professor}} in the Institute for Astronomy at ETH Zürich in Switzerland. He is the secretary of the Swiss Society of Astrophysics and Astronomy, the professional society of astronomers in Switzerland. In 2017, he was awarded the European Astronomical Society's MERAC Prize as the best junior researcher in observational astrophysics. He launched the space.ml platform with Ce Zhang <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> astrophysics research.|$|R
5000|$|Edward Tsang is the Director (and co-founder) of Centre for Computational Finance and Economic Agents (CCFEA) at University of Essex.CCFEA is an interdisciplinaryresearch centre, which <b>applies</b> <b>artificial</b> <b>intelligence</b> methods <b>to</b> {{problems}} in finance and economics.|$|R
5000|$|Kouno had {{attempted}} to present the idea at pitch meetings twice {{in the early part}} of 2005 but was turned away. [...] While management was able to understand the mechanic of tilting the world, they could not understand Kouno's vision of <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the LocoRoco or other creatures in the game. On the second rejection, the management staff suggested that Kouno return with something more concrete to explain his ideas. Kouno spent one month with a four-person team to create a simple pre-prototype version of the game that demonstrated the rolling gameplay aspect. [...] The pre-prototype version was well received, and Kouno was given further resources to develop the full game. A complete prototype was created by an eight-person team over three months to establish the rest of the game's core mechanics, including the joining and splitting of the Loco Roco and the dynamic music. [...] The remainder of the game was completed in the following 11 months by the full 16-person staff at Sony Computer Entertainment Japan.|$|E
40|$|ALADIN is a {{knowledge-based}} system that aids metallurgists {{in the design}} of new aluminum alloys. Alloy design is characterized by creativity, intuition and conceptual reasoning. In this paper, the authors describe their approach to the challenges of <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> this domain, including: how to focus the search, how to deal with subproblem interactions, how to integrate multiple, incomplete design models, and how to represent complex, metallurgical structure knowledg...|$|E
40|$|This paper {{represents}} an ex post rethinking of {{the contribution of}} artificial intelligence techniques to safety management, based on a long work experience in <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> several engineering fields, from dam safety to environmental protection, from seismic monitoring {{to the protection of}} cultural heritage. The main issue is that developing models for assessing safety is a hard task, but integrating partial models may provide good results. Even if each single model is poor, a sort of epiphenomenal intelligence emerges from the behaviour of a system made of small partial models and users perceive it as a reliable assistant...|$|E
50|$|Basis Technology Corp. is a {{software}} company specializing in <b>applying</b> <b>artificial</b> <b>intelligence</b> techniques <b>to</b> understanding documents and unstructured data written in different languages. It has headquarters in Cambridge, Massachusetts and offices in San Francisco, Washington, D.C., London, and Tokyo.|$|R
40|$|The dynamic {{nature of}} the Cargo Transfer Vehicle's (CTV) mission and {{the high level of}} {{autonomy}} required mandate a complete fault management system capable of operating under uncertain conditions. Such a fault management system must take into account the current mission phase and the environment (including the target vehicle), as well as the CTV's state of health. This level of capability {{is beyond the scope of}} current on-board fault management systems. This presentation will discuss work in progress at TRW <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the problem of on-board fault management. The goal of this work is to develop fault management systems. This presentation will discuss work in progress at TRW <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the problem of on-board fault management. The goal of this work is to develop fault management systems that can meet the needs of spacecraft that have long-range autonomy requirements. We have implemented a model-based approach to fault detection and isolation that does not require explicit characterization of failures prior to launch. It is thus able to detect failures that were not considered in the failure and effects analysis. We have applied this technique to several different subsystems and tested our approach against both simulations and an electrical power system hardware testbed. We present findings from simulation and hardware tests which demonstrate the ability of our model-based system to detect and isolate failures, and describe our work in porting the Ada version of this system to a flight-qualified processor. We also discuss current research aimed at expanding our system to monitor the entire spacecraft...|$|R
40|$|International audienceThe {{design and}} {{construction}} of virtual reality environments involve technologies such ascomputer graphics, image processing, pattern recognition, intelligent interface, artificialintelligence, voice recognition, network, parallel processing, and high-performance computing. Some researchers insist that object-oriented and agent-oriented technologies are fundamental forvirtual reality system design. This paper <b>applies</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the design of virtualreality systems. Agents are constructed by using object-oriented methods {{and a set of}} underlyingcomputing models, such as neural networks, genetic algorithms, expert systems, and planmanagers. Some object-oriented frameworks of these computing models are presented toillustrate this approach. The example of a spaceship game will illustrate interactions amongenvironments, agents, and underlying computing models. The approach and reusable class librarypresented herein can be applied to various virtual reality environment simulations and intelligentapplications...|$|R
40|$|Windshear microbursts {{and extreme}} air {{turbulence}} caused by sudden intense changes in wind direction or speed {{are difficult to}} detect and thus dangerous to air traffic. They have been positively identified {{as the cause of}} 28 aviation accidents that claimed 491 lives. Many groups are investigating ways to detect and predict windshear. The Federal Aviation Consulting Services, Ltd. (FACS) is <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> windshear prediction. FACS' artificial intelligence based airline dispatcher program is intended as a backup not a replacement for human dispatcher. It would incorporate the same data that a human would request to make a decision and then draw a conclusion using the same rules of logic as the human expert...|$|E
30|$|<b>Applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> stroke sign {{detection}} {{can potentially}} aid in diagnosis or predicting prognosis [12]. One of the motivations in applying machine learning and deep learning methods to medical images is the automatic extraction of non-trivial and non-linear features from the imaging data. These features {{can then be}} applied for example in tissue classification or lesion volumetry. If the imaging data are inherently three-dimensional (3 D), it is natural to use methods based on 3 D features. One such AI algorithm family is 3 D convolutional neural networks (CNNs). They increase computational costs and hardware requirements compared to their two-dimensional counterparts. However, parallel processing, especially utilization of graphics processing units, {{as well as the}} recent advancement in deep learning algorithms, have allowed 3 D CNN-based software tools to take further steps toward clinical deployment.|$|E
40|$|In recent years, many {{researches}} {{about how}} to efficiently handling information on logistics in supply chain have been proposed. Some focus on <b>applying</b> <b>artificial</b> <b>intelligence</b> <b>to</b> solving the problem. Most of the above approaches, however, usually consider multiple agents and simulation systems in single companies. Besides, they seldom consider using semantics to allow flexible information query with different linguistic terms. In this paper, we have proposed a framework for an e-SCM multiple-agents decision support system (called e-SCM multi-agents system), which combines ontology, to efficiently integrate data and information in supply chains. There are five layers in the system, including access layer, communication layer, application layer, ontology layer and database layer. These layers are linked together to integrate different access tools, data formats, management information systems, semantic web and databases. Different agents execute different tasks in each layer to achieve the purpose of integration and communication in a supply chain with less human intervention. Our approach emphasizes on the transparent connection manner among businesses. The proposed system can assist in business data and information sharing in a complex supply chain management...|$|E
40|$|The {{design and}} {{construction}} of virtual reality environments involve technologies such as computer graphics, image processing, pattern recognition, intelligent interface, <b>artificial</b> <b>intelligence,</b> voice recognition, network, parallel processing, and high-performance computing. Some researchers insist that object-oriented and agent-oriented technologies are fundamental for virtual reality system design. This paper <b>applies</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the design of virtual reality systems. Agents are constructed by using object-oriented methods {{and a set of}} underlying computing models, such as neural networks, genetic algorithms, expert systems, and plan managers. Some object-oriented frameworks of these computing models are presented to illustrate this approach. The example of a spaceship game will illustrate interactions among environments, agents, and underlying computing models. The approach and reusable class library presented herein can be applied to various virtual reality environment simulations and intelligent applications. 1...|$|R
50|$|In {{the early}} 1980s the United States Air Force {{realized}} that they had received significant benefits from <b>applying</b> <b>artificial</b> <b>intelligence</b> technologies <b>to</b> solving expert problems such as the diagnosis of faults in aircraft. The air force commissioned a group of researchers from the <b>artificial</b> <b>intelligence</b> and formal methods communities to develop a report on how such technologies {{might be used to}} aid in the more general problem of software development.|$|R
40|$|This {{project has}} <b>applied</b> <b>Artificial</b> <b>Intelligence</b> {{techniques}} <b>to</b> diagnose faults in an automatic sampling and preparation {{system in a}} Chemical laboratory- The expert system Mas written in microPROLOB {{to take advantage of}} the pattern matching and searching facilities as well as the ease of changing rules. A natural language interface was added to the system to enable those unskilled in the use of PROLOG to use the system...|$|R
40|$|Neural {{networks}} {{have been proven}} to successfully predict the results of complex non-linear problems {{in a variety of}} research fields, including medical research. Yet there is paucity of models utilising intelligent systems in the field of thermoregulation. They are under-utilized for predicting seemingly random physiological responses and in particular never used to predict local skin temperatures; or core temperature with a large dataset. In fact, most predictive models in this field (non-artificial intelligence based) focused on predicting body temperature and average skin temperature using relatively small gender-unbalanced databases or data from thermal dummies {{due to a lack of}} larger datasets. This paper aimed to address these limitations by <b>applying</b> <b>Artificial</b> <b>Intelligence</b> <b>to</b> create predictive models of core body temperature and local skin temperature (specifically at forehead, chest, upper arms, abdomen, knees and calves) while using a large and gender-balanced experimental database collected in office-type situations. A range of Neural Networks were developed for each local temperature, with topologies of 1 - 2 hidden layers and up to 20 neurons per layer, using Bayesian and the Levemberg-Marquardt back-propagation algorithms, and using various sets of input parameters (2520 NNs for each of the local skin temperatures and 1760 for the core temperature, i. e. a total of 19400 NNs). All topologies and configurations were assessed and the most suited recommended. The recommended Neural Networks trained well, with no sign of over-fitting, and with good performance when predicting unseen data. The recommended Neural Network for each case was compared with previously reported multi-linear models. Core temperature was avoided as a parameter for local skin temperatures as it is impractical for non-contact monitoring systems and does not significantly improve the precision despite it is the most stable parameter. The recommended NNs substantially im...|$|E
40|$|This {{dissertation}} {{defends the}} use of nonmonotonic logic to represent rule-based legal reasoning, as exemplified by a particular, complex statute: the Internal Revenue Code. The dissertation motivates and provides a theoretical basis for formalizing the United States tax code (and perhaps other statutes). Formalization of statutory language will make statutes more precise. Formalized statutory language that tracks the actual structure of the tax law {{will make it easier}} for theoretical work to converge with the law, and may lay the groundwork <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> tax compliance and avoidance. To this end, the dissertation investigates and refines John Horty’s work, especially Reasons as Defaults, with particular focus on examples in that book of inappropriate equilibria—scenarios that Horty’s approach endorses that Horty finds problematic or unintuitive. The dissertation looks at Horty’s work in service of applying Horty’s work, and default logic more generally, to legal reasoning, and in particular rule-based legal reasoning...|$|R
40|$|Different motivations led {{scientists}} <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> (AI) techniques <b>to</b> {{educational software}} and training software. On one hand, courseware developers were seeking for more powerful techniques for building systems. On the other hand, researchers {{in computer science}} and in cognitive psychology found a...|$|R
40|$|Research {{activity}} {{has shifted from}} computer graphics and vision systems to the broader scope of <b>applying</b> concepts of <b>artificial</b> <b>intelligence</b> <b>to</b> robotics. Specifically, the research is directed toward developing Artificial Neural Networks, Expert Systems, and Laser Imaging Techniques for Autonomous Space Robots...|$|R
40|$|The {{application}} of <b>applied</b> <b>artificial</b> <b>intelligence</b> techniques <b>to</b> geographical data processing has attracted considerable interest {{in recent years}} as reflected in the research agendas of major GIS research institutions. This paper illustrates how a simple spatial problem, formalised using predicate calculus, may be readily implemented as a rule base in which the production rules represent Horn clauses. The theoretical significance of this exercise is the search for a high-level mathematical language capable of expressing the spatial relationships and queries typically handled by a GIS. -from Authorlink_to_subscribed_fulltex...|$|R
5000|$|Matthew Yee-King, is an {{electronic}} musician, percussionist and researcher based in London, performing music as Yee-King. He {{is known for}} bringing an education in science and genetics into music, including his celebrated 2001 Drill 'n' bass release SuperUser released by Rephlex Records, his work with Finn Peters in making music from brainwaves, and his doctoral work on <b>applying</b> <b>Artificial</b> <b>intelligence</b> techniques <b>to</b> automatic synthesizer programming. [...] "Goodnight Toby", a track from SuperUser, was listed in the top 100 greatest IDM tracks by FACT magazine.|$|R
40|$|International audienceConcept or Galois Lattices (CL) {{provide a}} {{productive}} framework {{for a variety}} of problems that arise in Knowledge Discovery in Databases (KDD). This paper introduces this special issue of <b>Applied</b> <b>Artiﬁcial</b> <b>Intelligence</b> devoted <b>to</b> applications of Concept Lattices for KDD (CLKDD). The papers in this volume come from a call for papers issued after the ﬁrst International Workshop on Concept Lattice-based Theory, Methods and Tools for KDD held in July 2001 at Stanford University. Another special issue devoted to algorithms and methods of CLKDDwill appear in the Journal of Experimental and Theoretical <b>Artiﬁcial</b> <b>Intelligence...</b>|$|R
40|$|E-mail {{is one of}} {{the most}} popular and widely used form of {{electronic}} communication used today. The patterns in the social interactions or contacts between people by e-mail can be analysed using social network analysis and user behaviour analysis. In this paper we provide a review of the work related to the areas of dynamic modelling and link prediction of social networks, and anomaly detection for detecting changes in the behaviour of e-mail usage. We then discuss about the benefits of <b>applying</b> <b>artificial</b> <b>intelligence</b> techniques <b>to</b> these fields. 1...|$|R
40|$|The {{electromagnetic}} interference prediction problem is characteristically ill-defined and complicated. Severe EMI problems are prevalent throughout the U. S. Navy, causing both expected and unexpected {{impacts on the}} operational performance of electronic combat systems onboard ships. This paper focuses on <b>applying</b> <b>artificial</b> <b>intelligence</b> (AI) technology <b>to</b> the prediction of ship related {{electromagnetic interference}} (EMI) problems...|$|R
40|$|NASA Lewis Research Center has <b>applied</b> <b>artificial</b> <b>intelligence</b> <b>to</b> an {{advanced}} ground terminal. This software application is being deployed as an experimenter interface to the link evaluation terminal (LET) {{and was named}} Space Communication <b>Artificial</b> <b>Intelligence</b> for the Link Evaluation Terminal (SCAILET). The high-burst-rate (HBR) LET provides 30 -GHz-transmitting and 20 -GHz-receiving, 220 -Mbps capability for wide band communications technology experiments with the Advanced Communication Technology Satellite (ACTS). The HBR-LET terminal consists of seven major subsystems. A minicomputer controls and monitors these subsystems through an IEEE- 488 or RS- 232 protocol interface. Programming scripts (test procedures defined by design engineers) configure the HBR-LET and permit data acquisition. However, the scripts are difficult to use, require a steep learning curve, are cryptic, and are hard to maintain. This discourages experimenters from utilizing the full capabilities of the HBR-LET system. An intelligent assistant module was developed {{as part of the}} SCAILET software. The intelligent assistant addresses critical experimenter needs by solving and resolving problems that are encountered during the configuring of the HBR-LET system. The intelligent assistant is a graphical user interface with an expert system running in the background. In order to further assist and familiarize an experimenter, an on-line hypertext documentation module was developed and included in the SCAILET software...|$|R
40|$|Ontobroker <b>applies</b> <b>Artificial</b> <b>Intelligence</b> {{techniques}} <b>to</b> improve {{access to}} heterogeneous, distributed and semistructured information sources {{as they are}} presented in the World Wide Web or organization-wide intranets. It relies {{on the use of}} ontologies to annotate web pages, formulate queries and derive answers. In the paper we will briefly sketch Ontobroker. Then we will discuss its main shortcomings, i. e. we will share the lessons we learned from our exercise. We will also show how On 2 broker overcomes these limitations. Most important is the separation of the query and inference engines and the integration of new web standards like XML and RDF...|$|R
40|$|The {{inherent}} {{difficulties in}} eliciting domain knowledge from experts are often encountered when <b>applying</b> <b>artificial</b> <b>intelligence</b> techniques <b>to</b> real-world problems characterised by multiple conflicting constraints. Definitions of optimal solutions are often subjective and {{highly dependent on}} the opinions and work practices of individual experts. We developed a case-based reasoning approach to capture concepts of optimality through the storage, reuse, and adaptation of previous repairs of constraint violations. The technique {{is applied to the}} problem of rostering nurses at the Queens Medical Centre, Nottingham. An iterative roster repair system is presented that learns repair techniques from nurses with rostering experience...|$|R
40|$|Digital {{forensics}} {{plays an}} increasingly important role within society as the approach to the identification of criminal and cybercriminal activities. It is however widely known {{that a combination of}} the time taken to undertake a forensic investigation, the volume of data to be analysed and the number of cases to be processed are all significantly increasing resulting in an ever growing backlog of investigations and mounting costs. Automation approaches have already been widely adopted within digital forensic processes to speed up the identification of relevant evidence – hashing for notable files, file signature analysis and data carving to name a few. However, to date, little research has been undertaken in identifying how more advanced techniques could be applied to perform “intelligent” processing of cases. This paper proposes one such approach, the Automated Forensic Examiner (AFE) that seeks <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> <b>to</b> the problem of sorting and identifying relevant artefacts. The proposed approach utilises a number of techniques, including a technical competency measure, a dynamic criminal knowledge base and visualisation to provide an investigator with an in depth understanding of the case. The paper also describes how its implementation within a cloud based infrastructure will also permit a more timely and cost effective solution...|$|R
40|$|AbstractThe {{performance}} of piezoelectric transducers {{is affected by}} variation of the acoustic loads. Correction of the excitation frequency are needed to maintain the performance. This paper presents an algorithm for dynamic correction of the operating frequency based on adaptive systems able to correct the frequency <b>applying</b> <b>artificial</b> <b>intelligence</b> techniques <b>to</b> a survey of impedance profiles. When the impedance is changed, a searching of similar values is performed in previously stored files and an iterative process finds the final frequency closer to the original impedance for the required performance. Simulations {{has been carried out}} using electric models. The results show the system is robust and the average response time is 6 ms...|$|R
40|$|AbstractThe CODER (COmposite Document Expert/extended/effective Retrieval) {{project is}} a multi-yeare effort to {{investigate}} how best <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> methods <b>to</b> increase the effectiveness of information retrieval systems handling collections of composite documents. To ensure system adaptability and to allow controlled experimentation, CODER has been designed as a distributed expert system. The use of individually tailored specialist experts, coupled with standardized blackboard modules for communication and control and external knowledge bases for maintenance of factual world knowledge, allows for quick prototyping, incremental development, and flexibility under change. The {{system as a whole}} is being implemented under UNIX as a set of MU-Prolog and C modules communicating through pipes and TCP/IP sockets...|$|R
40|$|Abstract. Ontobroker <b>applies</b> <b>Artificial</b> <b>Intelligence</b> {{techniques}} <b>to</b> improve {{access to}} heterogeneous, distributed and semistructured information sources {{as they are}} presented in the World Wide Web or organization-wide intranets. It relies {{on the use of}} ontologies to annotate web pages, formulate queries and derive answers. In the paper we will briefly sketch Ontobroker. Then we will discuss its main shortcomings, i. e. we will share the lessons we learned from our exercise. We will also show how On 2 broker overcomes these limitations. Most important is the separation of the query and inference engines and the integration of new web standards like XML and RDF. ...|$|R
40|$|It is quit {{difficult}} {{to improve the}} performance of urban traffic signal control system efficiently by using traditional methods of modeling and control because of time-variability, non-linearity, fuzzyness and nondeterminacy in the system. It becomes the research hotspot in this area <b>to</b> <b>apply</b> <b>artificial</b> <b>intelligence</b> methods <b>to</b> urban traffic signal control system. This paper, based on analysis for a general model of urban traffic signal control, makes {{a summary of the}} applications of intelligence methods such as fuzzy logic, neural networks, evolutionary algorithms and agent reinforcement learning to urban traffic signal control, and analyzes the superiority and inferiority of these methods in applications. Finally, some points of view about the future research in this area are proposed. Key words...|$|R
40|$|A Robot Scientist is a {{physically}} implemented system that <b>applies</b> <b>artificial</b> <b>intelligence</b> <b>to</b> autonomously discover new knowledge through cycles of scientific experimentation. Additionally, our Robot Scientist {{is able to}} execute experiments that have been requested by human biologists. There arises a multi-objective problem {{in the selection of}} batches of trials to be run together on the robot hardware. We describe the use of the jMetal framework to assess the suitability of a number of multi-objective metaheuristics to optimise the flow of experiments run on a Robot Scientist. Experiments are selected in batches, chosen in order to maximise the information gain and minimise the use of resources. The evolutionary multi-objective algorithms evaluated here perform well in finding solutions to this problem, either finding a long, fairly efficient Pareto optimal front, or a shorter, highly efficient Pareto optimal front. recently built Robot Scientist 1 {{is one of the most}} advanced laboratory automation systems in existence. The advances that distinguish our Robot Scientist are its AI software, the design of the hardware and the complete autonomy of the knowledge discovery process [3]. A Robot Scientist has hardware that permits it to carry out experiments (see Figure 1). These experiments may have been devised by the Robot Scientist itself or they may have been requested by human biologists. For details of how our Robot Scientist autonomously devises and executes cycles of scientific discovery, see Section 2...|$|R
