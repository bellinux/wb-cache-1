15|10000|Public
50|$|In {{distributed}} mode Fastest {{works as}} a client-server application. The application can be installed {{in a number of}} computers each acting as client, a server or both. Users access the application through clients which send test classes to servers (called testing servers) which try to find an <b>abstract</b> <b>test</b> <b>case</b> out of them. In this way the heaviest task is distributed across as many computers as possible. Since the calculation of an <b>abstract</b> <b>test</b> <b>case</b> from a test class is completely independent from each other, this architecture speeds up the entire process proportionally with respect to the number of testing servers.|$|E
5000|$|An <b>abstract</b> <b>test</b> <b>case</b> is {{an element}} {{belonging}} to a test class. The TTF prescribes that abstract test cases should be derived only from {{the leaves of the}} testing tree. Abstract test cases can also be written as Z schema boxes. Let [...] be some operation, let [...] be the VIS of , let [...] be all the variables declared in , let [...] be a (leaf) test class of the testing tree associated to , let [...] be the characteristic predicates of each test class from [...] up to [...] (by following the edges from child to parent), and let [...] be [...] constant values satisfying [...] Then, an <b>abstract</b> <b>test</b> <b>case</b> of [...] is the Z schema box defined by [...]|$|E
50|$|In general a test class' {{predicate}} is a {{conjunction of}} two or more predicates. It is likely, then, that some test classes are empty because their predicates are contradictions. These test classes must be pruned from the testing tree because they represent impossible combinations of input values, i.e. no <b>abstract</b> <b>test</b> <b>case</b> can be derived out of them.|$|E
5000|$|Find {{one or more}} <b>abstract</b> <b>test</b> <b>cases</b> {{from each}} leaf in each testing tree.|$|R
25|$|The {{selection}} of classes typically follows {{the principle of}} equivalence partitioning for <b>abstract</b> <b>test</b> <b>cases</b> and boundary-value analysis for concrete <b>test</b> <b>cases.</b>|$|R
40|$|Test case {{instantiation}} is {{the transformation}} of <b>abstract</b> <b>test</b> <b>cases</b> into executable <b>test</b> scripts. <b>Abstract</b> <b>test</b> <b>cases</b> are either created during model based <b>test</b> <b>case</b> generation or are manually defined in a suitable modeling notation. The transformation varies depending on different testing concerns, such as test goal, test setup and test phase. Thus, for each testing concern a new transformation must be defined. This paper introduces AspectT, an aspect-oriented lan-guage for the instantiation of <b>abstract</b> <b>test</b> <b>cases.</b> We reduce the ef-fort of <b>test</b> <b>case</b> instantiation by modularizing testing concerns {{in the form of}} aspects to enable their reuse in different testing con-texts. The approach is implemented and integrated in an existing testing framework and has been successfully applied to test an elec-tronic control unit of an automotive infotainment system at BMW Group...|$|R
5000|$|Fastest {{presents}} a command-line user interface. The user first needs to load a Z specification written in LaTeX format verifying the ISO standard [...] Then, the user has {{to enter a}} list of the operations to test as well as the testing tactics to apply to each of them. In a third step Fastest generates the testing tree of each operation. After testing trees have been generated, users can browse them and their test classes, and, more importantly, they can prune any test class both automatically or manually. Once testing trees have been pruned, users can instruct Fastest to find one <b>abstract</b> <b>test</b> <b>case</b> for each leaf in each testing tree.|$|E
5000|$|The tool finds {{abstract}} {{test cases}} by calculating a finite model for each leaf in a testing tree [...] Finite models are calculated by restricting {{the type of}} each VIS variable to a finite set and then by calculating the Cartesian product between these sets. Each leaf predicate is evaluated on each element of this Cartesian product until one satisfies the predicate (meaning that an <b>abstract</b> <b>test</b> <b>case</b> was found) or until it is exhausted (meaning that either the test class is unsatisfiable or the finite model is inadequate). In the last case, the user has the chance to assist the tool in finding the right finite model or to prune the test class because it is unsatisfiable.|$|E
40|$|Abstract: Generally, {{test cases}} {{derived from a}} formal model can not be {{directly}} fed into implementations under test (IUT), because model based test generation techniques produce abstract test cases. In order to run an <b>abstract</b> <b>test</b> <b>case</b> against an IUT the ab-stract test case either has to be transformed to a concrete test case or an execution of the <b>abstract</b> <b>test</b> <b>case</b> is needed. In this {{paper we propose a}} rule based test execution frame-work, which allows the execution of abstract test cases. Furthermore, we present first results from testing a so called SIP Registrar by executing abstract test cases derived with the TGV tool from a formal specification. ...|$|E
40|$|We {{present an}} {{approach}} for the systematic development of <b>abstract</b> <b>test</b> <b>cases</b> for service <b>testing.</b> The approach is rigorous {{in the sense}} that the <b>abstract</b> <b>test</b> <b>cases</b> are derived from a rigorous specification. The approach is practical {{in the sense that}} it contains guidelines for choosing amongst the many possible execution paths. The method has been developed in practice. Tools are used for interactive simulation and for certain translation steps...|$|R
40|$|<b>Abstract</b> <b>test</b> <b>cases</b> {{contain a}} huge amount of domain {{specific}} knowledge of experts. Engineers may not use this knowledge only for validation purposes, e. g., applying <b>test</b> <b>cases</b> to software and hardware units, it can be also a starting point for modeling the product prototypically. This paper addresses the issue of generating a functional behavior model from <b>abstract</b> <b>test</b> <b>cases.</b> The <b>test</b> <b>cases</b> belong to the conformity and interoperability test standard for train-borne control units of the European Train Control System (ETCS). The contribution illustrates the approach and its advantages...|$|R
50|$|In model-based testing, one {{distinguishes between}} <b>abstract</b> <b>test</b> suites, which are {{collections}} of <b>abstract</b> <b>test</b> <b>cases</b> {{derived from a}} high-level model of the system under test, and executable test suites, which are derived from <b>abstract</b> <b>test</b> suites by providing the concrete, lower-level details needed to execute this suite by a program. An <b>abstract</b> <b>test</b> suite cannot be directly used on the actual system under <b>test</b> (SUT) because <b>abstract</b> <b>test</b> <b>cases</b> remain at a high abstraction level and lack concrete details about the SUT and its environment. An executable test suite works on a sufficiently detailed level to correctly communicate with the SUT and a test harness is usually present to interface the executable test suite with the SUT.|$|R
40|$|Tabular {{expressions}} {{have been}} used in industry for many years to precisely document software in a readable notation. In this paper, we propose a fault-based testing technique that traces the propagation of faults from the expression in each cell of a tabular expression to the output of the program under test. The technique has been formalized in the form of <b>abstract</b> <b>test</b> <b>case</b> constraints also represented by tabular expressions, {{so that it can be}} easily applied and automated. published_or_final_versio...|$|E
40|$|<b>Abstract.</b> <b>Test</b> <b>case</b> {{prioritization}} is {{a difficult}} problem of Software Engineering, since several factors may be considered {{in order to find}} the best order for test cases. Search-based techniques have been applied to find solutions for test case prioritization problem. Some of these works apply Ant Colony based algorithms, but the precedence of test cases was not considered. We propose an Ant Colony Optimization based algorithm to prioritize test cases with precedence. Each ant builds a solution, and when it is necessary to choose a new vertex (test case), only allowed test cases are seen by the ant, implementing the precedence constraint of the problem...|$|E
40|$|<b>Abstract.</b> <b>Test</b> <b>case</b> {{selection}} and prioritization are well studied and understood regression testing techniques. Equally, test case generation {{is an active}} research area. Yet {{the combination of these}} techniques re-mains largely unexplored. This paper proposes to use a multi objective approach to combine a test case generation technique, the Classification Tree Method, with a test case {{selection and}} prioritization method. Our work aims to generate optimized test suites, containing test cases or-dered according to their importance with respect to test goals. We plan to incorporate the algorithms we develop during this work into the Clas-sification Tree Editor, an industrial strength testing tool provided by Berner & Mattner, and will be empirically evaluating our approach on a set of benchmark systems...|$|E
40|$|AbstractDeveloping test suites is {{a costly}} and {{error-prone}} process. Model-based test generation tools facilitate this process by automatically generating <b>test</b> <b>cases</b> from system models. The applicability of these tools, however, {{depends on the}} size of the target systems. Here, we propose an approach to generate <b>test</b> <b>cases</b> by combining data abstraction, enumerative test generation and constraint-solving. Given the concrete specification of a possibly infinite system, data abstraction allows to derive an abstract system, which is finite and thus suitable for the automatic generation of <b>abstract</b> <b>test</b> <b>cases</b> with enumerative tools. To execute <b>abstract</b> <b>test</b> <b>cases,</b> we have to instantiate them with concrete data. For data selection we make use of constraint-solving techniques...|$|R
30|$|BETA (Bbased testing {{approach}}) is a toolsupported {{approach to}} generate <b>test</b> <b>cases</b> from Bmethod specifications {{through the application}} of input space partitioning and logical coverage criteria. The BETA tool automates the whole process, from the design of <b>abstract</b> <b>test</b> <b>cases</b> to the generation of executable test scripts.|$|R
2500|$|The {{classification}} tree method first {{was intended for}} the design and specification of <b>abstract</b> <b>test</b> <b>cases.</b> With the {{classification tree}} method for embedded systems, test implementation can also be performed. Several additional features are integrated with the method: ...|$|R
40|$|International audienceThis paper {{reports about}} the VETESS project results and {{experience}} with building a model-based testing toolchain to validate automotive embedded systems. This approach, based on existing test generation and test execution tools, {{makes it possible to}} automatically derive and execute functional test cases from UML or SysML models. This process is composed of the following steps: modelling (UML or SysML functional view), <b>abstract</b> <b>test</b> <b>case</b> generation (symbolic execution of the model), concretization (generation of executable test scripts from abstract test cases) and analysis (assignation of the test verdict). This process is automated by a toolchain based on Topcased modeler, Smartesting test generator and Clemessy TestInView. This developed prototype made it possible to demonstrate that model-based testing from UML/SysML models is an efficient way to automate testing process for systems mixing software and hardware parts...|$|E
40|$|<b>Abstract.</b> <b>Test</b> <b>case</b> {{design is}} the most {{important}} test activity with respect to test quality. For this reason, a large number of testing methods have been developed to assist the tester with the definition of appropriate, error-sensitive test data. Besides black-box tests, white-box tests are the most prevalent. In both cases, complete automation of test case design is difficult. Automation of black-box test is only meaningfully possible if a formal specification exists, and, due to the limits of symbolic execution, tools supporting white-box tests are limited to program code instrumentation and coverage measurement. Evolutionary testing is a promising approach for automating structure-oriented test case design completely. In many experiments, high coverage degrees were reached using evolutionary testing. In this paper we shall investigate the suitability of structure-based complexity measures to assess whether or not evolutionary testing is appropriate for the structure-oriented test of given test objects. ...|$|E
40|$|<b>Abstract.</b> <b>Test</b> <b>case</b> {{prioritization}} (TCP) {{is aimed}} at finding an ideal ordering for executing the available test cases to reveal faults earlier. To solve this problem greedy algorithms and meta-heuristics have been widely investigated, {{but in most cases}} there is no statistically significant difference between them in terms of effectiveness. The fitness function used to guide meta-heuristics condenses the cumulative coverage scores achieved by a test case ordering using the Area Under Curve (AUC) met-ric. In this paper we notice that the AUC metric represents a simplified version of the hypervolume metric used in many objective optimization and we propose HGA, a Hypervolume-based Genetic Algorithm, to solve the TCP problem when using multiple test criteria. The results shows that HGA is more cost-effective than the additional greedy algorithm on large systems and on average requires 36 % of the execution time required by the additional greedy algorithm...|$|E
5000|$|The TTF is a {{specific}} proposal of model-based testing (MBT). It considers models to be Z specifications. Each operation within the specification is analyzed to derive or generate <b>abstract</b> <b>test</b> <b>cases.</b> This analysis consists of the following steps: ...|$|R
50|$|One of {{the main}} {{advantages}} of the TTF {{is that all of}} these concepts are expressed in the same notation of the specification, i.e. the Z notation. Hence, the engineer has to know only one notation to perform the analysis down to the generation of <b>abstract</b> <b>test</b> <b>cases.</b>|$|R
30|$|An {{internal}} validity threat for {{our study was}} the manual implementation of the <b>test</b> <b>cases</b> in the Lua API case study. The gap between the model and the Lua’s API code may have caused some mismatch between <b>abstract</b> <b>test</b> <b>cases</b> as produced by BETA and concrete <b>test</b> <b>cases</b> executed on the API implementation, influencing the obtained results. We expect that the new features of BETA, such as the test data concretization and test script generation, can minimize this threat in future applications.|$|R
40|$|Abstract: In the {{standardization}} of test specifications, {{it is common}} that no actual systems exist against which the tests can be executed. Test specifications are developed abstractly in high level languages such as the Testing and Test Control Notation (TTCN- 3), but they can only be executed when aseparate adaptation layer is implemented. Static syntactical and semantical analyses as provided by the compiler and proper manual code reviews are the only means to find mistakes in such test specifications at early stages of design. In this paper, wedemonstrate {{that it is possible}} to execute abstract test specifications when the system does not exist yet. We use the information provided within the test cases to simulate answers of the system by generating inverse messages to expected messages in the <b>abstract</b> <b>test</b> <b>case.</b> By following a specific coverage-criterion strategy, weare able to execute asufficient amount of test paths to reverse-engineer behavioral models of test cases which can then again be used for the analyses of potential problems. ...|$|E
40|$|<b>Abstract</b> <b>Test</b> <b>case</b> {{prioritization}} {{provides a}} way to run test cases with the highest priority earliest. Numerous empirical {{studies have shown that}} prioritization can improve a test suite`s rate of fault detection, but {{the extent to which these}} results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites. In particular, Java and the JUnit testing framework are being used extensively to build software systems in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm. To investigate the practical implications of these results, we present a set of cost-benefits models for test case prioritization, and show how the effectiveness differences observed can result in savings in practice, but vary substantially with the cost factors associated with particular testing processes...|$|E
40|$|Modern {{vehicles}} {{are equipped with}} electrical and electronic systems that implement highly complex functions, such as anti-lock braking, cruise control, etc. To realize and integrate such complex embedded systems, the automotive development process requires an updated methodology that takes into consideration the system’s intricate features and examines both their functional and extra-functional requirements. Early design artifacts like architectural models represent convenient abstractions for reasoning about the system’s structure and functionality. In this context, the EAST-ADL language has been developed as a domain-specific architectural language that targets the automotive industry and is aligned with the AUTOSAR automotive standard. To fully enjoy the benefits of these abstract system descriptions, architectural models need to be integrated into a model-driven development framework that enables also verification by, e. g., model checking and model-based testing. One major drawback in developing such a framework {{lies in the fact}} that architectural models, while capturing the system’s structure and inter-component communication, often lack direct means to represent the desired internal behavior of the system in a semantically well-defined way. To overcome this, one needs to provide means of integrating both structural as well as behavioral information, desirably within the same framework backed by formal semantics, in order to enable the model’s formal verification. In this thesis, we propose a tool-supported integrated formal modeling and verification framework tailored for automotive embedded systems that are originally described in the EAST-ADL architectural language. To achieve this, we first provide formal semantics to the architectural model and its behavior by proposing an equivalent formal description as a network of timed automata. This enables us to analyze the resulting network of timed automata formally by model checking, using both the UPPAAL PORT and UPPAAL SMC model checkers. UPPAAL PORT is providing efficient component-aware verification via the partial order reduction technique, while UPPAAL SMC is extending UPPAAL with statistical model-checking capabilities via probabilistic algorithms. We focus the analysis on functional and timing requirements, but also on the system’s resource usage with respect to different resources specified in the model, such as memory and energy. In an attempt to narrow the gap between the original architectural model and the eventual system implementation, we define an executable semantics of the UPPAAL PORT components that guarantees that the implementation preserves the invariant properties of the model. Assuming a system implementation that conforms to the formal model, we investigate how to provide test cases suitable for the eventual verification of such implementation, by exploiting the model checker’s ability to generate witness traces for reachability verification. Such a witness trace represents a execution of the system from its initial state to the goal state encoded by the reachability property, and becomes our <b>abstract</b> <b>test</b> <b>case.</b> By pairing the automated model-based test case generator with an automatic transformation from the abstract test cases to Python scripts, we enable the execution of the generated Python scripts on the system under test, which ends up in pass/fail testing verdicts. Dependency analysis is a method that is able to identify crucial intra- and inter-component dependencies early in the system’s development life cycle, if applied on architectural models. In this thesis, we also investigate how such dependencies, resulting from applying dependency analysis on EAST-ADL models, can be exploited during formal verification in order to reduce the verified state-spaces during model checking. The framework is supported by the ViTAL tool and its applicability is shown on an automotive industrial prototype, namely a Brake-by-Wire system. ...|$|E
40|$|Formal model-based {{specifications}} provide precise {{descriptions of}} the behavior of software components. These formal specifications are written using pre- and post-condition assertions. They can serve as a basis for formally verifying the correctness of an implementation. But a formal specification is really only useful when it captures the desired functionality. How can the specifier be confident that the specification is correct?;The Abstract Test Tool supports the direct execution of C++ class specifications through the incremental development and automated generation of <b>abstract</b> <b>test</b> <b>cases</b> and the display of abstract results [...] -both in terms of the abstract model used in the specification. The Class Validation System builds upon the Abstract Test Tool to support the automated and extensive testing of C++ class implementations. The Class Validation System provides a potentially one-to-many mapping from <b>abstract</b> <b>test</b> <b>cases</b> to implementation <b>test</b> <b>cases.</b> The results of executing the implementation are automatically compared to the results of executing the specification by mapping the implementation results to the abstract model. Thus, the Class Validation System supports fully automated implementation testing...|$|R
40|$|The aim of {{this thesis}} is to {{investigate}} how Model-Based Testing (MBT) can improve the traditional testing process for the Radio Unit (RU) product within Ericsson and how Ericsson could introduce MBT into their working environment. This thesis work is a proof of concept. The scope of the thesis {{is limited to the}} functional testing of RU LTE Carrier setup. The MBT tool used is Spec Explorer, an extension of Visual Studios. The language used in Spec Explorer to create the model is C#. The RU MBT model is created based on multiple RU-specifications and design documentations given by Ericsson. The model created is a rainy day scenario model. To be able to cover the given functional requirements stated in the documentation, multiple iterations of the model are created. The final iteration of the model is able to implicitly cover most of the functional requirements. 86 <b>Abstract</b> <b>test</b> <b>cases</b> were generated from the RU MBT model. The correctness of the model was verified by comparing these <b>abstract</b> <b>test</b> <b>cases</b> to the documentation. The <b>abstract</b> <b>test</b> <b>cases</b> were parsed into readable format for the new test framework in Ericsson. 86 concrete <b>test</b> <b>cases</b> were run in the new test framework to test a live RU, two errors were found. Analyzing and comparing the MBT process with the traditional testing process, the conclusion was that MBT shows an improvement in testing cost, testing quality, requirement traceability, defect detection and model behavior evolution. However MBT is still a demanding process with drawbacks. The recommendation is to first implement smaller models of RU functions and then successively implement more complex RU functions and extend the models when needed...|$|R
40|$|As {{reactive}} and {{embedded systems}} continuously {{interact with their}} environment, {{it is important to}} test as many as possible interactions. The use of qualitative models of the environ-ment and hardware has the potential to provide <b>test</b> <b>cases</b> that might not be considered with traditional testing methods. We present an approach that derives <b>abstract</b> <b>test</b> <b>cases</b> from such models using qualitative reasoning, which is a well known artificial intelligence technique to represent and reason about physical behavior. For this purpose we introduce the underly-ing concepts of qualitative reasoning, show the <b>test</b> <b>case</b> gen-eration process, and provide the results of a case study...|$|R
30|$|In [5], an {{automatic}} test environment for B specifications called ProTest is presented. It uses model-checking techniques to find test sequences that satisfy its test generation parameters. ProTest uses offline technology and only generates <b>abstract</b> <b>test</b> <b>cases.</b> It requires specifications {{to be in}} a single B machine, and {{it is up to the}} user to define the requirements to be satisfied by the <b>test</b> <b>cases.</b> These requirements are made of operations to cover and predicates that must hold true at the end of each <b>test</b> <b>case.</b> In [5], the authors presented a simple case study to evaluate ProTest and discuss theoretical differences concerning other approaches.|$|R
40|$|International audienceWe {{propose a}} syntax-driven test {{generation}} technique to automatically derive <b>abstract</b> <b>test</b> <b>cases</b> from {{a set of}} requirements expressed in a linear temporal logic. Assuming that an elementary <b>test</b> <b>case</b> (called a “tile”) is associated to each basic predicate of the formula, we show how to generate a set of test controllers associated to each logical operator, and able to coordinate the whole test execution. The <b>test</b> <b>cases</b> produced are expressed in a process algebraic style, allowing {{to take into account}} the test environment constraints. We illustrate this approach in the context of network security testing, for which more classical model-based techniques are not always suitable...|$|R
5000|$|A model {{describing}} a SUT is usually an abstract, partial {{presentation of the}} SUT's desired behavior.Test cases derived from such a model are functional tests {{on the same level}} of abstraction as the model.These <b>test</b> <b>cases</b> are collectively known as an <b>abstract</b> <b>test</b> suite.An <b>abstract</b> <b>test</b> suite cannot be directly executed against an SUT because the suite is on the wrong level of abstraction.An executable test suite needs to be derived from a corresponding <b>abstract</b> <b>test</b> suite.The executable test suite can communicate directly with the system under test.This is achieved by mapping the <b>abstract</b> <b>test</b> <b>cases</b> to concrete <b>test</b> <b>cases</b> suitable for execution. In some model-based testing environments, models contain enough information to generate executable test suites directly.In others, elements in the <b>abstract</b> <b>test</b> suite must be mapped to specific statements or method calls in the software to create a concrete test suite. This is called solving the [...] "mapping problem".In the <b>case</b> of online <b>testing</b> (see below), <b>abstract</b> <b>test</b> suites exist only conceptually but not as explicit artifacts.|$|R
30|$|TTF (test {{template}} framework [6]) is {{an approach}} that generates unit tests from Z specifications. TTF is automated in the tool Fastest [7]. BETA and TTF/Fastest have some similarities; both generate unit tests using input space partitioning techniques and have similar steps in their approach. The input partitioning approach is different however. TTF uses as reference for coverage a unique formula describing the complete set of valid data, generating a set of positive <b>test</b> <b>cases,</b> while BETA defines positive and negative <b>test</b> <b>cases</b> by partitioning and combining specific characteristics for each input variable. TTF/Fastest was evaluated in several case studies, mainly focused on the evaluation of <b>abstract</b> <b>test</b> <b>cases</b> and comparisons with other approaches.|$|R
40|$|In this {{technical}} report we {{present the results}} of a case study on the application of a model-based testing method (MBT) to a real-world problem from the aviation industry. 							The requirements wereproposed by engineers working for the European aviation industry and comprise the landing gear system (LGS) of an aircraft. 							We developed a complete Z specification of the control software of the LGS. 							Then, we automatically generated <b>abstract</b> <b>test</b> <b>cases</b> by applying FASTEST (a tool implementing the Test Template Framework, which is a MBT method). 							These <b>test</b> <b>cases</b> cover all the functional and real-time scenarios described in the requirements. The manual work required to generate them is minimum...|$|R
40|$|Abstract—In {{this paper}} we {{address the problem of}} {{generating}} <b>abstract</b> <b>test</b> <b>cases</b> from a system modelled by a push-down automaton. Existing classical coverage criteria are based on states, transitions or loops in the automaton. This paper is based on a known theoretical result claiming that the accessible stack configurations in a push-down automaton form a regular language. We propose a new coverage criteria based both on states and on the configurations of the stack. Experimental results on a model of the Shunting Yard Algorithm are also presented. Keywords-Model based Testing, Push-down automaton, Cover-age criterio...|$|R
