32|1|Public
5000|$|The album {{includes}} a stripped-down cover of Tracy Chapman's [...] "Fast Car", of which Stewart said [...] "That song really, very, very, very directly shaped how I {{wanted to write}} lyrics or what I wanted songs to be <b>about...I</b> wanted them to turn out {{in so far as}} the song very specifically narrates some particular horrible things that happen to somebody and there's no positive resolution in the end at all." ...|$|E
5000|$|After a nine-month negotiation, in May 1985, Malcolm's {{book was}} {{accepted}} for publication, subject to certain revisions, by the OUP general books editor, Henry Hardy. In correspondence, Malcolm {{stated that he}} would only do further work if he received Oxford’s firm commitment to the book’s publication. Hardy gave him this commitment in a telephone call, which Malcolm recorded. In a subsequent letter, Hardy wrote,'I'm pleased {{that we are going}} to do your book, and hope that it's a terrific success.' OUP also had the book refereed by two Oxford philosophers, Alan Ryan of New College and Galen Strawson of St Hugh's. Ryan wrote, 'It's philosophically rather good, I think - it makes one of the shrewdest cases for a sort of Collingwoodian Idealism that I've read....Making Names is well worth doing, both because it is interesting in itself, and because it's a bold attempt to do philosophy in an unusual literary format.' Strawson reported, [...] "Making Names is really quite an attractive book. It is in no way crazy. It is very easy to read. Malcolm has a real gift for informal exposition....He is very clear and he knows what he's talking <b>about....I</b> think Making Names might prove extremely effective as an introduction to philosophical problems and procedures." ...|$|E
5000|$|... "So that's what I'm {{trying to}} do with the white wax pieces I'm doing now - they're about those times that are almost perfect but not quite. You go searching to meet them again, and you're all excited, and it's never quite the same - but you always have the memory. So it's not just about people passing, it's more about friendships that have gone awry or people who have strayed. Just basically, humanity. That's what all these pieces are <b>about.I</b> wanted to shift away from black, and I didn't know what I wanted to do, so I began to work with Irene Hultman. We did this whole installation, half black, half white, and there was also a {{performance}} in which she wore the pieces, or her dancers did. A lot of them come out of hat shapes or chandeliers. The wax is not a normal wax, it's made by a chemist so that it won't melt except at very high temperatures. It can get up to 180 degrees before it melts. In the summer my studio can get up to 120, 125, and in the winter I don't have heat so it's very cold. So these pieces {{have to be able to}} freeze." [...] Petah Coyne March 24, 1994. In her Studio, Greenpoint, Brooklyn ...|$|E
5000|$|To {{take your}} {{chance in the}} thick of a rush, with firing all <b>about,Is</b> nothing so bad when you've cover to 'and, an' leave an' likin' to shout;But to stand an' be still to the Birken'ead drill is a damn tough bullet to chew,An' they done it, the Jollies -- 'Er Majesty's Jollies -- soldier an' sailor too!Their work was done when it 'adn't begun; they was younger nor me an' you;Their choice it was plain between drownin' in 'eaps an' bein' mopped by the screw,So they stood an' was still to the Birken'ead drill, soldier an' sailor too ...|$|R
5000|$|Bashung {{said about}} the album in French {{magazine}} Les Inrockuptibles: [...] "J'ai volontairement fait disparaître cet album de mon intégrale, parce que j'estime qu'il ne me ressemble pas du tout. À l'origine, il devait s'intituler « Maquettes » et, avec le recul, il aurait vraiment mérité de rester dans les tiroirs. J'étais simplement satisfait d'avoir pu faire un album, à une époque où c'était encore un privilège réservé aux grands de la chanson, mais je n'avais encore <b>abouti</b> aucune de mes réflexions. J'avais un truc en tête, mais je cherchais encore confusément la manière de le faire. La production discographique n'était pas vraiment prête à investir dans le particularisme et, partant de ce constat, je n'avais pas ma place. Pas encore, en tout cas." [...] ("I voluntarily left this album out my integral, as I think it does not reflect me at all. Originally, {{it was to be}} named [...] "Maquettes" [...] (Demos), and after time passed, I really thought it should not have been released. I was just satisfied to have made an album, at a time when only the big names in music could afford the privilege, but I was not through thinking about what I wanted to achieve. I had things in my head, but I was still unsure about how to do it. The producers were not really ready to invest in uncommon things and, starting from there, I didn't have my place. Not yet, at least.") ...|$|E
5000|$|The Addy Series {{are based}} on the American Girl doll of the same name, one of the {{original}} Historical Characters. Pleasant Company, the owner of American Girl, approached Porter to write the series. The books follow Addy Walker, a young slave who runs away with her mother to freedom in Philadelphia during the American Civil War. There, she grows up and faces challenges in the urban North, such as learning to read and discrimination. [...] preparation for writing a novel with the heavy issue of slavery as the basis of the book, Ms. Porter did extensive research about 1864 to accurately portray the era and environment to help the reader achieve a real sense of being in 1864. [...] "As a writer, you have to put yourself in the place of other people and other worlds". She also refused to negotiate on the book's slavery theme, which was caused some controversy. Porter also tried to accurately portray the hardships surrounding slavery {{in a way that would}} engage her readers and not alienate them. [...] "Slavery is difficult for even adults to talk <b>about...I</b> didn't want to portray slavery as everyone on a porch or wearing beautiful clothes. It was a very awful period, but we're also talking about people who had loving families". [...] When Addy was introduced in 1993, over 1 million books were sold, Porter's popularity soared and more than 15,000 came to meet her. The Addy novels have sold more than 3 million copies.|$|E
50|$|En laissant à part les aspects dualistes propre à la Ve République, qui peuvent eux aussi contribuer à expliquer la solidité des gouvernements face à l'Assemblée, on a souvent remarqué les limites de la {{rationalisation}} technique du parlementarisme, celle qui repose sur l'introduction dans la constitution de mesures entravant le vote des motions de censures. Les premiers examples de rationalisation technique, qu'on trouve dans les constitutions rédigées à l'issue de la Première Guerre mondiale, notamment celle de la République de Weimar, elle aussi dotée d'un chef de l'État fort, ont totalement échoué, comme elles ont échoué sous la IVe République, ou encore dans la République italienne. En revanche, le parlementarisme britannique, totalement dépourvu de telles mesures, montre une grande stabilité. Cette stabilité est souvent attribuée au bipartisme, notamment par Michel Debré dans son discours au Conseil d'État ou encore par René Capitant, un des inspirateurs du régime, tous deux admirateurs du système britannique, fondé sur le scrutin majoritaire à un tour et le bipartisme. Bien que Michel Debré ait jugé sa transposition impossible en France, le scrutin majoritaire à deux tours, introduit en 1958 a <b>abouti</b> à des effets proches, avec des coalitions formées avant le scrutin et demeurant stables après. Lors qu'il ne relève que de la loi ordinaire (article 34), ce mode de scrutin contribue sans doute plus à la stabilité du gouvernement que le détail de l'article 49. On peut remarquer de même l'effet du changement de mode de scrutin en Italie en 1993, en l'absence de réforme majeure de la constitution, qui reste d'un strict parlementarisme moniste, d'ailleurs assez proche de celle de la IVe République. Cependant, le mode de scrutin ne garantit pas toujours l'existence de majorité, comme le montre l'example anglais dans les années 1920, ou encore la composition de l'Assemblée nationale entre 1988 et 1993.|$|E
5000|$|... "What Kind of Journey?It only remains now {{to reconsider}} the images that were made during {{a period in which}} I photographed tulips so assiduously; to {{question}} the nature of the tulip journey.At the conclusion of any extended piece of work, one inevitably questions the results. What have I learned during this journey? What has this intense period of activity been <b>about?I</b> learned little about tulips, not much - less perhaps than I could have learned in a few afternoons at the library. My search then was not a botanical one, nor, though I learned a little history (I hadn’t previously known of the period characterized as ‘tulipomania’), a historical one. I looked at images that might not otherwise have engaged my attention - obscure flower paintings, botanical illustrations - not however, as an art historian but as an image-maker seeking ideas and correspondences.The tulip journey, then was ultimately a visual journey, an investigation and discovery of visual possibilities. The tulip became an object of attention and fascination. It became both text and pretext for an activity of picture-making. The photographs are not finally, or not primarily, about tulips: they contain tulips. To say this is not to diminish the role of the tulip. Had the vase of flowers on the table when I made the first tentative exposures exploring the space of my kitchen been, let’s say daffodils, then the journey, if it had ever begun, would in all probability have been shorter.The daffodil, although it is a delightful flower, exhibits a stubborn rigidity of form; it lives and dies at attention. The tulip, however, is a flower of constant metamorphosis; it stretches towards the light and gestures to occupy the space.I spent much time just contemplating the flowers, with the camera far from my thoughts. I delighted in the tulips’ voluptuous presence. Such periods of contemplation, of visual pleasure, are always a necessary part of my work process. It is a deepening of my experience of, and of my relationship to, my subject.One cannot photograph experience, but to have lived it can change and develop habitual ways of seeing and of knowing." ...|$|E
40|$|Well I {{have nothing}} of {{interest}} to Wright <b>aboutI</b> will quit for this timeby asking you to Answersoon and give me all thenews for a soldier is never betterpleased than when they here fromhomeGive all my Respects toall inquiring friends Especiallyto W. L. ? Russell h Cripe sends his Compliments to you Your friend F. H. Kiser To Miss KateI will that gift for you first chance I ge...|$|E
40|$|NOTRE RECHERCHE S'INSCRIT DANS LE DOMAINE DU GENIE DES SYSTEMES INDUSTRIELS ET PLUS PARTICULIEREMENT DANS LE DOMAINE RELATIF AUX PLANS DE REPRISES D'ACTIVITE INFORMATIQUE (P. R. A.). LE BUT D'UN P. R. A. D'ASSURER LA PERENNITE DE L'ENTREPRISE EN CAS DE DISPARITION ACCIDENTELLE DES OUTILS INFORMATIQUES. LA PREMIERE PHASE DE CE TRAVAIL A <b>ABOUTI</b> A LA CREATION D'UNE LISTE DE PRE-REQUIS NECESSAIRE A L'UTILISATION D'UNE METHODOLOGIE DE MISE EN PLACE D'UN P. R. A [...] LA DEUXIEME PHASE, QUI L'ANALYSE LES DIFFERENTES METHODOLOGIES EXISTANTES RELATIVES AUX PLANS DE REPRISES D'ACTIVITE INFORMATIQUE, A MONTRE LE BESOIN D'UNE NOUVELLE METHODOLOGIE REPONDANT AUX EXIGENCES D'UN SITE DE PRODUCTION INDUSTRIEL. NOUS NOUS SOMMES BASES SUR UNE DEMARCHE DE GESTION DE PROJET. LA CREATION D'UNE METHODOLOGIE NOUS A AMENE A ETUDIER ET A COMPARER DES MODELES DE REPRESENTATIONS GRAPHIQUES ET A ADAPTER L'UN D'EUX, AFIN DE FORMALISER LES MODES OPERATOIRES GENERES ET FACILITER LA COMPREHENSION DES UTILISATEURS DE L'ENTREPRISE. NOUS AVONS AUSSI MONTRE QUE LES CONTRAINTES RELATIVES A L'ASSURANCE QUALITE ONT ETE UN ATOUT POUR PREPARER PUIS ACCOMPAGNER LA REALISATION ET LA VALIDATION DE LA METHODOLOGIE PROPOSEE. CE TRAVAIL A EGALEMENT PRIS EN COMPTE LES ASPECTS TECHNIQUES, ORGANISATIONNELS ET HUMAINS, BIEN QU'ILS RELEVENT DE DISCIPLINES SCIENTIFIQUES DIFFERENTES. NOTRE RECHERCHE A DONC ETE TRANSVERSALE. L'ANALYSE DES EXPERIMENTATIONS ISSUES DES DIFFERENTES PHASES DE LA RECHERCHE A <b>ABOUTI</b> A UNE METHODOLOGIE STRUCTUREE COMPOSEE DE DIFFERENTES ETAPES. CETTE METHODOLOGIE A ETE ELABOREE AVEC ET POUR TOUS LES COLLABORATEURS DE L'ENTREPRISE. NOUS PENSONS QUE CE TRAVAIL APPORTE UNE CONTRIBUTION A L'ELABORATION D'UN PLAN DE REPRISE D'ACTIVITE EN MILIEU INDUSTRIEL. NOUS ENVISAGEONS ENFIN DE NOUVEAUX AXES DE RECHERCHE POUR DEVELOPPER CETTE METHODOLOGIE. ANGERS-BU Lettres et Sciences (490072106) / SudocSudocFranceF...|$|E
40|$|The WTO {{exists and}} Algeria has applied to be a member. This {{application}} {{process has been}} going on for over 23 years and has not reached a conclusioon yet. The aim of this thesis is to explain the reasons both obvious and hidden for the slowness of the process based on the situation of Algeria and on the working rules of the multilateral trade system. L'OMC existe et l'Algérie a présenté sa candidature. Cette procédure d'accession dure depuis plus de 23 ans et n'a toujours pas <b>abouti.</b> L'objet de cette thèse est d'expliquer les raisons apparentes et profondes de la lenteur de cette candidature en s'appuyant sur la situation de l'Algérie et sur les modalités de fonctionnement du système commercial multilatéral...|$|E
40|$|International audienceThe {{recourse}} against notes {{taken on}} landlaw violation from article 183 {{is not only}} the prerogative of courts and tribunals owing to the fact that article 184 of the same law lays down administrative recourse for correction or cancellation. Due to the theory of the detachable act, the ordinances as well as the decrees which validate the contracts of concessions are likely of a jurisdictional recourse if the administrative recourse did not succeed. While the contracts of concession are likely legal recourse, i. e., in front of the judge of the legal order when the administrative recourse did not also succeed. The law approving the contract can be the subject of the control of constitutionality by the constitutional judge. Le recours contre les actes pris en violation de l'article 183 de la loi foncière n'est pas seulement l'apanage des cours et tribunaux du fait que l'article 184 de la même loi prévoit le recours administratif préalable pour correction ou annulation. En vertu de la théorie de l'acte détachable, les ordonnances ainsi que les arrêtés qui valident les contrats de concession sont susceptibles d'un recours juridictionnel au cas où le recours administratif n'a pas <b>abouti.</b> Tandis que les contrats de concession sont susceptibles du recours judiciaire, c'est-à-dire, devant le juge de l'ordre judiciaire lorsque le recours administratif n'a pas aussi <b>abouti.</b> La loi approuvant le contrat peut faire l'objet du contrôle de constitutionalité par le juge constitutionnel. ABSTRACT: The recourse against notes taken on landlaw violation from article 183 {{is not only the}} prerogative of courts and tribunals owing to the fact that article 184 of the same law lays down administrative recourse for correction or cancellation. Due to the theory of the detachable act, the ordinances as well as the decrees which validate the contracts of concessions are likely of a jurisdictional recourse if the administrative recourse did not succeed. While the contracts of concession are likely legal recourse, i. e., in front of the judge of the legal order when the administrative recourse did not also succeed. The law approving the contract can be the subject of the control of constitutionality by the constitutional judge...|$|E
40|$|CE TRAVAIL A ETE DEVELOPPE AUTOUR DE TROIS AXES. LE PREMIER EST CELUI DE LA MODELISATION MATHEMATIQUE QUI A <b>ABOUTI</b> A LA MISE AU POINT D'UN MODELE MIXTE DERIVE-DIFFUSION/HYDRODYNAMIQUE SIMPLIFIE SUR UN TRANSISTOR DE TYPE MODFET ALGAAS/INGAAS/GAAS. UN DEUXIEME AXE SE SITUE DANS LE CADRE DE L'ANALYSE MATHEMATIQUE OU LE SYSTEME ETUDIE EST ELLIPTIQUE NON LINEAIRE COUPLE, ISSU DU MODELE HYDRODYNAMIQUE SIMPLIFIE. IL EST CONSTITUE DES EQUATIONS DE POISSON ET DE CONTINUITE, RESPECTIVEMENT POUR LE POTENTIEL ELECTROSTATIQUE ET LE COURANT, ET D'UNE EQUATION D'ENERGIE POUR LA TEMPERATURE ELECTRONIQUE. NOUS AVONS MONTRE, DANS LE CAS D'UNE GEOMETRIE DE TYPE DIODE, OU LES BORDS DE DIRICHLET ET DE NEUMANN SE REJOIGNENT A ANGLES DROITS UNIQUEMENT, QUE TOUTE SOLUTION DE CE SYSTEME ADMET UNE SOUS-SUITE DE SOLUTIONS QUI CONVERGE, EN NORME L, VERS UNE SOLUTION DU PROBLEME CONTINU. DES RESULTATS DANS LE CAS D'UN MESFET SONT AUSSI DONNES. LA TROISIEME PARTIE DE CE TRAVAIL PORTE SUR DES SIMULATIONS NUMERIQUES D'UN TRANSISTOR ALGAAS/INGAAS/GAAS. NOUS TESTONS LES TROIS MODELES : DERIVE-DIFFUSION, HYDRODYNAMIQUE SIMPLIFIE ET MIXTE. LA DISCRETISATION S'EFFECTUE PAR ELEMENTS FINIS : STANDARDS POUR LES EQUATIONS DE POISSON ET DE CONTINUITE ET NON STANDARDS, DE TYPE STREAMLINE DIFFUSION POUR L'EQUATION DE LA TEMPERATURE. L'ANALYSE DES CARACTERISTIQUES DRAIN-SOURCE MONTRE QUE LE MODELE MIXTE REPRESENTE UN BON COMPROMIS POUVANT DONNER DE MEILLEURS RESULTATS QUE LE MODELE DERIVE-DIFFUSION EN FOURNISSANT UN EFFORT NUMERIQUE MOINDRE QUE LE MODELE HYDRODYNAMIQUE SIMPLIFIE. REIMS-BU Sciences (514542101) / SudocSudocFranceF...|$|E
40|$|International audienceThis article {{seeks to}} examine {{the role of the}} grammaticalization process in two “modal” structures: happen to and appear to. The first part adopts a {{synchronic}} approach and, after considering some basic facts about the grammaticalization process, happen to and appear to, it highlights the four main characteristics of the grammaticalization process, and examines them in relation to the structures under study. The second part adopts a diachronic point of view; research on the Corpus of Historical English and on The Complete Shakespeare’s Works provides an insight into the development of happen to and appear to. The question of the degree of grammaticalization for these two structures will also be tackled, to reach the conclusion that even if happen to and appear to were created through a grammaticalization process, {{it can be argued that}} the process is incomplete, morphosyntactically speaking. Cet article examine le rôle du processus de grammaticalisation dans deux structures de type « modal » : happen to and appear to. La première partie adopte une approche synchronique ; après avoir rappelé quelques principes fondamentaux du processus de grammaticalisation, elle les rapproche de ces structures. La seconde partie adopte, quant à elle, une approche diachronique, des recherches effectuées sur le Corpus of Historical English et sur The Complete Shakespeare’s Works tente d’expliquer le développement de happen to et de appear to. La question du degré de grammaticalisation de ces deux structures est finalement abordée, pour conclure que si happen to et appear to ont bien été créé par un processus de grammaticalisation, on peut cependant avancer que ce dernier n’est pas <b>abouti,</b> morphosyntaxiquement parlant...|$|E
40|$|ABSTRACT. Alikomiak and Tatamigana {{were the}} first Inuit tried and {{executed}} for murder under Canadian law. The case was the third {{in a series of}} killings of outsiders by Inuit in the western Arctic which had begun in 1912; the first two had resulted in more lenient treatment. The trial of these two men, which took place in the summer of 1923, was in the nature of a show trial, designed by the federal government to show the Inuit that the authorities would no longer tolerate such acts of violence. It was also meant to be a demonstration to the world of Canada’s sovereign rights in the Arctic, rights which had an uncertain foundation in international law. The conviction and execution of Alikomiak and Tatamigana caused controversy at the time; sentiment for clemency was based on claims (made then and subsequently) that Inuit were simple and primitive, and did not understand the principle of execution for murder. It is asserted here, however, that the sentence was entirely in keeping with Inuit custom, and that alternatives to execution suggested by those with better knowledge of the North were in some ways even harsher than capital punishment. Extracts from the capital case file and the transcripts of the trial make these points clear. Key words: Alikomiak, Tatamigana, law enforcement, sovereignty, Inuit RÉSUMÉ. Alikomiak et Tatamigana ont été les premiers Inuit poursuivis en justice et exécutés pour meurtre en vertu de la loi canadienne. Cette cause était la troisième d’une série de meurtres d’étrangers commis par les Inuit dans l’Arctique occidental, série qui avait débuté en 1912; les deux premiers meurtres avaient <b>abouti</b> à un traitement moins sévère. Le procès de ces deux hommes, qui eut lieu durant l’été de 1923, avait quelque chose d’un procès de justification, conçu par le gouvernement fédéral pour montre...|$|E
40|$|EN 1975, LA GUERRE ECLATE SUR LE SOL LIBANAIS, ELLE NE PREND FIN QU'A LA SUITE DE L'INTERVENTION DES ACCORDS DE TAEF EN 1989. EN 1990, LA CONSTITUTION LIBANAISE DE 1926 EST REVISEE ET DONNE NAISSANCE A UNE DEUXIEME REPUBLIQUE. DEPUIS CETTE DATE, LES BOMBARDEMENTS ONT CESSE. CE N'EST POUR AUTANT PAS LA NAISSANCE D'UN LIBAN NOUVEAU. LE PAYS SURVIT TANT BIEN QUE MAL SOUS PLUSIEURS OCCUPATIONS, MEURTRI, RUINE, DEVASTE. SUITE A UNE REUNION MINISTERIELLE DE LA LIGUE ARABE, UN COMITE ARABE TRIPARTITE FORME DE L'ALGERIE, LE MAROC ET L'ARABIE SEOUDITE S'EST CONSTITUE EN 1989 AFIN D'ETUDIER UN DOCUMENT QUI METTRAIT FIN A LA "CRISE LIBANAISE". LES TRAVAUX DU TRIUMVIRAT ONT <b>ABOUTI,</b> LE 22 OCTOBRE 1989, A LA CONCLUSION DES ACCORDS DE TAEF. CEUX-CI SONT APPROUVES, A LA CONCLUSION DES ACCORDS DE TAEF. CEUX-CI SONT APPROUVES, LE 5 NOVEMBRE DE LA MEME ANNEE PAR 52 DES 57 DEPUTES LIBANAIS ELUS EN 1972. ILS TRAITENT SUCCESSIVEMENT DES REFORMES CONSITUTIONNELLES, DU RETABLISSEMENT DE LA SOUVERAINETE LIBANAISE SUR TOUT LE TERRITOIRE, DE LA LIBERATION DU SUD LIBAN ET DES RELATIONS LIBANO-SYRIENNES. IN 1975, THE WAR BROKE OUT ON THE LEBANESE SOIL. IT ENDED ONLY AFTER THE INTERVENTION OF THE TA'IF ACCORDS IN 1989. IN 1990, THE LEBANESE 1926 CONSTITUTION WAS REVISED AND GAVE BIRTH TO A SECOND REPUBLIC. SINCE THEN BOMBING HAS CEASED. NEVERTHELESS IT IS NOT A NEW LEBANON. THE CONTRY SOMEHOW OR OTHER MANAGES TO SURVIVE UNDER SEVERAL OCCUPATIONS, BRUISED, RUINED, DEVASTATED. FOLLOWING A MINISTERIAL SUMMIT OF THE ARAB LEAGUE, A TRIPARTITE ARAB COMMITTEE COMPOSED OF ALGERIA, MOROCO AND SAOUDI ARABIA WAS SET UP IN 1989 SO AS TO PREPARE A DOCUMENT THAT WOULD PUT AN END TO THE "LEBANESE CRISIS". THE WORK OF THE TRIUMVIRATE INDED, IN OCTOBER 22 1989, WITH THE CONCLUSION OF THE TA'IF ACCORDS. THEY WERE APPROVED, NOVEMBER 5 OF THE SAME YEAR, BY 52 OF THE 57 MEMBERS OF THE LEBANESE PARLIAMENT ELECTED IN 1972. THEY DEAL SUCCESSIVELY WITH CONSTITUTIONAL REFORMS, REESTABLISHING LEBANESE SOVEREIGNITY OVER THE WHOLE THE TERRITORY, THE LIBERATION OF SOUTHERN LEBANON AND THE LIBANO-SYRIAN RELATION...|$|E
40|$|CE MEMOIRE SE PROPOSE DE DEMONTRER LA FAISABILITE DE L'INTEGRATION SUR SILICIUM D'UNE ARCHITECTURE A BASSE-FREQUENCE INTERMEDIAIRE GSM. DANS LE CHAPITRE I, NOUS AVONS RAPPELE, LES ATOUTS DU SYSTEME CELLULAIRE GSM; STANDARD MONDIALEMENT UTILISE. DANS LE CHAPITRE II, QUELQUES NOTIONS FACILITANT LA COMPREHENSION DU FONCTIONNEMENT DES RECEPTEURS ONT ETE DETAILLEES. DANS LE CHAPITRE III, NOUS AVONS COMPARE LES PERFORMANCES DES TROIS ARCHITECTURES INTEGRABLES : A CONVERSION DIRECTE, A QUASI-FI ET A BASSE-FI, EN SE BASANT SUR LES CRITERES DE COMPARAISONS SUIVANTS : CONSOMMATION, COUTS, SENSIBILITE AU BRUIT, A LA COMPOSANTE CONTINUE AINSI QU'A LA REJECTION IMAGE. LE CHAPITRE IV A ETE CONSACRE A L'ETUDE DE LA REJECTION IMAGE ET AVONS <b>ABOUTI</b> A UN CHOIX OPTIMAL DE LA FREQUENCE INTERMEDIAIRE. APRES AVOIR EXPLIQUE L'INFLUENCE SUR LA REJECTION IMAGE DES ERREURS D'APPARIEMENT ENTRE LES VOIES EN QUADRATURES D'UN RECEPTEUR, NOUS AVONS PROPOSE UNE METHODE INNOVANTE DE COMPENSATION NUMERIQUE. DANS LE CHAPITRE V, UNE ETUDE SYSTEME VISANT A DETERMINER LES CARACTERISTIQUES DES ETAGES COMPOSANT LA CHAINE DE RECEPTION A BASSE-FI A ETE DECRITE AFIN DE SATISFAIRE AUX EXIGENCES DU STANDARD GSM. L'INFLUENCE DES PERFORMANCES EN BRUIT, GAIN, LINEARITE ET REJECTION, AINSI QUE LE CHOIX DES COMPROMIS ENTRE LES ETAGES RF ET BF ONT PERMIS D'ABOUTIR AU DIMENSIONNEMENT D'UNE CHAINE DE RECEPTION. DANS LE CHAPITRE VI, NOUS AVONS MENE UNE ETUDE THEORIQUE SUR LES ORIGINES DU BRUIT DE PHASE DANS UN OSCILLATEUR EN PROPOSANT UN NOUVEAU MODELE EQUIVALENT NON-LINEAIRE PERMETTANT D'INTERPRETER ET DE QUANTIFIER LE REPLIEMENT DU BRUIT AUTOUR DE LA RAIE D'OSCILLATION. CE MODELE PERMET DE COMPARER LES DIFFERENTES CAUSES DE REPLIEMENTS ET D'EFFECTUER UNE ANALYSE DE SENSIBILITE DU BRUIT DE PHASE AUX PARAMETRES MIS EN JEU TELS QUE : LE FACTEUR DE QUALITE DE L'INDUCTANCE INTEGREE, OU LES PERFORMANCES DE LINEARITE L'AMPLIFICATEUR. ENFIN, LE CHAPITRE V FUT CONSACRE A LA CONCEPTION DES FONCTIONS, ET A L'INTEGRATION COMPLETE D'UN DEMONSTRATEUR GSM A BASSE-FI EN TECHNOLOGIE BICMOS 6 M. LES DIFFERENTES MESURES REALISEES MONTRENT QUE L'ARCHITECTURE A BASSE-FI NE CONNAIT PAS D'OBSTACLE MAJEUR A L'INTEGRATION. LA CONSOMMATION GLOBALE DE CETTE ARCHITECTURE TOTALEMENT INTEGREE RESTE TOUTEFOIS LEGEREMENT SUPERIEURE A CELLES RELEVEES SUR LES DIFFERENTS RECEPTEURS HETERODYNES. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|E
40|$|De zoektocht naar transnationaal gezag. De anti-apartheidsbewegingen in de Europese Gemeenschap De anti-apartheidsbeweging is wijd erkend als een belangrijke mondiale actor uit de tweede helft van de twintigste eeuw. Dit artikel levert een bijdrage tot recente inspanningen om anti-apartheid te begrijpen als een trans{{nationale}} sociale beweging. De aandacht gaat vooral naar de anti-apartheidsorganisaties in de landen van de Europese Gemeenschap (eg) en hun pogingen om het besluitvormingsproces van nationale overheden te beïnvloeden. Het eg-proces van samenwerking rond buitenlands beleid, zo wordt betoogd, vormde een ernstige beperking op lobbyactiviteiten die gericht waren op het nationale niveau. Anti-apartheidsorganisaties voelden zich dan ook genoodzaakt om hun activiteiten op een transnationaal niveau te coördineren. Dit bereikte een hoogtepunt met de oprichting van de Liaison Group of National Anti-Apartheid Movements in the Countries of the European Community. The Quest for Transnational Authority. The Anti-Apartheid Movements of the European Community The {{anti-apartheid movement}} has been widely recognized as an important global actor {{in the second half}} of the twentieth century. This article makes a contribution to recent efforts to understand anti-apartheid as a transnational social movement. Its primary focus lies with the anti-apartheid organizations in the countries of the European Community (ec) and their attempts to influence the decision making of their national governments. The ec process of foreign policy cooperation, it is argued, offered a serious constraint to lobbying activities geared at the national level. Antiapartheid organizations therefore felt compelled to coordinate their activities at a transnational level. This culminated in the creation of the Liaison Group of National Anti-Apartheid Movements in the Countries of the European Community. La quête de l’autorité transnationale. Les mouvements anti-apartheid dans la Communauté européenne Le mouvement anti-apartheid est largement reconnu comme un acteur mondial important dans la seconde moitié du xxe siècle. Cet article étudie ce mouvement en tant que mouvement social transnational. L’accent est mis sur les organisations anti-apartheid dans les pays de la Communauté européenne et leurs tentatives pour influencer les décisions de leurs gouvernements nationaux. D’après l’auteur, le processus de coopération européenne concernant la politique étrangère entravait le lobbying à l’échelon national. Par conséquent, les organisations anti-apartheid se sentaient obligées de coordonner leurs activités au niveau transnational. Cela a <b>abouti</b> à la création du Groupe de liaison des mouvements nationaux anti-apartheid dans les pays de la Communauté européenne. Goedertier Wouter. The Quest for Transnational Authority, the Anti-Apartheid Movements of the European Community. In: Revue belge de philologie et d'histoire, tome 89, fasc. 3 - 4, 2011. pp. 1249 - 1276...|$|E
40|$|THIS STUDY WAS AIMED AT SHOWING THAT OPTICAL METHODS CAN BE USED IN NON-INVASIVE, BIOPHYSICAL INVESTIGATIONS TO ASSESS THE EXTENT OF ACUTE CUTANEOUS RADIATION SYNDROME FURTHER TO EXPOSURE TO g RADIATIONS. THE FIRST PART EXPLAINS WHY WE CHOSE POLARIZATION AS CONTRAST AGENT WITHIN THE FRAMEWORK OF THIS STUDY. THE SKIN BEING A HIGHLY SCATTERING MEDIUM, IT LED US TO SELECT THE MUELLER POLARIMETRY, WELL-SUITED FOR SUCH INVESTIGATIONS. THE SECOND PART INTRODUCES THE THEORETICAL FOUNDATIONS USED IN THE ANALYSIS OF THE EXPERIMENTAL RESULTS FROM MEASUREMENT OF THE MEDIUM MUELLER MATRIX. BUT, THE MATRIX INTERPRETATION IS NOT IMMEDIATE. WE, THUS, DEVELOPED A DEPOLARIZING AND NON-DEPOLARIZING MUELLER MATRICES DECOMPOSITION AND CLASSIFICATION ALGORITHM, WHICH WAS VALIDATED WITH A SET OF SAMPLES WITH DIFFERENT POLARIMETRIC PROPERTIES. THE THIRD PART DESCRIBES THE POLARIMETER ALONG WITH THE ANGULAR AND SPECTRAL RESULTS OBTAINED WITH IRRADIATED SKIN SAMPLES FREE FROM VISIBLE CLINICAL SIGN. THE ABOVE ALGORITHM ALLOWED US TO EVIDENCE TWO POLARIMETRIC CONTRAST AGENTS, WHICH ALONE OR TOGETHER PERMITTED US TO DIFFERENTIATE AN IRRADIATED SKIN FROM A HEALTHY ONE. THE GOOD CORRELATION OF OUR RESULTS WITH HISTOLOGICAL STUDIES MADE BY BIOLOGIST SCIENTISTS DEMONSTRATES THAT POLARIZATION CAN BE USED AS CONTRAST AGENT AT LOW IRRADIATION RATES. THE LAST PART HIGHLIGHTS THE INTEREST OF COMPLEMENTING POLARIMETRY WITH A COHERENT OPTICAL METHODOLOGY (SPECKLE, OPTICAL COHERENCE TOMOGRAPHY) IN ORDER TO LOCALIZE CUTANEOUS LESIONS. CE TRAVAIL AVAIT POUR OBJECTIF DE MONTRER LA POSSIBILITE D'UTILISER DES METHODES OPTIQUES DANS L'INVESTIGATION BIOPHYSIQUE NON INVASIVE DU SYNDROME CUTANE D'IRRADIATION AIGUË, POUR DES DOSES D'IRRADIATION RELATIVEMENT FAIBLES. LA PREMIERE PARTIE REVIENT SUR LES MOTIVATIONS QUI ONT <b>ABOUTI</b> AU CHOIX DE LA POLARISATION COMME AGENT DE CONTRASTE DANS LE CADRE DE CETTE ETUDE. NOUS AVONS ALORS OPTE POUR LA POLARIMETRIE DE MUELLER, TECHNIQUE ADAPTEE A L'ETUDE D'UN MILIEU DEPOLARISANT TEL QUE LA PEAU. LA DEUXIEME PARTIE POSE LES BASES THEORIQUES DANS L'INTERPRETATION DES RESULTATS OBTENUS A PARTIR DE LA MESURE DE LA MATRICE DE MUELLER D'UN MILIEU. LA LECTURE DE CETTE DERNIERE N'ETANT PAS IMMEDIATE, UN ALGORITHME DE DECOMPOSITION ET DE CLASSIFICATION DES MATRICES DE MUELLER DEPOLARISANTES ET NON DEPOLARISANTES A ETE DEVELOPPE. CELUI-CI EST VALIDE SUR UNE SERIE D'ECHANTILLONS DE NATURES TRES DIVERSES. LA TROISIEME PARTIE PRESENTE LE POLARIMETRE ET LES RESULTATS, ANGULAIRES ET SPECTRAUX, OBTENUS SUR DES ECHANTILLONS DE PEAU IRRADIES OU IL N'Y A AUCUN SIGNE CLINIQUE VISIBLE. EN UTILISANT L'ALGORITHME PRECEDENT NOUS AVONS MIS EN EVIDENCE DEUX AGENTS DE CONTRASTE POLARIMETRIQUE QUI SEULS OU COMBINES APPORTENT UNE INFORMATION SUR LE TAUX D'IRRADIATION. CES RESULTATS SONT CONFORTES PAR UNE ETUDE HISTOLOGIQUE MENEE EN PARALLELE PAR L'IRSN. NOUS AVONS AINSI MONTRE QUE LA POLARISATION PEUT ETRE UN AGENT DE CONTRASTE POUR DE FAIBLES DOSES D'IRRADIATION. LA DERNIERE PARTIE MONTRE L'INTERET D'UTILISER EN COMPLEMENT A LA POLARIMETRIE UNE METHODOLOGIE D'OPTIQUE COHERENTE (SPECKLE, TOMOGRAPHIE PAR COHERENCE OPTIQUE) POUR LOCALISER LES ALTERATIONS CUTANEES...|$|E
40|$|DE PAR SON APPROCHE NOUVELLE, LA RADIO IMPULSIONNELLE ULTRA LARGE BANDE (UWB-IR) EST PORTEUSE DE NOMBREUSES PROMESSES EN TERMES DE DEBIT, DE ROBUSTESSE ET DE FAIBLE CONSOMMATION. DU FAIT DE SA LARGEUR DE BANDE (SUPERIEURE A 500 MHZ), ELLE OFFRE EGALEMENT LA POSSIBILITE DE FAIRE DE LA GEOLOCALISATION AVEC UNE PRECISION SUBMETRIQUE. CE TRAVAIL DE THESE A DEBUTE ALORS QUE LES PREMIERES PUBLICATIONS SUR L'UWB PRESENTAIENT DES RESULTATS DE SIMULATION EXTRAORDINAIRES. AFIN DE MIEUX CERNER SON REEL POTENTIEL DANS LE DOMAINE DES COMMUNICATIONS BAS DEBIT AUQUEL L'UWB-IR SEMBLE PARTICULIEREMENT ADAPTEE, CETTE ETUDE A ETE ORIENTEE VERS LA REALISATION MATERIELLE D'UNE CHAINE DE COMMUNICATION, SOUS-TENDUE PAR LA CONTRAINTE DE FORTE REDUCTION DE COMPLEXITE. CETTE ETUDE COMMENCE PAR LA SPECIFICATION D'UNE COUCHE PHYSIQUE ADAPTEE A LA TECHNOLOGIE ET AU DOMAINE D'APPLICATION ENVISAGE, QUI REPOSE SUR UN SCHEMA DE TRANSMISSION TRES SIMPLE. LA PLATE-FORME D'EMISSION REALISEE EST BASEE SUR UNE ARCHITECTURE TRES SIMPLE ET DES COMPOSANTS DISCRETS DE BAS DE GAMME. ELLE DEMONTRE AINSI LA POSSIBILITE D'EMBARQUER UNE TELLE STRUCTURE DANS UN OBJET COMMUNICANT AUTONOME DE FAIBLE COUT. LA CHAINE DE RECEPTION SUIT UNE APPROCHE ORIGINALE BASEE SUR UN DETECTEUR D'ENVELOPPE ET UN COMPARATEUR A SEUIL VARIABLE, CE QUI PERMET DE RELACHER CERTAINES CONTRAINTES BLOQUANTES COMME CELLES LIEES A L'ACQUISITION DE SYNCHRONISATION. UN ENSEMBLE D'ALGORITHMES DE RECEPTION A FAIBLE COMPLEXITE PERMET D'EXPLOITER AU MIEUX CETTE STRUCTURE DE DETECTION EN LEVANT DIFFERENTS VERROUS TECHNOLOGIQUES. PAR CE TRAVAIL, UNE REFLEXION GLOBALE SUR UN SYSTEME UWB-IR BAS DEBIT A ETE MENEE ET A <b>ABOUTI</b> A LA REALISATION D'UN LIEN RADIO PHYSIQUE QUI DEMONTRE LA VIABILITE TECHNIQUE DE CETTE TECHNOLOGIE EN RUPTURE. DE PLUS, LES RESULTATS OBTENUS ONT ETE LA BASE D'UNE PROPOSITION COMPLETE PORTEE EN NORMALISATION. FROM ITS NEW APPROACH, THE IMPULSE RADIO ULTRA WIDE BAND (IR-UWB) TECHNOLOGY IS CARRYING MANY PROMISES IN TERMS OF TROUGHPUT, ROBUSTNESS, AND LOW ENERGY CONSUMPTION. BECAUSE HIS BANDWIDTH (HIGHER THAN 500 MHZ), IT ALSO MAKES IT POSSIBLE TO DO HIGH ACCURACY GEOLOCALISATION. THIS THESIS WORK BEGAN WHEREAS THE FIRST PUBLICATIONS ON UWB HAD EXTRAORDINARY SIMULATION RESULTS. IN ORDER TO BETTER DETERMINE IR-UWB REAL POTENTIAL IN THE FIELD OF LOW THROUGHPUT COMMUNICATIONS TO WHICH THEIR ONE SEEMS PARTICULARLY ADAPTED, THIS STUDY WAS DIRECTED TOWARDS THE PROTOTYPING OF AN UWB LINK, UNDERLAIN BY THE CONSTRAINT OF STRONG COMPLEXITY REDUCTION. THIS STUDY STARTS WITH THE SPECIFICATION OF A PHYSICAL LAYER ADAPTED TO THE TECHNOLOGY AND THE APPLICABILITY CONSIDERED, WHICH IS BASED ON A VERY SIMPLE TRANSMISSION SCHEME. THE TRANSMITTER IS BASED ON A VERY SIMPLE ARCHITECTURE BUILD WITH LOW COST "OFF THE SHELF" COMPONENTS AND THUS SHOWS THE POSSIBILITY OF EMBARKING SUCH A STRUCTURE IN AN AUTONOMOUS LOW COST COMMUNICATING OBJECT. THE RECEIVER STRUCTURE FOLLOWS AN ORIGINAL APPROACH BASED ON A VIDEO DETECTOR AND A VARIABLE THRESHOLD COMPARATOR, WHICH MAKES IT POSSIBLE TO SLACKEN CERTAIN BLOCKING CONSTRAINTS LIKE THOSE RELATED TO SYNCHRONIZATION. A SET OF LOW COMPLEXITY RECEIVING ALGORITHMS IS ALSO DEVELOPED TO ENABLE THE USE OF THIS STRUCTURE WHILE RAISING VARIOUS TECHNOLOGICAL DEADLOCK. BY THIS WORK, A GLOBAL REFLEXION ON AN LOW DATA RATE IR-UWB SYSTEM WAS CONDUCTED AND LED TO THE REALIZATION OF A PHYSICAL RADIO LINK WHICH SHOWS THE TECHNICAL VIABILITY OF THIS TECHNOLOGY. MOREOVER, THE RESULTS OBTAINED WERE THE BASE OF A FULL PROPOSAL CARRIED IN STANDARDIZATION. RENNES-INSA (352382210) / SudocSudocFranceF...|$|E
40|$|Durant quatre années l'Azienda Generale Italiana Petroli (AGIP), la Société Nationale Elf Aquitaine, l'Institut Français du Pétrole (IFP) et Total Compagnie Française des Pétroles ont joint leurs efforts pour développer un {{outil de}} géologie structurale et un {{programme}} de rétro-déformation. Le but était d'améliorer le pointé des sections sismiques en validant l'interprétation faite par des tests de cohérence géométrique. Une section qui peut être rétro-déformée en suivant le principe de conservation de la matière est dite équilibrée. Ce projet de recherche a <b>abouti</b> à la réalisation de deux logiciels LOCACE et BALISS. LOCACE (LOgiciel de Construction Assistée de Coupes Equilibrées) travaille à partir d'une section géologique. BALISS (BALanced Interpretation of Seismic Sections) permet d'appliquer à l'image sismique profondeur les principes de l'équilibrage. Cet article rappelle les fondements de cette méthode, les différents types de déformation - rigide, couche à couche, cisaillement simple - leur mise en oeuvre dans LOCACE/BALISS et la philosophie de l'utilisation de ces logiciels. The {{need to use}} concepts of structural geology for seismic interpretation {{is something that is}} often difficult to apply because of the complexity of the zones investigated and especially {{because of the lack of}} interaction among specialists in the different areas of exploration. Likewise, quantitative geology is fairly recent, and computerized tools for structural geology are still rare although there is a multitude of them in geophysics. The existence of graphic workstations and computerized tools favoring the effective interactivity of software packages now makes it possible to integrate this implementation. The two French oil companies, Total Compagnie Française des Pétroles and Elf Aquitaine, together with Institut Français du Pétrole (IFP) have joined their efforts since 1987 to develop a tool for testing the geologic and geometrical coherence of sections. Azienda Generale Petroli (AGIP) joined this research group in 1988. Two software packages have been created. The first one, LOCACE (LOgiciel de Construction Assistée de Coupes Equilibrées), is a tool for geologists wanting to balance a section, i. e. to check to see whether the present geometry is compatible with a probable prior state and that a deformation path exists without any loss of material between the two states. The cross-section can then be restored to build its geometry before its deformation and thus to quantify the history of the burial and/or emersion of series. The second software, BALISS (BALanced Interpretation of Seismic Sections), has to do with the depth seismic image and serves to improve picking by using the results of a retrodeformation performed with LOCACE. In particular, it horizontalizes reflectors, not solely by an offsetting of traces along the vertical, but according to geologic deformation criteria that combine the conservation of areas and lengths. Four deformation modes are used to describe large-scale upper-crustal deformations. First, the flexural-slip method in which the slip occurs parallel to the beds (Fig. 1). Flexural slip is a line-balancing technique based on the deformation of a dip-domain grid and preserving both bed lengths and area. LOCACE is use to apply this method even for layers with thickness variations (Fig. 2). Then the simple shear method, in which shear is imposed in a given direction), might be the right method for synsedimentary faulting in an extensional area but might lead to wrong results in a compressional area (Fig. 4). Rigid deformation, i. e. rotation and translation, can be added to any of these methods together with decompaction, which is computed by standard backstripping method that takes various lithologies into account. For incompetent layers, such as a salt sheet for instance, overall area conservation may be applied. After describing the basic principles of the balancing of geologic sections and the different deformation modes formulated as equations by LOCACE, we show how these principles can be used for restoring a seismic image as can be done with BALISS...|$|E
40|$|En reprenant l’ensemble des {{publications}} de Jacques Cauvin, on montre comment s’est progressivement dégagé le rôle de la « religion » ou plus exactement l’évolution du « milieu intérieur », comme moteur de la néolithisation du Proche-Orient. À l’époque, cette idée prenait alors le contre-pied des idées dominantes dans la communauté scientifique qui faisait de la néolithisation une réponse à des contraintes environnementales extérieures à l’homme. Les découvertes récentes semblent bien aller dans le sens d’un primat du symbolique sur le climatique ou l’économique dans le processus qui a <b>abouti</b> à la « révolution » néolithique. His university {{studies of}} literature and philosophy set J. Cauvin apart in the “ tribe” of French prehistorians. The genesis of his theory {{on the relationship between}} the «birth of the gods » and the « birth of agriculture » dates back to the 1970 s. He was already opposed to the «dominant materialist model » according to which neolithisation was an adaptive response to external environmental constraints. This was a model championed by prehistorians of the Anglo-Saxon school. Cauvin proposed on the contrary that the evolutionary emergence of an internal factor was the «prime mover » of the Neolithic revolution. He had announced this theory in a little book published in 1972, entitled Religions néolithiques de Syro-Palestine. He set it in line of descent from A. Leroi-Gourhan, who had published his own Les religions de la Préhistoire in 1964. Cauvin returned to the question in his second book, which was also somewhat limited in its readership, Les premiers villages de Syrie-Palestine, published in 1978; there, an entire chapter was dedicated to « artistic and religious documents ». Over that decade, his original ideas scarcely reached beyond a narrow circle of specialists, {{with the exception of a}} short article that he contributed to La Recherche in 1973. In that article, he underlined the role of a «bull cult » and the multiplication of female figurines, the ancestors of the later representations of a «fertility goddess (or mother-goddess) ». In the 1980 s, Cauvin increased the number of articles directed at a wider public (1985 e, 1987), and at more specialised audiences, such as theologians (1983, 1986 a, 1985 f), or philosophers and historians (1985 d, 1986 b). At that time professional prehistorians were not his target audience. However, two subjects that were more clearly meant for them— animal representations and prehistoric matriarchy— were dealt with in the same year (1985 a and 1985 b). The 1990 s saw the publication of Cauvin’s magnum opus (1994), which returned to all the themes that he had previously discussed. This book took up the challenge of addressing both his Francophone prehistorian colleagues and the general public. Its success was such that a second edition was soon needed (1997). But it was necessary to wait until 2000 and the English translation by T. Watkins (2000 a) before Cauvin’s ideas finally reached the community of Near Eastern prehistorians, of whom the majority are Anglophone. His theory gave rise to debates, in which his premature death prevented him from taking part, save for two short articles in English (2000 c and 2001). Thus it was that, after a period of slow maturation, and having been put to the test in different public arenas, Jacques Cauvin’s ideas stand out as an important stage in the long history of theories that propose explanations for the nature of the Neolithic phenomenon. Aurenche Olivier. Jacques Cauvin et la religion néolithique. Genèse d’une théorie. In: Paléorient, 2011, vol. 37, n° 1. Néolithisations : nouvelles données, nouvelles interprétations. À propos du modèle théorique de Jacques Cauvin. pp. 15 - 27...|$|E
40|$|The {{resumption}} of excavations at Glanum {{and the start}} of restoration work in the House of the Two Alcoves (XVIII) has, by providing new data, resulted in a new and more satisfactory interpretation of the paintings both in position and in fragments. The chronological phases of decoration can be reduced to two main periods : the first is dated 60 - 50 B. C. (room with the alcoves) and the second 35 - 30 B. C. (room with the voluled stalks and restoration in the neighbouring room), while the destruction of the house took place between 30 and 20 B. C. The construction goes {{back to the beginning of}} the 1 st century B. C. Re-examination of the fragmentary decorations has revealed some dubious joins, while new examination of paintings in position, carried out after careful cleaning, has made it possible to reconstruct each wall on paper. These reconstructions are much more complete than before and more consonant with what we know of the II Pompeian Style, to which the decoration is closely related. The making of a model of the two rooms has established the position of the windows and the existence of two vaults over each of the alcoves, which accords with an arrangement known in the great villas of Campania. A similar re-examination has been carried out on room D in the House of Sulla (XII), where reconstruction drawings of the two long walls have demonstrated the bipartite nature of the decoration; the presence of a painted pilaster a third of the way along the wall suggests that we are dealing with a triclinium rather than a cubiculum, an inference which is supported by the mosaic pavement. The decoration, again of the II Pompeian Style, and datable to the middle of the 1 st century B. C., is highly structured and shows anthropomorphic consoles which had escaped notice at the time of the earlier study. La reprise des fouilles à Glanum et la mise en œuvre de travaux de restauration dans la maison aux Deux Alcôves (XVIII) ont <b>abouti</b> à une relecture des peintures en place ou en fragments. Les phases chronologiques du décor se réduisent à deux périodes : la première vers 60 - 50 avant J. -C. (pièce aux alcôves) et la seconde, vers 35 - 30 avant J. -C. (pièce aux hampes à volutes et réfection dans la pièce voisine), avant la destruction de la maison, advenue entre 30 et 20 avant J. -C. et dont la construction remonte au début du Ier s. avant J. -C. Le réexamen des décors fragmentaires a révélé des recollages douteux; des relevés nouveaux effectués sur des peintures en place ont permis de proposer des restitutions graphiques de chaque mur, plus complètes et plus conformes à ce que l'on connaît du IIe style pompéien. La réalisation d'une maquette des deux pièces a mis en évidence la place des fenêtres, l'existence probable de deux voûtes pour chacune des alcôves, selon un aménagement connu dans les grandes villas de Campanie. La même révision a été faite pour la pièce D de la maison de Sulla (XII), où la bipartition du décor a été traduite par une restitution graphique des deux longs murs, où un pilastre peint à un tiers de la paroi suggère que nous aurions affaire à un triclinum plutôt qu'à un cubiculum, impression renforcée par la mosaïque de sol. Le décor, également du IIe style pompéien, du milieu du Ier s. avant J. -C., est très structuré et montre des consoles anthropomorphes qui avaient échappé à la lecture lors de la précédente enquête. Barbet Alix. Les peintures de Glanum : une relecture. In: Gallia, tome 47, 1990. pp. 103 - 134...|$|E
40|$|The sediments flux in Amazon Basin have an {{important}} role on the aquatic biodiversity and richness in the floodplains because the nutrients and organic matter attached on suspended sediments are deposited in these zones. The aim {{of this study is to}} understand the spatial and temporal distribution of sediments flux in the Amazon River, therefore were select four gauging station located along of Amazon Riven from Peru to Brazil. In each gauging station was make superficial samples each ten days and samples in the section in different times of hydrological period. Turbidity profiles and granulometry measuring were made too in each gauging station. In the Andean region, it is observed a relationship between the suspended sediments concentration and discharge, however, this relationship become a hysteresis in the plain especially in the Óbidos gauging station located at 870 km before of mouth. This result can be by the contribution of influx poor in suspended sediments from Guyanese and Brazilian shields. In 3000 km of long from Peru to Brazil plain, the suspended sediments is composed by two well-defined types of suspended sediments: fine sediments (10 - 20 µm) and coarse sediments (100 - 250 µm). The percentage of each type of sediments in the main river is different during the hydrologic regime. Peak of fine sediments is observed in the same period of peak of rainfall (December to March) and peak of coarse sediments is observed in flood period (May to July). The Andean and sub-Andean basin gauging station show the coarse sediments in surface due to great turbulence and low depths. Therefore, this gauging station show a relationship between the suspended sediments concentration in surface and average suspended sediments concentration in section, with this relation is possible to calculate the suspended sediments flux. Hence the Peruvian basin provide 540 Mt year - 1. However in the Brazilian plain the context is different, the depth is from 40 to 100 m, becoming almost null the presence of coarse sediments in the surface. Therefore, cannot use the relationship between suspended sediments concentration in surface and average suspended sediments concentration in section. When the Óbidos gauging station is analysed, it found there is a relationship between suspended sediments concentration in surface and average of fine suspended sediments concentration. It is observed too, that there is the relationship between coarse suspended sediments concentration and discharge. Therefore, it is possible to calculate of suspended sediments flux using these two relationships. The Amazon River export 1100 Mt year- 1 of suspended sediments at Óbidos gauging station, of which 60 % correspond at fine sediments flux and 40 % to coarse sediments flux. It is observed that the suspended sediments are sensitivity of climate variability, generally El Niño events is associate with increase of fine suspended sediments and La Niña events increase a percentage of coarse sediments in Amazon River. It is using the turbidity for determinate of suspended sediments concentration, we use this technique due the high frequency in acquisition of data. However for use the turbidity is necessary the previous calibration. It was observed that the turbidity signal is an addition to the signal emitted by the particles in one sample and with this assumption the Rose model was used to separate the concentration signal obtained by the turbidity of the two types of sediments present in the Amazon River, fine particles and sand. Therefore, it was obtained the concentration profiles to fine sediments and the concentration profiles to the sand. It is observed during the rising period that the fine sediments profiles show a strong gradient, however in the flood periods this gradient reduce come a constant in all section. These results show that turbidity and Rouse model can be used for prediction of suspended concentration in Amazon River. Le bassin amazonien est considéré au niveau mondial comme l'un des principaux apports de sédiments à l'Océan Atlantique. Comprendre la distribution spatiale et temporelle des flux sédimentaires est l'objectif de cette étude pour laquelle on a choisi quatre stations hydrométriques de suivi réparties tout au long de l'Amazone depuis sa formation au Pérou jusqu'à environ 800 km de son embouchure au Brésil (Óbidos). Pour atteindre cet objectif, on a mis en place pour chaque station un échantillonnage décadaire en surface et une exploration totale de leur section à différentes périodes de l'année. Des profils de turbidité et des échantillons pour la granulométrie sur toute la section faisaient également partie de ce suivi. Sur les stations andines et sub-andines, la turbulence de l'écoulement jointe aux faibles profondeurs permet l'ascension de sédiments grossiers vers la surface. Par conséquent, on observe une relation directe entre la concentration de sédiments en suspension de surface et la concentration moyenne dans la section, ce qui permet un calcul simple des flux sédimentaires et d'arriver à une valeur de 540 Mt/an pour qui concerne l'apport du basin péruvien de l'Amazone. Dans la plaine brésilienne, le contexte change, les profondeurs moyennes se situent entre 40 et 100 m de telle sorte que la présence de sable en surface est quasi nulle. Cependant, l'analyse des résultats à la station d'Óbidos montre qu'il existe une relation directe entre la concentration de sédiments de surface et la concentration moyenne de sédiments fins dans la section alors que la concentration moyenne de sédiments grossiers dans la section est, elle, em relation directe avec le débit liquide. En différenciant ainsi le calcul suivant ces deux types de sédiments, on arrive à une valeur de flux de 1100 Mt/an transitant par Óbidos, dont 60 % correspond au flux des sédiments fins et 40 % aux grossiers. On a utilisé la turbidité pour par courbes de calibration em funtion da granulometrie parvenir aux valeurs de concentration. On a utilisé le modèle de Rouse pour différencier le signal de concentration obtenu avec la turbidité résultat du signal de le deux types de sediment. On a constaté que les granulométries en présence sont les mêmes tout au long du régime hydrologique mais que ce sont les proportions de chacune d'entre elles qui varient. Aussi a-t-on <b>abouti</b> à des profils de concentration pour sédiments fins et des profils pour sédiments grossiers. En montée de crue, les profils de concentration présentent un gradient bien marqué pour les sédiments fins, alors qu'en période de crue ce gradient est contrôlé par les sables et les profils de fines sont alors verticaux et constants sur toute la section. Ces résultats montrent qu'il est possible de prédire, en Amazonie, les profils de concentration à partir de la turbidité...|$|E
40|$|Le but de cette étude est de caractériser, à l'échelle du pore, la mouillabilité des roches réservoir, en {{relation}} avec leur géométrie et/ou leur minéralogie. Cette caractérisation se fait, après congélation des échantillons, par l'observation de la distribution des fluides au sein du milieu poreux (saumure et huile brute), en microscopie électronique à balayage. Les expériences ont d'abord été effectuées sur des roches modèles parfaitement mouillables à l'eau, verre fritté et grès naturels. Certains de ces minéraux ont été rendus hydrophobes par greffage de silane. L'étude de ces systèmes a mis en évidence une corrélation entre la mouillabilité et la distribution des fluides. Puis, une roche réservoir (grès argileux de {{la formation}} de Brent, de mer du Nord) connue comme étant de mouillabilité intermédiaire a été étudiée. Un travail précédent (étude de déplacements eau/huile par tomographie X) avait <b>abouti</b> à la conclusion que si ces roches présentaient des hétérogénéités de mouillabilité, l'échelle de ces hétérogénéités devait être inférieure au millimètre. Les études de cryomicroscopie ont montré le caractère hydrophobe de la kaolinite, tandis que les illites, le quartz et les feldspaths sont préférentiellement mouillables à l'eau. L'imbibition spontanée d'huile pourrait ainsi être attribuée à l'existence au sein de la roche d'un réseau de kaolinite, tandis que l'imbibition spontanée de saumure serait due à l'existence d'un second réseau plus ou moins imbriqué avec le premier et constitué des autres minéraux. Un autre cas de roche réservoir a été étudié, à savoir un carbonate du Moyen-Orient. Les mésopores intergranulaires y ont été observés comme étant mouillables à l'huile tandis que les micropores restaient mouillables à l'eau. Dans ce cas, la mouillabilité intermédiaire de ces échantillons s'explique par la géométrie plutôt que par la minéralogie qui est plutôt homogène. Wettability {{is generally considered}} {{to be one of the}} principal parameters influencing the distribution, saturation and flow of fluids in porous media. Reservoir rock wettability has long been approached by overall or indirect methods [1] (capillary pressure or relative permeability curves, contact angle, fluid displacement, etc.). Few studies until now have led to a detailed description of porous media with intermediate wettability. - Is there any evidence of an intermediate behavior of fluids in contact with minerals distributed homogeneously throughout the medium, or is there a heterogeneous distribution of water- and oil-wettabilities within the porous medium?- What influence does the local heterogeneity of the minerals (size, geometry, surface chemistry, etc.) have on fluid distribution [2 to 7]?The answer to these questions requires a microscopic-scale description of saturated porous media [8 to 11]. By using the imaging and analytical capabilities of a scanning electron microscope coupled with a cold stage unit, fluids can be visualized and identified by detection of their natural tracer element (sulfur for oil and chlorine for brine), and their relative distribution within the pore space can be analyzed in terms of wettability. Results presented here illustrate both the interest of the method and its applicability to actual reservoir rocks. Small cores of the chosen porous media were first saturated with brine, flooded to irreducible water saturation by centrifuging in oil, aged in oil for one month and finally flooded to residual oil saturation by centrifuging in brine. Samples were then frozen in nitrogen slush, freeze fractured and coated before being transferred to the cold stage of the microscope for observation. Experiments were first conducted on porous media with controlled wettability : model sintered glass media, natural clean sandstone (Fontainebleau) and clayey sandstones (Vosges, Velaines). All these porous media are naturally water wet. Some of them were treated with alkyltrichlorosilane so as to become oil wet. - The analysis of the relative distribution of fluids within the pore space enables conclusions to be drawn about the wettability of the pore walls. The residual non wetting fluid appears as globules trapped in the center of the pores, while the irreducible wetting fluid appears as films surrounding some grains. Films observed were rather thick (1 to 5 microns) and rare, but this does not exclude the systematic presence of a film less than 0. 1 micron thick, this being the limit resolution in the operating conditions. - The porous media made of spherical glass beads, eroded quartz grains or silica overgrowth with no or low content of clays, have a comparable behavior. - Quartz and feldspars are naturally water-wet; the wettability of the quartz grains is efficiently reversed by silanation. - Illite has a marked affinity for brine even after silanation. - The presence of small size minerals (weathered feldspars and clays) enhances oil entrapment by reducing the pore throats. Experiments were then conducted on actual field cores with intermediate wettability (i. e. spontaneously imbibing both brine and oil), a sandstone and a limestone. Both led to interesting conclusions concerning the origin of their behavior. The main results concerning a sandstone from the North Sea (Brent formation) composed of quartz and feldspar grains, and a high content of clay minerals (mainly kaolinite and some illite) are as follows :- residual oil is systematically associated with kaolinite,- illite and weathered feldspars are always observed associated with brine,- quartz and feldspar grains are preferentially water-wet. In some cases, detrital feldspar grains were observed partially covered by oil. Dissolution roughness then seems to play a role in oil entrapment. The hydrophobic character of this sandstone can then be attributed mainly to the presence of kaolinite and its affinity for oil. Spontaneous imbibition in both crude oil and brine can be interpreted as a consequence of two coexisting networks that are oil- or water-wet : kaolinite on one hand (due to high content, and homogeneous distribution), and essentially quartz, feldspar and illite on the other. A quantitative analysis of phase distribution could confirm this hypothesis. - The difference between the Wettability Index measured for oil and water zone samples could be due to differences in clay distribution (depending on the diagenetic history). For an actual field limestone, cryo-SEM observations of fluid distribution lead to an interpretation of sample behavior during displacement tests. They show that intergranular mesopores are preferentially oil-wet, whilst cement micropores remain water-wet. This points out a wettability heterogeneity at the pore scale, leading to an intermediate wettability on a macroscopic scale and thus demonstrates the importance of pore size and geometry. Wettability alteration could be related to geometric parameters during oil invasion. When oil invaded the initially waterwet pore space, its distribution was controlled by both pore size and prevailing capillary pressure : the largest pores were invaded by oil while the smaller ones remained oil free. Aging then caused adsorption of polar oil compounds on the exposed surface. Spontaneous imbibition of oil could therefore be due to a continuous pore network within the oil-wet intergranular mesopores, whilst spontaneous imbibition of brine could be related to brine circulation in water-wet micropores of the calcitic cement. Cryo-SEM has a resolution below the size of minerals constituting natural porous media. It makes it possible to study in situ the influence of various parameters (pore mineralogy, geometry, surface chemistry, etc.) on wettability. Microscopic studies of oil-brine-rock systems (associated with other imaging techniques such as the X-ray computed tomography) contribute to a better understanding of intermediate wettability causes and can thus explain the macroscopic behavior of some reservoir rocks...|$|E
40|$|Executive summary : One of {{the most}} internationally used bioassays for {{toxicity}} screening of chemicals and for toxicity monitoring of effluents and contaminated waters is the acute toxicity test with daphnid crustaceans, and in particular that performed with Daphnia magna. Standard methods {{have been developed for}} this assay that were gradually endorsed by national and international organisations dealing with toxicity testing procedures, in view of its application within a regulatory framework. As for all toxicity tests, the organisms used for the acute D. magna assay have to be obtained from live stocks which are cultured in the laboratory on live food (micro-algae). Unsurprisingly the various standard protocols of this particular assay differ – at least to a certain extent – with regard to the test organism culturing conditions. In addition, some technical aspects of the toxicity test such as the effect criterion (mortality of immobility), the exposure time, the type of dilution water, etc., also vary from one standard to another. Although this particular assay is currently used in many countries, the technical and biological problems inherent in year-round culturing and availability of the biological material and the culturing/maintenance costs of live stocks restrict its application to a limited number of highly specialised laboratories. This fundamental bottleneck in toxicity testing triggered investigations which brought forward the concept of “microbiotests” or “small-scale” toxicity tests. “Culture/maintenance free” aquatic microbiotests with species of different phylogenetic groups were developed in the early 1990 s at the Laboratory for Environmental Toxicology and Aquatic Ecology at the Ghent University in Belgium. These assays which were given the generic name “Toxkits”, are unique in that they employ dormant stages (“cryptobiotic eggs”) of the test species, which can be stored {{for long periods of time}} and “hatched” at the time of performance of the assays. One of these microbiotests is the Daphtoxkit F magna, which is currently used in many laboratories worldwide for research as well as for toxicity monitoring purposes. The microbiotest technology has several advantages in comparison to the “traditional” tests based on laboratory cultures, especially its independence of the stock culturing burden. However, the acceptance (or possible non-acceptance) of performing assays with test organisms obtained from “dormant eggs” should be clearly dictated by the “sensitivity” and “precision” criteria of the former assays in comparison to the latter. The first part of this review therefore thoroughly reviews the scientific literature and of data obtained from various laboratories for assays performed with either D. magna test organisms obtained from lab cultures or hatched from dormant eggs. Attention has focused on data of quality control tests performed on reference chemicals, and in particular on potassium dichromate (K 2 Cr 2 O 7) for which an acceptability range of 0. 6 – 2. 1 mg·L– 1 has been set in ISO standard 6341 for the 24 h EC 50 of the acute D. magna assay. Mean EC 50 s, standard deviations and variation coefficients were calculated from the collected data, all of which are presented in tables and figures and discussed in detail. The major conclusions drawn from the analysis of the large number of quality control (QC) data on the acute D. magna toxicity test are that : (1) Virtually all results from assays performed with Daphnias taken from lab cultures or with Daphnia microbiotests are within the acceptability range set by ISO standard 6341 for the reference chemical potassium dichromate. (2) The mean 24 h EC 50 s of the Daphnia microbiotests performed in different laboratories are within the range of the mean EC 50 s of the assays based on lab cultures, and the variation coefficients (20 to 30 %) are similar. (3) The precision – in terms of the long term in house variability – of the quality control Daphnia microbiotests is as good as that of the QC tests based on lab cultures. The review further reports on intra-laboratory sensitivity comparison studies performed during the last 15 years on pure chemicals and on natural samples, with both laboratory cultured organisms and Daphnias hatched from dormant eggs. These studies carried out in different laboratories showed EC 50 correlation coefficients of 0. 86 to 0. 98, corroborating a similar sensitivity of the two types of test organisms. The third part of the review reports and analyses data on proficiency ringtests on the acute D. magna assay which have been organised in different countries since 2002 with either reference chemicals or with natural samples, and in which part of the laboratories performed their assays with Daphnia microbiotests and others with lab cultured Daphnias. The conclusions drawn from all the ringtests indicate that the sensitivity of Daphnia neonates hatched from dormant eggs is similar to that of test organisms taken from lab cultures and that in most cases the precision of the Daphnia microbiotest is superior to that of the assays based on lab cultures. The review finally addresses the issue of possible sensitivity differences of Daphnias hatched from dormant eggs which are produced by different D.  magna strains. From these investigations it appeared that the EC 50 s from assays performed with Daphnias hatched from dormant eggs of different strains did not differ significantly from those from assays undertaken with daphnids from lab cultures. The obvious advantages of Daphnia microbiotests over tests with Daphnias stemming from lab cultures have led to the worldwide use of these culture/maintenance free and low cost small-scale assays in both research and toxicity monitoring. The Daphnia microbiotest is in current use in several countries for toxicity testing in a regulatory framework, and recent calculations indicate that about 10   000 acute D. magna assays are now performed annually with neonates hatched from dormant eggs. The use of dormant eggs to obtain test organisms independently of stock culturing has recently also been accepted in international standards for toxicity testing. ISO standard 20665 (2008) related to the determination of chronic toxicity with Ceriodaphnia dubia, and ISO standard 20666 (2008) for the determination of the chronic toxicity with Brachionus calyciflorus in 48  h, both indicate that the assays can be conducted with organisms hatched from dormant eggs. On the basis of the extensive scientific evidence provided in this review that is justifiably supported by the two ISO methods mentioned above, the authors therefore recommend that the use of Daphnias hatched from dormant eggs should also be incorporated in national and international standards, as an alternative to the use of Daphnias taken from laboratory cultures. Résumé étendu : L’essai de toxicité aigu vis-à-vis de daphnies, en particulier celui entrepris avec Daphnia magna, est l’un des plus populaires utilisés au niveau international pour le dépistage de la toxicité de produits chimiques et la surveillance d’effluents et d’eaux contaminées. Avec le temps, des méthodes normalisées développées avec cet essai ont été reconnues par des organismes nationaux et internationaux intéressés par les tests biologiques, en vue de son application dans un contexte réglementaire. À l’instar des autres tests biologiques, les organismes intervenant dans l’essai aigu avec D.  magna proviennent d’élevages de laboratoires nourris de micro-algues. Il n’est pas surprenant de constater que les différentes procédures standard varient jusqu’à un certain point en ce qui a trait aux conditions d’élevage des animaux tests. Par ailleurs, pour cet essai il ne semble pas non plus exister de constance sur certains aspects techniques liés par exemple au paramètre de toxicité (létalité ou immobilité), au temps d’exposition, à la composition de l’eau de dilution, lesquels peuvent varier de façon marquée. Bien que cet essai soit maintenant conduit dans beaucoup de pays, les contraintes techniques et biologiques associées à l’élevage des organismes, ainsi qu’à leur disponibilité continuelle, restreignent « de facto » son application à un nombre limité de laboratoires spécialisés. Ce problème, critique pour l’entreprise de tests de toxicité, a promu des études qui ont <b>abouti</b> à des essais « à petite échelle », connus maintenant sous l’appellation de « microbiotests ». De tels microbiotests, indépendants de tout souci d’élevage et d’entretien, ont été développés au début des années 1990 avec des organismes de différents groupes phylogéniques, au Laboratoire de Toxicologie environmentale et d’Écologie aquatique de l’Université de Gand en Belgique. Baptisés « Toxkits », ces essais s’avèrent uniques par leur emploi de stades d’œufs de dormance (œufs cryptobiotiques) pour chaque espèce d’organisme concernée, ce qui permet au matériel biologique « au stade inerte » d’être entreposé durant de longues périodes et éclos au moment du démarrage d’un test de toxicité. L’un de ces microbiotests est le Daphtoxkit F magna, présentement en usage à l’échelle mondiale pour des besoins de recherche ou de surveillance de toxicité. La technologie des microbiotests offre plusieurs avantages sur celle des essais traditionnels puisqu’elle assure l’indépendance du besoin d’élevage et de maintien de stocks des organismes tests. Cependant, la reconnaissance par la communauté scientifique de la conduite d’essais avec des organismes éclos d’œufs de dormance passe obligatoirement par une preuve de sensibilité et de précision équivalente à celle de l’essai traditionnel. La première partie de cette revue rappelle les principales études et résultats obtenus par divers laboratoires avec des organismes provenant d’élevages en laboratoire, et de daphnies éclos d’œufs de dormance. Des données de contrôle de qualité entrepris avec des produits chimiques de référence, notamment le bichromate de potassium (K 2 Cr 2 O 7), ont été collectées, pour lequel une fourchette de CE 50 - 24 h variant entre 0, 6 – 2, 1 mg·L– 1 est une condition d’acceptation pour l’essai aigu de D. magna selon la norme ISO 6341. Les moyennes des CE 50 s, ainsi que leurs écarts-types et coefficients de variation sont rapportées et discutées en détail, avec tableaux et figures à l’appui. À partir de l’analyse des nombreuses données portant sur le contrôle de qualité du test aigu de D. magna, il est possible de conclure que : (1) Quasiment tous les essais effectués avec les daphnies provenant de cultures de laboratoire ou de microbiotests Daphnia sont conformes à la norme ISO 6341 pour ce qui est des résultats générés avec le bichromate de potassium. (2) Les moyennes de CE 50 - 24 h rapportées pour le microbiotest Daphnia par différents laboratoires sont dans la fourchette des résultats de l’essai traditionnel et les coefficients de variation (entre 20 – 30 %) sont semblables. (3) La précision des tests de contrôle de qualité – qui témoigne de la variabilité sur une échelle de temps relativement longue pour chaque laboratoire – est équivalente pour les deux méthodes. La revue, par la suite, fait état des études de comparaison de sensibilité intra-laboratoire réalisées avec les deux méthodes (traditionnelle et microbiotest) depuis 15  ans pour l’évaluation de produits chimiques et d’échantillons environnementaux. Provenant de laboratoires indépendants, les données démontrent des coefficients de corrélation situés entre 0, 86 et 0, 98, ce qui confirme la sensibilité équivalente des tests avec animaux d’élevage et ceux provenant des œufs de durée de D.  magna. La troisième partie de la revue offre un constat de l’analyse des données de tests d’intercalibration lancés depuis 2002 par différents pays avec le test aigu D. magna et dans lesquels certains laboratoires ont effectué leurs essais avec des organismes d’élevage et d’autres avec le microbiotest Daphnia. Les conclusions émanant des tests d’intercalibration confirment que la sensibilité des daphnies issues d’œufs de dormance est comparable à celle des animaux d’élevage et que dans la plupart des cas, la précision du microbiotest Daphnia est même supérieure à celle des essais réalisés en conditions d’élevage. Enfin, la revue discute des différences possibles de sensibilité de daphnies d’œufs de dormance provenant de souches différentes de D. magna. De ces études, il apparaît qu’aucune différence significative n’est démontrée entre les CE 50 s issues d’essais de daphnies éclos des œufs de dormance produits par des souches différentes de D. magna et celles mesurées avec des animaux d’élevage. Les avantages incontestables de l’essai entrepris avec des daphnies d’œufs de dormance (qui exclut le maintien de cultures) sur celui faisant appel aux animaux d’élevage, ont contribué à l’emploi international du microbiotest Daphnia à des fins de recherche et de surveillance. Présentement employé dans plusieurs pays tant à des fins de réglementation de la toxicité que de recherches en écotoxicologie, il est estimé que quelque 10   000 essais aigus vis-à-vis de D. magna sont entrepris sur une base annuelle avec le microbiotest Daphnia. L’emploi d’œufs de dormance a déjà été reconnu par des instances normatives internationales à des fins d’études de toxicité. La norme ISO 20665 de 2008 décrivant l’essai de toxicité chronique vis-à-vis de Ceriodaphnia dubia, par exemple, ainsi que la norme ISO 20666 de 2008 décrivant celui de 48 h conduit avec Brachionus calyciflorus précisent que ces tests peuvent être appliqués avec des organismes éclos d’œufs cryptobiotiques. À la lumière de l’envergure des données scientifiques validées et rapportées dans cette revue, lesquelles sont logiquement appuyées par les normes ISO mentionnées ci-dessus, les auteurs s’accordent unanimement à recommander l’emploi de daphnies provenant d’œufs de dormance pour l’entreprise d’essais normatifs nationaux et internationaux au même titre que les daphnies provenant d’élevage en laboratoire...|$|E
40|$|This {{manuscript}} {{presents a}} large part of my research since the end of my PhD. Most of mywork is related to numerical (also referred to as continuous) optimization, at the exception of onecontribution done during my postdoc in Zurich introducing a new stochastic algorithm to simulatechemical or biochemical systems [23]. The optimization algorithms at the core of my work are adaptive derivative-free stochastic (orrandomized) optimization methods. The algorithms are tailored to tackle dificult numerical optimizationproblems in a so-called black-box context where the objective function to be optimized isseen as a black-box. For a given input solution, the black-box returns solely the objective functionvalue but no gradient or higher order derivatives are assumed. The optimization algorithm canuse the information returned by the black-box, i. e. the history of function values associated tothe queried search points, but no other knowledge that could be within the black-box (parametersdescribing the class of functions the function belongs to, [...] .). This black-box context is verynatural in industrial settings where the function to be optimized can be given by an executablefile for which the source code is not provided. It is also natural in situations where the functionis given by a large simulation code from which it is hard to extract any useful information for theoptimization. This context is also called derivative-free optimization (DFO) in the mathematical optimizationcommunity. Well-known DFO methods are the Nelder-Mead algorithm [79, 77], pattern searchmethods [54, 90, 6] or more recently the NEW Unconstraint Optimization Algorithm (NEWUOA) developed by Powell [82, 81]. In this context, I have been focusing on DFO methods in the literal sense. However the methodsmy research is centered on have a large stochastic component and originate from the community ofbio-inspired algorithms mainly composed of computer scientists and engineers. The methods wereintroduced {{at the end of the}} 70 's. A parallel with Darwin's theory of the evolution of species basedon blind variation and natural selection was recognized and served as source of inspiration for thosemethods. Nowadays this field of bio-inspired methods is referred to as evolutionary computation(EC) and a generic term for the methods is evolutionary algorithms. The probably most famousexamples of bio-inspired methods are genetic algorithms (GAs). However today GAs are known tobe not competitive for numerical optimization. Evolution Strategies (ES) introduced in the endof the 70 's [83] have emerged as the main sub-branch of EC devoted to continuous optimization. One important feature of ES is that they are comparison-based algorithms. The present mostadvanced ES algorithm, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [50]is a variable metric method recognized as the state-of-the-art method for stochastic numericaloptimization. It is used in many applications in industry and academy. Because of historical reasons, the developments and work on Evolution Strategies are mainlycarried out in the EC field where practice and effectiveness is definitely as (or more) importantas having a theorem proven about an algorithm. However ES algorithms are simply adaptivestochastic iterative methods and they need to be studied from a mathematical perspective aswell as any other iterative method in optimization or other domain in order to understand themethods better and convince a broader class of people about their soundness. Questions like theirconvergence and speed of convergence central in optimization need to be addressed. My research is encompassed within this general context: I am particularly interested by themathematical aspects of adaptive stochastic methods like ES (and of course CMA-ES) or moregenerally adaptive stochastic optimization algorithms. Evolution strategies have this attractivefacet that while introduced in the bio-inspired and engineering context, they turn out to bemethods with deep theoretical foundations related to invariance, information geometry, stochasticapproximation and strongly connected to Markov chain Monte Carlo (MCMC) algorithms. Thosefoundations and connections are relatively new and to a small (for some topics) or large (forothers) extent partly related to some of my contributions. They will be explained within themanuscript. I particularly care that the theory I am working on relates to practical algorithms orhas an impact on (new) algorithm designs. I attempt to illustrate this within the manuscript. While optimization is the central theme of my research, I have been tackling various aspect ofoptimization. Although most of my work is devoted to single-objective optimization, I have alsobeen working on multi-objective optimization where the goal is to optimize simultaneously severalconflicting objectives and where instead of a single solution, a set of solutions, the so-called Paretoset composed of the best compromises is searched. In the field of single-objective optimization, I have been tackling diverse contexts like noisyoptimization where for a given point in a search space we do not observe one deterministic valuebut a distribution of possible function values, large-scale optimization where one is interested intackling problems of the order of 104 (medium large-scale) to 106 variables (large-scale) and to asmaller extent constrained optimization. In addition to investigating theoretical questions, I have been also working on designing newalgorithms that calls for theory complemented with numerical simulations. Last I have tackledsome applications mainly in the context of the PhD of Mohamed Jebalia with an application inchromatography and of the PhD of Zyed Bouzarkouna (PhD financed by the French Institute forpetrol) on the placement of oil wells. Furthermore, a non neglect-able part of my research those past years has been devoted tobenchmarking of algorithms. Benchmarking complements theory as it is difficult to assess theoreticallythe performance of algorithms on all typical functions one is interested. The mainmotivation has then been to improve the standards on how benchmarking is done. Those contributionswere done along with the development of the Comparing COntinuous Optimizers platform(COCO). My work is articulated around three main complementary axis, namely theory / algorithmdesign and applications. An overview of the contributions presented within this habilitationorganized along those axes is given in Figure 3. 1. Ce mémoire décrit l'essentiel de mon travail scientifique depuis la fin de ma thèse. Mes travauxsont centrés sur l'optimisation numérique dite "boîte-noire" à l'exception d'un article effectuédurant mon séjour post-doctoral à l'ETH Zurich qui introduit un nouvel algorithme d'optimisationstochastique pour simuler des systèmes en chimie ou bio-chimie [23]. Les algorithmes d'optimisation au coeur de mon travail sont des algorithmes adaptatifs sansdérivées et stochastiques. Ils sont particulièrement adaptés à l'optimisation de problèmes difficiles dans des contextes oèu la fonction n'est accessible qu'à travers une -noire" retournantl'information d'ordre zero, c'est-à-dire que la seule information disponible et utilisable parl'algorithme sont les couples (points de l'espace de recherche, valeur de fonction objectif associée). Ce contexte est très courant dans l'industrie oèu les problèmes d'optimisation rencontrés font appelà des codes de simulations numériques pour lesquels, souvent, simplement un executable du codeest disponible. L'aspect "sans-dérivées" est aussi très commun car le calcul d'un gradient (quiprésuppose la fonction sous-jacente dérivable) sur des codes de simulations numériques, par exempleen utilisant une méthode d'adjoint ou de differentiation automatique peut ^etre couteux entemps de développement. Il est par ailleurs usuel que la formulation d'un problème d'optimisationchange au fur et à mesure de sa résolution, adapter le code de calcul de gradient peut alors s'avérertrès lourd et peut motiver l'utilisation d'une méthode d'optimisation boîte-noire. Ce contexte d'optimisation boîte-noire s'appelle également optimisation sans dérivées dans lacommunauté programming" et l'acronyme anglais associé est DFO pour optimization". Les méthodes qualifiées de DFO sont généralement deterministes. Lesméthodes DFO les plus connues à l'heure actuelle sont l'algorithme du simplexe ou de Nelder-Mead [79, 77], les algorithmes de "pattern search" [54, 90, 6] et l'algorithme NEWUOA (NEWUnconstraint Optimization Algorithm) développé par Powell [82, 81]. Ce dernier algorithme est àl'heure actuelle considéré comme l'algorithme DFO déterministe état de l'art. Mon travail porte ainsi sur des méthodes DFO au sens littéral du terme. En revanche, lesméthodes auxquelles je me suis intéressées ont une large composante stochastique et ont étédéveloppées dans la communauté des algorithmes bio-inspirés qui se compose essentiellementd'ingénieurs et d'informaticiens. Les premiers algorithmes ont été introduits dans les années 70. Un parallèle entre la théorie de Darwin de l'évolution des espèces et l'optimisation a servià l'origine de source d'inspiration pour leur développement. A l'heure actuelle, ce domaine desméthodes bio-inspirées est également appelé Computation". Un terme génériquepour les algorithmes est algorithme évolutionnaire (EA). Pour beaucoup de chercheurs (dont je faispartie) dans ce domaine, l'aspect bio-inspiré n'est plus présent et le développement des algorithmesest seulement motivé par des considérations mathématiques et numériques. Parmi les algorithmes évolutionnaires, les algorithmes génétiques (GA) sont probablementencore les plus célèbres en dehors de la communauté EC. En revanche, les GAs ne sont pasdes algorithmes compétitifs pour l'optimisation numériquece fait est reconnu depuis plus d'unedizaine d'années. Les strategies d'évolutions (ES), introduites à la fin des annéees 70 [83], se sont imposées comme les algorithmes évolutionnaires pour l'optimisation numérique. A l'heure actuelle,l'algorithme ES le plus <b>abouti</b> est l'algorithme Covariance Matrix Adaptation Evolution Strategy(CMA-ES) [50]. L'algorithme adapte un vecteur Gaussien (paramétré par vecteur moyenne etmatrice de covariance) qui encode la métrique sous-jacente. Cette métrique apprend sur desfonctions convexes quadratiques l'information d'ordre 2, c'est à dire que la matrice de covariancedevient proportionnelle à l'inverse de la matrice Hessienne. Ainsi, CMA-ES peut ^etre vu comme lependant stochastique d'une méthode de quasi-Newton. Une particularité essentielle de CMA-ESet des ES en général est d^u au fait qu'ils n'utilisent que des comparaisons pour les difrérentesmises à jour. Plus précisément, nous avons vu que les ESs sont des algorithmes d'optimisationsans dérivées, ils n'utilisent cependant qu'une information " de ce que la boîte-noire leurfournit, à savoir simplement le résultat de la comparaison des solutions candidates, i. e. étant donnédeux solutions x 1 et x 2, est ce que f(x 1) est plus grand ou plus petit que f(x 2). En conséquenceils optimisent de la m^eme façcon une fonction f : Rn ! R ou n'importe quelle fonction g o f oùg : f(Rn) ! R est une fonction strictement croissante: ils sont invariants à la composition àgauche par une fonction monotone strictement croissante. L'algorithme CMA-ES est reconnu comme la méthode état de l'art pour l'optimisation stochastiquenumérique. Il est utilisé dans de nombreuses applications dans l'industrie ou dans le mondeacadémique. Pour des raisons historiques, les algorithmes ESs ont été développés dans la communauté ECoù la mise au point d'un algorithme est la plupart du temps découplée du soucis de prouverun théorème de convergence sur la méthode et repose essentiellement sur l'utilisation de modèlesmathématiques approximatifs simplifiés et de simulations numériques sur des fonctions tests. Bienque ce découplage entre mise au point pratique et théorie puisse ^etre vu comme un inconvenient,il présente l'avantage que le développement d'une méthode n'est pas restreinte (ou bridée) parune contrainte technique liée à une preuve mathématique. Cela a permis à un algorithme commeCMA-ES de voir le jour bien avant que l'on comprenne certains de ses fondements théoriques etbien avant que l'on puisse établir une preuve de convergence. En revanche, cela implique aussique les études théoriques de convergence par exemple s'avèrent relativement compliquées. Ma recherche se situe dans ce contexte général: je suis particulièrement intéressée par l'étudemathématique d'algorithmes adaptatifs stochastiques comme les algorithmes ESs (en particulierCMA-ES) et par l'établissement de preuves de convergence. Ces algorithmes ont une particularité attractive: bien qu'introduits dans un contexte où les performances pratiques sont plusimportantes que les preuves théoriques, ils s'avèrent avoir des fondements mathématiques profondsliés en particulier aux notions d'invariance et de géométrie de l'information. Par ailleurs, ilss'inscrivent dans le cadre plus général d'algorithmes d'approximation stochastique et ils sont fortementconnectés aux méthodes Monte-Carlo par chaînes de Markov (MCMC). Ces deux dernierspoints fournissent des outils mathématiques puissants pour établir des preuves de convergence(linéaire). La comprehension de ces fondements et connexions est reliée en partie à mon travailcomme cela sera illustré dans ce mémoire. J'ai abordé plusieurs facettes de l'optimisation numérique. Bien que l'essentiel de mes travauxporte sur l'optimisation mono-objectif, i. e. minimizer f : X Rn ! R, j'ai également travaillé en optimisation multi-objectif, i. e. où l'on s'intéresse à minimiser une fonction vectoriellef : X Rn ! Rk. Dans ce cas là, la notion d'optimum est remplacée par celle d'ensemblede points de Pareto composé des meilleurs compromis possibles. Mes contributions portent surl'étude d'algorithmes à base d'hypervolume qui quantifient la qualité d'un ensemble de solutionsen calculant le volume compris entre les solutions et un point de reference. Les algorithmes utilisantl'hypervolume sont à l'heure actuelle les algorithmes état de l'art. Nous avons pu établirdes caractérisations théoriques de l'ensemble des solutions optimales au sens de l'hypervolume. En optimisation mono-objectif, j'ai travaillé sur l'optimisation bruitée où étant donné un point del'espace de recherche, on observe une distribution de valeurs de fonction objectif, sur l'optimisationà grande échelle où l'on s'intéresse à l'optimisation de problèmes avec de l'ordre de 104 à 106 variableset sur l'optimisation sous contrainte. Mes travaux s'articulent autour de trois grands axes: théorie / nouveaux algorithmes / applications (voir Figure 3. 1). Ces trois axes sont complémentaires et couplés: par exemple, la miseau point de nouveaux algorithmes repose sur l'établissement de bornes théoriques de convergenceet est ensuite complémentée par des simulations numériques. Ceci est illustré au Chapitre 6. Parailleurs le développement d'algorithmes pour l'optimisation en grande dimension repose sur laconnexion entre CMA-ES et la géométrie de l'information (voir Chapitre 4). Un autre exemplede complémentarité est le suivant: les applications abordées notamment pour l'optimisation duplacement de puits de pétrole ont motivé l'introduction de nouvelles variantes de CMA-ES (voirChapitre 9). Par ailleurs, une partie non négligeable de mes travaux porte sur le test (benchmarking) d'algorithmes. La motivation principale est d'améliorer les méthodologies pour tester et comparerles algorithmes d'optimisation numériques. Ces travaux ont été accompagnés du développementd'une plateforme, Comparing COntinuous Optimizers (COCO) et ont un impact maintenant surla mise au point de nouveaux algorithmes mais également sur le test d'hypothèses théoriques...|$|E
40|$|The aquatic {{environment}} is exposed continuously and increasingly to chemical substances such as pharmaceuticals. These medical compounds are {{released into the}} environment after having being consumed and body-excreted by patients. Pharmaceutical residues are synthetic molecules that are not always removed by traditional sewage treatment processes and thus escape degradation. Among pharmaceuticals that escape sewage treatment plants (STPs), the anticancer drugs were measured in STP effluents and natural waters. In the {{aquatic environment}}, their long-term effects at low concentrations are sparsely known on non-target species. Tamoxifen is an anticancer drug that is widely prescribed worldwide for the prevention and treatment of hormone receptor-positive breast cancers. Two of its metabolites, i. e., endoxifen and 4 -hydroxy- tamoxifen (4 OHTam), have high pharmacological potency in vivo and such as tamoxifen, they are excreted via faeces by patients. Tamoxifen was measured in STP effluents and natural waters but, {{to the best of}} our knowledge, its metabolites concentrations in waters have never been reported. Imatinib is another and recent anticancer compound that targets specific tumour cells. This pharmaceutical is also body excreted and because of its increasing use in cancer treatment, imatinib may reach the natural water. The effects of tamoxifen and imatinib are unknown upon more than one generation of aquatic species. And the effects of 4 OHTam, endoxifen have never been studied in ecotoxicology so far. The aims of this thesis were threefold. First, the sensitivity of D. pulex exposed to tamoxifen, 4 OHTam, endoxifen or imatinib was assessed using ecotoxicological experiments. Ecotoxicology is the science that considers the toxic effects of natural or synthetic substances, such as pharmaceuticals, on organisms, populations, community and ecosystem. Acute and multigenerational (2 - 4 generations) tests were performed on daphnids considering several studied endpoints, such as immobilisation, size, reproduction, viability and intrinsic rate of natural increase. Additional prospective assays were designed to evaluate whether 1) low concentrations of tamoxifen and 4 OHTam were able to induce toxic effects when used in combination, and 2) daphnids were able to recover when offspring were withdrawn from solutions carrying the pharmaceutical. Second, the stability of tamoxifen, 4 OHTam and endoxifen in incubation medium was evaluated in solution exempted from daphnids. Because the nominal concentrations of tamoxifen, 4 OHTam and endoxifen did not correspond to the measured, we provide a predictive method to estimate the concentrations of these chemicals during long-term ecotoxicological tests. Finally, changes in protein expressions were analysed in D. pulex exposed 2 or 7 seven days to tamoxifen using ecotoxicoproteomic experiments with a shot-gun approach inducing a peptide fractionation step. Our results show that tamoxifen, 4 OHTam and endoxifen induced adverse effects in D. pulex at environmentally relevant concentrations. At very low concentrations, these molecules displayed unusual and teratogenic effects because morphological abnormalities were observed in offspring, such as thick and short antennas, curved spines, premature neonates and aborted eggs. Tamoxifen was the most toxic compound among the test chemicals, followed by 4 OHTam, endoxifen and imatinib. Tamoxifen no-observed effect concentrations (NOECs) that were calculated for size, reproduction and intrinsic rate were below or in the range of the concentrations measured in natural waters, i. e., between 0. 12 µg/L and 0. 67 µg/L. For instance, the tamoxifen NOECs that were calculated for reproduction were between 0. 67 and 0. 72 µg/L, whereas the NOEC was < 0. 15 µg/L when based on morphological abnormalities. The NOECs of 4 OHTam were higher but still in the same order of magnitude as tamoxifen environmental concentrations, with a value of 1. 48 µg/L. Endoxifen NOEC for the intrinsic rate of natural increase (r) and the reproduction were 0. 4 and 4. 3 µg/L, respectively. Daphnids that were withdrawn from tamoxifen and 4 OHTam were not able to recover. Also, the reproduction of D. pulex was reduced when the treated animals were exposed to the combination of tamoxifen and 4 OHTam while no effects were observed when these chemicals were tested individually at the same concentration. Among the anticancer drugs that were tested during this thesis, imatinib was the less toxic molecule towards D. pulex. No effects on size and reproduction were observed within two generations, except for the first whose reproduction decreased at the highest test concentration, i. e., 626 µg/L. Our results also underline the need to use measured or predicted concentrations instead of the nominal during aquatic experiments, particularly when lipophilic molecules are tested. Indeed, notable differences between nominal (i. e., theoretical) and measured concentrations were found with tamoxifen, 4 OHTam and endoxifen at all test concentrations. A cost and time sustainable method was proposed to predict the test exposure levels of these chemicals during long-term experiments. This predictive method was efficient particularly for low concentrations, which corresponded to the test concentrations in multigenerational tests. In the ecotoxicoproteomic experiments a total of 3940 proteins were identified and quantified in D. pulex exposed to tamoxifen. These results are currently the largest dataset from D. pulex that is published and the results of proteomic analyses are available for the scientific community. Among these 3940 proteins, 189 were significantly different from controls. After protein annotation, we assumed that treated daphnids with tamoxifen had shifted cost-energy functions, such as reproduction, to maintain their basic metabolism necessary to survive. This metabolic cost hypothesis was supported by the presence of proteins involved in oxidative stress. Biomarkers for early detection of tamoxifen harmful effects on D. pulex were not discovered but the proteins of the vitellogenin- 2 family (E 9 H 8 K 5) and the ryanodine receptor (E 9 FTU 9) are promising potential biomarkers because their expression was already modified after 2 days of treatment. In this thesis, the effects of tamoxifen, 4 OHTam and endoxifen on daphnids raise questions about the potential impact of tamoxifen and 4 OHTam in other aquatic ecosystems, and therefore, about metabolites in ecotoxicology. Because the NOECs were environmentally relevant, these results suggest that tamoxifen and 4 OHTam may be interesting pharmaceuticals to consider in risk assessment. Our findings also emphasize the importance of performing long-term experiments and of considering multi-endpoints instead of the standard reproductive endpoint. Finally, we open the discussion about the importance to measure test exposures or not, during ecotoxicological studies. [...] Les milieux aquatiques sont exposés continuellement à un nombre croissant de substances chimiques, notamment les médicaments issus de la médecine vétérinaire et humaine. Chez les patients, les substances administrées sont utilisées par le corps avant d'être éliminées par l'intermédiaire des excrétas dans le système d'eaux usées de la ville. Ces eaux rejoignent ensuite une station de traitement afin d'y éliminer les déchets. Dans le cas des molécules chimiques, il arrive que les processus de traitement d'eaux usées ne soient pas suffisamment efficaces et que ces molécules ne soient pas dégradées. Elles sont alors libérées dans le milieu aquatique avec les effluents de la station d'épuration. Une fois dans l'environnement, ces résidus de médicaments sont susceptibles d'induire des effets sur la faune et la flore aquatique, dont les conséquences à long terme et à faibles concentrations sont peu connues. Les anticancéreux sont une famille de médicaments qui peuvent échapper aux traitements des stations d'épuration et qui sont retrouvées dans le milieu aquatique naturel. Parmi ces substances, le tamoxifen est une molécule utilisée dans le monde entier pour prévenir et traiter les cancers hormonaux dépendant du sein, notamment. Une fois ingéré, le tamoxifen est transformé par le foie en métabolites dont deux d'entre eux, le 4 -hydroxy-tamoxifen (4 OHTam) et l'endoxifen, possèdent un affinité pour les récepteurs aux estrogènes et une efficacité sur les cellules tumorales supérieure au tamoxifen lui- même. Tout comme la molécule mère, ces métabolites sont principalement éliminés par l'intermédiaire des fèces. Le tamoxifen a déjà été mesuré dans les effluents de stations d'épuration et dans les eaux naturelles, mais aucune valeur n'a été reportée pour ses métabolites jusqu'à présent. Un autre anticancéreux, également éliminé par voie biliaire et susceptible d'atteindre l'environnement, est l'imatinib. Cette récente molécule a révolutionné le traitement et la survie des patients souffrant de leucémie myéloïde chronique et de tumeur stromales gastrointestinales. Les effets du tamoxifen et de l'imatinib sur plusieurs générations d'organismes aquatiques, tels que les microcrustacés Daphnia, sont inconnus et le 4 OHTam et l'endoxifen n'ont même jamais été testés en écotoxicologie. Cette thèse s'est articulée autour de trois objectifs principaux. Premièrement, la sensibilité des D. pulex exposés au tamoxifen, 4 OHTam, endoxifen et imatinib a été évaluée par l'intermédiaire de tests aigus et de tests sur deux à quatre générations. La mobilité, la taille, la reproduction, la viabilité et la croissance potentielle de la population ont été relevées au cours de ces expériences. Des tests supplémentaires, à but prospectifs, ont également été réalisés afin d'évaluer 1) la capacité de récupération des daphnies, lorsque leurs descendants ont été placés dans un milieu exempté de tamoxifen ou de 4 OHTam, 2) les effets chez les daphnies exposées à une solution contenant de faibles concentration de tamoxifen et de 4 OHTam mélangés. Le deuxième objectif a été d'évaluer la stabilité du tamoxifen, 4 OHTam et endoxifen dilué dans le milieu des daphnies. Après analyses, les concentrations mesurées ne correspondaient pas aux concentrations nominales (c. -à-d., théoriques) et il a été nécessaire de développer une méthode efficace de prédiction des niveaux d'exposition lors de tests de longue durée réalisés avec ces trois molécules. Finalement, des changements dans l'expression des protéines chez des daphnies exposées au tamoxifen ont été investigués par l'intermédiaire d'expériences écotoxicoprotéomiques avec une approche dite de shot-gun avec une étape de fractionnement des protéines. Les résultats obtenus dans cette thèse montrent que le tamoxifen, le 4 OHTam et l'endoxifen induisent des effets indésirables chez les daphnies à des niveaux d'exposition proches ou identiques aux concentrations du tamoxifen mesurées dans l'environnement, c'est-à-dire 0. 12 et 0. 67 µg/L de tamoxifen. Ces molécules ont induit des effets inhabituels tels que la production de : nouveau-nés anormaux, avec des antennes et des queues déformées, des prématurés et des oeufs avortés. Le tamoxifen fut la molécule la plus toxique pour les D. pulex suivie du 4 OHTam, de l'endoxifen et enfin de l'imatinib. Lors des expériences sur plusieurs générations, les concentrations n'ayant statistiquement pas d'effet (c. à. d. NOEC en anglais) sur la taille, la reproduction et la croissance intrinsèque de la population étaient du même ordre de grandeur que les concentrations environnementales du tamoxifen. Par exemple, les NOECs du tamoxifen calculées pour la reproduction étaient de 0. 67 et 0. 72 µg/L, tandis que celle calculée sur la base des anomalies chez les nouveau-nés était < 0. 15 µg/L. Les NOECs du 4 OHTam se situaient entre 0. 16 et 1. 48 µg/L et celles de l'endoxifen pour la croissance intrinsèque de la population, ainsi que pour la reproduction, étaient de 0. 4 et 4. 3 µg/L, respectivement. Dans l'expérience basée sur la récupération des daphnies, la taille et la reproduction ont diminué bien que la descendance fût placée dans un milieu sans substances chimiques. Les daphnies exposées au mélange de tamoxifen et de 4 OHTam ont produit moins de nouveau-nés que les contrôles, alors que ces concentrations n'ont pas induit d'effets lorsque testées individuellement. Finalement, l'imatinib n'a pas montré d'effets sur les deux générations testées. Seule la première génération exposée à la plus haute concentration (626 µg/L) a montré une diminution de la reproduction. Les résultats obtenus lors de l'évaluation de la stabilité du tamoxifen, 4 OHTam et endoxifen dans le milieu des daphnies ont souligné l'importance d'utiliser des concentrations mesurées ou prédites en écotoxicologie. En effet, des différences notables entre concentrations nominales et mesurées ont été observées à toutes les concentrations et l'hypothèse d'un phénomène d'adsorption sur le verre des récipients a été posée. De ce fait, il a été nécessaire d'élaborer une méthode prédictive efficace et acceptable, en terme de temps et de coûts. Une régression polynomiale basée sur des concentrations mesurées et nominales a permis de prédire avec efficacité les faibles niveaux d'exposition utilisés lors d'expériences écotoxicologiques à long terme, sur plusieurs générations. Suite aux expériences d'écotoxicoprotéomiques, un total de 3940 protéines ont été identifiées et quantifiées chez des daphnies exposées au tamoxifen. Ce nombre est actuellement la plus large série de données publiées et mises à disposition pour la communauté scientifique. Parmi ces protéines, 189 sont significatives et possiblement reliées à des processus de reproduction et de stress. Sur cette base, nous avons émis l'hypothèse que les individus subissant un stress, lié à l'exposition au tamoxifen, ont utilisé leur énergie de base pour favoriser leur survie plutôt que la reproduction. Enfin, la détermination de bio-marqueurs exprimant des dommages précoces des daphnies exposées au tamoxifen n'a pas <b>abouti</b> en tant que telle, mais des protéines prometteuses, telle que la famille de viellogenin- 2 (E 9 H 8 K 5) et le récepteur à la ryanodine (E 9 FTU 9), ont été exprimées après deux jours d'exposition déjà. Ces protéines pourraient faire l'objet d'investigations écotoxicoprotéomiques futures. Les résultats de cette thèse posent certaines questions quant au risque du tamoxifen, du 4 OHTam et de l'endoxifen sur la faune et la flore aquatique et plus particulièrement sur les anticancéreux présents dans l'environnement. Les effets toxiques de ces molécules ont été observés à des concentrations environnementales et sur plusieurs générations. La question de considérer les métabolites, et ainsi les pro-médicaments, en écotoxicologie est soulevée, notamment parce que ces molécules peuvent être plus actives et efficaces que la molécule mère. Les expériences chroniques, sur plusieurs générations sont également à favoriser car elles offrent un meilleur reflet de la réalité environnementale que des essais aigus ou d'une génération. L'utilisation de la protéomique permet d'agrandir les connaissances sur les effets des médicaments à un niveau inférieur de l'organisation biologique et ainsi, de mieux comprendre de potentiels mécanismes d'action ou de déterminer de potentiels biomarqueurs. Finalement, il semble important de discuter de l'opportunité de mesurer les concentrations qui sont testées en écotoxicologie afin de ne pas sous-estimer le risque pour la faune et la flore aquatique...|$|E
40|$|The Northern Andes {{differ from}} the Central Andes by the {{presence}} of oceanic terranes accreted in the upper Cretaceous and Paleogene. As a consequence, the Andes of Ecuador comprise accreted oceanic terranes to the West (Costa, Cordillera Occidental), and the South American continental margin to the East (Cordillera Real, Subandean zone and Oriente basin). This work aims to characterize the quartz-rich clastic sediments deposited during successive accretions and to propose a model for the chain uplift and erosion. In addition, this aims to verify the hypothesis of Guillier et al. (2001) et Jaillard et al. (2002), which propose that the relief of the Andes of Ecuador does not result from tectonic shortening of the continental margin, as in the Central Andes, but from the addition, by accretion and underplating during oceanic terranes accretions, of buoyant oceanic crustal material, exerting an Archimedes force responsible for the chain uplift, which then induced the reactivation of the erosion, and the deposition of quartz-rich clastic sediments, that seal the accretions. We studied sandstones and conglomerates of Campanian to Eocene age, coeval with the accretion and underplating of oceanic terranes, {{in order to determine the}} tectonic consequences of these processes on the tectonic behaviour of the Ecuadorian continental margin, specially on the Occidental Cordillera and the Real Cordillera, but also on the back-arc and fore-arc zones. Therefore, we analyzed : (1) the paleodepths of deposition of the syntectonic sediments, and the erosional discontinuities due to each accretion; (2) the position of the paleo-shoreline and of the marine sedimentation on either side of the emergent margin; (3) the topographic gradient between the source zones and the depocentres, expressed by the grain size variations of the sediments; and (4) the exhumation of the source zones, through the petrographic evolution of the studied clastic sediments. Three basins of the Occidental Cordillera were studied in detail: the Cuenca basin to the South, and those of Saquisilí and Apagua in the Central part of Ecuador. The vertical succession of sedimentary environments in the studied basins shows the presence of major erosive unconformities defining four main depositional sequences, and points out the accretion periods. These accretions are: (1) Late Campanian (≈ 75 – 72 Ma), (2) Late Maastrichtian (≈ 69 – 65 Ma), and (3) Late Paleocene (≈ 58 – 56 Ma) in age. A last tectonic event of Late Eocene age (Bartonian - Priabonian) was well recorded throughout the margin. From South to North of the Occidental Cordillera, the 4 sequences evidence, except for the Maastrichtian, a jerked shallowing upward trend of the paleodepths of deposition from the Campanian to the Eocene. This study also evidences the retreat of the paleoshoreline during the upper Cretaceous - Paleogene interval. Indeed, the sedimentary basins of the Cordillera Occidental are younger, as they migrate to the west, and also northward. As a matter of fact, the southern Cuenca basin is infilled by middle Campanian to Paleocene p. p. deposits, the sediments of the Saquisilí basin are of Early Maastrichtian to early Middle Eocene age, and the Apagua basin contains eocene sediments. It is worth to note that farther Northwest, the La Cubera basin was infilled by sediments of Paleocene to Late Eocene age. Each accretion corresponds to an increasing grain size of sandstones and conglomerates, in the western basins (Cordillera Occidental and Costa) as well as in the Eastern zones (Subandean Zone and Oriente basin), except for the upper Maastrichtian accretion, which did not provoke an increase of the grain size in the sediments, but an erosional hiatus and a noticeable change in the source zones. In the Cuenca, Saquisilí and Apagua basins, a coarsening upward trend is recorded within each sequence (Campanian, Paleocene and Eocene), as well as in the succession of the sequences, confirming both the steepening of the margin paleoslope, and the uplift of the source area, essentially constituted by the Cordillera Real. Within each sequence, the increasing paleoslope is expressed by the rapid evolution from sandstones to conglomerates, which announces the tectonic event responsible for the overlying unconformity. For the same time-spans, similar evolutions are recorded, although in a more distal manner, on the Coast and in the eastern zones of Ecuador. Since the Cretaceous until the Paleogene, the mineral assemblages of the clastic sediments of the Cordillera Occidental show a three-stage evolution : (1) surrection and exhumation of the Cordillera Real, from the Campanian to the Paleocene, and to a minor extent until the upper Eocene; (2) increasing amount, from the Paleocene to the Eocene, of volcanic and sedimentary lithic fragments deriving from the deformation and erosion of the previously accreted terrains; and (3) resumption of the magmatic arc activity from upper Eocene onward, indicated by the enrichment in plagioclase crystals. During stage 1, the sediments show a clear enrichment in stable minerals and lithic grains (mono- and polycrystaline quartz) at the expense of feldspars, thus reflecting a greater maturity of Paleogene sediments, with respect to those of the latest Cretaceous. At that time, the main source area was the Cordillera Real. From upper Paleocene on (stage 2), the increasing amount of lithic fragments deriving from the oceanic terranes shows that the deformed and eroded zone extends westward, and eventually includes the accreted terranes. In addition, this analysis allowed to point out two original results. The rectangle-shaped basins of the Cordillera Occidental (Cuenca, Saquisilí, Apagua, La Cubera) show tectonically active borders corresponding to major, NNE-trending dextral strike-slip faults, which reactivates accretionary sutures, and important subsidence rates, which suggest they are pull apart basins. We observe also an increase of deposition rates through time, suggesting an increasing instability of these basins located on the accretion sutures. The creation of these basins seems to coincide with the accretion of oceanic terranes, that would have occurred, therefore, in a dextral transpressive regime. The northward migration of these basins through time remains, however, to be explained. The eastern areas of Ecuador are classically interpreted as representing the flexural back arc basin (foredeep) of the Andean chain since the upper Cretaceous. Nevertheless, our study shows that, at a regional scale: (1) the hiatuses and erosions are more important at the foot of the Cordillera Real than to the East; (2) the thicknesses of the Campanian to Eocene deposits increase from West to the East; and (3) the Maastrichtian, Paleocene and Eocene sequences seem to retrograde toward the West. This set of observations point out that this part of the margin underwent uplift, rather than flexural subsidence. We propose that the Andes of Ecuador were built up by isostatic uplift associated with the successive accretion and underplating, beneath the margin, of crustal oceanic material of relatively weak density. These terranes would have generated an Archimedes force, responsible for the isostatic surrection of the margin following each accretion. In this model, the viscous reaction of the uplifted and flexured lithosphere would had accentuated its curvature radius, allowing the retrogradation of back-arc clastic deposits toward the chain, and explaining the observed lag-time of the sedimentary response in the eastern areas. Les Andes du Nord diffèrent des Andes centrales par la présence de terrains océaniques accrétés au Crétacé supérieur et Paléogène. Ainsi les Andes d'Equateur comprennent des terrains océaniques accrétés à l'Ouest (Côte, Cordillère Occidentale), et la marge continentale de l'Amérique du Sud à l'Est (Cordillère Royale, zone Subandine et bassin Oriente). Cette étude a pour but de caractériser les dépôts de sédiments détritiques quartzeux mis en place au cours des accrétions successives et de bien cadrer un modèle de surrection de la chaîne et de son érosion. On espère ainsi apporter des éléments pour tester le modèle de Guillier et al. (2001) et Jaillard et al. (2002) selon lequel le relief des Andes d'Equateur résulte non du raccourcissement tectonique de la marge continentale comme c'est le cas des Andes centrales, mais de l'addition en profondeur, par accrétion et sous-placage lors des accrétions de terrains océaniques, de matériel crustal océanique. Celui-ci exercerait une poussée d'Archimède responsable de la surrection de la chaîne, puis de l'érosion et du dépôt de sédiments détritiques quartzeux, qui scellent ces accrétions. Dans ce but, nous avons étudié les grès et conglomérats d'âge Campanien à Eocène, contemporains de l'accrétion et du sous-placage des terrains océaniques, afin de déterminer les conséquences tectoniques de ces processus sur le comportement tectonique de la marge continentale de l'Équateur, en particulier sur la Cordillère Occidentale et la Cordillère Royale, mais aussi sur les zones d'arrière-arc et d'avant-arc Pour cela, on a analysé : (1) les paléo-profondeurs des dépôts de sédiments syntectoniques, et les discontinuités érosives liées à chaque accrétion; (2) la position des paléo-rivages et des l'aires de sédimentation marine de chaque côté de la marge émergée; (3) le gradient topographique entre zones sources et zones de dépôt, traduit par la granulométrie des dépôts; et (4) l'exhumation des zones sources, au travers de l'évolution pétrographique des sédiments clastiques étudiés. L'étude a porté sur trois bassins de la Cordillère Occidentale : le bassin de Cuenca au Sud, et ceux de Saquisilí et d'Apagua au Centre de l'Equateur. La succession verticale des milieux sédimentaires dans les bassins étudiés montre la présence de discordances érosives majeures définissant 4 grandes séquences de dépôt, et encadrant les périodes d'accrétion. Ces accrétions peuvent ainsi être datées : (1) du Campanien supérieur (≈ 75 – 72 Ma), (2) du Maastrichtien supérieur (≈ 69 – 65 Ma), (3) du Paléocène supérieur (≈ 58 – 56 Ma). Un dernier événement tectonique de l'Éocène supérieur (Bartonien - Priabonien) est bien ressenti sur la marge. Du Sud au Nord de la Cordillère Occidentale ces 4 séquences indiquent, mise à part celle du Maastrichtien, une diminution globale de la paléo-profondeur du Campanien à l'Eocène. Cette étude met également en évidence le recul de la paléo-ligne de côte au cours du Crétacé supérieur–Paléogène. En effet, les bassins sédimentaires de la Cordillère Occidentale sont de plus en plus jeunes, d'une part à mesure que l'on s'éloigne de la Cordillère Royale (vers l'Ouest), et d'autre part lorsqu'on se déplace vers le Nord. Ainsi, le bassin de Cuenca au Sud montre un remplissage du Campanien moyen à Paléocène p. p., les dépôts du bassin de Saquisilí vont du Maastrichtien inférieur à la base de l'Éocène moyen, et le bassin d'Apagua contient des sédiments éocènes. Notons que plus au Nord-Ouest, le bassin de la Cubera est rempli de sédiments d'âge Paléocène à Eocène supérieur. À chaque accrétion correspond une augmentation de la granulométrie des grès et des conglomérats, dans les bassins occidentaux (Cordillère Occidentale et côte) comme dans les zones orientales (Zone subandine et bassin Oriente). Seule l'accrétion du Maastrichtien supérieur ne se traduit pas par l'augmentation de la granulométrie, mais par une lacune d'érosion et un changement de zones sources. Dans les bassins de Cuenca, de Saquisilí et d'Apagua, la taille des grains augmente vers le haut, au sein de chaque séquence (Campanien, Paléocène et Eocène), mais aussi d'une séquence à l'autre, confirmant l'accentuation de la paléo-pente de la marge et la surrection de la zone source, constituée essentiellement par la Cordillère Royale. L'augmentation de la paléo-pente se traduit aussi, au sein de chaque séquence, par le passage rapide des grès à des conglomérats en réponse à l'événement tectonique responsable de la discontinuité sus-jacente. Les mêmes évolutions sont observées aux mêmes périodes, mais enregistrées de façon plus distale, sur la côte et dans les zones orientales de l'Equateur. Au cours du Crétacé–Paléogène, l'étude des cortèges de minéraux détritiques des sédiments clastiques de la Cordillère Occidentale montre une évolution en trois étapes : (1) surrection et exhumation de la Cordillère Royale, du Campanien au Paléocène, et dans une moindre mesure jusqu'à l'Éocène supérieur; (2) abondance croissante, du Paléocène à l'Eocène, de fragments lithiques volcaniques et sédimentaires dérivant de la déformation et de l'érosion des terrains préalablement accrétés, et (3) reprise de l'activité de l'arc magmatique à partir de l'Éocène supérieur-Oligocène inférieur marqué par un enrichissement en plagioclases. L'étape 1 montre un enrichissement des sédiments en grains minéraux et lithiques de plus en plus stables (quartz mono- et polycristallin) au détriment des feldspaths, qui reflète une plus grande maturité des sédiments du Paléogène par rapport à ceux du Crétacé terminal. La source principale est alors la Cordillère Royale. À partir du Paléocène supérieur (étape 2), l'augmentation progressive des fragments lithiques issus de couvertures océaniques montre que la zone déformée et soumise à l'érosion s'élargit, et inclut les terrains accrétés. De plus, cette analyse a permis de mettre en évidence deux résultats supplémentaires. Les bassins de la Cordillère Occidentale (Cuenca, Saquisilí, Apagua et La Cubera), présentent une géométrie rectangulaire, des bordures actives correspondant à des failles décrochantes dextres orientées NNE réactivant les sutures, et une subsidence importante, qui suggèrent qu'il s'agit de bassins en «pull apart». On note de plus une augmentation de leur taux de sédimentation avec le temps, suggérant une instabilité croissante de ces bassins installés sur les sutures des accrétions. La création de ces bassins semble coïncider avec les accrétions de terrains océaniques, qui se seraient donc effectuées en contexte transpressif. Leur migration vers le Nord dans le temps reste cependant à expliquer. Les zones orientales sont classiquement interprétées comme un bassin flexural d'avant-pays de la chaîne andine depuis le Crétacé supérieur. Cependant, notre étude montre qu'à l'échelle du bassin : (1) les hiatus et les érosions sont plus importants au pied de la Cordillère Royale qu'à l'Est; (2) l'épaisseur des dépôts campaniens à éocènes augmente d'Ouest en Est; et (3) les séquences du Maastrichtien, du Paléocène et de l'Eocène sont rétrogradantes en direction de l'Ouest. L'ensemble de ces observations indique donc que la marge est en surrection, plutôt qu'en subsidence. Nous proposons donc un modèle de construction de la chaîne andine par surrection isostatique liée à l'introduction successive sous la marge, par accrétion et sous-placage, de matériel crustal océanique de densité relativement faible. Ces terrains auraient causé une poussée d'Archimède responsable de la surrection isostatique de la marge après chaque accrétion. Dans ce modèle, après chaque surrection, la réaction visqueuse de la lithosphère soulevée et flexurée aurait <b>abouti</b> à accentuer le rayon de courbure de la lithosphère, permettant la rétrogradation des dépôts clastiques d'arrière-arc en direction de la chaîne, et expliquant le délai observé de la réponse sédimentaire dans ces zones orientales...|$|E
40|$|Travaux du Laboratoire de géologie de l'Université de Grenoble - Mémoires n° 9 The area studied {{runs from}} the Isère valley (Tarentaise) at the city of Bourg-Saint-Maurice, in France, to the italo-swiss border, between the eastern {{sedimentary}} coyer of the outer crystalline massifs and {{the front of the}} "Briançonnais zone". Its mainly corresponds to the "Brèches de Tarentaise" facies belt, once defined by R. BARBIER, South of the Isère valley. I. - STRATIGRAPHIC AND PALEOGEOGRAPHIC OUTLINE The Brèches de Tarentaise facies belt is an intermediate paleogeographic realm, between the delphino-helvetic (autochtonous or parautochtonous) realm and the Subbriançonnais ~) realm. According to the classical view, it has been stratigraphically characterized by two separate groups of rocks each of which corresponds to a quite different phase {{in the evolution of the}} sedimentary basin. The substratum includes aIl the formations from the crystalline basement up to the Lias. It generally exhibits a shallow water facies, with numerous gaps. The "série détritique" (detrital formation) or "Flysch de Tarentaise", is much thicker. Certain part of it only exhibits a flysch facies. It results from the early movements of the alpine orogeny. 1. - The substratum As the outcrops are rather widely scattered, the area studied does not lend itself to a detailed survey of lts various formations. Yet, some details are given on out the Permian which is chiefly represented here by facies close to the Briançonnais verrucano, instead of the violet shales (Schistes de la Bagnaz) which were regarded up to now as a typical fracture of the zone. The Pointe Rousse "gneisses" (vallon du Breuil. Italy) are seen as resulting from an acidic late hercynian volcanism or (of permotriassic age). Comparison with the Briançonnais formations of the Vanoise, permitted a more detailed study of the Triassic formations. The occurence of breccias, which can be attribuated to the Upper Triassic i 8 worth mentionning. There is no new truly significant datas on the stratigraphy of the Liassic formations. No evidence of the former existence of the Dogger or of the Malm has been found (a gap is likely, but no proved beyond aIl doubt). 2. - the Anteflysch group : The above-mentionned classical stratigraphic picture of the Brèches de Tarentaise facies belt must be somewhat changed, as a newly defined formation: the anteflysch group is to be inserted between the substratum and the detrital formation. This new formation chiefly consists of black shales, of calcschists, and of thin layers of microbreccias. On the Eastern margin of the zone (Versoyen formation) it includes abundant submarine volcanic outflows. One of the more important features of the present study is the proof which is given of a former connection between the Versoyen formation and the detrital formation. The age of the anteflysch group is not known with much accuracy; it probably ranges somwhere between the Lower cretaceous and the Midcretaceous. 3. - The Tarentaise detrital formation The three lithological subdivisions established as early as 1929 by H. SCHOELLER, 1929, have been kept unchanged. From top to bottom, on can observe: the flysch (Saint-Christophe beds) the black shales carrying green quartzites (Marmontains beds) the basal beds. A sedimentological investigation of the above mentionned layers has allowed, an approximative reconstitution of the evolution of the Sedimentary basin to be made: a) The basal layers are characterized by a graduaI change in the facies from the polygenic conglomerates in the SW to microbreccia limestones and thin bedded limestones to the NE. In the area studied the main source of detritus is located to the South West (Hautecour cordillera). The type of deposition (probably of shallow water origin) is very close by its sedimentological features to a molasse. b) The green quartzites-bearing black shales. :This is a very typical, usually rather thin, layer. From a sedimentological point of view, its features are variable, comprised between those of a flysch ant those of a molasse. Here, the main sources of detritus seem to be located to the North East. c) The flysch. Its thickness is less than was believed up to now (from 600 to 700 m). It chiefly consists of limestone layers interbedded with black or grey shales. In sorne parts of the basin, alld in the upper part now visible, there is a large increase in this formation of stratas thickness (trending towards a molassic facies ?). d) Age of the detrital formation. Rare discoveries of microfaunas in the anteflysch group and in the basal layer, give an Upper cretaceous age (probably Maestrichtian) for most of the detrital layer. At the moment there is no evidence that Tertiary sediments may be present in it. II. - STRUCTURAL INVESTIGATIONS The structural investigation bears upon three important points. First it provided the geometrical arguments for proving that the Versoyen formation was initially bound to the Tarentaise detrital formation of which it represents the basement. Then the structural investigation permitted a more accurate definition of the paleogeographical location of the Petit-Saint -Bernard tectonic unit. The latter, being squeezed between the Roignais -Versoyen and the Salin tectonic units, belongs to the Brèches de Tarentaise facies belt (up to now, some authors still believed that Petit Saint Bernard unit originated from the Piemont "Schistes lustrés" facies belt). Finally the structural study allowed to reinvestigate the mechanism of the "nappe structure" setting (by mean of a study of the internaI geometry of each tectonic unit). It is now necessary to restrict the extension of the Moutiers tectonic unit, North of Isère valley by defining a new unit, the Roignais Versoyen tectonic unit. Finally, going from the outer to the inner parts of the range, the sequence of the different tectonic units constituting the Brèches de Tarentaise facies belt seems to be as follows : Moûtiers t. u - Roignais Versoyen t. u - Petit Saint-Bernard t. u - (Pierre Avoi t. u) - Salins and "des Cols"t. u. For the time being, the Ferret Tectonic unit is linked with a transition zone between the Ultrahelvetic realm and the Brèches de Tarentaise facies belt. Its connections with latter cannot now be accurately defined. Many deformation stages have been outlined : a first stage related to a very progressive compression movement gave rise to the major folds and then ro the shearing of the major tectonic units (the metamorphism is of same age). The paroxysm probably occured at the time of the thrusting out of the Prealps units. A later compressional stage, directed E-W, is not equally intensive over the whole area investigated. Finally, there seems to have occured in a last stage a general translation of the several units towards the major tectonic units. There is however no direct evidence of such a movement. The time sequence of these different stages cannot be accurately determined on account of the lack of accurate age determinations for the different formations. III. - PALEOGEOGRAPHICAL CONCLUSIONS The Stratigraphical, Sedimentological and Structural analysis on the investigated area leads to the following conclusions : On a cross section of the investigated zone, the length of the Brèches de Tarentaise facies realm can be estimated to half the length of the "Briançonnais patform". It is only separated from the latter by the "through" corresponding to the "Subbriançonnais zone" (s. str.) which thins out to the E -N -E. - The geodynamical evolution of the Brèches de Tarentaise realm from the Triassic to the Lower cretaceous (not included) is very similar to that of the Briançonnais platform (positive tendancy). On account of this similarity, the term outer penninic platform is proposed to characrize this zone during the period considered. - The evolution of the Brèches de Tarentaise realm starts individualising from the beginning of the Cretaceous. As a matter of fact, after the emersion period from the Dogger to the Malm, there is a very clear subsidence reversal. At first, the latter was very slight (deposit of anteflysch group in conditions still very similar to those of a platform). The subsidence is however stronger from West ta East (Volcanism of the Versoyen formation). Then it gradually increases and brings out the deposition of the thick detrital formation, which are of Upper Cretaceous age. The sedimentological investigation of the latter formation shows that the rate of subsidence initially highly uneven at the different points of the sedimentary basin (occurence of ridges and cordilleras when basal layers were being deposed) was becoming more and more even with time. As a matter of fact the flysch facies which concludes the sedimentary evolution of the basin becomes very homogeneous on a wide area. The evolution of the realm during the Tertiary times is not known. Thus, the very concept of a Valaisian geosyncline must be sharply restricted in rime to the single Cretaceous period. La région étudiée s'étend de la vallée de l'Isère (Tarentaise) au niveau de Bourg-Saint-Maurice, en France, jusqu'à la frontière italo-suisse, entre la couverture sédimentaire oriental. e des massifs cristallins externes et le front briançonnais. Elle correspond pour l'essentiel à la zone des Brèches de Tarentaise définie par R. BARBIER au Sud de l'Isère. 1. - APERCU STRATIGRAPHIQUE ET PALEOGEOGRAPHIQUE La zone des Brèches de Tarentaise est un domaine paléogéographique intermédiaire entre la zone delphinohelvétique (autochtone ou parautochtone) et la zone subbriançonnaise (s. str.). Dans la conception classique, elle est caractérisée sur le plan stratigraphique par deux ensembles lithologiques distincts, qui correspondent à deux phases bien différentes dans l'évolution du bassin sédimentaire. Le substratum regroupe tous les termes depuis le socle cristallin jusqu'au Lias. Il présente des faciès généralement peu profonds, ainsi que de nombreuses lacunes. La série détritique ou "Flysch" de Tarentaise est beaucoup plus épaisse. Certains de ses termes seulement présentent le faciès flysch. Son apparition est la conséquence des premières manifestations de l'orogénie alpine. 1. - Le substratum La région étudiée se prête mal à un examen approfondi des terrains le constituant (dispersion des affleurements). Des précisions sont cependant apportées en ce qui concerne le Permien représenté surtout par des faciès proches du verrucano briançonnais au lieu des schistes violets (schistes de la Bagnaz) considérés jusque là comme caractéristiques de la zone. Les "gneiss" de la Pointe Rousse (vallon du Breuil, Italie) sont attribués à un volcanisme acide finihercynien (âge permo-triasique). La stratigraphie du Trias a pu être précisée par comparaison avec les séries briançonnaises de la Vanoise. A noter la présence de brèches du Trias supérieur. Aucune donnée nouvelle importante n'a pu être établie à propos du Lias en ce qui concerne la stratigraphie. Aucun indice d'une existence ancienne du Dogger et du Malm n'a été découvert (lacune probable mais non prouvée avec certitude). 2. - L'ensemble antéflysch : Le schéma stratigraphique classique de la zone des Brèches de Tarentaise, rappelé ci-dessus, doit être quelque peu modifié. Un nouveau terme vient en effet s'intercaler entre substratum et série détritique: l'ensemble antéflysch. Cette nouvelle formation est représentée surtout par des calcschistes, schistes noirs, et niveaux micro-bréchiques; elle voit apparaître d'abondantes émissions sous -marines (série du Versoyen) dans le secteur oriental. La démonstration de la liaison stratigraphique primaire de la série du Versoyen avec la série détritique (ou "Flysch de Tarentaise"), dont elle constitue le soubassement, est un point important du présent travail. L'âge de l'ensemble antéflysch n'est pas connu avec grande précision; il correspond sans doute au Crétacé inférieur et moyen. 3. - La série détritique de Tarentaise Les trois subdivisions lithologiques établies dès 1929 par H. SC HOELLER, ont été maintenues, De haut en bas on rencontre: Le flysch (couches de Saint-Christophe) Les schistes noirs à quartzites verts (couches des Marmontains) La formation basale, L'étude sédimentologique de ces trois ensembles a permis de reconstituer de façon approximative l 'histoire du bassin. a) La formation basale est caractérisée par une variation progressive du faciès depuis des conglomérats polygéniques au Sud Ouest, vers des calcaires microbréchoides et calcaires plaquettés au Nord Est. Dans le secteur étudié la zone d'alimentation principale se trouve au Sud Ouest (cordillère de Hautecour). Le type de dépôt (probablement en eau peu profonde) s'apparente par ses caractères sédimentologiques, à une molasse. b) Les schistes noirs à quartzites verts Formation très caractéristique en général peu puissante. Sur le plan sédimentologique ses caractères sont variables intermédiaires entre flysch et molasse. Les zones d'alimentation principales paraissent situées cette fois vers le Nord Est. c) Le flysch D'épaisseur moindre qu'il était admis jusque là (6 à 700 m) il s'agit surtout d'un flysch calcaire. En certains points du bassin et à la partie supérieure actuellement observable on note un accroissement considérable de l'épaisseur des strates (tendance vers un faciès molassique ?). d) Age de la série détritique De rares découvertes de microfaunes dans l'ensemble antéflysch et dans la formation basale, indiquent un âge crétacé supérieur (Maestrichtien probablement) pour la plus grande partie de la série détritique. Il n'existe pour l'instant aucun indice que le Tertiaire puisse y être représenté. II. - ETUDE STRUCTURALE L'étude structurale porte sur trois points principaux. Elle a fourni tout d'abord les arguments géométriques prouvant que la série du Versoyen était liée à la série détritique de Tarentaise dont elle représente le soubassement. Elle a permi ensuite de préciser la position paléogéographique de l'unité du Petit Saint-Bernard. Celle-ci étant pincée entre les unités du Roignais-Versoyen et de Salins appartient bien au domaine paléogéographique des Brèches de Tarentaise (et non au domaine piémontais comme cela était encore admis par certains auteurs). Elle a enfin permis de reconsidérer la question de mise en place des "nappes"(par l'étude de la géométrie interne des diverses unités). Il est apparu nécessaire de restreindre l'extension de l'unité de Moûtiers au Nord de l'Isère par la définition de l'unité nouvelle du Roignais-Versoyen sur des critères tout à la fois stratigraphiques et structuraux. Finalement la succession paléogéographique des diverses unités constituant le domaine des Brèches de Tarentaise parait être la suivante de l'extérieur vers l'intérieur de la chaîne : Unité de Moûtiers; Unité du Roignais-Versoyen; Unité du Petit Saint-Bernard (Unité. de la Pierre Avoi); Unité de Salins et Unité des Cols. L'Unité de Ferret est pour l'instant rattachée à une zone de transition entre l'ultrahelvétique et la zone des Brèches de Tarentaise. Ses rapports avec cette dernière ne peuvent être précisés. Plusieurs phases de déformation ont pu être mises en évidence: une première phase de compression très progressive a <b>abouti</b> au plissement puis au clivage des principales unités (le métamorphisme est contemporain de cet épisode). Le paroxysme a probablement correspondu à l'expulsion des Préalpes. Une phase de serrage tardive de direction est-ouest se fait inégalement sentir sur le domaine étudié. Enfin il apparaft, sans toutefois de preuves décisives, qu'un coulissage dans le sens des structures s'est opéré postérieurement au paroxysme. La chronologie de ces divers mouvements ne peut être établie directement par manque de datation précise des terrains. III. - CONCLUSIONS D'ORDRE PALEOGEOGRAPHIQUE L'analyse stratigraphique, sédimentologique, et structurale du domaine étudié, conduit aux conclusions suivantes: - Le domaine paléogéographique de la zone des Brèches de Tarentaise présente, sur la transversale étudiée, une extension qui peut être estimée à la moitié de celle de la zone briançonnaise. Il n'en est séparé que par le "sillon" des unités subbriançonnaises (s. str.) lequel va en s'amincissant vers l'Est Nord Est. - L'évolution géodynamique de la zone des Brèches de Tarentaise du Trias au Crétacé inférieur exclu, est très voisine de celle de la palte-forme briançonnaise (tendance positive persistante). Par raison de similitude le terme de plateforme pennique externe est proposé pour définir la zone en question pendant la période de temps considérée. - L'évolution de la zone des Brèches de Tarentaise devient spécifique à partir du Crétacé. En effet après la période d'émersion du Dogger Malm un renversement de subsidence très net s'opère. D'abord modéré (dépôt de l'ensemble antéflysch dans des conditions proches encore d'une plate-forme. La subsidence est toutefois plus active d'Ouest en Est; volcanisme du Versoyen), celle-ci prend peu à peu de l'importance et voi t le dépôt de l'épaisse série détritique terminale d'âge crétacé supérieur et un mode de dépôt particulier. L'étude sédimentologique de cette dernière a montré que la subsidence d'abord très inégale selon les points du bassin (existence de rides et de cordillères lors du dépôt de la formation basale) allaient en s'égalisant avec le temps. De fait le flysch avec lequel s'achève la série lithologique actuellement observable devient très homogène sur une vaste région. L'évolution du domaine au Tertiaire est inconnue. Ainsi la conception même d'un géosynclinal valaisan doit-elle être très limitée dans le temps au seul Crétacé...|$|E
40|$|In the {{framework}} of the conservation of early reinforced concrete structures from the last third of the 19 th century up to 1914, this research deals with superstructures (excluding foundations, roads, pipes, etc.) in reinforced concrete (in the modern sense of the term – i. e. concrete made with artificial cement and rebars supplying tensile strength; thus, the combination of a metal profile embedded in concrete is excluded). The development of reinforced concrete as a building material started around 1880 and became widespread {{around the time of the}} First World War. Some of the structures concerned are listed as heritage properties today. Therefore they deserve specific and careful study to ensure long-term preservation of their historic, architectural, technical and socio-economic value. They bear witness to a period in construction history when reinforced concrete was a new material. The outbreak of the First World War marked the end of the initial period of innovation, exploration and experimentation. By then, reinforced concrete had become widely accepted and adopted as a suitable and effective building material. However, present-day attempts at restoration often prove inadequate, due to incomplete understanding of this period of construction and the characteristics of the first generation of reinforced concrete. If the causes of degradation are incorrectly diagnosed, the repairs are likely to be inappropriate. Moreover, the number of reinforced concrete structures requiring repair work is currently increasing with the natural ageing of the material. This phenomenon will continue to grow in the coming years. With this in mind, the present research aims at identifying the specific structural characteristics of reinforced concrete structures erected before the First World War. Several axes of investigation were pursued in this PhD research and have resulted in the main observations detailed below. - Based on a case study of the region of Brussels (Belgium), a database of structures built in reinforced concrete prior to 1914 was drawn up in order to place the material in its historical and geographical context. The inventory currently contains 507 examples and provides a panorama of the uses of reinforced concrete, ranging from numerous foundations and slabs to a complete structure from the end of the 1890 s. This list is supplemented by a survey of a total of 605 patents filed for reinforced concrete in Belgium before the First World War. The early development of reinforced concrete was strongly related to national patenting, with a considerable number of systems being patented by private inventors for commercial purposes. Reinforced concrete profoundly transformed the building industry. All the professions working with the composite material had to change their approach, from the planning stage through to execution on the site. From the viewpoint of construction history, all these modifications make the time of the advent of reinforced concrete a particularly fruitful period to study. - From the survey of early reinforced concrete structures in Brussels and the database of Belgian patents, the supremacy of the Frenchman François Hennebique and his system on the Brussels market for reinforced concrete (and, by extension, on the Belgian market) before 1914 is incontestable. This commercial achievement resulted from a combination of factors: an efficient structural system, meticulous attention to the quality of on-site reinforced concrete execution, and the commercial acumen to develop the business through advertising and other media. The well-known Hennebique system represents a monolithic structure including slabs, beams and columns. In fact, this system changed over the decades of operation of Hennebique’s company, not so much in relation to the design methods (his original semi-empirical method continued to be used) but particularly in practical terms (the type and location of the rebars among others). The evolution of the system is analysed by means of technical drawings from about 30 Belgian projects designed by Hennebique between 1900 and 1930. - After the building contractors, who had been the first to believe in the structural and economic potential of reinforced concrete, engineers invented the calculation models and architects started developing new shapes. The Belgian engineer Paul Christophe was among the first theorists of reinforced concrete. The publication of his book Le béton armé et ses applications in 1899 is internationally recognised as a milestone in the rational modelling of structural reinforced concrete elements. Prior to the present study, details of his life and work remained largely uninvestigated, but the discovery of large parts of his personal archives has allowed clarification of his role in the popularisation of reinforced concrete, especially at the theoretical level. - Reinforced concrete structures around the beginning of the 20 th century were initially governed by empirical models of calculation (and execution) developed by the individual constructors. Gradually, reinforced concrete standards, published between 1904 and 1923 and based on working stress analysis and elastic modular ratio theory, replaced the utility of the patented systems. The different theoretical approaches are briefly described in this research. Mastering the theoretical assumptions and calculation methods used at the time represents the first step towards an appreciation of the structural behaviour and the possible weaknesses that can be expected. - A review, based on literature published at that time, of the properties of the components of reinforced concrete allows identification of the characteristic materials used in the concrete matrix and the metal reinforcements. The execution process and the available technological tools for erecting a reinforced concrete structure are also addressed, as these would have had a direct influence on the quality of construction. Non-destructive and destructive experimental laboratory tests were performed on original samples, mainly removed from the Colo-Hugues viaduct (1904, Braine-l’Alleud, Hennebique system) in order to assess the mechanical properties, chemical features and durability issues for concrete and ferrous reinforcements. Comparing the results obtained using different techniques also makes it possible to determine the extent to which these techniques are reliable for the appraisal of early reinforced concrete structures. - The structural efficiency of the Hennebique system is assessed based on an understanding of the principles of Hennebique’s semi-empirical method of calculation, but also – and primarily – by means of observations from experimental tests carried out on full-sized beams removed from the Colo-Hugues viaduct. Analysing and understanding the behaviour of the new composite material was a critical issue for promoting the use of reinforced concrete at the beginning of the 20 th century. Today, what is required is a re-assessment of its structural behaviour. Three bending tests up to failure in simply supported conditions were performed at the BATir Department of the Université libre de Bruxelles on T-beams from the Colo-Hugues viaduct. This case study is representative of the majority of Hennebique structures, because the typical continuous straight T-beam is the main structural element of any Hennebique structure (bridge, building, etc.). The first test is a four-point bending test on a complete span (6 m) of the viaduct to obtain the response of the central part under positive bending moment. The flexural failure was ductile and occurred through yielding of the reinforcements followed by crushing of the concrete at mid-span. The second and third tests are three-point bending tests on 4 m long specimens centred on the column, representing the behaviour of the beam around the supports. These showed a sudden slipping failure due to loss of the adhesive bond between rebars. The results of these three experiments combined reproduce the actual behaviour of the viaduct in service. The bearing capacity of the Hennebique system in service and at ultimate has been demonstrated, at least for one loading case. These experimental tests provide essential data for a better understanding of the mechanisms of failure and reveal the main weaknesses of the Hennebique T-beam. Two strengthening solutions are suggested as supplementary information. - The pathologies observed in early reinforced concrete structures (honeycombs, corrosion of the rebars, and so on) are mainly attributable to the tools and techniques that the builders had at their disposal (handmade compaction, high water-to-cement ratio, etc.) and by the limited contemporary knowledge of the physical and chemical phenomena, especially with regard to long-term effects. In fact, the concrete quality of the viaduct is surprisingly satisfactory despite its great age, due to the fact that the whole structure was covered with plaster, like the majority of reinforced concrete structures designed at that time. This research establishes that reinforced concrete structures from 1880 to 1914 differ from later reinforced concrete structures. Taking into consideration the features of early reinforced concrete structures will contribute to ensuring sustainable conservation with limited intervention, thus preserving as much as possible of the original structure when restoration work is undertaken. Working on existing buildings often requires a multidisciplinary and holistic approach. The present study could thus be extended in various areas. For example, other structural aspects could be studied more in depth, such as demonstration of the shear strength of the Hennebique system or detailed consideration of the reinforcements (low adherence, particular anchorage devices, etc.) / C'est dans le cadre de la conservation, au sens large du terme, que s'inscrit cette recherche sur les constructions en béton armé de première génération, c'est-à-dire de la fin du 19 ème siècle au début du 20 ème siècle. Cette recherche traite uniquement des superstructures, à l'exclusion des fondations, routes, tuyaux, etc. et en béton armé au sens moderne du terme, c'est-à-dire un béton réalisé à base de ciment artificiel et dont les armatures interviennent surtout pour reprendre les efforts de traction, ce qui exclut par exemple les utilisations de poutrelles métalliques enrobées de béton. Certains de ces ouvrages, réalisés entre 1880 et 1914, font aujourd'hui partie intégrante du patrimoine bâti, pour leurs valeurs architecturale, historique, technique ou aussi socio-économique. Ils jalonnent désormais l'histoire de la construction comme témoins d'une époque où le béton armé était un matériau nouveau. La Première Guerre mondiale marque la fin de cette période de premières innovations, d'explorations et d'expérimentations. Elle entérine l'acceptation et la diffusion du béton armé comme matériau de construction à part entière. Cependant, ainsi que le montrent certains projets de restauration actuels aux interventions inadéquates, il y a encore une méconnaissance des spécificités du béton armé de cette époque. Les causes de leurs dégradations mal diagnostiquées sont traitées de façon inappropriée. Or, dans les prochaines années, nombre de structures en béton armé construites dans la première moitié du 20 ème siècle seront amenées à subir une rénovation suite au vieillissement naturel du matériau. C'est pourquoi pour conserver au mieux ces structures, il est indispensable d'étudier en détails leurs caractéristiques techniques pour ensuite intervenir, si nécessaire, de façon précise et adaptée. Ce doctorat s'attèle donc à identifier les particularités des constructions en béton armé construites avant l'avènement de la Première Guerre mondiale, et plus spécifiquement à étudier leurs aspects structuraux. Plusieurs axes de recherche ont été développés et ont <b>abouti</b> aux principaux résultats suivants. - Basé sur le cas de la région de Bruxelles-Capitale (Belgique), un inventaire des interventions en béton armé, construites avant 1914, a été dressé pour replacer le matériau dans son contexte historique et géographique. Cette base de données, comprenant 507 biens jusqu'à présent, illustre les types d'utilisation du béton armé dans la construction au début du 20 ème siècle, d'abord des fondations ou simples planchers, jusqu'à une structure monolithique complète dès la fin des années 1890. Cet inventaire est complété par le relevé détaillé des brevets, au nombre de 605, déposés à ce sujet en Belgique avant la Première Guerre mondiale. Les brevets ont joué un rôle fondamental dans le développement du béton armé. Celui-ci était, en effet, régi par un foisonnement de systèmes commerciaux, majoritairement brevetés. L'introduction du béton armé a transformé en profondeur le secteur de la construction et notamment les professions liées tant à la phase de conception qu'au chantier lui-même. Du point de vue de l'histoire de la construction, toutes ces mutations font de l'avènement du béton armé une période historique riche. - A la lecture du panorama offert par les inventaires des constructions et des brevets, la prééminence de la compagnie du Français François Hennebique, et donc de son système, sur le marché bruxellois (et par extrapolation sur le marché belge) du béton armé avant 1914 est indéniable. La réussite commerciale de Hennebique résulte d'une combinaison de facteurs: un système efficace sur le plan structural, une qualité d'exécution de béton coulé en place fiable et méticuleuse ainsi qu'un sens développé des affaires, en maîtrisant l'art de la promotion et de la publicité notamment. Le système bien connu de Hennebique comprend un ensemble monolithique formé par des dalles (hourdis), poutres et colonnes. Ce système a, en réalité, évolué dans le temps, pas tant d'un point de vue théorique (les calculs de dimensionnement sont les mêmes) mais plutôt pratique (positionnement, type d'armatures, etc.). Cette évolution a été observée par l'étude d'une trentaine de cas pratiques exécutés par Hennebique entre 1900 et 1930 en Belgique. - Après les entrepreneurs, qui ont été les premiers à croire aux nouvelles possibilités constructives qu'offre le béton armé ainsi qu'à son succès commercial, les ingénieurs en inventent les principes de calcul et les architectes en révolutionnent les formes. L'ingénieur belge Paul Christophe fut parmi les premiers théoriciens du béton armé. La publication de son ouvrage Le béton armé et ses applications en 1899 constitue une étape importante, et internationalement reconnue, pour le dimensionnement rationnel d'éléments structuraux en béton armé. Jusqu'à la présente recherche, sa vie et son œuvre étaient restées assez confidentielles mais la découverte d'une partie de ses archives personnelles a permis de clarifier son rôle dans la diffusion, surtout théorique, du béton armé. - Les structures en béton armé d'avant la Première Guerre mondiale furent d'abord gouvernées par des méthodes empiriques de dimensionnement (et d'exécution) développées par chaque constructeur. L'apparition des premières règlementations entre 1904 et 1923, basées sur une analyse en contraintes admissibles et la théorie du coefficient d'équivalence, remplace ensuite peu-à-peu l'utilité des systèmes brevetés. Les différentes approches théoriques sont brièvement décrites dans cette recherche. Maitriser les hypothèses et les méthodes de calculs employées à l'époque est, en effet, une première étape pour comprendre le fonctionnement structural prévu et les potentielles défaillances de dimensionnement. - A travers une lecture attentive de la littérature publiée à cette période, les matériaux intervenants dans la fabrication du béton armé (c'est-à-dire le béton et les armatures) et utilisés couramment au début du 20 ème siècle ont été identifiés ainsi que les moyens disponibles à cette époque pour produire des structures en béton armé. Des méthodes d'essais non-destructives et destructives ont été appliquées principalement, sur le viaduc Colo-Hugues (1904, Braine-l'Alleud, système Hennebique) afin d'évaluer les caractéristiques mécaniques, les propriétés chimiques et la durabilité tant du béton que des renforcements métalliques. Comparer les résultats de ces différentes méthodes permet d'aborder les limites d'utilisation de ces techniques, lorsqu'il s'agit d'évaluer structuralement des bétons armés de première génération. - Grâce à la compréhension des principes, semi-empiriques, de dimensionnement appliqués par le bureau Hennebique en son temps mais surtout grâce aux observations déduites des essais expérimentaux réalisés sur des poutres de grandeur réelle, prélevées sur le viaduc Colo-Hugues, le fonctionnement structural réel du système Hennebique est évalué. Comprendre et modéliser le comportement du nouveau matériau composite fut une problématique fondamentale pour accroître l'usage du béton armé au début du 20 ème siècle. Actuellement, il s'agit de réévaluer le comportement de ces structures. Trois essais jusqu'à rupture ont été menés, au département BATir de l'Université libre de Bruxelles, sur des poutres à gousset en T provenant du viaduc Colo-Hugues en conditions isostatiques et soumises à flexion. Ce viaduc des chemins de fer vicinaux est un cas d'étude représentatif de la majorité des constructions Hennebique, car la poutre de section en T est la structure typique du système Hennebique, utilisée tant dans les ouvrages d'art que dans les bâtiments. Le premier essai est une flexion 4 points sur une travée complète du viaduc (6 m de portée) pour obtenir la réponse en zone de moment maximum positif. La rupture ductile a eu lieu par plastification des armatures suivie d'un écrasement du béton en zone centrale, c'est-à-dire dans la zone la plus sollicitée. Deux éléments identiques de longueur de 4 m ont été essayés en flexion 3 points pour représenter le comportement sur appuis. La rupture de ces deux dernières expériences s'est produite suite à un glissement des armatures sur appuis (goussets à côté de la colonne). Il s'agit donc d'une rupture à caractère fragile. Les trois essais combinés représentent correctement la structure hyperstatique du viaduc dans son fonctionnement en service. La capacité portante réelle du système Hennebique en service et à l'état limite ultime, du moins dans un cas de chargement, a pu être expliquée. Ces essais fournissent les données essentielles pour estimer l'efficacité structurale du système Hennebique et identifier ses faiblesses. Deux solutions de renforcement sont proposées en complément d'information. - Les pathologies observées dans les bétons armés datant du début du 20 ème siècle (nids de graviers, corrosion des armatures, etc.) sont, la plupart du temps, causées par les outils sommaires à la disposition des constructeurs (vibration à la main, rapport eau/ciment plus élevé qu'aujourd'hui, etc.) et par une connaissance limitée des phénomènes physiques et chimiques, surtout à long terme. En fait, la qualité du béton du viaduc Colo-Hugues est particulièrement satisfaisante malgré l'âge avancé du béton, grâce notamment à l'enduit recouvrant l'ensemble du viaduc, ce qui est le cas pour la majorité des structures de la période étudiée. Cette recherche démontre que les constructions en béton armé datant de 1880 à 1914 diffèrent des ouvrages postérieurs en béton armé et qu'il serait utile pour leur restauration de tenir compte de ces spécificités. La connaissance approfondie des particularités des constructions en béton armé de première génération permettra, espérons-le, de contribuer à leur longévité en intervenant le moins possible sur les structures d'origine. Etant donné que l'étude des structures existantes nécessite le plus souvent une approche pluridisciplinaire, ce travail pourrait être poursuivi dans plusieurs domaines variés. Il resterait notamment à approfondir d'autres aspects de stabilité, comme par exemple la démonstration de l'efficacité à l'effort tranchant du système Hennebique ou encore la prise en considération plus détaillée des armatures (adhérence limitée, forme d'ancrage particulier, etc.). Doctorat en Sciences de l'ingénieurinfo:eu-repo/semantics/nonPublishe...|$|E
40|$|Summary Background: Health impact {{assessment}} (HIA) is an interdisciplinary approach that aims at predicting and managing potential {{positive and negative}} health effects of policies, programmes and projects on affected communities and populations. HIA has been developed {{over the past two}} decades and became an integral part of public health policies of many governments in the industrialised world. However, in many developing countries, where two thirds of the world’s population are now concentrated, HIA has yet to be institutionalized. This is particularly important in view of the high burden of disease and pronounced health inequalities in tropical areas. Furthermore, it is anticipated that major drivers of global change, such as population growth and urbanisation, increasing demand in natural resources and regional climate change, will have severe health implications, particularly in the developing world. This will require modification of existing, and development of new policies and programmes in various sectors. Thus HIA, utilized as a systematic approach for the assessment of health impacts, is an important tool and strategy to assist decision-makers for health promotion in low- and middle-income countries. Against this background, the lack of well-defined HIA methodologies that are designed for the purpose of a typical tropical country context was identified as an important constraint for the promotion of HIA in developing countries. Objectives: Four specific objectives were pursued in this PhD: (i) to develop and advance HIA tools and methods that are readily adapted to complex eco-epidemiological settings in the humid tropics; (ii) to validate these tools and methods within the frame of HIA of industrial development projects in developing country contexts; (iii) to systematise key findings and discuss lessons learned so that the tools and methods become available to HIA practitioners; and (iv) to deepen the understanding of the complex linkages of project-related activities with affected communities and their environment. Research partnership: This PhD project was carried out within the frame of a public-private partnership between the Swiss Tropical and Public Health Institute (Swiss TPH) and NewFields, an international consultancy company with long-standing expertise in HIA in developing countries. Collaboration in selected HIA assignments for private and public clients of NewFields served as platforms for the present research. Method: The PhD thesis entailed field work in the frame of 11 HIA for different industrial development projects, particularly in sub-Saharan Africa. Each of these HIA assignments held the opportunity for the development, validation and consolidation of tools and methods according to the respective stage of the HIA process. Results: Over the course of this PhD thesis, a HIA-trilogy was generated, consisting of three parts, each of which is built on a case study and introduces a set of methodological contributions to the overall HIA process in complex eco-epidemiological settings in tropical regions. In the first part, the concept of environmental health areas (EHAs) is used and further developed, potentially-affected communities (PACs) are stratified, and different information sources are employed, including participatory methods, to obtain quality baseline health data. Feeding these data into a novel risk analysis matrix facilitates the ranking of potential health impacts for subsequent prioritization of mitigation strategies. The tools were developed within the frame of a HIA of a large gold mining project in a remote area of the Democratic Republic of the Congo. The outcomes encapsulate a multitude of environmental and health determinants in a systematic manner. In the second part, the centrality of the scoping phase is illustrated with specific examples drawn from an ongoing HIA of a large iron ore project in the Republic of Guinea. Data from stakeholder consultations, limited community involvement and a desktop review of available health statistics is integrated via an analytical framework for the systematic selection of health outcomes and determinants of major concern. A subsequent gap analysis is utilized to assess the need for further baseline health data collection and to facilitate the specification of a set of potential indicators and strategies to inform the required evidence-base. It is argued that this more rigorous approach to scoping than heretofore is a prerequisite for the planning and implementation of any baseline health survey as part of the overall HIA process in multilayered socio-economic and eco-epidemiological contexts. Last but not least, in the third part, a modular cross-sectional baseline health survey study design, which has specifically been developed for HIA of industrial development projects in tropical areas, is presented. The modular framework can be readily adapted to the prevailing eco-epidemiological characteristics of a given project setting. A broad set of key performance indicators (KPIs) is underlying the modular methodology, covering a multiplicity of health outcomes and determinants at different levels. Findings of a baseline health survey carried out in the project region of the aforementioned iron ore mining project in the Republic of Guinea illustrate the use and value of the proposed methodology. This study demonstrates that quantitative assessment of health impacts is an important feature for realising the full potential of HIA as it will not only allow to further our understanding of how communities are affected by projects, but also improve the predictive validity of HIA in areas where demographic, ecological, environmental, epidemiological, health and socio-economic data are sparse. Conclusions: The systematic HIA approach that evolved within the frame of this 3 -year PhD thesis bodes well with the four core values of HIA – ethical use of evidence, democracy, equity and sustainable development – and is thus an important methodological contribution to the science of HIA. Moreover, the developed HIA methodology lends itself well to routine HIA of large-scale development projects in a tropical country context, especially since it has proven to be broadly applicable to different types of projects and environments. However, in order to yield the full potential of HIA in developing countries, similar research efforts are needed on policy and programmatic level in different sectors. At the same time we propose that the World Health Organization and the HIA community at large should make any effort possible to further advocate and expedite excellence and capacities in HIA that are integrated in academia and governments of developing countries. [...] Zusammenfassung Hintergrund: Für den englischen Begriff ‘health {{impact assessment}}’ (HIA) gibt es im Deutschen unterschiedliche Übersetzungen, darunter Formulierungen wie ‚gesundheitliche Folgenabschätzung‘, ‚gesundheitliche Wirkungsbilanz‘ oder ‚Gesundheitsverträglichkeits-prüfung‘. Bisher hat sich keiner dieser Begriffe allgemein durchgesetzt, weshalb hier die Abkürzung HIA verwendet wird. HIA ist ein interdisziplinäres Verfahren zur Vorhersage und Einschätzung von positiven und negativen gesundheitlichen Folgen von Strategien, Programmen oder Projekten auf betroffene Bevölkerungsgruppen. Die HIA-Methode wurde in den vergangenen zwei Jahrzehnten entwickelt und wurde zum integralen Bestandteil des öffentlichen Gesundheitswesens in westlichen Industrieländern. In vielen Entwicklungsländern, wo zwei Drittel der Weltbevölkerung lebt, muss die HIA-Methode noch institutionalisiert werden. Dies ist, in Anbetracht der hohen Krankheitslast und der sozialen Ungleichheiten in Ländern mit niedrigem Einkommen, besonders wichtig. Zudem werden sich Einflussfaktoren des globalen Wandels wie Bevölkerungswachstum, zunehmende Urbanisierung, steigende Nachfrage nach natürlichen Ressourcen und Klimaerwärmung besonders stark auf Entwicklungsländer auswirken. Gravierende Implikationen im Gesundheitsbereich sind Folgen dieser globalen Wandlungen. Dies wird unter anderem dazu führen, dass eine Vielzahl von Strategien, Programmen und Projekten in verschiedenen Bereichen neu entwickelt oder den sich verändernden Bedingungen angepasst werden müssen. Daher wäre das HIA als systematisches Verfahren für die Einschätzung von Auswirkungen auf die Gesundheit eine wichtige Strategie für den Entscheidungs-findungsprozesses zur Gesundheitsförderung in Entwicklungsländern. Allerdings wurde der Mangel an spezifischen Methoden, die gezielt auf die Rahmenbedingungen in tropischen Ländern ausgelegt sind, als eine der Einschränkungen identifiziert, die dazu führt, dass die HIA-Methode in Entwicklungsländern heutzutage kaum Anwendung findet. Ziel: Der vorliegenden Dissertation liegen die vier folgenden Ziele zugrunde: Erstens müssen Verfahren und Methoden entwickelt und angepasst werden, die geeignet sind, die vielen Faktoren in komplexen öko-epidemiologischen und sozio-ökonomischen Bedingungen, wie sie in den Tropen anzutreffen sind, zusammen zu führen. Zweitens wären die entstandenen Verfahren und Methoden im Rahmen von industriellen Entwicklunsprojekten in Entwicklungsländern zu validieren. Drittens sollten bestätigte Verfahren systematisiert, diskutiert und zukünftigen HIA-Fachleuten zugänglich gemacht werden. Viertens galt es, das Verständnis über die komplexen Wechselwirkungen zwischen den Aktivitäten von industriellen Entwicklungsprojekten und der betroffenen Bevölkerung zu vertiefen. Forschungspartnerschaft: Die vorliegende Arbeit wurde durch eine öffentlich-private Partnerschaft zwischen dem ‚Schweizerischen Tropen- und Public Health-Institut‘ (Swiss TPH) und NewFields, einem US-amerikanischen Beratungsunternehmen mit langjähriger Erfahrung im Bereich von HIA in Entwicklungsländern, ermöglicht. Ausgewählte HIA-Projekte für öffentliche und private Kunden von NewFields dienten als Grundlage für die vorliegende Forschungsarbeit Methode: Insgesamt umfasst die Dissertation Feldarbeit im Rahmen von 11 verschiedenen HIA-Aufträgen für industrielle Entwicklungsprojekte, insbesondere in Afrika. Jedes dieser Projekte bot die Gelegenheit für die Entwicklung, Validierung und Konsolidierung von Verfahren und Methoden gemäss der entsprechenden Phase im HIA Prozess. Ergebnisse: Im Verlaufe der Arbeit an der Dissertation ist eine ‚HIA-Trilogie‘ entstanden. Jeder Teil ist auf einem Fallbeispiel aufgebaut und präsentiert methodologische Beiträge zum HIA-Prozess im multifaktoriellen Kontext tropischer Länder. Im ersten Teil wird das Konzept von ‚Umwelt-Gesundheits-Bereichen‘ angewandt und weiter entwickelt, potentiell betroffene Bevölkerungen werden kategorisiert, und für die Beschreibung der gesundheitlichen Grundbedingungen werden verschiedene Informationsquellen wie unter anderem partizipative Erhebungen konsultiert. Für die Einschätzung der potentiellen Auswirkungen dieser Interventionen auf die Gesundheit werden die gewonnenen Daten mit einer neuartigen Risikoanalyse-Matrix verarbeitet, um somit Prioritäten für den Massnahmenkatalog zu setzen. Diese Methoden wurden im Rahmen eines HIA für ein gross angelegtes Goldminen-Projekt in einem abgelegenen Gebiet der Demokratischen Republik Kongo entwickelt und geprüft. Im zweiten Teil wird anhand eines Eisenerzminen-Projekts in der Republik Guinea die zentrale Relevanz der ‚scoping‘ Phase als zweitem Schritt im HIA-Prozess veranschaulicht. Für die systematische Selektion von massgebenden Gesundheitsauswirkungen werden Daten, die mit partizipativen Methoden generiert und durch lokale Gesundheitsstatistiken ergänzt wurden, in eine analytische Datenbank integriert. Die anschliessende Lücken-Analyse dient der Beurteilung der Notwendigkeit eines allfälligen ‚baseline health survey‘ (Basis-Gesundheitszustand-Studie) in der betroffenen Bevölkerung und ist bei der Bestimmung von potentiellen Indikatoren und Datenerfassungsstrategien behilflich. Es wird demnach dargelegt, dass dieses rigorose ‚scoping‘-Verfahren eine Voraussetzung ist für die Planung und Durchführung des ‚baseline health survey‘ als Bestandteil des HIA-Prozesses in multifaktoriellen Kontexten in den Tropen. Im dritten Teil wird ein Studienkonzept für modulare ‚baseline health surveys‘ präsentiert, das gezielt für das HIA von industriellen Entwicklungsprojekten in Entwicklungsländern entwickelt wurde. Der modulare Aufbau kann nach Belieben den öko-epidemiologischen und sozio-ökonomischen Charakteristika eines gegebenen Projekts angepasst werden. Eine breitgefächerte Auswahl an gesundheitsrelevanten, sozio-kulturellen und ökologischen Indikatoren bildet die Grundlage der modularen Methodologie. Ergebnisse eines ‚baseline health survey‘, der in der Region eines Eisenerz-Projektes in der Republik Guinea durchgeführt wurde, dienen der Veranschaulichung der Funktion und des Nutzens des vorgeschlagenen Studienkonzepts. Des Weitern demonstriert diese Fallstudie die Wichtigkeit quantitativer Datenerhebungs- und Beurteilungsmethoden für die Realisierung des vollen Potentials des HIA. Denn diese tragen nicht nur dazu bei, unser Verständnis darüber zu erweitern, wie die Gesundheit von betroffenen Bevölkerungen durch industrielle Entwicklungsprojekte beeinflusst wird, sondern verbessern auch massgeblich die Gültigkeit von Vorhersagen des HIA in Gebieten, wo demographische, epidemiologische, ökologische und sozio-ökonomische Daten kaum vorhanden sind. Schlussfolgerungen: Das systematische HIA-Verfahren, welches im Verlaufe dieser 3 -jährigen Dissertation entwickelt wurde, steht im Einklang mit den vier Grundwerten der hier diskutierten Methodologie (i. e. der ethische Gebrauch von Evidenz, Demokratie, Gleichheit und nachhaltige Entwicklung) und ist daher ein substantieller wissenschaftlicher Beitrag zum HIA. Zudem eignet sich das entwickelte Verfahren für die Routineanwendung in HIA von industriellen Entwicklungsprojekten in tropischen Ländern, insbesondere da es sich in den unterschiedlichsten Bedingungen bewährt hat. Um jedoch das Potential der HIA-Methode in Entwicklungsländern vollends auszuschöpfen, sind weitere Forschungsarbeiten im Bereich von Strategien und Programmen im öffentlichen Sektor nötig. Zugleich schlagen wir vor, dass die Weltgesundheitsorganisation in Zusammenarbeit mit HIA-Fachkräften ihre Bemühungen für die Förderung des HIA-Verfahrens verstärken und Fachkompetenzen ausbilden, die in Hochschulen und Regierungen von Entwicklungsländern integriert sind. [...] Résumé Introduction : L’évaluation d’impact sur la santé (EIS) est une approche interdisciplinaire dont le but consiste à prédire et surveiller les effets potentiels, positifs ou négatifs, d’une politique, d’un programme ou d’un projet sur la santé d’une population. L’EIS s’est développée ces vingt dernières années pour devenir un élément important des politiques de santé de nombreux gouvernements dans les pays industrialisés. Cependant, dans les pays en voie de développement, où vivent par ailleurs les deux tiers de la population mondiale, l’institutionnalisation de l’EIS reste inachevée. Cela est particulièrement important comte tenu de la pression énorme qu’exercent les maladies et les inégalités sanitaires, notamment dans les pays tropicaux. De plus, nous pouvons nous attendre à ce que les majeurs forces du changement global, tels que la croissance de la population et l’urbanisation, la demande croissante en ressources naturelles et les changements de climats au niveau régional, aient de lourdes conséquences sur la santé des populations, particulièrement dans les pays en voie de développement. Cette évolution exigera la modification des politiques actuelles ainsi que le développement de programmes et de projets dans différents secteurs. L’EIS, en tant qu’approche systématique dans l’évaluation des impacts sanitaires, représente ainsi une stratégie importante d’assistance à la promotion de la santé dans les pays en développement. Dans ce contexte, le manque de méthodologies d’EIS bien définies et applicables aux situations rencontrées dans les pays tropicaux a été identifié comme le facteur limitant d’une promotion plus globale de la démarche de l’EIS. Objectifs : Cette thèse de doctorat comporte 4 objectifs: (i) développer et améliorer les outils et les méthodes d’EIS qui s’adaptent facilement aux situations complexes rencontrées dans les pays à faible et moyen revenu; (ii) valider ces outils et ces méthodes dans le cadre de projets de développement industriel mis en place dans des pays tropicaux; (iii) systématiser les principales conclusions et discuter des leçons tirées afin que les outils et les méthodes deviennent disponibles aux futurs acteurs et utilisateurs de l’EIS; (iv) approfondir la compréhension des relations complexes entre les activités liées au développement de projet d’une part et les communautés concernées et leur environnement d’autre part. Cadre collaboratif : Ce projet de thèse a été réalisé dans le cadre d’une collaboration publique-privée entre l’Institut Tropical Suisse et de Santé Publique (Swiss TPH), à Bâle, et NewFields, une agence internationale de conseil comptant plusieurs années d’expertise dans l’EIS. Les collaborations de NewFields sur plusieurs sites d’EIS menées actuellement pour des clients privés ou publiques ont servi de plate-forme à cette recherche. Méthode : Les études de terrains de cette thèse de doctorat ont été réalisées dans le cadre d’onze EIS de différents projets de développement industriel dans un pays d’Amérique du Sud et cinq pays africains. Chacune de ces missions a tenu l'opportunité pour le développement, la validation et la consolidation des outils et méthodes en fonction de la phase correspondante dans le processus de l'EIS. Résultats : Cette recherche a mené au développement d’une trilogie d’EIS comportant trois parties, chacune d’entre elles basée sur une étude de cas. Elle introduit ensuite un panel de méthodologies s’appliquant aux EIS menées dans le cadre écologique et épidémiologique complexe des pays tropicaux. La première partie de ce travail utilise et développe le concept des zones de santé environnementale. Les communautés potentiellement affectées sont stratifiées et on fait appel à différentes méthodes, notamment participatives, dans le but d’obtenir des données sanitaires de base de qualité. L’intégration de ces données dans une matrice d’analyse de risques fournit un classement des impacts sanitaires potentiels, facilitant à terme la sélection des éléments prioritaires lors de la mise au point de stratégie de mitigation. Ces outils ont été développés dans le cadre de l’EIS d’un vaste projet de minerai de fer situé dans une région reculée de la République Démocratique du Congo. Cette recherche, exploitée en tant qu’étude de cas, a <b>abouti</b> à l’identification systématique de nombreux déterminants environnementaux et sanitaires. La seconde partie se concentre sur l'importance de la phase de cadrage, illustrée par des exemples spécifiques tirés d’une EIS menée actuellement dans le cadre d’un vaste projet de minerai de fer en République de Guinée. L’intégration de données obtenues par des démarches participatives et complétées par des données statistiques sanitaires locales à une banque de données analytique permet une sélection systématique des effets sanitaires déterminants. L'analyse d’écarts consécutive détermine, d’une part, la nécessité d'une récolte de données sanitaires de base supplémentaire dans la population concernée et facilite, d’autre part, l'identification d’indicateurs et de stratégies de récolte de données potentiels. Cette approche de cadrage plus rigoureuse est actuellement considérée comme un prérequis pour la planification et la mise en œuvre de toute étude sanitaire de base réalisée dans le contexte d’EIS menées dans un cadre complexe, notamment dans les pays en voie de développement. La dernière partie présente un design modulaire d’étude sanitaire transversale de base spécialement développé pour les EIS de projets industriels mis en place dans les pays en voie de développement. Le cadre modulaire s’adapte facilement aux caractéristiques écologiques et épidémiologiques d’un projet donné. Cette méthodologie se base sur une série d’indicateurs de performance comprenant différents impacts sanitaires ainsi que des déterminants à plusieurs niveaux. Les résultats d’une étude de base menée en République de Guinée, dans la région d’un projet de minerai de fer, illustre l’utilité et la valeur de la méthodologie développée dans ce chapitre. Cette étude démontre que l’évaluation quantitative des impacts sanitaires est une étape indispensable si l'on souhaite profiter pleinement du potentiel de l’EIS. En effet, cette approche permet non seulement de mieux comprendre comment les communautés sont affectées par de nouveaux projets mais aussi d’améliorer le potentiel de prédiction de l’EIS dans des régions où les données démographiques, écologiques, environnementales, épidémiologiques, sanitaires et socioéconomiques sont disperses. Conclusion : L’approche systématique d’EIS qui a évolué dans le cadre de cette thèse s’aligne parfaitement aux valeurs centrales de l’EIS – utilisation éthique des données probantes, démocratie, équité et développement durable – et apporte une contribution méthodologique remarquable à la science de l’EIS. En outre, cette méthodologie peut facilement être employée dans le cadre d'EIS de projets de développement de grande envergure dans les pays tropicaux, particulièrement depuis que nous avons pu montrer qu’elle s’applique largement à différents types de projets et d’environnements. Cependant,...|$|E
40|$|Summary Chloroethenes, {{and most}} {{particularly}} tetra- (PCE) and trichloroethene (TCE) are major groundwater pollutants {{due to their}} extensive industrial use as solvents since the 1920 s. The strong electronegativity of the chlorines renders them very stable under aerobic conditions. However, biodegradation of chloroethenes under anaerobic conditions {{has been shown to}} be a promising strategy for remediation of chloroethene-contaminated sites. To date around fifteen bacterial strains have been isolated with the property of using chloroethenes as terminal electron acceptor in a process called dehalorespiration. Anaerobic dehalorespiring bacteria show an unequal chloroethene substrate range and an unequal extent of dechlorination. While most of the dehalorespiring bacteria dechlorinate PCE and TCE to cis- 1, 2 -dichloroethene cis- 1, 2 -DCE) and belong to the phyla Firmicutes, δ- and ε-Proteobacteria, a few strains of the genus Dehalococcoides affiliated with the phylum Chloroflexi and are able to dechlorinate cis- 1, 2 -DCE and vinyl chloride (VC) to the non-toxic ethene. Dehalobacter restrictus and Dehalococcoides isolates were found to be completely restricted to dehalorespiration which gave rise to some basic evolutionary questions. Identification of the key enzyme in the dechlorination reaction, the reductive dehalogenase, has revealed a new class of enzymes containing a corrinoid and two ironsulfur clusters as cofactors. At the beginning of this thesis, nine chloroethene reductive dehalogenases have been characterized on biochemical level, while only little information was available on molecular level. Therefore, the overall goal of this thesis was to characterize on a molecular level the reductive dehalogenases involved in tetrachloroethene dehalorespiration and to get some indications on the evolution of this novel anaerobic respiration process. Starting from the N-terminal sequence of the PCE reductive dehalogenase (PceA) of Dehalobacter restrictus and from a conserved amino acid stretch found in two already sequenced reductive dehalogenases, a degenerate PCR approach allowed the isolation of the gene encoding PceA. Comparison with unpublished data from Desulfitobacterium sp. strain PCE-S showed 100 % sequence identity. The full sequence of the pceAB gene of strain PCE-S helped to isolate the corresponding gene cluster from D. restrictus and Desulfitobacterium hafniense strain TCE 1, which has also been shown to contain an identical N-terminal sequence. Sequence analysis confirmed the presence of a Twin-Arginine Translocation (Tat) signal peptide, which is involved in the incorporation of the reductive dehalogense into the cytoplasmic membrane. Detailed analysis of the iron-sulfur cluster binding motifs present in PceA of D. restrictus and the chlorophenol reductive dehalogenase (CprA) of Desulfitobacterium dehalogenans revealed differences in the second motif, which may explain results obtained by EPR spectroscopy, namely the presence of two [4 Fe- 4 S] clusters in the former enzyme and the presence of one [3 Fe- 4 S] and one [4 Fe- 4 S] cluster in the latter one. Structure breaking residues such as glycine and proline are present at the two extremities of the ten amino acid stretch separating the first and second ironbinding cysteine residues of the second motif in PceA, but not in CprA. This primary structure probably allows the formation of a loop in the tertiary structure and the participation of the first cyteine as a ligand in a [4 Fe- 4 S] cluster. In both new sequences, the presence of a short gene (pceB) encoding a hydrophobic protein with three conserved trans-membrane α-helices was confirmed, indicating a possible role in anchoring the catalytic unit of the reductive dehalogenase into the membrane. The complete sequence identity observed in the newly isolated reductive dehalogenases raised the question of a possible horizontal gene transfer between Dehalobacter restrictus and Desulfitobacterium hafniense strain TCE 1. Therefore, the flanking regions of the reductive dehalogenase genes (pceAB) in Desulfitobacterium hafniense strain TCE 1 and Dehalobacter restrictus were investigated. This study revealed the presence of a composite transposon (named Tn-Dha 1) in strain TCE 1 bordered with two identical insertion sequences (ISDha 1, including the transposase gene tnpA 1) and containing six open reading frames: the already characterized pceAB, two genes (pceCT) related to members of the o-chlorophenol reductive dehalogenase gene cluster of Desulfitobacterium dehalogenans, and two possibly truncated genes with homology to another transposase (tnpA 2) and to a subunit of the Tat machinery (tatA), respectively. In contrast, only the pceABCT gene cluster (i. e. without the transposon structure and the other two genes) was present in Dehalobacter restrictus, indicating that the genes encoding the key enzymes for the dechlorination activity are stably integrated into the genome. A detailed investigation of Tn-Dha 1 by PCR and Southern blot analysis indicated that Tn-Dha 1 may form various circular molecules, an indication for an active mobile genetic element. A model for the transposition of Tn-Dha 1 was proposed, in which the transposon may excise from the chromosome and circularize, forming an unstable structure with two abutted ISDha 1. The strong promoter formed by the junction of both IS would lead to high expression of the transposase, which in turn reacts with the circular element by either re-integrating it in the chromosome or excising one or both ISDha 1 from that element. The resulting structures would be single IS, IS tandems and circular molecules with one or no remaining IS, both latter structures being dead-end products of the transposition event. The hypothesis of mobile reductive dehalogenase genes was also investigated using a genomic approach in preliminary sequence data (released by The Institute for Genome Research, TIGR) of the genome of Dehalococcoides ethenogenes, a dehalorespiring bacterium capable to completely dechlorinate PCE to ethene. The genome was shown to contain the extraordinary number of eighteen different copies of reductive dehalogenase genes, including the well characterized tceA. A genomic signature of D. ethenogenes was obtained by calculating the frequency of 4 -letter DNA words along the genome and was graphically represented. Local disruptions of the genomic signature in certain segments of the genome were highlighted, corresponding to DNA, which may have been acquired by horizontal gene transfer, so-called original regions. It revealed that from the eighteen putative reductive dehalogenase genes present in the genome of D. ethenogenes, fifteen were located in original regions. Moreover, several genes encoding for recombinases (transposase, integrase) were found within these original regions, strongly indicating that these may have been acquired horizontally. The complete electron transport chain leading the electrons to the reductive dehalogenase has not yet been characterized for any of dehalorespiring bacteria and the direct electron-donor has not yet been elucidated for any of reductive dehalogenases. Therefore, the presence of cytochromes in cells of Desulfitobacterium hafniense strain TCE 1 was investigated with regard to the presence or absence of PCE in the growth medium. Detection of cytochromes using a sensitive detection method based on chemiluminescence revealed a strongly enhanced signal in the membrane fractions of strain TCE 1 cells grown on PCE instead of fumarate as terminal electron acceptor. Western blot analysis revealed the presence of a 45 kDa protein in membrane fraction, corresponding most probably to a c-type cytochrome. UV-visible spectroscopy confirmed the presence of c-type cytochromes in membrane fractions. This study, although further investigations are needed, indicated that a c-type cytochrome may be involved in the direct electron transfer to the PCE reductive dehalogenase of D. hafniense strain TCE 1. At present, numerous sequences of reductive dehalogenase genes have been reported and deposited on sequence databases, revealing the great interest shown for this new anaerobic respiration pathway. While several degenerate PCR approaches have led to the isolation of 22 mostly partial genes, analysis of preliminary genome sequence data from Dehalococcoides ethenogenes and Desulfitobacterium hafniense strain DCB- 2 has revealed 18 and 6 sequences, respectively. Sequence alignment and homology analysis of the 66 reductive dehalogenases genes available in August 2004 revealed four main clusters, two corresponding to chlorophenol and chloroethene reductive dehalogenases found in the phylum Firmicutes, one with sequences mostly isolated from ε-Proteobacteria, and one containing most of the genes isolated from the genus Dehalococcoides. Hence, the reductive dehalogenases appear to be rather conserved wihtin phylogenetic groups, indicating a relatively ancient enzyme class. Reductive dehalogenases show some features such as the presence of a Tat signal peptide and iron-sulfur clusters that are common to most of terminal reductases. However, the presence of a corrinoid at the catalytic center and of several specific conserved amino acid stretches makes them a new class of anaerobic reductases. Finally, the strong variation in the topology of the dehalorespiration chain and the variable presence and involvement of different electron transferring components such as quinones and cytochromes in dehalorespiring bacteria indicate that reductive dehalogenases may have been integrated into existing respiration chains rather than that dehalorespiration has evolved as a whole. Depuis 1920, les chloroéthènes, et plus particulièrement le tetra- (PCE) et le trichloroéthène (TCE) sont devenus des polluants majeurs dans les eaux souterraines de par leur utilisation intensive en tant que solvants dans l'industrie. La forte électronégativité des atomes de chlore leur confère une grande stabilité en conditions aérobies. Cependant, en conditions anaérobies, la biodégradation des chloroéthènes par des microorganismes s'est avérée être une technique de dépollution prometteuse pour favoriser leur élimination au sein des sites contaminés. A ce jour, une quinzaine de souches bactériennes ont été isolées avec la capacité d'utiliser les chloroéthènes comme accepteur final d'électrons au cours d'un processus appelé déhalorespiration. Alors que la plupart des souches isolées, appartenant aux phyla Firmicutes, Protéobactéries δ et ε, sont capables de déchlorurer le PCE et le TCE principalement jusqu'au cis- 1, 2 -dichloroéthène (cis- 1, 2 -DCE), quelques représentants du genre Dehalococcoides, affiliés au phylum Chloroflexi, peuvent en outre déchlorurer le cis- 1, 2 -DCE et le chlorure de vinyl (VC), rejetant ainsi de l'éthène, considéré comme non-toxique, dans l'environnement. Certaines bactéries, telles que Dehalobacter restrictus, ainsi que les membres du genre Dehalococcoides, sont même totalement dépendantes des chloroéthènes pour leur croissance, ce qui donna naissance à des questions d'ordre évolutif. L'identification des enzymes-clés de la réaction de déchloruration, appelées déhalogénases réductrices, a révélé une nouvelle classe d'enzymes contenant un corrinoïde et deux centres fersoufre comme cofacteurs. Neuf membres de cette nouvelle classe d'enzymes étaient caractérisés sur le plan biochimique au début de cette thèse, et très peu d'information était disponible au plan moléculaire. Ainsi le but principal de cette thèse était de caractériser sur le plan moléculaire les déhalogénases réductrices impliquées dans la déhalorespiration du tetrachloroéthène, et d'obtenir des indications concernant l'évolution de ce nouveau processus de respiration anaérobie. A partir de la séquence N-terminale de la PCE déhalogénase réductrice (PceA) de Dehalobacter restrictus, et d'une séquence de huit acides aminés conservée dans deux autres déhalogénases réductrices, une approche de type PCR dégénérée a permis l'isolement du gène pceA de D. restrictus. La comparaison de cette nouvelle séquence, encore partielle, avec une séquence similaire de Desulfitobacterium sp. PCE-S a révélé une identité proche de 100 %. Avec l'aide de cette dernière, la séquence totale des gènes pceAB a été isolée à partir de Dehalobacter restrictus, mais également de Desulfitobacterium hafniense TCE 1. L'analyse de cette nouvelle séquence confirma la présence d'un peptide signal de type Tat (pour Twin-Arginine Translocation), connu pour être impliqué dans l'incorporation de l'enzyme dans la membrane cytoplasmique de la bactérie. Une analyse détaillée des motifs de liaison des centres fer-soufre présents dans les séquences de la PCE déhalogénase réductrice de Dehalobacter restrictus et de la chlorophénol (CP) déhalogénase réductrice de Desulfitobacterium dehalogenans a mis à jour des différences structurales dans le deuxième motif. Ces différences corroborent les résultats obtenus par le passé au moyen de la spectroscopie RPE (résonance paramagnétique de l'électron). En effet, la présence de deux centres [4 Fe- 4 S] dans l'enzyme de D. restrictus et celle d'un centre [4 Fe- 4 S] et d'un centre [3 Fe- 4 S] dans l'enzyme de D. dehalogenans peut être expliquée par la présence dans la première enzyme, et non dans la dernière, de résidus glycine et proline aux extrémités du peptide séparant les résidus cystéine, responsables de lier les atomes de fer. Cette structure primaire de la chaîne polypeptidique permet très probablement la formation d'une boucle dans la structure tertiaire ainsi que la participation du premier résidu cystéine à la formation du deuxième centre [4 Fe- 4 S]. En outre, le gène pceB code pour une petite protéine à caractère hydrophobe,contenant trois hélices a structuralement conservées, indiquant un possible rôle dans l'ancrage dans la membrane de la sous-unité catalytique de la déhalogénase réductrice (PceA). L'identité parfaite de séquence des déhalogénases réductrices isolées à partir de Dehalobacter restrictus et de Desulfitobacterium hafniense TCE 1 a soulevé la question d'un possible transfert horizontal de gène entre ces deux souches. C'est pourquoi les régions voisines des gènes pceAB ont fait l'objet d'une nouvelle investigation. Cette étude a révélé la présence dans le génome de D. hafniense TCE 1 d'un transposon composite (appelé Tn-Dha 1), bordé par deux séquences d'insertion identiques (ISDha 1, incluant le gène pour la transposase, tnpA 1) et englobant six autres gènes : les gènes pceAB, préalablement caractérisés; deux gènes (pceCT) montrant une homologie avec des membres d'un groupe de gènes impliqués dans la synthèse de la CP déhalogénase réductrice de D. dehalogenans; et finalement deux autres gènes potentiellement tronqués avec une homologie envers une autre transposase (tnpA 2) et une sous-unité de la machinerie Tat (tatA), respectivement. Chez Dehalobacter restrictus, par contre, seul le groupe de gène pceABCT a été observé (c'est-à-dire sans la présence de la structure du transposon et des deux autres gènes), indiquant que les gènes responsables pour les enzymes-clés impliquées dans l'activité déchlorurante sont stables dans le génome de D. restrictus. Une étude détaillée de Tn-Dha 1 au moyen de la PCR et de l'analyse par Southern blot a montré que le transposon peut former différentes molécules circulaires, attestant de l'activité de cet élément génétique mobile. Sur la base des résultats obtenus ici, un modèle de transposition de Tn-Dha 1 a été proposé, selon lequel le transposon peut s'exciser du chromosome et former une molécule circulaire, contribuant ainsi à la formation d'une structure instable avec deux ISDha 1 côte à côte. En effet, le fort promoteur créé par la jonction des deux IS doit conduire à une surexpression de la transposase, qui à son tour réagit avec la structure circulaire soit en la réintégrant dans le chromosome, soit en y excisant un, voire les deux IS. Les structures résultant de ce phénomène seraient des IS isolées, un tandem d'IS, ainsi que des molécules circulaires contenant un ou aucun IS, ces dernières pouvant être considérées comme des déchets du processus de transposition. L'hypothèse selon laquelle les bactéries déhalorespirantes s'échangeraient les gènes-clés de la déhalorespiration par transfert de gène horizontal a également été testée au moyen d'une approche de type génomique. Les données issues du projet de séquençage du génome de Dehalococcoides ethenogenes (établie par « The Institute for Genome Research », TIGR), une bactérie capable de déchlorurer le PCE jusqu'à l'éthène et dont il a été montré qu'elle contenait jusqu'à 18 copies différentes de gènes codant potentiellement pour des déhalogénases réductrices, ont été analysées au moyen d'un outil bioinformatique permettant d'attribuer une signature génomique à l'ensemble de la séquence et d'en étudier les variations tout au long du génome. Pour ce faire, la fréquence des mots d'ADN de 4 lettres a été calculée au fil du génome et représentée graphiquement. Ainsi des perturbations locales de la signature génomique ont été mises en évidence dans certains segments du génome, appelés régions originales, correspondant à de l'ADN présumé avoir été acquis par transfert horizontal. Cette analyse révéla que des 18 copies de déhalogénases réductrices présentes dans le génome de D. ethenogenes, 15 d'entre elles sont localisées à l'intérieur de ces régions originales. De plus, de nombreux gènes codant pour des recombinases (transposase, integrase) ont été trouvés à l'intérieur de ces mêmes régions, corroborant l'hypothèse d'un transfert horizontal de gène. Pour aucune bactérie déhalorespirante, ni la chaîne de transport d'électrons menant les électrons à la déhalogénase réductrice, ni le donneur direct d'électrons pour aucune déhalogénase réductrice n'ont été elucidés. C'est pourquoi la présence de cytochromes au sein des cellules de Desulfitobacterium hafniense TCE 1 a été investiguée en fonction de la présence ou absence de PCE dans le milieu de culture. Ainsi la détection des cytochromes au moyen d'une méthode sensible basée sur la chémiluminescence a révélé un signal fortement amplifié dans la fraction membranaire de cellules cultivées sur PCE comme accepteur final d'électrons, en comparaison de la même fraction à partir de cellules cultivées sur acide fumarique. Une analyse de type Western blot a mis en évidence une protéine d'environ 45 kDa dans la fraction membranaire, correspondant très probablement à un cytochrome de type c. Une analyse des mêmes échantillons au moyen de la spectroscopie UV-visible a confirmé ces résultats. Bien que cette étude requiert d'autres analyses, une forte indication selon laquelle un cytochrome de type c serait impliqué dans le transfert direct d'électrons vers la PCE déhalogénase réductrice de D. hafniense TCE 1 a été apportée. Pendant la durée de cette thèse, de très nombreuses séquences de déhalogénases réductrices ont été déposées dans les banques de données, attestant de l'intérêt de la communauté scientifique pour cette nouvelle forme de respiration anaérobie. Ainsi plusieurs études basées sur une approche de PCR dégénérée ont <b>abouti</b> à l'identification de 22 gènes de déhalogénases réductrices, pour la plupart partiels. Parallèlement, l'analyse des séquences préliminaires des génomes de Dehalococcoides ethenogenes et de Desulfitobacterium hafniense DCB- 2 a révélé la présence de 18 et de 6 nouvelles séquences, respectivement. L'alignement de toutes ces séquences (66 au total), ainsi que l'analyse de leur homologie a révélé un classement en quatre groupes principaux, deux correspondant aux déhalogénases réductrices de type chlorophénols et chloroéthènes présentes au sein du phylum Firmicutes, un autre contenant les séquences isolées à partir des Protéobactéries de type ε, le dernier enfin correspondant à l'ensemble des gènes isolés du genre Dehalococcoides. Ainsi les déhalogénases réductrices semblent être conservées au sein des groupes phylogénétiques, formant une classe d'enzymes relativement ancienne. De plus, certaines caractéristiques présentes chez les déhalogénases réductrices, tels que le peptide signal de type Tat, ainsi que les centres fer-soufre, se retrouvent très fréquemment dans l'ensemble des réductases. Cependant, la présence d'un corrinoïde dans le site actif et la présence de plusieurs peptides spécifiques aux déhalogénases réductrices font de ces enzymes une classe à part. Finalement les fortes variations dans la topologie et la composition de la chaîne de transport d'électrons observées parmi les bactéries déhalorespirantes indiqueraient que les déhalogénases réductrices ont très probablement été intégrées à des chaînes de respiration existantes, et non pas que le processus de déhalorespiration a évolué en tant qu'entité distincte...|$|E

