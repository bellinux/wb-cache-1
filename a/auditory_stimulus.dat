789|2557|Public
25|$|Brainstem {{auditory}} evoked potentials {{are small}} AEPs that are recorded {{in response to an}} <b>auditory</b> <b>stimulus</b> from electrodes placed on the scalp.|$|E
25|$|The McGurk {{effect can}} be {{observed}} when the listener is also the speaker or articulator. While looking at oneself {{in the mirror and}} articulating visual stimuli while listening to another <b>auditory</b> <b>stimulus,</b> a strong McGurk {{effect can be}} observed. In the other condition, where the listener speaks auditory stimuli softly while watching another person articulate the conflicting visual gestures, a McGurk effect can still be seen, although it is weaker.|$|E
25|$|Auditory {{hallucinations}} (also {{known as}} paracusia) are {{the perception of}} sound without outside stimulus. Auditory hallucinations {{are the most common}} type of hallucination. Auditory hallucinations can be divided into two categories: elementary and complex. Elementary hallucinations are the perception of sounds such as hissing, whistling, an extended tone, and more. In many cases, tinnitus is an elementary auditory hallucination. However, some people who experience certain types of tinnitus, especially pulsatile tinnitus, are actually hearing the blood rushing through vessels near the ear. Because the <b>auditory</b> <b>stimulus</b> is present in this situation, it does not qualify as a hallucination.|$|E
50|$|From the {{cognitive}} perspective, the brain perceives <b>auditory</b> <b>stimuli</b> as music according to gestalt principles, or “principles of grouping.” Gestalt principles include proximity, similarity, closure, and continuation. Each of the gestalt principles illustrates a different element of <b>auditory</b> <b>stimuli</b> that {{cause them to}} be perceived as a group, or as one unit of music. Proximity dictates that <b>auditory</b> <b>stimuli</b> that are near to each other are seen as a group. Similarity dictates that when multiple <b>auditory</b> <b>stimuli</b> are present, the similar stimuli are perceived as a group. Closure is the tendency to perceive an incomplete auditory pattern as a whole—the brain “fills in” the gap. And continuation dictates that <b>auditory</b> <b>stimuli</b> are more likely {{to be perceived as}} a group when they follow a continuous, detectable pattern.|$|R
40|$|In {{face-to-face}} conversation {{speech is}} perceived by ear and eye. We studied the prerequisites of audio-visual speech perception by using perceptually ambiguous sine wave replicas of natural speech as <b>auditory</b> <b>stimuli.</b> When {{the subjects were}} not aware that the <b>auditory</b> <b>stimuli</b> were speech, they showed only negligible integration of <b>auditory</b> and visual <b>stimuli.</b> When the same subjects learned to perceive the same <b>auditory</b> <b>stimuli</b> as speech, they integrated the <b>auditory</b> and visual <b>stimuli</b> {{in a similar manner}} as natural speech. These results demonstrate the existence of a multisensory speech-specific mode of perception...|$|R
40|$|This article aims to {{investigate}} whether <b>auditory</b> <b>stimuli</b> in the horizontal plane, particularly originating from behind the participant, affect audiovisual integration by using behavioral and event-related potential (ERP) measurements. In this study, visual stimuli were presented {{directly in front of}} the participants, <b>auditory</b> <b>stimuli</b> were presented at one location in an equidistant horizontal plane at the front (0 °, the fixation point), right (90 °), back (180 °), or left (270 °) of the participants, and audiovisual stimuli that include both visual <b>stimuli</b> and <b>auditory</b> <b>stimuli</b> originating from one of the four locations were simultaneously presented. These stimuli were presented randomly with equal probability; during this time, participants were asked to attend to the visual stimulus and respond promptly only to visual target stimuli (a unimodal visual target stimulus and the visual target of the audiovisual stimulus). A significant facilitation of reaction times and hit rates was obtained following audiovisual stimulation, irrespective of whether the <b>auditory</b> <b>stimuli</b> were presented in the front or back of the participant. However, no significant interactions were found between visual <b>stimuli</b> and <b>auditory</b> <b>stimuli</b> from the right or left. Two main ERP components related to audiovisual integration were found: first, <b>auditory</b> <b>stimuli</b> from the front location produced an ERP reaction over the right temporal area and right occipital area at approximately 160 - 200 milliseconds; second, <b>auditory</b> <b>stimuli</b> from the back produced a reaction over the parietal and occipital areas at approximately 360 - 400 milliseconds. Our results confirmed that audiovisual integration was also elicited, even though <b>auditory</b> <b>stimuli</b> were presented behind the participant, but no integration occurred when <b>auditory</b> <b>stimuli</b> were presented in the right or left spaces, suggesting that the human brain might be particularly sensitive to information received from behind than both sides...|$|R
25|$|The neurobehavioral {{assessment}} battery {{that was}} used in the Van Dongen et al. study included the psychomotor vigilance task (PVT). The PVT - which determines alertness and the effects of fatigue on cognitive performance (as determined by lapses in response time and accuracy of responses) by measuring the speed with which subjects respond to a visual or <b>auditory</b> <b>stimulus</b> (by pressing a response button) - has become a standard laboratory tool for the assessment of sustained performance in a variety of experimental conditions. The PVT detects changes in basic neurobehavioral performance that involve vigilant attention, response speed, and impulsivity; and it has been extensively validated in ground-based laboratory studies to detect cognitive deficits that are caused by a variety of factors (e.g., restricted sleep, sleep/wake shifts, motion sickness, residual sedation from sleep medications). The PVT is an optimal tool for repeated use, in contrast to some other cognitive measures, as studies have shown no minimal learning effects and aptitude differences when using the PVT.|$|E
25|$|The double flash {{illusion}} {{was reported}} {{as the first}} illusion to show that visual stimuli can be qualitatively altered by audio stimuli. In the standard paradigm participants are presented combinations of one to four flashes accompanied by zero to 4 beeps. They {{were then asked to}} say how many flashes they perceived. Participants perceived illusory flashes when there were more beeps than flashes. fMRI studies have shown that there is crossmodal activation in early, low level visual areas, which was qualitatively similar to the perception of a real flash. This suggests that the illusion reflects subjective perception of the extra flash. Further, studies suggest that timing of multisensory activation in unisensory cortexes is too fast to be mediated by a higher order integration suggesting feed forward or lateral connections. One study has revealed the same effect but from vision to audition, as well as fission rather than fusion effects, although the level of the <b>auditory</b> <b>stimulus</b> was reduced to make it less salient for those illusions affecting audition.|$|E
25|$|Neural {{structures}} {{form and}} become more sophisticated {{as a result of}} experience. For example, the preference for consonance, the harmony or agreement of components, over dissonance, an unstable tone combination, is found early in development. Research suggests that this is due to both the experiencing of structured sounds and the fact they stem from development of the basilar membrane and auditory nerve, two early developing structures in the brain. An incoming <b>auditory</b> <b>stimulus</b> evokes responses measured {{in the form of an}} Event-related potential (ERP), measured brain responses resulting directly from a thought or perception. There is a difference in ERP measures for normally developing infants ranging from 2–6 months in age. Measures in infants 4 months and older demonstrate faster, more negative ERPs. In contrast, newborns and infants up to 4 months of age show slow, unsynchronized, positive ERPs. Trainor, et al. (2003) hypothesized that these results indicated that responses from infants less than four months of age are produced by subcortical auditory structures, whereas with older infants responses tend to originate in the higher cortical structures.|$|E
40|$|Restricted until 18 May 2010. This study {{investigated}} the relationship between autonomic reactivity and behavioral responses to auditory stimulation in 22 high-functioning children with autism (AD) and twenty typically-developing children (TD). A primary purpose was to examine whether the AD and TD groups differed with respect to autonomic activity at rest and following auditory stimulation. An additional purpose was to investigate whether the severity of behavioral difficulties with <b>auditory</b> <b>stimuli</b> in everyday life, as reported by parents, was associated with electrodermal responses to <b>auditory</b> <b>stimuli</b> presented in a controlled laboratory setting. Electrodermal activity (EDA) was measured at rest {{and in response to}} two <b>auditory</b> <b>stimuli,</b> a tone and a siren, using the Sensory Challenge Protocol. Behavioral difficulties were measured with the Sensory Processing Measure (SPM) Home Form. T-tests were applied to EDA variables to detect differences between the two study groups. Confounding effects of gender, age, and ethnicity on EDA findings were also analyzed. Associations between EDA measures and SPM scores were determined using Pearson correlation procedures.; Results showed that the AD group had higher resting EDA levels and stronger EDA reactivity to <b>auditory</b> <b>stimuli</b> than the TD group. These findings suggest that the children with autism generally had higher arousal levels and were more physiologically reactive to <b>auditory</b> <b>stimuli</b> than the typically-developing children. Parent responses on the SPM Home Form showed that 90 % of the participants with autism had behavioral difficulties with <b>auditory</b> <b>stimuli</b> in naturally occurring situations. Correlations between EDA and SPM measures indicated that more severe behavioral difficulties with <b>auditory</b> <b>stimuli</b> were associated with higher arousal levels and stronger physiological reactions to <b>auditory</b> <b>stimuli.</b> Overall, results suggested that high arousal levels may underlie some behavioral problems that children with autism experience in reaction to <b>auditory</b> <b>stimuli</b> in natural environments...|$|R
40|$|We {{investigated}} {{the effects of}} <b>auditory</b> <b>stimuli</b> on the perceived velocity of a moving visual stimulus. Previous studies have reported that the duration of visual events is perceived as being longer for events filled with <b>auditory</b> <b>stimuli</b> than for events not filled with <b>auditory</b> <b>stimuli,</b> ie, the so-called “filled-duration illusion. ” In this study, we have shown that <b>auditory</b> <b>stimuli</b> also affect the perceived velocity of a moving visual stimulus. In Experiment 1, a moving comparison stimulus (4. 2 ∼ 5. 8 deg/s) was presented together with filled (or unfilled) white-noise bursts or with no sound. The standard stimulus was a moving visual stimulus (5 deg/s) presented {{before or after the}} comparison stimulus. The participants had to judge which stimulus was moving faster. The results showed that the perceived velocity in the auditory-filled condition was lower than that in the auditory-unfilled and no-sound conditions. In Experiment 2, we {{investigated the}} effects of <b>auditory</b> <b>stimuli</b> on velocity adaptation. The results showed that the effects of velocity adaptation in the auditory-filled condition were weaker than those in the no-sound condition. These results indicate that <b>auditory</b> <b>stimuli</b> tend to decrease the perceived velocity of a moving visual stimulus...|$|R
40|$|People {{obtain a}} lot of {{information}} from visual and auditory sensation on daily life. Regarding the effect of visual stimuli on perception of <b>auditory</b> <b>stimuli,</b> studies of phonological perception and sound localization have been made in great numbers. This study examined the effect of visual stimuli on perception in loudness and pitch of <b>auditory</b> <b>stimuli.</b> We used the image of figures whose size or brightness was changed as visual stimuli, and the sound of pure tone whose loudness or pitch was changed as <b>auditory</b> <b>stimuli.</b> Those visual and <b>auditory</b> <b>stimuli</b> were combined independently to make four types of audio-visual multisensory stimuli for psychophysical experiments. In the experiments, participants judged change in loudness or pitch of <b>auditory</b> <b>stimuli,</b> while they judged the direction of size change or the kind of a presented figure in visual stimuli. Therefore they cannot neglect visual stimuli while they judged <b>auditory</b> <b>stimuli.</b> As a result, perception in loudness and pitch were promoted significantly around their difference limen, when the image was getting bigger or brighter, compared with the case in which the image had no changes. This indicates that perception in loudness and pitch were affected by change in size and brightness of visual stimuli...|$|R
25|$|Test: Finally, infants {{are tested}} in the lab using the Headturn-preference procedure, a {{behavioral}} data-collection tool that measures preferences for one kind of <b>auditory</b> <b>stimulus</b> over another. The Headturn-preference procedure maintains that an infant will turn its head towards a stimulus it prefers. This procedure is conducted in a testing booth, with the infant sitting on the lap {{of his or her}} mother. A light is located {{on either side of the}} infant. The trial begins when the infant is looking straight ahead. Mother and experimenter are required to wear tight-fitting earphones which deliver masking music for the duration of the entire procedure. This is done to guarantee that neither mother, nor experimenter bias the infant's response. During each trial, one sidelight flashes, urging the infant to look at it. Once the infant turns his or her head and looks at the light, the sound stimulus is played. The stimulus continues to play until the sound finishes or the infant looks away. When the infant turns away from the source for at least two seconds, sound and light turn off and the trial ends. A new trial begins when the infant looks at the center panel again.|$|E
25|$|Supra{{threshold}} tests, {{which provide}} intensities of taste stimuli above threshold levels, {{are used to}} assess the patient's ability to differentiate between different intensities of taste and to estimate the magnitude of suprathreshold loss of taste. From these tests, ratings of pleasantness can be obtained using either the direct scaling or magnitude matching method and may be of value in the diagnosis of dysgeusia. Direct scaling tests show the ability to discriminate among different intensities of stimuli and whether a stimulus of one quality (sweet) is stronger or weaker than a stimulus of another quality (sour). Direct scaling cannot be used to determine if a taste stimulus is being perceived at abnormal levels. In this case, magnitude matching is used, in which a patient is asked to rate the intensities of taste stimuli and stimuli of another sensory system, such as the loudness of a tone, on a similar scale. For example, the Connecticut Chemosensory Clinical Research Center asks patients to rate the intensities of NaCl, sucrose, citric acid and quinine-HCl stimuli, and the loudness of 1000Hz tones. Assuming normal hearing, the results of this cross-sensory test show the relative strength of the sense of taste in relation to the loudness of the <b>auditory</b> <b>stimulus.</b> Although many of the tests are based on ratings using the direct scaling method, some tests do use the magnitude-matching procedure.|$|E
25|$|People of all {{languages}} rely to {{some extent}} on visual information in speech perception, but {{the intensity of the}} McGurk effect can change between languages. Dutch, English, Spanish, German and Italian language listeners experience a robust McGurk effect, while it is weaker for Japanese and Chinese listeners. Most research on the McGurk effect between languages has been conducted between English and Japanese. There is a smaller McGurk effect in Japanese listeners than in English listeners. The cultural practice of face avoidance in Japanese people may {{have an effect on the}} McGurk effect, as well as tone and syllabic structures of the language. This could also be why Chinese listeners are less susceptible to visual cues, and similar to Japanese, produce a smaller effect than English listeners. Studies have also shown that Japanese listeners do not show a developmental increase in visual influence after the age of six, as English children do. Japanese listeners are more able to identify an incompatibility between the visual and <b>auditory</b> <b>stimulus</b> than English listeners are. This result could be in relation to the fact that in Japanese, consonant clusters do not exist. In noisy environments where speech is unintelligible, however, people of all languages resort to using visual stimuli and are then equally subject to the McGurk effect. The McGurk effect works with speech perceivers of every language for which it has been tested.|$|E
25|$|Temporal {{synchrony}} is {{not necessary}} for the McGurk effect to be present. Subjects are still strongly influenced by <b>auditory</b> <b>stimuli</b> even when it lagged the visual stimuli by 180 milliseconds (point at which McGurk effect begins to weaken). There was less tolerance {{for the lack of}} synchrony if the <b>auditory</b> <b>stimuli</b> preceded the visual stimuli. In order to produce a significant weakening of the McGurk effect, the <b>auditory</b> <b>stimuli</b> had to precede the visual stimuli by 60 milliseconds, or lag by 240 milliseconds.|$|R
40|$|Andersen. Responses to <b>auditory</b> <b>stimuli</b> in macaque lateral intrapa-rietal area. II. Behavioral modulation. J. Neurophysiol. 82 : 343 – 358, 1999. The lateral intraparietal area (LIP), {{a region}} of {{posterior}} parietal cortex, was {{once thought to be}} unresponsive to auditory stimulation. However, recent reports have indicated that neurons in area LIP respond to <b>auditory</b> <b>stimuli</b> during an auditory-saccade task. To what extent are auditory responses in area LIP dependent on the perfor-mance of an auditory-saccade task? To address this question, record-ings were made from 160 LIP neurons in two monkeys while the animals performed auditory and visual memory-saccade and fixation tasks. Responses to <b>auditory</b> <b>stimuli</b> were significantly stronger during the memory-saccade task than during the fixation task, whereas re-sponses to visual stimuli were not. Moreover, neurons responsive to <b>auditory</b> <b>stimuli</b> tended also to be visually responsive and to exhibit delay or saccade activity in the memory-saccade task. These results indicate that, in general, auditory responses in area LIP are modulated by behavioral context, are associated with visual responses, and are predictive of delay or saccade activity. Responses to <b>auditory</b> <b>stimuli</b> in area LIP may therefore be best interpreted as supramodal responses, and similar in nature to the delay activity, rather than as modality-specific sensory responses. The apparent link between auditory activ-ity and oculomotor behavior suggests that the behavioral modulation of responses to <b>auditory</b> <b>stimuli</b> in area LIP reflects the selection of <b>auditory</b> <b>stimuli</b> as targets for eye movements...|$|R
40|$|The {{effects of}} <b>auditory</b> <b>stimuli</b> {{in the form}} of {{synthetic}} speech output on the learning of graphic symbols were evaluated. Three adults with severe to profound mental retardation and communication impairments were taught to point to lexigrams when presented with words under two conditions. In the first condition, participants used a voice output communication aid to receive synthetic speech as antecedent and consequent stimuli. In the second condition, with a nonelectronic communications board, participants did not receive synthetic speech. A parallel treatments design was used to evaluate the effects of the synthetic speech output as an added component of the augmentative and alternative communication system. The 3 participants reached criterion when not provided with the <b>auditory</b> <b>stimuli.</b> Although 2 participants also reached criterion when not provided with the <b>auditory</b> <b>stimuli,</b> the addition of <b>auditory</b> <b>stimuli</b> resulted in more efficient learning and a decreased error rate. Maintenance results, however, indicated no differences between conditions. Finding suggest that <b>auditory</b> <b>stimuli</b> {{in the form of}} synthetic speech contribute to the efficient acquisition of graphic communication symbols...|$|R
2500|$|Music Therapy: Music {{has been}} shown to aid {{children}} with 1p36 deletion in various developmental areas. [...] It serves as an excellent <b>auditory</b> <b>stimulus</b> and can teach listening skills. [...] Songs with actions help the child to develop coordination and motor skills.|$|E
2500|$|Auditory evoked {{potentials}} (AEPs) are a subclass of event-related potentials (ERP)s. ERPs are brain {{responses that}} are time-locked to some [...] "event", {{such as a}} sensory stimulus, a mental event (such as recognition of a target stimulus), or the omission of a stimulus. For AEPs, the [...] "event" [...] is a sound. AEPs (and ERPs) are very small electrical voltage potentials originating from the brain recorded from the scalp {{in response to an}} <b>auditory</b> <b>stimulus,</b> such as different tones, speech sounds, etc.|$|E
2500|$|The primary two {{stimulus}} modalities {{used for}} negative priming research are visual and <b>auditory</b> <b>stimulus</b> materials. The stimulus presented varied from objects or symbols in visual field to human voices or artificial sounds. Stronger negative priming effects are found for <b>auditory</b> <b>stimulus</b> but the standardized effect sizes between the modalities did not vary. Evidence for negative priming {{has also been}} found across various modes of response including vocal naming, manual key press, and reaching. Negative priming was observed for various types of judgment such as identification, categorization, matching, counting and localization. The tasks used to find evidence for negative priming includes Stroop color–word task, lexical decision task, identification, matching, and localization tasks. The Stroop color–word task utilizes the Stroop effect to observe the distractor suppression and negative priming. Identification tasks present a set of [...] images, sounds, words, symbols, or letters and require the subject to select the prime target based a particular feature that differentiates the target from the distractor. Lexical decision utilizes semantic knowledge of the subject and tests the subject ability to remember the multiple meanings and uses of one word. For example, the word [...] "bank" [...] has multiple meanings and can be referred in different contexts such as [...] "bank {{is a place where}} money is deposited" [...] or [...] "banks of a river". Matching tasks require subjects to respond [...] "same" [...] or [...] "different" [...] by matching the target letters or shapes with the explicitly specified goal while ignoring the distractor. Localization tasks require some form of movement of subjects to respond to the location of the target stimulus. This type of localization task is especially used to test the feature mismatch hypothesis as it provides evidence for negative priming during the mismatch of the location and target stimuli.|$|E
40|$| the results, the <b>auditory</b> <b>stimuli</b> unassociated {{with the}} target locations|$|R
5000|$|<b>Auditory</b> <b>stimuli</b> {{may appear}} to last longer than visual stimuli ...|$|R
5000|$|The {{temporal}} lobes {{are involved}} in several functions of the body including: hearing, meaning, <b>auditory</b> <b>stimuli,</b> memory, and speech. They {{also play a role}} in emotion and learning [...] and are concerned with processing and interpreting <b>auditory</b> <b>stimuli.</b> This is a major location for memory storage and is associated with memory skills.|$|R
2500|$|The second paradigm, habituation, {{is one of}} {{the most}} {{successful}} ways of investigating fetal memory. Habituation has been demonstrated in fetuses as early as 22 weeks and corresponds to the onset of fetal auditory abilities. Both auditory and vibroacoustic stimulation have been used in habituation. Vibroacoustic stimulation is a technique involving the repetitive stimulation of the fetus, by applying a vibroacoustic stimulus (in predetermined intervals) to the abdomen of the mother. The movement and reaction of the fetus, in response to the stimulus, is recorded using ultrasound technology. This process is repeated until habituation, defined as a lack of response to the stimulus by the fetus, is reached. Stimulation trials continue into the neonatal period (first 28 days after birth) by presenting the same <b>auditory</b> <b>stimulus,</b> to test whether or not the fetus has memory of the stimulation events. A scientific control group of babies in the neonatal period, having not been exposed to the stimulus as a fetus, are used in neonatal trials to serve as a comparison.|$|E
50|$|Due {{to their}} small amplitude, 500 or more repetitions of the <b>auditory</b> <b>stimulus</b> are {{required}} {{in order to}} average out the random background electrical activity. Although it is possible to obtain a BAEP to a pure tone stimulus in the hearing range a more effective <b>auditory</b> <b>stimulus</b> contains a range of frequencies {{in the form of a}} short sharp click.|$|E
50|$|An RCT {{comparing}} the short-term effects of bright light, an <b>auditory</b> <b>stimulus,</b> and high- and low-density negative ions on mood and alertness in mildly depressed and non-depressed adults {{found that the}} three first (active) stimuli, but not the low-density placebo, reduced depression on the Beck Depression Inventory scale; the <b>auditory</b> <b>stimulus,</b> bright light and high-density ions all produced rapid mood changes - with small to medium effect sizes - in depressed and non-depressed subjects.|$|E
40|$|The visual motion aftereffect is {{a visual}} {{illusion}} in which exposure to continuous motion {{in one direction}} leads to a subsequent illusion of visual motion in the opposite direction. Previous findings have been mixed with regard to whether this visual illusion can be induced cross-modally by <b>auditory</b> <b>stimuli.</b> Based on research on multisensory perception demonstrating the profound influence auditory perception can have on the interpretation and perceived motion of visual stimuli, we hypothesized that exposure to <b>auditory</b> <b>stimuli</b> with strong directional motion cues should induce a visual motion aftereffect. Here, we demonstrate that horizontally moving <b>auditory</b> <b>stimuli</b> induced a significant visual motion aftereffect—an effect that was driven primarily by a change in visual motion perception following exposure to leftward moving <b>auditory</b> <b>stimuli.</b> This {{finding is consistent with}} the notion that visual and auditory motion perception rely on at least partially overlapping neural substrates...|$|R
40|$|The lateral intraparietal area (LIP) of macaque {{posterior}} {{parietal cortex}} {{participates in the}} sensorimotor transformations underlying visually guided eye movements. Area LIP has long been considered unresponsive to auditory stimulation. However, {{recent studies have shown}} that neurons in LIP respond to <b>auditory</b> <b>stimuli</b> during an auditory-saccade task, suggesting possible involvement of this area in auditory-to-oculomotor as well as visual-to-oculomotor processing. This dissertation describes investigations which clarify the role of area LIP in auditory-to-oculomotor processing. Extracellular recordings were obtained from a total of 332 LIP neurons in two macaque monkeys, while the animals performed fixation and saccade tasks involving <b>auditory</b> and visual <b>stimuli.</b> No <b>auditory</b> activity was observed in area LIP before animals were trained to make saccades to <b>auditory</b> <b>stimuli,</b> but responses to <b>auditory</b> <b>stimuli</b> did emerge after auditory-saccade training. Auditory responses in area LIP after auditory-saccade training were significantly stronger {{in the context of an}} auditory-saccade task than in the context of a fixation task. Compared to visual responses, auditory responses were also significantly more predictive of movement-related activity in the saccade task. Moreover, while visual responses often had a fast transient component, responses to <b>auditory</b> <b>stimuli</b> in area LIP tended to be gradual in onset and relatively prolonged in duration. Overall, the analyses demonstrate that responses to <b>auditory</b> <b>stimuli</b> in area LIP are dependent on auditory-saccade training, modulated by behavioral context, and characterized by slow-onset, sustained response profiles. These findings suggest that responses to <b>auditory</b> <b>stimuli</b> are best interpreted as supramodal (cognitive or motor) responses, rather than as modality-specific sensory responses. Auditory responses in area LIP seem to reflect the significance of <b>auditory</b> <b>stimuli</b> as potential targets for eye movements, and may differ from most visual responses in the extent to which they arc abstracted from the sensory parameters of the stimulus. ...|$|R
40|$| {{depended}} {{on the degree of}} association between <b>auditory</b> <b>stimuli</b> and target|$|R
50|$|Brainstem {{auditory}} evoked potentials {{are small}} AEPs that are recorded {{in response to an}} <b>auditory</b> <b>stimulus</b> from electrodes placed on the scalp.|$|E
50|$|The N100 is 10 to 20% {{larger than}} normal when the <b>auditory</b> <b>stimulus</b> is {{synchronized}} with the diastolic {{phase of the}} cardiac blood pressure pulse.|$|E
50|$|Deaf hearing {{refers to}} a {{condition}} in which a deaf individuals are able to react to an <b>auditory</b> <b>stimulus,</b> without actually being able to hear it.|$|E
40|$|On {{the basis}} of {{evidence}} from six areas of speech research, {{it is argued that}} {{there is no reason to}} assume that speech stimuli are processed by structures that are inherently different from structures used for other <b>auditory</b> <b>stimuli.</b> It is concluded that speech and non-speech <b>auditory</b> <b>stimuli</b> are probably perceived in the same way...|$|R
40|$|Purpose] Auditory {{hypersensitivity}} {{has been}} widely reported in patients with autism spectrum disorders. However, the neurological background of auditory hypersensitivity is currently not clear. The present study {{examined the relationship between}} sympathetic nervous system responses and auditory hypersensitivity induced by different types of <b>auditory</b> <b>stimuli.</b> [Methods] We exposed 20 healthy young adults to six different types of <b>auditory</b> <b>stimuli.</b> The amounts of palmar sweating resulting from the <b>auditory</b> <b>stimuli</b> were compared between groups with (hypersensitive) and without (non-hypersensitive) auditory hypersensitivity. [Results] Although no group × type of stimulus × first stimulus interaction was observed for the extent of reaction, significant type of stimulus × first stimulus interaction was noted for the extent of reaction. For an 80 dB- 6, 000 Hz stimulus, the trends for palmar sweating differed between the groups. For the first stimulus, the variance became larger in the hypersensitive group than in the non-hypersensitive group. [Conclusion] Subjects who regularly felt excessive reactions to <b>auditory</b> <b>stimuli</b> tended to have excessive sympathetic responses to repeated loud noises compared with subjects who did not feel excessive reactions. People with auditory hypersensitivity may be classified into several subtypes depending on their reaction patterns to <b>auditory</b> <b>stimuli...</b>|$|R
40|$| between <b>auditory</b> <b>stimuli</b> and the {{possible}} locations of a visual target. In|$|R
