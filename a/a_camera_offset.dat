1|10000|Public
30|$|Structured light {{projection}} Another {{technique that}} can simultaneously capture color and topography is the structured light projection technique [10],[12],[14]. A common setup {{consists of a}} projector and <b>a</b> <b>camera</b> <b>offset</b> by a certain distance and aimed {{at the scene of}} interest. A structured light of known structure is then projected on the scene and captured by a camera. The camera can then compute the path of the light coming out of the projector, hitting the surface and then entering the camera. This triangulation allows for computing the topography while also capturing the color, if the projector’s illuminant is neutral. Their main drawbacks are that they are limited to the resolution of the projector and problems due to specular reflections. The constraints set by the projector’s resolution can be circumvented by using the fringe projection technique. In order to make the quantization or pixel pattern invisible, the projector’s projection can be blurred by defocusing the lens, resulting in a smooth fringe pattern (convolution theorem). The resolution of the capture is now constrained by that of the camera, and so we can employ cameras with a large pixel count and capture a large quantity of data per capture. Such large captures both increase speed and reduce the need for image registration (data stitching). Multiple cameras can be used to observe the projected pattern from different angles. Such arrangements often employ the same algorithm for each individual camera without much synergy, and so are only used to reduce occlusions and increase accuracy. One common problem with fringe projection {{is the fact that the}} its intensity pattern has to be exactly sinusoidal. This is difficult due to the non-linearity in the illumination of the projector and the sensor of the camera. Not only do these have to be accurately calibrated and accounted for; both the projector and the camera need to be geometrically and optically calibrated. The projector calibration can be achieved by projecting patterns, however, this projection is then again limited to the resolution of the projector, making precise calibration difficult. Instead of using structured light to do projector-to-camera light-ray calculations, we can also use the projection to encode the surface, uniquely labeling each point on the surface [15]. This unique label then has to be observed by multiple cameras, immediately solving the correspondence problem we encounter in stereo vision. Having solved this, camera-to-camera triangulation can be performed as is done in a stereo vision setup. Fringe projection can be used for this labeling as our system is then not limited by the projector’s resolution. Each point is then labeled with phase values. However, since these fringes were repetitive, the cameras do not yet know which fringe in the left image belongs to the fringe observed by the right image. In other words, there exists an offset between the phase value observed in the left and right phase images. This offset can be calculated if we compare phase values of at least one part of both images that we know that correspond. This can be done with the common stereo-vision approach, where we first search for keypoints in both color images, and then find corresponding matches. The offset of phase value observed from both cameras can now be nullified as we know the phase value should be the same for each point in the scene. This fringe projection aided stereo vision approach is therefore our selected method, since it can be both highly efficient and accurate.|$|E
40|$|This work {{analyzes}} {{the ability to}} estimate and control the relative position and velocity of a Small Satellite {{with respect to a}} target vehicle using <b>a</b> single optical <b>camera.</b> Although the target range is generally unobservable when using anglesonly measurements, relative position/velocity observability can be achieved when the SmallSat is slowly rotating and the <b>camera</b> is <b>offset</b> from the center of gravity. The sensitivity of the navigation errors and trajectory dispersions to several simulation parameters is discussed, including SmallSat <b>camera</b> <b>offset,</b> spin rate, and range to target. Also included in the analysis is the effect of common sensor errors (e. g. camera and gyro bias/noise), external disturbances, and initial conditions. Future efforts are mentioned to extend the analysis to cooperative/uncooperative targets and to increase analysis efficiency through Linear Covariance analysis...|$|R
40|$|Kinematics, {{the study}} of motion {{exclusive}} of the influences of mass and force, {{is one of the}} primary methods used for the analysis of human biomechanical systems as well as other types of mechanical systems. The Anthropometry and Biomechanics Laboratory (ABL) in the Crew Interface Analysis section of the Man-Systems Division performs both human body kinematics as well as mechanical system kinematics using the Ariel Performance Analysis System (APAS). The APAS supports both analysis of analog signals (e. g. force plate data collection) as well as digitization and analysis of video data. The current evaluations address several methodology issues concerning the accuracy of the kinematic data collection and analysis used in the ABL. This document describes a series of evaluations performed to gain quantitative data pertaining to position and constant angular velocity movements under several operating conditions. Two-dimensional as well as three-dimensional data collection and analyses were completed in a controlled laboratory environment using typical hardware setups. In addition, an evaluation was performed to evaluate the accuracy impact due to <b>a</b> single axis <b>camera</b> <b>offset.</b> Segment length and positional data exhibited errors within 3 percent when using three-dimensional analysis and yielded errors within 8 percent through two-dimensional analysis (Direct Linear Software). Peak angular velocities displayed errors within 6 percent through three-dimensional analyses and exhibited errors of 12 percent when using two-dimensional analysis (Direct Linear Software). The specific results from this series of evaluations and their impacts on the methodology issues of kinematic data collection and analyses are presented in detail. The accuracy levels observed in these evaluations are also presented...|$|R
40|$|In this paper, {{we propose}} an {{algorithm}} for estimating dense depth information of dynamic scenes from multiple video streams captured using unsynchronized stationary cameras. We {{solve this problem}} by first imposing two assumptions about the scene motion and the temporal <b>offset</b> between <b>cameras.</b> The motion of a scene is described using a local constant velocity model and the <b>camera</b> temporal <b>offset</b> {{is assumed to be}} constant within a short of period of time. Based on these models, geometric relations between the images of moving scene points, the scene depth, the scene motions, and the <b>camera</b> temporal <b>offset</b> are investigated and an estimation method is developed to compute the <b>camera</b> temporal <b>offset.</b> The three main steps of the proposed algorithm are 1) the estimation of the temporal <b>offset</b> between <b>cameras,</b> 2) the synthesis of synchronized image pairs based on the estimated <b>camera</b> temporal <b>offset</b> and optical flow fields computed in each view, and 3) the stereo computation based on the synthesized synchronous image pairs. The proposed algorithm has been tested on both synthetic data and real image sequences. Promising quantitative and qualitative experimental results are demonstrated in the paper. ...|$|R
50|$|Another {{method is}} to set up both left and right virtual <b>cameras,</b> both <b>offset</b> from the {{original}} camera but splitting the offset difference, then painting out occlusion edges of isolated objects and characters. Essentially clean-plating several background, mid ground and foreground elements.|$|R
40|$|Parametric {{uncertainty}} {{is one of}} many possible causes of divergence for the Kalman filter. Frequently, state estimation errors caused by imperfect model parameters are reduced by including the uncertain parameters as states (i. e., augmenting the state vector). For many situations, this not only improves the state estimates, but also improves the accuracy and precision of the parameters themselves. Unfortunately, not all filters benefit from this augmentation due to computational restrictions or because the parameters are poorly observable. A parameter with low observability (e. g., a set of high order gravity coefficients, <b>a</b> set of <b>camera</b> <b>offsets,</b> lens calibration controls, etc.) may not acquire enough measurements along a particular trajectory to improve the parameter's accuracy, which can cause detrimental effects in the performance of the augmented filter. The problem is then how to reduce the dimension of the augmented state vector while minimizing information loss. This dissertation explored possible implementations of reduced-order filters which decrease computational loads while also minimizing state estimation errors. A theoretically rigorous approach using the ?consider" methodology was taken at discrete time intervals were explored for linear systems. The continuous dynamics, discretely measured (continuous-discrete) model was also expanded for use with nonlinear systems. Additional techniques for reduced-order filtering are presented including the use of additive process noise, an alternative consider derivation, and the minimum variance reduced-order filter. Multiple simulation examples are provided to help explain critical concepts. Finally, two hardware applications are also included to show the validity of the theory for real world applications. It was shown that the minimum variance consider Kalman filter (MVCKF) is the best reduced-order filter to date both theoretically and through hardware and software applications. The consider method of estimation provides a compromise between ignoring parameter error and completely accounting for it in a probabilistic sense. Based on multiple measures of optimality, the consider filtering framework can be used to account for parameter error without directly estimating any or all of the parameters. Furthermore, by accounting for the parameter error, the consider approach provides a rigorous path to improve state estimation through the reduction of both state estimation error and with a consistent variance estimate. While using the augmented state vector to estimate both states and parameters may further improve those estimates, the consider estimation framework is an attractive alternative for complex and computationally intensive systems, and provides a well justified path for parameter order reduction...|$|R
5000|$|CompactFlash is {{physically}} larger than other card formats. This limits its use, especially in miniature consumer devices where internal space is limited, such as point-and-shoot digital <b>cameras.</b> (An <b>offsetting</b> benefit of larger size {{is that the}} card is easier to insert and remove, and harder to misplace.) ...|$|R
40|$|Understanding what <b>a</b> <b>camera</b> {{measures}} <b>A</b> {{great deal}} of mathematical processing of pictures has been developed, to create the very active eld of im-age processing, but {{with little or no}} regard to what the numbers coming out of <b>a</b> <b>camera</b> actually mean. The {{purpose of this paper is}} to ask, and attempt to answer, the fundamental question does <b>a</b> <b>camera</b> mea-sure". Once we understand what <b>a</b> <b>camera</b> measures, we can introduce a new kind of image processing that works in lightspace rather than in image space. 1 What does <b>a</b> <b>camera</b> measure Cameras are often present in computer vision systems that are used to measure various quantities [1], yet an often overlooked question is does <b>a</b> <b>camera</b> itsel...|$|R
25|$|In The Lovely Bones, {{he appears}} as a {{customer}} in <b>a</b> <b>camera</b> store playing with <b>a</b> <b>camera.</b>|$|R
5000|$|I Am <b>a</b> <b>Camera,</b> {{directed}} by Henry Cornelius (1955, {{based on the}} play I Am <b>a</b> <b>Camera)</b> ...|$|R
5000|$|In The Lovely Bones, {{he appears}} as a {{customer}} in <b>a</b> <b>camera</b> store playing with <b>a</b> <b>camera.</b>|$|R
30|$|When <b>a</b> <b>camera</b> sensor {{is turned}} off, {{it can only}} receive control messages. Once <b>a</b> <b>camera</b> sensor is {{actuated}} (turned on), it generates video streams transmitted to the convergence point.|$|R
5000|$|<b>A</b> <b>camera</b> stabilizer, or camera-stabilizing mount, is {{a device}} {{designed}} to hold <b>a</b> <b>camera</b> in <b>a</b> manner that prevents or compensates for unwanted camera movement, such as [...] "camera shake".|$|R
50|$|The Camera Optics Module of KH-7 {{consists}} of three cameras: <b>a</b> single strip <b>camera,</b> <b>a</b> stellar <b>camera,</b> and <b>an</b> index <b>camera.</b>|$|R
50|$|Raja was {{not wearing}} <b>a</b> body <b>camera</b> and his vehicle was not {{equipped}} with <b>a</b> <b>camera.</b>|$|R
50|$|Furthermore, the i920 is {{targeted}} towards segments in which its upper management wants its subordinates {{to have a}} smartphone, but without <b>a</b> <b>camera,</b> since <b>a</b> <b>camera</b> is likely to pose security risks.|$|R
50|$|<b>A</b> pan-tilt-zoom <b>camera</b> (PTZ <b>camera)</b> is <b>a</b> <b>camera</b> that {{is capable}} of remote {{directional}} and zoom control.|$|R
50|$|There is {{no major}} {{difference}} in principle between a lens used for <b>a</b> still <b>camera,</b> <b>a</b> video <b>camera,</b> <b>a</b> telescope, a microscope, or other apparatus, but the detailed {{design and construction}} are different. A lens might be permanently fixed to <b>a</b> <b>camera,</b> {{or it might be}} interchangeable with lenses of different focal lengths, apertures, and other properties.|$|R
30|$|We {{review the}} literatures of human {{tracking}} within <b>a</b> <b>camera</b> {{based on the}} correlation among the human objects. Specifically, we hierarchically categorize the human tracking approaches within <b>a</b> <b>camera</b> into generative trackers and discriminative trackers.|$|R
50|$|<b>A</b> <b>camera</b> with <b>a</b> full-frame image sensor, and <b>a</b> <b>camera</b> with <b>an</b> APS-C image sensor, {{may have}} the same pixel count (for example, 16 MP), but the full-frame camera may allow better dynamic range, less noise, and {{improved}} low-light shooting performance than <b>an</b> APS-C <b>camera.</b> This is because the full-frame <b>camera</b> has <b>a</b> larger image sensor than the APS-C camera, therefore more information can be captured per pixel. <b>A</b> full-frame <b>camera</b> that shoots photographs at 36 megapixels has roughly the same pixel size as <b>an</b> APS-C <b>camera</b> that shoots at 16 megapixels.|$|R
5000|$|Monk with <b>a</b> <b>Camera,</b> <b>a</b> {{film about}} Nicholas Vreeland, who is Diana Vreeland's grandson.|$|R
50|$|<b>A</b> video <b>camera</b> is <b>a</b> <b>camera</b> {{used for}} {{electronic}} motion picture acquisition (as opposed to <b>a</b> movie <b>camera,</b> which records images on film), initially {{developed for the}} television industry but now common in other applications as well.|$|R
30|$|The {{relative}} {{positions and}} postures of <b>a</b> <b>camera,</b> <b>a</b> diffuser and light sources are known.|$|R
5000|$|... #Caption: <b>A</b> {{freestanding}} room-sized <b>camera</b> obscura in {{the shape}} of <b>a</b> <b>camera.</b> Cliff House, San Francisco ...|$|R
50|$|The Berlin Stories was the {{starting}} point for the John Van Druten play I Am <b>a</b> <b>Camera,</b> which in turn went on to inspire the film I Am <b>a</b> <b>Camera</b> as well as the stage musical and film versions of Cabaret.|$|R
5000|$|Human image {{synthesis}} {{has reached}} such levels of likeness since early 2000s that distinguishing {{what is a}} human imaged with <b>a</b> <b>camera</b> and what is a simulation of a human imaged with a simulation of <b>a</b> <b>camera</b> is often anyone's guess.|$|R
50|$|As {{with some}} other {{electronically}} controlled SLR film <b>cameras,</b> <b>a</b> working Canon T90 can be a bargain on the used market. Prices from used camera dealers providing warranties range from approximately US$120-140 for <b>a</b> <b>camera</b> in working condition to approximately US$240-260 for <b>a</b> <b>camera</b> body in mint condition with accessories, box and manual.|$|R
50|$|I Am <b>a</b> <b>Camera</b> is <b>a</b> British comedy-drama film {{released}} in 1955. Based on The Berlin Stories by Christopher Isherwood {{and the play}} I Am <b>a</b> <b>Camera</b> by John Van Druten, the film is a fictionalized account of Isherwood's time living in Berlin between the World Wars. Directed by Henry Cornelius from a script by John Collier, I Am <b>a</b> <b>Camera</b> stars Laurence Harvey as Isherwood and Julie Harris recreating her Tony Award winning performance as Sally Bowles.|$|R
5000|$|Wale {{began his}} career as <b>a</b> <b>camera</b> trainee on Chris Carters The X-Files. He worked on several films and {{television}} series as <b>a</b> <b>camera</b> assistant and later as <b>a</b> <b>camera</b> operator on So Weird, Just Cause, and The L Word. As a cinematographer his work included: SK8, Sub Zero, The Troop and Continuum. He served on the second season of CW Networks iZombie as director of [...] "The Hurt Stalker" [...] which premiered in late 2015.|$|R
30|$|As {{previously}} mentioned, the prototypes {{developed in}} this study were not equipped with <b>a</b> <b>camera.</b> In practical use, it is important to control the device from the viewpoint of <b>a</b> <b>camera</b> that is attached to the device. Therefore, in the future, we will attempt to attach <b>a</b> <b>camera</b> to the tip of the six braided-tube locomotive device to confirm controllability. Because we predict that the view of <b>a</b> <b>camera</b> attached {{at the tip of the}} device will be shaken by a motion of the tip, we must develop an image stabilizing method for the proposed device. We plan to synchronize an image update and a pressurizing pattern so that the view is updated with same device shape.|$|R
5000|$|Many <b>cameras</b> and {{displays}} <b>offset</b> the color components relative {{to each other}} or mix up temporal with spatial resolution:Image:Bayer matrix.svg|digital camera (Bayer color filter array)Image:Lcd_display_dead_pixel.jpg|LCD (Triangular pixel geometry)Image:Shadow_mask_closeup_cursor.jpg|CRT (shadow mask) ...|$|R
5000|$|David Noh of Film Journal International called Monk with <b>a</b> <b>Camera</b> <b>an</b> [...] "enthralling and {{uplifting}} documentary".|$|R
50|$|It {{came with}} <b>a</b> <b>camera</b> also.|$|R
30|$|We {{presented}} a novel method for finding the spatial relation between <b>a</b> <b>camera</b> and laser scanner based on stereo calibration. The algorithm enables {{the user to}} calibrate the laser scanner to <b>a</b> <b>camera</b> with high accuracy using only a single shot of a calibration scene.|$|R
50|$|The {{developers}} originally {{asked for}} bids from contractors for <b>a</b> <b>camera.</b> These were priced at around US$10 million. The team then bought a high-end observatory telescope costing around $20,000 and added <b>a</b> <b>camera</b> sensor ($2 million), delivering a sensor capable of 1m ground resolution.|$|R
40|$|Calibrating <b>a</b> <b>camera</b> {{consists}} {{in determining the}} analyt-ical relationship between the three-dimensional coordinates of a point and the two-dimensional coordinates of its im-age by the <b>camera.</b> Once <b>a</b> <b>camera</b> model is chosen, the calibration problem is to compute the particular numericalparameters for <b>a</b> given <b>camera.</b> The classical methods are model-based. They {{are based on the}} observation of an objetfor which the three-dimensional coordinates o...|$|R
