964|512|Public
5000|$|CompHEP [...] - [...] <b>automatic</b> <b>evaluation</b> of {{tree level}} matrix {{elements}} for event generation or export into other event generators ...|$|E
5000|$|Seen as such an immediate, <b>automatic</b> <b>evaluation</b> that it {{cannot be}} called a {{cognitive}} evaluation at all, but rather precedes cognition ...|$|E
5000|$|Duckworth, K. L. Bargh, J. A. Garcia M. and Chaiken.S. (2002). The <b>automatic</b> <b>evaluation</b> {{of novel}} stimuli. Psychol Sci, 13, 513-9 DOI ...|$|E
40|$|Accumulating {{research}} indicates that physical activity is motivated by <b>automatic</b> <b>evaluations</b> of physical activity. Little {{is known about the}} stability of <b>automatic</b> <b>evaluations</b> or how their dynamics impact physical activity. We tested the measurement invariance and stability of university students' (N = 164) <b>automatic</b> <b>evaluations</b> of physical activity. In addition, multiple regression and structural equation models with latent interaction variables were used to investigate how changes in <b>automatic</b> <b>evaluations</b> related to change in self-reported physical activity and differences in the level of directly measured physical activity. It was revealed that <b>automatic</b> <b>evaluations</b> had strict measurement invariance and that <b>automatic</b> <b>evaluations</b> have both stable and unstable components. People whose unfavorable <b>automatic</b> <b>evaluations</b> became more favorable over the week showed a larger increase in self-reported physical activity from the previous week than did people whose <b>automatic</b> <b>evaluations</b> remained unfavorable. These results indicated that the dynamics of <b>automatic</b> <b>evaluations</b> and physical activity can be intertwined...|$|R
40|$|The {{general purpose}} of this {{systematic}} review was to summarize, structure and evaluate the findings on <b>automatic</b> <b>evaluations</b> of exercising. Studies were eligible for inclusion if they reported measuring <b>automatic</b> <b>evaluations</b> of exercising with an implicit measure and assessed some kind of exercise variable. Fourteen nonexperimental and six experimental studies (out of a total N = 1, 928) were identified and rated by two independent reviewers. The main study characteristics were extracted and the grade of evidence for each study evaluated. First, results revealed a large heterogeneity in the applied measures to assess <b>automatic</b> <b>evaluations</b> of exercising and the exercise variables. Generally, small to large-sized significant relations between <b>automatic</b> <b>evaluations</b> of exercising and exercise variables were identified {{in the vast majority}} of studies. The review offers a systematization of the various examined exercise variables and prompts to differentiate more carefully between actually observed exercise behavior (proximal exercise indicator) and associated physiological or psychological variables (distal exercise indicator). Second, a lack of transparent reported reflections on the differing theoretical basis leading to the use of specific implicit measures was observed. Implicit measures should be applied purposefully, taking into consideration the individual advantages or disadvantages of the measures. Third, 12 studies were rated as providing first-grade evidence (lowest grade of evidence), five represent second-grade and three were rated as third-grade evidence. There is a dramatic lack of experimental studies, which are essential for illustrating the cause-effect relation between <b>automatic</b> <b>evaluations</b> of exercising and exercise and investigating under which conditions <b>automatic</b> <b>evaluations</b> of exercising influence behavior. Conclusions about the necessity of exercise interventions targeted at the alteration of <b>automatic</b> <b>evaluations</b> of exercising should therefore not be drawn too hastily...|$|R
40|$|<b>Automatic</b> <b>evaluations</b> of {{clinically}} {{anxious and}} nonanxious children (n = 40, aged 8 - 16, 18 girls) were compared using a pictorial performance-based measure of automatic affective associations. Results showed a threat-related evaluation bias in clinically anxious {{but not in}} nonanxious children. In anxious participants, <b>automatic</b> <b>evaluations</b> of anxiety-relevant stimuli were more negative than those of negative stimuli. In nonanxious participants, evaluations of negative and anxiety-relevant stimuli did not differ. Furthermore, anxious youth had stronger negative evaluations of anxiety-relevant stimuli than nonanxious children. <b>Automatic</b> <b>evaluations</b> of positive, neutral, and negative stimuli did not differ between groups. Threat-related evaluations were predictive of parent-reported, but not child-reported, anxiet...|$|R
50|$|Gawronski, B., Rydell, R. J., Vervliet, B., & De Houwer, J. (2010). Generalization versus contextualization in <b>automatic</b> <b>evaluation.</b> Journal of Experimental Psychology: General, 139, 683-701.|$|E
5000|$|Ferguson, M.J. & Bargh, J.A. (2004). Liking is for doing: The {{effects of}} goal pursuit on <b>automatic</b> <b>evaluation.</b> Journal of Personality and Social Psychology, 87, 557-572.|$|E
5000|$|... 2008: “Science 50”─ Scientific Achievements of the National Science Council 50th Anniversary (Science Education): Only 50 of {{the most}} {{remarkable}} scientific achievements were selected in celebration of NSC 50th Anniversary”. Dr. Chang’s research on “The ability to solve science problems and opening <b>automatic</b> <b>evaluation</b> system for science study and examination,” was chosen for this award.|$|E
40|$|Automatic threat-related {{information}} {{processes are}} involved in childhood anxiety (Bijttebier, Vasey, & Braet, 2003; Daleiden & Vasey, 1997). <b>Automatic</b> <b>evaluations</b> of clinically anxious and non-anxious children (n= 40, aged 8 - 16, 18 girls) were compared using a pictorial Extrinsic Affective Simon Task (de Houwer, 2003 a). Results showed a threat-related evaluation bias in clinically anxious, but not in non-anxious children. In anxious participants, <b>automatic</b> <b>evaluations</b> of anxiety-relevant stimuli were more negative than those of negative stimuli. In non-anxious participants, evaluations of negative and anxiety-relevant stimuli did not differ. Furthermore, anxious youth had stronger negative evaluations of anxiety-relevant stimuli than non-anxious children. <b>Automatic</b> <b>evaluations</b> of positive, neutral and negative stimuli did not differ between groups. Threat-related evaluations were predictive of parent-reported, but not child-reported anxiety. The present study shows an <b>automatic</b> threat-related <b>evaluation</b> bias in youth anxiety, thus increasing evidence for information processing theories of childhood anxiet...|$|R
5000|$|Ferguson, M. J., Bargh, J. A., & Nayak, D. A. (2005). After-affects: How <b>automatic</b> <b>evaluations</b> {{influence}} {{the interpretation of}} subsequent, unrelated stimuli. Journal of Experimental Social Psychology, 41, 182-191.|$|R
40|$|We {{examined}} whether <b>automatic</b> stimulus <b>evaluation</b> {{as measured}} by the Affect Misattribution Procedure (AMP) is moderated by the degree to which attention is assigned to the evaluative stimulus dimension (i. e., feature-specific attention allocation, FSAA). In two experiments, one group of participants completed a standard AMP while attending to evaluative stimulus information. A second group of participants completed the AMP while attending to non-evaluative stimulus information. In line with earlier work, larger AMP effects were observed when participants were encouraged to attend to evaluative stimulus information than when they were not. These observations support the idea that the impact of FSAA on measures of <b>automatic</b> stimulus <b>evaluation</b> results from a genuine change in the degree of <b>automatic</b> stimulus <b>evaluation</b> rather than a change in the degree to which <b>automatic</b> stimulus <b>evaluation</b> is picked up by these measures...|$|R
50|$|To {{compare the}} quality of {{different}} machine translation systems, users perform RTT and compare the resulting text to the original. The {{theory is that the}} closer the result of the RTT is to the original text, the higher {{the quality of}} the machine translation system. One of the problems with this technique is {{that if there is a}} problem with the resulting text it is impossible to know whether the error occurred in the forward translation, in the back translation, or in both. In addition it is possible to get a good back translation from a bad forward translation. A study using the <b>automatic</b> <b>evaluation</b> methods BLEU and F-score compared five different free on-line translation programs evaluating the quality of both the forward translation and the back translation and found no correlation between the quality of the forward translation and the quality of the back translation (i.e., a high quality forward translation did not always correspond to a high quality back translation). The author concluded that RTT was a poor method of predicting the quality of machine translation software. This conclusion was reinforced by a more in-depth study also using <b>automatic</b> <b>evaluation</b> methods. A subsequent study which included human evaluation of the back translation in addition to <b>automatic</b> <b>evaluation</b> methods found that RTT might have some ability to predict the quality of a machine translation system not on a sentence by sentence basis but for larger texts.|$|E
50|$|The amygdala, {{fusiform}} gyrus, insula, {{and superior}} and middle temporal regions {{have been identified}} as areas in the brain that play a role in visual emotional cues. It was found that there was greater activation in the bilateral anterior superior temporal gyrus and bilateral fusiform gyrus when it came to emotional stimuli. The amygdala has been connected with the <b>automatic</b> <b>evaluation</b> of threat, facial valence information, and trustworthiness of faces.|$|E
5000|$|Since IBM {{proposed}} and realized {{the system of}} BLEU [...] as the automatic metric for Machine Translation (MT) evaluation, many other methods have been proposed to revise or improve it, such as TER, METEOR, etc. However, there exist some problems in the traditional <b>automatic</b> <b>evaluation</b> metrics. Some metrics perform well on certain languages but weak on other languages, which is usually called as language bias problem. Some metrics rely {{on a lot of}} language features or linguistic information, which makes it difficult for other researchers to repeat the experiments. LEPOR is an <b>automatic</b> <b>evaluation</b> metric that tries to address some of the existing problems. LEPOR is designed with augmented factors and the corresponding tunable parameters to address the language bias problem. Furthermore, in the improved version of LEPOR, i.e. the hLEPOR, it tries to use the optimized linguistic features that are extracted from treebanks. Another advanced version of LEPOR is the nLEPOR metric, which adds the n-gram features into the previous factors. So far, the LEPOR metric has been developed into LEPOR series.|$|E
40|$|Researchers often employ {{implicit}} {{measures as}} dependent variables to investigate processes of attitude formation and change. In such studies, experimentally induced differences are typically interpreted as reflecting change in <b>automatic</b> <b>evaluations.</b> We argue that experimentally induced effects on implicit measures {{may not always}} reflect genuine changes in evaluative responses, but can be driven by the mechanisms underlying the measurement procedure. In line with this assumption, the present research shows that these mechanisms can produce opposite effects of the same experimental manipulation for otherwise equivalent implicit measures. These results indicate that merely observing experimental effects on implicit measures does not allow direct inferences regarding changes in <b>automatic</b> <b>evaluations.</b> Instead, psychological interpretations of such effects hinge upon the mechanics of how a given measurement procedure responds to variations in the context. Implications for research using implicit measures are discussed. " [author's abstract...|$|R
40|$|In this paper, we {{described}} our {{participation in}} the question-focused multi-document summarization task of DUC 2005. Our system {{was based on a}} supervised machine learning method in which feature extraction was an important issue. We present the whole procedure of our system, focusing on the features we used. We also analyze the results of manual and <b>automatic</b> <b>evaluations.</b> 1...|$|R
40|$|We {{investigate}} syntactic reordering {{within an}} English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) {{to the distant}} language pair, English-Arabic. We achieve significant improvements in translation quality over related approaches, measured by manual as well as <b>automatic</b> <b>evaluations.</b> These results prove the viability of this approach for distant languages. ...|$|R
50|$|Studies {{looking at}} implicit, <b>automatic</b> <b>evaluation</b> of art works have {{investigated}} how people react to abstract and figurative art {{works in the}} split-second before they {{had time to think}} about it. In implicit evaluation, people reacted more positively to the figurative art, where they could at least make out the shapes. In terms of explicit evaluation, when people had to think about the art, there was no real difference in judgement between abstract and representational art.|$|E
5000|$|The {{evaluator}} generally {{follows a}} script (Pre-written answers {{based on the}} test takers score.) originally devised by South African Scientologist Peter Greene around 1960/61, which Hubbard instructs [...] "must be studied and learned by heart" [...] by evaluators. Although the analysis is represented as being [...] "not our opinion of you, but ... a factual scientific analysis taken from your answers," [...] it relies heavily on scripted responses set out in detail in the [...] "OCA <b>Automatic</b> <b>Evaluation</b> Script".|$|E
5000|$|Explicit {{measures}} tend {{to rely on}} self-reports or easily observed behaviors. These tend {{to involve}} bipolar scales (e.g., good-bad, favorable-unfavorable, support-oppose, etc.). Explicit measures {{can also be used}} by measuring the straightforward attribution of characteristics to nominate groups. Explicit attitudes that develop in response to recent information, <b>automatic</b> <b>evaluation</b> were thought to reflect mental associations through early socialisation experiences. Once formed, these associations are highly robust and resistant to change, as well as stable across both context and time. Hence the impact of contextual influences was assumed to be obfuscate assessment of a person's [...] "true" [...] and enduring evaluative disposition as well as limit the capacity to predict subsequent behavior. Likert scales and other self-reports are also commonly used.|$|E
40|$|This paper {{proposes a}} {{post-editing}} {{model in which}} our three-level rule-based automatic post-editing engine called Grafix is presented to refine the output of machine translation systems. The type of corrections on sentences varies from lexical transformation to complex syntactical rearrangement. The experimental results both in manual and <b>automatic</b> <b>evaluations</b> show that the proposed system is able {{to improve the quality}} of our state-of-the-art English-Persian SMT system. ...|$|R
40|$|International audienceMulti-Sentence Compression (MSC) is {{the task}} of {{generating}} a short single sentence summary from a cluster of related sentences. This paper presents an N-best reranking method based on keyphrase extraction. Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain. Both manual and <b>automatic</b> <b>evaluations</b> were performed using a dataset made of clusters of newswire sentences. Results show that the proposed method significantly improves the informativity of the generated compressions...|$|R
40|$|Human {{emotional}} responses are highly individual. A comprehensive analysis of emotion research in cognitive psychology and physiology, including laboratory-based experiments, showed that understanding human emotions requires a dynamic systems approach incorporating insights from scientific disciplines beyond psychology. Importantly, subjective and <b>automatic</b> <b>evaluations</b> of emotive information are context-sensitive and changeable, confirming the dynamic nature of emotion {{and role of}} individual differences. Furthermore, a comparison of different statistical approaches established that statistical estimation, rather than averages, best captures our highly individual {{emotional responses}}. Emotion research needs a cross-disciplinary approach...|$|R
5000|$|Stimuli may be {{automatically}} {{evaluated in}} ways that affect behavior, an <b>automatic</b> <b>evaluation.</b> In {{a study conducted by}} Chen and Bargh, subjects were faster to pull a lever toward themselves (an approach tendency) when a word had a positive valence than a negative valence, and were similarly faster to push the lever away (an avoidance tendency) when the word had a negative valence compared to a positive valence. The [...] "sequential evaluative priming paradigm" [...] refers to the related phenomenon of response times reducing when primed by stimuli with congruent valence. In an examination of the generality of the effects of this paradigm, Bargh, Chaiken, Govender and Pratto show that simply seeing or hearing mention of stimuli triggers automatically activated evaluations. This occurs even when the subject has not been asked to think about their evaluation of the stimulus beforehand. It was further shown that novel stimuli are automatically evaluated and produce the same effect as nonnovel stimuli: when positively valenced novel stimuli prime positively valenced targets, reaction time is faster.|$|E
40|$|<b>Automatic</b> <b>evaluation</b> {{of output}} quality for machine {{translation}} systems {{is a difficult}} task. The Institute of Computational Linguistics of Peking University has developed an <b>automatic</b> <b>evaluation</b> system called MTE. This paper introduces {{the basic principles of}} MTE, its implementation techniques and the practice experiences...|$|E
40|$|This paper {{studies the}} impact of paraphrases on the {{accuracy}} of <b>automatic</b> <b>evaluation.</b> Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method {{in the context of}} machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of <b>automatic</b> <b>evaluation.</b> We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to <b>automatic</b> <b>evaluation.</b> ...|$|E
40|$|This work {{presents}} a HMT system for patent translation. The system exploits the high coverage of SMT {{and the high}} preci-sion of an RBMT system based on GF to deal with specific issues of the language. The translator is specifically developed to translate patents and it is evaluated in the English-French language pair. Although the number of issues tackled by the gram-mar are not extremely numerous yet, both manual and <b>automatic</b> <b>evaluations</b> consis-tently show their preference for the hybrid system {{in front of the}} two individual trans-lators. ...|$|R
3000|$|For this study, we {{applied a}} nonadapted ASR system for <b>automatic</b> speech <b>evaluation</b> that has {{previously}} been proven to be adequate for [...] "normal" [...] speech samples [16]. The <b>automatic</b> speech <b>evaluations</b> were compared to a control group of 40 speakers without speech pathology in this study. As increased age {{has been shown to}} have a negative influence on automatic speech recognition [29], the control group consisted of speakers of similar age compared to the patient groups. In our study, control speakers reached a word recognition rate of [...]...|$|R
40|$|We {{introduce}} the Helsinki Neural Machine Translation system (HNMT) {{and how it}} is applied in the news translation task at WMT 2017, where it ranked first in both the human and <b>automatic</b> <b>evaluations</b> for English [...] Finnish. We discuss the success of English [...] Finnish translations and the overall advantage of NMT over a strong SMT baseline. We also discuss our submissions for English [...] Latvian, English [...] Chinese and Chinese [...] English. Comment: Proceedings of the Second Conference on Machine Translation (WMT 2017) at EMNLP 2017, Copenhagen/Danmar...|$|R
40|$|Two {{experiments}} {{tested the}} effect of co-occurrence of a target object with affective stimuli on <b>automatic</b> <b>evaluation</b> of the target when {{the relation between the}} target and the affective stimuli suggests that they have opposite valence. Participants learned about targets that ended an unpleasant noise or a pleasant music. The valence of such targets is opposite to the valence of the affective stimuli that co-occur with them. Participants reported preference for targets that ended noise over targets that ended music, but <b>automatic</b> <b>evaluation</b> measures revealed the opposite preference. This suggests that <b>automatic</b> <b>evaluation</b> is sensitive to co-occurrence between stimuli more than to the relation between the stimuli, and that relational information has a stronger influence on deliberate evaluation than on <b>automatic</b> <b>evaluation.</b> These conclusions support the Associative-Propositional Evaluation model (Gawronski & Bodenhausen, 2006), and add evidence regarding the sensitivity of the Evaluative-Conditioning effect to relational information...|$|E
30|$|S. Lang et al. {{address the}} <b>automatic</b> <b>evaluation</b> of {{landmarks}} for image-based navigation updates.|$|E
40|$|Following {{the recent}} {{adoption}} by the machine translation community of <b>automatic</b> <b>evaluation</b> using the BLEU/NIST scoring process, we conduct an in-depth {{study of a}} similar idea for evaluating summaries. The results show that <b>automatic</b> <b>evaluation</b> using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. ...|$|E
40|$|In {{this paper}} {{we present a}} novel method for {{deriving}} paraphrases during <b>automatic</b> MT <b>evaluation</b> using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software. Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation. The method produces lexical and lowlevel syntactic paraphrases {{that are relevant to}} the domain in hand, does not use external knowledge resources, and can be combined with a variety of <b>automatic</b> MT <b>evaluation</b> system. ...|$|R
50|$|There {{are some}} machine {{translation}} evaluation survey works, where people introduced {{more details about}} what kinds of human evaluation methods they used and how they work, such as the intelligibility, fidelity, fluency, adequacy, comprehension and informativenes, etc. For <b>automatic</b> <b>evaluations,</b> the also did some clear classifications such as the lexical similarity methods, the linguistic features application, and the sub fields of these two aspects. For instance, for lexical similarity, it contains edit distance, precision, recall and word order; for linguistic feature, it is divided into syntactc feature and semantic feature respectively.|$|R
40|$|Objective: The Single-Category Implicit Association Test (SC-IAT) {{has been}} used as a method for {{assessing}} <b>automatic</b> <b>evaluations</b> of physical activity, but measurement artifact or consciously held attitudes could be confounding the outcome scores of these measures. The objective of these two studies was to address these measurement concerns by testing the validity of a novel SC-IATscoring technique. Design: Study 1 was a cross-sectional study, and study 2 was a prospective study. Method: In study 1, undergraduate students (N = 104) completed SC-IATs for physical activity, flowers, and sedentary behavior. In study 2, undergraduate students (N = 91) completed a SC-IAT for physical activity, self-reported affective and instrumental attitudes toward physical activity, physical activity intentions, and wore an accelerometer for two weeks. The EZ-diffusion model was used to decompose the SC-IAT into three process component scores including the information processing efficiency score. Results: In study 1, a series of structural equation model comparisons revealed that the information processing score did not share variability across distinct SC-IATs, suggesting it does not represent systematic measurement artifact. In study 2, the information processing efficiency score was shown to be unrelated to self-reported affective and instrumental attitudes toward physical activity, and positively related to physical activity behavior, above and beyond the traditional D-score of the SC-IAT. Conclusions: The information processing efficiency score is a valid measure of <b>automatic</b> <b>evaluations</b> of physical activity...|$|R
