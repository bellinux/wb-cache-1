43|1497|Public
50|$|If the longer/shorter test is not {{parallel}} to the current test, then the prediction will not be strictly accurate. For example, if a highly reliable test was lengthened by adding many poor items then the <b>achieved</b> <b>reliability</b> will probably be much lower than that predicted by this formula.|$|E
5000|$|These {{improvements}} {{promised a}} steam car that would at last provide {{virtually all of}} the convenience associated with a conventional automobile, but with higher speed, simpler controls, and what was a virtually noiseless power plant. The only defect sometimes noted throughout the Doble car era was less than perfect braking, which was common in automobiles of all types before 1930. Typically, a car of 1920s only had two rear-mounted mechanical drum brakes, although those fitted to Dobles were of larger than usual proportions. Dobles <b>achieved</b> <b>reliability</b> by eliminating most of the mechanical items that tended to malfunction in conventional automobiles: they had no clutch, no transmission, no distributor, and no points. Later Doble steam cars often achieved several hundred thousand miles of use before a major mechanical service was necessary ...|$|E
30|$|In survey research, it {{is common}} to report methods by which {{reliability}} and validity were <b>achieved.</b> <b>Reliability</b> is the consistency with which an instrument provides similar results across items, testing occasions, and raters (Cronbach 1947; Nunnally 1967). There are several commonly reported forms of evidence for instrument reliability, including internal consistency, test-retest, and inter-rater reliability.|$|E
40|$|Tactical weapon systems, while {{different}} {{in many respects}} from PTTI applications, face similar risks in <b>achieving</b> <b>reliability</b> in development. General principles derived from experience in <b>achieving</b> high <b>reliability</b> in tactical weapon systems are selectively summarized for application to new technologies in unusual environments...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedS costs {{and increasing the}} effectiveness of the soldiers. Many programs have had difficulty <b>achieving</b> their required <b>reliability.</b> Operational Testing data gathered by the Army Test and Evaluation Command indicates a decreasing trend in <b>achieving</b> <b>reliability</b> requirements with more than 80 % failing to achieve requirements. It is intuitive that it would be even more difficult to achieve ultra-reliability, a higher level of reliability and a proposed goal of the Future Combat Systems Program. To determine what successful practices should be used to <b>achieve</b> <b>reliability</b> requirements, we should look to successful programs to show us the way. To that end, this exploratory study questions successful Army programs for practices, recommendations, and lessons learned, that could be shared with other programs to <b>achieve</b> <b>reliability</b> requirements. If we are unsuccessful in our endeavors to improve reliability achievement, future forces will be unnecessarily burdened by our mistakes and incapable of progress to achieving the Objective Force. Captain, United States Arm...|$|R
30|$|Using {{wireless}} sensor {{networks in}} home automation is prevalent and cost effective. A routing algorithm for WSNHA must meet these requirements to <b>achieve</b> <b>reliability</b> and energy efficiency in data packet delivery.|$|R
40|$|The {{impact of}} mechanical-reliability {{practice}} on the Saturn/Apollo launch program is considered {{with reference to the}} interrelationship of analysts and designers with management. Rocket engine development, ground testing, and launch facilities in the Saturn/Apollo program are discussed, and the Saturn reliability approach is examined in regard to management style, decision making, human error control, and reliability analyses. It is noted that the use of conservative design philosophy contributed to <b>achieved</b> <b>reliability...</b>|$|E
40|$|Abstract. The CAD {{technology}} {{at home and}} abroad is analyzed. The necessity in research and development of cold extrusion die CAD system is proposed. CAD system of the cold extrusion die based on Visual Basic Programming platform is established. And in the CAD system, the functions such as tool set design, process analysis, optimization design of the cold extrusion die can be <b>achieved.</b> <b>Reliability</b> of the CAD system is tested by the instance of a bike steel bowl...|$|E
40|$|The {{estimated}} <b>achieved</b> <b>reliability</b> of SNAP 10 A space {{nuclear power}} units will be relatively low at the timeof the first SNAPSHOT flight test in April 1963 and the existing R&D program {{does not provide}} a significant reliabiity growth thereafter. The total costs of an 8 -satellite network using SNAP 10 A units over a 5 -year period has been approximated for the case where the total cost of a single satellite launched is 8 million dollars...|$|E
50|$|Communication {{protocols}} which implement reliable byte streams, generally {{over some}} unreliable lower level, use {{a number of}} mechanisms to provide that reliability. ARQ protocols {{have an important role}} for <b>achieving</b> <b>reliability.</b>|$|R
50|$|Design of {{the scheme}} was {{governed by the}} need to satisfy {{operating}} requirements of speed and headway. Besides employing what was then modern signal equipment, an associated comprehensive communication network was necessary to <b>achieve</b> <b>reliability</b> and efficiency.|$|R
40|$|Abstract — Reliability is {{critical}} {{to a variety of}} network applications. Unfortunately, due to lack of QoS support across ISP boundaries, it is difficult to achieve even two 9 s (99 %) reliability in today’s Internet. In this paper, we propose SmartTunnel, an end-to-end approach to <b>achieving</b> <b>reliability.</b> A SmartTunnel is a logical point-to-point tunnel between two end points that spans multiple physical network paths. It <b>achieves</b> <b>reliability</b> by strategically allocating traffic onto multiple paths and performing FEC coding. Such an end-to-end approach requires no explicit QoS support from intermediate ISPs, and is therefore easy to deploy in today’s Internet. To fully realize the potential of SmartTunnel, we analytically derive near-optimal traffic allocation schemes that minimize loss rates. We extensively evaluate our approach using trace-driven simulations, ns- 2 simulations, and experiments on PlanetLab. Our results clearly demonstrate that SmartTunnel is effective in <b>achieving</b> high <b>reliability.</b> I...|$|R
40|$|Abstract: To {{deal with}} the problem of target {{identification}} caused by the <b>achieved</b> <b>reliability</b> of multi sensors and the feature measurement uncertainty of the target, this paper proposes a new identification algorithm based on Modified Interval Dempster-Shafer Theory (MIDST), which models sensor’s reliability as scalar value and identification outputs of each sensor as interval values, and then combines the actual interval outputs through interval evidence combination rules. At last, one simulation is presented to demonstrate the identification capability of the MIDST algorithm...|$|E
40|$|MCSM is a {{recently}} proposed novel system concept {{to solve the}} massive access problem envisioned in future communication systems like 5 G and industry 4. 0 systems. This work focuses on the practical verification of the theoretical gains that MCSM provides using a Hardware-In-the-Loop (HIL) measurement setup. We present results in two different scenarios: (i) a LoS lab setup and (ii) a non-LoS machine hall. In both scenarios MCSM shows promising performance {{in terms of the}} number of supported users and the <b>achieved</b> <b>reliability...</b>|$|E
40|$|We {{discuss the}} {{capability}} of AKARI in recovering diffuse far-infrared emission, and examine the <b>achieved</b> <b>reliability.</b> Critical issues in making images of diffuse emission are the transient response and long-term stability of the far-infrared detectors. Quantitative evaluation of these characteristics {{are the key to}} achieving sensitivity comparable to or better than that for point sources (< 20 – 95 MJy sr- 1). We describe current activity and progress toward the production of high quality images of the diffuse far-infrared emission using the AKARI all-sky survey data...|$|E
50|$|The main {{issues facing}} a power {{engineer}} are reliability and cost. A good design attempts {{to strike a}} balance between these two, to <b>achieve</b> <b>reliability</b> without excessive cost. The design should also allow expansion of the station, when required.|$|R
40|$|For {{survival}} and <b>achieving</b> <b>reliability</b> in ultra long-life missions, fault tolerant design techniques need {{to handle the}} predominant failure mode, which is the wear-out of components. Conventional design methodologies will need excessive redundancy to <b>achieve</b> the required <b>reliability.</b> The objective {{of this paper is}} to present a new approach to design a more efficient fault-tolerant avionics system architecture that requires significantly fewer redundant components...|$|R
40|$|Title from PDF {{of title}} page viewed June 22, 2017 Thesis advisor: Cory BeardVitaIncludes bibliographical {{references}} (pages 26 - 29) Thesis (M. S.) [...] School of Computing and Engineering. University of Missouri [...] Kansas City, 2017 Mission-critical communication {{is one of}} the central design aspects of 5 G communications. But there are numerous challenges and explicit requirements for development of a successful mission-critical communication system. Reliability and delay optimization are the two most crucial among them. <b>Achieving</b> <b>reliability</b> is influenced by several difficulties, including but not limited to fading, mobility, interference, and inefficient resource utilization. <b>Achieving</b> <b>reliability</b> may cost {{one of the most critical}} features of mission critical communication, which is delay. This thesis discusses possible strategies to <b>achieve</b> <b>reliability</b> in a mission-critical network. Based on the strategies, a framework for a reliable mission-critical system has also been proposed. A simulation study of the effects of different pivotal factors that affect communication channel is described. This study provides a better understanding of the requirements for improving the reliability of a practical communication system. Introduction [...] Related works [...] Case studies for mission critical communication [...] Strategies to achieve ultra-reliable M 2 M [...] Adaptive mimo system with OSTBC [...] Simulation results [...] Conclusions and future aspect...|$|R
40|$|Abstract—Testing {{accounts}} for the major percentage of technical contribution in the software development process. Typically, it consumes more than 50 {{percent of the total}} cost of developing a piece of software. The selection of software tests is a very important activity within this process to ensure the software reliability requirements are met. Generally tests are run to achieve maximum coverage of the software code and very little attention is given to the <b>achieved</b> <b>reliability</b> of the software. Using an existing methodology, this paper describes how to use Bayesian Belief Networks (BBNs) to select unit tests based on their contribution to the reliability of the module under consideration. In particular the work examines how the approach can enhance test-first development by assessing the quality of test suites resulting from this development methodology and providing insight into additional tests that can significantly reduce the <b>achieved</b> <b>reliability.</b> In this way the method can produce an optimal selection of inputs and the order in which the tests are executed to maximize the software reliability. To illustrate this approach, a belief network is constructed for a modern software system incorporating the expert opinion, expressed through probabilities of the relative quality of the elements of the software, and the potential effectiveness of the software tests. The steps involved in constructing the Bayesian Network are explained as is a method to allow for the test suite resulting from test-driven development...|$|E
40|$|Abstract — This paper {{presents}} an automatic reliability-aware system-level design methodology to tolerate hardware defects caused by manufacturing tolerances {{as well as}} destructive agents and aging processes at the places of activity of the system components. This is achieved by (1) integrating the capability of a redundant placement of software tasks in an automatic design process {{to cope with the}} hardware defects, (2) providing an automatic lifetime reliability analysis to trade off the arising costs in favor of the <b>achieved</b> <b>reliability</b> increase, and (3) proposing a software architecture for the runtime phase. Real-life case studies from the automotive domain illustrate the effectiveness of the proposed techniques...|$|E
40|$|An {{attempt is}} made to provide simple {{identification}} and description of techniques that {{have proved to be}} most useful either in developing a new product or in improving reliability of an established product. The first reliability task is obtaining and organizing parts failure rate data. Other tasks are parts screening, tabulation of general failure rates, preventive maintenance, prediction of new product reliability, and statistical demonstration of <b>achieved</b> <b>reliability.</b> Five principal tasks for improving reliability involve the physics of failure research, derating of internal stresses, control of external stresses, functional redundancy, and failure effects control. A final task is the training and motivation of reliability specialist engineers...|$|E
5000|$|... emulating new {{hardware}} to <b>achieve</b> improved <b>reliability,</b> security and productivity.|$|R
40|$|BACKGROUND: Performance {{reporting}} is increasingly focused on physician practice sites and individual physicians. OBJECTIVE: To assess {{the reliability of}} performance measurement for practice sites and individual physicians. RESEARCH DESIGN: We used data collected across multiple payers {{as part of a}} statewide measurement collaborative to evaluate the observed measure reliability and sample size requirements to <b>achieve</b> acceptable <b>reliability</b> of 4 Health Care Effectiveness Data and Information Set measures of preventive care and 10 Health Care Effectiveness Data and Information Set measures of chronic care across 334 practice sites. We conducted a parallel set of physician-level analyses using data across 118 primary physicians practicing within a large multispecialty group. MEASURES: Observed reliabilities and estimated sample size requirements to <b>achieve</b> <b>reliability</b> >/= 0. 70. RESULTS: At the practice site level, sample sizes required to <b>achieve</b> a <b>reliability</b> of 0. 70 were less than 200 patients per site for all 4 measures of preventive care, all 4 process measures of diabetes care, and 2 outcomes measures of diabetes care. Larger samples were required to <b>achieve</b> <b>reliability</b> for cholesterol screening in the presence of cardiovascular disease (n = 249) and use of appropriate asthma medications (n = 351). At the physician level, less than 200 patients were required for all 4 measures of preventive care, but for many chronic care measures the samples of patients available per physician were not sufficient to <b>achieve</b> a <b>reliability</b> of 0. 70. CONCLUSION: In a multipayer collaborative, sample sizes were adequate to reliably assess clinical process and outcome measures at the practice site level. For individual physicians, sample sizes proved adequate to reliably measure preventive care, but may not be feasible for chronic care assessment...|$|R
40|$|Abstract — The {{important}} {{property of}} vehicular ad hoc network (VANET) is high dynamic mobility. This characteristic leads the unstable connection of its links among vehicle node. In this scenario <b>achieving</b> high <b>reliability</b> message delivery {{service is a}} big challenge in VANET. There are many routing protocols existing to solve about the challenge {{but they are not}} up to the mark for <b>achieving</b> <b>reliability.</b> Here we improved as new reliable routing protocol to increase reliability which keep the overall network performance by using reconstruction method with alternative path, the performance and reliability of our protocol achieved is accepted result...|$|R
40|$|The aim is {{to develop}} the {{mechanical}} oscillators of the torsional and pendular oscillations with the least possible dissipation and to reveal the factors determining the dissipation. The obtained results on the <b>achieved</b> <b>reliability</b> of the low-frequency mechanical oscillators determine a possibility to formulate the new experiments {{in the field of}} the fundamental physical investigations connected with discovery of the weak actions, specifically, a perspectivity to create the gravitational arrays possessing the sensitivity being sufficient for discovery of the signals forecasted by the astrophysicians. Application field: solids physics, material science, high-sensitive measuring systemsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|In {{the early}} 1970 s, 2 highly publicized {{studies showed that}} {{psychiatric}} diagnosis, {{as it was then}} conducted, was unreliable and inaccurate. The first found that British and U. S. psychiatrists came to different diagnostic conclusions when viewing the same patients on videotape (1). The second found that healthy volunteers claiming to hear voices were admitted to psychiatric hospitals for extended stays despite subsequently acting normally (2). Was psychiatry entitled to a place among the other medical specialties when its diagnoses were so random? The response was quick and effective. The Diagnostic and Statistical Manual of Mental Disorders, Third Edition (DSM-III), published in 1980, featured definitions of mental disorders that, when properly used, <b>achieved</b> <b>reliability</b> equivalent to that of most medical diagnosis. The DSM-III stimulated an outpourin...|$|E
40|$|This paper {{proposes a}} network-layer {{protocol}} for {{wireless sensor networks}} based on the IEEE 802. 15. 4 standard. Our protocol is devised to provide reliable data gathering in latency-constrained applications, and exploits both {{the flexibility of the}} IEEE 802. 15. 4 MAC layer and features of data aggregation techniques, such as implicit acknowledgment of reception. The proposed protocol acts as a routing module and a control entity for the MAC layer and provides reliable communication, while managing power saving and synchronizertion among nodes. Without relying on MAC-layer acknowledgments, the protocol implements caching and network-layer retransmissions, triggered upon detection of a link failure. The performance of the proposed approach is studied through simulations, in which we evaluate the <b>achieved</b> <b>reliability</b> and the energy consumption with varying network settings...|$|E
5000|$|To <b>achieve</b> high <b>reliability</b> and {{performance}} MooseFS offers the following features: ...|$|R
40|$|Infrared {{thermography}} {{is one of}} {{the more}} sophisticated NDT methods recently applied in detecting heat generating anomalies, and has become a major player to <b>achieve</b> <b>reliability</b> and quality for both mechanical and electrical equipments, through the implementation of Equipment Preventive Maintenance Programs, Heat is detected by an infrared camera. The camera translates heat to a visua...|$|R
40|$|Parallel disks {{can improve}} I/O {{performance}} in a manner analogous {{to the use of}} parallel processors to improve computation times. However, due to their data storage function, reliability issues become exceedingly important. Currently proposed schemes use shadowing or parity to <b>achieve</b> <b>reliability</b> in parallel disks. In this paper we introduce the idea of using the Information Dispersal Algorithm (IDA) of Michael O. Rabin [8] to distribute data and redundancy information uniformly among multiple disks and compare the performance and reliability characteristics of shadowing, parity, and IDA. We discuss some ways {{to take advantage of the}} uniformity of data placement and argue that IDA is the algorithm of choice for <b>achieving</b> <b>reliability</b> and performance in parallel disk systems. 1 Introduction Recently, parallel disk systems have emerged as a potential solution for achieving ultra-high capacity, performance and reliability at a reasonably low cost. This emerging technology is likely to hav [...] ...|$|R
40|$|A {{method was}} {{developed}} {{for the construction of}} probabilistic state-space models for nonrepairable systems. Models were developed for several systems which <b>achieved</b> <b>reliability</b> improvement by means of error-coding, modularized sparing, massive replication and other fault-tolerant techniques. From the models developed, sets of reliability and coverage equations for the systems were developed. Comparative analyses of the systems were performed using these equation sets. In addition, the effects of varying subunit reliabilities on system reliability and coverage were described. The results of these analyses indicated that a significant gain in system reliability may be achieved by use of combinations of modularized sparing, error coding, and software error control. For sufficiently reliable system subunits, this gain may far exceed the reliability gain achieved by use of massive replication techniques, yet result in a considerable saving in system cost...|$|E
40|$|From {{state of}} the art {{technologies}} for load balancing and reliability increase in VoIP infrastructures one learns that no homogenous solution exists. Thus compromises have been made as in the number of machines versus <b>achieved</b> <b>reliability,</b> or with regard to separation of load balancing and failure resilience. In this paper we present a distributed architecture for VoIP services based on peer-to-peer principles. In contrast to other peer-to-peer based approaches, relying upon an external DHT service, our architecture utilizes peer-to-peer technologies to integrate load balancing and failover requirements with a centralized VoIP server concept. This results in an integrated solution with focus on todays service provider requirements. Within the scope of this work, a proof-of-concept software solution has been designed and implemented. We show that the proposed architecture can be easily scaled up and provides efficient, redundant storage for user and session data...|$|E
40|$|Over {{a period}} of 10 years, we have {{developed}} a sustainable process of online portfolio assessment that demonstrates both reliability and validity, using both qualitative and quantitative measures. The sustainable cycle is that, each semester, we assess a random sampling of the students ’ work that they have posted, as per our instructions, in an online portfolio. During the reading, the faculty score the documents for 11 variables, including writing, content, audience awareness, and document design. We achieved validity by a modi-fied online Delphi {{that led to a}} redefinition of the construct of technical communication itself; we <b>achieved</b> <b>reliability</b> by adjudication resulting in adjacent scores. The results of our assessment meet the requirements of ABET and result in a continual cycle of improvement for our technical communication curriculum. Results from three semesters show an improving correlation between the course grade and the overall, holistic portfolio score...|$|E
50|$|The Shannon-Hartley theorem {{establishes}} {{what that}} channel capacity {{is for a}} finite-bandwidth continuous-time channel subject to Gaussian noise. It connects Hartley's result with Shannon's channel capacity theorem {{in a form that}} is equivalent to specifying the M in Hartley's line rate formula in terms of a signal-to-noise ratio, but <b>achieving</b> <b>reliability</b> through error-correction coding rather than through reliably distinguishable pulse levels.|$|R
40|$|One of the {{fundamental}} limits to high-performance, high-reliability file systems is memory's vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve {{the best of both}} worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To <b>achieve</b> <b>reliability,</b> we protect memory during a crash and restore it during a reboot (a "warm" reboot). Extensive crash tests show that even without protection, warm reboot enables memory to <b>achieve</b> <b>reliability</b> close to that of a write-through file system while perfo [...] ...|$|R
5000|$|Compressors: How to <b>Achieve</b> High <b>Reliability</b> and Availability, (with F.K. Geitner), McGraw-Hill,New York, NY, 2012 ...|$|R
