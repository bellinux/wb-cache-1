23|0|Public
50|$|The French {{developed}} an OTH radar called NOSTRADAMUS during the 1990s (NOSTRADAMUS stands for New Transhorizon Decametric System Applying Studio Methods (French: nouveau système transhorizon décamétrique <b>appliquant</b> les méthodes utilisées en studio).) In March 1999, the OTH radar NOSTRADAMUS {{was said to}} have detected two Northrop B2 Spirit flying to Kosovo. It entered service for the French army in 2005, and is still in development. It is based on a star shaped antenna field, used for emission and reception (monostatic), and able to detect every aircraft at a range of more than 2,000 kilometers, in a 360 degree arc. The frequency range used is from 6 to 30 MHz.|$|E
40|$|International audienceA {{study of}} the {{functioning}} of envy in The Merry Wives of Windsor by Shakespeare, applying but seeking {{to go beyond the}} theory of René Girard. Etude du functionnement de l'envie dans Les joyeuses commères de Windsor de Shakespeare, en <b>appliquant</b> mais dépassant la théorie de René Girard...|$|E
40|$|What {{would explain}} the {{political}} history in Canada as the initial question might ask, while the answers that follow range from the fairly unique to the rather ubiquitous. By applying then the principles from world-systems theory to those of “the control model, ” recurrent paradoxes in Canada—about a centralized confederation, the “mature-dependent ” economy, consensual democracy despite Westminster institutions, a two-party system without “alternation ” (more liberal, less conservative regimes), and multiple sovereign nations (First, Francophone, Anglophone) amid one hegemonic state—could now resolve themselves consistently as a single case-study via the comparative and international, historicist, context. In the final analysis, while no one account could fare any better or worse than all others, preliminary findings from Canada’s political history amount to the very organized effects—about interdisciplinary scope and methods—whose causes are just “out of control. ” Résumé Que expliquerais l’histoire politique dans Canada comme la question initial mai demander, pendant les réponses postéieur inclure assez unique et trés univesel. <b>Appliquant</b> les principes dè...|$|E
40|$|ABSTRACT. Tree Adjoining Grammar (TAG) is {{a useful}} {{formalism}} for describing the syntactic structure of natural languages. In practice, {{a large part of}} wide coverage TAGs is formed by trees that satisfy the restrictions imposed by Tree Insertion Grammar (TIG), a simpler formalism. This characteristic can be used to reduce the practical complexity of TAG parsing, applying the standard adjunction operation only in those cases in which the simpler cubic-time TIG adjunction cannot be applied. A major obstacle to this task is posed by the fact that simultaneous adjunctions are forbidden in TAG but they are allowed in TIG. In this article, we describe several algorithms for mixed parsing of TAG and TIG: a first one forbidding simultaneous adjunctions, a second one allowing this kind of adjunctions, and a third one which extends the second one to preserve the correct prefix property. RÉSUMÉ. La Grammaire d’Arbres Adjoints (TAG) est un formalisme utile pour décrire la structure syntaxique des langues naturelles. En pratique, la plupart des TAG à large couverture contiennent des arbres qui satisfont les restrictions imposées par la Grammaire d’Insertion d’Arbres (TIG), qui est un formalisme plus simple. Cette caractéristique peut être employée pour réduire la complexité pratique de l’analyse TAG, en <b>appliquant</b> l’opération d’adjonctio...|$|E
40|$|International audienceThis {{paper has}} a methodological purpose, as {{it seeks to}} {{understand}} the influence of pre knowledge on research design in management studies. In applying the insider/outsider model initially developed by Goodenough (1956) and Pike (1967), I analyse the influences of membership in the studied group on empirics and choice of a theoretical framework and methods. The conclusion drawn is that the pre knowledge implied by the insider or outsider status can have a strong influence on how research is conducted. Yet, the model is proved to be neither deterministic nor exclusive, if the researcher upstream develops a reflexive process whereby {{he or she can}} swap from one relation to the other. Cet article a un objectif méthodologique et cherche à comprendre l'influence de la pré-connaissance de l'objet sur le design de recherche en sciences de gestion. <b>Appliquant</b> le modèle insider/outsider initialement développé par Goodenough (1956) et Pike (1967), cette étude analyse les influences de l'appartenance au groupe étudié sur l'empirie ainsi que les choix théoriques et méthodologiques. Si la pré-connaissance offerte par le statut d'insider ou outsider peut avoir une influence très forte sur la manière dont est conduite la recherche, ce modèle ne s'avère ni déterministe ni exclusif, à condition que le chercheur procède à une réflexion préalable lui permettant de passer d'une relation à l'autre...|$|E
40|$|LE MUSCLE STRIE SQUELETTIQUE PRESENTE A L'ETAT D'ACTIVITE DES PROPRIETES VISCOELASTIQUES COMPLEXES. A CE JOUR, PEU DE TRAVAUX SE SONT INTERESSES A CES PROPRIETES VISQUEUSES, POURTANT A LA BASE DE LA CONTRACTION DYNAMIQUE. L'ETUDE PRESENTEE DANS CE MEMOIRE A ESSENTIELLEMENT PORTE SUR CE THEME. DEUX TYPES DE CONTRACTION ONT ETE ENVISAGES EN <b>APPLIQUANT</b> AU MUSCLE ACTIVE DES MOUVEMENTS ISOCINETIQUES OU SINUSOIDAUX. LES PHENOMENES MECANIQUES REGISSANT DE TELLES CONTRACTIONS ONT ETE ANALYSES SUR SOLEUS DE RAT TESTE EN CONDITION ISOLEE. AINSI, UN FACTEUR DE VISCOSITE ANALOGUE AYANT POUR ORIGINE LA CONVERSION D'ENERGIE CHIMIQUE EN ENERGIE MECANIQUE QUI S'OPERE LORS DE LA CONTRACTION MUSCULAIRE A ETE DEFINI. CETTE ETUDE A ENSUITE ETE RECONDUITE SUR MUSCLE HUMAIN IN SITU. LES TESTS MECANIQUES ONT ETE APPLIQUES AUX FLECHISSEURS PLANTAIRES A L'AIDE D'UN ERGOMETRE-CHEVILLE. L'ETUDE DETAILLEE DES DEUX MODALITES DE CONTRACTION NOUS A AMENE A DISTINGUER LA CONTRIBUTION D'UNE VISCOSITE ANALOGUE MUSCULAIRE ET D'UN COEFFICIENT D'AMORTISSEMENT DE STRUCTURES MUSCULO-ARTICULAIRES. LA CARACTERISATION DE LA PROPRIETE DE VISCOSITE ANALOGUE CHEZ L'HOMME NECESSITE LA MISE EN PLACE DE RELATIONS COUPLE-VITESSE ETABLIES A PARTIR DE CONTRACTIONS ISOCINETIQUES MAXIMALES DELICATES A REALISER. LA DERNIERE ETAPE DE NOS TRAVAUX S'EST ATTACHEE A TROUVER UN PROTOCOLE EXPERIMENTAL PLUS SIMPLE CONDUISANT A L'EXPRESSION DE CETTE PROPRIETE. A CET EFFET, NOUS AVONS DEVELOPPE UNE METHODE DE CALCUL PERMETTANT DE MODELISER UNE RELATION COUPLE-VITESSE REPRESENTATIVE DU SYSTEME CONTRACTILE A PARTIR DE TESTS REALISES SOUS CONTRACTION QUASI-ISOMETRIQUE SOUS-MAXIMALE. CE MODELE MATHEMATIQUE A ENSUITE ETE VALIDE EXPERIMENTALEMENT EN CONFRONTANT LES RELATIONS COUPLE-VITESSECOMPIEGNE-BU (601592101) / SudocSudocFranceF...|$|E
40|$|Corpus "AGORA"This {{conference was}} the closing {{event of a}} three-month stay of a team {{selected}} by the International Programme for Advanced Studies conducted by the Fondation Maison des Sciences de l'Homme (France) in collaboration with Columbia University (USA). At Fall 2008, Vasant Kaiwar and Sucheta Mazumdar from Duke University (USA), Carlos Antonio Aguirre Rojas from Universidad Nacional Autónoma de México (Mexico) and Thierry Labica from Paris X University (France) met for joint research on « Structure and Categories of Knowledge Production : The Universal and the Particular ». While the project has addressed a number of issues, with the participation of additional scholars, the team members submit here four presentations on « Existing Globalization and its Alternatives » commented upon by academics from Paris and Mexico : « Globalization’s Grand Narratives of Regress : Women, Religion and its Restoration » (Labica); « Race, Civilizational Model and Eugenics Thought in China » (Mazumdar); « What is Postcolonialist Orientalism and How does it Matter ? » (Kaiwar) and finally « Political Lessons of the New Anti-systemic Movements in Latin America » (Aguirre Rojas). Pour Carlos Antonio AGUIRRE ROJAS, il nous faut repenser la définition de la démocratie, à partir de ses racines grecques : démos (peuple), cratos (gouvernement). Mais selon lui, dans toute l’Histoire de l’humanité, il n’y a jamais eu de réel « gouvernement du peuple ». Sauf, quelques exceptions (la Commune de Paris, les premiers « Soviets » avant la révolution russe, le début de la révolution culturelle en Chine, ou aujourd’hui, le mouvement zapatiste). Mais comment arriver à aller au-delà de ces « impostures démocratiques » que ce sont les gouvernements basés sur la représentation ? En <b>appliquant</b> les principes des mouvements sociaux anti-systémiques actuels...|$|E
40|$|L'OBJECTIF DE CE TRAVAIL CONSISTE A OPTIMISER LES CONDITIONS D'EXPLOITATION D'UNE RIGOLE PRINCIPALE DE HAUT FOURNEAU SUR LA BASE DE CRITERES MECANIQUES. CETTE STRUCTURE REFRACTAIRE SUBIT D'INTENSES AGRESSIONS CHIMIQUES ET MECANIQUES GENEREES PAR LE CONTACT DE LA FONTE ET DU LAITIER PORTES A 1500. POUR COMMENCER, LES INFORMATIONS PERMETTANT DE TRAITER CE PROBLEME SONT REUNIES. LA NECESSITE DE CARACTERISER LE COMPORTEMENT THERMOMECANIQUE DU BETON REFRACTAIRE ALUMINEUX A TRES BASSE TENEUR EN CIMENT CONSTITUANT LA COUCHE D'USURE DE LA RIGOLE EST DEMONTREE. UN DISPOSITIF INEDIT D'ESSAIS MECANIQUES A HAUTE TEMPERATURE A DONC ETE CONCU. IL PERMET D'APPLIQUER DES EFFORTS ATTEIGNANT 400 KN, DE 20 A 1500. CETTE DEMARCHE EST EXPOSEE DANS LE CHAPITRE 4. DANS LE CHAPITRE 5, LES RESULTATS DES ESSAIS DE COMPRESSION, FENDAGE ET FLUAGE EFFECTUES DE 20 A 1300, EN ATMOSPHERE OXYDANTE OU REDUCTRICE, SONT PRESENTES. UN SOIN PARTICULIER EST ACCORDE A L'ETUDE DU MODE OPERATOIRE. LE COMPORTEMENT PASSE DE ENDOMMAGEABLE A L'AMBIANTE A VISCOPLASTIQUE-ENDOMMAGEABLE A CHAUD. LE CHAPITRE 6 MODELISE LE COMPORTEMENT THERMOMECANIQUE DU MATERIAU ETUDIE A L'AIDE DU MODELE DE MAZARS - ELASTICITE COUPLEE A L'ENDOMMAGEMENT - DONT LES PARAMETRES ONT ETE IDENTIFIES SUR TOUTE LA PLAGE D'ESSAIS. LA LOI DE NORTON DECRIT LE CARACTERE VISCOPLASTIQUE DU MATERIAU. DANS LE CHAPITRE 7, LA SIMULATION PAR ELEMENTS FINIS DU COMPORTEMENT THERMOMECANIQUE DE LA RIGOLE EST REALISEE GRACE AU LOGICIEL CASTEM 2000. CE CHAPITRE DEBUTE PAR L'ETUDE DU TRANSFERT DE CHALEUR QUI FIXE L'INTENSITE DES CONTRAINTES THERMIQUES. PUIS, ON DEMONTRE PAR DES CALCULS THERMOMECANIQUES QUE LA VISCOPLASTICITE INFLUENCE PEU L'INTENSITE DES CONTRAINTES. EN CONSEQUENCE, LA DEMARCHE D'OPTIMISATION S'EFFECTUE EN <b>APPLIQUANT</b> LE MODELE DE MAZARS. QUELQUES EXEMPLES SIMPLES LEGITIMENT LE TRAVAIL EFFECTUE EN MONTRANT QU'IL EST POSSIBLE DE DEFINIR RATIONNELLEMENT LA GEOMETRIE ET LES CONDITIONS D'EXPLOITATION DE LA RIGOLE GRACE AUX OUTILS DEVELOPPES. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|E
40|$|LA PROTEIN DATA BANK POSSEDE PLUS DE 11000 STRUCTURES DONT SEULEMENT 4 % SONT DES STRUCTURES DE COMPLEXES. DEUX METHODES MAJEURES PERMETTENT ACTUELLEMENT DE DETERMINER DE TELLES STRUCTURES. LA CRISTALLOGRAPHIE EST LIMITEE PAR L'ETAPE DE CO-CRISTALLISATION TANDIS QUE LA RESONANCE MAGNETIQUE NUCLEAIRE EST LIMITEE PAR LA MASSE MOLECULAIRE DES PROTEINES. L'ARRIMAGE MOLECULAIRE SOUS CONTRAINTES RMN PEUT ETRE UNE ALTERNATIVE A CES LIMITATIONS. L'EXEMPLE DU COMPLEXE CYTOCHROME C 5 5 3 /FERREDOXINE ETANT PARFAITEMENT INTEGRE DANS L'APPROXIMATION DES CORPS RIGIDES (PEU DE CHANGEMENTS CONFORMATIONNELS ENTRE LA FORME LIBRE ET LA FORME COMPLEXEE), NOUS AVONS MARQUE LES DEUX MOLECULES A L'AZOTE 1 5 N AFIN D'ENTREPRENDRE UNE ETUDE DE LA FORMATION DU COMPLEXE AVEC LE PARTENAIRE (NON MARQUE) PAR RMN HETERONUCLEAIRE. NOUS AVONS AINSI OBTENU LE SITE D'INTERACTION DE LA FERREDOXINE (NON MARQUEE) SUR LE CYTOCHROME (MARQUE) ET VICE-VERSA. LA DERNIERE ETAPE A CONCERNE L'ETUDE DE LA MODELISATION DE LA STRUCTURE TRIDIMENSIONNELLE PAR ARRIMAGE MOLECULAIRE SOUS CONTRAINTES RMN. NOUS AVONS INTEGRE UN NOUVEAU MODULE PERMETTANT D'INJECTER LES VALEURS DES DEPLACEMENTS CHIMIQUES OBTENUS PAR RMN HETERONUCLEAIRE, DANS LE LOGICIEL BIGGER. NOUS PROPOSONS ICI UN MODELE TRIDIMENSIONNEL CONCERNANT CE COMPLEXE D'OXYDOREDUCTION. CETTE APPROCHE OUVRE LA VOIE A TOUS LES AUTRES COMPLEXES REPONDANT AUX CONDITIONS D'APPROXIMATION DES CORPS RIGIDES. NOUS AVONS ENSUITE APPLIQUE CETTE STRATEGIE AU COMPLEXE HYDROGENASE / CYTOCHROME C 5 5 3 (COMPLEXE PHYSIOLOGIQUE DE 60 KDA) EN <b>APPLIQUANT</b> UNE NOUVELLE SEQUENCE D'IMPULSION DE RMN HETERONUCLEAIRE, LE TROSY, QUI PERMET DE REALISER L'ETUDE DE COMPLEXE A HAUT POIDS MOLECULAIRE PAR RMN EN DIMINUANT L'EFFET D'ELARGISSEMENT DE LA LARGEUR DE RAIE OBSERVEE. CETTE ETUDE A CONDUIT AU PREMIER MODELE STRUCTURAL DE CE COMPLEXE. LE CHEMIN DE TRANSFERT D'ELECTRON TROUVE IMPLIQUE LA CYSTEINE 10 DU CYTOCHROME ET LA CYSTEINE 38 DE L'HYDROGENASE. LA DISTANCE HEME/FES ETANT DE L'ORDRE DE 12 A. ORSAY-PARIS 11 -BU Sciences (914712101) / SudocSudocFranceF...|$|E
40|$|Cette thèse présente une étude des propriétés électroniques des objets nano sur des {{surfaces}} métalliques par la microscopie et la spectroscopie à effet tunnel (STM et STS), qui est un outil unique pour étudier les propriétés physiques {{en fonction}} de la structure à l’échelle nanométrique. À cet égard, des études du transport électronique ou du transport du spin à travers une molécule en fonction de sa configuration géométrique sur le substrat sont possible. Dans ce travail de thèse, l’intérêt a été focalisé sur différents phénomènes clés de la conductance électronique à travers des objets nanométrique. Nous commençons avec une caractérisation approfondie des substrats, qui nous permet de bien distinguer entre des effets proprement liés à l’interaction molécule-surface et celles liés au substrat. Des atomes individuels ainsi que des molécules de Phthalocyanine de cobalt ou de C 60 sont étudiés en <b>appliquant</b> des techniques à la pointe de la technologie de STM et STS, comme l’imagerie par manipulation d’atomes ou la cartographie de la conductance différentielle à une hauteur constante, en comparant toujours les résultats avec des calculs théoriques. One {{of the most}} urgent tasks {{in the field of}} molecular electronics is to understand and tune the contact between molecules and a metal lead. Scanning tunneling microscopy and spectroscopy (STM and STS) represent a unique tool to correlate electronic and structural properties at the nanoscale, such as the impact of geometry on the spin-transport through a single molecule. In this perspective, the work presented here employs STM/STS as well as spin-polarized STM/STS to focus on key phenomena of the conductance of nanoscaled objects, e. g. metallic islands, single atoms and molecules, the results always being compared to theoretical calculations. After a brief introduction of the STM/STS techniques employed, Chapter 1 provides some technical details regarding the sample and tip preparation. Chapter 2 is dedicated to the characterization of the Cu(111) surface and of cobalt islands grown on this surface. Chapter 3 focuses on individual atoms adsorbed on cobalt islands. The polarization of the local density of states (LDOS) above the atoms is discussed in relation with the polarization of the surface. Surface induced states and atomic resonances are shown to determine the LDOS above the atom, as well as the spin-polarization around the Fermi energy. In the last two chapters, results are obtained on Cobalt-Phthalocyanine and C 60, both molecules being model systems for future molecular electronic devices. Molecular conductance channels and the spin-polarization of a single molecule (Chapter 4), as well as a negative differential conductance, which originates from an energy-dependent match and mismatch of molecular orbitals and electronic tip states (Chapter 5), are revealed...|$|E
40|$|En <b>appliquant</b> la notion de système aux {{institutions}} scolaires, l’auteur établit les grandes lignes de ce que serait une administration scolaire plus humaine et redéfinit le rôle de l'administration du personnel dans ce nouveau contexte. In {{the course}} of the last decade, many changes took place in the education system of the Province of Québec. A network of organizations was set up so complex that it is almost impossible for people to get an overview of the structure and functionning of the entire system. The system concept is used because it invites people to focus their attention on the outputs or results to be achieved instead of looking only at the activities and the resources necessary for their accomplishment. Educating children and helping adults to adapt to a continuously changing environment are the main results that the population expects from its school system. Personnel management in this context can be looked upon as a sub-system whose output is to provide productive, stable and motivated human resources. That is the way the role of the personnel department is usually defined. The purpose in this paper is to reformulate the role of the personnel department in the light of a school system that can be made more human. But, first, is it possible to reach an agreement on what is meant by a « more human school system » ? I think a system is human when it gives to the people engaged in it the opportunity of growing, to develop themselves by doing meaningful work, that is, work that gives to somebody a feeling of self-identity and self-esteem. Going back to Maslow's needs hierarchy and Herzberg's two-factor theory, it is possible to identify two levels of humanization of a school system. The first one encompasses the conservation needs and the conditions of work that could satisfy this category of needs. Herzberg calls these conditions « hygienic factors ». The second level comprises the growth needs of individual that could be fulfilled by « motivational factors », that is, conditions of work that enhance self-esteem and self-actualization. Keep these two levels in mind, it is now possible to reformulate the role of a personnel department within a school system. In the short-run, the personnel department must keep on providing services and assistance to other departments in the school system. These services (recruitment, selection, training, contract administration, compensation, etc [...] .) are necessary to keep people within the system but fall short of motivating them to do their best. In performing this role, the personnel department help creating and maintaining working conditions that satisfy first-level needs of individuals. But, the personnel department can perform a larger role by « getting out of the kitchen work and enter the living-room ». This means that the personnel department must get a broaden outlook and play the role of a « change-agent » within the entire system. This can be accomplished by helping people in authority position to question their philosophy and behavior about managing other people. This also implies helping to create conditions for growth or self-actualization at each level of the organization. To play the role of internal change-agent, the personnel manager of educational institutions will have to be familiar with new ideas drawn from the behavioral sciences which are already put into practice in business and industry...|$|E
40|$|Today popular {{centralized}} Online Social Networks (OSN) such as Facebook or Twitter process {{massive amounts}} of information associated with user content in their platforms. Their approach creates several threats for the privacy of users. Firstly, user data can leak to authorized (e. g., advertising) or unauthorized third parties (external entities). Secondly, centralized OSN are prone to censorship. On the other hand, future decentralized OSN (DOSN) remove the central authority for data management and so effectively such threats. However, they require careful routing to be resilient to failures, access control and data storage at the peer level. This thesis investigates privacy-preserving protocols that aid in detecting and discouraging abusive behavior in future DOSN. In such settings, less metadata is available to participants for analysis. However, to detect abuse {{we may need to}} make use of metadata that represents neighborhood knowledge, namely a social graph or network size/structure. Thus we need to provide privacy-preserving protocols that protect such metadata and are compatible with decentralized settings. We first analyze abusive behavior in Twitter, an existing centralized, subscription based OSN platform. The data model of Twitter is a publish-subscribe messaging infrastructure that allows participants publishing and subscribing to various types of notifications (e. g., news, sports). At the individual level, we conjecture that attackers {{may be more likely to}} abuse potential victims if they can address them via the OSN interface with tools as mentions (e. g., tagging) in Twitter. To verify abuse at the individual level, we start collecting messages directed to potential victims using a data mining framework we build and that programmatically crawls Twitter APIs. We managed to retrieve in the order or hundreds of thousands of tweets and metadata of millions of social relationships from Twitter. Then, we extract a number of features, namely individual measurable properties related to the dataset. To obtain abuse ground truth, we develop a light-weight web platform that provides effective and customizable crowdsourcing of label annotation for a sample of our dataset. The sample contains messages directed towards potential victims of abuse and we ask humans to label the nature of messages following a set of abuse guidelines that provide a non-binary classification choice (undecided, abusive, acceptable). Initially, we consider the problem of abuse classification in a centralized OSN (Twitter). Next, we abstract from the Twitter model and consider DOSN where locally, users only have access to a partial view of the metadata available in the network to perform abuse detection. In turn, we analyze the impact of enforcing privacy into features involving neighborhood knowledge, which requires collection and computation of social graph metadata that is not available to the user locally. In order to use these features in DOSN, we design a signed Private Set Intersection (PSI) protocol that protects participant metadata collected and computed by the PSI. In addition, we analyze the resistance of our protocol against adversaries trying to tamper with the value of the PSI features. Finally, we perform data minimization by approximating PSI features and testing supervised learning algorithms for abuse detection. Our results show that approximation of the neighborhood fingerprint that PSI features use for abuse detection is still useful and compatible with DOSN. Le principal objectif de cette thèse est d'évaluer les protocoles qui prennent en considération la protection de la vie privée et qui nécessitent seulement des métadonnées locales pour détecter les comportements malveillants sur les réseaux sociaux décentralisés. En <b>appliquant</b> des techniques d'analyse de réseaux sociaux qui réduisent la quantité de métadonnées sensibles, nous obtenons des résultats acceptables comparé aux techniques qui ne préservent pas la vie privée. De plus, nous prévoyons d'élaborer une série de recommandations pour construire de futurs réseaux sociaux décentralisés qui découragent ce type des comportements abusifs...|$|E
40|$|This thesis {{presents}} several attacks which dismantle several code-based encryption schemes. We {{started by}} a cryptanalysis of {{a modified version}} of the Sidelnikov cryptosystem proposed by Gueye and Mboup which is based on Reed-Muller codes. This modified scheme consists in inserting random columns in the secret generating matrix or parity check matrix. The cryptanalysis relies on the computation of the square of the public code. The particular nature of Reed-Muller which are defined by means of multivariate binary polynomials, permits to predict the values of the dimensions of the square codes and then to fully recover in polynomial time the secret positions of the random columns. Our work shows that the insertion of random columns in the Sidelnikov scheme does not bring any security improvement. The second result is an improved cryptanalysis of several variants of the GPTcryptosystem which is a rank-metric scheme based on Gabidulin codes. We prove that any variant of the GPT cryptosystem which uses a right column scrambler over the extension field as advocated by the works of Gabidulin et al. with the goal to resist Overbeck’s structural attack, are actually still vulnerable to that attack. We show that by applying the Frobenius operator appropriately on the public key, it is possible to build a Gabidulin codehaving the same dimension as the original secret Gabidulin code, but with a lower length. In particular, the code obtained by this way corrects less errors than the secret one but its error correction capabilities are beyond the number of errors added by a sender, and consequently an attacker is able to decrypt any ciphertextwith this degraded Gabidulin code. We also considered the case where an isometric transformation is applied in conjunction with a right column scrambler which has its entries in the extension field. We proved that this protection is useless both in terms of performance and security. To finish, we studied the security of the Faure-Loidreau encryption scheme which is also a rank-metric scheme based on Gabidulin codes. Inspired by our precedent work and, although the structure of the scheme differs considerably from the classical setting of the GPT cryptosystem, we show that for a range of parameters, this scheme is also vulnerable to a polynomial-time attack that recovers the private key by applying Overbeck’s attack on an appropriate public code. As an example we break in a few seconds parameters with 80 -bit security claim. Cette thèse porte sur l'étude de la sécurité de plusieurs protocoles cryptographiques fondés sur la théorie des codes correcteurs d’erreurs. Le premier résultat porte sur la sécurité d’une version modifiée du cryptosystème de Sidelnikov, proposée par Gueye et Mboup et basée sur les codes de Reed-Muller. Nous montrons que l’insertion de colonnes aléatoires dans le schéma de Sidelnikov n’apporte aucune amélioration en matière de sécurité. Le résultat suivant est une cryptanalyse améliorée de plusieurs variantes du cryptosystème GPT qui est un schéma de chiffrement en métrique rang utilisant les codes de Gabidulin. Nous montrons qu’en utilisant le Frobenius de façon appropriée sur le code public, il est possible d’en extraire un code de Gabidulin ayant la même dimension que le code de Gabidulin secret mais, avec une longueur inférieure. Le code obtenu corrige ainsi moins d’erreurs que le code secret, mais sa capacité de correction d’erreurs dépasse le nombre d’erreurs ajoutées par l’expéditeur et par conséquent, un attaquant est capable de déchiffrer tout texte chiffré, à l’aide de ce code de Gabidulin dégradé. Nos résultats montrent qu’en fin de compte, toutes les techniques existantes visant à cacher la structure algébrique des codes de Gabidulin ont échoué. Enfin, nous avons étudié la sécurité du système de chiffrement de Faure-Loidreau, qui est également basé sur les codes de Gabidulin. Bien que la structure de ce schéma diffère considérablement du cadre classique du cryptosystème GPT, nous montrons que ce schéma est également vulnérable à une attaque polynomiale qui récupère la clé privée en <b>appliquant</b> l’attaque d’Overbeck sur un code public approprié...|$|E
40|$|In {{order to}} {{facilitate}} driving and prevent accidents due to lane departure, this thesis focuses {{on the development of}} an assistance device for lane keeping of a passenger vehicle. The review of the literature on this subject has highlighted the need for {{a better understanding of the}} interactions between the driver and the vehicle-road system. The goal is to develop a shared control mode based on the anticipation of risk and the prediction of the most probable actions of the driver. The thesis was mainly articulated around two axes: the cybernetic modeling of the driver in his task of lateral control of the vehicle, and the design of a shared steering control using a Driver-Vehicle-Road model (DVR). The proposed driver model is consistent with what is known about sensorimotor and cognitive control in humans. It represents the anticipatory and compensatory visual control of steering, as well as the execution of steering actions by the neuromuscular system. It has been identified from experimental data collected on a fixed-base driving Simulator SCANeRTM. The model showed a good fit to the driver behavior, supporting later the development of the global model DVR. Concerning the shared control, two experimental studies were performed. The first (socalled CoLat 1) is similar to already existing LKS systems. CoLat 1 is primarily designed as an "electronic pilot" that has to drive alone the vehicle. Without using a driver model, it results from applying the H 2 -preview control synthesis to a vehicle-road model. Shared steering is performed only in the implementation phase: the control that it provides is partially applied to the steering wheel leaving the complementary control part to be applied by the driver. The second (so-called CoLat 2) addresses a shared control resulting from applying the H 2 -preview control synthesis to the global model (DVR). The H 2 /LQ-Preview control law has been used in the both cases (CoLat 1 & 2) because it allows to include knowledge of the future road curvature and to consider relevant performance criterions. A generalization of the H 2 -Preview solution was accomplished in the case where the disturbance signal is predicted beyond the preview horizon on the basis of a generator model. Innovative criterions were proposed for assessing the risk of lane departure, the sharing level between the driver and the lateral assistance, as well as its cooperative or conflicting behavior. They have been formalized through the criteria supporting the synthesis of CoLat 2, enabling therefore a precise management of trade-off between the quality of lane keeping and the quality of driver-assistance interaction. CoLat 2 has shown better performance compared to CoLat 1 in terms of driver-assistance cooperation thanks to "online" feedback of the estimated driver state. The μ-Analysis of the overall system robustness with respect to the driver model uncertainties has shown that the stability is guaranteed for a wide range of parametric variations. Afin de faciliter la conduite et prévenir les accidents par sortie de voie, ce travail porte sur le développement d'un dispositif d'assistance au contrôle latéral de la trajectoire d'un véhicule automobile. L'état de l'art des travaux portant sur ce thème a mis en évidence la nécessité de mieux prendre en compte l'interaction du conducteur avec l'ensemble véhicule-route. Ceci nous a conduit à développer un mode de contrôle partagé basé sur l'anticipation du risque et sur la prédiction des actions les plus probables du conducteur. Le travail de la thèse a été articulé principalement autour de deux axes : la modélisation cybernétique du conducteur dans sa tâche de contrôle latéral, et la conception du contrôle partagé de la direction sur la base du modèle global CVR (conducteur-véhicule-route). Un modèle du conducteur a été développé en cohérence avec les connaissances actuelles sur la sensorimotricité humaine. Ses paramètres ont été identifiés en utilisant des données expérimentales issues d'un simulateur de conduite SCANeRTM. Ensuite, Deux stratégies du contrôle partagé ont été conçues et évaluées expérimentalement. La première (nommée CoLat 1) se repose sur un principe similaire à celui des assistances LKS commercialisées à ce jour. Sa conception repose sur l'utilisation d'un modèle véhicule-route, et ne fait appel à aucun modèle du conducteur. Le partage n'est fait que de la mise en œuvre en <b>appliquant</b> partiellement la commande qu'il produit. La deuxième (nommée CoLat 2) repose sur une conception tenant compte des modalités du partage de la conduite avec le conducteur. Sa conception repose sur l'utilisation d'un modèle global (CVR). La commande H 2 /LQ anticipative (with preview) a été choisie dans les deux cas (CoLat 1 & 2) pour ce qu'elle permet la prise en compte de la connaissance anticipée de la courbure de la route et la définition de critères de performance pertinents. Elle a été revisitée pour tenir compte de modèles prédicteur des signaux perturbateurs...|$|E
40|$|Le tritium, isotope radioactif de l'hydrogène, est considéré comme un élément dateur des eaux souterraines. Il permet à l'hydrogéologue d'estimer {{le temps}} de séjour moyen des eaux d'un aquifère. Produit naturellement dans l'atmosphère, les teneurs dans les précipitations ont tout d'abord augmenté, suite aux essais thermonucléaires aériens de 1952 à 1963, pour ensuite diminuer et se {{stabiliser}} des industries nucléaires, supérieure à la teneur naturelle de 5 UT. Actuellement, la teneur moyenne se situe entre 10 et 30 UT. Grâce au suivi des teneurs en 3 H des précipitations, il est possible de déterminer le temps de séjour moyen des eaux souterraines d'un aquifère à l'aide de modèles mathématiques qui, à partir d'un signal d'entrée (teneurs en 3 H dans les précipitations) et d'un signal de sortie (teneurs en 3 H dans les eaux souterraines d'un aquifère), calculent le meilleur ajustement possible entre les sorties calculées et les valeurs mesurées. De cet ajustement, on en déduit le temps de séjour moyen. Ces études isotopiques utilisent classiquement trois types de modèles suivant la façon dont les eaux se mélangent en traversant un milieu poreux: le modèle mélange ou exponentiel, le modèle piston et le modèle dispersif. Cet ensemble de modèle peut être généralisé en différenciant temps de séjour moyen et age d'une molécule d'eau. Ces modèles ont été utilisés pour étudier les systèmes des sources minérales de la Versoie et d'Evian (Haute-Savoie, France) en <b>appliquant</b> un modèle mélange à la source de la Versoie et un modèle dispersif à la source d'Evian, ce qui a permis d'estimer leur temps de séjour. Tritium, the radioactive isotope of hydrogen, is {{considered as a}} dating element for ground-water and allows the hydrogeologist to evaluate the aquifer waters average transit time T. This parameter is important for questions about the evaluation, exploitation and protection of water resources. Naturally produced in the atmosphere {{by the action of}} the neutronic component of cosmic radiation on nitrogen, 3 H is present in precipitation at levels of about 5 UT. As a result of thermonuclear aerial tests from 1952 to 1980, the 3 H content of precipitation increased markedly, reaching a maximum in 1963 of about 2500 UT and decreasing thereafter to about 50 UT in 1980. Since 1980, thermonuclear aerial tests have stopped but civil thermonuclear stations continue to influenced the 3 H content of precipitation, which currently lies between 10 and 30 UT. Thanks to the regular analysis of the tritium content of precipitation, it is possible to evaluate aquifer ground-water transit times, T, with mathematical models. This approach uses an "input signal" (3 H contents in precipitation) and an "output signal" (3 H contents in aquifer ground-water) and calculate the best fit between the calculated output and the measured values. This best fit yields an estimate of the transit time T. According to {{the manner in which the}} waters mix as they pass through a porous media, three kinds of models are used: the well-mixed or exponential model, which supposes that water introduced into the system is completely and uniformly mixed with the aquifer water, such that the water output from the system is representative of the aquifer waters; the piston-flow model, which supposes that the incoming water traverses the aquifer with a constant velocity; and the dispersion model, which supposes a dispersion phenomenon due to the heterogeneity of the aquifer material. The piston-flow model and the exponential model represent the two possible extreme cases of this dispersion model: in effect, as dispersion increases the calculated output will tend towards the exponential model output. Similarly, for low dispersion systems, the calculated output will tend towards the piston-flow model output. To generalise these models, a distinction can be introduced between the age of a water molecule, equivalent to the residence time, and the transit time. The residence time is the time elapsed since any element has entered into the system, as opposed to the transit time which is the time spent by an element between entry and outflow from the system. This distinction implies three cases: - the residence time is smaller than the transit time, so depending on whether or not there is mixing, we have the dispersion or piston-flow model; - the residence time is equal to the transit time, yielding the exponential model; - the residence time is greater than the transit time, which is an unutilized case in classical modeling and which may be qualified as a short-circuit model. These models have been applied to the Versoie and Evian mineral water systems (Haute-Savoie, France), for which we have long time-series of 3 H measurements. The Evian mineral water system uses a dispersion model, for which the average transit time is between 40 and 80 years. The Versoie mineral water system uses a mixing model, for which the average transit time is between 2 and 9 years. In conclusion, the long series of 3 H measurements realized in France, Switzerland, Italy and Spain ground-waters from 1990 to 1993 allow us to distinguish four categories of results: 3 H contents less than 2 UT, which characterizes an old ground-water with an average transit time greater than 2000 years; 3 H contents between 2 and 10 UT, which characterizes a ground-water with an average transit time between 200 and 300 years, or a mixing between an old and a recent water; 3 H contents between 10 and 40 UT, which characterizes a recent ground-water; 3 H contents greater than 40 UT, which characterizes a groundwater near a nuclear station...|$|E
40|$|Les étapes de désinfection de l'eau, telles que l'ozonation et la chloration, génèrent des sous-produits d'oxydation de nature variée. Ces composés sont soupçonnés d'être toxiques à plus ou moins long terme. Certains d'entre eux sont, de plus, {{facilement}} biodégradables et favorisent donc une reviviscence bactérienne dans le réseau de distribution. Enfin, à cause de leurs caractéristiques organoleptiques, ils peuvent conduire à la détérioration de la qualité sensorielle de l'eau distribuée. La recherche de plusieurs sous-produits d'oxydation dans l'eau potable a pu être effectuée grâce à de nouvelles techniques d'analyse quantitative par chromatographie en phase gazeuse: il s'agit des aldéhydes et des cétones de faible poids moléculaire, des acides haloacétiques et de certains céto-acides. Ces composés ont été recherchés dans des usines comportant une étape d'ozonation. L'influence de ce traitement sur {{la formation}} des aldéhydes et des céto-acides est démontrée dans cette étude. L'ozonation multiplie la concentration totale d'aldéhydes par un facteur variant de 2 à 4 suivant les usines et les trois céto-acides recherchés ont été trouvés en quantités importantes dans des eaux ozonées. La filtration sur charbon, lorsqu'elle existe, s'avère efficace pour l'élimination de ces composés. Les trois acides chloroacétiques sont présents dans des eaux chlorées, en sortie d'usines <b>appliquant</b> des taux de chloration assez importants. Enfin, I'évolution de ces sous-produits d'oxydation tout au long d'un réseau de distribution a pu être expliquée par leur biodégradabilité. New regulations are being {{considered by the}} US Environmental Protection Agency (US EPA) concerning a variety of disinfection by-products formed during chlorination and ozonation (halonitriles, haloketones, haloacids, low molecular weight aldehydes [...] .) and many surveys are underway to assess the presence of such products in drinking waters. This renewed interest for disinfection byproducts (DBPs) arises from their suspected carcinogenic or mutagenic properties. In addition to possible long term health effects, specific disinfection by-products may also induce immediate water quality deterioration due to their objectionable organoleptic properties. Biodegradable DBP's also probably contribute a substantial proportion of the biodegradable dissolved organic carbon (BDOC). As far as oxidation disinfection practices in France are concerned, the use of ozone is frequent. Also, most major French treatment plants include an activated carbon filtration process {{which is likely to}} remove some of the DBPs {{as well as some of}} their precursors. This paper summarizes the results obtained along various treatment plants in the Paris area, concerning three major families of DBPs considered for regulation: the DBPs investigated include 30 aldehydes and ketones, chloroacetic acids and 3 ketoacids. The aldehydes and ketones were measured by GC-ECD or GC-MS after derivatization with PFBHA; the chloroacetic acids were measured using a micro extraction method with methyltertiobutylether followed by diazomethane methylation and GC-ECD or GC-MS; the six plants investigated in the Paris area treat surface water from the river Seine upstream of Paris (Morsang, Vigneux, Orly, Ivry) or groundwater which is artificially recharged with Seine river water downstream of Paris (Le Pecq, Aubergenville). The Alençon plant which is located outside the Paris area treats raw water from the Sarthe river plus groundwater from various wells. All the treatment lines studied include an ozonation step which is followed at some plants (Morsang, Ivry, Vigneux, Le Pecq-Minor) by a granular activated carbon (GAC) filtration. The treatment line investigated at Vigneux used an ozone/hydrogen peroxide combination as an oxidation step. Three of these treatment lines (Morsang, Le Pecq-Minor, Alençon) comprise a prechlorination step; the other plants only use chlorination as a final disinfection step. The study compares the total concentration of aldehydes detected before and after ozonation as well as after GAC filtration. Approximately half of this total is usually due to formaldehyde while acetaldehyde, glyoxal and methylglyoxal represent most of the remainder. These concentrations which initially range from 1 to 25 µg/l show a drastic increase after ozonation. Depending on the water DOC and ozonation conditions, the total level of aldehydes is multiplied by a factor of 2 to 4. The final chlorine disinfection step used at all these plants does not significantly influence the total concentration of the aldehydes, therefore the level of these DBPs at the outlet of the plants is mainly determined by the ozonation or ozone/GAC filtration steps. The three aldo and ketoacids analysed were glyoxylic acid, pyruvic acid and ketomalonic acid. They were detected in ozonated water with total concentrations which range from 65 to 80 µg/l. Glyoxylic acid alone, represents half of these quantities. Chloroacetic acids were not detected at the outlet of the plants which are supplied by groundwater (Le Pecq-Minor, Aubergenville) and which apply a low chlorine dose (0. 1 - 0. 2 ppm) as a final disinfection stop. Although the Morsang treatment line investigated applies a low prechlorination dose ([smaller or equal] 1 ppm) in addition to the low postchlorination dose, no haloacids were detected at the outlet, which is agreement with the low levels of trihalomethanes usually detected at this plant. The absence of haloacids at the outlet of this plant can be attributed to the efficiency of the ozone/GAC combination as well as to the low prechlorination dose applied. Haloacids were only detected at the outlet of the Orly, Ivry and Alençon plants which apply a rather high chlorine dose during the final disinfection stop (between 0. 8 and 2. 2 ppm) in order to maintain a residual in the distribution system. Typical levels of haloacids are found between 10 and 35 µg/l, mainly under the form of dichloro and trichloroacetic acid. To summarize, the levels of the specific DBPs investigated remain well below their individual WHO recommendations (respectively 50, 100 and 900 µg/ 1 for dichloroacetic acid, trichloroacetic acid and formaldehyde). Unless more drastic national regulations are implemented, the interest in the fate of these DBP's mainly lies in their possible secondary effects such as enhancement of bacterial regrowth in distribution systems or degradation of drinking water organoleptic properties...|$|E
40|$|SUITE A LA REGLEMENTATION CLASSANT LE FORMALDEHYDE COMME COMPOSE CANCEROGENE POUR L'HOMME, IL EXISTE UNE DEMANDE EN CAPTEURS SENSIBLES, SELECTIFS ET DE FAIBLE COUT. DANS CET OBJECTIF, NOUS AVONS ELABORE UN CAPTEUR CHIMIQUE AYANT COMME COUCHE SENSIBLE DES MATRICES NANOPOREUSES ET FONCTIONNANT AVEC UNE DETECTION OPTIQUE (ABSORPTION OU FLUORESCENCE). LA MATRICE EST ELABOREE VIA LE PROCEDE SOL-GEL QUI PERMET, EN FONCTION DES PARAMETRES DE SYNTHESE, D AJUSTER LA TAILLE DES PORES. POUR AUGMENTER LA SELECTIVITE, NOUS AVONS DOPE CES MATRICES AVEC UNE MOLECULE-SONDE, LE FLUORAL-P, QUI REAGIT AVEC LE FORMALDEHYDE POUR FORMER UN COMPOSE CYCLIQUE LA 3, 5 -DIACETYL- 1, 4 -DIHYDROLUTIDINE (DDL). DIFFERENTS PARAMETRES INFLUANT SUR LA CINETIQUE DE LA REACTION ONT ETE ETUDIES: LA CONCENTRATION DE FLUORAL-P DANS LES FILMS, L'EFFET DU DEBIT ET DE L'HUMIDITE CONTENUE DANS LE FLUX AINSI QUE LA TENEUR EN FORMALDEHYDE DANS LE MELANGE GAZEUX. EN ETABLISSANT LES COURBES DE CALIBRATION A PARTIR DES CINETIQUES DE FORMATION DE LA DDL EN FONCTION DE LA TENEUR EN FORMALDEHYDE, NOUS MONTRONS LA POSSIBILITE DE DETECTER 2 PPB DE FORMALDEHYDE EN 30 MIN OU DES CONCENTRATIONS SUPERIEURES A 10 PPB EN 5 MIN. L'EAU EST LE SEUL INTERFERENT IMPORTANT. POUR CAPTER L'ENSEMBLE DES COMPOSES CARBONYLES, NOUS AVONS EGALEMENT ELABORE SUR LE MEME PRINCIPE UN CAPTEUR BASE SUR LA REACTIVITE DE LA 2, 4 -DINITROPHENYLHYDRAZINE AVEC LES COMPOSES CARBONYLES. UN BREVET A ETE DEPOSE SUR L ELABORATION DES CAPTEURS. NOUS AVONS DE PLUS MIS AU POINT UN SYSTEME DE DETECTION SEMI-MINIATURISE COMPRENANT UNE CELLULE A ECOULEMENT, UNE SOURCE LUMINEUSE FIBREE, UN SPECTROPHOTOMETRE MINIATURE ET UN ORDINATEUR PORTABLE. DEPUIS QUE LE FORMALDEHYDE EST CLASSE COMME COMPOSE CANCEROGENE POUR L'HOMME, IL EXISTE UNE DEMANDE CROISSANTE EN CAPTEURS SENSIBLES, SELECTIFS, RAPIDES ET PEU COUTEUX POUR SA DETECTION ET QUANTIFICATION. DANS CET OBJECTIF, NOUS AVONS ELABORE UN CAPTEUR CHIMIQUE EN <b>APPLIQUANT</b> DIVERSES STRATEGIES. LA PREMIERE EST BASEE SUR L'UTILISATION DE MATRICES NANOPOREUSES, AGISSANT COMME DES EPONGES ET CAPABLES DE DISCRIMINER LES POLLUANTS PAR LEUR TAILLE. EN DOPANT LA MATRICE D'UNE MOLECULE-SONDE, LE FLUORAL-P, CAPABLE DE REAGIR SPECIFIQUEMENT AVEC LE FORMALDEHYDE POUR FORMER LA 1, 6 -DIACETYL- 2, 5 -DIHYDROLUTIDINE (DDL), ON PEUT DETECTER OPTIQUEMENT LE POLLUANT VIA L'ABSORPTION OU LA FLUORESCENCE DE LA DDL. CETTE STRATEGIE PERMET D'OPTIMISER LA SELECTIVITE, LA SENSIBILITE ET LE TEMPS DE REPONSE DU CAPTEUR. POUR DETERMINER LE DOMAINE DE FONCTIONNEMENT DU CAPTEUR, DIFFERENTS PARAMETRES ONT ETE ETUDIES: LE TAUX DE DOPAGE EN FLUORAL-P, LA TENEUR DU POLLUANT DANS LES MELANGE GAZEUX, L'EFFET DU DEBIT ET DE L'HUMIDITE DU FLUX ET LES INTERFERENTS GAZEUX. AINSI, LE CAPTEUR EST UTILISABLE DANS UN DOMAINE D'HYGROMETRIE DE 0 A 60 % ET PEUT DETECTER 2 PPB DE FORMALDEHYDE EN 30 MIN SANS PROBLEME D'INTERFERENCE. PAR AILLEURS, POUR CAPTER L'ENSEMBLE DES COMPOSES CARBONYLES, NOUS AVONS MIS AU POINT UN CAPTEUR UTILISANT COMME MOLECULE-SONDE LA 2, 4 -DINITROPHENYLHYDRAZINE, QUI FORME AVEC CES COMPOSES, LES DERIVES D'HYDRAZONE CORRESPONDANTS. UN BREVET A ETE DEPOSE SUR L'ELABORATION DE CES DEUX TYPES DE CAPTEURS. NOUS AVONS DE PLUS MIS AU POINT UN PROTOTYPE DE DEMONSTRATION SEMI MINIATURISE, INCLUANT UNE CUVE A ECOULEMENT, UN SPECTROPHOTOMETRE MINIATURE, UNE SOURCE LUMINEUSE ET UN ORDINATEUR PORTABLE. FORMALDEHYDE, A WELL-IDENTIFIED INDOOR POLLUTANT, WAS RECENTLY CLASSIFIED AS CARCINOGENIC. NEW REGULATIONS FOR THE AIR QUALITY ARE EXPECTED, THEREFORE THERE IS A NEED FOR LOW-COST SENSORS, SENSITIVE AND SELECTIVE WITH A FAST RESPONSE TIME FOR THE DETECTION OF FORMALDEHYDE AT PPB LEVEL. IN THE PRESENT WORK, WE HAD DEVELOPED A CHEMICAL SENSOR BASED ON NANOPOROUS MATRICES DOPED WITH FLUORAL-P AND OPTICAL METHODS OF DETECTION. THE NANOPOROUS MATRICES, ELABORATED VIA THE SOL-GEL PROCESS, DISPLAY NANOPORES WHOSE CAVITY IS TAILORED FOR THE TRAPPING OF THE TARGETED POLLUTANT. THEY PROVIDE A FIRST SELECTIVITY WITH THE DISCRIMINATION OF THE POLLUTANTS BY THEIR SIZE. A SECOND SELECTIVITY IS OBTAINED WITH A MOLECULAR PROBE, FLUORAL-P, WHICH REACTS SPECIFICALLY WITH FORMALDEHYDE LEADING TO THE 3, 5 -DIACETYL- 1, 4 -DIHYDROLUTIDINE (DDL). THE KINETICS OF FORMATION OF DDL WAS STUDIED AS FUNCTION OF MANY PARAMETERS SUCH AS THE CONCENTRATION OF FLUORAL-P IN THE MATRIX, THE POLLUTANT CONTENT IN GAS MIXTURE, THE FLOW RATE, THE RELATIVE HUMIDITY OF THE GAS MIXTURES AND INTERFERENCE WITH OTHER CARBONYLATED COMPOUNDS. THE PRESENT CHEMICAL SENSOR CAN DETECT, VIA ABSORBANCE MEASUREMENTS, 2 PPB OF FORMALDEHYDE WITHIN 30 MIN OVER A 0 TO 60 % RELATIVE HUMIDITY RANGE. MOREOVER, TO DETECT THE TOTAL CARBONYLATED COMPOUNDS, WE ALSO EXPLORED THE POTENTIALITY OF A CHEMICAL SENSOR USING, AS A PROBE MOLECULE, THE 2, 4 -DINITROPHENYLHYDRAZINE WHICH FORMS WITH THESE COMPOUNDS THE CORRESPONDING HYDRAZONES DERIVATIVES. A PATENT WAS DEPOSITED FOR THESE TWO SENSORS. WE HAVE ALSO DEVELOPED A SEMI-MINIATURIZED PROTOTYPE FOR DEMONSTRATION, USING A FLOW CELL, A MINIATURIZED SPECTROPHOTOMETER, A LIGHT SOURCE AND A LAPTOP. SENSITIVE, SELECTIVE, QUICK AND LOW COST SENSORS FOR FORMALDEHYDE ARE NEEDED, SINCE THIS HAS BEEN CLASSIFIED AS A CARCINOGENIC COMPOUND FOR HUMAN. AN ANSWER TO THESE CONSTRAINS IS A CHEMICAL SENSOR USING DIFFERENT STRATEGIES. FIRST, WE CHOSE NANOPOROUS MATRIX, ELABORATED BY SOL-GEL PROCESS, WHICH WILL ACT AS SPONGES AND DISCRIMINATE POLLUTANTS BY THEIR SIZE. SECOND, WE DOPED THESE MATRIX WITH A MOLECULAR PROBE, FLUORAL-P, WHICH WILL REACT SPECIFICALLY WITH FORMALDEHYDE TO FORM 1, 6 -DIACÉTYL- 2, 5 -DIHYDROLUTIDINE (DDL). THEN, THE POLLUTANT CAN BE DETECTED VIA THE ABSORBANCE OR THE FLUORESCENCE OF DDL. THUS SENSOR'S SENSITIVITY, SELECTIVITY AND RESPONSE TIME ARE OPTIMIZED. TO DETERMINE SENSOR'S WORKING FIELD, DIFFERENT PARAMETERS HAS BEEN STUDIED: THE CONCENTRATION OF FLUORAL-P IN MATRIX, THE POLLUTANT CONTENT IN GASEOUS FLUX, THE EFFECT OF THE FLOW AND HUMIDITY RATE OF THE FLUX AND GASEOUS INTERFERENTS. THUS, THE SENSOR DETECTS 2 PPB OF FORMALDEHYDE WITHIN 30 MINUTES WITHOUT ANY INTERFERENT'S PROBLEM AND WORKS FROM 0 TO 60 % OF RELATIVE HUMIDITY. IN OTHER HAND, TO DETECT THE WHOLE CARBONYL COMPOUNDS, WE DEVELOPED A SENSOR USING, AS A PROBE MOLECULE, 2, 4 -DINITROPHENYLHYDRAZINE, WHICH FORMS WITH THESE COMPOUNDS, THE CORRESPONDING HYDRAZONES DERIVATIVES. A PATENT WAS TAKEN OUT FOR THESE TWO SENSORS. WE HAVE ALSO DEVELOPED A SEMI-MINIATURIZED PROTOTYPE FOR DEMONSTRATION, USING A FLOW CUVETTE, A MINIATURIZED SPECTROPHOTOMETER, A LIGHT SOURCE AND A LAPTOP. ORSAY-PARIS 11 -BU Sciences (914712101) / SudocSudocFranceF...|$|E
40|$|La notion d'appartenance {{partielle}} d'une station hydrométrique à une région hydrologique est modélisée par une fonction d'appartenance obtenue en <b>appliquant</b> les concepts de l'analyse floue. Les stations hydrométriques sont représentées dans des plans dont les axes sont des attributs hydrologiques et/ou physiographiques. Les régions hydrologiques sont considérées comme des sous-ensembles flous. Une méthode d'agrégation par cohérence (Iphigénie) permet d'établir des classes d'équivalence pour la relation floue "il n'y a pas d'incohérence entre les éléments d'une même classe": ce sont des classes d'équivalence qui représentent les régions floues. La fonction d'appartenance dans ce cas est stricte. Par opposition, la seconde méthode de type centres mobiles flous (ISODATA) permet d'attribuer un degré d'appartenance d'une station à une région floue dans l'intervalle [0, 1]. Celle-ci reflète le degré d'appartenance de la station à un groupe donné (le nombre de groupes étant préalablement choisi de façon heuristique). Pour le cas traité (réseau hydrométrique tunisien, débits maximums annuels de crue), il s'avère cependant que le caractère flou des stations n'est pas très prononcé. Sur la base des agrégats obtenus par la méthode Iphigénie et des régions floues obtenues par ISODATA, est effectuée une estimation régionale des débits maximums de crue de période de retour 100 ans. Celle-ci est ensuite comparée à l'estimation régionale obtenue par la méthode de la région d'influence ainsi qu'à l'estimation utilisant les seules données du site, sous l'hypothèse que les populations parentes sont des lois Gamma à deux paramètres et Pareto à trois paramètres. The {{concept of}} partial membership of a hydrometric station in a hydrologic region is modeled using fuzzy sets theory. Hydrometric stations {{are represented in}} spaces of hydrologic (coefficient of variation: CV, coefficient of skewness: CS, and their counterparts based on L- moments: L-CV and L-CS) and/or physiographic attributes (surface of watershed: S, specific flow: Qs=Qmoyen/S, and a shape index: Ic). Two fuzzy clustering methods are considered. First a clustering method by coherence (Iphigénie) is considered. It {{is based on the}} principle of transitivity: if two pairs of stations (A,B) and (B,C) are known to be "close" to one another, then it is incoherent to state that A is "far" from C. Using a Euclidean distance, all pairs of stations are sorted from the closest pairs to the farthest. Then, the pairs of stations starting and ending this list are removed and classified respectively as "close" and "far". The process is then continued until an incoherence is detected. Clusters of stations are then determined from the graph of "close" stations. A disadvantage of Iphigénie is that crisp (non fuzzy) membership functions are obtained. A second method of clustering is considered (ISODATA), which consists of minimizing fuzziness of clusters as measured by an objective function, and which can assign any degree of membership between 0 to 1 to a station to reflect its partial membership in a hydrologic region. It is a generalization of the classical method of mobile centers, in which crisp clusters minimizing entropy are obtained. When using Iphigénie, the number of clusters is determined automatically by the method, but for ISODATA it must be determined beforehand. An application of both methods of clustering to the Tunisian hydrometric network (which consists of 39 stations, see Figure 1) is considered, with the objective of obtaining regional estimates of the flood frequency curves. Four planes are considered: P 1 : (Qs,CV), P 2 : (CS,CV), P 3 : (L-CS,L-CV), and P 4 : (S,Ic), based on a correlation study of the available variables (Table 1). Figures 2, 3 a, 4 and 5 show the clusters obtained using Iphigénie for planes P 1 through P 4. Estimates of skewness (CS) being quite biased and variable for small sample sizes, it was decided to determine the influence of sample size in the clusters obtained for P 2. Figure 3 b shows the clusters obtained when the network is restricted to the 20 stations of the network for which at least 20 observations of maximum annual flood are available. Fewer clusters are obtained than in Figure 3, but it can be observed that the structure is the same: additional clusters appearing in Figure 3 may be obtained by breaking up certain large clusters of Figure 3 b. In Figure 3 c, the sample size of each of the 39 stations of the network is plotted in the plane (CS,CV), to see if extreme estimated values of CS and CV were caused by small samples. This {{does not seem to be}} the case, since many of the most extreme points correspond to long series. ISODATA was also applied to the network. Based on entropy criteria (Table 2, Figures 6 a and 6 b), the number of clusters for ISODATA was set to 4. It turns out that the groups obtained using ISODATA are not very fuzzy. The fuzzy groups determined by ISODATA are generally conditioned by only one variable, as shown by Figures 7 a- 7 d, which respectively show the fuzzy clusters obtained for planes P 1 -P 4. Only lines of iso-membership of level 0. 9 were plotted to facilitate the analysis. For hydrologic spaces (P 2 and P 3), it is skewness (CS and L-CS) and for physiographic spaces (P 1 and P 4) it is surface (Qs and S). Regionalization of the 100 -year return period flood is performed based on the homogeneous groups obtained (using an index-flood method), and compared to the well-known region of influence (ROI) approach, both under the hypothesis of a 2 -parameter Gamma distribution and a 3 -parameter Pareto distribution. For the ROI approach, the threshold corresponding to the size of the ROI of a station is taken to be the distance at which an incoherence first appeared when applying Iphigénie. Correlation of the regional estimate with a local estimation for space P 1 is 0. 91 for Iphigénie and 0. 85 both for ISODATA and the ROI approach. Relative bias of regional estimates of the 100 -year flood based on P 1 is plotted on Figures 9 (Gamma distribution) and Figure 10 (Pareto distribution). The three methods considered give similar results for a Gamma distribution, but Iphigénie estimates are less biased when a Pareto distribution is used. Thus Iphigénie appears superior, in this case, to ISODATA and ROI. Values of bias and standard error for all four planes are given for Iphigénie in Table 3. Application of an index-flood regionalization approach at ungauged sites requires the estimation of mean flow (also called the flood index) from physiographic attributes. A regression study shows that the best explanatory variables are watershed surface S, the shape index Ic and the average slope of the river. In Figure 8, the observed flood index is plotted against the flood index obtained by regression. The correlation coefficient is 0. 93. Iphigénie and ISODATA could also be used in conjunction with other regionalization methods. For example, when using the ROI approach, it is necessary to, quite arbitrarily, determine the ROI threshold. It has been shown that this is a byproduct of the use of Iphigénie. ISODATA is most useful for pattern identification when the data is very fuzzy, unlike the example considered in this paper. But even in the case of the Tunisian network, its application gives indications as to which variables (skewness and surface) are most useful for clustering...|$|E
40|$|Most of {{the bridges}} {{are less than}} 50 m (85 %) in France. For this type of bridges, the traffic load may govern the design and assessment. Road freight {{transportation}} has increased by 36. 2 % between 1995 and 2010 in Europe, and the volume of freight transport is projected to increase by 1. 7 % per year between 2005 and 2030. It is thus vital important to insure European highway structures to cater for this increasing demand in transport capacity. Traffic load model in standard or specification for bridge design should guarantee all newly designed bridges to have sufficient security margin for future traffic. For existing bridges, the task is to assess their safety under actual and future traffic, and a prioritization of the measures necessary to ensure their structural integrity and safety. In addition, to address this growth without compromising the competitiveness of Europe, some countries are contemplating the introduction of longer and heavier trucks for {{reducing the number of}} heavier vehicles for a given volume or mass of freight, reducing labour, fuel and other costs. Many different methods have been used to model extreme traffic load effects on bridges for predicting characteristic value for short or long return period. They include the fitting a Normal or Gumbel distribution to upper tail, the use of Rice formula for average level crossing rate, the block maxima method and the peaks over threshold method. A review of the fundament and the use of these methods for modelling maximum distribution of bridge is presented. In addition, a quantitative comparison work is carried out to investigate the differences between methods. The work involves two studies, one is based on numerical sample, and the other is based on traffic load effects. The accuracy of the methods is evaluated through the typical statistics of bias and root mean squared error on characteristic value and probability of failure. In general, the methods are less accurate on inferring the failure probability than on characteristic values, perhaps not surprising given such a small failure probability was being considered (〖 10 〗^(- 6) in a year). Although none of methods provides predictand as accurately as expected with 1000 days of data, the tail fitting methods, especially the peaks over threshold method, are better than the others. A study on peaks over threshold method is thus carried out in this thesis. In the POT method, the distribution of exceedances over a high enough threshold will be a member of generalized Pareto distribution (GPD) family. The peaks over threshold method is extensively used in the domains such as hydrology and finance, while seldom application can be found in bridge loading problem. There are numerous factors, which affect the application of peaks over threshold on modelling extreme value, such as the length and accuracy of data available, the criteria used to identify independent peaks, parameter estimation and the choice of threshold. In order to provide some guidance on selecting parameter estimation when applying POT to bridge traffic loading, we focus on the effect that method used to estimate the parameters of the GPD has on the accuracy of the estimated characteristic values. Many parameter estimators have been proposed in the statistical literature, and the performance of various estimators can vary greatly in terms of their bias, variance and sensitivity to threshold choice and consequently affect the accuracy of the estimated characteristic values. The conditions, assumptions, merits and demerits of each parameter estimation method are introduced; especially their applicability for traffic loading is discussed. Through this qualitative discussion on the methods, several available methods for traffic loading are selected. It includes the method of moments (MM), the probability weighted moments (PWM), the maximum likelihood (ML), the penalized maximum likelihood (PML), the minimum density power divergence (MDPD), the empirical percentile method (EPM), the maximum goodness-of-fit statistic and the likelihood moment (LM). To illustrate the behaviour and accuracy of these parameter estimators, three studies are conducted. Numerical simulation data, Monte Carlo simulation traffic load effects and in-field traffic load effect measurements are analyzed and presented. The comparative studies investigate the accuracy of the estimates in terms of bias and RMSE of parameters and quantile. As expected, the estimators have different performance, and the same method has different performance in these three sets of data. From the numerical simulation study, the MM and PWM methods are recommended for negative shape parameter case, especially for small size sample (less than 200), while the ML is recommended for positive shape parameter case. From the simulated traffic load effect study, the ML and PML provide more accuracy estimates of 1000 -year return level when the number of exceedances over 100, while the MM and PWM are better than others when sample size is less than 100. Moreover, application on monitored traffic load effects indicates that the outliers have significant influence on the parameter estimators as all investigated methods encounter feasibility problem. As been stated in statistical literature, a frequent cause of outlier is a mixture of two distributions, which may be two distinct sub-populations. In the case of bridge loading, this can be a potential reason to result in the feasible problem of parameter estimator. Literature points out that the traffic load effect is induced by loading event that involves different number of vehicles, and the distribution of the load effects from different loading events are not identically distributed, which violates the assumption of classic extreme value theory that the underlying distribution should be identically independent distributed. With respect to non-identical distribution in bridge traffic load effects, non-identical distribution needs to be addressed in extreme modelling to account for the impacts in inference. Methods using mixture distribution (exponential or generalized extreme value) has been proposed in the literature to model the extreme traffic load effect by loading event. However, it should be noticed that the generalized extreme value distribution is fitted to block maxima, which implies the possibility of losing some extremes, and the use of exponential distribution is objective. We intend to explicitly model the non-identically distributed behaviour of extremes for a stationary extreme time series within a mixture peaks over threshold (MPOT) model to avoid the loss of information and predetermination of distribution type. For bridges with length greater than 50 m, the governing traffic scenario is congested traffic, which is out of the scope of this study. Moreover, the traffic loading may not govern the design for long span bridge. However, the traffic loading may be also importance if the bridge encounter traffic induced fatigue problem, components like orthotropic steel deck is governed by traffic induced fatigue load effects. We intend to explore the influence of traffic load on the fatigue behaviour of orthotropic steel deck, especially the influence of the loading position in terms of transverse location of vehicle. Measurements of transverse location of vehicle collected from by weigh-in-motion (WIM) systems in 2010 and 2011 four French highways showed a completely different distribution model of transverse location of vehicle to that recommended in EC 1. Stress spectrum analysis and fatigue damage calculation was performed on the stresses induced traffic on orthotropic steel deck of Millau cable-stayed bridge. By comparing the stresses and damages induced by different traffic patterns (through distributions of transverse location of vehicle), {{it was found that the}} histogram of stress spectrum and cumulative fatigue damage were significantly affected by the distribution of transverse location of vehicle. Therefore, numerical analysis that integrates finite element modelling and traffic data with distributions of transverse location of vehicles can help to make an accurate predetermination of which welded connections should be sampled to represent the health of the deck. Bridge, traffic effects, traffic, extrapolation, extreme effectsUne grande majorité (85 %) des ponts français a une portée inférieure à 50 m. Pour ce type d’ouvrage d’art, la charge de trafic peut être déterminante pour la conception et le recalcul. Or, en Europe, le fret routier a augmenté de 36. 2 % entre 1995 et 2010, et la croissance annuelle du volume transporté par la route a été évaluée à 1. 7 % entre 2005 et 2030. Il est donc essentiel de s’assurer que les infrastructures européennes sont en mesure de répondre à cette demande croissante en capacité structurelle des ouvrages. Pour les ouvrages neufs, les modèles de trafic dans les normes ou les législations pour la conception des ponts incluent une marge de sécurité suffisante pour que la croissance du trafic soit prise en compte sans dommage par ces ouvrages. Mais pour les ouvrages existants, la résistance structurelle aux trafics actuels et futur est à vérifier et une priorisation des mesures doit être faite pour assurer leur intégrité structurelle et leur sécurité. De plus, afin de préserver leur infrastructure tout en ne menaçant pas leur compétitivité nationale, certains pays réfléchissent à l’introduction de poids lourds plus longs, plus lourds, ce qui permet de réduire le nombre de véhicules pour un volume ou un tonnage donné, ainsi que d’autres coûts (d’essence, de travail, [...] ), ce qui justifie encore plus les calculs effectués. Pour répondre à ce genre de problématique, différentes méthodes d’extrapolation ont déjà été utilisées pour modéliser les effets extrêmes du trafic, afin de déterminer les effets caractéristiques pour de grandes périodes de retour. Parmi celles-ci nous pouvons citer l’adaptation d’une gaussienne ou d’une loi de Gumbel sur la queue de distribution empirique, la formule de Rice appliquée à l’histogramme des dépassements de niveaux, la méthode des maxima par blocs ou celle des dépassements de seuils élevés. Les fondements et les utilisations faites de ces méthodes pour modéliser les effets extrêmes du trafic sur les ouvrages sont donnés dans un premier chapitre. De plus, une comparaison quantitative entre ces méthodes est réalisée. Deux études sont présentées, l’une basée sur un échantillon numérique et l’autre sur un échantillon réaliste d’effets du trafic. L’erreur induite par ces méthodes est évaluée à l’aide d’indicatifs statistiques simples, comme l’écart-type et les moindres carrés, évalués sur les valeurs caractéristiques et les probabilités de rupture. Nos conclusions sont, qu’en général, les méthodes sont moins précises lorsqu’il s’agit de déterminer des probabilités de rupture que lorsqu’elles cherchent des valeurs caractéristiques. Mais la raison peut en être les faibles probabilités recherchées (10 - 6 par an). De plus, bien qu’aucune méthode n’ait réalisée des extrapolations de manière correcte, les meilleures sont celles qui s’intéressent aux queues de probabilités, et en particulier des dépassements au-dessus d’un seuil élevé. Ainsi une étude de cette dernière méthode est réalisée : en effet, cette méthode, nommé "dépassements d’un seuil élevé", considère que les valeurs au-dessus d’un seuil correctement choisi, assez élevé, suit une distribution de Pareto généralisée (GPD). Cette méthode est utilisée de manière intensive dans les domaines de l’hydrologie et la finance, mais non encore appliquée dans le domaine des effets du trafic sur les ouvrages. Beaucoup de facteurs influencent le résultat lorsqu’on applique cette méthode, comme la quantité et la qualité des données à notre disposition, les critères utilisés pour déterminer les pics indépendants, l’estimation des paramètres et le choix du seuil. C’est pour cette raison qu’une étude et une comparaison des différentes méthodes d’estimation des paramètres de la distribution GPD sont effectuées : les conditions, hypothèses, avantages et inconvénients des différentes méthodes sont listés. Différentes méthodes sont ainsi étudiées, telles la méthode des moments (MM), la méthode des moments à poids (PWM), le maximum de vraisemblance (ML), le maximum de vraisemblance pénalisé (PML), le minimum de la densité de la divergence (MDPD), la méthode des fractiles empiriques (EPM), la statistique du maximum d’adaptation et la vraisemblance des moments (LM). Pour comparer ces méthodes, des échantillons numériques, des effets de trafic simulés par Monte Carlo et des effets mesurés sur un ouvrage réel sont utilisés. Comme prévu, les méthodes ont des performances différentes selon l’échantillon considéré. Néanmoins, pour des échantillons purement numériques, MM et PWM sont recommandées pour des distributions à paramètre de forme négatif et des échantillons de petite taille (moins de 200 valeurs). ML est conseillé pour des distributions à paramètre de forme positif. Pour des effets du trafic simulés, ML et PML donne des valeurs de retour plus correctes lorsque le nombre de valeurs au-dessus du seuil est supérieur à 100; dans le cas contraire, MM et PWM sont conseillés. De plus, comme c’est prouvé dans l’étude de valeurs réelles mesurées, les valeurs a priori aberrantes ("outliers") ont une influence notable sur le résultat et toutes les méthodes sont moins performantes. Comme cela a été montré dans la littérature, ces "outliers" proviennent souvent du mélange de deux distributions, qui peuvent être deux sous-populations. Dans le cas de l’effet du trafic sur les ouvrages, cela peut être la raison d’une estimation des paramètres non correcte. Les articles existant sur le sujet soulignent le fait que les effets du trafic sont dus à des chargements indépendants, qui correspondant au nombre de véhicules impliqués. Ils ne suivent pas la même distribution, ce qui contredit l’hypothèse classique en théorie des valeurs extrêmes que les événements doivent être indépendants et identiquement distribués. Des méthodes permettant de prendre en compte ce point et utilisant des distributions mélangées (exponentielles ou valeurs extrêmes généralisées) ont été proposées dans la littérature pour modéliser les effets du trafic. Nous proposons une méthode similaire, que nous appelons dépassement de seuils mélangés, afin de tenir des différentes distributions sous-jacentes dans l’échantillon tout en <b>appliquant</b> à chacune d’entre elles la méthode des dépassements de seuil. Pour des ponts ayant des portées supérieures à 50 m, le scénario déterminant est celui de la congestion, qui n’est pas ce qui est étudié ici. De plus, le trafic n’est pas la composante déterminante pour la conception des ponts de longue portée. Mais des problèmes de fatigue peuvent apparaître dans certains ponts, tels les ponts métalliques à dalle orthotrope, où l’étude du trafic peut devenir nécessaire. Ainsi nous avons fait une étude de l’influence de la position des véhicules sur le phénomène de fatigue. Pour cela, quatre fichiers de trafic réels, mesurés en 2010 et 2011 par quatre stations de pesage différentes, ont été utilisés. Ils ont mis à jour des comportements latéraux différents d’une station à l’autre. Si nous les appliquons au viaduc de Millau, qui est un pont métallique à haubans et à dalle orthotrope, nous voyons que l’histogramme des effets et l’effet de fatigue cumulé est beaucoup affecté par le comportement latéral des véhicules. Ainsi, des études approfondies utilisant les éléments finis pour modéliser les ouvrages et des enregistrements de trafic réel, peuvent être utilisées pour pré-déterminer quels éléments, donc quelles soudures, doivent être examinés dans les ponts afin d’estimer leur santé structurelle...|$|E
40|$|My {{research}} lies at {{the interface}} of Riemannian, contact, and symplectic geometry. It deals {{with the construction of}} Kähler and Sasaki-Einstein metrics, with the study of conformal Hamiltonian systems, the geometry of cosphere bundles, and proper Lie groupoids. The main theme of this thesis is the study of applications of Lie symmetries in differential geometry and dynamical systems. The first chapter of the thesis studies the singular reduction of cosphere fiber bundles. The copshere bundle of a differentiable manifold M (denoted by S^*(M)) is the quotient of its cotangent bundle without the zero section with respect to the action by multiplications of ^+ which covers the identity on M. It is a contact manifold which has the same privileged position in contact geometry that cotangent bundles have in symplectic geometry. Using a Riemannian metric on M, we can identify S^*(M) with its unitary tangent bundle and its Reeb vector field with the geodesic field on M. If M is endowed with the proper action of a Lie group G, the lift of this action on S^*(M) respects the contact structure and admits an equivariant momentum map J. We study the topological and geometrical properties of the reduced space of S^*(M) at zero momentum, i. e. (S^*(M)) _ 0 :=J^- 1 (0) /G. Thus, we generalize the results of dragulete [...] ornea [...] ratiu to the singular case. Applying the general theory of contact reduction developed by Lerman and Willett in lerman [...] willett and willett, one obtains contact stratified spaces that lose all information of the internal structure of the cosphere bundle. Even more, the cosphere bundle projection to the base manifold descends to a continuous surjective map from (S^*(M)) _ 0 to M/G, but it fails to be a morphism of stratified spaces if we endow (S^*(M)) _ 0 with its contact stratification and M/G with the customary orbit type stratification defined by the Lie group action. Based on the cotangent bundle reduction theorems, both in the regular and singular case, as well as regular cosphere bundle reduction, one expects additional bundle-like structure for the contact strata. To solve these problems, we introduce a new stratification of the contact quotient at zero, called the C-L stratification (standing for the coisotropic or Legendrian nature of its pieces). It is compatible with the contact stratification of (S^*(M)) _ 0 and the orbit type stratification of M/G. It is also finer than the contact stratification. Also, the natural projection of the C-L stratified quotient space (S^*(M)) _ 0 to its base space, stratified by orbit types, is a morphism of stratified spaces. Each C-L stratum is a bundle over an orbit type stratum of the base and it {{can be seen as a}} union of C-L pieces, one of them being open and dense in its corresponding contact stratum and contactomorphic to a cosphere bundle. Hence we have identified the maximal strata endowed with cosphere bundle structure. The other strata are coisotropic or Legendrian submanifolds in the contact components that contain them. Consequently, we can perform a complete geometric and topological analysis of the reduced space. We also study the behaviour of the projection on (S^*(M)) _ 0 of the Reeb flow (geodesic flow). The set of contact Hamiltonian vector fields (the analogous of Hamiltonian vector fields in symplectic geometry) form the "Lie" group of the algebra of contact transformations. In the first chapter we also present the reduction of contact systems (which locally are in bijective correspondence with the non-autonomus Hamilton-Jacobi equations) and time dependent Hamiltonian systems. In the second chapter of this thesis we study quotients of Kähler and Sasaki-Einstein manifolds. We construct a reduction procedure for symplectic and Kähler manifolds (endowed with symmetries generated by a Lie group) which uses the ray pre-images of the associated momentum map. More precisely, instead of considering as in the Marsden- Weinstein reduction (point reduction) the pre-image of a momentum value μ, we use the pre-image of ^+μ, its positive ray. We have three reasons to develop this construction. One is geometric: the construction of canonical reduced spaces of Kähler manifolds corresponding to a non zero momentum. By canonical we mean that the reduced Kähler structure is the projection of the initial Kähler structure. The point reduction (Marsden-Weinstein) given by M_μ:=J^- 1 (μ) /G_μ, where μ is a value of the momentum map J and G_μ the isotropy subgroup of μ with respect to the coadjoint action of G is not always well defined in the Kähler case (if G≠ G_μ). The problem is caused by the fact that the complex structure of M does not leave invariant the horizontal distribution of the Riemannian submersion which projects J^- 1 (μ) on M_μ. The solution proposed in the literature uses the reduced space at zero momentum of the symplectic difference of M with the coadjoint orbit of μ endowed with a unique Kähler-Einstein form (constructed, for insatnce, in besse, Chapter 8) and different from the Kostant-Kirillov-Souriau form. The uniqueness of the form on the coadjoint orbit ensures that the reduced space is well defined. On the other hand, not using the Kostant-Kirillov-Souriau form implies the fact that the reduced space is no longer canonical. The ray reduced space that we construct is canonical and can be defined for any momentum. It is the quotient of J^- 1 (^+μ) with respect to a certain normal subgroup of G_μ. The second reason is an application to the study of conformal Hamiltonian systems (see mclachlan [...] perlmutter). They are mechanical, non-autonomous systems with friction whose integral curves preserve, in the case of symmetries, the ray pre-images of the momentum map, but not the point (momentum) preimages of the Marsden-Weinstein quotient. We extend the notion of conformal Hamiltonian vector field by showing that one can thus include in this study new mechanical systems. Also, we present the reduction of conformal Hamiltonian systems. The third reason consists of finding the necessary and sufficient conditions for the ray reduced spaces of Kähler (Sasakian) -Einstein manifolds to be also Kähler (Sasakian) -Einstein. We deal with this problem in the second chapter of the thesis, in dragulete [...] ornea, and in dragulete [...] doi where we use techniques of A. Futaki. Thus, we can construct new Sasaki-Einstein structures. As examples of symplectic (Kähler) and contact (Sasakian) ray quotients we treat the case of cotangent and cosphere bundles and show that they are universal spaces for ray reductions. Examples of toric actions on spheres are also described. The third chapter of my thesis studies the space of orbits of a proper Lie groupoid. In weinstein [...] unu, weinstein [...] doi A. Weinstein has partially solved the problem of linearization of proper groupoids. In zung, N. T. Zung has completed it by showing a theorem of Bochner type for proper groupoids. Using ideas from foliation theory and the slice (linearization) theorem of Weinstein and Zung, we prove a stratification theorem for the orbit space of a proper groupoid. We show explicitely that the orbital foliation of a proper Lie groupoid is a Riemannian singular foliation in the sense of Molino. For all these we have two motivations. On one hand we want to prove that there is an equivalence between proper groupoids and orbispaces (the spaces which are locally quotients with respect to an action of a compact Lie group). On the other hand we would like to study the reduction of infinitesimal actions (actions of Lie algebras) which are not integrable to Lie group actions. These actions and their integrability have been studied, among others, by Palais (palais), Michor, Alekseevsky. Mes recherches se situent à l'interface de la géométrie Riemannienne et des géométries de contact et symplectique et portent sur la construction des métriques Kähler ou Sasakie-Einstein, sur l'étude des systèmes Hamiltonians conformes, la géométrie des fibrés cosphériques et les groupoïdes de Lie propres. Le thème principal de cette thèse est l'étude des applications des symétries Lie en géométrie différentielle et systèmes dynamiques. Le premier chapitre de cette thèse étudie la réduction singulière des symétries du fibré cosphérique, les propriétés conservatives des systèmes de contact et leurs réduction. Le fibré cosphérique d'une variété différentiable M (dénoté par S^*(M)) est le quotient de son fibré cotangent sans la section nulle par rapport à l'action par multiplication de ^+ qui couvre l'identité sur M. C'est une variété de contact qui détient en géométrie de contact la position analogue du fibré cotangent en géométrie symplectique. En utilisant une métrique Riemannienne sur M, on peut identifier S^*(M) avec son fibré tangent unitaire et son champ de Reeb avec le champ géodésique de M. Si M est munie de l'action propre d'un groupe de Lie G, le relèvement de cette action à S^*(M) respecte la structure de contact et admet une application moment équivariante J. Nous étudions les propriétés topologiques et géométriques de l'espace réduit à moment zéro de S^*(M), i. e. (S^*(M)) _ 0 :=J^- 1 (0) /G. Ainsi, nous généralisons les résultats de dragulete [...] ornea [...] ratiu au cas singulier. <b>Appliquant</b> la théorie générale de réduction de contact, théorie dévéloppée par Lerman et Willett dans lerman [...] willett et willett, on obtient des espaces qui perdent toute information sur la structure interne du fibré cosphérique. En plus, la projection du fibré cosphérique sur sa base descend à une surjection continue de (S^*(M)) _ 0 à M/G, mais qui n'est pas un morphisme d'espaces stratifiés si on munit l'espace réduit avec sa stratification de contact et l'espace de base avec la stratification standarde de type orbitale définie par l'action du groupe de Lie. Compte tenu des théorèmes de réduction du fibré cotangent (cas régulier et singulier) et du fibré cosphérique (cas régulier), on s'attend à ce que les strates de contact aient une structure fibrée additionnelle. Pour résoudre ces problèmes, nous introduisons une nouvelle stratification de (S^*(M)) _ 0, nommée la stratification C-L (les deux majuscules symbolisent la nature coisotrope ou Legendréenne de leurs strates). Elle est compatible avec la stratification de contact de (S^*(M)) _ 0 et la stratification de type orbital de M/G. Aussi, elle est plus fine que la stratification de contact et rend la projection de (S^*(M)) _ 0 sur M/G un morphism d'espaces stratifiés. Chaque strate C-L est un fibré sur une strate de type orbital de M/G et elle peut être vue comme une union de strates C-L, une d'entre elles étant ouverte et dense dans la strate de contact correspondante et difféomorphe à un fibré cosphérique. Ainsi, nous avons identifié les strates maximales munies de structure de fibrés cosférique. Les autres strates sont des sous-variétés coisotropes ou Legendre dans les composantes de contact qui les contiennent. Par conséquant nous faison une analyse géométrique et topologique complète de l'espace réduit. Nous analysons aussi le comportement de la projection sur (S^*(M)) _ 0 du flot de Reeb (flot géodésique). L'ensemble de champs de vecteurs de contact (les analogues des champs de vecteurs Hamiltonians en géométrie symplectique) forment le "groupe de Lie" de l'algèbre des transformations de contact. Dans le premier chapitre nous présentons aussi la réduction des systèmes de contact (qui, localement, sont en correspondence bijective avec les équations non-autonomes de Hamilton-Jacobi) et les systèmes Hamiltonians dépendants de temps. Dans le deuxième chapitre nous étudions les propriétés géométriques des quotients de variétés Sasaki et Kähler. Nous construisons une procédure de réduction pour les variétés symplectiques et Kähler (munies de symétries générées par un groupe de Lie) qui utilise les préimages rayon de l'application moment. Précisémmant, au lieu de considérer comme dans la réduction de Marsden-Weinstein (ponctuelle) la préimage d'une valeur moment μ, nous utilisons la préimage de ^+μ, le rayon positif de μ. Nous avons trois motivations pour développer cette construction. Une est géométrique: la construction des espaces réduits de variétés Kähler correspondant á un moment non nulle qui soient canoniques dans le sense que la structure Kähler réduite est la projection de la structure Kähler initiale. La réduction ponctuelle (Marsden-Weinstein) donnée par M_μ:=J^- 1 (μ) /G_μ où μ est une valeur de l'application moment J et G_μ est le sous-groupe d'isotropie de μ par rapport à l'action coadjointe de G n'est pas toujours bien définie dans le cas Kähler (si G≠ G_μ). Le problème est causé par le fait que la structure complexe de M ne préserve pas la distribution horizontale de la submersion Riemannienne qui projète J^- 1 (μ) sur M_μ. La solution proposée dans la litterature utilise l'espace réduit à moment zéro de la difference symplectique de M avec l'orbite coadjointe de μ munie d'une forme Kähler-Einstein unique (construite par exemple dans besse, Chapitre 8) et différente de la forme de Kostant-Kirillov-Souriau. L'unicité de la forme sur l'orbite coadjointe garantit un espace réduit bien défini. Par contre, ne plus utiliser la forme de Kostant-Kirillov-Souriau entraîne le fait que l'espace réduit n'est plus canonique. L'espace réduit rayon que nous construisons est canonique et peut être défini pour tout moment. Il est le quotient de J^- 1 (^+μ) par rapport à un certain sous-groupe normal de G_μ. La deuxième raison est une application à l'étude des systèmes Hamiltonians conformes (voir mclachlan [...] perlmutter). Ce sont des systèmes mécaniques non-autonomes, avec friction dont les courves intégrales préservent, dans le cas des symétries, les préimages rayons de l'application moment. Nous extendons la notion de champ Hamiltonian conforme, en montrant qu'on peut ainsi inclure dans cet étude de nouveaux systèmes mécaniques. également, nous présentons la réduction de systèmes Hamiltonians conformes. La troisième raison consiste à trouver des conditions necéssaires et suffisantes pour que les espaces réduits (rayons) des variétés Kähler (Sasakian) -Einstein soient aussi Kähler (Sasakian) -Einstein. Nous nous occupons de cela dans le deuxième chapitre de la thèse, dans dragulete [...] ornea et dans dragulete [...] doi où nous utilisons des techniques de A. Futaki. Ainsi, nous pouvons construire de nouvelles structures de Sasaki-Einstein. Comme exemples de réductions rayon symplectic (Kähler) et contact (Sasaki) nous traitons le cas des fibrés cotangent et cosphérique. Nous montrons qu'ils sont des espaces universels pour la réduction rayon. Des exemples d'actions toriques sur des sphères sont aussi décrits. Le troisième chapitre de cette thèse traite l'étude de l'espace des orbites d'un groupoïde propre. Dans weinstein [...] unu, weinstein [...] doi A. Weinstein a partiellement résolu le problème de la linéarisation des groupoïdes propres. En zung, N. T. Zung l'a achevé en démontrant un théorème de type Bochner pour les groupoïdes propres. Nous prouvons un théorème de stratification de l'espace d'orbites d'un groupoïde propre en utilisant des idées de la théorie des foliations et le théorème de "slice" (linéarisation) de Weinstein et Zung. Nous montrons explicitement que le feuilletage orbital d'un groupoïde propre est un feuilletage Riemannien singulier dans le sense de Molino. Pour cela nous avons deux motivations. D'un côté nous voulons montrer qu'il y ait une équivalence entre groupoïdes propres et "orbispaces" (des espaces qui sont localement des quotiens par rapport à l'action d'un groupe de Lie compact) et d'un autre nous voulons étudier la réduction des actions infinitésimales (actions d'algèbres de Lie) qui ne sont pas intégrables à l'action d'un groupe de Lie. Ces actions et leur intégrabilité ont été étudiées, entre autres, par Palais (palais), Michor, Alekseevsky...|$|E
40|$|Aiming {{to define}} {{irrigation}} strategies improving the water productivity by folder crops under water scarcity in the irrigated perimeter of Tadla (Morocco), this work combines field experimentation and modeling. The field study of crop response to water stress {{is important to}} maximize yield and improve agricultural water use efficiency (WUE) in areas where water resources are limited. On the silage maize, {{the results showed that}} water deficit affected plant height growth, accelerated the senescence of the leaves and reduced the leaf area index. Dry matter yields varied from 3. 9 t. ha- 1 under T 5 (20 % ETc) to 16. 4 t. ha- 1 under T 1 (100 % ETc). The establishment of the water budget by growth phase showed that the water use efficiency was higher during the linear phase of growth. WUE calculated at harvest varied between 2. 99 kg. m- 3 under T 1 and 1. 84 kg. m- 3 under T 5. The actual evapotranspiration under T 1 (100 % ETc) was 478 mm and 463 mm in 2009 and 2010, respectively. The yield response factor (Ky) for the silage maize for both growth seasons was 1. 12. The ETc of silage maize was determined using lysimeter drainage at 415 mm. the mean values of crop coefficients Kc were 0. 56, 1. 22 and 1. 05 for beginning phase, mid-season and at harvest (grain milky pasty stage) respectively. Drip irrigation allows obtaining dry matter yields similar to flood irrigation but with less water and saves about 30 % of irrigation water applied. Over five cycles, berseem dry biomass yields achieved under T 1 are 14. 3 and 13. 9 t/ha in 2009 / 10 and 2010 / 11 respectively. The yield reductions by applying 60 % of water requirements are 40 and 42 % in 2009 / 10 and 2010 / 11 respectively. Berseem daily productivity increases with more water applied with the highest value of 102 kg DM/ ha/ day. The dry matter content increases with water stress. The mean values range between 12. 3 and 23. 7 % under T 1 (100 % ETc) and T 4 (40 % ETc) respectively. The contribution of without irrigation cycles (rainy period) on the total annual yield may vary from 35 % to 52 % under treatments T 1 and T 4 respectively. Water balance achieved by water regime shows that drainage losses increase with more water applied especially in the first cycle. WUE is low during the first cycle, optimal in 2 nd, 3 rd and 4 th cycle and decreases in the last one with water stress. Global WUE of berseem determined over the entire crop period (slope of the regression line) is 3. 37 kg. m- 3. The yield response factor (Ky) for the berseem for both growth season was 1. 11. Berseem ETc determined by drainage lysimeter was 520 mm. The Kc values were estimated for each cycle for all three phases: initial, development (median) and mid-season. Maximum yield average under drip irrigation was 15. 7 t/ha and obtained with 411 mm of water supply allowing to save 57 % of water compared to traditional irrigation technique. Comparing the behavior of six alfalfa varieties most commonly practiced in the irrigated perimeter of Tadla shows that the "Super Siriver" cultivars followed by «Trifecta» have higher yield potential and higher tolerance to water deficit. Alfalfa maximum annual yield obtained was 24. 2 t. ha- 1. The contribution of the spring cycles to the annual yields range from 55 % under T 1 (100 % ETc) to 65 % under T 4 (40 % ETc). In addition to water quantities, alfalfa yields depend on time application during a growth cycle. WUE varies from cycle to another and from one season to another. The maximum value was 2. 57 kg. m- 3 and obtained in spring 2011, while the low value was 0. 64 kg. m- 3 and obtained in winter 2010. WUE decreases with water stress, with mean values of 1. 83, 1. 67, 1. 54 and 1. 23 kg. m- 3 under T 1 (100 % ETc), T 2 (80 % ETc), T 3 (60 % ETc) and T 4 (40 % ETc) respectively. The yield response factor (Ky) of alfalfa was 0. 92. The determination of the alfalfa water requirements was performed on the basis of cycle’s calendar during two years in 2010 and 2011. The values founded for flood irrigation are 1388 and 1364 mm respectively for the two years. Drip irrigation allows achieving similar dry mater yield to flood irrigation with less water and agronomic efficiency. Water applied under T 1 in drip irrigation with 50 cm of spacing between ramps was less than water requirements (of alfalfa) by about 7 % and 18 % in 2010 and 2011 respectively. Under the same treatment in flood irrigation, the water requirements are exceeded by 16 % and 21 % in 2010 and 2011 respectively. Two crop models, PILOTE and CropSyst, had been selected to be tested on their ability to simulate the growth and yield of the studied crops under the edaphic-climatic conditions of Tadla. Tested on silage maize, both models correctly simulated the growth and development of the crop under different water regimes. The parameters of both models are validated and shown effective for simulation of biomass, leaf area index and soil water storage. Although PILOTE requires less parameters and data than CropSyst, it often proves to be more successful in simulating the biomass of silage maize and water balance. As to berseem, predictions of biomass by CropSyst seem to be more accurate than PILOTE model. The latter was best at predicting the soil water reserve on the soil depth exploited by the roots (0 - 80 cm). Given its ease of integrating daily climatic data for several years, CropSyst model was chosen to test its ability to simulate the crop rotation of berseem and maize silage. The results show that this model correctly simulates the evolution of biomass and yields of the two crops considered in rotation during three years. Modeling the growth and production of alfalfa is made by both models outside the crop installation period (seeding year). After calibration and validation achieved, the model CropSyst simulates adequately biomass and soil water reserve under all water regimes considered while PILOTE best simulations were limited to non-stressed treatment T 1 (100 % ETc). Although CropSyst model takes into account several parameters in the simulation of alfalfa growth, their simplifications (unique values) reduce its performance in more water stress situation. Less parameters considered in the PILOTE model makes it validation difficult for perennial crops such as alfalfa. CropSyst model was used to evaluate irrigation practices of farmers and develop irrigation virtual scenarios for the three crops studied. The assessment shows that virtual scenario developed for alfalfa that applying 1600 mm of irrigation water amount through 14 applications divided into six irrigations during the spring, six in summer, one in the fall and another in early winter maximizes irrigation water efficiency (1. 21 kg/m 3) and achieve a yield of 23. 1 t/ha (95 % of the yield potential). In the case of maize, if water is available, application of 648 mm according to the combination [2 irrigations in initial phase (after sowing), 2 irrigations in linear and two in final phases] allows to achieve high biomass yield and better water use. On berseem, the simulation results confirm that the adoption of the scenario that provides 625 mm through 7 irrigations (3 in autumn, 2 in winter and 2 in spring) allows obtaining 14. 1 t/ha of dry matter which represents 94 % of the yield potential of the 6454 cultivar. This scenario allows greater water efficiency (1. 24 kg/m 3) and results in low water drainage losses estimated at about 17 % of applied water. The comparison of the two cropping systems represented by alfalfa and silage maize-berseem rotation shows that the rotation allows the better water use and mobilizes less water than alfalfa, which is distinguished by its profitability. Finally, the coupling of the results of three years (2008 to 2011) "in situ" experimentations with the simulations of scenarios by CropSyst and PILOTE models has shown to be highly effective in order to improve the folder crops irrigation in the Tadla irrigated area in Morocco. Dans l’objectif d’améliorer la productivité des principales cultures fourragères au périmètre irrigué de Tadla dans des situations hydriques de plus en plus sévères, le présent travail associe l’expérimentation sur terrain et la modélisation. L’étude de la réponse des cultures au déficit hydrique est importante pour maximiser les rendements et améliorer l’efficience d’utilisation de l’eau. Sur le maïs ensilage, les résultats ont montré que le déficit hydrique affecte la croissance en hauteur des plants, accélère la sénescence des feuilles et réduit l’indice de la surface foliaire. Les rendements en matière sèche ont varié de 3, 9 t. ha- 1 sous T 5 (20 % ETc) à 16, 4 t. ha- 1 sous T 1 (100 % ETc). L’efficience d’utilisation de l’eau (EUE) est plus élevée durant la phase linéaire de croissance. L’EUE calculée à la récolte varie entre 2, 99 kg. m- 3 sous T 1 à 1, 84 kg. m- 3 sous T 5. L’évapotranspiration réelle sous T 1 (100 % ETc) est de 478 mm et 463 mm en 2009 et 2010, respectivement. La valeur du coefficient de réponse à l’eau (Ky) du maïs ensilage est de 1, 12. L’ETc du maïs a été déterminé par lysimètre à drainage à 415 mm. Les valeurs moyennes obtenues des coefficients culturaux sont de 0, 56, 1, 22 et 1, 05 pour le stade initial, mi-saison et avant la récolte (stade grain laiteux pâteux) respectivement. L’irrigation localisée permet d’obtenir des rendements similaires au système gravitaire mais avec des apports hydriques très réduits et permet ainsi d’économiser environ 30 % de l’eau d’irrigation. Sur des périodes de culture du bersim de 5 cycles, les rendements en biomasse atteints sous le régime T 1 sont de 14, 3 et 13, 9 t/ha en 2009 / 10 et 2010 / 11 respectivement. Les réductions de rendements en <b>appliquant</b> 60 % des apports en eau sont de 40 et 42 % en 2009 / 10 et 2010 / 11 respectivement. La productivité journalière du bersim augmente avec plus d’apport en eau avec comme valeur maximale 102 kg MS/ha/jour. Le taux de matière sèche augmente avec le stress hydrique avec des valeurs moyennes qui varient entre 12, 3 et 23, 7 % sous T 1 et T 4 respectivement. La contribution des cycles sans irrigation (période pluvieuse) au rendement total annuel peut varier de 35 % à 52 % sous les régimes T 1 et T 4 respectivement. La réalisation des bilans hydriques par régime hydrique montre que les pertes par drainage augmentent avec plus d’apport en eau surtout en 1 er cycle. L’efficience d’utilisation de l’eau est faible pendant le cycle d’installation, optimale pendant les trois cycles 2, 3 et 4 et diminue au dernier cycle avec le stress hydrique. L’EUE globale du bersim déterminée sur toute la période de culture (pente de la droite de régression) est de 3, 37 kg. m- 3. Le coefficient de réponse du rendement du bersim à l’eau est de 1, 11. L’ETc déterminée moyennant un lysimètre à drainage est 520 mm. Les valeurs de Kc ont été estimées pour chaque cycle pour les trois phases : initiale, développement (valeur médiane) et mi-saison. Le rendement moyen maximal obtenu sous le goutte à goutte est de 15, 7 t/ha et a été obtenu avec un apport en eau de 411 mm ce qui a permis une économie d’eau de 57 % par rapport au gravitaire. La comparaison du comportement de six variétés de luzerne les plus pratiquées dans le périmètre irrigué de Tadla montre que la variété « Super Siriver » suivie de « Trifecta » ont des potentialités de production très élevées avec des meilleures capacités de tolérance du stress hydrique. Le rendement annuel maximal obtenu pour la luzerne est de 24, 2 t. ha- 1. La contribution des cycles de printemps au rendement total annuel varie de 55 % sous T 1 (100 % ETc) à 65 % sous T 4 (40 % ETc). Les rendements de la luzerne dépendent en plus des quantités d’eau apportées de l’emplacement des apports à l’intérieur d’un cycle. L’efficience d’utilisation de l’eau varie d’un cycle à l’autre et d’une saison à l’autre. La valeur maximale est de 2, 57 kg. m- 3 et obtenue au printemps 2011 alors que la faible valeur est de 0, 64 kg. m- 3 qui est obtenue en hiver 2010. L’EUE diminue avec le stress hydrique avec des valeurs moyennes de 1, 83, 1, 67, 1, 54 et 1, 23 kg. m- 3 sous T 1, T 2, T 3 et T 4 respectivement. Le coefficient de réponse de rendement à l’eau (Ky) de la luzerne est de 0, 92. La détermination des besoins en eau de la luzerne a été réalisée sur la base des calendriers des cycles de la culture durant les deux campagnes 2010 et 2011. Les valeurs trouvées sous irrigation gravitaire sont de 1388 et 1364 mm pour les deux campagnes respectivement. L’irrigation localisée permet de réaliser des rendements similaires à l’irrigation gravitaire avec moins d’eau et plus d’efficience agronomique. Les apports en eau effectivement réalisés sous le régime T 1 avec l’écartement entre rampes de 50 cm sont inférieurs au besoin net de la culture d’environ 7 % et 18 % en 2010 et 2011 respectivement. Sous le même régime en irrigation gravitaire, les besoins sont dépassés de 16 % et 21 % en 2010 et 2011 respectivement. Deux modèles de cultures, PILOTE et CropSyst, avaient été retenus pour être testés sur leurs aptitudes à simuler la croissance et les rendements des trois cultures étudiés sous les conditions édapho-climatiques de Tadla. Sur le maïs, les deux modèles ont simulé correctement la croissance et le développement de cette culture sous des régimes hydriques variés. Les paramètres des deux modèles se sont montrés validés et efficaces pour la simulation de la biomasse, le LAI et le stock hydrique. Bien que PILOTE nécessite moins de paramètres et de données que CropSyst, il s'avère être souvent plus performant dans la simulation de la biomasse du maïs et du bilan hydrique. Concernant le bersim, les prédictions de la biomasse par CropSyst semblent être plus précises que celles du modèle PILOTE qui est à son tour meilleur au niveau de la prédiction du stock hydrique sur la tranche du sol exploitée par les racines (0 - 80 cm). Le modèle CropSyst a été retenu pour tester la possibilité de simuler la rotation bersim-maïs ensilage vu sa facilité d’intégrer des données climatiques journalières de plusieurs années. Les résultats montrent que ce modèle simule correctement l’évolution de la biomasse et les rendements des deux cultures considérées en rotation durant trois ans. La modélisation de la croissance et la production de la luzerne est réalisée pour les deux modèles en dehors de l’année d’installation de la culture. A l’issue de la calibration et de la validation réalisées, le modèle CropSyst simule convenablement la biomasse et le stock hydrique sur tous les régimes hydriques étudiés alors que les meilleures simulations de ces sorties par PILOTE se sont limitées au traitement non stressant T 1. Bien que le modèle CropSyst prenne en considération plusieurs paramètres dans la simulation de la croissance de la luzerne, leurs simplifications (valeurs uniques) réduisent ses performances surtout en situation de stress hydrique accentuée. Le peu de paramètre pris en considération dans le modèle PILOTE rend difficile sa validation pour une culture pérenne telle que la luzerne dans des situations variables. Le modèle CropSyst a été retenu pour évaluer les pratiques d’irrigation des agriculteurs et élaborer des scénarios virtuels d’irrigation pour les trois cultures étudiées. L’évaluation des scénarios virtuels élaborés montre que pour la luzerne, l’application d’une dose d’irrigation de 1600 mm selon un scénario qui prévoit l’application de 14 irrigations réparties en six arrosages durant le printemps, six en été, un en automne et un autre en début d’hiver permet de maximiser l’efficience de l’eau d’irrigation (EEI) (1, 21 kg/m 3) et de réaliser un rendement de 23, 1 t/ha (soit 95 % du potentiel). Sur le maïs, l’application d’une dose de 648 mm selon la combinaison [2 en phase initiale (au semis), 2 en phase linéaire et 2 en étape finale] permet d’atteindre le double objectif de réaliser un rendement élevé et garantir une meilleure valorisation de l’eau. Concernant le bersim, les résultats de simulation confirment que l’adoption du scénario qui prévoit l’application de 625 mm en 7 irrigations (3 en automne, 2 en hiver et 2 au printemps) permet de réaliser un rendement de 14, 1 t/ha qui représente 94 % du potentiel de la variété 6454. Ce scénario permet la meilleure efficience de l’eau d’irrigation (1, 24 kg/m 3) et occasionne de faibles pertes par drainage estimées à environ 17 % des apports en eau. La comparaison des deux systèmes de cultures représentés par la luzerne et la rotation maïs ensilage-bersim montre que la rotation valorise mieux l’eau et mobilise moins d’eau que la luzerne qui se distingue par sa rentabilité économique. Ainsi, le couplage des résultats de trois années d’expérimentations "in situ" (2008 à 2011) à ceux du calage, de la validation et des simulations de scénarios par les deux modèles CropSyst et PILOTE s'est révélé d’un grand intérêt pour l’amélioration de la conduite de l’irrigation des fourrages dans le périmètre irrigué de Tadla au Maroc...|$|E
40|$|The {{increasing}} human pressure exerted along coastlines and {{the subsequent}} increase {{in the delivery of}} pollutants at sea is a matter of concern worldwide. Urban wastewaters contain a variety of pollutants (mainly N, P, and trace elements) which can be involved in the launching of eutrophication. This complex process is able to fundamentally alter the integrity of coastal ecosystems thereby impairing the sustainability of economic activities and involving health risks for human through the consumption of sea products. Eutrophication is considered for more than 40 years as a pervasive process and a priority issue for the preservation of the health status of coastal ecosystems. The Mediterranean Sea supports high economic pressures in relation with the continuously increasing number of inhabitants and tourists in coastal areas. The physico-chemical (e. g. oligotrophy, microtidal regime) features of waters make them sensitive to eutrophication and several heavily urbanized areas have been experiencing adverse effects of this process for decades (e. g. biodiversity losses, Harmful Algal Blooms, fishes’ kills,…etc). Nowadays, smaller localities discharging insufficiently treated or raw wastewaters at sea also begin to report eutrophication problems especially during summer months when the number of tourists considerably increases the resident populations. National monitoring programs generally focus on priority areas by using either toxicological (e. g. trace metals in molluscs) and / or structural (e. g. phytoplankton biomasses) parameters to follow the evolution of already impacted water bodies. However, if the most deleterious effects of eutrophication are well known, little information is actually available regarding the early symptoms of the process. The identification of time- and cost-efficient indicators along with analytical procedures that would deliver early warning signals of pollution is therefore required to assist local authorities in the implementation of environmental policies. This research aimed at implementing some easy-to-use and efficient tools to detect the impact of urban wastewater pollution in Mediterranean coastal areas. A new set of potential early bioindicators has been identified. The gastropods Patella caerulea and Monodonta turbinata inhabiting the Mediterranean rocky midlittoral zone and epilithic biofilms were selected as good candidates for monitoring purpose. Biofilms which are microbenthic communities mainly composed of microalgae have been used for decades in freshwater systems as early indicators of pollution. In contrast, marine biofilms have largely been neglected and little is known regarding their composition, their physiology, and the way they react to wastewater pollution. We focused our studies on the Calvi Bay (NW Mediterranean, France) which is, regarding the anthropogenic pressure exerted on its coastal fringe, representative of other moderately urbanized areas of the Mediterranean basin. The Calvi Bay area is indeed among the preferred summer destinations by tourists in Corsica and is influenced by a single point source of pollution which is secondary-treated urban wastewater. A control vs. impacted sites approach has been used to assess the responses of the selected bioindicators to wastewater pollution. Our first task was to characterize the nature and amounts of the main pollutants (nutrients, trace elements) discharged in the Calvi Bay. Since nutrient measurements are commonly used to infer into the trophic status of water bodies, we assessed whether this parameter was reliable to detect the influence of wastewater discharge in the study area. The main pollutants discharged at sea were ammonium, phosphorus and iron. Amounts considerably increased during summer months (July and August) which was related to the high frequentation of the Calvi Bay area by tourists. Measurements of nutrient concentrations in seawater samples from controls and impacted sites did not allow evidencing the influence of wastewater inputs. In contrast, the high spatial and temporal resolution achieved through a small scale sampling design conducted in a small harbour impacted by wastewater discharges showed large spatial variability in the dispersion of effluents and hourly variations in the amounts of pollutants. The extrapolation of these results at the scale of the Calvi Bay may have explained our failure to detect the pollution by using samples collected punctually in space and time. These results suggested that the use of bioindicators that integrate the variable influence of nutrient pulses was required to evidence pollution. We focused our second study on a toxicological approach to detect the bioavailability of anthropogenically-derived nutrients in the midlittoral zone of the Calvi Bay and of the Marseille harbour. Multi-spatial scales and seasonal dual C and N stable isotope analyses were performed on the limpet Patella caerulea, the snail Monodonta turbinata, epilithic biofilms, and the macroalga Rissoella verruculosa. All bioindicators exhibited strongly elevated δ 15 N values at impacted sites compared to pristine ones, which revealed the influence of wastewater pollution in the midlittoral zone and the biological availability of anthropogenically-derived nitrogen {{at the base of the}} food web. Gastropods provided a time-integrated response reflecting the control vs. impacted status of sites. Results indicated that one sampling campaign per year should be sufficient to evidence wastewater pollution likely because of the slow turnover rate of gastropods’ muscles. Macroalgae showed a reliable but less consistent signal of wastewater pollution compared to other indicators. Only epilithic biofilms tended to show the occurrence of nutrient pulses during the tourist season which suggested that wastewater discharges may have influenced the composition and / or the physiology of communities. However the sampling of biofilms developed on natural rocky substrates was destructive and did not allow investigating the fine biological structure of communities and thus to fully understand the output of community scale parameters such as stable isotopes. We therefore recommended using biofilms grown on artificial substrates to circumvent this problem. We then allowed biofilms to develop on glass slides which are the most currently used artificial substrates in freshwater systems. Our task was to find out the most suitable technique to isolate and identify benthic diatoms which are common colonizers on newly available substrates. The species-specific tolerance to pollution of diatoms has been used for decades in the assessment of the health status of freshwater bodies. However, little is known on their marine counterparts and on their ability to evidence wastewater pollution. A specific and time-cost-efficient technique was implemented for the processing of lightly silicified benthic marine diatoms from Mediterranean oligotrophic areas. This was achieved through the multiple comparisons of existing protocols used either in sea- or fresh-waters. We finally investigated, by means of mesocosm deployed in situ and field experiments, the responses of biofilms developed on glass slides to a range of urban wastewater exposures. Colonization experiments lasted for 24 days in summer conditions. A multi-parametric assessment was conducted using a combination of toxicological and structural approaches applied to different biological scales. Toxicological parameters such as C-N stable isotopes, C:N:P ratios, and Trace Elements were measured at the community scale while the structural parameters were considered at the community (standing crops), assemblage (densities of the main autotroph and heterotroph organisms), and the genus (diatoms) scales. The mesocosm experiments were highly efficient to demonstrate the good potential of biofilms as early indicators of wastewater pollution. The impact of wastewater pollution was mainly identified at the community and the genus (diatoms) scales. Standing crops and the C-N stable isotopes were the most useful parameters showing respectively a stimulation of microalgal biomasses (i. e. eutrophication) and the bioavailability of wastewater-derived nitrogen even at low pollution levels. At the genus scale, the composition of diatoms’ assemblages changed markedly especially in the most polluted mesocosms. Results notably highlighted the proliferation of the small-sized individuals of Entomoneis which was thought to outcompete the larger diatoms belonging to the genus Mastogloia for nutrient uptake. The responses of biofilms to pollution largely differed between mesocosm and field experiments. Nevertheless, standing crop parameters corroborated results obtained in mesocosms allowing to assume an influence of wastewater pollution in the Calvi Bay. Field samples were generally characterized by the presence of well developed hydrozoan colonies which were only seldom reported at mesocosm sites. The presumably impacted site also exhibited the highest densities in other heterotrophic eukaryote groups (e. g. nematodes, polychaetes, foraminifers) and primary producers. We interpreted these results either as a top-down effect or as an increase in habitat complexity. The genus scale determination of diatoms’ assemblages showed a decrease in the relative abundances of Mastogloia at the impacted site which was in accordance with results from the mesocosm experiments. The highest density values were also observed for Cylindrotheca at impacted site. The last part of this research gathers the multiple spatial and temporal responses provided by the selected bioindicators to urban wastewater pollution in order to validate their future routine use in the context of monitoring programs. The occurrence of potential confusion sources in the interpretation of data was critically reviewed. The time-cost-efficiency of the tested parameters was then evaluated in order to assist environmental managers in their choices of biofilm-based techniques for detecting wastewater pollution. Finally since biofilms exhibited some strong accumulations of toxic elements, ecological hypotheses dealing with the trophic role of biofilms and the transfer of pollutants through the food webs are provided. La pression démographique croissante exercée le long des zones côtières et l’augmentation des rejets de polluants qu’elle implique est une préoccupation environnementale majeure à l’échelle de la planète. Les eaux usées urbaines contiennent une grande variété de polluants (principalement N, P, et des éléments traces) qui peuvent conduire à une eutrophisation des systèmes aquatiques. Ce processus complexe est en mesure d’altérer de manière fondamentale le fonctionnement des écosystèmes côtiers et ainsi de compromettre la poursuite des activités économiques liées au milieu marin. De plus, les effets de l’eutrophisation peuvent engendrer des risques sanitaires pour l’homme lors de la consommation de produits de la mer. L’eutrophisation est considérée depuis plus de 40 ans comme une menace majeure pour la préservation de l’état de santé des écosystèmes côtiers. Les zones côtières Méditerranéennes sont soumises à d’intenses pressions économiques en raison de l’augmentation continue du nombre d’habitants et de touristes. Les propriétés physico-chimiques (e. g. oligotrophie, régime microtidal) des masses d’eau de la Méditerranée les rendent sensibles à l’eutrophisation et plusieurs secteurs densément peuplés font face depuis des décennies aux effets délétères du processus d’eutrophisation (e. g. pertes de biodiversité, poussées d’algues toxiques, mortalité de poissons,…etc). Actuellement, des localités moins peuplées rejetant des eaux usées insuffisamment ou non traitées rapportent également des problèmes d’eutrophisation durant les mois d’été, lorsque la fréquentation touristique est maximale. Les programmes nationaux de surveillance de la pollution marine s’attachent généralement au suivi de secteurs déjà impactés et utilisent des outils fournissant une information toxicologique (e. g. contenus en métaux traces dans les mollusques) et / ou structurelle (e. g. biomasses du phytoplankton) afin de statuer sur l’état de santé du milieu. Cependant, si les effets les plus néfastes associés au phénomène d’eutrophisation sont bien connus, peu d’informations sont disponibles en ce qui concerne ces premiers symptômes. L’identification d’indicateurs efficaces peu coûteux en argent et en temps ainsi que de techniques d’analyses appropriées permettant de fournir un signal précoce de détérioration de la qualité du milieu naturel est donc requise afin de guider les décisions politiques en matière d’environnement. La présente recherche a pour but de développer des outils facilement utilisables et efficaces afin de détecter l’impact de la pollution par les eaux usées urbaines en milieu côtier Méditerranéen. Un nouveau set d’indicateurs ayant un potentiel de détection précoce de la pollution a été identifié. Les gastéropodes Patella caerulea et Monodonta turbinata présents en zone rocheuse médiolittorale ainsi que les biofilms épilithes sont de bons candidats pour le suivi de la pollution. Les biofilms, qui sont des communautés microbenthiques essentiellement composées de microalgues, sont utilisés depuis des décennies en tant qu’indicateurs précoces de pollution en milieux d’eau douce. En revanche, leurs homologues des zones marines ont été négligés et peu d’informations sont disponibles concernant leurs compositions, leurs physiologies ainsi que la façon dont ils réagissent face à la pollution. Nous avons essentiellement réalisé nos études dans la Baie de Calvi (NO Méditerranée, France) dont la pression anthropique exercée le long de ses côtes est jugée représentative de celle ayant lieu dans d’autres secteurs modérément urbanisés de la mer Méditerranée. La région de Calvi est en effet parmi les destinations touristiques les plus prisées en Corse. De plus, elle est influencée par une unique source ponctuelle de pollution qui est matérialisée par des rejets d’eaux usées urbaines ayant subi un traitement d’épuration physico-chimique. Le potentiel de bioindicateurs des organismes ou groupes d’organismes sélectionnés a été évalué à partir de comparaisons des réponses obtenues entre des sites potentiellement impactés par la pollution et des sites jugés sains. Notre premier objectif était de caractériser les quantités de principaux polluants (nutriments, éléments traces) rejetées en Baie de Calvi. Puisque la mesure des concentrations en nutriments est un paramètre communément utilisé dans l’évaluation du statut trophique des masses d’eau, nous avons tout d’abord évalué l’efficacité de ce paramètre dans la détection de la pollution. Les principaux polluants contenus dans les effluents sont l’ammonium, le phosphore et le fer. Les quantités rejetées en mer ont augmenté considérablement en été (Juillet et Août) en relation avec l’augmentation de la fréquentation touristique. Les mesures de concentrations en nutriments dans l’eau de mer n’ont pas permis de détecter l’influence des rejets d’eaux usées. En revanche, des mesures répétées effectuées dans un port influencé par des rejets d’eaux usées ont permis de démontrer que l’influence de la pollution variait grandement dans le temps (heure) et l’espace (dizaine de mètres). L’extrapolation de ces résultats à l’échelle de la Baie de Calvi peut expliquer le manque de succès rencontré dans la détection de la pollution en utilisant des échantillons récoltés ponctuellement dans l’espace et le temps. Cela suggère également l’utilisation de bioindicateurs intégrant l’influence variable de la pollution dans leurs tissus. Notre deuxième étude s’est basée sur une approche toxicologique visant à détecter la biodisponibilité des nutriments dérivés des eaux usées dans la zone médiolittorale de la Baie de Calvi et du port de Marseille. Des mesures des isotopes stables de l’azote et du carbone ont été réalisées sur une base saisonnière et à plusieurs échelles spatiales sur Patella caerulea, Monodonta turbinata, les biofilms épilithes ainsi que sur une macroalgue Rissoella verruculosa. Tous les bioindicateurs ont montrés des valeurs δ 15 N particulièrement élevées au niveau des sites impactés par rapport aux sites sains, ce qui a permis de révéler l’influence de la pollution dans la zone médiolittorale et notamment la biodisponibilité de l’azote dérivé des eaux usées à la base du réseau trophique. Les gastéropodes ont procuré une réponse intégrée dans le temps reflétant le statut des sites. Ces résultats laissent supposer qu’une campagne de prélèvement annuelle serait suffisante afin de détecter l’influence de la pollution probablement en raison d’un temps de renouvellement relativement lent des muscles de gastéropodes. Les macroalgues ont également procuré un signal de pollution fiable mais cependant moins cohérent que celui des autres bioindicateurs. Seuls les biofilms épilithes ont eu tendance à démontrer une plus importante influence des rejets en période touristique suggérant que la composition et / ou la physiologie des communautés ont pu être modifiées par la pollution. Cependant, la récolte du biofilm sur les rochers ne permet plus l’identification des organismes qui le composent et n’autorise donc pas une interprétation complète des valeurs isotopiques. Par conséquent, nous avons proposé l’utilisation de substrats artificiels afin de contourner ce problème. Notre troisième étude a porté sur l’utilisation de lames de verre comme substrat artificiel pour le développement de biofilms. Cette technique est couramment employée en eau douce. Notre premier objectif était de mettre au point les méthodes les plus fiables afin d’isoler et d’identifier les diatomées benthiques qui sont parmi les colonisateurs les plus fréquents et abondants des substrats nouvellement disponibles. Les gammes de tolérance spécifiques des diatomées benthiques en fonction du degré de pollution sont bien connues en eau douce mais très peu étudiées en milieu marin. Un protocole peu coûteux, rapide et efficace a pu être mis en place afin de traiter les échantillons de diatomées benthiques finement silicifiées provenant de régions Méditerranéennes oligotrophes. Cela a été permis grâce à la comparaison des résultats obtenus en <b>appliquant</b> les protocoles couramment utilisés en eau douce ou en milieu marin. Dans un quatrième temps, nous avons utilisé les lames de verre dans le cadre d’expériences en mésocosmes et sur le terrain afin d’évaluer la réponse des biofilms face à différents degrés de pollution par les eaux usées. Ces expériences ont duré 24 jours et se sont déroulées durant la période estivale. L’évaluation multiparamétrique de la réponse des biofilms à la pollution a été réalisée selon une combinaison d’approches toxicologiques et structurelles portant sur différentes échelles biologiques. Les paramètres toxicologiques tels que les isotopes stables du carbone et de l’azote, les rapports C :N :P, et les contenus en éléments traces ont été mesurés à l’échelle des communautés. Les paramètres structuraux ont été considérés aux d’échelles des communautés (biomasses, Chl a), des assemblages (densités des principaux organismes autotrophes et hétérotrophes), et des genres (diatomées). Les expériences en mésocosmes ont été particulièrement efficaces pour démontrer le grand potentiel des biofilms en tant qu’indicateurs précoce de pollution par les eaux usées. L’impact de la pollution a principalement été identifié aux échelles des communautés et des genres. Les résultats des mesures de biomasses et des analyses isotopiques ont été les plus utiles afin de respectivement mettre en évidence les symptômes de l’eutrophisation et la biodisponibilité de l’azote dérivé des eaux usées et ce même à de faibles niveaux de pollution. A l’échelle des genres, la composition des assemblages de diatomées a été modifiée de manière prononcée principalement dans les mésocosmes les plus pollués. Les résultats ont témoigné de la prolifération de cellules de petites tailles appartenant au genre Entomoneis. Le rapport surface : volume élevé de ces cellules leur a probablement procuré un avantage compétitif pour l’uptake de nutriments par rapport aux plus grandes cellules du genre Mastogloia. La comparaison des résultats issus des expériences de terrain et de mesocosmes a montré de grandes différences concernant les réponses des biofilms face à la pollution. Néanmoins, les paramètres de biomasses ont corroboré les résultats obtenus dans les mésocosmes ce qui a permis de supposer une influence des effluents dans la Baie de Calvi. La composition des échantillons provenant de l’expérience de terrain a généralement révélé un grand nombre de colonies d’hydrozoaires lesquelles n’étaient que rarement présentes dans les biofilms issus des mésocosmes. Le site potentiellement impacté par la pollution a également témoigné des densités les plus importantes d’organismes appartenant aux autres groupes d’hétérotrophes eucaryotes (e. g. nématodes, polychètes, foraminifères) et de producteurs primaires. Nous avons interprété ces résultats comme étant probablement dû à un effet « top-down » ou à une augmentation de la complexité de l’habitat. La détermination des diatomées a montré une diminution des abondances relatives du genre Mastogloia au niveau du site potentiellement impacté par la pollution ce qui concorde avec les résultats obtenus dans les mésocosmes pollués. De plus, les densités les plus élevées de Cylindrotheca ont également été observées en ce même site. La cinquième partie de cette dissertation synthétise l’ensemble des réponses formulées par les bioindicateurs aux différentes échelles spatiales et temporelles. Cette étape est en effet indispensable afin de valider l’utilisation d...|$|E
40|$|Life Cycle Assessment (LCA) {{is a tool}} {{developed}} to evaluate {{the environmental impact of}} a product or a system. After a decade of research in the LCA field, significant progress has been achieved but methodologies for the assessment of toxicological impacts on human health are still in the development phase. This dissertation contributes to the research required in this field. More specifically, its main objective is to develop a Life Cycle Impact Assessment (LCIA) procedure for human health respecting the guidance developed under the umbrella of the Society of Environmental Toxicology and Chemistry (SETAC). This means that we aim to implement an original procedure to quantify the potential carcinogenic and noncarcinogenic effects of toxic releases on human health (chapters 2 and 3), and to develop a new method describing the fate of atmospheric releases and the resulting exposure on humans (chapter 4). A framework summarized in figure 5. 1 is also proposed to combine the effect assessment with the fate and exposure assessment, in order to derive a so-called human damage factor (chapter 5). A set of heavy metals (cadmium, chromium(VI), chromium(III), copper, methylmercury, beryllium, lead and inorganic arsenic) and of criteria air pollutants (CO, SO 2, NOx and fine particles) is chosen for a full application of the procedure developed in this dissertation. The use of this procedure to the Cycleaupe case study is also part of the objectives of this research. This study aims to determine whether systems using rainwater or reducing water consumption are "friendlier" from an environmental perspective than conventional toilet flushing (chapter 6). Figure 5. 1. Overview of the framework proposed in this thesis for assessing the damage induced on human health by a toxic released into air. In chapters 2 and 3, a new paradigm based on the effect dose ED 10 h is derived from the Risk Assessment concept of benchmark dose. It is proposed and explored {{for the first time in}} LCIA. The ED 10 h is defined as the best estimate of the dose which induces a 10 % added risk over background for humans. Carcinogenic and noncarcinogenic risks towards humans are characterized by drawing a straight line from the ED 10 h down to the origin of the dose-response function. The slope of this straight line is called the slope factor and is denoted βED 10. The linear dose-response function without threshold, which is assumed in this ED 10 -approach, is discussed. The ED 10 h is calculated for chemicals with bioassay data available in the Integrated Risk Information Service (IRIS) database provided by the US Environmental Protection Agency (US EPA). New correlations between the ED 10 h and the more widely available tumor dose TD 50 a (for carcinogenic effects) and the No Observable Adverse Effect Level NOAEL (for noncarcinogenic effects) are determined. They are applied to quantify the slope factor of more than 900 chemicals. A weighting of the different health outcomes associated with chemicals is proposed, based upon the Disability Adjusted Life Years per affected person (DALYp) concept. For carcinogenic endpoints, the DALYp is calculated for different types of tumors, using data reported in the literature. This shows that all cancers have more or less the same severity and an average DALYp of 11. 1 years of life lost per affected person is derived. For noncarcinogenic effects, a simplified classification of the adverse effects into three categories is chosen and a DALYp of 11. 1, 1. 1 and 0. 11 years of life lost per affected person is respectively assigned to each of the three categories. Finally, the slope factor βED 10 and the DALYp for each substance are combined together in an original way to derive its effect factor. This effect factor is expressed in years of life lost per absorbed mass. Appendix 1. 1 summarizes the effect factors calculated for more than 900 toxic releases. Effect factors for carcinogenic outcomes range from 1. 3 · 10 - 9 for cinnamyl anthranilate up to 3. 4 · 10 - 1 [yr lost / mg absorbed] for 2, 3, 7, 8 -tetrachlorodibenzo-p-dioxin. Effect factors for noncarcinogenic endpoints range from 4. 2 · 10 - 12 for 1 -Chloro- 1, 1 -difluoroethane to 1. 4 · 10 - 3 [yr lost / mg absorbed] for beryllium. In chapter 4, a semi-empirical approach is {{developed to}} evaluate the fate and exposure for atmospheric releases of metals, carbon monoxide (CO), sulfur dioxide (SO 2), nitrogen oxides (NOx) and fine particles. For that purpose, we apply for the first time in LCA the concept of exposure efficiency, which is defined as the ratio between the dose absorbed by the population and the emission inducing that absorption. Three types of exposure efficiency are defined for a world release into air of a given compound. A specific exposure efficiency is directly based on the rural and urban concentrations inhaled by humans. A continental exposure efficiency is defined by considering an uniform world continental concentration over urban and rural inhabited regions (marine and desert regions are excluded). A global exposure efficiency issimilarly defined from the global world concentration of a substance. Exposure efficiencies are calculated for fine particles, CO, NOx and SO 2. The specific exposure efficiency ranges from 3. 9 · 10 - 6 to 2. 4 · 10 - 5 [mg absorbed / mg emitted], demonstrating that only a very small fraction of an air release is inhaled by humans. The exposure efficiency for metals after inhalation is assumed to be equal to the exposure efficiency for fine particles, since airborne metals are attached to particulate matter. If atmospheric deposition on an agricultural soil occurs, humans can be exposed through a transfer into food products. A first evaluation of this transfer indicates that it can increase the exposure efficiency of metals released into air by a factor 5 up to 70. Specific exposure efficiencies are selected in this thesis to describe the fate and exposure of atmospheric releases. We show for the first time that specific exposure efficiencies are higher by a factor 3 than continental exposure efficiencies, indicating that the use of one-box continental models tend to underestimate the exposure efficiency that can be expected in the real world. This is due to the fact that higher emissions occur in highly populated regions. As a first approximation, the factor 3 could be used as a corrective factor to derive the specific exposure efficiency from the exposure efficiency predicted by one-box continental models. In chapter 5, exposure efficiencies presented in chapter 4 and effect factors presented in chapters 2 and 3 are multiplied to derive the so-called Human Damage Factors (HDF). The damage factors are expressed in years of life lost per emitted mass. Using that factor, the emission of a substance can be converted into its potential damage induced on humans. The damage factors are calculated for NOx, SO 2, CO and fine particles, as well as for the selected set of metals released into air or into agricultural soils (see appendix 1. 2 for the summarized results). When the transfer into food products is not accounted for, the damage factors for the studied metals range from 1. 7 · 10 - 11 for chromium(VI) up to 1. 3 · 10 - 8 [yr lost / mg emitted] for beryllium. Lead has the highest damage factor (1. 9 · 10 - 8 [yr lost / mg emitted]) if transfer into food products is considered. Damage factors ranging from 2. 7 · 10 - 10 to 6. 6 · 10 - 10 [yr lost / mg emitted] are found for NOx, SO 2 and fine particles, while carbon monoxide is characterized by a damage factor 103 -folds lower. Per emitted mass, metals inhaled by humans induce damages of the same order of magnitude than NOx, SO 2 and fine particles; when atmospheric deposition on agricultural soils and its subsequent transfer into food are accounted for, metals present higher damage factors. An indirect validation of the damage factors is presented for SO 2, NOx, CO, fine particles and some metals, by applying their damage factors to their total emissions over Switzerland and Europe. The evaluated damages are plausible and in accordance with results reported in other studies. In chapter 6, a Life Cycle Analysis is performed to compare five scenarios for toilets flushing. This LCA is the first one carried out on the whole water cycle, including both thesimilarly defined from the global world concentration of a substance. Exposure efficiencies are calculated for fine particles, CO, NOx and SO 2. The specific exposure efficiency ranges from 3. 9 · 10 - 6 to 2. 4 · 10 - 5 [mg absorbed / mg emitted], demonstrating that only a very small fraction of an air release is inhaled by humans. The exposure efficiency for metals after inhalation is assumed to be equal to the exposure efficiency for fine particles, since airborne metals are attached to particulate matter. If atmospheric deposition on an agricultural soil occurs, humans can be exposed through a transfer into food products. A first evaluation of this transfer indicates that it can increase the exposure efficiency of metals released into air by a factor 5 up to 70. Specific exposure efficiencies are selected in this thesis to describe the fate and exposure of atmospheric releases. We show for the first time that specific exposure efficiencies are higher by a factor 3 than continental exposure efficiencies, indicating that the use of one-box continental models tend to underestimate the exposure efficiency that can be expected in the real world. This is due to the fact that higher emissions occur in highly populated regions. As a first approximation, the factor 3 could be used as a corrective factor to derive the specific exposure efficiency from the exposure efficiency predicted by one-box continental models. In chapter 5, exposure efficiencies presented in chapter 4 and effect factors presented in chapters 2 and 3 are multiplied to derive the so-called Human Damage Factors (HDF). The damage factors are expressed in years of life lost per emitted mass. Using that factor, the emission of a substance can be converted into its potential damage induced on humans. The damage factors are calculated for NOx, SO 2, CO and fine particles, as well as for the selected set of metals released into air or into agricultural soils (see appendix 1. 2 for the summarized results). When the transfer into food products is not accounted for, the damage factors for the studied metals range from 1. 7 · 10 - 11 for chromium(VI) up to 1. 3 · 10 - 8 [yr lost / mg emitted] for beryllium. Lead has the highest damage factor (1. 9 · 10 - 8 [yr lost / mg emitted]) if transfer into food products is considered. Damage factors ranging from 2. 7 · 10 - 10 to 6. 6 · 10 - 10 [yr lost / mg emitted] are found for NOx, SO 2 and fine particles, while carbon monoxide is characterized by a damage factor 103 -folds lower. Per emitted mass, metals inhaled by humans induce damages of the same order of magnitude than NOx, SO 2 and fine particles; when atmospheric deposition on agricultural soils and its subsequent transfer into food are accounted for, metals present higher damage factors. An indirect validation of the damage factors is presented for SO 2, NOx, CO, fine particles and some metals, by applying their damage factors to their total emissions over Switzerland and Europe. The evaluated damages are plausible and in accordance with results reported in other studies. In chapter 6, a Life Cycle Analysis is performed to compare five scenarios for toilets flushing. This LCA is the first one carried out on the whole water cycle, including both the water supply and the wastewater treatment. The drinking water supply system, the rainwater recuperation system and the wastewater treatment system are included in the system boundaries. Results demonstrate that economic toilets (3. 5 [l/flushing]) lead to a significant reduction of the energy requirements compared to conventional toilets (9 [l/flushing]). A conventional water supply and a rainwater recuperation with a storage tank of 10 m 3 are characterized by similar energy consumption. A rainwater storage tank of 20 m 3, designed to be completely independent of the conventional water supply system, is energetically disadvantageous. Calorific losses, linked to the temperature increase of flushing water within the house, have a significant contribution to the energy requirement. The advantage of economic toilets is confirmed when looking at the inventory emissions. An initial LCIA was performed using the critical surface-time CST 95 method of Jolliet and Crettaz [1997]. It showed that the conventional scenario using economic toilets (CONVeco) is the most advantageous for all impact classes. When applying the human damage factors developed in this thesis (see chapter 5), the conventional scenario (CONVeco) is still characterized by lower impacts on humans than the recuperation scenario (REC 10 eco). However, the substances having the major effect on human health differ from those found with the CST 95 method; reasons for that change are discussed. L'Analyse de Cycle de Vie (ACV) permet d'évaluer l'impact environnemental d'un produit ou d'un système. Après plusieurs années de recherche dans le domaine des ACV, des progrès significatifs ont été réalisés. Cependant, les méthodologies d'évaluation de l'impact toxicologique des substances toxiques sur la santé humaine sont toujours en phase de développement. La présente thèse contribue à la recherche supplémentaire requise dans ce domaine. Plus spécifiquement, son objectif principal est de développer une méthode d'évaluation de l'impact environnemental pour la santé humaine. Ladite méthode doit respecter la structure développée par la société européenne de toxicologie et de chimie (SETAC). Cette thèse vise donc à mettre en place une procédure permettant de quantifier les effets cancérigènes et non cancérigènes des substances chimiques sur la santé humaine (chapitres 2 et 3). Une méthode décrivant le devenir des substances émises dans l'air et l'exposition en résultant sur les humains est également proposée (chapitre 4). Un cadre d'analyse (voir figure 5. 1) est proposé afin de combiner l'évaluation de l'effet toxique, avec l'évaluation du devenir des substances. Un facteur de dommage sur l'homme peut alors être déduit (chapitre 5). Un ensemble de métaux lourds (cadmium, chrome(VI), chrome(III), cuivre, méthylemercure, béryllium, plomb et arsenic inorganique) et de polluants atmosphériques (CO, SO 2, NOx et particules fines) est choisi pour une application complète de la méthode développée dans cette thèse. La méthode est testée à l'étude dénommée "Cycleaupe". Cette étude vise à déterminer si les systèmes utilisant l'eau pluviale ou réduisant la consommation d'eau induisent une charge environnementale moindre qu'un rinçage conventionnel des toilettes (chapitre 6). Figure 5. 1. Overview of the framework proposed in this thesis for assessing the damage induced on human health by a toxic released into air. Dans les chapitres 2 et 3, une nouvelle approche, basée sur la dose d'effet notée ED 10 h, est dérivée du concept de "benchmark dose" développé en Evaluation du Risque. Elle est proposée et explorée pour la première fois en ACV dans cette thèse. L'ED 10 h est définie comme la meilleure estimation de la dose induisant un risque pour les hommes de 10 % par rapport au niveau de base. Le risque cancérigène et non cancérigène pour les hommes se caractérise en traçant une ligne droite à partir de ED 10 h vers l'origine de la fonction "dose-réponse". La pente de cette droite est appelée le facteur de pente et est dénoté βED 10. La fonction "dose-réponse" linéaire et sans seuil, qui est supposée dans l'approche proposée, est discutée. L'ED 10 h est calculée pour des substances toxiques ayant des données d'essai sur animaux disponibles dans la base de données IRIS (Integrated Risk Information Service database) de l'Agence Américaine de l'Environnement (US EPA). Les corrélations entre l'ED 10 h et les paramètres plus largement disponibles comme la dose de tumeur TD 50 a (pour des effets cancérigènes) et la dose non associée à un effet nocif notable NOAEL (pour des effets non cancérigènes) sont déterminées. Elles sont appliquées afin de quantifier le facteur de pente de plus de 900 substances. Une pondération des différents types d'effets nocifs sur la santé humaine est proposée en se basant sur le concept des années de vie perdue par personne affectée (DALYp). Pour les effets cancérigènes, les DALYp sont calculés pour différents types de tumeur, en utilisant des données rapportées en littérature. Il ressort que tous les cancers ont plus ou moins la même sévérité et une valeur moyenne de 11. 1 ans de vie perdue par personne affectée est dérivée. Pour les effets non cancérigènes, une classification simplifiée en trois catégories est proposée et une DALYp de 11. 1, 1, 1 et 0, 11 ans de vie perdue par personne affectée est respectivement attribuée à chacune des trois catégories. Finalement, le facteur de pente βED 10 et la DALYp pour une substance donnée sont combinés afin de dériver son facteur d'effet. Ce facteur d'effet est exprimé en années de vie perdue par masse absorbée. L'annexe 1. 1 résume les facteurs d'effets calculés pour plus de 900 substances toxiques. Les facteurs d'effets pour les effets cancérigènes vont de 1. 3 · 10 - 9 pour l'anthranilate cinnamylique à 3. 4 · 10 - 1 [année perdue / mg absorbé] pour la 2, 3, 7, 8 -tetrachlorodibenzo-p-dioxine. Les facteurs d'effets pour les effets non cancérigènes vont de 4. 2 · 10 - 12 pour le 1 -Chloro- 1, 1 -difluoroethane à 1. 4 · 10 - 3 [année perdue / mg absorbé] pour le béryllium. En chapitre 4, une approche semi-empirique est développée afin d'évaluer le devenir et l'exposition pour des émissions atmosphériques de métaux lourds, de moNOxyde de carbone (CO), de dioxyde de souffre (SO 2), d'oxyde d'azote (NOx) et de particules fines. Pour ce faire, le concept d'efficacité d'exposition est utilisé. L'efficacité d'exposition est définie comme le rapport entre la dose absorbée par la population et l'émission induisant cette absorption. Trois types d'efficacité d'exposition sont définis pour une émission atmosphérique mondiale d'une substance donnée. Une efficacité spécifique d'exposition, directement basée sur les concentrations rurales et urbaines inhalées par les hommes, est définie. Une efficacité d'exposition continentale est également définie, en considérant la concentration continentale mondiale pour les régions habitées (les régions marines et désertiques sont exclues). Une efficacité d'exposition globale est définie de façon similaire à partir de la concentration globale mondiale d'une substance. L'efficacité d'exposition est calculée pour les particules fines, le CO, NOx et SO 2. L'efficacité d'exposition spécifique présente des valeurs allant de 3. 9 · 10 - 6 à 2. 4 · 10 - 5 [mg absorbé /mg émis], indiquant que seulement une très petite fraction d'une émission atmosphérique est inhalée par les humains. L'efficacité d'exposition pour les métaux après inhalation est supposée égale à l'efficacité d'exposition des particules fines, étant donné que les métaux dans l'air sont liés aux particules. Si une déposition atmosphérique sur un sol agricole a lieu, les hommes peuvent être exposés suite à un transfert dans des produits alimentaires. Une première évaluation de ce transfert indique qu'il peut augmenter l'efficacité d'exposition des métaux émis dans l'air d'un facteur 5 à 70. L'efficacité d'exposition spécifique est choisie dans cette thèse pour décrire le devenir et l'exposition des émissions atmosphériques. Elle est supérieure d'un facteur 3 à l'efficacité d'exposition continentale, indiquant que l'utilisation de modèles continentaux à un compartiment tend à sous-estimer l'efficacité d'exposition qui peut avoir lieu dans la réalité. Ceci est dû au fait que des émissions plus élevées ont lieu dans les régions les plus peuplées. Comme première approximation, le facteur 3 pourrait être utilisé comme facteur correctif, afin de dériver l'efficacité d'exposition spécifique à partir de l'efficacité prédite par les modèles continentaux à un compartiment. Dans le chapitre 5, l'efficacité d'exposition déterminée au chapitre 4 et les facteurs d'effet déterminés aux chapitres 2 et 3 sont multipliés afin de déduire les facteurs de dommage sur l'homme (HDF). Ces facteurs de dommage sont exprimés en années de vie perdue par masse émise. En utilisant ces facteurs, l'émission d'une substance peut être convertie en dommage potentiel qu'elle induit sur les humains. Les facteurs de dommage sont calculés pour NOx, SO 2, CO et les particules fines, ainsi que pour les métaux émis dans l'air ou dans un sol agricole (voir annexe 1. 2 pour le résumé des résultats). Lorsque le transfert dans les produits alimentaires n'est pas considéré, les facteurs de dommage pour les métaux étudiés présentent des valeurs allant de 1. 7 · 10 - 11 pour chromium(VI) à 1. 3 · 10 - 8 [année perdue / mg émis] pour le béryllium. Le plomb a le facteur de dommage le plus élevé (1. 9 · 10 - 8 [année perdue / mg émis]) si le transfert dans des produits alimentaires est considéré. Des facteurs de dommage allant de 2. 7 · 10 - 10 à 6. 6 · 10 - 10 [année perdue / mg émis] sont obtenus pour NOx, SO 2 et les particules fines, alors que le moNOxyde de carbone est caractérisé par un facteur de dommage 103 inférieur. Par masse émise, les métaux inhalés par les hommes induisent des dommages du même ordre de grandeur que le NOx, SO 2 et les particules fines; quand la déposition atmosphérique sur les sols agricoles et le transfert ultérieur dans la nourriture sont considérés, les métaux présentent des facteurs plus élevés. Une validation indirecte des facteurs de dommage est présentée pour le SO 2, NOx, CO, les particules fines et quelques métaux, en <b>appliquant</b> leur facteur de dommage à leurs émissions totales ayant lieu en Suisse et en Europe. Les dommages évalués sont plausibles...|$|E

