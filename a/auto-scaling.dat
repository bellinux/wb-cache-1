235|25|Public
25|$|Support for {{wireless}} networks {{is built}} into the network stack itself, and does not emulate wired connections, {{as was the case with}} previous versions of Windows. This allows implementation of wireless-specific features such as larger frame sizes and optimized error recovery procedures. Windows Vista uses various techniques like Receive Window <b>Auto-scaling,</b> Explicit Congestion Notification, TCP Chimney offload and Compound TCP to improve networking performance. Quality of Service (QoS) policies can be used to prioritize network traffic, with traffic shaping available to all applications, even those that do not explicitly use QoS APIs. Windows Vista includes in-built support for peer-to-peer networks and SMB 2.0. For improved network security, Windows Vista supports for 256-bit and 384-bit Diffie-Hellman (DH) algorithms, as well as for 128-bit, 192-bit and 256-bit Advanced Encryption Standard (AES) is included in the network stack itself, while integrating IPsec with Windows Firewall.|$|E
5000|$|Autoscaling - Allows {{application}} {{developers to}} scale Eucalyptus cloud resources {{up or down}} {{in order to maintain}} performance and meet SLAs. With <b>auto-scaling,</b> developers can add instances and virtual machines as traffic demands increase. <b>Auto-scaling</b> policies for Eucalyptus are defined using Amazon EC2-compatible APIs and tools.|$|E
5000|$|Computational {{resources}} can be federated (cloud, clusters, virtualized infrastructures, desktop machines) {{into a single}} virtual infrastructure. It provides <b>auto-scaling</b> and ease resource management strategies.|$|E
5000|$|File monitor: publishes events {{following}} detected {{file system}} changes (the default implementation uses <b>auto-scaled</b> disk polling) ...|$|R
30|$|Roy et al. [18] {{proposes a}} model-predictive {{resource}} allocation algorithm that <b>auto-scales</b> VMs, {{with the aim}} of optimizing the utility of the application over a limited prediction horizon. Empirical results demonstrate that the proposed method satisfies application QoS requirements, while minimizing operational costs.|$|R
30|$|This paper {{focuses on}} {{employing}} a broker that runs within an organization {{known as the}} Intermediary Enterprise (IE) and <b>auto-scales</b> the number of resources acquired from a public cloud provider. The resources are required to execute AR and OD requests sent by another organization known as a Single Client Enterprise (SCE) or the User. SCE is an entity comprising of multiple users that generate requests {{that need to be}} serviced. SCE sends all these requests to IE on behalf of these users. IE is charged by the public cloud provider to rent resources, and IE, in turn, charges SCE to execute requests to recover its operational expenditure. The broker <b>auto-scales</b> the resources to increase the profit earned from its operation while attempting to reduce the cost incurred by SCE (explained in more detail in “Pricing” section). The reduction in cost is justified by comparing with a system where the requests would be scheduled by SCE without broker intervention. This type of a system is defined in “Alternative systems” section.|$|R
50|$|The metrics {{collected}} by Amazon CloudWatch enables the <b>auto-scaling</b> feature to dynamically add or remove EC2 instances. The customers are {{charged by the}} number of monitoring instances.|$|E
5000|$|Scalability {{features}} differ between vendorssome offer <b>auto-scaling,</b> others {{enable the}} user to scale up using an API, but do not scale automatically. There is typically a commitment for {{a certain level of}} high availability (e.g. 99.9% or 99.99%).|$|E
50|$|Autoscaling, also spelled auto scaling or <b>auto-scaling,</b> and {{sometimes}} also called automatic scaling, {{is a method}} used in cloud computing, whereby the amount of computational resources in a server farm, typically {{measured in terms of}} the number of active servers, scales automatically based on the load on the farm. It is closely related to, and builds upon, the idea of load balancing.|$|E
40|$|International audienceA {{hardware}} simulator {{facilitates the}} test and validation cycles by replicating channel artefacts in a controllable and repeatable laboratory environment. In this paper, a new <b>Auto-Scale</b> Factor (ASF) based architecture of the digital block of the hardware simulator for MIMO radio propagation channel is introduced. A detailed study shows that this architecture increases the Signal-to-Noise Ratio (SNR) of the output signals when the input signals or the impulse responses of the channel (h) are attenuated. The architecture is implemented on a Xilinx Virtex-IV FPGA. the occupation on the FPGA and the accuracy of this architecture are analyzed...|$|R
30|$|A {{number of}} self-adaptive {{systems have been}} built to serve {{different}} purposes in several different application domains such as wireless sensor networks, multi-tenant SaaS applications, databases, cyber-physical systems etc. For example, In [51] Tsoumakos et al. present TIRAMOLA, a cloud-enabled framework. The main purpose of TIRAMOLA is to automatically perform resizing of NoSQL database clusters based on user-defined policies. In another similar work, Truyen et al. have proposed K 8 -Scalar [52], an extensible workbench for evaluating different self-adaptive approaches to <b>auto-scale</b> NoSQL database clusters. Similarly, Fetai et al. [53] presents Cumulus that minimizes distributed transactions through adaptive data repartitioning.|$|R
30|$|Having to {{configure}} a live VM, even a trivial configuration, {{is a significant}} overhead when dealing {{with large numbers of}} VMs. When deployments <b>auto-scale</b> and rapidly change, human administrators cannot be required to perform any form of manual intervention. An autonomic system is one which has the capacity for self management; {{to configure}} and optimise itself without the need for human interaction. There are many different levels of autonomic behaviour from simple configuration management tools to self optimising and self healing systems. At the very least, a cloud monitoring system must require no signifiant configuration or manipulation at runtime. Greater degrees of autonomic behaviour is however, incredibly desirable.|$|R
50|$|Amazon's <b>auto-scaling</b> {{feature of}} EC2 {{allows it to}} {{automatically}} adapt computing capacity to site traffic. The schedule-based (e.g. time-of-the-day) and rule-based (e.g. CPU utilization thresholds) auto scaling mechanisms are {{easy to use and}} efficient for simple applications. However, one potential problem is that VMs may take up to several minutes to be ready to use, which are not suitable for time critical applications. The VM startup time are dependent on image size, VM type, data center locations, etc.|$|E
50|$|The {{company is}} {{developing}} a data-driven infrastructure that combines AWS’s <b>auto-scaling</b> compute and storage infrastructure with a home-grown, inference-based AI capability that makes the site seem like signing up for a dating website. The idea, according to founder and CEO Elias Chavando, is to avoid the small charges with each query that Rentus.com would otherwise pay AWS for AI functionality, charges that would add up quickly when the start-up processes {{the hundreds of thousands}} of queries per hour.|$|E
50|$|Support for {{wireless}} networks {{is built}} into the network stack itself, and does not emulate wired connections, {{as was the case with}} previous versions of Windows. This allows implementation of wireless-specific features such as larger frame sizes and optimized error recovery procedures. Windows Vista uses various techniques like Receive Window <b>Auto-scaling,</b> Explicit Congestion Notification, TCP Chimney offload and Compound TCP to improve networking performance. Quality of Service (QoS) policies can be used to prioritize network traffic, with traffic shaping available to all applications, even those that do not explicitly use QoS APIs. Windows Vista includes in-built support for peer-to-peer networks and SMB 2.0. For improved network security, Windows Vista supports for 256-bit and 384-bit Diffie-Hellman (DH) algorithms, as well as for 128-bit, 192-bit and 256-bit Advanced Encryption Standard (AES) is included in the network stack itself, while integrating IPsec with Windows Firewall.|$|E
30|$|For each waveform, a {{qualitative}} evaluation was also performed. Each <b>auto-scaled</b> waveform was reviewed by two clinical scientists (medical physicists) blinded to the R value and {{scored on a}} 3 -point scale assessing its suitability for clinical use (score S of 0 = no respiratory signal, 1 = some respiratory-like signal but indeterminate, 2 = acceptable signal considered to be respiratory). The scores from the two readers were averaged. The axial location of the centre of each bed position, and hence each corresponding R and S values, was determined relative to the reference location at the most superior point of the liver as determined from the CT image.|$|R
30|$|A {{factorial}} ANOVA {{was performed}} on predicted fructose and glucose concentrations to {{examine the effect of}} fruit position, cultivar type, and location of mango fruit sugars. For both analyses, when a significant difference (p <  0.05) was detected in some variable, Tukey’s mean test was applied to evaluate the difference between the samples. In addition, the two fruit position means were compared by independent sample t test at a significant level of 0.05 (p <  0.05) using SPSS software version 19. Hierarchical component analysis (HCA) (minimum variance method; squared Euclidean distances) was carried out on the <b>auto-scaled</b> dataset of cultivars, {{in order to understand the}} variation within each cultivar.|$|R
5000|$|... {{to serve}} {{automotive}} scale. Commercial buildings {{that are designed}} to be legible from roadways assume a radically different shape. The human eye can distinguish about 3 objects or features per second. A pedestrian steadily walking along a 30 m length of department store can perceive about 68 features; a driver passing the same frontage at 50 km/h can perceive about six or seven features. <b>Auto-scale</b> buildings tend to be smooth and shallow, readable at a glance, simplified, presented outward, and with signage with bigger letters and fewer words. This urban form is traceable back to the innovations of developer A. W. Ross along Wilshire Boulevard in Los Angeles in 1920.|$|R
30|$|This section {{discusses}} {{a representative}} set of existing work on <b>auto-scaling</b> systems in cloud environments. “General <b>auto-scaling</b> approaches” section discusses general <b>auto-scaling</b> approaches, whereas “Resource management frameworks” section discusses resource management frameworks. “Hybrid <b>auto-scaling</b> approaches” section addresses work that highlights <b>auto-scaling</b> performed via hybrid approaches.|$|E
30|$|In {{this section}} {{we put the}} two {{components}} (i.e., the self-adaptive predictor and the cost driven decision maker) together and evaluate the resulting predictive <b>auto-scaling</b> system against the Amazon <b>auto-scaling</b> system, a well-known and popular <b>auto-scaling</b> system in the commercial cloud environments [2]. The Amazon <b>auto-scaling</b> system’s behavior is simulated and its <b>auto-scaling</b> cost is compared {{with the cost of}} our predictive <b>auto-scaling</b> system.|$|E
30|$|In a cloud {{computing}} environment {{there are two}} types of cost associated with the <b>auto-scaling</b> systems: resource cost and Service Level Agreement (SLA) violation cost. The goal of an <b>auto-scaling</b> system is to find a balance between these costs and minimize the total <b>auto-scaling</b> cost. However, the existing <b>auto-scaling</b> systems neglect the cloud client’s cost preferences in minimizing the total <b>auto-scaling</b> cost. This paper presents a cost-driven decision maker which considers the cloud client’s cost preferences and uses the genetic algorithm to configure a rule-based system to minimize the total <b>auto-scaling</b> cost. The proposed cost-driven decision maker together with a prediction suite makes a predictive <b>auto-scaling</b> system which is up to 25 % more accurate than the Amazon <b>auto-scaling</b> system. The proposed <b>auto-scaling</b> system is scoped to the business tier of the cloud services. Furthermore, a simulation package is built to simulate the effect of VM boot-up time, Smart Kill, and configuration parameters on the cost factors of a rule-based decision maker.|$|E
30|$|Statistical {{analyses}} were performed using SPSS 16.0 for windows. PCA technique was used to reduce the dimensionality of the data set while retaining the variability presented in a data set as much as possible. The Spearman’s correlation matrix was generated to determine any relationship between the observed parameters in order to explain factor loadings during PCA. In order to ensure normality of the data, all hydrochemical data (except pH) were log-transformed prior to statistical analyses. The hydrochemical data was also <b>auto-scaled</b> by calculating the standard scores (z scores) and ensuring that all z scores are <± 2.5. For trace metals with concentrations below their detection limits, one-half {{of the value of}} their respective detection limit was substituted and used in statistical analysis. A probability value of P <  0.05 was considered as statistically significant in this study.|$|R
30|$|In {{order to}} obtain the {{relative}} concentration of the metabolites the average normalized area of each peak per triplicate was calculated and arranged in a multivariate matrix (8 × 30), 8 groups and 30 peaks, used in statistical analysis. The multivariate statistical methods chosen in this work was PCA (SIMCA-P[*]+[*] 11; Umetrics) (Umetrics, 2007) using <b>auto-scaled</b> analysis, where all input variables’ variances were re-normalized to unit variance. This normalization provides a reliable identification of variation in metabolites with low concentration that normally would not be detectable by naked eyes (Griffin et al. 2001 a; Griffin et al. 2001 b). When clusters of points were observed in the score plots, loading plots were analyzed to identify the metabolites which {{are responsible for the}} clustering of similar samples. In parallel to the multivariate analysis, the differential metabolites pointed by PCA were validated at a univariate level using ANOVA and Fisher’s test. The critical α-value used as threshold in this study was set to 0.05.|$|R
40|$|ENTICE is an H 2020 European project {{aiming to}} {{research}} and create a novel Virtual Machine (VM) repository and operational environment for federated Cloud infrastructures to: (i) simplify the creation of lightweight and highly optimised VM images tuned for functional descriptions of applications; (ii) automatically decompose and distribute VM images based on multi-objective optimisation (performance, economic costs, storage size, and QoS needs) and a knowledge base and reasoning infrastructure to meet application runtime requirements; and (iii) elastically <b>auto-scale</b> applications on Cloud resources based on their fluctuating load with optimised VM interoperability across Cloud infrastructures and without provider lock-in, in order to finally fulfil the promises that virtualization technology has failed to deliver so far. In this chapter, we give an inside view into the ENTICE project architecture. Based on stakeholders that interact with ENTICE, we describe the different functionalities of the different components and services and how they interact with each other...|$|R
30|$|An {{evaluation}} of our predictive <b>auto-scaling</b> system [6] against the Amazon <b>auto-scaling</b> system.|$|E
30|$|In addition, the {{proposed}} system {{is limited to}} the business tier <b>auto-scaling.</b> Extending the <b>auto-scaling</b> system to manage the database tier can be investigated in the future studies [33]. The logic of the database tier and the business tier <b>auto-scaling</b> are similar. However, unlike the business tier, the database tier includes a set of datafiles which can greatly affect the <b>auto-scaling</b> strategy.|$|E
30|$|The <b>auto-scaling</b> {{accuracy}} {{is closely related}} to the cost incurred by the cloud clients. The more accurate the <b>auto-scaling</b> system, the lower the cost incurred by the cloud clients. Therefore, cost is the main metric that measures the accuracy of the <b>auto-scaling</b> systems. From the cloud client’s perspective, {{there are two types of}} costs associated with the <b>auto-scaling</b> systems: resource cost (CR) and SLA violation cost (CSLA).|$|E
40|$|A {{hardware}} simulator reproduces {{the behavior}} of the radio propagation channel, thus making it possible to test “on table ” the mobile radio equipments. The simulator can be used for LTE and WLAN 802. 11 ac applications, in indoor and outdoor environments. In this paper, the input signals parameters and the relative power of the impulse responses are related to the relative error and SNR of the output signals. After analyzing the influence of these parameters on the output error and SNR, an algorithm based on an <b>Auto-Scale</b> Factor (ASF) is analyzed in details to improve the precision of the output signals of the hardware simulator digital block architecture. Moreover, the circuit needed for the validation of this algorithm has been introduced, verified and realized. It is shown that this solution increases the output SNR if the relative powers of the impulse responses are attenuated. The new architecture of the digital block is presented and implemented on a Xilinx Virtex-IV FPGA. The occupation on the FPGA and the accuracy of the architecture are analyzed...|$|R
40|$|Another {{important}} contribution of this thesis {{is that we}} propose another novel and highly robust estimator: Kernel Density Estimation Sample Consensus (KDESAC) which employs Random Sample Consensus algorithm combined with Kernel Density Estimation (KDE). The main advantage of KDESAC is that no prior information and no scale estimators are required in the estimation of the parameters. The computational load of KDESAC is {{much lower than the}} robust algorithms which estimate the scale in every sample loop. The experiments on synthetic data show that the proposed method is more robust to the heavily corrupted data than other algorithms. KDESAC can tolerate more than 80 % outliers and multiple structures. Although Adaptive Scale Sample Consensus (ASSC) can obtain such good performance as KDESAC, ASSC is much slower than KDESAC. KDESAC is also applied to SFM problem and multi-motion estimation with real data. The experiments demonstrate that KDESAC is robust and efficient. Structure from motion (SFM), the problem of estimating 3 D structure from 2 D images hereof, {{is one of the most}} popular and well studied problems within computer vision. This thesis is a study within the area of SFM. The main objective of this work is to improve the robustness of the SFM algorithm so as to make it capable of tolerating a great number of outliers in the correspondences. For improving the robustness, a stereo image sequence is processed, so the random sampling algorithms can be employed in the structure and motion estimation. With this strategy, we employ Random Sample Consensus (RANSAC) in motion and structure estimation to exclude outliers. Since the RANSAC method needs the prior information about the scale of the inliers, we proposed an <b>auto-scale</b> RANSAC algorithm which determines the inliers by analyzing the probability density of the residuals. The experimental results demonstrate that SFM by the proposed <b>auto-scale</b> RANSAC is more robust and accurate than that by RANSAC. Chan Tai. "September 2006. "Adviser: Yun Hui Liu. Source: Dissertation Abstracts International, Volume: 68 - 03, Section: B, page: 1716. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2006. Includes bibliographical references (p. 113 - 120). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstracts in English and Chinese. School code: 1307...|$|R
50|$|According to the Internet Engineering Task Force (IETF), {{the most}} basic cloud-service model is that of {{providers}} offering computing infrastructure - virtual machines and other resources - as a service to subscribers. Infrastructure as a service (IaaS) refers to online services that provide high-level APIs used to dereference various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc. A hypervisor, such as Xen, Oracle VirtualBox, Oracle VM, KVM, VMware ESX/ESXi, or Hyper-V, LXD, runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines {{and the ability to}} scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization, because there is no hypervisor overhead. Also, container capacity <b>auto-scales</b> dynamically with computing load, which eliminates the problem of over-provisioning and enables usage-based billing. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.|$|R
30|$|Deciding {{the optimal}} amount of {{resources}} in an IaaS cloud environment {{is very difficult}} and can be a double-edged sword either leading to over-provisioning or under-provisioning. Under provisioning can lead to SLA violation while over provisioning can cause wastage and excessive energy consumption. <b>Auto-scaling</b> systems are built to balance the cost-performance trade-off of over- or under-provisioning. Furthermore, neglecting the VM boot-up time and the difficulty associated with configuration parameters are the two main shortcomings of the rule-based <b>auto-scaling</b> systems. This paper investigates {{the impact of the}} VM boot-up time and the configuration parameters on the accuracy and cost of the rule-based <b>auto-scaling</b> systems, and proposes a predictive <b>auto-scaling</b> system that consists of a self-adaptive prediction suite [6] and a cost driven decision maker. Our previous work [6] presents a self-adaptive prediction suite on which the prediction task of the proposed <b>auto-scaling</b> system is based. The proposed <b>auto-scaling</b> system bundles the prediction suite with a cost-driven decision maker to scale the business tier of the cloud services. In this paper we present the cost-driven decision maker component that uses a genetic algorithm to select optimum configuration parameters in a large search space for the <b>auto-scaling</b> system. According to the evaluation results, the proposed <b>auto-scaling</b> system can reduce the total <b>auto-scaling</b> cost by up to 25 % compared with the Amazon <b>auto-scaling</b> system.|$|E
30|$|In {{this section}} we present an {{overview}} of the existing <b>auto-scaling</b> systems, and describe the rule-based <b>auto-scaling</b> technique and introduce its configuration parameters. In addition, we summarize our previous work [6] on self-adaptive prediction <b>auto-scaling</b> suite. This summary is necessary for understanding the present research work in this paper. The authors in [3] group the existing <b>auto-scaling</b> approaches into five categories: rule based technique, reinforcement learning, queuing theory, control theory, and time-series analysis. Among these categories, the time-series analysis focuses on the prediction side of the resource provisioning task and is not a “decision making” technique per se. In contrast, the rule-based technique is a pure decision making mechanism {{while the rest of the}} <b>auto-scaling</b> categories plays the predicator and the decision maker roles at the same time. The rule based technique is the only approach which is widely used in the commercial <b>auto-scaling</b> systems [7 – 9].|$|E
30|$|The {{remainder}} {{of this paper is}} organized as follows: section 2.0 discusses the background, the related work, <b>auto-scaling</b> accuracy and cost driven decision maker. In section 3.0, experiments are presented that show the impact of VM boot-up time, configuration parameters, and smart kill on <b>auto-scaling</b> accuracy and cost. This is followed with the proposed optimum configuration for <b>auto-scaling</b> problem using genetic algorithm. The evaluation of the cost driven decision maker and the predictive <b>auto-scaling</b> system is presented in section 5.0. The conclusion and possible future directions for the research are discussed in section 6.0.|$|E
40|$|Cloud {{computing}} {{can offer}} a set of computing resources according to users' demand. It is suitable {{to be used to}} handle flash-crowd events in Web applications due to its elasticity and on-demand characteristics. Thus, when Web applications need more computing or storage capacity, they just instantiate new resources. However, providers have to estimate the amount of resources to instantiate to handle with the flash-crowd event. This estimation is far from trivial since each cloud environment provides several kinds of heterogeneous resources, each one with its own characteristics such as bandwidth, CPU, memory and financial cost. In this paper, the Flash Crowd Handling Problem (FCHP) is precisely defined and formulated as an integer programming problem. A new algorithm for handling with a flash crowd named FCHP-ILS is also proposed. With FCHP-ILS the Web applications can replicate contents in the already instantiated resources and define the types and amount of resources to instantiate in the cloud during a flash crowd. Our approach is evaluated considering real flash crowd traces obtained from the related literature. We also present a case study, based on a synthetic dataset representing flash-crowd events in small scenarios aiming at the comparison of the proposed approach against Amazon's <b>Auto-Scale</b> mechanism. Comment: Submitted to the 30 th Symposium On Applied Computing (2015...|$|R
40|$|Copyright © 2013 Bachir Habib et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A hardware simulator reproduces {{the behavior of the}} radio propagation channel, thus making it possible to test “on ta-ble ” the mobile radio equipments. The simulator can be used for LTE and WLAN 802. 11 ac applications, in indoor and outdoor environments. In this paper, the input signals parameters and the relative power of the impulse responses are related to the relative error and SNR of the output signals. After analyzing the influence of these parameters on the out-put error and SNR, an algorithm based on an <b>Auto-Scale</b> Factor (ASF) is analyzed in details to improve the precision of the output signals of the hardware simulator digital block architecture. Moreover, the circuit needed for the validation of this algorithm has been introduced, verified and realized. It is shown that this solution increases the output SNR if the relative powers of the impulse responses are attenuated. The new architecture of the digital block is presented and im-plemented on a Xilinx Virtex-IV FPGA. The occupation on the FPGA and the accuracy of the architecture are analyzed...|$|R
40|$|A {{hardware}} simulator {{facilitates the}} test and validation cycles by replicating channel artefacts in a controllable and repeatable laboratory environment. In this paper, a new <b>Auto-Scale</b> Factor (ASF) based architecture of the digital block of the hardware simulator for MIMO radio propagation channel is introduced. A detailed study shows that this architecture increases the Signal-to-Noise Ratio (SNR) of the output signals when the input signals or the impulse responses of the channel (h) are attenuated. The architecture is implemented on a Xilinx Virtex-IV FPGA. the occupation on the FPGA and the accuracy of this architecture are analyzed. CONCLUSION To decrease the error at {{the output of the}} hardware simulator, an ASF-based architecture is used. A detailed study shows that for high attenuation of the impulse responses and the input signal, the SNR of the output signals increases significantly. The KEYWORDS: average global SNR is increased by 37 dB for-/+ 7 dB variation of h in time. Moreover, it can attend 100 dB for an attenuation up to 15 dB of the impulse responses. Simulations will be made using a Virtex-VII [1] platform will allow us to simulate up to 8 × 8 MIMO channels and higher. A graphical user interface will be designed to allow the user to reconfigure the channel parameters. Also, cognitive hardware simulator for heterogeneous environments will be realized...|$|R
