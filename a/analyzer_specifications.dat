1|27|Public
40|$|Learn why {{spectrum}} analysis {{is important for}} a variety of applications and how to measure system and device performance using a spectrum analyzer. To introduce you to spectrum analyzers, the theory of operation will be discussed. In addition, the major components inside the analyzer and why they are important will be examined. Next, you will learn the spectrum <b>analyzer</b> <b>specifications</b> that are important for your application. Finally, features of a spectrum analyzer that make it more effective in making measurements will be introduced...|$|E
40|$|We {{describe}} QuARS (Quality <b>Analyzer</b> for Requirement <b>Specifications)</b> Express, {{a customized}} {{version of the}} QuARS tool. It is designed to evaluate natural language requirements, can handle complex and structured data formats containing metadata, {{and is able to}} produce an analysis report with categorized information...|$|R
40|$|Tis paper {{present a}} tool called QuARS (Quality <b>Analyzer</b> of Requirements <b>Specification)</b> for the {{analysis}} of natural language software requirements. The definition of QuARS has been based on a quality model for software requirements. The quality model aims at providing a quantitative, corrective and repeatable evaluation of software requirements documents...|$|R
40|$|This paper {{presents}} a tool called QuARS (Quality <b>Analyzer</b> of Requirements <b>Specification)</b> {{for the analysis}} of natural language software requirements. The definition of QuARS has been based on a quality model for software requirements. The quality model aims at providing a quantitative, corrective and repeatable evaluation of software requirement documents. To validate the quality model several real software requirements documents have been analyzed showing interesting results...|$|R
40|$|QuARS and QuARS Express (Quality <b>Analyzer</b> of Requirements <b>Specifications)</b> are {{tools that}} {{make it easier to}} extract {{structured}} information and metrics for detecting linguistic inaccuracies and defects in software requirements expressed in Natural language. The Express edition of QuARS represent a parallel evolution of the main tool exploiting the same core engine for an increased usability and a more expressive set of reports. In this article a comparison is presented...|$|R
40|$|This paper {{discusses}} a new {{methodology for}} scanner generation that supports language independent lexicon specification and automatic generation of stand alone lexical <b>analyzers</b> from <b>specifications.</b> The mechanism that {{sits at the}} basis of this methodology is the layering of the lexicon specification on two levels: in the first level, "universal" lexical constructs which are used as building blocks by most programming languages are defined, and in the second level, customized lexical constructs of specific programming languages are specified in terms of universal lexical constructs. The universal lexicon is specified by regular expressions over a global alphabet used by most programming languages, such as the character set of a keyboard, and is efficiently implemented by deterministic finite automata. The customized lexicon is conveniently specified by regular expressions of properties of universal lexical constructs and is implemented by nondeterministic automata whose transition functi [...] ...|$|R
40|$|Abstract. This work {{discusses}} {{implementation of}} partial redundancy elimination using the value flow graph, a syntactic program representation modeling semantic equivalences. It allows {{the combination of}} simple syntactic partial redundancy elimination with a powerful semantic analysis. This yields an optimization that is computationally optimal and simpler than traditional semantic methods. A source-to-source optimizer for C++ programs was implemented using the SATIrE program analysis and transformation system. Two tools integrated in SATIrE {{were used in the}} implementation: ROSE is a framework for arbitrary analyses and source-to-source transformations of C++ programs, PAG is a tool for generating data flow <b>analyzers</b> from functional <b>specifications.</b> ...|$|R
40|$|While in {{practice}} Natural Language (NL) {{is the most}} used mean for expressing requirements, {{there is a lack}} of supporting tools and techniques for the analysis of this kind of requirements. The QuARS (Quality <b>Analyzer</b> for Requirement <b>Specifications)</b> tool has been designed with the aim to automatize the analysis of NL requirements 2 ̆ 7 as they are 2 ̆ 7, i. e. with no need to move towards another formalism. The tool is able to perform an evaluation of the Expressiveness of an NL requirements document and also provides support for completeness and consistency analysis...|$|R
40|$|AbstractAbstract {{interpretation}} {{is a technique}} for the static detection of dynamic properties of programs. It is semantics-based, that is, it computes approximative properties of the semantics of programs. On this basis, it allows for correctness proofs of analyses. It replaces commonly used ad hoc techniques by systematic, provable ones, and it allows the automatic generation of <b>analyzers</b> from <b>specifications</b> as in the Program Analyzer Generator (PAG). In this paper, abstract {{interpretation is}} applied to the problem of predicting the cache behavior of programs. Abstract semantics of machine programs are defined which determine the contents of caches. For interprocedural analysis, existing methods are examined and a new approach that is especially tailored for the cache analysis is presented. This allows for a static classification of the cache behavior of memory references of programs. The calculated information can be used to sharpen worst-case execution time estimations. It is possible to analyze instruction, data, and combined instruction/data caches for common (re) placement and write strategies. Experimental results are presented that demonstrate the applicability of the analysis...|$|R
40|$|Abstract. Alloy is a {{specification}} language {{based on a}} relational first-order logic with built-in operators for transitive closure, set cardinality, and integer arithmetic. The Alloy <b>Analyzer</b> checks Alloy <b>specifications</b> automatically with respect to bounded domains. Thus, while suitable for finding counterexamples, it cannot, in general, provide correctness proofs. This paper presents Kelloy, a tool for verifying Alloy specifications with respect to potentially infinite domains. It describes an automatic translation of the full Alloy language to the first-order logic of the KeY theorem prover, and an Alloy-specific extension to KeY’s calculus. It discusses correctness and completeness conditions of the translation, and reports on our automatic and interactive experiments. ...|$|R
40|$|Wuhan Univ Sci & Technol, Huazhong Normal Univ, Wuhan Inst TechnolThis paper {{describes}} {{a framework for}} the design of a test tool that could generate test cases automatically based on given BPEL specifications. The key problems {{that need to be addressed}} are how to transform the BPEL specifications into a HPN, and how to design a script language to describe the test case generation that according to the characteristics of BPEL. A BPEL <b>Specification</b> <b>Analyzer</b> and a Test Script Language are presented. A tool called BPEL-based Testing Automatic has been designed and partially implemented. BTA will take a user-defined test case template and the set of test data generated to produce the executable test cases...|$|R
40|$|Using {{automatic}} {{tools for}} the quality analysis of Natural Language (NL) requirements is recognized as a key factor for achieving software quality. Unfortunately few tools and techniques for the NL requirements analysis are currently available. This paper presents a methodology and a tool (called QuARS- Quality <b>Analyzer</b> for Requirement <b>Specifications)</b> for analyzing NL requirements in a systematic and automatic way. QuARS allows requirements engineers to perform an initial parsing of the requirements in order to automatically detect potential linguistic defects that could cause interpretation problems at subsequent stages in developing the software. This tool is also able to partially support the consistency and completeness analysis by clustering the requirements according to specific topics...|$|R
40|$|To produce high qualitiy code, modern compilers use global {{optimization}} algorithms {{based on}} it abstract interpretation. These algorithms are rather complex; their implementation is therfore a non-trivial task and error-prone. However, since thez {{are based on}} a common theory, they have large similar parts. We conclude that analyzer writing better should be replaced with analyzer generation. We present the tool sf PAG that has a high level functional input language to specify data flow analyses. It offers th specifications of even recursive data structures and is therfore not limited to bit vector problems. sf PAG generates efficient analyzers wich can be easily integrated in existing compilers. The analyzers are interprocedural, they can handle recursive procedures with local variables and higher order functions. sf PAG has successfully been tested by generating several analyzers (e. g. alias analysis, constant propagation, inerval analysis) for an industrial quality ANSI-C and Fortran 90 compiler. This technical report consits of two parts; the first introduces the generation system and the second evaluates generated analyzers with respect to their space and time consumption. bf Keywords: data flow analysis, specification and generation of <b>analyzers,</b> lattice <b>specification,</b> abstract syntax specification, interprocedural analysis, compiler construction...|$|R
40|$|Nowadays common {{practice}} {{indicates that the}} Requirement Engineering (RE) process critically influences {{the success of the}} system development life cycle. Several commercial tools allow to classify, archive and manage requirements and then to print out reports and requirement documents. QuARS (Quality <b>Analyzer</b> for Requirements <b>Specifications)</b> is an automatic analyzer of such requirement documents, developed by ISTI - CNR, that can be adopted to evaluate the document quality by linguistic point of view. In this paper is presented how a requirement management tool, an automatic document generator and QuARS can be integrated to define an RE automation support. The case study investigates and highlights the efficacy and the role of such proposed support in the Siemens C. N. X. development process...|$|R
40|$|The {{availability}} of automatic tools {{for the quality}} analysis of Natural Language requirements is recognized as a key factor for achieving software quality. Unfortunately, {{the state of the}} art and practice witnesses a lack of tools and techniques for the Natural Language requirements analysis. This paper presents a methodology and a tool (called QuARS - Quality <b>Analyzer</b> for Requirement <b>Specifications),</b> for analyzing Natural Language requirements in a systematic and automatic way. The QuARS tool allows the requirements engineers to perform an initial parsing of the requirements for automatically detecting potential linguistic defects that can determine ambiguity problems at the following development stages of the software product. This tool is also able to support automaticaaly the consistency and completeness analysis by clustering the requirements according to a specific topic...|$|R
40|$|In {{this paper}} the Markov <b>analyzer</b> MOSES (MOdelling, <b>Specification</b> and Evaluation of Computer Systems) {{and the model}} {{description}} language MOSLANG - both developed at the Institute for Operating Systems at the University of Erlangen-Nuernberg - are described using two examples. Toevaluate {{the performance of a}} computer system the system has to be specified. In this paper the model description language MOSLANG is introduced and applied to some examples. The core of MOSLANG consists of constructions suitable for the specification of the possible states of the system and of RULE constructions which model the state transitions. This specification method is much more compressed than other comparable methods and enables the user to specify large systems when he has become familiar with MOSLANG. The Markov analyzer MOSES facilitates the input of MOSLANG and subsequently creates the Markovian system of equations automatically. For solving this system of equations five different methods are provided. [...] ...|$|R
40|$|Numerous {{tools and}} {{techniques}} {{are available for}} managing requirements. Many are designed to define requirements, provide configuration management, and control distribution. However, there are few automatic tools to support the quality analysis of natural language (NL) requirements. Ambiguity analysis and consistency and completeness verification are usually carried out by human reviewers who read requirements documents and look for defects. This clerical activity is boring, time consuming, and often ineffective. This report describes a disciplined method and a related automated tool {{that can be used}} for the analysis of NL requirements documents. The tool, called the Quality <b>Analyzer</b> for Requirements <b>Specifications</b> (QuARS), makes it easier to extract structured information and metrics for detecting linguistic inaccuracies and defects. QuARS allows requirements engineers to perform an initial parsing of requirements by automatically detecting potential linguistic defects that can cause ambiguity problems at later stages of software product development. The tool also provides support for the consistency and completeness analysis of the requirements...|$|R
40|$|Partial {{redundancy}} elimination is {{a common}} program optimization that attempts to improve execution time by removing superfluous computations from a program. There are two well-known classes of such techniques: syntactic and semantic methods. While semantic optimization is more powerful, traditional algorithms based on SSA from are complicated, heuristic in nature, and unable to perform certain useful optimizations. The value flow graph is a syntactic program representation modeling semantic equivalences; it allows the combination of simple syntactic partial redundancy elimination with a powerful semantic analysis. This yields an optimization that is computationally optimal and simpler than traditional semantic methods. This talk discusses partial redundancy elimination using the value flow graph. A source-to-source optimizer for C++ was implemented using the SATIrE program analysis and transformation system. Two tools integrated in SATIrE {{were used in the}} implementation: ROSE is a framework for arbitrary analyses and source-to-source transformations of C++ programs, PAG is a tool for generating data flow <b>analyzers</b> from functional <b>specifications...</b>|$|R
40|$|Automatic {{evaluation}} of natural language requirements documents {{has been proposed}} {{as a means to}} improve the quality of the system under development. QuARS (Quality <b>Analyzer</b> for Requirements <b>Specifications)</b> was introduced as an automatic analyzer of such requirement documents, developed by ISTI-CNR. In this paper we show how this tool has been applied to a large collection of requirements produced inside the EU/IP MODTRAIN project. In this project several industrial partners have contributed to the collection by proposing requirements coming from their own products, with the purpose to define an European standard for the Train Control and Monitoring Systems (TCMS). To better fit the project needs, QuARS has been extended to be interfaced with the requirement management tool adopted in the project, and adapted to specific and domain dependent aspects of the MODCONTROL requirements. In particular the tool is now able to handle a more complex and structured data format containing metadata. The calculation of readability metrics and other statistical indexes has also been improved...|$|R
40|$|Amalia is a {{generator}} framework for constructing analyzers for operationally defined formal notations. These generated analyzers are components {{that are designed}} for customization and integration into a larger environment. The customizability and efficiency of Amalia analyzers owe to a computational structure called an inference graph. This paper describes this structure, how inference graphs enable Amalia to generate <b>analyzers</b> for operational <b>specifications,</b> and how we build in assurance. On another level, this paper illustrates how to balance the need for assurance, which typically implies a formal proof obligation, against other design concerns, whose solutions leverage design techniques that are not (yet) accompanied by mature proof methods. We require Amalia-generated designs to be transparent {{with respect to the}} formal semantic models upon which they are based. Inference graphs are complex structures that incorporate many design optimizations. While not formally verifiable, their fidelity with respect to a formal operational semantics can be discharged by inspection...|$|R
40|$|The French Society of Clinical Biochemistry {{conducted}} {{this study}} to compare the accuracy and performances of the best creatinine enzymatic assays and the compensated Jaffe methods from the same manufacturers. Creatinine was measured in 3 serum pools with creatinine levels of 35. 9 ± 0. 9 μmol/L, 74. 4 ± 1. 4 μmol/L, and 97. 9 ± 1. 7 μmol/L (IDMS determination). The performances of the assays (total error that includes the contribution of bias and imprecision) were evaluated using Monte-Carlo simulations and compared against desirable NKDEP criteria. The enzymatic assays always fell within the desirable total Error of 7. 6 %. By contrast, this requirement was never obtained for the compensated Jaffe methods at the critical level of 74. 4 ± 1. 4 μmol/L. Only the compensated Jaffe creatinine on Olympus <b>analyzer</b> reached this <b>specification</b> at 35. 9 ± 0. 9 and 97. 9 ± 1. 7 μmol/L levels. This study demonstrates that, despite substantial improvement regarding traceability to the IDMS reference method and precision, compensated Jaffe creatinine methods, by contrast to enzymatic ones, do not reach the desirable specifications of NKDEP at normal levels of creatinine. Peer reviewe...|$|R
40|$|Constructing code analyzers may {{be costly}} and error prone if {{inadequate}} technologies and tools are used. If they {{are written in}} a conventional programming language, for instance, several thousand lines of code may be required even for relatively simple analyses. One way of facilitating the development of code analyzers is to define a very high-level domain-oriented language and implement an application generator that creates the <b>analyzers</b> from the <b>specification</b> of the analyses {{they are intended to}} perform. This paper presents a system for developing code analyzers that uses a database to store both a no-loss fine-grained intermediate representation and the results of the analyses. The system uses an algebraic representation, called F(p), as the user-visible intermediate representation. Analyzers are specified in a declarative language, called F(p) -l, which enables an analysis to be specified {{in the form of a}} traversal of an algebraic expression, with access to, and storage of, the database information the algebraic expression indices. A foreign language interface allows the analyzers to be embedded in C programs. This is useful for implementing the user interface of an analyzer, for example, or to facilitate interoperation of the generated analyzers with pre-existing tools. The paper evaluates the strengths and limitations of the proposed system, and compares it to other related approaches...|$|R
40|$|In {{this work}} the {{automatic}} generation of program <b>analyzers</b> from concise <b>specifications</b> is presented. It focuses on provably correct and complex interprocedural analyses for real world sized imperative programs. Thus, {{a powerful and}} flexible specification mechanism is required, enabling both correctness proofs and efficient implementations. The generation process relies on the theory of data flow analysis and on abstract interpretation. The theory of data flow analysis provides methods to efficiently implement analyses. Abstract interpretation provides the relation to the semantics of the programming language. This allows the systematic derivation of efficient provably correct, and terminating analyses. The approach has been implemented in the program analyzer generator PAG. It addresses analyses ranging from “simple ” intraprocedural bit vector frameworks to complex interprocedural alias analyses. A high level specialized functional language is used as specification mechanism enabling elegant and concise specifications even for complex analyses. Additionally, it allows the automatic selection of efficient implementations for the underlying abstract datatypes, such as balanced binary trees, binary decision diagrams, bit vectors, and arrays. For the interprocedural analysis the functional approach, the call string approach, and a novel approach especially targeting on the precise analysis of loops can be chosen. In this work the implementation of PAG {{as well as a}} large number of applications of PAG are presented...|$|R
40|$|High-resolution {{real-time}} particle mass measurements {{have not}} been achievable because the enormous amount of kinetic energy imparted to the particles upon expansion into vacuum competes with and overwhelms the forces applied to the charged particles within the mass spectrometer. It is possible to reduce the kinetic energy of a collimated particulate ion beam through collisions with a buffer gas while radially constraining their motion using a quadrupole guide or trap over a limited mass range. Controlling the pressure drop of the final expansion into a quadrupole trap permits a much broader mass range {{at the cost of}} sacrificing collimation. To achieve high-resolution mass analysis of massive particulate ions, an efficient trap with a large tolerance for radial divergence of the injected ions was developed that permits trapping a large range of ions for on-demand injection into an awaiting mass <b>analyzer.</b> The design <b>specifications</b> required that frequency of the trapping potential be adjustable to cover a large mass range and the trap radius be increased to increase the tolerance to divergent ion injection. The large-radius linear quadrupole ion trap was demonstrated by trapping singly-charged bovine serum albumin ions for on-demand injection into a mass analyzer. Additionally, this work demonstrates the ability to measure an electrophoretic mobility cross section (or ion mobility) of singly-charged intact proteins in the low-pressure regime. This work represents a large step toward the goal of high-resolution analysis of intact proteins, RNA, DNA, and viruses...|$|R
40|$|Language-based {{programming}} environments {{provide some}} or all of the functionality of a compiler, an interactive debugger, a browser, and a configuration manager behind a unified user interface based on an editing paradigm. As the user edits a program, the changes are processed incrementally, allowing for low-latency updates to derived information. This information can be made available to interactive environment services, such as browsing, navigation, and "real time" error-reporting. In this dissertation, we address an important subproblem in the construction of such environments, the generation of static semantic analyzers that operate in an incremental mode. Our work is embodied in the Colander II system, which introduces both a new metalanguage for the declarative specification of static semantic analyses and new techniques for generating an incremental <b>analyzer</b> from these <b>specifications</b> automatically. Our specification metalanguage melds the advantages of traditional attribute grammars, including amenability to extensive generation-time analysis, with the expressiveness and clientindependence characteristic of Ballance's Logical Constraint Grammars. In comparison to traditional attribute grammars, our metalanguage allows much more of the incrementality inherent in a particular analysis task to be exposed within the formalism itself, where it can be exploited automatically by our implementation. Our incremental analysis algorithms exploit the attributed objects and function-valued attributes provided by our metalanguage, mapping these expressive notations onto a fine-grained incremental implementation. We are thus able to automatically generate incremental analyzers that handle longdistance dependencies and aggregate attributes efficiently. Our methods allow unusua [...] ...|$|R
40|$|Objective: To {{assess the}} {{accuracy}} of PIMA Point-of-Care (POC) CD 4 testing in rural Rakai, Uganda. Methods: 903 HIV positive persons attending field clinics provided a venous blood sample assessed on site using PIMA <b>analyzers</b> per manufacturer’s <b>specifications.</b> The venous samples were then run on FACSCalibur flow cytometry at a central facility. The Bland–Altman method was used to estimate mean bias and 95 % limits of agreement (LOA). Sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) were calculated for a CD 4 threshold of, 350 and, 500 cells/uL for antiretroviral eligibility. Results: There was a high correlation between PIMA and FACSCalibur CD 4 counts (r = 0. 943, p, 0. 001). Relative to FACSCalibur, the PIMA POC CD 4 had negative mean bias of 234. 6 cells/uL (95 % LOA: 2219. 8 to 150. 6) overall. The dispersion at CD 4, 350 cells/uL was 5. 1 cells/uL (95 % LOA: 2126. 6 to 136. 8). For a threshold of CD 4, 350 cells/uL, PIMA venous blood had a sensitivity of 88. 6 % (95 %CI 84. 8 – 92. 4 %), specificity of 87. 5 % (95 %CI 84. 9 – 90. 1 %), NPV of 94. 9 % (95 %CI 93. 1 – 96. 7 %), and PPV of 74. 4 % (95 %CI 69. 6 – 79. 2 %). PIMA sensitivity and PPV significantly increased to 96. 1 % and 88. 3 % respectively with increased threshold of 500 cells/uL. Conclusions: Overall, PIMA POC CD 4 counts demonstrated negative bias compared to FACSCalibur. PIMA POC sensitivit...|$|R
40|$|To {{assess the}} {{accuracy}} of PIMA Point-of-Care (POC) CD 4 testing in rural Rakai, Uganda. 903 HIV positive persons attending field clinics provided a venous blood sample assessed on site using PIMA <b>analyzers</b> per manufacturer's <b>specifications.</b> The venous samples were then run on FACSCalibur flow cytometry at a central facility. The Bland-Altman method was used to estimate mean bias and 95 % limits of agreement (LOA). Sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) were calculated for a CD 4 threshold of < 350 and < 500 cells/uL for antiretroviral eligibility. There was a high correlation between PIMA and FACSCalibur CD 4 counts (r =  0. 943, p< 0. 001). Relative to FACSCalibur, the PIMA POC CD 4 had negative mean bias of - 34. 6 cells/uL (95 % LOA: - 219. 8 to 150. 6) overall. The dispersion at CD 4 < 350 cells/uL was 5. 1 cells/uL (95 % LOA: - 126. 6 to 136. 8). For a threshold of CD 4 < 350 cells/uL, PIMA venous blood had a sensitivity of 88. 6 % (95 %CI 84. 8 - 92. 4 %), specificity of 87. 5 % (95 %CI 84. 9 - 90. 1 %), NPV of 94. 9 % (95 %CI 93. 1 - 96. 7 %), and PPV of 74. 4 % (95 %CI 69. 6 - 79. 2 %). PIMA sensitivity and PPV significantly increased to 96. 1 % and 88. 3 % respectively with increased threshold of 500 cells/uL. Overall, PIMA POC CD 4 counts demonstrated negative bias compared to FACSCalibur. PIMA POC sensitivity improved significantly at a higher CD 4 threshold of 500 than a 350 cells/uL threshold...|$|R
40|$|My Bachelor´s thesis {{deals with}} the {{examination}} of celiac disease in connection to osteoporosis. The aim of the thesis was to specify the level of antidotes against tissue transglutaminase in the class IgA and IgG in the serum with osteoporosis diagnosed patients, to interpret, {{based on the results}} of the study, possible link between osteoporosis and celiac disease as well as to compare the outcome with scientific data. I focused on the present knowledge of osteoporosis and celiac disease in the first part. As for celiac disease, I described its history and characteristics. I also paid attention to diagnostics of celiac disease with the assistance of laboratory serological tests ELISA. I mentioned the free-gluten diet treatment after that. As regards osteoporosis, I described its history, symptoms, forms, perils and complications which the disease causes. I conducted the methodical part of my work in the biochemical haematological laboratory Stafila where I work, with its seat in České Budějovice. As for measurements, I took them in the department of microbiology, where immunologic tests are also taken. I carried out the screening of celiac disease from serum, namely the determination of antidotes against tissue transglutaminase in the class IgA and IgG, with the aid of ELISA - a sandwich method. I took measurements with 58 osteoporosis diagnosed patients. I obtained the sera thanks to the supervisor of my thesis Mrs. Marie Ládová, who is at the head of an osteological outpatient department in České Budějovice. I stated the serological markers of celiac disease with the assistance of enzymatic immunoanalysis conducted on automatic analyzer Nexgen Four, which was provided by the firm Test-Line. I describe the <b>analyzer,</b> its <b>specification,</b> checking and calibration of the gadget together with determination checks in this part of the thesis. Additionally, there is described the principle of the method ELISA- sandwich, which is non-competitive enzymatic immunoanalysis. At the end of the section there is depicted the methodical procedure of my work, the preparation of working solutions and checks, the preparation and dilution of samples for analysis and working procedure for semiquantitative interpretation with positivity index and for quantitative interpretation U/ml. I continued with my own measurements. while respecting standard operations procedures of the laboratory Stafila, a limited liability company. The results of my measurements are processed in the third part of the thesis. The acquired results are given in a table: quantitative interpretation of the antidote against tTg IgA and IgG (U/ml) level in the sera of the osteoporosis diagnosed patients. Then I presented a graph interpreting the proportional relation of the antidote against tTg IgA and IgG levels in the sera of the osteoporosis diagnosed patients. There are included calibration graphs for tissue transglutaminase IgA and IgG and tables of the obtained data of checks for the methods in the results. 3 out of the 58 examined samples of osteoporotic patients were positive, they had the level of antibodies higher than 22 U/ml. One of the samples had marginal figures, which means it had the antidotes against tTg IgA an IgG between 18 and 22 U/ml. Positive results were found with patients born in 1976 and 1978. A patient born in 1962 appeared to have a marginal result. The acquired antidote figures against tissue transglutaminase in the class IgA and IgG are presented proportionally in the graph 1. 3 out of the 58 measured samples were positive, which represents 5 %, and 1 sample was marginal, which is 2 %. My analysis is based on the availability of the examined material and the potential of our laboratory. Celiac disease is an illness which was not paid much attention to in the past. Hypothesis about the link between osteoporosis and celiac disease has been proved...|$|R

