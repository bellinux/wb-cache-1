11|33|Public
50|$|At {{the highest}} level, each 192 {{consecutive}} frames are grouped into an <b>audio</b> <b>block.</b> While samples repeat each frame time, metadata is only transmitted once per <b>audio</b> <b>block.</b>|$|E
5000|$|One, two, three, or six <b>Audio</b> <b>Block</b> {{sections}}. These sections contain additional decoding metadata, {{as well as}} the encoded and quantized frequency coefficients. Each <b>Audio</b> <b>Block</b> {{corresponds to}} 256 PCM samples in each channel.|$|E
50|$|This is a {{specially}} coded preamble that identify the subframe and its {{position within the}} <b>audio</b> <b>block.</b> They are not normal BMC-encoded data bits, although they do still have zero DC bias.|$|E
5000|$|At {{the default}} 48 kHz sample rate, there are 250 <b>audio</b> <b>blocks</b> per second, and 3,072 {{kilobits per second}} with a biphase clock of 6.144 MHz ...|$|R
5000|$|An Audio Frame section, which {{contains}} decoding information {{common to all}} <b>audio</b> <b>blocks</b> within the syncframe, including the necessary information to determine how exponents and mantissas are packed.|$|R
40|$|This {{bachelor}} thesis {{deals with}} {{design of a}} stereo amplifier, which consists of several functional blocks for the analog audio processing. This document describes the solution of individual functional <b>audio</b> <b>blocks,</b> detailed design including schematics and contains also other documents {{for the construction of}} the amplifier. The main topic is design, simulation, construction and evaluation of a power amplifier that uses power transistors ThermalTrak. The evaluation {{of the results of the}} whole implementation compares measured parameters with other types of amplifiers...|$|R
50|$|Z (or B) : 11101000 if {{previous}} {{time slot}} was 0, 00010111 {{if it was}} 1. (Equivalently, 10011100 NRZI encoded.) Marks a word for channel A (left) {{at the start of}} an <b>audio</b> <b>block.</b>|$|E
50|$|X (or M) : 11100010 if {{previous}} {{time slot}} was 0, 00011101 {{if it was}} 1. (Equivalently, 10010011 NRZI encoded.) Marks a word for channel A (left), other than {{at the start of}} an <b>audio</b> <b>block.</b>|$|E
50|$|The 8-bit {{preambles}} {{are transmitted}} in time {{allocated to the}} first four time slots of each subframe (time slots 0 to 3). Any of the three {{marks the beginning of}} a subframe. X or Z marks the beginning of a frame, and Z marks the beginning of an <b>audio</b> <b>block.</b>|$|E
5000|$|Exponents {{for each}} channel are encoded in a highly packed {{differential}} format, with the deltas between consecutive frequency bins (other than the first) being given in the stream. Three formats, or exponent strategies, are used; these are known as [...] "D15", [...] "D25", and [...] "D45". In D15, each bin has a unique exponent, in D25 and D45, delta values correspond to either pairs or quads of frequency bins. <b>Audio</b> <b>blocks</b> other than {{the first in a}} syncframe may additionally reuse the prior blocks exponent set (this is required for channels that use the Adaptive Hybrid Transform).|$|R
5000|$|Audio {{frequency}} chokes (AFC) - {{designed to}} <b>block</b> <b>audio</b> and power line frequencies while allowing DC to pass ...|$|R
5000|$|Figure 8 below shows a {{conceptual}} {{view of a}} possible real world SLIMbus system. The [...] "M" [...] and [...] "F" [...] represents the Manager and Framer Devices respectively. Alternatively, the multi-mic array could replace the single mic in the system. Any mixture of <b>audio</b> related <b>blocks</b> could be attached.|$|R
5000|$|The {{optional}} {{data byte}} contains four [...] "ancillary" [...] bits {{corresponding to the}} AES3 VUCP bits. However, the P (parity) bit {{is replaced by a}} B bit which is set on the first sample of each <b>audio</b> <b>block,</b> and clear at all other times. This serves the same function as the B (or Z) synchronization preamble.|$|E
50|$|With one exception, S/PDIF {{protocol}} {{is identical}} to AES3. The channel status bit differs in S/PDIF. There is one channel status bit in each subframe, making 384 bits in each <b>audio</b> <b>block.</b> The meaning of the channel status bits is completely different between AES3 and S/PDIF. For S/PDIF, the 192-bit block for each channel is divided into 12 words of 16 bits each. The first 6 bits of the first word are a control code. The meaning of its bits are shown in the accompanying table.|$|E
40|$|International audienceThe HTML 5 {{standard}} is widespread on mobile devices. In {{combination with the}} Web Audio API, it allows for massively distributed real-time audio rendering. But timing issues exist, {{mainly because of the}} lack of standard inter-device synchronisation. This paper proposes a synchronisation solution based on HTML 5. Using a shared reference time, we achieved the distributed rendering of audio events with an individual accuracy of 1 to 10 ms, 5 ms in standard deviation, which is more accurate than the <b>audio</b> <b>block</b> duration, for any device that we measured...|$|E
40|$|In this demonstration, {{we present}} an {{implementation}} of a modular synthesizer in Haskell using Yampa. A synthesizer, be it a hardware instrument or a pure software implementation, as here, {{is said to be}} modular if it provides sound-generating and sound-shaping components that can be interconnected in arbitrary ways. Yampa, a Haskell-embedded implementation of Functional Reactive Programming, supports flexible construction of hybrid systems. Since music is a hybrid continuous-time and discrete-time phenomenon, Yampa and is a good fit for such applications, offering some unique possibilities compared to most languages targeting music or audio applications. The demonstration illustrates this point by showing how simple <b>audio</b> <b>blocks</b> can be described and then interconnected in a network with dynamically changing structure, reflecting the changing demands of a musical performance. Categories and Subject Descriptors D. 3. 2 [Programming Languages]: Language Classifications—functional languages, dataflo...|$|R
50|$|In {{systems that}} have both, the {{anti-aliasing}} filter and a reconstruction filter may be of identical design. For example, both the input and the output for audio equipment may be sampled at 44.1 kHz. In this case, both <b>audio</b> filters <b>block</b> {{as much as possible}} above 22 kHz and pass as much as possible below 20 kHz.|$|R
50|$|Version 3 also {{includes}} four new optional {{types of data}} blocks: Video Data Blocks containing the aforementioned Short Video Descripter (SVD), <b>Audio</b> Data <b>Blocks</b> containing Short <b>Audio</b> Descriptors (SAD), Speaker Allocation Data Blocks containing information about the speaker configuration of the display device, and Vendor Specific Data Blocks which can contain information specific to a given vendor's use.|$|R
40|$|Presented at the 2 nd Web Audio Conference (WAC), April 4 - 6, 2016, Atlanta, Georgia. The HTML 5 {{standard}} is wide-spread on mobile devices. In {{combination with the}} Web Audio API, it allows for massively distributed real-time audio rendering. But timing issues exist, {{mainly because of the}} lack of standard inter-device synchronisation. This paper proposes a synchronisation solution based on HTML 5. Using a shared reference time, we achieved the distributed rendering of audio events with an individual accuracy of 1 to 10 ms, 5 ms in standard deviation, which is more accurate than the <b>audio</b> <b>block</b> duration, for any device that we measured...|$|E
40|$|Introduction In part 1 of the project, you are {{required}} to implement a set of block dissection functions and a block play back function, in C or C++. The set of block dissection functions can read an MPEG 1 Audio layer- 3 file 1 and then either create an index file or extract part of the block to a buffer and/or write to a file. The play back function is written for playing a memory buffer which is a MPEG <b>audio</b> <b>block.</b> You are advised to use the amp- 0. 7. 6 [1] package {{as a basis for}} the development to implement the block extraction and block play back functions. You may also consult the webpage in [2] for some information about the source of amp. Upon the completion of part 1, {{you will be able to}} understand how the MPEG audio blocks are stored in a file, and how to dissect it block by block. 2 Suggested Interface You are given the full freedom to work at your own coding style and here we are given a suggested programming interface as a reference onl...|$|E
3000|$|In all simulations, {{parameters}} {{were set}} {{according to the}} <b>audio</b> applications. The <b>block</b> size and window length were set to 2 N[*]=[*] 2048, the sampling frequency was f [...]...|$|R
40|$|A {{computer}} application {{was developed to}} simulate the process of microphone positioning in sound recording applications. A dense, regular grid of impulse responses pre-recorded on {{the region of the}} room under study allowed the sound captured by a virtual mi- crophone to be auralised through real-time convolution with an anechoic stream representing the sound source. Convolution was performed using a block-based variation on the overlap-add method where the summation of many small sub- convolutions produced each block of output data samples. As the applied RIR filter varied on successive <b>audio</b> output <b>blocks,</b> a short cross fade was applied to avoid glitches in the audio. The maximum possible length of impulse response applied was governed by the size of <b>audio</b> processing <b>block</b> (hence la- tency) employed by the program. Larger blocks allowed a lower processing time per sample. At 23. 2 ms latency (1024 samples at 44. 1 kHz), it was possible to apply 9 second impulse responses on a standard laptop computer. Fundação para a Ciência e a Tecnologia (FCT...|$|R
30|$|Several hybrid {{algorithms}} {{based on}} the SVD transform have been recently proposed in literature. In the algorithm proposed by [23], the audio signal is first converted into a matrix form using the short-time Fourier transform (STFT), the SVD transform is then applied on the matrix, and finally embedding is carried out by adaptively modifying the SVD coefficients with watermark bits. In the hybrid algorithm proposed by [24], the audio signal is partitioned into blocks, and the watermark bits are embedded using dither modulation quantization of the singular values of the blocks. In [23], an audio watermarking algorithm is proposed in which watermark embedding and extraction procedures are {{based on the}} quantization of the norms of the singular values of <b>audio</b> <b>blocks.</b> The same authors proposed in [25] a hybrid algorithm in which watermark bits are embedded by applying quantization index modulation (QIM) on the singular values of wavelet-domain blocks. All of the abovementioned SVD-based hybrid algorithms employ some sort of quantization to embed watermark bits. Although quantization is simple, an acceptable level of robustness against noise and filtering attack may not always be achieved.|$|R
40|$|A {{solo show}} {{commissioned}} for and by 'Outpost Gallery' in Norwich. This is a two- screen video installation with sculptural elements. The Advice Shape is an immersive double screen video installation projected {{in a darkened}} chamber Thomas has constructed within the existing gallery walls. An arrow directs the audience into the installation through black and white slit curtains familiar to nightclubs, strip joints and festivals. Inside the chamber the curtains line two ends of the space and are freighted {{with a sense of}} anticipation. At each end domestic projector screens face each other. On the farthest an uncanny doubling of the slit curtains sways gently in a breeze, alternating with an androgynous nurse wearing a rubber head. This queer presence has an obscure intelligence about it and gestures at the audience as if undertaking some kind of aptitude test to no apparent ends. On the facing screen is a hybrid semi-narrative of sensationalist still and moving images, <b>audio,</b> <b>blocks</b> of colour, spoken word, performance and text. Appropriated imagery of natural disasters, mutated animals, abject beauty, American genocide and stuffed effigies coexist with chirrupy 1950 s advertising music or romantic classical. The soundtrack manipulates your judgment through a rollercoaster of affective registers...|$|R
40|$|This thesis {{describes}} {{the design of}} a sound localization algorithm in audio-based games for visually impaired children. An algorithm was developed to allow real-time audio playback and manipulation, using overlapping <b>audio</b> <b>blocks</b> multiplied by a window function. The audio signal was played through headphones. Multiple sound localization cues are evaluated. Based on these cues, two basic sound localization algorithms are implemented. The first uses only binaural cues. The second expands this with spectral cues by using the head-related impulse response (HRIR). The HRIR was taken from a database, interpolated to obtain optimal resolution and truncated to minimize memory usage. Both localization algorithms work well for lateral localization, but front-back confusions using the HRIR are still common. The signal received from a sound source changes with the distance to the sound source. Both the distance attenuation and propagation delay are implemented. As an alternative means of resolving front-back ambiguities, the use of a head tracker was investigated. Experiments with a webcam based head tracker show that a head tracker is a powerful tool to resolve front- back confusions. However, the latency of webcam based head trackers is too high to be of practical use. Microelectronics & Computer EngineeringElectrical Engineering, Mathematics and Computer Scienc...|$|R
50|$|In 2007, Block was {{featured}} in an on-camera interview, discussing visual style, for the 40th Anniversary DVD release of the film The Graduate. <b>Block's</b> <b>audio</b> commentary as a film historian is also featured in the 2008 Collector's Edition DVD of the classic 1960 film The Apartment.|$|R
5000|$|... {{internet}} Speech Audio Codec (iSAC) is a wideband speech codec, {{developed by}} Global IP Solutions (GIPS) (acquired by Google Inc in 2011). It {{is suitable for}} VoIP applications and streaming <b>audio.</b> The encoded <b>blocks</b> have to be encapsulated in a suitable protocol for transport, e.g. RTP.|$|R
40|$|Includes {{descriptive}} metadata {{provided by}} producer in M 4 A file: "Law School - General Events - 'The Present Economic Crisis' (<b>audio)</b> - Walter <b>Block</b> speaks to The Federalist Society at Vanderbilt University Law School. Recorded on January 27 th, 2009 at Vanderbilt University Law School. " By Vanderbilt University. Law Schoo...|$|R
40|$|Bachelor’s {{thesis on}} the theme "Mixing audio console with {{microphone}} preamplifier" deals with the design of an audio device for mixing signals from different sources. After a short theoretical introduction to issues mixing <b>audio</b> consoles, <b>block</b> diagram and circuit diagrams of each part are designed. These are redrawn into OrCAD Capture and using software PSpice simulations are performed. Configuration of the proposed mixing audio console is as follows: Two microphone preamplifiers with true differential inputs with switchable sensitivity, with the quad-band equalizer with tunable mid bands and adjustable delay. One line level input and one instrument input with three bands corrector. The output section consists of headphone and line level out...|$|R
50|$|A ringer box {{consists}} of bells or gongs and an electromagnetically-driven clapper which strikes the gongs when actuated. The electromagnet of the clapper {{responds to the}} alternating current sent from a central office exchange or another phone via the telephone network wiring. The direct current required by the telephone's <b>audio</b> circuitry is <b>blocked</b> with a capacitor before entering the ringer to prevent the ringer from being triggered by circuit interruptions and pulse dialing. Typical ring voltages {{are in the range}} from 60 to over 100 Volts, and alternate at a frequency of 20-30 Hertz.|$|R
40|$|Abstract—The use of {{multiple}} entropy models for Huffman or arithmetic coding is widely used to improve the compression efficiency of many algorithms when the source probability dis-tribution varies. However, the use {{of multiple}} entropy models increases the memory requirements of both the encoder and decoder significantly. In this paper, we present an algorithm which maintains almost all of the compression gains of multiple entropy models for only a very small increase in memory over one which uses a single entropy model. This can be used for any entropy coding scheme such as Huffman or arithmetic coding. This is accomplished by employing multiple entropy models only for the most probable symbols and using fewer entropy models for the less probable symbols. We show that this algorithm reduces the audio coding bitrate by 5 %- 8 % over an existing algorithm which uses the same amount of table memory by allowing effective switching of the entropy model being used as source statistics change over an <b>audio</b> transform <b>block.</b> I...|$|R
50|$|Opus {{has very}} low {{algorithmic}} delay, {{a necessity for}} use {{as part of a}} low-audio-latency communication link, which can permit natural conversation, networked music performances, or lip sync at live events. Total algorithmic delay for an audio format is the sum of delays that must be incurred in the encoder and the decoder of a live audio stream regardless of processing speed and transmission speed, such as buffering <b>audio</b> samples into <b>blocks</b> or frames, allowing for window overlap and possibly allowing for noise-shaping look-ahead in a decoder and any other forms of look-ahead, or for an MP3 encoder, the use of bit reservoir.|$|R
40|$|In typical <b>audio</b> {{encoders}} the <b>block</b> {{decision is}} done either using time-domain techniques like energy computation or frequency domain techniques like temporal noise shaping (TNS) [1], [2]. The time-domain energy computation based decisions are less effective for detecting {{many of the}} stringent scenarios presented by test cases like castanets and fatboy. The frequency domain based algorithms have better decision making capabilities, however they are inherently complex as they require the computation of the FFT, additionally in case of TNS the computation of LPC (Linear Prediction coding) in the frequency domain. An improved time-domain technique with better block decision capability compared to TNS and with lesser computational complexity is proposed in this paper. QC 20130610 </p...|$|R
40|$|In this paper, an {{implementation}} of an AM radio receiver using a Field Programmable Gate Array (FPGA) is described. By using this approach, {{the flexibility of}} software and the speed of hardware can be combined, i. e., the receiver construction can be modified without new hardware. The receiver consists of a processor core and several logic <b>blocks.</b> <b>Audio</b> signal processing and adaptive control of the RF signals are performed by the processor, while the logic blocks handle high-speed signal processing tasks such as envelope detection. To program the FPGA, all of the circuits are first described using HDL, then the circuit description is downloaded to the FPGA. The experimental board that was built is described and some problems using this approach are reported...|$|R
40|$|WO 2005013491 A UPAB: 20050406 NOVELTY - Method for {{converting}} {{and handling}} audio data streams, especially MP 3 data streams has the following steps: {{combination of the}} determining <b>block</b> <b>audio</b> data (44, 46), that are assigned to a determining block, from at least two data blocks, {{in order to obtain}} a resultant combined determining block data (48) that forms part of a second audio data stream formed from two or more first data streams. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also made for five further methods for handling audio data streams and five device for handling audio data streams. USE - Audio file format conversion, especially for combining individual audio data channels into a multi-channel audio data stream. ADVANTAGE - Format conversion is possible without use of backward pointers...|$|R
5000|$|In {{addition}} to the MP4, 3GP and other container formats based on ISO base media file format for file storage, AAC audio data was first packaged in a file for the MPEG-2 standard using Audio Data Interchange Format (ADIF), consisting of a single header followed by the raw AAC <b>audio</b> data <b>blocks.</b> However, if the data is to be streamed within an MPEG-2 transport stream, a self-synchronizing format called an Audio Data Transport Stream (ADTS) is used, consisting {{of a series of}} frames, each frame having a header followed by the AAC audio data. This file and streaming-based format are defined in MPEG-2 Part 7, but are only considered informative by MPEG-4, so an MPEG-4 decoder does not need to support either format. These containers, as well as a raw AAC stream, may bear the [...]aac file extension. MPEG-4 Part 3 also defines its own self-synchronizing format called a Low Overhead Audio Stream (LOAS) that encapsulates not only AAC, but any MPEG-4 audio compression scheme such as TwinVQ and ALS. This format is what was defined for use in DVB transport streams when encoders use either SBR or parametric stereo AAC extensions. However, it is restricted to only a single non-multiplexed AAC stream. This format is also referred to as a Low Overhead Audio Transport Multiplex (LATM), which is just an interleaved multiple stream version of a LOAS.|$|R
