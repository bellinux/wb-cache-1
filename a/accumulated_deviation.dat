5|23|Public
40|$|Most {{gas turbine}} {{performance}} analysis based diagnostic methods {{use the information}} from steady state measurements. Unfortunately, steady state measurement may not be obtained easily in some situations, and some types of gas turbine fault contribute little to performance deviation at steady state operating conditions but significantly during transient processes. Therefore, gas turbine diagnostics with transient measurement is superior to that with steady state measurement. In this paper, an <b>accumulated</b> <b>deviation</b> is defined for gas turbine performance parameters {{in order to measure}} the level of performance deviation during transient processes. The features of the <b>accumulated</b> <b>deviation</b> are analysed and compared with traditionally defined performance deviation at a steady state condition. A non-linear model based diagnostic method, combined with a genetic algorithm (GA), is developed and applied to a model gas turbine engine to diagnose engine faults by using the <b>accumulated</b> <b>deviation</b> obtained from transient measurement. Typical transient measurable parameters of gas turbine engines are used for fault diagnostics, and a typical slam acceleration process from idle to maximum power is chosen in the analysis. The developed diagnostic approach is applied to the model engine implanted with three typical single-component faults and is shown to be very successful...|$|E
40|$|AbstractGears {{as the key}} {{transmission}} {{parts of}} machineries are widely used in mechanical and aeronautical fields. GeoSpelling, a framework standardized as ISO 17450 - 2005 supported tolerancing process {{by a set of}} concepts and mathematical algorithms, is a coherent and complete model to manage the shape variations of geometrical parts. Considering current gear specification standards incline to inspection convenient ignoring tolerance quality control, so it is a realistic necessary to develop a coherent gear specification model based on GeoSpelling language. This model should not only consider integrating functional requirements involved in design stage but also be able to correspond to manufacturing and inspection procedures. In this paper a new approach based on discrete geometry is introduced to design gear specification model. It is also focus on a perspective discussion on how SSA (Statistical Shape Analysis) method could be a solution to analyze the gear integrated errors into different individual errors, such as tangential composite error, tooth-to-tooth tangential composite error, pitch <b>accumulated</b> <b>deviation,</b> tooth-to-tooth pitch <b>accumulated</b> <b>deviation</b> and geometric eccentricity. The fundamentals mentioned above provide a reliable methodology for analyzing gear manufacturing errors. As a result it is able to enhance the computational processing capability and metrological traceability for gear quality control...|$|E
40|$|The paper {{examines}} whether {{security issues}} and repurchases adjust the capital structure toward the target. The time-series patterns of debt ratios imply that only debt reductions are initiated {{to offset the}} <b>accumulated</b> <b>deviation</b> from target leverage. The importance of target leverage in earlier debt-equity choice studies {{is driven by the}} subsample of equity issues accompanied by debt reductions. Unlike debt issues and reductions, equity issues and repurchases have no significant lasting effect on capital structure. Therefore, even firms that have target debt ratios can engage in timing the equity market. ...|$|E
40|$|We propose and {{demonstrate}} a new methodology to stabilize systems with complex dynamics like the supply chain. This method {{is based on}} the <b>accumulated</b> <b>deviations</b> from equilibrium (ADE). It is most beneficial for controlling system dynamic models characterized by multiple types of delays, many interacting variables, and feedback processes. We employ the classical version of particle swarm optimization as the optimization approach due to its performance in multidimensional space, stochastic properties, and global reach. We demonstrate the effectiveness of our method based on ADE using a manufacturing-supply-chain case study...|$|R
30|$|We {{combine the}} results from the binary {{classifier}} with a voting scheme: every binary classifier has a vote, and a data point is classified into the class with the maximum number of votes. To solve the clash of the same votes, we simply choose the class with the greater sequence number. In addition, we increase the sequence number for the classes with every multiclass SVM to avoid <b>accumulated</b> <b>deviations.</b> A simple example indicates the risks without this strategy: Assume we have three classes, A, B, and C, all with the same accuracy and scale. Then, samples classified into A are least.|$|R
40|$|This paper {{introduces}} a new method that facilitates the stability analysis of system dynamics models. The method {{is based on}} the concepts of asymptotic stability and <b>Accumulated</b> <b>Deviations</b> from Equilibrium (ADE) convergence. We prove several theorems that show that ADE convergence of a state variable will make its trajectory approach asymptotic stability. Achieving ADE convergence requires the solution of a policy optimization problem. We use an approach called Behavior Decomposition Weights (BDW) to reduce the search space associated with that optimization problem. We also demonstrate this method on two examples: a linear “inventory-workforce ” model and a non-linear “mass business cycle model”. These examples illustrate the features of this method and the potential for the development of efficient tools {{to improve the quality of}} the optimization policies...|$|R
40|$|Islamic Relief Organization Saudi Arabia channeled 21 units waqf {{mosque in}} 2015. They {{distributed}} into five districts in Yogyakarta. The development focused on managing triple constraints {{in order to}} complete the job within four months and to control risk factors. The method {{used to measure the}} success rate of this program is calculating the deviation value between realization progress and plan progress with the S Curve. This study aims to identify the successful implementation of triple constraints management which is able to make a major contribution to the risk management effectively and efficiently. The success of this project looks at the <b>accumulated</b> <b>deviation</b> of each month: M 1 = - 7. 75 %, - 19. 01 % M 2 = M 3 = 18 : 07 %, and M 4 = 8. 69 %...|$|E
40|$|OVERVIEW: There is {{a growing}} demand for {{renewable}} energy resources {{with the rise of}} concern for the environment. Many fear that the increasing distributed energy resources will degrade the power quality of the distribution systems. While microgrids can reduce the amount of degradation, they can also operate almost completely independently, thus complicating the balancing of supply and demand control in the grid. To solve this problem, we have developed a short-term balanced supply and demand control system. The system adopts a highly accurate method for correcting the <b>accumulated</b> <b>deviation</b> in the power generation targets setting, based on the short-term demand forecast. When we examined the verification using a real-time simulator, which simulates distributed resources, we confirmed the ability of the microgrid and distributed resources to balance supply and demand loads for short periods of time. The developed system enabled the supply and demand balance term to be adjusted about 1 / 3 of the conventional systems...|$|E
40|$|The B-factories {{results provide}} an {{impressive}} {{confirmation of the}} Standard Model (SM) description of flavor and CP violation. Nevertheless, as more data were <b>accumulated,</b> <b>deviations</b> in the 2. 5 - 3. 5 sigma range have emerged pointing to the exciting possibility of new CP-odd phase(s) and flavor violating parameters in B-decays. Primarily {{this seems to be}} the case in the time dependent CP asymmetries in penguin dominated modes (e. g. B -> phi (eta') Ks). We discuss these and other deviations from the SM and, as an illustration of possible new physics scenarios, we examine the role of the Top Two Higgs Doublet Model. This is a simple extension of the SM obtained by adding second Higgs doublet in which the Yukawa interactions of the two Higgs doublets are assigned in order to naturally account for the large top-quark mass. Of course, many other extensions of the Standard Model could also account for these experimental deviations. Clearly if one takes these deviations seriously then some new particles in the 300 GeV to few TeV with associated new CP-odd phase(s) are needed. Comment: 40 pages, 17 figures (png format), uses pdflate...|$|R
40|$|Empirical studies {{observe that}} {{currency}} exchange rates often deviate from the purchasing power parity (PPP) theoretical values. Previous literature also recognises that exchange rate deviations are self-correcting over time. This study shows that, for bilateral rates of eight developed country currencies {{for the period}} 1974 to 2004, exchange rate <b>deviations</b> <b>accumulated</b> {{over a five-year period}} were partially corrected over the following three years. Further, this research shows that cumulative PPP deviations can provide reliable information for predicting future directions of exchange rate movements. A new approach to managing currency risk is proposed in this study to improve international stock portfolio performance. Constructing internationally diversified portfolios, the strategy explicitly recognises currency risk and manages this risk by excluding assets denominated in over-valued currencies. Using quarterly data from eight countries with freely floating currencies over the period 1991 to 2004, the strategy outperformed the MSCI world index in terms of risk reduction and return improvement between 2. 18 and 5. 08 %. Depending on the country selected, the proposed strategy realized between 1. 50 and 4. 68 % higher annualised returns than an equally-weighted total diversification strategy. Most of the improvements remained after adjusting for risk. Next, a strategy for managing exposure to transactions denominated in foreign currencies is developed, which incorporates both forward rate premiums and predictions of exchange rate movements based on <b>accumulated</b> <b>deviations</b> from PPP. This strategy requires hedging long positions when a foreign currency is over-valued and the forward contract is trading at a premium. The exposures are left uncovered under other conditions. When this strategy was evaluated across seven currencies from the view points of eight countries; for the period 1986 to 2004, it increased domestic cash flow returns and reduced return volatilities when compared against an unhedged strategy...|$|R
40|$|International audienceNowadays, the {{requirements}} of customers concerning a product {{are more and more}} tight and high. The satisfaction of these, such as quality, reliability, robustness and cost, {{plays an important role in}} the context of global and concurrent economy. However, a product, during its life cycle, passes through manufacturing and assembly stages where geometrical deviations are generated and <b>accumulated.</b> These <b>deviations</b> that obviously have an influence on the performances of the product are not considered. Thus, a model based on the small displacement torsor that allows to model the geometrical deviations of the product is proposed in this paper...|$|R
40|$|The {{performance}} of the distributed slot synchronization algorithm is analyzed under the situation of slot drift due to different crystal frequencies. An interference model is established by setting the <b>accumulate</b> clock <b>deviation</b> to be equivalent to a slot phase deviation. It is proved that the accumulation of slot deviation can be avoided and the maximum of slot deviation can be controlled within a limited range by using the distributed slot synchronization algorithm. Finally, the simulation and reality test show that using the distributed slot synchronization algorithm can assure the slot synchronization of each node in ad hoc network...|$|R
40|$|To {{reliably}} {{detect the}} cosmic microwave background (CMB) anisotropy is of great importance in understanding the birth and evolution of the Universe. One of the difficulties in CMB experiments is the domination of measured CMB anisotropy maps by the Doppler dipole moment from the motion of the antenna relative to the CMB. For each measured temperature the expected dipole component has to be calculated separately and then subtracted from the data. A small error in dipole direction, antenna pointing direction, sidelobe pickup contamination, and/or timing synchronism, can raise significant deviation in the dipole cleaned CMB temperature. After a full-sky observational scan, the <b>accumulated</b> <b>deviations</b> will be structured with a pattern closely correlated to the observation pattern with artificial anisotropies on large scales, including artificial quadrupole, octopole etc in the final CMB map. Such scan-induced anisotropies on large scales can be predicted by the true dipole moment and observational scan scheme. Indeed, the expected scan-induced quadrupole pattern of the WMAP mission is perfectly in agreement with the published WMAP quadrupole. With the scan strategy of the Planck mission, we predict that scan-induced anisotropies will also produce an artificially aligned quadrupole. The scan-induced anisotropy is a common problem for all sweep missions and, like the foreground emissions, has to be removed from observed maps. Without doing so, CMB maps from COBE, WMAP, and Planck as well, are not reliable for studying the CMB anisotropy. Comment: Accepted for publication in Ap...|$|R
40|$|Currently, Indiana Department of Transportation (INDOT) {{is using}} the California Profilograph as the {{standard}} measuring device in its construction smoothness specifications. The output derived from the profilograph is called Profile Index (PI). PI represents the total <b>accumulated</b> <b>deviations</b> of the profilograph output traces beyond a tolerance zone (blanking band). At present, INDOT is using 0. 2 -inch blanking band to evaluate the profile traces, which has raised some concerns because some small unpleasant surface irregularities are covered by the blanking band. This study developed a rational method for interpreting profilograph traces using the 0. 0 -inch blanking band (zero tolerance) method and established the corresponding pavement smoothness specifications. The development of the preliminary PI 0. 0 smoothness specification was performed by converting the existing PI 0. 2 specification to the PI 0. 0 specification. Several Profile Index conversion models were used to perform the conversion. ^ In addition, current incentive/disincentive policies specified in the smoothness specification {{are based on the}} subjective engineering judgment. To which extent they can really reflect the long-term benefits of a smoother pavement by providing a longer service life is still unknown. Thus, the roughness progression model was developed using the Artificial Neural Network methodology to determine the effect of various initial specification smoothness limits on the future smoothness progression and the pavement service life. Finally, using the developed model, the preliminary converted specification was modified to account for the long term benefit of the pavement and thus justified the incentive/disincentive policies in the specification. ...|$|R
40|$|Offshore wind {{converter}} platforms {{are complex}} installations {{that increase the}} competitiveness of offshore wind as an energy source. Prior {{research in the field}} of offshore platform project execution has focused on early project phases and planning as means to increase project reliability. Later phases such as fabrication, transport and installation have not received the same attention from academia and industry. Projects of this type frequently suffer both large and small deviations. The further projects progress, the more <b>deviations</b> they <b>accumulate.</b> The <b>accumulated</b> <b>deviations</b> have to be resolved in a timely manner so as to avoid impairing the quality and scheduling of an overall project. This research explores the design of converter platforms and the management of engineering change in relation to fabrication, transport and installation in order to increase the overall reliability of projects. Two offshore platform projects in three case studies form the source of empirical data. The first of the three studies considered prior research connected to fabrication and installation of offshore platforms. In the second study, the effect of two different platform designs on the fabrication and installation process was investigated. The third study considered engineering change management as a tool to achieve changeability, and examined its ability to buffer against deviations affecting later project phases i. e. fabrication, transport and installation. The findings revealed that the design’s effects on a project’s outcome are often not the driver of reliability. Rather, it was found that engineering change management is essential to any project to manage the changeable nature of projects. This research also raises concerns as to how much engineering change to allow for and in what project phase. That engineering change, as a tool, should preferably be used sparingly in early phases and as necessary in later phases. The observed engineering change process in the studied projects was chaotic.   This research suggests that engineering change can be organised around change carriers. In this way, it is predicted that the processes of change can become more stable and predictable. ...|$|R
40|$|Currently, Indiana Department of Transportation (INDOT) {{is using}} the California Profilograph as the {{standard}} measuring device in its smoothness specifications. The output derived from the profilograph is called Profile Index (PI). PI represents the total <b>accumulated</b> <b>deviations</b> of the profilograph output traces beyond a tolerance zone (blanking band). At present, INDOT is using 0. 2 -inch blanking band to evaluate the profile traces, which has raised some concerns because some small unpleasant surface irregularities are covered by the blanking band. The major objective {{of this study was}} to develop a rational method for interpreting profilograph traces using 0. 0 -inch blanking band (zero tolerance) method and to establish corresponding pavement smoothness specifications. The secondary objective was to develop/adopt an automated system for the pavement profile analysis from printed profilograph traces. The study was divided into two parts. In the first part (synthesis study), a literature review was conducted to obtain information of smoothness specifications, smoothness measuring devices, and indices. Profilograph traces from several completed paving projects were analyzed using 0. 2 -inch and 0. 0 -inch blanking bands to develop manual reduction procedure for the 0. 0 -inch blanking band Profile Index. In the second part of the project, new PI 0. 0 construction smoothness specifications were developed by converting current PI 0. 2 smoothness specifications to the new PI 0. 0 specifications using developed conversion models. The converted PI 0. 0 specifications were then compared with the current Kansas DOT (KDOT) and other PI 0. 0 specifications. A partial verification of the converted PI 0. 0 specification was done by calculating pay factors for several recently completed paving projects measured using California profilograph. Measurement results were reduced manually and automatically by the Proscan system, which includes scanner and analysis program to reduce printed traces. It has been developed by Kansas State University and currently KDOT is using it in their construction QA procedures. The Proscan system showed excellent repeatability, and it saved considerable amount of time compared to the manual trace reduction. It is therefore recommended that INDOT uses Proscan system in their constitution QA operations. The converted PI 0. 0 specifications were also modified to comply with the Proscan reduction results...|$|R
40|$|The {{majority}} of scientific satellites investigating the Earth magnetosphere are spin stabilized. The attitude information comes usually from a sun sensor and {{is missing in}} the umbra; hence, the accurate experimental determination of vector quantities is not possible during eclipses. The spin period of the spacecraft is generally not constant during these times because the moment of inertia changes due to heat dissipation. The temperature dependence of the moment of inertia for each spacecraft has a specific signature determined by its design and distribution of mass. We developed an "eclipse-spin" model for the spacecraft spin period behaviour using magnetic field vector measurements close to the Earth, where the magnetic field {{is dominated by the}} dipole field, and in the magnetospheric lobes, where the magnetic field direction is mostly constant. The modelled spin periods give us extraordinarily good results with <b>accumulated</b> phase <b>deviations</b> over one hour of less than 10 degrees. Using the eclipse spin model satellite experiments depending on correct spin phase information can deliver science data even during eclipses. Two applications for THEMIS B, one in the lobe and the other in the lunar wake, are presented...|$|R
40|$|Magnet alignment, {{especially}} of the quadrupole magnet, {{is one of the}} most important factor to reduce the closed orbit distortion and thereby to stabilize the particle acceleration. The magnets were aligned in 0. 1 mm in the KEK-PS main ring. However, the floor subsidence has been mostly caused by the radiation shield blocks in the experimental hall. In order to examine the aged deterioration of the alignment, the relative level between neighboring quadrupole magnets has been measured once a year. When the magnet realignment was done in 1996 fall in order to correct the <b>accumulated</b> level <b>deviation,</b> the fast floor movement was observed. In order to confirm this phenomena, continuous measurement of the relative quadrupole magnet level is going on, and the detectors for the floor tilt were set up during 1997 summer shut down. Status of this work is reported. 1. CURRENT STATUS OF THE KEK-PS The KEK-PS complex consists of four accelerators: two 750 -keV Cockcroft-Walton pre-injectors, a 40 -MeV injector linac, a 500 -MeV booster synchrotron and a 12 -GeV main ring. Figure 1 shows a layout of the KEK-PS complex. The slow beems are extracted to the east and north counter halls by a half integer resonance, and the internal target has also served the secondary beam in the east counter hall [1]. Beam bunches accelerated in the booster except ones into the main ring are utilized i...|$|R
40|$|Shoulder {{instability}} is {{a common}} shoulder injury, and patients present with plastic deformation of the glenohumeral capsule. Gene expression analysis may be {{a useful tool for}} increasing the general understanding of capsule deformation, and reverse-transcription quantitative polymerase chain reaction (RT-qPCR) has become an effective method for such studies. Although RT-qPCR is highly sensitive and specific, it requires the use of suitable reference genes for data normalization to guarantee meaningful and reproducible results. In the present study, we evaluated the suitability of a set of reference genes using samples from the glenohumeral capsules of individuals with and without shoulder instability. We analyzed the expression of six commonly used reference genes (ACTB, B 2 M, GAPDH, HPRT 1, TBP and TFRC) in the antero-inferior, antero-superior and posterior portions of the glenohumeral capsules of cases and controls. The stability of the candidate reference gene expression was determined using four software packages: NormFinder, geNorm, BestKeeper and DataAssist. Overall, HPRT 1 was the best single reference gene, and HPRT 1 and B 2 M composed the best pair of reference genes from different analysis groups, including simultaneous analysis of all tissue samples. GenEx software was used to identify the optimal number of reference genes to be used for normalization and demonstrated that the <b>accumulated</b> standard <b>deviation</b> resulting from the use of 2 reference genes was similar to that resulting from the use of 3 or more reference genes. To identify the optimal combination of reference genes, we evaluated the expression of COL 1 A 1. Althoug...|$|R
40|$|The {{anterior}} cruciate ligament (ACL) {{is one of the}} most frequently injured structures during high-impact sporting activities. Gene expression analysis may be a useful tool for understanding ACL tears and healing failure. Reverse transcription-quantitative polymerase chain reaction (RT-qPCR) has emerged as an effective method for such studies. However, this technique requires the use of suitable reference genes for data normalization. Here, we evaluated the suitability of six reference genes (18 S, ACTB, B 2 M, GAPDH, HPRT 1, and TBP) by using ACL samples of 39 individuals with ACL tears (20 with isolated ACL tears and 19 with ACL tear and combined meniscal injury) and of 13 controls. The stability of the candidate reference genes was determined by using the NormFinder, geNorm, BestKeeper DataAssist, and RefFinder software packages and the comparative ΔCt method. ACTB was the best single reference gene and ACTB+TBP was the best gene pair. The GenEx software showed that the <b>accumulated</b> standard <b>deviation</b> is reduced when a larger number of reference genes is used for gene expression normalization. However, the use of a single reference gene may not be suitable. To identify the optimal combination of reference genes, we evaluated the expression of FN 1 and PLOD 1. We observed that at least 3 reference genes should be used. ACTB+HPRT 1 + 18 S is the best trio for the analyses involving isolated ACL tears and controls. Conversely, ACTB+TBP+ 18 S is the best trio for the analyses involving (1) injured ACL tears and controls, and (2) ACL tears of patients with meniscal tears and controls. Therefore, if the gene expression study aims to compare non-injured ACL, isolated ACL tears and ACL tears from patients with meniscal tear as three independent groups ACTB+TBP+ 18 S+HPRT 1 should be used. In conclusion, 3 or more genes should be used as reference genes for analysis of ACL samples of individuals with and without ACL tears...|$|R
40|$|An {{efficient}} {{method to}} compute local density-based outliers in high dimensional data was proposed. In our work, {{we have shown}} {{that this type of}} outlier is present even in any subset of the dataset. This property is used to partition the data set into random subsets to compute the outliers locally. The outliers are then combined from different subsets. Therefore, the local density-based outliers can be computed efficiently. Another challenge in outlier detection in high dimensional data is that the outliers are often suppressed when the majority of dimensions do not exhibit outliers. The contribution of this work is to introduce a filtering method whereby outlier scores are computed in sub-dimensions. The low sub-dimensional scores are filtered out and the high scores are aggregated into the final score. This aggregation with filtering eliminates the effect of <b>accumulating</b> delta <b>deviations</b> in multiple dimensions. Therefore, the outliers are identified correctly. In some cases, the set of outliers that form micro patterns are more interesting than individual outliers. These micro patterns are considered anomalous with respect to the dominant patterns in the dataset. In the area of anomalous pattern detection, there are two challenges. The first challenge is that the anomalous patterns are often overlooked by the dominant patterns using the existing clustering techniques. A common approach is to cluster the dataset using the k-nearest neighbor algorithm. The contribution of this work is to introduce the adaptive nearest neighbor and the concept of dual-neighbor to detect micro patterns more accurately. The next challenge is to compute the anomalous patterns very fast. Our contribution is to compute the patterns based on the correlation between the attributes. The correlation implies that the data can be partitioned into groups based on each attribute to learn the candidate patterns within the groups. Thus, a feature-based method is developed that can compute these patterns efficiently. Ph. D. Committee Chair: Mark, Leo; Committee Chair: Omiecinski, Edward; Committee Member: Kim, Jinho; Committee Member: Liu, Ling; Committee Member: Navathe, Sha...|$|R
60|$|If it {{be assumed}} that the {{characteristic}} differences between the various domestic races are due to descent from several aboriginal species, we must conclude that man chose for domestication in ancient times, either intentionally or by chance, a most abnormal set of pigeons; for that species resembling such birds as Pouters, Fantails, Carriers, Barbs, Short- faced Tumblers, Turbits, etc., {{would be in the}} highest degree abnormal, as compared with all the existing members of the great pigeon family, cannot be doubted. Thus we should have to believe that man not only formerly succeeded in thoroughly domesticating several highly abnormal species, but that these same species have since all become extinct, or are at least now unknown. This double accident is so extremely improbable that the assumed existence of so many abnormal species would require to be supported by the strongest evidence. On the other hand, if all the races are descended from C. livia, we can understand, as will hereafter be more fully explained, how any slight deviation in structure which first appeared would continually be augmented by the preservation of the most strongly marked individuals; and as the power of selection would be applied according to man's fancy, and not for the bird's own good, the <b>accumulated</b> amount of <b>deviation</b> would certainly be of an abnormal nature in comparison with the structure of pigeons living in a state of nature.|$|R
40|$|The World s {{energy demand}} is rapidly {{increasing}} {{and a good}} viable renewable energy source is wind power. The land-based knowledge and experience the onshore wind turbine industry possess is used to develop offshore wind turbines. With this knowledge together with the experience {{and knowledge of the}} marine industry we can design and produce a floating wind turbine. The main advantages of an offshore wind turbine are that the wind is stronger and less turbulent at sea, visual and noise annoyances can be avoided and there are large available areas at sea. In this thesis coupled time domain analyses of a floating spar-type wind turbine are performed with the intension to study parameters affecting fatigue damage at base of the tower. The software applied is SIMO/Riflex with the extension TDHmill, which gives the wind thrust force and gyro moment on the wind turbine as point loads in the tower top. Short term environment conditions are chosen from a joint distribution of simultaneous wind and waves which is based on measurements from a site in the North Sea in the period 1973 1999. In total 141 different environmental conditions are chosen for the sensitivity study. Mean value, standard deviation, skewness and kurtosis are calculated for axial stresses {{at the base of the}} tower. Fatigue damage is calculated from the Palmgren-Miner sum with a nominal stress SN-curve from the DNV fatigue standard. The axial stress-cycle distribution used in the Palmgren-Miner sum is found by rainflow counting. Time domain simulations are carried out for the different sea states and fatigue damage is calculated for each case. The statistical properties and fatigue damage are averaged over seven samples with different random seed number to ensure acceptable statistical uncertainty. <b>Accumulated</b> standard <b>deviation</b> shows that 5 samples of each load case are sufficient to ensure acceptable statistical uncertainty. Sensitivity study of different simulation length shows that 30 minute simulations give close to equal fatigue damage and standard deviation as 2. 5 hour simulations. Sensitivity of fatigue to wave height and peak period is carried out to study the effect of varying parameters. This study suggests that the highest waves dominate the fatigue damage for the smallest peak periods. For some small wave heights the damage will be constant for a given peak period range. From this sensitivity study it is shown that if the deviation of fatigue damage between the different load cases is small, then the dominating load case of the accumulated long term fatigue damage will be dominated by the marginal probability of each load case...|$|R
40|$|The {{land-based}} navigation, {{paying attention}} to precision farming, is the research topic: the final purpose is the design and development of a guidance-aided system focusing on a lew-cost GPS receiver able to provide a pseudorange-based solution only. Specific tests {{have been carried out}} to perform the particular trajectories followed by the vehicle in agricultural applications, whose accuracy target is typically 1 m. Results show that the low-cost receiver is affected by a drift in time which is mainly detected while turning and causing a deviation from the optimal reference solution. Thus, the goal is to correct this behaviour because the <b>deviation</b> <b>accumulates</b> during time and causes a not optimal treatment of the field (waste of material and money). Paying attention to the cost of the system, a new idea was proposed: the integration between the low-cost GPS with a magnetometer/digital compass. A dedicated algorithm was also implemented, taking the heading provided by the magnetometer and using it to correct the deviation in rums. Unluckily a magnetometer is deeply influenced by ferrous materials and the sensor is supposed to be installed on the vehicle, which is mainly made by metal. As a consequence, the sensed measurements are affected by a deviation from the actual magnetic field. Those disturbances need to be properly reduced by an autocalibration procedure. First of all, this paper deeply analyzes the performances of those particular low-cost GPS receivers; furthermore a solution is presented by means of the integration with a magnetometer. Results will be shown with great attention to future developments and improvements of the low-cost system suggested for supporting the guidance in precision farming applications...|$|R
40|$|Purpose: The aim of {{this work}} is to {{evaluate}} the geometric accuracy of a prerelease version of a new {{six degrees of freedom}} (6 DoF) couch. Additionally, a quality assurance method for 6 DoF couches is proposed. Methods: The main principle of the performance tests was to request a known shift for the 6 DoF couch and to compare this requested shift with the actually applied shift by independently measuring the applied shift using different methods (graph paper, laser, inclinometer, and imaging system). The performance of each of the six axes was tested separately as well as in combination with the other axes. Functional cases as well as realistic clinical cases were analyzed. The tests were performed without a couch load and with a couch load of up to 200 kg and shifts in the range between − 4 and + 4 cm for the translational axes and between − 3 ° and + 3 ° for the rotational axes were applied. The quality assurance method of the new 6 DoF couch was performed using a simple cube phantom and the imaging system. Results: The deviations (mean ± one standard <b>deviation)</b> <b>accumulated</b> over all performance tests between the requested shifts and the measurements of the applied shifts were − 0. 01 ± 0. 02, 0. 01 ± 0. 02, and 0. 01 ± 0. 02 cm for the longitudinal, lateral, and vertical axes, respectively. The corresponding values for the three rotational axes couch rotation, pitch, and roll were 0. 03 ° ± 0. 06 °, − 0. 04 ° ± 0. 12 °, and − 0. 01 ° ± 0. 08 °, respectively. There was no difference found between the tests with and without a couch load of up to 200 kg. Conclusions: The new 6 DoF couch is able to apply requested shifts with high accuracy. It {{has the potential to be}} used for treatment techniques with the highest demands in patient setup accuracy such as those needed in stereotactic treatments. Shifts can be applied efficiently and automatically. Daily quality assurance of the 6 DoF couch can be performed in an easy and efficient way. Long-term stability has to be evaluated in further tests...|$|R
40|$|The {{purpose of}} my thesis {{is to provide}} a {{theoretical}} analysis of the dynamics of optically excited carriers in semiconductor confined systems. In particular, I will focus the investigations on the effects due to the presence of a strong electron-hole Coulomb correlation. The Coulomb interaction leads to the formation of hydrogen-like bound states called excitons. The dynamics of these states is investigated in bare and cavity embedded quantum wells, and in quantum wires. I have investigated the dynamics of the relaxation of excitons in quantum wells due to the interaction with acoustic phonons and I have reproduced the temporal evolution of the photoluminescence emission. I have explained why the decay times observed in non resonant photoluminescence experiments are much slower than the radiative recombination time of a single exciton. Furthermore, deviation from the thermal equilibrium gives a characteristic dependence of these decay times on the temperature. The build-up of the photoluminescence is related to the relaxation by phonon emission of the excited electron-hole pairs. Initially, the pairs created at high energy are not bound, and the formation of bound excitons occurs during the relaxation. I have described the exciton formation due to the emission of acoustic and optical phonons, and I have calculated the characteristic times for this process in GaAs quantum wells. The formation can be geminate or bimolecular. In the geminate formation, the exciton is directly created by the photon of the external pump by simultaneous emission of an optical phonon, while in the bimolecular formation the exciton is created from thermalized electron-hole pairs. In the first case the formation occurs only during the laser pump, while in the second case the formation depends on the total density of carriers available in the crystal. The effects of the phonon assisted formation on the overall dynamics of free carriers, excitons and of the photoluminescence are discussed. Excitons confined in a quantum well coupled with the photons modes of a semiconductor microcavity gives mixed exciton-photon states called microcavity polaritons. The dynamics of the population of polaritons, which present an energy dispersion with an upper and lower branch, shows peculiar characteristics. In particular, a bottleneck region above the minimum of the lower polariton dispersion exists. In this region the population of polaritons is <b>accumulated,</b> and strong <b>deviations</b> from the thermal equilibrium are thus produced. Moreover, for the lower polariton states, a suppression of the scattering by acoustic phonons produces a strong inhibition of the thermal broadening. Finally, the dynamics of the photoluminescence spectra in semiconductor quantum wires is investigated. The time resolved photoluminescence experiments, the radiative recombination modifies adiabatically the total density, and the hypothesis quasi-thermal equilibrium can be applied. The optical spectra probes a system which ranges from a regime of ionized electron hole plasma to a gas of weakly interacting bound excitons. In order to describe such a complex system, I have used a Green's functions technique, and I have modeled the Coulomb interaction using a contact potential. This approach allows to observe in the absorption spectra a gain region close to the excitonic resonance. Moreover, the effects of the electron-hole correlations, calculated in a numerical self consistent way, explain the absence of the shift of the photoluminescence emission due to Coulomb induced band gap renormalization...|$|R

