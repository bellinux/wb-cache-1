4|2460|Public
40|$|The {{characteristics}} of automatic text retrieval systems are briefly described, and the available experimental evidence comparing manual with automatic retrieval is reviewed. Several automatic text analysis and indexing models are then examined, and a basic <b>automatic</b> <b>indexing</b> <b>process</b> is proposed. There {{is no evidence}} that an intellectual content analysis performed by human subject experts produces better retrieval results than comparable automatic text processing systems...|$|E
40|$|This paper {{explores the}} {{possible}} role of named entities in an <b>automatic</b> <b>indexing</b> <b>process,</b> based on text in subtitles. This {{is done by}} analyzing entity types, name density and name frequencies in subtitles and metadata records from different TV programs. The name density in metadata records {{is much higher than}} the name density in subtitles, and named entities with high frequencies in the subtitles {{are more likely to be}} mentioned in the metadata records. Personal names, geographical names and names of organizations where the most prominent entity types in both the news subtitles and news metadata, while persons, works and locations are the most prominent in culture programs...|$|E
40|$|This paper defines {{and studies}} {{the use of}} query by example (QBE) {{in the context of}} {{photograph}} retrieval. The novelty of our approach lies in considering an <b>automatic</b> <b>indexing</b> <b>process</b> of photographs as well as a representation of the features of image regions in a single knowledge representation formalism. Both symbolic and feature based representation are used during the query by example process. More precisely, the QBE process is able {{to take into account the}} symbolic descriptors of the images but also the extracted features from image under the form of histograms. The aim of this process is to detect the relative importance of the symbolic elements and of the feature elements according to the user's query by example. We experiment the query by example process on two collections of a total of 1100 photographs. The precision measures we have obtained are as good as a baseline defined as explicit textual queries processing...|$|E
50|$|<b>Automatic</b> <b>Indexing</b> is the <b>process</b> of {{analyzing}} an item {{to extract the}} information to be permanently kept in an index.|$|R
40|$|The terminological {{performance}} of the descriptors representing the Information Science domain in the SIBI/USP Controlled Vocabulary was evaluated in manual, <b>automatic</b> and semi-automatic <b>indexing</b> <b>processes.</b> It can be concluded that, {{in order to have}} a better performance (i. e., to adequately represent the content of the corpus), current Information Science descriptors of the SIBi/USP Controlled Vocabulary must be extended and put into context by means of terminological definitions so that information needs of users are fulfilled...|$|R
40|$|Co-word {{analysis}} {{offers the}} possibility of statiscally measuring the associative strength between pairs of keywords and, by using the values found, of mapping the dynamics of a scientific field in a given moment. The identification of clusters of keywords and {{the analysis of the}} strength of the links between pairs of keywords in the clusters show the way for important applications, ranging from the building up of special lexicons, to the development of logical tools for optimizing <b>automatic</b> <b>indexing</b> and retrieval <b>processes,</b> as well as the mapping of the evolution of interest on key topics in scientific research. An application of co-word analysis to identify the scope of the basic terminology related to indexing and retrieval is described...|$|R
40|$|Introduction: Discusses {{the use of}} noun phrases in the <b>automatic</b> <b>indexing</b> <b>process</b> of theses and {{dissertations}} {{deposited in}} the UFPE Digital Library of Theses and Dissertations (BDTD-UFPE), {{on the assumption that}} noun phrases consist of a better knowledge unit for indexing and information retrieval that individual words, allowing an adequate response to the users information need when searching for information. It presentes {{the state of the art}} of noun phrases and their automatic extraction process, as well as its applicability in automatic indexing and information retrieval. Method: Based on text analysis tool (OGMA), analyses the applicability of the extraction of noun phrases in automatic indexing and information retrieval of thesis and dissertations in the context of BDTD-UFPE. Applied to abstracts from Law, Computer and Nutrition thesis and dissertations, the variables could be observed, allowing the research team assess the extraction of noun phrases using: the percentage of accuracy of relevant noun phrases; the error rate extract strings that are not noun phrases, and; the percentage of non relevant noun phrases extracted. Results: The process of extracting noun phrases by OGMA showed different performances for each graduate program, with better performance (better accuracy rate) for abstracts from Law Thesis and Dissertations, followed by Computer and Nutrition ones. This performance difference can be partly explained by the different nature of technical terms presented in the abstracts. Conclusions: It concludes that although there are limitations in the available tools, the application of automated methods of extraction and indexing by noun phrases appears to be quite promising, since the noun phrases are configured as best descriptors and access to documents, eliminating the problems caused by synonymy and polysemy of isolated words...|$|E
40|$|AIR/X is a {{rule-based}} {{system for}} <b>automatic</b> <b>indexing</b> with a controlled vocabulary. The <b>indexing</b> <b>process</b> consists of several stages, with specific rule bases involved in each stage. Most of these rule bases are constructed automatically, especially {{the large number}} of term-descriptor rules. We describe the different stages and the overall architecture of the system. Then we present a specific application, the AIR/PHYS system developed for a large physics database. We illustrate the system by giving a detailed example and present experimental results for different system parameter settings. 1 Introduction The AIR/X system described in this paper performs an <b>automatic</b> <b>indexing</b> with index terms (called descriptors here) from a controlled vocabulary. The texts to be indexed are abstracts written in English. The <b>indexing</b> <b>process</b> consists of several stages, with specific rule bases involved in each stage. In order to cope with large subject fields, appropriate rule bases have to be developed [...] . ...|$|R
5000|$|<b>Automatic</b> <b>indexing</b> follows set <b>processes</b> of {{analyzing}} frequencies of word patterns and comparing results to other documents {{in order to}} assign to subject categories. This requires no understanding of the material being indexed. This therefore leads to more uniform indexing but this is {{at the expense of}} the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time consuming [...] An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document.|$|R
40|$|This paper {{presents}} {{the design and}} evaluation of a text categorization method based on the Hierarchical Mixture of Experts model. This model uses a divide and conquer principle to dene smaller categorization problems based on a predened hierarchical structure. The nal classier is a hierarchical array of neural networks. The method is evaluated using the UMLS Metathesaurus as the underlying hierarchical structure, and the OHSUMED test set of MEDLINE records. Comparisons with traditional Rocchio's algorithm adapted for text categorization, {{as well as at}} neural network classi- ers are provided. The results show that the use of the hierarchical structure improves text categorization performance signicantly. 1 Introduction Text categorization, also known as <b>automatic</b> <b>indexing,</b> is the <b>process</b> of algorithmically analyzing an electronic document to assign a set of categories (or index terms) that succinctly describe the content of the document. This assignment can be used for classic [...] ...|$|R
40|$|This {{panel will}} present an {{overview}} of the state-of-the-art in automatic and computer aided indexing systems, and focus on discussion of the factors that influence the success of the implementation of these system as well as the criteria for evaluation. We also will present several case studies of operational systems such as the Indexing Initiative at the National Library of Medicine, the American Water Works Association, the IEEE and ASIS&T Digital Library. Final recommendations and “best practices ” will be presented and discussed among panel members and the audience. Automatic text categorization, also known as <b>automatic</b> <b>indexing,</b> is the <b>process</b> of automatically analyzing an electronic document and assigning categories from a controlled vocabulary that summarize its contents. This area of research started has evolved since the 1960 ’s with Maron’s seminal work (Maron, 1961), through the 1970 ’s and 80 ’s development of rule based system...|$|R
40|$|A {{great many}} <b>automatic</b> <b>indexing</b> {{methods have been}} {{implemented}} and evaluated {{over the last few}} years, and automatic procedures comparable in effectiveness to conventional manual ones are now easy to generate. Two drawbacks of the available <b>automatic</b> <b>indexing</b> methods are the absence of reliable linguistic inputs during the <b>indexing</b> <b>process,</b> and the lack of formal, analytical proofs concerning the effectiveness of the proposed methods. The precision weighting procedure described in the present study uses relevance criteria to weight the terms occurring in user queries {{as a function of the}} balance between relevant and nonrelevant documents in which these terms occur; this approximates a semantic know-how of term importance. Formal mathematical proofs are given under well-defined conditions of the effectiveness of the method...|$|R
40|$|Purpose: {{comparing}} {{and examining}} {{the quality of}} the results of tagging, intellectual and automated <b>indexing</b> <b>processes.</b> Design/methodology/approach: analysis and graphical representation of annotation sets using the software "Semtinel". Findings: a combination of tagging, intellectual and <b>automatic</b> <b>indexing</b> is probably best suited to shape the annotation of literature more efficiently without compromising quality. Research limitations/implications: exploratory study on the base of three journals. Originality/value: the paper presents the open source software Semtinel offering a highly optimized toolbox for analysing thesauri and classifications...|$|R
40|$|This article revises {{the subject}} of {{computer}} assisted indexing systems by using library and documentation science concepts. We consider terminological aspects and we outline indexing base concepts and methodologies for <b>automatic</b> <b>indexing.</b> Theoretical aspects of the knowledge representation structures through taxonomies in these systems are considered. We also introduce areas and disciplines involved in computer assisted <b>indexing</b> <b>process.</b> Having described these matters, the article reflects upon {{the role of the}} structured information and unstructured information emphasizing their importance for the Semantic Web development. We then take a look and the present trends in our field...|$|R
40|$|Abstract Background: Indexing is {{a crucial}} step in any {{information}} retrieval system. In MEDLINE, a widely used database of the biomedical literature, the <b>indexing</b> <b>process</b> involves the selection of Medical Subject Headings in order to describe {{the subject matter of}} articles. The need for automatic tools to assist MEDLINE indexers in this task is growing with the increasing number of publications being added to MEDLINE. Methods: In this paper, we describe the use and the customization of Inductive Logic Programming (ILP) to infer indexing rules that may be used to produce <b>automatic</b> <b>indexing</b> recommendations for MEDLINE indexers. Results: Our results show that this original ILP-based approach outperforms manual rules when they exist. In addition, the use of ILP rules also improves the overall performance of the Medical Text Indexer (MTI), a system producing <b>automatic</b> <b>indexing</b> recommendations for MEDLINE. Conclusion: We expect the sets of ILP rules obtained in this experiment to be integrated into MTI. </p...|$|R
40|$|<b>Automatic</b> <b>indexing</b> of comic-page-image reposito- ries {{has evolved}} as an {{interesting}} research problem for graphics recognition research community. In this paper {{we present a}} sys- tem for automatically indexing the comic-page-images {{in order to achieve}} query by example (QBE) based focused content retrieval. In our system we represent the comic-page-images by attributed graphs and translate the problem of <b>automatic</b> <b>indexing</b> / QBE based focused content retrieval as a subgraph spotting problem. Our system uses an explicit graph embedding technique to embed the comic-page-image graphs into numeric feature vectors and then employs state-of-the-art machine learning tools for <b>automatic</b> <b>indexing</b> / QBE based focused content retrieval. Experimental results are presented for <b>automatic</b> <b>indexing</b> and QBE based focused content retrieval in a comic-page-image repository...|$|R
40|$|This paper {{reflects}} a thughful study {{about the use}} of natural language in <b>automatic</b> <b>indexing</b> systmes, from Data Bases and the Digital Management of Documents to full-text indexing of scientific electronic publications indexed in the Web. It offers a theoric approach to automatic intexing, pointing out the role that natural language (no controled) has demonstrated throughout its evolution. It also shows the last trends of <b>automatic</b> <b>indexing</b> founded in knowledge bases. The research finishes including the analysis of main features of new software for full-text <b>automatic</b> <b>indexing,</b> extrancting rigourous conclusions when comparing <b>automatic</b> <b>indexing</b> versus human indexing and planning new challenges for the future research in this field when the full-text electronic information is exponential and indexing by humans is really expensive...|$|R
40|$|Goal driven {{authoring}} {{of training}} material from existing technical manuals requires <b>automatic</b> <b>indexing</b> {{of the content}} of the manual. In this contribution we consider the different representation levels and document knowledge required to do the task. On that basis we have developed tools for <b>automatic</b> <b>indexing</b> in diverse domains...|$|R
40|$|Thesaurus-based <b>automatic</b> <b>indexing</b> and <b>automatic</b> {{authority}} control share common ground as word-matching processes. To demonstrate the resemblance, an experimental system utilizing <b>automatic</b> <b>indexing</b> as its core process was implemented to perform {{authority control}} on {{a collection of}} bibliographic records. Details of the system are given and results discussed. The benefits of exploiting the resemblance between the two systems are examine...|$|R
40|$|International audienceFaced {{with huge}} amounts of {{information}} to realize the accurate retrieval under the network environment, {{the first step is}} indexing words cannot appear ambiguity word. Because Chinese’s the basic unit is Chinese characters, Chinese characters form words, Word is divided into monosyllabic word and compound word, and there’s no space between Chinese keywords {{and there are a lot}} of ambiguous concept. Therefore a lot of ambiguity in the <b>indexing</b> <b>process</b> will be produced. The result detected information of irrelevant or mistakenly identified. The paper focuses on a method to eliminating the crossed meanings ambiguous words in the <b>automatic</b> <b>indexing.</b> The paper puts forward a method to eliminating ambiguous words combined algorithm of exhaustive method and disambiguation rules. Experiments show that it can avoid a great lot segmenting ambiguities with better segmenting results...|$|R
50|$|<b>Automatic</b> <b>indexing</b> is {{the ability}} for a {{computer}} to scan large volumes of documents against a controlled vocabulary, taxonomy, thesaurus or ontology and use those controlled terms to quickly and effectively index large document depositories. As the number of documents exponentially increases {{with the proliferation of}} the Internet, <b>automatic</b> <b>indexing</b> will become essential to maintaining the ability to find relevant information in a sea of ir-relevant information.|$|R
40|$|The {{performance}} of various indexing techniques, namely, manual <b>indexing,</b> <b>automatic</b> <b>indexing,</b> and combined manual and <b>automatic</b> <b>indexing</b> in retrieving Indonesian news articles is evaluated. The result of using structured and Boolean queries {{show that the}} combined indexing technique {{is more effective than}} the other techniques. The results demonstrate that the indexing techniques and query methods which work with English texts are also applicable to Indonesian texts. 1...|$|R
5000|$|A.R.D. Prasad. Prometheus: An <b>Automatic</b> <b>Indexing</b> System. 4th International Conference of the ISKO. 15-18 July 1996. Washington,DC.|$|R
40|$|Class number, {{descriptor}} and keyword {{are three}} kinds of subject concept identifiers, among which there exist some concept ual mapping relationships, i. e. compatibility. According to this principle, we construct a CLC Knowledge Base {{on the basis of}} Chinese Library Classification for <b>automatic</b> <b>indexing</b> and classification. We compare it with the CLC system to illuminate its obvious advantages over automatic information processing and concept searching. We then introduce some key technologies in the process of construction at length and describe in brief their application to <b>automatic</b> <b>indexing,</b> <b>automatic</b> classification and concept searching...|$|R
40|$|The THESEUS {{research}} programme {{was created to}} develop new semantic technologies that would achieve the pre-defined target vision These technologies allow the automatic evaluation, classification and linking of information found within different content types Instead of being limited to only finding content based on keywords or content fragments search engines are thereby enabled to independently determine the meaning of content, correlate it with other information, model classification systems and draw logical conclusions based on certain rules Researchers at the Fraunhofer HHI focused on innovative methods for the automatic extraction of metadata within THESEUS. Additional core technologies that have been researched and developed enable a faster processing of multimedia content. At Fraunhofer HHI innovative core technologies {{in the areas of}} image and video processing, metadata extraction as well as their international standardization were pursued. Key aspect for all of these was the optimization of <b>automatic</b> digitalization and <b>indexing</b> <b>processes</b> which require <b>automatic</b> quality assessment and on-demand solutions. In collaboration with partners within TEXO Fraunhofer IAO pursued the goals to describe application scenarios, to evaluate as wel as to develop and provide methods and tools for service innovation and engineering to support the development process. In addition, the Fraunhofer IAO coordinated the transverse group "Business Modells" with the intent to promote the awareness of the importance of business models based on the acquired results within the THESEUS {{research programme}}...|$|R
40|$|A {{variety of}} {{abstract}} <b>automatic</b> <b>indexing</b> {{models have been}} developed in recent times {{in an effort to}} produce indexing methods that are both effective and usable in practice. Among these are thee term discrimination model and the term precision system. These two indexing systems are briefly described and experimental evidence is cited showing that a combination of both theories produces better retrival performance than either one alone. Appropriate conclusions are reached concerning viable <b>automatic</b> <b>indexing</b> procedures usable in practice...|$|R
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaIn this article, {{we report}} {{research}} on an algorithmic approach to alleviating search uncertainty {{in a large}} information space. Grounded on object filtering, <b>automatic</b> <b>indexing,</b> and co-occurrence analysis, we performed a large-scale experiment using a parallel supercomputer (SGI Power Challenge) to analyze 400, 000 / abstracts in an INSPEC computer engineering collection. Two system-generated thesauri, one based on a combined object filtering and <b>automatic</b> <b>indexing</b> method, and the other based on <b>automatic</b> <b>indexing</b> only, were compared with the human-generated INSPEC subject thesaurus. Our user evaluation revealed that the system-generated thesauri were better than the INSPEC thesaurus in concept recall, but in concept precision the 3 thesauri were comparable. Our analysis also revealed that the terms suggested by the 3 thesauri were complementary and {{could be used to}} significantly increase â â varietyâ â in search terms and thereby reduce search uncertainty...|$|R
50|$|The series {{continues}} {{but mainly}} with add-on {{products for the}} VIEW word processor such as ViewIndex (an <b>automatic</b> <b>index</b> generator) and ViewSpell (spell-checker) as well as newer versions.|$|R
40|$|International audienceA {{three-dimensional}} Hough transform {{is designed}} {{for the detection of}} conic curves (hyperbolae and ellipses) formed by the gnomonic projection of diffraction Kossel cones. This new procedure is applied to a high-angular-accuracy analysis of electron backscatter diffraction (EBSD) patterns and to a fully <b>automatic</b> <b>indexing</b> of X-ray Kossel patterns in the SEM. The high-accuracy analysis of EBSD patterns allows for the determination of local elastic strains, without any reference pattern, and with a spatial resolution of a few tens of nanometres. An accuracy of 2 x 10 (- 4) is achieved on geometrically calculated diagrams. This paper presents also the first fully <b>automatic</b> <b>indexing</b> of Kossel patterns. This <b>automatic</b> <b>indexing</b> procedure can be applied to local texture analysis, as well as to local elastic strain measurements. Although the spatial resolution of Kossel is about 1 mu m, the accuracy of strain measurement is in this case much higher than that presently obtained on EBSD...|$|R
40|$|The LIVA {{research}} and development project (2005 - 2007) was conceived to integrate <b>automatic</b> <b>indexing,</b> <b>automatic</b> categorization, information visualization and information retrieval in library systems managing textual document collections. After {{a brief overview of}} some major information visualization methods, the user interface prototype is introduced. Sponsorship : Project funding: KK Stiftelsen</p...|$|R
5000|$|The FG {{requires}} <b>Automatic</b> <b>Indexing</b> (AI) Nikkor lenses for P and A modes to function. Lenses with AI capability include AI and AI-s type Nikkor {{lenses and}} Nikon Series E Lenses.|$|R
40|$|We {{introduce}} {{an approach}} to <b>automatic</b> <b>indexing</b> of e-prints based on a patternmatching technique making extensive use of an Associative Patterns Dictionary (APD), developed by us. Entries in the APD consist of natural language phrases with the same semantic interpretation {{as a set of}} keywords from a controlled vocabulary. The method also allows to recognize within e-prints formulae written in TEX notations that might also appear as keywords. We present an <b>automatic</b> <b>indexing</b> system, AUTEX, which we have applied to keyword index e-prints in selected areas in high energy physics (HEP) making use of the DESY-HEPI thesaurus as a controlled vocabulary. ...|$|R
40|$|International audienceSince natural {{language}} enter the computer retrieval system, {{due to the}} {{natural language}} retrieval is not restricted by professional experience, knowledge background, retrieval experience by users, and above reasons favored by the users. As {{the title of the}} Chinese literature is the concentrated reflection of Chinese literature content, it reflects the central idea of the literature. Retrieval methods of natural language described in this article is limited to literature title in subject indexing. The basic idea of this method is, with <b>automatic</b> <b>indexing</b> methods respectively the literature title in the database of retrieval system used in natural language retrieval for <b>automatic</b> word <b>indexing.</b> To control the concept of a given keyword, namely meaning transformation, form the final indexing words. Then, using the vector space model for the index data in the database will be “or” operation to retrieve, forming a document set B. For each document title in set B for <b>automatic</b> <b>indexing,</b> the title of each article for <b>automatic</b> <b>indexing,</b> indexing terms for the formation and retrieval of natural language indexing terms similarity calculation, sorted according to similarity of each document in set B. The first best match the requirements presented to the user documentation. This method is a simple and practical method of natural language retrieval...|$|R
40|$|The Web {{empowered}} {{the authors}} of grey literature to publish their work on their own. In case of selfpublished works their author is also their indexer. And because not many of the grey literature authors are professional indexers, this may result in poor or no indexing. Even though the Web made publishing easier, indexing is still hard. Nevertheless, {{we believe that the}} Web technologies and machine learning algorithms may help to reduce the cognitive overhead involved in indexing, and make it eventually as easy as publishing on the Web is. To help overcome the issue of quality and consistency of subject <b>indexing</b> <b>automatic</b> <b>indexing</b> systems can be used. Given enough full-texts already equipped with the terms from the controlled vocabulary that is to be used, machine learning algorithms can be employed. Our aim is to provide human-competitive <b>automatic</b> <b>indexing</b> to authors and producers of grey literature. We demonstrate how an <b>automatic</b> <b>indexing</b> system based on machine learning can be integrated into the document flow in an open source digital repository of grey literature. We build upon open source tools and a controlled subject heading vocabulary available in an open standard format. We will be using Maui Indexer as <b>automatic</b> <b>indexing</b> system, CDS Invenio as digital repository software, and Polythematic Structured Subject Heading System (PSH) as knowledge organisation system. Both Maui Indexer and CDS Invenio are open source, and CDS Invenio's modular architecture makes it possible to extend it with new functionality. Maui Indexer works with controlled vocabularies expressed in Simple Knowledge Organisation System format in which the PSH is available. From these components combined we will try to put together a solution for <b>automatic</b> <b>indexing</b> aimed at grey literature in the Czech language environment. Maui Indexer is domain and language independent so it is possible to adapt it for the field of Czech grey literature. The document samples we will test on will come from the National Repository of Grey Literature which is maintained by National Technical Library of Czech Republic. In the end, we will discuss integration of the <b>automatic</b> <b>indexing</b> component from the user perspective and sketch out how the user can interact with it through the user interface. Also we will provide details around the actual realization of the proposed system. The conclusion will deal with the evaluation of benefits of the implemented system for grey literature authors. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notes, Pratt student commentaryXAInternationa...|$|R
50|$|Paul Alexander Desmond DeMaine (October 11, 1924 - May 13, 1999) was {{a leading}} figure in the early {{development}} of computer based <b>automatic</b> <b>indexing</b> and information retrieval {{and one of the}} founders of academic computer science in the 1960s.|$|R
40|$|A {{self-organizing}} map (SOM) {{is used to}} classify software documents and the associated software components with the aim to facilitate software reuse. SOM learns from input stimuli rather than training data, therefore the quality of input data representation {{is crucial to the}} success of SOM. In this paper, we use <b>automatic</b> <b>indexing</b> method to represent a document collection as the input data to train a SOM. The <b>automatic</b> <b>indexing</b> uses a phrase formation method to promote precision and a domain dependent relational thesaurus to enhance recall. A retrieval experiment based on a document collection containing 97 Unix manual pages was conducted {{to evaluate the effectiveness of}} this input data representation scheme. Promising retrieval results were observed...|$|R
