391|10000|Public
25|$|Bader {{serves on}} the Steering Committees of the International Parallel and Distributed Processing Symposium (IPDPS) and HiPC conferences, and was the General co-Chair for IPDPS in 2004—2005, and Vice General Chair for HiPC in 2002—2004. David has {{previously}} chaired several conference program committees, was the Program Chair for HiPC 2005, and a Program Vice-Chair for IPDPS 2006. Bader was the General Chair of the 24th IPDPS, was held on April 19–23, 2010 in Atlanta, Georgia. He served as the Editor-in-Chief of the IEEE Transactions on Parallel and Distributed Systems (TPDS), from 2013-2017 and serves as an Associate Editor-in-Chief of the Journal of Parallel and Distributed Computing (JPDC). Bader has been {{an associate editor of}} several journals, including IEEE Transactions on Parallel and Distributed Systems, IEEE DSOnline, Parallel Computing, and the ACM Journal of Experimental <b>Algorithmics,</b> and has published over 210 articles in peer reviewed journals and conferences. From July 2003 to June 2007, Bader was also the chair of the IEEE Computer Society's Technical Committee on Parallel Processing.|$|E
2500|$|The result by Reed et al. was {{obtained}} using {{a completely new}} method, which later was called iterative compression and {{turned out to be}} an extremely useful algorithmic tool, especially in the field of fixed-parameter tractability. [...] This tool is now considered one of the fundamental tools for parameterized <b>algorithmics.</b>|$|E
2500|$|The edge bipartization {{problem is}} the {{algorithmic}} problem of deleting as few edges as possible to make a graph bipartite and {{is also an important}} problem in graph modification <b>algorithmics.</b> [...] This problem is also fixed-parameter tractable, and can be solved in time O(2k'm2), wherek is the number of edges to delete andm is the number of edges in the input graph.|$|E
40|$|In {{this paper}} we analyze methodological and {{philosophical}} implications of <b>algorithmic</b> aspects of unconventional computation. At first, we describe how the classical <b>algorithmic</b> universe developed and analyze why it became {{closed in the}} conventional approach to computation. Then we explain how new models of algorithms turned the classical closed <b>algorithmic</b> universe into the open world of <b>algorithmic</b> constellations, allowing higher flexibility and expressive power, supporting constructivism and creativity in mathematical modeling. As Gödel’s undecidability theorems demonstrate, the closed <b>algorithmic</b> universe restricts essential forms of mathematical cognition. In contrast, the open <b>algorithmic</b> universe, and even more the open world of <b>algorithmic</b> constellations, remove such restrictions and enable new, richer understanding of computation...|$|R
30|$|The {{sessions}} alternated between <b>algorithmic</b> {{and cognitive}} fusion (e.g., Day 2 : cognitive fusion, Day 3 : <b>algorithmic</b> fusion, Day 4 : cognitive fusion, Day 5 : <b>algorithmic</b> fusion).|$|R
5000|$|... #Subtitle level 2: Heat-bath <b>algorithmic</b> cooling (irreversible <b>algorithmic</b> cooling) ...|$|R
50|$|<b>Algorithmics</b> {{was voted}} {{as the leading}} {{enterprise}} risk firm for market risk, economic capital risk calculation, risk dashboards and collateral management in Risk magazine's 2010 Technology Rankings. <b>Algorithmics</b> was also selected as the best Risk Analytics Solution Provider in Waters magazine's annual financial technology rankings. In 2011, <b>Algorithmics</b> won the Life and Pension Risk award for Best Solvency II provider. In 2007, <b>Algorithmics</b> was selected as one of Canada's Top 100 Employers, as published in Maclean's magazine.|$|E
5000|$|Algorithm {{engineering}} {{does not}} intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental <b>algorithmics</b> (also called empirical <b>algorithmics).</b>|$|E
50|$|In October 2011, <b>Algorithmics</b> was {{acquired}} by IBM for $387 million. IBM OpenPages and <b>Algorithmics</b> will be combined {{to form a new}} Risk Analytics pillar within the Business Analytics software division.|$|E
40|$|AbstractThere is a {{dependency}} between computability of <b>algorithmic</b> {{complexity and}} decidability of different <b>algorithmic</b> problems. It {{is known that}} computability of the <b>algorithmic</b> complexity C(x) is equivalent to decidability of the halting problem for Turing machines. Here we extend this result {{to the realm of}} superrecursive algorithms, considering <b>algorithmic</b> complexity for inductive Turing machines. We study two types of <b>algorithmic</b> complexity: recursive (classical) and inductive <b>algorithmic</b> complexities. Relations between these types of <b>algorithmic</b> complexity and decidability of <b>algorithmic</b> problems for Turing machines and inductive Turing machines are considered. In particular, it is demonsrated that computability of <b>algorithmic</b> complexity is equivalent not only to decidability of the halting problem, but also to decidability by inductive Turing machines of the first order of many other problems for Turing machines, such as: if a Turing machine computes a recursive (total) function; if a Turing machine gives no result only for n inputs; if a Turing machine gives results only for n inputs...|$|R
5000|$|An <b>{{algorithm}}ic</b> paradigm, algorithm design paradigm, <b>algorithmic</b> technique, or <b>algorithmic</b> {{strategy is}} a generic method or approach which underlies {{the design of}} a class of algorithms. It is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program. [...] Examples of <b>algorithmic</b> paradigms include the greedy algorithm in optimization problems, dynamic programming, prune and search, and divide and conquer algorithms. More specialized <b>algorithmic</b> paradigms used in parameterized complexity include kernelization and iterative compression. In computational geometry, additional <b>algorithmic</b> paradigms include sweep line algorithms, rotating calipers, and randomized incremental construction.|$|R
5000|$|... 2013: <b>Algorithmic</b> Finance, on the {{potential}} for Twitter for <b>algorithmic</b> finance ...|$|R
50|$|Empirical <b>algorithmics</b> (sometimes {{also called}} {{experimental}} <b>algorithmics)</b> {{is the area}} within computer science that uses empirical methods to study the behaviour of algorithms. It {{can be used in}} the analysis of algorithms.|$|E
50|$|Research in {{empirical}} <b>algorithmics</b> {{is published}} in several journals, including theACM Journal on Experimental <b>Algorithmics</b> (JEA) and the Journal of Artificial Intelligence Research (JAIR), {{as well as}} at numerous conferences, including SEA, WEA, AAAI, IJCAI, CP and SLS.|$|E
50|$|Some ex-Whitechapel {{engineers}} {{went on to}} form <b>Algorithmics</b> Ltd., specialising in MIPS-based embedded systems. <b>Algorithmics</b> {{was later}} acquired by MIPS Technologies in 2002. The rights to the Hitech workstations were acquired by Mistral Computer Systems Ltd. in June 1988.|$|E
50|$|Distributed <b>algorithmic</b> {{mechanism}} design (DAMD) is {{an extension}} of <b>algorithmic</b> mechanism design.|$|R
50|$|In <b>algorithmic</b> {{information}} theory, sophistication is {{a measure}} of complexity related to <b>algorithmic</b> entropy.|$|R
5000|$|FIX now {{supports}} <b>algorithmic</b> trading {{by use of}} FIX <b>Algorithmic</b> Trading Definition Language FIXatdl.|$|R
50|$|For example, <b>Algorithmics</b> Incorporated, {{a company}} {{acquired}} by IBM in 2011, provides risk software solutions. Employees in <b>Algorithmics</b> specialize in software development, mathematics, financial engineering and risk analytics. <b>Algorithmics</b> expanded {{the breath of}} advice beyond its nine-member {{board of directors and}} built an advisory board that focused on the benefit of customer and potential customer input on product and market direction. Advisory board members include executives charged with enterprise risk management responsibility, who would compete against each other, but share an interest in providing suitable risk management underpinnings to their business. This common interest help ensure that the advisory board would be committed and pleased to meet with the CEO of <b>Algorithmics.</b>|$|E
5000|$|Research areas: {{models of}} {{computation}} and complexity (automata and [...] formal language theory and applications, natural computing, bioinformatics, riceInformatics, formal models for e-voting), <b>Algorithmics,</b> Designs and Implementations (visualization and implementations, <b>algorithmics</b> for hard problems, algorithmic game theory, scheduling problem), combinatorial networks, information technology in education.|$|E
5000|$|<b>Algorithmics</b> Inc. - {{competitor}} offering {{risk management}} solutions ...|$|E
40|$|<b>Algorithmic</b> trading has sharply {{increased}} over the past decade. Does it improve market quality, and should it be encouraged? We provide the first analysis of this question. The NYSE automated quote dissemination in 2003, andweuse this change in market structure that increases <b>algorithmic</b> trading as an exogenous instrument to measure the causal effect of <b>algorithmic</b> trading on liquidity. For large stocks in particular, <b>algorithmic</b> trading narrowsspreads,reducesadverse selection, and reduces trade-related price discovery. The findings indicate that <b>algorithmic</b> trading improves liquidity and enhances the informativeness of quotes...|$|R
50|$|<b>Algorithmic</b> {{learning}} theory is a mathematical framework for analyzing machine learning problems and algorithms. Synonyms include formal {{learning theory}} and <b>algorithmic</b> inductive inference. <b>Algorithmic</b> learning theory {{is different from}} statistical learning theory in {{that it does not}} make use of statistical assumptions and analysis. Both <b>algorithmic</b> and statistical learning theory are concerned with machine learning and can thus be viewed as branches of computational learning theory.|$|R
40|$|<b>Algorithmic</b> Chemistries are Artificial Chemistries {{that aim}} at algorithms. In this {{contribution}} {{we present a}} new algorithm to execute <b>Algorithmic</b> Chemistries during evolution. This algorithm ensures synthesizes of the whole program and cuts off execution of unneeded instructions without restricting the stochastic way of execution. We demonstrate benefits of the new algorithm for evolution of <b>Algorithmic</b> Chemistries and discuss the relation of <b>Algorithmic</b> Chemistries with Estimatio...|$|R
5000|$|Bioinformatics, and Empirical & Theoretical <b>Algorithmics</b> Lab (ß-Lab) ...|$|E
5000|$|... #Article: Bioinformatics, and Empirical & Theoretical <b>Algorithmics</b> Lab ...|$|E
5000|$|Catherine Cole McGeoch is an American {{computer}} scientist specializing in empirical <b>algorithmics</b> and heuristics for NP-hard problems. [...] She is currently Beitzel Professor in Technology and Society at Amherst College. She {{has been the}} Editor in Chief of ACM Journal of Experimental <b>Algorithmics</b> and is currently {{a member of the}} ACM Publications Board.|$|E
40|$|Abstract. There are {{different}} aspects and spheres of unconventional computations. In this paper, we analyze philosophical and methodological implications of <b>algorithmic</b> issues of unconventional computations. At first, we describe how the <b>algorithmic</b> universe {{was developed and}} analyze why it became closed in the conventional approach to computation. Then we explain how the new models of algorithms changed the <b>algorithmic</b> universe, making it open and allowing higher flexibility and superior creativity. As Gödel undecidability theorems imply, the closed <b>algorithmic</b> universe restricts essential forms of human cognition, while the open <b>algorithmic</b> universe eliminates such restrictions. ...|$|R
5000|$|Design: design {{games that}} have both good game-theoretical and <b>algorithmic</b> properties. This area is called <b>algorithmic</b> {{mechanism}} design ...|$|R
40|$|In {{this paper}} we study new <b>algorithmic</b> {{structures}} with Douglas- Rachford (DR) operators to solve convex feasibility problems. We propose to embed the basic two-set-DR <b>algorithmic</b> operator into the String-Averaging Projections (SAP) {{and into the}} Block-Iterative Pro- jection (BIP) <b>algorithmic</b> structures, thereby creating new DR algo- rithmic schemes that include the recently proposed cyclic Douglas- Rachford algorithm and the averaged DR algorithm as special cases. We further propose and investigate a new multiple-set-DR <b>algorithmic</b> operator. Convergence of all these <b>algorithmic</b> schemes is studied by using properties of strongly quasi-nonexpansive operators and firmly nonexpansive operators. Comment: SIAM Journal on Optimization, accepted for publicatio...|$|R
5000|$|LINMA 1691 : Discrete {{mathematics}} I : Graphs <b>algorithmics</b> {{and theory}} ...|$|E
5000|$|The Department Computational Biology and Applied <b>Algorithmics</b> {{is headed}} by Thomas Lengauer, ...|$|E
50|$|<b>Algorithmics</b> was a Toronto, Ontario based company {{founded by}} Ron Dembo that {{provided}} risk management software to financial institutions. Founded in 1989, <b>Algorithmics</b> employed over 850 people in 23 global offices, and served more than 350 clients, including 25 of the 30 largest {{banks in the}} world, and over {{two thirds of the}} CRO Forum of leading insurers.|$|E
40|$|The {{digital and}} {{information}} technology revolutions are based on <b>algorithmic</b> mathematics in many of their alternative forms. <b>Algorithmic</b> mathematics per se is not necessarily underpinned by the digital or the discrete only; analogue traditions of <b>algorithmic</b> mathematics have a noble pedigree, even in economics. Constructive mathematics of any variety, computability theory and non-standard analysis are intrinsically <b>algorithmic</b> at their foundations. Economic theory, game theory and mathematical finance theory, at many of their frontiers, appear to have embraced the digital {{and information technology}} revolutions via strong adherences to experimental, behavioural and so-called computational aspects of their domains - without, however, adapting the mathematical formalisms of their theoretical structures. Recent advances in mathematical economics, game theory, probability theory and statistics suggest that an <b>algorithmic</b> revolution {{in the social sciences}} is in the making. In this paper I try to trace the origins of the emergence of this revolution and suggest, via examples in mathematical economics, game theory and the foundations of statistics, where the common elements are and how they may define new frontiers of research and visions. Essentially, the conclusion is that the <b>algorithmic</b> social sciences are unified by an underpinning in Diophantine Decision Problems as their paradigmatic frameworkAlgorithmic Economics, <b>Algorithmic</b> Game Theory, <b>Algorithmic</b> Statistics, <b>Algorithmic</b> Social Science...|$|R
40|$|<b>Algorithmic</b> debuggers for higher-order {{functional}} languages have {{to display}} functional values. Originally functional values had been represented as partial applications of function and constructor symbols, {{but a recent}} approach represents functional values as finite maps. The two representations require the computation tree that is central to <b>algorithmic</b> debugging to be structured rather differently. In this paper we present a unifying framework that formally defines <b>algorithmic</b> debugging for both representations in an implementation-independent way. On this basis we prove the soundness of <b>algorithmic</b> debugging with finite maps. Our framework shows how a single implementation can support both forms of <b>algorithmic</b> debugging. The proof exposed that <b>algorithmic</b> debugging with finite maps does not handle arbitrary functional programs, but in current practice the problematic ones are excluded by Haskell’s type system. Both framework and proof suggest variations of <b>algorithmic</b> debugging with finite maps and thus are tools for further improvement of this form of debugging...|$|R
40|$|Algorithms {{currently}} have direct implications in our democracies and societies, {{but they also}} define mostly all our daily activities as users, defining our decisions and promoting different behaviors. In this context, {{it is necessary to}} define and think about how to design the different implications that these algorithms have from a user centered perspective, particularly in social media platforms that have such relevance in our information sources and flow. Therefore, the current thesis provides an introduction to the concept of <b>algorithmic</b> experience, trying to study how to implement it for social media services in cellphone devices. Using a Research through Design methodology supported by interface analysis, document analysis and user design workshops, the present paper provides results grouped in five different areas: <b>algorithmic</b> profiling transparency, <b>algorithmic</b> profiling management, <b>algorithmic</b> awareness, <b>algorithmic</b> user-control and selective <b>algorithmic</b> remembering. These five areas provide a framework capable of promoting requirements and guide the evaluation of <b>algorithmic</b> experience in social media contexts...|$|R
