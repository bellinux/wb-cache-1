8|53|Public
5000|$|Cassandra's superiority {{in combat}} results {{not just from}} her {{excellent}} physical condition, but from her cognitive functions (the result of her idiosyncratic upbringing) that enables extraordinary feats of coordination as well as perceiving minute changes in an opponent's movements and body language. In Batgirl #14 (May 2001), the writer, Kelley Puckett, places Cassandra in a position within the story in which her skills are analyzed {{by a group of}} government experts. The creative team reveal to the reader that the character has no metagene. Yet her genetic status was felt to be incompatible with her recorded abilities. One expert states, [...] "Her individual moves are borderline human. It's her <b>aggregate</b> <b>speed</b> that's metahuman. Look—humans can throw a 100 miles-per-hour fastball, smash concrete blocks with their heads, and run 4.2 forties. What they can't do is all of that at once. It's not so much physical as... as mentally impossible. Too much to coordinate." ...|$|E
40|$|We {{present a}} neural network {{approach}} for tomographic imaging problem using interpolation methods and fan-beam projections. This approach uses a partially connected neural network especially assembled for solving tomographic reconstruction with {{no need of}} training. We extended the calculations to perform reconstruction with interpolation and to allow tomography of fan-beam geometry. The main goal is to <b>aggregate</b> <b>speed</b> while maintaining or {{improving the quality of}} the tomographic reconstruction process...|$|E
40|$|This paper {{presents}} a simple formula that relates the tail {{index of the}} firm size distribution to the <b>aggregate</b> <b>speed</b> with which an economy converges to its balanced growth path. The {{fact that there are}} so many firms in the right tail implies that aggregate shocks that permanently destroy employment among incumbent firms, rather than cause these firms to scale back temporarily, are followed by slow recoveries. This is true despite the presence of many rapidly growing firms. Aggregate convergence rates are non-linear: they can be very high for economies far below the balanced growth path and very low for advanced economies. JEL classification: E 1, L...|$|E
50|$|Bluefire is {{the second}} phase of a system called the Integrated Computing Environment for Scientific Simulation (ICESS) at NCAR. The first phase was called Blueice. After {{undergoing}} acceptance testing, Bluefire began full operations in August 2008 and replaced three supercomputers with an <b>aggregate</b> peak <b>speed</b> of 20 teraflop.|$|R
5000|$|In 1965 he won 23 times, {{including}} the Manx International, three laps of the Snaefell mountain {{course on the}} Isle of Man. Living once more in Leeds, he cycled from there to Liverpool, slept in a telephone box and then caught the ferry to the island.In 1966 he won the national amateur road race championship and, as an afterthought, the BBAR with a record average speed of 24.797 mph. The BBAR <b>aggregated</b> <b>speeds</b> of riders over 50 miles, 100 miles and 12 hours and specialist time-triallists usually devote their whole season to it. Metcalfe won in three straight rides, almost as an afterthought. [...] "I remember thinking I needed a change. I'd ridden a few time trials {{in the past and}} so I thought I'd have another go," [...] he said.|$|R
40|$|The paper {{introduces}} an algorithm that disaggregate speed {{data collected}} with automatic road detectors {{which can only}} measure speed frequency in intervals only. The objective is to obtain back-calculated “artificial” individual speeds to operate with continuous distribution functions rather than discrete ones. This allows the derivation of more robust, basic descriptive measures (average, variance, and percentiles) according to Normal, LogNormal, and Gamma probability distribution functions. The information produced {{in this way is}} more useful than that calculated from standard <b>aggregated</b> <b>speed</b> reports and can be used for further processing purposes. In this investigation, individual speed data collected from video cameras were used to derive reference distributions and descriptive measures on the same road sections where inductive double-loop detectors were installed. The comparisons between the back-calculated individual speeds and those collected from video cameras support the validity of the proposed algorith...|$|R
40|$|With {{a goal of}} {{supporting}} the timely and cost-effective analysis of Terabyte datasets on commodity components, we present and evaluate StoreTorrent, a simple distributed filesystem with integrated fault tolerance for efficient handling of small data records. Our contributions include an application-OS pipelining technique and metadata structure to increase small write and read performance {{by a factor of}} 1 - 10, and the use of peer-to-peer communication of replica-location indexes to avoid transferring data during parallel analysis even in a degraded state. We evaluated StoreTorrent, PVFS, and Gluster filesystems using 70 storage nodes and 560 parallel clients on an 8 -core/node Ethernet cluster with directly attached SATA disks. StoreTorrent performed parallel small writes at an aggregate rate of 1. 69 GB/s, and supported reads over the network at 8. 47 GB/s. We ported a parallel analysis task and demonstrate it achieved parallel reads at the full <b>aggregate</b> <b>speed</b> of the storage node local filesystems. Comment: 13 pages, 7 figure...|$|E
40|$|We {{carry out}} a {{performance}} study using the Cray T 3 D parallel supercomputer to illustrate some important features of this machine. Timing experiments show the speed of various basic operations while more complicated operations give some measure of its parallel performance. 1 Introduction Recently, high-performance computers have become an important tool for obtaining the solution of complex scientific problems. In spite of the enormous advances in performance of machines and method, they fall short of providing computational solutions to many important applications. To successfully solve these problems, one needs an increase in computational power of several orders of magnitude. Since {{the speed of the}} fastest processor already approaches the limits set by the laws of physics, such an increase will only be feasible through the integration of hundreds or thousands of powerful processors into a massively parallel computer. In principle, there is no limit to the <b>aggregate</b> <b>speed</b> of paral [...] ...|$|E
40|$|Highway {{and traffic}} {{engineers}} collect vehicular speed data with detectors {{based on a}} variety of fixed and mobile device technologies, to support analysis and design activities. Most acquisition units <b>aggregate</b> <b>speed</b> data into speed classes for ease of management and storage. An unfortunate result of this practice is a significant loss of content associated with individual speed data. Moreover, the use of individual speeds is often necessary to support road safety analysis and speed management decisions. For bridging of this gap, this paper introduces an algorithm that disaggregates speed data collected with automatic road detectors that can measure speed frequency only in intervals. The objective is to obtain backcalculated individual speeds that operate with continuous distribution functions rather than discrete ones. This information allows the derivation of more robust, basic descriptive measures (average, variance, and percentiles) according to normal, lognormal, and gamma probability distribution functions. Therefore, the information produced is more useful than that calculated from standard aggregated speed reports. In this investigation, individual speed data collected from video cameras were used to derive reference distributions and descriptive measures on the same road sections where inductive double-loop detectors were installed. Comparisons of the backcalculated individual speeds and those collected from video cameras support the validity of the proposed algorithm...|$|E
40|$|Abstract. This paper {{presents}} a statistical approach to <b>aggregating</b> <b>speed</b> and phase (directional) information for vascular segmentation in phase contrast magnetic resonance angiograms (PC-MRA), and proposes a Maxwell-Gaussian finite mixture distribution {{to model the}} background noise distribution. In this paper, we extend our previous work [6] to the segmentation of phase-difference PC-MRA speed images. We demonstrate that, {{rather than relying on}} speed information alone, as done by others [12, 14, 15], including phase information as a priori knowledge in a Markov random field (MRF) model can improve the quality of segmentation, especially the region within an aneurysm where there is a heterogeneous intensity pattern and significant vascular signal loss. Mixture model parameters are estimated by the Expectation-Maximization (EM) algorithm [3]. In addition, it is shown that a Maxwell-Gaussian finite mixture distribution models the background noise more accurately than a Maxwell distribution and exhibits a better fit to clinical data...|$|R
40|$|Abstract—In the future, {{buildings}} will be lit using {{solid state}} lighting, particularly white {{light emitting diodes}} (LEDs). Solid state lighting is a rapidly expanding area of research due to the reliability of LEDs and the numerous applications possible including Visible Light Communications (VLC), where a white LED is used to illuminate {{a room at the}} same time as providing a data communications system. VLC systems are restricted by limited modulation bandwidths of typically only a few MHz. This can be overcome however, since multiple arrays of LEDs are manipulated, allowing a multiple-input multiple-output (MIMO) technique which simultaneously illuminates the room uniformly and provides parallel data transfer. In this paper, a non-imaging MIMO technique is employed, with results showing that the system does not perform consistently at all receiver positions, owing to symmetry. The simulations performed show that <b>aggregate</b> <b>speeds</b> up to 12 Mbps can be achieved using this transmitter distribution. Keywords-LED; Optical wireless communications; Visible light communications;Optical MIMO; Non-Imgaing. I...|$|R
40|$|Evaluating and {{estimating}} vehicle {{energy consumption}} {{has always been}} an important issue while determining CO 2 emissions. Several studies have focused on the energy consumption of various types of vehicles (thermal, hybrid or electric) with the aim of reducing consumption. Eco-routing is our first concern and our work aims to develop energy-efficient routing tools to promote the use of electric vehicles (EV). It should be noted that several factors make challenging the development of this type of e-mobility. The most significant are autonomy, the lack of charging stations, battery recharge time and recuperation capability (e. g. braking phases or downhill’s). The main objective {{of this paper is to}} present a preliminary study for the electric vehicle route planning. We determine a consumption cost function based on a dynamic consumption model with instantaneous speed data. The main contribution is an evaluation of the transition impact from this data type to <b>aggregated</b> <b>speed</b> data derived from travel time information...|$|R
40|$|In {{analogy to}} {{the flow of}} fluids, it is {{expected}} that the aggregate density and the velocity of vehicles in a section of a freeway adequately describe the traffic flow dynamics. The conservation of mass equation together with the aggregation of the vehicle following dynamics of controlled vehicles describes the evolution of the traffic density and the <b>aggregate</b> <b>speed</b> of a traffic flow. There are two kinds of stability associated with traffic flow problems - string stability (or car-following stability) and traffic flow stability. We make a clear distinction between traffic flow stability and string stability, and such a dis- tinction has not been recognized in the literature, thus far. String stability is stability with respect to intervehicular spacing; intuitively, it ensures the knowledge of the position and velocity of every vehicle in the traffic, within reasonable bounds of error, from the knowledge of the position and velocity of a vehicle in the traffic. String stability is analyzed without adding vehicles to or removing vehicles from the traffic. On the other hand, traffic flow stability deals with the evolution of traffic velocity and density in response to the ad- dition and/or removal of vehicles from the flow. Traffic flow stability can be guaranteed only if the velocity and density solutions of the coupled set of equa- tions is stable, i. e., only if stability with respect to automatic vehicle following and stability with respect to density evolution is guaranteed. Therefore, the ow stability and critical capacity of any section of a highway is dependent not only on the vehicle following control laws and the information used in their synthesis, but also on the spacing policy employed by the control system. Such a dependence has practical consequences in the choice of a spacing policy for adaptive cruise control laws and on the stability of the traffic ow consisting of vehicles equipped with adaptive cruise control features on the existing and future highways. This critical dependence is the subject of investigation in this paper. This problem is analyzed in two steps: The first step is to understand the effect of spacing policy employed by the Intelligent Cruise Control (ICC) systems on traffic flow stability. The second step is to understand how the dynamics of ICC system affects traffic flow stability. Using such an analysis, it is shown that cruise control systems that employ a constant time headway policy lead to unacceptable characteristics for the traffic flows. Key Words: Intelligent Cruise Control Systems, Traffic Flow Stability, String Stability, Advanced Vehicle Control Systems, Advanced Traffic Management Systems. Automobiles [...] Automatic control [...] Mathematical models, Traffic flow [...] Mathematical models, Automobiles [...] Speed [...] Mathematical models, automated highways, Advanced vehicle control systems...|$|E
40|$|Daily average wind speeds are {{dynamically}} modelled by a continuous-time {{autoregressive model}} with seasonal mean and volatility. Futures prices {{based on an}} index of <b>aggregated</b> wind <b>speeds</b> are derived, and it is shown that the Samuelson effect breaks down. The volatility of these futures will decrease when approaching maturity, an effect which is explained by the memory in higher-order autoregressive models. Wind power Weather derivatives Wind futures Hedging Continuous-time autoregressive process Seasonality Samuelson effect...|$|R
40|$|Recent {{growth in}} demand for {{proactive}} real-time transportation management systems has led to major advances in short-time traffic forecasting methods. Recent studies have introduced time series theory, neural networks, and genetic algorithms to short-term traffic forecasting to make forecasts more reliable, efficient, and accurate. However, most of these methods can only deal with data recorded at regular time intervals, which restricts the range of data collection tools to presence-type detectors or other equipment that generates regular data. The study reported here {{is an attempt to}} extend several existing time series forecasting methods to accommodate data recorded at irregular time intervals, which would allow transportation management systems to obtain predicted traffic speeds from intermittent data sources such as Global Positioning System (GPS). To improve forecasting performance, acceleration information was introduced, and information from segments adjacent to the current forecasting segment was adopted. The study tested several methods using GPS data from 480 Hong Kong taxis. The results show that the best performance in terms of mean absolute relative error is obtained by using a neural network model that <b>aggregates</b> <b>speed</b> information and acceleration information from the current forecasting segment and adjacent segments. published_or_final_versio...|$|R
50|$|In {{computer}} networking, port trunking {{is the use}} {{of multiple}} concurrent network connections to <b>aggregate</b> the link <b>speed</b> of each participating port and cable, also called link aggregation. Such high-bandwidth link groups may be used to interconnect switches or to connect high-performance servers to a network.|$|R
40|$|This paper {{presents}} a statistical approach to <b>aggregating</b> <b>speed</b> and phase (directional) information for vascular segmentation of phase contrast magnetic resonance angiograms (PC-MRA). Rather {{than relying on}} speed information alone, as done by others and in our own work, we demonstrate that including phase information as a priori knowledge in a Markov random field (MRF) model can {{improve the quality of}} segmentation. This is particularly true in the region within an aneurysm where there is a heterogeneous intensity pattern and significant vascular signal loss. We propose to use a Maxwell-Gaussian mixture density to model the background signal distribution and combine this with a uniform distribution for modelling vascular signal to give a Maxwell-Gaussian-uniform (MGU) mixture model of image intensity. The MGU model parameters are estimated by the modified expectation-maximisation (EM) algorithm. In addition, it is shown that the Maxwell-Gaussian mixture distribution (a) models the background signal more accurately than a Maxwell distribution, (b) exhibits a better fit to clinical data and (c) gives fewer false positive voxels (misclassified vessel voxels) in segmentation. The new segmentation algorithm is tested on an aneurysm phantom data set and two clinical data sets. The experimental results show that the proposed method can provide a better quality of segmentation when both speed and phase information are utilised. © 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|This paper {{describes}} the finished goods inventory behavior {{of more than}} 700 U. S. manufacturing firms between 1985 - 93 using a new Census Bureau longitudinal data base. Three key results emerge. First, there is a broad mix of production-smoothing and production-bunching firms, with about two-fifths smoothing production. Second, firm-level inventory adjustment speeds are about {{an order of magnitude}} larger than <b>aggregate</b> adjustment <b>speeds</b> due to econometric aggregation bias. Finally, accounting for time variation in the inventory adjustment speed due to fluctuations in firm size improves the fit of a traditional aggregate inventory model by one-fifth. Inventories...|$|R
50|$|The 2703 {{supported}} up to 176 half-duplex start-stop or Binary Synchronous communication lines. The {{maximum speed}} of one line was 2400 bit/s but the total <b>aggregate</b> line <b>speed</b> was limited. By 1970 the {{maximum line speed}} had been raised to 4800 bit/s. The 2703 attached to a single multiplexer channel; each communication line occupied a subchannel. It had a four or eight byte buffer per line to reduce data transfer {{to and from the}} host computer. The IBM 2712 Remote Multiplexer allowed up to fourteen slow speed devices to be multiplexed over one high speed line to a 2703.|$|R
40|$|Quantitative image {{analysis}} {{is a form}} of imaging that includes microscopic histological quantification, video microscopy, {{image analysis}}, and image processing. Hallmarks are the generation of reliable, reproducible, and efficient measurements via strict calibration and step-by-step control of the acquisition, storage and evaluation of images with dedicated hardware and software. Major advantages of quantitative image analysis over traditional techniques include sophisticated calibration systems, interaction, speed, and control of inter- and intraobserver variation. This results in a well controlled environment, which is essential for quality control and reproducibility, and helps to optimize sensitivity and specificity. To achieve this, an optimal quantitative image analysis system combines solid software engineering with easy interactivity with the operator. Moreover, the system also needs to be as transparent as possible in generating the data because a "black box design" will deliver uncontrollable results. In addition to these more general aspects, specifically for the analysis of synovial tissue the necessity of interactivity is highlighted by the added value of identification and quantification of information as present in areas such as the intimal lining layer, blood vessels, and lymphocyte <b>aggregates.</b> <b>Speed</b> is another important aspect of digital cytometry. Currently, rapidly increasing numbers of samples, together with accumulation of a variety of markers and detection techniques has made the use of traditional analysis techniques such as manual quantification and semi-quantitative analysis unpractical. It can be anticipated that the development of even more powerful computer systems with sophisticated software will further facilitate reliable analysis at high spee...|$|R
40|$|In this paper, {{we propose}} deep {{learning}} architectures (FNN, CNN and LSTM) to forecast a regression model for time dependent data. These algorithm's {{are designed to}} handle Floating Car Data (FCD) historic speeds to predict road traffic data. For this we <b>aggregate</b> the <b>speeds</b> into the network inputs in an innovative way. We compare the RMSE thus obtained {{with the results of}} a simpler physical model, and show that the latter achieves better RMSE accuracy. We also propose a new indicator, which evaluates the algorithms improvement when compared to a benchmark prediction. We conclude by questioning the interest of using deep learning methods for this specific regression task...|$|R
40|$|This {{dissertation}} {{explores the}} relationships between measures of crash occurrence, the crash rate and the crash density, and various parameters of speed distributions as measured utilizing automatic traffic recorders (ATR) on highways in Iowa, with {{special attention to the}} implications of the findings with regard to highway safety policies such as speed limits and their enforcement. The goal of the research was to determine if crash risk is more related to absolute speed or to some measure of variation of the speed distribution. Data on crashes were obtained from the Iowa DOT crash data base. Roadway segments were selected utilizing criteria to avoid problems of over-long sections as were encountered by Solomon in his 1964 report. <b>Aggregated</b> <b>speed</b> metrics were calculated from raw ATR data provided by the Iowa DOT. Visual Basic programs were developed to calculate the basic speed metrics. Standard statistical tests were used to compare the speed distributions as well as their mean and variance. Logistic regression models were developed to explore the relationship between the dependent variable crash probability and the explanatory variables variance, road type, number of lanes, time of day, and day of week. The evaluation included considering two cases, one with all crashes in the segment and the other with weather-related crashes removed. The hypothesis, that {{one or more of the}} speed metrics could be used to determine crash risk, was not supported by the results of the analyses. Recommendations for further research include utilization of new technology, such as (in-vehicle) event data recorders and passive speed measuring devices, to collect non-aggregated speed data...|$|R
40|$|The {{introduction}} of mobile sensors, i. e. probe vehicles with GPS-enabled smart phones or connected vehicle technology, will potentially provide more comprehensive information on roadway conditions than conventional point detection alone. Several mobility applications {{have been proposed}} that utilize this new vehicle-specific data rather than <b>aggregated</b> <b>speed,</b> density, and flow. Because of bandwidth limitations of cellular and an expected slow deployment of connected vehicles, {{only a portion of}} vehicles on the roadway will be able to report their positions at any given time. This paper proposes a novel technique to analyze the behavior of freeway vehicles equipped with GPS receivers and accelerometers to estimate the quantity, locations, and speeds of those vehicles that do not have similar equipment. If an equipped vehicle deviates significantly from a car-following model’s expected behavior, the deviation is assumed {{to be the result of}} an interaction with an unequipped vehicle (i. e. an undetectable “ghost ” vehicle). This unequipped vehicle is then inserted into a rolling estimation of individual vehicle movements. Because this technique is dependent on vehicles interacting during congestion, a second scenario uses an upstream detector to detect and insert unequipped vehicles at the point of detection, essentially “seeding ” the network. An evaluation using the NGSIM US- 101 dataset shows realistic vehicle density estimations during and immediately after congestion. Introducing an upstream detector to supply initial locations of unequipped vehicles improves accuracy in free flow conditions, thereby improving the root mean squared error of the number of vehicles within a 120 -foot cell from 3. 8 vehicles without a detector, to 2. 4 vehicles with a detector, as compared to ground truth...|$|R
40|$|AbstractThe aim of {{this paper}} is to {{investigate}} MFD estimation methods from traffic states observations. To eliminate all the experimental bias, traffic situations will be determined from a simulation tool that encompasses an equilibrium model (the LWR model). Three methods will be compared: (i) the analytical method that provides the upper bound of the MFD, (ii) the “production” method that requires vehicles trajectories and (iii) the “loop detector” method that <b>aggregates</b> flow, <b>speed</b> and occupancy observations. This latter method will be thoroughly examined to quantify the impact of loop detector positions (spatial distribution, distance from traffic signals …) and of heterogeneities in traffic states. Recommendations will finally be drawn to improve the global understanding on MFD and to better define methods to calibrate them...|$|R
40|$|International audienceA {{complete}} {{model of}} the spatiotemporal behavior of multimode vertical-cavity surface-emitting-lasers (VCSEL), through static and dynamic response, noise, thermal effects and its coupling to multi mode fibers (MMF) has been investigated in low cost 10 Gigabit Ethernet (10 GbE) prototype optoelectronic packages using GaAs VCSEL arrays coupled to MMF ribbons for short distance optical links. Eye diagram simulations compared to measurements allow the analysis of critical parameters, such as launching conditions {{in order to maximize}} the MMF bandwidth as well as VCSEL static and dynamic conditions. A new BER estimation method is proposed by taking into account not only the Q factor but random and deterministic jitter and digital pattern effects. Global system optimization is discussed for BER less than 10 - 12 for an <b>aggregate</b> link <b>speed</b> of 120 Gb/s over 300 m of a 12 fiber MMF ribbon...|$|R
40|$|Road traffic {{increases}} {{constantly and}} the negative consequences {{in the form of}} traffic jams can be realized especially in urban areas. In order to provide real time traffic information to road users and traffic managers, accurate computer models gain relevance. A software called Mobile Millennium Stockholm (MMS) was developed to estimate and predict travel times and has been implemented on a 7 km test stretch in the north of Stockholm. The core of the software is the cell transmission model (CTM) which is a macroscopic traffic flow model based on <b>aggregated</b> <b>speed</b> observations. This thesis focuses on different calibration techniques of the so called fundamental diagram as an important input factor to the CTM. The diagrams illustrate the mathematical function which defines the relation between traffic flow, density and speed. The calibration is performed in different scenarios based on the least square (LS) and total least square (TLS) error minimization. Furthermore, sources, representing the traffic demand, and sinks, representing the surrounding of the modeled network, are implemented as dynamic parameters to model the change in traffic behavior throughout the day. Split ratios, as a representation of the drivers 9 ̆ 1 route choice in the CTM are estimated and implemented as well. For the framework of this work, the MMS software is run in a pure prediction mode. The CTM is based on the source, sink, split and fundamental diagram parameters only and run forward in time. For each fundamental diagram calibration scenario an independent model run is performed. The evaluation of the scenarios is based on the output of the model. The results are compared to existing Bluetooth travel time measurements for the test stretch, which are used as ground truth observations, and a mean average percentage error (MAPE) is calculated. This leads to a most reasonable technique for the fundamental diagram calibration 9 ̆ 6 the total least square error minimization...|$|R
40|$|Mineral {{matter is}} an {{important}} component of airborne particles in urban areas. In northern cities of the world, mineral matter dominates PM 10 during spring because of enhanced road abrasion caused by the use of antiskid methods, including studded tires and traction sanding. In this study, factors that affect formation of abrasion components of springtime road dust were assessed. Effects of traction sanding and tires on concentrations, mass size distribution, and composition of the particles were studied in a test facility. Lowest particle concentrations were observed in tests without traction sanding. The concentrations increased when traction sand was introduced and continued to increase {{as a function of the}} amount of aggregate dispersed. Emissions were additionally affected by type of tire, properties of traction sand <b>aggregate,</b> and driving <b>speed.</b> <b>Aggregates</b> with high fragmentation resistance and coarse grain size distribution had the lowest emissions. Over 90 % of PM 10 was mineral particles. Mineralogy of the dust and source apportionment showed that they originated from both traction sand and pavement aggregates. The remaining portion was mostly carbonaceous and originated from tires and road bitumen. Mass size distributions were dominated by coarse particles. Contribution of fine and submicron size ranges were approximately 15 and 10 % in PM 10, respectively...|$|R
40|$|Most {{deep ocean}} carbon flux {{profiles}} show low and almost constant fluxes of {{particulate organic carbon}} (POC) in the deep ocean. However, {{the reason for the}} non-changing POC fluxes at depths is unknown. This study presents direct measurements of formation, degradation, and sinking velocity of diatom aggregates from laboratory studies performed at 15 °C and 4 °C during a three-week experiment. The average carbon-specific respiration rate during the experiment was 0. 12 ± 0. 03 at 15 °C, and decreased 3. 5 -fold when the temperature was lowered to 4 °C. No direct influence of temperature on <b>aggregate</b> sinking <b>speed</b> was observed. Using the remineralisation rate measured at 4 °C and an average particle sinking speed of 150 m d − 1, calculated carbon fluxes were similar to those collected in deep ocean sediment traps from a global data set, indicating that temperature plays a major role for deep ocean fluxes of POC...|$|R
40|$|Sinking {{velocities}} of {{more than}} 300 Nitzschia closterium aggregates were determined during roller table incubation using digital image analysis. To examine the influence of transparent exopolymer particles (TEP) on <b>aggregate</b> settling <b>speed,</b> 3 experiments with different ratios of TEP to cell volume concentration were conducted. The results showed that, for N. closterium aggregates without TEP, sinking velocity (U) {{was significantly related to}} the equivalent spherical diameter (ESD) of the aggregates, yielding U (cm s- 1) = 1. 89 (ESD, cm) 0. 55. The higher was the specific TEP content of an aggregate, the lower was the sinking velocity and the less pronounced was the size versus velocity relationship. Excess densities (Δρ) of aggregates were derived from velocity measurements and 3 -dimensional fractal dimensions (D 3) of aggregates were calculated from scaling properties of Δρ. Values for D 3 never exceeded 2 and fit well to values of the 2 -dimensional fractal dimension (D 2) attained from image analysis...|$|R
40|$|We {{consider}} traffic-update mobile {{applications that}} let users learn traffic conditions based on reports from other users. These applications {{are becoming increasingly}} popular (e. g., Waze reported 30 million users in 2013) since they aggregate real-time road traffic updates from actual users traveling on the roads. However, the providers of these mobile services have access to such sensitive information as timestamped locations and movements of its users. In this paper, we de-scribe Haze, a protocol for traffic-update applications that supports the creation of traffic statistics from user reports while protecting {{the privacy of the}} users. Haze relies on a small subset of users to jointly <b>aggregate</b> encrypted <b>speed</b> and alert data and report the result to the service provider. We use jury-voting protocols based on threshold cryptosys-tem and differential privacy techniques to hide user data from anyone participating in the protocol while allowing only aggregate information to be extracted and sent to the ser-vice provider. We show that Haze is effective in practice by developing a prototype implementation and performing experiments on a real-world dataset of car trajectories. 1...|$|R
40|$|This paper {{develops}} a dynamic general-equilibrium model of capital adjustments under monopolistic competition. Investments are partially irreversible. The model includes microfoundations for consumption decisions and capital-adjustment strategies. The {{effects of the}} model parameters on the optimal capital-adjustment strategy are determined analytically. A major {{result is that the}} aggregate net investment is proportional to the difference between the desired and previous <b>aggregate</b> capital. The <b>speed</b> of adjustment decreases with the cost of reversibility, is invariant to the shares of labor and capital, and increases with the level of macroeconomic uncertainty. However, the latter effect is not quantitatively important. (Copyright: Elsevier) New investment theory; Macroeconomic uncertainty; Trigger strategy; Aggregate dynamics...|$|R
40|$|We {{address the}} problem of {{automatically}} constructing basis functions for linear approximation of the value function of a Markov Decision Process (MDP). Our work builds on results by Bertsekas and Castañon (1989) who proposed a method for automatically <b>aggregating</b> states to <b>speed</b> up value iteration. We propose to use neighborhood component analysis (Goldberger et al., 2005), a dimensionality reduction technique created for supervised learning, in order to map a high-dimensional state space to a lowdimensional space, based on the Bellman error, or on the temporal difference (TD) error. We then place basis function in the lower-dimensional space. These are added as new features for the linear function approximator. This approach is applied to a high-dimensional inventory control problem. 1...|$|R
40|$|The {{production}} by batch cold-mix technique of foam bitumen-based bound graded aggregates is studied at laboratory scale. The influence of some mixing parameters (mixing system, mixing time, shaft <b>speed,</b> <b>aggregates</b> temperature and foam characteristics) and bitumen content is analyzed. An original developed test permits {{a measure of}} the mixture microstructure. A twin-shaft mixer gives better results than a planetary mixer. The mixture temperature during mixing, resulting from aggregates and foam temperatures, seems being the most influent mixing parameter. It favours the dispersion of bitumen in smaller particles of mastic. It is shown that cohesion of compacted material, measured by direct compression test, is highly dependent on the grading of mastic particle before compaction...|$|R
40|$|The {{objective}} {{of the study was}} to gain in-depth knowledge of speed relationships for urban streets. The speed characteristics were examined using a number of methods for data collection. Throughout the research, a special focus was placed on capturing the influence on driver speed of interactions with pedestrians, cyclists and other road users, called sidefriction events in this study. First, driver behaviour and travel time data was collected from field and driving simulator studies for a range of street types and traffic conditions. The collected data was used to calibrate a microscopic traffic simulation model. Production runs with this model were performed for various traffic conditions. Second, <b>aggregated</b> <b>speed</b> data was collected at the link level, i. e. the macro level, for three street types. In combination with street site variables, speed and flow data was analysed using multiple regression techniques with space mean speed as dependent variable. This analysis was also performed for average travel speed data produced by microscopic traffic simulation. Two central results were attained and utilized for the model development: - In-depth knowledge of which factors influence speed choice on urban street links with minor intersections, on a micro and macro level. - A comprehensive research methodology for study of speed characteristics on urban streets in which the knowledge gained at the micro and macro level was applied. Results from the micro study showed that Average number of crossing pedestrians and Traffic flow had significant impact on average travel speed (R 2 = 0. 91). Results from the macro study performed for three street types showed that Street function and Number of lanes also had a high degree of explanation (R 2 close to 0. 70). The variables Separated bicycle lane, Roadside parking permitted and Number of minor intersections per 1 km were significant for some of the street types modelled in the macro study. The variables Ratio of through vehicles and Gender of the driver were also investigated and were found not to influence space-mean speed. The macro study demonstrated that speed choice and driver behaviour were consistent for each street type investigated regardless of city type and population size. The speed-flow relationships of the micro model for an urban street type showed good agreement with the macro model for traffic flows in the upper range. In conclusion, the research effort showed that the included side-friction variables added explanatory value to the estimation of speed, and thus can enhance the knowledge of traffic impacts of different urban street designs. QC 2010063...|$|R
40|$|Abstract—Integrated {{circuits}} {{for very}} high-speed telecommu-nication protocols often use ASICs, {{due to their}} strict timing constraints. This scenario is changing, since modern FPGAs, implemented in 65 or 45 nm technologies achieve high operating frequencies, and serializer/deserializer hardwired modules enable the reception of high <b>speed</b> <b>aggregated</b> rates (e. g. 10 Gbps or more), spanning the input stream for internal parallel computation. This paper presents a complete solution for an Optical Transport Network framer using FPGA devices. The framer receives a 10 Gbps stream originated from optical fiber medium, extracts its payload information, and transmits payload data at 10 Gbps. A working prototype was implemented in Virtex- 4 and Virtex- 5 devices. (Abstract) Keywords-FPGA, OTN (Optical Transport Network), Telecommunication Circuits, Framer (key words) I...|$|R
40|$|Recent {{efforts to}} add new {{services}} to the Internet have increased interest in software-based routers {{that are easy to}} extend and evolve. This paper describes our experiences using emerging network processors [...] -in particular, the Intel IXP 1200 [...] -to implement a router. We show it is possible to combine an IXP 1200 development board and a PC to build an inexpensive router that forwards minimumsized packets at a rate of 3 # 47 Mpps. This is nearly an order of magnitude faster than existing pure PC-based routers, and sufficient to support 1 # 77 Gbps of aggregate link bandwidth. At lesser <b>aggregate</b> line <b>speeds,</b> our design also allows the excess resources available on the IXP 1200 to be used robustly for extra packet processing. For example, with 8 # 100 Mbps links, 240 register operations and 96 bytes of state storage are available for each 64 -byte packet. Using a hierarchical architecture we can guarantee line-speed forwarding rates for simple packets with the IXP 1200, and still have extra capacity to process exceptional packets with the Pentium. Up to 310 Kpps of the traffic can be routed through the Pentium to receive 1510 cycles of extra per-packet processing. 1...|$|R
