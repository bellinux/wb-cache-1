0|10000|Public
50|$|Operating <b>automated</b> <b>filing</b> and {{recovery}} <b>system</b> and technology.|$|R
50|$|When {{the work}} evolves to a stage where it {{requires}} an external review or {{is ready to}} be pushed on to the next process, the user checks in his or her files. This check-in process hands files off to TACTIC, which acts as a gatekeeper to the repository where all the files are kept. TACTIC also acts as a librarian, sorting the files it receives into the appropriate location in the repository. If specified by the <b>system,</b> <b>file</b> names will also be renamed accordingly to make their locations more identifiable (a function cognate to card cataloguing). TACTIC's <b>automated</b> <b>file</b> management <b>system</b> ensures that <b>files</b> are uniformly named and appear in predictable and reliable locations, effectively eliminating human error in the repository.|$|R
40|$|This {{handbook}} is a {{guide for}} the use of all personnel engaged in handling NASA files. It is issued in accordance with the regulations of the National Archives and Records Administration, in the Code of Federal Regulations Title 36, Part 1224, Files Management; and the Federal Information Resources Management Regulation, Subpart 201 - 45. 108, Files Management. It is intended to provide a standardized classification and filing scheme to achieve maximum uniformity and ease in maintaining and using agency records. It is a framework for consistent organization of information in an arrangement that will be useful to current and future researchers. The NASA Uniform Files Index coding structure is composed of the subject classification table used for NASA management directives and the subject groups in the NASA scientific and technical information system. It is designed to correlate files throughout NASA and it is anticipated that it may be useful with <b>automated</b> <b>filing</b> <b>systems.</b> It is expected that in the conversion of current files to this arrangement {{it will be necessary to}} add tertiary subjects and make further subdivisions under the existing categories. Established primary and secondary subject categories may not be changed arbitrarily. Proposals for additional subject categories of NASA-wide applicability, and suggestions for improvement in this handbook, should be addressed to the Records Program Manager at the pertinent installation who will forward it to the NASA Records Management Office, Code NTR, for approval. This handbook is issued in loose-leaf form and will be revised by page changes...|$|R
40|$|The present {{invention}} {{provides a}} fail-over file transfer process to handle data file transfer when the transfer is unsuccessful {{in order to}} avoid unnecessary network congestion and enhance reliability in an <b>automated</b> data <b>file</b> transfer <b>system.</b> If a <b>file</b> cannot be delivered after attempting to send the file to a receiver up to a preset number of times, and the receiver has indicated the availability of other backup receiving locations, then the file delivery is automatically attempted to one of the backup receiving locations up to the preset number of times. Failure of the file transfer to one of the backup receiving locations results in a failure notification being sent to the receiver, and the receiver may retrieve the file from the location indicated in the failure notification when ready...|$|R
50|$|On April 23, 1996, GroupLogic {{announced}} MassTransit (MT), {{a secure}} managed <b>file</b> transfer (MFT) <b>system</b> {{for business and}} other institutional customers. MassTransit operates as an <b>automated</b> <b>file</b> transfer solution and as an end user tool allowing the easy and secure transfer of digital content of all sizes, up to 100GB.|$|R
40|$|The {{paradigm}} of Ubiquitous computing seeks {{to build a}} computing environment that responds to user context. An ideal <b>file</b> <b>system</b> for the Ubiquitous environment is one that can successfully recognize the present context and <b>automate</b> <b>file</b> management. The intelligence in the Ubiquitous file management is achieved by applying a heuristics based clustering approach to the system. The applied heuristics {{are those that are}} used on file attributes by users to manually manage files in a traditional <b>file</b> <b>system.</b> <b>File</b> attributes can be used to relate files to the most appropriate work-context and also draw inter-file relationships. We discuss methods to harness the given file information from the file-system to form a context-relation wrapper over disparate files. This enables management of files as context related working sets rather than as individual files. A survey was conducted among regular computer users and the compiled results supported our context based file clustering approach. The experiments also showed promising results that confirm our model on the file heuristics, thus finding semantic relations between those files...|$|R
5000|$|Schedule and <b>automate</b> <b>file</b> {{transfers}} from <b>automated</b> <b>systems</b> and repositories ...|$|R
5000|$|Ingest content via Ethernet or FTP and <b>automate</b> <b>file</b> {{transfer}} to servers ...|$|R
5000|$|Each iForem Virtual Vault also {{includes}} tools to manage documents and <b>automate</b> <b>file</b> operations: ...|$|R
5000|$|Switch, a {{workflow}} tool which integrates and <b>automates</b> <b>file</b> {{handling and}} third party software.|$|R
5000|$|<b>Automate</b> <b>file</b> {{transfer}} processes between {{trading partners}} and exchanges including detection and handling of failed file transfers.|$|R
50|$|<b>File</b> <b>system</b> types can be {{classified}} into disk/tape <b>file</b> <b>systems,</b> network <b>file</b> <b>systems</b> and special-purpose <b>file</b> <b>systems.</b>|$|R
40|$|From its inception, UNIX {{has been}} built around two {{fundamental}} entities: processes and files. In this chapter, {{we look at the}} implementation of files in Solaris and discuss the framework for <b>file</b> <b>systems.</b> 14. 1 <b>File</b> <b>System</b> Framework Solaris OS includes a framework, the virtual <b>file</b> <b>system</b> framework, under which multiple <b>file</b> <b>system</b> types are implemented. Earlier implementations of UNIX used a single <b>file</b> <b>system</b> type for all of the mounted <b>file</b> <b>systems,</b> typically, the UFS <b>file</b> <b>system</b> from BSD UNIX. The virtual <b>file</b> <b>system</b> framework was developed to allow Sun’s distributed computing <b>file</b> <b>system</b> (NFS) to coexist with the UFS <b>file</b> <b>system</b> in SunOS 2. 0; it became a standard part of System V in SVR 4 and Solaris OS. We can categorize Solaris <b>file</b> <b>systems</b> into the following types: Storage-based. Regular <b>file</b> <b>systems</b> that provide facilities for persistent storage and management of data. The Solaris UFS and PC/DOS <b>file</b> <b>systems</b> are examples. Network <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that provide <b>files</b> that are accessible in a local directory structure but are stored on a remote network server; for example, NFS. Pseudo <b>file</b> <b>systems.</b> <b>File</b> <b>systems</b> that present various abstractions as files in a <b>file</b> <b>system.</b> The /proc pseudo <b>file</b> <b>system</b> represents the address space of a process as a series of files. 657 658 Chapter 14 <b>File</b> <b>System</b> Framework The framework provides a single set of well-defined interfaces that are <b>file</b> <b>system</b> independent; the implementation details of each <b>file</b> <b>system</b> are hidden behind these interfaces. Two key objects represent these interfaces: the virtual file, or vnode, and the virtual <b>file</b> <b>system,</b> or vfs objects. The vnode interfaces implement file-related functions, and the vfs interfaces implement <b>file</b> <b>system</b> management functions. The vnode and vfs interfaces direct functions to specific <b>file</b> <b>systems,</b> {{depending on the type of}} <b>file</b> <b>system</b> being operated on. Figure 14. 1 shows the <b>file</b> <b>system</b> layers. File-related functions are initiated through a system call or from another kernel subsystem and are directed to the appropriate <b>file</b> <b>system</b> by the vnode/vfs layer...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis investigates measures of effectiveness (MOE) and defines the data elements for an automated USMC repair parts initial provisioning evaluation system. Twenty-three specific MOEs, applicable to any new weapon system, are proposed from five general criteria categories: weapon system readiness, supply support, cost, essentiality and range/depth. Then, each MOE is examined for practical implementation potential by identifying and/or modifying data elements resident in USMC <b>automated</b> <b>files.</b> To {{assist in the}} database programming of MOEs, Appendices B through E define and cross-reference the MOEs, <b>automated</b> <b>files</b> and data elements. [URL] United States Marine Corp...|$|R
50|$|Distributed <b>file</b> <b>systems</b> can be {{optimized}} for different purposes. Some, {{such as those}} designed for internet services, including GFS, are {{optimized for}} scalability. Other designs for distributed <b>file</b> <b>systems</b> support performance-0intensive applications usually executed in parallel. Some examples include: MapR <b>File</b> <b>System</b> (MapR-FS), Ceph-FS, Fraunhofer <b>File</b> <b>System</b> (BeeGFS), Lustre <b>File</b> <b>System,</b> IBM General Parallel <b>File</b> <b>System</b> (GPFS), and Parallel Virtual <b>File</b> <b>System.</b>|$|R
50|$|For example, {{to migrate}} a FAT32 <b>file</b> <b>system</b> to an ext2 <b>file</b> <b>system.</b> First {{create a new}} ext2 <b>file</b> <b>system,</b> then copy the data to the <b>file</b> <b>system,</b> then delete the FAT32 <b>file</b> <b>system.</b>|$|R
5000|$|Blue Whale Clustered <b>file</b> <b>system</b> (BWFS) is {{a shared}} disk <b>file</b> <b>system</b> (also called {{clustered}} <b>file</b> <b>system,</b> shared storage <b>file</b> <b>systems</b> or SAN <b>file</b> <b>system)</b> made by Tianjin Zhongke Blue Whale Information Technologies Company in China.|$|R
5000|$|Virtual <b>file</b> <b>system</b> (VFS): A VFS is a <b>file</b> <b>system</b> used to {{help the}} user to hide the {{different}} <b>file</b> <b>systems</b> complexities. A user can use the same standard <b>file</b> <b>system</b> related calls to access different <b>file</b> <b>systems.</b>|$|R
5000|$|Use default {{settings}}. Default {{settings are}} defined per <b>file</b> <b>system</b> at the <b>file</b> <b>system</b> level. For ext3 <b>file</b> <b>systems</b> {{these can be}} set with the tune2fs command. The normal default for Ext3 <b>file</b> <b>systems</b> is equivalent to (no acl support). Modern Red Hat based systems set acl support as default on the root <b>file</b> <b>system</b> but not on user created Ext3 <b>file</b> <b>systems.</b> Some <b>file</b> <b>systems</b> such as XFS enable acls by default. Default <b>file</b> <b>system</b> mount attributes can be overridden in /etc/fstab.|$|R
40|$|Abstract. HFS+ <b>file</b> <b>system</b> is a <b>file</b> <b>system</b> of the Mac OS. In {{order to}} achieve data {{manipulation}} of the <b>file</b> <b>system</b> based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ <b>file</b> <b>system,</b> but also propose a efficient method to analyze the <b>file</b> <b>system.</b> Research contains the exploration of the <b>file</b> <b>system</b> and program implementation to analyze the <b>file</b> <b>system...</b>|$|R
40|$|With the {{emergence}} of Storage Networking, distributed <b>file</b> <b>systems</b> that allow data sharing through shared disks will become vital. We refer to Cluster <b>File</b> <b>Systems</b> as a distributed <b>file</b> <b>systems</b> optimized for environments of clustered servers. The requirements such <b>file</b> <b>systems</b> is that they guarantee <b>file</b> <b>systems</b> consistency while allowing shared access from multiple nodes in a shared-disk environment. In this paper we evaluate three approaches for designing a cluster <b>file</b> <b>system</b> - conventional client/server distributed <b>file</b> <b>systems,</b> symmetric shared <b>file</b> <b>systems</b> and asymmetric shared <b>file</b> <b>systems.</b> These alternatives are considered by using our prototype cluster <b>file</b> <b>system,</b> HAMFS (Highly Available Multi-server <b>File</b> <b>System).</b> HAMFS is classified as an asymmetric shared <b>file</b> <b>system.</b> Its technologies are incorporated into our commercial cluster <b>file</b> <b>system</b> product named SafeFILE. SafeFILE offers a disk pooling facility that supports off-the-shelf disks, and balances file load across these disks automatically and dynamically. From our measurements, we identify the required disk capabilities, such as multi-node tag queuing. We also identify the advantages of an asymmetric shared <b>file</b> <b>system</b> over other alternatives...|$|R
50|$|There {{are various}} User Mode <b>File</b> <b>System</b> (FUSE)-based <b>file</b> <b>systems</b> for Unix-like {{operating}} systems (Linux, etc.) {{that can be}} used to mount an S3 bucket as a <b>file</b> <b>system.</b> Note that as the semantics of the S3 <b>file</b> <b>system</b> are not that of a Posix <b>file</b> <b>system,</b> the <b>file</b> <b>system</b> may not behave entirely as expected.|$|R
50|$|Other Unix virtual <b>file</b> <b>systems</b> {{include the}} <b>File</b> <b>System</b> Switch in System V Release 3, the Generic <b>File</b> <b>System</b> in Ultrix, and the VFS in Linux. In OS/2 and Microsoft Windows, the virtual <b>file</b> <b>system</b> {{mechanism}} {{is called the}} Installable <b>File</b> <b>System.</b>|$|R
40|$|Abstract—Researches on {{technologies}} about testing {{aggregate bandwidth}} of <b>file</b> <b>systems</b> in cloud storage systems. Through the memory <b>file</b> <b>system,</b> network <b>file</b> <b>system,</b> parallel <b>file</b> <b>system</b> theory analysis, {{according to the}} cloud storage system polymerization bandwidth and concept, developed to cloud storage environment <b>file</b> <b>system</b> polymerization bandwidth test software called FSPoly. In this paper, use FSpoly to luster <b>file</b> <b>system</b> testing, find reasonable test methods, and then evaluations latest development in cloud storage <b>system</b> <b>file</b> <b>system</b> performance by using FSPoly. Keywords-cloud storage, aggregate bandwidth, <b>file</b> <b>system,</b> performance evaluation I...|$|R
40|$|We propose and {{evaluate}} an approach for decoupling persistent-cache management from general <b>file</b> <b>system</b> design. Several distributed <b>file</b> <b>systems</b> maintain a persistent cache {{of data to}} speed up accesses. Most of these <b>file</b> <b>systems</b> retain complete control over various aspects of cache management, such as granularity of caching, and policies for cache placement and eviction. Hardcoding cache management into the <b>file</b> <b>system</b> often results in sub-optimal performance as the clients of the <b>file</b> <b>system</b> are prevented from exploiting information about their workload in order to tune cache management. We introduce xCachefs, a framework that allows clients to transparently augment the cache management of the <b>file</b> <b>system</b> and customize the caching policy based on their resources and workload. xCachefs {{can be used to}} cache data persistently from any slow <b>file</b> <b>system</b> to a faster <b>file</b> <b>system.</b> It mounts over two underlying <b>file</b> <b>systems,</b> which can be local disk <b>file</b> <b>systems</b> like Ext 2 or remote <b>file</b> <b>systems</b> like NFS. xCachefs maintains the same directory structure as in the source <b>file</b> <b>system,</b> so that disconnected reads are possible when the source <b>file</b> <b>system</b> is down. ...|$|R
40|$|In this note, we {{introduce}} a simple <b>file</b> <b>system</b> implementation, known as vsfs (the Very Simple <b>File</b> <b>System).</b> This <b>file</b> <b>system</b> is a simplified {{version of a}} typical UNIX <b>file</b> <b>system</b> and thus serves to introduce {{some of the basic}} on-disk structures, access methods, and policies that you will find in many <b>file</b> <b>systems</b> today. The <b>file</b> <b>system</b> is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the <b>file</b> <b>system</b> work better (though we will want to pay attention to device characteristics to make sure the <b>file</b> <b>system</b> works well). Because of the great flexibility we have in building a <b>file</b> <b>system,</b> many different ones have been built, literally from AFS (the Andrew <b>File</b> <b>System)</b> to ZFS (Sun’s Zettabyte <b>File</b> <b>System).</b> All of these <b>file</b> <b>systems</b> have different data structures and and do some things better or worse than their peers. Thus, the way we will be learning about <b>file</b> <b>systems</b> is through case studies: first, a simple <b>file</b> <b>system</b> (vsfs) in this chapter to introduce most concepts, and then a series of studies of real <b>file</b> <b>systems</b> to understand how they can differ in practice...|$|R
5000|$|FFS2, Unix <b>File</b> <b>System,</b> Berkeley Fast <b>File</b> <b>System,</b> the BSD Fast <b>File</b> <b>System</b> or FFS ...|$|R
40|$|File Allocation Table (FAT) <b>file</b> <b>system</b> is {{the most}} common <b>file</b> <b>system</b> used in {{embedded}} devices such as smart phones, digital cameras, smart TVs, tablets, etc. Typically these embedded devices use Solid State Drives (SSD) as storage devices. The ExFAT <b>file</b> <b>system</b> is future <b>file</b> <b>system</b> for embedded devices and it is optimal for SSDs. This paper discourses the methodologies for Geotagging as a <b>file</b> <b>system</b> metadata instead of file data in FAT and ExFAT <b>file</b> <b>systems.</b> The designed methodologies of this paper adheresthe compatibility with the FAT <b>file</b> <b>system</b> specification and existing ExFAT <b>file</b> <b>system</b> implementations...|$|R
40|$|Abstract—As <b>file</b> <b>system</b> {{capacities}} {{reach the}} petascale, {{it is becoming}} increasingly difficult for users to organize, find, and manage their data. <b>File</b> <b>system</b> search has the potential to greatly improve how users manage and access files. Unfortunately, existing <b>file</b> <b>system</b> search is designed for smaller scale systems, making it difficult for existing solutions to scale to petascale <b>files</b> <b>systems.</b> In this paper, we motivate the importance of <b>file</b> <b>system</b> search in petascale <b>file</b> <b>systems</b> and present a new fulltext <b>file</b> <b>system</b> search design for petascale <b>file</b> <b>systems.</b> Unlike existing solutions, our design exploits <b>file</b> <b>system</b> properties. Using a novel index partitioning mechanism that utilizes <b>file</b> <b>system</b> namespace locality, we are able to improve search scalability and performance and we discuss how such a design can potentially improve search security and ranking. We describe how our design can be implemented within the Ceph petascale <b>file</b> <b>system.</b> I...|$|R
40|$|HDFS is a {{distributed}} <b>file</b> <b>system</b> {{designed to}} hold very large amounts of data (terabytes or even petabytes), and provide high-throughput access to this information. Files are stored in a redundant fashion across multiple machines to ensure their durability to failure and high availability to very parallel applications. This paper includes the step by step introduction to the <b>file</b> <b>system</b> to distributed <b>file</b> <b>system</b> and to the Hadoop Distributed <b>File</b> <b>System.</b> Section I introduces What is <b>file</b> <b>System,</b> Need of <b>File</b> <b>System,</b> Conventional <b>File</b> <b>System,</b> its advantages, Need of Distributed <b>File</b> <b>System,</b> What is Distributed <b>File</b> <b>System</b> and Benefits of Distributed <b>File</b> <b>System.</b> Also the analysis of large dataset and comparison of mapreducce with RDBMS, HPC and Grid Computing communities have been doing large-scale data processing for years. Sections II introduce the concept of Hadoop Distributed <b>File</b> <b>System.</b> Lastly section III contains Conclusion followed with the References...|$|R
40|$|In {{this paper}} we {{describe}} an architecture for extensible <b>file</b> <b>systems.</b> The architecture enables {{the extension of}} <b>file</b> <b>system</b> functionality by composing (or stacking) new <b>file</b> <b>systems</b> on top of existing <b>file</b> <b>systems.</b> A <b>file</b> <b>system</b> that is stacked {{on top of an}} existing <b>file</b> <b>system</b> can access the existing <b>file</b> <b>system's</b> <b>files</b> via a well-defined naming interface and can share the same underlying file data in a coherent manner. We describe extending <b>file</b> <b>systems</b> {{in the context of the}} Spring operating <b>system.</b> Composing <b>file</b> <b>systems</b> in Spring is facilitated by basic Spring features such as its virtual memory architecture, its strongly-typed well-defined interfaces, its location-independent object invocation mechanism, and its flexible naming architecture. <b>File</b> <b>systems</b> in Spring can reside in the kernel, in user-mode, or on remote machines, and composing them can be done in a very flexible manner...|$|R
50|$|Modern {{journaling}} <b>file</b> <b>systems</b> for Amiga are the Smart <b>File</b> <b>System</b> (SFS) and Professional <b>File</b> <b>System</b> (PFS).|$|R
5000|$|File systems: High Reliability <b>File</b> <b>System</b> (HRFS), FAT-based <b>file</b> <b>system</b> (DOSFS), Network <b>File</b> <b>System</b> (NFS), and TFFS ...|$|R
40|$|This paper {{analyzes}} algorithms for <b>automated</b> {{placement of}} <b>file</b> replicas in the Farsite [3] system, using both theory and simulation. In the Farsite distributed <b>file</b> <b>system,</b> multiple replicas of files are stored on multiple machines, so that files {{can be accessed}} even {{if some of the}} machines are down or inaccessible. The purpose of the placement algorithm is to determine an assignment of file replicas to machines that maximally exploits the availability provided by machine...|$|R
40|$|The Hurricane <b>File</b> <b>System</b> (HFS) {{is a new}} <b>file</b> <b>system</b> being {{developed}} for large-scale shared memory multiprocessors with distributed disks. The main goal of this <b>file</b> <b>system</b> is scalability; that is, the <b>file</b> <b>system</b> is designed to handle demands {{that are expected to}} grow linearly with the number of processors in the system. To achieve this goal, HFS is designed using a new structuring technique called Hierarchical Clustering. HFS is also designed to be flexible in supporting a variety of policies for managing file data and for managing <b>file</b> <b>system</b> state. This flexibility is necessary to support in a scalable fashion the diverse workloads we expect for a multiprocessor <b>file</b> <b>system.</b> 1 Introduction The Hurricane <b>File</b> <b>System</b> (HFS) is a new <b>file</b> <b>system</b> {{being developed}} for large-scale shared memory multiprocessors. In this paper the goals and basic architecture of this <b>file</b> <b>system</b> are introduced. The main goal of this <b>file</b> <b>system</b> is scalability; we expect the <b>file</b> <b>system</b> load to grow linearly [...] ...|$|R
40|$|Conference Name:International Conference on Materials Science and Engineering Science. Conference Address: Shenzhen, PEOPLES R CHINA. Time:DEC 11 - 12, 2010. HFS+ <b>file</b> <b>system</b> is a <b>file</b> <b>system</b> of the Mac OS. In {{order to}} achieve data {{manipulation}} of the <b>file</b> <b>system</b> based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ <b>file</b> <b>system,</b> but also propose a efficient method to analyze the <b>file</b> <b>system.</b> Research contains the exploration of the <b>file</b> <b>system</b> and program implementation to analyze the <b>file</b> <b>system...</b>|$|R
