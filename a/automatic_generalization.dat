29|24|Public
40|$|<b>Automatic</b> <b>generalization</b> is {{a process}} for {{representing}} geographical objects with different degrees of detail on a digital map. The positional error for each geographical object is propagated through the process and a generalization error is also introduced by the generalization. Previous research has focused mainly on measuring the generalization error. This paper presents an analytical model for assessing the positional error in the generalized object by considering both error propagation from the original data and the generalization error. The analytical model provides a shape dissimilarity value that indicates the shape difference between the original data with a positional error and its simplified version. This model is able to objectively and automatically determine {{the applicability of the}} generalized data for further applications to geographical information system (GIS) problems. It can also deal with a large amount of data in GIS. Therefore, the analytical model presented, which provides a more comprehensive shape measure for assessing positional error in data derived from the generalization, is valuable in the development of <b>automatic</b> <b>generalization.</b> Department of Land Surveying and Geo-Informatic...|$|E
40|$|The {{problem of}} generalizing spatial data is {{studied in the}} context of land cover data. The theory behind <b>automatic</b> <b>generalization</b> is {{reviewed}} and a raster based deterministic iterative generalization method is proposed. The methodology is based on the Map Algebra. The multivariate statistical testing methods are extended to deal also with generalized land cover data. The case study is related to the production of a European CORINE Land Cover map from Finland. It presents an implementation of a new methodological concept for land cover data production, supervised classification and <b>automatic</b> <b>generalization.</b> In the case study, the existing supervised classification is supported with digital maps and attribute databases. All input data is combined to a detailed land cover with a very small minimum feature size, and automatically generalized to the standard European Land Cover. According to the quality tests performed, the automatically generalized method used here meets the present quality specifications. The method is fast, and it gives an opportunity for multiple data products in various scales, optimized for different purposes...|$|E
40|$|The <b>automatic</b> <b>generalization</b> of 3 D {{building}} models {{has been}} a topic of research for almost a decade. Several approaches for the simplification of single objects have been proposed and shown to be valid. Such models of low geometric detail are needed for maplike presentation. In this paper, a generalization algorithm is presented {{that is based on}} the decomposition of space along the major planes of the building. In contrast to previous publications, {{the focus is on the}} mathematical description of the approach. 1...|$|E
40|$|We propose <b>automatic</b> <b>generalizations</b> of the KPSS-test for {{the null}} {{hypothesis}} of stationarity of a univariate time series. We can use these tests for the null hypotheses of trend stationarity, level stationarity and zero mean stationarity. We introduce the asymptotic null distributions and we determine consistency against relevant nonstationary alternatives. We compare {{the properties of the}} tests with those of other proposed tests for stationarity. Monte Carlo simulations support the relevance of the tests when an autoregressive process with large positive autocorrelations is likely under {{the null hypothesis}}...|$|R
40|$|Since {{the concept}} was {{proposed}} {{at the beginning of}} last century, cartography generalization has been gradually changed from full manual operation mode to computer automation mode after decades of improvement and development. However, the level of map automation is still relatively low until now. So, this paper puts forward an integrated representation of geographical model for entity and symbol on the basic of innovative model units and spatial relation model. Then the automatic methods of topological compensation for model entity and cartography compensation for model symbolization are established, we want to explore a solution to solve the worldwide problem of <b>automatic</b> cartography <b>generalization.</b> Finally, this paper look into the future and developmental direction of <b>automatic</b> cartography <b>generalization...</b>|$|R
40|$|We propose <b>automatic</b> <b>generalizations</b> of the KPSS-test for {{the null}} {{hypothesis}} of stationarity of a univariate time series. We can use the tests for the null hypotheses of trend stationarity, level stationarity and zero mean stationarity. We derive the asymptotic null distributions and we determine the rates of consistency against relevant nonstationary alternatives. We compare {{the properties of the}} tests with those of other recently proposed tests for stationarity. Theoretical results and Monte Carlo simulations support the relevance of the tests when an autoregressive process with large positive autocorrelations is likely under {{the null hypothesis}}. Econometric Institute Report, no. 9802 #A [URL] Keywords I# 0 # test, rate of consistency, long run variance, heteroskedasticity and autocorrelation consistent covariance estimation, Choi's test, Leybourne and McCabe's test JEL Code C 22, C 12 Acknowledgements We thank In Choi and Benedikt P#otscher for providing us with useful references...|$|R
40|$|The {{process of}} {{performing}} cartographic generalization in an automatic way applied on geographic information is of highly {{interest in the}} field of cartography, both in academia and industry. Many research e↵orts have been done to implement di↵erent <b>automatic</b> <b>generalization</b> approaches. Being able to answer the research question on <b>automatic</b> <b>generalization,</b> another interesting question opens up: ”Is it possible to retrieve and visualize geographic information in any arbitrary scale?” This is the question {{in the field of}} vario-scale geoinformation. Potential research works should answer this question with solutions which provide valid and efficient representation of geoinformation in any on-demand scale. More brilliant solutions will also provide smooth transitions between these on-demand arbitrary scales. Space-Scale-Cube (Meijers and Van Oosterom 2011) is a reactive tree (Van Oosterom 1991) data structure which shows positive potential for achieving smooth automatic vario-scale generalization of area features. The topic of this research work is investigation of adaptation of this approach on an interesting class of geographic information: road networks datasets. Firstly theoretical background will be introduced and discussed and afterwards, implementing the adaptation would be described. This research work includes development of a hierarchical data structure based on road network datasets and the potential use of this data structure in vario-scale geoinformation retrieval and visualization...|$|E
40|$|The {{development}} of remote sensing have provided higher resolution images so {{it made the}} task of observation, analysis of surface and produce maps faster. But to produce topographic maps from these images is needed vectoring procedures. When you need to build maps on smaller scales, such as screens cell, for example, problems arise from congestion of symbols because several features became overlap and collapse. This makes the representation cartographic confuse and affects their communication. This problem is solved {{by a set of}} operations called cartographic generalization. The procedure executed in this research consisted of: collection of image and its georeferencing, vectorization of the buildings, and application of cartographic generalization. In this paper were evaluated methods of <b>automatic</b> <b>generalization,</b> more specifically with respect to operators simplification and aggregation with different parameters. Tests were performed modifying the input parameters of the programs <b>automatic</b> <b>generalization.</b> The data were vectorized in scale of 1 : 1000 and these were represented after generalization in the scales of 1 : 2000 and 1 : 5000. The best result reached resembles the result produced through the generalization which is considered a manual and subjective process. Here it was arbitrated all distances {{less than or equal to}} 1 meter would be aggregated or simplified. Pages: 2476 - 248...|$|E
40|$|Abstract. Recent {{developments}} in computational terminology {{call for the}} design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes {{to bridge the gap between}} term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. Hypernym links acquired through an information extraction procedure are projected on multi-word terms through the recognition of semantic variations. The induced hierarchy is incomplete but provides an <b>automatic</b> <b>generalization</b> of singleword terms relations to multi-word terms that are pervasive in technical thesauri and corpora...|$|E
40|$|In <b>automatic</b> cartographic <b>generalization,</b> {{the most}} {{important}} problem which must be resolved {{is to keep the}} equivalence of spatial relations between spatial objects, these spatial relations are distance relation, directional relation, topological relations and so on. Among all these spatial relations, topological relation is considered as the relation that can reflect the best spatial information, so it should be the dominant relation. Because of this, {{it is important to keep}} the equivalence of topological relations in the progress of <b>automatic</b> progressive cartographic <b>generalization.</b> In this paper, the combinational reasoning method is used to represent topological relations, and the equivalence of topological relations in the process of cartographic generalization is discussed. Besides these, a method for topological relation abstraction is presented. Combining with some rules to deal with spatial relations in cartographic generalization, equivalence evaluation model of topological relations in spatial scene is established...|$|R
50|$|As GIS {{developed}} from about the late 1960s onward, {{the need for}} <b>automatic,</b> algorithmic <b>generalization</b> techniques became clear. Ideally, agencies responsible for collecting and maintaining spatial data should try to keep only one canonical representation of a given feature, at the highest possible level of detail. That way {{there is only one}} record to update when that feature changes in the real world. From this large-scale data, it should ideally be possible, through automated generalization, to produce maps and other data products at any scale required. The alternative is to maintain separate databases each at the scale required for a given set of mapping projects, each of which requires attention when something changes in the real world.|$|R
40|$|Summary: In {{recent years}} {{national}} mapping agencies have increasingly integrated <b>automatic</b> map <b>generalization</b> methods in their production lines. This {{raises the question}} of how to assess and assure the quality of mapping products such as digital landscape models. Generalization must not only en-sure specified standards for an output scale, but also needs to keep semantics as similar as possible under these given requirements. In order to allow for objective comparisons of different generaliza-tion results we introduce a semantic distance measure. We present results that optimize this measure subject to constraints reflecting database specifications and show how this measure can be used to compare the results of different methods, including exact and heuristic approaches. Zusammenfassung: Gewährleistung logischer Konsistenz und semantischer Genauigkeit in de...|$|R
40|$|As {{one of the}} key {{techniques}} in multi-representation database, GIS, mapmaking and Digital Earth, automatic cartographic generalization has been intensively researched since decades. Current developments worldwide have been concentrated on the efforts towards adaptive and easily embeddable <b>automatic</b> <b>generalization</b> modules or services. Based on an analysis of the common rules concerning the quality issues that a cartographic generalization system has to obey, this paper put forward an “automatic cartographic generalization chain”. This chain integrates cartographic generalization knowledge, algorithms, operations, real-time monitoring, quality assessment and process control together. With examples, we show how to apply the common rules in an automatic cartographic generalization system. 1...|$|E
40|$|Abstract: From {{traditional}} cartography {{to digital}} and information cartography, the cartography in china has made great development in 62 years since 1949. This report summarized the achievement in cartography and press techniques, <b>automatic</b> <b>generalization</b> of digital map, atlas, GIS software, visualization of geographic information and VGE techniques, spatial data mining and knowledge discovery, uncertainty of spatial data and cartography theory. Based on summarize, {{the direction of}} development is put forward by 6 fields: assimilation of spatial data, geo-analysis and spatial data mining, geographic information services based on internet, intelligence of spatial data generalization, integration of GIS and VGE and the theory cartography based on ‘multi-pattern’, ‘space –time’, ‘integration ’ cognition model...|$|E
40|$|In {{this paper}} we {{present the results}} and {{evaluation}} of an <b>automatic</b> <b>generalization</b> process based on a raster-vector data model applied to nine urban city block maps when reducing scale from S 25 k (1 / 25000 scale map) to S 50 K (1 / 50000 scale map). Results of the process {{can not be considered}} as definitive but rather incomplete of the presence of feature conflicts. Nevertheless this process give us greater objectivity than the purely interactive production process. The fact that less time is needed for edition and conflict resolution reduces the human workload by a quantity of up to 50 % when compared with current vector processes. ...|$|E
40|$|Aiming at multi-scale {{representation}} of spatial data {{not supported by}} R-tree, a transmutation index structure of R-tree used for multi-scale {{representation of}} spatial data is proposed: (1) Spatial objects may appear at non-leaf nodes; (2) Depth of the tree is made use for reflecting change of spatial resolutions, and resolution dimensions is supported; (3) Structure of the tree's branches supports algorithms of <b>automatic</b> cartography <b>generalization.</b> Query of spatial datas among multiple scales of based on the R-tree's transmutation index structure is analyzed, and constraint conditions, insert algorithms and divide algorithms among creations of the index structure are emphasized. For the same data source, compared with multi-scale index based on quadtree for spatial data, experiments prove that the transmutation structure can search spatial data organized by multi-resolution and memorize generalization results with high efficiency...|$|R
40|$|Abstract. This paper {{proposes a}} Domotic OSGi Gateway (DOG) able to expose {{different}} domotic networks as a single, technology neutral, home automation system. The {{adoption of a}} standard framework such as OSGi, and of sophisticated modeling techniques stemming from the Semantic Web research community, allows DOG to go beyond simple automation and to support reasoning-based intelligence inside home environments. By exploiting the DogOnt ontology for <b>automatic</b> device <b>generalization,</b> syntactic and semantic command validation, and internetwork scenario definition, DOG provides the building blocks for evolving current, isolated, home automation plants into so-called Intelligent Domotic Environments, where heterogeneous devices and domotic systems are coordinated to behave as a single, intelligent, proactive system. The paper introduces the DOG architecture by looking at functionalities provided by each of its components and by describing features that exploit ontology-modeling. ...|$|R
40|$|This paper moves a {{first step}} towards the {{creation}} of Intelligent Domotic Environments (IDE) in real-life home-living. A new Domotic OSGi Gateway (DOG) is presented, able to manage different domotic networks as a single, technology neutral, home automation system. The adoption of a standard framework such as OSGi, and of sophisticated modeling techniques stemming from the Semantic Web research community, allows DOG to go beyond simple automation and to support reasoning-based intelligence inside home environments. By exploiting <b>automatic</b> device <b>generalization,</b> syntactic and semantic command validation, and internetwork scenario definition, DOG provides the building blocks for supporting the evolution of current, isolated, home automation plants into IDEs, where heterogeneous devices and domotic systems are coordinated to behave as a single, intelligent, proactive system. The paper introduces the DOG architecture and the underlying ontology modeling. A case study is also illustrated, where DOG controls a laboratory reconstruction of a simple domotic environmen...|$|R
40|$|The <b>automatic</b> <b>generalization</b> {{processes}} consist {{to derive}} less detailed spatial data from data too detailed. The {{objective of this}} process is to provide the user a spatial data adapted to those needs. Several approaches are proposed to automate this process. These approaches include the agent-based approach. But the central problem of this approach is the selection of the optimal action performed by the agent in a given moment. In this paper, we proposed an approach that can optimize this process by satisfying the cartographic constraints. Our approach consist to provide agents geographic, genetic patrimony, to enable them choosing the optimal action, from where the concept of genetic agent...|$|E
40|$|The paper {{shows the}} results {{achieved}} by the research group of the University of Cagliari in the field mapping <b>automatic</b> <b>generalization.</b> Particularly the work concerned this objectives: - study of {{a model for the}} generalization of geographical data from 1 : 10. 000 to 1 : 25. 000, 1 : 50. 000, 1 : 100. 000 and 1 : 250. 000; - development of the model for automatic generalization; - test on digital cartography; - verification of the results achieved in terms of precision, of comprehensibility and truthfulness. The process is mostly carried out according to auto and semi-automatic procedures within an ESRI ArcGIS software platform, version 8. 1...|$|E
40|$|This article {{presents}} {{a novel approach}} for the <b>automatic</b> <b>generalization</b> of 3 D building models with regard to a cartographic 3 D visualization. Here, the {{goal is not to}} render realistic, but rather visually appealing images of an urban scene. The geometric simplification is realized by remodeling the object by means of a process similar to half space modeling. Approximating planes are determined from the polygonal faces of the original model, which are then used as space dividing primitives to create façade and roof structures that are of simpler shape. In order to improve the final shape of the simplified building models, form optimization steps like hole filling, fragment removal and vertex contraction are further used. 1...|$|E
40|$|Abstract — This paper moves a {{first step}} towards the {{creation}} of Intelligent Domotic Environments (IDE) in real-life home-living. A new Domotic OSGi Gateway (DOG) is presented, able to manage different domotic networks as a single, technology neutral, home automation system. The adoption of a standard framework such as OSGi, and of sophisticated modeling techniques stemming from the Semantic Web research community, allows DOG to go beyond simple automation and to support reasoning-based intelligence inside home environments. By exploiting <b>automatic</b> device <b>generalization,</b> syntactic and semantic command validation, and inter-network scenario definition, DOG provides the building blocks for supporting the evolution of current, isolated, home automation plants into IDEs, where heterogeneous devices and domotic systems are coordinated to behave as a single, intelligent, proactive system. The paper introduces the DOG architecture and the underlying ontology modeling. A case study is also illustrated, where DOG controls a laboratory reconstruction of a simple domotic environment 1. Index Terms — System architectures, integration and modeling, Ubiquitous computing, Rule-based processing, Knowledge modeling...|$|R
40|$|Nowadays Spatial Databases {{are gaining}} more {{importance}} {{for being the}} base of National Information Systems which forces National Map Agencies to convert existing data structures {{that have been used}} only for map productions for many years into Spatial Databases. Türkiye upgraded map production system framework from the cartographer-aided method to the computer aided way in early 1990 s. The upgrade process was completed in 1999. Up to now, 2200 sheets of Cartographic Vector Maps were produced and plotted. In 2001, an <b>automatic</b> cartographic <b>generalization</b> project started to produce 1 : 100 000 scale cartographic vector maps. Therefore, a study on the existing data structure and data quality had to be done in order to establish infrastructure of the Generalization Project in General Command of Mapping (GCM) in Türkiye. In this paper, we try to explain, how GCM surmounted such an enterprise framework study and created a geodatabase, by using huge amount of tile-based cartographic vector data, covering almost half of Türkiye...|$|R
40|$|Pigeons' key pecks were {{reinforced}} in {{the presence}} of pictures from one of two categories, cats or cars. A single picture associated with reinforcement was used in Experiment 1, and 20 pictures from the same category were associated with reinforcement in Experiment 2. Pigeons then were presented with novel test pictures from the training category and from the other, previously unseen, category. During Session 1 of testing, pigeons pecked no more often at pictures from the reinforced category than at pictures from the previously unseen category. When pigeons were trained with pictures associated with reinforcement or its absence from different categories in Experiment 3, differential responding to novel pictures from different categories appeared during Session 1. These findings argue against a process of <b>automatic</b> stimulus <b>generalization</b> within natural categories and in favor of the position that category distinctions are not made until members of at least two categories are compared with one another...|$|R
40|$|One of {{the primary}} {{problems}} in knowledge representation and learning is determining how multiple instances of concepts should be organized and represented. Symbolic approaches, such as semantic networks, have been successful at representing structured knowledge for parallel access. However, such approaches have had difficulty organizing multiple instances for <b>automatic</b> <b>generalization</b> and efficient retrieval. Parallel distributed processing systems (PDP) appear to offer a solution to these problems. Unfortunately, current PDP models {{have not yet been}} able to satisfactorily represent complex knowledge structures and they remain sequential at the knowledge level. This paper presents an approach which stores multiple instances in ensembles of PDP units and organizes the ensembles in a semantic network for parallelism and structure. Thus, the best features of both styles of representation are obtained. ...|$|E
40|$|Because of {{the limited}} space in densely {{populated}} the Netherlands, careful land use planning, incorporating the views of all stakeholders, is of utmost importance. Dutch law recognizes two types of physical plan: structure plans and land use plans. A land use plan has a legal status and indicates what is (not) allowed on a parcel of land, whereas a structure plan is only indicative for possible future land use development. Map displays {{play an important role}} in the communication of physical planning information and since only recently the Dutch law prescribes that physical planning maps should be digital (instead of printed on paper). Therefore, physical planning maps will increasingly become available in digital, interactive, dynamic and exchangeable format for consultation by various user groups in a national web portal. Physical planning maps cannot do without a topographic base. As in many other thematic mapping applications use is often made of already existing base map layers, but, ideally, the topographic base should be better adjusted to purpose and use, scale of representation and the planning information layer that is projected on it. This can be obtained by good map design, but particularly also by an appropriate systematic cartographic generalization, based on the intended use of the map. This paper describes two user research projects (focus group and on-line survey) that were executed in the framework of a larger investigation of the <b>automatic</b> <b>generalization</b> of base maps for physical planning maps. The results of the user research can be used for steering a possible <b>automatic</b> <b>generalization</b> process, starting from an existing topographic database. Examples are a priority listing for the selection of elements of the topographic base of different physical planning maps at different resolutions and directions for mutual harmonization with the cartographic elements of the planning information layer. 1...|$|E
40|$|The {{generalization}} {{process works}} on independent image-objects formed by a closed-boundary and a corresponding enclosed-region on the classified image. Initial image-objects are obtained by integrating the classified image with a geographically corresponding satellite-derived edge-segmented image.;The generalization process is organised in two main levels of abstraction. Firstly Geometric generalization, {{responsible for the}} spatial and thematic simplification of the classified image, elaborating the initial image-objects in to higher-level polygons (the spatial basis of the final product).;Secondly Semantic generalization, responsible for the thematic conversion of the simplified product, associating each higher-level polygon to the most appropriate land use class.;The CORINE land cover classification scheme was taken as the target product during this thesis. The classification scheme is however overly detailed for direct comparison with satellite-derived products. To overcome this an intermediate classification was defined in this thesis: Pseudo CORINE (Pcor), which is a 1 -level scheme containing: bottom-level CORINE classes which are automatically recognisable by image processing techniques, and 2 nd-level CORINE classes as substitutes for those CORINE classes not automatically recognisable.;The definition of the Pcor scheme allowed an automatic nomenclature conversion, organised in two steps: 1) re-labelling, based on syntax matching, of low-level classes presenting a one-to-one relationship with a single Pcor class. 2) contextual reasoning, based on mutually exclusive hierarchical rules, for the conversion of low-level classes which present a one-to-many relationship with Pcor classes.;A fully <b>automatic</b> <b>generalization</b> process has been developed and verified during this work. The <b>automatic</b> <b>generalization</b> process has produced generalized products which are in excellent agreement with the target CORINE map. The simplification of geometry {{and content of the}} input information based on image-statistics and contextual rules is fully automatic, unsupervised, consistent, objective, repeatable and generally applicable...|$|E
40|$|As a {{main part}} of {{nautical}} chart generalization, sounding generalization {{is also one of}} the bottlenecks in the way of <b>automatic</b> chart <b>generalization.</b> Because submarine topography is mostly represented by sounding on chart, to some extent, generalization of chart mainly means sounding selection. Even in manual generalization, it is one of the most difficult and time-consuming links in the production chain, not to mention in the automated charting procedures. In this paper, author proposed a new theoretical model, which based on recognition of topographic features. With the aid of Delaunay triangulation, the information, which contains the topology, distribution character, distribution density, can all be gained. All the methods mentioned in the paper have been proved to be feasible and sound. 1. Rules of Sounding Selection The major rules for sounding selection include: Soundings which are at the top of a raised under-water area (shoals, rocks, etc.) have the highest priority to be selected...|$|R
40|$|A {{program of}} {{research}} in the field of artificial intelligence is presented. The research areas discussed include automatic theorem proving, representations of real-world environments, problem-solving methods, the design of a programming system for problem-solving research, techniques for general scene analysis based upon television data, and the problems of assembling an integrated robot system. Major accomplishments include {{the development of a new}} problem-solving system that uses both formal logical inference and informal heuristic methods, the development of a method of <b>automatic</b> learning by <b>generalization,</b> and the design of the overall structure of a new complete robot system. Eight appendices to the report contain extensive technical details of the work described...|$|R
40|$|Recurrent Cascade-Correlation (RCC) is a {{recurrent}} {{version of the}} Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections {{are added to the}} network one at a time, as they are needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good <b>generalization,</b> <b>automatic</b> construction of a near-minimal multi-layered network, and the ability to learn complex behaviors through a sequence of simple lessons. The power of RCC is demonstrated on two tasks: learning a finite-state grammar from examples of legal strings, and learning to recognize characters in Morse code...|$|R
40|$|Recent {{developments}} in computational terminology {{call for the}} design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes {{to bridge the gap between}} term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. First, we present a system for corpus-based acquisition of terminological relationships through discursive patterns. This system is built on previous work on automatic extraction of hyponymy links through shallow parsing. Second, we show how hypernym links between single-word terms can be extended to semantic links between multi-word terms through corpus-based extraction of semantic variants. The induced hierarchy is incomplete but provides an <b>automatic</b> <b>generalization</b> of single-word terms relations to multi-word terms that are pervasive in technical thesauri and corpora...|$|E
40|$|Cartographic {{generalization}} {{has been}} a research topic for more than thirty years. In spite of recent advances, a complete automation of this process still faces fundamental problems. Furthermore, recent needs such as web mapping and Spatial On-Line Analytical Processing (SOLAP) require geospatial data that are generated on-the-fly at any scale. This requires processing times and a degree of automation that today's <b>automatic</b> <b>generalization</b> algorithms cannot reach with all data types. This paper presents the concept of geometric patterns and explains {{how they can be}} built and stored in a pattern database for further use in a map generalization process. Geometric patterns are typical shapes that are representative of several occurrences of a cartographic object class. They support simple operations such as rotation, translation or stretch and allow adapting them to the exact geometry of individual objects. 1...|$|E
40|$|In {{this paper}} we discuss how to define in a {{mathematical}} framework the set of operations that cartographers call generalization. First we make {{a clear distinction between}} topological and metric aspects of maps, and describe suitable mathematical tools for treating both cathegories. Then we analyse the basic operations of generalization, pointing out whether each one affects the metric or topological structure of a map. Finally we discuss how generalization operations can fit into the described mathematical framework. This is intended as a step towards a global approach to <b>automatic</b> <b>generalization.</b> Keywords: map generalization, topology, shape, abstract cell complex, homotopy. 1 - Introduction It is well known that the structure of maps can be suitably represented by means of planar graphs, possibly with dangling edges, multiple edges, self-loops and isolated vertices. Semantic information is associated with points, edges and faces [3, 6, 7]. The problem of representing and automaticall [...] ...|$|E
40|$|Abstract: 2 ̆ 2 Recurrent Cascade-Correlation (RCC) is a {{recurrent}} {{version of the}} Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections {{are added to the}} network one at a time, as they are needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good <b>generalization,</b> <b>automatic</b> construction of a near-minimal multi-layered network, and the ability to learn complex behaviors through a sequence of simple lessons. The power of RCC is demonstrated on two tasks: learning a finite- state grammar from examples of legal strings, and learning to recognize characters in Morse code. 2 ̆...|$|R
40|$|We {{develop a}} {{procedure}} for analyzing multivariate nonstationary time series using the SLEX library (smooth localized complex exponentials), {{which is a}} collection of bases, each basis consisting of waveforms that are orthogonal and time-localized versions of the Fourier complex exponentials. Under the SLEX framework, we build a family of multivariate models that can explicitly characterize the timevarying spectral and coherence properties. Every model has a spectral representation in terms of a unique SLEX basis. Before selecting a model, we first decompose the multivariate time series into nonstationary components with uncorrelated (nonredundant) spectral information. The best SLEX model is selected using the penalized log energy criterion, which we derive in this article to be the Kullback–Leibler distance between a model and the SLEX principal components of the multivariate time series. The model selection criterion takes into account all of the pairwise cross-correlation simultaneously in the multivariate time series. The proposed SLEX analysis gives results that are easy to interpret, because it is an <b>automatic</b> time-dependent <b>generalization</b> of the classical Fourier analysis of stationary time series. Moreover, the SLEX method uses computationally efficient algorithms and hence is able to provide a systematic framework for extracting spectral features from a massive dataset. We illustrate the SLEX analysis with an application to a multichannel brain wave dataset recorded during an epileptic seizure...|$|R
40|$|Index Terms [...] - fuzzy utility, fuzzy Cournot equilibrium, fuzzy {{abstract}} economies, qualitative reasoning I. FUZZY CHOICE AND INTELLIGENT AGENTS E LECTRONIC commerce allows {{buyers and}} sellers to use increasingly complicated decision procedures. Large amounts of information are available, and computers can process this to advise the economic agent {{in the choice of}} an alternative. The information gathered, over the web for example, is often inconsistent. If it is to be used in decision making, the decision making process will {{have to be able to}} deal with uncertainty. Humans deal with uncertainty in a natural way, via <b>generalization.</b> <b>Automatic</b> procedures use either probability theory or fuzzy logic. We prefer fuzzy logic because it deals with linguistic variables in a more intuitive way that probability theory. The linguistic variables are classes that have evolved over time, in a certain application domain, to be effective in generalization. Linguistic variables are the method adopted by humans to indicate choice and preference...|$|R
